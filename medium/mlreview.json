[
    {
        "url": "https://medium.com/mlreview/multi-modal-methods-part-one-49361832bc7e?source=---------0",
        "title": "Multi-Modal Methods: Visual Speech Recognition (Lip Reading)",
        "text": "Previous work from the team detailed some of the many advancements within the field of Computer Vision. In practice, research isn\u2019t siloed into isolated fields and, with this in mind, we present a short exploration of an intersection between Computer Vision (CV) and Natural Language Processing (NLP) \u2014 namely, Visual Speech Recognition, also more commonly known as lip reading.\n\nIt was not so long ago that lip reading was heralded to be a difficult problem, much like the difficulty ascribed to the game of Go; albeit not quite as well-known. In addition to solving this problem, advancements in lip reading may potentially enable several new applications. For instance, dictating messages in a noisy environment, dealing with multiple simultaneous speakers better, and improving the performance of speech recognition systems in general. Conversely, extracting conversations from video alone may be an area of concern in the future.\n\nOur focus on this niche application, one hopes, is both illustrative and informative. A relatively small body of deep learning work on lip reading was enough to upset the traditional primacy of the expertly-trained lip reader. Meanwhile, the combinatorial nature of AI research and the technologies at the centre of these advancements blend the demarcations between fields in a scintillating way. Where, if ever, such advancements plateau is the question on everyone\u2019s lips.\n\nIrrespective of the bar set by the expert, we think it best to delve into what makes this a tough challenge to master. Visemes, analogous to the lip-movements that comprise a lip reading alphabet, pose a clear challenge to those who\u2019ve ever attempted to apply them. Namely, that multiple sounds share the same shape. There exists a level of ambiguity between consonants, which cannot be dispensed with \u2014 a problem well documented by Fisher in his extensive study on visemes [8].\n\nSince there are only so many shapes that one\u2019s mouth can make in articulation, mapping said shapes accurately to the underlying words is challenging [10]. Especially when much communication relies more on sound than on visual information; vocal communication is sound-dependent. Hence, achieving high accuracy without the context of the speech [11] is extremely difficult \u2014 for people and machines.\n\nWith these limitations it\u2019s not surprising that early studies focused on simplified versions of the problem. Initially, feature engineering produced improvements using facial recognition models which placed bounding boxes around the mouth, and extracted a model of the lips independent from the orientation of the face. Some common features used were the width-height ratio of a bounding box for detecting mouths, the appearance of the tongue (pixel intensity in the red channel of the image) and an approximation of the amount of teeth from the \u2018whiteness\u2019 in the image [12].\n\nThese approaches obtained impressive results (over 70% word accuracy) for tests performed with classifiers trained on the same speaker they were tested on. But performance was heavily damaged when trying to lip read from individuals not included in the training set. Lip detection in males with moustaches was also more difficult and, therefore, the performance on such cases was poor. Hence, the feature engineering approaches, while an improvement, ultimately failed to generalise well.\n\nFollowing this, using different viseme classification methods with defined language models improved state of the art (SOTA) performance.[14] Language models help filter results that are obviously incorrect and improve results by selecting from only plausible options, e.g. \u2019n\u2019 for the 4th character in \u201csoon\u201d rather than \u201csoow\u201d or \u201csoog\u201d. Greater improvements still were made by \u201cfine-tuning\u201d the viseme classifier for phoneme classification, which enabled them to deal with multiple possible solutions for words containing the same visemes in similar intervals. This improved accuracy and performed comparatively better than previous approaches.\n\nMcGurk and MacDonald argue in their 1976 paper[16] that speech is best understood as bimodal, that is taking both visual and audio inputs \u2014 and that comprehension in individuals may be compromised if either of these two domains are absent. Intuitively, many of us can recall mishearing speech while on the phone, or the difficulties one has in pairing sound and lips in a noisy environment. The requirement of bimodal inputs, as well as contextual constraints, hampers the ability of people and machines to read lips with accuracy. This pointed to the need for further studies on the use of these combined information sources. A direction which brings us into the most recent epoch of lip reading approaches.\n\nOn a high level in the architecture, the frames extracted from a video sequence are processed in small sets within a Convolutional Neural Network (CNN),[23] while an LSTM-variant runs on the CNN output sequentially to generate output characters. More precisely, a 10-frame sequence is grouped together in a block (width x height x 10), sequence length may vary, but the consecutive nature of these frames creates a Spatiotemporal CNN.\n\nThen the output of this LSTM-variant, called a Gated Recurrent Unit (GRU),[24] is processed by a multi-layered perceptron (MLP) to output values for the different characters derived from the Spatiotemporal CNN. Lastly, a Connectionist Temporal Classification (CTC) provides final processing on the sequence outputs to make it more intelligible in terms of precise outputs, i.e. words and sentences. This approach allows information to be passed through the time periods comprising both words and, ultimately, sentences, improving the accuracy of network predictions.\n\nLipNet also makes use of an additional algorithm typically used in speech recognition systems \u2014 a Connectionist Temporal Classification (CTC) output. After the classification of framewise characters, which in combination with more characters define an output sequence, CTC can group the probabilities of several sequences (e.g. \u201cc__aa_tt\u201d and \u201cccaaaa__t\u201d) into the same word candidates (in this case \u201ccat\u201d) for the final sentence prediction. Thus the algorithm is alignment-free. CTC solves the problem of matching sequences where timing is variable.\n\nBy predicting the alphabet characters and an additional \u201c_\u201d (space) character, it\u2019s possible to generate a word prediction by removing repeated letters and empty spaces, as can be seen in fig. 5 for the classification of the word \u201cplease\u201d. In practical terms this means that elongated pronunciations, variations in emphasis and timings, as well as pauses between syllables and words can still produce consistent predictions using the CTC for outputs.\n\nCTC is a function for output alignment and a loss correction function based on that alignment, and is independent of the CNN and LSTM-variants. One can also think of CTC as similar to a softmax due to converting the raw output of a network (e.g. raw class scores or in our case, characters) into the expected output (e.g. a probability distribution or in this case, words and sentences). CTC makes matching a single character output to word level possible. Awni Hannun provides an excellent dynamic publication that explains CTC operation; available here.[29]\n\nA hallmark of this method is that the output labels are not conditioned on each other. For example, the letter \u2018a\u2019 in \u2018cat\u2019 is not conditioned on \u2018c\u2019 or \u2018t\u2019. Instead this relation is extracted by three spatio-temporal convolutions, followed by two GRUs which process a set number of the input images. The output from the GRUs then goes through a MLP to compute CTC loss (see fig. 6).\n\nNot long after LipNet, DeepMind released \u2018Lip Reading Sentences in the Wild\u2019, [33] and addressed some of the concerns around LipNet\u2019s generalisability. Taking inspiration from both CNNs for visual feature extraction[34] and the use of LSTMs for speech transcription,[35] the authors present an innovative approach to the problem of lip reading. By adding individual attention mechanisms for each of the input types, and combining them afterwards to produce character outputs, improvements in both the accuracy and generalisability of the original LipNet architecture were realised.\n\nAttention mechanisms have been an enabler of some the recent success within deep learning; due to more efficient and clever processing of data. It also allows these models to have more interpretability, i.e. if asking why a network thinks a certain image is a dog it is often hard to look at and understand the internals of the network to find out why. Attention allows the network to highlight the salient parts of the image used in its prediction, e.g. a snout and pointed ears. Attention has become such a common technique that it spawned papers like \u201cattention is all you need\u201d, which foregoes convolution and recurrence techniques entirely for the problem of machine translation.\n\nReturning to \u201cLip Reading Sentences in the Wild\u201d, Chung et al. (2017) present their WLAS Network. Composed of three main submodules (watch, listen spell) \u2014 with attention sprinkled into the spell module. The system is as follows:\n\nWatch is a VGG-M[38] that extracts a framewise feature representation to be consumed by an LSTM, which generates a state vector and an output signal. The Watch module looks at each frame in the video and extracts the relevant features that the module has learned to look for, i.e. certain lip movements/positions. This is done by a regular VGG-M CNN which outputs a feature representation for each frame.\n\nThis sequence of feature representations are then fed into a regular LSTM which generates a state vector (or cell state) and an output signal. With LSTMs and GRUs there\u2019s an output and a \u201cstate\u201d input to the next LSTM cell. The output is a character prediction (or a probability distribution of predicted character), while state is what encodes \u201cthe past\u201d, i.e. what an LSTM has computed/stored of the past which is used to predict the next output.\n\nThe Listen module uses the Mel-frequency cepstral coefficients (MFCCs)[39] as its input. These parameters define a representation of the short-term power spectrum of a sound based on signal transformations. MFCCs ensure transformations are scaled to a frequency which simulates the human hearing range. Following this, independent attention mechanisms in the Attend module for each of the audio and video inputs are combined. These are then in turn passed through the Spell module. With a multi-layered perceptron (MLP) at each time step, the output from the LSTM ends up in a softmax to define the probabilities of the output characters.\n\nChung et al. (2017) created a pipeline to automatically generate the dataset(s)[41] from BBC recordings as well as from the contained closed captions, which enabled progress in a data-intensive research area. Their creation is a \u2018Lip Reading Sentences\u2019 (LRS) dataset for visual speech recognition, consisting of over 100,000 natural sentences from British television.\u2019\n\nThe authors also corrupt said datum with storm noises (i.e. weather storms[42]), demonstrating the network\u2019s ability to use distorted and low volume data, or to discard audio completely for prediction purposes. Determining whether there\u2019s value to the prediction in listening or not. For those wishing to see more, Joon Son Chung presents a fantastic overview of the authors\u2019 work at CVPR.[43]\n\nCurious as to what would follow the approaches detailed previously, we turn our attention to some of the most recent work in this space. Although not exhaustive, here\u2019s a smattering of the best improvements we came across in this domain:"
    },
    {
        "url": "https://medium.com/mlreview/making-sense-of-the-bias-variance-trade-off-in-deep-reinforcement-learning-79cf1e83d565?source=---------1",
        "title": "Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning",
        "text": "Making Sense of the Bias / Variance Trade-off in (Deep) Reinforcement Learning What goes into a stable, accurate reinforcement signal?\n\nSince the launch of the ML-Agents platform a few months ago, I have been surprised and delighted to find that thanks to it and other tools like OpenAI Gym, a new, wider audience of individuals are building Reinforcement Learning (RL) environments, and using them to train state-of-the-art models. The ability to work with these algorithms, previously something reserved for ML PhDs, is opening up to a wider world. As a result, I have had the unique opportunity to not just write about applying RL to existing problems, but also to help developers and researchers debug their models in a more active way. In doing so, I often get questions which come down to a matter of understanding the unique hyperparameters and learning process around the RL paradigm. In this article, I want to attempt to highlight one of these conceptual pieces: bias and variance in RL, and attempt to demystify it to some extent. My hope is that in doing so a greater number of people will be able to debug their agent\u2019s learning process with greater confidence.\n\nMany machine learning practitioners are familiar with the traditional bias-variance trade-off. For those who aren\u2019t, it goes as follows: on the one hand, a \u201cbiased\u201d model generalizes well, but doesn\u2019t fit the data perfectly (\u201cunder-fitting\u201d). On the other hand, a high-variance model fits the training data well, too well in-fact, to the detriment of generalization (\u201coverfitting\u201d). In this situation, the problem becomes one of limiting the capacity of a model with some regularization method. In many cases, dropout or L2 regularization with a large enough data set is enough to do the trick. That is the story for typical supervised learning. RL is a little different, as it has its own separate bias-variance trade-off which operates in addition to, and at a higher level than the typical ML one.\n\nIn RL, bias and variance no longer just refer to how well the model fits the training data, as in supervised learning, but also to how well the reinforcement signal reflects the true reward structure of the environment. To understand that statement, we have to backup a little. In reinforcement learning, instead of a set of labeled training examples to derive a signal from, an agent receives a reward at every decision-point in an environment. The goal of an agent is to learn a policy (method for taking actions) which will lead to obtaining the greatest reward over time. We must do this using only the individual rewards that agent receives, without the help of an outside oracle to designate what count as \u201cgood\u201d or \u201cbad\u201d actions.\n\nA naive approach to an RL learning algorithm would be to encourage actions which were associated with positive rewards, and discourage actions associated with negative rewards. Instead of updating our agent\u2019s policy based on immediate rewards though, we often want to account for actions (and the states of the environment when those actions were taken) which lead up to rewards. For example, imagine walking down a corridor to a rewarding object. It isn\u2019t just the final step we want to perform again, but all the steps up to that rewarding one. There are a number of approaches for doing this, all of which involving doing a form of credit assignment. This means giving some credit to the series of actions which led to a positive reward, not just the most recent action. This credit assignment is often referred to as learning a value estimate: V(s) for state, and Q(s, a) for state-action pair.\n\nWe control how rewarding past actions and states are considered to be by using a discount factor (\u03b3, ranging from 0 to 1). Large values of \u03b3 lead to assigning credit to states and actions far into the past, while a small value leads to only assigning credit to more recent states and actions. In the case of RL, variance now refers to a noisy, but on average accurate value estimate, whereas bias refers to a stable, but inaccurate value estimate. To make this more concrete, imagine a game of darts. A high-bias player is one who always hits close to the target, but is always consistently off in some direction. A high-variance player, on the other hand, is one who sometimes hits the target, and is sometimes off, but on average near the target.\n\nThere is a multitude of ways of assigning credit, given an agent\u2019s trajectory through an environment, each with different amounts of variance or bias. Monte-Carlo sampling of action trajectories as well as Temporal-Difference learning are two classic algorithms used for value estimation, and both are prototypical examples of methods which are variance and bias heavy, respectively.\n\nIn Monte-Carlo (MC) sampling, we rely on full trajectories of an agent acting within an episode of the environment to compute the reinforcement signal. Given a trajectory, we produce a value estimate R(s, a) for each step in the path by calculating a discounted sum of future rewards for each step in the trajectory. The problem is that the policies we are learning (and often the environments we are learning in) are stochastic, which means there is a certain level of noise to account for. This stochasticity leads to variance in the rewards received in any given trajectory. Imagine again the example with the reward at the end of the corridor. Given that an agent\u2019s policy might be stochastic, it could be the case that in some trajectories the agent is able to walk to the rewarding state at the end, and in other trajectories it fails to do so. These two kinds of trajectories would provide very different value estimates, with the former suggesting the end of the corridor is valuable, and the latter suggesting it isn\u2019t. This variance is typically mitigated by using a large number of action trajectories, with the hope that the variance introduced in any one trajectory will be reduced in aggregate, and provide an estimate of the \u201ctrue\u201d reward structure of the environment.\n\nOn the other end of the spectrum is one-step Temporal Difference (TD) learning. In this approach, the reward signal for each step in a trajectory is composed of the immediate reward plus a learned estimate of the value at the next step. By relying on a value estimate rather than a Monte-Carlo rollout there is much less stochasticity in the reward signal, since our value estimate is relatively stable over time. The problem is that the signal is now biased, due to the fact that our estimate is never completely accurate. In our corridor example, we might have some estimate of the value of the end of the corridor, but it may suggest that the corridor is less valuable than it actually is, since our estimate may not be able to distinguish between it and other similar unrewarding corridors. Furthermore, in the case of Deep Reinforcement Learning, the value estimate is often modeled using a deep neural network, making things worse. In Deep Q-Networks for example, the Q-estimates (value estimates over actions) are computed using an old copy of the network (a \u201ctarget\u201d network), which will provide \u201colder\u201d Q-estimates, with a very specific kind of bias, relating to the belief of an outdated model.\n\nNow that we understand bias and variance and their causes, how do we address them? There are a number of approaches which attempt to mitigate the negative effect of too much bias or too much variance in the reward signal. I am going to highlight a few of the most commonly used approaches in modern systems such as Proximal Policy Optimization (PPO), Asynchronous Advantage Actor-Critic (A3C), Trust Region Policy Optimization (TRPO), and others.\n\nOne of the most common approaches to reducing the variance of an estimate is to employ a baseline which is subtracted from the reward signal to produce a more stable value. Many of the baselines chosen fall into the category of Advantage-based Actor-Critic methods, which utilize both an actor which defines the policy, and a critic (often a parameterized value estimate) which provides a more reduced variance reward signal to update the actor. The thinking goes that variance can simply be subtracted out from a Monte-Carlo sample (R/Q) using a more stable learned value function V(s) in the critic. This value function is typically a neural network, and can be learned using either Monte-Carlo sampling, or Temporal difference (TD) learning. The resulting Advantage A(s, a) is then the difference between the two estimates. This advantage estimate has the other nice property of corresponding to how much better the agent actually performed than was expected on average, thus allowing for intuitively interpretable values.\n\nWe can also arrive at advantage functions in other ways than employing a simple baseline. For example, the value function can be applied to directly smooth the reinforcement signal obtained from a series of trajectories. The Generalized Advantage Estimate (GAE), introduced by John Schulman in 2016 does just this. The GAE formulation allows for an interpolation between pure TD learning and pure Monte-Carlo sampling using a lambda parameter. By setting lambda to 0, the algorithm reduces to TD learning, while setting it to 1 produces Monte-Carlo sampling. Values in-between (particularly those in the 0.9 to 0.999 range) produce better empirical performance by trading off the bias of V(s) with the variance of the trajectory.\n\nOutside of calculating an advantage function, the bias-variance trade-off presents itself when deciding what to do at the end of a trajectory when learning. Instead of waiting for an entire episode to complete before collecting a trajectory of experience, modern RL algorithms often break experience batches down into smaller sub-trajectories, and use a value-estimate to bootstrap the Monte-Carlo signal when that trajectory doesn\u2019t end with the termination of the episode. By using a bootstrap signal, that estimate can contain information about the rewards the agent might have gotten, if it continued going to the end of the episode. It is essentially a guess about how the episode will turn out from that point onward. Take again our example of the corridor. If we are using a time horizon for our trajectories that ends halfway through the corridor, and if our value estimate reflects the fact that there is a rewarding state at the end, we will be able to assign value to the early part of the corridor, even though the agent didn\u2019t experience the reward. As one might expect, the longer the trajectory length we use, the less frequently value estimates are used for bootstrapping, and thus the greater the variance (and lower the bias). In contrast, using short trajectories means relying more on the value estimate, creating a more biased reinforcement signal. By deciding how long the trajectory needs to be before cutting it off and bootstrapping it, we can propagate the reward signal in a more efficient way, but only if we get the balance right.\n\nSay you have some environment you\u2019d like to have an agent learn to perform a task within (for example, an environment made using Unity ML-Agents). How do you decide how to control the GAE lambda and/or trajectory time horizon? The outcome of setting these hyperparameters in various ways often depends on the task, and come down to a couple of factors:\n\nUltimately, correctly balancing the trade-off comes down to a few things: gaining an intuition for the kind of problem under consideration, and knowing what hyperparameters for any given algorithm correspond to what changes in the learning process. In the case of an algorithm like PPO, this corresponds to the discount factor, GAE lambda, and bootstrapping time horizon. Below are a few guidelines which may be helpful:\n\nWith all the tweaking and tuning that often goes into the process, it can sometimes feel overwhelming, and like black magic, but hopefully, the information presented above can help contribute, even in a small way, to ensure that Deep Reinforcement Learning is a little more interpretable to those practicing it.\n\nIf you have questions about the bias-variance trade-off in RL, or if you are an RL researcher and have additional insight (or corrections) to share, please feel free to comment below!\n\nThanks to Marwan 'Moe' Mattar for the helpful feedback when reviewing a draft of this post."
    },
    {
        "url": "https://medium.com/mlreview/how-to-apply-distance-metric-learning-for-street-to-shop-problem-d21247723d2a?source=---------2",
        "title": "How to Apply Distance Metric Learning to Street-to-Shop Problem",
        "text": "How to Apply Distance Metric Learning to Street-to-Shop Problem Let\u2019s start with a definition of street-to-shop problem \u2014 identifying a fashion item in a user image and finding it in an online shop. Have you ever seen somebody in the street and thought \u201cWow, this is a nice dress, I wonder where I can buy it?\u201d I haven\u2019t. But for me, it was the cool task to try distance metric learning techniques. I hope that you will find it interesting too. Firstly, we need a dataset for it. Actually, I came to this idea after I found out that there are tons of images taken by users on Aliexpress. And I thought \u201cWow, I can make a search by image using this data, just for fun of course\u201d. I have decided to focus on women\u2019s top clothing for simplicity. Below is the list of the categories I used for scrapping: I used requests and BeautifulSoup for scrapping. Seller images can be obtained from the main page of the item, but for user\u2019s images, we need to go through feedback pages. There is a thing called \u201ccolors\u201d on item\u2019s page. Color can be just item of another color or even completely other items. So we will consider different colors as different items. You can find the code that I have used to get all information about one item (it scraps even more than we need for our task) by link https://github.com/movchan74/street_to_shop_experiments/blob/master/get_item_info.py. All we need is to go through search pages by each category, take URLs of all items and use the function above to get the info about each item. Finally, we will have two sets of images for each item: images from a seller ( field for each element in ) and images from users (field for each element in ). For each color, we have only one image from a seller, but it can be more than one image for each color from users (sometimes there are no images for color at all). Great! We got data. However, the collected dataset is noisy: There are noisy images from users (photos of package boxes, photos of texture or only some part of an item, unpacked items, unrelated photos).\n\nTo mitigate this problem I have labeled 5000 images into two categories: good images and noise images. In the beginning, my plan was to train a classifier for two categories and use it to clean dataset. But later I decided to leave this idea for future work and just added cleaned images to the test and validation sets. The second problem is there are items that sold by several sellers. Sellers even have the same images sometime (or slightly edited images). But how to deal with it? The easiest way is to do nothing and use a robust algorithm for distance metric learning. It can however affect validation because we can have the same item in the validation and training data. So it leads to a data leak. Another way is to use something to find similar (or even identical images) and merge them into one item. We can use perceptual hashing to find identical images (like phash or whash), or we can train a model on noisy data and apply the model to find similar images. I chose the second option because it allows merging even slightly edited images. One of the most popular distance metric learning method is the triplet loss: where max(x, 0) is the hinge function, d(x, y) is the distance function between x and y, F(x) is deep neural network, M is the margin, a is the anchor, p is the positive point, n is the negative point. F(a), F(p), F(n) are points in high dimensional space (embeddings) produced by a deep neural network. It is worth mentioning that the embeddings often needs to be normalized to have unit length, i.e., ||x|| = 1, in order to be robust to illumination and contrast changes and for training stability. The anchor and the positive samples belong to the same class, the negative sample is the instance of another class. So the main idea of the triplet loss is to separate embeddings of the positive pair (anchor and positive) from embeddings of the negative pair (anchor and negative) by a distance margin M. But how to select the triplet (a, p, n)? We can just randomly select samples as a triplet but it causes following problems. Firstly, there are N\u00b3 possible triplets. It means that we need a lot of time to go through all possible triplets. But actually, we don\u2019t need to do it, because after few iterations of training there will be many triplets which don\u2019t violate the triplet constraint (give zero loss). It means that these triplets are useless for a training. One of the most common way of triplet selection is hard negative mining: Selecting the hardest negatives can in practice lead to bad local minima early on in training. Specifically, it can result in a collapsed model (i.e. F(x) = 0). In order to mitigate this we can use semi-hard negative mining. Semi-hard negative samples are further away from the anchor than the positive sample but they are still hard (violate triplet constraint) because they lie inside the margin M. There are two way to generate semi-hard (and hard) negative samples: online and offline. Online means that we randomly select samples from the train dataset as a mini-batch and select triplets from samples inside it. However, we need to have a large mini-batch size for the online method. It is not possible in my case because I have only one GTX 1070 with 8Gb RAM. In the offline method we need to stop training after some time, predict embeddings for some amount of samples, select triplets and train model with these triplets. It\u2019s mean that we need to do forward pass two times but it\u2019s price for the offline method. Good! We already can start train the model with the triplet loss and offline semi-hard negative mining. But! There is always a \u201cbut\u201d in this imperfect world. We need one more trick to successfully solve street-to-shop problem. Our task is to find seller\u2019s image most similar to user\u2019s image. However, usually seller\u2019s images have much better quality (in terms on lighting, camera, position) than user\u2019s images so we have two domains: seller\u2019s images and user\u2019s images. In order to get efficient model we need to reduce a gap between these two domains. This problem is called domain adaptation.\n\nI propose a really simple technique to reduce domain gap: let\u2019s select anchors from seller\u2019s images, positive and negative samples from user\u2019s images. That\u2019s all! Simple yet effective. To implement my ideas and to do fast experimenting I have used Keras library with Tensorflow backend. I chose Inception V3 model as base CNN for my model. As usual, I initialized CNN with ImageNet weights. I have added two fully connected layers after global pooling with L2-normalization at the end of the network. The size of embedding is 128. We also need to implement the triple loss function. We pass the anchor, the positive/negative samples as single mini-batch and divide it into 3 tensors inside the loss function. The distance function is squared euclidean distance. Retrieval results. First column \u2014 query (user\u2019s image), next 5 \u2014 most similar seller\u2019s images. Performance is measured in terms of recall at K (R@K). Let\u2019s take a look how to calculate R@K. Each user\u2019s image from validation set was used as a query and we need to find the corresponding seller\u2019s image. We take one query image, calculate embedding vector and search nearest neighbors of this vector among vectors of all seller\u2019s images. We use not only seller\u2019s images from the validation set but images from the train set too because it allows to increase the number of distractors and makes our task more challenging. So we have a query image and a list of the most similar seller\u2019s images. If there is a corresponding seller image in the K most similar images then we return 1 for this query else return 0. Now we need to make it for each user\u2019s image in the validation set and find an average of scores from each query. It will be R@K. As I said before I have cleaned the small amount of user\u2019s images from noisy images. So I have measured a quality of the model on two validation datasets: full validation set and a subset of only clean images."
    },
    {
        "url": "https://medium.com/mlreview/behind-the-chat-how-e-commerce-bot-alime-works-1b352391172a?source=---------3",
        "title": "Behind the Chat: How E-commerce Robot Assistant AliMe Works",
        "text": "The majority of intelligent matching processes in use today fall into three main categories- rule-based matching, retrieval, and DL. The technology behind AliMe is based on a combination of all three.\n\nThe dialogue system is thus divided into the following strata:\n\nThis stratum identifies the underlying intention for each message, classifying them and then extracting their attributes. Since intentions determine the subsequent domain identification flow, the intention stratum is a necessary first step in initiating contextual and domain data model processes.\n\nQuestions are matched and identified to generate answers; AliMe\u2019s dialogue system employs three answering strategies according to different intentions:\n\na. FAQs such as \u201cWhat should I do if I\u2019ve forgotten my password?\u201d trigger a query on knowledge graph or retrieval model.\n\nThe knowledge graph is constructed by mining entities and phrases, the relations of which are predefined, from the vast pool of data available. Though knowledge graph-based methods accurately identify answers, they also accrue higher maintenance costs and looser initial data structures AliMe\u2019s Q&A design overcomes this by integrating traditional retrieval models.\n\nb. Tasks such as \u201cI\u2019d like to book a one-way flight from New York to Paris for tomorrow\u201d can be solved by the intention commitment + slot filing matching or deep reinforcement learning (DRL) model.\n\nc. Chitchatting, such as \u201cI\u2019m in a bad mood\u201d, pulls up a method that marries the retrieval model with deep learning (DL).\n\nThe chitchat domain mainly involves two kinds of models- the retrieval-based model and the deep generative model. The former makes selections from a fixed corpus of answers relevant to a given query, while the latter is more advanced, generating answers without relying on any corpus. The integrated merits of the two models form the core of AliMe\u2019s chat engine. First, the candidate data sets are brought up using the traditional retrieval model; then, candidate sets are re-ranked through the Seq2Seq model; the top answer candidate is chosen when the ranking score is higher than the preset threshold, failing which the seq2seq model is activated to generate an answer."
    },
    {
        "url": "https://medium.com/mlreview/using-your-idle-deep-learning-hardware-for-mining-c1b9887491fa?source=---------4",
        "title": "Using your idle Deep Learning hardware for mining \u2013 ML Review \u2013",
        "text": "Modern Deep Learning is not possible without GPUs, even simple tutorials on MNIST dataset are showing from 10..100-fold speed up running on modern GPU versus CPU. But how all those teraflops are used when you\u2019re not optimizing anything?\n\nAs Bitcoin is skyrocketing, you may consider utilizing those idle resources for something profitable. In fact, it\u2019s not that hard, what you need to do is to setup a wallet, choose what to mine, setup a miner software and run it. Just google \u201chow to start mining on gpu\u201d, there are tons of articles with detailed step-by step instructions how to do that.\n\nIn this article, I\u2019ll address to another question I was puzzled with this summer: how to make the mining convenient, automatic and non-disturbing when I suddenly want to throw all my horsepower to a new DL problem. The ideal solution would be to have some background thing constantly checking GPU utilization and when nobody is using it, just start miner. But when TensorFlow or PyTorch or anything useful wants to crunch some numbers, this monitor has to stop mining as soon as possible to free computation cores to useful tasks.\n\nDespite the simplicity of the problem, I haven\u2019t found anything similar, so I\u2019ve written such GPU monitor myself. It\u2019s generic enough to be useful not only for mining, so you can try to adapt it for something else.\n\nBefore we get into setting it up and configuring, a small word of warning: I hope that you understand that using your employer\u2019s resources for personal profit is a bad idea, so I\u2019m not responsible for any bad consequences of your utilization of my software.\n\nOk, my project called gpu_mon and source code is available here: https://github.com/Shmuma/gpu_mon. It was written on python 3 and has no dependencies besides standard library, but it is supposed to be run on Linux system, so, if you\u2019re using windows on your Deep Learning box, gpu_mon won\u2019t work, sorry.\n\nThe overall logic is exactly as I`ve just described above: it sits and periodically checks your GPUs and if nobody is using them, it runs a program you specified in config file. If some process opens the GPU device, running miner will be killed to free up resources. So, after setting everything up and starting the monitor, what you need to do is just use your GPU box as usual, overlap between the miner and the DL optimisation will be only a couple of seconds.\n\nTo get the list of processes accessing the GPU device, which assumed to be , command line tool is used. In debian-based distributions, like ubuntu or debian, it\u2019s provided by package, which, if I\u2019m not mistaken, is included in the base system installation, so, nothing needs to be installed. If your system doesn\u2019t provide (which you could check by running \u201c \u201d command in the shell), go and install it.\n\nThe whole project is configured in one single configuration file which has ini-file format and is expected to be in your home directory in file. The example configuration file is shown below and also available in project\u2019s sources.\n\nThe configuration file can contain four groups of sections:\n\nThe example of the minimalistic configuration file I use on my box with 2 GPU cards is below.\n\nThis configuration allows the fine-grained control over GPU usage by specifying individual processes for every card in the system. So, if I run DL process which occupies only the first GPU (by exporting ), the miner process started on the second GPU will continue working. But if my optimisation opens both GPU cards, both miners will be preempted and the resources will be freed.\n\nAs I\u2019ve said, everything is transparent, so, you shouldn\u2019t bother much about starting and stopping of miner process, just think about how many GPUs you\u2019re going to allocate for your DL optimisation when you\u2019re running TF or PyTorch or whatever you\u2019re using. That\u2019s basically it, below are several additional pieces of information I can provide, but now you know enough to start using gpu_mon for fun and profit.\n\nTo make gpu_mon completely botherless, we need to ensure it will be started during the system boot in the background. There are lots of ways you can start a process, but my favorite solution is using , which is a small daemon checking for processes running and restarting them if they crash. To start gpu_mon, that\u2019s enough to setup supervisord if it\u2019s not already installed and put configuration file in . Below is the configuration I use:\n\nThat\u2019s it, then you need to restart supervisord and check that gpu_mon was started by running the command: , which should return you something like this:\n\nIf gpu_mon is running as one user, but DL software can be run as another user or by multiple users, gpu_mon can fail to preempt the miner. That happens due to the security limitations in command, which doesn\u2019t show you other user processes opening the device file. If you have this scenario, but still want to use gpu_mon (you remember what I\u2019ve said about mining on your employer\u2019s resources, right?), you have two options here:\n\nThere are plenty of options available, thanks to cryptocurrency boom happening now. My personal favorite is equihash-based currencies, like ZCash or Komodo, all of them could be mined with one miner. I use modified version EWBF miner which is 10% faster than original version.\n\nAs I\u2019ve said, gpu_mon doesn\u2019t mine itself, it is just a GPU access tracker, so, you can run any CUDA-optimised miner you\u2019d like.\n\nYep, but don\u2019t expect to mine millions of dollars using one 1080 card, cryptocurrency complexity is growing. But the income from mining minus electricity bills are still positive, so, why not? There is a service which will tell you approximate mining inflow given current complexity, exchange rate and your computation power: https://whattomine.com/\n\nThat\u2019s it, get fun! If you find gpu_mon useful, don\u2019t hesitate to donate bitcoin or two to addresses here. Or, at least, give a star to the repo."
    },
    {
        "url": "https://medium.com/mlreview/modern-theory-of-deep-learning-why-does-it-works-so-well-9ee1f7fb2808?source=---------5",
        "title": "Modern Theory of Deep Learning: Why Does It Work so Well",
        "text": "Deep Learning is currently being used for a variety of different applications. But is frequently criticised for lacking a fundamental theory that can fully answer why does it work so well. It\u2019s only recently that the winner of the Test-of-Time award at the Conference on Neural Information Processing (NIPS) compared Deep Learning to Alchemy.\n\nAlthough Generalization Theory that explains why Deep Learning generalizes so well is an open problem, in this article we would discuss most recent theoretical and empirical advances in the field that attempt to explain it.\n\nAn \u201capparent paradox\u201d with Deep Learning is that it can generalize well in practice despite its large capacity, numerical instability, sharp minima, and nonrobustness.\n\nIn a recent paper \u201cUnderstanding deep learning requires rethinking generalization\u201d it was shown that Deep Neural Networks (DNN) has enough capacity to memorize ImageNet and CIFAR10 datasets with random labels. And it is unclear why they find generalizable solutions on real data.\n\nAnother important issue with deep architectures is numerical instabilities. Numerical instabilities in derivative-based learning algorithms are commonly called exploding or vanishing gradients. Additional difficulties stem from instabilities of the underlying forward model. That is the output of some networks can be unstable with respect to small perturbations in the original features. In machine learning it is called non-robustness. An example of it is an adversarial attack illustrated in Figure 1.\n\nSeveral studies based their arguments for generalization in deep learning on the flatness of minima of the loss function found by Stochastic Gradient Descent (SGD). However, recently it was shown that \u201cSharp Minima Can Generalize For Deep Nets\u201d as well. More specifically, flat minima can be turned into sharp minima via re-parameterization without changing the generalization. Consequently, generalization can not be explained merely by the robustness of parameter space only.\n\nThe goal of generalization theory is to explain and justify why and how improving accuracy on a training set improves accuracy on a test set. The difference between these two accuracies is called generalization error or \u201cgeneralization gap\u201d. More rigorously generalization gap can be defined as a difference between the non-computable expected risk and the computable empirical risk of a function f on a dataset Sm given a learning algorithm A:\n\nEssentially if we bound the generalization gap with a small value it would guarantee that Deep Learning algorithm f generalizes well in practice. Multiple theoretical bounds exist for generalization gap based on model complexity, stability, robustness etc.\n\nTwo measures of model complexity are Rademacher complexity and Vapnik\u2011Chervonenkis (VC) dimension. Unfortunately, for the deep learning function f known bounds based on Radamacher complexity grow exponentially in depth of DNN. Which contradicts practical observation that deeper nets fit better to the training data and achieve smaller empirical errors. Similarly, the generalization gap bound based on VC dimension grows linearly in the number of trainable parameters and cannot account for the practical observations with deep learning. In other words, these two bounds are too conservative.\n\nA much more useful approach was recently proposed by K Kawaguchi, LP Kaelbling, and Y Bengio. Unlike others, they embrace the fact that usually deep learning models are trained using train-validation paradigm. Instead of bounding the non-computable expected risk with train error, they use validation error. In this view, they propose the following intuition on why deep learning generalizes well in practice: \u201cwe can generalize well because we can obtain a good model via model search with validation errors\u201d. And prove the following bound for any \u03b4 > 0, with probability at least 1 \u2212 \u03b4:\n\nImportantly |Fval| is the number of times we use the validation dataset in our decision making to choose a final model, m is the validation set size. This bound explains why deep learning had the ability to generalize well, despite possible nonstability, nonrobustness and sharp minimas. Somewhat an open question still remains why we are able to find architectures and parameters that result in low validation errors. Usually, architectures are inspired by real-world observations and good parameters are searched using SGD that we would discuss next.\n\nSGD is an inherent part of modern Deep Learning and apparently one of the main reasons behind its generalization. So we would discuss its generalization properties next.\n\nIn a recent paper \u201cData-Dependent Stability of Stochastic Gradient Descent\u201d authors were able to prove that SGD is on-average stable algorithm under some additional loss conditions. These conditions are fulfilled in commonly used loss functions such as logistic/softmax losses in neural nets with sigmoid activations. Stability, in this case, means how sensitive is SGD to small perturbations in the training set. They take it further to prove a data-dependant on-average bound for generalization gap of SGD in non-convex functions such as deep neural nets:\n\nwhere m is a training set size, T number of training steps and \u03b3 characterizes how the curvature at the initialization point affects stability. Which lead to at-least two conclusions. First, the curvature of the objective function around the initialization point has crucial influence. Starting from a point in a less curved region with low risk should yield higher stability, i.e faster generalization. In practice it can be a good pre-screen strategy for picking good initialization params. Second, considering the full pass, that is m = O(T), we simplify the bound to O(m\u207b\u00b9). That is the bigger training set, the smaller generalization gap.\n\nInterestingly, there is a number of studies investigating learning curves. Most of them show power-law generalization error, scaling as \u03b5(m) \u223c m\u1d5d with exponent \u03b2 = \u22120.5 or \u22121. Which is also aligned with the previously discussed paper. However, it\u2019s important to mention the large-scale research by Baidu that was able to empirically observe this power-law (see Figure 2). However, the exponent \u03b2 in real applications was between\u22120.07 and \u22120.35. Which has to be still explained by theory.\n\nAdditionally, there is both theoretical and empirical evidence of batch size effect on generalization of SGD. Intuitively a small batch training introduces noise to the gradients, and this noise drives the SGD away from sharp minima, thus enhancing generalization. In a recent paper from Google it was shown that the optimum batch size is proportional to the learning rate and the training set size. Or simply put in other words, \u201cDon\u2019t Decay the Learning Rate, Increase the Batch Size\u201d. Similarly scaling rules were derived for SGD with momentum: Bopt ~1/(1 \u2212 m), where Bopt is an optimal batch size and m is momentum. Alternatively, all conclusions can be summarised by the following equation:\n\nwhere \ud835\udf16 is learning rate, N is training set size, m is momentum and B is batch size.\n\nIn the last few years, there has been an increasingly growing interest in the fundamental theory behind the paradoxical effectiveness of Deep Learning. And although there are still open research problems, modern Deep Learning by any means is far from being called Alchemy. In this article we discussed the generalization perspective on the problem which leads us to a number of practical conclusions:\n\nDon\u2019t forget to clap and share the article if you found it interesting. You may also reach me on Linkedin and Twitter."
    },
    {
        "url": "https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d?source=---------6",
        "title": "Gradient Boosting from scratch \u2013 ML Review \u2013",
        "text": "Although most of the Kaggle competition winners use stack/ensemble of various models, one particular model that is part of most of the ensembles is some variant of Gradient Boosting (GBM) algorithm. Take for an example the winner of latest Kaggle competition: Michael Jahrer\u2019s solution with representation learning in Safe Driver Prediction. His solution was a blend of 6 models. 1 LightGBM (a variant of GBM) and 5 Neural Nets. Although his success is attributed to the semi-supervised learning that he used for the structured data, but gradient boosting model has done the useful part too.\n\nEven though GBM is being used widely, many practitioners still treat it as complex black-box algorithm and just run the models using pre-built libraries. The purpose of this post is to simplify a supposedly complex algorithm and to help the reader to understand the algorithm intuitively. I am going to explain the pure vanilla version of the gradient boosting algorithm and will share links for its different variants at the end. I have taken base DecisionTree code from fast.ai library (fastai/courses/ml1/lesson3-rf_foundations.ipynb) and on top of that, I have built my own simple version of basic gradient boosting model.\n\nWhen we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error)\n\nAn ensemble is just a collection of predictors which come together (e.g. mean of all predictions) to give a final prediction. The reason we use ensembles is that many different predictors trying to predict same target variable will perform a better job than any single predictor alone. Ensembling techniques are further classified into Bagging and Boosting.\n\nWe typically take random sub-sample/bootstrap of data for each model, so that all the models are little different from each other. Each observation has the same probability to appear in all the models. Because this technique takes many uncorrelated learners to make a final model, it reduces error by reducing variance. Example of bagging ensemble is Random Forest models.\n\nThis technique employs the logic in which the subsequent predictors learn from the mistakes of the previous predictors. Therefore, the observations have an unequal probability of appearing in subsequent models and ones with the highest error appear most. The predictors can be chosen from a range of models like decision trees, regressors, classifiers etc. Because new predictors are learning from mistakes committed by previous predictors, it takes less time/iterations to reach close to actual predictions. But we have to choose the stopping criteria carefully or it could lead to overfitting on training data. Gradient Boosting is an example of boosting algorithm.\n\nThe objective of any supervised learning algorithm is to define a loss function and minimize it. Let\u2019s see how maths work out for Gradient Boosting algorithm. Say we have mean squared error (MSE) as loss defined as:\n\nWe want our predictions, such that our loss function (MSE) is minimum. By using gradient descent and updating our predictions based on a learning rate, we can find the values where MSE is minimum.\n\nSo, we are basically updating the predictions such that the sum of our residuals is close to 0 (or minimum) and predicted values are sufficiently close to actual values.\n\nThe logic behind gradient boosting is simple, (can be understood intuitively, without using mathematical notation). I expect that whoever is reading this post might be familiar with modeling.\n\nA basic assumption of linear regression is that sum of its residuals is 0, i.e. the residuals should be spread randomly around zero.\n\nNow think of these residuals as mistakes committed by our predictor model. Although, tree-based models (considering decision tree as base models for our gradient boosting here) are not based on such assumptions, but if we think logically (not statistically) about this assumption, we might argue that, if we are able to see some pattern of residuals around 0, we can leverage that pattern to fit a model.\n\nSo, the intuition behind algorithm is to repetitively leverage the patterns in residuals and strengthen a model with weak predictions and make it better. Once we reach a stage that residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting). Algorithmically, we are minimizing our loss function, such that test loss reach its minima.\n\nA more technical quotation of the same logic is written in Probably Approximately Correct: Nature\u2019s Algorithms for Learning and Prospering in a Complex World,\n\n\u201cThe idea is to use the weak learning method several times to get a succession of hypotheses, each one refocused on the examples that the previous ones found difficult and misclassified. \u2026 Note, however, it is not obvious at all how this can be done\u201d\n\nLet\u2019s consider simulated data as shown in scatter plot below with 1 input (x) and 1 output (y) variables.\n\nData for above shown plot is generated using below python code:\n\nTo aid the understanding of the underlying concepts, here is the link with complete implementation of a simple gradient boosting model from scratch. [Link: Gradient Boosting from scratch]\n\nShared code is a non-optimized vanilla implementation of gradient boosting. Most of the gradient boosting models available in libraries are well optimized and have many hyper-parameters.\n\nBlue dots (left) plots are input (x) vs. output (y) \u2022 Red line (left) shows values predicted by decision tree \u2022 Green dots (right) shows residuals vs. input (x) for ith iteration \u2022 Iteration represent sequential order of fitting gradient boosting tree\n\nWe observe that after 20th iteration , residuals are randomly distributed (I am not saying random normal here) around 0 and our predictions are very close to true values. (iterations are called n_estimators in sklearn implementation). This would be a good point to stop or out model will start overfitting.\n\nLet\u2019s see how our model look like for 50th iteration.\n\nWe see that even after 50th iteration, residuals vs. x plot look similar to what we see at 20th iteration. But the model is becoming more complex and predictions are overfitting on the training data and are trying to learn each training data. So, it would have been better to stop at 20th iteration.\n\nPython code snippet used for plotting all the above figures."
    },
    {
        "url": "https://medium.com/mlreview/parfit-hyper-parameter-optimization-77253e7e175e?source=---------7",
        "title": "Parfit \u2014 quick and powerful hyper-parameter optimization with visualizations",
        "text": "Let\u2019s work with an example here: RandomForestClassifier (implemented in sklearn). For reasons taught in Jeremy\u2019s Introductory Machine Learning course (soon to be available as a MOOC), the two key parameters we are interested in are max_features and min_samples_leaf. That is, we want to know what combination of max_features and min_samples_leaf give us the best score on our validation set.\n\nTo do this, we typically perform an exhaustive grid search over all combinations of max_features and min_samples_leaf. The most commonly used package for this technique is GridSearchCV (from sklearn). This package is highly effective if you are using a completely randomized training set that does not contain a time component as, by default, it internally scores using cross-validation. This is an optimal method for estimating hyper-parameters because it takes the average score over all folds in the cross-validation routine and gives the best set of hyper-parameters based on this average.\n\nCross-validation using GridSearchCV optimizes on the training data! As mentioned in Rachel\u2019s post, this is dangerous and not applicable to most real-world modeling problems (especially time series data). Instead, we want to optimize our hyper-parameters on the validation set.\n\nNote that in GridSearchCV it is possible to specify a PredefinedSplit object for separating the training and validation sets within the GridSearchCV, but it is better practice to split the training and validation sets beforehand and enter the validation set as the scoring set to avoid confusion and bleeding over between your training and validation sets.\n\nNow that we understand the problem, what\u2019s our solution?"
    },
    {
        "url": "https://medium.com/mlreview/making-ai-art-with-style-transfer-using-keras-8bb5fa44b216?source=---------8",
        "title": "Making AI Art with Style Transfer using Keras \u2013 ML Review \u2013",
        "text": "Over the past several years, Convolutional Neural Networks (CNNs) have established themselves as a state-of-the-art computer vision tool both in industry and academia. Being used in applications ranging from facial recognition to self-driving cars, they have become incredibly popular for deep learning developers. In my work at Galaxy.AI, I\u2019ve implemented CNNs for some of the more \u201ctraditional\u201d computer vision tasks such as image classification and object localization.\n\nIn addition to these sorts of tasks, however, CNNs have been shown to be particularly good at recognizing artistic style. Specifically, in this paper from 2015, the authors discuss how deep convolutional neural networks can distinguish between \u201ccontent\u201d and \u201cstyle\u201d in images. By writing separate loss functions for each, the authors demonstrate how CNNs can combine the style from one image with the content from other, to create new, visually appealing images. One impressive aspect of this technique is that no new network training is required \u2014 pre-trained weights such as from ImageNet work quite well.\n\nStyle transfer is a fun and interesting way to showcase the capabilities of neural networks. I wanted to take a stab at creating a bare-bones working example using the popular library, . In this post I\u2019ll walk you through my approach, mimicking as closely as possible the methods from the paper. The full code from this post can be found at https://github.com/walid0925/AI_Artistry .\n\nUsing only two base images at a time, we\u2019ll be able to create AI artwork that looks something like this:"
    },
    {
        "url": "https://medium.com/mlreview/speeding-up-dqn-on-pytorch-solving-pong-in-30-minutes-81a1bd2dff55?source=---------9",
        "title": "Speeding up DQN on PyTorch: how to solve Pong in 30 minutes",
        "text": "Initial version of code runned on GTX 1080Ti shows the speed of 154 observations per second during training and can solve Pong from 60 to 90 minutes depending on the initial random seed. That\u2019s our starting point.\n\nThe wrappers applied to the environment are very important for both speed and convergence (some time ago I\u2019ve wasted two days of my life trying to find a bug in the working code which refused to converge just because of missing \u201cFire at reset\u201d wrapper. So, the list of the used wrappers I\u2019ve borrowed from OpenAI baselines project some time ago:\n\nAs a starting point, I\u2019ve taken the classical DQN version with the following hyperparameters:\n\nAs my computational resources are very limited by two 1080Ti + one 1080, (which is very modest nowadays), the only way to proceed is to make the code faster.\n\nNevertheless you always should keep a balance here: trying to squeeze as much performance as possible, you can introduce bugs, which will dramatically complicate already complex debugging and implementation process. So, after all systems from the rainbow paper were implemented, I asked myself a question: will it be possible to make my implementation faster, to be able to train not only on Pong, but challenge the rest of the games, which require at least 50M frames to train, like SeaQuest, River Raid, Breakout, etc.\n\nTo debug and test it I\u2019ve used Pong game from Atari suite, mostly due to its simplicity, fast convergence, and hyperparameters robustness: you can use from 10 to 100 smaller size of replay buffer and it still will converge nicely. This is extremely helpful for a Deep RL enthusiast without access to the computational resources Google employees have. During implementation and debugging of the code, I was needed to run about 100\u2013200 optimisations, so, it does matter how long one run takes: 2\u20133 days or just an hour.\n\nSome time ago I\u2019ve implemented all models from the article Rainbow: Combining Improvements in Deep Reinforcement Learning using PyTorch and my small RL library called PTAN. The code of eight systems is here if you\u2019re curious.\n\nTo put this in perspective, 100M frames which is normally used by RL papers will took us 7.5 days of patient waiting.\n\nThe first idea we usually apply to speed up Deep Learning training is larger batch size. It\u2019s applicable to the domain of Deep Reinforcement Learning, but you need to be careful here. In the normal Supervised Learning case, a simple rule \u201clarge batch is better\u201d is usually true: you just increase your batch until your GPU memory allows and larger batch normally means more samples will be processed in a unit of time, thanks to the enormous GPU parallelism.\n\nReinforcement Learning case is slightly different. During the training, two things happen simultaneously:\n\nAs an agent explores the environment and learns about the outcome of its actions, the training data is changing. For example, in a shooter your agent can run randomly for a while beeing shot by monsters, having only miserable \u201cdeath is everywhere\u201d experience in the training buffer. But after a while, the agent can discover that he has a weapon it can use. This new experience can dramatically change the data we\u2019re using for training.\n\nRL convergence usually lays on fragile balance between training and exploration. If we just increase a batch size without tweaking other options we can easily overfit to the current data (for our shooter example above, your agent can start thinking that \u201cdie young\u201d is the only option to minimize suffering and can never discover the gun it has).\n\nSo, in 02_play_steps.py we do several steps every training loop and use batch sizes multiplied by this number of steps. But we need to be careful with this number of steps parameter. More steps mean a larger batch size, which should lead to faster training, but at the same time doing lots of steps between training can populate our buffer with samples obtained from the old network.\n\nTo find a sweet spot, I\u2019ve fixed the training process with a random seed (which you need to pass both numpy and pytorch) and trained it for various steps.\n\nThe convergence dynamics is almost the same (see image below ), but speed the increase saturates around 4 steps, so, I\u2019ve decided to stick to this number for further experiments.\n\nIn this step we\u2019re going to check our training loop, which basically contains repetition of the following steps:\n\nThe purpose of the first two steps is to populate the replay buffer with samples from the environment (which are observation, action, reward and next observation). The last two steps are training our network.\n\nThe illustration of the above steps and their communication with the environment, DQN on GPU and replay buffer is on the diagram below.\n\nAs we can see, the environment is being used only by the first step and the only connection between top and bottom halves of our training is our replay buffer. Due to this data independence, we can run both processes in parallel:\n\nBoth activities should run in sync, to keep training/exploration balance we\u2019ve discussed in the previous section.\n\nThis idea was implemented in 03_parallel.py and is using torch.multiprocessing module to parallelize playing and training still being able to work with GPU concurrently. To minimize the modifications in other classes, only the first step (environment communication) was put in separate process. The obtained observations were transferred to the training loop using the Queue class.\n\nBenchmarking of this new version shows impressive 395 frames/s, which is 74% increase versus the previous version and 156% increase in comparison to the original version of the code.\n\nThe next step is simple: every time we call cuda() method of Tensor we pass async=True argument, which disables waiting for transfer to complete. It won\u2019t give you very impressive speed up, but sometimes gives you something and very simple to implement.\n\nThis version is in file 04_cuda_async.py and the only difference is passing cuda_async=True to calc_loss function.\n\nAfter benchmarking I\u2019ve got 406 frames/s training speed, which is 3.5% speed up to the previous step and 165% increase versus the original DQN.\n\nAs I\u2019ve said before, original version of DQN used some old Atari wrappers from OpenAI baselines project. Several days ago those wrappers were changed with commit named \u201cchange atari preprocessing to use faster opencv\u201d, which is definetely worth to try.\n\nHere is the new code of the wrappers in the baselines repo. Next version of the DQN is in 05_new_wrapper.py. As I haven\u2019t pulled new wrappers into ptan library, they are in the separate lib in examples.\n\nBenchmarking result is 484 frames/s, which is 18% increase to the previous step and final 214% gain to the original version.\n\nWith several not very complicated tricks we\u2019ve got more than 3 times increase in speed of DQN, without sacrificing readability and adding extra complexity to the code (the training loop is still less than 100 lines of python code). And now, the latest version is able to reach 18 score in Pong in 20\u201330 minutes, which opens lots of new possibilities to experiment with other Atari games, as 484 frames per second means less than 2.5 days to process 100M observations.\n\nIf you know more things that can increase performance of PyTorch code, please leave comments, I am really interested to know them."
    },
    {
        "url": "https://medium.com/mlreview/choosing-components-for-personal-deep-learning-machine-56bae813e34a",
        "title": "Choosing Components for Personal Deep Learning Machine",
        "text": "Once the PCI-e Lane requirement has been decided, We can now choose the Motherboard Chipset:\n\nThe below table gives you the no of PCI-e Lanes available with different Chipsets available:\n\nSo, even though Chipsets like B150,B250, H110,H170,H270 support Intel processors, They are seldom used for deep learning builds since the number of PCIe lanes will not be enough for Deep learning applications.\n\nHence, chipsets that are commonly preferred are:\n\nZ170 \u2014 Support both 6th/7th Gen Intel Processor. Usage of 7th Gen might require a BIOS Update.\n\nZ270 \u2014 Support both 6th/7th Gen Intel Processor. \n\n(Latest) Z370 \u2014 Supports 8th Gen Intel Processor.\n\nOnce you have decided on the chipset, Use PC Partpicker to select the motherboard : Link to select the motherboard of your choice.\n\nThings to Keep in Mind:\n\nThrough the selection of motherboards, We have narrowed down the choice of processor based on constraints like socket type, But the choice of CPU might further dependent on GPU. For Deep learning applications, As mentioned earlier, The CPU is responsible mainly for the data processing and communicating with GPU. Hence, The number of cores and threads per core is important if we want to parallelize all that data preparation. It is advised to choose a multi core system (Preferably 4 Cores)to handle these tasks.\n\nThings to Keep in Mind:\n\nPS. Some processors may need the user to get their own Cooler Fan. Usually, Unboxed Processor doesn\u2019t come with a cooler fan but allows the user to overclock.\n\nUse PC Partpicker to select the Processor : Link\n\nWhen working with large/big datasets we might need to have them in memory. Size of the RAM decide how much of dataset you can hold in memory. For Deep learning applications it is suggested to have a minimum of 16GB memory (Jeremy Howard Advises to get 32GB). Regarding the Clock, The higher the better. It ideally signifies the Speed \u2014 Access Time but a minimum of 2400 MHz is advised.\n\nAlways try to get more memory in a single stick as it will allow for further expansion in remaining slots.I have seen many people who get 4*8 GB RAM instead of 2*16 GB ending up using all 4 Slots and no room for upgrade just because they are bit cheap than the latter.\n\nThe price of HDD is decreasing continuosly as SSD become more affordable and faster.\n\nIts always better to get a small size SSD and a large HDD. SSD\u2019s are preffered to store and retrieve data that is actively used. On the other hand HDD should be used to store data that are to be used in future.\n\nGPU\u2019s are the heart of Deep learning Build. They decide the performance gain that you get during training of neural networks. As most of the computation involved in Deep Learning are Matrix operations, GPU outperforms conventional CPU by running the same as parallel operations. They have small computation units called cores that can have threads which enable them to run the matrix operations faster. The Memory bandwidth of the GPU also enables to operate on large batches of data.\n\nTim Dettmers has a great article on choosing a GPU for Deep Learning, which he regularly updates as new cards come on the market. Please check them out before choosing your GPU.\n\nI strongly recommend a beginner to get a 1060 6gb (New/Used) if they are on a budget. If the budget can go up a bit then you can get a 1070ti (MSRP around 430 USD)that was released recently ie.OCT 26th which offers almost the same performance as 1080 but at a lower cost (Almost Same as 1070). Dont buy a 1070 unless you have a strong reason to, instead get a 1070ti as it has more number of cores. If you have enough money then get a 1080ti.(No Second Thoughts). Again if you are very active in performing research consider buying 2 X 1070ti instead of 1 X 1080ti as it gives flexibility thats was discussed earlier.\n\nFor readers wondering about different editions of GPU like Founder\u2019s Edition, OC, FTW etc.\n\nHere\u2019s the Info that you need:\n\nDifference between Editions: Fundamentally all of them have the same GPU processor inside them. The main difference would be variation in quality of the PCB and usually high end models would have higher binned chips(Best Quality).\n\nDifference between Brands: Brands build their custom PCB components and aesthetics like lighting,Multiple fans, water cooled or back plate. These are done in order to improve the performance on the reference boards by just keeping the reference design on the card and add custom coolers on it. The base clocks out of the box matters very little generally.\n\nWater vs Air cooled GPU :\u2014 Nvidia lowers the clock rate on your GPU as it gets hot. I don\u2019t know if there are set temperatures that trigger this, or if it\u2019s just linear. Water cooling will keep your GPU running at top speed.\n\nAgain! please research through the different editions. I have heard that FTW to be the coolest one to get. (Silent and No heating issues)\n\nOnce the Components are selected using PCpartpicker site, it will give a rough estimate of power usage. Its always better to get a PSU, large (1.2 to 1.5 times estimated power) enough to handle the power. In case if you plan to add more GPU( Add 100 W per GPU) then consider buying a PSU such that it can handle that requirement too. Some PSU tend to generate noise hence research on different products based on reviews before buying. I have found that EVGA G2 series seems to be solid option to consider.\n\nNote: Gold, Silver,Platinum described along with the product refers to the efficiency of the PSU(Heat Generation). It directly correlates to power savings.\n\nFor buying the components, I strongly recommend the reader to keep an eye on r/buildapcsales and r/hardwareswap (In US) for deals and grab them instead of buying them at retail price. Open-Box components seem to be cheaper and should be considered when on stringent budget. Try to get the components in installments instead of getting them all at once if you are not in urgent need.\n\nCheaper Alternative: Try to get a Open-Box Pre-Built System and modify the components as per your requirement.(For Lazy Folks!)\n\nThe information described are based on my research and understanding from multiple articles and build guides from the internet. If there are any errors please kindly notify me so that i can fix them. Feel free to contact me or comment below if have any questions.Thanks for reading !\n\nPCIe lane denotes the maximum bandwidth that is available for graphics cards\u2019 communication with the CPU. Having more lanes than you need won\u2019t increase performance, you just don\u2019t want to have so few that it starts restricting CPU/GPU intercommunication. Generally an x8 lane of PCIe 3.0 has more than enough bandwidth for any gaming card, so 16 lanes for dual cards or 24 lanes for triple cards is fine. In applications outside of gaming, such as when the GPU is being used to accelerate CPU computation for workstations and servers, there is a lot more communication between the CPU and GPU than in games, so 40 lanes might be helpful there. The X99 platform is derived from Intel\u2019s server/workstation chips, so that\u2019s why they have so many lanes."
    },
    {
        "url": "https://medium.com/mlreview/our-nips-2017-learning-to-run-approach-b80a295d3bb5",
        "title": "Our NIPS 2017: Learning to Run approach \u2013 ML Review \u2013",
        "text": "For 3 months, from July to 13 November, me and my friend Piotr Jarosik participated in the NIPS 2017: Learning to Run competition. In this post we describe how it went. We release the full source code.\n\ntl;dr 22nd place in the end, the final skeleton has a cheerful gait, we used PPO trained on 80 cores in a couple of days with manually prepared observation vector + a bit of reward hacking. The final result:\n\nCompetitors were given a model of a human skeleton and OpenSim simulator. The task was to write a program that activates legs muscles in order to maximize the number of meters passed in 1000 timesteps. In the reinforcement learning setting, one would say that an action had 18 float values from 0 to 1, corresponding to muscle activations:\n\nActions were taken depending on the current observation (positions, velocities and so on):\n\nA typical program would read the initial observation and output an action, activating some muscles. The simulator would update its state and return the observation for the next timestep + a reward (distance passed) for the previous timestep. So the learning algorithm received feedback if it did good or bad. Based on that, it updated a value/policy function approximations, neural networks. The simulation ended if pelvis.y dropped below 65 cm (assuming the skeleton has fallen) or after 1000 timesteps. The final score was the number of meters pelvis.x moved from the initial position.\n\nRound 1 finished on 4 November, our place on the leaderboard was 63rd with 17.51 metres, it looked like this:\n\nWe had a model which scored 22.71 (which would be 42nd place), but due to some server-side bug it didn\u2019t register on the leaderboard. It didn\u2019t matter that much at that point, because the rules changed and not top 10 was advancing to the final Round 2 but competitors with score 15 or higher. Since we already had 17.51, we solely focused on training for Round 2, which was harder, since the number of obstacles (these spheres on the ground) was increased from 3 to 10. In the Round 2 we placed 22nd with 18.62 meters (GIF at the top of this post).\n\nSince the beginning we learned that the simulator was slow af (compared to e.g. MuJoCo). It could take more than a minute for only 1 episode (full run from start to falling or 1000 timesteps). So we wanted to make the simulations as fast as possible. Speeding up network libraries (TensorFlow) or using GPU didn\u2019t bring any value since OpenSim simulator took ~99% of the time.\n\nOne of the participants (Qin Yongliang) posted a line of code which was changing the accuracy of the simulation to 3%. We recompiled OpenSim, that was tedious, especially on many hosts. However, with this change the simulations became ~3x faster. During the competition we were worried, that this would cause problems, because on the server the default accuracy of 0.1% was used. However, we didn\u2019t notice any significant difference in the scores, so the \u201csetAccuracy hack\u201d seemed like a big win.\n\nWe couldn\u2019t make OpenSim any faster than that, so we wanted to run them in parallel. The quick-start implementation given by organizers used DDPG from keras-rl library. In the beginning we tried to use it, but we weren\u2019t quite happy. It was non-trivial how to run it with many parallel OpenSim environments. We also didn\u2019t enjoy that the average score during training was fluctuating a lot and what\u2019s worse \u2014 sometimes it could just drop and never regain. We tried to tune hyperparams, without any gain though.\n\nOne week after we started, on July 20, OpenAI released their PPO implementation. It looked like it would be out-of-the-box parallelizable on multiple CPUs (with MPI). Their results were very impressive, they also used it for running (in harder setting, in 3D, our competition was really 2D, because the depth was fixed, skeleton couldn\u2019t move along Z axis). It seemed perfect. It took about the same time to integrate as DDPG, we added plots, saving/loading state etc. We noticed that the average score is not fluctuating as much as with our tries with DDPG. Even better \u2014 it didn\u2019t suddenly drop. So we switched to PPO and stayed with it until the end of competition.\n\nWe noticed that the RAM consumption grew constantly and after a day it could take 30 GB for a single process. Yep, memory leak. In the OpenSim simulator (or its Python wrappers). Some contestants were just automatically killing those leaky OpenSim processes and starting them again every say, 8 hours. We didn\u2019t like it, it introduced another complexity, where things were already complex (DDPG, PPO and so on). We found that a combination of Python 2 + older version of OpenSim didn\u2019t leak. With Python 3 it leaked.\n\nSo finally we could run multiple processes easily. We only had to use Python 2\u2026 Naturally, OpenAI baselines code required Python 3 :) However, Python 3 features were only used in a couple of places, so it was easy to backport the files we used.\n\nIn the beginning we used our laptops, 4 cores each. The computing power was close to zero and it was horrible to hear the fan during the night. Piotr gained an access to 16 cores machine. We used it for a couple of weeks.\n\nThen a lucky thing happened. We got an access to a machine with\u2026 80 cores (Intel Xeon). That was a game changer. Now we could run dozens of experiments. To feed 8M timesteps of data into learning algorithm, it took about 24 hours, on 40 cores.\n\nWe also explored an option of running things on a cluster which had 24 nodes, each having 12 cores. However, the software there was very old and it was cumbersome to run things on it. We managed to compile a newer version of GCC and other stuff from dependency hell but finally our mana depleted.\n\nIn the middle of the competition top 100 received 300$ for AWS. We used that credit in the last 4 days, renting c5.9xlarge (1.728$/h) and c4.8xlarge (1.811$/h), each having 36 cores. About 86$ per 24h. Yikes. We wanted to use some instances with more cores, but all of the beefier ones had some default limit set to\u2026 0. For any region. So, one could submit a ticket\u2026 for each instance type\u2026 and wait. We did that for m4.16xlarge (64 cores) and after 4 days AWS bumped the limit. That was 4 hours before the end of the whole competition. We could have requested it much earlier, but we didn\u2019t know that \u201con-demand\u201d meant \u201con-demand after 4 days after sending a ticket\u201d.\n\nWe found another interesting OpenAI blog post (all of them are great) about Evolution Strategies. We had 80 cores instead of 720, but we thought that maybe this was enough, even if it would be 9x slower (we still had more than a month till the end of Round 1). What we liked about ES is simplicity (compared to PPO), they are embarrassingly parallel. We didn\u2019t have experience with in-memory database Redis that OpenAI implementation used. Also the code seemed tightly integrated with AWS, but after some struggle we managed to set it up and run locally. That came late though, 3 days before the end of Round 2. We couldn\u2019t beat PPO, however the comparison isn\u2019t fair, because on PPO we spent 2 months and on ES \u2014 about a week.\n\nOk, we had PPO from baselines running on 80 cores. We got some improvement when we extended the observation vector. 41 was already a lot, however it didn\u2019t have all the velocities. Nor any accelerations. And RL algorithms work well if observation is Markovian, i.e. you can predict what happens next just from the given observation (in other words, you can forget about the past). So we added the remaining velocities and accelerations. Another important thing were obstacles. Original observation had only information about the next obstacle. It\u2019s helpful to make an agent \u201cremember\u201d more, e.g. the previous obstacle as well. Otherwise he would \u201cforget\u201d about the one underneath it when seeing the next one. Original observations also didn\u2019t have the force with which feet touch the ground. So we added an approximation of that. In the end we had 82 observations instead of 41. You can refer to our code for the details.\n\nTo have more insight into what\u2019s going on, we logged the mean and std (standard deviation) of all the observations, to see if the normalization is done well. Because normalization is one of 10 things that can easily go wrong (great article). By default, PPO code uses a filter, which automatically normalizes every feature. For every feature, it keeps its running mean with std and does normalization by subtracting the mean and dividing by std. This works for most of the features. However, imagine a constant, like the strength of psoas (a muscle). The std is 0. The code in that case was just passing this value as it is. The magnitude of that value is treated as an importance when passed to a network. If it\u2019s big, it will saturate all the other smaller inputs (which may be more important). Another problem was that the very first strength of psoas (due to a bug, later fixed) had some different value than the following ones. So that filter would calculate some arbitrary mean + std and later use them. Another problem are spiky features. Take velocity for example. Most of the time it\u2019s quite low, but there is a moment it shoots up and it probably is an important moment. On one hand you want it big enough, so that the network picks it up, but not enormously huge, so that it doesn\u2019t saturate other features. Because all of these problems, we skipped the auto normalizing and just manually normalized every feature. We would run our model, collect means with std and visualize:\n\nIf we saw that some value was not \u201cin line\u201d (mean not close to 0, std not close to 1), we would tweak the code and repeat. We ended up with something not ideal, but probably good enough so that the network can at least \u201csee\u201d all the given observations.\n\nAnother important thing was to use relative positions, not absolute. All our positions were relative to pelvis. Thanks to that similar poses were represented by a similiar observations. Think of a skeleton standing still, but in two positions, the initial one and the one 10 meters away from it. Absolute values for X in the initial position would be close to 0. 10 meters away they would be close to 10. However, relative X would be always close to 0 for skeleton standing still, no matter where on the absolute X axis.\n\nThe last thing that guided learning well was reward hacking. Essentially during training it\u2019s worth to use a different reward than the one used during grading (the distance in meters). First reward hack came from DeepMind\u2019s \u201cEmergence of Locomotion Behaviours in Rich Environment\u201d research paper. They used velocity instead of distance, which is better. Using velocity as a reward puts emphasis on passing the same distance in less timesteps. Using distance doesn\u2019t, no matter if you passed 10 meters in 10 or 1000 timesteps. Problem we got was that lots of experiments got stuck in local maxima, which resembled jumping in place:\n\nSo, we added a small reward for every timestep \u201csurvived\u201d. This helped it to take the first step and get out of local maximum. Naturally, we couldn\u2019t give it too much reward for not falling, because then such things happened:\n\nWe were hesitant to adding more reward hacking throughout the challenge, it\u2019s a bit ugly, we hoped for a more general solution. However, when the challenge was about to end and our models still performed so-so, we thought, well ok, let\u2019s do it, there\u2019s nothing else we could do. The nice thing about reward hacking is that the changes are quickly visible in training and they are easy to implement. However one could end up exploring not enough.\n\nIn the end we added two more. The first one was: \u201ckeep pelvis.x behind head.x\u201d. It was causing it to lean a bit forward, because it\u2019s obvious you must do that when running, but our models were not discovering it, they liked to wobble the head back and forth.\n\nThe second one was a penalty for straight legs. Our models all the time had two legs straight (or one in the best case). The penalty probably helps, but we didn\u2019t have time to train with this long enough. Pity we didn\u2019t try it earlier, it\u2019s a simple one.\n\nWe tried many other things, for example reducing action to only 9 values for the right leg and copying them over to the left leg. We hoped to produce a fast hopper, kangaroo-like (with a simpler network). Our PPO however couldn\u2019t train it well, skeleton was poor on obstacles. Here is how a good hopper looks like (this one is Henryk Michalewski\u2019s result, it uses full 18 action values though, it just converged to this quite good local maximum):\n\nThat was the longest competition we participated in, it was mentally tiring. However, lots of fun! We\u2019ve also learnt much more than we anticipated.\n\nIn hindsight, we started to tweak the hyperparams of DDPG and PPO too early. It took a long time and almost all that work went to the bin, because we abandoned DDPG and in PPO we changed features and rewards (so we needed to tweak hyperparams again). In the end we used the original PPO parameters, 2 hidden layers with 64 neurons. It\u2019s hard to decide if some hyperparam is better, we finally run out of time. Also we felt it was better to give a couple of days of CPU to already walking model instead changing hyperparams and starting from scratch.\n\nIt may be that we too early abandoned DDPG. Some participants were using it. We could have tried with baselines DDPG implementation, especially after OpenAI injected noise in the parameters to improve exploration. We also feel that ES option still has a big potential. We saw TRPO, but PPO seems like improved TRPO, that\u2019s why we didn\u2019t try it. But maybe we\u2019re wrong.\n\nWhen taking some new algorithm, it will be better if we first replicate it on some easy/known environment first. To be sure that we got it working + gain a knowledge of how it trains. After that, plug in the complex environment. Because if we can\u2019t learn the easier environment, we won\u2019t learn the harder one for sure.\n\nThis manual normalization also took very long. Now it seems it might be faster to use auto normalization for most of the features but only normalize manually a few chosen ones. I thought about it and tried, but my TensorFlow fu was too weak. I went for a simpler (but probably more time-consuming in hindsight) manually normalizing all the features.\n\nI would implement logging earlier, to gain as much insight into that black-box as possible. That was our biggest problem I would say, lack of visibility of what\u2019s going on during training. We could spend more time to learn TensorBoard or similar perhaps.\n\nOk, that\u2019s it! Thanks again to the organizers and competitors! We\u2019re looking forward to other write-ups, we\u2019re very curious of other approaches.\n\nIn the beginning I think nobody was sure if it would be possible to run fast, but the results are really impressive and the answer is clearly: yes.\n\nTo see what\u2019s possible, USTC-IMCL\u2019s solution winning Round 1 with 44.61m:\n\nNNAISENSE (Wojciech Ja\u015bkowski et al.) won the final Round 2 with a very impressive result of 45.96m, congratulations! State of the art running\u2026 In Round 1 they were 5th with 42.71m. All the videos from Round 1 and 2 are available on the leaderboard.\n\nUpdate: Award for the best comment goes to Imnimo reddit user:"
    },
    {
        "url": "https://medium.com/mlreview/aann-absolute-artificial-neural-network-ae8f1a65fa67",
        "title": "Absolute ANN: A simplified approach for structuring the learnt representations",
        "text": "Since structurally, the AANN is similar to an FCNN which has been around for about decades now. So, what makes the AANN different from the FCNN? There are three key distinctions between them: first is the way it is trained, second the cost function (mathematical objective function) that it optimises and finally the activation function it uses. Let\u2019s look at how the network is trained:\n\nI firstly worked with two separate neural networks for the encoder and the decoder part and made the technique work for it. Upon experimenting further, I discovered that we can also use the same encoder network for the decoder part as well (Tying the weights of an autoencoder). So, conceptually, it can be visualised as using a fully connected NN in the reverse direction. This makes the structure exactly same as that of an FCNN. On a single neuron level, it can be imagined as a network of the bidirectional neurons depicted below.\n\nThis is the link to the code for this technique (The repo contains some more ideas that I have in mind). In the following sections, I will step through the modifications that I made to the AE architecture in order to reach the AANN. The video is not a replacement for this article (or vice versa). There are certain aspects that I touch here while the others are covered in the video. In this article, I\u2019ll also share some of the details of the experimentation that I did in order to make this architecture work.\n\nThe explanation of the AANN technique is presented using the MNIST dataset, which is regarded as the \u2018Drosophila\u2019 of Deep Learning by Geoffrey Hinton. Here is the link to the video in which I explain the technique from a Neural Network\u2019s standpoint (without mentioning the GANs or AEs).\n\nBasically, the last layer ensures that the representation vectors are just as long as there are labels. And the vectors tend to be closer to the label axis due to the cost defined using the cosines of the angles made by the vector with the label axes. Thus, in the last but one layer, the representations belong to a more complicated (even tangled) and high dimensional space (depends on the no. of hidden neurons used in that layer. I used 512 neurons); but, in the last layer the representations belong to a very structured m-dimensional space. Due to the cost function, what happens is that the network tries to separate the vectors and make them as close as possible to the label axis. The trick here is that I am only clustering based on the angles, thus the magnitude of the vectors allow them to encode the mapping information on the corresponding axes. As an example, let\u2019s say there are only four digits \u2014 0, 1, 2 and 3. So the representation vectors of 3 would look like: [0, 0, 0, 10.33] or [0, 1e-6, 0.001, 99.63] and so on.. That is, all the information of the digit 3 is encoded in a positive (will explain why positive? in the next section) real number range in the specific position for that digit in the representation.\n\nThe input n-dimensional feature vector is passed through the neural network consisting of hidden layers, constructed from the bidirectional neurons, to obtain an m-dimensional representation vector; where m corresponds to the number of labels. The obtained representation vector is then converted into a unit vector, which corresponds to the cosines of the angles made by the representation vector with the coordinate axes. Finally, the forward cost is computed as the mean absolute difference between the unit representation vector Y\u2019 and the one-hot-encoded-label vector Y.\n\nThus the network now not only needs to be able to classify the MNIST digits, but also be able to generate them back given a digit query vector.\n\nSo, after performing the forward and the backward passes, we obtain the forward and the backward costs and finally to define the main cost function for optimization, it is just the sum of the two.\n\nFor the reverse pass, note that I feed in the original representation vector and not the directional cosines normalised (unit) vector back in the network. By feeding backwards, I mean the network computations are done using the transpose of the weight matrices and a different set of bias vectors for the reverse direction. The cost computed is again just the mean absolute difference between the original image (input vector) and the reconstructed image (the backward generated vector). There is no trick here, it is same as the decoder part of an Autoencoder.\n\nThe network obtains a forward accuracy of 99.86% on the train set (containing 39900 images) and 97.43% on the dev set (containing 2100 images). These results are alright! I mean on MNIST dataset, you can do that easily. But where the network shines is in generating the digits back from the same set of weight values. Take a look:\n\nI feel that there should be some metric that allows us to calculate a score for how well the network could generate the images in the backward direction. So, only looking at the accuracy should not be it.\n\nThere are some more key observations here, but first take a look at this video of the digits visualisation that I made:\n\nThe first graph is for the vectors that are fed into the network with the linear interpolation of values in the the range [0\u201380]. The second graph is the activations generated on the last but one layer in the forward direction and the third one is the actual digit generated by the network.\n\nNote how simple it is to feed in representation vectors into the network in order to generate the digits. (No sampling from a random distribution required). Next is that the generated digits transform from one form to another (while being what they are) smoothly. So, the network has learnt a differentiable function from the representations to the images, which means that these are not simple input output mappings (which is the case for a simple forward fully connected network). Also, notice the representations at the last but one layer (middle graph of the video). They correspond to the typical representations that we obtain using a traditional AE (for feature extraction).\n\nThis concludes the explanation of the AANN technique. I would like to mention a few more points about the activation function used for this network and would also like to make some final comments regarding the future scope of it.\n\nWell, the answer is both yes and no. The cost function definition was quite lucid given all the already done work on the AEs. I had the direction-magnitude trick for penalizing the cost function of an AE in mind for a long time. However, making this cost function work was the difficult part.\n\nThis architecture didn\u2019t work directly for the first time (in fact, I tried at least 25 different models before I found the above explained model). I realised very soon that it is the activation function used in the network that is keeping the network from adjusting itself to minimise both the costs. As it turns out, using the Absolute valued function as the activation function for the network allows it to optimize this hybrid objective function. (I mentioned above positive real number ranges. This is the reason why the activations are always positive. We are using the absolute function as the activation function). And, hence the name: \u201cAbsolute Artificial Neural Network\u201d. This was the difficult part, as I mentioned, because, it is not one of the standard activation functions used for the Neural Networks. I thought of this function since I was trying to create a symmetric ReLU.\n\nThese are some of the observations that I made while trying out the available activation functions and that is how I finally concluded with the use of abs function. (a) Upon using the ReLU, i.e. Rectified Linear-Unit, function as the activation function for this architecture, all the activations shoot to nan in the forward direction leading to proliferation of nan in the reverse direction as well (gradients exploding). If the Linear activation function is used, the network performs poorly in the forward direction, leading to very high classification error rates, while, the network converges to the point that it outputs the same structure as shown in (b), for every possible representation vector. On activating the hidden neurons with a ReLU in the forward direction and with an Abs in the reverse direction, the network kills all the activations, i.e. outputs the zero vector for every input, in the forward direction. In the backward direction, the network converges to the structure shown in (c). Upon using the Abs function in the forward direction and the ReLU in the backward direction, the network this time kills all the activations in the backward direction as visualized in (d). The (e) in above figure is the output achieved by using the Sigmoid activation function in the network. The result obtained is very similar to the result of using Linear activation function, as in (b).\n\nI would like to especially highlight the case where we use ReLU in the forward direction and Abs in the backward direction. This is what lead me to the use of absolute function everywhere. Firstly, by using ReLU forward and linear backward generated some grey coloured images, which I knew are caused by negative values. So, I thought how about I use the abs function to visualize what is getting generated in the backward direction. When I did this, the network converged to a point (forward cost decreased and backward cost increased) that the network outputted zero vectors for all the inputs in the forward direction. In fact, this convergence was so strong that when I tried to train the network only in the forward direction with ReLU, the weights didn\u2019t move. All the gradients vanished. This is something that I am still trying to understand why such a phenomenon occurs. Anyway, this lead me to try Abs in the forward direction as well, and that\u2019s it. It worked!"
    },
    {
        "url": "https://medium.com/mlreview/a-simple-deep-learning-model-for-stock-price-prediction-using-tensorflow-30505541d877",
        "title": "A simple deep learning model for stock price prediction using TensorFlow",
        "text": "For a recent hackathon that we did at STATWORX, some of our team members scraped minutely S&P 500 data from the Google Finance API. The data consisted of index as well as stock prices of the S&P\u2019s 500 constituents. Having this data at hand, the idea of developing a deep learning model for predicting the S&P 500 index based on the 500 constituents prices one minute ago came immediately on my mind.\n\nPlaying around with the data and building the deep learning model with TensorFlow was fun and so I decided to write my first Medium.com story: a little TensorFlow tutorial on predicting S&P 500 stock prices. What you will read is not an in-depth tutorial, but more a high-level introduction to the important building blocks and concepts of TensorFlow models. The Python code I\u2019ve created is not optimized for efficiency but understandability. The dataset I\u2019ve used can be downloaded from here (40MB).\n\nOur team exported the scraped stock data from our scraping server as a csv file. The dataset contains minutes of data ranging from April to August 2017 on 500 stocks as well as the total S&P 500 index price. Index and stocks are arranged in wide format.\n\nThe data was already cleaned and prepared, meaning missing stock and index prices were LOCF\u2019ed (last observation carried forward), so that the file did not contain any missing values.\n\nA quick look at the S&P time series using :\n\nNote: This is actually the lead of the S&P 500 index, meaning, its value is shifted 1 minute into the future. This operation is necessary since we want to predict the next minute of the index and not the current minute.\n\nThe dataset was split into training and test data. The training data contained 80% of the total dataset. The data was not shuffled but sequentially sliced. The training data ranges from April to approx. end of July 2017, the test data ends end of August 2017.\n\nThere are a lot of different approaches to time series cross validation, such as rolling forecasts with and without refitting or more elaborate concepts such as time series bootstrap resampling. The latter involves repeated samples from the remainder of the seasonal decomposition of the time series in order to simulate samples that follow the same seasonal pattern as the original time series but are not exact copies of its values.\n\nMost neural network architectures benefit from scaling the inputs (sometimes also the output). Why? Because most common activation functions of the network\u2019s neurons such as tanh or sigmoid are defined on the or interval respectively. Nowadays, rectified linear unit (ReLU) activations are commonly used activations which are unbounded on the axis of possible activation values. However, we will scale both the inputs and targets anyway. Scaling can be easily accomplished in Python using sklearn\u2019s .\n\nRemark: Caution must be undertaken regarding what part of the data is scaled and when. A common mistake is to scale the whole dataset before training and test split are being applied. Why is this a mistake? Because scaling invokes the calculation of statistics e.g. the min/max of a variable. When performing time series forecasting in real life, you do not have information from future observations at the time of forecasting. Therefore, calculation of scaling statistics has to be conducted on training data and must then be applied to the test data. Otherwise, you use future information at the time of forecasting which commonly biases forecasting metrics in a positive direction.\n\nTensorFlow is a great piece of software and currently the leading deep learning and neural network computation framework. It is based on a low level backend but is usually controlled via Python (there is also a neat TensorFlow library for R, maintained by RStudio). TensorFlow operates on a graph representation of the underlying computational task. This approach allows the user to specify mathematical operations as elements in a graph of data, variables and operators. Since neural networks are actually graphs of data and mathematical operations, TensorFlow is just perfect for neural networks and deep learning. Check out this simple example (stolen from our deep learning introduction from our blog):\n\nIn the figure above, two numbers are supposed to be added. Those numbers are stored in two variables, and . The two values are flowing through the graph and arrive at the square node, where they are being added. The result of the addition is stored into another variable, . Actually, , and can be considered as placeholders. Any numbers that are fed into and get added and are stored into . This is exactly how TensorFlow works. The user defines an abstract representation of the model (neural network) through placeholders and variables. Afterwards, the placeholders get \"filled\" with real data and the actual computations take place. The following code implements the toy example from above in TensorFlow:\n\nAfter having imported the TensorFlow library, two placeholders are defined using . They correspond to the two blue circles on the left of the image above. Afterwards, the mathematical addition is defined via . The result of the computation is . With placeholders set up, the graph can be executed with any integer value for and . Of course, the former problem is just a toy example. The required graphs and computations in a neural network are much more complex.\n\nAs mentioned before, it all starts with placeholders. We need two placeholders in order to fit our model: contains the network's inputs (the stock prices of all S&P 500 constituents at time ) and Y the network's outputs (the index value of the S&P 500 at time ).\n\nThe shape of the placeholders correspond to with meaning that the inputs are a 2-dimensional matrix and the outputs are a 1-dimensional vector. It is crucial to understand which input and output dimensions the neural net needs in order to design it properly.\n\nThe argument indicates that at this point we do not yet know the number of observations that flow through the neural net graph in each batch, so we keep if flexible. We will later define the variable that controls the number of observations per training batch.\n\nBesides placeholders, variables are another cornerstone of the TensorFlow universe. While placeholders are used to store input and target data in the graph, variables are used as flexible containers within the graph that are allowed to change during graph execution. Weights and biases are represented as variables in order to adapt during training. Variables need to be initialized, prior to model training. We will get into that a litte later in more detail.\n\nThe model consists of four hidden layers. The first layer contains 1024 neurons, slightly more than double the size of the inputs. Subsequent hidden layers are always half the size of the previous layer, which means 512, 256 and finally 128 neurons. A reduction of the number of neurons for each subsequent layer compresses the information the network identifies in the previous layers. Of course, other network architectures and neuron configurations are possible but are out of scope for this introduction level article.\n\nIt is important to understand the required variable dimensions between input, hidden and output layers. As a rule of thumb in multilayer perceptrons (MLPs, the type of networks used here), the second dimension of the previous layer is the first dimension in the current layer for weight matrices. This might sound complicated but is essentially just each layer passing its output as input to the next layer. The biases dimension equals the second dimension of the current layer\u2019s weight matrix, which corresponds the number of neurons in this layer.\n\nAfter definition of the required weight and bias variables, the network topology, the architecture of the network, needs to be specified. Hereby, placeholders (data) and variables (weighs and biases) need to be combined into a system of sequential matrix multiplications.\n\nFurthermore, the hidden layers of the network are transformed by activation functions. Activation functions are important elements of the network architecture since they introduce non-linearity to the system. There are dozens of possible activation functions out there, one of the most common is the rectified linear unit (ReLU) which will also be used in this model.\n\nThe image below illustrates the network architecture. The model consists of three major building blocks. The input layer, the hidden layers and the output layer. This architecture is called a feedforward network. Feedforward indicates that the batch of data solely flows from left to right. Other network architectures, such as recurrent neural networks, also allow data flowing \u201cbackwards\u201d in the network.\n\nThe cost function of the network is used to generate a measure of deviation between the network\u2019s predictions and the actual observed training targets. For regression problems, the mean squared error (MSE) function is commonly used. MSE computes the average squared deviation between predictions and targets. Basically, any differentiable function can be implemented in order to compute a deviation measure between predictions and targets.\n\nHowever, the MSE exhibits certain properties that are advantageous for the general optimization problem to be solved.\n\nThe optimizer takes care of the necessary computations that are used to adapt the network\u2019s weight and bias variables during training. Those computations invoke the calculation of so called gradients, that indicate the direction in which the weights and biases have to be changed during training in order to minimize the network\u2019s cost function. The development of stable and speedy optimizers is a major field in neural network an deep learning research.\n\nHere the Adam Optimizer is used, which is one of the current default optimizers in deep learning development. Adam stands for \u201cAdaptive Moment Estimation\u201d and can be considered as a combination between two other popular optimizers AdaGrad and RMSProp.\n\nInitializers are used to initialize the network\u2019s variables before training. Since neural networks are trained using numerical optimization techniques, the starting point of the optimization problem is one the key factors to find good solutions to the underlying problem. There are different initializers available in TensorFlow, each with different initialization approaches. Here, I use the , which is one of the default initialization strategies.\n\nNote, that with TensorFlow it is possible to define multiple initialization functions for different variables within the graph. However, in most cases, a unified initialization is sufficient.\n\nAfter having defined the placeholders, variables, initializers, cost functions and optimizers of the network, the model needs to be trained. Usually, this is done by minibatch training. During minibatch training random data samples of are drawn from the training data and fed into the network. The training dataset gets divided into batches that are sequentially fed into the network. At this point the placeholders and come into play. They store the input and target data and present them to the network as inputs and targets.\n\nA sampled data batch of flows through the network until it reaches the output layer. There, TensorFlow compares the models predictions against the actual observed targets in the current batch. Afterwards, TensorFlow conducts an optimization step and updates the networks parameters, corresponding to the selected learning scheme. After having updated the weights and biases, the next batch is sampled and the process repeats itself. The procedure continues until all batches have been presented to the network. One full sweep over all batches is called an epoch.\n\nThe training of the network stops once the maximum number of epochs is reached or another stopping criterion defined by the user applies.\n\nDuring the training, we evaluate the networks predictions on the test set \u2014 the data which is not learned, but set aside \u2014 for every 5th batch and visualize it. Additionally, the images are exported to disk and later combined into a video animation of the training process (see below). The model quickly learns the shape und location of the time series in the test data and is able to produce an accurate prediction after some epochs. Nice!\n\nOne can see that the networks rapidly adapts to the basic shape of the time series and continues to learn finer patterns of the data. This also corresponds to the Adam learning scheme that lowers the learning rate during model training in order not to overshoot the optimization minimum. After 10 epochs, we have a pretty close fit to the test data! The final test MSE equals 0.00078 (it is very low, because the target is scaled). The mean absolute percentage error of the forecast on the test set is equal to 5.31% which is pretty good. Note, that this is just a fit to the test data, no actual out of sample metrics in a real world scenario.\n\nPlease note that there are tons of ways of further improving this result: design of layers and neurons, choosing different initialization and activation schemes, introduction of dropout layers of neurons, early stopping and so on. Furthermore, different types of deep learning models, such as recurrent neural networks might achieve better performance on this task. However, this is not the scope of this introductory post.\n\nThe release of TensorFlow was a landmark event in deep learning research. Its flexibility and performance allows researchers to develop all kinds of sophisticated neural network architectures as well as other ML algorithms. However, flexibility comes at the cost of longer time-to-model cycles compared to higher level APIs such as Keras or MxNet. Nonetheless, I am sure that TensorFlow will make its way to the de-facto standard in neural network and deep learning development in research and practical applications. Many of our customers are already using TensorFlow or start developing projects that employ TensorFlow models. Also our data science consultants at STATWORX are heavily using TensorFlow for deep learning and neural net research and development. Let\u2019s see what Google has planned for the future of TensorFlow. One thing that is missing, at least in my opinion, is a neat graphical user interface for designing and developing neural net architectures with TensorFlow backend. Maybe, this is something Google is already working on ;)\n\nIf you have any comments or questions on my first Medium story, feel free to comment below! I will try to answer them. Also, feel free to use my code or share this story with your peers on social platforms of your choice.\n\nUpdate: I\u2019ve added both the Python script as well as a (zipped) dataset to a Github repository. Feel free to clone and fork."
    },
    {
        "url": "https://medium.com/mlreview/publish-with-ml-review-c814c54ca28d",
        "title": "Publish with ML Review \u2013 ML Review \u2013",
        "text": "We are happy to promote and share your content with our audience. We mainly focus on Machine Learning, Data Science, Statistics, and adjacent scientific fields such as Bioinformatics, Computational Economics and more.\n\nAfter we have published your article, you still remain the only owner of it and will be able to modify or remove it at any time. Before submitting please follow the checklist.\n\nTo become a writer, please send your article to mlreviewdotcom@gmail.com. If we find it interesting we would come back to you."
    },
    {
        "url": "https://medium.com/mlreview/spotify-analyzing-and-predicting-songs-58827a0fa42b",
        "title": "Spotify: Analyzing and Predicting Songs \u2013 ML Review \u2013",
        "text": "If there\u2019s one thing I can\u2019t live without, it\u2019s not my phone or my laptop or my car \u2014 it\u2019s music. I love music and getting lost in it. My inspiration for this project is finding out what it is about a song that I enjoy so much.\n\nI compare 2 of my playlists from Spotify:\n\nAfter using Python and some data wrangling techniques, the data frame below is what I use to do some exploratory data analysis (EDA).\n\n1. BPM \u2014 Beats per minute. The tempo of the song.\n\n2. Energy \u2014 The energy of a song \u2014 the higher the value, the more energetic the song\n\n3. Dance \u2014 The higher the value, the easier it is to dance to this song.\n\n4. Loud \u2014 The higher the value, the louder the song.\n\n5. Valence \u2014 The higher the value, the more positive mood for the song.\n\n6. Acoustic \u2014 The higher the value the more acoustic the song is.\n\n7. Popularity \u2014 The higher the value the more popular the song is.\n\nAgain, using Python, I was able to create this visualization of distributions between my Liked (blue) and Disliked (red) songs.\n\nLooking at the distributions of each feature, there are clear distinctions between my Liked and Disliked songs, especially in the ENERGY, DANCE, LOUD, and ACOUSTIC features.\n\nNow that I\u2019ve determined that there are clear differences between songs I like and songs I dislike, I create a predictive model.\n\nI use supervised learning, classification algorithms to predict whether I like or dislike a song. The 3 models I use are: k-Nearest Neighbor, Logistic Regression, and Random Forest.\n\nAfter balancing the data and splitting it into training and testing sets, I run the 3 models on the data. I decided to use the following metrics to score the quality of each model: ROC AUC, Accuracy, Precision, Recall.\n\nBelow are the results, using the default parameters for each classifier:\n\nLooking at the metrics and ROC Curves, the Random Forest Classifier is the clear winner. With an ROC AUC score of 91.94% and an accuracy score of 83.87%, the model performed fairly well on the test set with using just the default parameters.\n\nOn the next section, I will be performing hyperparameter tuning on the Random Forest classifier to see if the model can be improved.\n\nThe Random Forest Classifier has a number of parameters available. But I will only be tuning the following parameters with the following ranges and values:\n\nAfter using Scikit-Learn\u2019s to tune the parameters, the optimized parameters are as follows:\n\nAfter using the new optimized parameters, I compare the results of the model using default parameters and the model using the optimized parameters:\n\nThere is a definite improvement when using the optimized parameters, especially in Recall, with a 2.42% improvement.\n\nThe goals of this project were to find out what features of a song I like/dislike and to predict whether I like or dislike a song. Through exploratory data analysis and machine learning, these goals were accomplished.\n\nAfter doing some exploratory data analysis, I found that I like songs that are lower in ENERGY, lower in VALENCE (less positive songs), and are more ACOUSTIC and I dislike songs that have a higher BPM, are less DANCEABLE to, are LOUDER, and are less POPULAR.\n\nAfter trying three different models to predict whether I will like or dislike a song, the best performing model is Random Forest with hypertuned parameters. Overall, I am pleased with the results and believe the model can be useful in predicting whether or not you will like a song."
    },
    {
        "url": "https://medium.com/mlreview/case-study-sign-to-speech-converter-facilitated-wireless-communication-358b3914d398",
        "title": "Human-Computer Interaction Case Study: Sign to speech converter facilitated wireless communication",
        "text": "Quick Summary:\n\nWe designed this project particularly for specially abled people who are speech impaired. We created a wearable hand glove which they can wear and it converts the sign language (American Sign Language) to speech output. The glove works on the principles of Machine Learning Algorithm that identifies the gestures regardless of different hand sizes. Also, it is facilitated with a Bluetooth Module so that, two speech impaired people can talk to each other remotely within the range of 50 meters using the sign language.\n\nThis project was a part of our Final Semester project at the School of Engineering and Applied Science, Ahmedabad University. We were a team of two students I and Ms. Rachana Solanki interested in Interaction Design and Human-Computer Interaction and currently, pursuing our Masters\u2019 study in the same respectively.\n\nHow it was started?\n\nI and Rachana both had keen interest towards learning the concepts that how humans pursue their interaction with the real world, what are the problems they face and how they can be solved. So, when the project announcements were made, we decided to go for a project that allow us to work on gestures and their conversion. We kept searching on the internet, also did some secondary research and found different projects that had gesture interactions involved. From all those inspiring ideas, we selected this particular project. Obviously, there were many projects which were developed on this line before we started it and so, we decided to go through available research papers and find out the problems faced by such people and tried to give our best to solve them to the extent we could.\n\nThe challenge\n\nAfter going through a few research papers published by IEEE, we came to know about three major issues:\n\n1. Designing of a glove in a way that it can be worn and used at ease.\n\n2. The glove was generating different outputs for different hand sizes as the defined gestures were different for different hand sizes.\n\n3. There was no any alternate through which two speech impaired people can communicate remotely apart from video calls! (For that, they always need a laptop to carry with them! Much obvious reason!)\n\nSo, this was the basic motivation behind commencement of the project.\n\nDefining the roles, I was mainly responsible for defining the gestures and writing a Machine Learning Algorithm which can help us to provide an accurate output whereas, Rachana did a great job by making the glove and facilitate it with wireless communication.\n\n1. Approach and the interview\n\nFor starting out this project, we took the User Centred Design Approach (with some of the concepts of Genius Model) to design the user experience, on the basis of two courses of UCD we had taken during our semester studies. So, first we identified the problems (which are mentioned in The Challenges section) from our secondary research and later when we were discussing the problem, we decided to ask to some of the people who can guide us in the right directions. So, we went to one of the schools of specially abled people in Bhavnagar (My hometown) and did an interview with one of the faculties of that school, Mrs Kavita Shah and after talking to her, we found out some of the insights which are mentioned below:\n\nAll these points motivated us to pursue this project keenly with a humble intention to solve their problems as much as we can.\n\n2. Design and implementation of the glove:\n\nWe decided to use the flex sensors to identify the gestures as they are used as bend sensors to identify the value of bending. Our system is completely based on Arduino Platform where we have used Arduino Nano for the glove.\n\nAs it is shown in the block diagram, five flex sensors have been used for five fingers which are connected to the micro-controller (Arduino Nano in this case!) and a transmitter (Bluetooth Module) is connected to it to transmit the signs to the other systems.\n\nWe implemented k-NN algorithm for Machine Learning concepts and used supervised learning method to train our model. As our primary focus was to deploy simple English Alphabets, we took gesture inputs of 21 people and recorded all the alphabetical gesture instances which were about 33,000 and we used 66.66% of the total instances as our Training Data Set and rest of the instances as our Testing Data Set. And we were surprised enough that, our system worked with 96.9372% accuracy which is the highest in all the research papers we have read. We also tried different algorithms like ZeroR and Neural Network but, k-NN gave us the best result with just 0.03 second model building time.\n\nThe third major implementation was of Communication Protocols. We used HC05 Bluetooth Module to establish communication between two systems. As this system is still in the development mode, we are able to achieve one sided communication through the system.\n\nAnd that\u2019s how we designed and implemented our system.\n\nProblems faced\n\nThe main problem we have faced while working on this project was related to the design of glove. We were not able to attach the flex sensors on the glove as they were quite loose and were getting slipped easily. \n\nThe second problem was about establishing the communication system. HC05 modules are very hard to use especially with Arduino. I still remember that we were able to establish the communication and were able to work with that just two weeks before the final submission deadline.\n\nFailures and Successes: \n\nWhile working on our project, where we failed the most was that our research paper didn\u2019t get selected for IHCI 2017 conference. This happened just because we could not be able to develop the complete system which is end-to-end communication by the deadline.\n\nThe successful outcome that I loved the most about the project is its accuracy and ability to transmit and receive signs from the transmitter to the receiver. Another reasons to feel fortunate were the joy and happiness we experienced while and after doing this project. That were because we could do our bit for the people who actually need it instead of doing a repetitive kind of project usually done on Facebook or BookmyShow or Uber.\n\nWhat\u2019s next\n\nIn future, as an extension to our past work, we wish to develop the end-to-end system that comprises of two-way communication and later we plan to test it with the actual users with a view to receive their feedbacks and iterate our product.\n\nYou can see the demo videos on YouTube from the links given below:\n\nGeneral Scenario Demo: https://youtu.be/diUHu5csByY\n\nBluetooth Demo: https://youtu.be/hkx75P8xdxw\n\nIf you want to discuss more about this project, you can connect to me on LinkedIn! We would love to hear your advices or future possibilities for this project.\n\nWe would love to hear your loud applause appreciating the idea and outcome if it touched your heart."
    },
    {
        "url": "https://medium.com/mlreview/deep-neural-network-capsules-137be2877d44",
        "title": "Deep Neural Network Capsules \u2013 ML Review \u2013",
        "text": "A recent paper on Capsules has many important insights for revolutionizing learning in Deep Neural Networks. Let us see why!\n\nHere is a list of important points in the paper and video lecture:\n\nHere is a picture of CapsNet, the neural network architecture using Capsules. The interesting dynamic routing occurs between PrimaryCaps and DigitCaps.\n\nDynamic routing is implemented with two main transformation as reported in these equations (2 in paper). U are the outputs of Capsules in the layer below, and S are outputs from Capsules on layer above. U_hat is a prediction of what the output from a Capsule j above would be given the input from the Capsule i in layer below. This is very interesting as an instantiation of predictive neural networks.\n\nW_ij is a matrix of weights (like a linear layer) going from all capsules from one layer to the next. Notice there are as many W matrices as i*j. c_ij is another matrix that combines the contribution of lower layer capsules into the next layer output. Coefficients c_ij are computed with the dynamic routing algorithms described in the paper. The important point is that this is done by computing the agreement between the real output of next layer v and the prediction h_hat: b_ij \u2190 b_ij + u\u02c6_j|i * v_j\n\nNote 1: We like this paper because Capsules agrees with a lot of the work and thoughts we had in previous years, and that we named \u201cClustering Learning\u201d. See our previous publications here:\n\nhttps://lnkd.in/dnSgjJU\n\nhttps://lnkd.in/dmNbuVs\n\nNote 3: great blog post on this part I and part II.\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/mlreview/machine-learning-weekly-review-8-ccfe1354f1f3",
        "title": "Machine Learning Weekly Review \u21168 \u2013 ML Review \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/mlreview/experiments-with-a-new-loss-term-added-to-the-standard-cross-entropy-85b080c42446",
        "title": "Experiments with a New Loss Term Added to the Standard Cross entropy",
        "text": "Recently I came across this idea of center loss described in this paper. You define the outputs from the second last layers of the neural network as embeddings. For this loss, you define a per class center which serves as the centroid of embeddings corresponding to that class. The center loss term is defined as:\n\nAs the network gets updated with gradient descent, the per class center term needs to be updated. Ideally the update would involve going through the entire training data, but that is not feasible in practice. Thus the update is done over the mini-batch and a hyperparameter \u2018alpha\u2019 controls the learning rates of the centers. The update is given by:\n\nAnother scalar \u2018lambda\u2019 is used to balance the two loss functions. The total loss used for training the neural network is given by:\n\nTo see how the distribution of learned feature changes with the addition of this loss term, authors trained a neural network having embedding of size 2 with different values of lambda on the mnist dataset. This is what the features looked like when plotted:\n\nAs it can be seen, as lambda increases features are more spread apart from each other.\n\nThe authors of the paper used various face recognition datasets to test their results. Following were the results on different datasets:\n\nmodel A was trained using standard softmax loss, model B using softmax loss with contrastive loss, model C using softmax loss with center loss.\n\nAs it can be seen, on Megaface (which is a very challenging benchmark) center loss smashed all the previously published results.\n\nI wanted to check whether this idea will work with datasets involving smaller number of classes. I conducted experiments on 4 datasets in that directions. The datasets were cluttered-mnist, fashion-mnist, cifar-10 and cifar-100."
    },
    {
        "url": "https://medium.com/mlreview/machine-learning-weekly-review-7-93b9a7a7516c",
        "title": "Machine Learning Weekly Review \u21167 \u2013 ML Review \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/mlreview/generalizing-from-few-examples-with-meta-learning-25dbcea94564",
        "title": "Generalizing from Few Examples with Meta-Learning \u2013 ML Review \u2013",
        "text": "Hugo Larochelle is a Research Scientist for Google Brain\u2019s Montreal team, Adjunct Professor at Universit\u00e9 de Sherbrooke, Adjunct Professor at Universit\u00e9 de Montr\u00e9al, and Associate Director for Canadian Institute for Advanced Research. Larochelle co-founded Whetlab (acquired by Twitter) and later worked in Twitter Cortex\u2019s group as a Research Scientist. He received his Ph.D from Universit\u00e9 de Montr\u00e9al under the supervision of Yoshua Bengio. He is most interested in applications of deep learning to generative modeling, meta-learning, natural language processing and computer vision.\n\nWith The Best had the opportunity ask him a few questions about his work and expertise in deep learning and neural networks.\n\nQ: How has deep learning changed since you first started as a student?\n\nWell, when I first started as a PhD student, \u201cdeep learning\u201d wasn\u2019t even a common expression, so it has changed a lot! We weren\u2019t yet using GPUs to train models and were mostly focused on developing unsupervised pre-training algorithms (RBMs, autoencoders, etc.). There weren\u2019t a lot of mainstream tools for implementing neural networks (Theano didn\u2019t even exist yet) and arXiv wasn\u2019t as popular a medium for science dissemination in deep learning. In fact, the largest machine learning conferences would feature only a handful of papers on neural networks.\n\nQ: What are the biggest challenges with deep learning and neural networks?\n\nOne big challenge is developing valuable theories of properties of neural networks. A lot of our understanding of neural networks comes from experimentation, as opposed to theorems. This makes it harder to reliably deepen our understanding of these methods, since only glancing at the mathematical formulation of neural networks doesn\u2019t provide as much insight as one might expect. Instead, we must design experiments on many datasets, which don\u2019t even provide solid guaranties that insights coming from these will generalize to other datasets not covered by these experiments.\n\nQ: Earlier this year you released a dataset called GuessWhat?! Do you see the dataset evolving from answering \u2018Yes/No\u2019 to answering with full sentences and description?\n\nGuessWhat?! is a great project, for which my collaborators Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin and Aaron Courville deserve a lot of credit as well.\n\nThere\u2019s actually already a lot of work on visual question answering that goes beyond \u201cYes/No\u201d questions. Devi Parikh, Dhruv Batra and collaborators have done a lot of very interesting work there, including on multi-turn visual dialog where responses go beyond \u201cYes/No\u201d.\n\nSo the next challenge for this kind of work might instead be in discovering how such datasets / tasks can be useful for learning better image / text representations, that can be useful to improve computer vision / natural language understanding systems in general.\n\nQ: What trends do you see happening in the future of deep learning?\n\nI\u2019m particularly excited about few-shot learning, i.e. the problem of designing methods that are able to learn new concepts from a handful of examples (as opposed to many thousands). Right now, deep learning methods based on the idea of meta-learning seem promising. In meta-learning (also referred to as \u201clearning to learn\u201d), the deep learning model is itself a learning algorithm, that is end-to-end trainable. The hope is that by (meta-)training this model on a lot of different few-shot learning tasks, we will be able to learn a procedure that can \u201cunderstand\u201d new concepts from a handful of examples.\n\nQ: What advice do you have for students and newbies to the deep learning world?\n\nI would recommend finding a good balance between coding and reading papers, as both are important to become a strong deep learning researcher. For coding, thankfully there are plenty of open source implementations of deep learning models that one can start and tinker with. For reading papers, the pace of deep learning research right now makes it important to keep an eye on arXiv preprints as they are made available, instead of waiting for conference proceedings. Thankfully, tools like Andrej Karpathy\u2019s arXiv-sanity (http://www.arxiv-sanity.com/, which can recommend papers based on your preferences) and social media (where you can follow researchers that work on the same topic as you) makes the task of keeping up with work that\u2019s relevant to your research a bit less daunting.\n\nFinally, to start with, I would suggest perhaps focusing on only one topic in deep learning. The videos from the CIFAR Deep Learning summer schools in Montreal (available on http://videolectures.net/) provide a good overview of recent research topics. I\u2019d recommend going over those and pick the research topic that is most inspiring or interesting to you.\n\nLarochelle offers an online deep learning and neural network course which is free and accessible on Youtube. There\u2019s plenty of time to study his videos before his talk with us on 14\u201315 October! With The Best is proud to have him as a speaker for AI WTB."
    },
    {
        "url": "https://medium.com/mlreview/getting-inception-architectures-to-work-with-style-transfer-767d53475bf8",
        "title": "Getting Inception Architectures to Work with Style Transfer",
        "text": "Getting Inception Architectures to Work with Style Transfer Style transfer typically requires neural networks with a well-developed hierarchy of features for calculating the loss. Thus for this purpose, the good old vgg-16 and vgg-19 architectures work very well. But inception architectures (unlike resnet architectures) also have the same property. I wanted to see how inception architectures could be used for style transfer. Getting these architectures to work with style transfer required some tweaks. Here is a blog post describing the tweaks I had to make. The following content images were used for these experiments:\n\nAll pretrained loss networks used in these experiments were downloaded from tensorflow slim-models repository. inception-v3 trained on openimages was obtained from this script. The code used in these experiments is available on github. To find out, which layers I mean by Conv2d_2c_3x3, Mixed_3b etc for inception-v1, run in the repo. Similarly for inception-v2, inception-v3, inception-v4, vgg-16 and vgg-19. Checkerboard artifacts can occur in images generated from neural networks. They are typically caused when we use transposed 2d convolution with kernel size not divisible by stride. For a more in depth discussion on checkerboard artifacts, read this post. Backpropagation through a convolution is transposed convolution. Thus when training an image using a loss network, checkerboard artifacts can occur when the loss network has a convolution layer with kernel size not divisible by stride. In inception-v1, v2, v3 and v4 architectures. The first layer has stride 2, and kernel size 7 (in v1 and v2) or 3 (in v3 and v4). Both 7 and 3 are not divisible by 2. So there was a possibility that checkerboard artifacts will be created here. To check whether this was the case or not, I trained a noise image on only the content loss using Conv2d_1a_7x7 (from inception-v1 architecture). This image was generated. This looks, normal but on zooming in, checkerboard artifacts become visible.\n\nWhen vgg-16 and vgg-19 networks are used as loss networks, max pooling layer is replaced with average pooling. This improves the gradient flow through the loss network and causes the image to converge faster. In inception networks, downsampling pooling layers are of stride 2, with kernel size 3. There are also pooling layers that don\u2019t downsample (stride 1) in inception blocks and kernel size 3. I replaced all max pooling layers with avg pooling. My intuition was that because of higher kernel size, the distribution represented by avg pooling differs from the distribution of max pooling significantly. To test this hypothesis, I first tried recreating the original content image using max pool and avg pool. The content loss was calculated from the layer \u2018Conv2d_2c_3x3\u2019 of inception-v1 network. The following were the images generated:\n\nClearly image generated using avg pooling is no good. Comparing content loss of image with avg and max pooling Next, I plotted the content loss (with max and average pooling), to see what was going on. As visible, the content loss for avg pooling fluctuates while content loss for max pooling converges to a small value. Next, I tried generating the image using average pooling layer with kernel size 2, instead of 3. And it worked wonderfully well. The below images show the difference:\n\nPlot of content loss for avg pooling with ksize 2 and ksize 3 The left is a plot of content loss between avg pooling with kernel size 2 and avg pooling with kernel size 3. As it is visible, the loss network with average pooling of size 2 converges to a small value while kernel size 3 fluctuates. All the following experiments can be recreated using python slow_style.py with the following command line arguments As specified in this repo. Default values of all parameters was used unless specified. Experiment #1: Reconstructing content images from different layers of inception-v1 and compare with vgg-16 I tried reconstructing the image of brad pitt using different layers of inception-v1 and vgg-16 and then comparing. The following layers were used for inception-v1: Conv2d_2c_3x3, Mixed_3b, Mixed_3c, Mixed_4b. For reconstruction from vgg-16, the following layers were used: conv2_2, conv3_1, conv3_2, conv4_1. The rationale behind choosing these layers was their relative distance from respective pooling layers. conv2_2 and Conv2d_2c_3x3 are last layers before second pooling in vgg-16 and inception-v1 respectively, similarly conv3_1 and Mixed_3b are first layers after second pooling and so on. style weight and tv weight was set to zero.\n\nIn both inception-v1 and vgg-16, content is captured very well by first few layers, the original image can be completely reconstructed till conv3_2/Mixed_3c. After that, the drop in the quality of the reconstructed image is quite significant for inception-v1. Experiment #2: Reconstructing style images from different layers of inception-v1 and compare with vgg-16 I tried reconstructing the pastiches of the style image using different layers of inception-v1 and vgg-16 and then comparing. The following layers were used in the experiment for inception-v1: Mixed_3b, Mixed_3c, Mixed_4b, Mixed_4c, Mixed_5b. content weight and tv weight was set to zero. For vgg-16, layers used were: conv3_1, conv3_2, conv4_1, conv4_2 and conv5_1. The layers chosen were again in reference to their distance from respective pooling layers. conv3_1 and Mixed_3b are outputs of first convolution layers after second pooling in vgg-16 and inception-v1 respectively, conv3_2 and Mixed_3c after second convolution after second pooling and so on. Below were the results when using inception-v1:\n\nThe pastiches generated by vgg-16 are much richer than the ones generated by inception-v1. Moreover, the pastiches of inception-v1 look more like crayons, while those of vgg-16 look like oil paintings. Experiment #3: Train using different layers of inception-v1 network and compare with vgg-16 outputs For inception, I used Conv2d_4a_3x3 for calculating the content loss. For style loss, I used Mixed_3b, Mixed_3c and Mixed_4b layers. For vgg-16, I used conv2_2 for calculating the content loss. For style loss, I used conv3_1, conv3_2 and conv4_1 layers. The content weight was 8, style weight was 3200, tv weight was 10 for both the networks.\n\nAnd these results do look like oil paintings, like the pastiches did in the previous experiment :). Experiment #4: Train using inception-v3 networks trained on openimages and imagenet Next, to check what difference between the images generated by inception-v3 architecture trained on imagenet and openimages, I did another experiment. For content loss, I used the layer Mixed_5b. For style loss, I used Mixed_5b, Mixed_5c, Mixed_6a one by one. Again, as before I first tried generating the pastiches by setting content loss and tv loss to zero for both inception-v3 trained on openimages and imagenet. The following images were generated:\n\nThe trend here is quite similar to the trend when using vgg-16 network. content loss converges to a higher value with max pooing than avg pooling. Again, like the previous experiment, I used inception-v2, content layer was Mixed_3b, style layer was Mixed_3c. content weight was 8, style weight was 43400, tv weight was 10, pooling used was \u2018avg\u2019. I chose such a high style weight because with lower style weight, none of the style was visible.\n\nAs it can be observed, content loss converges to a much much higher value for noise initialization, then content. More importantly, none of the content is visible in the generated image. Next, I tried gradually increasing the content weights, 8 to 80, 160, 320, 640. to see how the stylized image generated using \u2018noise\u2019 initialization changes. The following images were generated:\n\nAs expected, the content is more and more preserved as we increase the content weight, and the image does match the pastiches generated in the previous experiment. But none of the stylized images look as good as the ones with \u2018content\u2019 initialization. Next I tried generating the stylized image with \u2018content\u2019 initialization with zero content weight and zero tv weight. image generated with zero content weight and tv weight This looks much better than any of the images generated with noise initialization. What I can conclude from here is, to generate good images using inception-v2, initialization makes a big difference. If you liked this article, please help others find it by clicking the little clap icon below. Thanks a lot!"
    },
    {
        "url": "https://medium.com/mlreview/you-are-not-prepared-some-advice-ive-received-on-how-to-be-a-professor-85350cccabdb",
        "title": "You Are Not Prepared: Some Advice I\u2019ve Received on How to Be a Professor",
        "text": "Professors wear many hats: researcher is just one of those hats. The other hats include: mentor, teacher, fund-raiser, steward, advocate, marketer, governer, manager and yarn spinner.\n\nAs a grad student or postdoc, you are mostly trained to be good a researcher. You\u2019re also probably exposed to teaching, and you might even play some role in fund-raising, marketing or yarn spinning. But, you are rarely The Person Responsible in any role apart from researcher.\n\nIn other words, everything you\u2019ve done to get this job is only one part of the job. It\u2019s perhaps the most important part of the job, but, as a proportion of how you will be spending your time, it\u2019s just a small part.\n\nThe upshot of all this is that \u201cyou are not prepared\u201d. But, neither are you unique in that: nobody is prepared. So, expect to be overwhelmed and be prepared to adapt to shifting time demands.\n\nKeep sight of why you chose to be a professor through all of the noise of the job. Let that vision guide you even when it feels like you\u2019re drowning. Do that, and, apparently, you may be alright.\n\nA corollary to wearing many hats is that there are many rules associated with the each hat you wear. Writing grants has a set of rules and processes; teaching has a set of rules and processes; advising has a set of rules and processes and so on and so forth.\n\nYour university likely has administrative structures and personnel in place to help you navigate these different aspects of your job. Learn about these resources and the staff who can help you as soon as you get on campus. Maybe even before. Some common examples include: a center for teaching and learning, dedicated staff who will create your budget sheets for NSF grants, staff who are dedicated to interfacing with non-government funding agencies and foundations, and a media office who may try and get your work some publicity.\n\nUniversities are built for students. You were hired to be an adviser, a mentor, and a teacher to the students of your institution. Everything you do as a professor should be, directly or indirectly, in service of your students. Luckily, your success is largely evaluated by your students\u2019 success so, in that way, your motivations should be aligned with this responsibility.\n\nYou\u2019ve probably heard that your first year as an Assistant Professor is stressful, overwhelming, unpredictable, difficult, exhausting and painful. It can be all of those things. Apparently, though, you will also never have more free time as a professor than in this first year.\n\nThe administrative support staff your department provides will help you. A lot. With grant submissions, with hiring GRAs, with interfacing with the internal systems your university uses and requires you to use. And, as they will be assisting many faculty in your department, they will probably have a long backlog of work that they need to do. Appreciate and befriend them as soon as you get on campus. Buy them a coffee. You\u2019re going to need them.\n\nIf you\u2019re anything like me, it might seem\u2026an interesting choice that you have been trusted with managing all the money you will be asked to manage as faculty. You can use that money to make important purchases that you need for your research. However, every once in a while, your university / department might get audited by a funding agency in order to make sure that their grant money is being put to good use.\n\nMake sure you clearly document why purchasing equipment / resources is relevant to a research grant.\n\nIn theory, you should not be being judged for the number of dollars you bring in through grants and gifts. In practice, running a research lab and advising students takes a lot of money. Also in practice, raising money takes time, work and luck. Depending on your field and research interests, grants can range from being very competitive (10\u201315% or so) to extremely competitive (<5%).\n\nApply for funding early \u2014 don\u2019t wait for your start-up money to run low before you think about getting grants. Even if you don\u2019t get awarded the grant, the effort you put in getting funding will be appreciated when it comes time for your department to re-appoint you.\n\nThe other faculty in your department are going to be your colleagues for a long time. Try to be their friend, too. The best research ideas often come about from unexpected collaborations and the best way to make that happen is to have friends who know you, like you and can make research connections both with you and for you.\n\nA good way to spend time your first semester is taking other faculty in your department out to coffee. Introduce yourself and get to know them.\n\nYou\u2019ve probably heard of the three pillars of professing: research, teaching and service. Service can be external or internal: external being things like taking on administrative responsibilities in service of your broader academic community (e.g., being the program chair for a conference), internal being taking on responsibilities in service of your university.\n\nThis internal service can take on many forms: joining committees to discuss new degree programs or student admissions or faculty hiring, serving on a faculty senate to help with the governance of the university, taking on administrative duties to help ensure that the faculty in your department can quickly and easily access the resources they need.\n\nThe point is this: as a member of the faculty of the university, your university is investing a lot in hopes of your success. Part of the expected return on this investment is your being a good citizen and helping with the governance of the university and department.\n\nService is a necessary part of your being a good academic citizen. But, when you are just hired, you have this big hurdle you need to get past: tenure. Service is likely the smallest consideration in your tenure case (relative to your research and teaching), so many universities try to \u201cprotect\u201d their junior faculty from taking on too much service. As a green-horn professor, you should learn to say \u201cno\u201d. Not to everything, of course, but no one will know how thin your time is already spread better than you. Your highest priority should be getting your research agenda up and running. It\u2019ll be hard to do that if you take on too much service.\n\nGood service responsibilities to take on as an assistant professor are those that can help you spool up your research: for example, the Ph.D. student admissions committee. You\u2019ll probably need good students to do good research (this is largely field driven, and there are exceptions) so being able to directly have a say in who gets in may be useful.\n\nThis advice was echoed by pretty much everyone. You\u2019ll need a mentor at your university who is already tenured and is well acquainted with your subfield who can answer questions like: What grants should I apply to? What kind of lab should I create? What should I do my first N years? Who should I talk to about this research idea? What can I do to recruit students?\n\nMany universities will have a formal mentorship program in place where they will set you up with an appropriate mentor who you will meet at least once a semester. If yours does not, or if you don\u2019t get along well with your assigned mentor, don\u2019t be afraid to just find someone who you think would be a good fit, take them out to coffee, and ask them any questions you might have. Your department has spent a lot of time, energy and money to get you there \u2014 they\u2019re invested in your success.\n\nYour need to be well known not just to the people in your university but also to your broader academic community. One effective way is to ask senior colleagues who you respect to be, informally, a part of an \u201cadvisory\u201d board. You\u2019ll very occasionally ask them questions or ask for their advice on big career decisions, and they\u2019ll get to know who you are and keep track of your progress.\n\nYou might be asking: What\u2019s in it for them? I\u2019m not sure. Again, this is just advice I\u2019ve received. I can speculate, though. As a newly hired Assistant Professor, you are part of the future of their discipline. Being able to pass down their approach to academia to others outside of their immediate students is a strong form of impact. Moreover, it\u2019s flattering to be asked to serve as someone\u2019s mentor and academics like to give advice. Anyway \u2014 the worst that can happen if you ask is that they say they are too busy or do not reply. That\u2019s not so bad.\n\nAdvice from senior colleagues is important, but commiserating with junior faculty is also important. The other junior faculty in your department are the people you will be \u201cgrowing up\u201d with as a faculty member. They\u2019ll share similar struggles and will have their own strategies for dealing with overwhelm, doubt and time management. Moreover, they, along with you, will be the future leaders of your field and department. You should get to know them.\n\nAcademic work is interdependent and unpredictable. Recognize that your work will rarely go according to plan. Expect and embrace interruptions and sudden breaches in schedule.\n\nThere should always be certain barriers between advisers and advisees but you must know your students in order to be an effective adviser and understand the constraints with which they are dealing.\n\nIf your student is dealing with a family death, or is recently engaged, or is working a part-time consulting job so that she can pay rent, you should know that so you can contextualize their progress, calibrate your expectations and provide help if necessary or appropriate.\n\nThis one was a bit counter-intuitive. The basic idea is that tenure is a matter of your reputation in your field. Your reputation is assessed through letters of recommendation by known leaders in your field who do not have a conflict of interest with you. A conflict of interest would be, for example, being a family member, a close friend or a direct collaborator.\n\nSo, if there is someone at University X who would be the perfect tenure letter writer for you, but also the perfect collaborator, you may need to strategically hold off that collaboration until after tenure.\n\nI\u2019m not sure how much I like this advice, but it is pragmatic.\n\nTeaching takes a lot of time, especially the first few times you do it. Teaching, in fact, can take up all of your time if you\u2019re not careful. A friend told me he spent somewhere on the order of 40 hours/week on teaching the first time he taught. But, teaching is also important and a necessary part of your job. So, you should time box.\n\nSet aside distinct blocks of time in your week for teaching related responsibilities: prep, grading, office hours, etc. These distinct blocks of time should be significant \u2014 I\u2019ve heard that you generally need 3\u20135 hours of prep per 1 hour in the classroom. Do not exceed those pre-allocated blocks of time so that you can also make progress on your many other responsibilities.\n\nThat ratio I mentioned before: 3\u20135 hours of prep per 1 hour in the classroom? That only applies the first time you teach a course. The amount of prep you need the second time you teach the same course is significantly reduced, and significantly reduced again the third time.\n\nIf possible, then, you should try and repeat teaching assignments pre-tenure. This will help you save time and allocate it towards spooling up and strengthening your research agenda. There\u2019s a trade-off, of course: teach a course too many times and it might become boring. If you find a course boring, the quality of your instruction might drop.\n\nLimiting course preparations pre-tenure is commonly offered advice. I also heard a counter point that plays off one of the first pieces of advice I mentioned above: \u201cYou will never more time than you do now\u201d. If that\u2019s true, then perhaps you should do multiple course preps now so that later, when you have less time, you will be prepared to teach a variety of different courses as the need arises. The person who gave me this advice mentioned doing 4 course preparations pre-tenure and then not having to do another for 8 years.\n\nThis advice should be obvious enough, but it can be difficult to execute if, for example, a new department head is hired from an external search at some point before you go up for tenure.\n\nYour department head is probably the most important Position of Power standing in between you and tenure. Every other Position of Power (e.g., the Dean, Provost and President) is usually at least one step removed from you and will largely rely on the judgement of your department head when it comes time to deciding if you should or should not get tenure. Naturally, then, you should make some effort to get to know your department head and keep them up to date on your efforts.\n\nAgain, tenure is largely about establishing a (positive) global reputation for yourself and your work. You should be recognized as a Leader in Your Discipline by other Leaders in Your Discipline, both inside and outside of your university.\n\nA good way to acquaint yourself with these Leaders in Your Discipline is by inviting them to give an all-expenses-paid seminar talk at your department. Many departments pre-allocate funds towards a seminar series and allow their faculty to invite interesting speakers with those funds. If your department doesn\u2019t already have such a seminar series, try and set one up. These seminars are a good way to expose faculty and students to interesting ideas going on in other discplines and universities, and can also help start cross-university collaborations.\n\nIf there is no departmental financial support for a seminar series, invite external speakers anyway. Invited talks look good on a CV, so maybe the person you invite will try and do it anyway. You can probably increase your chances if you invite them at a time they\u2019ll be in or close to your city anyway. Be upfront about not being able to pay for their expenses, of course.\n\nIt\u2019s even better if you give a seminar talk at another university, industry lab, government agency or conference. Especially if doing so doesn\u2019t come out of your own travel budget. These opportunities will arise every once in a while. If your schedule permits, you should absolutely do them. Travel can be a pain, especially when you are juggling all of the responsibilities of being a new professor and trying to maintain a speaking relationship with your family. Still, your primary job pre-tenure is to get yourself and your work known and appreciated. So, try not to turn these opportunities down unless you really must.\n\nAs you come closer to tenure, it\u2019s a good idea to strategically \u201cinvite yourself\u201d to give seminar talks at other universities. How? First, enumerate people who you expect a tenure committee would ask to write a letter for you. Then, starting about two years before you\u2019re up for tenure, e-mail the people on your list that you\u2019d love to give a talk on your research at their department. Even if they can\u2019t compensate you, just say that you were planning to be in town anyway.\n\nMost departments would be happy to host a seminar speaker, especially if they don\u2019t have to foot the bill. Try and pool multiple talks in geographically close locations together \u2014 that way, your expense is limited and perhaps you might even find a way to be compensated for part of your trip by one of the places you are giving a talk.\n\nYour job is not to write grants, it\u2019s to do good research. Writing grants is a means to an end. Consider alternative paths to that end. Depending on your discipline, there\u2019s a good chance that there are many industry-sponsored funding opportunities. Also remember that industry has more than just money: often they have expensive equipment as well as rich datasets. So, pursue collaborations with academic colleagues in industry research labs that might be mutually beneficial. Industry collaborations are also a great way to have impact beyond just papers and citations.\n\nA good way to establish these relationships is by encouraging your students to pursue internships in industry. Your students can be a good bridge between a university and a company. They can take your ideas into industry and bring their ideas back.\n\nYou\u2019ve been an advisee, but being an adviser is a different sort of beast. It\u2019s a skill that you must cultivate.\n\nImportantly, no two students are alike, and your advising style should be adaptable and personalized to the student. It may be tempting to borrow your adviser\u2019s style but change everything you would have wanted done differntly. That may be a good starting point. But, remember, you are not your adviser and your student is not you.\n\nYour role as adviser is one that you can define, but generally requires you to be a guide, a mentor, a champion, an advocate and a facilitator among other things. Remember the point above: \u201cYour university exists for the student\u201d.\n\nI got this advice from pretty much everyone. Your first semester, you will have a lot to do: you\u2019ll be learning about the job, figuring out how to write grants, setting up your living situation, getting to know students and colleagues. It may be tempting to want a teaching release just so that you can figure things out.\n\nBut, you shouldn\u2019t take a teaching release, if you were offered one, in your first semester. Teaching can be a concrete thing for you to do that can help you structure your time. Everything else may be total chaos, but at least you know you have a class to teach on Tuesdays on Thursdays. Moreover, because you won\u2019t yet have any Ph.D. students, teaching a class can be a good way interact with students and get some of them interested in your research.\n\nYou\u2019ll want to save your teaching release for sometime later when you\u2019ve found your groove: when you have Ph.D. students and are making progress towards your research agenda. That way, you can make the most effective use of the time you save from not having to teach.\n\nIf possible, try and teach a small, graduate seminar about your subfield your first semester. If that\u2019s not possible, try and teach a course that is already well organized and structured so that you have do not have to start from scratch."
    },
    {
        "url": "https://medium.com/mlreview/machine-learning-weekly-review-6-f429f11a8ce9",
        "title": "Machine Learning Weekly Review \u21166 \u2013 ML Review \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/mlreview/machine-learning-weekly-review-5-453de779e612",
        "title": "Machine Learning Weekly Review \u21165 \u2013 ML Review \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/mlreview/machine-learning-weekly-review-4-2679686e7134",
        "title": "Machine Learning Weekly Review \u21164 \u2013 ML Review \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/mlreview/machine-learning-weekly-review-3-5d275bfdd5ad",
        "title": "Machine Learning Weekly Review \u21163 \u2013 ML Review \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/mlreview/machine-learning-weekly-review-2-8789ca50c5db",
        "title": "Machine Learning Weekly Review \u21162 \u2013 ML Review \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/mlreview/machine-learning-weekly-review-992c2c6ea47",
        "title": "Machine Learning Weekly Review \u21161 \u2013 ML Review \u2013",
        "text": "Thanks for reading! If you find the content valuable, you will find more on our Twitter @ML_Review.\n\nPlease tell us if you think we forgot anything interesting or any ideas how we can be more useful for you.\n\nSee you next week!"
    },
    {
        "url": "https://medium.com/mlreview/10-deep-learning-projects-based-on-apache-mxnet-8231109f3f64",
        "title": "10 Deep Learning projects based on Apache MXNet \u2013 ML Review \u2013",
        "text": "So far, we ran our MXNet code on Amazon EC2 instances, just like any other Python application. As you may know, there are alternative ways to run code on AWS and they can be obviously applied to MXNet.\n\nUsing a CloudFormation template, this project will create an automated workflow that will provision, configure and orchestrate a pipeline triggering deployment of any changes to your MXNet model or application code. You will orchestrate all of the changes into a deployment pipeline to achieve continuous delivery using CodePipeline and CodeBuild. You can deploy new MXNet APIs and make those available to your users in just minutes, not days or weeks.\n\nMore information in the companion blog post:\n\nThis is a reference application that predicts labels along with their probabilities for an image using a pre-trained model with Apache MXNet deployed on AWS Lambda. A Serverless Application Model template (SAM) and instructions are provided to automate the creation of an API endpoint.\n\nYou can leverage this package and its precompiled libraries to build your prediction pipeline on AWS Lambda with MXNet. Additional models can be found in the Model Zoo"
    },
    {
        "url": "https://medium.com/mlreview/implementing-malstm-on-kaggles-quora-question-pairs-competition-8b31b0b16a07",
        "title": "How to predict Quora Question Pairs using Siamese Manhattan LSTM",
        "text": "In the past few years, deep learning is all the fuss in the tech industry.\n\nTo keep up on things I like to get my hands dirty implementing interesting network architectures I come across in article readings.\n\nFew months ago I came across a very nice article called Siamese Recurrent Architectures for Learning Sentence Similarity.It offers a pretty straightforward approach to the common problem of sentence similarity.\n\nNamed MaLSTM (\u201cMa\u201d for Manhattan distance), its architecture is depicted in figure 1 (diagram excludes the sentence preprocessing part). \n\nNotice that since this is a Siamese network, it is easier to train because it shares weights on both sides.\n\nSo first of all, what is a \u201cSiamese network\u201d? \n\nSiamese networks are networks that have two or more identical sub-networks in them.\n\nSiamese networks seem to perform well on similarity tasks and have been used for tasks like sentence semantic similarity, recognizing forged signatures and many more.\n\nIn MaLSTM the identical sub-network is all the way from the embedding up to the last LSTM hidden state.\n\nWord embedding is a modern way to represent words in deep learning models. More about it can be found in this nice blog post.\n\nEssentially it\u2019s a method to give words semantic meaning in a vector representation.\n\nInputs to the network are zero-padded sequences of word indices. These inputs are vectors of fixed length, where the first zeros are being ignored and the nonzeros are indices that uniquely identify words.\n\nThose vectors are then fed into the embedding layer. This layer looks up the corresponding embedding for each word and encapsulates all them into a matrix. This matrix represents the given text as a series of embeddings.\n\nI use Google\u2019s word2vec embedding, same as in the original paper.\n\nThe process is depicted in figure 2.\n\nWe have two embedded matrices that represent a candidate of two similar questions. Then we feed them into the LSTM (practically, there is only one) and the final state of the LSTM for each question is a 50-dimensional vector. It is trained to capture semantic meaning of the question.\n\nIn figure 1, this vector is denoted by the letter h.\n\nIf you don\u2019t entirely understand LSTMs, I suggest reading this wonderful post.\n\nBy now we have the two vectors that hold the semantic meaning of each question. We put them through the defined similarity function (below)\n\nand since we have an exponent of a negative the output (the prediction in our case) will be between 0 and 1.\n\nThe optimizer of choice in the article is the Adadelta optimizer, which can be read about in this article. We also use gradient clipping to avoid the exploding gradient problem. You may find a nice explanation of the gradient clipping in this video from the Udacity deep learning course.\n\nThis is where I will diverge a little from the original paper. For the sake of simplicity, I do not use a specific weight initialization scheme and do not pretrain it on a different task.\n\nOther parameters such as batch size, epochs, and the gradient clipping norm value are chosen by me."
    },
    {
        "url": "https://medium.com/mlreview/a-guide-to-receptive-field-arithmetic-for-convolutional-neural-networks-e0f514068807",
        "title": "A guide to receptive field arithmetic for Convolutional Neural Networks",
        "text": "The receptive field is perhaps one of the most important concepts in Convolutional Neural Networks (CNNs) that deserves more attention from the literature. All of the state-of-the-art object recognition methods design their model architectures around this idea. However, to my best knowledge, currently there is no complete guide on how to calculate and visualize the receptive field information of a CNN. This post fills in the gap by introducing a new way to visualize feature maps in a CNN that exposes the receptive field information, accompanied by a complete receptive field calculation that can be used for any CNN architecture. I\u2019ve also implemented a simple program to demonstrate the calculation so that anyone can start computing the receptive field and gain better knowledge about the CNN architecture that they are working with.\n\nTo follow this post, I assume that you are familiar with the CNN concept, especially the convolutional and pooling operations. You can refresh your CNN knowledge by going through the paper \u201cA guide to convolution arithmetic for deep learning [1]\u201d. It will not take you more than half an hour if you have some prior knowledge about CNNs. This post is in fact inspired by that paper and uses similar notations.\n\nThe receptive field is defined as the region in the input space that a particular CNN\u2019s feature is looking at (i.e. be affected by). A receptive field of a feature can be described by its center location and its size. (Edit later) However, not all pixels in a receptive field is equally important to its corresponding CNN\u2019s feature. Within a receptive field, the closer a pixel to the center of the field, the more it contributes to the calculation of the output feature. Which means that a feature does not only look at a particular region (i.e. its receptive field) in the input image, but also focus exponentially more to the middle of that region. This important insight will be explained further in another blog post. For now, we focus on calculating the location and size of a particular receptive field.\n\nFigure 1 shows some receptive field examples. By applying a convolution C with kernel size k = 3x3, padding size p = 1x1, stride s = 2x2 on an input map 5x5, we will get an output feature map 3x3 (green map). Applying the same convolution on top of the 3x3 feature map, we will get a 2x2 feature map (orange map). The number of output features in each dimension can be calculated using the following formula, which is explained in detail in [1].\n\nNote that in this post, to simplify things, I assume the CNN architecture to be symmetric, and the input image to be square. So both dimensions have the same values for all variables. If the CNN architecture or the input image is asymmetric, you can calculate the feature map attributes separately for each dimension."
    },
    {
        "url": "https://medium.com/mlreview/topic-modeling-with-scikit-learn-e80d33668730",
        "title": "Topic Modeling with Scikit Learn \u2013 ML Review \u2013",
        "text": "Latent Dirichlet Allocation (LDA) is a algorithms used to discover the topics that are present in a corpus. A few open source libraries exist, but if you are using Python then the main contender is Gensim. Gensim is an awesome library and scales really well to large text corpuses. Gensim, however does not include Non-negative Matrix Factorization (NMF), which can also be used to find topics in text. The mathematical basis underpinning NMF is quite different from LDA. I have found it interesting to compare the results of both of the algorithms and have found that NMF sometimes produces more meaningful topics for smaller datasets. NMF has been included in Scikit Learn for quite a while but LDA has only recently (late 2015) been included. The great thing about using Scikit Learn is that it brings API consistency which makes it almost trivial to perform Topic Modeling using both LDA and NMF. Scikit Learn also includes seeding options for NMF which greatly helps with algorithm convergence and offers both online and batch variants of LDA.\n\nI won\u2019t go into any lengthy mathematical detail \u2014 there are many blogs posts and academic journal articles that do. While LDA and NMF have differing mathematical underpinning, both algorithms are able to return the documents that belong to a topic in a corpus and the words that belong to a topic. LDA is based on probabilistic graphical modeling while NMF relies on linear algebra. Both algorithms take as input a bag of words matrix (i.e., each document represented as a row, with each columns containing the count of words in the corpus). The aim of each algorithm is then to produce 2 smaller matrices; a document to topic matrix and a word to topic matrix that when multiplied together reproduce the bag of words matrix with the lowest error.\n\nHow many topics? Well that is the question! Both NMF and LDA are not able to automatically determine the number of topics and this must be specified.\n\nI searched far and wide for an exciting dataset and finally selected the 20 Newsgoups dataset. I\u2019m just being sarcastic \u2014 I selected a dataset that is both easy to interpret and load in Scikit Learn. The dataset is easy to interpret because the 20 Newsgroups are known and the generated topics can be compared to the known topics being discussed. Headers, footers and quotes are excluded from the dataset.\n\nThe creation of the bag of words matrix is very easy in Scikit Learn \u2014 all the heavy lifting is done by the feature extraction functionality provided for text datasets. A tf-idf transformer is applied to the bag of words matrix that NMF must process with the TfidfVectorizer. LDA on the other hand, being a probabilistic graphical model (i.e. dealing with probabilities) only requires raw counts, so a CountVectorizer is used. Stop words are removed and the number of terms included in the bag of words matrix is restricted to the top 1000.\n\nAs mentioned previously the algorithms are not able to automatically determine the number of topics and this value must be set when running the algorithm. Comprehensive documentation on available parameters is available for both NMF and LDA. Initialising the W and H matrices in NMF with \u2018nndsvd\u2019 rather than random initialisation improves the time it takes for NMF to converge. LDA can also be set to run in either batch or online mode.\n\nThe structure of the resulting matrices returned by both NMF and LDA is the same and the Scikit Learn interface to access the returned matrices is also the same. This is great and allows for a common Python method that is able to display the top words in a topic. Topics are not labeled by the algorithm \u2014 a numeric index is assigned.\n\nThe derived topics from NMF and LDA are displayed below. From the NMF derived topics, Topic 0 and 8 don\u2019t seem to be about anything in particular but the other topics can be interpreted based upon there top words. LDA for the 20 Newsgroups dataset produces 2 topics with noisy data (i.e., Topic 4 and 7) and also some topics that are hard to interpret (i.e., Topic 3 and Topic 9). I\u2019d say the NMF was able to find more meaningful topics in the 20 Newsgroups dataset.\n\nNMF Topics:\n\nTopic 0: people don think like know time right good did say\n\nTopic 1: windows file use dos files window using program problem card\n\nTopic 2: god jesus bible christ faith believe christian christians church sin\n\nTopic 3: drive scsi drives hard disk ide controller floppy cd mac\n\nTopic 4: game team year games season players play hockey win player\n\nTopic 5: key chip encryption clipper keys government escrow public use algorithm\n\nTopic 6: thanks does know mail advance hi anybody info looking help\n\nTopic 7: car new 00 sale price 10 offer condition shipping 20\n\nTopic 8: just like don thought ll got oh tell mean fine\n\nTopic 9: edu soon cs university com email internet article ftp send\n\nLDA Topics:\n\nTopic 0: government people mr law gun state president states public use\n\nTopic 1: drive card disk bit scsi use mac memory thanks pc\n\nTopic 2: said people armenian armenians turkish did saw went came women\n\nTopic 3: year good just time game car team years like think\n\nTopic 4: 10 00 15 25 12 11 20 14 17 16\n\nTopic 5: windows window program version file dos use files available display\n\nTopic 6: edu file space com information mail data send available program\n\nTopic 7: ax max b8f g9v a86 pl 145 1d9 0t 34u\n\nTopic 8: god people jesus believe does say think israel christian true\n\nTopic 9: don know like just think ve want does use good\n\nIn my next blog post, I\u2019ll discuss topic interpretation and show how top documents within a theme can also be displayed.\n\nIt\u2019s amazing how much can be achieved with just 36 lines of Python code and some Scikit Learn magic. The full code listing is provided below:"
    },
    {
        "url": "https://medium.com/mlreview/l1-norm-regularization-and-sparsity-explained-for-dummies-5b0e4be3938a",
        "title": "L1 Norm Regularization and Sparsity Explained for Dummies",
        "text": "Well, I think I\u2019m just dumb. When understanding an abstract/mathematical idea, I have to really put it into images, I have to see and touch it in my head. I need the geometry, the object, the intuition behind the idea and better with vivid metaphors in real life.\n\nSometimes when I found people don\u2019t think or at least don\u2019t explain things this way, pointing me to equations and papers, saying there are no simple explanations, I got angry. And often after I thought stuff through, I could find silly intuitive explanations to those ideas. One such an experience was yesterday when I tried to understand L1 norm regularization applied to machine learning. Thus, I\u2019d like to make this silly but intuitive piece to explain this idea to fellow dummies like myself.\n\nWhen performing a machine learning task on a small dataset, one often suffers from the over-fitting problem, where the model accurately remembers all training data, including noise and unrelated features. Such a model often performs badly on new test or real data that have not been seen before. Because the model treats the training data too seriously, it failed to learn any meaningful pattern out of it, but simply memorizing everything it has seen.\n\nNow, one solution to solve this issue is called regularization. The idea is applying an L1 norm to the solution vector of your machine learning problem (In case of deep learning, it\u2019s the neural network weights.), and trying to make it as small as possible. So if your initial goal is finding the best vector x to minimize a loss function f(x), your new task should incorporate the L1 norm of x into the formula, finding the minimum (f(x) + L1norm(x)). The big claim they often throw at you is this: An x with small L1 norm tends to be a sparse solution. Being sparse means that the majority of x\u2019s components (weights) are zeros, only few are non-zeros. And a sparse solution could avoid over-fitting.\n\nThat\u2019s it, that\u2019s how they explain it in most of the articles, textbooks, materials. Giving an idea without explanation feels like pushing a spear through the back of my head.\n\nNot sure about you guys, but the reason for using an L1 norm to ensure sparsity and therefore avoid over-fitting wasn\u2019t so obvious to me. It took me some time to figure out why. Essentially, I had these questions:\n\nMy initial confusion came from the fact that I only looked at the L1 norm and only thought about what it means for L1 norm to be small. What I should really do, however, is thinking the loss function and the L1 norm penalty as a whole.\n\nLet me explain it from the beginning, the over-fitting problem. I\u2019d like to use a concrete example. Suppose you purchased a robot and you want to teach him to classify the Chinese characters by looking at the following example:\n\nThe first 5 characters belong to the first category, the last 5 are the second category. And these 10 characters are the only training data you have.\n\nNow, unfortunately, the robot is too smart for the task. It has large enough memory to remember 5 characters. After seeing all the 10 characters, the robot learned a way to categorize them: It remembers all the first 5 characters exactly. As long as a character is not one of those 5, the robot will put the character into the second category.\n\nOf course, this method will work very well on the 10 training characters, as the robot can achieve 100% accuracy. However, you provide a new character:\n\nThis character should belong to the first category. But because it never appeared in the training data, the robot hasn\u2019t seen it before. Based on its algorithm, the robot will put this character into the second category, which is wrong.\n\nIt should be pretty obvious for us human to see the pattern here. All characters that belong to the first category have a common part. The robot failed the task because it\u2019s too smart and the training data is too small.\n\nThis is the problem of over-fitting. But what is regularization and why can sparsity avoid over-fitting?\n\nNow suppose you got angry at your robot. You banged the head of the robot with a hammer, and while doing it, you shook some of its memory chips off his head. You essentially have made the robot dumber. Now, instead of being able to memorize 5 characters, the robot can only remember a character part.\n\nYou let the robot do the training again by looking at all 10 characters and still force him to achieve the same accuracy. Because he can\u2019t remember all 5 characters this time, you essentially force him to look for a simpler pattern. Now he discovers the common part of all the category A characters!\n\nThis is exactly what L1 norm regularization does. It bangs on your machine (model) to make it \u201cdumber\u201d. So instead of simply memorizing stuff, it has to look for simpler patterns from the data. In the case of the robot, when he could remember 5 characters, his \u201cbrain\u201d has a vector of size 5: [\u628a, \u6253, \u6252, \u6355, \u62c9]. Now after regularization (banging), 4 slots of his memory became unusable. Therefore the newly learned vector is: [\u624c, 0, 0, 0, 0] and clearly, this is a sparse vector.\n\nMore formally, when you are solving a large vector x with less training data. The solutions to x could be a lot.\n\nHere A is a matrix that contains all the training data. x is the solution vector you are looking for. b is the label vector.\n\nWhen data is not enough and your model\u2019s parameter size is large, your matrix A will not be \u201ctall\u201d enough and your x is very long. So the above equation will look like this:\n\nFor a system like this, the solutions to x could be infinite. To find a good one out of those solutions, you want to make sure each component of your selected solution x captures a useful feature of your data. By L1 regularization, you essentially make the vector x smaller (sparse), as most of its components are useless (zeros), and at the same time, the remaining non-zero components are very \u201cuseful\u201d.\n\nAnother metaphor I can think of is this: Suppose you are the king of a kingdom that has a large population and an OK overall GDP, but the per capita is very low. Each one of your citizens is lazy and unproductive and you are mad. Therefore you command \u201cbe productive, strong and hard working, or you die!\u201d And you enforce the same GDP as before. As a result, many people died due to your harshness, those who survived your tyranny became really capable and productive. You can think the population here is the size of your solution vector x, and commanding people to be productive or die is essentially regularization. In the regularized sparse solution, you ensure that each component of the vector x is very capable. Each component must capture some useful feature or pattern of the data.\n\nAnother way of regularization in deep learning is dropout. The idea is simple, removing some random neural connections from the neural network while training and adding them back after a while. Essentially this is still trying to make your model \u201cdumber\u201d by reducing the size of the neural network and put more responsibilities and pressure on the remaining weights to learn something useful. Once those weights have learned good features, then adding back other connections to embrace new data. I\u2019d like to think this adding back connection thing as \u201cintroducing immigrants to your kingdom when your are in short hands\u201d in the above metaphor.\n\nBased on this \u201cmaking model dumber\u201d idea, I guess we can come up with other similar ways to avoid over-fitting, such as starting with a small network and gradually adding new neurons and connections to the network when more data is available. Or performing a pruning while training to get rid of connections that are close to zero.\n\nSo far we have demonstrated why sparsity can avoid over-fitting. But why adding an L1 norm to the loss function and forcing the L1 norm of the solution to be small can produce sparsity?\n\nYesterday when I first thought about this, I used two example vectors [0.1, 0.1] and [1000, 0]. The first vector is obviously not sparse, but it has the smaller L1 norm. That\u2019s why I was confused, because looking at the L1 norm alone won\u2019t make this idea understandable. I have to consider the entire loss function as a whole.\n\nLet\u2019s go back to the problem of Ax = b, with a simple and concrete example. Suppose we want to find a line that matches a set of points in 2D space. We all know that you need at least 2 points to fix a line. But what if the training data has only one point? Then you will have infinite solutions: every line that passes through the point is a solution. Suppose the point is at [10, 5], and a line is defined as a function y = a * x + b. Then the problem is finding a solution to this equation:\n\nSince b = 5 \u2013 10 * a, all points on this following line b = 5 \u2013 10 * a should be a solution:\n\nBut how to find the sparse one with L1 norm?\n\nL1 norm is defined as the summation of absolute values of a vector\u2019s all components. For example, if a vector is [x, y], its L1 norm is |x| + |y|.\n\nNow if we draw all points that have a L1 norm equals to a constant c, those points should form something (in red) like this:\n\nThis shape looks like a tilted square. In high dimension space, it will be an octahedron. Notice that on this red shape, not all points are sparse. Only on the tips, points are sparse. That is, either x or y component of a point is zero. Now the way to find a sparse solution is enlarging this red shape from the origin by giving an ever-growing c to \u201ctouch\u201d the blue solution line. The intuition is that the touch point is most likely at a tip of the shape. Since the tip is a sparse point, the solution defined by the touch point is also a sparse solution.\n\nAs an example, in this graph, the red shape grows 3 times till it touches the blue line b = 5\u201310 * a. The touch point, as you can see, is at a tip of the red shape. The touch point [0.5, 0] is a sparse vector. Therefore we say, by finding the solution point with the smallest L1 norm (0.5) out of all possible solutions (points on the blue line), we find a sparse solution [0.5, 0] to our problem. At the touch point, the constant c is the smallest L1 norm you could find within all possible solutions.\n\nThe intuition of using L1 norm is that the shape formed by all points whose L1 norm equals to a constant c has many tips (spikes) that happen to be sparse (lays on one of the axises of the coordinate system). Now we grow this shape to touch the solutions we find for our problem (usually a surface or a cross-section in high dimension). The probability that the touch point of the 2 shapes is at one of the \u201ctips\u201d or \u201cspikes\u201d of the L1 norm shape is very high. That\u2019s why you want to put L1 norm into your loss function formula so that you can keep looking for a solution with a smaller c (at the \u201csparse\u201d tip of the L1 norm). (So in the real loss function case, you are essentially shrinking the red shape to find a touch point, not enlarging it from the origin.)\n\nDoes L1 norm always touch the solution at a tip and find us a sparse solution? Not necessarily. Suppose we still want to find a line out of 2D points, but this time, the only training data is a point [1, 1000]. In this case, the solution line b = 1000 -a is in parallel to one of the edges of the L1 norm shape:\n\nEventually, they touch on an edge, not by a tip. Not only you can\u2019t have a unique solution this time, most of your regularized solutions are still not sparse (other than the two tip points.)\n\nBut again, the probability of touching a tip is very high. I guess this is even more true for high dimension, real-world problems. As when your coordinate system has more axises, your L1 norm shape should have more spikes or tips. It must look like a cactus or a hedgehog! I can\u2019t imagine.\n\nIf you push a person towards a cactus, the probability of he being pricked by the needles is pretty high. That\u2019s also why they invented this pervert weapon and that\u2019s why they want to use L1 norm.\n\nBut is the L1 norm the best kind of norm to find a sparse solution? Well, it turns out that the Lp norm when 0 <= p < 1 gives the best result. This can be explained by looking at the shapes of different norms:\n\nAs you can see, when p < 1, the shape is more \u201cscary\u201d, with more sharpen, outbreaking spikes. Whereas when p = 2, the shape becomes a smooth, non-threatening ball. Then why not letting p < 1? That\u2019s because when p < 1, there are calculation difficulties.\n\nIn conclusion, over-fitting is a problem you see when your machine learning model is too large (has too many parameters) comparing to your available training data. In this case, the model tends to remember all training cases including noisy to achieve better training score. To avoid this, regularization is applied to the model to (essentially) reduce its size. One way of regularization is making sure the trained model is sparse so that the majority of it\u2019s components are zeros. Those zeros are essentially useless, and your model size is in fact reduced.\n\nThe reason for using L1 norm to find a sparse solution is due to its special shape. It has spikes that happen to be at sparse points. Using it to touch the solution surface will very likely to find a touch point on a spike tip and thus a sparse solution.\n\nJust think about this:\n\nWhen there is a zombie outbreak, which one should be the weapon of choice?"
    },
    {
        "url": "https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714",
        "title": "Understanding LSTM and its diagrams \u2013 ML Review \u2013",
        "text": "Although we don\u2019t know how brain functions yet, we have the feeling that it must have a logic unit and a memory unit. We make decisions by reasoning and by experience. So do computers, we have the logic units, CPUs and GPUs and we also have memories.\n\nBut when you look at a neural network, it functions like a black box. You feed in some inputs from one side, you receive some outputs from the other side. The decision it makes is mostly based on the current inputs.\n\nI think it\u2019s unfair to say that neural network has no memory at all. After all, those learnt weights are some kind of memory of the training data. But this memory is more static. Sometimes we want to remember an input for later use. There are many examples of such a situation, such as the stock market. To make a good investment judgement, we have to at least look at the stock data from a time window.\n\nThe naive way to let neural network accept a time series data is connecting several neural networks together. Each of the neural networks handles one time step. Instead of feeding the data at each individual time step, you provide data at all time steps within a window, or a context, to the neural network.\n\nA lot of times, you need to process data that has periodic patterns. As a silly example, suppose you want to predict christmas tree sales. This is a very seasonal thing and likely to peak only once a year. So a good strategy to predict christmas tree sale is looking at the data from exactly a year back. For this kind of problems, you either need to have a big context to include ancient data points, or you have a good memory. You know what data is valuable to remember for later use and what needs to be forgotten when it is useless.\n\nTheoretically the naively connected neural network, so called recurrent neural network, can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.\n\nThen later, LSTM (long short term memory) was invented to solve this issue by explicitly introducing a memory unit, called the cell into the network. This is the diagram of a LSTM building block.\n\nAt a first sight, this looks intimidating. Let\u2019s ignore the internals, but only look at the inputs and outputs of the unit. The network takes three inputs. X_t is the input of the current time step. h_t-1 is the output from the previous LSTM unit and C_t-1 is the \u201cmemory\u201d of the previous unit, which I think is the most important input. As for outputs, h_t is the output of the current network. C_t is the memory of the current unit.\n\nTherefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generates a new output and alters its memory.\n\nThe way its internal memory C_t changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves.\n\nThe first valve is called the forget valve. If you shut it, no old memory will be kept. If you fully open this valve, all old memory will pass through.\n\nThe second valve is the new memory valve. New memory will come in through a T shaped joint like above and merge with the old memory. Exactly how much new memory should come in is controlled by the second valve.\n\nOn the LSTM diagram, the top \u201cpipe\u201d is the memory pipe. The input is the old memory (a vector). The first cross \u2716 it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.\n\nThen the second operation the memory flow will go through is this + operator. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the \u2716 below the + sign.\n\nAfter these two operations, you have the old memory C_t-1 changed to the new memory C_t.\n\nNow lets look at the valves. The first one is called the forget valve. It is controlled by a simple one layer neural network. The inputs of the neural network is h_t-1, the output of the previous LSTM block, X_t, the input for the current LSTM block, C_t-1, the memory of the previous block and finally a bias vector b_0. This neural network has a sigmoid function as activation, and it\u2019s output vector is the forget valve, which will applied to the old memory C_t-1 by element-wise multiplication.\n\nNow the second valve is called the new memory valve. Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory.\n\nThe new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.\n\nThese two \u2716 signs are the forget valve and the new memory valve.\n\nAnd finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. This valve controls how much new memory should output to the next LSTM unit.\n\nThe above diagram is inspired by Christopher\u2019s blog post. But most of the time, you will see a diagram like below. The major difference between the two variations is that the following diagram doesn\u2019t treat the memory unit C as an input to the unit. Instead, it treats it as an internal thing \u201cCell\u201d.\n\nI like the Christopher\u2019s diagram, in that it explicitly shows how this memory C gets passed from the previous unit to the next. But in the following image, you can\u2019t easily see that C_t-1 is actually from the previous unit. and C_t is part of the output.\n\nThe second reason I don\u2019t like the following diagram is that the computation you perform within the unit should be ordered, but you can\u2019t see it clearly from the following diagram. For example to calculate the output of this unit, you need to have C_t, the new memory ready. Therefore, the first step should be evaluating C_t.\n\nThe following diagram tries to represent this \u201cdelay\u201d or \u201corder\u201d with dash lines and solid lines (there are errors in this picture). Dash lines means the old memory, which is available at the beginning. Some solid lines means the new memory. Operations require the new memory have to wait until C_t is available.\n\nBut these two diagrams are essentially the same. Here, I want to use the same symbols and colors of the first diagram to redraw the above diagram:\n\nThis is the forget gate (valve) that shuts the old memory:\n\nThis is the new memory valve and the new memory:\n\nThese are the two valves and the element-wise summation to merge the old memory and the new memory to form C_t (in green, flows back to the big \u201cCell\u201d):\n\nThis is the output valve and output of the LSTM unit:"
    }
]