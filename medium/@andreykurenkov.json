[
    {
        "url": "https://medium.com/skynet-today/why-skynet-today-fc9626aa0834?source=user_profile---------1----------------",
        "title": "Why Skynet Today \u2013 Skynet Today \u2013",
        "text": "Back in 2016, headlines like these started popping up more and more often:\n\nAt the time, AI hype was still building towards its current fever pitch and I was already getting increasingly frustrated with this trend and wanted to do something about it. Having taken many classes and done research in AI, it was shocking to me how many such flagrantly wrong headlines got published and the frequency with which they got published. I thought about writing a blog post titled \u201cAI Headlines These Days Are Wildly Inaccurate \u2014 So I Corrected Some\u201d, but quickly abandoned the idea since the deluge of misinformation was too massive for one blog post to have any real impact. So, I just gritted my teeth with each successive crazy headline.\n\nWhy do I get so annoyed by these headlines? After all, this happens all the time in journalism, it\u2019s nothing new. Here\u2019s why: the societal impact of AI is growing faster than ever, and everyone (including non-AI researchers) will be affected, and so everyone needs to have an accurate understanding of the scope and nature of this impact. This understanding will not come easily, since AI is easy to make up stories about and sensationalize; we are intelligent yet do not understand our own intelligence, and consequently wonder when our creations will surpass us. Accordingly, our culture has built up a century\u2019s worth of fanciful science fiction portrayals of futuristic AI, and present day media now routinely interprets factual research through the lens of science fiction. But the fruits of AI research are now very real, and it is important that peoples\u2019 perceptions of them are based on fact rather than fiction.\n\nStill, I thought I was powerless to do much about it, and so sat by and waited for things to improve. A year passed, nothing changed; actually, things got worse. I started seeing ever more absurd headlines such as \u201cAn AI backed by Elon Musk just \u2018evolved\u2019 to learn by itself\u201d and \u201cFacebook AI Invents Language That Humans Can\u2019t Understand: System Shut Down Before It Evolves Into Skynet\u201d. And then, that latter headline was so annoying and the story was so pervasive it gave me a sudden idea:\n\nAnd here we are!\n\nNewsletters like This Week In Wild AI and Import AI do a fantastic job of accurately covering exciting developments in the field. Likewise, sites such as Wired and The Verge do a good job of deflating innacurate stories every once in a while, but they focus on many more things besides AI. Lastly, AI researchers such as Andrej Karpathy and Denny Britz have on occasion written detailed blog posts putting some of the biggest AI events into perspective. All of these are great resources, but it remains the case that no publication today specifically and solely focuses on deflating the increasingly sensationalist and often downright misleading coverage of AI. Something akin to an AI-specific Snopes needs to exist.\n\nAnd that is why you are reading these words now. From the humble origins of an abandoned blog post and an impromptu idea shared on Facebook, this now Skynet Today, a site solely dedicated to providing accessible and informed coverage of the latest AI hype and panic.\n\nThe contributors to this effort will be people like me: not professional writers, but technical people involved in AI who want to contribute to others\u2019 understanding of it. We will seek to promote this understanding through short, easy-to-understand summaries of recent major media stories such as those already on this site, as well as longer editorials aimed at discussing overall trends and ideas. None of the contributors will profit monetarily from these articles, and all will do so in their free time motivated by the simple belief that someone qualified needs to explain what\u2019s really going on to a general audience.\n\nSo, if you agree with this site\u2019s mission and are such a qualified person, get in touch! We are currently looking for additional contributing writers. And if you are not such a technical person, but a person interested in this content, get in touch! As we are just starting out, feedback and ideas are invaluable.\n\nWe believe understanding of AI and its ramifications should be more accessible, that some of those with the technical knowledge to seperate fact from fiction should help make it accessible, and that those unsure of its ramifications should seek to become more informed. Plus, we won\u2019t be shy about poking fun at the sheer absurdity of the stories we will criticize, so we can all have some fun in the process :)"
    },
    {
        "url": "https://medium.com/skynet-today/alphago-zero-is-not-a-sign-of-imminent-human-level-ai-4649f2cc5f61?source=user_profile---------2----------------",
        "title": "AlphaGo Zero Is Not A Sign of Imminent Human-Level AI",
        "text": "Let\u2019s start with the coverage about DeepMind\u2019s recent successor to AlphaGo1, AlphaGo Zero:\n\nPoint being: AlphaGo Zero (which we\u2019ll go ahead and shorten to AG0) is arguably the most impressive and definitely the most praised2recent AI accomplishment3. Roughly speaking, AG0 is just a Deep Neural Network that takes the current state of a Go board as input, and outputs a Go move. Not only is this much simpler than the original AlphaGo4, but it is also trained purely through self-play (pitting different AlphaGo Zero neural nets against each other; the original AlphaGo was \u2018warmed up\u2019 by training to mimic human expert Go players). It\u2019s not exactly right that it learns \u2018with no human help\u2019, since the very rules of Go are hand-coded by humans rather than learned by AlphaGo, but the basic idea that it learns through self-play rather without any mimicry of human Go players is correct. I\u2019ll let the key researcher behind it expand on that:\n\nDeepMind\u2019s own explanation of why AG0 is so exciting.\n\nSo, surely DeepMind\u2019s demonstration that an AI algorithm can achieve superhuman Go and Chess play purely through self-play is a testament to the usefulness of such techniques for solving the hard problems of AI? Well, to some extent yes \u2014 it has taken the field decades to get here, since the branching factor of Go does indeed make it a challenging board game. This is also the first a time the same Deep Learning algorithm was used to crack both Chess and Go5, and was not specifically tailored for it such as was the case with Deep Blue (the much heralded machine of IBM that was the first to beat humanity\u2019s best at Chess) and the original AlphaGo. Therefore, AG0 is certainly monumental and exciting work (and great PR).\n\nWith those positive things having been said, some perspective: AG0 is not really a testament to the usefulness of such techniques for solving the hard problems of AI. You see, Go is only hard within the context of the simplest category of AI problems. That is, it is in the category of problems with every property that makes a learning task easy: it is deterministic, discrete, static, fully observable, fully-known, single-agent, episodic, cheap and easy to simulate, easy to score\u2026 Literally the only challenging aspect of Go is its huge branching factor. Predictions that AGI (Artificial General Intelligence) is imminent based only on AlphaGo\u2019s success can be safely dismissed \u2014 the real world is vastly more complex than a simple game like Go. Even fairly similar problems that have most but not all of the properties that make a learning task easy, such as the strategic video game DotA II, are far beyond our grasp right now.\n\nAnother important thing to understand beyond the categorical simplicity of Go is its narrowness. AG0 is a definite example of Weak AI, also known as narrow AI. Weak AI agents are characterized by only being able to perform one \u2018narrow\u2019 task, such as playing a 19 by 19 game of Go. Though AG0 has the impressive ability to learn to play 3 different board games, it does so separately per game 6. And, it can only learn a vary narrow range of games: basically just 2-player grid based board games without any necessary memorization of prior positions or moves7.\n\nIn a lengthy interview with Wired, then-president Obama displayed an impressively nuanced understanding of the state of AI. If only some unnamed billionaires would communicate to the public similarly\u2026\n\nSo, while AG0 works and its achievement is impressive, it is fundamentally similar to Deep Blue in being an expensive system engineered over many years with millions of dollars of investment purely for the task of playing a game \u2014 nothing else. Though Deep Blue was great PR for IBM, all that work and investment is not usually seen as having contributed much to the progress of broader AI research, having been ultra-specific to solving the problem of playing Chess. Just as with the algorithms that power AG0, human-tweaked heuristics and sheer computational brute force can definitely be used to solve some challenging problems \u2014 but they ultimately did not get us far beyond Chess, not even to Go. We should ask ourselves: can the techniques behind AG0 get us far beyond Go?\n\nGary Kasparov, the very man who faced and ultimately lost to Deep Blue, on the limitations of Deep Blue and implicitly AlphaGo.\n\nProbably, yes; the algorithms behind AG0 (Deep Learning and self-play) are inherently more general than human-defined heuristics and brute computation8. Still, it is important to understand and remember the parallels between Deep Blue and AG0: at the end of the day, both Deep Blue and AG0 are narrow AI programs that were built (at least in part) as PR boons for large companies by huge teams at the costs of millions of dollars; they deal with problems which are difficult for humans, but which are also relatively simple for computers.\n\nI write this not to be controversial or take away from DeepMind\u2019s fantastic work, but rather to fight against all the unwarranted hype AG0\u2019s success has generated and encourage more conversation about the limitations of deep learning and self-play. More people need to step up and say this kind of stuff for the general public as well as the AI research community to not be led astray by hype and PR.\n\nAnd all that aside, it should still be asked: might there be a better for AI agents to learn to play Go? The very name AlphaGo Zero is in reference to the idea that the model learns to play Go \u201cfrom scratch\u201d, without any further human input or explanation. But is learning \u2018from scratch\u2019 really such a good thing? Imagine you knew nothing about Go and decided to start learning it. You would definitely read the rules, some high level strategies, recall how you played similar games in the past, get some advice\u2026 right? And it indeed at least partially because of the learning \u2018from scratch\u2019 limitation of AlphaGo Zero that it is not truly impressive compared to human learning: like Deep Blue, it still relies on seeing orders of magnitude more Go games and planning for orders of magnitude more scenarios in any given game than any human ever does.\n\nSo, let\u2019s sum up: though AlphaGo and AG0\u2019s achievements are historic and impressive, they also represent little if any progress in tackling the truly hard problems of AI (not to mention AGI). Still, as with any field all AI researchers stand on the shoulders of their predecessors; though these techniques may not foreshadow the coming of AGI, they are definitely part of the Deep Learning Revolution the field is still in the midst of and the ideas that they are based on will doubtlessly enable future progress. As with Deep Learning as a whole, it is important to appreciate these fantastic accomplishments for the field of AI without losing perspective about their limitations."
    },
    {
        "url": "https://medium.com/@andreykurenkov/the-2018-best-picture-nominees-ranked-reviewed-and-reflected-upona-b17fc5be5a4?source=user_profile---------3----------------",
        "title": "Reflecting Upon The 2018 Best Picture Nominees \u2013 Andrey Kurenkov \u2013",
        "text": "When the 2018 Oscar best picture nominees were released, I realized I had already seen most of them purely due to being excited for them as works of art. Then, just for fun, I endeavored to watch the rest and cataloged my personal preferences. But then I realized I could do more! I had reviews and further thoughts on most of these films, and could write up a whole big summary \u2014 what you are reading now! So then, without further ado, (ordered from least to most liked, for the sake of ending on the most positive note) reviews and thoughts on all the best pictures nominees:\n\nA showy movie of romance and splendor \u2014 too much romance and splendor, in fact. Especially in its first half, the loud soundtrack, fancy panning/zooming/flying shots, and weird bouts of humor all add up to annoy and exhaust rather than highlight the excellent central performance. This improves in the second half, with show-off shots that are quite nice and serve to highlight the peril Churchill stands in. But, then the romantic bit cuts in and ruins that improvement with a bastardized version of history that does not seem realistic and indeed is utterly false. Still, the movie looks great, Oldman crafts a compelling portrayal, and overindulgence combined with a liberal interpretation of history are at least a far lesser sin than being boring or shallow.\n\nThe only movie among all of these I would not call great. This is an unusual case of a movie on which I am split, as I can find both great and not so great elements within it. Sadly, great plus not so great equals not so great.\n\nAs with the rest of these, my perspective on it has been affected a good deal by external listening and reading, in this case The Q&A with Jeff Goldsmith which reveals many interesting tidbits such as:\n\nA timely and smart allegory for the importance of a free press and the pervasiveness of sexism in the 1970s (and rather obviously, in the present). Strikes just the right balance of gripping-character-driven story and not-at-all-subtle social critique, and Spielberg is as good as ever at the basic craft of cinema. Hanks is a little predictable if solid, but Meryl Streep absolutely owns the role and makes the character come alive. Plenty of critics will just dismiss this as forgettable awards fluff, but when watched without cynicism it\u2019s clear this is an excellently made, thoroughly enjoyable, and unusually intelligent film.\n\nI have seen this one recently, and unlike Darkest Hour I don\u2019t think it is schmaltzy Oscat-bait at all despite looking like it might be. Spielberg smartly made it a lean and direct message-movie, and its direct address of the importance of the press and the reality of sexism in society is still as relevant as it was in the 1970s. And again, Meryl Streep is just so sublime, the movie is worth seeing for that alone. I leave you with a fantastic interview that delves into the impressive real life figure Streep portrayed so well:\n\nOne of the multiple movies among these for which my esteem has only grown. As with others, listening to conversations with its makers played an important role in evolving my view of it. In particular, the Q&A with Jeff Goldsmith interview with the writer and director Jordan Peele revealed many suble and smart aspects:\n\nAdditionally, others have done an excellent job aspousing how smart the movie actually is:\n\nA great coming-of-age movie with fantastic acting, editing, writing, and\u2026 just about everything. A few bits of dialogue feel a bit too clever for their own good, and having a few scenes go slower might\u2019ve been good, but the strength of the central performance and so many individual memorable moments alone make this utterly worth watching.\n\nA rare film for which my esteem has somewhat dropped. Some of the writing just feels too clever for its own good (like the \u2018my mother made one mistake\u2019 scene notably highlighted in the trailer), and like its protagonist the movie feels like its trying hard to be cool and smart but is not being vulnerable and honest in the process. Still, a fantastic growing of age movie, in particular because of the brilliance of lead actress Saoirse Ronan and writer/director Greta Gerwig \u2014 definitely some of the most exciting young talents in film today.\n\nAnd now we get to the set of movies I unabashedly love! Once again my appreciation has grown due to BBC Film Programme interview:\n\nLike the fantastical creature at its core, The Shape of Water is beautiful, strange, and exquisitely crafted.\n\nWhile watching the movie, I was struck by how unabashedly and unreservedly beautiful it was. So I was glad to hear that was Guillermo del Toro\u2019s intent; as stated on the BBC Film Programme\n\nThat being said, it is also one of the few movies I consider great for which my esteem. A big part of the reason is the lengthy discussion on the Next Picture Show podcast, in which most of the participants felt tepid about the film. I agree with their perspective that the movie feels a little artificial in all its beauty, and that going as far as it does in portraying the romance explicitly undercuts it somewhat. Still, this is a rare simple allegory of sensual delights that is definitely worth seeing.\n\nVisceral. Unlike Nolan\u2019s many other neat-idea or plot-puzzle movies, this one is made with the obvious intent of making you feel an experience to your bones. But, some of Nolan\u2019s fondness for fancy intellectually fun structure is here \u2014 there is no conventional 3 act structure or protagonist with an arc and instead an ensemble cast going through overlapping but temporally offset narratives that come together by the end. Yet more daringly, the ensemble story has minimalist character development and plot, which leaves it free to almost entirely avoid exposition and fully immerse you in the characters\u2019 dreaded pulse-pounding experiences. And does it ever do that \u2014 especially in the perfect-for-this-film IMAX, it\u2019s great big stark melancholy colored images, seemingly never stopping soundtrack, and clearly big budget production all work to grab you and not let you go til the movie is over and you can cry along with the soldiers at the plane sight of English greenery.\n\nThis all is great and for the most part executed beautifully, except that I am not sure if the 3 narrative threads really work well together, and by the end the cross-cutting between narrative threads gets overdone and lessens the impact of the ending a good deal. And, after that ending there is just not a whole lot to think back on or relate to; it\u2019s a hell of a ride, but not much more. It made me think back to Gravity, which is likewise a non-stop visceral tale of survival, but one that I think managed to still have a resonant theme and message. But damn, is this one of a hell of a ride.\n\nSo many thoughts\u2026 as indicated in the review I was unsure of Nolan\u2019s layered fancy structure was actually warranted, but I have since grown to appreciate it more. Once, much insight comes from an interview on the BBC Film Programme:\n\nLuscious, sensual, rapturous \u2014 perfectly captures the feeling of being young during a care free summer of discovery that is equal parts exciting and nerve wrecking. And it does so beautifully, artistically; there are so many close ups and frames in this movie that convey an endless depth of emotion and thought without a word being spoken. And it does so intelligently, thematically tying back to the greeks\u2019 views on sensuality and love; unlike so much of recent 80-set media, this does not feel nostalgic \u2014 it feels timeless.\n\nAnd the music! Sufjan Stevens is such a perfect fit for this, singing in his soft voice of the \u2018Mystery of Love\u2019. And the writing! Adapted by now 89-year old James Ivory, it feels wise and fully aware of the subtle unspeakable eternal truths of life. I credit Ivory too, with the unusual lack of tension over being found out as gay and the character\u2019s fully supportive parts; as discussed in an interview , this felt like a welcome change from more conventional cinema:\n\nSo, the movie is joy, happiness, love. And the final shot! Watch this film, if only for the final conversation, and the final shot.\n\nWell, my appreciation of this movie has certainly not lessened \u2014 everything that I said in that original review, I still feel. But as with Three Billboards, hearing interviews and watching videos has also deepened my appreciation of the movie. In particular, the lengthy and detailed interview of the director and co-author by Luca Guadagnino on the The Q&A with Jeff Goldsmith highlights many great elements:\n\nLastly, what I\u2019ve grown to appreciate even more than I did after just watching the movie is this element of portraying an utterly happy love story (even if it ends with separation, rather than marriage). As stated perfectly by the book\u2019s author in an interview of both him and the movie\u2019s director,\n\nA rare movie for which my esteem has only grown, and in just one month.\n\nI think what I appreciate so much about this film (and why I think it deserves to win best picture) is that it is far more challenging that any of the other films on here. As eloquently stated by the writer and director Martin McDonagh on the BBC Film Programme, the core of the film is two characters going to war in which neither one is really the bad guy. It would be so easy to make the grieving mother the one to clearly root for and to portray the authorities as incapable and uncaring. But as McDonah says:\n\nThe very simple and clear setup of this conflict \u2014 the titular three billboards \u2014 is also utterly inspired and immediately interesting. So it\u2019s unsurprising that such a striking moment is actually drawn from the real world:\n\nThere was some controversy and negative takes about this film, with regard to whether it ultimately redeemed the racist and violent police officer played by Sam Rockwell. I am sure this controversy will likely make this movie not receive the best picture, but I also just don\u2019t agree with it at all. Like the non-perfection of the central character, the \u2018redemption\u2019 of Rockwell\u2019s character is actually completely subverted in opposition to what would typically be done in Hollywood. The morally ambiguous and subversive ending is actually one of my favorite things about the movie, and I recommend it to anyone who is a fan of black comedy, complicated explorations of morality, and just great cinema."
    },
    {
        "url": "https://medium.com/@andreykurenkov/habits-and-tools-old-and-new-f53ecd0eeb48?source=user_profile---------4----------------",
        "title": "Habits and Tools, Old and New \u2013 Andrey Kurenkov \u2013",
        "text": "Common wisdom, right? Yet, somehow it took me until recently to seriously take stock of my time management and habits. In high school and undergrad, my time management could be boiled down to \u201cdo the next thing\u201d. I more or less kept adding work until it was just about too much, and then did whatever needed doing to keep up.\n\nTurns out, that\u2019s not a great way to go about things. After graduating, I had a glut of free time on my hands and little idea of what to do with it all. So I started getting more methodical by consistently making time for writing, excercising, and all the fun splendor of life. And I found various tools \u2014 apps, subscriptions, things \u2014 to make that much easier. Now, a few years later, I no longer have a glut of free time but still want to do all this stuff (blog writing, being healthy, etc). And so it finally hit me that I need to double down on this habits and routine thing; my no calendar, no todo list, no consistency lifestlye will not work anymore.\n\nSo, despite not being a new year resolutions kind of person, the beginning of this year was marked for me with a whole lot of reflection about what habbits I wanted to commit to going forward, and what tools I would use to fullfil that commitment. This, on top of several years of slowly building a set of habits and tools I had not used prior to graduating undergrad. So, here you are, reading my reflection on all these wonderful habits and tools. If that sounds boring, stop here, but if not \u2014 I truly do think these things have made a positive impact in my life, and recommend you consider them for your own.\n\nWhen I\u2019ve described my often surprising workloads, people have often asked me \u2018how much do you sleep\u2019 or commented that I must sleep very little. No. Going back many years, to at least the sophemore year of college, I recognized the importance of sleep and the stupidity of all nighters. This may have translated to averaging only 7 or 6 hours most weeks \u2014 but average 7 or 6 hours I did.\n\nThis is a tough one.\n\nSince the end of high school, I have never managed to wake up at a consistent time. Turns out, not waking up at a consistent time makes it hard to have a consistent morning routine, and a morning routine is pretty important for having a routine at all. And, it makes it harder to wake up at all. So, time for fancy alarm clocks, lights that turn on at the same time as the alarm, all that.\n\nBut, more importantly, I have decided to have better sleep. Specifically, to fall asleep more easily and calmly. And so, I decided to stick to the whole array of recommended pre-sleep activities \u2014 dim the lights, don\u2019t look at screens, drink special sleep-time tea with honey, read.\n\nAnd it\u2019s great! Commiting to spending the last half hour prior to sleep in this fashion has made me feel far more tranquil as I go to sleep than I used to.\n\nYou know what\u2019s great? Fitness classes. Seriously, I\u2019ve not once excercised as rigorously by myself as when led by a good instructor. Practically every time I went to a boxing gym I felt that I was barely able to keep up. So, I continue taking kickboxing classes to this day and hope to not stop.\n\nYou know what else is great? Not having to go to the gym to excercise. And you dont! All one needs to excercise is a hard floor (though a mat does help). Going back even to my undergrad days, I was often unable to make it to the gym. So I just excercised at home, and kept in quite good shape doing so.\n\nBut what\u2019s better than either of these? The best of both! Last year I realized apps like FitStar (and Nike Traning Club, and Skimble) could be used to excercise at home while still pushing me to my limit like trainers do. Of course, they are not quite as effective, but still better than excercising driven by my motivation alone.\n\nAll too often, work would get heavy, I would have to pick my battles, and I\u2019d miss the gym for days, even weeks. That does not feel good, seem good, or is good. So my new idea? Excercise for 10\u201315 minutes right upon waking up. Specifically, do a 10\u201315 Fitstar excercise. Not only does that keep me not feeling like I\u2019ve gone for weeks without excercising, but it also makes it easier for me to wake up and get going. Not easy to keep up, but worth the effort.\n\nI already made my case for Soylent (TDLR \u2014 convenient, healthy, affordable, futuristic, makes real food taste even better), and years later the case still holds up. More than holds up, actually \u2014 with the introduction of Soylent 2.0 and Soylent Coffee and Cocoa, the convenience and flavor have both seen a significant boost in the past few years. Most I tell this still scoff, but whenever I am so inconvenienced as to have to eat real food for lunch I am reminded of how much time it gets just to get food and how sluggish I feel afterwards.\n\nBut, let\u2019s face it, drinking two bottles of nutritious goo can get a bit repititious. So, I was most excited by my recent discovery of MealSquares. These little nutrition bricks are dry, dense, and chalky \u2014 but also contain bits of chocolate, and go down fantastically when paired with a drink such as tea (or Soylent!).\n\nThis too, I have written about. More and more, I am convinced that meditation will gradually become as commonly acknowledged as a crucial component of a healthy life as physical excercise. Nowdays it\u2019s easier than ever to start \u2014 Calm, Headspace, and many other apps tout their ability to teach you the magical skill of meditation. I used both Calm and Headspace early on, and do recommend them for getting started. But I quickly found their idea of \u2018listen to lectures about life\u2019 to be pretty far removed from actual meditation, and largely rely on the far simpler Insight Timer.\n\nI started keeping a journal more than a year ago now. Not for mental health reasons, just for\u2026 fun. Something I found when catching up on this journaling (sometimes whole weeks at a time, sometimes even more), is that trying to recall some fun event from a while ago almost made me feel like how I felt when it actually happened. Furthermore, consistently writing down this journal just made me appreciate the mundane little milestones of life just a little more.\n\nSo I was not surprised to find out there was another surprisingly simple scientificaly backed way of maintaing mental health \u2014 gratitude journaling. It used to be this would seem to touchy-feely for me, but no more. My plan, given my whole experience with journaling, is to stick to the weekly cadence by writing down some things I am grateful for from the past week. Too early to tell if I\u2019ll stick to this, but I am eager to try it out for this next year.\n\nThis stuff is boring but useful, so let\u2019s keep it brief:\n\nDespite having this email and OneNote and Calendar system, I was still not as productive as I wanted. I found that I never managed to make time for stuff like writing for this blog, and often forgot smaller chores til far too late. So this year I had a new idea: spend 5\u201310 minutes in the morning reflecting on my goal for the day, and jot them down in a todo app. I had always found todo apps pretty useless, as I inevitably got behind and abandoned the endeavour. So this time, I plan to stick to keeping the todo items on at most a day-week timeline, and striving for zero tasks just as with my email inbox. This has already paid off in just a few weeks, with me being better able to keep track of various small chores as well as get around to things like writing the very text you are reading.\n\nAs you may have noticed, this post has quite a few metrics and graphs. Partly this is just for fun, but I do also think that the knowledge that these metrics and graphs are out there serves as extra motivation to follow through on my ambitions. Inspired somewhat by Andrej Karpathy, I decided to take this to the next level this year with rescuetime. Basically, it logs every single thing I do on my phone and computer, and tells me how much time I spend doing things that are productive (like this) or unproductive (like browsing reddit). I honestly doubt it being there will change my behavior much, but then again I have felt that it has gotten me to visit distracting websites already so I may be wrong there.\n\nSo there you had it, an infodump on all the habits I have cultivated and plan to cultivate, along with the best tools for said cultivation. Hopefully, if you were so inclined as to read all this a few of these might prove a fruitful addition to your own life."
    },
    {
        "url": "https://medium.com/invisible-illness/the-sickness-that-is-depression-510159738a0b?source=user_profile---------5----------------",
        "title": "The Sickness That Is Depression \u2013 Invisible Illness \u2013",
        "text": "This is how I remember depression. \n\nIt was not exactly like this; there were highs, lows, bad days, good days.\n\nBut this is how I remember it.\n\nThat\u2019s depression. Except, these are just words.\n\nDepression cannot be conveyed with just such words.\n\nYou know how being in a bad mood is like a filter that squeezes out the joy from otherwise pleasant things? \n\nImagine being in the worst mood of your life, for months.\n\nYou know the difference between dragging yourself to work after waking up with a killer hangover, and heading home having just exercised? \n\nBeing depressed was like a perpetual hangover, and being healthy again was like having a perpetual dopamine high.\n\nYou know how when you are drunk, or high, you think thoughts that seem absurd in retrospect? \n\nThat happens all the time, constantly, with depression. Eventually I learned to distrust my \u2018depressed self\u2019.\n\nYou know that feeling of getting in a cold shower and having your body revolt, the feeling of \u2018make this stop\u2019? \n\nIt was like that, every morning, being hit with a wave of formless misery moments after waking up, like \u2018oh, right, this\u2019.\n\nYou know how it feels when you are at a social event, and just can\u2019t seem to get along with anyone \u2014 alienated ? \n\nDepression makes you feel like that, even among friends.\n\nYou know how nice it is to warm up by a fire after being cold, and how painful it is to then leave that fire? \n\nSpending time with friends or family felt like that; that warmth soon went away to be replaced by the familiar cold.\n\nYou know the difference between feeling dead tired and ambivalent about everything except sleep and the feeling of having just awoken on the first day of a vacation? \n\nOne day I awoke and felt that I really was healed of depression, and it was like that.\n\nYou know that feeling of doing something you could not care less about, just wanting it to be done with? \n\nBeing alive felt like that.\n\nI had supportive family, friends, security, and prospects. I excercised, meditated, slept well, took on a lighter workload, had a good diet.\n\nBut it did not matter. It still took me months, and medication, to improve.\n\nI still had suicidal thoughts.\n\nThat\u2019s depression. Except, it was not a feeling.\n\nIt was a state, a condition, a sickness. A sickness I was stuck with.\n\nIt was like I was no longer really myself. \n\nI did not have interests, energy, drive \u2014 all that was left was a dull miserable husk of myself.\n\nIt was like I was just play-acting as Andrey. \n\nThe play-acting turned out to be easy, and occasionally it made me feel better.\n\nIt was like a temporary state of insanity. \n\nIf you have ever been in love, the feeling of temporary insanity was like that.\n\nIt was like sleep was my only goal, my only escape, every day. \n\nI just wanted to close my eyes. I just wanted to lie down still in a dark silent room.\n\nIt was like being up high and struggling for breath, or underwater running out of air. \n\n I thought about thinking about suicide \u2014 it became mundane and tiresome.\n\nIt was like I could no longer see a future, hated the present, was haunted by the past. \n\nI wanted nothing, was incapable of wanting to strive for anything.\n\nIt was like life was just a thing that I did. \n\n And that thing had lost its appeal.\n\nIt was like there was some horrible droning static always there. \n\nThe music that once brought me joy just added to that static.\n\nIt was like I was some Dickensian street urchin, scraping by on every piece of happiness. \n\nEvery free meal I got felt like an odd victory that I devoured.\n\nIt was like life had become some modern piece of art, inscrutable and cold, nonsensical. \n\nLife never felt more absurd, more grotesque, more pointless.\n\nIt was like all my anxieties and insecurities grew to a mammoth size and crushed me. \n\nI am and was hard working, fit, smart, gregarious. And still I felt like a worthless worm.\n\nIt was like a dream where I walked around in a snow storm wearing only t-shirt and shorts, in a big dead city surrounded by towering gray skyscrapers. \n\nAnd somehow this is supposed to make sense.\n\nIt was like no matter how hard I tried, I could not feel good. \n\nOften the best I could do was numb the pain. \n\nMy failure to improve made me feel worse. \n\nI meditated, exercised, socialized, went for long walks, read, watched movies.\n\nAll of it helped, but none of it made the depression end. \n\nIt just did not make sense, being so helpless.\n\nThat\u2019s depression. \n\nExcept, it did make sense.\n\nI was afflicted with one of the most common but least understood of sicknesses.\n\nIt did make sense, just not to my sick mind.\n\nIt did make sense that I needed treatment and when I got it I got better. \n\nMost dont.\n\nIt did make sense that I spoke of it infrequently. \n\nI knew it was a sickness, yet I still felt ashamed, like I had allowed myself to fall to a stupid first world \u2018illness\u2019.\n\nIt did make sense that I could not think clearly, could not make decisions easily. \n\nI was lucky to have had others to help me seek treatment and not to give up on my aspirations.\n\nIt did make sense that I got sick. \n\nI had stress, burnout, family history, prior episodes, failures, mistakes, changes. \n\nSo much to explain it.\n\nThat\u2019s depression. \n\nExcept, that was my depression. \n\nI got treatment, rest, support, and I am no longer sick.\n\nHaving healed, I relish just being alive, just being myself.\n\nHaving healed, my nightmares mostly revolve around having a relapse.\n\nHaving healed, my energy and enthusiasm feel boundless.\n\nHaving healed, I am still taking antidepressants, still seeing a therapist, doing all I can to remain healthy.\n\nHaving healed, the sounds of music often make me indescribably ecstatic.\n\nHaving healed, I understand depression better, and hate and fear and loathe it like few other things.\n\nHaving healed, the anxieties and uncertainties and insecurities that so haunted me before barely bother me.\n\nHaving healed, what I remember most vividly is not the suicidal thoughts, the inability to focus, the anxiety, the shame; what I remember most is lying in my room at 8PM, and feeling like there was nothing else at all that could make me feel better \u2014 that sensation of nothing else but this quiet, this stillness, this darkness.\n\nHaving healed, I write this, reflecting on that period of surreal sickness and memories like that. \n\nI see the need to de-stigmatize and explain my experience far more than I did before.\n\nSo, don\u2019t feel bad for me. I seriously feel superhuman, healed of this sickness.\n\nJust, try to understand."
    },
    {
        "url": "https://medium.com/@andreykurenkov/habits-and-tools-old-and-new-35587b3dd31c?source=user_profile---------6----------------",
        "title": "Habits and Tools, Old and New \u2013 Andrey Kurenkov \u2013",
        "text": "When I\u2019ve described my often surprising workloads, people have often asked me \u2018how much do you sleep\u2019 or commented that I must sleep very little. No. Going back many years, to at least the sophemore year of college, I recognized the importance of sleep and the stupidity of all nighters. This may have translated to averaging only 7 or 6 hours most weeks \u2014 but average 7 or 6 hours I did.\n\nThis is a tough one. Since the end of high school, I have never managed to wake up at a consistent time. Not even when I started working as a software engineer \u2014 my initial enthusiasm for showing up early quickly faded and I went back to winging it based on how I felt on the day. Turns out, not waking up at a consistent time makes it hard to have a consistent morning routine, and a morning routine is pretty important for having a routine at all. And, it makes it harder to wake up at all. So, time for fancy alarm clocks, lights that turn on at the same time as the alarm, all that.\n\nBut, more importantly, I have decided to have better sleep. Specifically, to fall asleep more easily and calmly. And so, I decided to stick to the whole array of recommended pre-sleep activities \u2014 dim the lights, don\u2019t look at screens, drink special sleep-time tea with honey, read. And it\u2019s great! Commiting to spending the last half hour prior to sleep in this fashion has made me feel far more tranquil as I go to sleep than I used to.\n\nYou know what\u2019s great? Fitness classes. Seriously, I\u2019ve not once excercised as rigorously by myself as when led by a good instructor. Practically every time I went to a boxing gym I felt that I was barely able to keep up. So, I continue taking kickboxing classes to this day and hope to not stop.\n\nYou know what else is great? Not having to go to the gym to excercise. And you dont! All one needs to excercise is a hard floor (though a mat does help). Going back even to my undergrad days, I was often unable to make it to the gym. So I just excercised at home, and kept in quite good shape doing so.\n\nBut what\u2019s better than either of these? The best of both! Last year I realized apps like FitStar (and Nike Traning Club, and Skimble) could be used to excercise at home while still pushing me to my limit like trainers do. Of course, they are not quite as effective, but still better than excercising driven by my motivation alone.\n\nAll too often, work would get heavy, I would have to pick my battles, and I\u2019d miss the gym for days, even weeks. That does not feel good, seem good, or is good. So my new idea? Excercise for 10\u201315 minutes right upon waking up. Specifically, do a 10\u201315 Fitstar excercise. Not only does that keep me not feeling like I\u2019ve gone for weeks without excercising, but it also makes it easier for me to wake up and get going. Not easy to keep up, but worth the effort.\n\nI already made my case for Soylent (TDLR \u2014 convenient, healthy, affordable, futuristic, makes real food taste even better), and years later the case still holds up. More than holds up, actually \u2014 with the introduction of Soylent 2.0 and Soylent Coffee and Cocoa, the convenience and flavor have both seen a significant boost in the past few years. Most I tell this still scoff, but whenever I am so inconvenienced as to have to eat real food for lunch I am reminded of how much time it gets just to get food and how sluggish I feel afterwards.\n\nBut, let\u2019s face it, drinking two bottles of nutritious goo can get a bit repititious. So, I was most excited by my recent discovery of MealSquares. These little nutirition bricks are dry, dense, and chalky \u2014 but also contain bits of chocolate, and go down fantastically when paired with a drink such as tea (or Soylent!).\n\nThis too, I have written about. More and more, I am convinced that meditation will gradually become as commonly acknowledged as a crucial component of a healthy life as physical excercise. Nowdays it\u2019s easier than ever to start \u2014 Calm, Headspace, and many other apps tout their ability to teach you the magical skill of meditation. I used both Calm and Headspace early on, and do recommend them for getting started. But I quickly found their idea of \u2018listen to lectures about life\u2019 to be pretty far removed from actual meditation, and largely rely on the far simpler Insight Timer.\n\nI started keeping a journal more than a year ago now. Not for mental health reasons, just for\u2026 fun. Something I found when catching up on this journaling (sometimes whole weeks at a time, sometimes even more), is that trying to recall some fun event from a while ago almost made me feel like how I felt when it actually happened. Furthermore, consistently writing down this journal just made me appreciate the mundane little milestones of life just a little more.\n\nSo I was not surprised to find out there was another surprisingly simple scientificaly backed way of maintaing mental health \u2014 gratitude journaling. It used to be this would seem to touchy-feely for me, but no more. My plan, given my whole experience with journaling, is to stick to the weekly cadence by writing down some things I am grateful for from the past week. Too early to tell if I\u2019ll stick to this, but I am eager to try it out for this next year.\n\nThis stuff is boring but useful, so let\u2019s keep it brief:\n\nDespite having this email and OneNote and Calendar system, I was still not as productive as I wanted. I found that I never managed to make time for stuff like writing for this blog, and often forgot smaller chores til far too late. So this year I had a new idea: spend 5\u201310 minutes in the morning reflecting on my goal for the day, and jot them down in a todo app. I had always found todo apps pretty useless, as I inevitably got behind and abandoned the endeavour. So this time, I plan to stick to keeping the todo items on at most a day-week timeline, and striving for zero tasks just as with my email inbox. This has already paid off in just a few weeks, with me being better able to keep track of various small chores as well as get around to things like writing the very text you are reading.\n\nAs you may have noticed, this post has quite a few metrics and graphs. Partly this is just for fun, but I do also think that the knowledge that these metrics and graphs are out there serves as extra motivation to follow through on my ambitions. Inspired somewhat by Andrej Karpathy, I decided to take this to the next level this year with rescuetime. Basically, it logs every single thing I do on my phone and computer, and tells me how much time I spend doing things that are productive (like this) or unproductive (like browsing reddit). I honestly doubt it being there will change my behavior much, but then again I have felt that it has gotten me to visit distracting websites already so I may be wrong there.\n\nSo there you had it, an infodump on all the habits I have cultivated and plan to cultivate, along with the best tools for said cultivation. Hopefully, if you were so inclined as to read all this a few of these might prove a fruitful addition to your own life."
    },
    {
        "url": "https://medium.com/@andreykurenkov/a-brief-history-of-neural-nets-and-deep-learning-part-4-61be90639182?source=user_profile---------7----------------",
        "title": "A \u2018Brief\u2019 History of Neural Nets and Deep Learning, Part 4",
        "text": "Originally published at http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-4/. Best read there!\n\nThis is the fourth part in \u2018A Brief History of Neural Nets and Deep Learning\u2019. Parts 1\u20133 here, here, and here. In this part, we will get to the end of our story and see how deep learning emerged from the slump neural nets found themselves in by the late 90s, and the amazing state of the art results it has achieved since.\n\nWhen you want a revolution, start with a conspiracy. With the ascent of Support Vector Machines and the failure of backpropagation, the early 2000s were a dark time for neural net research. LeCun and Hinton variously mention how in this period their papers or the papers of their students were routinely rejected from being published due to their subject being Neural Nets. The above quote is probably an exaggeration \u2014 certainly research in Machine Learning and AI was still very active, and other people were also still working with neural nets \u2014 but citation counts from the time make it clear that the excitement had leveled off, even if it did not completely evaporate. Still, they persevered. And they found a strong ally outside the research realm: The Canadian government. Funding from the Canadian Institute for Advanced Research (CIFAR), which encourages basic research without direct application, was what motivated Hinton to move to Canada in 1987, and funded his work afterward. But, the funding was ended in the mid 90s just as sentiment towards neural nets was becoming negative again. Rather than relenting and switching his focus, Hinton fought to continue work on neural nets, and managed to secure more funding from CIFAR as told well in this exemplary piece1:\n\nThe funding was modest, but sufficient to enable a small group of researchers to keep working on the topic. As Hinton tells it, they hatched a conspiracy: \u201crebrand\u201d the frowned-upon field of neural nets with the moniker \u201cDeep Learning\u201d 1. Then, what every researcher must dream of actually happened: Hinton, Simon Osindero, and Yee-Whye Teh published a paper in 2006 that was seen as a breakthrough, a breakthrough significant enough to rekindle interest in neural nets: A fast learning algorithm for deep belief nets2. Though, as we\u2019ll see, the approaches used in the paper have been superceded by newer work, the movement that is \u2018Deep Learning\u2019 can very persuasively be said to have started precisely with this paper. But, more important than the name was the idea \u2014 that neural networks with many layers really could be trained well, if the weights are initialized in a clever way rather than randomly. Hinton once expressed the need for such an advance at the time:\n\nSo what was the clever way of initializing weights? The basic idea is to train each layer one by one with unsupervised training, which starts off the weights much better than just giving them random values, and then finishing with a round of supervised learning just as is normal for neural nets. Each layer starts out as a Restricted Boltzmann Machine (RBM), which is just a Boltzmann Machine without connections between hidden and visible units as illustrated above, and is taught a generative model of data in an unsupervised fashion. It turns out that this form of Boltzmann machine can be trained in an efficient manner introduced by Hinton in the 2002 \u201cTraining Products of Experts by Minimizing Contrastive Divergence\u201d3. Basically, this algorithm maximizes something other than the probability of the units generating the training data, which allows for a nice approximation and turns out to still work well. So, using this method, the algorithm is as such:\n\nThe paper concluded by showing that deep belief networks (DBNs) had state of the art performance on the standard MNIST character recognition dataset, significantly outperforming normal neural nets with only a few layers. Yoshua Bengio et al. followed up on this work in 2007 with \u201cGreedy Layer-Wise Training of Deep Networks\u201d4, in which they present a strong argument that deep machine learning methods (that is, methods with many processing steps, or equivalently with hierarchical feature representations of the data) are more efficient for difficult problems than shallow methods (which two-layer ANNs or support vector machines are examples of).\n\nThey also present reasons for why the addition of unsupervised pre-training works, and conclude that this not only initializes the weights in a more optimal way, but perhaps more importantly leads to more useful learned representations of the data. In fact, using RBMs is not that important \u2014 unsupervised pre-training of normal neural net layers using backpropagation with plain Autoencoders layers proved to also work well. Likewise, at the same time another approach called Sparse Coding also showed that unsupervised feature learning was a powerful approach for improving supervised learning performance.\n\nSo, the key really was having many layers of computing units so that good high-level representation of data could be learned \u2014 in complete disagreement with the traditional approach of hand-designing some nice feature extraction steps and only then doing learning using those features. Hinton and Bengio\u2019s work had empirically demonstrated that fact, but more importantly, showed the premise that deep neural nets could not be trained well to be false. This, LeCun had already demonstrated with CNNs throughout the 90s, but neural nets still went out of favor. Bengio, in collaboration with Yann LeCun, reiterated this on \u201cScaling Algorithms Towards AI\u201d5:\n\nAnd inspire they did. Or at least, they started; though deep learning had not yet gained the tsumani momentum that it has today, the wave had unmistakably begun. Still, the results at that point were not that impressive \u2014 most of the demonstrated performance in the papers up to this point was for the MNIST dataset, a classic machine learning task that had been the standard benchmark for algorithms for about a decade. Hinton\u2019s 2006 publication demonstrated a very impressive error rate of only 1.25% on the test set, but SVMs had already gotten an error rate of 1.4%, and even simple algorithms could get error rates in the low single digits. And, as was pointed out in the paper, Yann LeCun already demonstrated error rates of 0.95% in 1998 using CNNs.\n\nSo, doing well on MNIST was not necessarily that big a deal. Aware of this and confident that it was time for deep learning to take the stage, Hinton and two of his graduate students, Abdel-rahman Mohamed and George Dahl, demonstrated their effectiveness at a far more challenging AI task: Speech Recognition6. Using DBNs, the two students and Hinton managed to improve on a decade-old performance record on a standard speech recognition dataset. This was an impressive achievement, but in retrospect seems like only a hint at what was coming \u2014 in short, many more broken records.\n\nThe algorithmic advances described above were undoubtedly important to the emergence of deep learning, but there was another essential component that had emerged in the decade since the 1990s: pure computational power. Following Moore\u2019s law, computers got dozens of times faster since the slow days of the 90s, making learning with large datasets and many layers much more tractable. But even this was not enough \u2014 CPUs were starting to hit a ceiling in terms of speed growth, and computer power was starting to increase mainly through weakly parallel computations with several CPUs. To learn the millions of weights typical in deep models, the limitations of weak CPU parallelism had to be left behind and replaced with the massively parallel computing powers of GPUs. Realizing this is, in part, how Abdel-rahman Mohamed, George Dahl, and Geoff Hinton accomplished their record breaking speech recognition performance7:\n\nIt\u2019s hard to say just how much faster using GPUs over CPUs was in this case, but the paper \u201cLarge-scale Deep Unsupervised Learning using Graphics Processors\u201d8of the same year suggests a number: 70 times faster. Yes, 70 times \u2014 reducing weeks of work into days, even a single day. The authors, who had previously developed Sparse Coding, included the prolific Machine Learning researcher Andrew Ng, who increasingly realized that making use of lots of training data and of fast computation had been greatly undervalued by researchers in favor of incremental changes in learning algorithms. This idea was strongly supported by 2010\u2019s \u201cDeep Big Simple Neural Nets Excel on Handwritten Digit Recognition\u201d9(notably co-written by J. Schmidhuber, one of the inventors of the recurrent LTSM networks), which showed a whopping %0.35 error rate could be achieved on the MNIST dataset without anything more special than really big neural nets, a lot of variations on the input, and efficient GPU implementations of backpropagation. These ideas had existed for decades, so although it could not be said that algorithmic advancements did not matter, this result did strongly support the notion that the brute force approach of big training sets and fast parallelized computations were also crucial.\n\nDahl and Mohamed\u2019s use of a GPU to get record breaking results was an early and relatively modest success, but it was sufficient to incite excitement and for the two to be invited to intern at Microsoft Research1. Here, they would have the benefit from another trend in computing that had emerged by then: Big Data. That loosest of terms, which in the context of machine learning is easy to understand \u2014 lots of training data. And lots of training data is important, because without it neural nets still did not do great \u2014 they tended to overfit (perfectly work on the training data, but not generalize to new test data). This makes sense \u2014 the complexity of what large neural nets can compute is such that a lot of data is needed to avoid them learning every little unimportant aspect of the training set \u2014 but was a major challenge for researchers in the past. So now, the computing and data gathering powers of large companies proved invaluable. The two students handily proved the power of deep learning during their three month internship, and Microsoft Research has been at the forefront of deep learning speech recognition ever since.\n\nMicrosoft was not the only BigCompany to recognize the power of deep learning (though it was handily the first). Navdeep Jaitly, another student of Hinton\u2019s, went off to a summer internship at Google in 2011. There, he worked on Google\u2019s speech recognition, and showed their existing setup could be much improved by incorporating deep learning. The revised approach soon powered Android\u2019s speech recognition, replacing much of Google\u2019s carefully crafted prior solution 1.\n\nBesides the impressive effects of humble PhD interns on these gigantic companies\u2019 products, what is notable here is that both companies were making use of the same ideas \u2014 ideas that were out in the open for anyone to work with. And in fact, the work by Microsoft and Google, as well as IBM and Hinton\u2019s lab, resulted in the impressively titled \u201cDeep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups\u201d10in 2012. Four research groups \u2014 three from companies that could certainly benefit from a briefcase full of patents on the emerging wonder technology of deep learning, and the university research group that popularized that technology \u2014 working together and publishing their results to the broader research community. If there was ever an ideal scenario for industry adopting an idea from research, this seems like it.\n\nNot to say the companies were doing this for charity. This was the beginning of all of them exploring how to commercialize the technology, and most of all Google. But it was perhaps not Hinton, but Andrew Ng who incited the company to become likely the world\u2019s biggest commercial adopter and proponent of the technology. In 2011, Ng incidentally met with the legendary Googler Jeff Dean while visiting the company, and chatted about his efforts to train neural nets with Google\u2019s fantastic computational resources. This intrigued Dean, and together with Ng they formed Google Brain \u2014 an effort to build truly giant neural nets and explore what they could do. The work resulted in unsupervised neural net learning of an unprecedented scale \u2014 16,000 CPU cores powering the learning of a whopping 1 billion weights (for comparison, Hinton\u2019s breakthrough 2006 DBN had about 1 million weights). The neural net was trained on Youtube videos, entirely without labels, and learned to recognize the most common objects in those videos \u2014 leading of course to the internet\u2019s collective glee over the net\u2019s discovery of cats:\n\nCute as that was, it was also useful. As they reported in a regularly published paper, the features learned by the model could be used for record setting performance on a standard computer vision benchmark11. With that, Google\u2019s internal tools for training massive neural nets were born, and they have only continued to evolve since. The wave of deep learning research that began in 2006 had now undeniably made it into industry.\n\nWhile deep learning was making it into industry, the research community was hardly keeping still. The discovery that efficient use of GPUs and computing power in general was so important made people examine long-held assumptions and ask questions that should have perhaps been asked long ago \u2014 namely, why exactly does backpropagation not work well? The insight to ask why the old approaches did not work, rather than why the new approaches did, led Xavier Glort and Yoshua Bengio to write \u201cUnderstanding the difficulty of training deep feedforward neural networks\u201d in 201012. In it, they discussed two very meaningful findings:\n\nThe second point is quite clear, but the first opens the question: \u2018what, then, is the best activation function\u2019? Three different groups explored the question (a group with LeCun, with \u201cWhat is the best multi-stage architecture for object recognition?\u201d13, a group with Hinton, in \u201cRectified linear units improve restricted boltzmann machines\u201d14, and a group with Bengio -\u201cDeep Sparse Rectifier Neural Networks\u201d15), and they all found the same surprising answer: the very much non-differentiable and very simple function f(x)=max(0,x) tends to be the best. Surprising, because the function is kind of weird \u2014 it is not strictly differentiable, or rather is not differentiable precisely at zero, so on paper as far as math goes it looks pretty ugly. But, clearly the zero case is a pretty small mathematical quibble \u2014 a bigger question is why such a simple function, with constant derivatives on either side of 0, is so good. The answer is not precisely known, but a few ideas seem pretty well established:\n\nAt this point, with all these discoveries since 2006, it had become clear that unsupervised pre-training is not essential to deep learning. It was helpful, no doubt, but it was also shown that in some cases well-done, purely supervised training (with the correct starting weight scales and activation function) could outperform training that included the unsupervised step. So, why indeed, did purely supervised learning with backpropagation not work well in the past? Geoffrey Hinton summarized the findings up to today in these four points:\n\nSo here we are. Deep learning. The culmination of decades of research, all leading to this:\n\nNot to say all there was to figure out was figured out by this point. Far from it. What had been figured out is exactly the opposite: that peoples\u2019 intuition was often wrong, and in particular unquestioned decisions and assumptions were often very unfounded. Asking simple questions, trying simple things \u2014 these had the power to greatly improve state of the art techniques. And precisely that has been happening, with many more ideas and approaches being explored and shared in deep learning since. An example: \u201cImproving neural networks by preventing co-adaptation of feature detectors\u201d17by G. E. Hinton et al. The idea is very simple: to prevent overfitting, randomly pretend some neurons are not there while training. This straightforward idea \u2014 called Dropout \u2014 is a very efficient means of implementing the hugely powerful approach of ensemble learning, which just means learning in many different ways from the training data. Random Forests, a dominating technique in machine learning to this day, is chiefly effective due to being a form of ensemble learning. Training many different neural nets is possible but is far too computationally expensive, yet this simple idea in essence achieves the same thing and indeed significantly improves performance.\n\nStill, having all these research discoveries since 2006 is not what made the computer vision or other research communities again respect neural nets. What did do it was something somewhat less noble: completely destroying non-deep learning methods on a modern competitive benchmark. Geoffrey Hinton enlisted two of his Dropout co-writers, Alex Krizhevsky and Ilya Sutskever, to apply the ideas discovered to create an entry to the ILSVRC-2012 computer vision competition. To me, it is very striking to now understand that their work, described in \u201cImageNet Classification with deep convolutional neural networks\u201d18, is the combination of very old concepts (a CNN with pooling and convolution layers, variations on the input data) with several new key insight (very efficient GPU implementation, ReLU neurons, dropout), and that this, precisely this, is what modern deep learning is. So, how did they do? Far, far better than the next closest entry: their error rate was %15.3, whereas the second closest was %26.2. This, the first and only CNN entry in that competition, was an undisputed sign that CNNs, and deep learning in general, had to be taken seriously for computer vision. Now, almost all entries to the competition are CNNs \u2014 a neural net model Yann LeCun was working with since 1989. And, remember LSTM recurrent neural nets, devised in the 90s by Sepp Hochreiter and J\u00fcrgen Schmidhuber to solve the backpropagation problem? Those, too, are now state of the art for sequential tasks such as speech processing.\n\nThis was the turning point. A mounting wave of excitement about possible progress has culminated in undeniable achievements that far surpassed what other known techniques could manage. The tsunami metaphor that we started with in part 1, this is where it began, and it has been growing and intensifying to this day. Deep learning is here, and no winter is in sight.\n\nIf this were a movie, the 2012 ImageNet competition would likely have been the climax, and now we would have a progression of text describing \u2018where are they now\u2019. Yann LeCun \u2014 Facebook. Geoffrey Hinton \u2014 Google. Andrew Ng \u2014 Coursera, Google, Baidu. Bengio, Schmidhuber, and Hochreiter actually still in academia \u2014 but presumably with many more citations and/or grad students19. Though the ideas and achievements of deep learning are definitely exciting, while writing this I was inevitably also moved that these people, who worked in this field for decades (even as most abandoned it), are now rich, successful, and most of all better situated to do research than ever. All these peoples\u2019 ideas are still very much out in the open, and in fact basically all these companies are open sourcing their deep learning frameworks, like some sort of utopian vision of industry-led research. What a story.\n\nI was foolish enough to hope I could fit a summary of the most impressive results of the past several years in this part, but at this point it is clear I will not have the space to do so. Perhaps one day there will be a part five of this that can finish out the tale by describing these things, but for now let me provide a brief list:\n\n1 \u2014 The resurgence of LTSM RNNs + representing \u2018ideas\u2019 with distributed representations\n\n2 \u2014 Using deep learning for reinforcement learning (again, but better)\n\n3 \u2014 Adding external memory writable and readable to by the neural net"
    },
    {
        "url": "https://medium.com/@andreykurenkov/a-brief-history-of-neural-nets-and-deep-learning-part-3-a62d1500b5a3?source=user_profile---------8----------------",
        "title": "A \u2018Brief\u2019 History of Neural Nets and Deep Learning, Part 3",
        "text": "Originally published at http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-3/. Best read there!\n\nThis is the third part of \u2018A Brief History of Neural Nets and Deep Learning\u2019. Parts 1 and 2 are here and here, and part 4 is here. In this part, we will continue to see the swift pace of research in the 90s, and see why neural nets ultimately lost favor much as they did in the late 60s.\n\nHaving discovered the application of neural nets to unsupervised learning, let us also quickly see how they were used in the third branch of machine learning: reinforcement learning. This one requires the most mathy notation to explain formally, but also has a goal that is very easy to describe informally: learn to make good decisions. Given some theoretical agent (a little software program, for instance), the idea is to make that agent able to decide on an action based on its current state, with the reception of some reward for each action and the intent of getting the maximum utility in the long term. So, whereas supervised learning tells the learning algorithm exactly what it should learn to output, reinforcement learning provides \u2018rewards\u2019 as a by-product of making good decisions over time, and does not directly tell the algorithm the correct decisions to choose. From the outset it was a very abstracted decision making model \u2014 there were a finite number of states, and a known set of actions with known rewards for each state. This made it easy to write very elegant equations for finding the optimal set of actions, but hard to apply to real problems \u2014 problems with continuous states or hard-to-define rewards.\n\nThis is where neural nets come in. Machine learning in general, and neural nets in particular, are good at dealing with messy continuous data or dealing with hard to define functions by learning them from examples. Although classification is the bread and butter of neural nets, they are general enough to be useful for many types of problems \u2014 the descendants of Bernard Widrow\u2019s and Ted Hoff\u2019s Adaline were used for adaptive filters in the context of electrical circuits, for instance. And so, following the resurgence of research caused by backpropagation, people soon devised ways of leveraging the power of neural nets to perform reinforcement learning. One of the early examples of this was solving a simple yet classic problem: the balancing of a stick on a moving platform, known to students in control classes everywhere as the inverted pendulum problem 1.\n\nAs with adaptive filtering, this research was strongly relevant to the field of Electrical Engineering, where control theory had been a major subfield for many decades prior to neural nets\u2019 arrival. Though the field had devised ways to deal with many problems through direct analysis, having a means to deal with more complex situations through learning proved useful as evidenced by the hefty 7000 (!) citations of the 1990 \u201cIdentification and control of dynamical systems using neural networks\u201d2. Perhaps predictably, there was another field separate from Machine Learning where neural nets were useful \u2014 robotics. A major example of early neural net use for robotics came from CMU\u2019s NavLab with 1989\u2019s \u201cAlvinn: An autonomous land vehicle in a neural network\u201d3:\n\nAs discussed in the paper, the neural net in this system learned to control the vehicle through plain supervised learning using sensor and steering data recorded while a human drove. There was also research into teaching robots using reinforcement learning specifically, as exemplified by the 1993 PhD thesis \u201cReinforcement learning for robots using neural networks\u201d4. The thesis showed that robots could be taught behaviors such as wall following and door passing in reasonable amounts of time, which was a good thing considering the prior inverted pendulum work requires impractical lengths of training.\n\nThese disparate applications in other fields are certainly cool, but of course the most research on reinforcement learning and neural nets was happening within AI and Machine Learning. And here, one of the most significant results in the history of reinforcement learning was achieved: a neural net that learned to be a world class backgammon player. Dubbed TD-Gammon, the neural net was trained using a standard reinforcement learning algorithm and was one of the first demonstrations of reinforcement learning being able to outperform humans on relatively complicated tasks 5. And it was specifically a reinforcement learning approach that worked here, as the same research showed just using a neural net without reinforcement learning did not work nearly as well.\n\nBut, as we have seen happen before and will see happen again in AI, research hit a dead end. The predictable next problem to tackle using the TD-Gammon approach was investigated by Sebastian Thrun in the 1995 \u201cLearning To Play the Game of Chess\u201d, and the results were not good 6. Though the neural net learned decent play, certainly better than a complete novice at the game, it was still far worse than a standard computer program (GNU-Chess) implemented long before. The same was true for the other perennial challenge of AI, Go 7. See, TD-Gammon sort of cheated \u2014 it learned to evaluate positions quite well, and so could get away with not doing any \u2018search\u2019 over multiple future moves and instead just picking the one that led to the best next position. But the same is simply not possible in chess or Go, games which are a challenge to AI precisely because of needing to look many moves ahead and having so many possible move combinations. Besides, even if the algorithm were smarter, the hardware of the time just was not up to the task \u2014 Thrun reported that \u201cNeuroChess does a poor job, because it spends most of its time computing board evaluations. Computing a large neural network function takes two orders of magnitude longer than evaluating an optimized linear evaluation function (like that of GNU-Chess).\u201d The weakness of computers of the time relative to the needs of the neural nets was a very real issue, and as we shall see not the only one\u2026\n\nAs neat as unsupervised and reinforcement learning are, I think supervised learning is still my favorite use case for neural nets. Sure, learning probabilistic models of data is cool, but it\u2019s simply much easier to get excited for the sorts of concrete problems solved by backpropagation. We already saw how Yann Lecun achieved quite good recognition of handwritten text (a technology which went on to be nationally deployed for check-reading, and much more a while later\u2026), but there was another obvious and greatly important task being worked on at the same time: understanding human speech.\n\nAs with writing, understanding human speech is quite difficult due to the practically infinite variation in how the same word can be spoken. But, here there is an extra challenge: long sequences of input. See, for images it\u2019s fairly simple to crop out a single letter from an image and have a neural net tell you which letter that is, input->output style. But with audio it\u2019s not so simple \u2014 separating out speech into characters is completely impractical, and even finding individual words within speech is less simple. Plus, if you think about human speech, generally hearing words in context makes them easier to understand than being separated. While this structure works quite well for processing things such as images one at a time, input->output style, it is not at all well suited to long streams of information such as audio or text. The neural net has no \u2018memory\u2019 with which an input can affect another input processed afterward, but this is precisely how we humans process audio or text \u2014 a string of word or sound inputs, rather than a single large input. Point being: to tackle the problem of understanding speech, researchers sought to modify neural nets to process input as a stream of input as in speech rather than one batch as with an image.\n\nOne approach to this, by Alexander Waibel et. al (including Hinton), was introduced in the 1989 \u201cPhoneme recognition using time-delay neural networks\u201d8. These time-delay neural networks (TDNN) were very similar to normal neural networks, except each neuron processed only a subset of the input and had several sets of weights for different delays of the input data. In other words, for a sequence of audio input, a \u2018moving window\u2019 of the audio is input into the network and as the window moves the same bits of audio are processed by each neuron with different sets of weights based on where in the window the bit of audio is. This is best understood with a quick illustration:\n\nIn a sense, this is quite similar to what CNNs do \u2014 instead of looking at the whole input at once, each unit looks at just a subset of the input at a time and does the same computation for each small subset. The main difference here is that there is no idea of time in a CNN, and the \u2018window\u2019 of input for each neuron is always moved across the whole input image to compute a result, whereas in a TDNN there actually is sequential input and output of data. Fun fact: according to Hinton, the idea of TDNNs is what inspired LeCun to develop convolutional neural nets. But, funnily enough CNNs became essential for image processing, whereas in speech recognition TDNNs have been surpassed to another approach \u2014 recurrent neural nets (RNNs). See, all the networks that have been discussed so far have been feedforward networks, meaning that the output of neurons in a given layer acts as input to only neurons in a next layer. But, it does not have to be so \u2014 there is nothing prohibiting us brave computer scientists from connecting output of the last layer act as an input to the first layer, or just connecting the output of a neuron to itself. By having the output of the network \u2018loop\u2019 back into the network, the problem of giving the network memory as to past inputs is solved so elegantly!\n\nWell, it\u2019s not quite so simple. Notice the problem \u2014 if backpropagation relies on \u2018propagating\u2019 the error from the output layer backward, how do things work if the first layer connects back to the output layer? The error would go ahead and propagate from the first layer back to the output layer, and could just keep looping through the network, infinitely. The solution, independently derived by multiple groups, is backpropagation through time. Basically, the idea is to \u2018unroll\u2019 the recurrent neural network by treating each loop through the neural network as an input to another neural network, and looping only a limited number of times.\n\nThis fairly simple idea actually worked \u2014 it was possible to train recurrent neural nets. And indeed, multiple people explored the application of RNNs to speech recognition. But, here is a twist you should now be able to predict: this approach did not work very well. To find out why, let\u2019s meet another modern giant of Deep Learning: Yoshua Bengio. Starting work on speech recognition with neural nets around 1986, he co-wrote many papers on using ANNs and RNNs for speech recognition, and ended up working at the AT&T Bell Labs on the problem just as Yann LeCun was working with CNNs there. In fact, in 1995 they co-wrote the summary paper \u201cConvolutional Networks for Images, Speech, and Time-Series\u201d9, the first of many collaborations among them. But, before then Bengio wrote the 1993 \u201cA Connectionist Approach to Speech Recognition\u201d10. Here, he summarized the general failure of effectively teaching RNNs:\n\nSo, there was a problem. A big problem. And the problem, basically, was what so recently was a huge advance: backpropagation. See, convolutional neural nets were important in part because backpropagation just did not work well for normal neural nets with many layers. And that\u2019s the real key to deep-learning \u2014 having many layers, in today\u2019s systems as many as 20 or more. But already by the late 1980\u2019s, it was known that deep neural nets trained with backpropagation just did not work very well, and particularly did not work as well as nets with fewer layers. The reason, in basic terms, is that backpropagation relies on finding the error at the output layer and successively splitting up blame for it for prior layers. Well, with many layers this calculus-based splitting of blame ends up with either huge or tiny numbers and the resulting neural net just does not work very well \u2014 the \u2018vanishing or exploding gradient problem\u2019. Jurgen Schmidhuber, another Deep Learning luminary, summarizes the more formal explanation well11:\n\nBackpropagation through time is essentially equivalent to a neural net with a whole lot of layers, so RNNs were particularly difficult to train with Backpropagation. Both Sepp Hochreiter, advised by Schmidhuber, and Yoshua Bengio published papers on the inability of learning long-term information due to limitations of backpropagation 1213. The analysis of the problem did reveal a solution \u2014 Schmidhuber and Hochreiter introduced a very important concept in 1997 that essentially solved the problem of how to train recurrent neural nets, much as CNNs did for feedforward neural nets \u2014 Long Short Term Memory (LSTM)14. In simple terms, as with CNNs the LTSM breakthrough ended up being a small alteration to the normal neural net model 11:\n\nBut, this did little to fix the larger perception problem that neural nets were janky and did not work very well. They were seen as a hassle to work with \u2014 the computers were not fast enough, the algorithms were not smart enough, and people were not happy. So, around the mid 90s, a new AI Winter for neural nets began to emerge \u2014 the community once again lost faith in them. A new method called Support Vector Machines, which in the very simplest terms could be described as a mathematically optimal way of training an equivalent to a two layer neural net, was developed and started to be seen as superior to the difficult to work with neural nets. In fact, the 1995 \u201cComparison of Learning Algorithms For Handwritten Digit Recognition\u201d15by LeCun et al. found that this new approach worked better or the same as all but the best designed neural nets:\n\nOther new methods, notably Random Forests, also proved to be very effective and with lovely mathematical theory behind them. So, despite the fact that CNNs consistently had state of the art performance, enthusiasm for neural nets dissipated and the machine learning community at large once again disavowed them. Winter was back. In part 4, we shall see how a small group of researchers persevered in this research climate and ultimately made Deep Learning what it is today."
    },
    {
        "url": "https://medium.com/@andreykurenkov/a-brief-history-of-neural-nets-and-deep-learning-part-2-a2d94d8f2fee?source=user_profile---------9----------------",
        "title": "A \u2018Brief\u2019 History of Neural Nets and Deep Learning, Part 2",
        "text": "Originally published at http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning-part-2/. Better read there, but this is fine too.\n\nThis is the second part of \u2018A Brief History of Neural Nets and Deep Learning\u2019. Part 1 is here, and Parts 3 and 4 are here and here. In this part, we will look into several strains of research that made rapid progress following the development of backpropagation and until the late 90s, which we shall see later are the essential foundations of Deep Learning.\n\nWith the secret to training multilayer neural nets uncovered, the topic was once again ember-hot and the lofty ambitions of Rosenblatt seemed to perhaps be in reach. It took only until 1989 for another key finding now universally cited in textbooks and lectures to be published1: \u201cMultilayer feedforward networks are universal approximators\u201d. Essentially, it mathematically proved that multiple layers allow neural nets to theoretically implement any function, and certainly XOR.\n\nBut, this is mathematics, where you could imagine having endless memory and computation power should it be needed \u2014 did backpropagation allow neural nets to be used for anything in the real world? Oh yes. Also in 1989, Yann LeCun et al. at the AT&T Bell Labs demonstrated a very significant real-world application of backpropagation in \u201c\u201dBackpropagation Applied to Handwritten Zip Code Recognition\u201d 2. You may think it fairly unimpressive for a computer to be able to correctly understand handwritten digits, and these days it is indeed quite quaint, but prior to the publication the messy and inconsistent scrawls of us humans proved a major challenge to the much more tidy minds of computers. The publication, working with a large dataset from the US Postal Service, showed neural nets were entirely capable of this task. And much more importantly, it was first to highlight the practical need for a key modifications of neural nets beyond plain backpropagation toward modern deep learning:\n\nOr, more concretely: the first hidden layer of the neural net was convolutional \u2014 instead of each neuron having a different weight for each pixel of the input image (40x60=2400 weights), the neurons only have a small set of weights (5x5=25) that were applied a whole bunch of small subsets of the image of the same size. So, for instance instead of having 4 different neurons learn to detect 45 degree lines in each of the 4 corners of the input image, a single neuron could learn to detect 45 degree lines on subsets of the image and do that everywhere within it. Layers past the first work in a similar way, but take in the \u2018local\u2019 features found in the previous hidden layer rather than pixel images, and so \u2018see\u2019 successively larger portions of the image since they are combining information about increasingly larger subsets of the image. Finally, the last two layers are just plain normal neural net layers that use the higher-order larger features generated by the convolutional layers to determine which digit the input image corresponds to. The method proposed in this 1989 paper went on to be the basis of nationally deployed check-reading systems, as demonstrated by LeCun in this gem of a video:\n\nThe reason for why this is helpful is intuitively if not mathematically clear: without such constraints the network would have to learn the same simple things (such as detecting 45 degree lines, small circles, etc) a whole bunch of times for each portion of the image. But with the constraint there, only one neuron would need to learn each simple feature \u2014 and with far fewer weights overall, it could do so much faster! Moreover, since the pixel-exact locations of such features do not matter the neuron could basically skip neighboring subsets of the image \u2014 subsampling, now known as a type of pooling \u2014 when applying the weights, further reducing the training time. The addition of these two types of layers \u2014 convolutional and pooling layers \u2014 are the primary distinctions of Convolutional Neural Nets (CNNs/ConvNets) from plain old neural nets.\n\nAt that time, the convolution idea was called \u2018weight sharing\u2019, and it was actually discussed in the 1986 extended analysis of backpropagation by Rumelhart, Hinton, and Williams3. Actually, the credit goes even further back \u2014 Minsky and Papert\u2019s 1969 analysis of Perceptrons was thorough enough to pose a problem that motivated this idea. But, as before, others have also independently explored the concept \u2014 namely, Kunihiko Fukushima in 1980 with his notion of the Neurocognitron4. And, as before, the ideas behind it drew inspiration from studies of the brain:\n\nLeCun continued to be a major proponent of CNNs at Bell Labs, and his work on them resulted in major commercial use for check-reading in the mid 90s \u2014 his talks and interviews often include the fact that \u201cAt some point in the late 1990s, one of these systems was reading 10 to 20% of all the checks in the US.\u201d5.\n\nAutomating the rote and utterly uninteresting task of reading checks is a great instance of what Machine Learning can be used for. Perhaps a less predictable application? Compression. Meaning, of course, finding a smaller representation of some data from which the original data can be reconstructed. Learned compression may very well outperform stock compression schemes, when the learning algorithm can find features within the data stock methods would miss. And it is very easy to do \u2014 just train a neural net with a small hidden layer to just output the input:\n\nThis is an autoencoder neural net, and is a method for learning compression \u2014 efficiently translating (encoding) data to a compact format and back to itself (auto). See, the output layer computes its outputs, which ideally are the same as the input to the neural net, using only the hidden layer\u2019s outputs. Since the hidden layer has fewer outputs than does the input layer, the output of the hidden layer is the compressed representation of the input data, which can be reconstructed with the output layer.\n\nNotice a neat thing here: the only thing we need for training is some input data. This is in contrast to the requirement of supervised machine learning, which needs a training set of input-output pairs (labeled data) in order to approximate a function that can compute such outputs from such inputs. And indeed, autoencoders are not a form of supervised learning; they are a form of unsupervised learning, which only needs a set of input data (unlabeled data) in order to find some hidden structure within that data. In other words, unsupervised learning does not approximate a function so much as it derives one from the input data to another useful representation of that data. In this case, this representation is just a smaller one from which the original data can still be reconstructed, but it can also be used for finding groups of similar data (clustering) or other inference of latent variables (some aspect that is known to exist for the data but the value of which is not known).\n\nThere were other unsupervised applications of neural networks explored prior to and after the discovery of backpropagation, most notably Self Organizing Maps 6, which produce a low-dimensional representation of data good for visualization, and Adapative Resonance Theory7, which can learn to classify arbitrary input data without being told correct classifications. If you think about it, it is intuitive that quite a lot can be learned from unlabeled data. Say you have a dataset of a bunch of images of handwritten digits, without labels of which digit each image corresponds to. Well, an image with some digit in that dataset most likely looks most like all the other images with that same digit, and so though a computer may not know which digit all those images correspond to, it should still be able to find that they all correspond to the same one. This, pattern recognition, is really what most of machine learning is all about, and arguably also is the basis for the great powers of the human brain. But, let us not digress from our exciting deep learning journey, and get back to autoencoders.\n\nAs with weight-sharing, the idea of autoencoders was first discussed in the aforementioned extensive 1986 analysis of backpropagation 3, and as with weight-sharing it resurfaced in more research in the following years89, including by Hinton himself 10. This paper, with the fun title \u201cAutoencoders, Minimum Description Length, and Helmholts Free Energy\u201d, posits that \u201cA natural approach to unsupervised learning is to use a model that defines probability distribution over observable vectors\u201d and uses a neural net to learn such a model. So here\u2019s another neat thing you can do with neural nets: approximate probability distributions.\n\nIn fact, before being co-author of the seminal 1986 paper on backpropagation learning algorithm, Hinton worked on a neural net approach for learning probability distributions in the 1985 \u201cA Learning Algorithm for Boltzmann Machines\u201d 11. Boltzmann Machines are networks just like neural nets and have units that are very similar to Perceptrons, but instead of computing an output based on inputs and weights, each unit in the network can compute a probability of it having a value of 1 or 0 given the values of connected units and weights. The units are therefore stochastic \u2014 they behave according to a probability distribution, rather than in a known deterministic way. The Boltzmann part refers to a probability distribution that has to do with the states of particles in a system based the particles\u2019 energy and on the thermodynamic temperature of that system. This distribution defines not only the mathematics of the Boltzmann machines, but also the interpretation \u2014 the units in the network themselves have energies and states, and learning is done by minimizing the energy of the system and with direct inspirartion from thermodynamics. Though a bit unintuitive, this energy-based interpretation is actually just one example of an energy-based model, and fits in the energy-based learning theoretical framework with which many learning algorithms can be expressed12.\n\nBack to Boltzmann Machines. When such units are put together into a network, they form a graph, and so are a graphical model of data. Essentially, they can do something very similar to normal neural nets: some hidden units compute the probability of some hidden variables (the outputs \u2014 classifications or features for data) given known values of visible units that represent visible variables (the inputs \u2014 pixels of images, characters in text, etc.). In our classic example of classifying images of digits, the hidden variables are the actual digit values, and the visible variables are the pixels of the image; given an image of the digit \u20181\u2019 as input, the value of visible units is known and the hidden unit modeling the probability of the image representing a \u20181\u2019 should have a high output probability.\n\nSo, for the classification task, there is now a nice way of computing the probability of each category. This is very analogous to actually computing the output values of a normal classification neural net, but these nets have another neat trick: they can generate plausible looking input data. This follows from the probability equations involved \u2014 not only does the net learn the probabilities of values for the hidden variables given known values for the visible variables, but also the inverse of that \u2014 visible probabilities given known hidden values. So, if we want to generate a \u20181\u2019 digit image, the units corresponding to the pixel variables have known probabilities of outputting a 1 and an image can be probabilistically generated; these networks are generative graphical models. Though it is possible to do supervised learning with very similar goals as normal neural nets, the unsupervised learning task of learning a good generative model \u2014 probabilistically learning the hidden structure of some data \u2014 is commonly what these nets are used for. Most of this was not really that novel, but the learning algorithm presented and the particular formulation that enabled it were, as stated in the paper itself:\n\nWithout delving into the full details of the algorithm, here are some highlights: it is a variant on maximum-likelihood algorithms, which simply means that it seeks to maximize the probability of the net\u2019s visible unit values matching with their known correct values. Computing the actual most likely value for each unit all at the same time turns out to be much too computationally expensive, so in training Gibbs Sampling \u2014 starting the net with random unit values and iteratively reassigning values to units given their connections\u2019 values \u2014 is used to give some actual known values. When learning using a training set, the visible units are just set to have the value of the current training example, so sampling is done to get values for the hidden units. Once some \u2018real\u2019 values are sampled, we can do something similar to backpropagation \u2014 take a derivative for each weight to see how we can change so as to increase the probability of the net doing the right thing.\n\nAs with neural net, the algorithm can be done both in a supervised fashion (with known values for the hidden units) or in an unsupervised fashion. Though the algorithm was demonstrated to work (notably, with the same \u2018encoding\u2019 problem that autoencoder neural nets solve), it was soon apparent that it just did not work very well \u2014 Redford M. Neal\u2019s 1992 \u201cConnectionist learning of belief networks\u201d13justified a need for a faster approach by stating that: \u201cThese capabilities would make the Boltzmann machine attractive in many applications, were it not that its learning procedure is generally seen as being painfully slow\u201d. And so Neal introduced a similar idea in the belief net, which is essentially like a Boltzmann machine with directed, forward connections (so that there are again layers, as with the the neural nets we have seen before, and unlike the Boltzmann machine image above). Without getting into mucky probability math, this change allowed the nets to be trained with a faster learning algorithm. We actually saw a \u2018belief net\u2019 just above with the sprinkler and rain variables, and the term was chosen precisely because this sort of probability-based modeling has a close relationship to ideas from the mathematical field of probability, in addition to its link to the field of Machine Learning.\n\nThough this approach was an advance upon Boltzmann machines, it was still just too slow \u2014 the math for correctly deriving probabilistic relations between variables is such that a ton of computation is typically required without some simplifying tricks. And so Hinton, along with Neal and two other co-authors, soon came up with extra tricks in the 1995 \u201cThe wake-sleep algorithm for unsupervised neural networks\u201d14. These tricks called for a slightly different belief net setup, which was now deemed \u201cThe Helmholtz Machine\u201d15. Skirting the details once again, the key idea was to have separate sets of weights for inferring hidden variables from visible variables (recognition weights) and vice versa (generative weights), and to keep the directed aspect of Neal\u2019s belief nets. This allows the training to be done much faster, while being applicable to the unsupervised and supervised learning problems of Boltzmann Machines.\n\nFinally, belief nets could be trained somewhat fast! Though not quite as influential, this algorithmic advance was a significant enough forward step for unsupervised training of belief nets that it could be seen as a companion to the now almost decade-old publication on backpropagation. But, by this point new machine learning methods had begun to also emerge, and people were again beginning to be skeptical of neural nets since they seemed so intuition-based and since computers were still barely able to meet their computational needs. As we\u2019ll see in part 3, a new AI Winter for neural nets began just a few years later\u2026"
    },
    {
        "url": "https://medium.com/@andreykurenkov/a-brief-history-of-neural-nets-and-deep-learning-part-1-3fb758bbc6ca?source=user_profile---------10----------------",
        "title": "A \u2018Brief\u2019 History of Neural Nets and Deep Learning, Part 1",
        "text": "Originally published at http://www.andreykurenkov.com/writing/a-brief-history-of-neural-nets-and-deep-learning/. Better read there, but this is fine too.\n\nThis is the first part of \u2018A Brief History of Neural Nets and Deep Learning\u2019. Part 2 is here, and parts 3 and 4 are here and here. In this part, we shall cover the birth of neural nets with the Perceptron in 1958, the AI Winter of the 70s, and neural nets\u2019 return to popularity with backpropagation in 1986.\n\nThis may sound hyperbolic \u2014 to say the established methods of an entire field of research are quickly being superseded by a new discovery, as if hit by a research \u2018tsunami\u2019. But, this catastrophic language is appropriate for describing the meteoric rise of Deep Learning over the last several years \u2014 a rise characterized by drastic improvements over reigning approaches towards the hardest problems in AI, massive investments from industry giants such as Google, and exponential growth in research publications (and Machine Learning graduate students). Having taken several classes on Machine Learning, and even used it in undergraduate research, I could not help but wonder if this new \u2018Deep Learning\u2019 was anything fancy or just a scaled up version of the \u2018artificial neural nets\u2019 that were already developed by the late 80s. And let me tell you, the answer is quite a story \u2014 the story of not just neural nets, not just of a sequence of research breakthroughs that make Deep Learning somewhat more interesting than \u2018big neural nets\u2019 (that I will attempt to explain in a way that just about anyone can understand), but most of all of how several unyielding researchers made it through dark decades of banishment to finally redeem neural nets and achieve the dream of Deep Learning.\n\nLet\u2019s start with a brief primer on what Machine Learning is. Take some points on a 2D graph, and draw a line that fits them as well as possible. What you have just done is generalized from a few example of pairs of input values (x) and output values (y) to a general function that can map any input value to an output value. This is known as linear regression, and it is a wonderful little 200 year old technique for extrapolating a general function from some set of input-output pairs. And here\u2019s why having such a technique is wonderful: there is an incalculable number of functions that are hard to develop equations for directly, but are easy to collect examples of input and output pairs for in the real world \u2014 for instance, the function mapping an input of recorded audio of a spoken word to an output of what that spoken word is.\n\nLinear regression is a bit too wimpy a technique to solve the problem of speech recognition, but what it does is essentially what supervised Machine Learning is all about: \u2018learning\u2019 a function given a training set of examples, where each example is a pair of an input and output from the function (we shall touch on the unsupervised flavor in a little while). In particular, machine learning methods should derive a function that can generalize well to inputs not in the training set, since then we can actually apply it to inputs for which we do not have an output. For instance, Google\u2019s current speech recognition technology is powered by Machine Learning with a massive training set, but not nearly as big a training set as all the possible speech inputs you might task your phone with understanding.\n\nThis generalization principle is so important that there is almost always a test set of data (more examples of inputs and outputs) that is not part of the training set. The separate set can be used to evaluate the effectiveness of the machine learning technique by seeing how many of the examples the method correctly computes outputs for given the inputs. The nemesis of generalization is overfitting \u2014 learning a function that works really well for the training set but badly on the test set. Since machine learning researchers needed means to compare the effectiveness of their methods, over time there appeared standard datasets of training and testing sets that could be used to evaluate machine learning algorithms.\n\nOkay okay, enough definitions. Point is \u2014 our line drawing exercise is a very simple example of supervised machine learning: the points are the training set (X is input and Y is output), the line is the approximated function, and we can use the line to find Y values for X values that don\u2019t match any of the points we started with. Don\u2019t worry, the rest of this history will not be nearly so dry as all this. Here we go.\n\nWhy have all this prologue with linear regression, since the topic here is ostensibly neural nets? Well, in fact linear regression bears some resemblance to the first idea conceived specifically as a method to make machines learn: Frank Rosenblatt\u2019s Perceptron2.\n\nA psychologist, Rosenblatt conceived of the Percetron as a simplified mathematical model of how the neurons in our brains operate: it takes a set of binary inputs (nearby neurons), multiplies each input by a continuous valued weight (the synapse strength to each nearby neuron), and thresholds the sum of these weighted inputs to output a 1 if the sum is big enough and otherwise a 0 (in the same way neurons either fire or do not). Most of the inputs to a Perceptron are either some data or the output of another Perceptron, but an extra detail is that Perceptrons also have one special \u2018bias\u2019 input, which just has a value of 1 and basically ensures that more functions are computable with the same input by being able to offset the summed value. This model of the neuron built on the work of Warren McCulloch and Walter Pitts Mcculoch-Pitts3, who showed that a neuron model that sums binary inputs and outputs a 1 if the sum exceeds a certain threshold value, and otherwise outputs a 0, can model the basic OR/AND/NOT functions. This, in the early days of AI, was a big deal \u2014 the predominant thought at the time was that making computers able to perform formal logical reasoning would essentially solve AI.\n\nHowever, the Mcculoch-Pitts model lacked a mechanism for learning, which was crucial for it to be usable for AI. This is where the Perceptron excelled \u2014 Rosenblatt came up with a way to make such artificial neurons learn, inspired by the foundational work4of Donald Hebb. Hebb put forth the unexpected and hugely influential idea that knowledge and learning occurs in the brain primarily through the formation and change of synapses between neurons \u2014 concisely stated as Hebb\u2019s Rule:\n\nThe Perceptron did not follow this idea exactly, but having weights on the inputs allowed for a very simple and intuitive learning scheme: given a training set of input-output examples the Perceptron should \u2018learn\u2019 a function from, for each example increase the weights if the Perceptron output for that example\u2019s input is too low compared to the example, and otherwise decrease the weights if the output is too high. Stated ever so slightly more formally, the algorithm is as follows:\n\n4. Go to the next example in the training set and repeat steps 2\u20134 until the Perceptron makes no more mistakes\n\nThis procedure is simple, and produces a simple result: an input linear function (the weighted sum), just as with linear regression, \u2018squashed\u2019 by a non-linear activation function (the thresholding of the sum). It\u2019s fine to threshold the sum when the function can only have a finite set of output values (as with logical functions, in which case there are only two \u2014 True/1 and False/0), and so the problem is not so much to generate a continuous-numbered output for any set of inputs \u2014 regression \u2014 as to categorize the inputs with a correct label \u2014 classification.\n\nRosenblatt implemented the idea of the Perceptron in custom hardware (this being before fancy programming languages were in common use), and showed it could be used to learn to classify simple shapes correctly with 20x20 pixel-like inputs. And so, machine learning was born \u2014 a computer was built that could approximate a function given known input and output pairs from it. In this case it learned a little toy function, but it was not difficult to envision useful applications such as converting the mess that is human handwriting into machine-readable text.\n\nBut wait, so far we\u2019ve only seen how one Perceptron is able to learn to output a one or a zero \u2014 how can this be extended to work for classification tasks with many categories, such as human handwriting (in which there are many letters and digits as the categories)? This is impossible for one Perceptron, since it has only one output, but functions with multiple outputs can be learned by having multiple Perceptrons in a layer, such that all these Perceptrons receive the same input and each one is responsible for one output of the function. Indeed, neural nets (or, formally, \u2018Artificial Neural Networks\u2019 \u2014 ANNs) are nothing more than layers of Perceptrons \u2014 or neurons, or units, as they are usually called today \u2014 and at this stage there was just one layer \u2014 the output layer. So, a prototypical example of neural net use is to classify an image of a handwritten digit. The inputs are the pixels of the image , and there are 10 output neurons with each one corresponding to one of the 10 possible digit values. In this case only one of the 10 neurons output 1, the highest weighted sum is taken to be the correct output, and the rest output 0.\n\nIt is also possible to conceive of neural nets with artificial neurons different from the Perceptron. For instance, the thresholding activation function is not strictly necessary; Bernard Widrow and Tedd Hoff soon explored the option of just outputting the weight input in 1960 with \u201cAn adaptive \u201cADALINE\u201d neuron using chemical \u201cmemistors\u201d5, and showed how these \u2018Adaptive Linear Neurons\u2019 could be incorporated into electrical circuits with \u2018memistors\u2019 \u2014 resistors with memory. They also showed that not having the threshold activation function is mathematically nice, because the neuron\u2019s learning mechanism can be formally based on minimizing the error through good ol\u2019 calculus. See, with the neuron\u2019s function not being made weird by this sharp thresholding jump from 0 to 1, a measure of how much the error changes when each weight is changed (the derivative) can be used to drive the error down and find the optimal weight values. As we shall see, finding the right weights using the derivatives of the training error with respect to each weight is exactly how neural nets are typically trained to this day.\n\nIf we think about ADALINE a bit more we will come up with a further insight: finding a set of weights for a number of inputs is really just a form of linear regression. And again, as with linear regression, this would not be enough to solve the complex AI problems of Speech Recognition or Computer Vision. What McCullough and Pitts and Rosenblatt were really excited about is the broad idea of Connectionism: that networks of such simple computational units can be vastly more powerful and solve the hard problems of AI. And, Rosenblatt said as much, as in this frankly ridiculous New York Times quote from the times:\n\nOr, have a look at this TV segment from the time:\n\nThis sort of talk no doubt irked other researchers in AI, many of whom were focusing on approaches based on manipulation of symbols with concrete rules that followed from the mathematical laws of logic. Marvin Minsky, founder of the MIT AI Lab, and Seymour Papert, director of the lab at the time, were some of the researchers who were skeptical of the hype and in 1969 published their skepticism in the form of rigorous analysis on of the limitations of Perceptrons in a seminal book aptly named Perceptrons7. Interestingly, Minksy may have actually been the first researcher to implement a hardware neural net with 1951\u2019s SNARC (Stochastic Neural Analog Reinforcement Calculator) 8, which preceded Rosenblatt\u2019s work by many years. But the lack of any trace of his work on this system and the critical nature of the analysis in Perceptrons suggests that he concluded this approach to AI was a dead end. The most widely discussed element of this analysis is the elucidation of the limits of a Perceptron \u2014 they could not, for instance, learn the simple boolean function XOR because it is not linearly separable. Though the history here is vague, this publication is widely believed to have helped usher in the first of the AI Winters \u2014 a period following a massive wave of hype for AI characterized by disillusionment that causes a freeze to funding and publications.\n\nSo, things were not good for neural nets. But why? The idea, after all, was to combine a bunch of simple mathematical neurons to do complicated things, not to use a single one. In other terms, instead of just having one output layer, to send an input to arbitrarily many neurons which are called a hidden layer because their output acts as input to another hidden layer or the output layer of neurons. Only the output layer\u2019s output is \u2018seen\u2019 \u2014 it is the answer of the neural net \u2014 but all the intermediate computations done by the hidden layer(s) can tackle vastly more complicated problems than just a single layer.\n\nThe reason hidden layers are good, in basic terms, is that the hidden layers can find features within the data and allow following layers to operate on those features rather than the noisy and large raw data. For example, in the very common neural net task of finding human faces in an image, the first hidden layer could take in the raw pixel values and find lines, circles, ovals, and so on within the image. The next layer would receive the position of these lines, circles, ovals, and so on within the image and use those to find the location of human faces \u2014 much easier! And people, basically, understood this. In fact, until recently machine learning techniques were commonly not applied directly to raw data inputs such as images or audio. Instead, machine learning was done on data after it had passed through feature extraction \u2014 that is, to make learning easier machine learning was done on preprocessed data from which more useful features such as angles or shapes had been already extracted.\n\nAside: why have non-linear activation functions \u00bb\n\nSo, it is important to note Minsky and Papert\u2019s analysis of Perceptrons did not merely show the impossibility of computing XOR with a single Perceptron, but specifically argued that it had to be done with multiple layers of Perceptrons \u2014 what we now call multilayer neural nets \u2014 and that Rosenblatt\u2019s learning algorithm did not work for multiple layers. And that was the real problem: the simple learning rule previously outlined for the Perceptron does not work for multiple layers. To see why, let\u2019s reiterate how a single layer of Perceptrons would learn to compute some function:\n\nThe reason why this does not work for multiple layers should be intuitively clear: the example only specifies the correct output for the final output layer, so how in the world should we know how to adjust the weights of Perceptrons in layers before that? The answer, despite taking some time to derive, proved to be once again based on age-old calculus: the chain rule. The key realization was that if the neural net neurons were not quite Perceptrons, but were made to compute the output with an activation function that was still non-linear but also differentiable, as with Adaline, not only could the derivative be used to adjust the weight to minimize error, but the chain rule could also be used to compute the derivative for all the neurons in a prior layer and thus the way to adjust their weights would also be known. Or, more simply: we can use calculus to assign some of the blame for any training set mistakes in the output layer to each neuron in the previous hidden layer, and then we can further split up blame if there is another hidden layer, and so on \u2014 we backpropagate the error. And so, we can find how much the error changes if we change any weight in the neural net, including those in the hidden layers, and use an optimization technique (for a long time, typically stochastic gradient descent) to find the optimal weights to minimize the error.\n\nBackpropagation was derived by multiple researchers in the early 60\u2019s and implemented to run on computers much as it is today as early as 1970 by Seppo Linnainmaa9, but Paul Werbos was first in the US to propose that it could be used for neural nets after analyzing it in depth in his 1974 PhD Thesis10. Interestingly, as with Perceptrons he was loosely inspired by work modeling the human mind, in this case the psychological theories of Freud as he himself recounts11:\n\nDespite solving the question of how multilayer neural nets could be trained, and seeing it as such while working on his PhD thesis, Werbos did not publish on the application of backprop to neural nets until 1982 due to the chilling effects of the AI Winter. In fact, Werbos thought the approach would make sense for solving the problems pointed out in Perceptrons, but the community at large lost any faith in tackling those problems:\n\nIt seems that it was because of this lack of academic interest that it was not until more than a decade later, in 1986, that this approach was popularized in \u201cLearning representations by back-propagating errors\u201d by David Rumelhart, Geoffrey Hinton, and Ronald Williams 12. Despite the numerous discoveries of the method (the paper even explicitly mentions David Parker and Yann LeCun as two people who discovered it beforehand) the 1986 publication stands out for how concisely and clearly the idea is stated. In fact, as a student of Machine Learning it is easy to see that the description in their paper is essentially identical to the way the concept is still explained in textbooks and AI classes. A retrospective in IEEE13echoes this notion:\n\nBut the three authors went much further than just present this new learning algorithm. In the same year they published the much more in-depth \u201cLearning internal representations by error propagation\u201d14, which specifically addressed the problems discussed by Minsky in Perceptrons. Though the idea was conceived by people in the past, it was precisely this formulation in 1986 that made it widely understood how multilayer neural nets could be trained to tackle complex learning problems. And so, neural nets were back! In part 2, we shall see how just a few years later backpropagation and some other tricks discussed in \u201cLearning internal representations by error propagation\u201d were applied to a very significant problem: enabling computers to read human handwriting."
    },
    {
        "url": "https://medium.com/@andreykurenkov/fun-visualizations-of-the-2015-stackoverflow-developer-survey-1116b86cc034?source=user_profile---------11----------------",
        "title": "Fun Visualizations of the 2015 StackOverflow Developer Survey",
        "text": "What for, where, and how do software developers work?\n\nWhat will follow are a selection of what I think are cool visualizations of data from the StackOverflow 2015 Developer Survey. The survey asked software developers a bunch of questions concerning their background and work, and got an impressive 26086 responses. Due o there being multiple \u2018Select up to\u2019 questions, the data contains 222 observations for every response \u2014 lots of data to play with and visualize! The visualizations were made with R, as part of a project for the Data Analysis with R class.\n\nHaving loaded and cleaned up the data, the natural place to start is looking at who responded to the survey:\n\nYep, big surprise, most respondents are male and younger than 35. Though, interestingly, females in the 20\u201324 range outnumber 25\u201329 range, which is not true for males. Also predictably, the graph shows that people with more experience tend to be older, though there are developers past their 40s with less than 10 years of experience which shows that it is possible to become a developer even at a later age. There is also the strong suggestion some respondents are jokesters, since there exist some people who are not yet 20 but have 11+ years of experience.\n\nThe fact that there is a strong correlation between age and experience (0.6424202, in fact) is hardly surprising, but it begs a question: does the increased experience translate to better compensation? With this huge spreadsheet in hand, that question is easy to answer:\n\nYep, the world is (at least to some degree and in this context) just. It is also very unequal, though those who like me who have made it to the US can at least feel good about rather cushy average compensations. Besides being in the US, I also happen to be the sort of person to work on these sorts of blog posts in my free time, so a follow up question that makes sense is whether hours spent programming as a hobby also correlates positively with compensation. Perhaps lots of hobbyist programming indicates a more skilled programmer who might get better positions, or perhaps increased compensation translates to less time for hobby programming. Once again, let\u2019s have a look at the data:\n\nIt seems the second is correct \u2014 more hours programming for fun does not equate to better compensation, though we can of course be optimistic and hope those with bigger compensations and less hobby programming relish their jobs and are in no need for extra projects. In fact, we can go ahead and look at the job satisfaction data and see:\n\nSo, seems having a higher paid job does somewhat correlate with having a more satisfying job \u2014 all those making big cash are not secretly miserable, good to know. Now then, all these line graphs are fun, but not very efficient, so using R magic we can make a graph that communicates quite a bit more about the variables we have been exploring thus far (click for larger version):\n\nFancy graphs, right? The fun is just getting started, trust me, the best is yet ahead\u2026\n\nNow, the \u2018in US\u2019/\u2019not in US\u2019 dichotomy is rather simplistic (and perhaps stereotypically American), so it\u2019d be interesting to see what others countries programmers fare well in:\n\nThe results, after filtering to require at least twenty observations per country, are as America and Europe heavy as this posts\u2019 likely readership \u2014 no surprise there. It is somewhat surprising that high tech countries such as South Korea and Germany are ranked relatively low in the list, though. But, as we said before money is not everything, so let\u2019s go ahead and see how the countries fare by job satisfaction:\n\nQuite a different result, though seemingly one with less disparity than compensation averages. The US is merely 20th! It looks like Denmark and Israel really hit the sweet spot in terms of balance on both measures. Alas, I live in the US, and that will not change anytime soon. And as a software developer in the US, the next question for me to ask is what industries and occupations correlate with high compensations here:\n\nAgain, a few unexpected details here \u2014 data scientists are apparently not making as much as I thought they might \u2014 but most of this seems plausible. Seems it is a good thing I have no particular desire to work in gaming and non-profit industries or as a mobile dev \u2014 but then these are just averages, and money is much less important than job fulfillment anyway. So, once again, let\u2019s have a look at that too:\n\nWell, good news for the gaming developer \u2014 you may statistically be likely to earn less than most occupations, but also top the charts in terms of job satisfaction. As we saw before, it is possible to display the count and variance of such data fairly nicely with a combination of boxplot and translucent points, so may as well do just that:\n\nThe data is discrete, but R makes it very easy to introduce jitter to be able to roughtly have a sense of the distributions underling the means we\u2019ve been looking at. There is quite a lot going on these graphs, but on the whole I think they do a good job of combining and conveying all involved information. Let\u2019s skip the whole tradition of doing the same for satisfaction, as I am sure you are getting tired of all this satisfaction and money talk and boring bar graphs, and change things up.\n\nHaving played this much with occupations and industries, I naturally started to wonder which industries have the most of each occupation. The only sane way I could envision tackling the question is with heatmaps, and so after several fervent hours of massaging the data and plotting I got this:\n\nI know, not fun \u2014 the full-stack web dev occupation and Software Products industry dwarf all the others in terms of counts and so make most of the heatmap a bland monotonous red. One option is to try to log scale the coloring, but I think that\u2019d be sort of cheating in this case (my mind is built to assume heatmaps are linear), so instead we can produce a heatmap where the coloring is scaled within one row:\n\nNot bad! The intersection of \u2018Student\u2019 and \u2018I\u2019m a student\u2019 is bright yellow, so this is at least somewhat correct. There are lots of little neat nuggets of info here, such as the amount of mobile devs and designers in consulting or the prevalence of embedded developers in telecommunications. Admittedly, I produced this heatmap only after another one I knew I would want to make from the start \u2014 a heatmap showing the technologies developers in different occupations use:\n\nAgain with the huge majority of web devs! Well, no matter, we can do the same thing as before and view the data with per-row scaling:\n\nTurns out executives rely mostly on JavaScript and the Cloud, who knew. As these technologies are sorted by overall usage and in fact only the top 20 are shown, I was surprised the niche ones such as LAMP and Redis still have more users than favorites of mine like Django or R. Overall though, this heatmap is nice for simply confirming common sense expectations about technology usage for each occupation. Still, I was not too big a fan of this look, so I also generated the same heatmap with a different:\n\nLet\u2019s finish up by looking at a topic this post is implicitly really about \u2014 how people learn. The survey helpfully asked what sort of training (education, online classes, even mentorships) each responder had. And so, again, we can make use of the information-dense wonder of a heatmap:\n\nSo, most devs get at most a Bachelors degree, learn on the job, or have no formal training at all. However, online classes are also popular, no doubt in addition to these other forms of training. There are also exceptions in the fields of Machine Learning devs and data scientists, which I have some interest in. Just to take a break from all these heatmaps, we can take a closer look at these two with good ol\u2019 bar graphs:\n\nBased on the low average salary of data scientists and the large numbers of them with non-formal or on the job training, I think it is likely the term has just gotten to be used very loosely to encompass many data-oriented positions. Machine learning developers, on the other hand, are still in a more exclusive club of more educated types who no doubt meddle with more math and algorithms. Regardless, the size of both of these occupations (that were not even really around a decade back) is as good a sign of our times as it gets.\n\nIt would be tiresome to do this sort of visualization for all the jobs types, but why not go ahead and cap off this spade of visuals with something truly over the top:\n\nFor fun! But also, for learning. One of the great things about being a person who writes software for fun (and a living) is the lack of barriers to self-teaching new skills \u2014 all one needs is a laptop, an internet connection, and large quantities of time and perseverance to go ahead and learn something new. Though in theory just about anything can be self taught with a textbook and time, learning this way can be very difficult if the knowledge is not applied and tested along the way. Computer Science allows for the best of both worlds here in that there are massive amounts of tutorials and information for free online, and using that information requires no breadboards, no tools, no chemicals \u2014 just a computer and a mediocre internet connection.\n\nWhat barriers do exist have been further diminished by the rise of online classes in the vein of those on Udacity and Coursera, which are particularly well suited to teaching software skills \u2014 there are now dozens of such classes about algorithms, mobile dev, web dev, machine learning, and so on. Having taken and completed several of these classes I think they can very effectively help with learning through briskly paced lessons and high quality assignments. So, when I found out about the Udacity Data Analyst Nanodegree I was naturally intrigued given my existing fondness for machine learning and lack of experience with data visualization \u2014 intrigued enough to sign up for the program (after confirming my company was willing to foot the bill here).\n\n\u2018Data Analysis with R\u2019 is the fourth class in the program so far, and was my introduction to the language (a sort of mix between Python and SQL, known for being good for data analysis and visualization). The final project for the class called for using the R skills so far for self-led exploration of some data, with the option to choose among a few supplied datasets or find your own \u2014 precisely the sort of freeform class assignment I can get excited about. It took me some time to settle on a fun dataset to work with, but when I remembered about the StackOverflow survey data results I became sure there had to be many more opportunities for neat data analysis there \u2014 and I\u2019d say I was right!"
    }
]