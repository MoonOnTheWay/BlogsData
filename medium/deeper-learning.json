[
    {
        "url": "https://medium.com/deeper-learning/glossary-of-deep-learning-batch-normalisation-8266dcd2fa82?source=---------0",
        "title": "Glossary of Deep Learning: Batch Normalisation \u2013 Deeper Learning \u2013",
        "text": "Batch normalisation is a technique for improving the performance and stability of neural networks, and also makes more sophisticated deep learning architectures work in practice (like DCGANs).\n\nThe idea is to normalise the inputs of each layer in such a way that they have a mean output activation of zero and standard deviation of one. This is analogous to how the inputs to networks are standardised.\n\nHow does this help? We know that normalising the inputs to a network helps it learn. But a network is just a series of layers, where the output of one layer becomes the input to the next. That means we can think of any layer in a neural network as the first layer of a smaller subsequent network.\n\nThought of as a series of neural networks feeding into each other, we normalising the output of one layer before applying the activation function, and then feed it into the following layer (sub-network).\n\nIn Keras, it is implemented using the following code. Note how the BatchNormalization call occurs after each fully-connected layer, but before the activation function and dropout.\n\nBatch normalisation was introduced in Ioffe & Szegedy\u2019s 2015 paper. The idea being that, instead of just normalising the inputs to the network, we normalise the inputs to layers within the network. It\u2019s called \u201cbatch\u201d normalization because during training, we normalise the activations of the previous layer for each batch, i.e. apply a transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n\nBeyond the intuitive reasons, there are good mathematical reasons why it helps the network learn better, too. It helps combat what the authors call internal covariate shift. This is discussed in the original paper and the Deep Learning book (Goodfellow et al), in section 8.7.1 of Chapter 8."
    },
    {
        "url": "https://medium.com/deeper-learning/glossary-of-deep-learning-autoencoder-1044ec82c300?source=---------1",
        "title": "Glossary of Deep Learning: Autoencoder \u2013 Deeper Learning \u2013",
        "text": "An Autoencoder is neural network capable of unsupervised feature learning.\n\nNeural networks are typically used for supervised learning problems, trying to predict a target vector y from input vectors x. An Autoencoder network, however, tries to predict x from x, without the need for labels. Here the challenge is recreating the essence of the original input from compressed, noisy or corrupted data.\n\nThe idea behind the Autoencoder is to build a network with a narrow hidden layer between Encoder and Decoder that serves as a compressed representation of the input data.\n\nThe middle layer (\u201cbottleneck\u201d) is the compressed representation of the input data, from which the original data can be reconstructed.\n\nPassing input data through the Encoder creates a compressed representation that can then be passed through a Decoder to reconstruct the original input. The difference between an Autoencoder and an engineered compression algorithm is that both compression and decompression functions are learned from the data itself, rather than being consciously designed and implemented programmatically (like jpg or zip).\n\nBoth Encoder and Decoder are feed-forward neural networks, and the whole network is trained by minimising the difference between the input and output, as per normal. This does mean that Autoencoders once trained are quite specific, and will have trouble generalizing to data sets other than those they were trained on."
    },
    {
        "url": "https://medium.com/deeper-learning/glossary-of-deep-learning-dynamic-memory-networks-1ada00d0400a?source=---------2",
        "title": "Glossary of Deep Learning: Dynamic Memory Networks \u2013 Deeper Learning \u2013",
        "text": "The reference papers for DMNs are Xiong et al 2016 and Kumar et al 2015, they describe an architecture with the following components:\n\nThe Semantic Memory Module (analogous to a knowledge base) consists of pre-trained GloVe vectors that are used to create sequences of word embeddings from input sentences. These vectors will act as inputs to the model.\n\nThe Input Module processes the input vectors associated with a question into a set of vectors termed facts. This module is implemented using a GRU, or Gated Recurrent Unit, similar to an LSTM but simpler, and so more computationally efficient. The GRU enables the network to learn if the sentence currently under consideration is relevant or nothing to do with the answer.\n\nThe Question Module processes the question word by word, and outputs a vector using the same GRU as the input module, and the same weights. Both facts and questions are encoded as embeddings.\n\nThe Episodic Memory Module receives the fact and question vectors extracted from the input and encoded as embeddings. This uses a process inspired by the brain\u2019s hippocampus, which can retrieve temporal states that are triggered by some response, like sights or sounds.\n\nOn silicon, episodic memory is composed of two nested GRUs.\n\nThe inner GRU (the lower line in diagram above) generates what are called episodes. The outer GRU (upper line in diagram) generates the final memory vector by working over a sequence of these episodes.\n\nEpisode generation involves the inner GRU passing over the facts from the input module. When updating its inner state, it takes into account the output of an attention function on the current fact \u2014 which gives a score between 0 and 1 to each fact. This allows the GRU to ignore facts with low scores.\n\nAfter each full pass on all the facts, the inner GRU outputs an episode which is then fed to the outer GRU, whose state has been initialised by the question vector.\n\nThe reason the DMN uses multiple episodes is so the model can learn what part of a sentence it should pay attention to, and so realise after one pass that something else was important. Multiple passes allow it to gather increasingly relevant information.\n\nFinally, the Answer Module generates an appropriate response. By the final pass, the episodic memory should contain all the information required to answer the question. This module uses another GRU, trained with the cross-entropy error classification of the correct sequence, which can then be converted back to natural language."
    },
    {
        "url": "https://medium.com/deeper-learning/glossary-of-deep-learning-error-1f70d9bb88e9?source=---------3",
        "title": "Glossary of Deep Learning: Error \u2013 Deeper Learning \u2013",
        "text": "Describing problem solving as a kind of numerical optimisation might make the process of machine learning sound like it\u2019s basically just finding the solution of some particularly complicated equations. But it isn\u2019t as simple as that, and the clue to the reason is in the name: machine learning needs to be able to learn \u2014 to be able to generalise from what\u2019s been seen in the training data, and apply it to examples it\u2019s never seen before.\n\nEnsuring we have sufficient flexibility to generalise introduces two potential sources of error: being too simplistic, and being too complicated. You\u2019ll see these referred to as bias and variance respectively, and ideally we\u2019d like to minimise both, but in practice, it\u2019s a trade-off. Bias is like a misaligned sight, whilst variance is like not aiming at all.\n\nBias is the tendency to keep getting the same results incorrectly. It\u2019s an error that arises when a model is too simple, and so fails to represent the complexity of the underlying data. This can cause an algorithm to miss relevant relations between salient features and the expected outputs, despite having more than enough training data \u2014 a problem called underfitting.\n\nBias is a consequence of an oversimplified model. An example might be a classifier that can only partition objects by colour, but is too simplistic to also classify by shape, or other discriminating features. High Bias tends to show up as a high residual error in the training set \u2014 if it fails to fall as you add more training data, the model is likely not sophisticated enough.\n\nUnderfitting is depicted as the orange line in the diagram below, which shows how 3 different classifiers might partition a 2-dimensional problem space. The underfitted model simply considers red dots to be more likely in the lower area than blue dots, and fails to capture any subtleties.\n\nNow look at the Good Fit line (in black), the really interesting aspect to notice here is how it still has errors: the dividing line has coloured dots on the \u2018wrong\u2019 side. Yet it\u2019s also possible to draw another wriggly line (shown in green) with no error at all, one that gets everything completely right. So, here\u2019s a question for you: would we want to?\n\nThe answer is no. And the reason is variance. The real world is a noisy and unpredictable place. If we overcommit to the exact patterns seen during training, how will we be able to handle new and surprising situations?\n\nVariance is tendency to learn random things that seem significant, rather than the truly important salient features. An analogy is a student who memorises the answers to practice tests, they may score well initially, only to do badly on the final test, because they never really understood the subject.\n\nThis presents a challenge known as Overfitting, and is often encountered when the training set lacks sufficient diversity. A model can end up being trained on just obvious features or noise, and never be given the chance to train on the full range of examples it might encounter. Ideally, a model should perform as well on data it\u2019s never seen before, as it does on data it\u2019s been trained on. If the error is significantly bigger in the unseen test set, that\u2019s an indication the model might be overfitting."
    },
    {
        "url": "https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca?source=---------4",
        "title": "Glossary of Deep Learning: Word Embedding \u2013 Deeper Learning \u2013",
        "text": "The Big Idea of Word Embedding is to turn text into numbers.\n\nThis transformation is necessary because many machine learning algorithms (including deep nets) require their input to be vectors of continuous values; they just won\u2019t work on strings of plain text.\n\nSo a natural language modelling technique like Word Embedding is used to map words or phrases from a vocabulary to a corresponding vector of real numbers. As well as being amenable to processing by learning algorithms, this vector representation has two important and advantageous properties:\n\nIf you\u2019re familiar with the Bag of Words approach, you\u2019ll know it results in huge, very sparse one-hot encoded vectors, where the dimensionality of the vectors representing each document is equal to the size of the supported vocabulary. Word Embedding aims to create a vector representation with a much lower dimensional space. These are called Word Vectors.\n\nWord Vectors are used for semantic parsing, to extract meaning from text to enable natural language understanding. For a language model to be able to predict the meaning of text, it needs to be aware of the contextual similarity of words. For instance, that we tend to find fruit words (like apple or orange) in sentences where they\u2019re grown, picked, eaten and juiced, but wouldn\u2019t expect to find those same concepts in such close proximity to, say, the word aeroplane.\n\nThe vectors created by Word Embedding preserve these similarities, so words that regularly occur nearby in text will also be in close proximity in vector space. For examples of why this is useful, check out The amazing power of word vectors or this intro to Distributed Word Vectors on Kaggle.\n\nSo an answer to \u201cWhat is word embedding?\u201d is: it\u2019s a means of building a low-dimensional vector representation from corpus of text, which preserves the contextual similarity of words."
    },
    {
        "url": "https://medium.com/deeper-learning/glossary-of-deep-learning-bias-cf49d9c895e2?source=---------5",
        "title": "Glossary of Deep Learning: Bias \u2013 Deeper Learning \u2013",
        "text": "The activation of a node in a neural network is determined by the following:\n\nThis means when calculating the output of a node, the inputs are multiplied by weights, and a bias value is added to the result. The bias value allows the activation function to be shifted to the left or right, to better fit the data. Hence changes to the weights alter the steepness of the sigmoid curve, whilst the bias offsets it, shifting the entire curve so it fits better. Note also how the bias only influences the output values, it doesn\u2019t interact with the actual input data.\n\nYou can think of the bias as a measure of how easy it is to get a node to fire. For a node with a large bias, the output will tend to be intrinsically high, with small positive weights and inputs producing large positive outputs (near to 1). Biases can be also negative, leading to sigmoid outputs near to 0. If the bias is very small (or 0), the output will be decided by the values of weights and inputs alone.\n\nBear in mind, though, that the bias in a neural network nodes is not equivalent to the threshold of a perceptron, which only outputs 1 if sufficient input is supplied. Neurons don\u2019t have binary silent/fire thresholds, instead they have activation functions that produce a non-linear output between 0 and 1. So the role of bias isn\u2019t to act as a threshold, but to help ensure the output best fits the incoming signal.\n\nBiases are tuned alongside weights by learning algorithms such as gradient descent. Where biases differ from weights is that they are independent of the output from previous layers. Conceptually bias is caused by input from a neuron with a fixed activation of 1, and so is updated by subtracting the just the product of the delta value and learning rate.\n\nTypically biases are initialised to be zero, since asymmetry breaking is provided by the small random numbers in the weights (see Weight Initialisation).\n\nNote: the term Bias is also used to refer to the systematic errors in a model, the tendency to keep getting the same results incorrectly. This tends to arise when a model is too simple, and so fails to represent the complexity of the underlying data, causing an algorithm to miss relevant relations between salient features and the expected outputs, despite having more than enough training data \u2014 a problem called underfitting. Something I explain here."
    },
    {
        "url": "https://medium.com/deeper-learning/a-glossary-of-deep-learning-9cb6292e087e?source=---------6",
        "title": "A Glossary of Deep Learning \u2013 Deeper Learning \u2013",
        "text": "The image of The Pleiades above provides what I consider a fitting visual metaphor for the nodes of a neural network. Some activating brilliantly, others dimly. All widely distributed, yet still interacting subtly. Considered individually, each is just a blazing star, but collectively, at the right distance, beauty emerges\u2026\n\nThe format of this glossary is inspired by Marvin Minsky\u2019s AI classic The Society of Mind, a collection of short posts, where I can record what I\u2019ve encountered, and perhaps teach what I\u2019ve found.\n\nBeing a glossary, there\u2019s no need to read posts in chronological order, just read whatever topics catch your interest. As there\u2019s no narrative, this post will serve as the index, and will be updated as new posts are added. If you find it useful, you might like to bookmark it.\n\nMedium is a collaborative environment, so if you have something you\u2019d like to add to anything that I\u2019ve written, just highlight the text and add it as a margin note, or click on the speech bubble to enter a comment. And if you do find an entry particularly useful, do click on the green heart, it will help others find it.\n\nI hope you find it useful."
    },
    {
        "url": "https://medium.com/deeper-learning/glossary-of-deep-learning-backpropagation-e6d748d36a0e?source=---------7",
        "title": "Glossary of Deep Learning: Backpropagation \u2013 Deeper Learning \u2013",
        "text": "The mathematical foundations of neural networks are differentiable graphs \u2014 chains of computations whose derivative can be calculated. It is this property that allows the weights between nodes to be optimised through a process called Backpropagation, and what allows a network to learn from its mistakes.\n\nBackpropagation is so named because the weights are updated backwards, from output towards input. This involves the following steps:\n\nLet\u2019s consider each step in turn.\n\nSay we have a simple 2 layer network with a linear node l1\u200b\u200b, a sigmoid node s, and another linear node l\u200b2\u200b\u200b, followed by a node that calculates the cost, C.\n\nThe forward pass generates predictions for each sample in the batch, which we can compare to the true label y. This gives us the information necessary to calculate the output layer error, which is the difference between desired target and actual output.\n\nAs each of the values of the nodes\u2019 weights flow forwards, they eventually contribute to the cost C. Accordingly, a change in l2 \u200bwill produce a change in C, and the relationship between these changes can be written as a gradient: \u2202C / \u2202l2\n\nThe gradient is a slope that tells us the change in the cost \u2202C given a change in l2\u200b\u200b, (\u2202l\u200b2)\u200b\u200b. So a node with a larger gradient with respect to the cost is going to contribute a larger change to the cost. In this way, we can assign blame for the cost to each node. The larger the gradient for a node, the more \u201cblame\u201d it gets for the final cost \u2014 and the more we\u2019ll update this node in the gradient descent step.\n\nUpdating a particular node\u2019s weights with gradient descent requires the gradient of the cost with respect to those weights. Hence in the second layer, l2\u200b\u200b, we calculate the gradient of C with respect to w\u200b2\u200b\u200b, written: \u2202C / \u2202w2\n\nAs you can see in the graph, weight w\u200b2\u200b\u200b contributes to the output of l\u200b2\u200b\u200b, so any change in w\u200b2\u200b\u200b will propagate and create a change in C. Ideally, this will be a constructive change and the error will decrease. This is achieved by assigning blame to w\u200b2\u200b\u200b by sending the cost gradient back through the network. First, there\u2019s how much l\u200b2\u200b\u200b affected C, then how much w\u200b2\u200b\u200b affected l\u200b2\u200b\u200b. Multiplying these gradients together gets the total blame attributed to w\u200b2\u200b\u200b.\n\nThis is the basic idea behind backpropagation. To find the gradient for a particular node, we multiply the gradients for all nodes in front of it, going backwards from the cost. The gradients are passed backwards through the network and used within the gradient descent calculation to update the weights and biases. If a node has multiple outgoing nodes, we just sum up the gradients from each subsequent node.\n\nWhat about layers further back? To calculate the gradient for w\u200b1\u200b we use the same method as before, walking backwards through the graph, so the derivative calculations look like this:\n\nAs CS231n explains, backpropagation is an elegantly local process. During the forward pass, each nodes receives its own local inputs and can immediately compute both its output value and the local gradient of its inputs with respect to its output value. They can do this independently without being aware of any of the details of the wider network they are embedded in.\n\nOnce the forward pass is over, backpropagation will ensure the nodes eventually learn about the gradient of its output value on the final output of the entire network. Applying the Chain Rule means each node takes the gradient and multiplies it into every gradient it normally computes for all of its inputs. This extra multiplication (for each input) due to the chain rule can thus turn a single and seemingly useless node into a key cog in a complex distributed machine that is the entire neural network.\n\nBackpropagation in Python using basic linear algebra functions from NumPy looks like this (full code here):"
    },
    {
        "url": "https://medium.com/deeper-learning/glossary-of-deep-learning-activation-function-20262327a907?source=---------8",
        "title": "Glossary of Deep Learning: Activation Function \u2013 Deeper Learning \u2013",
        "text": "Fundamentally, neural networks are layered collections of nodes, each of which receives a set of inputs and individually makes the decision whether to fire, and propagate information downstream to subsequent layers.\n\nThe inputs are combined with weights and biases local to each node, which are updated by a learning algorithm in response to the observed error on training examples. This enables patterns in data to be learned by the neural network, where the value of the weight is proportional to its importance. For an example of how weights contribute to problem solving, see this interactive demo, or this one too.\n\nThe weighted input data from all sources is summed to produce a single value, (called the linear combination), which is then fed into an activation function that turns it into an output signal.\n\nOne of the original designs for an artificial neuron: the perceptron, had a binary output behaviour. Perceptrons would compare their weighted inputs to a threshold and if it was exceeded, the perceptron would be activated and output a 1, (otherwise a 0). But this output is a step function, which is not continuous or differentiable, and that\u2019s not as useful because differentiation (finding how incorrect we are) is what makes gradient descent possible.\n\nA better choice would be something like a sigmoid function, which replaces the step thresholding with an S-shaped curve. This activates like a perceptron, firing in response to the sum of its inputs, but the sigmoid function makes the output continuous, and so differentiable. Conceptually, the activation function is what makes decisions: when given weighted features from some data, it indicates whether or not the features are important enough to contribute to a classification.\n\nThe sigmoid function also has the very useful property that small changes in the weights and bias cause only a small change in output. That\u2019s crucial to allowing a network of neurons to learn.\n\nHence the purpose of the activation function is to introduce non-linearity into the neural network.\n\nThis allows us to model a response variable that varies non-linearly with its explanatory variables (one where the output couldn\u2019t be reproduced from a linear combination of inputs). Without a non-linear activation function, the neural network would behave just like a single-layer perceptron, because no matter how many layers it had, the composition of linear functions is always just a linear function (effectively a least-squares calculation).\n\nIn Python, an activation function looks like this; notice how it\u2019s a non-linear function that\u2019s applied to the dot product of weights, inputs and biases:"
    },
    {
        "url": "https://medium.com/deeper-learning/monkware-59316575107b?source=---------9",
        "title": "Monkware \u2013 Deeper Learning \u2013",
        "text": "The subtitle of the very first science-fiction novel was \u201cThe Modern Prometheus\u201d. And for as long as humanity has coveted the Fire of the Gods, we\u2019ve also worried about our ability to wield our new powers, and control our own creations.\n\nArtificial Intelligence (AI) is our latest gift of Promethean Fire.\n\nAs Chris Dixon explains, it\u2019s likely we\u2019re entering a new Golden Age of AI. For decades, the foundations of our civilisation have been technologies that have been intentionally programmed by human minds. But now we\u2019re beginning to imbue our modern world with autonomous intelligence, software with the ability to learn and generalise. We can create software that\u2019s now sufficiently complex, that we can\u2019t necessarily explain how it works.\n\nIn 2015, an open letter was written, signed by 150 prominent academics, entrepreneurs and AI researchers, among them Stephen Hawking and Elon Musk. Its signatories asked: how can we create AI systems that are beneficial to society rather than a threat? How do we ensure intelligent software we create is safe and robust?\n\nSome consider a possible Artificial Intelligence Revolution to be one of humanity\u2019s greatest existential threats. Should we be worried?\n\nIt\u2019s not often software makes the national news, but in March 2016 an AI system called AlphaGo managed to beat a Go Grandmaster, widely considered to be the best in the world. This was rightly considered by many to be a stunning achievement, Go is a highly intuitive strategy game, and it was thought that mastering Go might take another decade\u2019s work.\n\nThe success of AlphaGo relied on two key capabilities: one was the ability to imagine the future states of a game of Go, the other was the ability to form intuitions about which possible move was the best. It wasn\u2019t a perfect player, sometimes it made mistakes, yet it was still able to contrive rousing comebacks. Imagination and intuition, with a splash of fallibility; put like that, it\u2019s no wonder we can\u2019t help but see a kindred cognitive spirit.\n\nAlphaGo was made possible by a technology called Deep Learning, an approach based on neural networks, themselves an ersatz simulation of how the neurons in our own brains work. The same technology is already used to do all kinds of clever stuff, to translate between languages, to automatically recognise and categorise your photos, even to control self-driving cars.\n\nSo is all this further evidence that AI is beginning to acquire mental skills that we previously believed belonged only to ourselves?\n\nTo answer that, it\u2019s worth taking a moment to explain a couple of concepts: Weak AI, and Strong AI.\n\nWeak AI is a non-sentient automated intelligence that are capable of performing narrow specialised tasks. Personal assistants like Siri are a good example, they can understand natural language and follow instructions, but only if that instruction is something they\u2019ve been previously equipped to handle.\n\nBy contrast, Strong AI (also known as Artificial General Intelligence) is a self-aware intelligence that would be able to successfully perform any intellectual task that a human mind could. Note the use of the future tense here, Strong AIs are entirely hypothetical; right now, no Strong AI exists, or is even close to being built \u2014 the only place you\u2019ll likely to encounter them is in cinemas.\n\nSo, it\u2019s worth bearing in mind that despite its apparent cleverness, AlphaGo is still a Weak AI, a highly specialised intelligence that achieved its skills by training for thousands of hours on a massive database of archived Go games.\n\nIt\u2019s as if AlphaGo had given away its possessions, donned saffron robes, and gone off to some remote mountaintop Go dojo; and then done nothing else but practice playing Go for a decade or two.\n\nJust playing and playing. Perpetual self-improvement, every microsecond of every minute, 24 hours a day. Never resting. Absolutely focussed. Relentless.\n\nAnd as you\u2019d expect, when the monkware finally did come down from the dojo, it kicked ass. Practice does indeed make perfect. Just don\u2019t challenge it to a game of chess, it\u2019s never even heard of it."
    },
    {
        "url": "https://medium.com/deeper-learning/learning-1bc69ce0f57e",
        "title": "Learning \u2013 Deeper Learning \u2013",
        "text": "Generally speaking, there are 3 kinds of learning system:\n\nThis is an inductive process (from examples to general rules), where an algorithm is supplied with labelled data (correct answers). This is typically used for approximation tasks such as:\n\nIn addition to Linear and Logistic Regression, common supervised learning approaches include Decision Trees and Support Vector Machines.\n\nIn terms of optimisation, supervised learning attempts to find a model with the smallest difference between the known values and the model\u2019s own predictions (with regression, this uses a measure of average error)."
    },
    {
        "url": "https://medium.com/deeper-learning/reasoning-f2d96e0e553e",
        "title": "Reasoning \u2013 Deeper Learning \u2013",
        "text": "Reasoning is the use of logical inference to generate new information.\n\nThe ability to reason is of fundamental importance to any intelligent system, as it enables it to use new observations to exceed its original programmed capabilities. There are three kinds of reasoning of particular interest in Artificial Intelligence:"
    }
]