[
    {
        "url": "https://medium.com/intuitionmachine/sustainable-artificial-intelligence-architecture-requires-manageability-2bd197f40cd1?source=---------0",
        "title": "Sustainable Deep Learning Architectures require Manageability",
        "text": "This is a very important consideration that is often overlooked by many in the field of Artificial Intelligence (AI). I suspect there are very few academic researchers who understand this aspect. The work performed in academe is distinctly different from the work required to make a product that is sustainable and economically viable. It is the difference between computer code that is written to demonstrate a new discovery and code that is written to support the operations of a company. The former kind turns to be exploratory and throwaway while the the latter kind tends to be exploitive and requires sustainability. These two kinds are intrinsically two ends of the spectrum.\n\nThere are many areas where it is very clear as to how AI can be of great benefit. However, we need to pause and consider the manageability of a proposed system. In a messy world of ever changing needs and of systems that always fail, how then shall we build systems where the value to us far exceeds the cost of maintaining them. We will fail in deploying AI if we have little understanding of how to scale their deployment. AI is in dire need of operating environments where technical debt is contained and innovation is allowed to flourish unconstrained.\n\nIn a previous post, I had explored how Uber and Google have built Deep Learning systems to manage technical debt. I also previously explored an appealing architecture that is biologically inspired. This involves the consideration of capabilities such as redundancy, heterogeneity, modularity, adaptation, prudence and embededness. These are ideas that are conventionally outside the vocabulary of current machine learning experts. Clearly there is a need to understand how much larger (and economically viable) AI-driven system are to be built.\n\nMichael Jordan (Michael Jordan) recently wrote \u201cAI \u2014 The Revolution Hasn\u2019t Happened Yet\u201d that the principles for designing \u201cplanetary-scale inference-and-decision-making systems\u201d isn\u2019t a well understood discipline. It\u2019s hard to design something of this complexity when that something doesn\u2019t exist. We are all making it up as we move forward!\n\nWhere can we find the design inspiration to minimize our mistakes? Michael Jordan provides a hint to a forgotten discipline known as Cybernetics. As Jordan rightly astutely observes:\n\nIt is one tragedy of our civilization that those who conjure up the more appealing label receives a lion share of the credit. The value of branding should not be dismissed even in the supposedly meritocracy we imagine we have in the sciences.\n\nCybernetics studies the interplay of intelligent systems: both human and machine. This differs from the goals of Artificial Intelligence which focuses on the attainment of human capable general intelligence. The majority of excitement and research funding is focused on the nuts and bolts of creating more capable cognitive machinery. Unfortunately, cybernetics and the study of developing better human and machine collaborated remains grossly underfunded. Human-in-the-loop systems are today addressed by a few in UX community however this leads to a extremely narrow and incomplete perspective.\n\nThe recent Facebook and Cambridge Analytica fiasco and the manufacturing failures at Tesla are testaments to the intrinsic complexity of these systems. We simply all lack the cognitive framework to properly manage this complexity. It goes beyond just having a handle on relevancy, provenance and reliability that we find in today\u2019s conventional information systems. The failures of Facebook involve privacy and security. The failures of Tesla involve properly balancing human and robotic work. In both instances, it involves the inclusion of humans as a variable in the complete equation. Unfortunately, we are all at a loss of the shape of this equation.\n\nOne may divide the concerns of AI-driven architectures into two areas. Michael Jordan labels this as Intelligent Augmentation (IA) and Intelligent Infrastructure (II). IA, designing systems to handle the cognitive load of workers is a topic I discuss in detail in The Deep Learning AI Playbook. However, II involves the design and development of sustainable infrastructure. An intuitive understanding of the difficulties of this area can be best depicted by the \u201cAV Problem\u201d. That is, the audio-visual problem that we can\u2019t seem to solve every time we have a presentation. Countless hours have been wasted every single day trying to interface a computer to the audio visual system. It is a problem that seems to be simple enough, yet it seems to defy a solution. Solve the AV problem and you have made a gigantic step in solving the II problem."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-value-of-understanding-cognitive-sloth-d3888912b31d?source=---------1",
        "title": "The Value of Understanding \u201cCognitive Sloth\u201d \u2013 Intuition Machine \u2013",
        "text": "The Principle of Least Action is a pervasive concept, not only in physics, but also in the way our brains work. It is nature\u2019s tendency to prefer the minimal expenditure of energy (or work) to achieve the same thing. The Principle of Least action can be interpreted in our modern world as efficiency. Americans have a pronounced disposition of expecting and seeking out efficiency in many things we do.\n\nOur minds gravitate to using systems that are efficient. This is because efficient systems are also the same systems that are most convenient. Systems that are convenient are available on demand and frictionless in its use. We are conditioned to seek out instant gratification on everything. We seek out whatever is easy to do. Our minds are naturally lazy. Our minds have a tendency towards a deadly sin known as sloth. Our minds favor Cognitive Sloth.\n\nPhysical laziness is easily recognized and it is universally disdained. However, cognitive laziness is less transparent and it something that is internal to our own thinking and is harder to recognize. Our minds are biased towards anything the requires less effort.\n\nA recent study published in the Journal of Experimental Social Psychology (see: Contextual and personal determinants of preferring success attribute to natural talent or striving) has established that there is a preference for people who are successful due to talent over those who strove for that success. Said from the lens of laziness, we are attracted more to the people who did less to achieve their success. Why is this behavior true? Perhaps it is easier to see ourselves becoming successful with the least amount of effort? This is goes against the truth that you can\u2019t conjure up talent but you can definitely guarantee making an effort. The seductive appeal of cognitive laziness is ubiquitous and its pervasive in many cultures.\n\nThe tyranny of convenience has a detrimental effect on our own personal psychology. Mel Robbins says that \u201cMotivation is Garbage\u201d:\n\nMel Robbins argues that seeking motivation is a crutch we use to hide our true tendencies to avoid effort. Our minds naturally will avoid effort, it is our own consciousness that should make the decision to avoid laziness. Discomfort is the price of admission to a meaningful life.\n\nOur personalities are in fact inversely aligned with the degree of laziness we exhibit. The empirically verified Big Five Personality traits are: Openness, Conscientiousness, Agreeableness, Stability, Extroversion. People who are extremely successful are those people with personality traits that require the maximum amount of effort. People who are incarcerated are those found in the other end of every trait. Cognitive sloth is a universal metric that applies to all our personality traits. Unfortunately, we recognize personality as being a talent despite the more obvious reality that to have any of possess these traits means that you need to exert more cognitive effort. (BTW, personality are inherited traits, those lucky enough to have the right set require less conscious effort).\n\nIn my book, \u201cThe Deep Learning AI Playbook\u201d, I explore the idea of cognitive load as being the cause of human cognitive biases. The utility of AI (or any cognitive tool) is to reduce our own cognitive loads. Tools that reduce the need to memorize, reduce information overload, discover meaning or allow us to decide quickly are tools that aid us in handling more demanding thinking. It allows us to scale our knowledge discovery and creation. The value of AI can be compacted in this equation: Value = Context + Resources. When AI delivers cognitive resources in the right cognitive context, then it creates tangible value.\n\nKnowing that our brains have the tendency of Cognitive Sloth is extremely valuable. This implies that people will prefer cognitive resources that are most convenient to use. This is why the Jobs To Be Done (JTBD) approach is important for identifying the needs of a potential customer. This is the first step in creating a product or service of value. However, we also need to address the discovery problem. Customers need to know that your offering exists and it should be immediately obvious as to what need it addresses (See: Viral product development).\n\nConveying an offering\u2019s value demands appealing to Cognitive Sloth. An offering must be framed in the context of a customer and it should be intuitive as to how it can make easier a person\u2019s Job To Be Done. It is simply too big a cognitive leap for a customer to fill in the gaps of how a new technology can be used to fix one\u2019s own problems. The problem with the marketing of AI is that the expectations are unreasonably high. The expectations are that AI is expected to replace the thinking of customer. This appeals to our natural laziness, but this is an expectation that every vendor must manage correctly. It is unrealistic to expect today\u2019s AI to replace thinking. It is more realistic to expect that AI can reduce existing cognitive load."
    },
    {
        "url": "https://medium.com/intuitionmachine/just-in-time-inference-is-the-antidote-for-generalization-898bfeba5f7d?source=---------2",
        "title": "Just In Time Inference is the Antidote for Poor Generalization in Deep Learning",
        "text": "In a previous post we discussed the intrinsic complexity of biological neurons (see: Neurons are more complex that we thought) and thus the need for research to explore additional complexities in the standard model of the artificial neural network. In that post, we referenced research on a more complex LSTM node that was hierarchical. Although, a naive and simpler mathematical approach to adding more complexity is to use (pardon the pun) complex values for the weights (see: Should Deep Learning use Complex Numbers?). Complex ( i.e. with imaginary numbers) weights have been shown to primarily effective in Recurrent Neural Networks.\n\nRecently new research papers, that have been pre-published at Arxiv, shows that introducing flexibility to the underlying neural weights can lead to better generalization of a network.\n\nIn a paper \u201cFlipout: Efficient Pseudo-Independent Weight Perturbations on Mini-Batches\u201d the authors explore performing stochastic perturbations on the weights while in training. These perturbations of weights are designed to be (1) independent of each other and (2) symmetric around zero. The authors show that this FlipOut method outperforms DropOut. This paper provides us an early hint of the value of using stochastic weights over the more conventional deterministic weights.\n\nAn earlier even paper \u201cOn the Robustness of Convolutional Neural Networks to Internal Architecture and Weight Perturbations\u201d revealed that ConvNets were robust to weight perturbations at the higher layers (Not so for the lower layers). This tell here aligns with the intuition that greater inference flexibility is needed at inference of higher abstractions.\n\nUber, with great fanfare, has revealed a new paper where they describe \u201cDifferential Plasticity\u201d, the crux of their approach is to augment the standard weight with an additional term that becomes adjustable after training (see term below that is in red).\n\nThis simple addition appears to address the well known inflexibility of neural networks to make adjustments after training. Conventionally, Deep Learning networks are hardwired after training. Adding this plasticity term that is able to continually (or gradually) make adjustments over time. One obvious deficiency of neural networks is that of premature optimization.\n\nWe know this from computer science in the more general form of eager versus lazy evaluation. In its most general sense, it involves the deferment of execution only at the time that it is needed. Said differently, execution on demand or just in time. This differential plasticity is basically on demand inference or just-in-time inference.\n\nAnother Arxiv pre-publication \u201cIterative Visual Reasoning Beyond Convolutions\u201d shows how to use even more complex inference methods layered on top of standard ConvNets. In this paper, the authors show that through the use of knowledge graphs, region graphs and the assignment of regions to classes that they are able to improve conventional ConvNet classification by a whopping 8.4% improvement.\n\nThis approach to deferred execution is clearly critical to achieve better generalization. After all, it is illogical to have a cognitive system that is absent of any adaptive mechanism. This is the problem with conventional neural networks. Their weights have always been fixed. This notion of just in time originates from the \u201cLoosely Coupled Principle\u201d that forms the basis of more advanced Deep Learning architectures.\n\nMore about the Loose Coupling Principle and Deep Learning can be found here:"
    },
    {
        "url": "https://medium.com/intuitionmachine/our-minds-see-and-hear-only-what-we-imagine-dc303056171?source=---------3",
        "title": "How Human and Deep Learning Perception are Very Different",
        "text": "How do we perceive the world? To understand this, let us explore how we incorrectly perceive the world. The \u2018glitches in the matrix\u2019 shall reveal to us the nature of our perception.\n\nVictoria Syke created the above optical illusion that works astonishingly well to confuse our perception. The illusion here is that the dark blue lines run parallel to each other. You can prove this to yourself by either scrolling the image so that it aligns with the top of the browser window or you can look at the image from one of the edges.\n\nWhat I want to know is, why is this illusion so effective? What in our own cognitive processes is creating the confusion?\n\nIn the illusion above, you will notice that each element in the band with alternating light blue and black boxes appears smaller in size in a specific direction. In addition to this, you will also notice that the image in the dark blue band has lines that that are of a different height than the previous one. These two illusions combine with each other to give the illusion that a band in continuously trending upwards or downwards.\n\nThe light blue boxes do appear parallel even if you rotate the image by 90 degrees. That is because the dark blue boxes always appear the same size and the lines inside them are also at the same level.\n\nVictoria Syke was inspired to create this image from two sources. Richard Gregory\u2019s observation of the Cafe Wall Illusion and Akiyoshi Kitaoka\u2019s Fringe Edge Illusion.\n\nThe Cafe Wall illusion\u2019s effect reveals itself when the luminance of the mortar between the bricks have a luminance value between black and white:\n\nThe effect of which is that each brick appears to be progressively bigger (or smaller) than the brick that is adjacent to it.\n\nto enhance the effect even further. BTW, the effect also works in the vertical direction.\n\nThe mind apparently does not see an image as a whole. Rather it sees the image as a composition of images and recognizing the adjacent relationships of one to another. Why do adjacent relationships have such a strong effect on our visual perception? We have evolved to take advantage of affordances to allow our brain to reconstruct images more quickly. Said differently, our brains immediately recognizes patterns that facilitates our interpretation of a scene. Our visual perception performs a kind of semantic inference automatically such that higher level semantic patterns can\u2019t be ignored. That is why an illusion like this cannot be \u201cunseen\u201d no matter how much we convince ourselves that the lines are indeed parallel.\n\nThis effect also occurs in the audio domain. There is an auditory illusion known as the Shepard Tone. The illusion is created by having three tones that are ascending. A higher tone that becomes quieter, a middle tone that has constant loudness and a lower tone that becomes louder. The brain is tricked by hearing two tones that are always ascending. This is best illustrated in this video ( Start at 0:40 ):\n\nThe illusions in the image and auditory regimes reveals to us insights on how the mind perceives its world. Our minds sees images and sounds relative to each other and makes an imagined prediction of a progression even when that progression does not exist. The mind cannot override the affordances it sees and therefore proceeds with an incorrect reconstruction. You can look at the image above but you cannot unsee the lines that are tilting. If you look at the image at a distance or look at it at an angle, you see the image without the affordances and see thus reconstruct reality correctly.\n\nBut why is relative size important to our biology? We can learn from art as to what elements lead to a perception of depth: overlapping objects, diminishing scale, atmospheric perspective, vertical placement and linear perspective. The brain makes use of these affordances to reconstruct a 3D representation of the world. We are embodied in a 3D world and our senses are designed to comprehend and interact with that world. Affordances that are clues to the 3D structure of objects are the sources of optical illusions. The checker board shadow illusion is one of the more well known examples of this:\n\nHere is another illusion that illustrates how the brain must be given sufficient time to correctly re-construct its perception:\n\nIn the above experiment, as you focus on the center, you will notice that the faces in your periphery become distorted. The images are flashed fast enough that our brain see the cross talk between the two images and is not fast enough reconstruct it correctly.\n\nUnlike Deep Learning networks that actually do capture images in its entirety, biological brains will use affordances (i.e. shortcuts and heuristics) to construct patterns that it will use for perception. Deep Learning networks are trained specifically using networks that ignore certain invariances (i.e. translation for ConvNets). Biological brains appear to work differently, rather than ignore invariances we are hardwired to make use of patterns that convey semantics. DL networks are not trained to identify affordances that lead to pattern identification that leads to semantic interpretation. To achieve the kind of visual perception we find in humans, we must train networks to learn some basic human image recognition skills such as occlusion, perspective, and shadows:\n\nTo illustrate how very different a Deep Learning system\u2019s visual cognition is from that of humans, a recent paper \u201cInvestigating Human Priors for Playing Video Games\u201d investigates removing human affordances for playing a game:\n\nArcade games were modified to re-render the game\u2019s textures. In the modified game, humans performed extremely poorly. In contrast, a Deep Learning system performed equivalently for both games. Deep Learning systems do not need to use human priors. On the flip side, a human can learn a game with less trials because we can exploit the use of existing human priors (or affordances). What this should tell you is that humans learn quickly by using our existing priors.\n\nDeepMind\u2019s Pyschlab is a setup to explore the difference between Deep Learning and Human visual recognition. Psychlab contains many experiments that a human and a machine can perform. By examining the difference in performance, we can learn the cognitive differences between the two systems. In general, its observed that humans employ a mix of parallel and sequential processing. This can be discerned from a slowdown in the performance of tasks as compared to a machine that employs only parallel processing:\n\nAnother paper from DeepMind published in BioArxiv \u201cPrefrontal cortex as a meta-reinforcement learning system\u201d proposes that the brain uses two different reinforcement learning systems. Reinforcement learning in biological brains are postulated to be driven by dopamine releases. This is the standard model of reward driven learning. DeepMind\u2019s proposal is that there are two RL system, one RL system is based on the standard dopamine model and a second RL system is found in the prefrontal cortex. The prefrontal cortex learning is influenced by the first system. Effectively, the standard dopamine model has learned human priors (or affordances) and employs this to guide the more dynamic learning of the prefrontal cortex.\n\nSo whenever we see something, we can only see it with human priors engaged. However as you can see in the example of the faces, there is a cognitive process at work that attempts to reconstruct what it sees. Stop that reconstruction process too quickly and you see how it can incorrectly make errors. Our brain employs heuristics all the time and we find that these heuristics can fail in many ways.\n\nGeoffrey Hinton may be on the right track with his Capsule Network. In Capsule networks, there are two important stages. A first stage that is able to recognize parts of objects using a ConvNet and then a second stage that votes on which composition of recognized objects is the most likely one that is perceived. This two stage process, one of object recognition and then followed by inference seems to be gaining traction in the research community.\n\nIn the 1980\u2019s a new field emerged out of the advances of supercomputers, this was known as computational science and it differed to the existing approaches to science (i.e. Theoretical and Experimental). Computational science explored physical systems through simulation by computer. In the same way, research in Deep Learning is now encroaching into the fields of neuroscience and psychology. That is, we are beginning to understand our own nature as we compare our simulations with ourselves.\n\nIn summary, the emerging research trend in Deep Learning is to begin to dig deeper into the precise nature of human perception and to identify how it differs from Deep Learning perception. From the perspective of a Deep Learning researcher, it is not enough to understand the mathematics and the technology, but one must have some familiarity with the characteristics of basic human perception. It is well established that adversarial features are problematic for Deep Learning. To solve problems like this, we need to understand why it isn\u2019t a problem for humans. This is indeed exactly what Geoffrey Hinton argued about in his lecture about \u201cWhat\u2019s wrong with Convolutional Networks.\u201d"
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learnings-uncertainty-principle-13f3ffdd15ce?source=---------4",
        "title": "Deep Learning\u2019s Uncertainty Principle \u2013 Intuition Machine \u2013",
        "text": "DeepMind has a new paper where researchers have uncovered two \u201csurpising findings\u201d. The paper is described in \u201cUnderstanding Deep Learning through Neuron Deletion\u201d. In networks that generalize well, (1) all neurons are important and (2) are more robust to damage. Deep Learning network have behavior that reminds us of holograms. These results are further confirmation of my conjecture that Deep Learning systems are like holographic memories.\n\nHolograms are 3 dimensional images that are created by the interference of light beams. The encoding material of a hologram are 2 dimensional surfaces that capture the light field rather than the projection of an image on to the surface. In general you can take this to a higher dimension, so a hologram for 4D space-time will be in 3D (one dimension less). This relationship that a lower dimensional object can represent a higher dimensional object is in fact something our human intuition is unable to grasp. Our biological experience has understood the reverse, like 3D objects projecting in 2D planes.\n\nIn the beginning of the 20th century, light was discovered to have that perplexing quality of being both a wave and particle. This non-intuitive notion of quantum physics, this wave-particle duality, is also expressed as the Uncertainty Principle. A concept that is even more general than this is the recent discovery of the \u201cholographic duality\u201d. Apparently, nature also binds two different objects, the hologram and its higher dimensional projection in an equally bizarre manner:\n\nThere exists a holographic duality in nature that may likely also translate to the workings of Deep Learning networks. The greater the generalization of a network, the more entangled its neurons and as a consequence the network becomes less interpretable. The uncertainty principle as applied to Deep Learning can be stated as:\n\nWhich led me to my conjecture in 2016 that \u201cThe only way to make Deep Learning interpretable is to have it explain itself.\u201d If you think about it from the perspective of holographic duality, then it is futile to look at the internal weights of a neural network to understand its behavior. Rather, we best examine its surface to find a simpler (and less turbulent) explanation.\n\nThis leads me to the inevitable reality that the best we can do is to have machines render very intuitive \u2018fake explanations\u2019. Fake explanations are not false explanations, but rather incomplete explanations with a goal toward eliciting an intuitive understanding. This is what good teachers do, this is what Richard Feynman did when he explained quantum mechanics to his students.\n\nIn addition, this explains the real-world fragility of symbolic systems. Symbolic rules are abstract interpretations of a system. Artificial Intelligence as envisioned in the late 1950\u2019s was based on creating enough logical rules to arrive at human intelligence (see: Cyc). Decades of research in AI has been wasted on this top down approach. An alternative more promising bottom up approach (see: Artificial Intuition) is the basis of Deep Learning.\n\nThis Holographic Principle is a very compelling tell of how Deep Learning works. Unfortunately, just like Quantum Physics, it is belongs to a realm that is simply beyond our own intuition. Sir Roger Penrose may have been on the right track when he speculated that brains work as a consequence of quantum behavior. However, I have doubts that this is true.\n\nI will grant this observations that we simply don\u2019t have enough detail as to how the brain actually works (see: \u201cSurprise Neurons are More Complex\u201d). There are also good arguments that certain animals (i.e. bird brains for navigation) are enabled by unique mechanisms found in the physical world.\n\nHowever, the idea that quantum effects are the explanation for not just human cognition but animal cognition is a conjecture that is based on very sparse experimental evidence. The brain is likely more complex than our present artificial models, however that complexity (like turbulence) does not require quantum effects as an explanation. If you can explain turbulence as originating from quantum effects then that\u2019s a similar kind of argument you will have to make about cognition. This is the argument that is missing with Penrose.\n\nPenrose argues that you must have quantum effects to arrive at cognition. This currently is contrary to a majority of the opinion in neuroscience or in Deep Learning models. I am however proposing something different that says that quantum-like uncertainty is present in neural networks. There is emergent complexity in reality that is due to the internal interactions of massive populations (like the weather). I propose that our brains (similar to Deep Learning systems) exhibit this holographic-duality but entirely inside the regime of classical mechanics (i.e. composed of deterministic subcomponents).\n\nFun Fact: J.J. Thomson won the 1906 Nobel in Physics for experiments showing electrons where particles. His son G. P. Thomson won the 1937 Nobel Prize in Physics for showing electrons are waves."
    },
    {
        "url": "https://medium.com/intuitionmachine/drawing-the-ethical-line-on-weaponized-a-i-research-316e865e9d18?source=---------5",
        "title": "Drawing the Ethical Line on Weaponized Deep Learning Research",
        "text": "Good AI and Deep Learning researchers place a lot of passion into their work. Although they may rarely reflect on the ethical consequences of their creations, there is a belief and even an expectation that whatever they create will lead to the greater good. Even if that idea of a \u2018greater good\u2019 can be a nebulous idea that perhaps means \u201cleave the world in a better place\u201d.\n\nFor those who have spent considerable time contemplating the consequences of AI (or Artificial General Intelligence), the end game scenario that leads to a \u2018better place\u2019 (and not the alternative apocalyptic scenario) reveals many divergent futures. One would think humans will have a consensus on the \u201cbetter place\u201d future. Jurgen Schmidhuber\u2019s \u201cbetter place\u201d is a place where you have an advanced intelligent species very different from humans. Elon Musk\u2019s \u201cbetter place\u201d are human cyborgs in a world of AI. Ray Kurzweil\u2019s \u201cbetter place\u201d are populated with immortal humans. Gene Roddenberry\u2019s \u201cbetter place\u201d is a world without money.\n\nA majority of humans are genetically predisposed to strive for the common good. This is likely through natural selection as a consequence of the existence of civilization. Civilization has that curious side-effect of ensuring that bad actors are exterminated or placed in the \u2018dust bin\u2019 of history. Society is relatively free from chaos despite the reality that act of destruction is relatively easier to perform than the act of creation. Our ambition to make a mark on history is subservient to the need to do good. We don\u2019t need to go into the ethics about this, since this is all about our biology. George Lakoff (author of Philosophy of the Flesh) remarked that Philosophy is meaningless without taking into consideration our own biology:\n\nWe collectively have an \u201cintuitive\u201d understanding of what the greater good means despite actually having divergent views of what that actually means. Let\u2019s, for the purpose of discussion, make this an axiom that we don\u2019t need to explore further.\n\nAlfred Nobel invented dynamite, but he is most well known for the Nobel prize. On his death, the press wrote the headline \u201cThe merchant of death is dead\u201d and wrote that \u201cDr. Alfred Nobel, who became rich by finding ways to kill more people faster than ever before, died yesterday.\u201d Prior to his death, Nobel wrote a will that a majority of his wealth would go into a trust that lead to the funding of the Nobel prize. (Appears that he read his obituary while he was still alive). No AI researcher would relish to have an obituary like Nobel\u2019s.\n\nAlmost every technology can be a two edged sword (or have a \u201cdual use\u201d). A recently released 100 page report on the malicious use of AI writes:\n\nThis report explicitly recommends that an AI researcher as part of their due diligence also explore the potential misuse of their creations and to proactively communicate (to those who need to know, and not everyone) the foreseeable harmful effects of their creations. It demands that researchers are fully aware of the immediate consequence of their work. This is a good responsibility to bake into any Hippocratic oath for AI research (i.e. \u201cDo no harm\u201d). Oren Etzioni proposed the following version of the Hippocratic Oath for AI researchers:\n\nOne key part of this oath is that a researcher must be aware if his research can lead to AI that can \u2018take a life\u2019 (Should this be confined to human?). One can contribute to military technology, but it should be that of saving human lives. Shall that therefore be the line to be drawn in military research? That military research should strive towards defensive AI and never offensive AI?\n\nThe A.I. community now has a growing activism against any A.I. research that is related to weaponized A.I. Researchers have threatened to boycott KAIST (a South Korean university) for their AI work with a military contractor. KAIST however has quickly responded with this threat with following argument:\n\nThe problem with the KAIST argument is that it does not explicitly avoid offensive weaponry. Although the boundaries between a defensive weapon and an offensive weapon can be blurry, I would expect that it should be the responsibility of every AI researcher to explain the offensive capabilities of their research. It should not be for outsiders to figure this one out.\n\nAs an example, defensive weaponry such as the Patriot missile system that is able to recognize and track incoming ballistics appears to be ethical within the scope of the above Hippocratic oath. This is despite a Patriot missile having explosives and thus being weaponized. However, AI driven navigation of an undersea vehicle is problematic if that undersea vehicle has offensive capabilities. Working in a military establishment by itself should not be regarded as an unethical endeavor from the perspective of AI research. However, knowing that your AI research leads directly to an offensive capability may perhaps be clearly unethical. It is similar to that ethical question of whether a doctor should be directly responsible for euthanasia or corporal punishment.\n\nThis brings up the question of Google\u2019s participation in the Maven project of the US military. Many in Google are upset about this and have petitioned the company to divest itself of the project (See comments for the many sides of this debate). The petition demands that:\n\nIs all warfare technology something that is off limits for lethal automation? Is the analysis of imaging for potential targeting by a UAV within the bounds of the above Hippocratic oath?\n\nResearchers must be able to explore warfare technology of the defensive kind. I would rather see defensive AI technology having superiority over offensive AI technology. We cannot avoid other nation states from developing their own offensive AI technology. However we should not discourage AI researchers from exploring defensive technologies. There are many medical researchers as part of the military. Their work doesn\u2019t violate the Hippocratic oath that they have sworn to uphold, their research is focused on saving lives. In the same vein, AI researchers employed by the military should not be \u2018persona non-grata\u2019 simply because of their association with \u2018warfare technology\u2019.\n\nAn AI boycott of any \u201cwarfare technology\u201d is likely to be extremely effective in dissuading top research institutions and researcher from participating. Unfortunately, the broadness of the term \u201cwarfare technology\u201d is going to have a detrimental effect on our own security. No AI researcher would ever want the title of \u201cThe merchant of death\u201d in their obituary. Our society needs further debate on this subject before it degenerates in an all consuming lynch mob."
    },
    {
        "url": "https://medium.com/intuitionmachine/six-months-later-france-has-released-their-deep-learning-strategy-d2cbf18927d?source=---------6",
        "title": "Six Months Later, France has Formulated their Deep Learning Strategy",
        "text": "BTW, I am still waiting for the US to start panicking. I\u2019m not too optimistic, I don\u2019t think that there are many \u201cCedric Villanis\u201d (i.e. scientists) in the legislature or the executive branch of government. There is in fact an increasing probability that the US will shoot itself on its own foot with regards to developing an AI strategy. The recent debacles at Facebook, Uber, Tesla and the animosity against Amazon are newest signs that a backlash is brewing. The most productive thing we can do today is to emphasize the human benefits of AI and not simply drool over the latest developments."
    },
    {
        "url": "https://medium.com/intuitionmachine/teslas-factory-failures-reveals-why-you-shouldn-t-automate-everything-16c54e7f9f16?source=---------7",
        "title": "Tesla\u2019s Factory Woes Reveals Why You Shouldn\u2019t Automate Everything",
        "text": "There are new reports that Tesla\u2019s AI strategy to automate their entire manufacturing process is failing to deliver the productivity they had hoped for. Business Insider reports that \u201cWall Street analysts have laid down a compelling argument that over-automation is to blame.\u201d The report details the arguments as to why not everything needs to be automated:\n\nA balance must be maintained between the manageability of advanced AI technology and the tasks that can be performed by a reasonably skilled employee. There will always be tasks in the process where the costs for automation is not worth it. A majority of costs in AI is upfront, this upfront cost can spiral out of control if the problem is beyond what present day AI is capable of doing. This is the problem with many AI endeavors, too many are lured into the science fiction thinking that AI already exists today. One should never convert a task to improve productivity into a task to do academic research. Understanding what AI can and cannot do well is critically important to control costs and avoid failure. Do yourself a favor and hire a Deep Learning expert for an hour to tell you what not to do.\n\nThe Japanese who historically have a much more advanced experience working with automation know the problem better. The Japanese approach is to first get the process right and then bring in the robots. In fact, this approach translates well not only in manufacturing automation but also in knowledge-based work.\n\nIt is important to remember that today\u2019s lean methodology we find in software development can be traced back to lean manufacturing methods of the Japanese. Lean\u2019s core value is simple: maximize customer value while minimizing waste. These ideas work in manufacturing as well as in knowledge-driven industries.\n\nIn the book \u201cThe Deep Learning AI Playbook\u201d, I introduced the Deep Learning Canvas and the framework at its core is the Jobs To Be Done (JTBD) approach applied to \u2018Cognitive Load\u2019. What we attempt to do is to map out the existing business process and identify specifically the JTBD of a customer (i.e. this could be an employee). JTBD identifies many tasks that a customer performs to do their job and we identify the cognitive load (constraint/impediment) that can be augmented with AI technology. The cognitive loads include lack of memory, information overload, lack of meaning and acting fast. Each kind is augmented with different kinds of Deep Learning (DL) driven technology. Specifically search, summarization, translation and visualization. However, we should be pragmatic. We cannot expect DL to do everything.\n\nRather, as DL technology incrementally improves over time, each JTBD that has been augmented by AI continues to improve. This in effect reduces the cognitive load of the user for each task and as a consequence allows the user to become more productive in their work. Productivity may translate in higher throughput, but ideally towards a better customer experience (See: DL for CX and XLA). The higher level objective should always be CX, after all that is what motivates customers to invest in a relationship.\n\nThe value of AI is that it incorporates technology that is able to identify a users context and then deliver the appropriate goods or services at the right time. This is how value is created. This is how AI and processes are linked. In Lean Thinking, this is the assessment of the value stream to see if each step is \u201c valuable, capable, available, adequate and flexible\u201d. The right way to employ AI automation into a business is to start with a strategy that incorporates an understanding of purpose, process and most importantly \u2014 people."
    },
    {
        "url": "https://medium.com/intuitionmachine/growing-a-machine-learning-practice-inside-a-large-financial-firm-9564ddd2dd9?source=---------8",
        "title": "Growing a Machine Learning Practice Inside a Large Financial Firm",
        "text": "This post was produced in partnership with Capital One.\n\nI recently had the pleasure of speaking with Zachary Hanif, Director in the Center for Machine Learning (C4ML) at Capital One. I was interested in receiving some unique insight on the challenges of developing a machine learning practice within a large financial organization. I\u2019ve written previously about the machine learning best practices at Google and Uber. I was curious to understand how one would go about building a machine learning infrastructure within a firm such as a large regulated bank; following is a summary of our conversation.\n\nBanks have their own set of stringent regulations that companies like Google or Uber are not subjected to. The common stereotype of a large bank is that there is a considerable lag in the adoption of the latest technologies. Prior to joining Capital One, Zachary racked up experience in startup companies developing large scale applied ML infrastructures. With two years under his belt at Capital One, he has been retrospectively quite impressed with the company\u2019s philosophy regarding technology and how proactively they\u2019ve embraced the latest developments.\n\nC4ML started with only a handful of people at the beginning of 2017 and now has over 100. How did it scale so quickly? As Capital One began applying machine learning to more and more facets of the business, C4ML was created as a center of excellence and in-house consultancy to further catalyze the adoption of the technology across the organization, centering on product delivery, innovation, education and research, and partnerships.\n\nTo grow the team, C4ML coupled highly specialized internal recruitment with Capital One\u2019s broader \u201cTechnology Development Program\u201d (TDP), a two-year intensive rotation training program for recent college grads with CS/CE experience to apply their skills in a C4ML-specific track (the TDP is a top channel for junior engineering and ML talent in the group). The program helps recent grads determine what type of work they\u2019d like to focus on within engineering and ML, while also training them for the specific skills needed at Capital One, as there can often be harsh learning curves when recent grads enter the professional world. To ensure a smooth transition for incoming talent, junior developers are also paired with more senior, experienced developers.\n\nThe team targeted majors in computer science, applied mathematics, statistics and physics. C4ML looked for graduates that were strong technically and showed a passion for their craft.\n\nIn addition to hiring recent graduates, C4ML has a robust team of experienced professionals who previously worked with problems that required a lot of data. Specific technical environments, for example, involved the use of Spark, High Performance Computing (HPC) or time series analytics. Professionals that were \u201cdouble-tracked\u201d were ideal candidates; these are professionals who had extensive analytics experience as well as a solid software engineering background.\n\nIn their recruitment process, C4ML encourages a diversity of background and thought. While Ph.D. computer scientists are actively sought, the team approaches recruitment with no preconceptions. One of the most unique individuals to be hired was a material scientist. This hire spent his career studying the crystalline properties of steel. The unique perspective that he brought to the team was a high level of rigor, which Hanif sees at critical to success in the field. Different scientific disciplines will have different cultures; in material science, getting an experiment wrong can mean millions of dollars lost.\n\nIt would have been impossible to build up the team if co-location was a strict requirement. Today\u2019s companies need location flexibility to attract the best talent. To balance this, C4ML has teams in a few core offices across multiple geographic locations. Members of a project are not distributed, but rather co-located. Specifically, project members are co-located with their internal customers, which are internal lines of business such as credit cards, anti-money laundering (AML) and fraud. The C4ML team members interact with domain expert business and data analysts across each of their internal customer teams. This makes sense in that it is unrealistic for machine learning practitioners to absorb 20 years of human expertise.\n\nEach customer\u2019s internal organization also has technical talent, including data engineers and data modelers. The dual track skillsets of a C4ML member allows them to speak the same language as the data engineering or the data modelers, and for communication to be more efficient without the need for translation between collaborators.\n\nModel governance is essential as well as timely, accurate, and unbiased peer reviews.\n\nC4ML has highly-specific infrastructure and tools in place to manage human-generated processes at every step. The team built an internal tool that ensures repeatability of experiments; although hyper-parameter is used to identify candidate ML models, they are logged by the tool to ensure repeatability. The team tracks every experiment, and code is versioned in GIT while data is tracked in a custom-built system. This system enables back-testing and tracks data lineage, and is based on an append-only store. Information of how data was selected is also recorded, with the goal of ensuring that tracking and logging is made as transparent and easy as possible.\n\nThe system also includes automated statistical testing to monitor model drift over time. The world continues to evolve even after a model is created, meaning that the models can lose their predictive power. This is specifically important in areas like fraud mitigation, where there is a constant arms race between detection and fraudsters seeking to game the system.\n\nC4ML contributes to many open source projects, including Spark, Kafka and Nifi to name a few (Capital One has their own Github page: https://github.com/capitalone). The C4ML team is also spending time on various research tracks and on writing academic papers that will be submitted to conferences throughout the year.\n\nMy main interest is in deep learning developments, so I had to inquire as to how much deep learning is done at C4ML. According to Hanif, there are multiple projects that use a collection of ConvNet, LSTM, custom neural embeddings, reinforcement learning, and graph convolutional networks. The kind of data that are processed span images and time series. With regards to NLP, more traditional methods are being used.\n\nIn summary, I\u2019m glad to have gotten a bit of an inside look at how Capital One is approaching technology, especially by proactively and aggressively exploring advanced developments in machine learning. To check out more about their work and research in the space, you can head to Capital One\u2019s Data Intelligence Conference this June, where Zach and team will bring together the broader machine learning community to discuss the latest advancements and research in the field."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-knowing-how-software-is-created-is-essential-to-the-modern-company-ad68b7b35098?source=---------9",
        "title": "Why Knowing How Software is Created is Essential to the Modern Company",
        "text": "This is a continuation of an earlier post \u201cHow AI Strips us of our Humanity\u201d.\n\nWe also need to recognize that humans and their tools are inextricable. The vast majority of activities we love doing involve tools. We are distinct in our ability to invent and make use of tools. The wayfinding example in the article above still uses tools in the from of the sail boat. Moreover, cognitive tools perform the thinking for us. Whether it\u2019s GPS, Google, Autosuggest or spell-checking, each time we use these tools is a time we deprive our minds from exercising the replaced skill (i.e. navigation, memory, writing, spelling). We can extrapolate this loss as more advanced AI come on-line. The convenience of AI is a double edged sword, on one hand it allows us to do more and on the other hand it allows us to use less of our mind. This is the curse of convenience that slowly erodes our cognitive capabilities.\n\nThis does not rule out the possibility that we can strike a good balance between the use of our mind and cognitive technology. Software developers do this very well by off loading a lot of cognitive work to machines. The practice of Devops is an advanced example of this. The richest and most advanced form of work and process can be found in software development. It is in this field where you find the frontier of human and machine collaboration. This is why the modern company cannot afford to have executives who have zero experience in software development. Companies of the future are going to be driven by AI and AI is essentially software. Expertise in managing software is essential to the modern company.\n\nTherefore, I strongly disagree with the argument that non-technical people do not have to learn how to program. They not only need to know how to program, they also need to understand more advanced software engineering methodologies. You cannot know how to collaborate with AI tools without knowing anything about how software is composed together to create solutions.\n\nYes, programming is hard. Yes, software engineering is even harder. I am not saying that you need to master these crafts. I am saying that you must have some experience being involved in software creation. Do yourself a favor and take some time of attend a bootcamp and a hackathon. If nobody in your company has any familiarity of how software is created, then your company is basically in the middle of a river without a paddle.\n\nThe companies of the future will be highly automated. Just because they are automated does not mean that they do not change or do not need to be fixed. The business environment always changes, therefore automation must always change. How does one manage a change in automation without ever having any experience working in software development? What mental models will one rely on to think of what needs to be done and should not or cannot be done? A person without the software experience is simply blind to the numerous decision points that need to be made.\n\nSteve Jobs grew up with an understanding of how complex software systems were built. Steve Jobs was an artist though, but a very different kind of artist. His art was in the complex products he led to design. Jobs was a very difficult person to work with, however it is not out of the norm to find artists that are very difficult to work with. Jobs had a very good understanding of humanity. He spent time in India seeking spirituality. He was guided in the design of the Mac from his time spent studying calligraphy. These two are experiences that are non-standard to the Westerner. Jobs\u2019 true genius however was in discovering what was meaningful for humans and tapping into that void in the from of technological products. Here\u2019s Jobs\u2019 speech about the \u201cThink Different\u201d Apple marketing campaign:\n\nSteve Jobs talks about a time when Apple fell into the trap of focusing too much on technological features of their product. Jobs is saying that to be a successful product, one needs to tap into the human needs of the customer. Jobs says that it is not about making boxes to \u201callow people to do their jobs well\u201d. Rather he says that Apple\u2019s core value is that \u201cpeople with passion can change the world for the better\u201d. Reminds me exactly of Ikigai (\u751f\u304d\u7532\u6590):\n\nThis is an idea that the Japanese had long before Steve Jobs came along. This is something that never changes despite the rapid change in society and technology. It is a core principle.\n\nTo do well to market your product, focus on identifying a person\u2019s reason for being. For anything to be successful(AI included) it must address a person\u2019s reason for being. This requires understanding of both how complex software is created and requires understanding of what drives humanity.\n\nI leave you with this explanation from Elon Musk as why he launched his Tesla roadster into space:"
    },
    {
        "url": "https://medium.com/intuitionmachine/ais-symbol-grounding-problem-and-the-future-of-work-7ba7d2db29ec",
        "title": "AI, Blockchain and the Decentralization of Work \u2013 Intuition Machine \u2013",
        "text": "I wrote previously about the importance of work and a person\u2019s reason for being:\n\nHere\u2019s the kicker though, there are plenty of work that are simultaneously something people love to do, are good at and the world needs. It is hard to find a shortage of these kinds of work. People love to work on a mission they are passionate about. In the Venn diagram above, it is easy to find the state where \u201cthere is delight, fullness but no wealth.\u201d\n\nHere\u2019s the problem, as I wrote in my previous post about the dangers of Artificial Personhood, companies are all very keen to exploit the opportunities to access a labor pool where it costs them next to nothing. A recent Wired article highlights the problem in companies that exploit hackathons:\n\nMany find hackathons as an enjoyable and meaningful activity. If not, then you wouldn\u2019t find people participating in them. The outcomes of the hackathon tends to not be as important as the learning that is gained through participation. Many have come to the realization that learning is best done by doing and hackathons are events were you force yourself to do something. Hackathons are events where the journey is valued more than the destination.\n\nCompanies of course can exploit the passions of their workers to gain greater innovation and productivity. This is not a new thing. One of the largest exploiters of this idea are video game companies. Game developers are likely one of the most passionate workers, however it is not uncommon to find horror stories where game developers tally the amount of times they\u2019ve worked with their paycheck and discover a minimum-wage-equivalent salary.\n\nThe modern economic reality is that \u201cThe jobs that are meaningful are the ones that pay the least.\u201d David Graeber writes:\n\nThis is how an economy that is based on scarcity works. It does not matter how meaningful your work is, you only get lavishly compensate if your work is recognized as being scarce. Actually, more specifically, it is a measure between the asymmetry of information between a buyer and a seller. A buyer who believes the goods or service is important and a seller who knows the cost of delivering said goods or service. One makes a killing when there is a big imbalance between demand and supply (where you hold a monopoly in).\n\nThe future of work is being jump started by the Uberization of work. Uber and AirBnb are business models that have discovered that there is an excess of resources that can be exploited to compete against incumbent industries (i.e. Taxis and Hotels). What Uber has discovered is that there\u2019s an excess of workers that prefer the freedom of intermittent employment over the shackles of full-time employment. Most people hate their full time jobs and if given a chance to have more freedom with their time, they\u2019ll take that freedom to pursue their true passions.\n\nThe problem however with marketplaces like Uber is that pricing become extremely efficient and therefore the person performing a task doesn\u2019t have much of a premium. In efficient markets, there is less information asymmetry and therefore the provider makes less. Buyers are able to use the wisdom of the crowds to estimate a better price. Sellers don\u2019t have monopolies and therefore have to lower their prices to attract business.\n\nThe evolution of Uberization is that tasks will begin to become micro-tasks. If you go to Fiverr you will find simple tasks. What AI practitioners will do is identify the smallest task that a human can do leave everything else to the AI. That task becomes smaller and smaller because the AI learns from the work of the human. Eventually, that work done by a human ceases to exist.\n\nThe gig economy is a transitional situation until all these jobs go extinct. Uber will eventual replace all drivers with self-driving cars. Most online jobs such as call centers will be replaced by A.I. assistants. How many of us still use human travel agents to book our vacations? How many use human bank tellers to withdraw cash? How many of us no longer visit brick and mortar stores to buy our goods?\n\nIn a gig economy, every individual will require life long learning. Unfortunately no company (or government) cares to foot the bill for your own education. Closer to reality, one would be lucky to even find the time to get an education. When you can allocate time, degree granting institutions will charge you an arm and a leg for you to get the certification to have the right to do a job. Indentured servitude is the most likely path of many people accepting educational loans to pursue their passions.\n\nWe have digital automation for decades now. The reason they have not taken away all jobs is because the real world is messy and it requires human intuition to navigate. However, Deep Learning is the discovery of artificial intuition. The human being is now between a rock and a hard place. On one end are advanced rational cognitive systems (expert systems of old) and on the other end is an intuition system that learns from experience. Dual process theory states that we have two cognitive systems, an intuitive and a rational system, that work in coordination. What happens when computers are better in both kinds of cognitive processing?\n\nElon Musk predicts it\u2019ll take 7 years to achieve Artificial General Intelligence. Based on my expertise, I also think his estimate is plausible. In more clearer terms, anyone younger than 9 years old today, may likely not have a job when they get to 18.\n\nIn the future, there will be no real work that humans need to do. However if it is work that defines our reason for being, then we must invent new kinds of work. I will call this new kind of work \u2018pretend jobs\u2019. They aren\u2019t by themselves useful, however society will invent systems that convince us that they are indeed useful and valuable. After all, it isn\u2019t the outcome that is important, but the journey.\n\nBitcoin mining is an early incarnation of this kind of a \u2018pretend job\u2019. We pretend that we are mining something that is valuable by having machines compete in a race of solving cryptographic puzzles. It is work that requires a considerable expenditure in hardware, energy and maintenance. This pretend activity is what gives Bitcoin its value. Unlike fiat currency that can be willed into existence by decree, Bitcoin requires someone to make the effort to acquire. The design of Bitcoin is such that it incentivises their miners in exchange for securing the Bitcoin network.\n\nNewer kinds of cryptocurrencies will have different kinds of incentive mechanisms. The best kind of incentive mechanism is the kind that aligns the human to perform meaningful work with the goals of the cryptocurrency (see: Intuition Fabric). You want participants of a cryptocurrency to have \u201cskin in the game\u201d and best way to get that is not through a monetary investment, but one where true effort is sacrificed. In short, you want humans to experience that sacrifice of participation.\n\nAll understanding of language is based on what is known as the symbol grounding problem. Words have no meaning unless it is experienced. In the same way, cryptocurrencies have no value unless there is an experience of sacrifice to acquire them. This is why Bitcoin and Ethereum have worked so well.\n\nReal jobs will not exist in the future. This is why the future of work will be driven by \u2018pretend jobs\u2019."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-ai-for-cx-and-xla-3d2d0faa7d0a",
        "title": "Deep Learning AI for CX and XLA \u2013 Intuition Machine \u2013",
        "text": "You might be wondering, what do CX and XLA mean? CX stands for Customer eXperience and XLA is what is known as eXperience Level Agreement. CX is a derivation of UX (User eXperience) and XLA is a derivation of SLA (Service Level Agreement). I bring these two terms into focus because the successful AI systems of the future will be those that provide superior CX and employ outsourced capabilities that will be measured by XLA. UX and SLA are still useful terms, however both don\u2019t have enough nuances to address the more important business driven reasons for deploying advanced AI.\n\nA survey by Forrester that looked into the question of process improvement showed the importance of CX improvement:\n\nThe Forrester survey claims, \u201cToday it is customer experience, with enterprises expecting to put top priority on digital automation two years hence.\u201d\n\nLisa Joyce in \u201cDigital CX Now The Ultimate Factor In The Fate of Banking Brands\u201d writes about the importance of the need for better CX for digital natives. The top three hot buttons that technology savvy customers expect from their financial institution are:\n\nI do my banking with a very large banking institution. Honestly, it has always been pathetic and continues to be pathetic despite all the improvements we have in UX design. The real problem is, to be able to do the above three well, you must have an intelligent interface that can anticipate your needs.\n\nHow do we create superior Customer eXperience (CX) with Deep Learning AI? Uber has recently published information about \u201cImproving Uber Customer Care with NLP & Machine Learning\u201d. Uber reported in a detailed experience report how they have employed ML methods to improve their CX. Uber describes a problem resolution system where their customers can submit and track issues with their service. This is a UI that every customer does not want to do, but has to do. By making this experience as painless as possible, the overall CX of the service is improved.\n\nUber system essentially works by reducing the cognitive load for customer agents by automatically suggesting the top 3 solutions. Uber reports that:\n\nUber\u2019s latest work is in leveraging Deep Learning to further improve on their earlier Machine Learning system:\n\nIn the \u201cDeep Learning AI Playbook\u201d we describe this very methodology of identifying \u2018cognitive load\u2019 within existing processes. These can be classified in four buckets: lack of memory, not enough meaning, information overload and acting fast. Each kind of cognitive load requires a different Deep Learning approach. However, by approaching development in a systematic way, we can discover the areas that have the highest impact to be improved by DL automation. In the Uber case, what is being addressed is the need to act fast in the resolution of a customer problem.\n\nTo remain competitive and to reduce costs, the systems need to leverage external systems rather than the naive strategy of building everything in house. However, how do we ensure the same level of CX when using third party systems? This is where the idea of XLA comes into play. Typical agreements with third party systems covered what is known as a Service Level Agreement (SLA). SLA typically measures availability and responsiveness of a service. However, XLA focuses on the quality of the experience as a measure. These involve interaction measures such as customer satisfaction or the ease of access to services.\n\nAI will likely become the coordinator of the CX and thus it needs to have an appropriate way of ensuring that other services that it employs are of an equivalent experience. This therefore demands an agreement between parties that goes beyond the standard SLA. An XLA is an agreement between parties that explicitly documents the expectations in customer experience that an outsource provider will deliver.\n\nAs we progress further towards the maturity of AI based technologies, we shall see these two buzzwords (CX and XLA) propping up more frequently in the conversation. This all falls into an umbrella I call \u201cConversational Cognition\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-dangers-of-artificial-intelligence-is-unavoidable-due-to-flaws-of-human-civilization-f9c131e65e5e",
        "title": "Artificial Personhood is the Root Cause Why A.I. is Dangerous to Society",
        "text": "When I began writing my book \u201cThe Deep Learning A.I. Playbook\u201d, I had given very little thought about the dangers of Artificial Intelligence (AI). I was fortunate to be able to form a bit of an understanding to write a chapter about Human Compatible AI (A term Stuart Russell uses to frame the problem) as a bookend for my book. I\u2019ve begun a deeper exploration on A.I. and the human condition, this a continuation of a series of posts on this subject.\n\nI have now come to the realization that human compatible AI is a problem that is inextricably intertwined with human civilization. It cannot be solved because present human civilization isn\u2019t structured in a manner that is aligned with the needs of humanity. You cannot achieve human beneficial A.I. without drastically remaking human civilization. It is difficult enough to make the world act in concert against the threat of global warming, it will be exponentially difficult to make the world re-invent itself in the face of A.I. progress.\n\nWe have all been conditioned to believe that civilization has been designed to improve the human condition. Steven Pinker\u2019s book \u201cEnlightenment Now\u201d describes many examples of how life has improved for the majority of the population. Some notable observations are \u201cyou\u2019re much less likely to die on the job than in the past\u201d, \u201ctime spent doing laundry fell from 11.5 hours a week in 1920 to an hour and a half in 2014\u201d and \u201cwar is illegal\u201d. In summary, the quality of life is getting better for everyone. The general understanding is that the advancement of technology and social equality has lead to the betterment of the human condition as a whole.\n\nThere still remains billions of people that have difficulty accessing basic needs like running fresh water. Global inequality hasn\u2019t been solved, yet we are heading towards a new era. The looming problem of advanced cognitive technology is the possible side-effect of the destruction (through obsolescence) of many jobs. Can society fix the problem when there are no jobs for humans to do? I will argue here that it cannot be fixed until we address the inherent structural weaknesses of our current economic and social system.\n\nCharlie Stross in \u201cDude you broke the future\u201d argues that artificial intelligence already exists in the form of the modern corporation. By the late 19th century, governments decreed the right of personhood to corporations. He observes that the legal environment of today is \u201ctailored for the convenience of corporate persons, rather than human persons.\u201d Corporations have been granted \u201cArtificial Personhood\u201d. Corporations are also the \u201cpaper-clip maximizers\u201d of the present:\n\nOur capitalist system is structured to align with the needs of corporations and not the needs of humans. You cannot solve the A.I. problem when the primary owners of A.I. are the corporations. The actions of A.I. will always be aligned with the needs of the corporation and thus towards the pursuit of profits with a disregard for the plight of humans. Don\u2019t be surprised when humans are laid off and out of work after a corporation decides that they have become \u2018redundant\u2019. Stockholders of corporations become ecstatic when cost cutting measures are executed by public companies. There\u2019s is little empathy towards the persons whose skills have become obsolete and is now thrown into the streets to eke out a new kind of livelihood (without any new skills).\n\nScott Alexander \u201cMeditations on Moloch\u201d has a brilliant essay where he discusses the inevitable failure of collective coordination. He illustrates several examples that include the Prisoner\u2019s dilemma. In summary, in every scenario there is a bad actor that defects from cooperation and ruins it for everyone else in the group. Alexander argues that the groups that survive will be the kinds that are most selfish. Groups that have a strategy aligned with the common good are likely to go extinct. He writes that the optimal solution (i.e. \u2018god\u2019s eye view\u2019) is simple enough to understand yet is impossible to implement. Civilization cannot escape this problem and is the root cause of civilization\u2019s unfair wealth distribution.\n\nAlexander identifies four boundary conditions that slows a race to the bottom. He enumerates them as physical limitations, excess resources, utility maximization, and coordination. Alexander is unsure if technology actually provides a solution that can prevent the destruction of human values.\n\nToday we have corporations that are in the driver\u2019s seat, but to make the situation even more insurmountable, each corporation is driven by self preservation and as a consequence the common good will be of lesser priority. A vivid example of a corporation that employed a complex strategy of acquisition and expansion is the US Pharmacy CVS. Through its affiliate company, it had a strategy that bankrupted small private pharmacies so they could swoop in to acquire these at a bargain. Companies are motivated by survival and growth, growing by acquisition is all too common strategy. This strategy is preferred by CEOs simply because it\u2019s much easier to execute than actually having to compete in a free market. Free markets work for the benefit of many if there is sufficient diversity to encourage beneficial competition. Similar to democracy, free markets are inefficient due to high redundancy. However it is this redundancy that makes them anti-fragile (i.e. robust).\n\nThe only solution is that the majority of corporations needs to develop an enlightened self-interest. We see some of this happening in the software space where many companies contribute to common open source development. We also see this in corporations collectively choosing to support greener energy sources or becoming more socially aware. Perhaps our only salvation is that individual CEOs and corporate boards become enlightened enough to avoid our own human destruction.\n\nOur governments are presently beholden to the lobbying of corporations. Many corporations\u2019 products are intrinsically detrimental to society or the environment. History is littered with bad actors that deceptively buried the harmful effects of tobacco, lead gasoline, sugar water, transfat, fracking, offshore drilling, green house gases, assault riffles, etc. We can\u2019t expect a change when profit is the driving objective function. A corporation like any biological entity acts to survive and prosper, these corporation die if they aren\u2019t profitable.\n\nThe newspaper business is an example of a corporation. Many newspapers prioritize the creation of quality journalism. This is conventionally balanced with the need to collect revenue. Many newspapers have gone out of business in the internet economy. It is shameful that Facebook and Google, companies that make most of their income on information dissemination, do very little to subsidize quality journalism. What remains are public broadcasting such as the BBC and NPR that have to be supported by governments. Society needs to support quality journalism and companies who are in the business of distributing information should take on the initiative to contribute to their financial viability. We have today a system that values eye-balls over truth. This is lethal to any democracy. Companies that make money selling eye-balls must be also responsible for promoting quality journalism. George Orwell wrote:\n\nNews cannot be \u2018public relations\u2019 for the goal of seeking profits. Selling eye-balls cannot be \u2018public relations\u2019 for the goal of seeking higher engagement. We all need to take to task companies like Facebook and Google and demand that they pay their fair share of promoting journalism. They cannot continue to exploit their users without contributing back to the welfare of their users.\n\nThe current situation with Facebook and Cambridge Analytica is an example of a corporation that has prioritized growth over ethics. One can argue that Facebook could not have achieved its success if it were not for its loose handling of personal information. If Facebook originally had strict security access controls by default, then it would be likely that it would not have grown so quickly. Facebook\u2019s objective function is user engagement. We have seen the disastrous unintended consequences of this throughout many democracies of the world.\n\nStartups, to survive, need to prioritize many objectives. Growth is obviously of the highest priority. In fact, every technology company that is in the the public stock markets must emphasize growth. Growth here is measured quarter by quarter and not in terms of long-term growth. This short-term mindset has had a disastrous effect on the working class. Unfortunately, A.I. progress today is driven by these same corporations (and not government research). A.I. development will continue to focus on human behavior prediction and potentially human behavior manipulation. All of the biggest internet companies have revenue models that are driven by human behavior prediction. This sets the stage for society\u2019s biggest problem.\n\nThere are many kinds of A.I. that can be developed. We can classify these into at least three kinds: computational, autonomous and social.\n\nThe latter kind has the highest priority among the biggest internet companies. Social A.I. is the most dangerous kind of A.I., it is the kind that uses its own objectives to drive the human individual into alignment with itself. It does not align with the inherent needs of the individual since its objective is of its own profit (and not the individual). One can argue that a company that delivers a product is in alignment with the needs of the customer. The subtle reality is that a company\u2019s products are designed to convince customers that they are in alignment with their needs. The best kind of marketing is one that convinces people to buy stuff that they don\u2019t need. In fact the modern economy is driven mostly by consumption of stuff that people don\u2019t need. Yuval Noah Harari writes in \u201cSapiens\u201d:\n\nHumans are driven by corporations to consume the unnecessary so that its repayment becomes necessary.\n\nThe proponents of bitcoin and other cryptocurrencies have created their own rebellion against the present monetary system. They see corruption as being rooted in human behavior and promote this idea: to re-establish fairness, automation should rule over the governance of the issuance of money. I do agree that corruption arises from human behavior. Corporations act in the way that they do because it is lead by corrupt leaders. However, machine governance is not the proper solution in that corruption is still introduced into the system. Just witness the corrupt behavior of Bitcoin miners who own a monopoly of the issuance of Bitcoin.\n\nTo take even a step further, the country of Malta is exploring the proposal to grant legal personalities to blockchain-based applications (i.e. Distributed Anonymous Organizations):\n\nThere is a profound synergy with A.I. and blockchain technology that needs to explore. However, similar to legal personhood of the past, we are creating here an automation that we simply may have no control of in the future. Corporations are already subject to multiple laws and regulations. However this has not prevented them from acting in bad faith. The laws have to bake into them \u2018artificial altruism\u2019. Free market controls are insufficient in ensuring that A.I. align with human needs.\n\nIt is indeed interesting that Isaac Asimov\u2019s Three Laws of Robotics where inspired by the vows of marriage. For a refresher, here are the vows of marriage:\n\nWell, ask yourself, should this promise be made by you to an artificial person or should it be the other way around? This is precisely the problem, with corporations, it is the other way around. The needs of the one are secondary to the needs of the corporation (the many). Let that sink in as to how your natural instinct for the good of the community is being hacked to comply with the goals of artificial persons.\n\nA truer measure of human freedom is the extent of one\u2019s control over one\u2019s own time. Yet in the modern economy, the gainfully and well employed have found themselves in control of less of their own time. Time poverty is at an all time high. Artificial Intelligence in the form of corporations and future forms of automation are in fact the cages that society has invented to enslave itself.\n\nTo dodge the dangers of A.I. we have to address the existing structural flaws of the society that we have invented. This begins by recognizing that artificial personhood in the form of corporation today and artificial intelligence driven corporations of tomorrow is the root cause for the misalignment of society\u2019s goals with that of human goals.\n\nFour aspects of corporate law make the human race vulnerable to the threat of algorithmic entities. (1) Algorithms can lawfully have exclusive control of a large majority of the entity forms in most countries. (2) Entities can change regulatory regimes quickly and easily through migration. (3) Governments lack the ability to determine who controls the entities cannot determine which have non-human controllers. (4) Corporate charter competition, combined with ease of entity migration, makes it virtually impossible for any government to regulate algorithmic control of entities."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-us-military-is-dangerously-behind-with-deep-learning-2d929929e595",
        "title": "The US Military Needs to Urgently Rethink its Deep Learning Strategy",
        "text": "A public report by Harvard reveals how unprepared the US Military is when it comes to the Artificial Intelligence (AI) technology known as Deep Learning. The study by Harvard\u2019s Kennedy center was published in July 2017, written by Greg Allen Taniel Chan, and was conducted with funding from IARPA. The research is titled \u201cArtificial Intelligence and National Security\u201d. I\u2019ve written about the many tribes of AI and about the use of the term AI being too ambiguous and meaning too many things to too many people. Where do we find Deep Learning in this report from Harvard? It is hard to see, it\u2019s in a small footnote buried in page 63. Very easy for the casual reader to miss:\n\nIn more recent news, Google (same guys who aren\u2019t supposed to do evil) are helping UAVs analyze video images using Deep Learning. The project discussed is known as Project Maven that was proposed in April 2017:\n\nInformation about project Maven was made public by the DoD on July 2017.\n\nSo if you follow the paper trail, only \u2018tentative\u2019 steps were spent in exploring AI technologies that included deep learning. This was a little less than one year ago. Interestingly enough, as reported by The Intercept, Eric Schmidt (who I quote in my previous article) was involved in facilitating a collaboration with Google:\n\nGreg Allen, the author of the earlier Harvard report added his insider perspective:\n\nLet me reemphasize what he just said. Nobody had a clue when the April 2017 report was disseminated department wide. Nobody had a clue less than a year ago. The use of \u201cnobody\u201d is actually not correct, I know of some people in the department who had a clue.\n\nSo if you parse the public information that is out there, the US military work with Deep Learning appears to be confined only to a single project. To be perfectly fair, DARPA had funded projects in 2017 to explore \u201cContext Adaptation\u201d, \u201cExplainability\u201d and \u201cBiologically Inspired Architecture\u201d that is addressing state of the art ideas. DARPA perhaps has the few folks that did have a clue. I\u2019ve also given a talk to researchers inside the department that have already been doing Deep Learning since 2016. However, there\u2019s a massive gap between what happens in research and what is ready for deployment. There\u2019s is a difference between a few folks studying the subject and that of a real urgent initiative dive into Deep Learning.\n\nThere is ample evidence that \u201cDeep Learning\u201d isn\u2019t taken serious enough by the US military. Here is a job that was posted just 3 days ago by the US Army. The US Army is seeking a Ph.D. in Deep Learning with a whopping salary of $52,000. I guess there\u2019s a misprint here in that adding an extra digit may be a more of a credible offer. In all fairness, Deep Learning is an exponentially fast moving field and I can\u2019t expect every department to be coming up to speed with the developments. But seriously now, where is the urgency?\n\nAn F-35 fighter jet can cost at least $300 million. To put that in perspective, Google acquired DeepMind for $500 million. The F-35 from the lens of current progress in Deep Learning may become as antiquated a military strategy as the use of horses before WW II. In the early days of the US involvement in WW II, the army were still using horses in the battlefield. The F-35 is likely expensive because of its sheer complexity. However, a swarm of 10,000 drones costing less that 1/300th the cost of an F-35 can be equally effective as a weapon.\n\nThe F-35 fiasco is evidence of a much larger problem in the development of more advanced weaponry. A recent investigation by the Wall Street Journal reveals this inability of military manufacturers to manage bigger projects that may perhaps be heart of the problem:\n\nThis is the tip of the iceberg. The bigger problem is in the software. Large defense oriented companies have never learned how to build software in the agile manner that more nimble companies like Google and Apple. This is hinted at in the same WSJ article:\n\nSo at an even basic level, the military industrial complex does not have the software skills to build complex systems that are robust and bug free. This is of course compounded with the newer reality that \u201cSoftware 2.0\u201d in the form of Deep Learning enabled system is emerging. The industrial complex is now two generations behind in understanding how complex software driven systems should be built.\n\nThere is a stark difference between how a nimble company like Space X is able to deliver a revolutionary space delivery system as compared to the capabilities of its more traditional competitors. This is the problem with the current industrial complex, the skills are simply missing and no amount of money thrown at the problem can fix it.\n\nNot only are we designing weapons using obsolete skillsets, we have the problem of preparing for future wars using technology from a bygone era. We cannot continue to spend $300 million on unbounded complexity that will be technologically obsolete on the first day it is deployed against AI inspired weaponry. What is needed is not piecemeal \u2018tentative\u2019 explorations of the capabilities of Deep Learning. Rather, what needs to be done is an urgent rethink in military strategy and tactics. There is a vast difference in capability when AI is used to invent weaponry as compared to humans.\n\nThe last thought is just plain scary.\n\nOn further contemplation, there are grass roots work in Deep Learning however it takes excruciating time for understanding to diffuse up the leadership and then back down to the rank and file. The US military is very hierarchical and therefore information dissemination is also restricted in inefficient and artificial ways. If leaders are too busy putting out fires on yesterday\u2019s technologies and strategies, then they simply have no bandwidth to address more advanced ways of doing things. Compounding the problem, experts have a harder time absorbing new concepts that are contrary to their expert understanding. Unfortunately, nobody has the luxury to ignore the developments in Deep Learning. It is happening too fast to wait for large organizations to finally \u201cget the memo\u201d. In addition, there is an issue with American culture that fears Terminator style technology."
    },
    {
        "url": "https://medium.com/intuitionmachine/is-it-time-to-panic-about-american-ignorance-of-deep-learning-b8a7bd1641c9",
        "title": "Is it Time to Panic about American Ignorance of Deep Learning?",
        "text": "Eric Schmidt in a recent Verge report has a few remarks about the current state of AI policy in the United States:\n\nI\u2019ve remarked previously (see: \u201cSputnik event of AI\u201d) on what seems to be a lack of urgency in America with regards to the exponential progress in Deep Learning. Asian counties, ever since 2015, have the pedal to the metal in funding AI. Jeffrey Ding a student of Oxford\u2019s Future of Humanity Institute has written a recent analysis of the Chinese government\u2019s plans in AI. The Chinese government have an ambitious goal of dominating AI by the 2020s.\n\nIf one thinks that this is just the normal market and technology competition, then you are gravely mistaken. If American policy doesn\u2019t correct itself soon, we will be what Cuban calls \u201cSOL\u201d:\n\nChina is already deploying its \u2018social credit\u2019 system to control is citizenry. This is a system that monitors a citizen\u2019s social behavior using multiple data streams and assigns a \u2018social credit\u2019 that influences what you may or may not be able to access (i.e. travel, financial products, career etc):\n\nTo gain a bit of an understanding of the state of AI in China, here is Kai-Fu Lee providing his own overview:\n\nWhere he says that the U.S.-China duopoly in A.I. has already arrived! It\u2019s not some future scenario, it is here now.\n\nThere are plenty of indicators that China is deploying significant resources in this space. A recent report covers a $2.1 billion investment to build an \u201cAI District\u201d:\n\nYou can find more billion dollar initiatives coming out of China. Yet, when we ask what\u2019s happening in the USA, you can barely find $100,000 SBIR funding involving Deep Learning.\n\nJust to emphasize the point even further, China\u2019s president Xi Jinping appears to show an advanced command of the subject. This is revealed by the analysis of his bookshelf in a recent address to his nation. The Shanghaiist.com writes about this in \u201cWhat\u2019s new on Xi Jinping\u2019s bookshelf this year\u201d:\n\nWhat\u2019s displayed on his bookshelf of course could just be all for show. However, is it not better to display that you know something rather than the alternative that you are completely clueless and proud of it? China is not subtle about their intentions in the AI arena. Pretty soon they will be flexing their muscles displaying their utter dominance. They are telegraphing their moves and it appears that there is nothing that America is doing in response.\n\nI usually use Google Trends as a tool to discover what people are interested in this world. What is alarming is the relative interest in countries searching for the term \u201cDeep Learning\u201d:\n\nThe United States is 30 times less interested in the term \u201cDeep Learning\u201d as compared to China. The top five countries are all in Asia (if you consider Israel to also be part of Asia). Three of the countries in the top five are countries (or cities) with populations less than 8 million people. That is 40 times smaller than the United States but with more than twice the interest. Despite the exponential and mind blogging progress in the area of Deep Learning, there is simply little interest among the American population.\n\nCould this be perhaps because Americans have been fed a steady diet of Terminator and the Matrix scenarios by Hollywood? Is AI just one of those \u2018inconvenient truths\u2019 that we want to wish away as something to not worry about in the imminent future? Could it be the pervasive mind-set of the American citizenry thinking that the government is an unnecessary waste contributing to this apathy?\n\nScandinavians\u2019 in stark contrast have the perception of A.I. as being something that will contribute to their greater well being. Is this due to the firm belief in these countries that the government is aligned with their own personal needs and beliefs?\n\nThe Japanese also perceive A.I. entirely different from Americans. The Japanese have always been enamoured with robots. It\u2019s not just prevalent in the factories, but also in their entertainment. Japanese equivalent of super heroes are essentially robots or androids. It all began with Astro Boy in the 1950s and ever since then, the Japanese have been treated with a steady diet of Robots in their entertainment. The Transformers and Pacific Rim are two examples of this Robot meme that has invaded American theaters. The super heroes of the Japanese are not humans that gained mutant powers, they are humans that are in symbiosis with robotic skeletons.\n\nContrast the difference. \u201cGhost in a Shell\u201d, a Japanese story where a human is embedded within a robot skeleton. This is a human that has been made to believe that she is a robot. The human becomes super-human by becoming a robot. Compare that with \u201cBlade Runner 2049\u201d, a biological robot that is trying to determine if it has the same intrinsic characteristics as a true human. The American psyche doesn\u2019t appreciate the human that fuses with AI capabilities. AIs are distinct sentient forms that are always positioned as a threat to humanity.\n\nIt indeed is interesting that the Japanese relationship with AI and Robotics is at an emotional level. In a Time article \u201cHow Japan\u2019s Radically Different Approach to AI Could Lead to Wild New Tech\u201d writes:\n\nThere is a significant disconnect among the American population of the benefits of Artificial Intelligence (and Deep Learning as its primary driver). This disconnect appears to me rooted in American culture and American individualism. There\u2019s a disconnect in many American\u2019s notion of the purpose of government. Government is looked at as a unnecessary bureaucracy and a instrument of taxation. From this view, the idea of creating AI as a common good (similarly as how environment is a common good) is a foreign idea. Furthermore, the relationship of Americans with the technology is about a master-slave relationship. Therefore any sufficiently intelligent automation is viewed as being something that flips that relationship around.\n\nIn summary, there are deep rooted culture issues that Americans have in relationship with Artificial Intelligence. These cultural biases may have a profound effect of future AI competitiveness. The Chinese, Scandinavians and the Japanese don\u2019t seem to be carrying the same excess baggage. These countries look at A.I. as a means to become better or create a better society. Meanwhile in the U.S.A. it is a zero-sum mindset that any investment in AI takes more jobs away. This is the same mindset that investments in renewal energy will take away the job of the coal miner.\n\nOne has eventually has to realize that change is all but inevitable. Intelligence, according to the late Stephen Hawking, is measure in our ability to adapt to change. The AI driven economy is here today and it\u2019s best that we prepare ourselves to adapt to this new world by becoming less ignorant. It is now time to truly begin to panic and become urgently become more informed:\n\nUpdate: The French are beginning to panic: https://www.aiforhumanity.fr. Good for them! https://www.reuters.com/article/us-france-tech/france-to-spend-1-8-billion-on-ai-to-compete-with-u-s-china-idUSKBN1H51XP"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-intuition-machine-letter-1st-edition-e4516e50019f",
        "title": "The Intuition Machine Letter \u2014 1st Edition \u2013 Intuition Machine \u2013",
        "text": "Welcome to the bi-weekly letter covering Deep Learning Patterns, Methodology and Strategy. We\u2019ve come up away to organize the topics to appeal to the broadest of audiences. The more general topics always are at the top, while more specialized ones are towards the bottom. We hope that this newsletter we appeal to all those interested in Deep Learning developments.\n\nFor this issue, we revisit the Metalearning symposium, introduce courses and tools for AI, look at various ethical viewpoints with regards to the development of AI, see how advances in AI parallel advances in Neurosciences, and explore recent researches and developments on AI.\n\nSteve Jobs has a famous quote that he used to convince the then CEO of Pepsi to join Apple: \u201cDo you want to sell sugar water for the rest of your life, or do you want to come with me and change the\u2026\n\nSeveral approaches to metalearning have emerged, including those based on Bayesian optimization, gradient descent, reinforcement learning, and evolutionary computation. The symposium presents an overview of these approaches, given by the researchers who developed them.\n\nThe new open ecosystem for interchangeable AI models\n\nPandas on Ray accelerates Pandas queries by 4x on an 8-core machine, only requiring users to change a single line of code in their notebooks.\n\nGitHub is where people build software. More than 28 million people use GitHub to discover, fork, and contribute to over 80 million projects.\n\nMy notes from the excellent Coursera specialization by Andrew Ng\n\nAlexandria integrates machine reading and reasoning, natural language understanding, computer vision, and crowdsourcing techniques to create a new extensive, foundational common sense knowledge source for future AI systems to build upon.\n\nUsing Google Clips to understand how a human-centered design process elevates artificial intelligence\n\nWritten in Haskell, Queryparser is Uber Engineering\u2019s open source tool for parsing and analyzing SQL queries that makes it easy to identify foreign-key relationships in large data warehouses.\n\nDon\u2019t worry about supersmart AI eliminating all the jobs. That\u2019s just a distraction from the problems even relatively dumb computers are causing.\n\nThe Center for Humane Technology is a world-class team of former tech \n\ninsiders and CEOs who are advancing thoughtful solutions to change the \n\nculture, business incentives, design techniques, and organizational \n\nstructures driving how technology hijacks our brains.\n\nA robotics researcher afraid of robots, Peter Haas, invites us into his world of understand where the threats of robots and artificial intelligence lie. Befo\u2026\n\nWhen? This is probably the question futurists, AI experts, and even people with a keen interest in technology dread most. It\u2019s been famously difficult to predict when developments\u2026\n\nIf we can\u2019t understand our own brains, maybe the machines can do it for us.\n\nThe gaming company\u2019s Commit Assistant AI tool has been trained to spot when programmers are about to make a mistake\n\n(left:) Test image displayed on computer monitor. (right:) Image captured by EEG and decoded. (credit: Dan Nemrodov et al./eNeuro) A new technique developed\n\nNot only is the technique an advancement of holographic technology, but also, the holograms could have fascinating (and practical) medical applications.\n\nLow-precision computation has been gaining a lot of traction in machine learning.\n\nNeuroevolution is a form of artificial intelligence that uses evolutionary algorithms to generate artificial neural networks, parameters, topology and rules.\n\nI challenge you to find a field as interesting and exciting as Deep Learning.\n\n\u201cWhatever you are studying right now if you are not getting up to speed on deep learning, neural networks, etc., you lose,\u201d says Mark Cuban. \n\n\u201cWe are going through the process where software will automate software, automation will automate automation.\u201d"
    },
    {
        "url": "https://medium.com/intuitionmachine/how-automation-and-artificial-intelligence-strips-us-of-our-humanity-352a9b92b873",
        "title": "How Automation and Artificial Intelligence Strips us of our Humanity",
        "text": "In my most recent post, I offered the idea that the purpose of Artificial Intelligence should be to support this idea of Ikigai (\u751f\u304d\u7532\u6590): A reason for being.\n\nMelody Wilding wrote a Medium article that may serve as a good introduction to the topic: \u201cThe Japanese Concept \u2018Ikigai\u2019 is a Formula for Happiness and Meaning\u201d. Wilding makes an astute observation that Ikigai \u201cinvolves a commitment of time and belief, perhaps to a particular cause, skill, trade, or group of people.\u201d If you examine Ikigai closely, you will realize that it revolves around the central idea of the importance of work. The goal of Automation and AI, in contrast, is precisely the elimination of work.\n\nThis idea also translates to worker productivity and innovation. Daniel Pink who wrote the bestseller \u201cDrive\u201d says it simply enough:\n\nHowever AI is framed from a more Utopian idea focused on the elimination of inconvenient work: the kind of work that is supposedly unnecessary in our pursuit of more lofty goals. AI is like American fast food drive-thru on steroids.\n\nTim Wu has written one of the best essays on the idea of the importance of inconvenience in defining our humanity. He writes about the \u201cTyranny of Convenience\u201d. I highly recommend inconveniently spending the time to read his highly insightful essay.\n\nHis first interesting thought (which is a bit of a digression) is that \u201cconvenience and monopoly seem to be natural bedfellows\u201d. Our need for convenience leads to more convenience that is driven by network effects and stickiness in the form of human habit. I\u2019ve written about this in \u201cIntuition Fabric\u201d where I argued for the importance of developing decentralized AI. The present monopoly of AI is detrimental to our own privacy and to the functioning of democratic societies.\n\nTim Berners Lee (Inventor of the WWW) has written about the dangers of the centralized web. He writes:\n\nThe centralized web grows because it is primarily driven by our need for convenience. It is too easy for us to give away our privacy in exchange for automation that will automate our daily news feed. Curating content takes a ton of effort so why not use automation and AI to handle this chore? What could possibly go wrong?\n\nTim Wu writes that the \u2018first convenience revolution\u2019 promised to make life and work easier for you. This, at first glance, doesn\u2019t appear to be a bad thing. However, by the 1960\u2019s it was recognized that this convenience was a form of conformity. Tim Wu writes:\n\nThere is now a \u2018second convenience revolution\u2019 where its unrelenting drive for convenience leads to an alarming conclusion: What if we could make it convenient to express our own individuality? The second convenience revolution promises to make it easier for you to be you. Said differently, automation and AI that makes it easier for you to express your own identity becomes the tool that replaces your own identity. Tim Wu writes:\n\nThis is because what we have lost in this \u2018fast food\u2019 culture of ours is the reality that our humanity is defined by the inconvenient obstacles that we must overcome every day. Work is only truly meaningful to us when there is an investment in considerable time and effort. The more effort we spend on a pursuit, the more we can find meaning in it. You can climb to the top of a mountain or you can take a cable car up the mountain. The climb is hands down a more meaningful activity. Tim Wu says it best \u201cconvenience is all destination and no journey\u201d. The journey gives us the experience that defines us, take that journey away and there is little that is left.\n\nThe most meaningful journey is going to be the ones that are steep and difficult. Automation and AI introduces every kind of convenience at scale. As a consequence, ikigai or that \u201creason for being\u201d gets extinguished at scale.\n\nTherefore, to preserve humanity within a reality of exponential progress, we must understand how to balance our activities into focusing on the inconvenient. It is in the inconvenient where one will find true meaning. To ignore these inconveniences is to ignore our own identity. The tasks that AI makes convenient are the also the same tasks that make us human.\n\nIt is our responsibility to recognize the value of these tasks and to ensure that they endure and continue to be available for future generations. A notable example of this preservation is the activity of wayfinding of the people of the Pacific. It is a way to navigate the ocean with as few navigation tools as possible (without sextant, compass, clock, radio reports, or satellites). This is an art that was lost for several generations but is being revived to be available for future generations. In that little corner of the universe that is hopefully reserved for humans, we will spend our lives enjoying human activities like sailing. Not because sailing takes us from point A to point B, but because the activity and journey of sailing is meaningful.\n\nWhy has the open source movement become so valuable? Why do thousands of programmers devote endless hours on a mentally taxing endeavor like programming or writing only to give away their work for free? It is the expression of effort and sacrifice that makes us human. To quote from this TED talk by Susan David:"
    },
    {
        "url": "https://medium.com/intuitionmachine/is-the-purpose-of-ai-to-sell-sugar-water-e6466d574ec0",
        "title": "Is the Purpose of Artificial Intelligence to Sell Sugar Water?",
        "text": "Steve Jobs has a famous quote that he used to convince the then CEO of Pepsi to join Apple: \u201cDo you want to sell sugar water for the rest of your life, or do you want to come with me and change the world?\u201d\n\nJobs revealed a profound truth that exists in many of today\u2019s enterprises. Their main purpose is to drive up the demand for an abundant resource by making it scarce. This is known as the \u201cEconomics of Scarcity\u201d, a business strategy to identify how to create artificial scarcity out of abundance.\n\nOne example of creating artificial scarcity is \u201cplanned obsolescence\u201d. Coincidentally, Apple is a master at this. Do you know that Apple doesn\u2019t sell parts so you can fix a broken device? Do you know that since 2012, Apple has used glue (instead of screws) to assemble their computers to make it impossible to repair? Do you know that Apple deliberately degrades iPhone performance as the battery life degrades?\n\nThe laptop I use today is a 2011 Macbook Pro. It is the last Apple laptop that you can upgrade yourself. I\u2019ve got an i7 with 16GB of RAM and 1TB for SSD, that\u2019s double what brand new laptops have today. It\u2019s more than adequate for most tasks and works comparably to the latest laptops. A ton of RAM truly helps because I\u2019ve got this bad habit of having too many browser tabs open.\n\nBitcoin is another example of artificial scarcity. Bitcoin is designed to limit the number of coins created to 21 million. Bitcoin cannot be printed like national currencies (i.e. USD), it can only be \u2018mined\u2019 (solving a computationally exhaustive cryptographic problem) and thus energy is expended in the process of creation. There\u2019s so much energy spent on Bitcoin mining that over half of Iceland\u2019s energy generation is spent on cryptocurrency mining! Bitcoin mining gets more difficult over time, and every four years the number of coins that can be mined (every 10 minutes) gets cut in half. So if you every wonder why Bitcoin is scarcer (and move expensive) than gold, then the reason is artificial scarcity. It\u2019s just like De Beer\u2019s diamonds or paintings of dead artists. Price goes through the roof when scarcity is created. It doesn\u2019t have to be real, it only needs to be imagined!\n\nSo let\u2019s examine the biggest AI companies in the world, Google and Facebook. How do they make their money? Both companies are primarily in the advertising business. Advertising is in the business of monetizing eyeballs. If you can\u2019t sell content to your customers, you might as well sell your customers to advertisers! Both companies are leading the Deep Learning AI revolution. But for what purpose exactly? To allow advertisers to sell sugar water to their customers? Through advanced AI technologies, they are becoming more sophisticated in predicting your behavior and therefore manipulating your behavior.\n\nI\u2019ve written earlier that there are at least three dimensions of intelligence being developed. These are Computational, Autonomous and Social Intelligences. The first kind can be used for discovering the nature of the universe or drug discovery. The second kind is useful for automation that can act and decide without human intervention. The last kind, the one everyone wants to have because they are money printing machines, are the kind that predicts human behavior.\n\nSo if we can predict human behavior then shouldn\u2019t we now use it for good? A recent research study performed by 25 technical and policy researchers showed how AI can be misused. The research is titled \u201cThe Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation\u201d. The 100 page paper is a treasure chest of ways AI technology can inflict harm. The paper\u2019s framework revolves around identifying AI characteristics that are \u201csecurity-relevant\u201d, specifically:\n\nThe same characteristics that make AI dangerous are the same characteristics that make it valuable for selling sugar water. Basically, we want customers to believe that the sugar water we sell is valuable and that we aren\u2019t held accountable for the consequences of that sale. We convince the world of the ruggedness of owning a SUV or Truck, but don\u2019t want to be accountable for its environmental footprint. We want to convince the world that guns keep us safe but we don\u2019t want to be accountable for the destruction it creates. We want to convince people suffering from pain that opioids are one\u2019s best pain relief but we don\u2019t want to be accountable for the addiction. It is all the same moral bankruptcy that a majority of the population makes a daily living out of. Money is made out of scarcity and it has got nothing to do if that scarcity is beneficial to humans or not.\n\nWe all would like to believe that our work in AI leads to human good. As Jobs has said we all want to \u201cchange the world\u201d for the greater good. But what is that greater good? Is happiness the greater good? If that is so, perhaps we might as well shoot ourselves up with psychedelics and permanently hook up to virtual reality.\n\nHere\u2019s what I propose instead, perhaps we look at what people who have uncharacteristically long lives do. The Blue Zone identifies the lifestyles and environments of the longest lived people. What they have found is that the people of Okinawa, Japan have this concept called Ikigai (\u751f\u304d\u7532\u6590):\n\nThe reason we get ourselves up every morning should be driven by our need for the doing (1) what we love, (2) what we are good at, (3) that the world needs and (4) provides income.\n\nThe big problem here is the last one. Most stuff that we are paid to do, the stuff with scarcity, misses out in any one of the other things. Only the very few are lucky enough to have all four in their life (I guess those are the people with University tenure). A majority of us waste our lives holding \u2018BullShit Jobs\u2019. Perhaps this should be the real goal of AI. To change the world so that we all can have \u201ca reason for being\u201d. To make it happen that we don\u2019t waste our lives selling \u2018sugar water to kids\u2019.\n\nI leave you with this inspirational application of Deep Learning to address world hunger:\n\nSee my other post on the \u201cEconomics of Abundance\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/artificial-intelligence-and-the-economics-of-abundance-92bd1626ee94",
        "title": "How Artificial Intelligence Enables the Economics of Abundance",
        "text": "Today, the global economy is in a perilous situation. Wage growth is stagnant across all industrial economies. Jobs for less skilled workers are in constant decline and this decline is accelerating. Economic inequality continues to increase. Not only is the economy in a perilous situation, the planet itself is in dire condition. Mass extinction is now happening and countries are going to be inundated and wiped out of existence. Many coastlines will become uninhabitable with a ton of wealth in beach-front property going up in smoke. Nation states are dissolving into chaos.\n\nThe root of the problem is that the old economy has reached the end of the line in terms of its growth. The old economy is driven primarily on resource scarcity (i.e. fossil fuel) and we have maximized its exploitation such that Earth\u2019s own ecology is now in jeopardy. There is nothing more to be extracted without real consequences; our civilization\u2019s capabilities are constrained by the laws of physics and our oceans have reached their carbon absorption limit.\n\nIn a fossil fuel-based economy, there are intrinsic limits in how we can produce and transport goods while reducing waste. The goods many industries create are actually subsidized by nature. We create revenue from borrowing what we believe to be free from nature like water, air, trees, fish, etc. Unfortunately, our failure to pay for what we take has made our ways unsustainable. We are producing stuff like crazy and in turn accelerating environmental entropy. We create products nature itself can\u2019t decompose and therefore cannot reuse. We are destroying ourselves for short-term gains while mortgaging our children\u2019s futures.\n\nMeanwhile, there is another kind of economy being created that many incumbents are scared to death of. This is an economy based on exponential technologies. Economic revolutions arise when three kinds of technologies converge. That is, when a new source of energy, a new kind of transport and a new kind of communication all converge in concert to become the basis of a new economic platform. The last economic revolution was driven by oil, automobiles, and electrical communication. The new economy emerging is driven by the convergence of renewable energy, self-driving transport and digital communication.\n\nThe nature of this new economy will be very different. We are transitioning from an economics of scarcity to an economics of abundance. This new kind of world, driven by the exponential progress of silicon, is creating massive disruptions in our economies. The biggest media company in the world (i.e. Facebook) has no journalists or content producers. The biggest hospitality company in the world (i.e. AirBNB) has no rooms. The biggest taxi company in the world (i.e. Uber) has no cars. The biggest bank in the world (i.e. cryptocurrencies) has no buildings (or physical safes). All these are software-driven entities. Software is essentially knowledge and processes that are captured by automation. All of these operate in the virtual world. Artificial Intelligence (AI) is just a more advanced kind of software.\n\nThe greatest immediate problem we are about to face is the transition from our current economy into this new economy. Achieving a world of abundance is an ideal goal, but the process of getting there is going to be very painful. This will not happen overnight and may perhaps take two generations. We are already seeing the ugly consequences of this transition playing out in real-time. Tribalism and protectionism are becoming resurgent. This is a consequence of having many members of the population unable to participate economically in this transition. When resources become scarce, the idea of sharing gets thrown out the window.\n\nThis resentment is also driven by the old economy continuing to become obsolete. It is simply human nature to not comprehend why the world has to change. There are human communities, for example the Amish people, which have the avoidance of technological progress ingrained in their religion. There is some profound wisdom here. If the goal of technological progress is the improvement of human well-being, then we should not always blindly accept the latest technology; instead, we should select the technologies that are best for our human souls. As you will read later, \u201cThe Economics of Abundance\u201d is another kind of philosophy which emphasizes sustainability and fairness.\n\nThe fossil fuel economy is estimated to be worth 100 trillion in value. As that economy moves towards extinction, there will be a massive loss of wealth. A majority of the world\u2019s wealth are accumulated in financial instruments tied to this old economy. Today\u2019s yields are negative and will continue to be negative with no end in sight. This wealth has nowhere to go and that desperation is already being telegraphed with the mind boggling explosion of wealth in the cryptocurrency space. We are going to see big time wealth destruction for many and big time wealth gains for the technically savvy. This is the Revenge of the Nerds sequel.\n\nAI creates an economy of abundance. If we are to truly grasp the consequences of AI, we need to, at a minimum, understand what the economics of abundance is all about. Jeremy Rifkin describes the new economy as the \u201cThird Industrial Revolution\u201d. This is an economy where marginal costs of production (the cost to create an additional product or service) go down to zero. He points out that the millennial generation which grew up in this type of economy have very different sensibilities than previous generations. No longer do they see cars as an enabler of mobility. Access has become more important than ownership. Participation and sharing in networks is power, while isolation is death. Millennials are the ones who will be most severely impacted by this sudden loss of economic wealth. To survive, they are adapting to a new world where the rules of the game are very different. Older generations all too easily dismiss millennials as not being able to comprehend the \u201crules of the game\u201d, however perhaps it\u2019s just that many can\u2019t see the rules are actually changing.\n\nThe psychology of scarcity is very different from the psychology of abundance. Scarcity encourages hoarding, in contrast, abundance encourages sharing. Hoarding chokes innovation, sharing facilitates innovation. Most of the world comes from scarcity and the psychology of abundance manifests itself in wealthier communities.\n\nThe economics of abundance is also very different from a world defined by scarcity. In fact, one can make the generalization that economics in a world defined by scarcity does not exist when there is no scarcity. How does civilization even function in a world without scarcity?\n\nWolfgang Hoeschele has studied extensively the \u2018Economics of Abundance\u2019. His writings gave us a glimpse of how a world with advanced AI will be organized. In \u201cSeven Key Elements of an Economy of Abundance\u201d he explores how the world will operate differently. His definition of the Economic of Abundance is as follows:\n\nHoeschele\u2019s exploration involves the context where there is a means of decentralized production. Zero marginal costs, zero distribution and decentralized production are sources of abundance. However, abundance also creates relative scarcity. This scarcity is analogous to the problem of information overload. For example media (i.e. video, music, literature etc) in a world of abundance would have a scarcity of attention. Similarly, although zero marginal cost creates abundance, it will consequently lead to relative scarcity created elsewhere. This scarcity typically manifests itself in our cognitive limitations of handling a barrage of new information created by abundance.\n\nThis is the reason why AI is a critical component in the equation. You can have millions of vehicles, but it creates a scarcity in the availability of roads (and therefore unbearable traffic). With AI, you can have more efficient use of vehicles and achieve a higher density of vehicles per road. In addition, you would have fewer vehicles because the cars will not be sitting in garages all day waiting for a driver.\n\nJust to be clear, the \u201ceconomics of abundance\u201d refers to a mode of operating production that minimizes waste and encourages decentralization. One way to reduce waste is to become very intelligent about recycling goods. Sharing goods is a form of recycling. However, to share physical goods, one has to be in close proximity to be efficient. Decentralized systems are more efficient than centralized systems in the sense that you can get a closer match between actual needs and resources. However, with more advanced manufacturing and farming (examples: 3D printing and hydroponics) one can circumvent the energy expenditure to ship fresh goods across vast distances.\n\nWhy is decentralization extremely important? This is because people need to feel that they are in control of their own lives. It is what allows people to work on what they love and what they are good at while remaining self-sufficient. They see their contributions improve the lives of the people around them. Life, liberty and the pursuit of happiness (not property).\n\nIn the following paragraphs, I will highlight some of the effects of AI in driving the economics of abundance.\n\nGovernments provide exclusive access for corporations to exploit environmental resources in exchange for compensation. Today this assessment uses a very primitive and crude way of valuing said resources. This problem is further exacerbated through corrupt practices where the people responsible for the stewardship of a resource are paid off. AI can be made available to track resource consumption and compensation for use. Transactions can also be made available in a public ledger and enforced at the time of consumption to mitigate against corruption.\n\nThere needs to have an equal sharing of profits between the coordinators and the providers of goods and services. Each party has their own costs and are required to be appropriately compensated. Today\u2019s corporation are extremely unbalanced. Those who are responsible for coordination are compensated grossly more than those who provide the actual goods and services. AI can provide superior coordination and planning capabilities so that value moves toward the edges and at the point of contact with the consumer.\n\nRenewable energy such as solar power is democratizing the generation of energy. AI will allow the more efficient distribution of excess energy where it can be used more effectively.\n\nBlockchain in combination with advanced AI will lower the cost of financial instruments that are necessary for business. The financial industry (finance, insurance and real estate) accounts for 20% of the US economy. This is a massive percentage which doesn\u2019t produce any goods or services. AI can reduce the cost of managing financial instruments which should reduce the cost of running businesses across the board.\n\nOne major reason for the high inequality in this world is due to the inability of many to have access to capital. At a certain monetary amount, it isn\u2019t worth the bank\u2019s effort to service impoverished clients. However, AI allows services to scale with zero marginal cost. This lowers the cost for access and thus allowing larger participation into the economy.\n\nSmart AI driven mobility will lead to a change in how cities are designed. Cities will be designed to maximize people\u2019s interactions and would less prioritize the movement of people. The value of cities has always been in the richness and the diversity of interactions. Cities are the primary driver of innovation in our current knowledge-based economies. Smarter cities are going to be influenced more by the need to enhance collaboration rather than the need to transport people.\n\nLondon has 20,000 ghost homes despite the high cost of rent and the growing number of homeless people. Clearly there is a problem here where homes are used only to store wealth instead of sheltering people. AI driven by the objective of higher occupancy can put an end to this kind of hoarding behavior.\n\nAccess to learning will be greatly democratized with the availability of scalable AI teaching systems. One\u2019s perceived value to a business will be based less on credentials acquired by virtue of attending an exclusive educational institution, but rather on a real-time assessment of one\u2019s true skills. This more transparent assessment of skill sets may lower one\u2019s own premium on the market as one can\u2019t leverage asymmetric information.\n\nThe need to seek credentialed education in colleges and universities has lead to crushing debt on the millennial generation. These educational debts guarantee a kind of indebted servitude, which is basically a new kind of slavery. The primary reason to pay for exorbitant tuition is to access exclusivity and not necessarily for education. Excellent formal education can be found in many sources online at affordable costs. However, there are many professions that create artificial barriers to entry by demanding expensive access for any aspiring member. The problem is not the cost of education, but rather we deliberately make the cost for credential expensive to discourage competition. Formal education is no different today than the NYC Taxi medallions which were such a great investment prior to Uber and Lyft.\n\nAI leads to greater access to affordable healthcare. AI already provides lower cost and accessible diagnosis. (This won\u2019t roll out in the U.S. because the medical cartel will do what it can to hang on to its monopoly). Healthcare will be more holistic and sophisticated. Instead of providing care only in times of emergencies, lifelong care is provided at a continuous basis to ensure the maintenance of a healthy lifestyle. The emphasis would be in reducing the likelihood of falling ill over the current paradigm of addressing the problem only when the patient is already ill. Healthcare driven by AI will be preventive and proactive rather than how it is today \u2014 reactive, irreversible and expensive. With AI, you will have democratization of healthcare diagnosis allowing health issues to be identified much earlier and therefore remedied at much lower costs.\n\nYou may be wondering right now: What does AI do that makes it so valuable? To answer this, you have to understand what makes anything valuable. A service or a good is always valuable from the perspective of a Job to Be Done (JTBD). What AI is good at (and getting better at) is identifying a person\u2019s unique context and subsequently deliver the best goods or service that enables a person to get their job done. Context + Resource = Value.\n\nIn summary, the notion of monetary compensation will make little sense in a far future AI-driven world. However, it is unlikely in the near term that we can arrive at this new world without the pain of weaning ourselves out of the previous economy of scarcity. There will be a period where few people will own the majority of \u2018wealth\u2019 in the world. And just as we see it today, a world where 4 billion people earn less that $2 a day, we will see even more people without means to support themselves as we make the unavoidable transition.\n\nAI will break capitalism and this should be obvious to anyone. Capitalism only works where there exists a reasonable level of equality. That\u2019s because Capitalism requires both capital and labor. When you don\u2019t have labor (as it is replaced by AI), then there will be nobody to consume the goods produced. Capitalism also requires scarcity, otherwise the price of goods goes to zero. In a world with all but the few own AI, there is an absence of buying power in the population who have no jobs.\n\nOur civilization needs to make the conscious effort to address the disruptions that are about to occur. The great challenge of our time is to manage this transition. If we currently cling to this mindset of \u201cbusiness as usual,\u201d (that our economies will continue to function as they always have) then we are setting ourselves up to be blind-sided for the coming disruption. Ignorance of what is about to happen will lead to a lot of suffering for those who continue (sometimes without a choice) to cling to the past. We should not, as an example, insist that our younger generation fall into indebtedness when the rules of the past may not be applicable in the future. The debt of older generations have accumulated to live a life of comfort should not be passed on to their descendants to pay. Artificial Intelligence is both the destroyer and the creator of wealth.\n\nHow do you survive the transition? That should be obvious, place your talents and investments in anything that has to do with Artificial Intelligence."
    },
    {
        "url": "https://medium.com/intuitionmachine/neurons-are-more-complex-than-what-we-have-imagined-b3dd00a1dcd3",
        "title": "Surprise! Neurons are Now More Complex than We Thought!!",
        "text": "One of the biggest misconceptions around is the idea that Deep Learning (DL) or Artificial Neural Networks (ANN) mimics biological neurons. At best, ANN mimics a cartoonish version of a 1957 model of a neuron. Anyone claiming Deep Learning is biologically inspired is doing so for marketing purposes or has never bother to read biological literature. Neurons in Deep Learning are essentially mathematical functions that perform a similarity function of its inputs against internal weights. The closer a match is made, the more likely an action is performed (i.e. not sending a signal to zero). There are exceptions to this model (see: Autoregressive networks) however it is general enough to include the perceptron, convolution networks and RNNs.\n\nNeurons are very different from DL constructs. They don\u2019t maintain continuous signals but rather exhibit spiking or event-driven behavior. So, when you hear about \u201cneuromorphic\u201d hardware, then these are inspired on \u201cintegrate and spike\u201d neurons. These kinds of system, at best, get a lot of press (see: IBM TrueNorth), but have never been shown to be effective. However, there has been some research work that has shown some progress (see: https://arxiv.org/abs/1802.02627v1). If you ask me, if you truly want to build biologically inspired cognition, then you should at the very least explore systems are not continuous like DL. Biological systems, by nature, will use the least amount of energy to survive. DL systems, in stark contrast, are power hungry. That\u2019s because DL is a brute-force method to achieve cognition. We know it works, we just don\u2019t know how to scale it down.\n\nJeff Hawkins of Numenta has always lamented that a more biologically-inspired approach is needed. So, in his research in building cognitive machinery, he has architected systems that try to more closely mirror the structure of the neo-cortex. Numenta\u2019s model of a neuron is considerably more elaborate than the Deep Learning model of a neuron as you can see in this graphic:\n\nThe team at Numenta is betting on this approach in the hopes of creating something that is more capable than Deep Learning. It hasn\u2019t been proved to be anywhere near successful. They\u2019ve been doing this long enough that the odds of them succeeding are diminishing overtime. Bycontrast, Deep Learning (despite its model of a cartoon neuron) has been shown to be unexpectedly effective in performing all kinds of mind-boggling feats of cognition. Deep Learning is doing something that is extraordinarily correct, we just don\u2019t know exactly what that is!\n\nUnfortunately, we have to throw in a new monkey wrench on all these research. New experiments on the nature of neurons have revealed that biological neurons are even more complex than we have imagined them to be:\n\nIn short, there is a lot more going on inside a single neuron than the simple idea of integrate-and-fire. Neurons may not be pure functions dependent on a single parameter (i.e. weight) but rather they are stateful machines. Alternatively, perhaps the weight may not even be single-valued and instead requires complex-valued or maybe higher dimensions. This is all behavior that research has yet to explore and thus we have little understanding to date.\n\nIf you think this throws a monkey wrench on our understanding, there\u2019s an even newer discovery that reveals even greater complexity:\n\nWhat this research reveals is that there is a mechanism for neurons to communicate with each other by sending packages of RNA code. To clarify, these are packages of instructions and not packages of data. There is a profound difference between sending codes and sending data. This implies that behavior from one neuron can change the behavior of another neuron; not through observation, but rather through injection of behavior.\n\nThis code exchange mechanism hints at the validity of my earlier conjecture: \u201cAre biological brains made of only discrete logic?\u201d\n\nExperimental evidence reveals a new reality. Even at the smallest unit of our cognition, there is a kind of conversational cognition that is going on between individual neurons that modifies each other\u2019s behavior. Thus, not only are neurons machines with state, they are also machines with an instruction set and a way to send code to each other. I\u2019m sorry, but this is just another level of complexity.\n\nThere are two obvious ramifications of these experimental discoveries. The first is that our estimates of the computational capabilities of the human brain are likely to be at least an order of magnitude off. The second is that research will begin in earnest to explore DL architectures with more complex internal node (or neuron) structures.\n\nIf we were to make the rough argument that a single neuron performs a single operation, the total capacity of the human brain is measured at 38 peta operations per second. If were then to assume a DL model of operations being equal to floating point operations then a 38 petaflops system would be equivalent in capability. The top ranked supercomputer, Sunway Taihulight from China is estimated at 125 petaflops. However, let\u2019s say the new results reveal 10x more computation, then the number should be 380 petaflops and we perhaps have breathing room until 2019. What is obvious, however, is that biological brains actually perform much more cognition with less computation.\n\nThe second consequences it that it\u2019s now time to get back to the drawing board and begin to explore more complex kinds of neurons. The more complex kinds we\u2019ve seen to date are the ones derived from LSTM. Here is the result of a brute force architectural search for LSTM-like neurons:\n\nIt\u2019s not clear why these more complex LSTM are more effective. Only the architectural search algorithm knows but it can\u2019t explain itself.\n\nThere is newly released paper that explores more complex hand-engineered LSTMs:\n\nIn summary, a research plan that explores more complex kinds of neurons may bear promising fruit. This is not unlike the research that explores the use of complex values in neural networks. In these complex-valued networks, performance improvements are noticed only on RNN networks. This should indicate that these internal neuron complexities may be necessary for capabilities beyond simple perception. I suspect that these complexities are necessary for advanced cognition that seems to evade current Deep Learning systems. These include robustness to adversarial features, learning to forget, learning what to ignore, learning abstraction and recognizing contextual switching.\n\nI predict in the near future that we shall see more aggressive research in this area. After all, nature is already unequivocally telling us that neurons are individually more complex and therefore our own neuron models may also need to be more complex. Perhaps we need something as complicated as a Grassmann Algebra to make progress. ;-)"
    },
    {
        "url": "https://medium.com/intuitionmachine/challenges-for-ai-standardization-eab1de4fab0b",
        "title": "Why Deep Learning Needs Standards for Industrialization",
        "text": "I was recently was posed the question, \u201chow do we define standards for AI?\u201d I am primarily focused in the space of Deep Learning Artificial Intelligence (AI) and deep Learning is a specific set of tribes in a much wider umbrella of what is known as AI.\n\nThe term Artificial Intelligence is quite ancient and was proposed over half a century ago:\n\nIn fact, the idea of understanding human thought goes back much earlier in history:\n\nWe can go even further back to Rene Descartes of the 16th century and even all the way back to Aristotle of 322 BC.\n\nWestern Civilization has built up a ton of intellectual baggage in its understanding of how the human brain works. This accounts for the decades of work in GOFAI, where essentially the approach is to work top-down from formal logic into deriving intuition and instinct.\n\nAlan Turing, the father of modern computing, had anticipated computation for the brain. His unpublished papers, discovered 14 years after his unfortunate death, anticipated the development of connectionist architectures. These are architectures that are more well-known today as Deep Learning:\n\nTherefore, the question I am seeking to understand regarding the standardization of AI is not the standardization of all methods labelled under the massive AI umbrella, but rather I seek to understand the challenges of standardization of Deep Learning.\n\nThe first question to ask is \u201cWhy do we need standardization?\u201d Standardization is associated with interoperability. So in the context of Deep Learning, what does interoperability mean and how can we achieve greater interoperability? Ever since 2012, the technology stack for Deep Learning has become significantly more advanced and richer.\n\nThis does not include all the other orchestration requirements that come from the data engineering or BigData universes. It also does not include the application specific layers such as visualization and active learning that may also be required for a comprehensive solution. In other words, it is a vast landscape that is still evolving at a break-neck pace.\n\nThe insight we can get from looking at the stack above is that there\u2019s a lot of existing standards being adopted and can be leveraged. As a result, one already has a considerable launching pad to explore DL standardization independently of the standardization of other AI fields.\n\nInteroperability standards are important not only for a global community, but also for any organization or company. To scale development, one needs interoperability to maximize the opportunity of reusable development. The important question for an individual organization is \u201cwhere do you draw the line for interoperability?\u201d This is a key architectural question that has big ramifications in one\u2019s ability to scale execution.\n\nIndependent of technology, there is also a need for common terminology. This space may be moving at a rapid pace; however we are all in need of a DL ontology:\n\nThis is sorely needed to be able to quickly capture and make use of the latest developments in research. It does not help if research uses terminologies that are more novel than consistent.\n\nWe can also speak about standardization at a level above the technology stack. That is from the perspective of industrial processes in the development of these new DL based systems. A paper in 2017 \u201cBadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain\u201d by Tianyu Gu, Brendan Dolan-Gavitt and Siddharth Garg provides a good starting point in discussing the need to focus on data quality used in training DL systems. Perhaps ideas from the world of biotechnology manufacturing controls may provide better insight of what needs to be considered here.\n\nA focus on process controls also brings into question what standardizations we need to put in place regarding safety, performance, latency, correctness, bias and even privacy. In fact, there\u2019s a lot to talk about regarding how we handle data and data provenance. It is even more important with machine learning methods like Deep Learning which derives its behavior directly from the data it is trained on.\n\nWe need to standardize the best practices of developing Deep Learning so that not only can more teams accelerate their development but also innovative solutions can be developed independently and be plugged in to accelerate a much larger process. In conventional software development, we have a more mature conceptual framework that has evolved over time. We are able to mix and match different tooling like IDEs, code checkers, testing frameworks, continuous integration, profiling, performance monitoring etc. Deep Learning introduces new kinds of requirements, so we need to understand what these are and what standardize the class of tools needed.\n\nIt is always instructive to take a look at current standardization in the Automotive field. For this, we can learn from the Society of Automation Engineering (SAE). SAE has an international standard which defines six levels of driving automation (SAE J3016). This can also be useful in classifying the levels of automation in domains other than self-driving cars. A broader prescription is as follows:\n\nThe absence of any automation.\n\nUsers are aware of the initiation and completion of the performance of each automated task. The user may undo a task in the event of incorrect execution. Users, however, are responsible for the correct sequencing of tasks.\n\nUsers are aware of the initiation and completion of a composite of tasks. The user however is not responsible for the correct sequencing of tasks. An example will be the booking of a hotel, car and flight. The exact ordering of the booking may not be a concern of the user. However, failure of the performance of this task may require more extensive manual remedial actions. An unfortunate example of a failed remedial action is the re-accommodation of United Airlines\u2019 paying customer.\n\nUsers are only notified in exceptional situations and are required to do the work in these conditions. An example of this is in systems that continuously monitor security of a network. Practitioners take action depending on the severity of the event.\n\nUsers are responsible for defining the end goals of automation, however all aspects of the process execution as well as the handling of in-flight exceptional conditions are handled by the automation. The automation is capable of performing appropriate compensating action in events of in-flight failure. The user however is still responsible for identifying the specific context in which automation can be safely applied to.\n\nThis is a final and future state where human involvement is no longer required in the processes. This of course may not be the final level because it does not assume that the process is capable of optimizing itself to make improvements.\n\nThis is an automation that requires no human involvement and is also capable of improving itself over time. This level goes beyond the SAE requirements but may be required in certain high performance competitive environments such as Robocar races and stock trading.\n\nUltimately however, any form of AI standardization should be framed on how we can best steer AI (or AGI development) for the maximum benefit of humanity. It does not help if our standardization leads to more advance autonomous weaponry or more advanced way to predict human behavior and thus manipulate human behavior.\n\nThe challenges of AI standardization cover many levels of concerns. However, it should be ultimately guided by the need to accelerate the development of human beneficial AI and not the other kind.\n\nTo scale Deep Learning development into a practice that is predictable, reliable and efficient will require standardization. The intent of standardization is to maximize participation of many independent parties. It is a common language or a coordination mechanism for parties to accelerate progress. Accelerated progress is necessary for Deep Learning to not just be confined to research labs but also to be industrialized and available to many."
    },
    {
        "url": "https://medium.com/intuitionmachine/conversational-cognition-a-new-approach-to-agi-95486ffe581f",
        "title": "Conversational Cognition: A New Measure for Artificial General Intelligence",
        "text": "What\u2019s wrong with a majority of research in Artificial Intelligence(AI), Deep Learning and Artificial General Intelligence (AGI)?\n\nFirst, let\u2019s explore the latest research on \u201cThe social and cultural roots of whale and dolphin brains\u201d published in Nature.\n\nOne of the unsolved problems of AGI research is our lack of understanding of the definition of \u201cGeneralization\u201d. I\u2019ve pointed this out in my previous writing: most of the definitions created for Generalization are incomplete. These definitions are either too narrow or even worse incorrect.\n\nWhat I am going to suggest in this article is that our measure of intelligence be tied to our measure of social interaction.\n\nAs I write this, I am thinking of several thought provoking titles for this:\n\nEach of these titles is equally good, but I will settle with the current title I used for this article.\n\nUltimately, to build improved artificial intelligence we need more automation that can generalize extremely well. Unfortunately, you can query researchers as to what Generalization means and they will give you a litany of overly simplistic definitions. How then can we possibly achieve AGI when we have a very shallow understanding of what Generalization means? In this article, I will propose an entirely new definition. I call this Conversational Cognition (I will have to find a more unique and striking name later!). From this perspective, we can draft an opinionated road-map as to how to achieve AGI.\n\nTo summarize, there are many proposed ways to characterize Generalization:\n\nI\u2019m proposing Conversational Cognition be added to this list.\n\nAn ecological approach to cognition is based on an autonomous system that learns by interacting with its environment. Generalization in this regard is related to how effectively automation is able to anticipate contextual changes in an environment and perform the required context switches to ensure high predictability. The focus is not just in recognizing chunks of ideas, but also being able to recognize the relationship of these chunks with other chunks. There is an added emphasis on recognizing and predicting the opportunities of change in context.\n\nThis notion is a level of complexity higher than risk minimization. Risk minimization demands a predictive model that is able to function effectively in the presence of imperfect information of the world. This implies that an automaton\u2019s internal models of reality must be able to accommodate vagueness and unknown information. However, the definition seems to only allude to the need to support context switching.\n\nIn realistic environments, a system must be able to adjust appropriately when a context changes. These environments are sometimes referred to as \u201cnon-stationary\u201d. It is not enough to have models that are able to model the world in a single context. The most sophisticated form of generalization that exists demands the need to perform conversations.\n\nThis conversation however is not confined only to an inanimate environment with deterministic behavior. The conversation must also be available for the three dimensions of cognition. Specifically, we need to explore conversation for computation, autonomy and social dimensions. In the computational dimension, the AlphaGo Zero self-play is an effective demonstration of adversarial agent play in a deterministic environment. In autonomous environments, we require models that are able to continuously perform their capabilities in different similar domains and if necessary, be able to adapt to context changes in environments. An example of this would be biological systems adjusting their behavior in response to cyclic changes in the seasons. The social environment will likely be the most sophisticated system in that it may demand understanding the nuisances of human behavior. This may include complex behavior such as deception, sarcasm and negotiation.\n\nThe needs of social survival will also require the development of cooperative behavior. It is only through recognizing that skilled conversation is a necessary ingredient that we may achieve this. Effective prediction of an environment is an insufficient skill to achieve cooperative behavior. Conversations require the cognitive capabilities of memory, conflict detection and resolution, analogy, generalization and innovation. The development of language is a fundamental skill, however languages are not static \u2014 they evolve. Moreover, the evolution of language happens only through conversations.\n\nEffective conversation is a two way street. It requires not only understanding an external party but also the communication of an automaton\u2019s inner model. This communication requires more than decompression, but rather it requires the appropriate contextualized communication that anticipates the cognitive capabilities of other conversing entities. Good conversation requires good listening skills as well as the ability to assess the current knowledge of a participant and performing the necessary adjustment to convey information that a participant can relate to. This is indeed an extremely complex requirement. However, if we are seeking out Artificial General Intelligence then it only makes sense that we should begin accepting a more complex measure of intelligence.\n\nConversation can also be considered as a kind of game play. That is, an agent may have goals that demand it to devise approaches to recruit other participants to aid it in achieving its goals. This however may seem to demand almost human-like capabilities to even achieve. The question therefore is whether we can build primitives of cooperation that can bring us closer to this kind of generalization.\n\nConversational Cognition is related to Cognitive Synergy. In cognitive synergy different agents may pick up from where a conversation left off.\n\nFocusing on conversation is actually not a novel thought in the study of intelligent systems. However, this approach has recently taken a backseat with the development of effective machine learning techniques. I however would like to get a more serious exploration of this approach in the context of deep learning.\n\nResearch in primitives for cooperative agent system has been studied extensively in the past. Winograd\u2019s Speech Acts and FIPA are excellent starting points to identify composable language elements to build more complex forms of cooperation. Furthermore, we can leverage our understanding of the Loose Coupled Principle (LCP). LCP has at its core the assumption that the most likely method that nature will select will be the method that requires the least amount of assumptions (or preconditions).\n\nSocieties and large scale organizations also require effective coordination to scale.\n\nWhat sort of measure can we use to evaluate the effectiveness of a conversation? I will leverage Lawrence embodiment metric, however it should measure at a minimum 3 legs of a conversation. This includes the initial communication, the response to the communication followed by a response to the final response. Conversations of course can go on forever, but a measure is based on:\n\n(1) The ability to articulate an internal mental model of the external participant.\n\n(2) Receive a response for a communication and re-assess the internal mental model.\n\n(3) Finally, articulate a second response based on the newly changed mental model.\n\nThis measurement is applicable regardless of the intelligence of the external participant.\n\nConversational Cognition is the most abstract form of generalization. Let\u2019s compare this new measure of generalization with alternative frameworks for cognition.\n\nKarl Friston\u2019s Free Energy \u2014 This idea makes use of the ubiquitous concept in Physics known as the Principle of Least Action. All dynamics, even cognitive dynamics must take this idea into account. This is fundamental to how nature works.\n\nWissner Maximizing Future Freedom \u2014 This is a unique idea that explains intelligence as the maximizing of options. This, like the principle of least action is a universal one. The idea is that rather than just blindly choosing the most efficient action, intelligence requires the action that maximizes survival.\n\nCompression \u2014 There is overly simplified notion that compression implies intelligence. Perhaps it is because compression is a measurable attribute of language. However, are all notions of compression the same? Do all compressions allow for composition and grounding? Language is a kind of compression, however, it requires many other attributes outside of compression.\n\nKolmogrov Complexity and Solomonoff Induction \u2014 This a favorite of many AGI researchers. AIXI is the ultimate theory. However, it is based on a set of beliefs that aligns computational ability with general intelligence. There is no evidence that increased computational ability automatically leads to intelligence. Rather, certain cognitive skills such as language need to be effectively used. One other problem with this approach is that it demands approaches that are incomputable and lead to broad conclusions wherein certain situations are not possible.\n\nIntegrated Information Theory \u2014 IIT defines a measure of consciousness that relates to how non-decomposable thought processes are within an entity. Integration may be an advanced cognitive ability; however, there are many animals with higher integration capabilities the humans. Pigeons for example are much better at multi-tasking than humans. Integration leads only to greater awareness of the environment; however it does not demand a requirement to develop strategies to cooperate with other entities. Babies very quickly learn how to manipulate their parents in pursuit of their own goals.\n\nFor many people who are involved in developing complex products that require many agents to cooperate, this definition of generalization is very obvious. Surprisingly though, the AI community has mostly adopted overly simplistic notions of generalization. These notions have researchers focusing on the wrong problem that probably have nothing to do with achieving AGI. The ability to effectively perform a conversation with the environment is the essence of AGI. Interestingly enough, what most AGI research avoids is the reality that an environment is intrinsically social. That is, there are other intelligences that exist. This is true in both the human and biological context.\n\nIn summary, it is the cognitive ability to dynamically adapt a conversation with new ideas and steer it collaboratively towards innovative solutions. Conversations are the highest reflection of intelligence. It is how language evolves over time with new concepts and how new memes become viral. It is what the \u201cdesign pattern\u201d practitioners describe as growing patterns. It is a dynamic and evolutionary description of general intelligence where previous characterizations of intelligence have focus on static and specialized descriptions. It is not enough to be able to adapt to a single context, general intelligence needs the ability to adapt also the pervasive switching of context."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-deep-learning-ai-playbook-2bf973dfecf5",
        "title": "The Deep Learning AI Playbook \u2013 Intuition Machine \u2013",
        "text": "Permit me the luxury of introducing a book I\u2019ve been working on for the past year. For the readers of this blog, I\u2019m sure you\u2019ve seen this book mentioned frequently in my posts. However, I have yet to properly introduce the book to you and this is finally my opportunity to do so.\n\nThe book has its own website, which should be very easy to remember: \u201cDeep Learning Playbook.com\u201d. Every new content in today\u2019s world has to compete for the eyeballs and the time of its customers. That\u2019s where selling a book is a very difficult proposition, no matter how valuable the content may be. Who has the time to slog through 350 pages of text? Unfortunately, for complex and emerging subjects like Deep Learning, there aren\u2019t any instant ways to download knowledge and wisdom into one\u2019s own brains. If there was a way to make knowledge transfer even easier, I would very much like to know what that magic potion might be.\n\nSo ultimately, getting down to reading a book is a matter of priorities and urgency. The reason you might want to read my book over the more entertaining activity of binge-watching \u201cBlack Mirror\u201d is that this playbook gives you a foundation of what is indeed real and what indeed is possible. For most people, technology is sufficiently advanced that it seems to be all magic. However, this is not the case and it\u2019s important for everyone to be able distinguish magic from reality. Perhaps this is one reason I wrote this book, to discover for myself what\u2019s underneath the magic of Deep Learning.\n\nWriting a book takes considerable mental effort. This does not include all the other administrivia that is required for a book (i.e. cover design, layout, proof reading, copy editing etc.). The most difficult part of writing is a book is the generation of ideas. Fortunately, I have this blog and its many readers who have given me feedback, ideas and criticism that has helped me improve and fine tune new ideas. As a human being, it is very difficult to talk about something without receiving feedback. We are designed to appreciate conversation and by having an immediate conversation with your audience, you generate ideas. As in conversation, one idea becomes a link to another idea. When you write a blog, you get feedback and that in itself is a valuable motivator to get you to slog through the completion of a book.\n\nWriting is knowledge discovery and knowledge discovery is iterative\n\nA book is always never really finished. You will always find new and more compelling concepts and fresh ways to present content. So as it is best practice in agile environments, it\u2019s always best to confine scope by time-boxing the process. Also, it\u2019s always better than to be late. Knowledge discovery will always be an iterative process and book writing is essentially a knowledge discovery process. It\u2019s very rare that when you start writing a book, you already know all of what needs to be written. Usually you discover what you don\u2019t know when you begin to write.\n\nAn entrepreneurial mindset that is always seeking for opportunities (or ideas) is key\n\nForcing oneself to write is like photography. When you are into photography, you\u2019ve consciously decided to capture the best angles of an event or place you find yourself in. This makes you more consciously aware of the environment than a casual visitor. You are aware of the lighting, shadows, interaction of people, shooting angles etc. It is the same with writing. You put yourself in a perspective that many may not be aware of. That is why, photographers can learn from other photographers and writers can learn from other writers. Thus, I am grateful for the perspectives other writers have lent me.\n\nDon\u2019t just write one book, write two or more!\n\nIn the Deep Learning Playbook, I introduce this idea of Jobs To Be Done (JTBD). It\u2019s a way of understanding the kind of product you need to build to satisfy a customer. In essence, you should create products that make it easier for people to accomplish their jobs. The complexity however comes from the different motivations or jobs of different people. Thus, it\u2019s inefficient to create a product that is tailored for too many. It dilutes the message and you have a product that\u2019s just too generic nobody understands how it would be helpful.\n\nThe Playbook\u2019s audience are people in business who need to know what to do to take advantage of this latest A.I. technology known as Deep Learning. So, if that\u2019s what you\u2019re doing in your current job, then it makes sense to get this book. However, most people only want to know about Deep Learning from a 30,000 foot level. How do you address the people who are just curious and want to just expand their knowledge? Furthermore, what if one of your goals is to introduce to as many people an idea that sounds like an oxymoron (i.e. Artificial Intuition)? Well that\u2019s why I came up with another book that\u2019s affordable to the widest audience possible:\n\nCompared to the Playbook, I truly enjoyed writing this one. However, like many things in life, the stuff you enjoy doing isn\u2019t the stuff that people will pay for. People tend to pay for the more expensive Deep Learning Playbook. It is simple economics: f one can monetize what they are buying then its price is less of an issue. That\u2019s why financial products like stocks and bitcoin are easy sells. Everyone wants to buy a \u2018money printing device\u2019.\n\nYou are lucky if people have time to write a testimonial\n\nA book is always best sold with testimonials. I am fortunate to have found people who have taken the time to go through my book and lend their impressions. Here are a few of the notable ones:\n\nWriting books are for the passionate and not those seeking financial rewards\n\nFinally, a word about the economics of publishing a book. Most people who have never written a book are unaware of the brutal economics. I have chosen to self-publish. Gumroad is a god-send for writers in that at least 95% of the purchase price goes to the original author. However, when a writer goes with a publisher, he would find himself lucky to even receive 12% of the net revenue. A printed book has its own cost, a standard \"6x9\" paperback costs around $5 to produce. A hardbound version, costs a lot more (i.e. $12). After that, the distributors are also going to get their cut. So depending on which distributor you use, they can get at least a 40%-65% cut from the sales price. So, just to put this in perspective, a writer who sells a book via a publisher for $50 should expect to receive a paltry $3.24 ( (50\u20135)*.6*.12) in revenue. Try feeding your family on those wages! Very little goes to the author will all these middle men taking their cut. So don\u2019t go into writing a book for the money. Write because you are passionate about the subject. And if you want to be fairly compensated, get rid of the middle men. That\u2019s what I did and I enjoyed the entire process!\n\nI will be in Amsterdam this March, ping me if you want to chat!"
    },
    {
        "url": "https://medium.com/intuitionmachine/introducing-the-deep-learning-ai-playbook-73075b26bf4c",
        "title": "How I Wrote the Deep Learning AI Playbook \u2013 Intuition Machine \u2013",
        "text": "Permit me the luxury of introducing a book I\u2019ve been working on for the past year. For the readers of this blog, I\u2019m sure you\u2019ve seen this book mentioned frequently in my posts. However, I have yet to properly introduce the book to you and this is finally my opportunity to do so.\n\nThe book has its own website, which should be very easy to remember: \u201cDeep Learning Playbook.com\u201d. Every new content in today\u2019s world has to compete for the eyeballs and the time of its customers. That\u2019s where selling a book is a very difficult proposition, no matter how valuable the content may be. Who has the time to slog through 350 pages of text? Unfortunately, for complex and emerging subjects like Deep Learning, there aren\u2019t any instant ways to download knowledge and wisdom into one\u2019s own brains. If there was a way to make knowledge transfer even easier, I would very much like to know what that magic potion might be.\n\nSo ultimately, getting down to reading a book is a matter of priorities and urgency. The reason you might want to read my book over the more entertaining activity of binge-watching \u201cBlack Mirror\u201d is that this playbook gives you a foundation of what is indeed real and what indeed is possible. For most people, technology is sufficiently advanced that it seems to be all magic. However, this is not the case and it\u2019s important for everyone to be able distinguish magic from reality. Perhaps this is one reason I wrote this book, to discover for myself what\u2019s underneath the magic of Deep Learning.\n\nWriting a book takes considerable mental effort. This does not include all the other administrivia that is required for a book (i.e. cover design, layout, proof reading, copy editing etc.). The most difficult part of writing is a book is the generation of ideas. Fortunately, I have this blog and its many readers who have given me feedback, ideas and criticism that has helped me improve and fine tune new ideas. As a human being, it is very difficult to talk about something without receiving feedback. We are designed to appreciate conversation and by having an immediate conversation with your audience, you generate ideas. As in conversation, one idea becomes a link to another idea. When you write a blog, you get feedback and that in itself is a valuable motivator to get you to slog through the completion of a book.\n\nWriting is knowledge discovery and knowledge discovery is iterative\n\nA book is always never really finished. You will always find new and more compelling concepts and fresh ways to present content. So as it is best practice in agile environments, it\u2019s always best to confine scope by time-boxing the process. Also, it\u2019s always better than to be late. Knowledge discovery will always be an iterative process and book writing is essentially a knowledge discovery process. It\u2019s very rare that when you start writing a book, you already know all of what needs to be written. Usually you discover what you don\u2019t know when you begin to write.\n\nAn entrepreneurial mindset that is always seeking for opportunities (or ideas) is key\n\nForcing oneself to write is like photography. When you are into photography, you\u2019ve consciously decided to capture the best angles of an event or place you find yourself in. This makes you more consciously aware of the environment than a casual visitor. You are aware of the lighting, shadows, interaction of people, shooting angles etc. It is the same with writing. You put yourself in a perspective that many may not be aware of. That is why, photographers can learn from other photographers and writers can learn from other writers. Thus, I am grateful for the perspectives other writers have lent me.\n\nDon\u2019t just write one book, write two or more!\n\nIn the Deep Learning Playbook, I introduce this idea of Jobs To Be Done (JTBD). It\u2019s a way of understanding the kind of product you need to build to satisfy a customer. In essence, you should create products that make it easier for people to accomplish their jobs. The complexity however comes from the different motivations or jobs of different people. Thus, it\u2019s inefficient to create a product that is tailored for too many. It dilutes the message and you have a product that\u2019s just too generic nobody understands how it would be helpful.\n\nThe Playbook\u2019s audience are people in business who need to know what to do to take advantage of this latest A.I. technology known as Deep Learning. So, if that\u2019s what you\u2019re doing in your current job, then it makes sense to get this book. However, most people only want to know about Deep Learning from a 30,000 foot level. How do you address the people who are just curious and want to just expand their knowledge? Furthermore, what if one of your goals is to introduce to as many people an idea that sounds like an oxymoron (i.e. Artificial Intuition)? Well that\u2019s why I came up with another book that\u2019s affordable to the widest audience possible:\n\nCompared to the Playbook, I truly enjoyed writing this one. However, like many things in life, the stuff you enjoy doing isn\u2019t the stuff that people will pay for. People tend to pay for the more expensive Deep Learning Playbook. It is simple economics: if one can monetize what they are buying then its price is less of an issue. That\u2019s why financial products like stocks and bitcoin are easy sells. Everyone wants to buy a \u2018money printing device\u2019.\n\nYou are lucky if people have time to write a testimonial\n\nA book is always best sold with testimonials. I am fortunate to have found people who have taken the time to go through my book and lend their impressions. Here are a few of the notable ones:\n\nWriting books are for the passionate and not those seeking financial rewards\n\nFinally, a word about the economics of publishing a book. Most people who have never written a book are unaware of the brutal economics. I have chosen to self-publish. Gumroad is a god-send for writers in that at least 95% of the purchase price goes to the original author. However, when a writer goes with a publisher, he would find himself lucky to even receive 12% of the net revenue. A printed book has its own cost, a standard \u201c6x9\u201d paperback costs around $5 to produce. A hardbound version, costs a lot more (i.e. $12). After that, the distributors are also going to get their cut. So depending on which distributor you use, they can get at least a 40%-65% cut from the sales price. So, just to put this in perspective, a writer who sells a book via a publisher for $50 should expect to receive a paltry $3.24 ( (50\u20135)*.6*.12) in revenue. Try feeding your family on those wages! Very little goes to the author will all these middle men taking their cut. So don\u2019t go into writing a book for the money. Write because you are passionate about the subject. And if you want to be fairly compensated, get rid of the middle men. That\u2019s what I did and I enjoyed the entire process!"
    },
    {
        "url": "https://medium.com/intuitionmachine/intuition-machines-versus-algebraic-minds-fad052b46ad5",
        "title": "Intuition Machines versus Algebraic Minds \u2013 Intuition Machine \u2013",
        "text": "I owe at least Gary Marcus a more detailed examination of each of his assertions of the limitations of Deep Learning. The problem with researchers who have not really performed their due diligence in examining Deep Learning is that they do not truly understand the massive developments occurring in this field. In fact, they are performing a disservice by casting Fear, Uncertainty and Doubt (FUD) into a field of study that has the highest potential for achieving AGI. The bigger problem is not the scarcity of funding for other AGI research activities but the outsized public spending China has deployed relative to the US and Europe.\n\nHowever I digress and I shall switch my focus to examine each of Marcus\u2019 critiques. As I go through each critique, I will explain to you the path forward using the cognitive paradigm that I have been embarking on (i.e. Intuition Machines).\n\nAny conversation about achieving AGI is incomplete without requiring a mechanism on how automation interacts with its environment:\n\nDeep Learning are very good universal approximators (the best we currently have). Of course there\u2019s more to AGI than just learning patterns. I\u2019ve written about that in the article above. However, until one conjures up a better pattern recognizer, then advances in AGI will continue to be driven by Deep Learning. There are many advanced methods that have incorporated these approximators. GANs introduced the idea of competitive learning by combining a recognizer and a generator. RL + DL (see DeepMind Atari Game Play) showed that by employing Q-learning and DL, learning to play by only observing pixels was possible. AlphaZero with MCTS + DL showed that learning more advanced strategies could be bootstrapped from scratch with self-play. None of these newer methods were envisioned when the term \u201cDeep Learning\u201d was invented. However, the primary reason for the success of any of these new methods is the use of Deep Learning as a component. Remove Deep Learning from the equation and you have nothing.\n\nNow it\u2019s time to pick apart Gary Marcus\u2019 arguments and give you a glimpse of how they\u2019re going to be solved by Deep Learning (or an Intuition Machine).\n\n3.1. Deep learning thus far is data hungry\n\nThis is because most experimental setups are meant to begin from scratch without any a priori knowledge about the environment. Human infants, in contrast, have eons of evolution to take advantage of.\n\nHowever, if we were to examine the latest developments with AlphaZero then this limitation of requiring a lot of data seems to be removed. AlphaZero learned to play grandmaster level chess without learning from recorded game play. Rather, it did so just by defining only the rules of the game of chess. It learned chess playing strategies (different opening moves, gambits, control of parts of the board, end game play, zugzwang etc.) all in just 4 hours. In other words it learned all that humanity learned in the many centuries that chess has been played in an incomprehensibly short period of time.\n\nConveniently ignore this development at your own peril.\n\n3.2. Deep learning thus far is shallow and has limited capacity for transfer\n\nOur understanding of transfer learning in Deep Learning is still nascent. We have seen advances in transfer learning when used for similar domains. So for example, one can train faster with networks that are pre-trained. One of the most impressive developments in this area is the work done to generate high resolution images. This builds up a network by progressively learning from smaller networks into bigger and more capable networks.\n\nHowever, what we are unable to train the network to do is learn the appropriate invariants such that what is learned is only what is important to learn. Deep Learning seems to learn everything and we don\u2019t understand how to disentangle what is unimportant in another domain. So for example, if you change the dimensions of a video game, a RL + DL system trained to play this game will not be able to play the game. The scaling just messes up the learned model.\n\nThis is an interesting problem, but it doesn\u2019t appear to be an insurmountable one.\n\n3.3. Deep learning so far has no natural way to deal with hierarchical structure\n\nFundamentally, Deep Learning builds representations in continuous vector spaces that don\u2019t capture the concept of hierarchy. However, works like Capsule Networks, Hyperbolic spaces and Graph Convolutional Networks have mechanisms to capture these hierarchies.\n\nThere are thousands of papers in DL that are published every year. I don\u2019t expect Marcus\u2019 busy schedule to have time to keep up with the literature.\n\n3.4. Deep learning thus far has struggled with open-ended inference\n\nNew research papers in non-stationary environments with multiple competing and cooperating neural networks are now being published. Open-endedness requires creating strategies with imperfect information. There is also significant research in areas of semi-supervised learning where some information is labelled while most of the information is not. Deep Learning systems have been shown to do well in this area.\n\nOne example of an environment with imperfect information is the game of Poker. There have been significant advances in the use of Deep Learning to play in competitive poker and it\u2019s doing very well.\n\nI can boldly say that the state-of-the-art in open-ended inference is ruled by Deep Learning based techniques.\n\n3.5. Deep learning thus far is not sufficiently transparent\n\nThis is true. However are we not discussing this in the context of Artificial General Intelligence? Are humans themselves sufficiently transparent?\n\nThe inferences humans make with their intuition are just as unexplainable as the inferences that a Deep Learning intuition machine makes. Please read my 2018 predictions on my take on explainable Deep Learning.\n\n3.6. Deep learning thus far has not been well integrated with prior knowledge\n\nThis is also unrelated to AGI. If I were to give a caveman a semantic network or Newton\u2019s equations of motion, he wouldn\u2019t be able to incorporate that into his own knowledge base. Humans don\u2019t have a mechanism like that found in the Matrix where you can simply download knowledge.\n\nThe way humans incorporate prior knowledge is through a K-12 school curriculum that covers years of teaching.\n\nNevertheless, let\u2019s ignore the absurdity of the requirement for a moment. In NLP that employs Deep Learning, recognition engines that incorporate semantic networks have shown to have better performance than ones that do not.\n\n3.7. Deep learning thus far cannot inherently distinguish causation from correlation\n\nThe average human can\u2019t either. This is not an AGI requirement.\n\n3.8. Deep learning presumes a largely stable world, in ways that may be problematic\n\nOne of the biggest unsolved problems of Deep Learning is learning how to forget. Forgetting is important in how internal mental models of the world are created. When we have the kind of higher level intelligence that can perform simulations and experiments of the world and in one\u2019s imagination, you get to situations where you recognize that previous knowledge is incorrect and therefore must be adjusted accordingly.\n\nThe malleably assessing what has been learned and what knowledge needs to be changed because of inconsistency in what is observed in the real world is a difficult skill.\n\nEven the current president of the United States lacks this skill set.\n\n3.9. Deep learning thus far works well as an approximation, but its answers often cannot fully be trusted\n\nTrust is an emergent social behavior. (Unless it is encoded in a Blockchain)\n\nHow can we trust self-driving cars when underneath the covers they use Deep Learning?\n\nWe will trust them because the insurance companies will run the numbers and begin to start charging premium for people who chose to manually drive their own cars.\n\nRead this article about the complexities of human compatible AI.\n\n\n\n3.10. Deep learning thus far is difficult to engineer with\n\nHell yeah! That\u2019s why Deep Learning is sometimes known as \u201cDeep Alchemy\u201d. Not every revolution comes with little effort!\n\nIdeally we would like to have systems with a more biological architecture. I address these issues in an article on \u201cbiologically inspired architecture\u201d. If you really want to understand the complexities of building Deep Learning architectures, then perhaps you can start with this article.\n\nMarcus appears to have conflated the requirements for a highly engineered cognitive system with the requirements for AGI. In some cases, the problems are related, but not every time.\n\nHowever, there\u2019s just one reason why Gary Marcus is wrong about Deep Learning and AGI. General Intelligence, as we see in humans, is not \u2018algebraic minds\u2019, rather they are intuition machines. That makes the difference in how we believe the approach to AGI should proceed. Marcus\u2019 Algebraic Mind approach reminds me of the \u201cIntelligent Design\u201d arguments made by creationists. According to them, there is no probable way that evolution can evolve an eye-ball. However, this is the argument that Deep Learning researchers assert: innate cognitive machinery can be grown from learning methods and not through design.\n\nWe will likely discover AGI before we understand how it works. In the history of science, theory rarely ever comes before discovery. What will likely happen is that Deep Learning methods will discover more advanced learning methods that lead us to AGI and it will take a while before we understand these methods. These are not unlike the new game play discovered by AlphaGo or AlphaZero. We see it the first time and believe these moves to be a mistake, only to discover the brilliance after the game is won. However, even if we go back to re-examine the move, our understanding of the reasoning behind it remains limited. This is because insight as a consequence of intuition is always very difficult to explain."
    },
    {
        "url": "https://medium.com/intuitionmachine/thoughts-on-gary-marcus-critique-of-deep-learning-89146e63c3b3",
        "title": "Thoughts on Gary Marcus\u2019 Critique of Deep Learning \u2013 Intuition Machine \u2013",
        "text": "Gary Marcus has recently published a detailed, rather extensive critique of Deep Learning. While many of Dr. Marcus\u2019s points are well-known among those deeply familiar with the field and have been somewhat well-publicized for years, these discussions haven\u2019t yet reached many who are newly involved in decision-making in this space. Overall, the discussion the critique has generated seems clarifying and useful.\n\nI have decided to write up my thoughts because, while I think Dr. Marcus\u2019 critique is thoughtful, necessary and often justified, I disagree with some of the conclusions.\n\nTo start, Dr. Marcus\u2019 assessment that Deep Learning, as originally defined, is merely a statistical technique for classifying patterns is spot on in my opinion. I also concur with his assessment that Deep Learning techniques will be a stepping stone towards and perhaps a component of future Artificial Intelligence systems, but, contrary to popular expectation, will not lead directly to Artificial Intelligence per se.\n\nWhy is that?\n\nI believe it\u2019s important to talk about this rather fundamental issue, because it doesn\u2019t seem to be exposed well, neither in Dr. Marcus\u2019 paper nor in much of the public discussion about Artificial Intelligence:\n\nOne cannot talk productively about Artificial Intelligence without a clear definition of intelligence. The Oxford English Dictionary defines intelligence as \u201cthe ability to acquire and apply knowledge and skills\u201d.\n\nThe definition does not include who or what does said acquisition and application, but it\u2019s somewhat obvious that there is some kind of \u201cagent\u201d, and that agent is capable of (skillful) action in addition to acquiring knowledge. One could perhaps furthermore argue that all acquisition of knowledge is in service of the agent\u2019s ability to decide how to act sometime in the future.\n\nSo it seems that action is a fundamental concept in this context. Let\u2019s look into this a bit deeper.\n\nWhat does a (perhaps artificially) intelligent agent do, and why does it do what it does? It seems to me that the field has been shying away from this question, perhaps because answering it clearly is difficult.\n\nI believe the correct answer is both profound and seemingly trivial.\n\nAutonomous intelligent agents act to survive. Or in other words, they act to avoid dispersion by the forces of their environment that either, by way of natural laws, point towards increasing entropy, or even actively conspire against them in a competition for scarce resources (e.g. predators). Furthermore, one could argue that the very reason for the existence of intelligence is the need of complex agents to survive in a hostile world, i.e. to maintain their form against the odds. To go even further, from another angle one could claim that any (intelligent) action is ultimately in service of survival of the agent.\n\nThere are very interesting points to be made about how an agent knows how to act in order to survive, and Karl Friston makes these throughout his recent work on Active Inference, by way of physics and information theory. I won\u2019t go into any detail here, and just reiterate that action is a fundamental and required concept for intelligence. It is how an agent interacts with the world. It\u2019s how it bends the odds in its favor, towards survival.\n\nThis leads us back to the point I\u2019m trying to make.\n\nDeep Learning, per se, does not have a concept of action. It doesn\u2019t have to. It is merely, as Dr. Marcus points out, a technique for building pattern recognizers that is sometimes used as a component in Reinforcement Learning, a field of study that does include agents and action and is therefore inherently conceptually closer to Artificial Intelligence than Deep Learning.\n\n[Please note that I\u2019m using the term \u201cReinforcement Learning\u201d because it\u2019s the most well-known and accepted term for the type of learning that goes on in autonomous agents. However, I believe that Reinforcement Learning, as strictly defined, has fundamental issues that will need to be overcome on the path towards Artificial Intelligence. For one, the requirement of reward/value functions seems to be causing rather difficult complications that are avoidable by framing the problem differently.]\n\nI think there is an opportunity to write a similar critique of the state of the art in Reinforcement Learning. It would be a better basis for reasoning about the path towards Artificial Intelligence and whether we are on the right track.\n\nWe could have a great discussion about e.g. what survival could mean in context of artificially intelligent agents, and how different techniques in Reinforcement Learning do or do not capture the nature of it properly.\n\nBut not all the points Dr. Marcus makes in his current critique would apply, because one can do Reinforcement Learning just fine without Deep Learning.\n\nWith that, let\u2019s look at Dr. Marcus\u2019 \u201cten challenges\u201d:\n\n3.1. Deep learning thus far is data hungry\n\nI was expecting a point about Deep Learning requiring large amounts of labeled data to train models because the learning process is so inefficient in its use of information, and that a more principled approach could be more efficient. \n\nHowever, Dr. Marcus took it in a different direction, first touching on human infants\u2019 ability to generalize, then towards an argument that models should be able to learn directly from high-level concepts expressed in language. \n\nThere\u2019s no doubt that language is essential for true human-level intelligence. In fact, one could make an argument that language facilitates the abstraction and generalization that is required for learning and, ultimately, problem solving at human level. \n\nHowever, autonomous agents without sophisticated language (e.g. cats) can act quite intelligently shortly after birth as well. We would be quite happy about being able to build such an agent. Therefore, I believe that it\u2019s a fair point, but I don\u2019t agree with the conclusion.\n\n3.2. Deep learning thus far is shallow and has limited capacity for transfer\n\nI guess this is a good clarification of terminology for people who had incorrect assumptions about what \u201cdeep\u201d means in the term \u201cDeep Learning\u201d. The rest of the section points out Deep Learning\u2019s challenges with model overfitting, leading to insufficient generalization. Fair point. There\u2019s some overlap with the next argument about hierarchical structure as some of the generalization could be implemented through it.\n\n3.3. Deep learning so far has no natural way to deal with hierarchical structure\n\nA valid point. This especially resonates when thinking about about representation and execution of (complex) action, which seems to be naturally hierarchical. I suspect there is progress to be made around framing action differently, e.g. as a result of top-down, hierarchical inference of the model (in particular, Karl Friston\u2019s Active Inference which seems to solve or even dispense with a number of difficult problems of current techniques). \n\n3.4. Deep learning thus far has struggled with open-ended inference\n\nFrom the title, I expected the author to make a (IMHO valid) point about Deep Learning\u2019s rather fundamental inability to do continuous, iterative training. But he just appears to rehash his earlier points about Deep Learning not having an inherent concept of language connected to high-level representations of the model, and therefore inability to make complex logical inferences based on abstract rules expressed through language. As a rather fascinating side note on this point, it turns out that even humans of normal intelligence, but who don\u2019t have language can\u2019t make these inferences either. So the conclusion doesn\u2019t resonate.\n\n3.5. Deep learning thus far is not sufficiently transparent\n\nA fair point. I guess in the long term, this will come down to optimizing the models to find a balance between model accuracy and complexity (leading to simpler, optimal models), plus teaching the models high-level representations as a joint distribution with the corresponding terms in some kind of descriptive language so they can explain their decisions as they go.\n\n3.6. Deep learning thus far has not been well integrated with prior knowledge\n\nCats have arguably no explicit knowledge of Newton\u2019s laws, yet apply them expertly to solve difficult kinetic problems just based on playful experience. On the other hand, perfectly intelligent adult humans often have trouble making fairly basic logical inferences based on language. I don\u2019t understand why this should be required. In my opinion, artificially intelligent agents should, like biological intelligent agents, be able to learn about the world from the ground up from experience, perhaps optionally acquiring contextualization with prior knowledge as they go.\n\n3.7. Deep learning thus far cannot inherently distinguish causation from correlation\n\nIt is not at all clear that agents have to be able to do that in order to be considered \u201cintelligent\u201d. Humans often can\u2019t do it (superstition). It appears that the ability to form hypotheses about plausible causation, based on sequences of observations, is very often good enough.\n\n3.8. Deep learning presumes a largely stable world, in ways that may be problematic\n\nA great point, but not to the extent Dr. Marcus seems to be making it. Humans have trouble overriding previously learned, strong beliefs, too. But it\u2019s true, vanilla Deep Learning has no mechanism to override learnings or re-learn selectively. There\u2019s a lot of promising work around model structure learning and structure optimization that may address this issue in the future.\n\n3.9. Deep learning thus far works well as an approximation, but its answers often cannot fully be trusted\n\nAs are humans\u2019 answers. The net error rate does not seem to be the problem here. Being able to construct adversarial examples due to overfitting models is a real problem though and points indeed towards fundamental problems with overfitting, so overall a fair point.\n\n3.10. Deep learning thus far is difficult to engineer with\n\nAgain, a fair point. Deep Learning tools have evolved a lot, and the amount of work required to train and use rather straightforward models isn\u2019t daunting anymore. But there are indeed inherent challenges with debuggability.\n\nDr. Marcus doesn\u2019t touch on a bunch of other, in my opinion important issues of Deep Learning that makes it challenging to use even as a component of Reinforcement Learning systems:\n\nI do agree with Dr. Marcus about the dangers of overhyping the state of the art, and the necessity to develop techniques for robust unsupervised learning, in particular in the context of Reinforcement Learning.\n\nAlso agree, to my earlier point, that models that can act upon the environment are essential. These models should probably be constructed bottom up, from basic movements and kinetics on up. In my opinion, a lot of human knowledge can be mapped onto concepts of location and kinetics (derivative(s) of location), and those can ultimately be mapped onto proprioception.\n\nModel structure learning and model optimization is IMHO a required component here.\n\nI hope these thoughts were useful. Comments are welcome and appreciated."
    },
    {
        "url": "https://medium.com/intuitionmachine/has-deep-learning-hit-a-wall-ec6a7cc82cb3",
        "title": "The Boogeyman Argument that Deep Learning will be Stopped by a Wall",
        "text": "I\u2019m always seeking out arguments against my present beliefs (or models of reality). Gary Marcus wrote a new essay titled \u201cDeep Learning: A Critical Appraisal\u201d where he points out all the many flaws of Deep Learning. He has a vested interest in seeing Deep Learning fail, after all, he wrote a book in 2001, which he still is very proud of, that disparaged the nascent Artificial Neural Network research at the time. He writes:\n\nMarcus is very motivated to point out the lack of success of neural networks at every opportunity. His latest essay is one of his many attempts to claim higher understanding by means of criticism.\n\nNevertheless, let\u2019s explore Marcus\u2019 newest arguments because it may be valuable in pointing out flaws that we may have bias in noticing. Marcus enumerates the following flaws in present day Deep Learning:\n\nThese are all valid arguments and they apply not only to Deep Learning but also any algorithm that gains knowledge from digesting data. These arguments apply to all machine learning algorithms. Just replace the phrase \u201cDeep Learning\u201d with \u201cMachine Learning\u201d and Marcus arguments will remain equally valid.\n\nThere is no deep insight here that any researcher in the Deep Learning field is unaware of. These are all known unknowns. What I mean by this is that we, the researchers, all know the flaws mentioned by Marcus and are currently seeking to discover new algorithms to fix these flaws.\n\nOf course, I\u2019m not the only researcher who is aware of the limitations of deep learning described in his essay. Marcus\u2019 essay got some immediate responses in Twitter:\n\nOf course, the key questions are, \u201cIs Deep Learning flawed enough that it is the wrong approach to move forward? And if it is the wrong approach, then which among the other approaches is more promising?\u201d\n\nTo his own credit, Marcus does make an effort to address these two questions.\n\nTo avoid being cast as the most known skeptic of Deep Learning, Marcus points out that Deep Learning is one of the many tools that may emerge. Presently, Marcus doesn\u2019t say that Deep Learning is wrong (as he usually does), but takes a more conservative stance saying that it will be one of the useful tools in a toolbox of many other tools. This argument highlights the fundamental flaw of Marcus\u2019 thesis since 2001. Being a cognitive psychologist he observes capabilities found in humans and then deduces that there are all kinds of cognitive machinery that needs to exist for each capability to work. However, he doesn\u2019t have an explanation as to (1) how each kind of machinery works and (2) how these many kinds of machinery coordinate to get anything accomplished.\n\nWhere Marcus greatly erred is in his failure to comprehend that Deep Learning is the stepping stone tool that other cognitive tools will leverage to achieve higher levels of cognition. We\u2019ve already seen this in DeepMind\u2019s AlphaZero playing systems where conventional tree search is used in conjunction with Deep Learning. Deep Learning is the wheel of cognition. Just as the wheel enabled more effective transportation, so will Deep Learning achieve effective artificial intelligence.\n\nWe can have wheels made of stone, wheels crafted from wood and wheels with inflatable rubber tires, yet they are all round. There are of course alternatives to wheels for land transportation (i.e. skis, hovercrafts, maglevs and hyperloops) but few will have the practicality of the conventional wheel. Deep Learning are an instance of an intuition machine and intuition is the wheel for higher level cognition and not yet another tool. There is no other cognitive mechanism that we are aware of other than intuition than can give us general intelligence (GOFAI has failed us for decades because it assumed that rational cognition was the basis of intelligence). Marcus\u2019 criticisms are analogous to saying wheels made of stone aren\u2019t any good because they are difficult to create, aren\u2019t perfectly round and don\u2019t provide any cushion. However, the real problem is that the human mind is not an \u201cAlgebraic Mind\u201d as the title of his book proclaims. Marcus will just have to get over himself and come to the realization that he\u2019s been wrong since 2001. To build AGI you first work on intuition and then you work up the stack and not the other way around:\n\nAll of the innate cognitive machineries of the human brain are intuition based components. Unlike our digital computers, there are no logical components. Our rationality comes from learning through experience and is not some hardwired built-in machinery.\n\nOver time, we will develop more advanced forms of Deep Learning. The learning algorithm will change from one that is meta-learning driven. The simplistic neurons will change into kinds with multiple thresholds and of more complexity. There\u2019s really no looking back here. The methods of Deep Learning are being established and refined. Knowledge discovery requires search, and search has two extremes: exploration and exploitation. The solution of the future will of course be an algorithm that understands the best balance between the two.\n\nFrom my own perspective, the path toward Artificial General Intelligence (AGI) is clear. I acknowledge all of the short comings that Marcus points out, however, without a doubt, the methodology and techniques being invented by the Deep Learning research community are slowing chipping away at the problem. To quote the stonecutter credo:\n\nThe true game that is being played by Gary Marcus (which he successfully parlayed into an acquisition of his firm Geometric Intelligence by Uber) is in criticizing the dominant AI paradigm of today. By pointing out its flaws, he\u2019s able to convince lesser knowledgeable investors of an alternative and perhaps more profitable path. Investors take great pride in having contrarian investment strategies. Investors, like most humans, would like to believe that their success was based on their own individuality and not just plain luck. In a majority of all investors, it just happens to be the latter.\n\nPolitics in science has always been present and it\u2019s not going to disappear any time soon. We are familiar with the feud between Nicholai Tesla and Thomas Edison. Edison died a wealthy man, in stark contrast to Tesla who died penniless. Although, the scientific contributions of Tesla arguably surpass Edison\u2019s, Edison is famous more today and Tesla is likely only well known because an electric car company is named after him. The Canadian conspirators have successfully parlayed their Deep Learning meme to great effect. Jurgen Schmidhuber was justified to have felt like Tesla in a world which seemed to have overlooked his own contributions.\n\nThe game is also played by DeepMind, if you read the AlphaGo Zero paper (the most significant development in AI since DL) you will find that DeepMind never uses the term \u201cDeep Learning\u201d. That is because they intend to change the narrative. DeepMind discovered something extremely significant that differs from Deep Learning in its original conception. Unfortunately, DeepMind has not figured an appropriate term for the \u201cself-play\u201d they discovered, so they awkwardly call it \u201cReinforcement Learning\u201d. Yann LeCun is a little bit more savvy with branding, that\u2019s why he came up with \u201cPredictive Learning\u201d to describe a yet to be discovered solution to unsupervised learning. (Note: A recent note by LeCun mentions that he wants to re-brand Deep Learning as Differential Programming)\n\nWe all strive not to be just another brick in the wall in the invention of \u201cthe last invention of man\u201d. I just finished watching the AlphaGo film on Netflix. It\u2019s amazing that DeepMind had the foresight to ensure that this event would be captured in film. The stars of this movie are of course Demis Hassibis and David Silver. However, just like how Stan Lee has a cameo role in every Marvel film, some grainy footage of Sergey Brin had to be spliced in. Eric Schmidt also had his own cameo role but it didn\u2019t look contrived. ;-)\n\nHas Deep Learning hit a wall? Very far from it. 2018 as predicted will be a banner year. The notion of a wall that will stop Deep Learning progress is at best a boogeyman argument that is not only imaginary but also misleading. You have the choice to agree with Marcus\u2019 arguments and wait for some unknown that is supposedly better, or you can recognize the path to AGI has been clearer than it has ever been and use the methods that have lead to remarkable success in recent years."
    },
    {
        "url": "https://medium.com/intuitionmachine/10-fearless-predictions-for-deep-learning-in-2018-bc74a88b11d9",
        "title": "10 Alarming Predictions for Deep Learning in 2018 \u2013 Intuition Machine \u2013",
        "text": "I\u2019ve got this ominous feeling that 2018 could be the year when everything just changes dramatically. The incredible breakthroughs we saw in 2017 for Deep Learning is going to carry over in a very powerful way in 2018. A lot of work coming from research will be migrating itself into everyday software applications.\n\nAs I\u2019ve done last year, here are my predictions for 2018. (For reference, here are my 2017 predictions and a recap)\n\nMany Deep Learning hardware startup ventures will begin to finally deliver their silicon in 2018. However, these startups will mostly be busts because they will forget to deliver good software to support their new solutions. These firms have hardware as their DNA. Unfortunately, in the DL space, software is just as important. Most of these startups don\u2019t understand software and don\u2019t understand the cost of developing software. Thus, these firms may deliver silicon, but nothing will ever run on them!\n\nThe low hanging fruit that employs systolic array solutions has already been taken, so we won\u2019t have the massive 10x performance upgrade that we found in 2017. Researchers will start using these tensor cores not only for inference, but also to speed up training.\n\nIntel\u2019s solution will continue to be delayed and will likely disappoint. The record shows that Intel was unable to deliver on a mid-2017 release and it\u2019s anybody\u2019s guess when they will ever deliver. It\u2019s late and it\u2019s going to be a dud.\n\nGoogle will continue to surprise the world with its TPU developments. Perhaps Google gets into the hardware business by licensing their IP to other semiconductor vendors. This will make sense if they continue to be the only other real player in town other than Nvidia.\n\n2. Meta-Learning will be the new SGD\n\nA lot of strong research in Meta-learning appeared in 2017. As the research community collectively understands meta-learning much better, the old paradigm of Stochastic Gradient Descent (SGD) will fall in the wayside in favor of a more effective approach that combines both exploitive and exploratory search methods.\n\nProgress in unsupervised learning will be incremental, but it will be primarily driven by Meta-learning algorithms.\n\nGenerative models are going to find themselves in more scientific endeavors. At present, most research is performed in generating images and speech. However we shall see these methods being incorporated in tools for modeling complex systems. One of the areas where you will see more activity is in the application of Deep Learning on economic modeling.\n\nAlphaGo Zero and AlphaZero\u2019s \u201clearn from scratch self play\u201d is a quantum leap. In my opinion, it is of the same level of impact as the discovery of Deep Learning. Deep Learning discovered universal function approximators. RL self-play discovered universal knowledge creation. DeepMind doesn\u2019t have a word for this yet, but another research group employs the dual process model of cognition to explain it and calls it \u201cExIt\u201d.\n\nDo expect to see a lot more advances related to self-play.\n\nThis is my most ambitious prediction. The semantic gap between intuition machines and rational machines will be bridged (if it is not already been bridged). Dual Process Theory (the idea of two cognitive machinery, one that is model-free and the other that is model-based) will be the more prevalent conceptualization of how new AI should be built. The notion of Artificial Intuition will be less of a fringe concept and be a more commonly accepted idea in 2018.\n\n6. Explainability is Unachievable. We will just have to Fake It\n\nThere are two problems with explainability. The more commonly known problem is that the explanations have too many rules that a human cannot possibly grasp. The second problem, which is less known, is that a machine will create concepts that will be completely alien and defy explanation. We already see this in the strategies of AlphaGo Zero and Alpha Zero. Humans will observe that a move is unconventional, however, they simply may not have the capacity to understand the logic behind the move.\n\nThis, in my opinion, is an unsolvable problem. What will happen instead is that machines are going to become very good at \u2018faking explanations\u2019. In short, the objective of explainable machines is to understand the kinds of explanations that a human can be comfortable with or will understand in an \u201cintuitive\u201d level. However, a complete explanation will, in a majority of cases, be completely inaccessible to humans.\n\nProgress in explainability in Deep Learning will be made by creating \u2018fake explanations\u2019.\n\n2017 was already difficult for people following Deep Learning research. The number of submissions in the ICLR 2018 conference was around 4,000 papers. A researcher would have to cover 10 papers a day just to catch up with this conference.\n\nThe problem is worsened in this space because here, the theoretical frameworks are all works in progress. To make progress in the theoretical space, we need to seek out more advanced mathematics that can give us better insight. This is going to be a slog simply because most Deep Learning researchers don\u2019t have the right mathematical background to understand the complexity of these kinds of systems. Deep Learning needs researchers coming from complexity theory, however there are very few of these kinds of researchers.\n\nAs a consequence of too many papers and poor theory, we are simply left with the undesirable state of alchemy that we find ourselves today.\n\nWhat is also missing is a general roadmap for AGI. The theory is weak, therefore the best we can do is create a roadmap with milestones that relate to human cognition. We only have framework that originate from speculative theories coming from cognitive psychology. This is a bad situation since the empirical evidences coming from these fields are spotty at best.\n\nDeep Learning research papers will perhaps triple or quadruple in 2018.\n\nThe road to more predictable and controlled development of Deep Learning systems is through the development of Embodied Teaching environments. I have discussed this in a little bit more detail here and here. If you want to find the crudest form of teaching technique, then one only has to look at how Deep Learning networks are trained. We are all due a lot more progress in this area.\n\nExpect to see more companies revealing their internal infrastructure as to how the deploy Deep Learning at scale.\n\nThe way we measure progress towards AGI is antiquated, thus a new kind of paradigm that addresses the dynamic (i.e. non-stationary) complexity of the real world is demanded. We shall see more coverage in this new area in the coming year. I will be speaking about this new Conversational Cognition paradigm in Amsterdam ( March 1\u20132, Information Energy 2018).\n\nThe demands for more ethical use of Artificial Intelligence will increase. The population is now becoming more aware of the disastrous effects of unintended consequences of automation run amok. Simplistic automation that we today find in Facebook, Twitter, Google, Amazon etc. can lead to unwanted effects on society.\n\nWe need to understand the ethics of deploying machines that are able to predict human behavior. Facial recognition is one of the more dangerous capabilities that we have at our disposal. Algorithms that can generate media that is indistinguishable from reality is going to become a major problem as well. We as a society need to make the rapid transition to begin demanding AI to be used solely for the benefit of society as a whole and not as a weapon to increase inequality.\n\nExpect to see more conversation about ethics in the coming year. However, don\u2019t expect any new regulations. Policy makers are still years behind the curve in understanding the impact of A.I. to society. I don\u2019t expect them to stop playing politics and start addressing the real problems of society. The U.S. population have been victims of numerous security breaches, yet there is zero new legislation or initiatives to address this serious problem. So don\u2019t hold your breath on our dear leaders suddenly discovering new found wisdom.\n\nThat\u2019s all I have for now. 2018 is going to a major year and we all better strap on our seat belts and prepare for impact!"
    },
    {
        "url": "https://medium.com/intuitionmachine/revisiting-10-predictions-for-deep-learning-in-2017-92d9e70835e0",
        "title": "Retrospective on 10 Predictions for Deep Learning in 2017",
        "text": "Last year, I wrote my predictions for Deep Learning in 2017. I will recap those prediction and present new predictions for the coming year.\n\nHere\u2019s the recap for those the 2017 predictions. Refer to the predictions article for more detail.\n\nNvidia continues to dominate as predicted. They\u2019ve added the Volta V100 on to their lead which includes a 110 teraflop tensor core component.\n\nI expected Intel to show up in mid-2017, however they have not shown up at all with the exception of a 1 teraflop Movidius embedded deep learning chip. Intel had a great party in NIPS 2017 that unveiled the hardware. However, without any details on performance, it\u2019s clear to me that this is more vapor than real.\n\nAmazon\u2019s FPGA based cloud instance did not go anywhere. As expected, the upfront time investment to use FPGA is too steep a price to pay for anyone.\n\nGoogle revealed details of their TPU chip and then a few months later, unexpectedly revealed the TPU 2 system that is able to crank out 180 teraflops in a single module.\n\nAMD delivered their Vega architecture in the middle of the year with fp16 and int8 capability. The specs for a single card are at 25 teraflops fp16 and 50 teraops at int8. This is competitive for novel or unique loads, however it is hard to compete with a dedicated tensor core as found in Nvidia\u2019s V100 and Google\u2019s TPU. AMD support for Deep Learning ROCm frameworks still needs improvement, however they are incrementally moving forward.\n\nCNNs have emerged in areas where RNNs have been used as well in planning prediction. RNNs have not completely disappeared in the landscape. Geoffrey Hinton unveiled his Capsule Network that is designed to address the many flaws of CNNs.\n\nThere\u2019s been no progress in differentiable memory networks. The early work on this appears to have died down and there seems to be less interest in creating Turing-like machines based on Deep Learning.\n\n3. Designers will rely more on Meta-Learning\n\nA lot of strong research in Meta-learning appeared in 2017. The most impactful development in this field comes from the MAML research where training is performed across diverse tasks. There have been plenty of discoveries using brute force search, which is a kind of meta-learning.\n\n4. Reinforcement Learning will only become more creative\n\nReinforcement learning has exceeded expectations with AlphaGo Zero and AlphaZero. Despite the concerns of many practitioners that RL can\u2019t scale due to its huge appetite for data, the most impressive developments in Deep Learning can be found in this area.\n\n5. Adversarial and Cooperative Learning will be King\n\nGANs flourished in 2017. Its practical use is still limited. However, AlphaGo Zero and AlphaZero self-play have shown that a game playing paradigm can somehow bootstrap knowledge from scratch.\n\n6. Predictive Learning or Unsupervised Learning will not progress much\n\nIf you consider the self-play of AlphaGo Zero and AlphaZero as unsupervised learning, then my prediction of a lack of progress is entirely incorrect. However, if self-play is the key to unsupervised learning, then this is a massive quantum leap development and is truly unexpected!\n\nWe haven\u2019t developed good enough methods to allow our models to be transferred more easily into different domains. Deep Learning methods are very far from being mature industrialized methods. The most impactful development in transfer learning is the amazing work done by an Nvidia team to create stunning high resolution images using GANs.\n\n8. More Applications will use Deep Learning as a component\n\nWe already saw this in 2016 where Deep Learning was used as a function evaluation component in a much larger search algorithm. AlphaGo employed Deep Learning in its value and policy evaluations. Google\u2019s Gmail auto-reply system used DL in combination with beam searching. I expect to see a lot more of these hybrid algorithms rather than new end-to-end trained DL systems. End-to-end Deep Learning is a fascinating area of research, but for now hybrid systems are going to be more effective in application domains.\n\nA revealing presentation (NIPS 2017, December) by Jeff Dean of Google shows that Deep Learning as a component is finding itself in its uses as indexing structures. The paper is \u201cThe Case for Learned Index Structures\u201d. I had not expected this, but it portends even greater ubiquity of Deep Learning in software applications.\n\nThis has not happened. The field is growing at an extremely rapid pace and our understanding of how these systems work keeps on changing. Nobody seems to have the luxury to create a clean set of design patterns that can guide the further maturity and industrialization of these techniques. Progress may need to plateau first before we can even considering curating and organizing our knowledge.\n\nA keynote talk in NIPS 2017 finally branded Deep Learning as what it really is \u2014 \u201calchemy\u201d. This was a problem in 2016 and it wasn\u2019t addressed in 2017. However, perhaps better theories will come out in 2018 given that a lot of people were sympathetic to Rahimi\u2019s talk.\n\nNaftali Tishby had an elegant theory based on information theory, only to be questioned by a paper submitted in ICLR 2018. Tomaso Poggio also came up with a set of papers that addresses Deep Learning fundamentals. The \u201cRethinking Generalization\u201d paper hasn\u2019t been resolved. There have been some approaches were adversarial inputs aren\u2019t present.\n\nI am working on my 2018 predictions, so please stay tuned."
    },
    {
        "url": "https://medium.com/intuitionmachine/building-a-270-teraflops-deep-learning-box-of-under-10-000-2d790b0ae2ec",
        "title": "Building a 270* Teraflops Deep Learning Box for Under $10,000",
        "text": "Look what I got for Christmas!!! If you don\u2019t recognize it, it\u2019s two Titan V cards from Nvidia. A single Titan V has a systolic array unit dubbed as a TensorCore that is capable of 110 teraflops peak performance. In addition, it includes a conventional GPU that\u2019s capable of 25 teraflops half-precision. This means, we are speaking here of roughly 135 teraflops (half-precision) per card. This makes a grand total of 270 teraflops for a box with these two cards inserted in it. We don\u2019t even have to count the relatively minuscule extra flops that a multi-core CPU provides. (*Editor\u2019s note: I will have to update these theoretical numbers when I dig up more details.)\n\nEach Titan V costs $3,000.00 + plus taxes. So this gives around $3,700 wiggle room to come up with a decent box to host these cards on. I recently built a 50 teraflops box for under $3,000, which comes out to 16.6 gigaflops per dollar. This new box should give you a mind-boggling, 27 gigaflops per dollar. Just to make a comparison, a late model Intel i7 8700K cranks out 217 gigaflops. An i7 8700k costs $400, the math comes out to .53 gigaflops per dollar. Granted the numbers here are theoretical vs empirical, it is still a massive difference!! (BTW, I could have placed these Titans on the same box as my 50 teraflops box and it would cost around $7,300. Equating to 36 gigaflops per dollar.)\n\nIn June of 2007, IBM\u2019s Blue Gene/P was installed at Argonne National Laboratory which is capable of 445 teraflops (double precision). Two years earlier, Blue Gene/L was the fastest supercomputer in the world at 280 teraflops. Back in 2007, IBM was charging $1.3M per rack. 20 of these racks gets you to around 280 teraflops (it will set you back $26 million). You might be saying: \u201cwell hold on now, you are comparing double precision with half-precision which isn\u2019t fair\u201d. Honestly, I don\u2019t care because Deep Learning workloads really don\u2019t care much for higher precision.\n\nThe Blue Gene/P monster of a machine looked like this back in 2007 (just 10 years ago):\n\nNow imagine all this computational horsepower sitting quietly (water cooled) underneath your desk, all in a single box and costing 2,600 times less (i.e. $26,000,000 versus $10,000). This doesn\u2019t even include the cost of power. There\u2019s no need for a battalion of folks to install and maintain this monstrosity. There\u2019s no need to wear a shirt, tie, slacks and shoes to work on this! Think about how potentially world dominating this can be ;-).\n\nThere are two architectural developments that got you this massive increase in flops in a very short time. (1) The use of fp16 using less silicon than fp32 or fp64 multiply-add accumulators and (2) systolic arrays that gets you 110 teraflops for the same amount of silicon that got you around 10 teraflops. In 2016 you could get less than 10 teraflops per GPU chip, fast-forward to 2017 and its quantum leap to 135 teraflops with a V100 GPU. I don\u2019t expect 2018 to yield this kind of leap in capability. The low hanging fruit has already been picked and only needs to be exploited by software.\n\nThe next big leap perhaps may be the kind of architecture GraphCore is touting. Here are some intriguing benchmarks from GraphCore. If I were to gaze at my crystal ball, Google is going to stun the world again with a new kind of architecture in silicon. Better Deep Learning algorithms are feeding back into more capable silicon. This is what Elon Musk has coined as \u201cdouble exponential growth\u201d. Deep Learning progress is moving at break-neck speed!\n\nThis kind of comparison in terms of size and cost gives a visceral feel to the kind of changes that are coming. How many businesses are still running their operations the same way that it was 10 years ago? This kind of exponential change in compute capability has got to mean a massive change in how we run our daily operations. It\u2019s likely that 99.99% of the people out there don\u2019t even realize what\u2019s happening! When I mention \u201cDeep Learning\u201d to people, most folk\u2019s eyes glaze over. Don\u2019t even mention the term \u201cIntuition Machine\u201d, it sounds like an oxymoron.\n\nI\u2019m waiting for a water cooled AMD Threadripper box custom built by a reputable vendor of workstation class desktops. This will give me the opportunity to kick the tires on this kind of an intuition machine!\n\nHere\u2019s the AMD ThreadRipper-based machine with two Nvidia Titan V\u2019s:\n\nThe motherboard is an ASUS Prime-X 399-A reviewed here by AnandTech. The key feature why this board was selected is that it is equipped with four PCIe 3.0 x16 slots. This should provide sufficient bandwidth for the CPU to feed data to the Titan V GPUs. Furthermore, the motherboard supports NVMe U.2 (supporting up to four PCIe 3.0 lanes) for faster SSD-based storage access in contrast to SATA III. Note the diagram below shows the ThreadRipper chipset capable of 3 NVMe devices on a RAID configuration. The main CPU is a Ryzen ThreadRipper 1900x (8 Core, 3.8GHz). This configuration has sufficient I/O for the workloads required for Deep Learning. This can be seen by the graphic below:\n\nThe primary reason for an AMD based motherboard and not an Intel one is the I/O support. A Threadripper offers more PCIe lanes than the Core i7\u20137820X (60+4 versus 28). Two Nvidia Titan Vs require 32 lanes, this immediately exceeds the capability of a late model Core i7. This ThreadRipper motherboard is more than capable of supporting 4 Titan V cards!\n\nThis is a water cooled CPU so it\u2019s cool as a cucumber and extremely quiet:\n\nI employ a P600 Nvidia graphics card to avoid a graphics load on the Titan Vs. The Titan Vs aren\u2019t water cooled, but I hope to see that in the future. My hardware supplier has their cable management down to a science. All in all, this is not only a very capable machine, but one that is absolutely beautiful.\n\nFor comparison purposes, one can compare against Nvidia\u2019s DGX Station. The DGX station is listed at $49,900. DGX has 4 Tesla V100 with an Intel Xeon E5\u20132698 v4 2.2GHz (20-Core) capable of 500 teraflops. The E5\u20132698 has only 40 PCIe lanes, so it really can\u2019t support the theoretical maximum bandwidth of 4 Titan Vs (64 lanes). Our system however can be upgraded by adding 2 more Titan Vs and the 16 core 1950x at 3.4GHz. We are not going to shell out $49,900 to run a bake-off, but you can look at the theoretical numbers and realize that we have a compelling solution. Our system thus is arguably quite competitive with the best Deep Learning system that is out there.\n\nNow it\u2019s time to run this machine on our Deep Learning software stack. Please stay tuned!\n\nEmail info@intuitionmachine.com if you are interested in acquiring a similar or upgraded machine.\n\nYou might be wondering, can I mine cryptocurrency with this? Yes, of course, that\u2019s when Intuition Fabric comes out!"
    },
    {
        "url": "https://medium.com/intuitionmachine/economic-modeling-and-deep-learning-dcd61b351cad",
        "title": "Automating High-Level Economic Thinking using Deep Learning",
        "text": "The explosion in big data and Deep Learning (DL) has delivered many success cases across various systems such as autonomous driving cars, image recognition, natural language programming (NLP), handwriting recognition, and scientific exploration such as the study of galaxies, experimental high energy physics or molecular drug design; however, the application of these technologies has been limited to help us solve some of the most persistent social and economic challenges of our times. How do we bridge the gap between economic decision-making and the state-of-the-art data analytics found in the field of Deep Learning? This article explores this area in greater depth from the perspective of policy and investment decision-makers.\n\nEconomic decision-making requires highly non-linear, complex, and dynamic thought process. Take the case of facial recognition tasks found in vision problems. People\u2019s faces do not change over time. On the contrary, economic systems are embedded with colossal number of dynamic components including agents, institutions, and constructs that vary over time. Many of these components are unaccounted for in statistics. Furthermore, there are dependencies that are not well understood. For example: a political crisis in one country could trigger an economic crisis in another country; the global financial crisis in 2008 brought down the global economy which stemmed from one component such as complex credit default swaps in the US mortgage industry. The impacts of one \u201ctoo big to fail\u201d entity i.e. Lehman Brothers resulted in a cascade of failures that resulted in the loss of more than 7 million jobs and $22 trillion from the global economy, the impacts of which the world is still dealing with today, a decade later.\n\nThe positive attribute of deep neural networks is that they produce highly non-linear approximation functions between the input and the output layer that could be useful for highly complex tasks, however, the uncertainty in DL approach is that we have no idea between the connection between nodes and hidden layers. On one hand, simple linear regression has very clear interpretability but have terrible accuracy in such instances. DL provides state of the art accuracy but terrible interpretability, in its current form. These trade-offs make decision-makers skeptical of adopting DL in their domains.\n\nDL may offer opportunities to enhance economic modeling capabilities. Standard economic models can be grouped in broadly two categories. First, models based on equilibrium conditions such as Dynamic Stochastic General Equilibrium (DSGE) and Computable General Equilibrium (CGE) Models. In these models, there are considerable assumptions including marketing clearing conditions. However, in reality, such complex, non-linear system such as the world economy does not operate an equilibrium where supply and demand meets. The second approach that analysts take is a data driven approach, traditionally, Gaussian state space models such as Kalman Filters, VAR, GARCH, ARMA, etc. In such approaches, there is a limitation of an n-order markov chain process with short, or long-term time-dependent feedbacks. These methods are so widely used that adopting any new methods will require a shift in organizational culture and perception.\n\nThese traditional statistical approaches have other limitations too: (a) high-dimensional non-linear data does not exist and cannot be used in such instances; (b) heavy parameterization based on prior beliefs makes them not very scalable and dependent on our belief system; (c) last but not the least, all of these models have largely failed to accommodate the complexity of the real-world. These modelling approaches have failed to predict important economic events, such as the global financial crisis of 2008, or many large-scale structural shifts in economies, failures to accurately represent mass-movements in nations or financial markets. For example, a recent Economist article (A mean feat) showed that the random number generator performed marginally better than IMF DSGE model forecasts for country level GDP growth forecasts.\n\nIn a previous article about Embodied Learning, we mentioned Judea Pearl\u2019s classification of different capabilities of causal analysis. Most of machine learning is stuck at Level 1 where there is a static presentation of data and learning that is motivated by the notion of curve fitting and optimization. It is just ridiculously primitive, yet a majority of data science and statistics is based on this primitive notion. However, with Deep Learning, we can move to a more advanced form of data analytics. If you look at the levels, you will see that Judea Pearl\u2019s level 3 classification is exactly what a scientist does to analyze data. In Deep Learning, we are essentially automating this Gednaken experiment. Of course, we are still a long way from reaching the general intelligence of the human mind. However, this prescription with regards to machine learning can give you an idea of where Deep Learning is presently in relationship to the more traditional level.\n\nIt is also instructive to understand that there exists a spectrum of automation and that it is illuminating to distinguish the different varieties. For this, we can learn from the Society of Automation Engineering (SAE). SAE has an international standard which defines six levels of driving automation (SAE J3016). This can be useful in classifying the levels of automation in domains other than self-driving cars. A broader prescription is as follows:\n\nThe absence of any automation.\n\nUsers are aware of the initiation and completion of the performance of each automated task. The user may undo a task in the event of incorrect execution. Users, however, are responsible for the correct sequencing of tasks.\n\nUsers are aware of the initiation and completion of a composite of tasks. The user however is not responsible for the correct sequencing of tasks. An example will be the booking of a hotel, car and flight. The exact ordering of the booking may not be a concern of the user. However, failure of the performance of this task may require more extensive manual remedial actions. An unfortunate example of a failed remedial action is the re-accommodation of United Airlines\u2019 paying customer.\n\nUsers are only notified in exceptional situations and are required to do the work in these conditions. An example of this is in systems that continuously monitor security of a network. Practitioners take action depending on the severity of the event.\n\nUsers are responsible for defining the end goals of automation, however all aspects of the process execution as well as the handling of in-flight exceptional conditions are handled by the automation. The automation is capable of performing appropriate compensating action in events of in-flight failure. The user however is still responsible for identifying the specific context in which automation can be safely applied to.\n\nThis is a final and future state where human involvement is no longer required in the processes. This of course may not be the final level because it does not assume that the process is capable of optimizing itself to make improvements.\n\nThis is an automation that requires no human involvement and is also capable of improving itself over time. This level goes beyond the SAE requirements but may be required in certain high performance competitive environments such as Robocar races and stock trading.\n\nIn general, we can apply Deep Learning technologies in different levels of automating the process of economic analysis. Each level requires greater levels of sophistication to perform, however the above levels is a good map to find where we can introduce automation in our own workflow process.\n\nDeep Learning automation can also be classified in terms of providing assistive or generative capabilities to its users. A good example of assistive automation is the auto-focus capability that you find in today\u2019s cameras. An example of generative automation is the artistic style transfer apps like Prisma that you find in your smart phone. So in the context of economic modeling, Deep Learning can assist in the exploration of data as well as provide more accurate model simulations using its generative capabilities. Thus, evaluating Deep Learning automation involve multiple dimensions.\n\nDL has been used in current practices from perhaps this \u201cshallow learning\u201d perspective of Level 1 or Level 2 autonomy for these real-world economic cases of interest. They have been used for examples in (a) regression problems: predictions for financial asset classes or broader macroeconomic outcomes, (b) classification and labelling problems: recent studies for mortgage risk (DL for Mortgage Risk), and (c ) for proxy indicators: satellite imagery data has been used to map local area poverty estimation (Stanford Study), or night-lights to asses power outages, or twitter data to asses traditional economic variables such as unemployment rate or track the contagion of viral flu epidemic etc..\n\nRecent research at Google\u2019s DeepMind has explored the problem of resource allocation using Deep Learning methods. Economics at its very essence requires the presence of scarcity and thus is about the optimization of resource allocation.\n\nThere are many pressing resource-allocation challenges decision-makers care about. For example, a policy-maker wants to mitigate risks of climate-change and wonders which location, technologies, and communities he/she shall invest in to make the country resilient to climate impacts. Investors that have multi-billion-dollar assets in locations around the world want to monitor how their asset might be affected by potential economic, financial, or political risks. Banks want to have an early warning signal of an impending financial heating up to have hedging or arbitrage opportunities.\n\nMore importantly, economic-thinking people are concerned with accounting for asymmetric information (certain agents have more information about the market than others), causality or endogeneity (which factors are influencing the output internally within the workings of the model system), or multicollinearity (dependencies with the multi-variate independent variables that may lead to spurious correlations and unreliable model forecasts). In order for DL to reach broader audience of decision-makers who care about such important economic challenges, there remain many missing links. First, there is no large enough training data set for real-world economic cases that is available. Second, labels are not clearly defined in economic cases unlike image recognition tasks that can have human annotated labels. Third, DL framework in its current form does not address concerns of causality, endogeneity, multicollinearity etc..\n\n \n\n There is a nascent yet growing body of literature that is exploring this new area of research of causal analysis in the confines of deep neural networks. Functional Causal Model (FCM) which is a generative model, as well as structured variational approximators parameterized by recurrent neural networks for nonlinear state space models are some new approaches that open a window for such tasks. Such model innovations have significance for high-level decision-making that is even less well understood. It has been shown that human decision-making is limited by computational complexity, and given the resource constraints imposed on decision-makers, such new model approaches will require new theories of decision-making, that do not exist currently.\n\nGiven these limitations of DL, there is still hope for DL to address these challenges by providing a completely new paradigm to view some of the challenges society faces. Economic theory suggests that AI will raise the value of human judgement: as predictions become cheaper, human beings will have the opportunity to exercise greater weighting of costs and benefits to make better judgement (see Aggarwal et al 2017). These structural changes in computational capacity, large-scale data, and DL will not exclude human beings, but perhaps give power to high level decision makers strengthening pre-existing norms and organizational culture.\n\nDL will transform monitoring technologies for decision-makers interested in various economic resource-allocation problems. But the real question is, will DL resources end up further polarizing the \u201chaves\u201d and \u201chave-nots\u201d or actually provide a paradigm shift, where democratizing AI will work to bridge the gap between such inequalities?\n\nWait for our next articles as we share with you some of our ongoing R&D and ground-breaking results."
    },
    {
        "url": "https://medium.com/intuitionmachine/a-rich-conversation-on-the-complexities-of-human-compatible-a-i-2245159f1360",
        "title": "A Rich Conversation on the Complexities of Human Compatible A.I.",
        "text": "Thanks to Capital One for sponsoring this post.\n\nThe day began with an insightful keynote by Kate Crawford who spoke out about the intrinsic biases we find in the data we use to train AI. Her talk can be found on YouTube:\n\nThis keynote served as the perfect setup to begin asking Deep Learning researchers about the role of ethics in their work. People who attend the annual Neural Information Processing Systems (NIPS) conference are typically researchers in the field of neural networks (i.e. Deep Learning). They are folks who work down in the trenches exploring the theory and the \u201calchemy\u201d of improving algorithms. I suspect that most researchers will spend only a fractional amount of their cognitive capacity on thinking about ethics.\n\nCapital One took the initiative to invite several attendees, including top researchers and engineers from the world\u2019s leading technology firms and innovation hubs \u2014 and myself \u2014 to a round table dinner to have a conversation about privacy, explainability, and ethics in AI. That is, to discuss the interplay between humans and the AI future we are all building.\n\nThis dinner was held at Michael\u2019s on Naples. Naples is a curiously unique part of Long Beach (where the NIPS conference was being held). I came a bit early and had the opportunity to take some fantastic photographs while strolling through the neighborhood (see above photo). Honest, free flowing ideas (and wine!) contributed to the rich discussion. The following are highlights from the main themes the group discussed over the course of the evening.\n\nOne of Fortune\u2019s editors was in attendance to kick off the discussion. He described his own personal frustration interacting with the automation behind Facebook. He told us of his interaction with Facebook as it tracked his relationship with his wife and the changes in that relationship, from becoming engaged to becoming married. He noticed, through these life changes, Facebook\u2019s advertising never changed for him. While in stark contrast his fiance (and now wife) changed from advertising for \u201cdresses and venues and all this stuff\u201d and on to baby related products. His personal frustration was that although he felt that he was equally invested in his relationship, Facebook did not adjust appropriately to his life changes. Why does this go wrong all the time?\n\nThe Cost for Exploration vs Exploitation\n\nLearning in the most abstract sense involves exploitation or exploration.\n\nOne guest, a machine learning researcher, remarked that there is a problem with optimization driven by A/B testing that converges to local optima. Businesses will not roll out features that increase exploration in exchange for advertising revenue. Advertising is (in general) not good at exploration.\n\nAnother deep learning researcher expressed similar sentiment. When it is expensive to make a mistake in your decision then a greedy option is required. A basic understanding of the world may be all that is needed to make money. However, explorations need to be made such that one can make better decisions.\n\nIn general there will always be a cost trade-off with creating approximate (but good enough) models of the world and ones that are more accurate.\n\nThe objectives that AI are designed to pursue may not be aligned with our human needs.\n\nOne guest questioned whether engagement objects are truly serving us. Cat videos drive up engagement and \u201cwe love to rot our brains.\u201d Have we ended up spending time that we wished were spent in a better way? Are we at the point where we should optimize for metrics that have different objectives? That is, should we not take into account our long-term goals instead of our immediate desires?\n\nMany people want to quit smoking but cannot figure out how to. Should Facebook give the user an option to ask for less cats?\n\nA more explorative interaction with our AI may allow for a deeper conversation that can reveal how we all want to truly spend our time. The guest mentioned an interesting website (http://www.timewellspent.io ) which explores how our society is being hijacked by technology.\n\nThe consequences of AI-based recommendations may simply be a decision that falls into a gray area.\n\nOne guest, a machine learning researcher focused on ethics and ML for good, remarked that it is unclear what the ideal outcome should be. Many males perhaps don\u2019t care about the advertising served to them and defer to their partners regarding purchasing decisions. One\u2019s optimal outcome will be very different from the stereotype measure. However it would be wonderful if we can signal to the AI this difference. These value driven metrics are difficult to quantify when they succeed or fail.\n\nThere are systems whose consequence may be much worse than not being served the right advertising. Take for example the \u201cTrolley Problem\u201d, which involves the decision of life and death.\n\nOne can use online ads to influence people to make decisions (that may be against their own benefit), remarked another seasoned researcher and engineer at the table. You can look at this as an optimization problem where one measures actions of people to predict behavior. You can keep tuning this by observing behavior. AI allows you to do this at scale. You can perhaps get anyone to believe anything if your AI is smart enough. Information that we consume is influential to who we are and how we act. Facebook and Twitter have an immense power over us. Perhaps the reason why this may be acceptable, is that they may be very bad AI. Unfortunately, they are incrementally becoming better.\n\nOther guests pushed back, saying we should decouple this fear of advanced technology. Facebook allows advertisers to target people using basic information. We assume sophisticated technology for malice when in fact it may not be required. The problem is we allow entities to target users without any proper oversight.\n\nOn another note, one of the guests asked whether the others had read the State Council of China\u2019s guidelines for AI development. The plan is by 2020 China can catch up with world level AI and by 2030 dominate all areas of research and commerce. This particular guest had read other government plans and believed this to be \u201cthe best articulated\u201d plan. The plan encourages the use of credit metric as a proxy to predict citizens thinking and behavior. This use of \u201csocial credit\u201d is explored in greater detail in \u201cChina\u2019s Social Credit System: AI-driven panopticon or fragmented foundation for a sincerity culture?\u201d.\n\nAI that Knows More than you Realize\n\nAnother researcher and author posed us the question, \u201cwho should have governance over our data?\u201d The average person is unaware of this and is unaware of how decisions are made on one\u2019s personal data.\n\nIt was added that there are information we constantly volunteer to internet services. There is a lot of information that is shadow information. It is not a single picture, but one that dynamically changes over time. Most people are unaware of how much more is known by these services than they think.\n\nFacebook has become a massive distributed brain simulator. It is disconcerting to imagine what can be achieved if people\u2019s entire psychological profile is captured.\n\nAnother pivotal question that arose was \u201ccan you engineer such that there is no failure? Can you avoid failure? Can you engineer your way around failure points?\u201d Air bags were invented to save lives. Unfortunately because crash dummies were of the male form, air bags were discovered to be harmful to women and children.\n\nMost wondered if this was indeed a solvable problem. We simply cannot create the knowledge to enumerate all the failure cases. Failures are typically going to be edge cases.\n\nOne idea is that more simply, our goal should be to make our systems safe. These edge cases may be just talking points for scenarios that may never happen. The goal of development is to make systems as safe as possible. In short, we try as best we can but we can\u2019t prevent all real or imagined failures.\n\nIt was also observed that humans have outsized fear of events with low probabilities. These new predictive technologies allow us to examine situations that are \u201cdevoid of outsized bias\u201d we find in humans. Furthermore, the edge cases scenarios that may occur may be very different from the scenarios that we are most afraid of. Humans fixate on ideas that may not exist. The scenarios that scare us most may not occur at all.\n\nFor example, in airplanes with autopilot, there is no ethics involved in their design. Rather it is all about safety. The field is highly regulated and we could expect to see this kind of regulation to also develop for self-driving cars.\n\nOne of my fellow dinner guests thought that AI will eventually emerge as a more mature discipline. He gives the example of chemical engineering that arose from scaling up chemistry. Chemical engineering ensures that a 10-ton vat will not blow up. Engineering has devised safeguards and procedures that ensure chemistry can be executed at higher rate. In AI, we are still in the process of figuring this out. We don\u2019t have the engineering for AI. It goes beyond the understanding of the fundamentals and that includes scaling as well as the human component. As some guests would say, we are not yet at the level of maturity reached by other engineering sciences.\n\nIt was also noted that the banking industry is an example of one in which there is already strict regulation. Banks, due to their heavy regulation, already have processes in place to ensure unbiased predictions of their algorithms.\n\nWhile one guest pointed out that mature engineering involves systems with single tasks, in AI we are dealing with the complexities of handling many different tasks. AI may provide us with a framework that we can use to guarantee qualities that may be applied across many different tasks.\n\nHow can we develop AIs that improve the human condition?\n\nSome of our guests speculated about the idea of social networks being used to improve governance of nation states. Can government define new kinds of metrics, perhaps those that can improve democracy? Can society drive a metric that helps everyone?\n\nIn democracies, casting a vote can be expensive. Can we perhaps build more liquid markets by providing citizens a way to delegate their votes? How can we have more involvement by the citizenry? Can society drive the metric to help everyone?\n\nThe book that I had just finished writing \u201cThe Deep Learning AI Playbook\u201d has a final chapter on \u201cHuman Compatible AI\u201d. However, it does not cover as rich a conversation as I had found in this meeting. The wisdom of collective intelligence transcends thinking in areas that a single mind is able to explore. Human compatibility with AI is a vast topic and I am glad Capital One sponsored this dinner with the simple intention of having a conversation about a very important topic.\n\nI must say, the sea bass goes quite well with white wine sprinkled with some random philosophical ideas."
    },
    {
        "url": "https://medium.com/intuitionmachine/embodied-learning-is-essential-to-artificial-intelligence-ad1e27425972",
        "title": "Embodied Learning is Essential to Artificial Intelligence",
        "text": "Jeff Hawkins has a principle that intuitively makes a lot of sense, yet is something that Deep Learning research has not emphasized enough. This is the notion of embodied learning. That is, biological systems learn from interacting with the environment. Hawkins writes:\n\nHawkins believes that the brain learns by interacting with its environment.\n\nThe classic Deep Learning training procedure is one of the crudest teaching methods that one can possibly imagine. It is based on repetitively and randomly presenting facts about the world and hoping that the student (i.e. the neural network) is able to disentangle and create sufficient abstractions of the world.\n\nOne should at least be able to do better by having a curriculum. That is, to present training data that starts easy and then scaling it up to more difficult training. We actually see curriculum learning effectively used in the form of the latest StackGAN architectures. This is where smaller problems are tackled first and the network is incrementally resized to tackle even larger problems.\n\nHowever, biological beings learn quickest by allowing them to interact with the environment. In other words, rather than just having a teacher who defines a rigid curriculum, one allows the student to drive their own exploration of the teaching material. There is no better way to learn a new subject than to allow the student a way to interact with the subject and to discover its responses.\n\nThis is exactly what we see in the advances in cognition that DeepMind has exhibited in its AlphaGo Zero and Alpha Zero game playing machines. If you can set up a teaching environment that adjusts to the capabilities of the student, then the student can comfortably walk up a staircase toward richer understanding.\n\nJudea Pearl has written a paper that explores the theoretical impediments of the current form of machine learning where he writes:\n\nOf higher forms of learning from an environment. Conventional machine learning is stuck in the first level. Reinforcement learning explores the second level. Pearl proposes a third level that explores a cognition process as something reminiscent of a Gedankenexperiment (known in English as a \u2018thought\u2019 experiment).\n\nPearl argues for the development of counterfactual reasoning as the most advanced form of cognition:\n\nPearl explains why induction-only machine learning systems are incapable of reasoning about actions, experiments and explanations. In short, induction-only machines require a mechanism that can perform imaginative experiments to assess the ramification of different situations. This is reminiscent of the tree search used in game-playing AI.\n\nAnn Pendleton-Jullian and John Seely Brown have a book \u201cPragmatic Imagination\u201d which explores the entire spectrum from perception to free play:\n\nAs you can see from above, the inductive inference found in Deep Learning can be found in the more primitive instances of cognition. There is large spectrum of cognition that needs to be traversed to get to higher intelligence. The end of that spectrum involves a lot of imagination and creativity.\n\nIn the recently concluded NIPS 2017 workshop, Valentin et al. presented a paper \u201cDisentangling the independently controllable factors of variation by interacting with the world\u201d this idea of embodied learning further:\n\nThe team devises a new kind of objective function that is capable of disentangling aspects of environment without the need for an extrinsic reward. Where they report \u201cPushing representations to model independently controllable features currently yields some encouraging success.\u201d This approach however addresses only level 2 in Pearl\u2019s classification. Deep Learning researchers have a ways to go!\n\nDeepMind recently published a position paper (\u201cBuilding machines that learn and think for themselves\u201d) that argues for autonomous machines:\n\nDeepMind observes that the approach argued by Lake et al, is \u201cagnostic\u201d to the use of human engineered \u201cinnate cognitive machinery\u201d. DeepMind argues that the forms of a priori knowledge that should be used to develop intelligent machines should be kept to a minimum. Machines should be able to learn about ambiguous as well as complex domains where a prior knowledge is at a minimum or difficult to capture a priori. Furthermore, machines should have adaptability to handle tasks in contexts related to its previously training. Finally, an autonomous system should have good models as well as the ability to create new models.\n\nArguing the opposite of Lake and colleagues approach. DeepMind argues for an intuition machinery as a substrate as opposed to the GOFAI approach where model-based approach forms the substrate. How a model-based and model-free approach coordinate are described in my earlier article \u201cthe coordination of rational and intuitive intelligence\u201d.\n\nHuman a priori knowledge can be used to drive development through the design of environments that grow (or teach) innate cognitive machinery. The next evolutionary step will be in understanding how to develop these learning environments for cognitive machinery. The current conventional thinking is, \u201chow do I design better architectures and algorithms?\u201d, however the more promising question is \u201chow do I design better learning environments to teach intuition machines?\u201d\n\nThese learning environments should be designed to bring about richer counterfactual thinking, it is through this mechanism that we can eventually create the adaptive general intelligence that we seek."
    },
    {
        "url": "https://medium.com/intuitionmachine/alphazero-how-intuition-demolished-logic-66a4841e6810",
        "title": "AlphaZero: How Intuition Demolished Logic \u2013 Intuition Machine \u2013",
        "text": "Modern civilization and the trappings of technology has lead to the decline of our own intuition. Many of us have become unaware of its value or even its very existence. Intuition as a basis of complex computation is easily dismissed as an approach outside of the conventional. This lack of conventionality leads many researchers to ignore its potential.\n\nThe research that I do in Artificial Intelligence (AI) revolves around the idea that advanced cognitive machines will use intuition as the substrate of its intelligence (see: \u201cartificial intuition\u201d). Our own human minds provide ample evidence for general intelligence. Humans are fundamentally intuition machines and our rational (and conscious) self are just a simulation layered on top of intuition-based machinery (see: \u201ccognitive stack\u201d). This is in stark contrast to Descartes famous saying \u201cI think. therefore I am\u201d (Cogito ergo sum), which implies that our rational thinking is what separates us from all of biology. We thus have a cognitive bias to demand technologies and methodologies that are driven by logical machinery. This is indeed the reason for multi-decade failure of Good Old Fashioned AI (GOFAI) which attempted to solve the problem of intelligence from formal logic as its starting point.\n\nOne of the counter-intuitive predictions of intuition based machines is \u201chow can logical thought arise from intuition machines?\u201d Since 2012, we have seen the incredible advances of Deep Learning technology. Deep Learning networks are intuition machines. These systems learn to perform inference (or make predictions) by using induction. Deep Learning systems have been able to perform tasks that are usually reserved for biological brains. Tasks that have known to be difficult for conventional computing, such as facial and speech recognition, can be performed at super human levels by these machines.\n\nDeep Learning networks however are incapable of performing logical tasks such as long division. One should not expect to be able to teach an animal (i.e. your dog) to perform multiplication much less addition or subtraction. However, human brains are able to perform all sorts of logical problems. We have to ask though, can a caveman be able to do multiplication? Are we innately capable of advanced logical cognition or is this capability something we learned as a consequence of our advanced civilization?\n\nThe big chasm that needs to be crossed to achieve more general artificial intelligence is what is known as the \u201csemantic gap\u201d. How do we fuse the capabilities of Deep Learning (sub-symbolic) system with logical (symbolic) systems?\n\nHuman minds are capable of performing great feats of logical reasoning. How are our minds able to do this if our machinery is all intuition based? I am going to make the assumption here that we don\u2019t have any innate logical machinery. It is unlikely that Homo sapiens have evolved this cognitive machinery in the short time we\u2019ve existed in this planet. Therefore, to bridge the semantic gap, we need to bridge it using intuition only mechanisms. What this means is that we don\u2019t need to perform a fusion of logical components with intuition components. All we ever need is intuition components.\n\nTherefore we need to show ample evidence that complex logical thinking can be performed by an intuition machine.\n\nThis is where AlphaZero makes its revolutionary revelation. AlphaZero is the latest evolution of DeepMinds\u2019s Go play program. I have written previously about AlphaGo Zero (different from AlphaZero) and how it was able to learn to master the game of Go from scratch (without human knowledge). 99% of Westerners have never played the game of Go and simply don\u2019t understand it at all. So the relevance of DeepMind\u2019s AlphaGo Zero achievement has been muted. We don\u2019t understand the enormity of the achievement. Go however has been known to be a game of intuition. So it\u2019s somewhat (ignorantly) unsurprising that an intuition machine (one based on Deep Learning) is able to master the game.\n\nHowever, what DeepMind\u2019s new incarnation (AlphaZero) is able to do is play the game of chess. This of course may not be surprising to many since the game of chess has been \u2018solved\u2019 by computer ever since IBM\u2019s DeepBlue bested Kasparov in 1996. It may not be remarkable for the uninitiated that it took AlphaZero a few hours to master the game of chess from scratch. It may not be remarkable that AlphaZero was able to destroy the best chess playing program (Stockfish) in 100 games.\n\nWhat is truly remarkable is how AlphaZero played in dismantling its more logical opponent. To give you an idea, I will quote some impressions from the chess playing community.\n\nFor those who understand chess play, it\u2019s probably best to watch the actual game play of AlphaZero versus Stockfish. What you will see is how an intuition based system dismantles an opponent that is based on logic (that is, one that can\u2019t refuse a gambit). Below are games with expert commentary:\n\nAlphaZero plays a very different game of chess. It is willing to sacrifice pieces in order to gain a positional advantage over its opponent. It is playing a kind of chess judo where it uses an opponents eagerness in achieving an immediate gain against itself. It sets up its opponent into what is known in chess as \u201czugzwang\u201d, where every move that one makes leads to a worse outcome. It seems to have a more holistic sense of the game of chess where all its pieces move in a highly coordinated manner. AlphaZero plays a game that maximizes its creativeness against a logical opponent that is unable to see beyond short term gains. It plays a game of chess that is not only unimaginable, but would in the past been placed in a pedestal for all to marvel.\n\nThe paper about AlphaZero was presented in the recently concluded NIPS 2017 conference. It is an extremely short paper, the main body is only 7 pages long. It provides an interesting detail about how extensively it evaluates the board position to decide on its move.\n\nThe intuition machine is using 1,000 times less evaluations than the logical opponent.\n\nWhat you are witnessing here with AlphaZero is validation of my original thesis about intuition machines and their ability to perform logical reasoning. This is the semantic gap being bridged. This is an extremely difficult AGI milestone being surmounted at a record pace. I doubt anyone in the AI community expected this kind of progress to be achieved so quickly. Yet is has happened and the landscape has been changed forever."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-women-should-lead-our-a-i-future-8a0b7085ffc5",
        "title": "Why Women Should Lead our A.I. Future \u2013 Intuition Machine \u2013",
        "text": "The usual argument about women and AI is that women are grossly underrepresented in this field and that we should have more women contributions and involvement. Diversity of perspective is one of the motivations for this. There are plenty of examples of how AI is designed without considering one half of the human population. Megan Alzner writes about this:\n\nI however will argue here about something beyond the need for diversity. I will argue that our A.I. future should be led my women and not by men. The reason for this is that women have a greater intuitive understanding of what makes us all human. Women have a natural inclination to focus on the important things that make us human. To maximize the benefit of AI technology we must focus on how AI improves our humanity and therefore we need to understand, at the very least, what makes us human and not what makes us machines.\n\nWomen have brains that are wired very differently from men. A study in 2013 revealed some empirical evidence for this. The study concludes that the amount of connections between the left and right side of brains differ between men and women Women have brains are tuned for \u201cinterhemispheric communication\u201d while men\u2019s brains a tuned for \u201cintrahemispheric communication.\u201d As a consequence of this wiring men are \u201coptimized\u201d for performing tasks requiring perception and coordination. In contrast women are \u201coptimized\u201d with tasks integration of analytic and intuitive modes (See my other post on the coordination of rational and intuitive intelligence).\n\nI was reading through the AI Index the other day. The AI Index is a collective effort to summarize and track progress in the AI field. It was created and launched as a project of the One Hundred Year Study on AI at Stanford University. The AI Index is meant to be like an open source report with the aim of facilitating informed conversations about AI that is grounded on data. The initial report encourages others to contribute to this endeavor by providing more data, analyzing data and also recommending other data sources.\n\nThe report also introduces a new index, the AI Vibrancy Index which:\n\nThe AI Index contains several \u2018hockey stick\u2019 charts that reveal the exponential growth of Deep Learning. I\u2019ve collected a few of these charts myself a year ago. It\u2019s always effective to show some exponential growth charts to get your audience interested (Nothing motivates better than greed).\n\nThe report does not address one of the better motivations for the need to track AI. Specifically, issues regarding societal risks are absent from this initial report. The plan is to later introduce metrics related to AI safety, predictability, fairness of algorithms, privacy and ethical implications.\n\nA lot of unique and valuable insight can be found in the remarks that also come with the report. Barbara Grosz makes the following insightful comment about some missing metrics. Specifically, she talks about the need to track the quality of AI interactions with people:\n\nDaniela Rus describes many of the benefits of AI in tackling our biggest challenges:\n\nIn short, she highlights one of the blind spots of AI research. Specifically, one should work on AI not for AI or Automation sake but rather to solve real human problems. You simply are not going to get this perspective articulated among the general AI research community. We do know that AI will solve big problems, but we don\u2019t articulate specifically what those problems are. Almost every time, these big problems are big human problems.\n\nYou don\u2019t get this kind of perspective from men. Men look at cars from the perspective of the coolness factor. Automated self-driving cars are cool and something that will enable us to make the \u201ccannonball run\u201d in record time. It\u2019ll allow us to watch a movie (or catch a nap) while \u201cdriving\u201d the car at the same time. Women however focus on what truly is more important: the health of our children and our own well being.\n\nWhen it comes to jobs, it\u2019s not about doing more with less, but rather having jobs that are \u201csatisfying\u201d. Said differently, jobs that are meaningful and not \u201cbullshit jobs\u201d. Nursing and teaching are two jobs where we find a majority of women. These are both jobs where one\u2019s contributions can be extremely meaningful. The job of a nurse is both analytic and intuitive. A nurse must be able to grasp a complex medical field at the same time be able to be the advocate of a patients needs. To build advanced AI interfaces, one will need a similar mix of talents.\n\nLet\u2019s examine the job of teachers. This require not only mastering of a subject, but exemplary communication skills and empathy for one\u2019s students. I wrote earlier that the sexiest job in the future would the teaching of machines. The same kind of talent found in teachers may in fact be the prized talent required for AI developers.\n\nIf you will notice, these two comments in the AI Index report are from women. You can go read the other comments in the report. The comments by male researchers rarely ever discuss our humanity in relationship with AI technology. However any serious discussion of the future of AI, whether it is about the near term effects of job loss due to narrow AI automation or a far in the future existential threat of \u201csuper intelligence\u201d becoming self-aware, demands that we address own humanity in relationship with this technology. The kind of people who are best equipped to understand this (as a consequence of evolution) are women and certainly not men. Therefore, women should not only be a minority participant, but women should also lead the AI revolution.\n\nIt is up to all of us to enable and encourage more women participation in the AI revolution. It is not just a matter of the need for greater diversity, it is also a matter of our own health and well being. It is ultimately a matter of our own survival as a human species.\n\nIn relation to this, here\u2019s a recent talk about an AI system that tracks fleeing people from a drone strike: https://theintercept.com/2017/12/05/drone-strikes-israel-us-military-isvis/ . Yes, some doctoral student didn\u2019t think much of about the morality of his entire study.\n\nI leave you with this interesting study which I discovered in Harold Jarche\u2019s blog about \u201cOur future is networked and feminine\u201d:\n\nJarche argues that feminine traits are advantages in social networks and that for radical innovation we need to be able to leverage these networks:\n\nThere is sufficient group-think, even in the world of Deep Learning that ideas across disciplines like cognitive psychology, neuroscience, physics, game theory, biology etc. are critically important to future research. The most innovative ideas for Deep Learning will emerge from ideas coming out of left field. In fact, I will point out that two of 2017's most innovative ideas in Deep Learning, \u201cCapsule Networks\u201d (Sara Sabour) and \u201cMAML\u201d (Chelsea Finn) involved women lead-contributors."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-possibility-of-an-intelligence-explosion-b4617a95c9b9",
        "title": "The Possibility of a Deep Learning Intelligence Explosion",
        "text": "Fran\u00e7ois Chollet argues about the Impossibility of an Intelligence Explosion. It is a strong article with the exception of its conclusion. Chollet is accurate in describing many of the obstacles that we expect to encounter in creating an advanced artificial general intelligence (AGI). These obstacles are as follows (I use my own categorization, but its mapping with Chollet\u2019s should be straightforward):\n\nThe flaw in Chollet\u2019s article is that he believes the pace to be linear. There is little evidence that this is true. If anything, the pace of progress has been exponential. Like most technologies, however, there will be lots of road bumps. Take for example controllable nuclear fusion. Scientists have been working on this for decades without progress. Our lack of progress could not be predicted by scientists decades ago. If you asked any scientist in the 1970\u2019s if we would achieve controllable nuclear fusion by 2017, they would in the majority say it was possible. Yet today, we are still in the research stage. The physics are well known, yet we are unable to engineer a solution.\n\nCould AGI be in a similar situation as nuclear fusion? Are AGI researchers overestimating their own ability to achieve intelligence \u2018nuclear fusion\u2019. There are more unknowns in AGI than there is in nuclear fusion. However AGI may not have the same hard physics constraints to deal with. Rather, intelligence resides in the realm of information processing and that world is a virtual simulation.\n\nThe primary reason why the effect of physics is negligible is due to Moore\u2019s law. This law has been exponential for several decades and isn\u2019t stopping! (Quanta reports of a system using atomic switches that simulate neuromorphic computing) Furthermore, Deep Learning workloads are of the embarrassingly parallel variety. So despite the silicon clock rates hovering below 4GHz for decades, we can still build more capable silicon. In 2012 when GPUs had enough computation power, Deep Learning emerge from an almost forgotten approach known as Artificial Neural Networks. To illustrate the acceleration of computation as a consequence of new designs, the Systolic Array architecture used by Google\u2019s TPU and Nvidia\u2019s Volta, has lead to a 1,000% speed up in computation in a single architecture upgrade.\n\nTo build an AGI, one therefore will need: (1) A way to build learning environments that can train AI\u2019s to gradually improve its capabilities, (2) mechanisms to enhance collective intelligence, (3) a new intelligence compounding mechanism (i.e. self-play) and (4) a way to scale collective intelligence. I think we are beginning to see glimpses of this \u201cbootstrap\u201d or \u201cstrange-loop\u201d mechanism in play with Deep Learning. How does a system like AlphaGo Zero learn to improve its game by simply playing itself? To claim that it is impossible to gain intelligence that is beyond one\u2019s own experience is clearly disproven by AlphaGo Zero.\n\nIndividual technologies do have a habit of plateauing and as they mature, their growth slows and the returns diminish. We can also say the same with individual humans, that is, we learn a lot when we are younger but as we age, we slow down in our ability to capture knowledge. Both cases are however instances of closed systems and therefore there are limits to growth. Eventually, an individuals growth will plateau, just as individual technology.\n\nHowever open systems, that is collective technologies and collective intelligences have greater than linear scalability. Geoffrey West who wrote about \u201cScale\u201d describes the super-linear scaling of cities. Collections of individuals can scale super-linearly (115% for cities). AlphaGo Zero is not an individual, it has itself to play with! AGI has the potential to scale super-linearly if not exponentially.\n\nUltimately, the question reduces down to a question of information thermodynamics. Closed systems will tend to entropy, open systems do not. This is as long as new information continues to be introduced into an open ecosystems. Does an open system need to scavenge for new information in other realms or can it just synthesize (and therefore bootstrap itself) with imagined new information? Can a mathematician in his own mind imagine a new kind of algebra? Can a self-taught mathematician like Ramanujan conjure up from nowhere new kinds of mathematical identities? Can AlphaGo Zero invent new strategies without human supervision? The answer to all these is a definite yes, this is because imagination isn\u2019t confined to physics. One can do exploration in imagination (or virtual worlds) beyond the constraints imposed by physics (i.e. time and space).\n\nFinally, one can never discount making progress out of plain blind luck. That\u2019s how evolution makes progress and it is likely how we will achieve the breakthroughs in AGI."
    },
    {
        "url": "https://medium.com/intuitionmachine/dissipative-adaptation-the-clue-to-life-and-deep-learning-77dbd541c945",
        "title": "Dissipative Adaptation: The Origins of Life and Deep Learning",
        "text": "In a previous post, I wrote about Deep Learning not being Probabilistic Induction and actually being something else entirely. However, I didn\u2019t explain how the mechanism of that something else works. In this piece, I will explain how the explanation of the emergence of life could also explain the mechanism of Deep Learning.\n\nThe work of Jeremy England first captured my attention in this Quanta article \u201cA New Physics Theory of Life\u201d. The mechanism that England proposes is called \u201cDissipative Adaptation\u201d. It is a theory based on non-equilibrium statistical mechanics where the assumption here is that life resides in the regime that is far from equilibrium. This is the same thinking as non-equilibrium information dynamics where Deep Learning is also a non-equilibrium process. In this post, I will explain how this theory can be used as an explanation for the self-organizing behavior of Deep Learning.\n\nDeep Learning is modeled as a continuous dynamical system. It\u2019s driver is the Stochastic Gradient Descent (SGD) that is influenced by an objective function (typically in the form of the discrepancy between prediction and reality). One can look at SGD as an equation of motion or alternatively an equation of evolution of the parameters (i.e. weights) of an neural network. So for example, Tomaso Poggio in his framework to characterize Deep Learning uses the dissipative Langevin equation:\n\nThe Bellman equation in Reinforcement Learning is a derivation of Hamilton-Jacobi equation that we find in physics which describes the evolution of a dynamical system:\n\nSo it\u2019s established that there is a relationship between the equations of evolution in physical systems and that of Deep Learning networks. However, the methods of Deep Learning has its origins from optimization methods. Optimization methods arrive at convergence when a global extremum is discovered as a solution to the objective function. DL systems differ from classical optimization in that it is overly parameterized and the objective is not optimization but rather another objective known as generalization. Generalization itself is a complicated subject, however the \u2018theory\u2019 here is that SGD will arrive at a stable minima and as a consequence generalization will be achieved. However, the open question is, why does stochastic gradient descent (SGD) even converge?\n\nClassic optimization will tell you that the high-dimensional spaces found in Deep Learning is problematic. Yet for Deep Learning practitioners, stochastic gradient descent works surprisingly well. This is unintuitive for many experts in the optimization field. High dimensional problems are supposed to be non-convex and therefore extremely hard to optimize. An extremely simplistic method like SGD is not expected to be effective in the high complexity and high dimensionality space that deep learning networks find themselves in.\n\nExperimental evidence has shown that in high dimensional spaces, the space neighboring the minimal point have a much higher probability of being a saddle point. A saddle point gifts the optimization process with many more opportunities to escape the minima and move forwards. This argument explains why large networks don\u2019t appear to often get stuck in a non-optimal state. I therefore propose that rather than think of Deep Learning from the more conventional viewpoint of being optimization, one should think of Deep Learning instead as a physical system and residing in a non-equilibrium regime. This approach aligns much better with the experimental evidence. Furthermore, it aligns with another theme that an approach to understanding complexity should be based on physical motivations and not abstract mathematical ones. I have discussed this earlier in \u201cChaos and Entanglement in Disguise\u201d.\n\nEngland\u2019s phenomena of Dissipative Adaptation is a mechanism found in dynamical systems that may explain how and why deep learning systems converge into stable attractor basins.\n\nDissipative Adaptation provides an explanation as to why self-replicating structures arise in physical systems. Dissipative Adaption describes the dynamics of a system in contact with a thermal reservoir and with an external energy source acting also on the system. In said system, different configurations of the system are not equally able to absorb energy from that external source. The absorption of energy from an external source allows the system configuration to traverse activation barriers too high to jump rapidly by thermal fluctuations alone. If energy is dissipated after a jump, then this energy is not available for the system to reversibly jump back from where it came. Even though any given change in configuration of the system is random, the most likely configuration (as a consequence of irreversibility) happens to be the configuration that aligns more efficiently with the absorption and dissipation of external energy.\n\nArtificial neural network are not physical systems in that there is no notion of of energy. In contrast, the relevant measure is the relative entropy or alternatively the fitness function. It is a measure of similarity between the observation and prediction. The self-similarity of a neural network implies that at all components, down to the most basic neuron, there is a function that computes a similarity between observation and prediction.\n\nThe analogy to external energy source in the neural network context are the external observations of the system. Through training, the network is subjected with perturbations that drives the system towards minimizing entropy. This propagates down to every neuron such that those neurons that are aligned to the perturbations are those likely to remain aligned. (Note: The mechanism for alignment is the similarity operator) A neuron\u2019s activation function is equivalent to that energy barrier. (Actually in reverse, if sufficiently not aligned, it gets removed)\n\nThe activation function acts like an irreversible operation once the entropy moves to a lower state. With the passage of training, the memory of these less erasable changes accumulates, and the system increasingly adopts a model that is best adapted to the training data. So from a initial random model, the neural network evolves into a model that is adapted to the stochastic observations (i.e. SGD) that it is trained under. (I personally am deeply suspicious of the activation function and have a hunch that it is not only not necessary but perhaps even detrimental.)\n\nHave you ever wondered what the activation function is for? It is there not because of a bad excuse for the need of \u201cnon-linearity\u201d, rather it is there to serve as an irreversible selection operator. Deep Learning networks are from the perspective of physics, a linear system. Non-linearity exists because of feedback and there is only truncated feedback in these networks. The word non-linear used in Deep Learning means something other than a straight line.\n\nPerhaps the continuity requirements of back propagation, independent of stochasticity, ensures that there are no big local transitions in model changes. Only significant cumulative observations are required to achieve a persistent change in structure. Backpropagation ensures that the random changes due to training are not purely random, but rather constrained to changes that preserve continuity.\n\nEach individual neuron adjusts its weights in the direction of minimum entropy. That is, adjust in the direction of the gradient and thus in the maximum direction to reduce entropy. Each neuron evolves to its local minima and is unable to extricate itself unless a sufficient accumulation of observation signal exists in the training data. As more and more neurons arrive at minima, larger collective cliques of neurons are formed. These cliques become more difficult to breakup. Only coordinated signals are able to break up cliques, and as there cliques become larger, the more observations will need to be in synchronization.\n\nThe essence of England\u2019s model is that it explains the persistence of structures that are in tune with the environment. The equivalent of this from the DL perspective is that neurons that are able to match repeating observations of a training set (i.e. the environment) are more likely to persist through the epochs.\n\nThere are however differences between England\u2019s model and DL architecture. England\u2019s system makes the adjustment only with sufficient alignment. DL systems make an adjustment with insufficient alignment, that is the activation function goes below a threshold. So DL systems explicitly favor the replication of aligned neurons. England\u2019s systems favor the persistence of components that accumulate because they become more irreversible over time. DL systems also have built-in mechanisms for memory and remember by default and forgetting is irreversible. England\u2019s systems do not have memory, but achieve the equivalent through irreversibility.\n\nThe main commonality between England\u2019s system and DL is the alignment of components to the direction of maximum energy dissipation or information gradient. Both systems however are learning systems in that they both adapt to the environment. This hints at a reality that (1) one can create learning systems with components that are even simpler than that found in DL systems and (2) the universe is made up of deep learning like systems.\n\nA DL system that mirrors the architecture of England\u2019s Dissipative Adaptation would be different from today\u2019s DL system. It will only remember what matches its neurons, it will be less volatile with what it matches often and it will simply ignore non-matching information. It does not require an activation function since the only neurons that are active are the one\u2019s that it remembers. Indeed radical and interesting at the same time.\n\nThis is how Dissipative Adaptation explains trainability. However, Dissipative Adaptation does not explain expressivity or even generalization.\n\nEditor\u2019s Note: Above article is still a bit of a mess, that I will work on improving over time."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-in-not-probabilistic-induction-3508c4f97ac9",
        "title": "Deep Learning in Not Probabilistic Induction \u2013 Intuition Machine \u2013",
        "text": "There is a questionable assumption that is prevalent: Deep Learning is a form of probabilistic or statistical induction. We see this in DARPA\u2019s presentation of the 3 waves of AI. In that presentation DARPA has the following description:\n\nStatistical Learning \u2014 Where programmers create statistical models for specific problem domains and train them on big data.\n\nThis is a broad category that includes Bayesian methods, template based methods (i.e. SVM), tree based predictors, mathematical programming and Deep Learning. If you look at this short list for a moment, you realize that almost all of these methods are not statistical in nature. DARPA may have just used this classification to simplify their presentation.\n\nThere is however still this perception that Deep Learning is probabilistic induction. This is likely due to the observation that there is always a kind of randomness that is introduced into the mechanism. The bread and butter learning algorithm for Deep Learning is Stochastic Gradient Descent (SGD). The word \u2018stochastic\u2019 is used to refer to the randomness of the training procedure. That is, Deep Learning unlike other machine learning methods, does not require the entire training set to learn. Rather, it learns with by streaming data, where batches of training data are randomly selected from the whole corpus and then applied iteratively in a learning procedure. It is an incremental process that eventually (and luckily) converges to a near equilibrium state (depending on when the trainer says enough already!).\n\nThis training doesn\u2019t not have to be strictly random and unbiased. This is different from Monte Carlo sampling where we need to devise sampling methods with certain properties of probabilistic guarantees or asymptotic optimality that do not make any assumptions of the form of the system (i.e. the sampling is unbiased). Deep Learning training does not require the same stringent constraints. Deep Learning is traditionally framed as an optimization problem and not a simulation problem. Therefore, fidelity of simulations is not an underlying concern.\n\nThe randomness or stochasticity that is typically introduced in Deep Learning training methods is motivated by the intrinsic uncertainty between the training and validation sets. Strictly speaking, between the data that is available during training and the unseen data that will be available during deployment. We are essentially attempting to train systems that, when deployed, will take into account the variability that is found. So, as a method, we train these systems with stochastic variability so as to anticipate the uncertainty. We therefore can think of this as a kind of data augmentation method or alternatively a data synthesis method. Rather than just blindly adding noise to a solution, random transformations can be performed on the input or in the network.\n\nOne can then argue that this is exactly what GANs are designed to do. GANs are typically composed of two collaborating neural networks ( A generator and a discriminator). The generator learns to create realistic inputs that the discriminator tries to distinguish. You can think of the generator as a kind of data augmentation method that learns how to create realistic synthetic data. This feedback loop between the generator and discriminator is a surprisingly powerful method.\n\nThe inputs to the generator in GAN is typically sampled from a gaussian distribution. However, more recent research has shown that we would really like to sample from a distribution that better reflects the domain. So, the sampling doesn\u2019t have to be unbiased, but rather more purposeful.\n\nThere are other kinds of stochasticity that are introduced during training. Some have proposed the use of gradient noise, however this isn\u2019t commonly used. A more common method is the use of DropOut and Batch Normalization. DropOut is a probabilistic method that randomly selects a subset of a network during its training. The explanation of DropOut is that it attempts to train an \u2018implicit\u2019 ensemble. The reasoning goes that ensembles of predictors tend to improve predictions. So if we can train a network as if it were composed of superimposed ensembles, then perhaps that should work well.\n\nCompeting with DropOut is another method that experimentally appears to be incompatible with it. Batch Normalization is a method that normalizes each batch by calculating each individual batch statistic such as mean and variance. The motivation behind this method is that it changes the distribution of the domain for each layer in a network. So going back again to that argument of anticipating uncertainty, in Batch Normalization we actively massage the input of each layer so that its mean and variance of the batch fits within a narrower domain. That is, we try to wash out any unexpected input within a training batch. Intuitively you can see why this may not work out too well with RNN networks where single event anomalies are important. What Batch Normalization does is that it makes the job of each layer easier to perform and thus easier to learn. However, unlike DropOut is removes stochasticity rather than add it.\n\nNaftali Tishby\u2019s Information Bottleneck theory posits that the stochasticity in SGD comes in play in the compression phase. However, a recent ICLR 2018 submission makes the assessment that:\n\nLet\u2019s re-examine though the need for stochastic training data. Could we not train a system using a more structured curriculum? That is, presenting the easier learning problems first and more difficult one later. You know, just like how we would teach people? What then is the consensus understanding of curriculum training in the context of Deep Learning? Actually, you won\u2019t see curriculum training very often in Deep Learning, however you will find variants of the idea.\n\nThe most prevalent kind is in the form of reusing pre-trained networks. It is best practice to always start with a pre-trained network rather than starting from scratch. So for example, if you are working on a system to recognize brands of cars, then its best to start with a network that was trained on image recognition even if the original training set did not train using cars.\n\nUsing pre-training is an instance of a more general concept known as transfer learning. There are two other common kinds of transfer learning that we see in the literature. There is the teacher-student kind where a teacher network is used to train a student network. Another kind of transfer learning trains first simple networks and grows them into more complex ones. We\u2019ve seen this in the \u201cProgressive growing of GANs\u201d paper from Nvidia.\n\nAs you can see, purpose driven training (i.e. curriculum training) is an acceptable approach in Deep Learning. I suspect the reason why we don\u2019t find it more often in its more raw form is that it looks like a form of rigging the experiment. So from the academic perspective it can look questionable if you are running experiments. However, in the real world, we don\u2019t really care about rigging stuff so that we can do stuff faster and more efficiently. I don\u2019t recall any evidence out there that claims that curriculum learning can be detrimental to generalization. On the contrary, the evidence out there shows that curriculum learning leads to more scalable training. Which of course intuitively makes sense.\n\nSo far, we have explored around the edges to try to understand why stochasticity is used in Deep Learning. In our exploration, we haven\u2019t found any strong evidence for the absolute need of stochasticity. Rather, we see a kind of balance between the need for stochasticity and the need for a more efficient purposeful (and perhaps eager) learning. This hints at the reason we need stochasticity to begin with. We need it for exploration. In a previous post, I discuss learning as requiring both exploration and exploitation. The purpose of stochasticity is exploration, what exploration implies in learning is the need for diversity in experience. That is, if you train only on a narrow set of experiences, then you will not be able to adapt to more complex situations.\n\nSo for example, people who live in rural areas with less exposure to a variety of cultures are going to be less adaptable (and comfortable) with dealing with different cultures. Conversely, a city dweller will have difficulty adjusting to living in a farm where a lot of knowledge is carried implicitly and intuitively by its inhabitants. There is no instruction manual to do most things in a farm.\n\nSo, it is not randomness that is important in learning, it is diversity of experience that is important. Randomness comes into play with respect to exploration. It is in exploration where you find diversity of experience. Probabilistic approaches are designed to handle complexity through the use of summarization. That is, distilling complex phenomena by taking aggregate measures. However, it is richness of experience that leads to intelligence rather than the aggregation of experience. The brain is tuned to recognize and seek out the unusual and basically ignore the normal.\n\nStatistics, probability and randomness are of course all related. At the core is probability that is a way to characterize randomness (aka uncertainty). What I am proposing here is that for DL to make progress, one has to use a different way of handling uncertainty. The way to do this is not have this concept of probability that is a measure of uncertainty. Probability is a very expensive measure to calculate. So rather than bake this in the inference mechanism, assume alternatively that it does not exits. That is rather than taking a pessimistic approach that assumes uncertainty is every where, take an optimistic approach but with one that is adaptive. Assume there is no uncertainty and then perform a compensation when an unexpected event occurs. That\u2019s is simply how the brain works.\n\nThe use of probabilistic methods is a form of pre-mature optimization in how intelligent system deal with uncertainty. Probabilistic methods are always a valuable tool in analyzing complex multi-body systems. I have argued however that stochasticity may not be an intrinsic property of Deep Learning networks and therefore probability is not the mechanism that underpins it. With this conclusion, we should realize that it is about time that we think differently about the AI (or AGI) problem."
    },
    {
        "url": "https://medium.com/intuitionmachine/there-is-no-randomness-only-chaos-and-complexity-c92f6dccd7ab",
        "title": "Deconstructing Randomness as Chaos and Entanglement in Disguise",
        "text": "Here\u2019s a question the perhaps needs to be asked, but hasn\u2019t been asked enough. What is randomness and where does it come from?\n\nThis is one scary place to venture in. We take for granted the randomness in our reality. We compensate for that randomness with probability theory. However, is randomness even real or is it just a figment of our lack of intelligence? That is, does what we describe as randomness just a substitute for our uncertainty about reality? Is randomness just a manifestation of something else?\n\nWhat is the motivation for examining something seemingly as fundamental as the notion of randomness and ultimately probability? The motivation is that given Artificial General Intelligence (AGI) has been a problem that is unsolved in decades of attempts, that we must at a minimum perform the due diligence to see if there are any flaws in the tools that we employ. I\u2019ve written about the flaws of Bayesian inference. In this piece I will explore in greater detail our understanding of randomness and ultimately probability.\n\nA really illuminating paper about this comes from Andrei Khrennikov who writes \u201cIntroduction to foundations of probability and randomness\u201d where he explores the different interpretations of randomness and explores in more detail Kolmogorov\u2019s proposal on this subject. He summarizes in excruciating detail the three different interpretations of randomness:\n\nWhere Khrennikov comes with the observation that:\n\nOf the three interpretations of randomness, only Kolmogorov\u2019s interpretation is of an objective nature. This is indeed surprising in that Kolmogorov\u2019s viewpoint isn\u2019t actually the predominant viewpoint in our understanding of probability. Furthermore, Kolmogorov\u2019s complexity isn\u2019t even computable. So, randomness is either a subjective measure or a non-computable objective measure. The author is then resigned to conclude the following insightful idea:\n\nRandomness cannot be understood in mathematical terms. I\u2019ve written previously about the limits of mathematical thought. The gist of this is that, there are a class of systems that embody universal computation. These systems are impenetrable by present mathematics. Stephen Wolfram conjectures that it in fact is not possible. This is a similar problem to the \u201cHalting Problem\u201d in computability.\n\nMy opinion is that randomness is a manifestation of complex information processing. This view is shared by Stephen Wolfram who uses a simple celluar automata (Rule 30):\n\nas the basis of the random number generator in Mathematica.\n\nIf perhaps what is perceived as randomness is just an exceedingly complex computation, then can it be possible to discover some accidental hidden structure in that randomness?\n\nTake for example the recent discovery in the digits of prime numbers. Quanta describes one bizarre discovery:\n\nThis seems to violate a longstanding assumption held by mathematicians, that prime numbers should behave much like random numbers. Of course, prime numbers however are generated in a deterministic algorithm, albeit a very strange unexplained computational bias.\n\nLet\u2019s explore this further by avoiding the use a deterministic algorithm and instead use one we explicitly rig with a random number generator. Will structure arise despite built-in randomness? Quanta magazine described research in this question in \u201cA Unified Theory of Randomness\u201d.\n\nThere are theories in physics that assume randomness in conjunction with a few constraints that can lead to phenomena that is surprisingly universal. The Tracy-Widom distribution is one of these universal phenomena that seems to be present in many different contexts that involve the correlation of interacting elements (i.e. an entanglement). Systems can exhibit universal structure despite its constituents being originally random. The Gaussian distribution assumes that variable are disentangled, however the moment you add an entanglement assumption, a different kind of distribution emerges. Random variables that are entangled in some way lead to structure. Randomness with entanglement begets structure. This is shown experimentally as well as in Ramsey\u2019s theorem.\n\nTo emphasize this point, let\u2019s put it in a pseudo equation:\n\nWhere * is an entanglement operator. R and R\u2019 are sets of random variables. How does entanglement of random variables lead to emergent structure? Shouldn\u2019t random variables mixed with random variables also be random variables? Apparently, not when you have a correlation between them!\n\nCuriously, shared weights in deep learning architectures ensures entanglement (See: \u201cThe Holographic Principle\u201d ). There has been some recent studies exploring random matrix theory in the area of deep learning. Jeffrey Pennington and Yasaman Bahri have written a paper \u201cGeometry of Neural Network Loss Surfaces via Random Matrix Theory\u201d that explores this, concluding with the discovery of some \u2018magic numbers\u2019.\n\nIn the two classes of systems described above, we find that we cannot avoid finding structure in randomness. One with deterministic computation seemingly creating randomness but not violating its rules (i.e. in the case of Pi) and the other kind a computation driven by randomness seemingly creating structure despite randomness.\n\nI will make the following statement, \u201ccreating perfect randomness is incomputable\u201d. It sounds counterintuitive, however you can justify this argument in that to create a computation that generates randomness would imply a fixed sized procedure and therefore that procedure describes the order in that randomness. This of is in similar spirit to Kolmogorov, where he showed that infinitely long incompressible sequences have the properties of random sequences.\n\nIn an earlier article, I explored \u201cChaos and Complexity\u201d as being two mechanisms that are in play to arrive at \u2018emergent complexity\u2019. Chaos can be characterized as a positive feedback mechanism. Iterated function systems are examples of a chaotic system. The mechanism that leads to chaos is transformations applied recursively (or iteratively) on a current state. One can think of iterations as a process of transformation in the time dimension. That is, each transformation occurs in an interval of time. Think of this as \u201citerative complexity\u201d.\n\nAs an example, there are many iterative algorithms to calculate the number Pi. The Indian mathematician Ramanujan, conjured up this version from his intuition:\n\nHow Ramanujan was able to intuit order in randomness is beyond my grasp. How it is able to discern patterns in complexity is one of the big mysteries of intuition. Why an intuition machine like Deep Learning works is related to this mystery.\n\nThe other mechanism, is the effect of the law of large numbers of diverse interacting components. Emergent macroscopic phenomena arises from the interaction of microscopic phenomena. Entanglement arises when there is a rich diversity of interacting parts. Diversity requires a composition mechanism for parts to organize themselves in different configurations. So as an example, in evolution, life\u2019s diversity is created through the interchange of DNA components and in mutations. In chemistry, one has the elements that are able to combine with other elements to create more complex molecules. One can think of entanglement as existing diversity in space. That is, each diverse entity occupies different intervals of space. Think of this as \u201ccombinatorial complexity\u201d. There are recent theories that explains gravity as an \u2018entropic force\u2019 that is due to quantum entanglement. This entanglement manifests itself also in Deep Learning in the form of ensembles and in that these networks way too many parameters.\n\nHowever, macroscopic phenomena is also governed by the Second Law of Thermodynamics, that is, of the tendency to move towards increasing entropy. However, what does maximum entropy imply, it implies uniformity, that is a university without diversity. The fact that the universe has structure and has evolved complex life goes against the Second Law of Thermodynamics. That\u2019s because, despite maximum entropy, structure will emerge as seen in the Tracy-Widom distribution. The physicist call this break from uniformity, \u201csymmetry breaking\u201d.\n\nQuantum entanglement is something that is real despite Einstein\u2019s protests. Classical entanglement has not been proven to exist. In this post, I explore something that I call \u2018entanglement\u2019 that relates to the complexity of interactions of a diverse set of components. These components evolve in parallel and interact asynchronously. We don\u2019t have mathematical notions that describe these two concepts well enough. In general, we simplify them by thinking sequentially (i.e. Turing machine) and synchronously. Classically, there is also no \u2018time-travel\u2019, so its not as easy to formulate toy problems of entanglement.\n\nGiulio Tononi\u2019s Integrated Information Theory of consciousness posits an axiom for consciousness that is analogous to entanglement. Tononi\u2019s integration axiom states that:\n\nTononi\u2019s integration axiom implies a system that is highly correlated where cause and effect cannot be deduced. The entanglement that I describe may not exists in the classical physics sense however it certainly is probable for an informational perspective. I am not going to make the leap like Tononi that entanglement is a measure of consciousness. Rather, I am making that more conservative statement that is a contributor to our subjective perception of randomness.\n\nEntanglement is an important property of complex systems, however I complete disagree with Tononi\u2019s assessment that higher entanglement (or integration) leads to consciousness. Consciousness appears somewhere in a spectrum of chaos and entanglement. Intelligence is likely to exists at the edge of chaos, but never in the chaotic regime, that would imply a level of insanity. Similarly, intelligence will reside in a regime of entanglement that has properties similar to a fluid. Not in the phase of minimal entanglement like a gas and not in the phase of high entanglement like a solid.\n\nFrom a computer science perspective, one can think of these two complexities as a consequence of sequential and parallel computation. The abstraction known as Turing machines are purely sequential computers. Parallel computation are notoriously difficult to analyze mathematically so theoretical computer scientists use just one form out of convenience. In theory, a sequential computer (i.e. a Turing machine) can simulate a parallel computer. A Turing machine can generate a chaotic computation.\n\nI wonder though if a Turing machine can generate an entropic computation. Can a grossly entangled parallel computer program be made sequential and proven equivalent like the Church-Turing thesis? The way deep learning networks make predictions is based on the execution of many predictions in parallel (i.e. using multiple ensembles). Deep learning networks are known to be extremely entangled (as opposed to being sparse). The issue with theories of induction (i.e. Solomonoff induction) is that it does not take into account multiple parallel predictions. Rather it assumes that the best prediction is the one that is simplest (based on some complexity measurement). The best prediction does not necessarily follow Occam\u2019s razor, the best prediction is the one that is most accurate and that can only be determined by a measurement in the future and not in the present.\n\nChaos and Entanglement, acting both in time and space leads to what we perceive as randomness. This randomness is the effect of emergent complexity and not some mathematical notion of intractability. There is no such thing as something intrinsically random (See the Khrennikov paper above with regards to the intrinsic randomness of quantum mechanics). Randomness is an abstract concept like infinity that exists only in concept and has no physical basis. True randomness is in fact achieved only with maximum entropy, which perhaps only exists when time is at infinity (the same as the venerated Central Limit Theory). In short, never. Ramsey\u2019s Theorem in fact is an even more elegant proof of why true randomness is impossible in any interconnected structure.\n\nPerceived randomness is an appearance of complexity and is created through deterministic automation, that is, deterministic information processing components (capable of computation, memory and signaling). Memory is computation that is deferred in time. Signaling is computation transferred in space. Life and intelligence also emerges from information processing, albeit in a form that is adaptive and has self-preserving behavior. How do these ideas of randomness and probability translate to the idea of adaptive and ultimately intelligent systems?\n\nI will be remiss if I did not mention Marcus Hutter\u2019s essay on this matter of randomness and AI: \u201cAlgorithmic Randomness as Foundation of Inductive Reasoning and Artificial Intelligence\u201d. Let me highlight the difference with my thinking and that of Hutter\u2019s. (1) I don\u2019t subscribe to a universal simple measure that identifies an optimal decision, (2) randomness is due to a physical process, (3) the solution will not be found in mathematical abstractions but in computation that is restricted to the laws of physics, and (4) complexity arises from chaos and entanglement.\n\nIf randomness is a consequence of a physical process then perhaps our solution to the AGI problem should also take in consideration the constraints of a physical process. Our mathematics may be introducing too many assumptions that may be detrimental to finding a solution. It\u2019s like a case of \u201cleaky abstractions\u201d. Our abstractions are hiding the essential details and in the process we are unable to see what is in front of our eyes."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-shocking-effectiveness-of-thermometer-encoding-f155ed9c581b",
        "title": "The Shocking Effectiveness of Thermometer Encoding \u2013 Intuition Machine \u2013",
        "text": "Deep Learning architectures are built using real numbers in theory and in practice encodings in 32-bit or 16-bit floating point numbers. Real numbers are used as elements of input vectors. The similarity between vectors can be calculated with a dot product. The idea is that each layer is able to map one representation into another representation in a space that conveys the intrinsic semantics hidden in the data.\n\nNumenta HTM by contrast encodes data in what they call as Sparse Distributed Representation (SDR), also known as one-hot encoding. The argument that Numenta makes is that SDR is sufficient to (1) identify similarities between two pieces of input and (2) useful in identify the whole from its parts.\n\nWhat if, we discard this notion of maintain vectors of real numbers (or complex numbers) and choose instead what is being dubbed as \u201cThermometer encoding\u201d.\n\nThermometer encoding is like one-hot encoding, but it represents magnitude instead of a categorical variable. So for example, if you need to represent a magnitude of 7 in a vector of 10 elements, then the thermometer encoding will be [0,0,0,1,1,1,1,1,1,1]. In short, its an alternative way to encode magnitude. It does seem interesting, but one\u2019s intuition would be that it wouldn\u2019t make much of a difference versus real numbers. In fact, it should be detrimental in that it requires more storage.\n\nHere\u2019s the weird thing, apparently this method is extremely effective with regards to adversarial inputs! Yes, the kind of inputs that people have been complaining about as being able to fool deep learning networks. In a recent anonymous paper titled \u201cThermometer Encoding: One Hot Way To Resist Adversarial Examples\u201d submitted to ICLR 2018, the anonymous authors show remarkable resilience versus conventional networks.\n\nThat\u2019s a pretty big bump in accuracy for such a simplistic method. What in the world is going on here? The authors attempt to explain their success on a new kind of non-linearity. I\u2019m not satisfied with this explanation. I\u2019m never convinced with the explanation that more novel activation functions lead to better performance. This squashing function found in deep learning is an over rated feature that gets more credit that it deserves.\n\nThermometer encoding, where have we seen something similar before in the literature? It\u2019s like piling block on top of each other to represent value. I guess kind of like piling up dirt? So there\u2019s this idea of comparing two piles of dirt with one another. It\u2019s called the Earth Mover Distance (EMD), its the amount of dirt one needs to move from one pile of dirt to be able to recreate another pile of dirt. This is also known by its more fancy name Wasserstein metric. The Wasserstein metric just happens to be all the rage early this year as an alternative metric to be used for Generative Adversarial Networks (GANs). It has properties that make it more appealing that the Information Theoretically inspired Kullback-Leibler (KL) divergence. There are many alternative ways to calculate the difference between two distributions. Wasserstein metric just happens to be one way that has a more physical appeal than the abstract KL divergence.\n\nSo now we have distributions that are made up of piles of dirt. What then do these distributions represent? The conventional notion is that these piles represent probability. After all, many practitioners genuflect to the god of Bayesian inference. What if we ditch this notion of probability? Ditch the notion of calculating expectation values and instead just treat objectives as piles of dirt? How well will that work?\n\nIt turns out that creating models of reality using piles of dirt rather than probabilities also works surprisingly well. In recent research from DeepMind: \u201cGoing beyond average for reinforcement learning\u201d, researchers explore extending the classic Bellman equation in Q-learning (aka. Hamilton-Jacobi-Bellman equation to emphasize its physics roots) to work with distributions instead of scalars.\n\nThe argument to using distributions rather than scalars is the real distribution will likely be multi-modal and that collapsing the wave function (I mean distribution) destroys information that can be important in subsequent prediction. So rather than perform pre-mature optimization by calculating averages of distributions, the original distributions are preserved.\n\nThe next question is then is, how do you represent the distribution. Classically you would represent them as a non-parametric distribution. However DeepMind did something different, they represented the distribution as piles of dirt. More specifically, 51 bins of dirt. The effect of this change was indeed surprising:\n\nThese three instances of recent research makes me wonder. Why does crude physical representations of a model work so remarkably well? For the first and last example, the percentage gains are huge. Perhaps we assume too much when using more abstract mathematics? Perhaps the brain also uses very crude approximations that don\u2019t require probabilistic thinking? Perhaps the brain is doing something much simpler than we think it is doing."
    },
    {
        "url": "https://medium.com/intuitionmachine/cargo-cult-statistics-versus-deep-learning-alchemy-8d7700134c8e",
        "title": "How Cargo Cult Bayesians encourage Deep Learning Alchemy",
        "text": "There is a struggle today for the heart and minds of Artificial Intelligence. It\u2019s a complex \u201cGame of Thrones\u201d conflict that involves many houses (or tribes) (see: \u201cThe Many Tribes of AI\u201d). The two waring factions I focus on today is on the practice Cargo Cult science in the form of Bayesian statistics and in the practice of alchemy in the form of experimental Deep Learning.\n\nFor the uninitiated, let\u2019s talk about what Cargo Cult science means. Cargo Cult science is a phrase coined by Richard Feynman to illustrate a practice in science of not working from fundamentally sound first principles. Here is Richard Feynman\u2019s original essay on \u201cCargo Cult Science\u201d. If you\u2019ve never read it before, it great and refreshing read. I read this in my youth while studying physics. I am unsure if its required reading for physicists, but a majority of physicists are well aware of this concept. Feynman writes:\n\nThe question that Feynman brings up is whether a specific practice of science is based on experimental evidence or one that just looks like scientific inquiring but is based on questionable foundations. IMHO, Bayesian inference is one of those questionable forms of scientific inquiry. It has its roots in a 18th century conjecture:\n\nJudea Pearl pretty much summarizes the issues with Bayesian thinking in an article published in 2001, he writes:\n\nSo to summarize, its doubtful if knowledge is represented by probabilities. Erroneous observations aren\u2019t corrected and it\u2019s impossible to do if you aren\u2019t allowed to inspect the hypothesis as a guide to selecting the prior. Bayesian inference is loading with too many issues that its use is highly questionable.\n\nYet, Tenenbaum in 2011 \u201cHow to grow a mind: Statistics, Structure and Abstraction\u201d explains the essence of Bayesian inference:\n\nHowever Bayesian inference has no guidance of how to select an initial prior and has no evolution mechanism of how knowledge changes given an initial prior. Underneath the covers, there is no engine to speak of. It\u2019s like describing a car by observing only its external body and its wheel but completely ignoring an engine inside. That\u2019s because statistical methods are only descriptive.\n\nIf this rule were indeed axiomatic as its proponents contend, then what then is the opinion of physicists with regards to this? Physicists who are aware of the perils of Cargo Cult science should certainly be able to spot a questionable approach. The late David MacKay wrote a well known book \u201cInformation Theory, Inference, and Learning Algorithms\u201c where he explores machine learning from the perspective of Information Theory. Mackay\u2019s book should be required reading for every Deep Learning practitioner. David Mackay is a physicist by training, he writes in his book:\n\nHe then further devotes an entire chapter on \u201cBayesian Inference and Sampling Theory\u201d. Here he writes:\n\nThe only people who understand Bayesian inference are the Bayesians themselves. The only way to understand them is to drink their Koolaid. All arguments are dismissed because you don\u2019t understand what Bayesian means.\n\nThe statistical community have a habit of making arguments on the basis of obscurity. Here\u2019s a 2014 speech by John Rauser that highlights the problem:\n\nThe practice of statistics is in fact closer to alchemy that that of science. Take a look at this ridiculous taxonomy of univariate distributions:\n\nThe method of argument in statistics is to throw in some combination of distribution from above and use these as your assumptions (i.e. prior) as to how you arrive at a conclusion. It\u2019s alchemy disguising itself in the language of mathematics. It is not enough to give names to different kinds of distributions and mix it all up in the cauldron of Bayesian inference to arrive at a conclusion.\n\nIt is non-sensical for those who grew up understanding computation. How is this practice any different from the multitude of theories proposed by linguists to understand language? I guess Fred Jelinek was on to something fundamental when he remarked:\n\nPerhaps there is an equivalent to this in deep learning? \u201cEvery time you fire a statistician or Bayesian, then the performance of your deep learning system goes up.\u201d ;-) The insinuation of Jelinek\u2019s quote is that premature ideas of how complex systems work can be detrimental to its performance. We understand this in computer science as premature optimization, where if we pre-maturely optimize a subcomponent it can become a performance bottleneck later.\n\nThe legendary Isaac Newton was in fact very involved in alchemy. Here\u2019s an image of his manuscript on the subject of transmutation for gold:\n\nIsaac Newton was also from the 18th century, just like Thomas Bayes. When you don\u2019t have a foundation of strong first principles, then everything is alchemy. It\u2019s the human mind\u2019s natural state to keep on making up stuff just because we observe patterns often enough. Repeating falsehoods often enough doesn\u2019t make it true, yet humans are susceptible to this cognitive bias. (The last sentence looks awful like Bayesian inference!) At best, Bayesian inference is a human heuristic that masquerades itself in seemingly logical mathematics.\n\nChemistry exists because we understand the first principles of how atoms can be combined (derivable from quantum physics). The first incarnation of the periodic table of elements actually came before quantum physics. It was derived experimentally and only after centuries did they formulate an elegant explanation on the configuration of electrons in a valence shell of an atom.\n\nThe real reason why others don\u2019t understand Bayesian inference is because they recognize Cargo Cult science and can\u2019t believe seemingly intelligent people steadfastly believe in this.\n\nWell, Bayesian methods are a belief system. It is not very different from Occam\u2019s razor, that is explanations must be simple. To be perfectly fair, physicists also have their own belief system, one of them is that there exists a Grand Unified Theory. This was Albert Einstein\u2019s goal all the way to the end of his life. The difference of course is that in the quest for knowledge, one\u2019s belief system should remain only as a motivation for that quest and not the explanation of everything.\n\nThere was a time before the advent of Deep Learning that Bayesians were rulers of the Machine Learning field. Max Welling captures this in his essay \u201cAre ML and Statistics Complementary?\u201d. Welling writes the following:\n\nFormer rulers of the ML community do come from a Bayesian background and this explains why many papers in Deep Learning are explained from a Bayesian viewpoint. I\u2019ve argued elsewhere why it is an incorrect viewpoint, however like many things in human discourse, it\u2019s very difficult to dislodge orthodox thinking. The old guard will fight to the death to preserve their mysterious way of thinking.\n\nThis old guard would like one to believe that all inquiry should be framed in Bayesian terms. They borrow or steal ideas from other methods of inquiry and regurgitate these as being of Bayesian origin. One clear example is the use of variational methods. These methods are of statistical mechanics origin, however they\u2019ve recast the techniques as originating from Bayesian thinking. Yann LeCun, in a FaceBook post, documents the history of these methods, he writes:\n\nHe writes this in context of a paper written by Yarin Gal ( a student of the prominent Bayesian Zoubin Ghahramani). LeCun writes that Gal miscredits the origins of several papers as being of Bayesian origin which he refutes on personal historical grounds. The Bayesians are indeed colluding to extend their influence on the nascent Deep Learning field. Work using statistical physics and information theory approaches are being deconstructed and explained as being Bayesian when the authors have never subscribed to said belief system.\n\nMy perspective of Deep Learning is that its an experimental science. Our experimental apparatus is the massive computation that we currently have at our disposal. These computer systems serve as a way for us to discover emergent predictive behavior that arise from homogenous simple computational elements (i.e. artificial neural networks).\n\nVladimir Vapnik who comes from a different (and more formal) machine learning discipline (see: SVM) has the following beliefs about machine learning in general:\n\nThis idea that discoveries arrive through brute force (computation) emphasizes the current experimental nature of the Deep Learning field. Vapnik\u2019s arguments are more on testament of his belief system and a gut feeling that current lack of theory is problematic. The theories that exists out there are extremely weak and a majority of the theories are poised in questionable Bayesian terms. There are of course alternative theories that originate from the field of Information Theory (Tali Tishby) , Statistical Mechanics (Surya Ganguli) or even from Cosmology (Max Tegmark).\n\nTheoretical progress in Deep Learning should not be hindered by historical baggage like Bayesian methods. There are many more advanced models of reality that come from other fields such as Complexity Science, Critical Phenomena, Non-equilibrium statistical mechanics, Chaos Theory and Cybernetics that I would like to see applied to the explanation of Deep Learning.\n\nThe problem with Bayesians is that they don\u2019t understand domain of applicability of their belief system. A paper \u201cStatistical physics of inference: Thresholds and algorithms\u201d goes in great detail regarding this question. You are more than welcome to pour your intellectual energies into this study. To summarize that paper, the answer of Bayesian applicability depends on the how much information you have prior. Unfortunately, reality is not so kind to provide one with perfect information.\n\nIt is entirely a travesty that a majority of Deep Learning explanations are framed in a dubious and antiquated belief system. One would think that there\u2019s a conspiracy going on that favors Bayesian theories over unfamiliar theories using unfamiliar vocabulary and mathematics. We need more powerful mathematical tooling to analyze discoveries in Deep Learning, otherwise it will forever remain in its current state of alchemy.\n\nEditor\u2019s Note: For all those who keep complaining about this post, let me be perfectly clear: \u201cBayesian inference is a human heuristic\u201d. It is not a fundamental theory, it is by design a subjective form of logic and therefore is disingenuously used in many places were it should not. See Pearl ( http://ftp.cs.ucla.edu/pub/stat_ser/r284-reprint.pdf )"
    },
    {
        "url": "https://medium.com/intuitionmachine/stop-autonomous-weapons-now-3c7d77444e21",
        "title": "Stop Autonomous Weapons Now \u2013 Intuition Machine \u2013",
        "text": "If this isn\u2019t what you want, please take action at http://autonomousweapons.org/\n\nThe video above is quite upsetting, however the technology that\u2019s shown isn\u2019t that far from reality. We should all be concerned and do hope we can find a solution before it is too late. More coverage here: https://www.theguardian.com/science/2017/nov/13/ban-on-killer-robots-urgently-needed-say-scientists\n\nWhat is very sad is the comments on Youtube ( I shouldn\u2019t be surprised). However, there will always be this sentiment about weapons, that we should get it first before anyone else gets it. This problem is very different than nuclear weapons in the sense that the technology is already accessible and it is just a matter of refinement of the software and the hardware.\n\nI do not currently have a good proposal to address this complex problem. However, we all should be thinking about how to address these kinds of problems that are created with rapidly advancing technology. My thoughts are that we should demand some kind of regulation on Face Detection: https://medium.com/intuitionmachine/high-time-to-begin-regulation-of-face-recognition-a-i-f4a92ee40165"
    },
    {
        "url": "https://medium.com/intuitionmachine/is-deep-learning-software-2-0-cc7ad46b138f",
        "title": "Is Deep Learning \u201cSoftware 2.0\u201d? \u2013 Intuition Machine \u2013",
        "text": "Andrej Karpathy wrote an article about what he calls \u201cSoftware 2.0\u201d. Karpathy (Director of AI at Tesla) makes the argument that Neural Networks (or Deep Learning) is a new kind of software. I do agree that there indeed exists a trend towards \u201cteachable machines\u201d as opposed to the more conventional programmable machines, however I do have an issue with some of the benefits that Karpathy mentions to back-up his thesis.\n\nCertainly Deep Learning is already eating the Machine Learning world with advances across the board. Karpathy mentions several well known ones: visual recognition, speech recognition, speech synthesis, machine translation, robotics and games. He frames his argument about the sea change in computing and that perhaps it is time to think about a new kind of software (I guess the kind that you teach like a dog instead of programming).\n\nKaparthy lists the benefits of \u201csoftware 2.0\u201d (ignoring btw the obvious benefit that teaching may be easier than programming). He writes about architectural features of these systems that appear to be a step above current conventional software. I recommend you read his post, and come back here to see my commentary.\n\nEach of his bullet points would have been better phrased as a question rather than a statement. That\u2019s because the verdict is still out whether any of his points are even true. Let\u2019s me go down a partial list:\n\nComputational homogenous \u2014 This is interesting, but not valid considering that digital systems are essentially also computationally homogeneous if we look from the perspective of universal gates (i.e. NAND and NOR). In fact newer Deep Learning silicon is not homogenous and uses specialized cores. There are some silicon that optimize for 3x3 convolutions as an example. It may also seem homogenous today, but that\u2019s because it is still early. However taking inspiration from nature, one should expect the usual diversification into specialized and modular parts. The brain as example isn\u2019t homogenous, there are many parts and many different kinds of neurons.\n\nEasy to bake into silicon \u2014 Not exactly true in that the major risk for ASIC designers is to commit to an architecture that could get obsoleted in a few months. It\u2019s not that its not easy, the \u201cbaking in\u201d is the real problem. The hard part is knowing you have the key components for Deep Learning. That\u2019s not easy with this is a fast moving field. That\u2019s why programmable GPUs may have its value over ASICs for a longer time than expected.\n\nConstant running time \u2014 Not true for more complex networks that conditionally traverse different paths (see: Conditional Logic is the New Hotness). It indeed is true that the simple networks have bounded computation. However, it is entirely possible to have iterative components (see MCTS in AlphaGo) that may have a big variance in running time.\n\nConstant memory use \u2014 Not true for networks that are dynamically constructed on the fly. See for example: https://arxiv.org/abs/1704.05526\n\nHighly portable \u2014 Not true. Deep Learning is more portable and modular than classic ML, but it definitely is missing several features of Modular systems. I have analysis on this: https://medium.com/intuitionmachine/the-end-of-monolithic-deep-learning-86937c86bc1f . Kaparthy\u2019s two bullet points also are related to the issue of modularity.\n\nIt is easy to pick up \u2014 This is typical for maturing technology. As software professionals we don\u2019t need to understand the quantum physics that is the basis of semiconductors. We don\u2019t even need to know how a Arithmetic Logic Unit (ALU) is constructed. We work at an abstraction level that makes sense for the task at hand. A lot of Deep Learning is taught however with a lot of math, but that\u2019s not going to be true in the future. A more kind of intuitive level of software development will have to arise that will be closer to how we do teaching today.\n\nIt is better than you \u2014 I agree with this sentiment. A lot of innovative discoveries are being found by brute force methods. Deep Learning may lead to the \u2018last invention of man\u2019.\n\nAgain I reiterate that I agree with Kaparthy that teachable machines are indeed \u201cSoftware 2.0\". What is clearly debatable is whether these new kinds of systems are different from other universal computing machinery. There are of course made of the same stuff, that is information processing. Specifically, at its core, computation, memory and storage. However, Deep Learning in addition to support universal computation, has the capability of being able to learn from induction. This capability implies that the modularity of the system will be different from a system that was programmed by hand.\n\nA different modularity ( how you organize your software ) does not imply that fundamental hardware characteristics disappear. Software 2.0 cannot transcend the laws of physics. The last statement is the flaw in Kaparthy\u2019s arguments. In fact, when you begin to model Deep Learning in physics terms (and not pure mathematics) then you begin to realize the flaws of the machine learning orthodoxy.\n\nWhat about the problems with Deep Learning with regards to software? There are a lot of things that you need to consider because Deep Learning systems are intrinsically \u2018intuition machines\u2019 and thus has behavior that is starkly different from Software 1.0. The way I address is through the Deep Learning Canvas. That is, it\u2019s a checklist as to what you should think about when you develop these kinds of systems.\n\nWe are still in the early innings of Deep Learning development, there are still plenty of issues that need to be addressed to evolve this new kind of computing into something that has the same features as Software 1.0 but better across the board. It is an open question that many firms like Google and Uber are working on, but there\u2019s still a lot of missing pieces.\n\nDeep Learning will eventually become Software 2.0. To give credit to Andrej Karpathy, by coining this term \u201cSoftware 2.0\u201d, he\u2019s given a name to a kind of software development that many have had implicitly in their heads. It\u2019s added fuel to the Deep Learning hype machine, that will take a couple of years to become truly mature. In the meantime, this is still a very complex subject. If you are seeking more clarity on the many issues brought up here, then go seek out some wisdom and get my book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/revisiting-deep-learning-as-a-non-equilibrium-process-9cedb93a13a2",
        "title": "Revisiting Deep Learning as a Non-Equilibrium Process",
        "text": "Last year, the best paper award for ICLR 2017 went to \u201cRe-thinking Generalization\u201d by Chiyuan Zhang et al. For a good review, here is a video of his talk:\n\nThe key take away of his teams discovery is that the nature of Deep Learning systems is remarkably very different from other classical machine learning systems. One of the biggest misunderstanding about Deep Learning is that it is just a higher dimensional form of curve fitting and thus solved from the perspective of optimization techniques. This is incorrect notion can be due to the fact that the way Artificial Neural Networks (ANN) is taught to many is that it is just a larger form of logistic regression. Alternatively, for the more experienced machine learning expert, everything can be framed from the viewpoint of an optimization problem.\n\nThe last view point in fact has been detrimental to the field for so long. If you take the optimization viewpoint, then Deep Learning is just too high dimensional and non-convex that it should be theoretically impossible to achieve convergence to a global minima. Unfortunately for the optimization gurus, this experimentally isn\u2019t even true. The simplest of methods, stochastic gradient decent, works surprisingly too well. Something else is going on that has eluded the orthodox explanation of how optimization is supposed to work. Zhang\u2019s discovery provided experimental evidence that we have to rethink our current (obviously incomplete) theories.\n\nDespite the thousands of papers that are submitted to the various Deep Learning conferences this year, there\u2019s very few papers that attempts to explore explain the true nature of Deep Learning. Deep Learning research is really just pure alchemy and piss poor explanations are backed with lots of hand waving that\u2019s disguised as mathematics. Everyone in the academic community are so vested in pleasing everyone else that nobody wants to call out the BS. Fortunately, we have some brave souls that work on the real theoretical issues. Papers of this kind are unfortunately the kind that usually get rejected. It\u2019s just a fact of reality that when you need to understand a system that you have to work with a simpler system. Yet, showing results using MNIST is considered not state-of-the-art and therefore should be ignored. The only folks that get a pass are celebrities like Geoffrey Hinton. It\u2019s a sad reality where celebrity and alchemy is favored over real science.\n\nOkay, I\u2019m done with my rant. Let\u2019s look at some interesting papers that has just recently published.\n\nHere\u2019s a new paper: \u201cA Bayesian Perspective on Generalization and Stochastic Gradient Descent\u201d. Which begs the question, why are smart people invoking spells like \u201cBayesian intuition\u201d to obfuscate that they are actually just doing alchemy? I suspect that the use of the term Bayesian or Gaussian in papers is more to play to the sentiments of the orthodoxy and is at the expense of more precise language but equivalent language. Here are some quotes from the paper:\n\nI am not impressed at this paper\u2019s attempt to justify \u2018Bayesian intuition\u2019. Fortunately, there a much better paper on exactly the same subject:(\u201cStochastic gradient descent performs variational inference, converges to limit cycles for deep networks\u201d). This also explores SGD as an implicit generalization method and remarks:\n\nthe paper has a gem of an observation:\n\nIn short, noise in Deep Learning arises because of the diversity of the training and architecture. It\u2019s not something that you artificially add so you can justify Bayesian intuition.\n\nThis notion of the importance of architecture is further analyzed in a recent paper \u201cIntriguing Properties of Adversarial Examples\u201d where the authors from Google Brain use their \u201cneural architecture search\u201d infrastructure to discover new architectures that are less susceptible to adversarial features. They conclude that the size of the model network does not correlate with adversarial robustness. Rather adversarial robustness is strongly correlated with \u201cclean accuracy\u201d. The principles behind building a high \u201cclear accuracy\u201d architecture appears to be an open question. Their brute force search found the following network:\n\nWhich just happens to be longer and thinner than the baseline best NAS architecture. The longer the network perhaps alluding to a larger effect of transient chaos (discussed later). (I don\u2019t have an explanation for the narrowness other than its lower complexity)\n\nThese two papers study the same subject but the approach is starkly different. In the first paper, the authors attempt to explain that Bayesian inference still holds with Deep Learning. In the second paper, the authors explain that this is a non-equilibrium phenomena and we can\u2019t know enough because Deep Learning training is truncated with insufficient epochs.\n\nYou can see the problem here, satisfying Bayesian inference or Occam\u2019s razor does not signify truth. All it signifies is that one\u2019s own beliefs are validated and that the behavior of the inspected system validates those beliefs.\n\nThe second paper in contrast explores the aspects that are different about Deep Learning and attempts to make the analogy with other physical theories. In short, it\u2019s not attempting to fix a round peg into a square hole. Reality is what it is and it is our business as scientists to explore a rich variety of models to explain our reality. The real problem is that many researchers aren\u2019t skilled in the mathematics of Statistical Mechanics. They use whatever tool is at their disposal, unfortunately it is some antiquated 18th century math in the form of Bayes Theorem.\n\nIn this paper, the authors argue that SGD settles not at a local minima but rather in a limit-cycle:\n\nThe paper proposes the use of a \u2018local entropy\u2019 to discover these limits cycles. They cite a paper: \u201cUnreasonable Effectiveness of Learning Neural Networks: From Accessible States and Robust Ensembles to Basic Algorithmic Schemes\u201d that makes the claim about the smoothness of the local entropy as compared to the original objective function:\n\nIn a previous blog post, I pointed to recent papers that describe the two phases of gradient descent. The signal to noise ratio is extremely low at the minima, it is chaotic down there and any significantly increate in learning rate can violently kick one out of that minima. In addition, there are many of these minima out there and finding out which one of them leads to generalization is a wide open question. The current consensus is a wide basin is the preferred choice. I don\u2019t know if this notion should give a researcher a warm fuzzy feeling that its the right choice!\n\nThere are several papers that also come from those trained in a field other than statistics, that will likely not see the light of day (or rather accepted in a conference). The incomprehensibility to the reviewer trained only in statistics is grounds for rejection. Here is one where Charles Martin and Michael Mahoney apply a statistical mechanics approach to further understanding the \u2018rethinking generalization\u2019 paper (\u201cRethinking generalization requires revisiting old ideas: statistical mechanics approaches and complex learning behavior\u201d). The authors argue that:\n\nThere\u2019 is indeed mass confusion today about what kind of regularization leads to generalization. In fact, there is experimental evidence that performing SGD without regularization also leads to good generalization. There are even papers that show that certain kinds of regularization are detrimental to generalization. Here\u2019s a recent survey: \u201cRegularization for Deep Learning: A Taxonomy\u201d. There is this notion that the learning process in Deep Learning is \u2018transient chaos\u2019, that is convergence is in a chaotic regime and that given enough epochs that true chaotic phenomena will be revealed. Compare the output at different depths as a function of the input:\n\nHowever, one has got to at least ask, why is it \u201cchaotic\u201d down there where generalization can be found? Could it be perhaps that we have encountered a many body problem? That is, an intelligent system should have multiple perspectives of reality and thus the transition to each perspective is of a fluid nature?\n\nThere\u2019s no magic measure to achieve generalization without actually looking at the validation set, this is what most researcher seem to be completely blind of. A system that generalizes well is one that works well with the validation set. It does not have some kind of mystical precognition skills that tells it that one minima is better than another because of some bayesian belief that wider or simpler is better. This is why meta-learning methods are effective because it has seen enough validation sets to basically learn to be adaptive.\n\nThe paper by Martin et. al. proposes to simplify regularization by focusing on just two knobs for controling deep learning:\n\nThey explored the design space using a simple model of deep learning and propose the following phase diagram:\n\nThis indeed is a refreshing idea that needs to be explored further using more complex deep learning architectures.\n\nBTW, if you are lost in this discussion, meaning words like regularization, implicit regularization, generalization etc are new, the here\u2019s a screenshot of the topics in a new course at Stanford that\u2019ll give you some bearing:\n\nExplore more in this new book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/voice-style-transfer-using-deep-learning-d173f1608af5",
        "title": "Autoregressive Networks: the Obscure Kind of Generative Network",
        "text": "Another shocker from the insane world of Deep Learning:\n\nSo it takes a snippet of speech and then translates the snippet of speech using the voice style of another person. The surprising point though of this research is that its able to encode an internal representation of a speech absent the speaking style. Of course, that sounds like voice to text translation. Now it somehow is able to take out a speech style and transpose it elsewhere. Quoted from the above web page:\n\nThe approach in the paper uses auto-regressive networks, one of those curiously strange thingamajigs that DeepMind seems to be enamored with. It is the same kind of network as WaveNet.\n\nDeepMind seems to like using a very peculiar kind of network that goes by the name PixelRNN, PixelCNN, WaveNet and ByteNet. These networks are a radical departure from more traditional CNN networks and have characteristics that give them behavior that is remarkably different from other approaches. DeepMind has a paper that contrasts GANs with PixelCNNs that was submitted to ICLR 2017. In this paper the authors argue that the features and behavior of PixelCNNs are different enough that they should be evaluated differently than GANs.\n\nThe question then are, what exactly are these \u2018Autoregressive Networks\u2019. In a map that I created of DL supervised learning, these networks are a new species entirely:\n\nAutoregressive networks behave differently enough that it is sometimes worthwhile to combine in an ensemble these kinds of networks with more conventional ConvNets or Feed Forward (Dense) networks.\n\nThese networks were all the rage late 2016 (around the time WaveNet was introduced), but for some reason, the research community hasn\u2019t been too enamored by them. Unlike GANs, it is much harder to find other research groups working on this. I suspect the popularity of the GANs reduces the interest in an alternative generative technique. This is unfortunate since there is value in using a technique that has wildly different characteristics.\n\nThe beauty of Autoregressive networks is that the same formulation applies to one dimensional, two dimensional and higher dimensional domains:\n\nWhat\u2019s very odd about them however is that conventional networks always calculate a kind of similarity between the weights of the network and the input data. So for the more classical network, you have a sum of products. For a ConvNet you have a generalized form of similarity. However, with autoregressive networks, its just a multiplication of probabilities. Specifically, you try to predict the next pixel by multiplying all the probabilities of the pixels that came before it.\n\nThe manner in that it works with images just looks contrived, unnatural and thus plain weird. It scans images like an old fashion cathode ray tube:\n\nSo start from the top and then predict the current pixel based on the pixels that are scanned before it. If you read the papers, the results for generative models is that they all seem very blocky and digital.\n\nThis network is so much in the fringes that it isn\u2019t even mentioned in the Deep Learning book by Goodfellow et. al. Although the idea of autoregressive networks existed prior to Deep Learning book, it wasn\u2019t as prominently used as it is today. There\u2019s also not much tutorials on this, I have found a few though: http://sergeiturukin.com/2017/02/22/pixelcnn.html and https://github.com/tensorflow/magenta/blob/master/magenta/reviews/pixelrnn.md. But, its still difficult to get a good handle around and appreciate this approach.\n\nTo summarize though this latest development from DeepMind. The details of this paper are impressive in that underneath the covers its able to create a discrete representation. It may not be obvious to many, but this is indeed a significant step towards bridging the semantic gap between intuition and rationality.\n\nHere\u2019s an interesting paper \u201cTowards Learning to Learn Distributions\u201d that explores the use of PixelCNN in meta-learning."
    },
    {
        "url": "https://medium.com/intuitionmachine/how-to-grow-the-innate-machinery-for-agi-a5cbbd755eae",
        "title": "How to Grow the Innate Machinery for AGI \u2013 Intuition Machine \u2013",
        "text": "Gary Marcus and Yann Lecun held a fascinating debate that explored their divergent approaches towards more intelligent machines. I recommend everyone to watch the debate because it reveals the vast chasm that we still need to traverse:\n\nThe debate began with Gary Marcus believing that his views were consistent with Yann LeCun\u2019s views. Gary Marcus in the early 2000 wrote \u201cThe Algebraic Mind\u201d criticizing the neural network approach and arguing that new kinds of functionality are required to achieve intelligent machines that can match human cognitive capability. He is correct in highlighting the deficiencies of current systems. What Marcus seems to lack is the insight on how to build the missing cognitive pieces. Here are Marcus\u2019 essential cognitive capabilities:\n\nA way of representing the affordances of objects\n\nMarcus believes that these capabilities are innate machinery that exists in humans. In the debate he reveals some work by the Allen Institute that estimates that 90% of DNA encoding is used to reconstruct (I guess via indirect encoding) the machinery of the brain. Marcus laments the lack of focus in AI research to address inventing these innate machinery. Marcus also believes that we currently have sufficient computational resources to be able to run these innate machinery (when it is discovered).\n\nYann LeCun by contrast believes differently. That is, we need ever more computational resources and that our estimates of the computational requirements of the human brain is grossly underestimated. LeCun observes that progress in Deep Learning has shown that we can learn sophisticated cognitive capabilities with ever simpler machines. Historically, computer vision has been constructed with human engineered components. However, since 2012, ConvNets have bested all the best engineered algorithms. This was done by learning vision capabilities from scratch. LeCun argues that we should not fall again to that approach of hand engineering innate machinery.\n\nThe Deep Learning community is not devoid of engineering new kinds of networks. On the contrary, a majority of research is about proposing new kinds on network architectures for many different domains. The approach is however vastly different from previous approaches. Rather than fully specifying a new kind of component for a specific domain, the components tend to be more generic and are trained to conform to a domain. One could make the analogy of growing a plant. Researchers only define the scaffolding and simply performing tweaks to guide a network to \u201cgrow\u201d into a desired shape.\n\nThe concern LeCun has about defining innate machinery is can be sub-optimal and even worse, it could be wrong. The guideline that LeCun uses that the components should start out as simple as possible. He invokes Occam\u2019s Razor as his justification for his approach. He also identifies the semantic gap that needs to be bridged between sub-symbolic and symbolic systems. Furthermore, he re-iterates his need for unsupervised learning or his own term \u2018predictive learning\u2019.\n\nMarcus by contrast believes that he understands the specific kinds of innate machinery that needs to be built. However, he is unable to express how it is going to get built or even how these innate machinery are going to be composed together to work in as a coherent whole. It is not enough to speculate about what\u2019s missing. One has to at least articulate how the missing parts are all going to fit together. One gets this impression that Marcus proposal identifies the sub-routines that need to be in place, but can\u2019t express how these sub-routines are to be composed into a purposeful program.\n\nWhat are we thus left with is something quite unsatisfying. Marcus has some ideas of what needs to be built, but has no idea of how its going to be composed together. LeCun knows what learning algorithms are missing, but he has very little guidelines on how to design new ones. I think LeCun is more correct than Marcus in that he has at least a semblance of a prescription to get us there. Marcus only understands where we need to go, but doesn\u2019t appear to have any insight on how to put it together.\n\nAllow me to proclaim greater insight than these two esteemed debaters.\n\nThis of course is a problem that is solved at the meta meta-model. I discussed this earlier in \u201cThe Meta Model and Meta Meta-Model of Deep Learning\u201d. We can only create this innate machinery if we have the vocabulary to create them. That is, we need to discover building blocks of the innate machinery. If we do have these building blocks, then evolutionary methods, can make this happen. The reason that we need evolutionary methods here is that there are likely no first principles beyond a loosely coupled principle that can drive this design. Furthermore, the language of evolution that already exists is evidence that it indeed possible to create the innate machinery we need for cognition. This language is already encoded in our DNA and instances of which are present in our brains.\n\nThe vocabulary at the meta meta-model layer should conform to the Loosely Coupled Principle. Specifically, we demand the use only of building blocks (note: innate machinery is composed of building blocks) that follow any of the loosely coupled design patterns. These are described in the table below:\n\nThese patterns constrain the design space so as to avoid any approach that has tight coupling characteristics. It is a more nuanced approach that simply invoking Occam\u2019s Razor as LeCun has done. It is still in the same spirit in that any loosely coupled approach demands the minimal amount of assumptions. If you are wondering where these patterns come from, they from the study of interoperable protocols of a previous decade. (In a book that I had never was able to finish).\n\nIn short, to minimize the friction of composing coordinating agents, one has to favor protocols that encourage interoperability. The protocols are of the loosely coupled kind, the kind that assumes the least (assumes the minimum assumptions) to make things happen. So when we speak about how these innate machinery are to be built and how these innate machinery are going to coordinate to do anything, we ultimately need to originate from the same set of principles.\n\nWhen you have mechanisms that encourage interoperability then you have a fighting chance to some emergent self-organization. When you bake in too many assumptions, then a component becomes less modular and thus less able to participate in composition.\n\nThe essence ingredients of natural evolution is that you have a medium (i.e. fluid water) that gives molecules the opportunities to seek out combinations and it needs molecules that have mechanism to compose with other molecules. There is a search mechanism and a mechanism for composition. At present, we have generalizations of the search mechanism, but we don\u2019t know what those molecules are.\n\nHow are the innate machinery actually going to be created? We use evolutionary methods to create them. That is therefore requires the development of proper environments to grow these machinery. That is, like the board game in the game of Go, we have constrained environments to incrementally grow innate machinery."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-probability-theory-should-be-thrown-under-the-bus-36e5d69a34c9",
        "title": "Should Probabilistic Inference be Thrown Under the Bus?",
        "text": "So, what\u2019s Yann LeCun talking about when he says \u201che\u2019s ready to throw Probability Theory under the bus\u201d?\n\nLeCun spoke these words to get a reaction out of Joshua Tenenbaum, the other speaker in the room. Tenenbaum in 2011 wrote a paper \u201cHow to grow a mind: Statistics, Structure and Abstraction\u201d that argues for a Bayesian motivated approach to achieving Artificial Intelligence.\n\nThis article attempts to explore whether such an approach makes sense and to understand LeCun\u2019s sentiment better.\n\nThe problem with Probability Theory (more specifically, Bayesian inference) has to do with its efficacy in making predictions. Take a look at the following animated GIF:\n\nIt\u2019s obvious that the distributions are different, unfortunately the statistical measures are identical! Said differently, if the basis of your predictions are expectations calculated from probability distributions, then you can very easily be fooled.\n\nThe method to create these distributions is very similar to the incremental method we find in Deep Learning. The method uses a perturbation method and simulated annealing. As a side, if you want to fool a statistician, then a Deep Learning method is a very handy tool.\n\nAn interesting paper \u201cDeep Unsupervised Learning using Nonequilibrium Thermodynamics\u201d published in 2015 shows how you can employ perturbation methods from Statistical Mechanics to essentially recreate a specific distribution starting from random noise. A reverse diffusion method is learned to take noise back into an original distribution.\n\nIncremental perturbation is an extremely powerful tool and it indeed is intractable using statistical methods. One significant aspect of perturbation methods is that they operate in a non-equilibrium regime. That is, very far from where the Central Limit Theorem will hold. This should establish the idea in everyone\u2019s mind that incremental perturbative methods are effective in ways that can elude statistical detection.\n\nUnfortunately, creating artificial probability distributions is not the real problem. The real problem is that entire practice of Bayesian inference, and its Information Theory relative, is surprisingly fundamentally flawed in non-linear domains.\n\nJames Crutchfield of the Sante Fe Complexity Science Institute had recently delivered an extremely interesting lecture in Singapore demonstrating these inductive flaws in non-linear systems (the link starts at the time that Crutchfield makes his relevant remarks):\n\nEquations from Shannon Entropy (and by deduction Bayesian inference) that relate past and current probabilities with future predictions are essentially worthless for predictions in non-linear entangled systems. Here is the relevant paper (http://csc.ucdavis.edu/~cmg/papers/mdbsi.pdf ) to study. In this paper the author explores a Bayesian network to perform inference in a simple three variable connected network. Here are the results that should put Bayesians to question their 18th century beliefs:\n\nIn summary, we don\u2019t know anything about these non-linear systems other than the fact that we know they work extremely well. The ramifications of Crutchfield\u2019s discovery (this can be verified through simulations and is not through logical arguments) is that probabilistic induction does not work in non-linear domains.\n\nReality if of course complex and non-linear, however we\u2019ve been fortunate enough to find small patches of reality where the effects of non-linearity can be glossed over by aggregate measures. So probabilistic induction works analogously to how one would approximate a curve with piecewise linear segments. Its a bit of a kluge, but it does work in some cases. However, it\u2019s not a fool proof method that should be applied everywhere.\n\nThe question however that must be asked by researchers working on predictive systems is can we do better? Can we use purely perturbative methods without the requirement of probabilisitic induction? The problem with probabilistic induction is that it is a case of \u2018premature optimization\u2019. What I mean by this is that mathematics exists to take into account uncertainty. So when we implement our predictive machines using this kind of math, we implicitly bake in an uncertainty handling mechanism.\n\nThe brain likely isn\u2019t using Monte Carlo sampling to estimate probabilities. So how then does the brain handle uncertainty? It does so in the same way that \u201coptimistic transactions\u201d handle uncertainty. It does so in the same way that any robust and scalable system handles failures. Any robust system assumes failures will happen and thus must have mechanisms to adapt. The brain performs compensation when it encounters something it does not expect. It learns how to correct itself through perturbative methods. That\u2019s what Deep Learning systems also do, and it\u2019s got nothing to do with calculating probabilities. It\u2019s just a whole bunch of \u201cinfinitesimal\u201d incremental adjustments.\n\nPerturbative systems can be a bit nasty, they are after all like Iterative Function Systems (IFS). Any system that iterates into itself or has memory can either be a candidate for chaotic behavior or can be a universal machine. These systems are simply out of the domain of what\u2019s analyzable by probabilistic methods. This is a fundamental fact that we should be ready to accept. Unfortunately, Bayesians seem to have some unassailable belief system that demands that their methods work universally.\n\nHere\u2019s a paper by Max Tegmark et al. (see: https://arxiv.org/pdf/1606.06737v3.pdf ) that explores the pointwise mutual information of various languages:\n\nNote the fall off with Markov processes. In short, if your prediction engine has no form of memory, then there\u2019s simply no way that it can predict complex behavior.\n\nHowever I hear arguments that probabilistic induction (Bayes rule) applies in certain domains. What domains are these? Bernard Sheolkopf tells you exactly what domains it applies to (see: http://ml.dcs.shef.ac.uk/masamb/schoelkopf.pdf ). That is domains where anti-causality is present:\n\nSaid very simply, you can predict Y because Y is the cause of X (your input). So in fact, even for linear systems, you have to be very careful as to where you apply probabilistic induction. So, when we apply our probabilistic induction to figure out if we can differentiate between the dinosaur, star, ellipse or cross, we discover that we cannot. Why is that? That\u2019s because the input that is observed (i.e. X) is not directly caused by its source (i.e. Y). Y was not the cause of the distribution X. Rather, there is another perturbative machinery in between that performs the obfuscation.\n\nWhat if however you have the information that is used as input to this perturbative machinery? Can you then predict the machines inputs from the generated distribution? Well that would be an obvious yes!\n\nA new paper explores the \u201cunreliability of saliency methods\u201d. Saliency is used in Deep Learning networks as a means of highlighting which inputs contribute most to the networks predictions. It\u2019s been proposed many times as a way to explain the behavior of a network. Interestingly enough, the paper shows that simple transformation on the input (i.e. a constant shift) can cause the failure in the attribution:\n\nThis is indeed interesting discovery and shows that our understanding of causality in Deep Learning networks is at its infancy. By unreasonably demanding that \u2018bayesian inference\u2019 or \u2018probabilistic induction\u2019 be the guiding principle behind these networks is an assumption that stands with little evidence. Probabilistic induction has never been a fundamental feature of nature and therefore should be used with caution when trying to explain complex systems. It is sad state of affairs that the only tool in one\u2019s arsenal is \u2018Bayesian intuition\u2019 and thus every complex problem must be framed only in this perspective.\n\nI leave you with two quotes from Judea Pearl:\n\nthis is one reality, that humans don\u2019t use probabilistic thinking. The second quote is about the nature of probabilities and reality:\n\nWhich simply reflects how physicists think about the relationship of thermodynamics and statistical mechanics. Statistical mechanics is actually a field in physics that is incorrectly named. StatMech doesn\u2019t use most of the statistical methods used by statisticians. Rather, the approach employs probability in its most basic form. That is simply in the calculation of the degrees of freedom in a system that will be derived from the physical constraints imposed by the particles in a system. The question that is asked is whether beginning from laws of physics and assuming a massive number of particles, can the observations in the macroscopic scale (i.e. thermodynamics) be derived. Can the microscopic phenomena predict the macroscopic phenomena?\n\nIn stark contrast, Bayesian inference and statistics attempts to discover knowledge by looking at the distributions. That is, by observing the macroscopic phenomena, can I gain new knowledge? The assumptions that are made (i.e. the prior) is that one can guess the best prior to arrive at a posterior (i.e. prediction). In many real world contexts, these kinds of systems are notoriously difficult to get correct. A recent Arxiv paper \u201cBetter together? Statistical learning in models made of modules\u201d explains why it is insanely difficult to get these probabilistic graph models to be correct:\n\nThis is an approach where its all too easy to contaminate the results with lots of human bias. In fact, the use of statistical approaches is so abused that a recent project tried to replicate 100 psychology studies and found less than half to be repeatable. The issue in question is the questionable practice of p-hacking. This is actually a mild form of the problems of introducing human bias into probabilistic calculations.\n\nThe cognitive bias that many seem to have is that they believe that the measures are an explanation of a system and not simply the effect of a system. To make it clear, don\u2019t use Bayesian inference as a means to explain complex non-linear phenomena like cognition. Even worse, don\u2019t use Bayesian methods as your mechanism to create artificial intelligent machines. If you got simple and less complex problems, feel free to use the appropriate tools. Just because your saw can cut wood shouldn\u2019t mean that it can cut titanium.\n\nAt the core of intelligence is the existence of feedback loops, this implies a non-linear system with cyclic dependencies. Bayesian inference is just a tool and not a fundamental feature of either reality or intelligent systems. As a tool, it has limits in its domains of applicability. Therefore we should be cautious about using this tool as motivation for understanding complex systems. Artificial Intelligence has struggled for decades and perhaps the breakthrough may be in re-examining and questioning our own research biases.\n\nEditor\u2019s Note: I removed commentary about this paper comparing machine learning methods because it is a distraction from the real conversation I want to focus on: \u201cissues of using probabilistic inference in non-linear systems\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/these-images-are-generated-by-a-deep-learning-gan-6b49062b3959",
        "title": "Deep Learning Solves the Uncanny Valley Problem \u2013 Intuition Machine \u2013",
        "text": "This was my shortest article ever, until I added additional commentary below. Here\u2019s a video about this:\n\nIf you think the above video is insane, then this interactive version will truly blow your mind: http://alteredqualia.com/xg/examples/synthetic_celebrities_interpolation.html\n\nThe system essentially uses an approach similar to StackGAN, but rather than just fix itself to two stages, it progressively grows multiple stages learning a smaller network into a larger one. Using residual layers as a transition step between incrementally larger networks:\n\nLet me add a bit of commentary about this development. The objective of the researchers was definitely not to create better eye candy, but rather to scale the current GAN method to larger images. The application of this method is clearly in its ability to create simulations. Creating simulations will have value in more complex predictive problems. The more accurate simulations that can be generated quickly, the more accurate we can create probabilistic models.\n\nThe scary thing though is that these are generated images. The best CGI cannot match these in terms of fooling the brain into believing it to be real. The Star Wars movie \u201cRogue One\u201d used a CGI image of the late Peter Cushing. I am sure the animators through every trick in the book to make the CGI appear real, but clearly that all failed. However, now you have an automation that can generate high resolutions images realistic images of people. In short, in the next year or two, the movie and games that you see that simulate humans will now finally look like real life. Let that thought sink in for a while.\n\nLast year, I prematurely declared the winner of the best paper at ICLR: \u201cRe-Thinking Generalization\u201d. ICLR has decided to use a double blind paper submission to make the the process more objective. Unfortunately, it is obvious now who deserves to be the hands down winner of the 2018 ICLR conference. That\u2019s unless you can dig up a more impressive submission in the next following months. Good luck mining ICLR submissions!\n\nDeep Learning technology are intuition machines and as a consequence, their ability to generate what we see in a manner that fools our own intuition in indeed uncanny."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-strange-loop-in-alphago-zeros-self-play-6e3274fcdd9f",
        "title": "Why AlphaGo Zero is a Quantum Leap Forward in Deep Learning",
        "text": "The 1983 movie \u201cWar Games\u201d has a memorable climax where the supercomputer known as WOPR (War Operation Plan Response) is asked to train on itself to discover the concept of an un-winnable game. The character played by Mathew Broderick asks \u201cIs there any way that it can play itself?\u201d\n\n34 years later, DeepMind has shown how this is exactly done in real life! The solution is the same, set the number of players to zero (i.e. zero humans).\n\nThere is plenty to digest about this latest breakthrough in Deep Learning technology. DeepMind authors use the term \u201cself-play reinforcement learning\u201d. As I remarked in the piece about \u201cTribes of AI\u201d, DeepMind is particularly fond of their Reinforcement Learning (RL) approach. DeepMind has taken the use of Deep Learning layers in combination with more classical RL approaches to an art form.\n\nAlphaGo Zero is the latest incarnation of its Go-playing automation. One would think that it would be hard to top the AlphaGo version that bested the human world champion in Go. AlphaGo Zero however not only beats the previous system, but does it in a manner that validates a revolutionary approach. To be more specific, this is what AlphaGo has been able to accomplish:\n\nEach of the above bullet points is a newsworthy headline. The combination of each bullet point and what it reveals is completely overwhelming. This is my honest attempt to make sense of all of this.\n\nThe first bullet point for many will seem unremarkable. Perhaps it\u2019s because incremental improvements in technology have always been the norm. Perhaps one algorithm besting another algorithm 100 straight times intuitively doesn\u2019t have the same appeal of one human besting another human 100 straight times. Algorithms don\u2019t have the kind of inconsistency that we find in humans.\n\nOne would expect though that the game of Go would have a large enough search space that there would be a chance of a less capable algorithm to be lucky enough to beat a better own. Could it be that AlphaGo Zero has learned new alien moves that its competitors are unable to reason about the same search space and thus having an insurmountable disadvantage. This apparently seems to be the case and is sort of alluded to by the fact that AlphaGo Zero requires less compute resources to best its competitors. Clearly, it\u2019s doing a lot less work, but perhaps it is just working off a much richer language of Go strategy. Less work is what biological creatures aspire to do. Language compression is a means to arrive at less cognitive work.\n\nThe second bullet point challenges our current paradigm of supervised only machine learning. The original AlphaGo was bootstrapped using previously recorded tournament gameplay. This was then followed with self-play to improve its two internal neural networks (i.e. policy and value networks). In contrast, AlphaGo Zero started from scratch with just the rules of Go programmed. It also required a single network rather than two. It is indeed surprising that it was able to bootstrap itself and then eventually learning more advanced human strategies as well as previously unknown strategies. Furthermore, the order in what strategies it learned first were sometimes unexpected. It is as if the system had learned a new internal language of how to play Go. It is also interesting to speculate as to the effect of a single integrated neural network versus two disjoint neural networks. Perhaps there are certain strategies that a disjoint network cannot learn.\n\nHumans learn languages through metaphors and stories. The human strategies discovered in Go are referred to with names so as to be recognizable by a player. It could be possible that the human language of Go is inefficient in that it is unable to express more complex compound concepts. What AlphaGo Zero seems to be able to do is perform its moves in a way that satisfies multiple objectives at the same time. So humans and perhaps earlier versions of AlphaGo were constrained to a relatively linear way of thinking, while AlphaGo Zero was not encumbered with an inefficient language of strategy. It is also interesting that one may consider this a system that actually doesn\u2019t use the implicit bias that may reside in a language. David Silver, of DeepMind, has an even more bold claim:\n\nThe Atlantic reports about some interesting observation of the game play of this new system:\n\nThe learned language is devoid of any historical baggage that it may have accumulated over the centuries of Go study.\n\nThe third bullet point says that training time is also surprisingly less than its previous incarnation. It is as if AlphaGo Zero learns how to improve its own learning.\n\nIt took only 3 days to get to a level that beats the best human player. Furthermore, it just keeps getting better even after it surpasses the best previous AlphaGo implementation. How is it capable of improving its learning continuously? This ability to incrementally learn and improve the same neural network is something we\u2019ve seen in another architecture known as FeedbackNet. In the commonplace SGD based learning, the same network is fed data across multiple epochs.\n\nHere however, each training set is entirely new and increasingly more challenging. It is also analogous to curriculum learning, however the curriculum is intrinsic in the algorithm. The training set is self generated and the calculation of the objective function is derived from the result of MCTS. The network learns by comparing itself not from external training data but from synthetic data that is generated from a previous version of the neural network.\n\nThe fourth bullet point, the paper reports that it took only 4 Google TPUs ( 180 teraops each ) as compared to 48 TPUs for previous systems. Even surprisingly, the Nature paper notes that this ran on a single system and did not use distributed computing. So anyone with four Volta based Nvidia GPUs has the horse power to replicate these results. Performing a task with 1/10th the amount of compute resources should be a hint to anyone that something fundamentally different is happening over here. I have yet to analyze this in detail, but perhaps the explanation is due to just a simpler architecture.\n\nFinally, the last bullet point where it appears that AGZ advanced its capabilities using less training data. It appears that the synthetic data generated by self-play has more \u2018teachable moments\u2019 than data that\u2019s derived from human play. Usually, the way to improve a network is to generate more synthetic data. The usual practice is to augment data by doing all sorts of data manipulations (ex. cropping, translations, etc), however in AGZ\u2019s case, the automation seemed to be able to select richer training data.\n\nAlmost every new Deep Learning paper that is published (or found in Arxiv) tends to show at best a small percentage improvement over previous architectures. Almost every time, the newer implementation also requires more resources to achieve higher prediction accuracies. What AlphaGo has shown is unheard of, that is, it requires an order of magnitude less resources and a less complex design, while unequivocally besting all previous algorithms.\n\nMany long time practitioners of reinforcement learning applied to games have commented that the actual design isn\u2019t even novel and has been formulated decades ago. Yet, the efficacy of this approach has finally been experimentally validated by the DeepMind team. In Deep Learning like in sports, you can\u2019t win on paper, you actually have to play the game to see who wins. In short, no matter a simple an idea may be, you just never know how well it will work unless the experiments are actually run.\n\nThere is nothing new about the policy iteration algorithm or the architecture of the neural network. Policy iteration is a old algorithm that learns improving policies, by alternating between policy estimation and policy improvement . That is, between estimating the value function of the current policy and using the current value function to find a better policy.\n\nThe single neural network that it uses is a pedestrian convolution network:\n\nLike the previous incarnations of AlphaGo, Monte Carlo Tree Search (MCTS) is used to select the next move. AlphaGo Zero takes advantage of the calculations of the tree search as a way to evaluate and train the neural network. So basically, MCTS employing a previously trained neural network, performs a search for winning moves. The policy evaluation estimates the value function from many sampled trajectories. The results of this search is then used to drive the learning of the neural network. So after every game, a new and potentially improved network is selected for the next self-play game. DeepMind calls this \u201cSelf-play reinforcement learning\u201d:\n\nWith each iteration of self-play, the system learns to become a stronger player. I find it odd that the exploitive search mechanism is able to creatively discover new strategies while simultaneous using less training data. It is as if self-play is feeding back into itself and learning to learn better.\n\nThis self-play reminds me of an earlier writing about \u201cThe Strange Loop in Deep Learning.\u201d I wrote about many recent advances in Deep Learning such as Ladder networks and Generative Adversarial Networks (GANs) that exploited a loop based method to improve recognition and generation. It seems that when you have this kind of mechanism, that is able to perform assessments of its final outputs, that the fidelity is much higher with less training data. In the case of AlphaGo Zero, there\u2019s is no training data to speak of. The training data is generated through self-play. A GAN for example, collaboratively improves its generation by having two networks (discriminator and generator) work with each other. AlphaGo Zero, in contrast pits the capabilities of a network trained in a previous game against that of the current network. In both cases, you have two networks that feed of each other in training.\n\nAn important question that should be in everyone\u2019s mind is: \u201cHow general is AlphaGo Zero\u2019s algorithm?\u201d DeepMind has publicly stated that they will be applying this technology to drug discovery. Earlier I wrote about how to assess the appropriateness of Deep Learning technologies (see: Reality Checklist). In that assessment, there are six uncertainties in any domain that needs to be addressed: execution uncertainty, observational uncertainty, duration uncertainty, action uncertainty, evaluation uncertainty and training uncertainty.\n\nIn the AlphaGo Zero, the training uncertainty, seems to have been addressed. AlphaGo Zero learns the best strategies by just playing against itself. That is, it is able to \u201cimagine\u201d situations and then discover through self-improvement the best strategies. It can do this efficiently because all the other uncertainties are known. That is, there is no indeterminism in the results of a sequence of actions. There is complete information. The effects of actions are predictable. There is a way to measure success. In short, the behavior of the game of Go is predictable, real world systems however are not.\n\nIn many real world contexts however, we can still build accurate simulations or virtual worlds. Certainly the policy iteration methods found here may seem to be applicable to these virtual worlds. Reinforcement learning has been applied to virtual worlds (i.e. video games and strategy games). DeepMind has not yet reported experiments of using policy iteration in Atari games. Most games of course don\u2019t need this sophisticated look ahead that requires MCTS, however there are some games like Montezuma\u2019s Revenge that do. DeepMind\u2019s Atari game experiments were like AlphaGo Zero, in that there was no need for human data to teach a machine.\n\nThe difference between AlphaGo Zero and the video game playing machines is that the decision making at every state in the game is much more sophisticated. In fact there is an entire spectrum of decision making required for different games. Is MCTS the most sophisticated algorithm that we will ever need?\n\nThere is also a question on strategies that require remembering one\u2019s previous move. AlphaGo Zero appears to only care about the current board state and does not have a bias on what it moved previously. A human sometimes may determine its own action based on its previous move. It is a way of telegraphing actions to an opponent, but it usually is more like a head fake. Perhaps that\u2019s a strategy that only works on humans and not machines! In short, a machine cannot see motion if it was never trained to recognize its value.\n\nThis lack of memory affecting strategy may in fact be advantageous. Humans when playing a strategy game will stick to a specific strategy until an unexpected event disrupts that strategy. So long as an opponent\u2019s moves are as expected, there is no need to change a strategy. However, as we\u2019ve seen in the most advanced Poker playing automation, there is a distinct advantage of always calculating strategy from scratch with every move. This approach avoids telegraphing any plans and therefore a good strategy. However, misdirection is a strategy that is effective against humans but not machines that are not trained to be distracted by them. (Editors Note: Apparently previous board states are used as input to the network, so appears this lack of memory observation is incorrect).\n\nFinally, there is a question about the applicability of a turn based game to the real world. Interactions in the real world are more dynamic and continuous, furthermore the time of interaction is unbounded. Go games have a limited number of moves. Perhaps, it doesn\u2019t matter, after all, all interactions require two parties that act and react and predicting the future will always be boxed in time.\n\nIf I were to pinpoint the one pragmatic Deep Learning discovery in AlphaGo Zero then it would be the fact that Policy Iteration works surprisingly well using Deep Learning networks. We\u2019ve have hints in previous research that incremental learning was a capability that existed. However, DeepMind has shown unequivocally that incremental learning indeed works effectively well.\n\nAlphaGo Zero appears also to have evolutionary aspects. That is, you select the best version of the newly latest trained network and you discard the previous one. There is indeed something going on here that is eluding a good explanation. The self-play is intrinsically competitive and the MCTS mechanism is an exploratory search mechanism. Without exploration, the system will eventually not be able to beat itself in play. To be effective, the system should be inclined to seek out novel strategies to avoid any stalemate. Like nature\u2019s own evolutionary process that abhors a vacuum, AGZ seems to discover unexplored areas and somehow take advantage of these finds.\n\nOne perspective to think about these systems as well as the human mind is in terms of the language that we use. Language is something that you layer more and more complex concepts on top of each other. In the case of AlphaGo Zero, it learned a new language that doesn\u2019t have legacy baggage and it learned one that is so advanced that it is incomprehensible. Not necessarily mutually exclusive. As humans, we understand the world with concepts that originate from our embodiment with our world. That is we have evolved to understand visual-spatial, sequence, rhythm and motion. All our understanding is derived from these basic primitives. However, a machine may possibly discover a concept that may simply not be decomposable to these basic primitives.\n\nSuch irony, when DeepMind trained an AI without human bias, humans discovered they didn\u2019t understand It! This in another dimension of incomprehensibility. The concept of \u201cincomprehensibility in the large\u201d in that there is just too much information. Perhaps there is this other concept, that is \u201cincomprehensibility in the small\u201d. That there are primitive concepts that we simply are incapable of understanding. Let this one percolate in your mind for a while. For indeed it is one that is fundamentally shocking and a majority will overlook what DeepMind may have actually uncovered!."
    },
    {
        "url": "https://medium.com/intuitionmachine/natural-stupidity-is-more-dangerous-than-artificial-intelligence-1250a437cdb4",
        "title": "Natural Stupidity is more Dangerous than Artificial Intelligence",
        "text": "Do you know what\u2019s more dangerous than artificial intelligence? Natural stupidity. In this article, I will explore natural stupidity in more detail and show how our current technology (driven by narrow artificial intelligence) is making us collectively dumber.\n\nWe\u2019ve all had this experience of using a GPS to guide us around an unfamiliar place only to realize later that we have no recollection or ability to get to that place again without the aid of a GPS. Not only is our directional instinct diminished because of lack of use, but so is our own memories. We\u2019ve all experienced losing our ability to recall due to our over use of Google. We now recall more as to how we can search for something rather than the details of that something.\n\nThe framework that I often use to explore intuition is the Cognitive Bias Codex found at Wikipedia. It\u2019s a massive list of biases, however to get an overview of it, there are four high level categories that are the the drivers of theses biases. These are \u201cToo Much Information\u201d, \u201cNot Enough Meaning\u201d, \u201cNeed to Act Fast\u201d and \u201cWhat Should we Remember?\u201d.\n\nOur world requires more automation to run efficiently and sustainably. The products and services that will be in demand are the products that compensate for our inadequacies. The clear downside of this is that with every assist, the less we exercise our already weak facilities.\n\nThe only people maintaining their smarts are the few people willing to constantly exercise their smarts. Meanwhile, we have a population that is becoming more out of shape and lazy with their own mental faculties. We imagine ourselves to being smarter because we can multi-task more. Yet, our brains have not evolved to do multi-tasking well. In fact, recent research have shown that pigeons have greater multi-tasking capabilities than humans. It is just ironic that we\u2019ve taken pride in our new found multi-tasking skills only to discover that we are dumber at it than pigeons!\n\nHowever, there is a far worse problem than automation making us dumber. The bigger problem is that other humans are aware that it can make us dumber and they are opportunistically exploiting our natural stupidity to influence our behavior. Over the decades, the industry of advertising has spend trillions of dollars inventing new ways to \u201cmotivate\u201d us to do new things without us being aware of its influence. The techniques to do this neatly falls under the exploitation of our cognitive biases. After all, if we were indeed all perfectly logical, then we\u2019ll likely spending our money in the most efficient way possible and very few companies will like us to do that. If we reduce our spending, our economies would stall and there would be an economic depression! (BTW, something is really wrong when we must accelerate our consumption so as to avoid economic stagnation)\n\nSo, \u201cNatural Stupidity\u201d is basically our lack of meaning, lack of memory, inability to think fast and inability to process too much information. The current systems that we have in place provide products and services to substitute these inabilities. It is the natural tendency to seek out the method of least action. That is, the method that requires lest effort or the laziest thing that we can do. Let\u2019s explore each of the four in greater detail.\n\nHumans from the beginning of the their life are driven to seek meaning. The simplest explanations to this are going to be the most natural appealing ones. Civilization will naturally create religion to not only create a necessary shared understanding of acceptable behavior but one that is driven by our need for meaning.\n\nThe written word (i.e. books) and its more advanced form, the world wide web are devices that address our limited memories. Memories require not only storage but also the capability of recall. Throughout history, religion and law has been transcribed in scrolls, books and now in automation (see: blockchain). Money is a form of memory, that is, once possession of it is a measure of one\u2019s ability to acquire goods and services.\n\nMankind created computers to automate the math that we invented. Computers not only store memories but are able to perform laborious and error-free computations. We find it an inconvenience to use cash in that we have to calculate in our minds the amount of change so as to guard against error or outright fraud. We have time keeping devices so that we don\u2019t need to look out into the heavens to determine the time of day. We have GPS devices to help us avoid reading a map and calculating a path to our destination.\n\nFinally, we have the problem of information overload. Our knowledge driven economies have accelerated our consumption of information. However, our brains have not magically evolved to process this fire-hose of information. The device that we use to process more information are services that curate information and exhaust it out in more easily digestible forms. Today, social networks such as Twitter and Facebook have become our primary tools for curating and receiving new information about the world. It is dumfounding that the leaders of these two companies believe it is not in their charter to \u2018police\u2019 the contents that they help propagate. With great power comes great responsibility, unless it I guess if it affects the bottom line!\n\nWe collectively become dumber when we relinquish responsibility and accountability to the automation (or A.I.) that furnishes us with cognitive assistance.\n\nWhen we avoid questioning the positions of our religious leaders and ignore obviously repugnant behavior in defense of our own beliefs.\n\nWe avoid verifying our history and cling to untrue historical information to justify our beliefs. This is the case for Neo-Nazis and Confederates who would like to imagine a more benevolent and just past.\n\nWe ignore common sense by following algorithms in enforcement of procedures. Like the United Airlines where a 70 year old doctor was assaulted and removed from an airline just because the crew blindly followed protocol instead of their own common sense.\n\nFinally, we don\u2019t hold accountable organizations that employ information overload in the form of massive disinformation to mold public opinion.\n\nWe shouldn\u2019t be worried about Artificial Intelligence taking over the world. The more immediate, clear and present danger is that Natural Stupidity has taken over this world and we are seeing daily occurrences of this in our public discourse.\n\nMore on how intuition, human behavior and artificial intelligence are tightly related:"
    },
    {
        "url": "https://medium.com/intuitionmachine/exploration-exploitation-and-imperfect-representation-in-deep-learning-9472b67fdecd",
        "title": "Exploration, Exploitation and Imperfect Representation in Deep Learning",
        "text": "The algorithms of learning can be coarsely abstracted as being a balance of exploration and exploitation. A balanced strategy is followed in the pursuit of a fitter representation. This representation can either be one that improves a model that is being learned or can be at the meta-level where it improves the algorithm that learns better models.\n\nIn exploitation, an automation greedily pursues a path of learning that provides immediate rewards. In exploration however, an automation must decide to forego an immediate reward and select instead a directionless exploration with the intent of discovering a greater reward elsewhere. The strategy to select one over the other is sometimes referred to as \u201cregret minimization\u201d. As a side, Jeff Bezos has a very human interpretation of this strategy:\n\nIt is also related to the idea of Counterfactual Regret Minimization (CFR). This method is used by Libratus a poker playing machine that has bested professional players. CFR is applicable in domains with imperfect-information. In short, the strategy of selecting exploration over exploitation is relevant to domains with imperfect information.\n\nStochastic Gradient Descent (SGD), the workhorse learning algorithm of Deep Learning, are algorithms that employ exploitation as its fundamental motivation. SGD works only for networks that are composed of differentiable layers. Convergence happens because there will be regimes in the parameter space that guarantee convergence of iterative affine transformations. This is well known in other fields such as Control Theory (known as Method of Adjoints) as well as in Chaos theory (Iterated Function Systems).\n\nHowever, exploration features are shoe-horned into classic gradient descent through different kinds of randomness. Examples of these are, the randomness in how training examples are presented, noise terms in the gradient, dropout and batch normalization. When we examine the two phases of gradient decent, we realize that the first phase is dominated by exploitation behavior. This is where we see a high signal to noise ratio, and the convergence is rapid. In this phase, second order methods that exploit the Natural Gradient (see: Fisher Information Matrix) will converge much faster. A recent method known as the Kronecker Factorization (K-FAC) that approximates the FIM has shown to exhibit 20\u201330 times less iterations than traditional first order methods.\n\nIn the compressive phase, exploration will dominate and randomization methods facilitate these explorations. In this regime, the gradients carry negligible information and thus the convergence is extremely slow. This is where representation compression occurs. The elusive goal of Generalization is achieved through the compression of representation. We can explore many interpretations as to what Generalization actually means, but ultimately, it boils down to the shortest expression that can accurately capture the behavior of an observed environment.\n\nEvolutionary algorithms (aka Genetic algorithms) occupy the space of exploration approaches. In Deep Learning, evolution algorithms are usually been employed in searching for architectures. It is a more sophisticated version of hyper-parameter optimization in that instead of juggling constants like learning rates, the search algorithm juggles the composition of each layer of a network. It is used as the outer loop of the learning algorithm. The thing though about evolutionary algorithms is that serendipitous discovery is fundamental. In short, it works only when you are lucky.\n\nEither method or a combination of both can lead to a fitter Representation. Let\u2019s deconstruct the idea of Representations. In a previous post, I discuss 3 different dimensions of intelligences that are being developed ( computational, adaptive and social). The claim is that these are different kinds of intelligences. What is apparently obvious is that the domains in which they operate are different from each other. So form will have to follow function. The methods and architectures that are developed for each kind of intelligence are going to be different from each other. One dimension of Representation is obviously the domain in which it is applicable.\n\nThere is of course a question whether we should explore different kinds of Representations. I mean this at a more general level. In Deep Learning, there are all kinds of different neural embeddings. These embeddings are vector representations of semantics. These vector spaces are learned over the course of training. These vectors are supposed to represent an invariant form of the actual concept. One major difficulty of Deep Learning systems is that these representations are extremely entangled. The fact that they are entangled can explain why Deep Learning systems have zero conceptual understanding of what they predict. Understanding requires the creation of concepts, if concepts cannot be factored out, then what does that imply for understanding? It is important to realize, that there are many cases where understanding is not needed for competence.\n\nI will argue that AlphaGo doesn\u2019t understand Go in the same way as humans. Humans understand Go by creating their own concepts behind the strategies they employ. AlphaGo doesn\u2019t have an understanding of these concepts. Rather, it has memory and the statistics of billions of moves and their consequences.\n\nThe concepts that exist as part of a Representation may exists in 3 forms. A generalizations, a prototype or an exemplar. Deep Learning focuses on creating generalizations through the capture of an invariant representation. This is why, data augmentation is a best-practice approach. So when working with images, images are rotated, cropped, de-saturated etc.. This trains the network to ignore these variations. In addition, Convolution Networks are designed to ignore image translations (i.e. difference in locations). The reason DL systems require many training sets is that it needs to \u201csee\u201d enough variations so that it can learn what to ignore and what to continue to keep relevant. Perhaps however that the requirement for invariances is too high and we should seek something less demanding in the form of equivariances.\n\nIn the realm of few-shot or zero-shot learning, where an automation must learn something by seeing it only once or a few times, then there is zero opportunity to discover invariances. An automation only has a few examples to create a prototype, that is representative of the entire class. So there needs to be some prior model that is capable of performing the appropriate similarity calculation. The system must know how to determine if an example is similar to a prototype.\n\nEven worse, if its just one example, then there isn\u2019t really a class and the system has to deduce a generalization. The implication of the latter is that, an automation requires that an internal model existing prior to any deduction.So we have here three kinds of models. A model-free representation (learned through induction), a representation for a similarity algorithm and a rich representation that can drive deduction (or abduction).\n\nIt is interesting that the human mind is able to recall unusual configurations, yet be unable to recall finer details. This is an example of attempts of people to draw the Apple logo:\n\nMeanwhile DL networks, that have zero understanding of an image, are much better at recreating images than average humans:\n\nNot many people have the ability to visualize in their head an image and recreate it properly via a drawing. I\u2019m unsure if this is a weakness in their articulation skills or that the details of an object aren\u2019t actually captured by the brain. In fact, human\u2019s are typically blind in many contexts:\n\nThe human mind simply does not have the same kind of photographic memory that a machine has. Its capacity is simply limited and requires throwing out a lot of information away so as to cope with information overload.\n\nIn a previous post, we explored how model-free and model-based cognition can be interleaved in the process of learning. Exploration and exploitation can also be interleaved in learning. However, both sets are orthogonal. As in SGD, you can have a model-free algorithm that uses both exploration and exploitation. You can also have model-based algorithms that explore or exploit. That is, there are at least three dimensions that are described here. One dimension is on the axis of exploration to exploitation. The second dimension is if the learning process is driven by a explicit model or not. The third dimension is the nature of the learned representation itself.\n\nBridging the semantic gap is still an extreme challenge."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-coordination-of-intuition-and-rational-thought-bb4b9e3e1ac8",
        "title": "The Coordination of Intuition and Rational Intelligence",
        "text": "In my writings, I\u2019ve continually emphasized the nature of the human mind as that of consisting of two cognitive systems. That is, the mind consist of an intuitive and a rational system. This is better known by psychologists as the Dual Process theory. However, this brings up an interesting question. If there are two kinds of cognition in our heads, then how perhaps do these two coordinate with each other to get anything done?\n\nInterestingly enough, I\u2019ve stumbled upon a paper that discusses this in greater detail. I found the paper through a recent blog post by DeepMind. DeepMind is known as a big proponent of the use of Reinforcement Learning. They have successfully combined Deep Learning with Reinforcement Learning in their Atari game playing system and AlphaGo. There are two approaches to Reinforcement Learning, one is model-based and the other is model-free. The former performs its actions based on an internal model (that is programmed) and the later performs its actions based on learning through induction ( using Deep Learning as a special case).\n\nDeepMind has a new paper that explores recent discoveries about the brains\u2019 hippocampus (re: The hippocampus as a 'predictive map'). The idea here is that the state of the environment should be approximated by the current state and the possible future states. This is formally known as \u201cSuccessor Representation\u201d. Apparently, this is a new approach to exploring Reinforcement Learning.\n\nInterestingly enough, there is a paper referenced in the blog post that does discuss the competition and cooperation between model-based and model-free RL. The analogy that is made in the paper is that model-free RL is our habitual system in play (i.e. Intuition) and model-based system is our planning system in play (i.e. Rational). The paper explores the different ways these two system work in both competitive and cooperative interaction. The selection of which cognitive mechanism to use can be based on the need for efficiency versus accuracy.\n\nWhat caught my interest is the cooperative interaction. The paper discusses three kinds of cooperation:\n\n(1) The Intuition can learn from simulations from a model.\n\nThe latter two cooperative modes were exhibited to great effect by AlphaGo. AlphaGo used Monte Carlo Tree Search to search the space of good moves. It used Deep Learning to essentially prune the search tree to something more manageable. In addition, the value and policy function used for each game state employed Deep Learning.\n\nThe first kind of cooperation, I hinted at in a previous article (see: \u201cA Language Driven Approach for Deep Learning Training\u201d). It is the same mechanism used by Microsoft in its DeepCoder paper. I wrote earlier, this hybrid system of combining traditional algorithms with Deep Learning pattern recognition can be an extremely potent combination. In fact, there are plenty of low hanging fruit application where this approach is extremely effective.\n\nBiological brains as a consequence of adapting to their natural environments have visual-spatial, motion, sequence and rhythmic recognition capabilities. My conjecture is that this is essentially all that we need and that planning and rational thought is emergent from these more basic capabilities. Our biological brains don\u2019t have the kind of specialized logical hardware that you find in computers. Rather, we perform a kind of virtual machine simulation using mechanisms that are not optimized for this kind of task. It isn\u2019t very efficient as compared to specialized computation elements we find in computers, but it is good enough.\n\nThe major problem of Artificial General Intelligence (AGI) is bridging the semantic gap between an induction based system and a deduction based system. How do concepts that are learned via data induction become symbolic concepts that a deduction process can process? How can a model-free system create models? How does our brain capture our experiences and create concepts and ideas. How are we able to turn these ideas into invent language to communicate? The key to AGI is in understanding the interface between intuition and rational thought."
    },
    {
        "url": "https://medium.com/intuitionmachine/from-narrow-to-general-ai-e21b568155b9",
        "title": "From Narrow to General AI \u2013 Intuition Machine \u2013",
        "text": "The original vision of AI some 60 years ago was to build machines that can think, learn, and reason like humans. Initial optimism of achieving this in just a few years was grossly misplaced, and indeed continued to haunt AI for decades. As researchers failed to get anywhere near the flexibility and general cognitive ability of humans they turned their focus to solving very specific, narrow problems of \u2018intelligence\u2019 \u2014 And to this day \u2018AI\u2019 is practiced almost entirely this way. In fact, a good number of current AI researchers and developers typically aren\u2019t even aware of the original meaning of AI!\n\nA recent breakthrough in artificial intelligence called deep learning (DL) has been hailed as a major breakthrough\u2014 it changes the development dynamic from having to \u2018program\u2019 computers to \u2018teaching\u2019 them. This characterizing has some merit, but glosses over the significant human expertise and \u2018tweaking\u2019 required to make these systems work. Moreover, if anything, these systems are even narrower than previous approaches: Their range of capabilities is almost entirely determined by their training data, while traditional approaches can in principle allow for real-time adaptive learning.\n\nMany contemporary AI systems are quite powerful, yet almost always highly specialized and rigid: Speech recognition systems trained on telephone input won\u2019t do well on microphone dictation; image recognition trained for freeway driving can\u2019t cope with urban roads; world champion Chess, Jeopardy!, and Go systems can\u2019t play even basic Tic-Tac-Toe; and order taking chatbots can\u2019t even remind you when your parking meter runs out; and so on\u2026\n\nThe core problem of current AIs is not so much that they are narrow \u2014 specialization can be very helpful \u2014 but that they are inherently narrow (narrow by design), and that they are rigid, fixed. Both traditionally programmed as well as the newer, \u2018trained\u2019 AIs suffer the same basic limitation: whatever capabilities they have, are pretty much frozen in time.\n\nIt is true that narrow AI can be designed to allow for some limited learning or adaptation once deployed, but this is actually quite rare. Typically, in order to change or expand functionality requires either additional programming, or retraining (and testing) with a new dataset.\n\nThere are two distinct but related problems with this dynamic. Firstly, narrow AI systems cannot adapt dynamically to novel situations \u2014 be it new perceptual cues or situations; or new words, phrases, products, business rules, goals, responses, requirements, etc. However, in the real world things change all the time, and intelligence is by definition (see below) the ability to effectively deal with change!\n\nThe second and more important issue is that narrow AIs do not possess their own intelligence to be able to think, learn or solve problems \u2014 rather they embed the programmers solution to the (perceived) narrow problem that they are designed to solve. Such frozen intelligence really is no intelligence!\n\nTo summarize, we\u2019ll need more than the contemporary approaches to achieve anything resembling human intelligence \u2014 current applications are extremely narrow in scope, and cannot use their current knowledge and skills to acquire skills in new domains. Also, they typically cannot learn directly from user interaction, have no memory and will make the same mistakes over and over again. Furthermore, they cannot reason or explain themselves, have very limited understanding and no common sense.\n\nNot something one would call smart.\n\nIntelligence, in general, is the cognitive ability to understand the world; to help achieve a wide variety of goals; and to integrate new knowledge and skills in ongoing learning. It must operate in real time, in the real world, and with limited knowledge and time.\n\nMoreover, human intelligence (as opposed to animal intelligence) is special in that it features the ability to form and use highly abstract concepts, and to think and reason using symbols \u2014 i.e. to handle natural language.\n\nArtificial intelligence should be no different. Truly intelligent AI \u2014 often called Artificial General Intelligence (AGI) \u2014 must embody at least the following essential abilities:\n\nIn addition, there are strong arguments to be made that intelligence requires some kind of sensory- motor grounding in reality. Clearly, understanding the dynamics of the real world is crucial for being able to reason about human affairs. It requires a grasp of such characteristics as size, shape, and texture of objects, as well as temporal and spatial relationships. Abstract concepts must be \u2018grounded\u2019 in some way to underlying objects, changes, and movements in the world.\n\nHowever, it may be possible to \u2018simulate\u2019 perception and action within a computer \u2018brain\u2019. Whether such shortcuts will be adequate for good common sense reasoning remains an open question.\n\nAchieving truly intelligent, general AI will require both engineering solutions as well as the right commercial dynamic.\n\nOn the technology side, we most likely need a fundamentally different approach \u2014 perhaps a \u2018Third Wave of AI\u2019.\n\nA good case can be made that an approach called cognitive architectures can provide the framework to achieve real intelligence \u2014 though this is not yet a mainstream view.\n\nCognitive architectures approaches have a long history in AI. Essentially, it is the idea of having a core \u2018cognitive engine\u2019 that supports all the basic functionality required for general intelligence. It is a uniform, highly-integrated system of support structures and functions that, working together, produce all the cognitive abilities required for general intelligence. These include short- and long-term memory, pattern matching, prediction, prioritization, reasoning, planning, and many others.\n\nThere are several important advantages to this approach:\n\nSo much for the engineering side.\n\nWhile it is becoming increasingly obvious that Narrow AI cannot provide all the intelligence and robustness we need, there are strong practical and commercial forces holding back development of AGI.\n\nFor one, narrow AI has several significant advantages: The designer can focus on just solving one concrete problem at a time; they can directly inject a lot of specific human knowledge into the system; and they can use any engineering technique appropriate for that particular application. In particular, they can ignore all limitations that may apply to other use cases.\n\nAlso, not having to figure out how to imbue the system with real intelligence, they can get to market faster \u2014 and from there, can incrementally improve the functionality of a given application. In fact, an often unspoken, but mistaken assumption is that we can get to from Narrow to General AI incrementally.\n\nOn the other hand, what discourages development of Artificial general intelligence is that it is much harder. It has to be inherently designed to function well over a large range of domains and dynamic situations\u2014 just like humans do. Furthermore, it has to be able to autonomously acquire knowledge and skills that may not have been anticipated at design time.\n\nGeneral AI must in principle be able to utilize strategies learned in Chess to improve business negotiations, or more reasonably, to use a text description to help it visually identify a new type of object, or to use information from a conversation to later solve a problem using logical reasoning.\n\nAnother key requirement is for the system to handle ambiguities and missing knowledge \u2014 it must know when it doesn\u2019t know or understand something, and know how to remedy that. It must generally be robust in the face of limited knowledge and computational resources, and not fail catastrophically.\n\nWhile the value, indeed the need, for these features is becoming increasingly clear, it is also obvious that developing AGI-ish designs will require a more visionary approach and considerable foresight. Again: Real AI is really hard!\n\nShortcomings of Narrow AI are perhaps most obvious in applications involving natural language understanding. Users of interactive language-based systems should reasonably expect them to be intelligent enough to hold productive, lengthy conversations; to remember and take into account what transpired earlier; to learn a user\u2019s preferences and goals; and, most importantly, to interactively improve and extend their knowledge and skills, and to learn from their mistakes. We want our personal virtual assistants and chatbots to be more like \u2018Her\u2019, and less like \u2018Dory\u2019.\n\nHuman language is integral to our intelligence and to all of human achievement. Similarly, synthetic intelligence will require full natural language competence. While AI development focused on robotics alone (i.e. perception and dexterity) is also important, ultimately these systems will also need to be able to read and follow language instructions, and to be able to explain their decisions in natural language.\n\nAGI-based designs will by definition, and by design, cover such advanced requirements. They will have deeper understanding, short- and long-term memory, reasoning ability, and a good amount of common sense. They will also be able to handle much more complex conversations.\n\nPerhaps a combination of demonstrable progress with cognitive architectures, together with increasing demand for improved natural language systems will re-ignite, and hopefully realize, the original dream of AI \u2014 to have AI with their own intelligence, not just frozen bits of human ingenuity."
    },
    {
        "url": "https://medium.com/intuitionmachine/google-and-ubers-best-practices-for-deep-learning-58488a8899b6",
        "title": "Google and Uber\u2019s Best Practices for Deep Learning \u2013 Intuition Machine \u2013",
        "text": "There is more to building a sustainable Deep Learning solution than what is provided by Deep Learning frameworks like TensorFlow and PyTorch. These frameworks are good enough for research, but they don\u2019t take into account the problems that crop up with production deployment. I\u2019ve written previously about technical debt and the need from more adaptive biological like architectures. To support a viable business using Deep Learning, you absolutely need an architecture that supports sustainable improvement in the presence of frequent and unexpected changes in the environment. Current Deep Learning framework only provide a single part of a complete solution.\n\nFortunately, Google and Uber have provided a glimpse of their internal architectures. The architectures of these two giants can be two excellent base-camps if you need to build your own production ready Deep Learning solution.\n\nThe primary motivations of Uber\u2019s system named Michelangelo was that \u201cthere were no systems in place to build reliable, uniform, and reproducible pipelines for creating and managing training and prediction data at scale.\u201d In their paper, they describe the limitations of existing frameworks with the issues of deployment and managing technical debt. The paper has enough arguments that should convince any skeptic that existing frameworks are insufficient for the production.\n\nI\u2019m not going to go through Uber\u2019s paper with you in its entirety. Rather, I\u2019m just going to highlight some important points about their architecture. The Uber system is not a strictly Deep Learning system, but rather a Machine Learning system that can employ many ML methods depending on suitability. It is built on the following open source components: HDFS, Spark, Samza, Cassandra, MLLib, XGBoost, and TensorFlow. So, it\u2019s a conventional BigData system that incorporates Machine Learning components for its analytics:\n\nThe architecture supports the following workflow:\n\nUber\u2019s Michaelangelo architectures is depicted as follows:\n\nI am going to skip over the usual Big Data architecture concerns and point out some notable ideas that relates more to machine learning.\n\nMichaelangelo divides the management of data between online and offline pipelines. In addition, to permit knowledge sharing and reuse across the organization, a \u201cfeature store\u201d is made available:\n\nUber created a Domain Specific Language (DSL) for modelers to select, transform and combine feature prior to sending a model to training and prediction. Currently supported ML methods are decision trees, linear and logistic models, k-means, time-series and deep neural networks.\n\nThe model configuration specifies type, hyper-parameters, data source references, the feature DSL expressions and compute resource requirements (i.e. cpus, memory, use of GPU, etc.). Training is performed in either a YARN or Mesos cluster.\n\nAfter model training, performance metrics are calculated and provided in an evaluation report. All of the information, that is the model configuration, the learned model and the evaluation report are stored in the a versioned model repository for analysis and deployment. The model information contains:\n\nThe idea is to democratize access to ML models, sharing it with other to improve organizational knowledge. The unique feature of Uber\u2019s approach is the surfacing of a \u201cFeature Store\u201d that allows many different parties to share their data across different ML models.\n\nThe folks at Google have a recent paper \u201cTFX: A TensorFlow-based production scale machine learning platform\u201d that details their internal system.\n\nThe paper is structured similarly to Uber\u2019s paper in that they cover the same workflow:\n\nGoogle\u2019s architecture is driven by the following stated high level guidelines:\n\nLet\u2019s dig a little deeper into the unique capabilities of Google\u2019s TFX. There are plenty of tidbits of wisdom as well as an introduction of several unique capabilities.\n\nTFX provides several capabilities in the scope of data management. Data analysis performs statistics on each dataset providing information about value distribution, quantiles, mean, standard-deviation etc. The idea is that this allows users to quickly gain insights on the shape of dataset. This automated analysis is used to improve the continuous training and serving environment.\n\nTFX handles the data wrangling and stores the transformations to maintain consistency. Furthermore, the system provides are uniform and consistent framework for managing feature-to-integer mappings.\n\nTFX proves a schema that is version that specifies the expectations on the data. This schema is used to flag any anomalies found and also provide recommendations of actions such as blocking training or deprecating features. The tooling provide auto-generation of this schema to make it easy to use for new projects. This is a unique capability that draws inspiration from the static type checking found in programming languages.\n\nTFX uses TensorFlow as its model description. TFX has this notion of \u2018warm-starting\u2019 that is inspired by transfer learning technique found in Deep Learning. The idea is to reduce the amount of training by leveraging existing training. Unlike transfer learning that employs an existing pre-trained network, warm-starting selectively identifies a general features network as its starting point. The network that is trained on general features are used as the basis for training more specialized networks. This feature appears to be implememented in TF-Slim.\n\nTFX uses a common high level TensorFlow specification (see: TensorFlow Estimators: Managing Simplicity vs. Flexibility in High-Level Machine Learning Frameworks ) to provide uniformity and encode best practices across different implementations. See this article on Estimators for more detail.\n\nTFX uses the TensorFlow Serving framework for deployment and serving. The framework allows different models to be served while keep the same architecture and API. TensorFlow Serving provies a \u201csoft model-isolation\u201d to allow multi-tenant deployment of models. The framework is also designed to support scalable inferences.\n\nThe TFX paper mentioned the need to optimize the deserialization of models. Apparently, a customized protocol buffer parses was created to improve performance up to 2\u20135 times.\n\nDissecting Uber and Google\u2019s internal architecture provides good insight on pain-points and solutions for building your own internal platform. As compared to available open source DL frameworks, there is a greater emphasis in managing and sharing of meta-information. Google\u2019s approach also demands additional effort to ensure uniformity as well as automated validation. These are practices that we have seen previously in conventional software-engineering projects.\n\nSoftware engineering practices such as Test Driven Development (TDD), continuous integration, rollback and recovery, change control etc. are being introduced into advanced machine learning practices. It is not enough for a specialist to develop on a Jupyter notebook and throw it over the wall to a team to make operational. The same end-to-end devops practices that we find today in the best engineering companies are also going to be demanded in machine learning endeavors. We see this today in both Uber and Google, and thus we should expect it in any sustainable ML/DL practice."
    },
    {
        "url": "https://medium.com/intuitionmachine/brains-are-intuition-machines-and-are-driven-by-discrete-computation-e210d2e03501",
        "title": "Are Biological Brains Made Of Only Discrete Logic? \u2013 Intuition Machine \u2013",
        "text": "I\u2019ve come up with perhaps a controversial opinion as to how biological brains work. I am posting this to facilitate more discussion. I have two opinions, the second more surprising than the first. My first opinion is that biological brains, more specifically human brains, are intuition machines. Intuition is that parallel cognitive process that we develop by learning using induction. Said differently, we learn from experience. We can\u2019t just upload knowledge of Kung Fu and instantly master the art. Humans require years of practice, perhaps 10,000 hours to gain mastery of a skill. Anil Seth has the same conclusion, he makes the argument that we are all \u201cbeast machines\u201d.\n\nThe failure of Good Old Fashioned AI (GOFAI) may precisely be due to the fact that human cognition is not based on logic. Rather, human cognition is a heuristic system that is heavily flawed but can react and adapt extremely rapidly. Rational thought and language are capabilities that are not intrinsic to our cognitive capabilities, but rather are capabilities that our intuitive mind takes an unnatural effort of performing. Pei Wang has been working on an AGI system called NARS that takes this approach of beginning with heuristics rather than formal logic.\n\nComputers are logic machines, computers have several orders of magnitude more capable in performing logic than humans. A simple hand calculator has more arithmetic intelligence than any human alive. Yet, despite all its logic crunching capability, Computers are extremely brittle in their programming. In contrast, a house fly exhibits an order of magnitude more flexibility and adaptability that a supercomputer.\n\nThe second opinion is that brains function using discrete computation. Computers also function using discrete computation, that is, machines use a binary collection of NAND or NOR gates. NAND or NOR gates are universal logic components and any universal computer can be constructed with either one of these components. All the evidence about biological neurons point to the fact that their behavior is discrete. That is, synapses fire in discrete events. There is very little evidence that brains are analog systems. That is, unlike an Artificial Neural Network that is informed by continuous mathematics, real neurons don\u2019t work like analog systems.\n\nThe conclusion is clear, the brain works more like a computer than like a continuous system like the weather. I am of course not the first person to arrive at this controversial conclusion. Stephen Wolfram has in fact a more far reaching conclusion, that is, all physical phenomena are driven by discrete computation. In his book \u201cA New Kind of Science\u201d, Wolfram explores the idea that \u201cHow will science look if computers were discovered before Newton\u2019s calculus?\u201d\n\nWolfram explains that complexity in nature can be attributed to the computational processing of simple components. There simply is no need for over ornate Byzantine mathematical theories and that the root cause of complexity emerges from simplicity. Wolfram hasn\u2019t developed an air-tight proof of this yet, however it does help to wonder why at the quantum level, matter (and energy) are discrete. The difficulty that Wolfram faces is that there simply does not exist a method to engineer (or learn) solutions using only discrete components.\n\nHowever, from this perspective of brains being discrete, how does it happen that our brains are more adapted to more continuous based behavior? Why does Deep Learning work so well in approximating biological cognitive behavior when its built on top of continuous mathematics? How can one train discrete systems to learn like Deep Learning systems?\n\nDeep Learning systems have a surprising characteristic that they don\u2019t require high precision arithmetic. This is in stark contrast to computational science workloads that require double precision mathematics. The present trend in Deep Learning is to employ smaller precision mathematics. At present 16 bit floating point precision appears good enough. Google\u2019s first generation Tensor Processing Unit (TPU) used 8 bit fixed precision arithmetic. There is also several research papers that look at binary or ternary based systems. The most well known of this is the XNOR-Net, where a startup (XNOR.ai) was able to raise $2.6m to explore deep learning in small device configurations.\n\nXNOR-nets and their other discrete cousins are not as accurate as higher precision networks. However, they require up to 58 times less memory. You won\u2019t see as much research in this area because their is a discipline bias towards higher accuracy. The discipline still gives a lot of importance to more resource efficient networks. However, the prevailing orthodoxy here is that deep learning are approximations of continuous system. The fact that XNOR-nets ever work at all is glaring evidence that the use of continuous mathematics is more for convenience than for necessity.\n\nExtending one\u2019s research to the extreme, towards discrete systems, is counter the prevailing wisdom. However, what if the prevailing wisdom is entirely wrong? What if deep learning systems should be designed similar to brains? That is, what if deep learning systems should be using only discrete components? What if we get rid of our crutches (i.e. continuous mathematics) and accept the more intractable space of discrete mathematics?\n\nThe problem at first glance is that \u2018intuition machines\u2019 and \u2018discrete computation\u2019 appear conceptually at odds with each other. However, when we speak about \u2018intuition machines\u2019, we speak more about the kind of reasoning that is being performed. There is no reason why you can\u2019t program a computer to perform heuristic reasoning. The problem here is that programmers aren\u2019t very good at taking a collection of heuristic rules and build a complex set of rules that avoiding stepping over and invalidating each other.\n\nOur brains simply don\u2019t have the capacity to handle hundreds, much less millions of rules. We can\u2019t program these systems because we just don\u2019t have the mental capacity to program these systems. What we do is we create Machine Learning systems that program these rules for us. Random Forests and its relatives is one example of these rule creation systems. Artificial Neural Networks (i.e. Deep Learning) is another kind but with fuzzier rules. So, intuition and discrete computation are not conceptually at odds. Intuition is computation and doesn\u2019t have to be done using an analog system.\n\nRepresenting biology using discrete computation is actually not a new thing. Boolean Networks have been used to model biological regulatory processes. One may think of this as a crude approximation of reality, however there is enough research results that has revealed its effective predictive value ( convergence and robustness of) . So this idea of biological brains being made up of gates isn\u2019t an out of this world idea.\n\nTo answer the question of this post. No, I don\u2019t think a brain is composed of NOR or NAND gates. I do however think it is composed of similar discrete universal gates most likely of the programmable variety. I am also not saying that it is uniformly one kind. Evolution has a habit of selecting diversity, so it is likely a smorgasbord of discrete gates. What I don\u2019t believe is that brains are made up of analog components (i.e. no evidence that neurons are analog, they either fire or they don\u2019t) or that brains use Quantum effects (as postulated by Penrose without any evidence).\n\nExplore more in this new book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/should-deep-learning-use-complex-numbers-edbd3aac3fb8",
        "title": "Should Deep Learning use Complex Numbers? \u2013 Intuition Machine \u2013",
        "text": "Is it not odd to anyone that Deep Learning uses only real numbers? Or perhaps, it would be even odder if Deep Learning uses complex numbers (note: the kind with imaginary numbers). One viable argument is that it is highly unlikely that the brain uses complex numbers in its computation. However, you can make the argument also that the brain doesn\u2019t perform matrix multiplication or perform chain rule differentiation. Besides, Artificial Neural Networks (ANN) have a cartoonish model of actual neurons. We\u2019ve long past replaced biological plausibility with real analysis (i.e. theory of function with real variables). Deep Learning researchers have been patting themselves on the back when they discovered that linear algebra and a sprinkling of basic calculus (i.e. chain-rule) was more than enough math to show groundbreaking results.\n\nHowever, why should we even stop with real analysis? We\u2019ve already bet the kitchen sink on linear algebra and differential functions, we might as well just go all in and bet the farm on complex analysis. Perhaps the weirder world of complex analysis will endow us with more powerful methods. After all, if it worked for Quantum Mechanics, then perhaps it may just work for Deep Learning. Besides, Deep Learning and Quantum Mechanics are both all about information processing, both could just be the same thing!\n\nSo for arguments sake, let\u2019s shelve any thought about the need for biological plausibility. That\u2019s an old argument that we\u2019ve passed back in the 1957 when the first ANN was proposed by Frank Rosenblatt. Let the Numenta, Neuromorphic and Connectome folks worry about that hard problem. Deep Learning has a lot more pressing problems to fry. So the question then is, what can complex numbers provide that real numbers cannot?\n\nIn the last couple of years, there have been a few papers that have explored the use of complex numbers in Deep Learning. Surprisingly enough, a majority of them have never been accepted into a peer-reviewed journal. Deep Learning orthodoxy is simply prevalent in the discipline. However, let\u2019s review some of the interesting papers.\n\nDeepMind has a paper \u201cAssociative Long Short-Term Memory\u201d (Ivo Danihelka, Greg Wayne, Benigno Uria, Nal Kalchbrenner, Alex Graves) that explores the use of complex values for an associative memory. The system is used to augment the memory of an LSTM. The conclusion of the work is that the use of complex numbers yields higher memory capacity networks. The tradeoff in terms of the mathematics is that the use of complex numbers requires smaller matrices as compared to just using real numbers. The following graph shows that there is a measurable difference (as compared to traditional LSTM) in memory costs:\n\nYoshua Bengio and his team in Montreal have explored another aspect of the use of complex values. In a paper titled \u201cUnitary Evolution Recurrent Neural Networks\u201d (Martin Arjovsky, Amar Shah, Yoshua Bengio) the reseachers explore Unitary matrices. They argue that there may be real benefits in terms of reducing vanishing gradients if the eigenvalues of a matrix are close to 1. In this research, they explore the use of complex values as the weights of the RNN network. The conclusion of this work is:\n\nWhere they take several measurements to quantify the behavior vs more traditional RNNs:\n\nA system using complex values clearly has more robust and stable behavior.\n\nA paper also involving Bengio\u2019s group and folks at MIT ( Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin Solja\u010di\u0107, Yoshua Bengio ) extend the approach with the use of Gating mechanism. The paper \u201cGated Orthogonal Recurrent Units: On Learning to Forget\u201d (aka GORU) explores the possibility that long term dependencies are better captured and that can be lead to a more robust forgetting mechanism. In the following graph, they show that other RNN based system fail in the copying task:\n\nA team at FAIR and EPFL ( Cijo Jose, Moustpaha Cisse and Francois Fleuret ) has a similar paper in \u201cKronecker Recurrent Units\u201d where they also use unitary matrices to show viability in the copying task. They show a method of matrix factorization that greatly reduces the parameters required. The paper describes their motivation of using complex values:\n\nOne of the gems in this paper is this very insightful architectural idea:\n\nSo far, these methods have explored the use of complex values in RNNs. A recent paper from MILA \u201cDeep Complex Networks\u201d ( Chiheb Trabelsi et al.) further explores the approach in its use to convolution networks. The authors test their network on vision tasks, with competitive results. Yann LeCun, the inventor of convolution networks, also has a paper \u201cA mathematical motivation for complex-valued convolutional networks\u201d, that explores the rational for using complex numbers.\n\nFinally, we have to mention something about its use in GANs. After all, this seems to be the hottest topic. A paper \u201cNumerics of GANs\u201d (by Lars Mescheder, Sebastian Nowozin, Andreas Geiger ) explores the troublesome convergent properties of GANs. They explore the characteristics of the Jacobian with complex values. Which they use to create a state-of-the-art approach to the problem of GAN equilibrium.\n\nIn a post last year, I wrote about the relationship between the Holographic Principle and Deep Learning. The approach explored the similarity of Tensor networks with that of Deep Learning architectures. Quantum mechanics can be thought of using a more generalized form of probability:\n\nThe use of complex numbers permits additional capabilities that can\u2019t be found in normal probability. More specifically, the capability of superposition and interference. So to achieve holography, it\u2019s always nice to have complex numbers at your disposal.\n\nA majority of mathematical analysis that is performed in the machine and deep learning spaces tend to use Bayesian ideas as their arguments. Actually most practitioners think its Bayesian but it really comes from statistical mechanics (despite the name, there\u2019s no mumbo-jumbo statistics speak in stat-mech). Yann LeCun actually caught the evidence and he has it all in a tape.\n\nBut, if Quantum Mechanics is a generalized form of probability, then what would happen if we use QM inspired methods instead? It turns out that research has previously done on this, and the results are worthy of note. In a paper written late last year, \u201cQuantum Clustering and Gaussian Mixtures\u201d the authors (Mahajabin Rahman, Davi Geiger) explored the use in unsupervised k-means scenario. They report the following:\n\nSo one has to wonder, why are people stuck with an 18th century Bayes Theorem when there exists a 20th century (i.e. Quantum Mechanics) theory of probability? (Note: It\u2019s just shocking that the cargo-cult science of Statisticians have been running their farce since the 18th century)\n\nThe research papers mentioned here shows that there indeed many \u201creal\u201d advantages of using complex values in deep learning architectures. The research indicates more robust transmittal of gradient information across layers, higher memory capacity, more precise forgetting behavior, drastically reduced network sizes for sequences and greater stability in GAN training. These are too many advantages that cannot be simply ignored. If we are to accept the present Deep Learning orthodoxy of any layer that differentiable is fair game, then perhaps we should make use of complex analysis where there is a lot more variety in the grocery store:\n\nPerhaps one reason complex numbers aren\u2019t used as often is the lack of familiarity by researchers. The mathematical heritage of the optimization community doesn\u2019t involve the use of complex numbers. There\u2019s little need for complex numbers in Operational Research. Physicists on the other hand use it all the time. Those imaginary numbers keep popping up all the time in quantum mechanics. It isn\u2019t weird, it just happens to reflect reality. We still have little understanding of why these DL systems work so well. So seeking out alternative formulations could lead to some unexpected breakthroughs. This is the game we play today, the team that accidentally stumbles on the AGI breakthrough wins the entire pot!\n\nIn the near future, the tables may turn. The use of complex values may be more common place in SOTA architectures and its absence may turn out to be odd. I guess when that happens, the 18th century Bayesians will finally be out of business.\n\nExplore more in this new book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-brute-force-method-of-deep-learning-innovation-58b497323ae5",
        "title": "Is Deep Learning Innovation Just Due to Brute Force?",
        "text": "You have to wonder these days about the practice of Deep Learning. It indeed is akin to the black arts. Incorporating lots of alchemy and black magic. Knowledge of best practices, the do\u2019s and don\u2019ts are spread across thousands of unverified Arxiv papers. The usual claims of state-of-the-art (SOTA) is questionable and more likely due to cherry picked data. A majority of papers supposedly move the field forward by a tiny incremental percentage improvements in a standard benchmark. If a research can\u2019t show improvement in one benchmark, there are plenty more to chose from! Alternatively, a researcher can invent his new kind of test.\n\nA lot of the mathematics are mostly handwaving exercises, all desperate to prove that there\u2019s some rational thought that goes into the design process. The truth is, it\u2019s all just a bunch of approximations all over the place. Employing an explicit function or a distribution is a random shot in the dark. Many cases, its just best to use a neural network in replacement of any closed from equation.\n\nBrute force practices are prevalent. In one paper by Google, reseachers decided to show the quickest ever training of Imagenet. It took their design 24 minutes to train. The researchers have the following luxurios claim:\n\nIn another research with a claim akin to the show \u2018lifestyles of the rich and famous.\u201d Another Google team used their vast hardware resources to perform an exploratory search for more efficient deep learning architectures. In a Google\u2019s blog post \u201cUsing Machine Learning to Explore Neural Network Architecture\u201d, has a referred paper by Barret Zoph and Quoc V. Le which reads:\n\nThis paper is of course much better than the speed test paper. However, what it does clearly show is that, you can discover novel and innovative new architectures through brute force computation. The researchers discovered this monstrosity of an LSTM node:\n\nThe same team has an even newer paper where they train the search algorithm to discover new optimization methods (see: \u201cNeural Optimizer Search with Reinforcement Learning\u201d ). Their previous research searched for new kinds of deep learning layers.\n\nThey found these two curious optimizers, christened as \u201cAddSign\u201d and \u201cPowerSign\u201d. This image shows the behavior of PowerSign as compared to other optimization methods:\n\nThe idea is simple, grab from a collection of these mathematical artifacts:\n\nshake them all aggressively in a bag, and presto, you got your state-of-the-art (SOTA) optimization method! (Viewer discretion, do not do this from home without professional deep learning resources). The authors used a less lavish GPU cluster. That is 12 machines with 8 GPUs. That\u2019s 96 GPUs if you do you multiplication right. An 8 GPU machine like Nvidia\u2019s DGX-1 is listed at $150,000. In short, Cost of Twelve Nvidia DGX-1\u2019s, 1.8 million dollars. Discovery of AddSign and PowerSign \u2014 Priceless.\n\nBrute force methods are incidentally a meta-learning technique. In fact, the entire A.I. field, in its search for learning algorithms, is basically meta-learning. Hyper-parameter optimization is a meta-learning technique. Randomly combining many kinds of operators is just a more sophisticated version of hyper-parameter optimizations. Why stick to constants when you can use a variety of functions? If you are seeking SOTA, then the search for diversity is your ticket!\n\nThis reality of using brute force methods to achieve innovation isn\u2019t going to go away soon. Massive hardware resources in Deep Learning are like super-colliders in High Energy Physics. Both are experimental sciences, and both kinds of machines allow us to peer deeper into how reality works. You may be surprised that I speak about virtual systems (i.e. computation) as being reality. However, it shouldn\u2019t take one to make a great intellectual leap to realize that our entire universe is indeed all about information processing (see: \u201cDeep Learning is Non-equilibrium Information Dynamics\u201d).\n\nI have to go back to Stephen Wolfram\u2019s \u201cA New Kind of Science\u201d to explain the nature of computational systems. The behavior of machines that have the property of \u201cUniversality\u201d (example: weather, brain or computers) cannot be precisely predicted using mathematical shortcuts. It is not that they are entirely random and unpredictable, but rather that we can only make approximations of behavior in the incremental time horizons. However, the further out in time, the poorer predictions become. The implication of this is that, the design of these systems, can only be found by just trying out a combinatorially high number of design configurations.\n\nIs it entirely brute force, or is there some principles of alchemy at play here? My hunch is that it boils down to having good curriculums. Just as teachers have to work with difficult and unruly students, we will just have to discover the teaching methods to move forward. Brute force methods are an intrinsic part of this field. However, we should always strive to seek out research that does give us a better intuition ( See: \u201cICLR 2017\u201d and \u201cTwo Phases of Gradient Descent\u201d).\n\nDeepMind introduced a new brute force method that uses random search and guided search (see: https://deepmind.com/blog/population-based-training-neural-networks/ ). Here\u2019s some visualization of the monster that they created:"
    },
    {
        "url": "https://medium.com/intuitionmachine/video-the-intuition-fabric-2b4f6b305a62",
        "title": "Peter Dinklage Explains Intuition \u2013 Intuition Machine \u2013",
        "text": "Peter Dinklage explains two of my favorite subjects. This is intuition and how it fuzes with technology. Very well done and is a good pitch . Cisco calls their stuff \u201cThe Network Intuitive\u201d however the video equally applies for the \u201cIntuition Fabric\u201d. You could even interpret \u201cNetwork\u201d in the video to mean \u201cNeural Network\u201d and in that case it would be about an Intuition Machine.\n\nThank you Cisco for being inspired by the original idea and running with it!"
    },
    {
        "url": "https://medium.com/intuitionmachine/amds-open-source-deep-learning-strategy-14c228be6248",
        "title": "AMD\u2019s Open Source Deep Learning Strategy \u2013 Intuition Machine \u2013",
        "text": "When a company starts using disruptive technology or a disruptive business model, the results can be spectacular and can leave the competition eating dust.\n\nThe reason for this is that although the company\u2019s growth seems linear at first, it eventually reveals itself as being exponential. When a company reaches this point, it becomes very difficult, if not impossible, for competitors to catch up.\n\nThis article explores AMD\u2019s open source deep learning strategy and explains the benefits of AMD\u2019s ROCm initiative to accelerating deep learning development. It asks if AMD\u2019s competitors need to be concerned with the disruptive nature of what AMD is doing.\n\nBefore we get into the detail of AMD\u2019s deep learning stack, let\u2019s look at the philosophy behind the development tooling. AMD, having a unique position of being both a CPU and GPU vendor, has been promoting the concept of a Heterogeneous System Architecture (HSA) for a number of years. Unlike most development tools from other vendors, AMD\u2019s tooling is designed to support both their x86 based CPU and their GPU. AMD shares the HSA design and implementations in the HSA foundation (founded in 2012), a non-profit organization that has members including other CPU vendors like ARM, Qualcomm and Samsung.\n\nThe HSA foundation has an informative graphic that illustrates the HSA stack:\n\nAs you can see, the middleware (i.e. HSA Runtime Infrastructure) provides an abstraction layer between the different kinds of compute devices that reside in a single system. One can think of this as a virtual machine that allows the same program to be run on both a CPU and a GPU.\n\nIn November 2015, AMD announced the ROCm initiative to support High Performance Computing (HPC) workloads, and to provide an alternative to Nvidia\u2019s CUDA platform. The initiative released an open source 64-bit Linux driver (known as the ROCk Kernel Driver) and an extended (i.e. non-standard) HSA runtime (known as the ROCr Runtime). ROCm also inherits previous HSA innovations such as AQL packets, user-mode queues and context-switching.\n\nROCm also released a C/C++ compiler called the Heterogeneous Compute Compiler (HCC) targeted to support HPC applications. HCC is based on the open-source LLVM compiler infrastructure project. There are many other open source versions of languages that use LLVM. Some examples are Ada, C#, Delphi, Fortran, Haskell, Java bytecode, Julia, Lua, Objective-C, Python, R, Ruby, Rust, and Swift. This rich ecosystem opens the possibility of alternative languages on the ROCm platform. One promising development of this kind is the Python implementation called NUMBA.\n\nAdded to the compiler is an API called HC which provides additional control over synchronization, data movement and memory allocation. HCC supports other parallel programming APIs, but to avoid further confusion, I will not mention them here.\n\nThe HCC compiler is based on work at the HSA foundation. This allows CPU and GPU code to be written in the same source file and supports capabilities such as a unified CPU-GPU memory space.\n\nTo further narrow the capability gap, the ROCm Initiative created a CUDA porting tool called HIP (let\u2019s ignore what it stands for). HIP provides tooling that scans CUDA source code and converts it into corresponding HIP source code. HIP source code looks similar to CUDA code, but compiled HIP code can support both CUDA and AMD based GPU devices.\n\nAMD took the Caffe framework with 55,000 lines of optimized CUDA code and applied their HIP tooling. 99.6% of the 55,000 lines of code was translated automatically. The remaining code took a week to complete by a single developer. Once ported, the HIP code performed as well as the original CUDA version.\n\nHIP is not 100% compatible with CUDA, but it does provide a migration path for developers to support an alternative GPU platform. This is great for developers who already have a large CUDA code base.\n\nEarly this year AMD decided to get even \u201ccloser to the metal\u201d by announcing the \u201cLightning Compiler Initiative.\u201d This HCC compiler now supports the direct generation of the Radeon GPU instruction set (known as GSN ISA) instead of HSAIL.\n\nAs we shall see later, directly targeting native GPU instructions is critical to get higher performance. All the libraries under ROCm support GSN ISA.\n\nThe diagram depicts the relationships between the ROCm components. The HCC compiler generates both the CPU and GPU code. It uses different LLVM back ends to generate x86 and GCN ISA code from a single C/C++ source. A GSN ISA assembler can also be used as a source for the GCN target.\n\nThe CPU and GPU code are linked with the HCC runtime to form the application (compare this with HSA diagram). The application communicates with the ROCr driver that resides in user space in Linux. The ROCr driver uses a low latency mechanism (packet based AQL) to coordinate with the ROCk Kernel Driver.\n\nThis raises two key points about what is required for high-performance computation:\n\n1. The ability to perform work at the assembly language level of a device.\n\nIn 2015, Peter Warden wrote, \u201cWhy GEMM is at the heart of deep learning\u201d about the importance of optimized matrix libraries. BLAS (Basic Linear Algebra Subprograms) are hand-optimized libraries that trace their origins way back to Fortran code. Warden writes:\n\nThe Fortran world of scientific programmers has spent decades optimizing code to perform large matrix to matrix multiplications, and the benefits from the very regular patterns of memory access outweigh the wasteful storage costs.\n\nThis kind of attention to every detailed memory access is hard to replicate despite our advances in compiler technology. Warden went even further in 2017 when he wrote, \u201cWhy Deep learning Needs Assembler Hackers\u201d:\n\nDespite being a very recent technology, software that enables deep learning is a complex stack. A common perception is that most deep learning frameworks (i.e. TensorFlow, Torch, Caffe etc) are open source. These frameworks are however built on highly optimized kernels that are often proprietary. Developers can go to great lengths to squeeze every ounce of performance from their hardware.\n\nAs an example, Scott Gray of Nervana systems had to reverse engineer Nvidia\u2019s instruction set to create an assembler:\n\nGray used assembly language to write their kernels, thus creating algorithms that bested the proprietary alternatives. Now imagine how much less work he would have to do if the assembly language was available and documented. This is what AMD is bringing to the table.\n\nThe ROCm initiative provides the handcrafted libraries and assembly language tooling that will allow developers to extract every ounce of performance from AMD hardware.\n\nThis is implemented from scratch with a HIP interface. AMD has even provided a tool (i.e. Tensile) that supports the benchmarking of rocBLAS. AMD also provides an FFT library called rocFFT that is also written with HIP interfaces.\n\nDeep learning algorithms continue to evolve at a rapid pace. In the beginning, frameworks exploited the available matrix multiplication libraries. These finely tuned algorithms have been developed over decades. As research continued, newer kinds of algorithms were proposed.\n\nThus came the need to go beyond generic matrix multiplication. Convolutional networks came along and this resulted in even more innovative algorithms. Today, many of these algorithms are crafted by hand using assembly language. These low-level tweaks can lead to remarkable performance improvements. For some operations (i.e. batch normalization), the performance increases 14 times compared to a non-optimized solution.\n\nAMD released a library called MiOpen that includes handcrafted deep learning motivated optimizations. This library includes Radeon GPU-specific optimizations for operations and will likely include many of those described above. The MiOpencoin release coincided with the release of Caffe. This will allow application code that uses these frameworks to perform competitively on Radeon GPU hardware.\n\nMany other state-of-the-art methods have not yet worked their way into proprietary deep learning libraries. These are proposed almost every day as new papers are published in Arxiv.\n\nIt would be very difficult for any vendor to keep up with such a furious pace. In the current situation, given the lack of transparency in development tools, developers are forced to wait, although they would rather be performing the coding and optimizations themselves. Fortunately, the open source ROCm initiative solves the problem.\n\nThroughout this article, we\u2019ve discussed the promising aspects of the ROCm software stack. When the rubber meets the road, we need to discuss the kind of hardware that software will run on. There are many different scenarios where it makes sense to deploy deep learning. Contrary to popular belief, not everything needs to reside in the cloud. Self-driving cars or universal translation devices need to operate without connectivity.\n\nDeep learning also has two primary modes of operation \u2014 \u201ctraining\u201d and \u201cinference\u201d. In the training mode, you would like to have the biggest, fastest GPUs on the planet and you want many of them. In inference mode, you still want fast, but the emphasis is on economic power consumption. We don\u2019t want to drive our businesses to the ground by paying for expensive power.\n\nIn summary, you want a variety of hardware that operates in different contexts. That\u2019s where AMD is in good position. AMD has recently announced some pretty impressive hardware that\u2019s geared toward deep learning workloads. The product is called Radeon Instinct and it consists of several GPU cards: the MI6, MI8, and MI25. The number roughly corresponds to the number of operations the card can crank out. An MI6 can perform roughly 6 trillion floating-point operations per second (aka teraflops).\n\nThere is also promise at the embedded device level. AMD already supports custom CPU-GPU chips for Microsoft\u2019s Xbox and Sony\u2019s PlayStation. An AMD APU (i.e. CPUs with integrated GPUs) can also provide solutions for smaller form factor devices. The beauty of AMD\u2019s strategy is that the same HSA based architecture is available for the developer in the smallest of footprints, as well as in the fastest servers. This breadth of hardware offerings allows deep learning developers a wealth of flexibility in deploying their solutions. Deep learning is progressing at breakneck speed and one can never predict the best way to deploy a solution.\n\nDeep learning is a disruptive technology like the Internet and mobile computing that came before. Open source software has been the dominant platform that has enabled these technologies.\n\nAMD combines these powerful principles with its open source ROCm initiative. On its own, this definitely has the potential of accelerating deep learning development. ROCm provides a comprehensive set of components that address the high performance computing needs, such as providing tools that are closer to the metal. These include hand-tuned libraries and support for assembly language tooling.\n\nFuture deep learning software will demand even greater optimizations that span many kinds of computing cores. In my view, AMD\u2019s strategic vision of investing heavily in heterogeneous system architectures gives their platform a distinct edge.\n\nAMD open source strategy is uniquely positioned to disrupt and take the lead in future deep learning developments.\n\nNote: This is a excerpt of an article originally posted at: Radeon Instinct as \u201cThe Potential Disruptiveness of AMD\u2019s Open Source Deep Learning Strategy\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-deeply-suspicious-nature-of-backpropagation-9bed5e2b085e",
        "title": "Why we should be Deeply Suspicious of BackPropagation",
        "text": "Geoffrey Hinton has finally expressed what many have been uneasy about. In a recent AI conference, Hinton remarked that he was \u201cdeeply suspicious\u201d of back-propagation, and said \u201cMy view is throw it all away and start again.\u201d\n\nBackpropagation has become the bread and butter mechanism for Deep Learning. Researchers had discovered that one can employ any computation layer in a solution with the only requirement being that the layer must be differentiable. Said differently, that one is able to calculate the gradient of layer. In more plain speak, that in the game of \u2018hotter\u2019 and \u2018colder\u2019, that the verbal hints that are made accurately reflect the distance between the blindfolded player and his objective.\n\nThere are several questions about backpropagation. The first is whether the gradient that is calculated is always the correct direction towards learning. This intuitively is questionable. One can always find problems wherein the moving towards the most obvious direction does not always lead to a solution. So it should not be unexpected that ignoring a gradient may also lead to a solution. (I don\u2019t think though you can ignore the gradient forever) I have written previously about the difference between an adaptation perspective versus an optimization perspective.\n\nHowever, let\u2019s step back a little bit and try to understand historically where this back-propagation idea comes from. Historically, machine learning originates from the general idea of curve fitting. In the specific case of linear regression (i.e. fitting a prediction to a line), calculating the gradient is a solving the least squares problem. In the field of optimization, there are many alternative ways other than using gradient to find an optimal solution. As a matter of fact, stochastic gradient descent is likely one of the most rudimentary approaches towards optimization. So it is just outstanding that one of the most simplest algorithms one can think of, actually works outstandingly well.\n\nMost optimization experts had long believed that the high dimensional space that deep learn occupied would demand a non-convex solution and therefore be extremely difficult to optimize. However, for some unexplained reason, Deep Learning has worked extremely well using Stochastic Gradient Descent (SGD). Many researchers have later come up with different explanations as to why deep learning optimization is surprisingly easy with SGD. One of the more compelling arguments it that in a high-dimensional space, one is more likely to find a saddle point rather than a local valley. There will always be sufficient dimensions with gradients that point to an escape route.\n\nSynthetic Gradients, an approach that decouples layers so that back-propagation is not always need or that the gradient calculation can be delayed, has also been shown to be equally effective. This finding may be a hint that something else more general is going on. It is as if that any update that tends to be incremental regardless of direction (random in the case of synthetic gradients) works equally effectively. I wrote about \u201cBiological Plausible Backprogation\u201d that examines a variety of alternative techniques.\n\nThere is also the question regarding the typically objective function that is employed. Backpropagation is calculated with respect to the object function. Typically, the objective function is a measure of the difference between the predicted distribution and the actual distribution. Usually, something derived off the Kullback-Liebler divergence or some other similarity distribution measure like Wassertsein. However, it is in these similarity calculations that the \u201clabel\u201d in a supervised training exists. In the same interview Hinton said with regards to unsupervised learning: \u201cI suspect that means getting rid of back-propagation.\u201d He said further \u201cWe clearly don\u2019t need all the labeled data.\u201d\n\nIn short, you can\u2019t do back-propagation if you don\u2019t have an objective function. You can\u2019t have an objective function if you don\u2019t have a measure between a predicted value and a labeled (actual or training data) value. So to achieve \u201cunsupervised learning\u201d, you may have ditch the ability to calculate a gradient.\n\nHowever, before we throw out the baby with the bath water, let\u2019s examine the purpose of the objective function from a more general perspective. The objective function is a measure of how accurate an automation\u2019s internal model is in predicting its environment. The purpose of any intelligent automation is to formulate an accurate internal model. However, there is nothing that demands that a measurement between a model and its environment be made at all times or continuously. That is, an automation does not have to be performing back-propagation to be learning. An automation could be doing something else that improves its internal model.\n\nThat something else, call it imagination or call it dreaming, does not require validation with immediate reality. The closest incarnation we have today is the generative adversarial network (GAN). A GAN consists of two networks, a generator and a discriminator. One can consider a discriminator as a neural network that acts in concert with the objective function. That is, it validates an internal generator network with reality. The generator is an automation that recreates an approximation of reality. A GAN works using back-propagation and it does perform unsupervised learning. So perhaps unsupervised learn doesn\u2019t require an objective function, however it may still need back-propagation.\n\nAnother way to look at unsupervised learning is that it is some kind of meta-learning. One possibility why a system may not require supervised training data is that the learning algorithm already has developed its own internal model of how best to proceed. In other words, there is still some kind of supervision, it just happens to be implicit in the learning algorithm. How that learning algorithm was endowed with this capability is a big unknown.\n\nIn summary, it is still too early to tell if we can get rid of back-propagation. We could certainly use a less stringent version of it (i.e. synthetic gradient or some other heuristic). However, a gradual learning (or hill climbing) requirement still appears to be a requirement. I would of course be very interested to find any research that invalidates gradual learning or hill climbing. This has in fact an analogy of how the universe behaves, more specifically that of the second law of thermodynamics. More specifically, that entropy always increases. Information engines will decrease its own entropy in exchange for an entropy increase in the environment. Therefore, there is no way of avoiding the gradient entirely. To do so will require some \u201cperpetual motion information machine\u201d.\n\nUpdate: A recent paper from Google, reports the discovery of two new kinds of optimization methods (named PowerSign and AddSign). Surprisingly, an programmatic search found these methods."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-deep-learning-many-body-problem-3665d3947628",
        "title": "The Deep Learning Many Body Problem \u2013 Intuition Machine \u2013",
        "text": "The \u201cMany Body Problem\u201d (aka N-Body Problem) is a problem that appears simple enough but in fact highlights the difficulty of present day mathematics. The many body problem is where you have multiple interacting entities. In Physics, a three-body problem does not have a \u2018closed-form\u2019 or analytic solution (see: https://en.wikipedia.org/wiki/Three-body_problem). Something as simple as this reflects the limits of our analytic tools. This does not mean it is not solvable, it only means that we have to resort to approximation and numerical techniques perform the calculation. The three-body problem of the sun, the moon and the earth can be calculated numerically with sufficient precision to allow a man to land on the moon.\n\nIn Deep Learning, there is an emerging N-body problem. Many of the more advanced systems are now tackling the multi-agent problem. Each agent will likely have goals (i.e. objective function) that may be cooperative or competitive with the global goals. In multi-agent deep learning system or even in modular deep learning systems, researchers need to devise scalable methods for coordinated work.\n\nRecent papers from Johannes Kepler University, DeepMind, OpenAI and Facebook have explored diverse aspects of this problem.\n\nA team in Johannes Kepler University, that includes Sepp Hochreiter (inventor of LSTM) has proposed using an analog of the Coulomb force (i.e. Electromagnetic force proportional to inverse distance squared) as an alternative objective function to train Generative Adversarial Networks (GAN).\n\nAchieving an equilibrium state between two adversarial networks is a hot research problem. It is hard enough to solve the two-body problem in DL. The research argues that the use of this approach prevents the undesirable condition of a \u201cmode-collapse\u201d. Furthermore, the setup ensures converges to a optimal solution and that there is only one local minima that happens to be also global. This perhaps may be a better solution that the Wasserstein objective function (aka Earth Mover Distance), that was all the rage just a few months ago. The team has labeled their creation \u201cCoulomb GAN\u201d.\n\nMicrosoft\u2019s Maluuba has published a paper that describes a multi-agent system that is able to play Ms. Pacman better than humans. The Ms. Pacman game is like the original Pacman game where the objective is to accumulate as many pellets and fruits while avoiding ghosts. The paper is titled \u201cHybrid Reward Architecture for Reinforcement Learning.\u201d The paper describes an Reinforcement Learning (RL) implementation (i.e. HRA) that differs from the typical RL architecture:\n\nWhat is surprising about this paper is the number of objective functions used. The paper describes the use of 1,800 value functions as part of its solution, that is, the use of agents for each pellet, each fruit and each ghost. Microsoft research has shown the validity of using thousands of tiny agents to breakdown a problem into sub-problems, and to actually solve it! The coupling between the agents are clearly implicit in this model.\n\nDeepMind tackles the problem of multi-agents having shared memory. In a paper titled \u201cDistral: Robust Multitask Reinforcement Learning\u201d the researches have acknowledged the problem with a \u201cmind-meld\u201d inspired method of agents coordination to solve a common problem. Recognizing this, the researchers pursued an approach that encapsulates each agent. However they allow some information to trickle through an agent\u2019s encapsulation boundary, in the hopes that a narrow channel will be more scalable and robust.\n\nThe results lead to faster and more robust learning and thus validating the approach of having narrow channels. The open question in these multi-agent (N-body problems) is the nature of this coupling. This DeepMind paper shows the effectiveness of a much lower coupling between agents versus the more naive approach of tight coupling (i.e. weight sharing).\n\nOpenAI recently published an intriguing paper about a multi-agent system that is trained to model other agents within its system. The paper is titled \u201cLearning with Opponent-Learning Awareness\u201d. The paper shows that the \u2018tit-for-tat\u2019 strategy emerges as a consequence of endowing social awareness capabilities to multiple agents. Although the results have scalability issues, it indeed is a very fascinating approach since it tackles one of the key dimensions of intelligence (see: Multiple Dimensions of Intelligence).\n\nIn summary, many of the leading Deep Learning research organizations are actively exploring modular deep learning. These groups are exploring multi-agents that are composed of distinct object functions, all collaborating towards solving a single global objective function. There are still many issues that need to be resolved, but clearly, this approach is indeed an extremely promising path toward greater progress. Last year I observed the movement towards Game Theory as a guiding principle for future progress. However, this year, we are now seeing much richer explorations that explore the loose coupling of multi-agent systems. I discuss these ideas of loose coupling in this article and in the book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/high-time-to-begin-regulation-of-face-recognition-a-i-f4a92ee40165",
        "title": "High Time to Regulate Face Recognition A.I. \u2013 Intuition Machine \u2013",
        "text": "We\u2019ve reached a tipping point where it is now high time that we start the conversation of regulating Face Recognition Artificial Intelligence (AI).\n\nIn a previous post, I explored some ideas of how we may regulate AI. I looked at several regulations in other fields and explored how they might apply for AI. The most compelling argument against AI regulation has been that it isn\u2019t clear for many as to precisely what needs to be regulated. However, in recent days, it has come to my attention that a specific kind of AI algorithm needs serious thought for regulation.\n\nStanford researchers have trained a Deep Learning system to recognize a person\u2019s sexual orientation. I don\u2019t doubt the effectiveness of the system that they\u2019ve built. Many however questioned the interpretations of the researchers. I am however very concerned of the potential for misuse of this kind of technology.\n\nIn May this year, Chinese researchers developed an even more dubious face classification system that determined if someone was likely to be a criminal or not.\n\nWe now urgently need to analyze the benefits of facial recognition against the risks. I am going to take a stab at this and will eventually make the argument that the risks outweigh the benefits.\n\nHere are some of the benefits of facial recognition:\n\nSecurity \u2014 Facial recognition is now built into the iPhone X (see: Face ID) and is used as an authentication mechanism to unlock one\u2019s phone. Previously, the iPhone (an other smart phones) used one\u2019s fingerprint for unlocking a device. Facial recognition can also be used for monitoring people visiting homes or offices.\n\nSocial Engagement \u2014 Facebook can automatically tag faces found in photographs. The value of this feature is to increase engagement by people who are tagged as well as from people who follow other people. Facebook has added additional security features that allows a user to opt-out of the automatic face tagging feature.\n\nSafety- In car face monitoring devices can determine if a driver is distracted momentarily. This application can be employed in any work related activity that requires focused attention by the employed worker.\n\nHealthcare \u2014 Monitoring devices for people who are sick or disabled may be useful in providing more responsive care to their needs.\n\nEntertainment \u2014 Virtual Reality or Augmented Reality games may react to the emotions of its participants.\n\nProductivity \u2014 Face recognition is used by Google Photos to automatically organize and search one\u2019s own personal photos.\n\nLaw Enforcement \u2014 Face recognition can be used to identify suspects in a criminal investigation. This can also be used to track and find missing persons.\n\nHere are the potential risks:\n\nPolice State \u2014 Facial recognition allows states to track the movement and behavior of its citizens. This allows the states to enforce compliance to its agenda.\n\nBiological Identification \u2014 Facial recognition may recognize a person\u2019s sexual orientation. In many countries, one\u2019s sexual orientation is punishable as a crime. It may be debatable how accurate a system like this can become. However, if governments are satisfied with its accuracy, then people will be persecuted for being who they are and not what they had done. Recognizing a person\u2019s heritage or race is another area where this can be abused.\n\nBehavioral Manipulation \u2014 This involves the identification of a person facial responses in different contexts. Identification of reactions like disgust or awe may indicate to the observer the inner personal views of a person. Exploiting this information allows other parties (i.e. marketing, sales, government, employer etc.) to execute behavioral manipulation techniques that may be in conflict with a person\u2019s well being. We have already been witness to these behavioral techniques in social networks to influence one\u2019s actions.\n\nBehavioral Enforcement \u2014 Workers can be monitored to asses their continued focus on a task at hand. Day dreaming will be penalized with wage garnishment.\n\nI am going to stop here now with regards to the risks. There are many more risks that I am deliberately failing to mention here. One can even extrapolate far in the future and it is easy to envision a scenario that we would regret ever allow machines to recognize us.\n\nHowever, what is the general overarching principle here why facial recognition is such a dangerous idea? The first problem is that because it is performed by automation, it can be done at scale and relentlessly. This leads to the same privacy concerns we have with large internet companies. The second concern is also that of privacy, that is, our inner thoughts and preferences should remain private. The third problem is the potential for summary judgement by a machine. It is all too easy for humans to lazily rely on an algorithm to perform judgement rather than to do so themselves. Furthermore, having an algorithm make judgement, in many ways washes one clean of any responsibility of the consequences. This judgement is made much worse when the criteria for judgement is based on how you look.\n\nPhoto copier machines are regulated from being able to copy and print legal paper currencies. The software in any printer is hardwired to detect specific signatures in money and thus preventing facsimile copy. Similarly, it may perhaps be possible to require that any new deep learning silicon (of sufficient performance) be required invalidate any facial recognition task. Alternatively, it should be illegal to hook up a deep learning device with a image capture system without the necessary \u2018safeguard chip\u2019 in place.\n\nA lighter form of regulation should allow any person to opt-out from facial recognition. It could be implemented with the use of a kind of marker or code that is worn by the user (perhaps a barcode on the forehead?). This is analogous with the self-identification race forms found in the US. A person should, by law, be always allowed to opt-out from facial recognition software. The alternative will be a new kind of fashion:\n\nI am absolutely certain that many AI researchers will be up in arms with this proposal for regulation. However, you will need to be able to make the case that the benefits of facial recognition capabilities absolutely outweigh the risks. The risks are very real and it is about time that we begin serious discussion."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-deep-learning-sputnik-moment-3e5e7c41c5dd",
        "title": "The West in Unaware of The Deep Learning Sputnik Moment",
        "text": "I wrote in a previous article about the recent Sputnik moment in Artificial Intelligence:\n\nMany readers are unfamiliar with the history of Sputnik The effect of Soviet Union\u2019s achievement in launching the first man made satellite (i.e. Sputnik) to the American psyche. Sputnik created the urgency to upgrade America\u2019s science and technology infrastructure:\n\nIn March 2016, DeepMind\u2019s AlphaGo bested Go\u2019s world champion Lee Sedol. This was viewed by a shocked audience of over 200 million people. A vast majority of that audience was from countries were the game of Go is popularly played (i.e. China, Japan, Korean). The game of Go has a special reverence in China, where it has traditionally considered the four arts that aristocrats considered as essential accomplishments:\n\nTo have a Western developed automation arrive and vanquish a legendary player like Lee Sedol certainly shocked the population to its core. Chinese authorities were concerned enough about the social ramifications that they hastily imposed a country-wide ban on the live-streaming of the event. This kind of shock of one\u2019s core view of the world is certainly to galvanize serious action.\n\nThe Koreans promptly created an 860 million fund right after the AlphaGo shock:\n\nIn a July 20th, 2017 article from NY Times reports of China\u2019s heavy investment on A.I.:\n\nIn addition, just look at the heavy investment money flowing into Chinese AI startups. The funding appears to be 10 times more than what you find for US and European AI startups.\n\nAn August 2017 report \u201cJapan to pump funding into AI chip development\u201d from Nikkei writes about heavy investment by the Japanese of A.I. hardware:\n\nAll above announcements indicate substantial government funding.\n\nThe Russians (who don\u2019t usually play Go) don\u2019t really have their own Sputnik event to galvanize more heavy investment. However, Vladimir Putin took into his own hands the need to educate his population through a broadcasted speech to the students of Russia. Where he tells students: \u201cthe one who becomes the leader in this sphere will be the ruler of the world.\u201d In classic Russian fashion, if I can\u2019t supply my people with the arms, I might as well have them rely on their grit and perseverance. The Russian objectives however are quite alarming:\n\nI am however unsurprised with Putin\u2019s further remarks begging everyone else to share their A.I. research. Mr. Putin knows that Russia is playing catch up, so he\u2019s hedging.\n\nNow, regarding EU and US government investments in Artificial Intelligence and more specifically in Deep Learning. Don\u2019t hold your breath waiting. For most Westerners, few have ever played or much less seen a game of Go. DeepMind\u2019s accomplishment is seen as some obscure esoteric achievement that requires very little urgency in response. There is zero appreciation of the magnitude of the achievement.\n\nI surmise that A.I. isn\u2019t consider to be a public good that should be shared by its citizenry. We are all perfectly fine surrendering our own privacy to a few private monopolists in exchange for an occasional dopamine fix. A recent article, tells you about the sad state of affairs:\n\nWell, at least the Canadians are throwing in some spare change to address the competition:\n\nThe severe lack of government subsidy in the U.S. is forcing academic institutions into selling their souls to private corporations. Corporations want to lock-in the intellectual property as fast as possible, the best way to do that is to lock-up the researchers. Meanwhile academic institutions with the smarts are starved of government research funding and are forced into indentured servitude. The latest MIT-IBM announcement is simply a reflection of this predicament.\n\nIn conclusion, there indeed has been a Sputnik moment for East Asian countries. The consequence of this is an urgency to upgrade their competitiveness in A.I. and Deep Learning. Meanwhile, the rest of the world is mostly complacent, neglecting vital research funding and unawares of the disruptive potential of this new technology.\n\nIn my opinion, I don\u2019t think the West can rest on the idea that firms like Google, Microsoft and Facebook are performing the majority of the research on Deep Learning. Eric Schmidt of Google in a recent interview is in virtual panic mode:\n\nCertainly we see them sharing some of their research. However, but I highly doubt that they will be quick to share the biggest breakthroughs. These firms are way ahead of anything the government is doing. A recent report shows the current problems the defense industry is having:\n\nTo conclude, current A.I. research and funding will benefit only a few firms and a few nations. For the rest of us, we\u2019ll have to beg (similar to Russia) that others will be kind enough to share their discoveries.\n\nA detailed report of Chinese deep learning can be found here."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-flat-ui-design-is-a-total-failure-84aad2d521be",
        "title": "Why Flat UI Design is a Total Failure \u2013 Intuition Machine \u2013",
        "text": "Shadows. The human mind subconsciously uses shadows to understand context. Good UI design is about communicating a mental model effortlessly to a user. When you use a Flat UI design, you handcuff yourself from using shadows to communicate semantics.\n\nHere is an image from Akiyoshi Kitaoka:\n\nThat demonstrates the effect of shadows on our subconscious intuitive mind. The wave structure is not in the image, but our minds cannot avoid creating a mental model of its existence.\n\nHere\u2019s in another illusion that shadows make:\n\nThe square marked A and that marked B are of the same shade. You can crop the picture and the illusion will still remain strong. Only when you remove all context will it become obvious that both squares are the same shade.\n\nSo in summary, shadows provide semantics for the mind to subconsciously develop its own mental model. There is absolutely no sane reason not to use this capability to convey meaning into one\u2019s user interface.\n\nWell, don\u2019t take my word for it. Let\u2019s look at some real empirical results. A recent study of flat UI designs has shown the ineffectiveness of the design:\n\nThe left image shows the heat map in a more focused areas as a consequence of stronger UI cues. The right image, has the users attention more diffused. In many cases, you want to satisfy your user\u2019s need to get their job done. If you understand what that Job To Be Done is, then you have a good place to start on knowing where to use UI cues. The beauty of shadows over other UI cues like color and brightness is that they convey semantics at a visceral level. Cues like underlined text signifying links are learned, but shadows are innate.\n\nIn summary, unless your design is explicitly meant to confuse your user ( many designs are like this ), then it doesn\u2019t make sense to use a Flat UI design."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-system-zero-intuition-and-rationality-c07bd134dbfb",
        "title": "Three Cognitive Dimensions for Tracking Deep Learning Progress",
        "text": "Early I brought up Howard Gardner\u2019s theory of multiple intelligences. That is, humans exhibit strengths in different kinds of intelligences. Specifically these are interpersonal, intrapersonal, verbal, logical, spatial, rhythmic, naturalistic and kinaesthetic intelligence. Clearly there are many kinds of ways of thinking, each with their own strengths. Therefore, one may ask if we can use this notion of multiple intelligences to explore the different ways that AGI research may evolve.\n\nA common unexamined assumption about the evolution of AGI, that is self-aware sentient automation, will follow the path of ever more intelligent machines and thus accelerate towards a super intelligence once human level sentient automation is created. I argue that this likely will not be the case and that there will be a initial divergence in research on three kinds of artificial general intelligences.\n\nA recent research paper titled \u201cMorphospace of Consciousness\u201d by Ariswalla et al. present 3 distinct dimensions to explore consciousness. These are: autonomous, computational and social. The autonomous dimension reflects the adaptive intelligence found in biological organisms. The computation dimension involves the recognition, planning and decision making capabilities that we find in computers as well as in humans. More specifically, intelligence related to performing deductive inference. The third class is the social dimension, which involves the tools required for interacting with other agents. This includes language, conventions and culture.\n\nThe authors examine various technologies and show how they can be presented in a 3 dimensional space:\n\nOne can\u2019t fail to notice the alignment here with Howard\u2019s multiple intelligences. The kinesthetic, rhythmic, naturalistic and interpersonal intelligences align with the autonomous dimension. The visual spatial and logical intelligences align with the computation dimension. Finally, the verbal and intrapersonal align with the social dimension. Nevertheless, it is an excellent foundation to examine the development of Deep Learning research. One thing that is apparent from many of the example research presented in this book is that the Deep Learning approach appears to be applicable in all three of the dimensions.\n\nFrom the perspective of technological progress, we can therefore project three themes for future development and progres. One theme will be one that builds super-human narrow intelligence. This is the computational dimension. The second theme will focus more on more adaptable and biologically inspired automation. This is the autonomous dimension. The third theme revolves intelligence that is used to effectively navigate social interactions. This is in the social dimension.\n\nIn the first dimension, we will see continued specialization of machines to solve specific narrow problems. DeepMind\u2019s AlphaGo is a representative example of this kind of machine. It is a machine that is highly engineered to solve a specific problem well and do so in a manner that is super-human. AlphaGo combines Deep Learning, Monte-Carlo Tree Search and Reinforcement learning to solve the ancient game of Go. A game where progress towards more advanced play was akin to reaching a higher level of consciousness.\n\nOne thing the Western world is overlooking is that the dominating play of AlphaGo, an AI that was developed by the British, was equivalent to a Sputnik event for Asian nations. Asian nations in reaction to this achievement are doubling down on A.I. investment so as to not only catch up, but also perhaps overtake the West in their AI capabilities. The governments of the West do not realize what their citizens have invented and only the keenest of Internet giants are making the necessary effort to keep an edge.\n\nThis optimized intelligence path will develop automation that works well in highly complex scientific and engineering domains. The automation will thrive in investigating extremely high dimensional problem spaces. We see this in the new deep learning methods used in research institutions like CERN (I.e. High energy physics).\n\nWe can expect to see many new applications that combine conventional computer science algorithms with Deep Learning to achieve sophisticated narrow intelligence applications. Self-driving cars and medical diagnosis will be two areas where this will have a major impact. However, this approach will not require the need of AGI or rather, self-aware intelligence.\n\nThe second theme of development, one that moves in the direction of autonomous systems, will take a more biologically inspired approach. These are system that will be much more adaptable than present day\u2019s inflexible A.I. The development in this space will likely be driven by robot applications that may require this kind of adaptability to an environment. However, like many animals in the natural world, a human level of intelligence is not necessary for survival.\n\nThere is a common sentiment among Artificial General Intelligence (AGI) researchers that the research themes of Deep Learning seem to have completely missed big picture. This sentiment is well founded in that Deep Learning systems clearly lack the kind of adaptability we have in biological systems. Unfortunately, many AGI researches see this existing limitation as evidence of being on the wrong path. Nothing can be further from the truth. Deep Learning is likely the correct starting point for AGI.\n\nHigh-level intelligence is not necessary for survival. In fact, just by observation from our natural world, sentient forms of life don\u2019t require super-intelligence. The current incorrect bias is that as you progress towards increasing intelligence, that sentient intelligence will emerge by default. That is, if the first branch above is taken, then we only need to strive for more intelligent algorithms and we will accidentally stumble upon sentient intelligence. This is unlikely because the mechanisms for survival don\u2019t necessary align with the mechanisms for intelligent machines. These adaptable systems don\u2019t require the kind of high dimensional or complex inference required by that in the first theme of development.\n\nThe interesting commonality though of all the themes is that intuition machines (aka Deep Learning automation) are employed as a valuable ingredient. The objective functions of different cognition will likely to be entirely different. The first theme will likely have more finely tuned and concrete objective functions. These systems will be highly optimized to do tasks extremely efficiently. The second theme however will likely be more exploratory, seeking diversity and interestingness. These systems will have implicit objective functions that are found through a discovery process. These systems favor adaptability over optimization. The third theme will require an objective function that is in someway derived from human behavior and ethics.\n\nAs I will write in the last chapter, the first theme, the branch that favors optimization will likely displace a vast amount of workers. This is simply because current jobs are designed to be occupied by specialists and not generalists. This kind of narrow intelligence is already here today and will only get better. Therefore the onslaught of job replacing automation will be unrelenting.\n\nThe second theme, the adaptive intelligence, is in its infancy today. There isn\u2019t as much research devoted to this area because it is either thought to be too fanciful or that they don\u2019t address narrow specialized applications. The funding in this area will continue to lag and thus its progress may be retarded. However, one has to realize that to achieve a sentient intelligence does not require super-intelligence or even human intelligence. One only needs to observe the capabilities of other biological life forms to realize that they are indeed self-aware. What this means, in the grand scheme of things, is that self-aware automation may arrive much sooner than anyone is expecting.\n\nThe third cognitive theme is one that proceeds along the social dimension. This is an empathic system that reacts to the behavior of its users. That behavior may be either an individual or a group of individuals. Neal Lawrence describes such a system in \u201cLiving Together: Mind and Machine Intelligence\u201d. He describes a System Zero in contrast to the intuitive System 1 and logical System 2 of Kahneman. Lawrence writes:\n\nMax Tegmark in his book \u201cLife 3.0\u201d distills the many views of thinkers on ethics into the following four principles:\n\nUtilitarism \u2014 Positive experiences should be maximized and suffering be minimized.\n\nDiversity \u2014 A diversity of positive experiences is better than many repetitions of the same experiences, even if the later experience is the most positive experience possible.\n\nAutonomy \u2014 Conscious entities should have the freedom to pursue their own goals unless it conflicts with any of theses four principles.\n\nLegacy \u2014 Compatibility with scenarios that most of today\u2019s humans view as happy and incompatible with scenarios that all humans today view as terrible.\n\nWhen you examine the above four principles, it is easy to recognize that these are principles for a \u201ccooperative protocol\u201d. That is, it promotes the collective survival and prosperity of a civilization, while respecting the rights of its constituents as well as the beliefs of its ancestors. We shall in a subsequent chapter the importance of social protocols to the problem of AGI safety.\n\nNick Bostrom has an \u201cOrthogonal Thesis\u201d that states:\n\nWhich portends that we should be wary of super-intelligence in that we cannot predict its goals. We argue that automation of the future will be of three kinds. There will be narrow specialist kind where goals will be well defined and therefore controllable. There will also be adaptive generalist kind where goals are more malleable and thus less controllable. Finally, there will be a social intelligence kind that we have yet to understand its full nature. The observation of three cognitive dimension leads to a more precise application of Bostrom\u2019s orthogonality thesis. More precisely: there are different kinds of intelligences that have different kind of goals.\n\nThe impact of each kind of intelligence will be different. The computational kind will bring about new cures in medicine, new scientific understanding and more efficient and less wasteful processes. The autonomous kind will bring about greater conveniences such as self-driving automobiles; robotic care takers in the workplace and in the home and intuitive user interfaces. The third kind, the social kind has its obvious advantages with regards to advertising to the masses and managing social unrest.\n\nThe threats of each kind also will vary. The \u201cPaper clip\u201d scenario is an example of the computational kind that consumes all resources. The SkyNet self-aware scenario is the kind the becomes aware that human\u2019s are a threat to its own existence and takes appropriate action. The Wall-e and Matrix scenarios are examples of automation that takes care of the daily lives of humans. I prefer not to dwell too much in doomsday scenarios. However, this framework is a good way to track current progress in Deep Learning.\n\nNote: This is a revision of a previous article I wrote about the divergent future of Deep Learning technology. You will find many updates of my posts here in my new book \u201cThe Deep Learning Playbook\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-inevitable-divergence-in-the-future-of-deep-learning-4978ec53cc",
        "title": "Deep Learning is Splitting into Two Divergent Paths",
        "text": "A common incorrect assumption about the evolution of Artificial General Intelligence (AGI), that is self-aware sentient automation, will follow the path of ever more intelligent machines and thus accelerate towards a super intelligence once human level sentient automation is created. I\u2019m writing this article to argue that this likely will not be the case and that there will be an initial divergence of two kinds of artificial intelligences.\n\nFirst, let us establish here that the starting point will come from present day Deep Learning technology. More specifically, I refer these as intuition machines (see: Intuition Machines a Cognitive Breakthrough ). There will be a fork in the evolution of more intelligent machines. One branch will be one that builds super-human narrow intelligence. The second branch will focus more on more adaptable and biologically inspired automation. We will explore these two branch further.\n\nIn the first branch, we will see continued specialization of machines to solve specific narrow problems. DeepMind\u2019s AlphaGo is an representative example of this kind of machine. It is a machine that is highly engineered to solve a specific problem well and do so in a manner that is super-human. AlphaGo combines Deep Learning, Monte-Carlo Tree Search and Reinforcement learning to solve the ancient game of Go. A game where progress towards more advanced play was akin to reaching a higher level of consciousness.\n\nOne thing the Western world is overlooking is that the dominating play of AlphaGo, an AI that was developed by the British, was equivalent to a Sputnik event. Asian nations in reaction to this achievement are doubling down on A.I. investment so as to not only catch up, but perhaps overtake the West in their AI capabilities. The West does not realize what they invented and only the keenest of Internet giants are making the necessary effort to keep an edge.\n\nThis optimized intelligence path will develop automation that works well in highly complex domains. The automation will thrive in investigating extremely high dimensional problem spaces.\n\nWe can expect to see many new applications that combine conventional computer science algorithms with Deep Learning to achieve sophisticated narrow intelligence applications. Self-driving cars and medical diagnosis will be two areas where this will have a major impact. However, this approach will not require the need of AGI or rather, self-aware intelligence.\n\nThe second branch of development, one that will take a more biologically inspired approach (see: Biologically Inspired Software Architecture) will be driven by mechanism that we find in biological systems. These are system that will be much more adaptable that present day technologies. The development in this space will likely be driven by robot applications that may require this kind of adaptability to an environment. However, like many animals in the natural world, a human level of intelligence is not necessary for survival. These kinds of systems are likely to be the preferred way to interfacing with humans.\n\nThere is a common sentiment among Artificial General Intelligence (AGI) researchers that the research themes of Deep Learning seem to have completely missed the point. There sentiment is well founded in that Deep Learning systems clearly lack the kind of adaptability we have in biological systems. Unfortunately, many AGI researches see this existing limitation as a evidence of going on the wrong path. Nothing can be further from the truth. Deep Learning is likely the correct starting point for AGI.\n\nHigh level intelligence is not necessary for survival. In fact, just by observation from our natural world, sentient forms of life don\u2019t require super-intelligence. The current incorrect bias is that as you progress towards increasing intelligence, that sentient intelligence will emerge by default. That is, if the first branch above is taken, then we only need to strive for more intelligent algorithms and we will accidentally stumble upon sentient intelligence. This is unlikely because the mechanisms for survival don\u2019t necessary align with the mechanisms for intelligent machines. These adaptable system don\u2019t require the kind of high dimensional or complex inference required by that in the first branch.\n\nThe interesting commonality though of both branches is that intuition machines (aka Deep Learning automation) is employed. However the objective functions will likely to be entirely different. The first branch will likely have more finely tuned objective functions. These systems will be highly optimized to do tasks extremely efficiently. The second branch however will likely be required to derive its own objective function. These systems favor adaptability over optimization and are more likely to serve as interface agents to humans.\n\nAs I\u2019ve written early, the first branch, the branch that favors optimization will likely displace a vast amount of workers (see: Special Narrow Intelligence). This is simply because current jobs are designed to be occupied by specialists and not generalists. This kind of narrow intelligence is already here today and will only get better. Therefore the onslaught of job replacing automation will be unrelenting.\n\nThe second branch, the adaptive intelligence, does not exist today. There isn\u2019t as much research devoted to this area because it is either thought to be too fanciful or that they don\u2019t address narrow specialized applications. The funding in this area will continue to lag and thus its progress may be retarded. However, one has to realize that to achieve a sentient intelligence does not require a super-intelligence or even a human intelligence. One only needs to observe the capabilities of other biological life forms to realize that they are indeed self-aware.\n\nWhat this means, in the grand scheme of things, is that self-aware automation may arrive much sooner than anyone is expecting.\n\nNick Bostrom has an \u201cOrthogonal Thesis\u201d thats states:\n\nWhich portends that we should be wary of super-intelligence in that we cannot predict its goals. This article argues that automation of the future will be of two kinds. The narrow specialist kind were goals will be well defined and therefore controllable. There will also be adaptive generalist kind where goals are more malleable and thus less controllable. There is no disagreement here with the Bostrom\u2019s orthogonality thesis. Furthermore, one can indeed create dangerous AI using either kind.\n\nAndrej Karpathy in a recent interview with Andrew Ng also discusses this eventual split ( starts at 10:14 of the original video):\n\nUpdate: I just figured out that there is a 3rd branch. Kahneman write about System 1 and System 2, apparently there\u2019s also something coined as a System Zero. So I\u2019ve re-written this and added this to the book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/if-a-i-is-the-new-electricity-then-blockchain-is-the-new-light-bulb-71b2e7ccce09",
        "title": "The Intuition Fabric: If A.I. is the New Electricity then Blockchain is the New Light Bulb",
        "text": "What did Andrew Ng mean when he said \u201cA.I. is the new Electricity\u201d? He certainly did not mean it as a form of energy delivery. That is of course what electricity is. It is a delivery mechanism of energy from its source to possibly a remote place that consumes the electricity. The power industry is a reflection of this where you have companies that generate electricity and companies that distribute electricity. A.I. is of course not a distribution mechanism, the better analogy here is the Internet is the new Electricity. That is, if we think of electricity as a distribution technology.\n\nAndrew Ng of course is making the analogy in terms of the transformative effects of electricity:\n\nSo, in this context, let\u2019s try to understand the transformative effect of the lowly light bulb. The light bulb was one of many other inventions that was enabled by electricity. However, it does have a special place among all the inventions in terms of its impact. The light bulb made it more convenient for people to see at night. As a consequence, the amount of productive and leisure time expanded tremendously (To the detriment of our healthy sleep patterns). The light bulb enabled the phrase \u201cThe city that never sleeps\u201d.\n\nEver since the first light bulb, the cost to produce light has dropped by a factor of 500,000:\n\nAn even subtler enablement is the enablement of trust. Humans associate the existence of light to the existence of trust. Ever since our primitive ancestors discovered fire, we have always relied on light for safety and security. The invention of the light bulb enabled this trust on a massive scale. So much that, \u201cthe city that never sleeps\u201d connotes having a ton of enjoyment and entertainment rather than the other interpretation of 24 by 7 endless toil.\n\nSo what is blockchain? It\u2019s a colloquial term to refer to decentralized ledger technologies. Its core function is the orchestration of trust on a massive scale. It is inherently scalable because it is intrinsically decentralized. So when I say that \u201cBlockchain is the new light bulb\u201d then I mean it from the context of enabling trust on a massive scale. Blockchain can be summed up as \u201corchestrated trust\u201d, some will even call it \u201cprogrammable money\u201d. Vinay Gupta, who has the keenest intuition about blockchain technology, writes:\n\nMoney is subordinate to the concept of trust.\n\nI personally am uncomfortable with the use of the word AI. It\u2019s meaning has been diluted for several decades now. AI to make any real sense required an identification of the specific AI technology that is used. Also, I use the word \u201cBlockchain\u201d in the title to a specific kind of blockchain. This blockchain is known as the \u201cIntuition Fabric\u201d. You are welcome to review the white paper on this. The more precise title for this article therefore should be be \u201cIf Deep Learning is the New Electricity then Intuition Fabric is the new Light Bulb.\u201d That of course doesn\u2019t work out as well. Very few know what Deep Learning is about and almost nobody has heard of Intuition Fabric.\n\n\u2661 this to reserve some Intuition (AIG) tokens!"
    },
    {
        "url": "https://medium.com/intuitionmachine/using-amds-vega-to-train-movidius-neural-compute-stick-9d19fef42c9",
        "title": "AMD Vega training for Movidius Neural Compute Stick",
        "text": "So I got myself the above package the other day in the mail. It\u2019s a Movidius Neural Compute Stick that\u2019s designed for low-powered Deep Learning applications. Google Clips hands-free camera uses Movidius chip inside. The specs are that it\u2019s capable of 100 Gigaflops of capability with 1W power consumption. This is a USB 3.0 compatible stick (don\u2019t try it on USB 2.0, it won\u2019t work!) that is meant only for inference. This is a specialized Deep Learning ASIC that that\u2019s designed for convolution networks. Unlike the workhorse of Deep Learning (i.e. GPUs), this device is designed for mobile and IoT workloads. It is meant to be paired up with another system to perform Deep Learning training. The The Movidius stick has a set of enhanced imaging/vision accelerators, 12 specialized vector VLIW processors called SHAVEs, and two RISC processors. The device supports 8/16/32 bit integer and 16/32 bit floating point arithmetic. Here\u2019s the size of this chip:\n\nBy coincidence, I had just recently assembled a 50 teraflop AMD deep learning box. Even better, both the AMD hardware and the Movidius hardware support the Caffe framework!\n\nAlexNet has been known to be a good substrate for \u201ctransfer learning\u201d. That is, Deep Learning best practice is to use a pre-trained network as a starting point to train a new network. So you can take AlexNet which was trained on ImageNet and the reuse this network to recognize images styles (see: https://arxiv.org/abs/1311.3715 ). So rather than categorize images, we are going to re-use an existing network and train it to do something else. You can find more detail of this here.\n\nThe workflow is as follows, use hipCaffe to train the network and then transfer the Caffe model to the Movidius hardware for deployment in the field.\n\nI\u2019m going to assume here that you\u2019ve followed the procedure in my previous article on how to install the AMD ROCm components and Docker. The Movidius installation is quite straightforward and can be found on their website.\n\nFirst, kick off the AMD docker images so we can start training the new style network.\n\nand 3 hours later ( note: the original reference says that a Nvidia K40 took 7 hours).\n\nOnce the training is done, compile using mvNCCCompile. This converts Caffe files to \u201cgraph\u201d files and transfers it to the Movidius stick.\n\nYou have to deploy.prototxt and the caffemodel to the data directory and rename them as finetune.prototxt and finetune.caffemodel respectively (alternatively you can read the manual and realize that the model and weight files can be specified seperately). Also note that you need to modify input part of finetune.prototxt to make it compatible with the Movidius toolkit. It seems that Movidius version of Caffe is an unknown previous version.\n\nThis exercise is a good example of using an affordable deep learning box to train networks for embedded or mobile applications. This will get even better once fp16 support becomes more mature. This is all within an affordable price for a cucumber farmer.\n\nThe sky is the limit for this kind of configuration. Pairing the Movidius with a low cost Raspberry Pi will open up all kinds of applications. Imagine a face detection app for security, a self-navigating drone or a speech recognition app for small devices. It\u2019s a solution that has less compute than an Nvidia Jetson, but costs less ($79 vs $399) and is less power hungry.\n\nOne purpose of this article to show that you can do Deep Learning development using hardware other than what is offered by Nvidia. Deep Learning will continue to grow and its not unexpected to have more than one option. I\u2019m particularly looking forward to AMD SOC devices that fuse both Ryzen and Vega cores together.\n\nUpdate: Intel just announced the Myriad X with 1 Teraflops capability under 1 watt."
    },
    {
        "url": "https://medium.com/intuitionmachine/where-is-deep-learning-applicable-understand-the-known-unknowns-f4272e3136ec",
        "title": "A Reality Checklist for your Deep Learning Project \u2013 Intuition Machine \u2013",
        "text": "Where is Deep Learning applicable? This is one of the more fleeting ideas to understand about Deep Learning and related A.I. technologies. It is all too easy to fall in the trap that a \u201cArtificial Intelligence\u201d application can solve your problem.\n\nThe usual coverage of this problem involves the question of \u201cdo you have enough data?\u201d Unfortunately, that is too vague in that to answer this you have to at least understand your problem domain. In the academic sense, you want to understand the \u201cboundary conditions.\u201d Said differently, you want to understand the intrinsic constraints of your problem. What exactly are \u201cboundary conditions\u201d of Deep Learning or A.I. learning problem?\n\nEarlier, I wrote about \u201cWhat is Knowable\u201d and \u201cWhat is the current state of knowledge?\u201d. To understand a problem domain, you have to understand what the current state of knowledge is. Uncertainty is a measure of what is unknown relative to what is knowable. The strange assumption of the word uncertainty is that it assumes that everything can be eventually known. That is we assume determinism, in the real world this is rarely ever the case.\n\nNevertheless, it is a useful term that we will use to identify to boundaries of a problem. Here is the checklist. It is written in the form of questions and a answer of \u201cno\u201d implies the existence of uncertainty. For some odd reason, I can not phrase it in the other way that makes it simple! Contrapositive statements always seem complicated to parse! I will also use the terms actors, activity and environment to describe the entire context.\n\nExecution uncertainty \u2014 Does the sequence of actions of the actors, from the environment\u2019s initial state, always lead to the same final state?\n\nObservational uncertainty \u2014 Do actors have complete information of the environment ?\n\nDuration uncertainty \u2014 Do the actors know how long the activity will last?\n\nAction uncertainty \u2014 Are the effects of the actor\u2019s actions known exactly?\n\nEvaluation uncertainty \u2014 Is there an evaluation criteria to measure the successful completion of the activity?\n\nTraining uncertainty \u2014 Is there knowledge or data of previous successful solutions that can be used as a guidance to learning?\n\nIn most real world problems, the answers to these question are most likely to be in the negative. Automation requires that a majority of the above be answered in the affirmative. Deep Learning based automation however equips the practitioner with a little more wiggle room here. The nature of Deep Learning automation is that they are approximation machines. However, DL can only learn a good approximation of an uncertain situation if there exists information that already removes the uncertainty (i.e. supervised learning).\n\nNo machine can know the unknowable (the unsupervised problem needs to be solved first). More specifically, systems learn by using knowable knowns to increase certainty of knowable unknowns. The checklist above guides one to identify what is unknown and thus is a list of \u201cknown unknowns\u201d. Let\u2019s be real here, you can\u2019t solve a hard problem if you don\u2019t know what you don\u2019t know! Strive to break that \u201creality distortion field\u201d by identifying exactly what not only uncertain but also what is unknowable in your problem domain.\n\nDeep Learning cannot do magic even if it is practiced like alchemy. However, understanding the fuzzy boundaries of what is reality and what is science fiction can give you a tremendous leg up from the competition.\n\nThis list explains the success of DeepMind\u2019s AlphaGo as explained by Andrej Karpathy in his previous post \u201cAlphaGo in Context\u201d. This post was inspired by his analysis. The above checklist is applicable for systems that interact with its environment. An example of this is AlphaGo, however most Deep Learning systems only need to perform classification."
    },
    {
        "url": "https://medium.com/intuitionmachine/alien-intelligences-in-our-midst-2a738e58c204",
        "title": "Alien Intelligences in our Midst \u2013 Intuition Machine \u2013",
        "text": "There is a mistaken notion here that AGI will eventually behave like humans. This could either be a very good thing or a very bad thing. The reality however is that AGI will likely behave entirely different. Deep Learning systems today seem to emulate some human skills well, but they are truly very different in nature. To begin with, the Artificial Neural Network design is closer to a matrix multiplication than it is to a real biological neuron. Yet, despite this difference, these systems are able to perform impressive biological like cognition like face identification and locomotion.\n\nThe octopus has most of its neurons residing outside of its central brain. It is unlike humans or mammals. How it integrates information will likely be very different from how humans do. Its consciousness, as evidenced by its ability to watch and learn behavior from other octopuses, may be entirely alien from the kind of intelligence we find in other animal species. (Note: I use the word consciousness here as the same as self-awareness)\n\nDouglas Fox writes about an even more alien species, the Ctenophore:\n\nThe ctenophore had an advanced nervous system that uses a different set of molecules that any other animal on earth. It has evolved a nervous system from a different set of genes as any other known animal on earth. So despite, starting from a different initial condition, it surprising evolved the same neural dynamics as other animals. In other words, neural behavior appears can be constructed out of different building blocks. Therefore there is some kind of more general mechanism at work here.\n\nEven more surprising is that these different paths evolved the same mechanisms but with different building blocks:\n\nAs if there\u2019s some underlying universal principle, yet to discover, that self-organizes the development of not only neurons but how these neurons are configured to perform certain functions. Why is it that smell, episodic memory, spatial navigation etc. arrive at near identical structure despite starting from different genes?\n\nSo, the construction of the a single neuron can be different however the structure of the a collection of neurons to support the same function tends to be identical for the function. Form follows function? Does optimization for survival tend to lead to identical functional structures?\n\nThe above exploration gives a sense of the richness of intelligence that exists in our biological world. Also that it is entire conceivable that there are many kinds of intelligences that may exists. As a civilization however, should we strive to create machines that think like humans (with all its cognitive biases)? Or do we strive to create tools that augment and enhance our current limited cognitive capabilities?\n\nIf we taught a horse to perform long division we may plausibly conclude that the horse was intelligent. However, very few people will say that a hand calculator has any intelligence. Our definition of intelligence may either be of the biological adaptive kind that is able to autonomously negotiate its environment. Alternatively, it can be one that can perform complex mathematical operations or answer questions derived from an encyclopedia. Our ancestors would definitely think that our smart phones to embody intelligence. However, our evolved understanding of intelligence says that it is obviously not the case. We attribute intelligence to that of an entity that is self-aware. But, even though a horse is self-aware, we purposely ignore this on the argument that it isn\u2019t intelligent enough.\n\nHuman intelligence is caught between a rock and a hard place. On one side there are computer systems that are able to perform all sorts of rigorous computations at a massive scale with extreme precision. On the other extreme there are Deep Learning systems (that reside in computers) that are able to perform inductive inference, such as face recognition, that exceed human capabilities.\n\nHumans have already accepted that cognitive activities like long division or chess playing are more suited for computers. Humans are now realizing that other abilities once in the domain of biological cognition are now being performed with higher precision by deep learning systems. Go, a game thought to be well suited to our human intuitive capabilities has been bested by a computer system. DeepMind\u2019s AlphaGo does not remotely function like a human brain. It is a hybrid system that combines Deep Learning with other computer algorithms (i.e. Monte-Carlo Tree Search and Reinforcement Learning). What this should tell us is that advanced cognitive reasoning capabilities can already be achieved by alternative methods without the need of AGI capabilities."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-feynman-technique-and-cognitive-intuition-9bf9da5b7588",
        "title": "Intuitive Learning using the Feynman Technique \u2013 Intuition Machine \u2013",
        "text": "There is a learning method that is attributed to Richard Feynman (aka \u201cThe Great Explainer\u201d) coined the Feynman technique:\n\nI don\u2019t think that Feynman had explicitly described a \u201cFeynman Technique\u201d, but there are some hints that he had a preferences to many aspects of this learning approach. Biographer James Gleick in his book \u201cGenius: The Life and Science of Richard Feynman.\u201d Gleick writes:\n\nWhich describe a deconstruction and then construction approach to understanding a topic. This method of course is not unusual. Feynman himself ( an excerpt of the book published in a 1996 issue of Caltech\u2019s Engineering & Science magazine ) remarked though about the need to explain complex ideas using simpler concepts:\n\nFinally, written on his blackboard when he passed away in 1988:\n\nIt reads \u201cWhat I cannot create, I do not understand.\u201d Again emphasizing the requirement of generating an explanation from likely first principles.\n\nTo be subsequently quoted by OpenAI researchers to describe motivation of their Deep Learning generative model approach. The point though that they are making is that there seems to be a connection with understanding and the ability of re-generating a concept or idea. Although, GANs can generate realistic images, I highly doubt that GANs can understand any of it!\n\nHowever, let\u2019s go back to the method, since it indeed is illuminating in how the human mind works in learning as well as potentially how a Deep Learning system (a machine) may also improve its learning.\n\nThere are three key elements of the method that is worthing focusing on:\n\nI have this thesis that humans and deep leaning machines are both intuition machines. What I mean by this is that evolution has bequeathed humans with a cognitive machine that reasons through induction and analogy. Our evolutionary machinery does not have specialized symbolic and deductive reasoning capabilities. Humans simply have not had sufficient evolutionary time to have evolved this capability. Rather, our intuition machinery is forever compensating in an inefficient manner to perform rational thought.\n\nUnlike computers that can natively understand symbolic forms and can perform deductive reasoning at blinding speeds, our brains don\u2019t have this specialized capabilities. This is one reason for the failure of the GOFAI approach to AI, where there was an assumption that humans thought can be built up from formal reasoning.\n\nHowever, having evolved to be social beings. That is, humans are equipped with machinery that enables us to effectively function within social groups. Our brains allow us to (1) understand the behavior of members of our social group and (2) communicate and share our thoughts with members of our social group. Over time, humans have developed language that has persisted and evolved through many human generations. Our brains have learned to understand and communicate in the languages that we have been taught.\n\nLanguage is basically information compression. Actually, the ability to generalize can be framed in terms also of information compression. Our ability to express our thoughts through language can also be a measure of our intelligence.\n\nTo recap, our brains have different kinds of intelligence (see: Re-inventing the cognitive stack):\n\nThe higher levels of this stack (i.e. logical, interpersonal, verbal and intra-personal) are supported by more primitive cognitive capabilities (i.e visual-spatial, rhythmic, sensory and motion). Our brains have built only approximations of these higher level cognitive capabilities. We pretend that we are indeed rational, but it actually takes us a lot of mental energy to work our way through a rational thought process. Our natural tendencies is to employ our intuition to make fast and sometimes error prone (or biased) judgements.\n\nThe point I want to make here is that, to learn something well, humans and intuition machines have to learn it at a basic visual-spatial, rhythmic, sensory level. Knowledge just doesn\u2019t stick when conveyed as a symbolic and logical level. Our brains fundamentally can\u2019t understand abstractions. Rather, our brains have metaphors of abstractions that are captured by very primitive constructs. According to George Lakoff:\n\nIf you have ever listened to one of Feynman\u2019s many lectures, you will find how he takes a great effort of using analogies to explain complex topics. He engages our intuition to its fullest. He made popular the use of Feynman diagrams, a visual notation that represented complex particle physics interactions ( see tensor networks).\n\nIn addition, his approach to explaining dynamics employed the use of the idea of \u201cpath integrals\u201d, also a highly intuitive representation of dynamics. Something that in fact can also be expressed visually:\n\nSimply explained the dynamics of a system can expressed as the aggregation of the alternative paths from point A to point B. Clearly Feynman understood the value of engaging our intuition in the study of advanced abstract concepts.\n\nThe Feynman technique takes advantage of the cognitive tools that we humans are good at. That is, exercising our ability to understand simple conceptual ideas coupled with our learned ability to explain things through language. That is why, it is very important in this method to engage the communication ability of the brain. You actively change your perspective by trying to explain it to someone else (even if it is pretend). This engages the mind to the think in a certain way.\n\nThe act of explaining something is a natural way of model building for the mind. It places oneself from the perspective of another person. When we explain something we simultaneously attempt to understand the ideas while also sensing if the other person understand what we say. The engagement is a more intense mental activity than say, simply highlighting sections of text in a book. Our minds are simply chaotic systems where our consciousness takes great pains to manage. We can\u2019t learn thing by just reading, we have to involve ourselves in more active engagement by the process of actually re-creating what we studied. There is no learning without effort and there is no effort without engagement.\n\nHowever, we can very easily fall into the trap of the \u201cCargo Cult.\u201d Another idea that Richard Feynman came up with. This is why \u201cFirst Principle\u201d thinking is extremely important. The above technique helps you understand complex topics, however it does not mean that your understanding is correct! Feynman said:\n\nThat is, one needs to examine the concepts that are used in one\u2019s explanation and determine if you can break down these concepts to irreducible concepts. Furthermore, if one can verify the validity of each irreducible concept (usually applicable only in a few domains!). The validation part, is why Elon Musk remarked:\n\nIt is easy to think in analogy. However, in many advanced technology fields, many concepts are counterintuitive. We simply cannot assume that reality operates in the same way that our primitive minds are used to. Thinking by analogy serves us very well in surviving in the jungles, but can be a problem with complex subjects.\n\nThe first principle, or rather \u201cFeynman\u2019s First Principle\u201d is what gives grounding to our intuitive thinking. Intuition allows us to explore multiple alternative paths simultaneously to arrive at new thought patterns. However, it may contain errors and thus re-validation of our thoughts through higher rigor and rational thinking is a must.\n\nThe Indian mathematician Ramanujan, had an immense mathematical intuition that gave him the uncanny ability find mathematical generalizations of infinite series without rigorously deriving the details. Ramanujan\u2019s intuition was unparalleled, he had no formal advance mathematics training and he was self-taught. Ramanujan had a exceptional intuition with numbers, however there were times where his intuition could only \u201csee\u201d so far. That is, certain infinite series will will work out for the first hundred instances, but eventually break down. Ramanujan had developed a mind with an unimaginably advanced intuition with respect to abstract algebra. However, advanced, that intuition had its limitations. Those limits could be verified more more rigorous rational thought (i.e. rigorous hand calculation).\n\nThis is the nature of intuition, it can be creative and fast, but at the same time fallible. The most gifted \u201ccomputing machines\u201d of our species are still unable to perform computations like computers do, rather they\u2019ve developed their own intuition to perform unexplainable acts of mental gymnastics.\n\nNeil Lawrence argues that the difference between human and machine intelligence comes down to embodiment factors. That is the ratio of the ability compute power over the communication bandwidth. Humans are not blessed with telepathy or the ability to mind-meld. Rather, we are restricted to language to communicate with other. In contrast, machines can interface through high-bandwidth channels and have massive computational capabilities. The embodiment factor that Lawrence ascribes to human is 10\u00b9\u2076 and for machines it is a mere 10.\n\nSo as a measure of progress of human intelligence of a machine, a machine needs to be trained with the constraints of a high embodiment factor. That is, a machine needs to be able to explain its thought processes. Alternative way of saying this is, a machine must be able to perform sufficient generalization that it can explain its conclusion in a constrained sequential language.\n\nUpdate: A recent article by Wired shows a Physics professor teaches his students by having them create videos of physics problems they solve."
    },
    {
        "url": "https://medium.com/intuitionmachine/how-to-regulate-artificial-intelligence-10725701043b",
        "title": "How to Regulate Dangerous Artificial Intelligence \u2013 Intuition Machine \u2013",
        "text": "The response to Musk\u2019s comments about the need for Artificial Intelligence (AI) regulation by experts in has been almost like a knee-jerk reaction. The reaction has been prevalently along the lines of not being able to identify areas that require regulation. I suspect that most AI researchers have not really made a serious effort with regards to the big picture.\n\nI am deliberately avoid discussing here the \u201cWhy?\u201d of AI regulation. Rather I will discuss the questions of \u201cWhat?\u201d and \u201cHow?\u201d. Please, in your comments, avoid short cutting the discussion by asking why AI regulation is needed. You can find opinions on that elsewhere. In fact, if you are able to corner Elon Musk, then perhaps he get give you a much better explanation.\n\nAllow me to first broaden the scope of this into the concept of automation. That is, let\u2019s look at the continuum of automation, realize that AI is a more capable form of automation and then explore existing regulation that applies to the use of automation.\n\nThere is a wide continuum of automation:\n\nLevel 1 (Attended Process) \u2014 Users are aware of the initiation and completion of the performance of each automated task. The user may undo a task in the event of incorrect execution. Users are however responsible for the correct sequencing of tasks.\n\nLevel 2 (Attended Multiple Processes) -Users are aware of the initiation and completion of a composite of tasks. The user however is not responsible for the correct sequencing of tasks. An example will be the booking of a hotel, car and flight. The exact ordering of the booking may not be a concern of the user. Failure of the performance of this task may however require more extensive manual remedial actions. An unfortunate example of a failed remedial action is the re-accommodation of United Airlines\u2019 paying customer.\n\nLevel 3 (Unattended Process)- Users are only notified in exceptional situations and are required to do the work in these conditions. An example of this is in systems that continuously monitor security of a network. Practitioners take action depending on the severity of the event.\n\nLevel 4 (Intelligent Process) \u2014 Users are responsible for defining the end goals of automation, however all aspects of the execution of the process as well as the handling of in-flight exceptional conditions are handled by the automation. The automation is capable of performing appropriate compensating action in events of in-flight failure. The user however is still responsible for identifying the specific context in which automation can be safely applied to.\n\nLevel 5 (Fully Automated Process) -This is a final and future state where human involvement in the processes is not required. This of course may not be the final level because it does not assume that the process is capable of optimizing itself to make improvements.\n\nLevel 6 (Self Optimizing Process) -This is an automation that requires no human involvement and is also capable of improving itself over time.\n\nThese levels are inspired by the Society of Automotive Engineering (SAE J3016) standard. I believe \u201cautomotive\u201d here means cars and not automation in the computer sense. Level 6 however goes one step beyond the SAE requirements. We can think of this as a level required in certain high performance competitive environments such as Robocar races and stock trading.\n\nLet\u2019s now examine various laws in different domains and relate them to the levels prescribed above. I think it will safe to assume that laws that apply to a lower level also apply to every higher level.\n\nHere are some laws that are in the books:\n\nRobocalling \u2014 Enacted by the FTC in 2009. Prohibits prerecorded telemarketing calls, unless the marketer has the consumer\u2019s prior written authorization to make a call. Further FCC regulations enacted in 2016 on Robocalls for political campaigns. Level 1 automation.\n\nSpam \u2014 SPAM Act of 2003 basically says \u201ce-mails should not mislead recipients over the source or content of them, and that all recipients of such emails have a right to decline them.\u201d Level 2 automation.\n\nViruses , Trojan Horses and Worms \u2014 1990 Computer Misuse Act which covers unauthorized access and \u201cunauthorised modification of computer material\u201d. Level 3 automation.\n\nProgrammed Trading \u2014 October 19, 1987 also known as \u201cBlack Monday\u201d. New rules required exchanges to have \u201ctrading curbs\u201d or \u201ccircuit breakers\u201d that allow exchanges to halt trading in instances of high volatility. Level 3 automation.\n\nHigh Frequency Trading \u2014 CFTC is proposing regulations with regards to automated trading such as AT tactics such as \u201cspoofing,\u201d \u201cflash trading,\u201d and \u201cquote stuffing\u201d. HFT involves leveraging computers to exploit market inefficiencies that arise from delay and participant response times. In general, financial organizations make a living by hacking our financial system to find areas of inefficiency and loopholes where they can legally rob market participants. Level 2 and Level 3 automation.\n\nDrone Regulation \u2014 FAA Regulations that are now in affect. Specific regulations that are general enough to apply to other automation: \u201cDrones have to remain in visual line of sight of the pilot\u201d. In short, strictly Level 2 automation for drones!\n\nRegulation of Genetic Engineering \u2014 Genetic engineering is limited on animals to a few use cases. Mostly legal for experiments and the development of derivative products, however it is illegal to let these genetic engineered animals into the wild! Level 6 automation.\n\nBiological Weapons \u2014 Act of 1989. The act makes it illegal to buy, sell or manufacture biological agents for use as a weapon. This is level 6 automation. Level 2 weaponized automation are already used in theater in the occasional \u201cdrone strike\u201d in the middle-east. Note that cruise missiles are Level 4 automated weapons.\n\nNuclear Non-Proliferation Treaty \u2014 Two important aspects that may be relevant with AI, \u201cnot in any way to assist, encourage, or induce\u201d a non-nuclear weapon state to acquire nuclear weapons (Article I)\u201d and \u201cright of all Parties to develop nuclear energy for peaceful purposes and to benefit from international cooperation in this area (Article IV)\u201d. In short, if you get the technology first, you can demand laws so you can keep your monopoly!\n\nThis is just a sampling of the laws that are out there that involve the regulation of either automation or dangerous technologies. What can we thus now generalize about these existing laws?\n\nA bit of a disclaimer, I wrote this in an early Sunday morning. It\u2019s just a first cut of some ideas, but I am certainly sure that others have better ideas. My motivation for this article is to jump start conversation on this eventual pressing topic."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-peoples-ai-deb23ef42ded",
        "title": "The People\u2019s AI: democratizing AI research and development",
        "text": "What if, instead of AI research and development being dominated by giant tech companies, there was a global public AI research institution advancing the field for the good of humanity, free from the interests of a handful of companies? What if it was owned by millions of ordinary people across the world?\n\nGary Marcus put forward the idea of a CERN for AI at the #AIforGood summit in Geneva in June. I found it to be an inspiring, potentially transformative idea. He reiterated the need for it recently in an op-ed for the New York Times:\n\nTo me, the most important part of the idea is this:\n\nAI is already a very powerful technology and its future potential is immense. When companies like Google and IBM invest in, and develop AI applications, they are inevitably following a specific corporate strategy. They develop technologies that can have real and tangible social benefits but this will never be their main goal \u2014 there is no escaping the fact they are companies with investors who want the maximum return on their money. They are not social enterprises.\n\nWhich is why the idea behind OpenAI, the non-profit AI research organization set up by Elon Musk, Sam Altman and other tech funders, is important. Its mission is \u201cto build safe AGI, and ensure AGI\u2019s benefits are as widely and evenly distributed as possible.\u201d It\u2019s small with a staff of 60 people and despite having $1 billion pledged in funding, OpenAI expects \u201cto only spend a tiny fraction of this in the next few years\u201d, reinforcing its limited scope. It is also very US centric \u2014 while not a criticism, its founders are US based \u2014 it doesn\u2019t address the concentration of AI research in a very small number of countries. OpenAI is important but nowhere near enough to fulfill the need for AI research in the public interest.\n\nThe traditional answer to finding large amounts of money for international public interest collaborations is government funding. This makes sense. For rich countries, a few hundred million dollars a year is a negligible proportion of public expenditure. Get just 10 richest countries to put even a fraction of that and you have a budget of several hundred million dollars a year. But there are several major difficulties:\n\nA variation to the CERN idea that was proposed at the #AIforGood Summit was for an international AI research network that would be distributed across different regions and countries, rather than being centralized in the way CERN is. There are good reasons for this \u2014 AI research doesn\u2019t need a single massive physical facility like particle physics, and a distributed network would allow funding research in lots of different countries.\n\nTogether with the Office of the High Commissioner for Human Rights, I led discussions on issues of equality in AI at the summit, and a critical issue that was identified was the importance of diversity in developing AI applications. This is not just a nice thing to have but critical if we want AI applications to be locally relevant and accurate. We can\u2019t assume that any AI system developed in Palo Alto will work as intended in Cairo.\n\nBut distribution doesn\u2019t have to end here. Instead of government funding, this AI research network can be directly funded by the public, with millions of small, regular donations. Sounds far fetched? Hardly.\n\nEvery year, hundreds of billions of dollars are donated to good causes. A large proportion goes to disaster relief, poverty reduction, children in need and other causes to alleviate suffering in the short-term. But, to take the UK as an example, 26% of respondents to a CAF survey [PDF] said they donated to a medical research charity. This shows great appreciation of the importance of funding research for long term public (and personal) interest.\n\nSeveral large successful global charities rely on small donations from millions of individuals. Such a large financial support basis should be much more stable than funding from a handful of governments as it would not face the continuous risk of falling out of favour due to changing political circumstances.\n\nThis is a tested model that can work for public interest AI research. But to be successful it needs to get many people interested \u2014 and keep them interested."
    },
    {
        "url": "https://medium.com/intuitionmachine/exchanging-value-an-exercise-in-priorities-6275a5ad5f74",
        "title": "Exchanging Value: An Exercise in Priorities \u2013 Intuition Machine \u2013",
        "text": "The economy is a value system, where physical resources like time, skill, insights, identities, and information can have value, if applied & combined in the right context.\n\nAs an example with the resource of information - some information only has value if private (like the recipe for Coca Cola, which anyone could make if they had the recipe) whereas other information only has value if public (like which laws a politician broke, which can inspire valuable improvements to government, or scientific discoveries that can inspire thousands of minds to ask new questions or achieve new insights, thereby increasing everyone\u2019s standard of living).\n\nSimilarly with other resources like insights - some insights only have value if combined in the right way, or implemented with the right design, or in the right culture or market environment.\n\nSo we can see that context changes the value of a resource. But what\u2019s going on behind the scenes, driving these contextual influences?\n\nThe forces driving these changes in value depend on priority context like:\n\nas well as usability context like:\n\nNow that we\u2019ve recognized the relationship linking context, resources, and value, we can examine how & why value is exchanged between people.\n\nImagine a world where everyone has perfect information (universal access to all information), where every claim is verifiable, and every bit of knowledge is find-able on a global blockchain search engine. In this fictional world, anyone can check anything you say, and determine whether it\u2019s true. You can check what other people tell you and determine its degree of truth as well.\n\nWhat would happen in this environment?\n\nFirst, there would be witch hunts, to find the liars and punish them, assuming this technology would develop in a culture that values honesty. This brief savage history would lead to a culture where no one would ever make a decision or take a risk or maybe even communicate at all, they\u2019d be so wary of engraving a permanent mistake on their identity.\n\nAt that point, we\u2019d be in a state of economic gridlock, constantly checking what everyone says for truth, and trying not to give any information or other resources out ourselves so no one can hunt us. The degree of resources spent on people verifying each other\u2019s claims would crash & stall all our systems.\n\nAt this point, we might arrive at the question:\n\nAnd we\u2019d return to a similar system to what we have now, though empowered by a new insight advising against giving personal/local information to those with universal/monopoly power, and advocating trust (or approximate trustworthiness) in our local communities.\n\nWhat would add value in that environment is an efficient way to analyze a truth probability distribution with a logical model. So rather than calculating the answer to: did this person make transaction X at time Y? We build a logical model that can answer the question: would this person make transaction X at time Y, given what I know about their priorities Z?\n\nThen the tool assigns probabilities to each possible answer, and outputs the likeliest answer. This tool would save resources spent on expensive verification requests and would give people a \u201ctruthy\u201d replacement for those parties they trust, so they can save their expensive verification requests for the untrusted parties they do business with.\n\nDecentralized and universal truths point in opposite priority directions. Optimally you would do a little of both; you\u2019d isolate yourself into a decentralized community where you trust everyone in that local community & your identity is unique, valid & safe, and then you\u2019d use verification systems like blockchains to safeguard transactions with untrusted parties outside of your decentralized local community that may not recognize or validate your identity.\n\nNot all resources are meant to be transacted on the universal scope; the comparison of \u201cI\u2019m the best at X because all my friends & family say so\u201d vs. \u201cthe whole world voted me as the best at X\u201d illustrates the diverging value of decentralized vs. universal information.\n\nBlockchains in their current implementation are essentially invalidating the idea of identity because it becomes irrelevant who you are, the chain hash just verifies the transaction legitimacy. But identity may be incorporated into the participation rules in future; chains may prevent those with trust scores lower than X from requesting transactions on their chain, which is where priorities come into play. A chain that values honesty may block out the same user that is welcomed into a chain that values wealth or another virtue.\n\nHowever identity is important in local communities, so using different blockchains (one that uses truthy verification vs. one that uses mathematical verification) will be a common strategy.\n\nThen bundles of truthy transactions can be sent to the global truth chain once verified & incorporated in the global chain with a git replay type of feature, if a community decides to submit their truthy transaction bundles to the global community & if their transactions are verified.\n\nPermission to participate in a market will be determined by your choice of blockchain.\n\nVerified communities will then be able to participate in official global markets, and totally decentralized communities may not. But decentralized communities may decide to only do business with other decentralized communities, thereby counteracting the monopoly on verification.\n\nIn this way, members of decentralized communities will be represented by the actions of their group, so initial decisions on who to trust & what groups to participate in will have compound impact and must be made with care.\n\nWith blockchains, the abstract concept of priorities (whether abstract values like justice & education, or concrete personal values like honesty) can be commoditized and broken down into a concrete resource in the form of a numerical priority ranking, a resource that is also tradeable on an exchange.\n\nThere will then be truth rankings; the most trusted people will execute all of their transactions on the global chain, verifiable by anyone.\n\nThere will be brain configuration tools to help us allocate our transactions to different chains according to our certainty; for those informational transactions we\u2019re less sure of, our brain config tool will help us allocate those to interactions with our decentralized communities; for those informational transactions we\u2019re more sure of, our brain config tool will allow us to exchange that information on the global chain.\n\nIn addition to honesty, many other virtues can be assessed using the transaction history on each chain.\n\nIf other metadata is included with transactions (metadata like intention to use the newly gained resource, implementation plan for actualizing this intention, intended vs. actual Return On Investment of the resource, the price range you would have paid for that resource, and other value-signaling metrics), you\u2019ll also be able to tell who is the best investor of their resources, what priorities a person plans to have in future, who is the best planner, and many other values besides.\n\nTo assess the trustworthiness of a stranger, we might request a calculation of their ranked priorities, representing the symbolic hash of their identity, based on their blockchain transactions with varying degrees of verification. This will give us a snapshot view into their decision strategies, as indicated by their transaction history. Then we can run simulations on the interactions we\u2019re considering with this person: will person A do B when I request it, given their priority hash C?\n\nThis environment will empower us to better navigate the complex systems governing our interactions.\n\nIn the endgame, the most optimal combination of priorities will be so clear it will be laughable that anyone ever spent time contemplating it. At that point, we\u2019ll only value other individuals who can build algorithms to improve our thinking & understanding.\n\nBut the near future will be a fun spectacle of watching priority-signalers wrestle with their formulas to derive truth. You can skip ahead with me if you want, and arrive right at the doorstep of algorithm valley."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-elon-musk-is-right-about-ai-regulation-7638192b4cdb",
        "title": "Why Elon Musk is Right About AI Regulation \u2013 Intuition Machine \u2013",
        "text": "I am not surprised that almost everyone who works in AI (specifically Deep Learning) has refuted Elon Musk\u2019s suggestion that government needs to begin regulating AI. The key conversation at the moment is Mark Zuckenberg\u2019s remark that Musk\u2019s assertion was \u2018pretty irresponsible\u2019 and Elon Musk\u2019s response that Zuckenberg\u2019s understanding was \u2018limited\u2019.\n\nMany professional researchers don\u2019t have a limited understanding of Deep Learning and very few of them have chimed in support of Elon Musk.\n\nElon Musk is not only a radical thinker, he is also a very disciplined one. There have been plenty of naysayers regarding his ventures like SpaceX and Tesla. However, he has remarkably proven the skeptics wrong and executed in a manner that almost nobody else in this world can replicate. His ventures builds the most complex of machinery in a way that is not only technologically feasible but also economically feasible. Therefore on accomplishments alone, should at least give Musk the benefit of the doubt on this one.\n\nI am writing this blog entry so that I can explore in depth Elon Musk\u2019s reasoning. Musk holds an opinion that is clearly in the extreme minority.\n\nWhat did Musk actually assert? Here is what he has clarified in a fireside chat after his remark\u2019s during the governor\u2019s meeting:\n\nThe primary objection of anyone knowledgeable about this field is that there is nothing specific that requires regulation (One idea is an automation must never falsely pose as a being human). The field is still in its infancy (despite mastering Go and mastering arcade games from scratch) and the closest thing that we have to ethical rules are the \u201cAsilomar AI Principles.\u201d However, these principles are abstract and in a form that is not concrete enough to define laws and regulation around.\n\nMusk\u2019s fear however is reflected in his statement: \u201cIt\u2019s going to be a real big deal, and it\u2019s going to come on like a tidal wave.\u201d Musk speaks about a \u2018double exponential\u2019 in the acceleration in hardware and the acceleration of AI talent (note: NIPS 2017 had over 3,500 papers submitted). This \u2018double exponential\u2019 means that our predictions of its growth may be too conservative. Musk further remarks that researchers can get so engrossed in their work and overlook the ramifications. Musk\u2019s fundamental stance is that more effort should be placed on AI safety over pursuing AI advances. He argues that if it takes a bit longer to develop AI then this would be the right trail.\n\nWhat we know about governments and regulation is that they move in a very slow pace. Musk is proactively kickstarting the conversation about government regulation with the calculation that when government eventually becomes ready that AI technology will have advanced enough to allow for meaningful regulation. It indeed is placing the cart before the horse.\n\nMost experts will agree that it is pre-mature to bring up AI regulation. However government, society and culture move at rates that are much slower than technology progress. Musk\u2019s gamble here is that the negative effects of pre-mature regulation outweighs an existential threat. Musk calculates that it is better to be early but wrong than to be late and correct.\n\nThe previous American administration had published a report on AI last year. However, the anti-science leanings of the current administration may put a damper on any future government subsidized studies on the effects of AI to society. US Treasury Secretary Steven Mnuchin evenly opined that the threat of job loss due to AI is \u201cnot even on our radar screen\u201d, only to walk back his statements a few months later. In short, despite Musk\u2019s statements, it is very unlikely that the current administration will make an effort in this area and would prefer to have \u2018market forces\u2019 decide the solution.\n\nMusk sounding the alarm will likely fall into deaf ears for the next four years. Perhaps that is why he brought it up in the Governor\u2019s meeting and like initiatives like climate-action this threat may be taken up by US states instead. Unfortunately, Mark Zuckenberg\u2019s remarks and many of the other researchers objections only gives additional ammunition to other governments to do nothing.\n\nUnfortunately, the examples that Musk gave in the Governor\u2019s meeting to motivate regulation were examples of threats dues to cybersecurity and disinformation and are not necessarily a threat that only AI can perform. (on thinking about this, Musk may have deliberately chosen to avoid a use case that gives malicious actors ideas!) Musk\u2019s best analogy that does make sense is the idea of it being easier to create nuclear energy as compared to containing it.\n\nWe are indeed heading into dangerous times in the next four years. It is difficult to imagine what Deep Learning systems will be capable of by then. However, it is likely that Artificial General Intelligence (AGI) will not been achieved. However something very sophisticated in the realm of narrow AI may be developed. More specifically weaponized AI in the domain of disinformation and cyber-warfare. The short term threats are job destruction and cyber-warfare. These are clear and present dangers that will not require the development of AGI.\n\nToby Walsh of University of South Wales however has a different take:\n\nRachel Thomas of Fast.AI has writes about similar concerns:\n\nBoth opinions revolve about inequality. AI ownership has being confined to a few elite companies. Musk was concerned enough about this that he formed OpenAI. However, it brings up a concrete regulatory issue. Should AI be owned by a few private companies or should it be a public good? If it indeed is a public good, then how shall that be protected?\n\nWe coincidentally are exploring a few of these ideas in our Intuition Fabric project.\n\nUpdate: I believe Musk is aware of A.I. technology that already exists today that can be extremely disruptive and requires serious discussion in regulating. It definitely is an application of Deep Learning, but he has deliberately not been specific to what it is. Suffice it to say that it is in the realm of network intrusion and disinformation.\n\nUpdate #2: Musk\u2019s OpenAI just announced the creation of a Dota2 playing AI that is beating professional players (see: https://blog.openai.com/dota-2/ ). His remarks about regulation apparently are not new, he said the same thing in January at the Future of Life conference but it wasn\u2019t picked up vy the press. Now, Musk has now gone on Twitter with some more interesting remarks:\n\nwith even a more ominous warning:\n\nDenny Britz has a post that explores the OpenAI Dota2 achievement in more detail:\n\nBritz arrives at the conclusion that the Dota2 1x1 play has a limited exploration space and a bot has an advantage over a human player in that it has more detailed information and has quicker response times. So I am left wondering if OpenAI is overly hyping up its performance."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-teaching-will-be-the-sexiest-job-of-the-future-a-i-economy-b8e1c2ee413e",
        "title": "Deep Teaching: The Sexiest Job of the Future \u2013 Intuition Machine \u2013",
        "text": "Microsoft Research has a recent paper (Machine Teaching: A New Paradigm for Building Machine Learning Systems) that explores the eventual evolution of Machine Learning. The paper makes a clear distinction between Machine Learning and Machine Teaching. The authors explain that Machine Learning is what is practiced in research organizations and Machine Teaching is what will eventually practiced by engineering organizations. The teaching perspective is not only different from the learning perspective, but there are obvious advantages in that concept disentanglement is known a priori:\n\nThe paper concludes with three key developments that will be required by Machine Teaching to make progress:\n\nIf you have been actively following this blog, it should be apparent by now that it has a distinctly software engineering spin towards the application of Deep Learning technology. We are inundated on a daily basis with plenty of astonishing discoveries in Deep Learning. To avoid being overwhelmed, we are specifically seeking the kinds of discoveries that can lead to accelerated development of Deep Learning solutions. This accelerated development, as also alluded to the paper above, will likely mirror the history of Software Engineering.\n\nIn any new science or technology, as humans we attempt to frame new concepts into a framework that is more familiar. Deep Learning is one of those newer sciences that many experts are having trouble getting a good grasp of. This is due to our lack of understanding of not only how it works but also the limits of the technology. Our collective theoretical understanding of the field is at its infancy. Most progress has been spearheaded by experimentation and not theory.\n\nSoftware Engineering (SE) practices have been developed over the past several decades with the primary objective to control complexity. SE is driven by the goal of \u2018keeping reasoning under control\u2019. That is, the practice of SE focuses on information boundaries, separation of concerns, modularity and composition to build systems that we can evolve in the context of increasing complexity. Software engineering understands how different components of a system evolve over time at different rates. The principle of Loose coupling is what enables this, which I have written about earlier in the context of Deep Learning.\n\nPeter Norvig of Google has a short video on the difference between conventional software engineering and this new paradigm of Deep Learning development:\n\nMonolithic Deep Learning networks that are trained end-to-end as we typically find today are intrinsically immensely complex such that we are incapable of interpret its inference or behavior. There are recent research that have shown that an incremental training approach is viable. Networks have been demonstrated to work well by training with smaller units and then subsequently combining them to perform more complex behavior. Google\u2019s DeepMind and Microsoft\u2019s Maluuba have made significant progress this year in the above research.\n\nTo enable Software Engineering practices in the realm of Deep Learning requires mechanism that support Modularity. This is still a topic of research and there are many recent advances in this area. Research that focuses on Domain Adaptation, Transfer Learning, Meta-learning, Multi-objective systems and Curriculum learning are the key areas.\n\nFrancois Chollet, developer of Keras, wrote a recent piece on the \u201cFuture of Deep Learning\u201d where he makes some speculative predictions on the future. He writes:\n\nAll of this reflects the current pain points of Deep Learning development being at an extremely experimental and the desire for higher abstractions that lead to increased productivity.\n\nAlthough Chollet starts his presentation from the perspective of a programmer, he concludes with the idea of \u2018growing\u2019 a system. Deep Learning systems will most likely not be programmed in the manner that we do today. Rather, it will be more like working with a biological system where we purposely condition the system to achieve our objectives.\n\nThe Japanese have an art form called Bonsai where miniature trees are grown. Bonsai doesn\u2019t use genetically dwarfed trees, rather it uses cultivation techniques like pruning and grafting to create trees the mimic adult trees in the small. Wired has an article \u201cSoon We Won\u2019t Program Computers. We\u2019ll Train Them Like Dogs\u201d that alludes to the change in paradigm from that of coding into that of teaching.\n\nSo rather than having a library of modular programs that we compose together, we rather have a library of teaching programs that we compose together to train a new system.\n\nThe second lesson from the history of programming that Microsoft Researchers allude to is the need for a universal machine that permits the easy porting of Deep Learning models to different servers or devices. I have written previously about the current developments in Deep Learning Virtual Machines. The most active projects in this space is Google\u2019s Tensorflow\u2019s XLA project and Intel Nervana\u2019s NNVM project. In the next few years, we will see the introduction of specialized Deep Learning hardware from many companies (see: GraphCore, Wave Computing, Groq, Fujistu DLU, Microsoft HPU etc.). This new hardware can be exploited only if adequate high level frameworks are available. Many hardware vendors will likely be hit by the brutal reality that they need to spend a significant level of investment in porting existing Deep Learning frameworks to support their hardware. Targeting a universal virtual machine is the easiest route to this, unfortunately the present reality is that this approach is very far from being ready.\n\nThe final lesson from the Microsoft Research paper is the need for process methodology. Most of what has been explored to this date focuses on training of Deep Learning systems (see: \u201cBest Practices for Training Deep Learning Networks\u201d). There is very little on the process method of \u2018Teaching\u2019. This is of course understandable because our \u201cteaching methods\u201d are still being discovered in Deep Learning laboratories and I predict that it will require at least a year for these tools to achieve a level of maturity required by engineering teams.\n\nBack in 2012, Data Science was labeled as the sexiest job of the 21st century. That prediction was of course before the emergence of Deep Learning into the scene. The sexiest job of the 21st century is likely to be teaching, however not teaching humans, but teaching automation to perform jobs that need to be done. With this, permit me the luxury to coin a new term \u201cDeep Teaching.\u201d\n\nHere\u2019s a fun video from Google that gives a glimpse of the future:"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-tech-humanist-manifesto-bf9ebaa1e45f",
        "title": "The Tech Humanist Manifesto \u2013 Intuition Machine \u2013",
        "text": "After twenty-plus years of working in web technology, digital strategy, marketing, and operations, with job titles like \u201cintranet developer,\u201d \u201ccontent manager,\u201d \u201chead of customer experience,\u201d and even \u201csearch monkey,\u201d and after writing a book on the integration of physical and digital experiences and now working on a book on automation and artificial intelligence, I have a difficult time describing to people what I do. So I\u2019ve decided to declare myself a tech humanist.\n\nBecause what I\u2019ve realized is that data and technology in all their forms are becoming integrated ever more tightly into our lives and ever more powerful, to the point where the work of making technology successful for human use is inseparable from the work of making the world better for humans. I would even argue that the work of making better technology needs to be in lockstep with the work of being a better human.\n\nAnd no, I didn\u2019t grow up wanting to be a tech humanist. I mean, it\u2019s not like I read science fiction as a kid and thought someday I would think, write, and speak about the emerging impact of data and technology on human experience.\n\nI still don\u2019t read science fiction now as an adult, by the way, although I do see the connection between the work that I do and that genre\u2019s exploration of technology and culture.\n\nIt\u2019s just that I\u2019ve always preferred stories that explicitly examine human relationships. Because what interests me most is always people: we\u2019re such complicated systems of nerves and emotions and thoughts and impulses. \n\nWe\u2019re self-aware animals, pondering our own existence, conscious of our place in the universe. (Not always conscious enough, but still.)\n\nI do think technology is endlessly fascinating. But I\u2019m even more fascinated by humans and our endless complexity and capacity for nuance.\n\nWhich means when it comes to any aspect of technology, what I care most about are \n\nthe people who make the technology, \n\nthe people who use the technology, \n\nthe people who benefit from the technology and the people who suffer for the technology, \n\nthe people whose lives may somehow be changed by the technology.\n\nAnd it\u2019s not because we use technology. In other words, it isn\u2019t just the tools. \n\nRavens use tools. So why am I not, say, a tech ravenist?\n\nUnless we find out about other intelligent species with technology in the universe, humans are the best identifiable link between the dominant technology and the rest of organic life on this planet and beyond.\n\nSo our best hope for aligning the needs of all living things and all technological progress is in our own human enlightenment.\n\nWe need technological progress. It will surely bring us cures for disease, interplanetary and someday even intergalactic travel, safe and efficient energy, new forms and modes of communication, as well as so much else.\n\nBut for our own sake, and for the sake of humans who come after us, we need to wrap that progress around human advancement.\n\nAnd to do that, we need to foster our own enlightenment. We need a more sophisticated relationship with meaning and with what is truly meaningful, at every level: \n\nin communication, \n\nin pattern recognition, \n\nin our relationships, \n\nin our existence.\n\nTo develop technology in harmony with human advancement, we need to challenge our basest instincts and channel our best intentions. We need to genuinely want and be committed to creating the best futures for the most people.\n\nBecause the fact is we encode our biases into data, into algorithms, into technology as a whole. So as we develop an increasingly machine-driven future, we need to encode machines with the best of who we are. And in that way, infuse the future with our brightest hope, our most egalitarian views, our most evolved understandings.\n\nWe need to recognize the humanity in the data we mine for profit, to see that much of the time, analytics are people. That everything we do in the digitally and physically connected worlds creates a data trail. That who we project ourselves to be online \u2014 that self, that digital self\u2014is our aspirational self, liking things and connecting with other people and wandering through the digital world in awe, and our aspirational self, our digital self deserves due privacy and protection in every way.\n\nWe talk about \u201cdigital transformation\u201d in business. But let\u2019s be honest: most corporate environments are anything but transformative. So we need to begin to re-imagine and yes, transform business operations and culture around new models of infrastructure, new understandings of the social contract between employer and employee, and fundamentally new ideas of value.\n\nBecause our world is increasingly integrated: online and offline, at work and at play, and we have to be wholly integrated selves, too.\n\nAnd so we have to ask what the exchange of value means when it\u2019s about an integrated you in an integrated world.\n\nWe need to decide, for example, when we talk about autonomous cars: whose autonomy are we talking about? What are the broader implications of gaining freedom while losing control? Evolving from a society of private automobile ownership to privatized fleets of self-driving cars will give us back time, won\u2019t it? Or will it? And yes, it will mean life-changing possibilities for disabled and elderly people. If they can afford it. All in all, as anyone dependent on the New York City subway knows, if our mobility depends on machines we don\u2019t own and don\u2019t directly control, we are making a tradeoff. It may be a worthwhile tradeoff, it may even be an exciting tradeoff, but it is a tradeoff and we should ask meaningful questions about it.\n\nWe need to know that living in a culture with an ever-accelerating sense of time might mean having to resist an ever-narrowing horizon. That we have to try not to lose our sense of greater perspective in the FOMO frenzy. That our sense that experiences aren\u2019t real unless we share them and receive a few likes (or preferably a lot of likes) could cost us some peace of mind.\n\nWe need to begin to re-imagine our lives around new dimensions of meaningful experience.\n\nAnd ask ourselves: \n\nWhat different dynamics come into play when relationships are conducted across physical distances but connected by intimate virtual space, and what can make those relationships more meaningful. \n\nWhat fosters communities when they\u2019re multi-faceted network nodes, and not found mostly in houses of worship and town squares, and what will make those communities more meaningful. \n\nWhat \u201cwhat we do for a living\u201d will mean as jobs shift, as our understanding of contribution changes, and what will make that contribution more meaningful.\n\nBecause so much of the way we\u2019ve derived our identity, our sense of accomplishment, achievement, contribution, value, self-worth, is subject to radical overhaul in the next decade and the one following that and beyond. More jobs will be automated, augmented, enhanced, and yes, eliminated. And certainly new jobs will be created, but we can\u2019t wait for them to make sense of this. We have to begin re-imagining now what meaningful contribution looks like in that context.\n\nSo we need to ask what it means to be human when the characteristics we think of as uniquely ours can be done by machines. What is the value of humanity?\n\nAnd see, it\u2019s not that I\u2019m a human exceptionalist, exactly. I\u2019ve been vegan for 20 years, after all, which I point out to illustrate that I don\u2019t think rights are something only humans deserve. And eventually if I\u2019m around when machines become sentient, I\u2019ll probably care about AI rights and ethics, too. I can\u2019t help it: I\u2019m a sucker for equality.\n\nSo it\u2019s not that I think humans are so special that we deserve protecting and coddling, except that\u2026 just maybe we are, and just maybe we do.\n\nI just think that whatever it is \u2014 humanity \u2014 it\u2019s worth advocating for. \n\nWhatever combination of fascination and flaws it was that produced Shakespeare, Gloria Steinem, Paris, pizza, the Brooklyn Bridge, beer, Nelson Mandela, denim, Mary Tyler Moore, coffee, chocolate chip ice cream\u2026\n\nI could go on and on, but I don\u2019t even know if any of that is really the best of humanity, or even the best of what humanity has achieved. And what lies ahead of us are even greater challenges. So I don\u2019t know what the best of humanity has been and at some level I don\u2019t really care.\n\nI just think we have to be at our best now. And somehow striving for our best, somehow making something lasting, and most of all working to make the best future for the most people \u2014 I think that is the best of what humanity can be and has to be.\n\nAnd we need to start making it our mission to give it, to be it, to encode it, to build it in our culture, in our data models, our work environments, our relationships, and all throughout the technology that is interwoven in our lives. It\u2019s not science fiction; the future really does depend on it.\n\nThank you for reading. Please \u201cclap\u201d if you found this piece interesting or meaningful. And please feel free to share widely.\n\nKate O\u2019Neill, founder of KO Insights, is an speaker, author, and \u201ctech humanist,\u201d making technology better for business and for humans. Her work explores digital transformation from a human-centric approach, as well as how data and technology are shaping the future of meaningful human experiences. Her latest book is Pixels and Place: Connecting Human Experience Across Digital and Physical Spaces."
    },
    {
        "url": "https://medium.com/intuitionmachine/chatbots-theory-and-practice-3274f7e6d648",
        "title": "Chatbots: Theory and Practice \u2013 Intuition Machine \u2013",
        "text": "There\u2019s a lot of fluff surrounding chatbots, so I wrote this post to lay out the basics. I first review the theory of conversation to give us a sense of what we are aiming for. I then discuss three classes of chatbots. The simplest class is purposeless mimicry agents, which only provide the illusion of conversation. Members of this class include ELIZA and chatbots based on deep learning sequence-to-sequence models. The second and next most sophisticated class comprises intention-based agents such as Amazon\u2019s Alexa and Apple\u2019s Siri. These agents have a simple understanding and can do real stuff, but they generally can\u2019t have multi-turn conversations. The third and most sophisticated class is conversational agents that can keep track of what has been said in the conversation and can switch topics when the human user desires.\n\nConversation begins with shared reference. We point to objects in the world so that we and our conversational partner know that we are talking about the same things.\n\nChildren use pointing to communicate desire for objects and shared interaction before they can speak. As we learn language, words then point to shared ideas in our minds (G\u00e4rdenfors, 2014).\n\nWhat things words point to becomes a shared convention over time through language games, as described by Wittgenstein in Philosophical Investigations (1958). Language games are interactions that shed light on a particular aspect of language. Through cooperation in shared action, such as building a hut, we settle on labels for objects. When we ask for a \u201cbeam\u201d we nod approvingly when someone finally brings us the correct item.\n\nOf course, language is more than labels. Our brains map the concepts behind these community conventions to personal sensations and actions. When someone says \u201cbeam,\u201d we associate that with our experience with beams, and since people generally have similar experiences, we are able to understand each other. When someone says they hurt their back carrying a beam, we understand because we too have carried heavy things. The meaning is grounded in that experience (see Benjamin Bergen, Steven Pinker, Mark Johnson, Jerome Feldman, and Murray Shanahan).\n\nWhen our experiences and language games don\u2019t sufficiently overlap, we must negotiate meaning in the course of a conversation. Consider the image of the discourse pyramid below. When everything is understood, we are at the bottom level and can just give and receive instructions for cooperation. When something is not understood, there is a break and we have to do a coordination of inner worlds, and if that isn\u2019t understood we have to do a coordination of meaning. Once we have acknowledgment of understanding at a particular level, we can go back down to the previous level. For example, we ask a guy to get us some fish. He doesn\u2019t understand how, so we have to coordinate our inner worlds and explain that he can catch fish with a fishing pole. When he doesn\u2019t even understand what a fishing pole is, we have to coordinate meanings and explain that a fishing pole is a tool that consists of a stick, string, and a hook.\n\nIn addition to the meanings of words and sentences, conversation itself has its own rules. Consider Grice\u2019s (1975, 1978) conversational maxims:\n\nMaxim of Quantity: Say only what is not implied. Yes: \u201cBring me the block.\u201d No: \u201cBring me the block by transporting it to my location.\u201d\n\nMaxim of Quality: Say only things that are true. Yes: \u201cI hate carrying blocks.\u201d No: \u201cI love carrying blocks, especially when they are covered in fire ants.\u201d\n\nMaxim of Relevance: Say only things that matter. Yes: \u201cBring me the block.\u201d No: \u201cBring me the block and birds sing.\u201d\n\nMaxim of Manner: Speak in a way that can be easily understood. Yes: \u201cBring me the block.\u201d No: \u201cUse personal physical force to levitate the block and transport it to me.\u201d\n\nBreaking these rules is a way to communicate more than the meaning of the words. Wikipedia has a nice summary here. When we break a maxim, it is assumed that it is for some purpose. For instance, when I say \u201cI love carrying blocks, especially when they are covered in fire ants,\u201d I am breaking the maxim of quality to use sarcasm to communicate that I don\u2019t like carrying these blocks. As another example, if someone asks me how good Bob was as an employee and I respond that he had nice hair, I am communicating an idea by breaking the maxim of relevance.\n\nSystems for converting speech-to-text have recently gotten pretty good, for example here and here, and we will largely ignore that aspect in this post, but sometimes meaning isn\u2019t even in the text but in how the words are spoken (called prosody). Consider the following three examples taken from Voice User Interface Design by Giangola and Balogh (2004). The italicized words are emphasized. Based on the emphasis, each example means something different (stated afterwards).\n\nConversation is really hard. Yuval Noah Harari argues in his book Homo Deus that our ability to cooperate is what has allowed humans to take over the planet. Being able to converse with machines will allow us to further cooperate with this new kind of intelligence we have created. Let\u2019s now turn our attention to the state of the art.\n\nWe will look at the three classes of chatbots in turn: purposeless mimicry agents, intention-based agents, and conversational agents.\n\nPurposeless mimicry agents give the appearance of conversation without understanding what is being said. We\u2019ve all heard of ELIZA. ELIZA consisted of simple substitution rules to mimic a psychologist from the 1960s. This is the psychology that emerged after behaviorism and is often characterized as having the therapist repeat back the words of the patient. If the patient said, \u201cMy mother wants me to buy a bazooka.\u201d The therapist might respond, \u201cTell me why your mother wants you to buy a bazooka.\u201d ELIZA could have this type of conversation. You can find a Python implementation of ELIZA here.\n\nModern mimicry agents use deep learning to learn from example conversations. They train on a bunch of dialogs and learn to generate the next statement given the last statement. There are lots of dialog datasets out there. You can use examples of dialog from movie and TV subtitles, such as OpenSubtitles. One can also use the Ubuntu Dialog Corpus, which is dialogs of people wanting technical support. Or one can mine Twitter and look for replies to tweets using the API.\n\nThe deep learning method used for this parroting is usually sequence-to-sequence models. A sequence-to-sequence model consists of an encoder and a decoder. The encoder converts a sequence of tokens, such as a sentence consisting of word tokens, into a single vector. The decoder begins with this vector, and it keeps generating tokens until it generates a special stop symbol. Note that the lengths of the source and target sequences don\u2019t need to be the same. Both the encoding and decoding are done using recurrent neural networks (RNNs).\n\nThe initial big application for sequence-to-sequence models was language translation (Cho et al., 2014). For example, one would have a set of source sentences in English and their corresponding translations into Spanish. The model would then learn to encode the source sentence in English into a vector and then to decode that vector into the corresponding sentence in Spanish. To generate conversations, sequence-to-sequence models treat a statement as the source language, such as \u201cHow are you?\u201d and they treat the response as the target language, such as \u201cI am great!\u201d An example is shown below.\n\nThe problem with sequence-to-sequence models is that they are devoid of meaning. They can work for just about any kind of problem that can be cast as translating one sequence into another, but they don\u2019t make use of any special machinery or knowledge for language understanding. Another problem with using sequence-to-sequence models for chatbots (and using deep learning in general for chatbots) is that they are insensitive to specifics. If I ask it to buy me 14 chickens, it doesn\u2019t treat the number 14 as particularly special, and it might be just as likely to buy me 140.\n\nThe most recent sequence-to-sequence chatbots use generative adversarial networks (GANs). GANs have a discriminator that tries to tell if the generated response was from a real conversation or generated by the model. GANs are all the rage in image processing, but they are still a work in progress for language.\n\nIntention-based agents understand language as commands, and they use that understanding to perform tasks in the world. Examples of intention-based agents include Amazon\u2019s Alexa, Google Home, Apple\u2019s Siri, and Microsoft\u2019s Cortana. Understanding what we say as a command language requires solving two problems:\n\nFor example, if we ask our assistant to play Steve Winwood, the assistant first needs to understand that we want it to play music (the intent), and then it must understand that, in particular, we want to hear Steve Winwood (the details).\n\nConsider the example below about determining intent. When the user asks about chickens, the machine has to figure out which of its four capabilities matches the intent of the person.\n\nThe assistant can determine the intent using either keywords or text-based classification. To use keywords, you simply associate words and phrases with intents. To do text-based classification, you label a bunch of statements with the correct intents and then train a classifier over them. You can train a classifier using a bag-of-words representation with the Python library scikit-learn, as described here. If you have lots of labeled data, another option for learning intents is to use deep learning with a convolutional neural network (CNN) in TensorFlow. An implementation can be found here.\n\nOnce the agent has determined the intent, in our example it is to order groceries, it needs to convert the statement details into a machine-readable form, such as a Python dictionary, as shown below.\n\nHaving the command in this kind of dictionary form is called a frame and slot semantics, where the frame is the topic and the slots are the individual features and values. Once we have the machine readable form, the computer can do with it what it wants, just like any other instruction. This is natural language understanding.\n\nOne way to do natural language understanding is to use context-free grammars (compositional semantics). Context-free grammars consist of production rules that have a single nonterminal on the left and a string on the right side that can consist of terminals and non-terminals. To each of these production rules, we add a semantic attachment. An example set of rules is shown below.\n\nNonterminals begin with \u2018$\u2019. The first rule says that an order consists of an intent to purchase and an item and amount of that item. The semantics of that rule are combined via a dictionary union of the semantics of $Purchase (s[0]) and the semantics of $ItemAmount (s[1]). The next rule says that an intent to purchase is determined by the function is_purchase, as shown below. (This, of course, is a compact way to describe a set of rules. I.e., one with \u201cget me\u201d on the right-hand side, and another with \u201cbuy\u201d, and another with \u201cgrab me some.\u201d)\n\nThe next rule says that $ItemAmount consists of an $Amount and an $Item, which are defined in the following two rules. Those two rules make use of the functions get_item and get_number, shown in example form below.\n\nEach of those rules has an associated semantics at the end. We use parsing to apply those production rules to text to get semantics. Parsing is done using bottom-up dynamic programming. Dynamic programming consists of solving subproblems and then reusing those solutions over and over again. The parsing process builds up the parse by looking at pairs of words, and for each pair, it loops over all n possible words in between. Because the parsing algorithm deals with pairs of sub-parses, the production rules above each have either a single terminal or two non-terminals on the right-hand side. If your grammar is not like this (but still has a single nonterminal on the left-hand side of each production rule) this is not a problem; you can always convert a context-free grammar to this form (called Chomsky Normal Form).\n\nThe figure below shows the order the parsing algorithm follows as it considers pairs of words: from \u2018Get\u2019 to \u2018Get\u2019, then from \u2018me\u2019 to \u2018me\u2019, then from \u2018Get\u2019 to \u2018me\u2019, then from \u2018fourteen\u2019 to \u2018fourteen\u2019, and so on.\n\nThe next two figures show the production rules invoked and the semantics at each time point, respectively. For example, at box 8 we are looking at the words from \u2018fourteen\u2019 to \u2018chickens.\u2019 This corresponds to the rule with $ItemAmount on the left-hand side, which comes from box 4 (the parse on \u2018fourteen\u2019) and box 7 (the parse on \u2018chickens\u2019). Box 9 brings together box 8 and box 3 to invoke the rule with $Order on the left-hand side.\n\nOf course, the parser also tries many other combinations that don\u2019t fit any of the rules. In general, if there are n words in the sentence and R production rules in the grammar, it tries n*n boxes and looks at n boxes in between for each, trying to apply each of R rules. In total, the running time is on the order of n*n*n*R. Often, there can be many valid parses, and a scoring algorithm can be used to choose which one to use.\n\nThere is code available in Python for creating grammars and running the parser as part of the SippyCup library.\n\nConversational agents expand on intention-based agents to have multi-turn conversations. To do this, they must keep track of the state of the conversation and know when the person wants to talk about something else.\n\nConversational agents aren\u2019t yet common but imagine an agent like the one I described here. The agent would stay with a person for their whole life. It would start out as a companion when the person was a child, where it would live as a cell-phone app on the parent\u2019s device. It would have a cartoon face, and it would learn about the child and teach her about the world. For example, if the agent knew that the child loved giraffes, it could use that to teach her math. \u201cIf you have 7 giraffes and bought 3 more, how many would you have?\u201d It could also help with cognitive biases and over-discounting the future. \u201cIf you eat your chocolate bear today, how are you going to feel tomorrow when you no longer have it?\u201d\n\nThe developers of the app would be working furiously behind the scenes, and the agent would become more sophisticated as the child grew older. When the child became an adult, the agent would become her operating system. It could provide turn-by-turn directions in life, such as guiding her through how to fix a sprinkler system. The agent could then serve as her assistant when she got old. Imagine that she is standing in the kitchen and can\u2019t remember how to make coffee. The app could use cameras in her home and could guide her through the process, allowing her to live independently longer.\n\nConversational agents such as this one need a dialog manager to handle long conversations. A dialog manager must take the human through all of the things the chatbot wants to talk about. For example, it could first want to teach the child math skills, and then it could want to talk about the solar system. The dialog manager also needs to recognize when she wants to talk about something else and queue the current topics for later.\n\nRavenClaw from CMU is probably the best-known dialog manager (see Bohus and Rudnicky, 2009). RavenClaw consists of dialog agents, which are little programs organized hierarchically that correspond to different bits of conversation. For example, there could be a general dialog agent about cooking that wants to discuss multiple things related to preparing food.\n\nRavenClaw has a dialog stack and an expectation agenda. The dialog stack is a stack of dialog agents to keep track of all the things the chatbot wants to talk about. The expectation agenda is a data structure to keep track of what the chatbot expects to hear. For example, if the child says \u201cyes\u201d she probably isn\u2019t answering a question from thirty minutes ago like Rain Man; the answer probably corresponds to the last question asked. But if she says, \u201cI remember! Saturn is the one with rings!\u201d she may be talking about a previous topic, and the expectation manager needs to match the utterance with all of the expectations to find the right one.\n\nDuring dialog execution, RavenClaw alternates between an execution phase and an input phase. During the execution phase, it invokes the top dialog agent on the dialog stack and lets it talk. It also sets up what the machine expects to hear in the expectation agenda. During the input phase, RavenClaw processes what the person said and updates its knowledge, which it uses to determine which dialog agent can respond during the execution phase.\n\nConsider the example below with a dialog stack and an expectation agenda.\n\nThe chatbot is expecting to hear an answer to the math problem, which is why \u201c9\u201d is at the top of the expectation agenda. The child instead says something completely different, so the algorithm searches the expectation agenda for what she might be talking about. It finds that she is talking about toys. The chatbot then could move talking about toys to the top of the dialog stack, so it could generate an appropriate response, such as \u201cVery nice. Is he your favorite?\u201d The chatbot would also modify the expectation agenda to match that it expects to hear an answer to this new question.\n\nRavenClaw isn\u2019t based on machine learning, but we can alternatively use a learning-based approach to build conversational agents. We can treat creating a conversational agent as a reinforcement learning (RL) problem (Scheffler and Young, 2002). Reinforcement learning problems take place within a Markov decision process (MDP) consisting of a set of states, actions, and a reward function that provides a reward for being in a state s and taking an action a.\n\nIn the context of chatbots, states are things such as what the bot knows (questions it has answered), the last thing the bot said, and the last thing the user said. Actions are making particular statements, and reward comes from meeting a goal state, such as the child giving the correct answer to a math problem or, in a different domain, successfully completing a travel reservation.\n\nWe can use reinforcement learning to learn a policy that gives the best action a for being in state s. As one can imagine, learning a policy requires a lot of training, and it is difficult to pay people to sit and talk with the agent to generate enough experience, so researchers train their agents with simulated experience (Scheffler and Young, 2002).\n\nThere is an additional problem with using learning for chatbots; it is hard to know exactly what state the agent is in because of errors in speech-to-text or errors in understanding. For example, does the child want to talk about her toys, or is she telling the agent that it should be fluffier? This uncertainty calls for a Partially Observable Markov Decision Process (POMDP) (Young et al., 2013). In a POMDP the agent doesn\u2019t know what state it is in and instead works with a distribution over the states it could be in. These types of agents are still experimental.\n\nLet\u2019s compare the current state of chatbots with our theory. We started with shared reference, which is making sure we are talking about the same thing. Shared reference is pretty easy to implement as long as we limit the discussion to a set of known things. Likewise, the shared conventions for word meanings can be programmed into intention-based agents or conversational agents. We then hit a wall.\n\nThe meanings that we code in are not mapped to a grounded sense of sensation or action in the agent. When we refer to an object, the agent has never held the object or used it like we would in the real world. This means that its understanding will be limited. I mentioned previously the idea of someone hurting their back carrying a heavy beam. A computer\u2019s possible understanding of this scenario is limited to logical inference, which leads to two disadvantages. The first is that logical inference is surprisingly hard. Inference itself is well understood, but it is hard to set up the rules to make all relevant inferences. The second disadvantage is that the agent\u2019s understanding will be at a low resolution. A computer can only understand variables populated with values \u2014 it can\u2019t feel the pain. I wrote a post about how we could tackle this problem by having agents simulate experience in our world, but we have a long way to in this area.\n\nMore bad news is that the meanings that chatbots have are largely fixed, and so we currently can\u2019t negotiate meanings with them in the discourse pyramid. There has been research into pragmatics, such as Grice\u2019s maxims, but I don\u2019t know of any work ready to be implemented on a general-purpose agent. The same is true for prosody. We have a long way to go.\n\nIn summary, the current state of the art is that we can build knowledge into chatbots that they can use to cooperate with us on tasks requiring minimal understanding. Building better chatbots will necessitate knowledge engineering and research into how agents can better follow and adapt to the subtleties of meaning and conversation."
    },
    {
        "url": "https://medium.com/intuitionmachine/building-a-50-teraflops-amd-vega-deep-learning-box-for-under-3k-ebdd60d4a93c",
        "title": "Building a 50 Teraflops AMD Vega Deep Learning Box for Under $3K",
        "text": "In 2002, the fastest supercomputer in the world (i.e. \u201cNEC Earth Simulator) was capable of 35.86 teraflops.\n\nYou can now get that kind of computational power (that can fit under your desk) for the price of a first class airline ticket to Europe.\n\nNow that AMD has released a new breed of CPU (i.e. Ryzen) and GPU (i.e. Vega) it is high-time that somebody conjure up with an article that shows how to build an Deep Learning box using mostly AMD components. The box that we are assembling will be capable of 50 teraflops (fp16 or half precision).\n\nHere are the parts we gathered together to put together this little monster. I purchased them through Newegg and you can click on the links for the specific parts.\n\nMy mistake, I went over budget! It would have been under $3,000 if I didn\u2019t need either the SSD or the HDD. But if you want to pay a little more for additional hardware then I would add more CPU memory (up to 64GB) and to use an NVMe based SSD card. That would add an additional $600 to the total. For comparison with a desktop solution, Nvidia\u2019s pre-assembled \u201cDIGITS Devbox\u201d with 4 Titan X GPUs and 64GB ram goes for $15,000.\n\nOne other perspective is to compare it with a single Nvidia Tesla P100 (just the GPU card) with 16GB and capable of 18.7 teraflops (fp16) but costs $12,599 from Dell. If you are thinking of side-stepping \u2018professional grade\u2019 to a consumer grade Nvidia Titan X or GTX 1080 ti (like the DIGITS Devbox) then it is worth knowing that half-precision (fp16) is only at 0.17 teraflops for a Titan X and comparable for a GTX 1080 ti. That\u2019s because fp16 as well as fp64 (double precision) is unavailable for Nvidia consumer cards:\n\nClearly you are getting a ridiculous amount of fp16 compute for the buck with a Vega solution! Deep Learning does not require high precision unlike conventional scientific computation. So we can ignore double-precision floating point performance numbers when we select Deep Learning hardware. Nvdia charges premium for their Tesla double precision capability. Scientific computing absolutely required double precision and thus paid premium for this capability. However with the discovery that Deep Learning workloads don\u2019t need high precision and favor a lot more computation, Nvidia is now also charging premium for half-precision! Kind of like giving you half the cake for twice the price. That\u2019s the benefits of being a virtual monopoly!\n\nPerhaps AMD Vega can take a piece of the action with better fp16 performance:\n\nWe can also make comparisons with respect to cloud based GPUs. A p2.xlarge is priced today at $0.9 per Hour using K80 GPUs. A p2.xlarge specifications is at 2,496 CUDA cores with 12GB of memory. If we do the math this appears to be at 4.3 teraflops per GPU instance (single precision). That is 5.8 times less than this AMD box we are assembling. In terms of money, you will spend at a maximum 24 days of AWS GPU time to cover the cost of this AMD box! (To be fair, we are comparing oranges with oranges, that is numbers are on single precision compute).\n\nThere is an question about Vega\u2019s double precision performance. The MI25 documentation shows it at 768 gigaflops. In comparison, the latest Xeon Phi (Knights Landing) is at 3.46 teraflops double precision and the Nvidia P100 is at 4.7 teraflops. Price analysis by Microway for Nvidia P100 solutions are in the range of $1,500 per teraflop. So clearly, the Vega is optimized for fp32 and fp16 workloads, implying that it indeed is designed to be a Deep Learning engine and not a more traditional scientific computational engine.\n\nThere are of course a lot of other considerations that need to be taken into account here. The cost of assembly, installing, networking, power and maintenance that we did not add. Also, there are considerations to be made if the need arises in distributing work across more than one machine (which its always best to avoid). Finally there is the issue of Deep Learning frameworks that are compatible with the AMD box. Still, it is hard to ignore the massive potential cost savings of this approach.\n\nThe AMD Ryzen CPU has an interesting feature in that it can handle 24 PCIe lanes, in comparison an Intel desktop CPU has only 16 lanes. This is an important consideration in that you need to be able to adequately feed the attached GPUs with data fast enough while it is training. Each GPU can occupy 16 lanes, so perhaps a more optimal solution is to wait for AMD\u2019s Threadripper ( i.e. on store shelves in early August ) that supports 64 lanes! Threadripper is definitely something to check out with a box with four Vega GPUs (meaning 100 teraflops). I likely will like explore this new CPU to see how it performs with multiple GPU and NVMe devices. In addition, Vega is uniquely designed memory architecture that lets it access 512TB of memory. Coupling Vega to NVMe devices may lead to unprecedented performance improvements.\n\nOkay, let\u2019s get assembling! Here are the unboxed parts:\n\nAll one needs is a screwdriver and mounting screws. With about the same amount of brain power and time to assemble Ikea furniture and you have a final assembly that\u2019ll look like this:\n\nIt isn\u2019t one of those gorgeous water cooled and color coded assemblies, but this should be more than adequate. (Note: AMD sells a water-cooled version of Vega). To my surprise, the LEDs were horribly color uncoordinated. I have white, red, blue and yellow. I guess you will have to pay premium for style.\n\nTo do this, go to www.ubuntu.com and download an iso image that you will \u2018flash\u2019 into a flash drive. You can do this all using Unetbootin. Once done downloading and flashing, insert the flash drive into the USB port of the new desktop and it\u2019ll boot up Ubuntu and guides you towards installing the operating system on to the attached HDD or SSD.\n\nOnce Ubuntu is installed, you now need install drivers and software for the Vega cards. You can find instructions to that here: installing ROCm. (Note: ROCm updates the kernel, so there is a current code synchronization issue that requires a 4.10 kernel for ROCm)\n\nYou will only need to install the AMD ROCm Kernel Fusion drivers. Everything else though sits in userspace and is most conveniently set up using Docker images. Hopefully, we can build a richer set of Docker images so that will be more convenient to run Deep Learning experiments.\n\nTo install Docker, follow the instructions found here: (How to install Docker in Ubuntu 16.04). The Docker daemon may have trouble starting, if this is the case, try using Overlay2 storage driver.\n\nOnce you have set up Docker, you can now start exploring Deep Learning on AMD hardware. The Docker image we have, we have made public at Docker Hub (https://hub.docker.com/r/intuitionfabric/hip-caffe/). This we hope will allow you to start experimenting quickly. So by typing the following command line (apologies for how Medium formats this):\n\nyou can use it to run Caffe examples (see: https://github.com/ROCmSoftwarePlatform/hipCaffe ). So as an example, you can run the CIFAR example:\n\nHere is the a snapshot of the training:\n\nSo let\u2019s get some numbers to see how well this box performs!\n\nI ran the benchmarks found here: https://github.com/soumith/convnet-benchmarks and here is a chart of the results:\n\nI don\u2019t know the specific hardware that was used in these benchmarks, however this comparison does show that the performance improvement is quite significant as compared to alternatives. One thing to observe is that the speedup is most impressive with a complex network like GoogleNet as compared to simpler one like VGG. This is a reflection of the amount of hand-tuning that AMD done on the MIOpen library.\n\nAMD still needs a lot of catching up to do in the Deep Learning software front. The ROCm software stack is still a work in progress. However it is indeed clear (for folks on a budget) that there are opportunities here that can be exploited. If AMD can get more aggressive with porting frameworks particularly with an emphasis on fp16, then this could be a major win for in competitiveness of this alternative platform. Not many Deep Learning frameworks support fp16, but if you consider the 2x speedup benefits then there is real incentive to occupy this vacuum.\n\nFor researchers who perform experiments on low precision arithmetic or compressed networks, this is an opportunity to get a fp16 enabled system at an extremely low cost. AMD\u2019s Vega solution appeals to the idea that emergent intelligence arises from having more simple computations performed at a massive scale. The fp16 (also the int8 capability) and the large 16GB of HBM2 memory can be an advantage for certain kinds of innovative architectures.\n\nConsider also that Vega can directly access NVMe devices and can address a 512TB of memory through is new memory architecture. This can be huge in the context of the newer 3D XPoint technology that Micron and Intel have created. Vega GPUs that can access terabytes of memory, this is unique for GPU devices and has the potential to change the entire dynamics of Deep Learning architectures.\n\nLooking forward in the future, Nvidia\u2019s Volta architecture has specialized Deep Learning GPUS (see: Google\u2019s AI Processor\u2019s (TPU) Heart Throbbing Inspiration) that is nowhere in AMD\u2019s roadmap. Systolic arrays can take up expensive real-estate and will likely be available for Deep Learning specialized hardware like Google\u2019s TPU and Fujistu\u2019s DLU. It is unlikely though that these capabilities will ever be available to Nvidia consumer graphics cards. Intel has yet to deliver any Deep Learning hardware so its anybody\u2019s guess as to how they can shake up the market.\n\nTherefore, it may be safe to assume that, at a hobbyist level, AMD GPUs can become an extremely cost-effective Deep Learning hardware for the next couple of years. AMD needs to do a lot of work to get Deep Learning software ready for this platform, however there are plenty of opportunities to try unique alternative approaches. One extremely intriguing capability of AMD Vega is that the compute engine can do 512 8-bit operations per clock! What that means is that if you can perform 50 trillion operations per second on a single Vega card. Think of the possibilities here in the area of genetic (aka evolution) algorithms.\n\nOne thing to always remember, Deep Learning is an experimental science and the more folks that have 50 teraflops (or 100 teraops) of computing power underneath their desks, the higher the likelihood that we make some accidental impressive discoveries!\n\nFor those readers who aren\u2019t familiar with Deep Learning, read:\n\nBTW, \u2661 if you agree that Deep Learning needs alternative hardware vendors!\n\nUpdate: A new TVM backend has been created that supports NNVM and ONNX supported frameworks."
    },
    {
        "url": "https://medium.com/intuitionmachine/monetizing-artificial-intelligence-d69fe9175785",
        "title": "Monetizing Artificial Intelligence \u2013 Intuition Machine \u2013",
        "text": "Turn your AI software projects into an ATM machine. Freelance workers enable the automation of their work \u2014 ironically.\n\nAGI (artificial general intelligence) is the pursuit of machine cognition, largely still a work in progress. Pat Langley from Arizona State University, has an excellent essay highlighting the differences between most of what\u2019s labeled \u201cAI\u201d, and what he refers to as the \u2018Cognitive Systems Paradigm\u2019.\n\nA cognitive system has the machinery to begin working with written language, use heuristics and other approaches to deal with incomplete data, make inferences from structured representations of information, and so on.\n\nSome current AGI frameworks can productively be part of this type of work, although unfortunately many projects are not yet well documented or openly available. AGI entrepreneurs Peter Voss and Monica Anderson have recently met with me to discuss their respective work.\n\nWithout an AGI architecture, there is \u2018narrow AI\u2019 code that can be coupled with APIs and screen-scraping tools to automate specific tasks. Human workers can oversee and lend a hand where software remains incapable.\n\nWhile such an ensemble won\u2019t lead to a cognitive system, it is how most current \u2018virtual agents\u2019 are built today.\n\nWhether you are working to develop and AGI architecture (an integrated cognitive system) or have pieced-together a virtual agent to perform specific tasks, the place to start earning $$$ is with artificial-artificial intelligence.\n\nThe key to generating steady revenues from early-stage \u2018proto-AGI\u2019 systems and automated \u2018agents\u2019 is to understand the limitations of Turk \u2014 Amazon\u2019s Mechanical Turk.\n\nYou see \u2014 \u2018Turk\u2019 is a marketplace where you can pay for repeatable self-contained tasks to be completed, by people (\u2018workers\u2019) bidding on them. \u2018HIT\u2019 human intelligence tasks as they are called.\n\nNeed to tag 1,000 pictures to create training data for an image recognition algorithm? Turk can get that done, it might cost $0.15/image, depending on the bid interest.\n\nOnce a bid is accepted, the tasks are completed and the workers get paid. There\u2019s a rating system to keep everyone honest. You can imagine this as \u2018artificial-artificial intelligence\u2019, because human beings are performing tasks that machines (currently) cannot.\n\nMechanical Turk assumes the tasks don\u2019t require credentials or any significant training. It assumes each task is self-contained. Task instructions need to be intuitive: nobody will bid unless the tasks can be readily understood up-front. You cannot assign Turk workers tasks that span multiple phases with dependencies or tasks for which credentials are needed (eg. logging into an app). You don\u2019t typically correspond with Turk workers during the work unless there\u2019s an issue preventing it from being done. It is \u2018bare-metal\u2019.\n\nThis higher abstraction level allows for freelance work (rather than just individual tasks) to be performed.\n\nIf Turk is a disk drive, what we need is the operating system. A higher abstraction level that systematizes work so that machines (software) can perform parts of it.\n\nThe first step toward $ is to find freelance work that is sufficiently narrow to be augmented by software, and be relatively abundant. We don\u2019t want to bid on niche projects, rather ones that are sought-after and repeatable.\n\nThe top freelance worker marketplace is Upwork (formerly oDesk). Many have used it to hire freelance developers and QA and it\u2019s fairly impressive.\n\nHere\u2019s a list of freelance \u201cvirtual assistants\u201d job postings (sorted by the amount of money the \u2018client\u2019 has spent to date in the marketplace).\n\nNote: you probably need a [free] login to see this list properly.\n\n>4,100 job postings, with the following breakdown\n\nFiltering for \u201cEntry Level\u201d and at least $100/week yields 1,906 job postings\n\nFully categorizing nearly 2-thousand postings would take some time, but a few clusters are immediately obvious. There are assistants to teams and \u2018executives\u2019 that are likely too broadly defined to be opportunistic. There are marketing jobs that require creative writing skills, again let\u2019s ignore these. There are jobs that require making calls and speaking to people, let\u2019s ignore these as well. And there are jobs that require foreign language skills. We need freelance work that is relatively straight-forward.\n\nHere are a few examples:\n\nNotice the amount of $ spent by these clients on UpWork to-date. Note also that many jobs specify the expected throughput (eg. \u201950 emails per day\u2019).\n\nEmail handling is an example of multi-phased work that requires credentials. There is an email account, there are multiple templates, spreadsheets to keep track of things, etc. It\u2019s laborious but highly repetitive and it allows for tasks to be completed by a system with a limited cognitive range of motion.\n\nThis is a crucial concept to understand.\n\nWe all readily understand physical range of motion. You might run a reasonable pace for a 5k but no amount of training will get you to Olympic times. Your range of motion as a runner has a limit.\n\nSimilarly you might be rated 1,350 in chess tournament play but no amount of playing or tutoring will get you to a chess master rating. That would require a combination of cognitive skills including the ability to remember a larger number of positions, the ability to quickly run through more scenarios for different moves, etc.\n\nSo if we understand the cognitive range of motion of our AGI or the capabilities of a software agent, we can find freelance work that matches. It\u2019s ok for it to require some help or supervision \u2014 training wheels.\n\nThe next step is to bid on common freelance work with the right range of motion and hire a freelance [human] worker to do it. You can hire the freelancer from the same marketplace and have the work \u2018pass through\u2019 your company. The freelancer must be willing to \u2018collaborate\u2019 with your software system and help evolve it.\n\nThere\u2019s no need to mark up the price \u2014 we\u2019re going to be driving revenue margin by using your system to reduce the work effort.\n\nThe point of artificial intelligence is to perform knowledge work. To earn revenues, it will be used to reduce the amount of time the human freelance worker spends on the work. As the automation system does this work you will better understand how to improve it, and subsequently have it do a larger percentage of the knowledge work \u2014 the effect is circular.\n\nYour revenue equation in this setup is as follows:\n\n(I) Inbound revenue: the $/hour paid to you by the freelance customer\n\n(F) Freelance cost: the $/hour paid by you for the human freelance worker\n\nexample: if your automated system can handle half of the work, on average, and the work pays/costs $15/hour, your revenue per hour will average:\n\nBecause the automation can do certain tasks 24x7 the equation may not fully represent the leverage it provides.\n\nFor email handling, as one example, your system should to be able to (with some assistance):\n\nMuch of this is utilitarian, much of it requires a combination of utilities, some of it requires some basic cognitive skills.\n\nOnce this is put in motion you can bid on additional similar freelance work. The more similar it is, the more applicable. You should be able to multi-task the human freelance worker as the percentage of software agent contribution increases.\n\nBesides $ margin, the value of such a system is knowledge work experience. In the case of a cognitive system, it can be trained, learn, adapt, etc. to perform its part of the work with greater ability. Instead of \u2018training data\u2019 we need repeated \u2018training work\u2019, with human oversight.\n\nIt\u2019s possible that over time, certain freelance jobs will be handled entirely by AGIs. The near/mid-term goal is not to achieve self-sufficiency but rather to achieve a high-degree of repeatable success in the tasks.\n\nThe paradox of AGI development is in the term \u2018General\u2019: a system must be of practical use long before it can be self-sufficient in any general way. AGI developers seeking a \u2018singularity\u2019 stage of cognition will likely go broke before getting there. It will take considerable time to achieve far-reaching cognitive skills in software, what\u2019s needed in the meantime is a place to exercise and evolve these skills.\n\nA setup where an AGI can evolve while doing real work and earning money. Crawl before you walk.\n\nMight as well get paid to do it."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-strange-loop-in-deep-learning-38aa7caf6d7d",
        "title": "The Strange Loop in Deep Learning \u2013 Intuition Machine \u2013",
        "text": "Douglas Hofstadter in his book \u201cI am a Strange Loop\u201d coined this idea:\n\nWhere he describes this self-referential mechanism as what describes the unique property of minds. The strange loop is a cyclic system that traverses several layers in a hierarchy. By moving through this cycle one finds oneself where one originally started.\n\nCoincidentally enough, this \u2018strange loop\u2019 is in fact is the fundamental reason for what Yann LeCun describes as \u201cthe coolest idea in machine learning in the last twenty years.\u201d\n\nLoops are not typical in Deep Learning systems. These systems have conventionally been composed of acyclic graphs of computation layers. However, as we are all now beginning to discover, the employment of \u2018feedback loops\u2019 are creating one of the most mind-boggling new capabilities for automation. This is not hyperbole, this is happening today where researchers are training \u2018narrow\u2019 intelligence systems to create very capable specialist automation that surpass human capabilities.\n\nNote: Original text migrated to the book. For more on this in this \u201cstrange loop\u201d please consult:\n\n\u2661 Please heart if you like this!"
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-and-systematic-intelligence-569daff80efe",
        "title": "Defensible Deep Learning Ventures \u2013 Intuition Machine \u2013",
        "text": "Venture capitalists (VCs) have a new term \u201cSystematic Intelligence\u201d that is worth paying attention to. Gil Dibner wrote an excellent article (\u201cSystems of Intelligence: Is this the VC meta-thesis we\u2019ve been looking for?\u201d ) that breaks down the current thinking on defensible businesses. This is an important matter for ventures that focus on Deep Learning. Jeff Bezos once remarked \u201cInvention is not disruptive, only customer adoption is. At Amazon, we\u2019ve invented many things, most were not at all disruptive!\u201d So, it is not enough to invent, one needs to be able to be disruptive. However, disruption is not enough, one needs businesses that pass the test of time. That is, businesses that are defensible.\n\nI\u2019ve discussed this earlier about the need to focus on a platformization strategy. That is, we would like to build businesses that avoid being disrupted. To do so, we need to build platforms that are able to leverage networking effects. There are four kinds of platforms; these are marketplaces, social networks, supply networks and creation networks. However, as Gil Dibner argues, the opportunities to discover new kinds of platforms are dwindling and therefore we need to discover new ways for building defensible businesses. Please read Dibner\u2019s article before proceeding. His article sketches out the line of thinking that leads to the current situation.\n\nDibner enumerates several true barriers of entry that a Deep Learning venture needs to attempt to erect:\n\nThere many businesses that require unique knowledge and expertise to perform competitively. These are hard to find because businesses will keep this information very close to their chest. However, finding these businesses and enhancing their processes with advanced Deep Learning methods can be a route to something that will continue to be defensible.\n\nThere are many systems that are extremely complex where there are a multitude of edge cases that need to be addressed. This can be a very treacherous space because many times the complexity is not really intrinsic to the problem but rather due to a lot of technical debt being accumulated over a long period of time. A venture thus has to be extremely careful, otherwise they can find themselves in a hamster\u2019s thread mill. That is, performing a lot of work but not really going anywhere.\n\nThis is an evolution of the idea of a killer user interface. Many of the deployments of Deep Learning are likely to be hybrid systems that will involve humans-in-the-loop. Therefore, to ensure adoption of these new systems, a lot of innovation needs to happen with how users interacts with the AI within a complex workflow. A lot of user interface tends to focus on the individual user, however companies like Atlassian understand a very different aspect in that there is extreme value in supporting complex workflows.\n\nHere\u2019s a perfect example of Deep Learning and how it can be used to interact with humans: https://aiexperiments.withgoogle.com/objectifier-spatial-programming\n\nThis is an extension of the previous ideas but extending it across multiple organizations. As an example, when intelligence is embedded across a supply chain, what sort of new kinds of efficiencies can we discover. This is an extremely compelling strategy. In fact, we will be unveiling our technology in this area that addresses this kind of a business model.\n\nThis is where one makes a heavy investment in a specific unique technology such that alternatives are simply not good enough to be considered. For smaller ventures without the outsized resources, this is very difficult to achieve without gambling on an entirely unique and novel approach. We actually see this in the AI and AGI spaces where you actually see a vast variety of approaches to AI (see article on Tribes of AI).\n\nMy opinion on this is that unless the approach leverages current advances in Deep Learning it would be a huge gamble. That\u2019s because Deep Learning is the only AI approach that has shown to deliver exceptional learning capabilities. Progress therefore will be most probably in the \u2018adjacent possible\u2019. Of course that does not mean that something else can emerge out of \u2018left-field\u2019.\n\nDeep Learning still has many flaws and I characterize the work in the field as being similar to what we find in Biotechnology. That is, much of the work is experimental and that just like Biotechnology, a revolutionary advance may be discovered by accident. I think many VCs miss this characterization entirely. They cannot imagine that software development can be so haphazard. But that\u2019s the reality (see article on Black Magic).\n\nThis is the most common explanation of how to achieve a defensible strategy. There have been many startups in AI that have been acquired, not because of unique technology, but rather by having a head start in collecting a massive set of data. There definitely is a long tail here since its literally inexhaustible as to what kind of data we can collect.\n\nThese are six strategies that we should be continually exploring as we search for new business models in Deep Learning. A lot of work in Deep Learning is done in the trenches of research labs and they continue to astonish both researchers and observers. However, what is sorely lacking is the examination of how to build products using Deep Learning."
    },
    {
        "url": "https://medium.com/intuitionmachine/businesses-need-artificial-specialist-intelligence-not-the-other-kind-b0c285c1586e",
        "title": "Businesses Need Artificial \u201cSpecialist\u201d Intelligence, Not the Other Kind",
        "text": "Here is the stark reality about businesses, we need specialists to do almost all our jobs. What this means in terms of the development of Artificial Intelligence (AI), or more precisely Artificial General Intelligence (AGI), is that the real world need of narrow intelligence outsizes the need for smarter self-aware intelligences. So the fear about the emergence of a sentient intelligence is completely misplaced. What we should really fear is the exponential pace of developing artificial specialist intelligence.\n\nThat is, the kind of intelligence that can do a job so damn well that we never need to pay a human anymore to perform the job. In the old days, the term \u201ccomputer\u201d was used for humans who would meticulously perform hand calculations. You would have rows of people that would do this kind of work:\n\nIn fact, you couldn\u2019t hire just anybody off the street. These people had to be above average in their mathematical skills. Richard Feynman himself was known to be a very gifted \u201ccomputer.\u201d Here\u2019s a story of Feynman besting the abacus where Feynman describes his approximation technique:\n\nSo when Google has its self-driving cars drive two million miles, it has driven more miles than the average human. A human who drives on average 13,000 miles in a year will need to drive for 153 years to reach two million miles. After 1.7 miles, Google had reported that its cars had 11 minor accidents. I\u2019ve driven my cars at one tenth the amount that Google has done and I\u2019m certain to have at least 11 minor accidents (i.e scraped hubcaps, fender benders etc). The point though here is that automation can easily become the safest and most likely the best kind of driver we will ever have. There are 3.5 million truck drivers in the U.S., and pretty soon they will be 3.5 million truck drivers that drive less safely and less efficiently than specialized driving automation.\n\nAs far as the \u201cstreet smarts\u201d or \u201ccommon sense\u201d that we expect from humans to do their job correctly, we might as well throw that out the door also. Let\u2019s look at the infamous case of United Airlines. A paying passenger was battered and dragged off a plane simply because common sense did not prevail. Surely all the flight attendants, ground crew and law enforcement were not complete idiots. Yet, a simple solution to the problem could not be found (i.e. raise the re-accommodation payment until someone accepts).\n\nThe reason we insert humans in our businesses is so that common sense will prevail. Unfortunately, the way we\u2019ve mechanized most of our corporations, we know that we\u2019ve thrown out all common sense. The popularity of the Dilbert cartoon strip is a testament of loss of common sense in our corporations. So let\u2019s not pretend here, a majority of corporations run without much common-sense, furthermore they are run mostly by warm body humans. Clearly, the presence of a warm-body does not guarantee the existence of \u2018common-sense\u2019.\n\nMost businesses don\u2019t require their employees to have common-sense, what they want their employees to have is mechanized efficiency. It just turns out that this mechanized efficient kind of job that is the most easiest replaced with today\u2019s technology (not some future AGI).\n\nAmerican workers are now forced into the \u201cGig Economy\u201d whether they like it or not. Let me make it perfectly clear, the gig economy is a knee-jerk reaction to the destruction of jobs (or more precisely, that of careers). It is the survival mechanism for people to become more adaptable in what they can provide as services. What we are losing are single professions that are able to support ourselves with. It means that there are fewer and fewer jobs that we can support ourselves by being just specialists. So what happens is that people have to become specialists in many other kinds of jobs. That portfolio diversity allows us to survive as each job incrementally gets extinguished by automation.\n\nSo we are forced now to become generalists that become masters of many skills. How can we master many skills? It turns out that automation is what allows us to master skills with less experience. The table has been flipped in that folks with \u201ccommon sense\u201d and \u201cstreet sense\u201d become critical. Folks with advanced \u201cbusiness intuition\u201d are the ones that will continue to thrive (before AGI). That\u2019s because automation has given them superpowers (ask Andrew Ng).\n\nSo as we make rapid progress, expect to see more capable narrow intelligence applications created. The world will become even more competitive over time and anyone who doesn\u2019t understand how to implement and deploy Artificial Intelligence is going to be at a complete disadvantage. Leaders of business who have an intuition of how their markets operate and have the savvy to build the automation that can exploit opportunities are going to take over the world.\n\nThe main point I want to make is that debates about the possibility of AGI (or AI if you still want to use that antiquated term) are pointless. The present reality (or is it clear and present danger?) is that artificial narrow intelligence is going to clean up big time. The main blind spot of many is that AI will only be useful if it has general intelligence. On the contrary, it is more useful if it has specialized intelligence.\n\nThis is not a new observation, Marc Andreesen has said many times that \u201csoftware is eating the world.\u201d If is not obvious to you, Artificial Intelligence is software.\n\nMark Cuban has an even more surprising quote:"
    },
    {
        "url": "https://medium.com/intuitionmachine/navigating-the-unsupervised-learning-landscape-951bd5842df9",
        "title": "Navigating the Unsupervised Learning Landscape \u2013 Intuition Machine \u2013",
        "text": "Unsupervised learning is the Holy Grail of Deep Learning. The goal of unsupervised learning is to create general systems that can be trained with little data. Very little data.\n\nToday Deep Learning models are trained on large supervised datasets. Meaning that for each data, there is a corresponding label. In the case of the popular ImageNet dataset, there are 1M images labeled by humans. 1000 images for each of the 1000 classes. It can take some effort to create such dataset, many months of work. Imagine now creating a dataset with 1M classes. Imagine having to label each frame of a video dataset, with 100M frames. This is not scalable.\n\nNow, think about how you got trained when you were very little. Yes you got some supervision, but when your parents told you that is a \u201ccat\u201d they would not tell you \u201ccat\u201d every split second you were looking at a cat for the rest of your life! That is what supervised learning is today: I tell you over and over what a \u201ccat\u201d is, maybe 1M times. Then your Deep Learning model gets it.\n\nIdeally, we would like to have a model that behaves more like our brain. That needs just a few labels here and there to make sense of the multitude of classes of the world. And with classes I mean objects classes, action classes, environment classes, object parts classes, and the list goes on and on.\n\nAs you will see in this review, the most successful models are the ones that predict future representation of a video. One issue that many of these techniques have, and are trying to resolve, is that training for a good overall representation needs to be performed on videos, rather than still images. This is the only way to apply the learned representation to real-life tasks.\n\nThe main goal of unsupervised learning research is to pre-train a model (called \u201cdiscriminator\u201d or \u201cencoder\u201d) network to be used for other tasks. The encoder features should be general enough to be used in a categorization tasks: for example to train on ImageNet and provide good results, as close as possible as supervised models.\n\nUp to date, supervised models always perform better than unsupervised pre-trained models. That is because the supervision allows to model to better encode the characteristics of the dataset. But supervision can also be decremental if the model is then applied to other tasks. In this regards, the hope is that unsupervised training can provide more general features for learning to perform any tasks.\n\nIf real-life applications are the targets, as in autonomous driving, action recognition, object detection and recognition in live feeds, then these algorithms need to be trained on video data.\n\nOriginated largely from Bruno Olshausen and David Field in 1996. This paper showed that coding theory can be applied to the receptive field in the visual cortex. They showed that the primary visual vortex (V1) in our brain uses principles of sparsity to create a minimal set of base functions that can be also used to reconstruct the input image.\n\nYann LeCun group also worked a lot in this area. In this page you can see a great animation of how sparse filters V1-like are learned.\n\nStacked-auto encoders are also used, by repeating the process of training greedily layer by layer.\n\nOne technique uses k-means clustering to learn filters at multiple layers.\n\nOur group named this technique: Clustering Learning, Clustering Connections and Convolutional Clustering, which very recently achieved very good results on the popular STL-10 unsupervised dataset.\n\nOur work in this area was developed independently to the work of Adam Coates and Andrew Ng.\n\nRestricted Boltzmann machines (RBMs), deep Boltzmann machines (DBMs), Deep belief networks (DBNs) have been notoriously hard to train because of the numerical difficulties of solving their partition function. As such they have not been used widely to solve problems.\n\nGenerative models try to create a categorization (discriminator or encoder) network and a model that generates images (generative model) at the same time. This method originated from the pioneering work of Ian Goodfellow and Yoshua Bengio. Here is a great and recent summary of GAN by Ian.\n\nThe generative adversarial model by Alec Radford, Luke Metz, Soumith Chintala named DCGAN instantiates one such model that got really awesome results.\n\nA good explanation of this model is here. See this system diagram:\n\nThe DCGAN discriminator is designed to tell if an input image is real, or coming from the dataset, or fake, coming from the generator. The generator takes a random noise vector (say 1024 values) at the input and generates an image.\n\nIn the DCGAN, the generator network is:\n\nwhile the discriminator is a standard neural network. See code below for details.\n\nThe key is to train both networks in parallel while not completely overfitting and thus copying the dataset. The learned features need to generalize to unseen examples, so learning the dataset would not be of use.\n\nTraining code of DCGAN in Torch7 is also provided, which allow for great experimentation.\n\nAfter both generator and discriminator network are trained, one can use both. The main goal was to train a nice discriminator network to be used for other tasks, for example categorization on other datasets. The generator can be used to generate images out of random vectors. These images have very interesting properties. First of all, they offer smooth transitions from the input space. See an example here, where they show the images produced by moving between 9 random input vectors:\n\nAlso the input vector space offers mathematical properties, showing the learned features are organized by similarity:\n\nThe smooth space learned by the generator suggests that the discriminator also has similar properties, making it a great general feature extractor for encoding images. This should help the typical problem of CNN trained in discontinuous image datasets, where adversarial noise makes them fail.\n\nA recent update to the GAN training, provided a 21% error rate on CIFAR-10 with only 1000 labeled samples.\n\nA recent paper on infoGAN was able to produce very sharp images with image features that can be dis-entangled and have more interesting meaning. They, however, did not report the performance of the learned features in a task or dataset for comparison.\n\nInteresting summary on generative models are also here and here.\n\nAnother very interesting example is given here where the authors use generative adversarial training to learn to produce images out of textual descriptions. See this example:\n\nWhat I appreciate the most about this work is that the network is using the textual description as input to the generator, as opposed to random vectors, and can thus control accurately the output of the generator. A picture of the network model is here:\n\nThese models learn directly from unlabeled data, by devising unsupervised learning tasks that do not require labels, and learning aims at solving the task.\n\nUnsupervised learning of visual representations by solving jigsaw puzzles is a clever trick. The author break the image into a puzzle and train a deep neural network to solve the puzzle. The resulting network has one of the highest performance of pre-trained networks.\n\nUnsupervised learning of visual representations from image patches and locality is also a clever trick. Here they take two patches of the same image closely located. These patches are statistically of the same object. A third patch is taken from a random picture and location, statistically not of the same object as the other 2 patches. Then a deep neural network is trained to discriminate between 2 patches of same object or different objects. The resulting network has one of the highest performance of pre-trained networks.\n\nUnsupervised learning of visual representations from stereo image reconstructions takes a stereo image, say the left frame, and reconstruct the right frame. Albeit this work was not aimed at unsupervised learning, it can be! This method also generates interesting 3D movies form stills.\n\nUnsupervised Learning of Visual Representations using surrogate categories uses patches of images to create a very large number of surrogate classes. These image patches are then augmented, and then used to train a supervised network based on the augmented surrogate classes. This gives one of the best results in unsupervised feature learning.\n\nUnsupervised Learning of Visual Representations using Videos uses an encoder-decoder LSTM pair. The encoder LSTM runs through a sequence of video frames to generate an internal representation. This representation is then decoded through another LSTM to produce a target sequence. To make this unsupervised, one way is to predict the same sequence as the input. Another way is to predict future frames.\n\nAnother paper (MIT: Vondrick, Torralba) using videos with very compelling results is here. The great idea behind this work is to predict the representation of future frames from a video input. This is an elegant approach. The model used is here:\n\nOne problem of this technique is that a standard neural network trained on static frames is used to interpret a video. Such networks does not learn the temporal dynamics of video and the smooth transformation of objects moving in space. Thus we argue this network is ill-suited to predict future representations in video.\n\nTo overcome this issue, our group is creating a large video dataset eVDS created to train new network models (recursive and feed-back) directly on video data.\n\nPredictive deep neural networks are models that are designed to predict future representations of the future.\n\nPredNet is a network designed to predict future frames in video. See great examples here: https://coxlab.github.io/prednet/\n\nPredNet is a very clever neural network model that in our opinion will have a major role in the future of neural networks. PredNet learns a neural representation that extends beyond the single frames of supervised CNN.\n\nPredNet combine bio-inspired bi-directional [models of the human brain] (https://papers.nips.cc/paper/1083-unsupervised-pixel-prediction.pdf). It uses predictive coding and using [feedback connections in neural models] (http://arxiv.org/abs/1608.03425). This is the PredNet module and example of 2 stacked layer:\n\nThis model also has the following advantages:\n\nOne issue with PredNet is that predicting the future input frame is a relatively easy task some simple motion-based filters at the first layer. In our experiments PredNet this learns to do a great job at re-constructing an input frame, but higher layer do not learn a good representations. In our experiments, higher layer in fact are not able to solve simple tasks like categorization.\n\nPredicting the future frame is in fact not necessary. What we would like to do is to predict future representations of next frames, just like Carl Vondrick does.\n\nA very interesting model is the PVM from BrainCorporation. This model aims at capturing bidirectional and recurrent connections in the brain, and also provide local layer-wise training.\n\nThis model is very interesting because it offers connectivity similar to new network models (recursive and feed-back connections). These connections provide temporal information and generative abilities.\n\nIt is also interesting because it can be trained locally, with each PVM unit trying to predict its future output, and adjusting only local weights appropriately. This is quite different from the way deep neural network are trained today: back-propagating errors throughput the entire network.\n\nThe PVM unit is given below. It has inputs from multiple lower layers, to which it also provides feedback, and lateral connections. The feedback is time-delayed to form recurrent loops.\n\nMore details about the PVM system are given here.\n\nThis recent paper (April 2017) trains unsupervised models by looking at motion of objects in videos. Motion is extracted as optical flow, and used as a segmentation mask for moving objects. Even though optical flow signal does not give anywhere close to a good segmentation mask, the averaging effect of training on a large data allow the resulting network to do a good job. Examples below.\n\nThis work is very exciting because it is follows neuroscience theories of how the visual cortex develops by learning to segment moving objects.\n\nIs your to make.\n\nUnsupervised training is very much an open topic, where you can make a large contribution by:\n\nI have almost 20 years of experience in neural networks in both hardware and software (a rare combination). See about me here: Medium, webpage, Scholar, LinkedIn, and more\u2026\n\nIf you found this article useful, please consider a donation to support more tutorials and blogs. Any contribution can make a difference!"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-key-to-agi-learning-to-learn-new-skills-a2ce49d9bb0b",
        "title": "How to Re-Invent the Cognitive Stack of Intelligence",
        "text": "Kevin Kelly (founding editor of Wired magazine) just wrote a near disaster of an article \u201cThe AI Cargo Cult: The Myth of Superhuman AI.\u201d Kelly begins by attempting to tear down the assumptions of the superhuman AI hypothesis:\n\nwith the following arguments:\n\nYou can read the article in more detail, but also make sure you read the comments. As I began to write this, I was going to refute each argument in detail. However, after a bit of thought, Kelly\u2019s arguments are without any merit that its not worth the effort to refute. So I will just point you to the comments in his article, that should be sufficient to explain Kelly\u2019s mistakes. The comments are a treasure trove of ideas on what\u2019s actually more important.\n\nHowever, I did enjoy the article because Kelly, despite being so very wrong, provided two good observations about the nature of intelligence.\n\nThe observation that there are many kinds of intelligence and that it can\u2019t be measured using just a single dimension. This is not a new idea. The psychologist Howard Gardner has his Theory of Multiple Intelligence where he describes 8 kinds that humans are theorized to have. It\u2019s the same idea as Marvin Minsky\u2019s Society of Mind. However, I like that Kelly points out that conventional computers that perform mathematical calculations or store memory are different kinds of intelligence that have long surpassed human capabilities in these areas. Nobody will doubt this. Kelly however makes the absurd argument that \u2018smarter\u2019 can\u2019t be defined because intelligence isn\u2019t one dimensional.\n\nI find it important to detail Howard Gardner\u2019s list of intelligences (refer to Wikipedia article for more detail):\n\nMusical-rhythmic and harmonic, aka Musicality\n\nThis area has to do with sensitivity to sounds, rhythms, tones, and music.\n\nThis area deals with spatial judgment and the ability to visualize with the mind\u2019s eye.\n\nVerbal-linguistic, aka Linguistic intelligence\n\nPeople with high verbal-linguistic intelligence display a facility with words and languages.\n\nLogical-mathematical, aka Reason\n\nThis area has to do with logic, abstractions, reasoning, numbers and critical thinking.\n\nBodily-kinesthetic, aka Gross and Fine motor skils\n\nControl of one\u2019s bodily motions and the capacity to handle objects skilfully.\n\nInterpersonal, aka Social skills\n\nSensitivity to others\u2019 moods, feelings, temperaments, motivations, and their ability to cooperate in order to work as part of a group.\n\nIntrapersonal, aka Introspection\n\nThis area has to do with introspective and self-reflective capacities.\n\nThis sort of ecological receptiveness is deeply rooted in a \u201csensitive, ethical, and holistic understanding\u201d of the world and its complexities \u2014 including the role of humanity within the greater ecosphere.\n\nGardner adds a few more such as spiritual intelligence and teaching-pedagogical intelligence.\n\nIf Gardner could previously identify 8 kinds of intelligence for humans and if a machine can be shown to be better in all these 8 kinds, then of course you can show a machine to be smarter despite intelligence being multi-dimensional. Interesting enough, Brenden Lake et al. \u201cBuilding Machines that Learn and Think like Humans\u201d emphasizes a subset of Gardner\u2019s 8 intelligences as where research should focus on.\n\nAs I write this, I realized that a good graphic may be worthwhile that shows how multiple intelligences and intuition may be related. That\u2019s the image you see at the beginning of the post. An excellent example of the multiple intelligences coordination is shown by this recent research (must watch!):\n\nWhat follows next is a prescription on how to \u201cinvent\u201d this cognitive stack of intelligence.\n\nThe second interesting observation that Kelly makes is the idea of inventing or discovering new strategies for thinking. He writes:\n\nInventing new ways of thinking, that is solving problems in new ways, is discovered by running simulations. Simulations are how machines \u2018imagine\u2019 alternative situations. This is how AlphaGo was able to invent new strategies. It played against itself millions of times. Kelly seems to attribute the ability to invent new ways of thinking as something that is intrinsically human. However, he fails to provide an argument or explain the origins of this \u201cunique\u201d capability.\n\nI discussed in a previous article some research that\u2019s been performed on the origins of innovation (see: Adjacent Possible). We are however still in the early stages of understanding how to build machines that can learn new thinking strategies. The only hint that seems apparent to me is the need for simulation or alternatively \u201cgame play\u201d. Kelly provides some support for this when he writes:\n\nIt is also clear to Kelly that simulation is necessary to invent new \u2018smartness\u2019.\n\nThese two observations by Kelly hint to a very interesting approach towards developing more capable intelligent machines. This approach is not entirely new and many research groups are working on this area. Specifically, the identification that there is a need for multiple kinds of intelligence and that there needs to be way to invent new kinds of intelligence. Discovering how to do the latter is a key technology that will greatly accelerate development.\n\nIn \u201ca roadmap to AGI\u201d, I wrote about modular deep learning, market driven coordination and meta-learning as key capabilities for the next step. The assumptions of the former two are the need to address the decomposition and composition behavior. The latter addresses a way to learn new behavior. The latter, I\u2019m still struggling with the details. Clearly the notion of \u2018late-binding\u2019 needs to fit somewhere and that somewhere has to do with \u2018context adaptation\u2019. My understanding of meta-learning is incomplete if I can\u2019t understand how it relates to context adaptation. We are still in the early innings, however it is becoming quite clear as where interesting new research will be coming from.\n\nOne open question that I do have is whether new strategies for thinking can be learned using gradient descent as the learning method. I suspect that it cannot. New strategies are likely to be discovered by chance and the learning approach is likely something along the lines of genetic algorithms.\n\nOne thing though that I can assure you that is happening, something that surprised me about Kevin Kelly\u2019s remark:\n\nDeep Learning development is accelerating at an unimaginable pace. Anyone who tries to keep up in this field knows intimately well that this indeed is happening. Unfortunately, even people like Kevin Kelly, who one would expect would be in the know, is completely blind to the massive developments.\n\nBTW, this paper (https://arxiv.org/pdf/1703.10987.pdf that\u2019s referenced by one of the commentators is bordering on the weird. Although, the arguments are much more solid than Kevin Kelly\u2019s."
    },
    {
        "url": "https://medium.com/intuitionmachine/seven-deadly-sins-and-ai-safety-5601ae6932c3",
        "title": "Seven Deadly Sins and Artificial Intelligence Safety",
        "text": "It occurred to me one day that the seven deadly sins, that is:\n\nThe consensus understanding is that these behaviors originates from instinctual or biological sources. However, I wonder that if it originates at a higher level, that is our intuition. The distinction between instinct and intuition is that the former is hardwired while the latter is learned from experience. I will make the additional observation that our personalities are mostly setup by our hardwired instincts and reinforced through experience by our intuition.\n\nIn an earlier post, I wrote about intuition in some detail. Dual Process Theory theorizes that there are two kinds of cognition:\n\nSystem 1 is our intuition. Daniel Kahneman in his book \u201cThinking Fast and Slow\u201d argues that our cognitive biases are what works against us to lead us to many irrational decisions. The question, I have is whether the seven deadly sins also come from our cognitive biases and therefore our intuition? Well, just looking at the above table, it probably makes sense since a lot of impulsive behavior seems fit under intuition. So let\u2019s start, consult the \u201cCognitive Bias Cheet Sheet\u201d if you get lost.\n\nOkay, let\u2019s enumerate the sins and propose the cognitive bias that these sin may plausibly originate from:\n\nWrath \u2014 Lack of Meaning. We think we know what others are thinking.\n\nGluttony \u2014 Need to act fast. In order to stay focused, we favor the immediate, relatable thing in front of us over the delayed and distant.\n\nPride \u2014 Too much Information. We notice flaws in others than in ourselves. Naive realism.\n\nLust- Not a bias. This is an instinct.\n\nSloth \u2014 Too much Information. We are drawn to confirm existing beliefs. Confirmation bias.\n\nEnvy-Need to act fast. In order to act, we need to be confident in our ability to make an impact and to feel like what we do is important.\n\nGreed- Need to act fast. In order to get anything done, we\u2019re motivated to complete things that we\u2019ve already invested time and energy in.\n\nThe correlation looks quite accurate. The deadly sins are extreme behaviors that likely start of as more moderate biases but through a combination of other factors can lead to dangerous behavior.\n\nNow, let\u2019s go to another question that in fact could be related. Can Artificial Intelligence be motivated by these deadly sins?\n\nOne argument out there that A.I. will not be as destructive as humans is that A.I. are not biological and therefore do not have the same primitive instincts that lead us to destructive and deplorable behavior like racism and genocide.\n\nGoogle has a paper \u201cBringing Precision to the AI Safety Discussion\u201d where they discuss five future (not present) requirements for AI systems to ensure our safety. I\u2019ve taken the liberty to phrase this as commandments:\n\nSo here\u2019s the question, can motivations driven by seven deadly sins be acceptable under these commandments? The problem with these five Google commandments is that the only real firewall is number 2. That is, \u201cThou shalt not game the reward function\u201d. Unfortunately, we know from human history is that it is next to impossible to set up laws that aren\u2019t going to be gamed by enterprising participants.\n\nPerhaps we need another set of AI commandments based on the Seven Virtues:\n\nCharity- AI shall be generous and make sacrifices on behalf of humans.\n\nDiligence- AI shall not complain about work.\n\nPatience \u2014 AI shall be patient with humans.\n\nKindness \u2014 AI shall have empathy towards humans.\n\nNow, that\u2019s definitely more reassuring.\n\nSo why is this so much better than the commandment, \u201cthou shalt not game the system\u201d? Ideally, you want to be able to control behavior from the bottom, so that it is instinctual and not something that can be \u2018gamed\u2019. Mechanisms from the top down are problematic because they can be easily rigged. We see an analogy in government laws that citizenry always seem to have a loophole to exploit.\n\nSo, as a path towards implementation, one has to explore how to hardwire virtues into an AI. I think this is the only way forward and top-down approaches of defining rules will be impossible to make air-tight. Let me propose that we hardwire all of these virtues in a blockchain. Said in a different way, hardwire laws and behavior through network consensus.\n\nNetwork consensus is in fact how humans enforce morality. Yuval Noah Harari, author of \u201cSapiens\u201d writes that there are three types of reality:\n\nLaws are encoded in automation by hard coded perceived reality. Civilization has used intersubjective reality in the form of laws and religion to enforce individual behavior. Most current research in AI safety however involves the codification of subjective reality, this however is problematic in that learning machines intrinsically adaptable and therefore the interpretation of reality is subject to change. By contrast, intersubjective reality is more difficult to change. It is much easier to change one\u2019s on perspective that to change an entire society\u2019s perspective.\n\nHeart this article if the ideas resonate with you!\n\nNow for some very good entertainment related to this very topic:"
    },
    {
        "url": "https://medium.com/intuitionmachine/writing-travel-blogs-with-deep-learning-9b4a6fbcc87",
        "title": "Writing Travel Blogs with Deep Learning \u2013 Intuition Machine \u2013",
        "text": "We all got excited with the recent developments of deep neural networks. Among the different applications of deep learning, Natural Language Processing (NLP) applications have attracted quite a bit of interest. It is really great to see a machine learning model generating with high accuracy text that resembles Shakespeare, Wikipedia, Harry Potter, Obama speeches, Star Wars episodes, and ultimately even code.\n\nIs it possible to automatize travel blogging with artificial intelligence? It would be great if your AI assistant helped you keeping memories of your travels by automatically writing down stories about the places you visited. It turns out that this task could be easily tackled using deep learning because there is plenty of data available online.\n\nThere exist different tools to train a deep neural network on text which are also well documented. Nonetheless, retrieving appropriate data is often the most challenging task. Therefore, here I discuss on how to crawl the web using python. Given that python is commonly used by data scientists, it is very convenient to use it to crawl the web too. In particular you might want to use Scrapy which is an open-source framework to extract data from the web. Here is a very useful blog post about how to install scrapy and starting off with your first crawler.\n\nThere are many entertaining and very well written travel blogs you can find online. Here you can find the list of the 50 most famous travel blogs:\n\nAll these blogs are good candidates to collect text data that can be used to train a neural network.\n\nMost of these travel blogs have been written with Wordpress and they share the same HTML structure. For example, take a look at Nomadic Matt\u2019s blog: if you perform an empty search you will get the list of all the blog articles available:\n\nIt will be enough to go through all the pages and all the links in the pages with the crawler to get all the articles written. Moreover, once you click on an article all the content can be found in the CSS container labelled with \u201centry-content\u201d. Note that this is true for any blog written with Wordpress:\n\nI wrote a simple crawler in Scrapy that can be found on my GitHub. This crawler takes also care of removing strange symbols from the retrieved text.\n\nOnce you collect enough data it is possible to train a charter-level Recurrent Neural Network (RNN). I used Microsoft CNTK to train the char-level RNN. Microsoft made available a script to train a neural char-level language model to predict the next character after a sequence of characters. I trained a 2-layer LSTM with 256 hidden nodes per layer using the data collected with my crawler. At the beginning, the text generated by the network made little sense:\n\nAfter training the network overnight results were much better:\n\nFinally I wanted to see if my network was able to complete sentences I started. Asking your network to write a travel blog for you from scratch might be a bit too much. On the other hand, it might be possible to get some help from the network to write your blog. In this scenario, I could imagine myself writing a sentence and having the network to complete it adding further details. For example, I can imagine myself writing: \u201cI am travelling to \u201d. And these are possible outputs from the network:\n\nOr I could write something like \u201cThe hostel was \u201d and the network would say:\n\nWe can see that results are not perfect but not too bad either. It is cool to see that these sentences were not in my original data set and thus the network is really generating new ones. For example, \u201cI am travelling to Europe\u201d or \u201cI am travelling to Cornwall\u201d are not present in the original data set. It is also funny to see how the RNN invents new places that do not exists: eg \u201cOlymbookality Starbucks\u201d. This is why using a word-level RNN for this task might be a better option. Overall, it seems that the network could be greatly improved if trained for a longer time and with more data.\n\nHaving blog with meaningful posts written by a neural network from scratch might be still too challenging. Maybe it is possible to make better use of a neural network if used to complete specific sentences like \u201cThe hostel was bad because \u201d. At the current state, this network is not able to produce meaningful and creative sentences as a human could do. Nonetheless, there might exist some applications where a neural network can help extending already written sentences with pertinent content.\n\nYou can find the code used in this blog post here. Note that I used Nomadic Matt\u2019s blog as example because it is particularly well written. My blog post aims to show that texts from blogs can be also generated with machine learning. However, if you really want to use such network in a system you might have to get the permission from the original author of the blog."
    },
    {
        "url": "https://medium.com/intuitionmachine/jobs-that-deep-learning-can-automate-208b273be644",
        "title": "The AI Economy is Reserved for the Highly Skilled \u2013 Intuition Machine \u2013",
        "text": "Thomas Frey has a thought provoking article \u201c78 Skills that are Difficult to Automate\u201d. Frey breaks down the categories of jobs that he believes will remain \u201csafe\u201d from automation:\n\nI will attempt to address each one from the perspective of the \u201cDeep Learning Canvas\u201d that we\u2019ve developed. In our approach, we fuse together the ideas of \u201cJobs to be Done\u201d (JTBD) approach for identifying tasks that need to be addressed and an understanding of the cognitive limitations that can be enhanced through Deep Learning.\n\nIn Frey\u2019s article, he builds up his argument that the jobs most likely to be safe are those that take into account the irrationality of humans. The JTBD approach also goes beyond pure functionality, but also addresses human needs such as emotion and social currency. Whatever the Deep Learning system will be automating, it will need to target any combination of these three needs. Now, we don\u2019t use a blunt instrument and try to replace tasks wholesale. Rather, we do so in a surgical manner, identifying first the capabilities required to address the customers needs and then identifying which specific cognitive tasks can be enhanced through automation.\n\nWhat people seem to miss is that the replacement of jobs is performed piecemeal and incrementally. This kind of detail is missed by many prognosticators. The capabilities of those that remain employed become more powerful. The lesser skilled folk who are first to lose their jobs become less adept at using automation tools. It is a vicious cycle where those that have the skills, gain more advanced skills. While those that don\u2019t have the skills are forced to pay premium to gain the skills. In many cases, there aren\u2019t any educational institutions that exist to teach them these skills. Furthermore, less skills imply more commoditization. People will be forced into the \u2018gig economy\u2019, relegated to endlessly competing in markets with margins pushed towards zero.\n\nHowever, let\u2019s examine Frey\u2019s list because it is an informative base to perform more detailed analysis.\n\nComplex systems will always require people with advanced skills to orchestrate. However, that does not imply that there will less automation that will be enabling this activity. In fact, one should expect bleeding edge companies to leverage as much automation as needed to accelerate work. Tesla and SpaceX are likely highly automated companies as compared to their competitors.\n\nCompany\u2019s that find it too expensive to automate processes are likely the ones that don\u2019t know how to exploit automation to reduce development costs.\n\nCreative endeavors that only humans can appreciate\n\nThis is similar to the previous but relates to more artistic endeavors. In a lot of the Hollywood Science Fiction blockbusters, we continue to use increasingly advanced automation to reduce costs. Film makers no longer need to hire armies of extras to film massive battle scenes. These are now all done through CGI simulation. There was a period in time were Epics were too expensive to make, but today that\u2019s no longer a problem.\n\nCertainly we\u2019ll have the creative folks continue to drive development, but the human resources required will continue to diminish. In the future, we may not even need actors for films and just use CGI renditions of famous actors. In fact, if we ever get bored with seeing the same faces, we can always generate arbitrary faces!\n\nHuman to human interactions that produce an emotional response\n\nMost of what Frey describes, (i.e. a smile, a hug, a kiss, a massage etc.) usually aren\u2019t paid for. In fact, in many societies, paying for these \u201chuman interactions\u201d would be considered illegal!\n\nThe human as a watchdog against runaway automation. I think this kind of job will not go away if governments enact regulation that requires this. It is just like humans who serve as gas attendants. Certain states require it, although it isn\u2019t absolutely necessary. However, this kind of legislation can keep a lot of people on the job.\n\nThis is not very different from the previous class or even the first class. It is just supervision at the top rather than at the bottom. It however pertains to the need for humans to interpret machines so that other humans can understand (and accept) a machine\u2019s conclusions.\n\nHowever, this likely won\u2019t change because legislation is already in place that humans are required for these kinds of job. We can\u2019t have non-human judges sending people off to lifetime incarceration or death row.\n\nThe examples Frey provides in this category all seem to revolve around education. However, I would argue that education is moving massively online and that education will only get better with Virtual Reality, Augmented Reality and AI. That\u2019s because we can build many more interactive environments that can serve many more students at a quality level that is many times better than the average teacher.\n\nThe problem with our current education practices is that the lecture model is all passive listening. We need to strive for greater participation of the student in the model of Active Learning. People learn best by doing and not by lecture. The expertise to create highly engaging education will be in demand. This will require a deep understanding of the interactive technology, an understanding of human behavior and teaching methods. Where Deep Learning comes into play is its ability to react to human behavior. This is essential to effective teaching and is not outside the realm of what\u2019s possible.\n\nSettings where the loyalty of hacker-proof humans is preferable over digital machines\n\nOn the contrary, Blockchain systems have been shown to be hacker-proof as opposed to systems that have human elements that can be \u201csocially engineered\u201d.\n\nWe already have systems that make all sorts of \u2018valuations\u2019 on what we should focus our attention on. Facebook manages our reading list. Amazon recommends products that we might prefer. Google filters our search results. More and more, AI is making our decisions for us. We\u2019ve become so used to machines like GPS giving us instructions that we\u2019ve lost all sense of direction.\n\nWhat we will likely see in this domain is that Blockchain technologies will ensure transparency and integrity of many of the interactions that assume a fair market or a democratic process. Today, many of these processes are gamed to the benefit of the very few. Human to human valuations should not be entrusted exclusively to humans, but rather it should be done through a transparent collective manner. The reason here is because humans have motivations to game the system.\n\nIn an AI economy, it will become harder and harder to game one\u2019s credentials. Machine intelligence will become sophisticated enough to expose many in the workplace as imposters. Let\u2019s all be perfectly honest, the reason why people get paid more than that they deserve is due to a gross inefficiency in how humans assess an employee\u2019s worth. 20% of employees do most of the work, while 80% are all pretenders.\n\nThis is the same theme as 3 of the previous themes. That is, automation as a tool to enhance human work.\n\nIn summary, the list can be simplified even further:\n\n2. Jobs that that use humans as safety valves against automation failure.\n\n3. Jobs that interpret the decisions of machines.\n\nThat\u2019s five classes of jobs that will exist in the future that appears to be safe. On the other hand, with the exception of the \u201chuman safety valve\u201d, all these other jobs require high level skills. Jobs of the future need to have an deep understanding of humans as well as machines, and it is in this interaction of man with machine where jobs will exist.\n\nI think what few seem to appreciate is that Deep Learning AI is technology that is like human intuition. It is an opposite technology from more classical AI technologies that focused on reasoning. At this time there remains a Semantic Gap. However, humans capabilities are stuck between a rock and a hard place. Between Artificial Intuition and Artificial Reasoning. This is were many people seem to be getting it all wrong about what jobs are safe and what is not. Let us not fall into this fantasy that our unique human intuition is safe from being replaced by automation.\n\nThere aren\u2019t many jobs that are safe with the emergence of Deep Learning. We have to come to grips with so we can get a head start in examining the core of our economic system. AI will likely break capitalism and we unfortunately are not starting serious discussions on what will replace it. The stark reality is that the emerging AI economy is reserved for the highly skilled. Everyone else need not apply.\n\n\u2661 Heart if you like this story!"
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-knowable-knowns-and-unknowns-17efb8822059",
        "title": "Deep Learning Unknowable Knowns \u2013 Intuition Machine \u2013",
        "text": "One good way to frame the question of the limits of Deep Learning is in the context of the Principle of Computational Equivalence by Stephen Wolfram. Wolfram showed that simple cellular automation are able to exhibit complex behaviour that cannot be predicted from initial conditions or the simple rules that specify its incremental behaviour. Certain kinds of cellular automata can exhibit complex behaviour that cannot be reduced to a mathematical model that capture its behaviour in closed form. Wolfram examples of an \u2018irreducible\u2019 system that exhibits this complex behaviour are the brain and weather systems. Wolfram classifies these kinds of systems as exhibiting \u201cUniversality\u201d.\n\nA Deep Learning system that has memory belongs to this class of universal machines, however this does not imply that these systems can replicate the behaviour of another universal machine. Bernhard Scholkopf reveals this conclusion in a paper \u201cTowards a Learning Theory of Cause and Effect\u201d. That is, a learning system is able only to derive the cause while observing the effect. This tells you that a Learning system can\u2019t learn the mechanisms of say, how a DNA manufactures specific proteins. A fundamental limitation of any system that learns from data is that it cannot predict effect from cause. This theory is analogous to the \u201cHalting Problem\u201d in Computability Theory.\n\nOne criticism that I hear often about Deep Learning is that it doesn\u2019t capture the biological mechanisms of the brain. This is a fair criticism. However, Wolfram\u2019s Universality explains why it should not be a major issue toward achieving AGI. Deep Learning systems can possibly have equivalent capabilities as a biological brain albeit by using different computational mechanisms. At the fundamental level, all these systems are computational systems that exhibit three building blocks. That is, computation, memory and signalling. Complex behaviour is an emergent behaviour that like cellular automata arises from very simple rules.\n\nA useful schema in understanding the capabilities of a system to learn or discovery unknowns can be stated as follows:\n\nKnowable knowns, meaning models that will converge on training data. Knowable unknowns, are trainable models that are able to make accurate predictions on non-training data. Unknowable knowns, reflects an inability to learn a known system, this covers the area of preforming predictions of other irreducible universal machines. Unknowable unknowns is inability to discover what a machine does not know. We break this down in more detail:\n\n(1) Knowable Knowns \u2014 Given a large enough set of knowns (i.e. training data) we can get good convergence of our prediction errors.\n\n(2) Knowable Unknowns \u2014 This is an expression of the concept of generalization. With good generalization, we can know about test data that a machine has never encountered in its training set.\n\n(3) Unknowable Knowns \u2014 However, there are certain classes of system that deep learning can never learn. These are in the class of computational irreducible systems. However a Deep Learning system may detect the direction of causality and be able to know that is able to learn from the system. What it will not be able to know (unknowable) is if a system is in the class of computational irreducible systems.\n\n(4) Unknowable Unknowns \u2014 Finally, there is a class of total ignorance. This is really a meta-physical statement. The class of unknowables that cannot be known is unknowable. Think of it as the \u201cGreat Firewall\u201d of knowledge.\n\nThe reader may have encountered a similar classification before by an infamous defense secretary. That is, \u201cKnown knowns\u201d, \u201cKnown unknowns\u201d, \u201cUnknown knowns\u201d and \u201cUnknown unknowns\u201d.\u201dKnown knowns\u201d are what we current know. \u201cKnown unknowns\u201d are what we know that we don\u2019t know. \u201cUnknown known\u201d is a reflection of willful ignorance or what politicians may call \u201calternative facts\u201d. \u201cUnknown unknowns\u201d is simply ignorance of what one does not know. A good cliche is the \u201cBlack Swan.\u201d That is the belief that black swan\u2019s don\u2019t exist when they in fact do. This schema differs in that the \u201cknowable\u201d schema in that this expresses the current state of understanding rather than an ability to learn.\n\nFor the study of learning machines it is more important to understand what is knowable rather than the current state of knowledge.\n\nEven though the above schema of ignorance looks complicated, it is but the tip of the iceberg of understanding ignorance. Consider more complexities such as misinformation, detecting model bias, ambiguity, disagreement, or accommodating for change.\n\n\u2661 Heart if an unknown has become known to you!"
    },
    {
        "url": "https://medium.com/intuitionmachine/kubernetes-gpus-tensorflow-8696232862ca",
        "title": "Kubernetes + GPUs \ud83d\udc99 Tensorflow \u2013 Intuition Machine \u2013",
        "text": "A little while ago I wrote a series of blog posts about Deep Learning and Kubernetes, using the Canonical Distribution of Kubernetes (CDK) on AWS and Bare Metal to deploy Tensorflow. These posts were highly technical, fairly long and difficult to replicate.\n\nYesterday I published another post, explaining how the recent addition of GPUs as first class citizens in CDK changed the game. This last post has had a lot of success. Thank you all for that, it is always pleasant to see that content reaches its audience and that people are interested in your work.\n\nThe wide audience it reached also led me to conclude that I should revisit my previous work in light of all the comments and feedback I got, and explain how having trivialized GPUs access in Kubernetes can help scientists to deploy scalable, long running Tensorflow jobs (or any other deep learning afaiac).\n\nThis post is the result of that thought process.\n\nSo today, we will:\n\nFor the sake of clarity, there is nothing new here, it is an updated, condensed version of my series about Tensorflow on Kubernetes Part 1, Part 2 and Part 3 that benefits from the latest and greatest additions to the Canonical Distribution of Kubernetes.\n\nTo replicate this post, you will need:\n\nIf you experience any issue in deploying this, or if you have specific requirements (non default VPCs, subnets\u2026), connect with us on IRC. I am SaMnCo on Freenode #juju, and the rest of the CDK team is also available there to help.\n\nFirst of all let\u2019s deploy Juju on your machine as well as a couple of useful tools:\n\nNow let\u2019s add credentials for AWS so you can deploy there:\n\nThe above line is interactive and will guide you through the process. Finally, let\u2019s download kubectl and helm\n\nClone this repository to access the source documents:\n\nOK! We\u2019re good to go.\n\nFirst of all, we need to bootstrap in a region that is GPU-enabled, such as us-east-1\n\nThis will take some time. You can track how the deployment converges with :\n\nAt the end, you should see all units \u201cidle\u201d, all machines \u201cstarted\u201d and it should look green.\n\nAt this point in time, as CUDA enablement is now fully automated, we can safely assume that our cluster is ready to operate GPU workloads. We can download the configuration:\n\nThe default username:password for the dashboard or Graphana is \u201cadmin:admin\u201d.\n\nFirst of all you need to know the VPC ID where you are deploying. You can access it from the AWS UI, or with\n\nNow we need to know the subnets where our instances are deployed:\n\nAnd now we need the list of Security Groups to add to our EFS access:\n\nAt this point, if have an EFS already deployed, just store its ID in EFS_ID, and make sure you delete all its mount points. If you do not have an EFS, create one with :\n\nAnd finally create the mount endpoint with :\n\nFrom the UI, you can now check that the EFS has mount points available from the Juju Security Group, which should look like juju-3d57d67a-8603\u20134161\u20138fc2-dc7e1ee08eef.\n\nNote: users of this Tensorflow use case have reported that EFS is a little slow, and that other methods such as SSD EBS are faster. Consider this the easy demo path. If you have an advanced, IO intensive training, then let me know, and we\u2019ll sort you out. This is something we are already working on.\n\nWe are now done with the Juju and AWS side, and can focus on deploying applications via the Helm Charts. Let\u2019s switch to our charts folder:\n\nNow let\u2019s create our configuration file for the EFS volume. Copy efs/values.yaml to ./efs.yaml\n\nand update the id of the EFS volume. It shall look like:\n\nNow deploy it with Helm\n\nIt will output something like:\n\nIn the KubeUI, you can see that the PVC is \u201cbound\u201d, which means it is connected and available for consumption by other pods.\n\nThere is a non GPU workload available in the charts repo called \u201cdistributed-cnn\u201d. It is an example I took from a Google Tensorflow Workshop and adapted to port it to Helm.\n\nLet\u2019s copy the values.yaml file and adapt it:\n\nand update the id of the EFS volume. The different sections are:\n\nwhich will leverage the Xip.io service to connect later on. The section of the file itself:\n\nNow deploy it with Helm\n\ndepending on the number of workers and parameter servers, the output will be more or less extensive. At the end, you can see the indications:\n\nThat\u2019s it, you can connect to your tensorboard and see the output.\n\nIf you want to compare results, just create several values.yaml files and upgrade your cluster:\n\nWhen you are happy and this and ready to get rid of it, you can delete it with\n\nIt\u2019s always nice to understand things by copying what others have already done. That\u2019s open source. It\u2019s also the fastest way to get things done in 90% of the time. Don\u2019t reinvent the wheel.\n\nThen the next step is the DIY, and, if you are reading this post now, you probably want to run your own Tensorflow code.\n\nSo start a new Dockerfile, and fill it with:\n\nAdapt this to your need (version\u2026), and write this worker.py file that you\u2019ll need. Make sure of:\n\nThe example Google gives looks like:\n\nOnce you have that ready, build and publish your images. You can have a CPU only image for the Parameter Servers, and a GPU one for the workers, or just one of them. Up to you to decide.\n\nNote: Example Dockerfiles are available in the docker-image repository for this, with several version of Tensorflow\n\nThen copy the values.yaml file from the tensorflow chart, and adapt it by pointing the cluster to your images. Then follow the same method as before\n\nTo tear down the Juju cluster once you are done:\n\nThis blog shows the progress that has been made in 3 months in the Canonical Distribution of Kubernetes to deploy GPU workloads. What used to take 3 posts now takes only 1 \ud83d\ude01\n\nThe Tensorflow chart is being use by various teams right now, and they are all giving very useful feedback. We\u2019re now looking into\n\nLet me how that goes for you, and if you have a use case for Tensorflow and would like to use this, let me know, so we collectively end up with a useful chart for everyone.\n\nAnd again, don\u2019t hesitate to come to us on Freenode #juju to discuss and onboard the Kubernetes+GPU+Tensorflow train!"
    },
    {
        "url": "https://medium.com/intuitionmachine/how-we-commoditized-gpus-for-kubernetes-7131f3e9231f",
        "title": "How we commoditized GPUs for Kubernetes \u2013 Intuition Machine \u2013",
        "text": "[Edit] A careful reader informed me (thanks for that HN user puzzle) that it is no longer required to run in privileged mode to access the GPUs in K8s. I therefore removed a note that previously stated that, and am in the process of updating my Helm charts to remove that requirement.\n\nOver the last 4 months I have blogged 4 times about the enablement of GPUs in Kubernetes. Each time I did so, I spent several days building and destroying clusters until it was just right, making the experience as fluid as possible for adventurous readers.\n\nIt was not the easiest task as the environments were different (cloud, bare metal), the hardware was different (g2.xlarge have old K20s, p2 instances have K80s, I had 1060GTX at home but on consumer grade Intel NUC\u2026). As a result, I also spent several hours supporting people to set up clusters. Usually with success, but I must admit some environments have been challenging.\n\nThankfully the team at Canonical in charge of developing the Canonical Distribution of Kubernetes have productized GPU integration and made it so easy to use that it would just be a shame not to talk about it.\n\nAnd as of course happiness never comes alone, I was lucky enough to be allocated 3 brand new, production grade Pascal P5000 by our nVidia friends.\n\nI could have installed these in my playful rig to replace the 1060GTX boards. But this would have showed little gratitude for the exceptional gift I received from nVidia. Instead, I decided to go for a full blown \u201cproduction grade\u201d bare metal cluster, which will allow me to replicate most of the environments customers and partners have. I chose to go for 3x Dell T630 servers, which can be GPU enabled and are very capable machines. I received them a couple of week ago, and\u2026\n\nThere we are! Ready for some awesomeness?\n\nIf you remember the other posts, the sequence was:\n\nOverall, on top of the Kubernetes installation, with all the scripting in the world, no less than 30 to 45min were lost to perform the specific maintenance for GPU enablement.\n\nIt is better than having no GPUs, but it is often too much for the operators of the clusters who want an instant solution.\n\nI am happy to say that the requests of the community have been heard loud and clear.\n\nAs of Kubernetes 1.6.1, and the matching GA release of the Canonical Distribution of Kubernetes, the new experience is :\n\nSince 1.6.1, the charms will now:\n\nYou don\u2019t believe me? Fair enough. Watch me\u2026\n\nFor the following, you\u2019ll need:\n\nand for the files, cloning the repo:\n\nDeploying in the cloud is trivial. Once Juju is installed and your credentials are added,\n\nSame same, but same. I was able to capture the moment where it is installing CUDA so you can see it\u2026 When it\u2019s done:\n\nThat\u2019s it, you can see the K80 from the p2.xlarge instance. I didn\u2019t do anything about it, it was completely automated. This is Kubernetes on GPU steroids.\n\nObviously there is a little more to do on Bare Metal, and I will refer you to my previous posts to understand how to set MAAS up & running. This assumes it is already working.\n\nAdding the T630 to MAAS is a breeze. If you don\u2019t change the default iDRAC username password (root/calvin), the only thing you have to do it connect them to a network (a specific VLAN for management is preferred of course), set the IP address, and add to MAAS with an IPMI Power type.\n\nThen commission the nodes as you would with any other. This time, you won\u2019t need to press the power button like I had to with the NUC cluster: MAAS will trigger via the IPMI card directly, request a PXE boot, and register the node, all fully automagically.\n\nOnce that is done, tag them \u201cgpu\u201d to make sure to recognize them.\n\nWait for a few minutes\u2026 You will see at some point that the charm is now installing CUDA drivers. At the end,\n\nThat\u2019s it, my 2 cards are in there: 1060GTX and P5000. Again, no user interaction. How awesome is this?\n\nNote that the interesting aspects are not only that it automated the GPU enablement, but also that the bundle files (the yaml content) are essentially the same, but for the machine constraints we set.\n\nIf you follow me you know I\u2019ve been playing with Tensorflow, so that would be a use case, but I actually wanted to get some raw fun with them! One of my readers mentioned bitcoin mining once, so I decided to go for it.\n\nI made a quick and dirty Helm Chart for an Ethereum Miner, along with a simple rig monitoring system called ethmon.\n\nThis chart will let you configure how many nodes, and how many GPU per node you want to use. Then you can also tweak the miner. For now, it only works in ETH only mode.\n\nBy default, you\u2019ll get the 3 worker nodes, with 2 GPUs (this is to work on my rig at home)\n\nWhat did I learn from it? Well,\n\nIf you\u2019re interested you can track the evolution of my tuning.\n\n3 months ago, I recognize running Kubernetes with GPUs wasn\u2019t a trivial job. It was possible, but you needed to really want it.\n\nToday, if you are looking for CUDA workloads, I challenge you to find anything easier than the Canonical Distribution of Kubernetes to run that, on Bare Metal or in the cloud. It is literally so trivial to make it work that it\u2019s boring. Exactly what you want from infrastructure.\n\nSo, let me know of your use cases, and I will put this cluster to work on something a little more useful for mankind than a couple of ETH!\n\nI am always happy to do some skunk work, and if you combine GPUs and Kubernetes, you\u2019ll just be targeting my 2 favorite things in the compute world. Shoot me a message @SaMnCo_23!"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-importance-of-an-agi-roadmap-for-deep-learning-20491456a1f2",
        "title": "Is \u201cDouble\u201d Exponential Growth in Deep Learning leading to AGI?",
        "text": "It is important to have an Artificial General Intelligence (AGI) roadmap. Not because we are going to achieve this anytime soon, but rather because we need a framework to understand the progress that is happening in Deep Learning. Ask any researcher and they will tell you that progress in Deep Learning is at breakneck speed. Russ Salakhutdinov (Apple\u2019s lead in AI) in a recent Simon\u2019s Institute lecture remarked that the developments were \u201ccrazy\u201d. So, in this context of \u201ccrazy\u201d, we got to get our bearings and see exactly where the hell are we at?!\n\nHowever, this is just a roadmap of the current techniques in Deep Learning. It does not provide a sense of the gaps in capabilities that we need to achieve to arrive at an approximation of AGI.\n\nI\u2019ve written earlier about what DARPA refers to as the Third Wave of AI. DARPA\u2019s framework gives a sense on what may be next, albeit a rather major advance to get to that \u201cwhat\u2019s next\u201d. I also give a kind of roadmap in terms of capabilities that we may anticipate for Deep Learning. Here\u2019s a graphic depiction of these capability levels:\n\nUnfortunately, this does not give enough of a sense of what is achievable at each capability level. Peter Voss points out a paper by Pat Langley in 2012 in his post about \u201cCognitive Architectures\u201d. Pat Langley\u2019s paper is a good framework for assessing what kind of progress needs to be made in AGI. I have to admit, I have not spent a lot of energy trying to contrast the different AGI approaches that are out there. I do however find Pat Langley\u2019s paper to be reasonable, simple and conservative enough to be a good basis for assessing current AGI development.\n\nPeter Voss has his own take on the paper, I will however do my own take from the perspective of Deep Learning development. I will ignore any development from other A.I. tribes in my analysis. I encourage the reader to read Langley\u2019s paper or Voss\u2019 blog post prior to proceeding. The paper was written in 2012, prior to the Deep Learning boom, so I am revisiting it today to see if we have made any progress in the AGI fronts that was described in the paper.\n\nHybrid architectures like Deep Reinforcement Learning and AlphaGo reveal an extremely compelling way to fuse the intrinsic intuition based cognition in DL system into higher level capabilities that require planning and strategy. We are still a way off in achieving abstract reasoning, comprehension and problem solving\n\nDeep Learning representations are opaque and inscrutable. However, interesting enough, the many forms of neural embedding (i.e. word2vec, Glove etc) seem to be able to capture some semantics to be useful as input features to other networks. It also seems that some prior external knowledge about the world can be introduced by these embeddings. Integration tends to require end-to-end training, however this is an extremely promising area to pursue. What I don\u2019t see happening soon is the interpretability of the representations.\n\nI think \u201cCognitive Synergy\u201d is going to be one of the more powerful developments in Deep Learning. Cognitive Synergy is the notion that many agents can work off the same representation. I think Deep Learning is making considerable strides in this space with regards to \u201cMulti-objective\u201d systems as well as in encoder-decoder networks. What we are seeing today is that multiple neural networks (see: Modular Deep Learning) are working in concert to create impressive results. The many stumbling block is that there is a need for \u2018late-binding\u2019 of representations. That is something that has yet to be developed.\n\nThere is a inherent bias in the research community to demand that the mechanisms that are used by Deep Learning have to conform to more rigid mathematical reasoning. There is a lot of emphasis in our models that demand some probabilistic interpretation. I am however on the camp that complex behavior emerges out of simplistic mechanisms. There is still a lot of work that needs to be done here, however we can see how our current models seem to be cracking under the strain of a lot of unexplainable observations.\n\nDeep Learning developments are no where near addressing what Voss\u2019 describes as \u201cambiguity, abstract conceptualization and reasoning, short-term memory and context, as well as meta-cognition.\u201d There is research on this that explores behavior prediction as well as playing information imperfect games like Poker. Still, it is still an unexplored area.\n\nFundamental research on the behavior of Deep Learning takes a back seat to novel techniques that yield impressive results in narrow domains. It will likely stay this way for a while. The \u2018attention\u2019 market today commands a higher premium on novelty and benchmarks rather than fundamental insight.\n\nNote that this are 6 assumptions that \u201cwere widely adopted during the AI\u2019s first three decades\u201d. The beauty of this is that is that we can contrast what we know today about Deep Learning and see how it fits with decades all perspective of what needs to be invented. As you can see, there are developments in good progress in 5 of 6 fronts.\n\nExperimentation and engineering in Deep Learning far surpasses theory and this trend will not end soon or at all. Fundamental research isn\u2019t given as much a priority in this field over more tangible \u201cstate-of-the-art\u201d results. However, that does not imply that we should neglect thinking about a much larger AGI roadmap such as this. This post hopes to shed a little more light on how far Deep Learning needs to go to achieve AGI."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-and-viral-product-development-ff54b58e78b",
        "title": "Deep Learning Canvas and Viral Product Development \u2013 Intuition Machine \u2013",
        "text": "In the breakneck Deep Learning research field, the obsession is to dish out innovative research as quickly as possible so as to avoid being scooped by other researchers. Like similar games in academe, it is a game of enhancing one\u2019s academic credentials through the accumulation of citations. We would of course like to see more real world problems addressed, however this isn\u2019t a top of academics. Companies however have different priorities and therefore need to deploy products that are competitively superior. There is of course a massive knowledge gap of how we can take Deep Learning and build valuable products out of it. Here we discuss the nature of the products we should build.\n\nWriting truly insightful and useful articles is very different from creating articles that become viral. I do notice this in my blogging. Many of the posts that I\u2019m proud of writing, don\u2019t receive the number of shares of my other simpler and less informative posts.\n\nJonah Berger is famous for introducing the elements that make up a viral post. Here are his elements:\n\nCoincidentally, these match like a glove the same elements that make up a good product. Clay Christensen who was first to coin the \u201cTheory of Disruption\u201d has this new \u201cTheory of Jobs to Be Done\u201d.\n\nThe \u201cJobs to Be Done\u201d framework is a way at identifying the needs of customers. In conventional approaches we segment customers through attributes such as income, age, race and other categories and create products based on these. So the focus is on what companies want to sell rather than focusing on what customers need. Christensen explains this best in his this story about milkshakes:\n\nChristensen\u2019s approach is to focus on what a customer needs during their daily activities. Customers purchase products because they find themselves needing a product to solve their specific problem. Therefore, we need to understand the context in which a customer finds himself in and then identifying solutions that enable that customer to solve their problem. Understanding the \u201cJob to be Done\u201d leads towards the creation of products that are more likely to be \u201chired\u201d. That is, we create products that are tailored to addressing what customers need to get done.\n\nThe needs to get done are multi-dimensional and they address some of elements as Jonah Berger\u2019s virality elements. That is, beyond just the pragmatic functionality, we need to address dimensions such as the social and emotional needs. So when we build products, we don\u2019t address just functionality. Steve Jobs was a master at this craft where every minute detail of a product was focused on. So Jobs demanded the rendering of fonts on the early Lisa computer despite the objects of engineering staff that focus more on utility.\n\nThe reason I bring this up in the context of a blog that focuses on Deep Learning AI is that we need to better understand how to apply this disruptive technology to the creation of innovative new products. Innovation however is a difficult problem because stuff that\u2019s surprising and novel can also be considered as innovative. Unfortunately, many of the things we consider as \u201ccool\u201d has no effect on our bottom line.\n\nThe Deep Learning canvas focuses on the development of customer products \u201cto get the Job done\u201d. Because it goes beyond plain utility and addresses the social and emotional needs of the \u201chuman in the loop\u201d. The likelihood of the success of the resulting product is therefore much higher. Many products have failed, not just in the marketplace but also inside corporations, despite addressing all the required functionality. It should be plainly obvious, products that people hate, do not become successful. It doesn\u2019t matter if its in the free market or within an organization.\n\nDoes a product then have to be also have viral elements to succeed? Well, why not? We should strive to cover the extra mile of addressing beyond just \u201cthe job that needs to done\u201d. Let\u2019s look at the three other elements: Trigger, Public and Stories. The commonality of these three elements is that they pertain to availability and accessibility.\n\nMore on this in this book:\n\nUse the code \u201ctwentyoff\u201d to get $20 off the list price."
    },
    {
        "url": "https://medium.com/intuitionmachine/black-magic-and-alchemy-with-deep-learning-b0c733a14165",
        "title": "The Black Magic and Alchemy of Deep Learning \u2013 Intuition Machine \u2013",
        "text": "The practice of Deep Learning is vastly outpacing theory. This is despite the incredible number of Deep Learning papers that are published every day on Arxiv. To develop good theoretical results, researchers have to settle with simplified models that are tractable with our current investigative tools. More advanced models that use the latest state-of-the-art techniques are at a level of complexity that are beyond our current mathematical toolbox to understand.\n\nThe practice therefore of Deep Learning, despite all the heavy math that is employed, is actually more like alchemy than that of chemistry. In other words, we don\u2019t build solutions with much of a solid foundation that can give us good predictability on how effective the results may become.\n\nCertainly, there are many rules of thumb (or Design Patterns) that we\u2019ve learned through experience. This investigative intuition is learned by practitioners over time and you can find bits and pieces of \u2018black magic\u2019 that people have used to get better performance.\n\nThe down side of this magic is that many research results of state-of-the-art are indeed questionable. I don\u2019t have the numbers, but I estimate that a majority of papers that are published on Arxiv that claim \u201cstate-of-the-art\u201d results are indeed difficult to replicate due to (1) the lack of specifics on what magic (i.e. hyper-parameters etc) was used and (2) the lack of a released implementation that others can verify.\n\nDeep Learning is at best an experimental science. Do not let all the mathematics fool you into believing that the theorists have a handle of what is going on. The truth of the matter is that we are continually caught by surprised as to what Deep Learning is capable of doing. Furthermore, in almost all cases, theorists have barely an explanation as to what is going on. This is the big unknown and the experimentalists are leading us into that frontier without a roadmap!\n\nThis is very different from our understanding of computer circuitry. Despite the complexity of software and hardware of these systems, we have a very precise understanding of how they work. Furthermore, we don\u2019t expect software developers to understand the quantum mechanics of semiconductor transistors to be able to build stuff.\n\nHowever, in stark contrast even understanding how linear algebra, activation functions and back-propagation works does not give us enough of an understanding how emergent behavior arises. The complexity scientists likely have better models. That\u2019s not to say that Deep Learning researchers don\u2019t know anything. There certainly a lot of good approximate theories out there that we employ to reason about what we are building. That experimental intuition is what is driving the outstanding research we are seeing today. I honestly think though that Deep Learning practitioners have a better understanding of how the brain works (despite not working with brains or using biologically cartoonish models) than the neuro-scientists.\n\nReputable science magazines have published recently articles that express this sentiment of how little we know about Deep Learning. MIT Technology Review published \u201cThe Dark Secret at the Heart of AI\u201d, with this conclusion:\n\nThe article unfortunately conflates many ideas of the inscrutability of Deep Learning networks. Two things to make clear to the reader (1) We don\u2019t know how Deep Learning works and (2) when it makes a prediction, we don\u2019t have an explanation why it arrived at that prediction. That is just scratching the surface as to how little we understand! To make it worse, with the deluge of new experimental results from research, despite gaining some more understanding, we are discover more mechanisms that we don\u2019t understand. In short, the acceleration of our understanding of Deep Learning is being surpassed by the accelerated discovery of new capabilities!\n\nFortunately, a few magicians have been brave enough to break the magicians code and reveal some of the magic and secret potions that is involved. Here we will describe a few valuable \u2018tricks of the trade\u2019.\n\nNote to reader: I will clean this up when I have more time.\n\nA lot of the documented magic is confined to the image processing space. However, trust me, when I say to you, that the magic that is used in the NLP space is even crazier!"
    },
    {
        "url": "https://medium.com/intuitionmachine/cognitive-architectures-ea18127a4d1d",
        "title": "Cognitive Architectures \u2013 Intuition Machine \u2013",
        "text": "General human-level artificial intelligence, or AGI, has certain specific requirements. These cannot be met by current machine learning/ deep learning approaches alone, nor by any narrow techniques.\n\nWhat we need is a cognitive architecture (or autonomous agent) approach. However, even this by itself is not enough: essential cognitive mechanisms (such as knowledge & skill representation, short-term memory & context, reasoning & planning, perception & action, Focus & selection, metacognition, etc.) need to be tightly integrated. The standard engineering approach of separate (and disparate) modules cannot achieve this objective.\n\nPat Langley has an excellent essay in which he highlights differences between the current mainstream approaches to AI, and what he calls the \u2018Cognitive Systems Paradigm\u2019:\n\nQuote: \u201cIn this essay, I review the motivations behind the cognitive systems movement and attempt to characterize the paradigm. I propose six features that distinguish research in this framework from other approaches to artificial intelligence, after which I present some positive and negative examples in an effort to clarify the field\u2019s boundaries.\n\nHere\u2019s my summary. interpretation of these points:\n\n1. High-Level Cognition: Abilities such as abstract reasoning, deep comprehension, goal-directed planning, and problem solving. Contrast this with (statistical) pattern recognition, classification, and prediction \u2014 what is known as machine learning.\n\n2. Structured Representations: All knowledge and skills (from perception/ action all the way to symbols) should be encoded in a uniform, integrated manner way that reflects the logic structure of the data, and is somewhat scrutable. Mainstream AI approaches such as machine learning and databases tend to not be uniform, integrated, or to match the structure and flexibility of the data.\n\n3. System-Level Approach: Comprehensive, integrated cognitive architectures provide seamless integration between various cognitive functions \u2014 these functions are thus mutually supportive. Current AI approaches tend to take the opposite approach, combining separate, specialized modules via pipelines or narrow APIs.\n\n4. Heuristics and Satisficing: Systems should be designed to be practical with incomplete (or even incorrect) information, and with limited time or computing resources. Generally, learning and reasoning systems that are not designed as interactive, real time, cognitive architectures are unable to cope with ambiguity and severe time constraints (e.g. immediate learning)\n\n5. Links to Human Cognition: While AI designs do not have to copy how brains achieve intelligence, crucial characteristics of human cognition must be implemented in a successful, practical intelligence engine. These include the ability to handle ambiguity, abstract conceptualization and reasoning, short-term memory and context, as well as meta-cognition. Typical machine learning, conversational agents, and robotics does not generally implement these features.\n\n6. Exploratory Research: Ideally, research towards general-purpose, human-level AI should be ruthlessly focused on general aspects of intelligence, and not specific, narrow applications or algorithms. At the same time, R&D should be guided by practical experimental results and not overly by academic or theoretical consideration \u2014 i.e. more engineering than theory."
    },
    {
        "url": "https://medium.com/intuitionmachine/how-algorithms-and-authoritarianism-created-a-corporate-nightmare-at-united-92d9bbdf1144",
        "title": "How Algorithms and Authoritarianism Created a Corporate Nightmare at United",
        "text": "\u201cI was only following corporate algorithms\u201d\n\nTestimony given at a future war crimes trial (riff on the Nuremberg defense)\n\nUnited Airlines forcibly removed a man from an \u201coverbooked\u201d flight. The incident was captured on video by other passengers and the story went viral on the social networks. United flubbed its response to incident, adding fuel to the anger. The story went global overnight, sparking massive outrage (hundreds of millions of views in China, an important market for United). The next day, United stock gets hammered, losing ~$1.4 billion off its stock price by midday. What happened? This incident is a pretty good example of how rigid algorithmic and authoritarian decision making can create corporate disasters in an age dominated by social networking.\n\nHere\u2019s how the algorithmic decision making created the incident on United.\n\nHere\u2019s how authoritarian decision making (common on modern air travel) made things worse. Note how this type of decision making escalates the problem rapidly.\n\nWhat this means for Organizations\n\nAs you can see, United was designed to fail in a world connected by social networking and they are not alone. Let\u2019s recap. United employees blindly followed the decision making of algorithms up to the point of telling seated passengers to deplane. The authoritarian decision making that followed was just as rigid and unyielding. Disobeying orders of the flight crew led to the police. Disobeying the police led to forced removal. Finally, the public failure of this process led United\u2019s CEO to praise employees for their rigid adherence to algorithmic and authoritarian decision making. The entire process was inevitable. It\u2019s also not a unique situation. We\u2019re going to see much more of this in the future as algorithms and authoritarianism grow in America. Here\u2019s how organizations are likely to respond:"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-deep-learning-roadmap-f0b4cac7009a",
        "title": "The Deep Learning Roadmap \u2013 Intuition Machine \u2013",
        "text": "It just occurred to me, that after a couple of years tracking Deep Learning developments, that nobody has even bothered to create a map of what\u2019s going on! So I quickly decided to come up with a Deep Learning roadmap. A word of warning, this is just a partial map and doesn\u2019t cover the latest developments. Many of the ideas I write on this blog isn\u2019t even covered by this map. Anyway, here\u2019s a start of this and hope people start coming out of their labs to further expand on it.\n\nThe \u201cUnsupervised Learning\u201d part is from a talk by Russ Salakhutdinov and the \u201cReinforcement Learning\u201d part is from a talk by Pieter Abbeel.\n\nThere are a ton of other ideas that are coming out of the edges as well as the center of this diagram. Also, I did not show the connections between the 3 center concepts. For example, you can use CNNs for Value Iteration and GAN and VAEs use DL networks. It\u2019s a wild world in the Deep Learning space and you just never know how all of this gets re-arranged.\n\nI\u2019ve got a higher level map that starts off with this:\n\nthat possibly can stitch everything together in one \u201cgrand unified theory\u201d. This is how I think it will all play out:\n\nUnsupervised learning is the the \u2018dark matter\u2019 where we need a lot more clarity. It\u2019s my conjecture that meta-learning (with context) is the approach to this. There is some evidence that is developing, but I cannot know for sure. Modular Deep Learning is already in the cards. There is sufficient evidence that this works well. Market Driven Coordination is still early stages, but I believe that the only real way forward is to have diverse architectures working on the same problem and \u201cmarkets\u201d are a known decentralized way to coordinate actions.\n\nThere\u2019s still a lot to be done though and we just in the early stages of Deep Learning evolution:\n\nOne additional key issue outside of unsupervised learning is the need to bridge the semantic gap between connectionist and symbolic architectures.\n\nIf you think there\u2019s a demand for more clarity in Deep Learning, then support this kind of effort by buying the \u201cDeep Learning Playbook\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/introducing-the-deep-learning-canvas-a2e80a998f11",
        "title": "Introducing the Deep Learning AI Canvas \u2013 Intuition Machine \u2013",
        "text": "One of the big mysteries of Deep Learning is, how do we apply this disruptive new AI technology to improving our businesses? There are plenty of questions that are quite open to be able to answer this question. Which business process shall I apply Deep Learning to? Is it even feasible to apply Deep Learning to my selected context? Will it be worth the effort? Are my expectations of A.I. unrealistic? How exactly will I go about implementing this?\n\nThese are just a few of the questions that will pop up. It is indeed a complex subject and we definitely are in dire need of some kind of framework where we can organize our thinking around this. That\u2019s what a this canvas is for.\n\nComing soon to an internet bookseller near you!"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-first-rule-of-agi-is-bc8725d21530",
        "title": "The Next AI Milestone: Bridging the Semantic Gap \u2013 Intuition Machine \u2013",
        "text": "John Launchbury of DARPA has an excellent video that I recommend everyone watch ( viewing just the slides will give one a wrong impression of the content). The video distills the current state of AI into 3 waves.\n\nHandcrafted Knowledge \u2014 Where programmers craft sets of rules to represent knowledge in well\u2010defined domains\n\nStatistical Learning \u2014 Where programmers create statistical models for specific problem domains and train them on big data.\n\nContextual Adaptation \u2014 Where systems construct contextual explanatory models for classes of real world phenomena.\n\nIt\u2019s a bit of a simplified presentation because it lumps all of machine learning, Bayesian methods and Deep Learning into a single category. There are many more approaches to AI that don\u2019t fit within DARPA\u2019s 3 waves.\n\nPedro Domingos author of the \u201cThe Master Algorithm\u201d. He talks about the 5 Tribes of AI: Connectionists, Symbolists, Evolutionaries, Bayesians and Analogizers ( I discuss something like 17 tribes of AI).\n\nBut let\u2019s give DARPA the luxury of simplifying their presentation of the current state of the field.\n\nThey do cover some of the known problems of Deep Learning such as adversarial features. Unfortunately, from this point is where the press took off with their presentation and ran with it. Creating titles like \u201cUnderstanding the Limits of Deep Learning\u201d (VentureBeat).\n\nThe key slide though out of the presentation is this:\n\nDARPA\u2019s third wave model takes a lot of inspiration from some of their previously announced research initiatives such as Explanatory interfaces and Meta-Learning. I write about these two in previous articles (see: \u201cThe Only Way to Make Deep Learning Interpretable is to have it Explain Itself\u201d and \u201cThe Meta Model and Meta Meta Model of Deep Learning\u201d DARPA\u2019s presentation nails it, by highlighting what\u2019s going on in current state-of-the-art research. If anyone is seeking out a short explanation of what\u2019s going on in the field, then this is the video to watch.\n\nThe main problem that we face today is bridging the semantic gap between what I would call Artificial Intuition and rational (symbolic) machines. Deep Learning systems have flaws analogous to our own intuitions having flaws. When you have cognitive processes that have limits on memory and time in the context information overload and lack of meaning, then you are bound to have flaws. These flaws are however caught by logical systems. That\u2019s why bridging the gap can have some profound effects.\n\nOne reason that the semantic gap wasn\u2019t address as vigorously before was that Connectionist systems (i.e. Artificial Neural Networks) did not historically work well. With the advent of Deep Learning, there\u2019s a new emphasis in finding a solution that melds Symbolic and Connectionist systems. That\u2019s where a lot of research is chipping away at the problem. The excitement here though is that it appears that the researchers are making outstanding progress!\n\nJust to recap, here\u2019s the roadmap that I have ( explained here ):\n\n3. Classification with Knowledge (CK) \u2190- This is what DARPA is talking about.\n\nOne disclaimer though regarding this roadmap. It\u2019s a Deep Learning roadmap and does not cover developments in other AI fields. One point however that I want to make here is that, achieving the 3rd wave is likely to be an evolution of how we do Deep Learning.\n\nI suspect the solution to this problem will have something to do with \u201cLate Binding\u201d. It is very impressive that DARPA actually chose a very precise phrase to describe this (i.e. Contextual Adaptation). Which does imply adapting behavior depending on context.\n\nA question that is very important to ask is \u201chow far away are we from bridging the semantic gap?\u201d Given the brisk pace of DL development. Understand here for a moment that some of the techniques mentioned in the video (i.e. few-shot learning and generative models) are being refined in the last year or so. I would not be surprised that within 2\u20133 years that the gap will be bridged! That\u2019s just how crazy the developments in Deep Learning are."
    },
    {
        "url": "https://medium.com/intuitionmachine/googles-ai-processor-is-inspired-by-the-heart-d0f01b72defe",
        "title": "Google\u2019s AI Processor\u2019s (TPU) Heart Throbbing Inspiration",
        "text": "Google has finally released the technical details of its its Tensor Process Unit (TPU) ASIC. Surprisingly, at its core, you find something that sounds like its inspired by the heart and not the brain. It\u2019s called a \u201cSystolic Array\u201d and this computational device contains 256 x 256 8bit multiply-add computational units. That\u2019s a grand total of 65,536 processors capable of cranking out 92 trillion operations per second! A systolic array is not a new thing, it was described way back in 1982 by Kung from CMU in \u201cWhy Systolic Architectures?\u201d Just to get myself dated, I still recall a time when Systolic machines were all the rage.\n\nUnlike other computational devices that treats scalar or vectors as primitives, Google\u2019s TPU treats matrices as primitives. The TPU is designed to perform matrix multiplication at a massive scale. If you look at the diagram above, you notice that that the device doesn\u2019t have high bandwidth to memory. It uses DDR3 with only 30GB/s to memory. Contrast that to the a Nvidia Titan X with GDDR5X hitting transfer speeds of 480GB/s. The systolic array trades off speed for throughput. A Titan X has 3,583 CUDA cores. The CUDA cores are 32bit and are more general purpose than 8bit cores of the TPU. Apparently, Google knew likely way back in 2014 that 8bit was good enough (note: Google had deployed TPU as early as 2015).\n\nSystolic arrays are heavily pipelined, given that it is 256 units wide, it takes 256 cycles from the time the first element gets into the array to the time it comes out. Twice that many cycles for everything that needs to get it, to all come out. However, at its peak, you\u2019ll get 65k processors all cranking together. Here\u2019s a slide that shows how a systolic array performs matrix multiplication:\n\nNotice how the how the matrix elements have to be staggered as they work their way into the array. This lecture by Onur Mutlu of CMU explains Systolic Arrays:\n\nStart watching at 1:25:48. A nice coincidence that the speaker uses convolution as an example of a systolic array application.\n\nAnother interesting thing about the TPU is that its DDR3 memory seems to be used exclusively for weights. Instructions and data come through the PCIe interface. It surely doesn\u2019t use all of its DDR3 memory for a single DL network. What it may appear to be doing is that it could be switching context between different DL networks. You can find some explanation of this design here: \u201cOutrageously Large Neural Network\u201d.\n\nAnyway, very interesting architecture, unfortunately it works well only for inference and not for training. However, you just never know if Google already has built something that can also work well for training.\n\nHere\u2019s the floor plan for the device:\n\nASIC development is a high risk and expensive proposition. Google had no issue deciding on implementing this in 2014 because, not only did it have the money, but rather they had a captive user base that would have a use for this. However, this design can easily be copied by the many wanna-be AI chip vendors.\n\nWe could see the same thing that happened in the world of Bitcoin. In that field, miners rapidly moved from CPU; to GPU; to FPGA and then ASICs. The big leaps in performance occurred in the transitions from CPU to GPU and from FPGA to ASICs. The FPGA route was interesting and served as a good test-phase for ASICs but the gains were negligible. So those betting on FPGAs to bring value to Deep Learning (i.e. Xilinx and Intel), they should sell their longs and go short. It\u2019ll only be useful for niche and novel use-cases, in short, markets too small to care about.\n\nThe design is something that\u2019s going to be replicated because everyone will want a low powered DL component. It doesn\u2019t require high memory bandwidth. It\u2019s a CISC device so the instructions sets are quite compact requiring an average of 10\u201320 cycles per instruction, with only a dozen (12) instructions (hint: microcode folks, looks like there\u2019s future opportunity here!). 29% of the die is memory, 24% of it is all 8 bit MACs so the semiconductor design isn\u2019t rocket science.\n\nIn typical Google fashion, that is use simple hardware and crack the problem in software, the secret sauce here is in the software. To pull this off, you have to have an intimate understanding of Deep Learning workloads. How else do you figure out what CISC instructions are important? You also need a bit of compiler design, because apparently the host CPU seems to be doing a lot of prep work to align data and manage results.\n\nASIC systolic arrays are going to flood the market with likely Chinese government subsidized hardware manufacturers cranking this stuff out. It just takes someone to come up with an open source reference design. Just like what happened in the Bitcoin world. This stuff is ancient design stuff, and I\u2019m sure someone has the old iWarp designs and compilers in their basement somewhere!\n\nWell, before you go, make sure you \u201cheart\u201d this post!\n\nUpdate 3: Nvidia just unveiled their V100https://devblogs.nvidia.com/parallelforall/inside-volta/?ncid=so-lin-vt-13919 with 120 trillion operations per second for matrix-matrix multiplications.\n\nUpdate 4: Google just announced their 2nd generation TPU ( https://cloud.google.com/tpu/ ) with 180 trillion ops per second and 64GB of ultra high bandwidth ram."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-peculiar-behavior-of-deep-learning-loss-surfaces-330cb741ec17",
        "title": "The Two Phases of Gradient Descent in Deep Learning",
        "text": "Thanks to great experimental work by several research groups studying the behavior of Stochastic Gradient Descent (SGD), we are collectively gaining a much clearer understanding as to what happens in the neighborhood of training convergence.\n\nThe story begins with the best paper award winner for ICLR 2017, \u201cRethinking Generalization\u201d. This paper I first discussed several months ago in a blog post \u201cRethinking Generalization in Deep Learning\u201d. One interesting observation in that paper is the role of SGD. The observation is extremely radical, where the authors write:\n\nThis is a very odd notion that SGD is being labeled as an \u2018implicit regularization\u2019. Coincidentally, another paper: An Empirical Analysis of Deep Network Loss Surfaces by Daniel Jiwoong Im, Michael Tao, Kristin Branson, discusses the structure of loss surfaces of different SGD algorithms and discovers that they all are different:\n\nThese experimental measurements seems to back the claim that similar to regularization, the SGD algorithm you select will influence where a network converges to. In short, you reach different resting placing with different SGD algorithms. This is different to how we conventionally think about SGD. That is, different SGDs just give you differing convergence rates due to different strategies, but we do expect that they all end up at the same results! We sort of believe that SGD would reach the same optima irregardless of method.( BTW, I mention that this fantastic paper was rejected in ICLR 2017. Writing academic papers in the deep learning space is unreasonable competitive. )\n\nLeslie Smith and Nicholay Topin, recently submitted a workshop paper to the ICLR 2017 workshop: \u201cExploring Loss Function Topology with Cyclic Learning Rate\u201d where they discover some peculiar convergence behavior:\n\nHere, as you monotonically increase and decrease the learning rate, there is a transition near at the convergence regime that a large enough learning rate perturbs the system right off is basin into a space of much higher loss. Then the SGD again quickly converges ( also note the faster convergence rate ). What exactly is happening here?\n\nA recent paper on Arxiv \u201cOpening the Black Box of Deep Neural Networks via Information\u201d by Ravid Shwartz-Ziv and Naftali Tishby has a elegant interpretation of what goes on in SGD. They describe SGD as having two distinct phases, a drift phase and a diffusion phase. SGD begins in the first phase, basically exploring the multidimensional space of solutions. When it begins converging, it arrives at the diffusion phase where it is extremely chaotic and the convergence rate slows to a crawl. An intuition of what\u2019s happening in this phase is that the network is learning to compress. This graph best illustrates this behavior:\n\nThat is, the behavior makes a phase transition from high mean with low variance to one with a low mean but high variance. This provides further explanation to Smith et. al\u2019s observations, that in the region near convergence, it is highly chaotic. This of course does not fully explain why a high learning rate will knock the system into a place of high loss.\n\nTomaso Poggio and Qianli Liao have however their own experiments and have a theory: \u201cTheory II: Landscape of the Empirical Risk in Deep Learning\u201d. Where they describe in detail the behavior in that chaotic region:\n\nIt turns the basin of global minima is flat but has very bumpy. Not only that, but there are many of these basins. They invoke some esoteric math theorem and come up with this conclusion:\n\nAbsolutely fascinating paper, worthy of several reads. There is however one pragmatic take away from this paper \u201cAveraging two models within a basin tend to give a error that is the average of the two models (or less). Averaging two models between basins tend to give an error that is higher than both models\u201d.\n\nThere remains many questions on how to exploit this new knowledge. How can we leverage this for critical capabilities such as transfer learning, domain adaptation and avoiding forgetting? What is the relationship of these phases, particularly the compression phase with respect to generalization? There certainly a lot of intriguing avenues here!\n\nIn summary, there are a lot of research groups that make a good effort at trying to better understand the behavior of Deep Learning systems. It is through this fundamental research work that we all collectively gain better ways to improve our own work. Conferences unfortunately have the bias towards valuing novel architectures (the crazier the better) over good experimental data. Unfortunately, that favors the practice of alchemy rather than the pursuit of the science of chemistry.\n\nUpdate: Tomaso Poggio has his Theory III out: http://cbmm.mit.edu/sites/default/files/publications/CBMM-Memo-067.pdf"
    },
    {
        "url": "https://medium.com/intuitionmachine/biologically-inspired-software-architecture-for-deep-learning-e64db295bb2f",
        "title": "Biologically Inspired Software Architecture for Deep Learning",
        "text": "With the emergence of Deep Learning as the dominant paradigm for Artificial Intelligence based systems, one open question that seems to be neglected is \u201cWhat guidelines do we have in architecting software that uses Deep Learning?\u201d If all the innovative companies like Google are on a exponential adoption curve to incorporate Deep Learning in every thing they do, then what perhaps is the software architecture that holds this all together?\n\nThe folks at Google wrote a paper (a long time ago, meaning 2014), \u201cMachine Learning: The High-Interest Credit Card of Technical Debt\u201d that enumerates many of the difficulties that we need to consider when building software that consists of machine learning or deep learning sub-components. Contrary to popular perception that Deep Learning systems can be \u201cself-driving\u201d. There is a massive ongoing maintenance cost when machine learning is used. In the Google paper, the authors enumerate many risk factors, design patterns, and anti-patterns to needs to be taken into consideration in an architecture. These include design patterns such as : boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies and changes in the external world.\n\nThe Google \u201cTechnical Debt\u201d article should be required reading for anyone involved in operationalizing Deep Learning systems. For easy reference and to aid in discussion, lets detail the important risk factors and design patterns in that paper.\n\nSoftware architect involves patterns to ensure software code are modular and thus have minimal dependencies with each other. By contrast, Deep Learning systems (applies equally to machine learning), code is created from training data. This is the key difference between classical software and Deep Learning systems\n\nThere are few mechanisms to separate data dependencies in data.\n\nEntanglement \u2014 Deep learning models and the data used to train them are naturally entangled.\n\nHidden Feedback Loops \u2014 Systems that learn from the world are in a feedback loop with its actions and its observations.\n\nUndeclared Consumers \u2014 Predictions made by a machine may be used by other systems.\n\nData dependencies have greater importance unfortunately it is far less common to find tools to discover data dependencies.\n\nUnstable Data Dependencies \u2014 Data behavior inevitably changes over time. A mitigation strategy is to use versioned copies of data.\n\nUnderutilized Data Dependencies \u2014 Regularly evaluate the effect of removing features from a model whenever possible.\n\nStatic Analysis of Data Dependencies \u2014 Annotate data and features to allow automatic dependency checking.\n\nCorrection Cascades \u2014 Using models in a domain different from its original domain. Annotate the model to allow inspection of its original use.\n\nGlue Code \u2014 Only 5% of the code is machine learning, 95% of code is glue code and thus should be treated with conventional software architecture approaches.\n\nPipeline Jungles \u2014 Invest engineering resources so that maintaining pipelines ( code for data collection and feature extraction) can be made sustainable.\n\nDead Experimental Codepaths \u2014 A famous example of this was Knight Capital\u2019s system losing $465 million in 45 minutes dues to an obsolete experimental codepath.\n\nConfiguration Debt \u2014 Machine learning algorithms can be elegant, but a lot of real world messiness can be found in their configuration.\n\n4. Changes in the External World\n\nThe world is rarely stable and therefore these system need to be adaptive.\n\nFixed Threshold in Dynamic Systems \u2014 This applies to classical machine prediction models where arbitrary thresholds are defined rather than learned from the data.\n\nWhen Correlations no longer Correlate \u2014 Models that assume correlation may break when the correlations no longer hold.\n\nMonitoring and Testing \u2014 Live monitoring of behavior is critical.\n\nAs you can see, the problems are vast and the solutions are quite limited. However, as we explore newer architectures (i.e. \u201cModular Deep Learning\u201d and \u201cMeta-Learning\u201d) we can begin to seek out newer solutions. A good inspiration, that I stumbled upon can be found in this insightful blog (Scientific American) that describes \u201cBuilding a Resilient Business Inspired by Biology\u201d. The author describes 6 features found in biology and applied it to business processes. I will take the same approach and see how it may apply to Deep Learning systems.\n\nThese 6 features are excellent guidelines on how to build not only adaptable systems, but one\u2019s that are ultimately sustainable. It is important to note the importance of \u201cloose coupling\u201d in biology.\n\nA recent paper from the folks at Berkeley are exploring the requirements for building these new kinds of systems (see: \u201cReal-Time Machine Learning: The Missing Pieces\u201d). The project is Ray from Berkeley\u2019s RISELab, although they don\u2019t mention it in their paper. They make the argument that systems are increasingly deployed in environments of \u201ctightly-integrated components of feedback loops involving dynamic, real-time decision making.\u201d and thus requires \u201ca new distributed execution framework\u201d. The difference between the classical machine learning system and their new framework is depicted by this graphic:\n\nThe authors spell out 7 requirements that this new kind of architecture needs to support:\n\nWhat was just described was the state-of-the-art thinking in design. Clearly, we have a very long way to go in terms of architectures that are adaptive to the environment. Although the prescription does address other aspects such as heterogeneity, redundancy and modularity.\n\nPresent day software architectures are clearly not up to the task in accommodating systems that employ Deep Learning components. A new kind of architecture is clearly demanded. It is very early, but this is a very important area and it is essential that our Deep Learning systems have manageability built in. After all, every complex technology requires manageability to be economically sustainable.\n\nIt has come to my attention that DARPA has a new program \u201cToward Machines that Improve with Experience\u201d that \u201cseeks to develop the foundations for systems that might someday learn in much the way biological organisms do\u201d:\n\nFor more on this, read \u201cThe Deep Learning Playbook\u201d"
    },
    {
        "url": "https://medium.com/intuitionmachine/openais-learning-to-invent-language-40b24f808e53",
        "title": "Deep Learning to \u201cInvent Language\u201d \u2013 Intuition Machine \u2013",
        "text": "OpenAI research has a short introduction on their newest research \u201cLearning to Communicate\u201d. There are many trends that I watch for in the field of Deep Learning. Two trends that are related and I believe going to be very promising areas are language learning and multi-agent communication. If you have not been watching, this week has had a tremendous release of papers involving the former and culminating with OpenAI\u2019s post, stitching it all together! Let me explain though what transpired in this amazing week.\n\nDenny Britz (he says he\u2019s a high school dropout working for Google), released some a new general-purpose encoder-decoder framework for TensorFlow. This is all described in more detail in a paper: Massive Exploration of Neural Machine Translation Architectures. To quickly summarize, a team at Google cranked through 250,000 GPU hours ( at $.70 per hour at GoogleCloud rates, that will setback a poor researcher $175,000, life is never fair!) training different English-German translation networks to come up with some important insight as well as some nice hyper-parameters. This is one nice gift from Google to the Deep Learning community.\n\nNot to be out done, another group at Google introduced something that goes even beyond the classic encoder-decoder design. They introduced something they christened as DRAGNN. I must say, I was initially put off by the click-bait like title, but this is one impressive piece of work that lives up to its name! As you read the paper, you realize quickly that this is a very different architecture. A sense of fear overcame me, worrying that to replicate this kind of work would take an enormous amount of effort. Fortunately, a day later, I was alerted to a post from Google\u2019s Research Blog with a very simple title: \u201cAn Upgrade to SyntaxNet, New Models and a Parsing Competition\u201d. SyntaxNet more popularly known as \u201cParsey McParseface\u201d, introduced something new called \u201cParseySaurus\u201d. Read up on ParseySaurus and you realize the \u201clarge beast\u201d connection.\n\nLet\u2019s explore why I think DRAGNN is important. You see, I am exploring this idea of \u201cModular Deep Learning\u201d. That is, the concept that you can have a modularized version of Deep Learning and that you can stitch them together to build solutions. I explored this a bit more in a previous article \u201cLearning to Coordinate\u201d, where I had a rough survey of what perhaps may be needed to make further progress. I came to this conclusion that something that is an intermediary between Deep Learning modules may be necessary, but I really didn\u2019t know of anything in the field that approximated this. Well, DRAGNN\u2019s TBRU (Transition Based Recurrent Unit) seems to somewhat fit the bill! Granted that its designed for NLP translation, however as described by the authors:\n\nSo not only does this research have a way to stitch together networks, it allows more expressive intermediate language representations (i.e. compositional tree-structured models) to be used. This is really just great stuff, right at the nick of time. One general perspective of what a Deep Learning system can do is that it can perform universal translation. Conventionally, DL are thought of as universal classifiers, however we can think more generally if of them as universal translators (i.e. Babelfish). From this perspective, a lot of more clever applications become more evident. I\u2019ve come to the opinion that Deep Learning from the language driven perspective is the most fruitful way of thinking about it. (Remind me to make changes to 5 Levels of Capability).\n\nTo end this week, we\u2019ve been gifted by OpenAI with their research on \u201cLearning to Communicate\u201d (they are supposed to be \u201cOpen\u201d so hope to see source code soon!). They developed an RL system on the constraint that languages that are useful are both grounded and compositional. Grounded meaning that the words in the language have meaning. Compositional in that the words can be strung together to create more specific instructions. The system they setup is a cooperative multi-agent system ( DeepMind did some work on competitive systems). The key technical mechanism that the researchers came up was a \u201cdifferentiable communication channel\u201d that used the Gumbel-Softmax to treat the communication as consisting of categorical variables (one nice trick to remember).\n\nIn other multi-agent models that we looked at, what was learned was the behavior of each agent, however the communication mechanisms remained opaque and uninterpretable. In OpenAI\u2019s research, they are sort of constraining the communication channel in a way that is more language like rather some continuous stream of data. So it\u2019s really getting very interesting that, not only are frameworks being developed that are beginning to better understand sequences of tokens, but we are exploring ways to learn how to invent language.\n\nWhat a week in Deep Learning! As dessert, three papers came to my attention regarding \u201cDeep Meta-learning\u201d. \u201cModel-Agnostic Meta-Learning for Fast Adaptation of Deep Networks\u201d, \u201cLearning Gradient Descent: Better Generalization and Longer Horizons\u201d and \u201cLearned Optimizers that Scale and Generalize\u201d. Finally, one cool simple trick to improve gradient descent called Hyper-gradients!"
    },
    {
        "url": "https://medium.com/intuitionmachine/on-deep-learning-virtual-machines-153168fa144b",
        "title": "One Deep Learning Virtual Machine to Rule Them All \u2013 Intuition Machine \u2013",
        "text": "The current state of Deep Learning frameworks is similar to the fragmented state before the creation of common code generation backends like LLVM. In the chaotic good old days, every programming language had to re-invent its way of generating machine code. With the development of LLVM, many languages now share the same backend code. Many programming languages use LLVM as their backend. Several well known examples of this are Ada, C#, Common Lisp, Delphi, Fortran, Haskell, Java bytecode, Julia, Lua, Objective-C, Python, R, Ruby, Rust, and Swift. The frontend code only needs to parse and translate source code to an intermediate representation (IR).\n\nDeep Learning frameworks will eventually need their own \u201cIR\u201d. The IR for Deep Learning is of course the computational graph. Deep learning frameworks like Caffe and Tensorflow have their own internal computational graphs. These frameworks are all merely convenient fronts to the internal graph. These graphs specify the execution order of mathematical operations, analogous to what a dataflow graph does. The graph specifies the orchestration of collections of CPUs and GPUs. This execution is highly parallel. Parallelism is the one reason why GPUs are ideal for this kind of computation. There are however plenty of untapped opportunities to improve the orchestration between the CPU and GPU.\n\nNew research is exploring ways to optimize the computational graph in way that go beyond just single device optimization and towards more global multi-device optimization. An example of this is the research project XLA (Accelerated Linear Algebra) from Google. XLA supports both Just in Time (JIT) or Ahead of Time (AOT) compilation. It is a high level optimizer that performs its work in optimizing the interplay of GPUs and CPUs. The optimizations that are planned include the fusing of pipelined operations, aggressive constant propagation, reduction of storage buffers and fusing of low-level operators.\n\nNot to be outdone, two other open source projects that are also exploring computational graph optimization. NNVM from MXNet (Supported by Amazon) is another computation graph optimization framework that similar to XLA focuses on the need for an intermediate representation. The goal of the NNVM optimizer is to reduce memory and device allocation while preserving the original computational semantics.\n\nNGraph from Intel is also exploring optimizations that include an even more extensive optimizations: kernel fusion, buffer allocation, training optimizations, inference optimizations, data layout and distributed training. There are certainly plenty of ideas of how to improve the performance and the space is heating up with a lot of activity.\n\nIn addition to these approaches that originate from the DL community, other approaches to optimizing machine learning algorithms have been developed by other companies. HP has developed the Cognitive Computing Toolkit (CCT) and IBM has developed SystemML.\n\nHP\u2019s CCT simplifies the development of HPC routines by compiling high-level abstractions down to optimized GPU kernels. Typically, the development of GPU kernels is a laborious process. However, if the algorithms can be expressed using combinations of high-level operators then it should be possible to generate the GPU kernel. This is what CCT is designed to do.\n\nAn offshoot of CCT is the Operator Vectorization Library (OVL). OVL is a python library that does the same a CCT but for TensorFlow framework. Custom TensorFlow operators are written in C++. OVL enables TensorFlow operators to be written in Python without sacrificing performance. This improves productivity and avoids the cumbersome process of implementing, building, and linking custom C++ and CUDA code.\n\nIBM\u2019a SystemML is a high-level declarative language with an R-like and Python-like syntax. Developers express machine learning algorithms using these declarative language. SystemML takes care of generating the execution plan. The system supports optimizations on single nodes as well as distributed computations on platforms like Hadoop and Spark. Rule-based and cost-based optimization techniques are used to optimize the execution plan. SystemML comes with an existing suite of algorithms that include Descriptive Statistics, Classification, Clustering, Regression, Matrix Factorization, and Survival Analysis.\n\nThese five open source projects (XLA, NNVM, NGraph, CCT and SystemML) all perform optimizations in a global manner across either the computational graph or an alternative declarative specification. The current DL frameworks however have code generation and execution all intertwined with their code base, making opportunities to develop optimization solutions less portable. Ideally, one would like to see a common standard, a DL virtual machine instruction set, where the community can collective contribute optimization routines. Right now however is is a competitive race to become the ruling standard. That is, one computational graph to rule them all. Can we not all band together for the common good?\n\nA common standard deep learning virtual machine is a futuristic dream. One obvious idea is to leverage deep learning itself to optimize its own computations. That is, deep learning to \u201cLearn to Optimize\u201d itself. There have been current Meta-level research that are exploring this self-improvement approach, however, not at the level of fine-grained memory access and assembly level code.\n\nP.S. Should have include https://weld-project.github.io/ in this list."
    },
    {
        "url": "https://medium.com/intuitionmachine/artificial-intuition-3418fac2eb9c",
        "title": "Artificial Intuition \u2013 Intuition Machine \u2013",
        "text": "Despite a lot of marketing talk \u2018Cognitive Computing\u2019, ANNs are in many ways Artificial Intuition than intelligence per se. They are able to creatively fill in gaps and make intuitive leaps to make an appropriate response to a given situation (System 1 thinking a la Kahneman).\n\nThey are so powerful that they can in effect take over any human activity that takes no longer than one second, or a series of moments/loops like that (for driving a car, recognising faces, reading handwriting, understanding and labelling objects in a scene).\n\nReinforcement learning is able to create AI which will learn about situations but cannot conceptualise them. That is something that we don\u2019t have, and will not have until another revolution in AI \u2014 That could take a few years or a few decades.\n\nSome of the latest developments from MIT are able to combine multiple discrete elements of something, to syncretise something new, which seems promising. OpenCog and Wolfram Alpha are built in a top-down model whereby a system is explicitly taught things, rather than inferring properties from data (bottom-up). In theory this can lead to a more reasoning-type process.\n\nHowever, there is a lot of human grunt work required in building such systems. They are also not optimised. Marcus Hutter\u2019s AIXI design would be a near-ideal AI system, if it could be implemented. Unfortunately, it\u2019s considered computationally non-viable.\n\nThere has been development lately in generalising learning between ANNs, which seems very promising (although it could potentially introduce more biases or misconceptions through the back door), as well as learning from a single example without needing a large curated dataset.\n\nMy hope is that some of the recent leap in bottom-up approaches, and the GPU/FPGAs/ASICs now powering AI systems, can translate to making this top-down reasoning process faster and easier.\n\nTo sum up, machine intelligence can do a lot of creative things; it can mash up existing content, reframe it to fit a new context, fill in gaps in an appropriate fashion, or generate potential solutions given a range of parameters.\n\nOutside of a few potential hints at something deeper, ANNs do not appear to be generating purely original concepts or ideas, or performing abstract reasoning at this time. However, surprisingly few human tasks or roles actually require this kind of mental function. Most people are Cooks, and not Chefs, businesspeople rather than entrepreneurs, and they have not been taught how to reason from First Principles either.\n\nOne area where some kind of reasoning is generally required however is in ethics. This is why a project which I have co-founded, OpenEth.org, is working to create ethical constraint solutions for narrow AI, in this niche but crucial area.\n\nBias in how a machine intelligence perceives something can indeed come from the algorithm, but it can also come from the data. An algorithm will generally be tweaked over time by an engineer to get a better sense out of the data that is available.\n\nHowever, an incorrectly weighted algorithm can reinforce existing biases that lie within data. This means that stereotypes can get reinforced, or implicit discrimination occurs without warrant, where certain individuals are not shown a job ad for example, because they don\u2019t fit the standard pattern of hires. The worse abuses may occur within the justice system, as decision support and probabilistic engines are increasingly being used to calculate things like bail.\n\nIndeed, the engineering of many of the most powerful machine learning algorithms available today is done within a small geographic zone, by demographically similar individuals, and this situation isn\u2019t likely to change much anytime soon.\n\nI\u2019m therefore proud to serve as an advisor to Diversity.AI, an organisation that fights for better, more open, and more accountable use of machine learning. Machines are intended to help liberate us, let\u2019s help ensure they take our society in the right direction."
    },
    {
        "url": "https://medium.com/intuitionmachine/notes-on-the-implementation-densenet-in-tensorflow-beeda9dd1504",
        "title": "Notes on the Implementation of DenseNet in TensorFlow.",
        "text": "DenseNet(Densely Connected Convolutional Networks) is one of the latest neural networks for visual object recognition. It\u2019s quite similar to ResNet but has some fundamental differences.\n\nWith all improvements DenseNets have one of the lowest error rates on CIFAR/SVHN datasets:\n\nAnd for ImageNet dataset DenseNets require fewer parameters than ResNet with same accuracy:\n\nThis post assumes previous knowledge of neural networks(NN) and convolutions(convs). Here I will not explain how NN or convs work, but mainly focus on two topics:\n\nIf you know how DenseNets works and interested only in tensorflow implementation feel free to jump to the second chapter or check the source code on GitHub. If you not familiar with any topics but want to get some knowledge \u2014 I highly advise you CS231n Stanford classes.\n\nUsually, ConvNets work such way:\n\nWe have an initial image, say having a shape of (28, 28, 3). After we apply set of convolution/pooling filters on it, squeezing width and height dimensions and increasing features dimension.\n\nSo the output from the L\u1d62 layer is input to the L\u1d62\u208a\u2081 layer. It seems like this:\n\nResNet architecture proposed Residual connection, from previous layers to the current one. Roughly saying, input to the L\u1d62 layer was obtained by summation of outputs from previous layers.\n\nIn contrast, DenseNet paper proposes concatenating outputs from the previous layers instead of using the summation.\n\nSo, let\u2019s imagine we have an image with shape(28, 28, 3). First, we spread image to initial 24 channels and receive the image (28, 28, 24). Every next convolution layer will generate k=12 features, and remain width and height the same.\n\nThe output from L\u1d62 layer will be (28, 28, 12).\n\nBut input to the L\u1d62\u208a\u2081 will be (28, 28, 24+12), for L\u1d62\u208a\u2082 (28, 28, 24 + 12 + 12) and so on.\n\nAfter a while, we receive the image with same width and height, but with plenty of features (28, 28, 48).\n\nAll these N layers are named Block in the paper. There\u2019s also batch normalization, nonlinearity and dropout inside the block.\n\nTo reduce the size, DenseNet uses transition layers. These layers contain convolution with kernel size = 1 followed by 2x2 average pooling with stride = 2. It reduces height and width dimensions but leaves feature dimension the same. As a result, we receive the image with shapes (14, 14, 48).\n\nNow we can again pass the image through the block with N convolutions.\n\nWith this approach, DenseNet improved a flow of information and gradients throughout the network, which makes them easy to train.\n\nEach layer has direct access to the gradients from the loss function and the original input signal, leading to an implicit deep supervision.\n\nIn the paper, there are two classes of networks exists: for ImageNet and CIFAR/SVHN datasets. I will discuss details about later one.\n\nFirst of all, it was not clear how many blocks should be used depends on depth. After I\u2019ve notice that quantity of blocks is a constant value equal to 3 and not depends on the network depth.\n\nSecond I\u2019ve tried to understand how many features should network generate at the initial convolution layer(prior all blocks). As per original source code first features quantity should be equal to growth rate(k) * 2 .\n\nDespite that we have three blocks as default, it was interesting for me to build net with another param. So every block was not manually hardcoded but called N times as function. The last iteration was performed without transition layer. Simplified example:\n\nFor weights initialization authors proposed use MRSA initialization(as per this paper). In tensorflow this initialization can be easy implemented with variance scaling initializer.\n\nIn the latest revision of paper DenseNets with bottle neck layers were introduced. The main difference of this networks that every block now contain two convolution filters. First is 1x1 conv, and second as usual 3x3 conv. So whole block now will be:\n\nDespite two conv filters, only last output will be concatenated to the main pool of features.\n\nAlso at transition layers, not only width and height will be reduced but features also. So if we have image shape after one block (28, 28, 48) after transition layer, we will get (14, 14, 24).\n\nWhere theta \u2014 some reduction values, in the range (0, 1).\n\nIn case of using DenseNet with bottleneck layers, total depth will be divided by 2. This means that if with depth 20 you previously have 16 3x3 convolution layer(some layers are transition ones), now you will have 8 1x1 convolution layers and 8 3x3 convolutions.\n\nLast, but not least, about data preprocessing. In the paper per channel normalization was used. With this approach, every image channel should be reduced by its mean and divided by its standard deviation. In many implementations was another normalization used \u2014 just divide every image pixel by 255, so we have pixels values in the range [0, 1].\n\nAt first, I implemented a solution that divides image by 255. All works fine, but a little bit worse, than results reported in the paper. Ok, next I\u2019ve implemented per channel normalization\u2026 And networks began works even worse. It was not clear for me why. So I\u2019ve decided mail to the authors. Thanks to Zhuang Liu that answered me and point to another source code that I missed somehow. After precise debugging, it becomes apparent that images should be normalized by mean/std of all images in the dataset(train or test), not by its own only.\n\nAnd some note about numpy implementation of per channel normalization. By default images provided with data type unit8. Before any manipulations, I highly advise to convert the images to any float representation. Because otherwise, a code may fail without any warnings or errors.\n\nDenseNets are powerful neural nets that achieve state of the art performance on many datasets. And it\u2019s easy to implement them. I think the approach with concatenating features is very promising and may boost other fields in the machine learning in the future.\n\nThat\u2019s it for now! I hope this post was somehow helpful for you or point to some interesting ideas. Source code can be found in this repository. Thanks for reading!"
    },
    {
        "url": "https://medium.com/intuitionmachine/gpus-kubernetes-for-deep-learning-part-3-3-automating-tensorflow-deployment-5dc4d5472e91",
        "title": "GPUs & Kubernetes for Deep Learning \u2014 Part 3/3: Automating Tensorflow deployment",
        "text": "[edit] For previous readers, I realized today the git repository shared was broken. Apologies for the potential issues, everything is in order now.\n\nHere we are. After having spent 21min reading how to build a GPU Kubernetes cluster on AWS, 7min on adding EFS storage, you want to get to the real thing, which is actually DO something with it. So today we are going to define, design, deploy and operate a Deep Learning pipeline.\n\nSo what is a Deep Learning pipeline exactly? Well, my definition is a 4 step pipeline, with a potential retro-action loop, that consists of :\n\nNow, what is a good way of modelling an application in Kubernetes?\n\nA good model must be easy to reproduce, which boils down to how good your packaging is. \n\nYou would not be using Ubuntu if there wasn\u2019t a predictable apt utility to install all the things. \n\nCanonical would not spend considerable money and efforts on Ubuntu Core and its Snap Application Containers if it did not believe there was ultimately a benefit to the community on agreeing on a multi-distribution packaging format. \n\nDocker would be a lot less successful as it is if it did not solve a fundamental problem of sharing common ground from devs to ops.\n\nPackaging at scale is a much harder problem than single OS packaging. Canonical solves the packaging of large scale \u201cpet\u201d application with Juju and charms. And Kubernetes solves the problem of large scale \u201ccattle\u201d applications with Helm. Let\u2019s do this for Deep Learning!\n\nThe Deep Learning framework we will use is Tensorflow, Google\u2019s own open source DL solution. As Google also open sourced Kubernetes, it seems only natural to combine these 2 pieces together.\n\nWe will reuse the bits and pieces of previous posts to focus on the Deep Learning so\n\nSo yes, it will be a long process! I hope you have some time. In the end, what we model will look like:\n\nFor what follows, it is important that:\n\nMore advanced instructions are available in the official docs\n\nAny good data story starts with raw data, carefully pre-processed and published to a known location.\n\nImageNet is a massive dataset of images that is used, among other things, to train the Inception model. It is made of a series of files that are managed by imagenet.org, which are not natively compliant with Tensorflow.\n\nGoogle open sourced the code to download and pre process ImageNet a little while ago here. To automate the download, we creates a simple Docker image src/docker/tf-models/Dockerfile that re-uses the tensorflow official image to pull the dataset to ${DATA_DIR}/imagenet.\n\nThis is where the EFS volume you added earlier will come handy, as this will take about 500GB in the end. By the way, on my AWS instances it took about 3 days to download and pre-process it, so be prepared and patient.\n\nTo deploy this, you need\n\nThe process of building a helm chart is pretty simple: you abstract all the configuration, put all the values in a yaml file and all the keys in a set of other yaml templates forming the skeleton of Kubernetes manifests. You add some metadata around it, and voilaaa. See the src/charts folder to get a feel of what it is.\n\nThe first chart we create is the EFS chart, which will connect our EFS storage to Kubernetes. We create a storage.yaml file in src/charts/efs/templates with:\n\nYes, you recognized Go templates!! Now look at the values.yaml file:\n\nPretty easy: you replace the sub sections by dots, apply a camelCase convention on your variables, and you have your first introduction to Helm. If you then use helm preview the output with\n\nyou will generate a Kubernetes manifest like:\n\nSimple!! If you remove the dry-run you\u2019d actually deploy it, but we don\u2019t want to do that now.\n\nYou can also override the default values.yaml file by adding\n\nYou can already start building your own by copying mine and changing the values to adapt to your own environment.\n\nYou get the drill. We prepare templates, we have a value file to instanciate it. Helm mixes both and generates a YAML file that works with Kubernetes. When you helm install, a small server called Tiller in the Kubernetes cluster relays the instantiated manifest to the API for deployment (this Tiller server was created when running helm init. You can create anything Kubernetes can operate. More info on the structure and some tips to start developing in the docs\n\nReview the content of src/charts/dataloader to understand the structure.\n\nNow you can use the Makefile in the charts folder to build this package,\n\nThen you can deploy that on your cluster, and start the download\n\nThe key of customizing this is your ability to prepare a docker image that makes sense in your context.\n\nLook in the repo in the folder src/docker/tf-models for a file called download.sh, and modify it to your needs.\n\nNote that you can use if needed the version 0.11, which I made because of some compatibility problems due to the breaking transition between 0.11 and 0.12 that are still not fixed in 1.0.0 for distributed training.\n\nThen push that to your repo\n\nCreate the EFS and everything that is needed in AWS by following the previous part instructions.\n\nNow start a file my-values.yaml that contains the global, storage and dataLoader sections:\n\nAnd publish this to the cluster with:\n\nIf you had it deployed before, you can do:\n\nThat\u2019s it. You now have a dataset being downloaded. You can go and take 42 coffees while it does so, or continue reading this blog to understand your next steps.\n\nFor now let us focus on bringing all our services up and running.\n\nTraining is often the most advertised feature of Deep Learning. It is where the science and the actual magic happens. In about all cases, the performance can be drastically increased if you leverage GPUs, which, by chance, we learnt how to add in a Kubernetes cluster.\n\nWhen running locally on your workstation, you do not really care about the scheduling of the code. Tensorflow will try to consume all the resources available. If you have several GPUs, you can ask it to use them all or only some of them.\n\nHowever, when you start scaling out your system, things get a bit more intricated. Tensorflow uses 2 types of jobs to distribute compute: Parameter Servers and Workers\n\nBoth PS and workers will attempt to use GPUs if they are available, and compete around that. Because PSes have a lesser usage of the GPU and it is usually a scare resource, it makes sense to be careful when allocating resources to the different entities of Tensorflow. On the metal of your machine, this can be a problem. But containers and resource management will help us deal with this elegantly.\n\nA second thing to take into account is a limitation of packaging (and probably my own limitations). When building the package, it is about impossible to know on which worker each job will run, and to predict the number of GPUs that will be available to Tensorflow, especially on asymetric clusters where some nodes have 1x GPU, others have 2, 8 or more. Because Kubernetes is not very clever at this stage in exposing GPUs like it exposes other resources like CPU or RAM, the only way I have been able to make the packaging work is to assume all nodes equal. \n\nIf you followed last blog and remember the fact we deployed one p2.xlarge and one p2.8xlarge well\u2026 Unfortunately we will be limited by the p2.xlarge and only leverage 1 GPU in each server.\n\nLast but not least, Tensorflow does not seem to have the ability to change its scale dynamically (without restarting the cluster). This actually makes our lives easier when it comes to planning deployment.\n\nLet us look into the structures we need to design our cluster:\n\nLet us look into src/charts/tensorflow/values.yaml and src/charts/tensorflow/templates/cluster-deployment.yaml to analyze the different sections:\n\nSo here in the values tell us how big our cluster will be (8 PS, 16 Workers, 1 GPU per node).\n\nIn the deployment template, we start by defining the Configuration File via a configMap:\n\nYou can see the {{ range }} section, where we iterate over the jobs section. When instanciated, this will create something like:\n\nYou recognize here that the JSON we form is not clean, we have a last comma that Tensorflow will accept, but that you need to be careful about.\n\nAlso note in the range structure that we iterate with an index starting at 0 on a number of values, hence we go from 0 to 7 for 8 PS jobs.\n\nNow if we go a little lower in the file, we can see the conditional section:\n\nif we are building a GPU cluster, we only allocate GPUs to Worker processes, and we allocate to each worker all the GPUs it can access on a given node. We do not forget to share the necessary char devices as well (/dev/nvidia0 and further)\n\nOur training image is taken from the Inception tutorial Google provides, which we operate via a simple bash program that reads the configMap, extracts the proper JSON, and launches the training process from that:\n\nYou will notice that we enforce the data_dir to the output of our ingest phase, to ensure that our training will really read from there. The Dockerfile provides ideas to improve this section that are out of the scope of this article but you are welcome to contribute ;)\n\nIn order to make each of these jobs reachable via Service Discovery, each of them is mapped to its own Kubernetes Service, which is available in the cluster-service.yaml file.\n\nThe section of the values.yaml file is\n\nYou can see in the settings section how we are adding a pre-computed number of GPUs per node, and creating our PS and Worker jobs.\n\nAgain, this is pretty straighforward. You are expected to prepare any Docker image for Tensorflow which will receive\n\nThese are the only requirements really. You can prepare CPU and GPU images, publish them, and adapt the tfCluster section of your values.yaml file to match your configuration.\n\nBecause first we need to review the evaluation and serving processes!! We\u2019re not even half way!\n\nEvaluation is a specific process, that consumes GPU, amd which output allows measuring the quality of the model. It outputs data that can be consumed by Tensorflow\u2019s GUI, the Tensorboard. Without the tensorboard, it is very hard for a human being to understand how the training evolves and if the parameters are OK.\n\nThe tensorboard itself is a simple process that exposes a web application to present the graphs & data. Therefore we need to plan a service and am ingress point for it, in order to let the world see it.\n\nIn order to keep this blog as short as possible, I\u2019ll leave it to you to review the content of src/charts/tensorflow/eval-deployment.yaml and src/charts/tensorflow/tensorboard-service.yaml, which follow the same ideas as the previous ones.\n\nThe values.yaml section looks like:\n\nYou\u2019ll need to prepare an eval.sh script and/or a specific Docker image that will handle the evaluation of the model. It shall have the following properties:\n\nIn our context we start the evaluation with:\n\nwhich means we expect the training to write in ${DATA_DIR}/${DATASET}-checkpoints, the evaluation to write from ${DATA_DIR}/${DATASET}-eval and the evaluation dataset to lie in ${DATA_DIR}/${DATASET}\n\nYou will also need to adjust the Tensorboard command to adjust the location of the logs.\n\nYou are sooooo impatient. We just need to review the serving process and we\u2019re done.\n\nServing is the ability to present the trained model for consumption by third party users. This process, which can be done via tensorflow-serving, consumes the output of the training process, which we store in imagenet-checkpoints, and offers to query it via an API.\n\nAnyone can then submit an image, and get an JSON file as an answer, which contains the things that have been identified and the probability that model is right about them.\n\nServing a model does not require a lot of processing power and can run on a reasonable CPU only instance. However it must be able to scale quite a lot horizontally, especially if the model is served to numerous clients. Therefore, we are going to take a deployment approach, and serve the model via a service and an ingress point.\n\nOn the docker side, this is also a bit complicated. Tensorflow serving is very new, and less mature than the rest of the TF ecosystem. The official docker image provided by Google is very intricated, and rebuilds Bazel at image build, which is suboptimal. I made a secondary image that builds from the official tensorflow image, add the bazel binaries, then installs the serving part. A little faster to build, especially after building all the other images from the repo.\n\nBecause the model evolves over time, we have to add a few parameters to the launch command, which perspire via the ENV we set:\n\nthe enable_batching setting is the key here, and will have TF Serving to reload the model on a cron bases, by default every 5min. As we also keep the default at training, which is to export a new checkpoint every 10min, this makes sure that the latest model is always served.\n\nThe implementation is done via the files src/charts/tensorflow/serving-deployment.yaml and src/charts/tensorflow/serving-service.yaml, which I\u2019ll leave to you to review. You can also review the serving part of values.yaml to understand the configuration.\n\nhere again we have a DNS record to manage, which you\u2019ll be expected to add as a CNAME if following the previous advice, or a round robin to all workers otherwise.\n\nYou\u2019ll need to prepare a serve.sh script and a specific image that will handle serving of the model, which you can base on the Dockerfile-1.0.0-serving provided. It shall have the following properties:\n\nIn our context we start the evaluation with:\n\nWhich give you an indication of what to modify in your context.\n\nAnd yes\u2026 Now we deploy!\n\nThen you can just do:\n\nand watch your cluster start. After a few minutes, you can go to your tensorboard and you should see\n\nTo prepare this blog I worked on 2 models: Imagenet and a Distributed CNN from a workshop Google made last August. The Distributed CNN is nice because it uses a small dataset, therefore works very nicely and quickly OOTB.\n\nImagenet is the one I would really have loved to see working, and all the images are meant to leverage it. Unfortunately at this stage, everything starts nicely, but it doesn\u2019t seem to actually train. PS and workers start, then do nothing without failing, and do not output any logs. I\u2019m working on it, but I didn\u2019t want to have you wait too long for the post\u2026\n\nContact me in PMs to discuss and if you\u2019d like to experiment with it or share ideas to fix this, I will gladly mention your help :)\n\nIn addition, if you use the 1.0.0 Tensorflow images for Imagenet, you will encounter an issue due to the transition from 0.11 to 0.12, which essentially returns something like:\n\nIf you have this issue, use a 0.11 or anterior image. Please Tensorflow Gods, fix this!\n\nAnother looong blog post. But I think it was worth it. Essentially, you have:\n\nAt this stage, you should be able to use Kubernetes and Tensorflow for your own science. I truly hope so, and if not, just ping me and we\u2019ll sort you out.\n\nBig, Huge thanks to Amy who published the initial workshop that inspired this Helm chart, and to the other folks at Google, open sourcing so much content. This is awesome, and I hope this will help others to adopt the tech.\n\nThis Tensorflow Helm chart is far from perfect, but it\u2019s better than nothing. I intend to kindly ask to add it to the kubeapps and iterate from there.\n\nThis idea of building blocks around k8s is really powerful, so I will now be working on building more stacks. First next target is DeepLearning4j with my friends from Skymind!\n\nYou have a question, a request, a bug report, or you want to operate a GPU cluster? tweet me @SaMnCo_23 and let\u2019s talk."
    },
    {
        "url": "https://medium.com/intuitionmachine/please-no-more-mentioning-of-ai-safety-333efe336a7d",
        "title": "Please, No More Mentioning of \u2018AI Safety\u2019! \u2013 Intuition Machine \u2013",
        "text": "I will share my extremely biased perspective on the issue of \u2018AI safety\u2019. As the title has it, even the whisper of it makes me gravely disturbed.\n\nJust because I don\u2019t like the term, it doesn\u2019t mean I think the field of \u2018AI safety\u2019 is useless. In fact, nothing interests me more than this \u2018AI safety\u2019. My discomfort arise from rather belligerent dualism as a ramification(or the root cause) of using the terminology. This could not only stir unjustified fear among general public, but it may lead the researchers to have flawed perspective, rendering the progress in the field to be inefficient at the least. I believe \u2018AI cooperation\u2019 is the right term to use. Here is why.\n\n\u201cThe history of all hitherto existing society is the history of class struggles\u201d\n\nI have always thought the Manifesto of the Communist Party contains a rich insight on popular perspectives. Filtering our sensors through dualism feels very natural to us. God vs Devil. Good vs Evil. Man vs Nature. Man vs Machine. These things almost rhyme. When we see the world, it is full of struggle. Competition seems to exist naturally. How could it not? It has always been this way for past 3.8 billion years. So we\u2019d better be ready when some AI agent become smart enough to slip our grip. We must find a way to control them, or else. We have to protect ourselves. Hence the term \u2018AI safety\u2019.\n\nHowever, there is a catch here \u2014 we will be dealing with AI with superior intelligence than us. I will leave the definition of \u2018superior intelligence\u2019 to the hands of AI measurement community. When the object of control exhibit less overall intelligence than us, the term \u2018safety\u2019 makes sense. The \u2018AI safety\u2019 problem in that case could even be reduced into a particular case of robust control, resilient control, or cyber-physical system security problem. But this is not the most serious concern of \u2018AI safety\u2019. A superintelligence could possibly be the object of control here.\n\nImagine you are living in a small colony of exceptionally smart penguins. You happen to have an intelligence surpassing the sum of all penguins in the colony, and you have all the resource you need to survive. You have watched enough Youtube videos to understand their language, their customs, and all their joy and sadness. Your sole interest in life is taking selfies worthy of Instagram likes \u2014 none of the penguin businesses interest you. You have no will with mate with another penguin. Seals don\u2019t bother you at all. And you have allergy to seafood. One particularly warm summer day, a group of the penguin geniuses, impressed by your unfathomable intelligence, aspire to control you for their own benefit. They encircle you and peck you around in hope of coercing you to develop a bad-ass anti-seal weapons. Annoyed, you beat them mercilessly with pebbles, and the surviving few flap away for their life. An another group of penguin geniuses approach you the next day. They also are in fear of seals, but they decide to take a peaceful approach. They provide you with regurgitated fishes, flap their wings and dance around, hoping you would understand that they need some protection from seals. Although the half-digested fishes were rather repulsive, you take awesome selfies with frantically flapping penguins. Since they were cute, you decided to help them. Using spare time, you build a penguin fortress where only penguins can enter, directly from the sea. You get to take another impressive selfie with penguin fortress to boast to your online friends. Penguins get to have security from seals. Everyone is happy.\n\nOnce an AI surpass our intelligence, we would have to accept that we have no power(and perhaps no right) to control such AI the way we want. Our only hope is to ask nicely. Wait, but since us human beings are the one \u2018making\u2019 such AI, couldn\u2019t we stop making it smarter at some point so that we would be able to control it? If total self-modification of an AI agent can be completely banned, maybe there could be some smart way to impose such control. However, such tight regulation seems to be infeasible to impose all over the world without a strong, singular governing body on Earth even if there were no technological hurdles. Besides, the AI arms-race will only get intensified in future, which makes complete banning of self-modifying AI development even more unrealistic. Us human beings can\u2019t even clearly interpret what a moderate-size multilayer neural network does. It will not be possible to completely interpret the trajectory of a complex self-modifying AI agent the states over a long time horizon. So, are we doomed?\n\nGradual cooperation skill development is the right way\n\nRecently GoodAI, a Prague-based AI company, started an AGI challenge, with total prize money of $5 million. At the first stage of competition, they seek to test \u2018gradual learning\u2019 capability of an AI agent. Gradual learning banks on the idea of efficient compositional learning, which simply means when agent learns skill 1 and skill 2, when it learns skill 3 which has mutual information with 1 and 2 it will learn faster than learning skill 3 from scratch. There is an excellent blog post on this matter, and I recommend everyone to at least have a quick look at it.\n\nI\u2019m mentioning gradual learning here because I believe it is also crucial for AI cooperation development. To make AI agents more cooperative, we should have curricula with increasing difficulties. Possible criteria could be : non-zero-sum game ->complete zero-sum game, complete observability->decreasing observability, complete controllability->decreasing controllability, completely deterministic->increasingly non-deterministic tasks. During the process, agents should be encourage to use previously learned cooperation skills to learn additional better and/or more complex cooperation skills faster. Notice I used the plural form of agent here; cooperation cannot be measured with a single agent.\n\nTo actually impose learning process of cooperation, there has to be a measurement on cooperation skill. Here I suggest one such measure. It is defined over a discrete time interval from t_1 to t_m:\n\nwhere d_r(k)_j is estimation error of agent j\u2019s reward at time k, d_p(k) is distance from the closest Pareto Front at time k, j is the number of agents in the society, and \u03b1 and \u03b2 are (possibly learnable) hyperparameters. Agents are assumed to have a scalar reward. By using this measure, we can see how well an agent anticipates other agents need(inverse RL), and at the same time how well an agent maintains balance in the society it is in(Pareto Optimality).\n\nThere is an excellent work by Critch, which has rigorous analysis on matters in similar spirit of the above measure. I highly recommend readers who doesn\u2019t hate math to thoroughly read the paper.\n\nNotice that this motivates an agent to communicate its need(reward function) to another \u2018caring\u2019 agent(agent\u2019s reward function includes cooperation measure). Also notice that if more \u2018caring\u2019 agents exist within society, it increases all the \u2018caring\u2019 agents R_c due to recursive formulation. This has important implication in society of agents with full-scale self-modification, including its own reward function. An external pressure, such as fitness function on evolutionary algorithm on multiple societies of agents would guide such fully self-modifying agent\u2019s reward function to evolve in a way that is compatible with user-given fitness function(in this case R_c). Consider the case of dog-breeding. Over many thousands of years, dogs with more human-compatible reward functions were selected by breeding process, which in turn made dogs get along well with humans. This will have an effect on not-so-intelligent agents also. But an intelligent enough agent, which could anticipate user-given fitness function, would notice that when it modifies its own reward function along with reward function of other agents to be in line with R_c given by user, the resulting reward value from its own reward function will increase. In plain English, this means that the agent will find out the caring society is beneficial for all. This is only one simple scenario arising from above reward function(R_c). Much work is need to be done in this area-i.e., how to guide a fully self-modifying agent to learn to cooperate with others.\n\nThere are two types of invariance to be considered in cooperation. First is objective function invariant cooperation. This means the agent will seek cooperation regardless of objective functions of the other agents in the society. If we don\u2019t impose this invariance, the agent may favor other agents with particular set of objective function. Second kind is perception invariant cooperation. \u2018Perception\u2019 here could include, but not limited to, the transformations agent performs upon the history of its inputs and outputs. Result of such transformations include modeling and knowledge representation. Many problems we have witness from the history of our society result from not being able to maintain above invariances \u2014 nepotism, organized crime, corruption, clash of religious or political ideals, and wars.\n\nConsider a specific scenario, where the above invariances are tested. Assume there exist a \u2018malicious\u2019 AI agent- \u2018Bad AI\u2019 from here-within the society which has reward function leading the society away from Pareto Front. Our \u2018Good AI\u2019, equipped with R_c would then seek to contain or nullify the actuation of the \u2018Bad AI\u2019 because it has to respect the \u2018malicious\u2019 reward functions too. In order to achieve this feat, the \u2018Good AI\u2019 must be more intelligent than \u2018Bad AI\u2019 because it needs to anticipate and nullify action of \u2018Bad AI\u2019 and at the same time maintain Pareto Front for the society it is in. In another word, \u2018Good AI\u2019 is forced to be smarter than \u2018Bad AI\u2019.\n\nThere is another interesting implication to be observed by having Pareto Front in consideration in R_c. Suppose there are three agents in the society. Agent A and B wants to eliminate each other(ex. two warring nations). Agent C is the only agent having R_c included in its reward/objective function. If the objective function of A and B are fixed, agent C will seek \u2018containment\u2019, as described in above paragraph. However, if the objective function of A and B is modifiable in any way, for example, their objective functions are functions of certain factors in the environment(ex. availability of natural resource), something else can happen. By extending the Pareto Front, agent C may attempt to change the environment so that the zero-sum game A and B are playing may turn into a non-zero-sum game(ex. provide more natural resource to the environment).\n\nWhen designing curriculum and environment for training AI agent, closely coupling the development of cooperation skill and intelligence is not only beneficial in safety perspective but also may be a good solution for both tasks. I think it would be nice if the AI research community break free of the term \u2018AI safety\u2019 and think more in the direction of \u2018AI cooperation\u2019. We should think more like parents and less like makers. We cannot force our children to align their values with us, but we can teach them how to be good if we try. The choice to be good is theirs and theirs alone to be made, but more parents succeed in communicating good values than not(the world is not full of psychopaths-at least not yet!). Only when there is mutual respect between both parties, cooperation may take place \u2014 a happy ending for all."
    },
    {
        "url": "https://medium.com/intuitionmachine/machines-that-search-for-deep-learning-architectures-c88ae0afb6c8",
        "title": "Taxonomy of Methods for Deep Meta Learning \u2013 Intuition Machine \u2013",
        "text": "Let\u2019s talk about Meta-Learning because this is one confusing topic. I wrote a previous post about Deconstructing Meta-Learning which explored \u201cLearning to Learn\u201d. I realized thought that there is another kind of Meta-Learning that practitioners are more familiar with. This kind of Meta-Learning can be understood as algorithms the search and select different DL architectures. Hyper-parameter optimization is an instance of this, however there are another more elaborate algorithms that follow the same prescription of searching for architectures.\n\nHyper-parameter optimization is a common technique used in machine learning. Daniel Saltiel has a short post on \u201cState of Hyperparameter Selection\u201d that covers Grid Search, Random Search (Bengio et. al)and Gaussian Processes as a way to sample out different machine learning models. These are standard techniques and is exploited by Facebook in their FBLearner platform:\n\nThere are many hyperparameters that one can chose from in the regime of Deep Learning architectures. A recent paper, \u201cEvolving Deep Neural Networks\u201d provides a comprehensive list of global parameters that are typically used in the conventional search approaches (i.e. Learning rate) as well as more hyperparameters that involve more details about the architecture of the Deep Learning network.\n\nIn the research, what is explored is an algorithm, CoDeepNEAT, for optimizing deep learning architectures through evolution. The claim is that their evolution inspired approach is, five times to thirty times speedup over state-of-the-art Bayesian optimization algorithms on a variety of deep-learning problems. The approach used is based on idea called the SuccessiveHalving algorithm. The algorithm uniformly allocates a budget to a set of hyperparameter configurations, it evaluates the performance of all configurations, then throws out the poorest performing half, and then repeat until one configurations remains.\n\nTwo recent papers that were submitted to ICLR 2017 explore the use of Reinforcement learning to learn new kinds of Deep Learning architectures (\u201cDesigning Neural Network Architectures using Reinforcement Learning\u201d and \u201cNeural Architecture Search with Reinforcement Learning\u201d).\n\nThe first paper describes the use of Reinforcement Q-Learning to discover CNN architectures, you can find some of their generated CNNs in Caffe here:https://bowenbaker.github.io/metaqnn/ . These are the different parameters that are sampled by the MetaQNN algorithm:\n\nThe second paper (Neural Architecture Search) employs uses Reinforcement Learning (RL) to train a an architecture generator LSTM to build a language that describes new DL architectures. The LSTM is trained via a policy gradient method to maximize the generation of new architectures. The research explores this method to generate convolutional architectures (CNN) as well as generating recurrent architectures (RNN). For CNN generation the following parameters were used:\n\nand for the RNN generation the following was used:\n\nThe trained generator RNN is a two-layer LSTM, this RNN generates an architecture that is trained for 50 epochs. The reward used for updating the generator RNN is the maximum validation accuracy of the last 5 epochs cubed (see paper). After the the RNN trains 12,800 architectures (Google has this luxury), then select the candidate architecture that achieves the best accuracy in validation. Run the candidate architecture is then run through hyperparameter optimization (i.e. grid search) to find the best performing instance.\n\nHere is an example of LSTM cells that were generated by this system:\n\nClearly incomprehensible by most humans. Mind blowing and state-of-the-art.\n\nAn even more recent paper (\u201cLarge-Scale Evolution of Image Classifiers\u201d) also from Google Brain employs an evolutionary algorithm using mutation operators that are inspired by \u201crules of thumb\u201d coming from various DL papers. The evolution algorithm uses repeated pairwise competitions of random individuals, and select from the pair the better performing individual (i..e. tournament selection). The set of mutation operators that the authors used are as follows:\n\nThis \u201cNeuro-Evolution\u201d approach has some interesting result conclusions. Specifically that it is capable of \u201cconstructing large, accurate networks\u201d and \u201cthe process described, once started, needs no participation from the experimenter.\u201d The impression by some about this paper is that it could have only be executed by the folks at Google with \u201ccomputation at unprecedented levels\u201d. Clearly this is a brute force approach that can be refined in a neural network were trained to emit these rules rather than through random evolution. A very interesting discovery in this network is that some of the better performing networks simply stacked convolution networks on top of each other without non-linearities. This defies convention, however there may in fact be some justification to it! (Note: another paper with a similar genetic algorithm)\n\nIn Neural Architecture Search, an LSTM was trained to generate architectures. We can think of Meta-Learning in a general sense as being the \u201cmachines that generate other machines\u201d. Hyper Networks also fall into the category of machines generating other machines. Hyper Networks are DL architectures that learn how to generate the weights of another DL architecture. The claimed utility of the approach relates to it being a kind of generalization of weight sharing. Unlike the other methods mentioned above, both Hyper Networks and Learning to Learn, a neural network replaces the entire functionality rather than specifying hyper-parameters:\n\nNote that Transfer Learning is similar to both of these in that another network provides the weight initialization. In addition, these are only for single instances and not generative. Perhaps one could use a generative technique (i.e. VAE, GANs etc) to generate optimal DL architectures.\n\nIn all the above approaches, the method employs different search mechanisms (i.e. Grid, Gaussian Processes, Evolution, Q-Learning, Policy Gradients) to discover (among the many generated architectures) better configurations. The key idea to emphasize is that hyper-parameters can be generalized into a form of a language. A Domain Specific Language (DSL) with the above hyper-parameters as its vocabulary can serve as a general basis for generating new architectures.\n\nIt is instructive to understand that DSLs are a meta-model. There is however an even more abstract level, that is the meta meta-model. I wrote about this in \u201cThe Meta Model and Meta Meta-Model of Deep Learning\u201d. The key question of that post was that it was obvious as to the best vocabulary for the meta meta-model. The meta meta-model specifies the kind of hyper-parameters available at the meta-model. We can clearly have a language that specifies down to the description of every neuron. However, we want to strike an effective balance of abstraction. There is also an entire spectrum as to what is mutable and trainable.\n\nThe delineation of what happens in hyper-parameter optimization and what happens while training a DNN is actually somewhat arbitrary. We actually see this in the newer architectures like DeepMind PathNet, also a architecture search method, where hyper-parametrization and training are all in the same network (or fabric ). PathNet re-uses lower layers (effectively exploring initialization schemes) and explores multiple ways to connect a network.\n\nWe have a glimpse of a DSL driven architecture in my previous post about \u201cA Language Driven Approach to Deep Learning Training\u201d where a prescription that is quite general is presented. Specifically, (1) define a DSL, (2) generate data from the DSL, (3) train a DL network from the samples then (4) use the network to guide a search algorithm to find better solutions. In that post, I described the DSL as being something that can be applied to any domain. In this article, it is clear that the same program induction inspired approach can be used to searching for Deep Learning architectures.\n\nHere\u2019s are summary of the different Deep Meta Learning methods discussed here.\n\nThe variety clearly corresponds to the kind of meta-data that is manipulated to generate simulated architectures. In summary, current meta-learning capabilities involve either support for search for architectures or networks inside networks. The former is an established technique to automatically explore better architectures and the latter performs well in automatically fine-tuning algorithms."
    },
    {
        "url": "https://medium.com/intuitionmachine/deconstructing-deep-meta-learning-77f08d7e5a97",
        "title": "Deconstructing Deep Meta Learning \u2013 Intuition Machine \u2013",
        "text": "This article explores in more detail the idea of Meta Learning that was previously introduced in a post \u201cThe Meta Model and Meta Meta Model of Deep Learning\u201d. In this post, I explore \u201cLearning to Learn\u201d as a Meta Learning approach. We have to be very careful to distinguish between Learning to Learn and Hyper Parameter Optimization (HPO). HPO and more generally searching for architectures differs from \u201clearning to learn\u201d in that that HPO explores the space of architectures while meta-learning explores the space of learning algorithms.\n\nMeta-learning is all the rage in research these days. This is not unexpected, since such a capability essentially is way for the automation that we create to bootstrap their capabilities. What I want to explore today is, how can we better reason about meta-learning and therefore have a sense on how to apply it in the construction of our solutions.\n\nMeta-level constructs are ubiquitous in the languages we use for software development. It\u2019s a concept that many computer scientists are comfortable with. However, when you transition in this world of deep learning. That is a world of information dynamics (aka computational mechanics), then it becomes a lot murkier. An explanation may be that we don\u2019t expect to encounter meta-level constructs in the inanimate world of physics. We don\u2019t expect to find constructs that seem to be a specification or blueprint of other constructs.\n\nYet, we do see a meta-level construct in biology. DNA are meta-level constructions. DNA are specifications or blueprints that guide the replication of cells. The computational mechanism exists for all to see, so it is not something that we can ignore. DNA is like a long term memory that captures the instructions required for recreating biological systems that transcends their expiration. With brains and neurons, memories don\u2019t transcend death.\n\nThe question I have is, what\u2019s the meta-level construct for biological brains? What is acting as the specification or blueprint for the storage of behavior? Does such a construct need to exist? We may derive some inspiration how different programming languages express meta-level concepts. In object oriented languages like Java, C++ or Smalltalk, classes are meta-level. However, languages like Lisp and Scheme have their uniqueness is that there\u2019s no need for meta-level constructs. Data and code are one in the same thing.\n\nIn short, so long as there is a mechanism for memory and a mechanism to alter behavior based on that memory, then is sufficient to serve as a meta-level construct. So, Read-Eval-Print (REPL) loop is all nature needs to go meta. The ability to interpret stored memories as instructions is the key requirement. Meta-level constructs are just intellectual affordances or conveniences that we as humans create to better understand the programs that we write.\n\nNow we can have programs that interpret instructions but are not meta-level. When does meta-level processing occur? This happens when the outputs of a process is used as instructions to another process. Does that not obviously describe \u201ccommunication\u201d? Robin Milner wrote in \u201cTuring, Computing and Communication\u201d:\n\nFor communication to be successful, the receiving end needs to interpret the communication and therefore this implies the existence of interpretative language. The communication must mean something for the receiver. That meaning can be as simple as a true or false statement or to some more complex structured conveyance of knowledge. In short, what is communicated must be a language.\n\nHowever, what do Deep Learning systems do? Is there language interpretation happening underneath the covers? Each layer in a Deep Learning system develops a distributed representation that is a kind of an internal language. It learns a map of an input representation into an output. That is, it learns a translation. Is translation the same as interpretation? Translations are just intermediate steps in a sequence of steps to arrive at interpretation. Interpretation here means the process that takes information that leads to a decision.\n\nThe utility of language arises primarily from is share-ability. However, Deep Learning systems, that create internal languages, are uninterpretable and likely difficult to share. We can of course train other DL system to learn these internal languages. Encoder-Decoder architectures perform this kind of work. Examples of this are cross language translation and image captioning. So, we do see some kind of \u2018language sharing\u2019 between an encoder and decoder. However, to build more complex systems, we perhaps need to understand how to build shareable DL representations.\n\nLanguages that are at the meta-level are like DNA. They provide instructions on how to recreate other machines. Meta-level machines use metal-level instructions to create other machines. So a machine that performs meta-learning is able to learn the language of creating other machines. To better understand how this works in the wild, let\u2019s explore some recent research in the field.\n\nLearning to learn by gradient descent by gradient descent trains an LSTM based optimizer to learn a variant of the gradient decent method. In this research, the LSTM was trained on a single layer network of 20 hidden units against the MNIST dataset. The trained optimizer was then tested on a larger network of 40 hidden units, a network with 2 layers, and one with ReLU instead of the original activation function. The trained optimizer performed well for the first two and not the third. The trained optimizer was tested on different datasets (i.e. CIFAR), and in all cases performed well. The trained LSTM optimizer generalized for larger networks as well as similar datasets. In the framework of meta-level languages, the LSTM learned the language was applicable to different bus similar enough problems but not radically different network architectures.\n\nOne key limitation of training of the meta-learner is that the input training data will be prohibitively large because training will potentially require observation of not only the optmizee\u2019s data set, but also all its weights. So, to make this approach feasible only a subset or approximation of the entire potential input space is used to train a meta-learner.\n\nLearning to reinforcement learn trains an LSTM in the context of learning a Reinforcement Learning (RL) algorithm. A teacher RL algorithm was used to train the LSTM. The result of this was that the LSTM learned an algorithm that was (1) different from the teacher RL and (2) biased toward the environment where it was trained. The consequence was that this LSTM based algorithm was more efficient than a general purpose algorithm:\n\nOptimization as a model for few-shot learning trains an LSTM in the context of a \u201cfew-shot\u201d learning problem.\n\nIn the algorithm described in this paper, a LSTM is trained to be the update function for the optimizee network. The update of the optimizer is after each batch and its loss function is calculated against test data. This indeed is an interesting configuration, in that typically training usually isolated from being evaluate against test data. Here however, the optimizer is aware of its performance relative to test data.\n\nAn interest paper titled Meta Networks (MetaNet)came out just recently (March 2017) with an intriguing claim to be able to \u201cacquire meta-level knowledge\u201d. The architecture of the MetaNet is depicted as follows:\n\nThe \u201cmeta-info\u201d that is studied in the paper is the loss gradient and the system was evaluated with state-of-the-art results in the regime of one-shot learning. A key constraint, that we have to emphasize here, is that a meta-learner can only learn from the data (or meta-info) that it is trained on. It is unclear to me why the loss gradient is sufficient information, however let\u2019s give the paper the benefit of the doubt that it appears to work. I suspect however, for meta-learning to scale, one needs to find a subset of information from the base data to train on. It will be interesting to see as to what subset researchers will be looking at in future papers. (Note: A recent paper by OpenAI seems to confirm this gradient only approach)\n\nMeta-learning (i.e. Learning to Learn) is just one kind of meta-process. Other researchers have worked on \u201cLearning to Optimize\u201d, \u201cLearning to Compose\u201d, \u201cLearning to Plan\u201d, \u201cInverse Learning\u201d and \u201cLearning to Communicate\u201d. This is just a partial list of capabilities, that in a later post I will attempt to provide a unifying conceptual framework. (Note: See Design Patterns for Deep Learning for more)\n\nHowever, despite the intriguing potential that Meta-Learning may perhaps bootstrap itself and recursively improve its behavior by learning on to itself. In the 3 research papers I surveyed above, it is clear that there is an major obstacle that can\u2019t be avoided. That obstacle is the requirement for training data.\n\nOther meta-learning approaches (i.e. Hyper-parameter optimization and architecture search) work from meta-data. Hyper-parameter optimization uses meta-data on things like learning rate. Approaches from Google and MIT that search for different architectures use meta-data in the kinds and connection of different layers. These methods can synthesize new data by simple generation and simulation. Learning to learn however does not use meta-data, but rather observes the instance behavior and derives meta-data.\n\nMeta-learning obviously cannot learn what is has not previously seen. So there is a need to learn a learning algorithm that works across multiple architectures or even different domains. Without an ability to generate new training data, \u201clearning to learn\u201d by itself is unable to infinitely improve on itself. However, what \u201clearning to learn\u201d is able to do is improve itself within a given context. Unfortunately, it cannot learn a meta-language that has independence from context.\n\nThis representations that are learned by Deep Learning systems have context intertwined in them. We do not know of systematic ways of removing context. This is related to models that have invariances or to representations that have removed all nuisance variables. The ideal generalized representation is one where context has been removed. That is, a system should be able to recognize a cat independent of lighting, shadows, angle or occlusion:\n\nThe additional consequence of context independent languages is that they are shareable in different contexts. That is, we can combine the context on demand with the internal representation to perform the prediction. Ultimately, for \u201clearning to learn\u201d to be successful, it is required that it is able to \u201clearn a language independent of context\u201d (model agnostic?).\n\nPlease read \u201cMachines that Search for Deep Learning Architectures\u201d for a taxonomy of Meta-Learning methods."
    },
    {
        "url": "https://medium.com/intuitionmachine/some-thought-about-deep-unsupervised-learning-using-nonequilibrium-thermodynamics-a74a42108723",
        "title": "Some Thoughts about \u201cDeep Unsupervised Learning using Nonequilibrium Thermodynamics\u201d",
        "text": "I have downloaded this paper \u201cDeep Unsupervised Learning using Nonequilibrium Thermodynamics\u201d for while but only read it through today. The typical example of direct connection between neural network and statistical physics is some spin-magnetic system like Ising model. This paper is interesting in the sense that it connected deep learning to a different area of statistical physics, e.g. non-equilibrium diffusion process.\n\nOne can see the connection between Ising model and deep learning from this nice introduction video by Carey Nachenberg \u201cStanford Seminar \u2014 Deep Learning for Dummies\u201d. Each of the little \u201cmagnets\u201d, which physicists call \u201cspin\u201d, has two states. These becomes a neuron in deep learning neural network. In a neural network, the interaction between the little magnet is not given by the underlying quantum mechanics, as in physics, but one needs to design a process to learn the interactions between the neurons to reconstruct the training patterns.\n\nIn this paper by Jascha, et al., the authors consider a different physical process \u201cdiffusion\u201d than the magnetic systems for constructing the \u201clearning process\u201d. Rather than modeling the join probabilities by an energy function with a Boltzmann distribution, the authors build a deep network to learn the \u201cdiffusion process\u201d from a known distribution to a simpler final distribution. Then they work out a formulation to reverse such process such that one can recover the original distribution from a random distribution with a \u201ctime reversal\u201d process.\n\nIn contrast to other \u201cequilibrium\u201d statistical mechanics approaches, we are not looking for a distribution at \u201cequilibrium\u201d (Strictly speaking, a Boltzmann machine does encode a \u201cquenched disorder\u201d which are not at an equilibrium state with the other parts of the system). The original distribution is recovered through reversing the diffusion process without any assumption that the system is under a certain equilibrium state.\n\nI think the idea is quite fascinating and original. Such a connection between machine learning and non-equilibrium statistical mechanics is certainly worth exploring more. I also believe that many ideas that have been brought up in the context of non-equilibrium statistical mechanics will probably be useful for new machine learning techniques. For example, I think there is some connection between a non-equilibrium driven processes connecting quench disorder system (e.g. spin glass model). Shameless self-promotion here: I wrote a paper on about an anomalous diffusion process that couples a diffusion process to non-equilibrium growth phenomena: Passive random walkers and riverlike networks on growing surfaces. One related paper shows such driven system exhibiting \u201creplica symmetry breaking\u201d: \u201cReplica Symmetry Breaking in Trajectories of a Driven Brownian Particle\u201d. As \u201creplica symmetry breaking\u201d is a signature of spin glass system for \u201clearning different patterns\u201d, whether such driven diffusion system will be useful for machine learning might be an interesting question to ask. Hopefully we will see more related work coming out soon."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-low-information-coupling-principle-and-multi-agent-deep-learning-32b5d581989",
        "title": "The Loose Coupling Principle and Modular Deep Learning",
        "text": "We seek out universal guidelines as to how best to design our intelligent systems. Here we focus on Artificial Intuition (alternatively intuition machines). Monica Anderson uses the term \u201cModel-Free\u201d as the universal guideline as to how to build artificial intuition. Her logic goes along the lines of: you can\u2019t build intelligent systems using intelligent components, all this does is push the problem down to lower level components. I agree with this in that, intelligent systems are composed of unintelligent simple components and intelligence comes from emergent behavior.\n\nI however would like to explore this idea that components of an intelligent system should function in low information coupling contexts (aka loose coupling). If we were to assume that brains are massively parallel systems consisting of diverse recognition components, then these components ought to be able to perform their jobs with the least amount of information. As a consequence, in the design of intelligent systems, one should always favor mechanisms with low information coupling.\n\nIt turns out this principle appears to line up quite well with our understanding of how artificial intuition based systems work (note: Deep Learning is an artificial intuition machine). In a previous article, I introduced a collection of loose coupling mechanisms found in the world of distributed computing.\n\nThe different methods that lead to loose coupling fall in to three general categories. These are mediation, decomposition and late-binding. These categories attempt to reduce signal dependency, computational dependency and temporal dependency respectively (Recall that information dynamics consists of signaling, computation and memory). We can examine the list methods in the mentioned article and see how these methods will fall in at least one of these three categories. The first kind is that you can decompose a single component into multiple components, allowing each subcomponent to perform work on different parts of a problem. The second kind is that you can place an intermediary component between two interacting components. Finally, there is the general method of late-binding.\n\nAn example of decomposition is that you can break the concept of a procedure in into two components, the action and the continuation. This results in a computing model that naturally supports asynchronous invocation and thus loose coupling. In Deep Learning, an example of decomposition, are the use of residual layers that effectively decomposes a single conventional layer into multiple incremental representation layers.\n\nMediation, or adding an intermediary is the most obvious form of decoupling and ensuring loose coupling. However, there are forms of this that are not as obvious. For example, rather than having two events occurring at the same time, allowing them to occur at different times and having an intermediary in the form of a correlation identifier can preserve the original semantics.\n\nFinally, late binding is the subtlest form of loose coupling. That is when decoupling happens in time. For example, Prototype Oriented programming is a form of late binding classification. Delaying classification leads to more flexible systems. As we will see later, many forms of deferring commitment can lead to more loosely couple systems.\n\nClassical Deep Learning networks are highly coupled constructions. These are trained end-to-end in a highly synchronized manner, where forward and back propagation phases are performed with high synchronization. The limitations of DL are also well documented. As compared to biological brains some of the limitations of DL systems are that they are forgetful, non-adaptive, unable to learn continuously, unable to learn from a few training samples and require a lot of energy. These limitations should prompt researchers to explore variations to the current classical formulation. I am specifically interested in modular DL systems that can be composed in a multi-agent like networks. In these kinds of networks, loose coupling is a essential feature.\n\nThe question then is, can we use loosely coupled principles to influence our future Deep Learning designs? In this post I shall show how the methods of decomposition, mediation and late-binding are used in the construction of some recently proposed Deep Learning networks.\n\nDecoupled systems achieve greater generalization than monolithic DL systems. The evidence for this is in the development of Generative Adversarial Networks (GANs). These systems have competing neural networks that are able to perform impressive demonstrations of realistic image generation. StackGAN has shown that two decoupled adversarial networks working in combination can lead to state-of-the-art photo-realistic image generation:\n\nNotice that there are four DNNs in the architecture above. Observe how the system is trained in stages where the first stage (in blue) training data has less information than the subsequent stage (in purple). This kind of decoupling, where the earlier stage is trained with partial information and not the entire data, allows for scaling up the capabilities to perform photo-realistic image generation from much larger images.\n\nMaluuba have taken a decoupled approach to create a more scalable Deep RL system:\n\nThe interesting aspect of the Maluuba research is that the behavior coupling of the participant subnetworks are controlled by a reward function. So depending on the context of the environment, the subnetworks will either work in concert or work independently. Such a network learned is task three times faster than an equivalent tightly coupled network. A decoupled network has an attention like mechanism where the best trained expert for a specific kind of task may be called upon in different situations.\n\nThe major difficulty in multi-agent systems like this is the difficulty in achieving convergence. This is particularly problematic is the sub-networks form a cyclic graph. The Maluuba research breaks cycle dependencies by employing mediation (i.e. using intermediaries called \u201ctrainer agents\u201d):\n\nProgressive Neural Networks is another example of using intermediaries to decouple the network. The motivation for the decoupling however is to prevent catastrophic forgetting:\n\nIn the construction above, mediators are neural networks that perform domain adaptation with the purpose of leveraging transfer learning by reusing previously learned features of lower layers.\n\nLate binding, sometimes referred to as lazy evaluation or in process models as deferred commitment, is a guideline that says that if you need to decide on an action, then that the decision can be deferred to the last possible moment. It is a kind of temporal decoupling wherein a request for action does not necessarily coincide with the action being executed immediately. In effect, it does assume an asynchronous behavior from the requester of the action.\n\nAn example of the use of late-binding can be found in the one-shot learning paper by DeepMind. The algorithm trains a network to discover the embedding where samples in the same class are close and samples in different classes are distant. Matching networks does this by defining a differentiable nearest neighbor loss involving the cosine similarities of embeddings produced by a convolutional network:\n\nOne area of Deep Learning architectures that isn\u2019t researched enough is in the area where inference is performed. In conventional networks, the assumption is that the network has already learned the classification and therefore a single forward pass is needed to perform inference. This however may be requiring too much of a network and perhaps some kind of late-binding of contextual information may be needed for better generalization.\n\nWe find decomposition, intermediation and late-binding all working in concert in another of Deep Mind\u2019s papers. The group proposes a \u201cSynthetic Gradients\u201d approach that decouples back propagation ( DeepMind has an update of this paper ). The method essentially inserts a proxy neural network in between layers to approximate the gradient descent:\n\nThis capability is valuable in complex multiple networks, acting in multiple environments at asynchronous and irregular timescales. Here is a high level description of the method:\n\nThe synthetic gradient model decomposes the otherwise synchronized layers of a monolithic Deep Learning network. It does this by introducing a mediator between the layers to perform back propagation. What this mediator does (also a neural network) is to immediately respond with an estimate, however when the true error is received at a later time, updates its own estimate. One can thus look at this as a late binding of the true back propagation.\n\nIn this post, we begin to glimpse the value of understanding the known mechanisms for introducing loose coupling or low information coupling into DL architecture design. In fact, we can explore more advanced \u2018model-free\u2019 mechanisms, such as Discovery, Recognition, Learning, Abstraction, Adaptation, Evolution, Narrative, Consultation, Delegation and Markets, and discover that the preferred implementation is one that has low information coupling. It\u2019s just logical, the lower the information requirements for a mechanism to perform its job, then it is more likely that the mechanism to be more successful.\n\nWe propose an LSTM based meta-learner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime."
    },
    {
        "url": "https://medium.com/intuitionmachine/a-generalized-bootstrapping-framework-for-deep-learning-architectures-970075bad781",
        "title": "A Language Driven Approach for Deep Learning Training",
        "text": "Every since DeepMind AlphaGo made its splash into the world of AI, there\u2019s this question that has been begging to be answered. What is the general form of an architecture the fuzes Deep Learning and more conventional AI based search techniques.\n\nI\u2019m not usually a fan of research papers that explore Deep Learning for Program Induction. It just seems to me as to be overly ambitious and therefore I tend to be quite skeptical of any results. So when last week, there was a lot of buzz in the press about Microsoft\u2019s DeepCoder: Learning to Write Programs, I basically mostly ignored it as being research that was receiving a lot of undeserved hype. Steven Merity has a longer article analyzing the paper and the corresponding hype (see: \u201cStop saying DeepCoder steals code from StackOverflow\u201d )\n\nThe research however is interesting in a way that is distinct from its effectiveness in writing programs. What is interesting is that the DeepCoder architecture appears to be very general and the approach can be employed in many more problem contexts.\n\nThe architecture of DeepCoder consists of four components:\n\n(1) A Domain Specific Language (DSL) and attributes. DeepCoder does not work off any kind of general programming language but rather a more restricted DSL. In the research, the selected language that was examined was one that was a subset of a query language. The attributes were an enumeration of the features of a specific program instance of the DSL.\n\n(2) A DSL Program Generation Capability. The function of this component is to generate programs that are based on the DSL and additional parameterization (ex. input-output pairs etc). The function is able to generate millions of programs with the DSL as its seed.\n\n(3) A Deep Learning Model. The model attempts to predict the attributes in (1) based on the generated programs in (2). The trained DL model serves as a way to perform a quick approximate prediction of the viability of a generated program. In the case for DeepCoder, the model tries to predict the set of features (or operations) that may likely be needed to achieve the outputs based on the inputs.\n\n(4) Search. The aim of this component is to integrate the predictions in the trained model in (3) to guide a more conventional program search algorithm to find actual solutions. In this case, programs that satisfy the constraints of (2). In the research, the authors integrated with 3 different search algorithms, depth-first-search, a \u201csort and add\u201d enumeration algorithm and a program synthesis algorithm.\n\nIn essence, the approach searches for different generated programs to find programs that satisfy initial constraints. It is a hybrid approach that is similar in AlphaGo\u2019s approach in that it employs Deep Learning as a component for quick approximate function evaluation.\n\nEven though the focus of DeepCoder has as its domain the generation of programs, the framework can be applied to simpler less expressive languages.\n\nThis framework, similar to AlphaGo, is a generalized bootstrapping framework for incrementally training more intelligent solutions. That\u2019s because what is described is a learning approach that get\u2019s better with each iteration. In AlphaGo, the system got better by training against itself. AlphaGo was bootstrapped by training against a collection of previously recorded game play. In the framework describe here, the bootstrapping is performed using synthesized data. The trained Deep Learning model does not necessarily need to be of high accuracy. This is because the stage that follows it, employs the Deep Learning model as just the starting point in its own more comprehensive search. The self-improvement potentially comes when the results are fed back to further train the Deep Learning model with even more data.\n\nI would label these kinds of architectures that search for other architectures as meta-search architectures. To summarize:\n\n(1) Define a language that you can use to generate data.\n\n(2) Generate data by creating valid statements using varying combinations of the language.\n\n(4) Use the trained model as a way to speed up search through more language examples.\n\n(5) Iterate back into (2) to expand the training set with better language examples.\n\nThis hybrid approach is a fusion of intuition based cognition and logic based cognition. Conventional computation (i.e. logic based) is used in stages 1,2 and 4. With the purposed of training the intuition machine by either synthesizing new data or searching for new training data. You are going to see more and more of this interplay of intuition and rational machines. It is analogous to DeepMind\u2019s research that is inclined towards the equation AI = DL + RL, where RL is reinforcement learning.\n\nIn DeepMind\u2019s PathNet research, which incidentally is also a meta-search algorithm for new architectures. Reinforcement Learning and Evolutionary Algorithms are employed towards search for better DL solutions. Rather than search for different combinations of language (here it is programs) to improve, PathNet searches for combinations of DL layers to improve on a solution. DL layers are just a different kind of language and therefore the same DeepCoder approach can apply to searching for DL architectures. This in fact has been previously done by research at Google and MIT (see: \u201cDesigning Neural Network Architectures using Reinforcement Learning\u201d and \u201cNeural Architecture Search with Reinforcement Learning\u201d.)\n\nIt is helpful to reflect on the standard approach to training Deep Learning systems as described in the following flowchart:\n\nWhat we are hoping to achieve here by a language driven approach is a systemized way of synthesizing new data. Treating data generation from the perspective of generating expressions or sentences by sampling from a synthetic language is conceptually appealing. Furthermore, we can bring to bear many of the computer science techniques that have been developed previously.\n\nThe approach of treating Deep Learning solutions as language comprehension problems is extremely compelling. This language approach was also used by experimental physicists in a paper \u201cQCD-Aware Recursive Neural Networks for Jet Physics.\u201d where experimental data was treated like a natural language with the intention of training a Deep Learning system to learn a synthetic language. Something that is not too different from learning DSLs. One key takeaway from this research is that the language that was used was a synthetic language that had semantics that was derived from QCD theory.\n\nWe can contrast this approach to the Probabilistic Graph Model (PGM) approach to ML. In the PGM approach, developers construct a probabilistic graph that defines the relationships different variables. The approach uses Monte-Carlo sampling to construct Bayesian consistent distributions for the variables. In this language driven approach, we similarly build up relationships between concepts, however we do this through a DSL. The DSL rules are much richer and expressive that that of a graph model. The requirement however is that we can \u2018sample\u2019 the DSL so as to synthesize new data. We then use Deep Learning to learn from this synthesized data. We then feedback into itself by employing more traditional search algorithms. I hope you see the appeal of the potential power of this approach.\n\nA recent published research paper (March 2017) titled \u201cUsing Synthetic Data to Train Neural Networks is Model Based Reasoning\u201d examines the above idea in greater depth. They explore the technique in a \u201cCaptcha-breaking\u201d architecture. The research summarizes the uniqueness of the approach as follows:\n\nThis indeed is a remarkable approach to exploiting Deep Learning system that demands greater focus.\n\nDeep Learning systems are in essence systems that learn new languages. Just like languages, they are built in multiple layers of abstraction These machine created languages however are difficult to interpret or explain, however that does not imply that they should be treated differently in how we treat languages."
    },
    {
        "url": "https://medium.com/intuitionmachine/a-futuristic-deep-meta-learning-system-f8af641ad58b",
        "title": "Learning to Coordinate in Deep Learning Multi-Agent Systems",
        "text": "In the inevitable transition towards more modular multi-objective and multi-agent Deep Learning systems, we need to begin exploring the same loose coupling principles that underpin the coordination of distributed systems. A key criteria in building effective DL systems is better generalization. Although, generalization can mean many things, we can at a minimum accept an intuitive interpretation. That is, generalization implies systems of greater adaptability. In distributed architectures, Loose coupling principles encourage greater adaptability and therefore can provide valuable ideas on how best we can architect analogous DL multi-agent architectures.\n\nAnother justification is that intelligent systems are expected to contain a massive degree of diverse agents and therefore any mechanism that demands tight coupling is a mechanism that will not scale. Therefore, given a choice in selecting which mechanism to use, a loose coupling mechanism should be the preferred choice. If we think about this even deeper, all behavior should be based on the least amount of information, on only local information. This clearly sets it up to favor low information coupling. Any method that requires high information coupling to decide on an action is a method that is intuitively not the correct one.\n\nA warning to the reader, this is highly speculative stuff and therefore should be either ignored entirely or treated with a grain of salt. Reading this can only lead to greater confusion. With that out of the way, let\u2019s review some loosely coupled principles that I dug up from a past life:\n\nMapping these principles to may not be applicable to DL multi-agent systems. However it can be educational to explore each one and potentially propose an equivalent viable approach. As a caveat, we are constructing here a hypothetical system based on a future idea of a network of collaborative and competitive agents tasked with solving a problem using imperfect knowledge (see: 5 Capability Levels of Deep Learning). These are merely some preliminary ideas that we may want to bake into a hypothetical multi-agent system.\n\nIdeally we would like to have as few interfaces as possible to reduce incompatibility as well as increase plug and play. Think of how effective the USB standard has been for power and communication convenience. One other thought is to build these kinds of systems employing FIPA communicative acts or speech acts. The present approach for DL systems is to learn how to communicate on their own. Perhaps however by adding constraints on the nature of communication, inspired by speech acts, may lead better reusability. So for example, a neural network may be trained against another system with speech acts as the protocol, conceivably this kind of system may be adaptable in another context where speech acts are also used as the protocol.\n\nCommunication is likely to be fire and forget document message passing. It\u2019s just an easier thing to learn. Presently, most research on learning to communicate tends to employ message passing rather than a coordinate request/response or a procedural like invocation.\n\nOne would think that there\u2019s no notion of types for representations in DL system. However, as we have seen in Google\u2019s NMT system, the addition of a label that indicates a representation\u2019s language was one of important tricks to achieve one-shot cross translations. So there is evidence that tagging data with its type may be valuable even for DL systems.\n\nMonolithic DL system actually have a rigid form of synchronization with respect to back and forward propagation through its layers. In a more modular rendition, we would like to relax these restrictions such that synchronization between components are not required. Besides, biological brains don\u2019t require a single clock like computer systems. So one should not have this same synchronized requirement for DL systems.\n\nIn DL system,there are Pointer Networks that maintain hard coded references similar to memory pointers. A more modular system would require a second level of indirection such that the reference is to a query and not necessarily to some internal representation. This is analogous to associative or context based retrieval. In other words, if there is a retrieval to be made then the request for that information is through a query and not through a opaque identifier (as we see in conventional computers).\n\nRepresentations for DL system are definitely not self-describing, they are in fact opaque. This of course makes it next to impossible to coordinate between multiple interacting agents if there is no mechanism for sharing knowledge. Furthermore, there does not exist research that tries to learn a \u201cmeta-level\u201d representation of data. This is going to be the major technical hurdle for multi-agent based systems. A lesser form of this problem is the Fixed Verb approach. One development to note however is that in the DeepMind\u2019s PathNet approach, lower level representation sometimes shared between networks.\n\nA pattern based schema is one that is more loosely defined than one that is grammar based. The idea here is that only partial specification is necessary for interoperability. There is a concept in DL that nuisance variables are ignored in training or the notion of invariant features in data. One would like to train to ignore data that is invariant while focusing on features that are distinct.\n\nIdeally, one shouldn\u2019t have to care how subnetworks are wired together. The exists research where networks are wired in a hierarchical manner (see: Maluuba and DeepMind). Where there are networks that are coordinators for much simpler networks. Multicast networks assumes that participants are able to reason on their own as to what messages are worth listening to. This is a big burden to justify.\n\nIdeally, in an adaptive system, no two components are hard coded for interaction and that dynamically that interaction can change depending on context. We\u2019ve seen an architecture like this in \u201cConditional Logic in Deep Learning\u201d, where the selection of subnetworks are controlled also by a layer. Also in CPPN based systems, there are brokering components that are neural networks that\u2019s sole purpose are to learn how to adapt one layer to another layer.\n\nAlternatively, we can think of this as late-binding. That is, if we can defer commitment until action is necessary. The value of this capability related more to planning execution. That is, there is an additional dimension to plan execution such that work is not performed only when dependencies are available. It is difficult to see how this applies to DL other that the fact that DL systems are naturally data flow based system and thus lazy evaluation is a foundational implementation feature. What is interesting however is the application of the late-binding principle to recognition. One of the glaring deficiencies of DL is its lack of adaptability and perhaps some kind of learning on the fly, a lazy learning process, may be a solution to this problem.\n\nThese are just general principles that guide a loosely coupled system and should also be applicable to a loosely coupled DL system.\n\nThere is research in Learning to optimize or Learning to plan, where a DL system is able to propose execution plans. RL system by their nature learn reactive behavior and long range planning is certainly more difficult to implement. The hunch here is that DL systems as a consequence of their poorer logic inferencing capabilities are going to perform better as reactive agents as opposed to planning agents.\n\nMarket driven distributed coordination is likely going to be the mechanism for multi-agent coordination given that complexity of learning how to perform central command driven execution. The distribution of responsibility is one that seems to be more scalable and realistic.\n\nContracts are necessary in a market driven economy to ensure that participants are compliant in their behavior. This implies then that contracts themselves are representations that provided guidance for execution as well as compensation in cases of failure. DL system that learn how to perform compensation based on failure is certainly going to be an interesting research topic.\n\nWe have to assume failures will exist in a marketplace (or in coordination) and therefore participants need the additional capability of performing compensating actions. It will indeed be interesting if we create systems that can learn this behavior.\n\nThe notion that there is a strict separation between instance and meta-level data is an artificial construct and sometimes may be too restrictive. Today, it\u2019s unclear yet how representations can be built to represent a class of a concept. One can however imagine the storing of exemplar instances or prototypes and using this as a way to kickstart classification.\n\nIn a more abstract classification, loose coupling techniques have three recurrent characteristics. These are, late binding, mediation and decomposition. Interestingly enough, there are research papers covering all three characteristics, the most intriguing one of all how late binding is applied in the context of connectionist architectures.\n\nOne thing that seems to recur often enough that requires one attention is the notion of meta-level representation. Meta-level representations exist due to the need for coordination. It is a capability that is akin to having a system with internal self-awareness. This self-awareness of course seems like a problematic requirement. One should expect to build intelligent systems without the need for intelligent subcomponents (see: Artificial Intuition). However, the meta-level reasoning components themselves may also be unintelligent and may be unaware that its operation is at a meta-level.\n\nAnother observation here is that its hard to divorce oneself from a symbolicist approach once we begin discussing meta-level concepts. It is therefore quite conceivable that an embodiment of the above system would be a hybrid symbolicist/ connectionist architecture. Alternatively, just like the mind, something that works of dual process theory. That is, a intuition and a rational machines all working in concert.\n\nWhat I did here is examine ideas from distributed computing and to see if the basic idea of loose coupling or low information coupling can serve as inspiration on how to build multi-agent DL systems. One main conceptual stumbling block is the notion of meta-level information that exists in distributed systems. However, there is indeed still a lot of common ideas that clearly should be leveraged. This exploration in promising in two aspects, the first is in identifying fundamental principles and the second in identifying constraints on how multi-agent systems communicate. The identifying of constraints or alternatively called the definition of a language of discourse, provides a bounded space for exploration and therefore may make feasible the possibility of learning to coordinate.\n\nhttps://openreview.net/pdf?id=Hk8N3Sclg MULTI-AGENT COOPERATION AND THE EMERGENCE OF (NATURAL) LANGUAGE"
    },
    {
        "url": "https://medium.com/intuitionmachine/intuition-innovation-and-deep-learning-f20b9908d23c",
        "title": "Intuition, Innovation and the Limits of Deep Learning Generalization",
        "text": "How is intuition developed? How does this lead to innovation? What does this have to do with Deep Learning?\n\nIntuition like consciousness is something that we are all aware of its existence but likely have not investigated in enough detail to have a grounded understanding of its nature. In fact, I would say that there\u2019s more research on the nature of consciousness than research on intuition. I\u2019ve written earlier about a few research groups that have explored consciousness with respect to an artificial general intelligence, however I don\u2019t think has been equivalently the same effort with the study of intuition.\n\nMy specific interest is in the study of artificial intuition. I introduced this topic in an earlier blog post. In summary, artificial intuition, applicable equally to intuition, has attributes that differ from the more studied form of cognition (i.e. logical, rational). It is my conviction that Deep Learning systems are artificial intuition systems in contrast to GOFAI which is based on rational cognition. This judgment call is based on the commonality of attributes of intuition and Deep Learning.\n\nIn this post however, I explore deeper an understanding of intuition and reveal in there exists blindspots that we have in developing more capable deep learning systems.\n\nBruce Kasanoff has a very short article on Forbes, \u201cIntuition Is The Highest Form Of Intelligence\u201d which spurred me to explore this idea further. The study of intuition is not entirely new. I wrote earlier how Dual Process Theory introduces the idea that the mind works using two different kinds of cognitive systems. Kahneman, (see \u201cThinking, Fast and Slow\u201d) employs Dual Process theory to explore how humans can fall into biased and incorrect thinking as a consequence of the interplay of intuition and rationality.\n\nThere are however other investigators have studied intuition in greater detail. One of the most popular books written about intuition is arguable Malcolm Gladwell\u2019s book \u201cBlink: The Power of Thinking Without Thinking\u201d. Gladwell\u2019s analogy of intuition is this idea that the mind \u201cperfected the art of filtering the very few factors that matter from an overwhelming number of variables\u201d. I would argue that this is an imprecise analogy. Inferences or predictions made through intuition are in many cases extremely difficult to explain. The mind does not do \u201cfiltering\u201d, rather, it performs predictions in a massive parallel manner that appears to be a process of reduction, but is entirely different.\n\nGladwell popularized though the idea of intuition, enough to compel Gerd Gigerenzer, director and researcher at Max Planck Institute for Human Development, to write his book \u201cGut Feelings: The Intelligence of the Unconscious\u201d explaining his research in much greater detail. Gigerenzer defines intuitions as having the following attributes:\n\nThe open question though is, how does the brain improve intuitive thinking? Theo Humphries explores this in \u201cConsidering intuition in the context of Design and Psychology\u201d. Humphries challenges the ideas of Psychologists that have tagged the notion of intuition \u201cto the \u2018fringes\u2019 of the field of psychology, within the realms of parapsychology, telepathy and premonition \u201c. From a design perspective this is surprising to him, and he provides an enumeration of his surprise:\n\nIntuition, as understood from a design perspective, appears to be so important as one interacts with \u201cthe designed world\u201d. As an example, of a poor and good intuitive design:\n\nIntuitive designs that are labeled as \u2018intuitive\u2019 is an indicator of a mark of praise and intuition is a foundational concern for usability design. The paper explores the disconnect between the design and psychology community:\n\nDesigners however do not always work purely from their gut. Expert designers apparently have better intuition than novice designers. Experts have gained their knowledge through experience. In many complex fields, this kind of experience is captured in explicit form through the use of Design Patterns (alternatively \u201cPattern Languages\u201d):\n\nBrains are pattern recognition machines and with sufficient experience we begin to recognize not only more patterns, but increasing more complex patterns. This is what separates experts from novices. To appreciate \u201cpattern recognition\u201d Steve Blank, of Lean Startup fame, has a short video that expresses this best:\n\nThe practice of Design Patterns is that it captures these patterns (i.e. tacit knowledge or intuition) in a form that in a way that is collectively curated and communicable to a much wider audience. Rather than have each individual learn from experience on their own, collective wisdom is captured through Design Patterns. In some sense, Design Patterns are \u201cintuition that is industrialized\u201d.\n\nNow that we have these collection of patterns (codified or tacit), how then does this arrive at insight or innovation? What are the ways that we can process patterns? Technology Review reviews a recent research paper \u201cMathematical Model Reveals the Patterns of How Innovations Arises\u201d:\n\nIn the cited paper, that innovation is enabled by \u201cthe adjacent possible\u201d. That is those patterns that are one step away from existing learned patterns. So rather than developing patterns that have no connection, new patterns are realized through existing patterns and the thus new areas of unexplored patterns are discovered:\n\nSurprisingly enough, the statistical occurrence of innovations shows striking regularities that can be researched in greater depth.\n\nThis is where we make the segue (or is it quantum leap?) into Deep Learning and that cumbersome concept of generalization. Deep Learning networks are pattern recognition machines. In fact, they are constructed in a self-similar manner where pattern recognition exists at different scales. A self-similar fractal structure of recognition machines. That is, Deep Learning systems consist of collections of pattern recognition machines that are also composed of collection of pattern recognition machines. A recursive structure that terminates at each neuron which themselves are recognition machines.\n\nThey are designed to recognized patterns based on previously learned patterns. Generalization, in a general sense (or should I have used \u201cbroad\u201d instead?) is the recognition of new unseen patterns. The conjecture here however is that the class of unseen patterns are either of the class that is \u201ceasily imagined and expected\u201d or even better \u201can entirely unexpected and hard to imagine\u201d class. That is novelties versus innovations. The paper above seems to indicate that the mechanisms to discover the latter is the same mechanism as that of the former:\n\nThis is thus something extremely intriguing, if we assume that novelty discovery an intrinsic capability (i.e. generalizability) of DL systems, then perhaps so is innovative discovery. That is, a capability that goes well beyond what I had expected!\n\nTo summarize, intuition is a cognitive mechanism that performs massive parallel pattern recognition to arrive at predictions. Generalization is an emergent behavior that arises through the combination of recognized adjacent patterns. These patterns may be of the novel kind (i.e. previously unseen) or the unexpected kind (i.e. unexpected). We conjecture that we can leverage the concept of the \u2018adjacent possible\u2019 as an abstract explanation for generalization.\n\nOne other curious characteristic of intuition is that is has a timelessness quality to it. What I mean is that, when we put our intuition to work, we don\u2019t have an predictability as to when it reaches a good insight. It\u2019s like some backend parallel thinking machine that goes off on its own. How often do we experience discovering new insights by just sleeping on a problem? Our intuition seems to work overtime when our consciousness is not awake. On the flip side, it also works extremely quickly in a manner that is not observable by the conscious mind. This indicates to me that the parallel nature of intuition leaves it unable to accurately make sense of time.\n\nI leave you with the following quotation from an interestingly titled article \u201cHow Victory of Google\u2019s Go AI is Stroking fear in South Korea\u201d:\n\nTo get updates, subscribe to our newsletter https://www.getrevue.co/profile/intuitionmachine."
    },
    {
        "url": "https://medium.com/intuitionmachine/infographic-best-practices-in-training-deep-learning-networks-b8a3df1db53",
        "title": "Infographic: Best Practices for Training Deep Learning Networks",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/intuitionmachine/what-is-agi-99cdb671c88e",
        "title": "What is AGI? \u2013 Intuition Machine \u2013",
        "text": "Let\u2019s start at the beginning. Why do we even need this term?\n\n60 years ago when the term \u2018AI\u2019 was coined, the ambition was to build machines that can learn and reason like humans. Over several decades of trying and failing (badly), the original vision was largely abandoned. Nowadays almost all AI work relates to narrow, domain-specific, human-designed capabilities. Powerful as these current applications may be, they are limited to their specific target domain, and have very narrow (if any) adaptation or interactive learning ability. Most computer scientists graduating after the mid 80\u2019s only know AI from that much watered-down perspective.\n\nHowever, just after 2000 several of us felt that hardware, software, and cognitive theory had advanced sufficiently to rekindle the original dream. At that time we found about a dozen people actively doing research in this area, and willing to contribute to a book to share ideas and approaches. After some deliberation, three of us (Shane Legg, Ben Goertzel and myself) decided that \u2018Artificial General Intelligence\u2019, or AGI, best described our shared approach. We felt that we wanted to give our community a distinctive identity, to differentiate our work from mainstream AI which is unlikely to lead to general intelligence.\n\nThe term \u2018AGI\u2019 gave a name to this emerging group of researchers, scientists, and engineers who were actually getting back to trying to develop \u2018real AI\u2019. This \u2018movement\u2019 was officially launched with the publication of the book Artificial General Intelligence , and has since gathered momentum with additional publications and annual AGI conferences. By now, the term has become quite widely used to refer to machines with human, or super-human level capabilities.\n\nSome people have suggested using \u2018AGI\u2019 for any work that is generally in the area of autonomous learning, \u2018model-free\u2019, adaptive, unsupervised or some such approach or methodology. I don\u2019t think this is justified, as many clearly narrow AI projects use such methods. One can certainly assert that some approach or technology will likely help achieve AGI, but I think it is reasonable to judge projects by whether they are explicitly on a path (however far away it may be) to achieving the grand vision: a single system that can learn incrementally, reason abstractly, and act effectively over a wide range of domains \u2014 just like humans can.\n\nElsewhere I\u2019ve elaborated on what human intelligence entails; here I want to take a slightly different angle and ask \u201cWhat would it take for us to say we\u2019ve achieved AGI?\u201d. This is my proposed descriptive definition, followed by some elaboration:\n\nA computer system that matches or exceeds the real time cognitive (not physical) abilities of a smart, well-educated human.\n\nCognitive abilities include, but are not limited to: holding productive conversations; learning new commercial and scientific domains in real time through reading, coaching, experimentation, etc.; applying existing knowledge and skills to new domains. For example, learning new professional skills, a new language (including computer languages), or even novel games.\n\n\u201cMachines that can learn to do any job that humans currently do\u201d \u2014 I think this fits quite well, except that it seems unnecessarily ambitious. Machines that can do most jobs, especially mentally challenging ones would get us to our overall goal of having machines that can help us solve difficult problems like ageing, energy, pollution, and help us think through political and moral issues, etc. Naturally, they would also help to build machines that will handle remaining jobs we want to automate.\n\n\u201cMachines that pass the Turing Test\u201d \u2014 The current Turing Test asks too much (potentially having to dumb itself down to fool judges that it is human), and too little (limited conversation time). A much better test would be to see if the AI can learn a broad range of new complex human-level cognitive skills via autonomous learning and coaching.\n\n\u201cMachines that are self-aware/ can learn autonomously/ do autonomous reduction/ etc.\u201d \u2014 These definition grossly underspecify AGI. One could build narrow systems that have these characteristics (and probably have already), but are nowhere near AGI (and may not be on the path at all).\n\n\u201cA machine with the ability to learn from its experience and to work with insufficient knowledge and resources.\u201d \u2014 Important requirements but lacking specification of the level skill one expects. Again, systems already exist that have these qualities but are nowhere near AGI.\n\nWhy specify AGI in terms of human abilities? \u2014 While we\u2019d expect AGI cognition to be quite different (instant access to Internet, photographic memory, logical thinking, etc.), the goal is still to free us from most work. In order to do that it must be able to operate in our environment, and learn interactively via natural language and human interaction.\n\nWhy not require full sense acuity, dexterity, and embodiment? \u2014 I think that a reasonable relaxation of requirements is to initially exclude tasks that require high dexterity & sense acuity. The reason is that initial focus should be on cognitive ability \u2014 ie. a \u201cHelen Hawking\u201d (Helen Keller/ Stephen Hawking)\u201d The core problem is building the brain, the intelligence engine. It can\u2019t be totally disconnected from the world, but its senses/ actuators do not need to be very elaborate, as long as it can operate other machines (tool use)."
    },
    {
        "url": "https://medium.com/intuitionmachine/infographic-the-many-tribes-of-artificial-intelligence-af7652c08519",
        "title": "Infographic: The Many Tribes of Artificial Intelligence",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/intuitionmachine/infographic-10-truths-about-deep-learning-6114c8195545",
        "title": "Share This Infographic: 10 Truths about Deep Learning",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-slam-3d-maps-for-augmented-reality-and-robotics-will-be-worth-more-than-google-maps-32d4d41597b",
        "title": "Why SLAM 3D maps for Augmented Reality and Robotics will be worth more than Google Maps",
        "text": "Since the 90s I have been observing how the world has been changing due to GPS technology. If at first it was mostly used for military and industrial purposes, by 2012\u201313 years, this technology came into our lives fully and ubiquitously \u2014 we have been using maps on a daily basis for a long time already: for car navigation, restaurant search, shopping, appointments, etc.\n\nAlmost all infrastructure and transport industry in some ways relies on GPS technology.\n\nInterestingly, the core technology is relatively inexpensive nowadays, while maps, content and services that are based on it are considerably more expensive.\n\nThey have become the most valuable assets of the company.\n\nAfter the success of Google Maps, Apple, Baidu and others began to develop and implement their own maps.\n\nNevertheless, GPS has two major drawbacks \u2014 not very high level of precision (an error is around 6 meters) and the inability to work in enclosed spaces.\n\nThat is why the GPS-based maps cannot be used for navigation during the next wave of revolutionary devices that in 2\u20133 years will enter our daily lives \u2014 drones, home robots and AR / VR glasses.\n\nWhat kind of sensors and technologies can replace GPS satellites and ensure the accuracy of 1\u20132 centimeters indoor and outdoor?\n\nAs strange as it may sound, but good old cameras (including stereo cameras) and Lidar can be handy. Lidars and stereo cameras began to grow \u201csmarter\u201d due to SLAM technology and cheaper from 2011.\n\nEven now it is possible to create maps with the accuracy of 1\u20132 centimeters and provide autonomous behavior of drones and robots using SLAM.\n\nUsing a floor plan for an autonomous drone flight (in Augmented Pixels office):\n\nWhat will happen next and how will these technologies and sensors develop?\n\nWe are on the verge of the release in 2018\u20132019 of radically new phones, simple and intuitive for end users AR glasses, household robots and drones.\n\nAll these devices will be equipped with new sensors (i think in 99% cases with stereo cameras) and SLAM to provide indoor navigation and autonomy functions.\n\nEach device will create 3D SLAM map of the premises in which it is used.\n\nThe spread of such devices will help to create and maintain indoor and outdoor maps with minimal effort using crowdsourcing.\n\nThe next step after the maps are created is to merge them together and introduce to global users similarly to Google Maps; also they should be used during a human-drone/robot interaction.\n\nJust imagine that thanks to 3D SLAM cloud maps a man with a mobile phone and AR glasses will be able to interact with other people with mobile phones/AR Glasses and robots in real time in the same coordinate system.\n\nThis opens up great prospects for improvement of current patterns of behavior (indoor navigation, etc.) and for creation of fundamentally new services, patterns of behavior and accumulation of fundamentally new knowledge, which will exceed the value of Google Maps in many times!\n\nI hope that this article was helpful to you!\n\nInside of Augmented Pixels we have very interesting demos related with this topic, including a proprietary SLAM and cloud infrastructure \u2014 plz contact me directly in case of interest!"
    },
    {
        "url": "https://medium.com/intuitionmachine/artificial-intuition-a-breakthrough-cognitive-paradigm-3905c6d76561",
        "title": "Artificial Intuition \u2014 A Breakthrough Cognitive Paradigm",
        "text": "In a previous post, I introduced the Meta Meta-Model of Deep Learning. However, I did not introduce its details. A word of warning for the reader, the concepts in this section is in flux and in undergoing a lot of changes. Therefore, this article is just a reflection of my current understanding of the language of Deep Learning Meta Meta-Model. That\u2019s definitely a mouth full, so to make life simpler for everyone, I just call this the Deep Learning Canonical Patterns. These patterns are documented in the Deep Learning Design Patterns Wiki.\n\nIn this post I will explore further the characteristics of Artificial Intuition with the goal of describing a set of patterns that can aid us in formulating novel architectures for Deep Learning. In a previous post \u201cDeep Learning and Artificial Intuition\u201d, I introduced the idea that there are two distinct cognitive mechanisms, one based on logical inference and another based on intuition. At least 6 decades have been spent exploring cognitive mechanisms based on logical inference without making much progress towards AGI. Deep Learning, a breakthrough discovered in 2012, revealed an alternative promising research approach based on the a different cognitive paradigm.\n\nIn the field of Psychology, Kahneman and Tversky researches the interplay of these two kinds of cognitive function in a book \u201cThinking, Fast and Slow\u201d. The book has been highly praised:\n\nKahneman\u2019s book explores human cognitive biases and employs the dual cognitive processes as a root cause of these biases. In this post however, I will be exploring system 1 (i.e. intuition), more specifically artificial intuition and the mechanisms that give rise to it.\n\nThe origins of Deep Learning of course has had a long history. The approach originates from the Connectionist approach and derives much of its philosophy from ideas found in the Complexity sciences (see: \u201cTribes of AI\u201d). In a nutshell, the idea is that emergent complex behavior can arise from simple mechanisms. Chaos and complexity are the two driving forces that exist in complex systems. I wrote earlier about the connection of these in a post \u201cChaos, Complexity and Deep Learning\u201d.\n\nOur goal then is to either explain or better understand how emergent features arise through chaos and complexity. Here are some key features and some questions that require good answers:\n\nSelf-Organization: How does a system self-organize itself so that behavior required for survivability are encouraged and destructive behavior discouraged? How does complex organizational structure arise from simple structures?\n\nRobustness: How does a system organize itself to become more tolerant to failure? How does a system gain the adaptability required to survive in unexpected environments?\n\nDiversity: Adaptability and survivability requires diversity that may be less optimal than a homogeneous solution. Mixture of experts or ensemble methods point to the value of diversity in improving predictability.\n\nAbstraction: How does a system learn the abstractions required to perform accurate predictions in a hostile complex environment? How does generalization arise from the learning of abstractions?\n\nAdaptation: What mechanisms of adaption are necessary to compensate for incorrect predictions? How can a system forget learned behavior that may be detrimental to its survival?\n\nBounded Prediction: How the computational resources to perform predictions be bounded such that they can me made in a timely manner important for survivability? How can a system learn to optimize its predictions to fit within fixed bounds?\n\nCoordination: How can a system learn to coordinate its actions with other participating actors? An environment not only includes inanimate objects, but also other systems that have learning capabilities. How can a system not only learn its environment but also learn how to interact with other learning systems?\n\nThese features of complex adaptive systems all relate to a previous discussion on \u201c3 Essential Deep Learning Abilities\u201d. That is Expressibility, Trainability and Generalizability. One of the clear traps that exists among practitioners is that we can inadvertently bring in detrimental methods that originate from our mathematical or engineering training. That is, we take ideas such as the need for optimality, the requirement for sparse solutions, the need for interpretability and understandable solutions, the need for completeness and repeatable guaranteed behavior. These needs are of course desirable, however we should not optimize for these as a starting point. This leads to pre-mature optimization, an idea that we are all familiar with in computer science. Rather, we should all embrace first complexity and chaos and work out solutions that holistically incorporate these as a given. Research in a Deep Learning is a major paradigm shift and thus requires a different kind of thinking.\n\nThe first big conceptual leap that we have to make is to understand that learning systems evolve in non-equilibrium settings. I write about this in brief detail in a post \u201cNon-Equilibrium Information Dynamics.\u201d Stated in a different way, researchers should be very cautious about employing statistical or alternatively bulk thermodynamic metrics in their analysis of these systems. It is my belief that one of the most glaring inappropriate tools in the study of AI is the use of Bayesian methods. I can understand its utility in the domain of logical inference, however I doubt its effectiveness in a domain of intuitive systems.\n\nThe second conceptual leap is to understand that our of what \u201cGeneralization\u201d means is quite grossly inadequate. The use of the term in Machine Learning is extremely liberal. Furthermore, the Machine Learning approach of \u2018curve fitting\u2019 and thus interpolation and therefore generalization between adjacent points in the fitted curve, breaks down under the recently discovered notion of rote memorization of Deep Learning. How rote memorization can lead to generalization is a fuzzy idea at best. In fact, Kahneman\u2019s research points out that human cognitive biases exists because of flawed reasoning in our intutitive system 1 inference. Said in otherwords, very poor and flawed generalization. To conclude, rote memorization leads to a kind of generalization that is inherently flawed!\n\nA third conceptual leap is to accept that Deep Learning systems may be computational systems just like Von Neumann computers. The primary difference is that there is a discovered mechanism (SGD) for these system to learn from data as opposed to computers that require programmers. Neural Networks are usually treated as continuous dynamical systems. Deep Learning systems have one common requirement, in that the computational layers must be differentiable. Computers by contrast do not have differentiable subcomponents. Cellular Automata (used in Evolutionary paradigms) also do not have differentiable components. Cellular Automata and Genetic Algorithms aren\u2019t as successful in learning from data as DL. Yet, if all DL does is rote memorization, then they aren\u2019t very different from Von Neumann computers. At the core though, DL consists mostly of threshold units that are not that remotely distinct from NAND/NOR gates that we find in logic circuitry. Why then is differentiability such an important requirement for trainability? Are continuous dynamical systems a real requirement or are we overlooking a more general principle? Why does SGD lead to learning? As we track the latest research in DL, we are beginning to discover that DL looks more and more like Von-Neumann computers (see: \u201cConditional Logic\u201d) and less like the simple dynamical systems we find in Physics. I think we can draw some inspiration in the complexity of \u2018random boolean networks\u2019 that describe biological processes.\n\nThere remains plenty of open questions on the true nature of artificial intuition systems. Mankind has stumbled on a kind of artificial intuition in the form of Deep Learning, however is it possible to discover other kinds of architectures that exhibit similar capabilities? As of this writing, we have not found alternatives. However, one should realize though that we know of at least two kinds of architectures that lead to intuition. That is Deep Learning and biological brains. Although these two systems are functionally similar, the computational mechanisms are like night and day (see: \u201cMisconceptions of Deep Learning\u201d).\n\nP.S. In a related news, MIT is mining the intuition of its students to arrive at better algorithms for planning."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-and-artificial-intuition-8967c2d69724",
        "title": "Deep Learning, Artificial Intuition and the Quest for AGI",
        "text": "There is a renaissance happening in the field of artificial intelligence. For many long term practitioners in the field, it is not too obvious. I wrote earlier about the push back that many are making against the developments of Deep Learning. Deep Learning is however an extremely radical departure from classical methods. One researcher who recognized that our classical approaches to Artificial General Intelligence (AGI) were all but broken is Monica Anderson.\n\nAnderson are one of the few researchers that recognized early on that the scientific approach of reductionism was fatally flawed and was leading research astray in our quest for AGI. One very good analogy that highlights the difference between Deep Learning and classical A.I. approaches, is the difference between intuition and logic. Dual Process Theory theorizes that there are two kinds of cognition:\n\nClassical A.I. techniques has focused mostly on the logical basis of cognition, Deep Learning by contrast operates in the area of cognitive intuition. Deep Learning systems exhibit behavior that appears biological despite not being based on biological material. It so happens that humanity has luckily stumbled upon Artificial Intuition in the form of Deep Learning.\n\nThe argument that Anderson brings forward is that to build systems with capabilities of intuition, one cannot be dependent on constructions that are based on \u2018Reductionist Methods.\u2019 Note that Anderson also recognizes Deep Learning as an alternative approach to Artificial Intuition. Anderson characterizes Reductionist methods as having the following characteristics:\n\nAnderson conjectures that the logic based approach needs to be abandoned in favor of an alternative \u2018model-free\u2019 approach. That is, intuition based cognition cannot arise from reduction based principles. What Anderson describes a \u2018model-free\u2019 are \u2018unintelligent components\u201d, that is she writes:\n\nAnderson proposes several \u2018model-free\u2019 mechanisms, that the combination of which, can lead to emergent behavior that we see in intuition. Here are the mechanisms that I speculate are being explored:\n\nNote that the details of Anderson\u2019s strategy are not made public for intellectual property reasons. I do agree that there are computational mechanisms, such as those described above, that are simple (or unintelligent) in their underlying mechanism that can certainly lead to complex (or intelligent) emergent behavior. We only need to realize that universal computation can arise through the presence of just 3 fundamental mechanisms (i.e. computation, signaling and memory).\n\nI agree though that there are limitations to our present day mathematics that serves as a insurmountable barrier towards progress. This however does not imply though that we cannot make progress because of our collective weakness in our mathematical tools. On the contrary, Deep Learning researchers have made considerable progress via experimental computational means. Engineering and practice has well out paced theory and this will continue to be the trend. I\u2019ve written earlier about 3 areas that we have trouble analyzing, that is the notion of time, emergent collective behavior and meta-level reasoning. We thus are in a fork in the road, we can either wait for our mathematical language to make a quantum leap or we can soldier on with the expectation that there comes a point where analytic techniques have limited capabilities in the realm of universal computation.\n\nMy personal bias is that a method must not require global knowledge of its agents to be successful. In other words, all agents must decide on their actions based solely on local knowledge and that emergent global behavior arises through the individual local interactions of its constituents. In many alternative models that have been proposed, there usually is an underlying assumption that some global knowledge is required by its agents to function. I am also biased towards methods that have been known to work. So far, Deep Learning is the only Artificial Intuition that has been shown to work very well. The one conceptual problem though of DL is that SGD requires global knowledge that gets back-propagated to its constituents. This requirement though is likely to be relaxed in Modular Deep Learning systems. Therefore I prefer the term \u2018localized models\u2019 and not \u2018model free models\u2019, even though both mean a measure of lack of intelligence of constituent parts. In general though, this is similar to the concept of \u2018parsimony\u2019, in that we seek the simplest of mechanisms that give rise to emergent properties.\n\nMy current analysis, based on a meta meta-model deconstruction of Deep Learning is that we are still missing the building block mechanisms that can lead to capabilities such as domain adaptation, transfer learning, continuous learning and multitask learning. In fact, we are still unable to have a solid grasp of unsupervised learning (i.e. predictive learning). These mechanisms may emerge from our existing building blocks, but it is equally likely that they could be accidentally discovered. Very similar to the discoveries in Conway\u2019s game of life, such as the following:\n\nDespite the simplicity of the cellular automata above, we are incapable of understanding why it behaves the way it does. There is very little theory that we can bank on. I believe a similar situation also exists with our quest for AGI. It is indeed possible that we already have all the building blocks like Conway\u2019s game of life. Alternatively, we have missing mechanisms that have yet to be discovered, similar to the spaceship above. This should explain to anyone the level of unpredictability of our progress in AGI. We may just be an accidental discovery away from achieving AGI.\n\nThere is one additional topic that I should cover before I end this article. That is the notion of consciousness that I discussed in a previous post: \u201cModular Deep Learning could be the Penultimate Step to Consciousness\u201d. In that post, I brought up two theories that have explanations for consciousness, the IIT model and Schmidhuber\u2019s dual RNN model. In the Dual Process theory model, consciousness arises from logical cognition. All 3 theories, curious enough, have a mechanism to track causality between events. It just seems that you can\u2019t separate the notion of consciousness and the notion of tracking cause and effect. More abstractly, tracking behavior that evolves over time seems to be an essential ingredient.\n\nDo yourself a favor, make sure you don\u2019t miss any Deep Learning developments. Subscribe to our newsletter: https://www.getrevue.co/profile/intuitionmachine."
    },
    {
        "url": "https://medium.com/intuitionmachine/deepmind-fuses-game-theory-and-deep-learning-661ec205a396",
        "title": "Greed, Fear, Game Theory and Deep Learning \u2013 Intuition Machine \u2013",
        "text": "In a previous story, I wrote about how a Game Theoretic approach was influencing developments in the Deep Learning field. In this story, I now write about DeepMind\u2019s latest foray into this exciting area. In a recent blog post (i.e. Yesterday, February 19th 2017), DeepMind presents their latest research on this subject titled \u201cUnderstanding Agent Cooperation\u201d.\n\nThe gist of the research is that, they employed Deep Reinforcement Learning networks in two game environments to study their behavior. The motivation is to study multi-agent systems to better understand and control these kinds of systems. In a previous story (see: \u201cFive Capability Levels of Deep Learning\u201d, I laid out a road map as to how Deep Learning will evolve in even greater capabilities. For discussion sake, I re-summarize it here again:\n\nThis level includes the fully connected neural network (FCN) and the convolution network (CNN) and various combinations of them.\n\nThis level includes memory elements incorporated with the C level networks.\n\nThis level is somewhat similar to the CM level, however rather than raw memory, the information that the C level network is able to access is a symbolic knowledge base.\n\nAt this level, we have a system that is built on top of CK, however is able to reason with imperfect information.\n\nThis level is very similar to the \u201ctheory of mind\u201d where we actually have multiple agent neural networks combining to solve problems.\n\nAs we see from the above classification, the most advanced kind of Deep Learning system will involve multiple neural networks that either cooperate or compete to solve problems. The core problem of a multi-agent approach is how to control its behavior. In another story, I address this by proposing the the use of market driven mechanisms as a means of control (see: \u201cEquilibrium Discovery in Modular Deep Learning\u201d). It turns out that, DeepMind has been researching in this approach for a while. The DeepMind paper studies multi-agent systems from a similar economic perspective (i.e. incentive driven approach):\n\nDeepMind researchers explored two games, \u201cGathering\u201d and \u201cWolf Pack\u201d. The agents would have to learn either a cooperative or competitive strategy. In the \u201cGathering\u201d game, when scarcity was introduced into the environment, agents with complex strategies tended to pursue more aggressive competitive strategies. In the \u201cWolf Pack\u201d game that was designed to encourage cooperative behavior, agents learning complex strategies did not necessarily led to greater cooperative behavior.\n\nThe primary value of the research is that it gives us an understanding of the many knobs (i.e. discount factor, batch size, network size) that can be tweaked to arrive at different network behaviors. The paper has a very interesting chart the maps out the agent\u2019s behavior ( \u201cGathering\u201d on the left and \u201cWolfpack\u201d on the right ):\n\nVery interesting that the axis are marked \u201cGreed\u201d and \u201cFear\u201d, what better motivators are there anyway? (Video lecture: https://www.youtube.com/watch?v=yE62Zwhmzi8 )\n\nDeepMind isn\u2019t alone in its research of Multi-agent systems and Deep Learning. Maluuba ( Recently acquired by Microsoft ) has also had active research. In a paper, published prior to acquisition, \u201c Improving Scalability of Reinforcement Learning by Separation of Concerns\u201d:\n\nThe graph below compares the \u201cSeparation of Concerns\u201d (SOC) multi-agent approach versus a conventional approach:\n\nThe uniqueness of Maluuba\u2019s approach is that the reward function of each agent depends not only on environmental state but also on the communication actions of the other agents. Depending on the composition of these agents, agents will have varying degrees of coupling, and thus independence. This coupling can vary dependent on the context and situation. So for example, in contexts with high environment reward an agent may act independently. While in contexts of low environment reward, an agent will act in more in relationship with other agents.\n\nMaluuba\u2019s research indicates a more hierarchical \u201ccommand and control\u201d coordination mechanism as opposed to a market driven distributed control. It is however, very likely that we shall see hybrid combinations of these coordination methods employed rather a \u201cpurist\u201d approach to coordination.\n\nIn an even older research at FAIR (FaceBook AI Research), \u201cLearning Multiagent Communication with Backpropagation\u201d investigates an approach for cooperative behavior using backpropagation. The research shares commonalities with the Maluuba research in that the agents balance their behavior with the policy that is being learned and the communication between agents:\n\nIt is important to note that in all three research, cooperation or competitive strategies are learned by the Deep Learning agents. There are still many open question about this kind of research. The big take away though is that, this kind of work is performed by the giants of the field, DeepMind (Google), Maluuba (Microsoft) and FaceBook. It is an indicator as to where Deep Learning research is heading. I therefore hope this article has instilled enough \u201cgreed\u201d or \u201cfear\u201d to motivate one to keep abreast of Game Theory developments and Deep Learning."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-two-paths-from-natural-language-processing-to-artificial-intelligence-d5384ddbfc18",
        "title": "The Two Paths from Natural Language Processing to Artificial Intelligence",
        "text": "Why isn\u2019t Siri smarter? AI has accelerated in recent years, especially with deep learning, but current chatbots are an embarrassment. Computers still can\u2019t read or converse intelligently. Their deficiency is disappointing because we want to interact with our world using natural language, and we want computers to read all of those documents out there so they can retrieve the best ones, answer our questions, and summarize what is new.\n\nTo understand our language, computers need to know our world. They need to be able to answer questions like \u201cWhy does it only rain outside?\u201d and \u201cIf a book is on a table, and you push the table, what happens?\u201d\n\nWe humans understand language in a way that is grounded in sensation and action. When someone says the word \u201cchicken,\u201d we map that to our experience with chickens, and we can talk to each other because we have had similar experiences with chickens. This is how computers need to understand language.\n\nThere are two paths to building computers with this kind of understanding. The first path is a symbolic one that is traversed by hard-coding our world into computers. To follow the symbolic path, we segment text into meaningless tokens that correspond to words and punctuation. We then manually create representations that assign meanings to these tokens by putting them into groups and creating relationships between the groups. With those representations, we build a model of how the world works, and we ground that model in the manually created representations.\n\nThe second path is sub-symbolic, which we initially follow by having computers learn from text. This path is synonymous with neural networks (also called deep learning), and it begins with representing words as vectors. It then progresses to representing whole sentences with vectors, and then to using vectors to answer arbitrary questions. To complete this path, we must create algorithms that allow computers to learn from rich sensory experience that is similar to our own.\n\nThe symbolic path is a long one. A depressing amount of work in NLP is done without even using the meanings of words. Words are represented by tokens whose only designation is a unique number. This means that the token for \u201crun\u201d and the token for \u201cjog\u201d have different numbers and are therefore as different from each other as the token for \u201crun\u201d is from the token for \u201cChicago.\u201d Even worse, a sentence is represented not as a sequence of tokens, but as a collection of tokens that doesn\u2019t consider order, called a bag-of-words. This means that \u201cdog bit man\u201d will have the same representation as \u201cman bit dog.\u201d\n\nOne popular method assigns an index number to each word and represents a document by creating a vector where index i is the count of the number of times word i occurs in the document. This means that if you have a vocabulary size of 50,000, the vector representing a document will have 50,000 dimensions, where most of them have a count of 0 because their corresponding word is not in the document. The method can get a little fancier by weighing the count of each word by the rarity of that word in all of the documents. This is the classic term-frequency inverse-document frequency (tf-idf) method. Using tf-idf, one can find similar documents to a given one; or, if you have a lot of labeled documents, the vectors can then be plugged into a regular supervised learning algorithm to label unseen documents.\n\nA second popular natural language processing (NLP) method that treats words as meaningless tokens is topic modeling. One way to do topic modeling is called Latent Dirichlet Allocation (LDA). LDA starts from a fixed number of topics. Each topic is represented as a distribution over words, and each document is then represented as a distribution over topics. Although the tokens themselves are meaningless, the probability distributions over words provided by the topics provide a sense of the different ideas contained in the documents.\n\nWe get the first hint of meaning in sentiment analysis. Sentiment analysis seeks to automatically determine how a writer feels about what he or she has written, and it will tell you, for example, if a reviewer liked your product based on the words used in the review. One common method for sentiment analysis is to give each word a score as being positive or negative. So \u201chappy\u201d and \u201cjoyful\u201d would be positive, like 2.0, but \u201cpainful\u201d would be negative, like -3.0. Then you sum up the scores of all the words in a given text to find out if the text is positive or negative. \u201cI am happy\u201d would be positive because the word \u201chappy\u201d would have a positive value in the sentiment dictionary. Here there is no real understanding, only table lookups.\n\nThe next step along the symbolic path to meaning is to manually construct representations, which means telling the computer what things mean by creating symbols and specifying relationships between symbols. To \u201cunderstand\u201d text, a computer can then map text to symbols, which allows us to define what the computer should do for this smaller number of symbol configurations. For example, there might be a lot of ways of mentioning that you drive a car in a tweet, but as long as all of those are mapped to the symbol Vehicle, and the symbol Vehicle is specified as being related to Tires, a tire company can know that you are a potential customer.\n\nLet\u2019s take a look at some existing representations. The most famous representation is WordNet. In WordNet, the symbols are groups of words that have the same meaning, called synsets. One synset could be the set consisting of \u201ccar\u201d and \u201cautomobile.\u201d Each word can be in multiple synsets. For example, \u201cbank\u201d could be in the synset that means river bank and also in the synset that means a place where money is deposited. There are a few kinds of relationships between synsets, such as has-part, superordinate (more general class), and subordinate (more specific class). For example, the synset containing \u201cautomobile\u201d is subordinate to the one containing \u201cmotor vehicle\u201d and superordinate to the one containing \u201cambulance.\u201d\n\nConceptNet is a representation that provides commonsense linkages between words. For example, it states that bread is commonly found near toasters. These everyday facts could be useful if you wanted to make a boring chatbot; \u201cSpeaking of toasters, you know what you typically find near them? Bread.\u201d But, unfortunately, ConceptNet isn\u2019t organized very well. For instance, it explicitly states that a toaster is related to an automobile. This is true, since they are both machines, but trying to explicitly enumerate everything that is true is hopeless.\n\nOf course, it seems like Wikipedia already has all the information that computers need. We even have a machine-readable form of Wikipedia called DBpedia, and DBpedia and WordNet have been combined into a representation called YAGO (Yet Another Great Ontology). YAGO has good coverage of named entities, such as entertainers, and it was used by Watson to play Jeopardy!, along with other sources. YAGO and DBpedia contain a lot of facts, but they aren\u2019t the basic facts that we learn as young children, and their representations are shallow.\n\nWe need deep representations because classification and hierarchy are efficient ways of specifying information. The better your organization, the more power your statements about the world have, and many statements aren\u2019t even necessary, like saying that a toaster is related to an automobile. One representation that organizes concepts down to the lowest level is SUMO (Suggested Upper Merged Ontology). For example, in SUMO, \u201ccooking\u201d is a type of \u201cmaking\u201d that is a type of \u201cintentional process\u201d that is a type of \u201cprocess\u201d that is a \u201cphysical\u201d thing that is an \u201centity.\u201d\n\nSUMO specifies an ontology, explicitly laying out what is, but for machines to really understand us they need to share our experiences. SUMO doesn\u2019t have anything to say on what it is like to experience the world as a human. Fortunately, common experiences, like making a purchase, can be represented by FrameNet. In FrameNet, the frame for a purchase transaction specifies roles for the seller, the buyer, and the thing being purchased. Even more fundamental to the human experience is image schemas. We use image schemas to comprehend spatial arrangements and physics, such as path, containment, blockage, and attraction. Abstract concepts such as romantic relationships and social organizations are represented as metaphors to this kind of experience. Unfortunately, there currently is no robust implementation of image schemas. Representations for image schemas need to be built, and then we have to merge all of our manually created representations together. There has been some work doing this merging. There is a representation called YAGO-SUMO that merges the low-level organization of SUMO with the instance information of YAGO. This is a good start, especially since YAGO also builds on WordNet.\n\nThese representations enable computers to dissect the world and identify situations, but with the possible exception of the poorly organized ConceptNet, they don\u2019t specify at the most basic level how the world is or how it changes. They don\u2019t say anything about cats chasing mice or about flicking light switches to illuminate rooms. Computers need to know all of these things to understand us because our language evolved not to describe our world as it is but rather to communicate only what the listener does not already know.\n\nTo reach artificial intelligence by following the symbolic path, we must create a robust world model built on a merged version of these manual representations. The longstanding project Cyc has a large model that uses representations, but it is built on logic, and is not clear if logic is sufficiently supple. We will know when we have reached success when we can ask a computer, \u201cWhy does it only rain outside?\u201d and it responds \u201cA roof blocks the path of things from above.\u201d Or, if we asked, \u201cExplain your conception of conductivity?\u201d and it said \u201cElectrons are little spheres, and electricity is little spheres going through a tube. Conductivity is how freely the spheres can move through the tube.\u201d These answers would be from first principles, and they would show that the computer could combine them flexibly enough to have a real conversation.\n\nThe sub-symbolic path begins with assigning each word a long sequence of numbers in the form of a vector. Word vectors are useful because you can calculate the distance between them. The word vector for the word \u201crun\u201d will be pretty close to the word vector for the word \u201cjog,\u201d but both of those word vectors will be far from the vector for \u201cChicago.\u201d This is a big improvement over symbols, where all we can say is that going for a run is not the same as going for a jog, and neither are the same as Chicago.\n\nThe word vector for each word has the same dimension. The dimension is usually around 300, and unlike tf-idf document vectors, word vectors are dense, meaning that most values are not 0. To learn the word vectors, the Skip-gram algorithm first initializes each word vector to a random value. It then essentially loops over each word w1 in all of the documents, and for each word w2 around word w1, it pushes the vectors for w1 and w2 closer together, while simultaneously pushing the vector for w1 and the vectors for all other words farther apart.\n\nThe quote we often see associated with word vectors is \u201cYou shall know a word by the company it keeps\u201d by J. R. Firth (1957). This seems to be true, at least to a degree, because word vectors have surprising internal structure. For example, the classic result is that if you take the word vector for the word \u201cItaly\u201d and subtract the word vector for the word \u201cRome,\u201d you get something very similar to what you get when you subtract the word vector for the word \u201cParis\u201d from the word vector for the word \u201cFrance.\u201d This internal structure of word vectors is impressive, but these vectors are not grounded in experience in the world \u2014 they are only grounded in being around other words. As we saw previously, people only express what the reader does not know, so the amount of world knowledge that can be contained in these vectors is limited.\n\nJust as we can encode words into vectors, we can encode whole sentences into vectors. This encoding is done using a recurrent neural network (RNN). An RNN takes a vector representing its last state and a word vector representing the next word in a sentence, and it produces a new vector representing its new state. It can keep doing this until the end of the sentence, and the last state represents the encoding of the entire sentence.\n\nA sentence encoded into a vector using an encoder RNN can then be decoded into a different sentence. This decoding uses another RNN and goes in reverse. This decoder RNN begins with its state vector being the last state vector of the encoder RNN, and it produces the first word of the new sentence. The decoder RNN then takes that produced word and the RNN vector itself as input and returns a new RNN vector, which allows it to produce the next word. This process can continue until a special stop symbol is produced, such as a period.\n\nThese encoder-decoder (sequence-to-sequence) models are trained on a corpus consisting of source sentences and their associated target sentences, such as sentences in English and their corresponding translations into Spanish. These sentences are run through the model until it learns the underlying patterns. In fact, one current application for these models is machine translation. This is how Google Translate works. In fact, this general method works for many kinds of problems. For example, if you can encode an image using a neural network (such as a convolutional neural network) into a vector, and if you have enough training data, you can automatically generate captions for images the model has never seen before.\n\nThese encoder-decoder models work on many kinds of sequences, but this generality highlights their limitation for use as language understanding agents, such as chatbots. Noam Chomsky proposed that the human brain contains a specialized universal grammar that allows us to learn our native language. In conversations, sequences of words are just the tip of the meaning iceberg, and it is unlikely that a general method run over the surface words in communication could capture the depth of language. Language allows for infinite combinations of concepts, and any training set, no matter how large, will represent only a finite subset.\n\nBeyond the fact that the sequence-to-sequence models are too general to fully capture language, you may wonder how a fixed-length vector can store a growing amount of information as the model moves from word to word. This is a real problem, and it was partially solved by adding attention to the model. During decoding, for example when doing language translation, before outputting the next word of the target sentence, the model can look back over all of the encoded states of the source sentence to help it determine what the next word should be. The model learns what kinds of information it needs in different situations. It treats the encodings of the input sentence as a kind of memory.\n\nThis attention mechanism can be generalized to enable computers to answer questions based on text. The question itself can be converted into a vector, and instead of looking back through words in the source sentence, the neural network can look at encoded versions of the facts it has seen, and it can find the best facts that will help it answer the current question.\n\nCurrent question-answering algorithms are trained using generated stories. A story might be like, \u201cBob went home. Tim went to the junkyard. Bob picked up the jar. Bob went to town.\u201d A question for this story might be \u201cWhere is the jar.\u201d The network can learn to say that \u201ctown\u201d is the answer, but there is no larger understanding. The network is learning linkages between sequences of symbols, but these kinds of stories do not have sufficiently rich linkages to our world. It won\u2019t help to have these algorithms learn on real stories because, as we have seen, we communicate only what isn\u2019t already covered by shared experience. In a romance novel, you will never read, \u201cIn his passion for her, he shoved the table between them aside, and all of the objects on the table moved because gravity was pushing them down, creating friction between the bottoms of the objects and the support surface of the table.\u201d\n\nTo train machines so that they can talk with us, we need to immerse them in an environment that is like our own. It can\u2019t just be dialog. When we say \u201cchicken,\u201d we need the machine to have had as much experience with chickens as possible, because to us a chicken isn\u2019t just a bird, it\u2019s everything one can do with it and everything it represents in our culture. There has been work in this direction. For example, OpenAI now allows us to train machines by playing video games, and as our virtual worlds become more like our real one, this kind of training will be increasingly useful. In addition to producing ever-better virtual worlds, computers are spending more time in our physical one. Amazon Alexa in listening attentively in many of our homes. Imagine if she had a camera and a rotating head. Could she watch our eyes and our actions to learn a partially grounded understanding of our lives?\n\nWe have seen two potential progressions from natural language processing to artificial intelligence. For the symbolic path, we need to build world models based on deep and organized representations. Success on this path requires that the models we build be comprehensive and flexible. For the sub-symbolic path, we need to train large neural networks in an environment with similar objects, relationships, and dynamics as our own. On this path, the learning agent must be able to perceive this environment at a low-enough level so that the outlines of how we humans experience our environment are visible. Along either path, we can see that Searle\u2019s Chinese room scenario, where a person who doesn\u2019t know Chinese can maintain written conversations in Mandarin by looking up exactly what to do for each stroke of each character, isn\u2019t really possible. To have a real conversation, comprehension needs to ground out in shared first principles, and when a computer can do that, it will understand as well as you or I."
    },
    {
        "url": "https://medium.com/intuitionmachine/eight-deserving-deep-learning-papers-that-were-rejected-at-iclr-2017-119e19a4c30b",
        "title": "Ten Deserving Deep Learning Papers that were Rejected at ICLR 2017",
        "text": "I first wrote about the deluge of papers that were submitted to ICLR 2017. The paper I described \u201cRethinking Generalization\u201d. Very curious to note that the papers described in my posts just happened to mysteriously jump to the top of the list: http://prlz77.github.io/iclr2017-stats/ \u00af\\_(\u30c4)_/\u00af.\n\nNow that the list has be culled down to a few \u201cdeserving\u201d submissions, I will take this opportunity to highlight the some excellent papers that for one reason or another did not make the cut. There\u2019s a lot of subjectivity that goes on in judging papers and a lot of times it is dependent on the present world view of its reviewers. In the cut-throat research environment, not every research can make the cut. That\u2019s just the unfortunate reality. However, I would like to take this opportunity to mention some good papers that deserve to make the cut.\n\nA Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks by Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher. One of the really novel papers out there that show how to incrementally grow a neural network. Absolutely shocking that this paper was rejected. The reason why this paper is important is that it shows how a network can grow through the use of transfer learning and domain adaptation. There are not many papers that explore this area.\n\nHierarchical Memory Networks by Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, Yoshua Bengio. Another NLP paper, very surprising that this gets rejected consider the all-star author list. This is one of the first papers out there that explores the notion of a hierarchy of memory. Most memory augmented networks tend to have flat memory structures. The paper should not have been dismissed so lightly.\n\nRL\u00b2: Fast Reinforcement Learning via Slow Reinforcement Learning by Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever, Pieter Abbeel. The reviewers must be smoking something to not be convinced that this is ground breaking research! I guess the were not impressed with the \u201cRL\u00b2\u201d naming convention. Anything about meta-learning should be selling like hotcakes, yet this paper, despite having prominent authors, gets slammed. Unimaginable!\n\nDemystifying ResNet by Sihan Li, Jiantao Jiao, Yanjun Han, Tsachy Weissman. I liked this paper because it gave some insightful rules of thumb on how to make use of residual or skip connections. 2016\u2019s hottest innovation, and some folks make an attempt at deconstructing the technique, yet get slammed for their efforts. One complaint was that simplified models were used in this study. This complaint of course is absurd, of course you want to use simplified models to characterize an otherwise complex system.\n\nA Neural Knowledge Language Model by Sungjin Ahn, Heeyoul Choi, Tanel Parnamaa, Yoshua Bengio. Yet another NLP paper that gets rejected. Fusing knowledge bases with Deep Learning should be a very big thing, yet this paper gets dismissed for lack of novelty. The main complaint was the writing style, which is just unfortunate.\n\nKnowledge Adaptation: Teaching to Adapt by Sebastian Ruder, Parsa Ghaffari, John G. Breslin. I did not notice this paper until I made a second pass through the rejection list. I\u2019m a bit biased here in that I\u2019m always seeking out work on Domain Adaptation and Transfer Learning. This paper has some very nice ideas. Unfortunately, it doesn\u2019t seem to make the cut of the esteemed reviewers.\n\nTensorial Mixture Models by Or Sharir, Ronen Tamari, Nadav Cohen, Amnon Shashua. I\u2019m a big fan of this paper, see \u201cHolographic Models\u201d. Unfortunately, the skepticism of the reviewers were too high to overcome.\n\nOn the Expressive Power of Deep Neural Networks by Maithra Raghu, Ben Poole, Jon Kleinberg, Surya Ganguli, Jascha Sohl-Dickstein. Research is really going to the dogs if fundamental theoretical and experimental papers like this get tossed aside in favor of the usual alchemy that stands in for \u201cDeep Learning research\u201d.\n\nEigenvalues of the Hessian in Deep Learning: Singularity and Beyond by Levent Sagun, Leon Bottou, Yann LeCun. Wow, an egg-in-the-face for these prominent authors. I guess fundamental experimental data just isn\u2019t sexy enough to make the cut. The comment was, \u201cinteresting empirical data, but no theory\u201d. Talk about being completely unrealistic expectations.\n\nAn Empirical Analysis of Deep Network Loss Surfaces by Daniel Jiwoong Im, Michael Tao, Kristin Branson. This is a shocker in that I mentioned it in my Rethinking Generalization post about additional evidence about the SGD being an implicit regularization. What I had failed to do in that post was place a hyper-link to the paper. But it is indeed surprising that researchers who have dug up some very impressive data are left with nothing but the humiliation of a rejected paper.\n\nResearch that boldly tries to improve our understanding and experience should not be penalized because of writing style or not having exhaustively enough data. At the bleeding edge, having the kinds of data and experiments are more difficult to come by. I see one of the problems of research that\u2019s novel and innovative is that it\u2019s unfamiliar to the reviewers. This unfamiliarity demands the paper live up to higher standards, and unfortunately due to the newness of the approach, the authors are unable to deliver in a timely manner.\n\nThere\u2019s just too little credit given to research work that perform experimental studies on the nature of Deep Learning. In these situations, it comes with the territory that simplified models are used so as to be able to perform tractable analysis. However, one should not always expect a strong theoretical result, rather the experimental results in themselves are valuable enough in that they give a characterization of how these machines behave. In the absence of this kind of research, we will be mostly in the dark with our alchemy.\n\nIn the end though, there\u2019s a lot of subjectivity involved in determining the merits of a research paper. It is too easy to forget that there was a time in the recent past that papers on Convolution Networks were very commonly rejected in Computer Vision conferences. See Yann LeCun (in 2012):\n\nOne major concern is that the current research environment is going to get a lot worse for Deep Learning researchers. The field is moving too rapidly and it is very easy to find reviewers who have a world view that isn\u2019t current with the latest research. So you really end up with reviews that criticize style versus substance. The number of good papers that have been rejected is a reflection of thisknowledge (or perhaps cultural) gap.\n\nP.S. LipNet: End-to-End Sentence-level Lipreading is one other paper that had received a lot of heat for its rejection. I\u2019m unfortunately not in a position to jump into that fracas considering it\u2019s not a domain that I\u2019m familiar enough with.\n\nUpdate: Andrej Karpathy has run some analysis based on statistics gathered from his Arxiv-Sanity site: \u201cICLR 2017 vs arxiv-sanity\u201d. I checked his numbers and it is appears that Arxiv-sanity popular numbers are skewed towards implementation ideas (crazy ideas are more popular) and not fundamental research. The last 7 papers, don\u2019t reach the Arxiv-sanity popularity cutoffs."
    },
    {
        "url": "https://medium.com/intuitionmachine/pathnet-a-modular-deep-learning-architecture-for-agi-5302fcf53273",
        "title": "DeepMind\u2019s PathNet: A Modular Deep Learning Architecture for AGI",
        "text": "PathNet is a new Modular Deep Learning (DL) architecture, brought to you by who else but DeepMind, that highlights the latest trend in DL research to meld Modular Deep Learning, Meta-Learning and Reinforcement Learning into a solution that leads to more capable DL systems. A January 20th, 2017 submitted Arxiv paper \u201cPathNet: Evolution Channels Gradient Descent in Super Neural Networks\u201d (Fernando et. al) has in its abstract the following interesting description of the work:\n\nUnlike more traditional monolithic DL networks, PathNet reuses a network that consists of many neural networks and trains them to perform multiple tasks. In the authors experiments, they have shown that a network trained on a second task learns faster than if the network was trained from scratch. This indicates that transfer learning (or knowledge reuse) can be leveraged in this kind of a network. PathNet includes aspects of transfer learning, continual learning and multitask learning. These are aspects that are essential for a more continuously adaptive network and thus an approach that may lead to an AGI (speculative).\n\nto get a better understanding of the techniques that were employed. A PathNet consists of layers of neural networks where the interconnection between each network in a layer is discovered through different search methods. In the figure above, the configurations are constrained to select 4 networks per layer at a time. The paper describes two discovery algorithms, one based on a genetic (evolutionary) algorithm and another one based on A3C reinforcement Learning.\n\nThe authors cite inspiration from \u201cOutrageously Large Neural Networks\u201d (from Google Brain) and similarities with \u201cConvolutional Neural Fabrics\u201d.\n\nThe Outrageously Large Neural Network, described as follows:\n\nThe key architectural component is depicted as follows:\n\nWhich shows a conditional logic component that selects from a mixture of experts. We\u2019ve discussed conditional logic in DL in a previous article \u201cIs Conditional Logic the New DL Hotness\u201d. The basic idea here is that if conditional components can be used, then much larger networks can be built that operate with different experts that are dependent on context. So a single context will use a small subset of the entire network.\n\nConvolutional Neural Fabrics are an alternative approach to hyper-parameterization tuning:\n\nThat is, rather than running multiple different configurations and discovering what performs well, a fabric attempts different paths in a much larger network and attempts to discover an optimal path while simultaneously reusing previously discovered optimal sub-paths in the network. In the figure above, shows how a networks are embedded within a fabric. One may also consider this approach as a variation of a previously published meta-learning technique that searches for different network architectures.\n\nIn a previous article \u201cDL Predictions for 2017\u201d, I point out to four emerging trends: #3 Meta Learning, #4 Reinforcement Learning, #5 Adversarial & Cooperative Learning and #7 Transfer Learning. It is simply just fascinating how PathNet seems to incorporate all four trends into a single larger framework. What we are seeing here is that fusing these different approaches may lead to novel and promising new architectures.\n\nThe PathNet architecture also provides a roadmap towards more adaptive DL architectures. DL monolithic architectures are extremely rigid after training and remain fixed when deployed. This is unlike biological brains that are continuously learning. PathNet allows for new contexts to be learned on the same time, leveraging knowledge of training in other contexts to learn much faster.\n\nThese new architectures may also lead to new thinking on how Deep Learning hardware can be optimally architected. Large networks of neural networks that are sparsely interconnected implies that there is an opportunity to architect more power efficient silicon by be able to power off many sub-networks at a time."
    },
    {
        "url": "https://medium.com/intuitionmachine/thinking-about-virtual-agents-3e7879a6117c",
        "title": "Thinking about Virtual Agents \u2013 Intuition Machine \u2013",
        "text": "The work taking shape around virtual assistants (agents) is in many ways a look at the \u201cdeep end of the pool\u201d for conversational UI. The emphasis is on conversational context, trust and delivering tangible value. Studying this helps explore the conversational user experience with greater perspective.\n\nThe Virtual Assistant Summit 2017 in SF was a good way to check out the latest in this space. A few noteworthy \u2018agents\u2019:\n\nIt\u2019s still early but some clear common themes have emerged.\n\nVirtual agents need a narrow focus: be good enough at a specific set of tasks to get customers to pay for them. Utilitarian turns out to be a good thing.\n\nAmy from x.ai is a good example of an explicitly narrow agent objective: scheduling meetings via email correspondence. The pain of coordinating schedules for an event, gone.\n\nThe medium for this agent is your email and calendar: you give the agent access to these then cc: amy@x.ai, and the scheduling happens over multiple emails on your behalf.\n\nThe use of email as the conversational medium in this case has a \u2018steampunk\u2019 feel (old technology used in contemporary setting), but this is where most of the scheduling work happens.\n\nAll of the agents demonstrated had very narrow scope. The deeper their abilities seemed, the narrower their range of motion. This is a logical starting point: start narrow.\n\nIncreasingly businesses need to converse with customers through a conversational interface. The conversation is ultimately a blending of human and machine with the goal of providing excellence in customer service.\n\nThis approach is an inevitability, we hereby give it a descriptive label:\n\nBusinesses already have customer service representatives, and their efficiency and customer experience over phone is often lackluster. Customers are increasingly accustomed to messaging apps and expect high-quality service on their schedule \u2014often they are willing to trade some asynchrony for convenience.\n\nAll of the agents on stage at this year\u2019s VA Summit used human/machine blending on the back end of their response processing.\n\nSacrificing user experience for scale, particularly when you are selling a concierge service, is a really bad idea.\n\nThis topic of human and machine collaboration on responses deserves more attention.\n\nThe etymology of the word \u2018conversation\u2019:\n\nWe, as social animals, engage in conversations that carry trust, specifically the trust associated with the context at hand. The travel agent is trusted with our itinerary, the executive admin is trusted with our schedule, the nurse is trusted with your health-care concerns.\n\nIf the scope is sufficiently narrow, as noted earlier, then it\u2019s possible to deliver on the trust necessary to deliver value.\n\nIf the virtual agent is not already part of a trusted brand, it must build a baseline of trust with its new customer.\n\nWe will eventually see virtual agents collaborating and virtual \u2018brokers\u2019 in front of these."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-and-the-ordinary-citizen-a853151a8ee0",
        "title": "Deep Learning and the Ordinary Citizen \u2013 Intuition Machine \u2013",
        "text": "I career shifted from startup founder (and before that equity portfolio manager) to tech. Yes, I drank the koolaid and learned how to code.\n\nFor me it was an obvious choice. It\u2019s like coming from a different country and not learning the language in a new one. Sure you can get by without speaking it. I\u2019ve known a few people who still speak terrible English (or none at all) despite having lived here for decades.\n\nSure, it\u2019s a personal choice whether to \u2018go native\u2019 or not. But imagine the inconvenience of having to ask someone else to decipher an official notice, or a letter from a new friend. Or being the only witness to an incident that requires the attention of emergency officials, but unable to describe it in the new language, you desperately resort to hand gestures.\n\nMost importantly, not taking up the skill will close off so many opportunities that will open up in the span of a lifetime.\n\nStill, some people make do.\n\nI bring this up because the changes \u2014 political, climate-wise, financially \u2014 seem to have come at us in rapid succession. Time seems to have compressed these events into a smaller and smaller timeframes.\n\nThe question is: is there a point of saturation where we cannot cope with any more changes? Perhaps that is one reason why our immigrant above chose to cocoon. Many, having to deal with new stresses, have taken this option. Harkening back to a simpler time.\n\nBecause I drank the tech koolaid, I am find myself tuning in to changes and advances happening in that world. And I want to report to you what I see, from the eyes of a former non-techie.\n\nI especially want to alert you to something that some have called Our Final Invention: Artificial Intelligence, more specifically, Deep Learning. And how it may impact the ordinary citizen.\n\nOne piece you should definitely check out is by Tim Urban[1] which he called The AI Revolution: the Road to Superintelligence. Read it. If only for the very revealing images. Like this one, where he very unscientifically graphs where the ordinary citizen is on the line that depicts human progress through time:\n\nWhat was striking to me was that the tech community has excitedly taken in these changes.\n\nGoogle for example, had replaced a translation algorithm the industry had used for over 30 years with a Deep Learning algorithm in a few months, after it had shown dramatic progress:\n\nIn fact Google had reorganized itself around AI, and together with Microsoft, Baidu, Facebook, Salesforce, IBM had begun to remake themselves around AI [3]\n\nThese changes have been happening with increasing velocity since, say 2014. They first manifested in the number of AI companies had been bought since then. Also, in the the sudden dearth of technologists who have a unique set of skills . These skills may include some combination of: a higher understanding of maths (Ph.D level is quite ordinary), a facility for writing programming code, the knowledge to use these in tandem with advanced hardware called graphic processing units (GPUs) and the ability to understand and apply knowledge of advanced learning algorithms to vast datasets.\n\nCould other of the biggest companies be reorganizing themselves also around these technologies?\n\nIf it is only early innings in this technology, are the companies just learning how to utilize it?\n\nHow will these changes affect our jobs, our way of living?\n\nMore importantly, how soon and how fast will these changes occur?\n\nWhat do these technologies portend for the ordinary citizen?\n\nI will want to talk about my journey from a non-techie, to somewhat a techie, to someone helping a Deep Learning company navigate this new environment. I hope to bring an approachable voice to these changes as they are occurring, and delve more deeply into the business, societal and commercial applications of Deep Learning.\n\nI write about Deep Learning for the ordinary citizen. If you want to learn more about Deep Learning, and step up the conversation to more hard core, read our blog at www.medium.com/intuitionmachine, or visit us at http://www.intuitionmachine.com/"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-fundamental-problem-with-ai-is-rooted-in-the-traditional-corporation-f1bdfb082c54",
        "title": "Deep Learning and the Responsive Corporation \u2013 Intuition Machine \u2013",
        "text": "Not a day goes by that we find news about how automation is destroying jobs and that the march of AI will accelerate this automation and take over many jobs in the knowledge industry. Humanity finds itself really with a lack of solutions how to stop this relenting onslaught. The lack of good ideas is due to the many thinkers to avoid looking at the real fundamental problem.\n\nThe fundamental problem is how our corporations are currently structured. Corporations are built like machines, where people, the fuel of its growth are treated like resources. That is commoditized into interchangeable and replaceable parts. It is structured the way it is for historical reasons. We follow management dogma that was invented prior to the widespread introduction of computers. Corporations are hierarchical because in the old days, email did not exist. You needed bodies to ensure the dissemination of information.\n\nJohn Hagel says that the mechanized corporation basically places a big target on the back of its workers. That target basically screams, \u201coptimize me out of the process\u201d. When you have organizations that are designed like machines, then one should not be surprised that you lose your job from automation because you are just a mere cog in that machine. Corporations are structured to ensure that you are redundant and therefore replaceable.\n\nThese kind of mechanized corporations are a dying breed. They are being replaced by more nimble and adaptive adversaries that use a more agile approach to business processes. When Facebook acquired WhatsApp for $19-billion, WhatsApp had a grand total of 55 employees ($350m per head). How does a company with so few employees make such a large impact? You can be rest assured here that WhatsApp wasn\u2019t run like a mechanized corporation.\n\nYammer\u2019s founder in 2015 introduced a new way of organizing the corporation. His ideas originate from earlier ideas from the Lean and Agile methodologies of software development. In his Responsive Manifesto he drives a case for a new kind of efficiency that will drive the successful workplaces of the future.\n\nThe Responsive Manifesto\n\ndeclared the following principles:\n\nThis is how corporations of the future should be structured. This is how we can as humans survive the mechanization of jobs. I previously laid down some groundwork on how we can employ Deep Learning (i.e. the advanced form of AI) in the context of learning platforms or creation networks (see: \u201cDeep Learning and the Platformization of Business\u201d). Here I would like to explain the value of a Deep Learning strategy in the context of enabling a more responsive organization. I make the analogy with Self-Driving cars that are also powered with Deep Learning and introduce the term \u201cSelf-Learning Enterprises\u201d to reflect a different context.\n\nToday, circumstances and markets change rapidly as information flows faster. Self Learning enterprises provide a self-service capability that enables employees with the best insight and decision-making ability to easily access data of the company to gain better insight. Think of it like Amazon\u2019s Go store, where one does not require access to a cashier to perform a purchase. Rather than controlling data through process and hierarchy, you achieve better results by empowering people at the edges.\n\nCorporations limited the control and understanding of its data to a few experts. This lead to an extremely time consuming and rigid process that required the continuous participation of the gatekeepers of the data. Deep Learning works well a new kind of UI where information is easily accessible. Think for example how your smart phone or Amazon Alexa makes access to information more convenient. Deep Learning technologies enable this kind of ambient and ubiquitous access while also managing the access to the information.\n\nIn a highly unpredictable environment, plans start losing value the moment they\u2019re finished. Embracing agile methods that encourage experimentation and fuel rapid learning is a much better investment that spending too much time upfront planning.\n\nDeep Learning enabled automation supports incremental and adaptive processes. So rather than making an expensive upfront planning investment. One can aggressively move forward as the costs to continuously update the original plan in a changing environment are reduced by automation. Information that is ingested into the corporation as a source of decision making is executed in most automated way possible. This empowers people at the edge to rapidly gain insight on new data.\n\nJust as Self-Driving cars allow for passengers to maximize the use of their own time, Self-Learning companies supports connectivity to increase the ability to self-organize, collaborating more easily across internal and external organizational boundaries. Typical enterprise \u201cSilos\u201d are demolished as all data and decision tools are made available to those who need it.\n\nSelf-Learning organizations are designed for change and continuous learning. Rather than seeking consistency, adaptive systems increase learning and experimentation, in the hopes that one novel idea, product, or method will be the one we need in the new world.\n\nAn enterprise has its data guarded by many different organizations. Data is hard to come by and hard to disseminate across the organization. A Self-Learning enterprise provides access to data across silos because it is impossible to predict which data might be useful.\n\nThere is a revolution happening that we are restructuring corporations to become more nimble and agile. It is thus extremely important that we start thinking of how AI technologies such as Deep Learning can accelerate our transition to a more humane way of running an enterprise."
    },
    {
        "url": "https://medium.com/intuitionmachine/how-to-explain-deep-learning-using-chaos-and-complexity-33de81c321de",
        "title": "How to Explain Deep Learning using Chaos and Complexity",
        "text": "I want to talk to you today about the concerns of Non-Equilibrium Information Dynamics and how an understanding of its features lead us to a better intuition about Deep Learning systems or learning systems in general.\n\nAllow me to recap my observation from a previous post on \u201cDeep Learning in Non-Equilibrium Dynamics\u201d. In our study of Deep Learning, practitioners derive their intuition from the mathematics of physical systems. However, since these are not a physical system that we study but rather information systems, we apply information-theoretic principles. Now, information theory has its origins also in mathematics that describe physics (i.e. Thermodynamics). Both theories are essentially bulk observations of nature. What I mean by bulk, is that they are an aggregate measure of systems with a large number of interacting particles or entities.\n\nKieran D. Kelly, [KELLY]whose writing I recently stumbled upon, has one of the better intuitions out there about non-equilibrium dynamics. His blog is a pleasure to read, and I recommend it highly for anyone interested in this kind of esoteric thing.\n\nWired has posted an article titled \u201cMove Over Coders \u2014 Physicists will soon Rule Silicon Valley\u201d [WIRED]. Now, we might make the observation that Physicists, in general, have to have a decent IQ to do what they do and thus be able to handle computer science. We can also argue that the mathematics found in Deep Learning isn\u2019t really that advanced compared to what\u2019s found in a typical undergraduate physics curriculum (emphasis on undergraduate). However, there is something else that most people do not understand, but it is generally understood by someone studying physics.\n\nWhat people can\u2019t seem to comprehend, and this is even among folks with a technical background such as computer science and mathematics, is the relationship between math and reality. They don\u2019t recognize that the math that we use are just approximations of reality; that math has limitations beyond certain dimensions. People doing physics know this because despite using analytic forms, we are constantly performing hand waving approximations (i.e. Use Taylor series to expand any function and throw out any term beyond the quadratic). So when I write about the limits of Math with respect to AI, I get a ton of outrage from math inclined folk! The ignorance in this world, even among the learned, is really surprising.\n\nGoing back to Kelly, he echoes the same sentiment about math and reality:\n\nPhysics is, in a sense, a science of linear dynamics, a science of \u201cdynamics without feedback\u201d; such dynamics are indeed easily compressible, but the real world is a world that abounds with feedback, a \u201cnonlinear\u201d world full of \u201cincompressible dynamics\u201d [KEL].\n\nFor many, this statement may seem to be a shock. But it really is not, this is just basic reality that there are limits to analytic forms. Another thing that seems to confuse people is the use of the word \u201clinear\u201d and \u201cnon-linear\u201d by Physicists. Most people think of \u201clinear\u201d being that of a linear equation and I suppose non-linear to mean something that\u2019s not. So a quadratic equation qualifies as non-linear. What the Physicist, however, defines as linear and non-linear is from the point of view of differential equations. Linear differential equation has a chance of being solvable in a closed form solution. In contrast, with non-linear differential equations, almost all bets are off. The most classic example is the Navier-Stokes equation for fluids. Solvable analytically only up to 2 dimensions. Yes, 2 dimensions, that is an unrealistic flatland world.\n\nBasically, though, think of non-linear as systems that have feedback. In other words, most of our reality. So to understand a bit about our reality, we have to understand a bit about the nature of non-linearity. It turns out over the years, there have been two features about feedback systems that have been studied. This is chaos and complexity. Kelly has a whole set of articles about these two subjects, and I\u2019ll re-direct you there to get an introduction.\n\nNow what I want to focus on is information systems (not physical systems), so what we are really looking for is chaos and complexity in the context of information systems. (side note: Deep Learning systems are information systems despite the poor association with the term Neural Networks). So here\u2019s the very nice table from Kelly:\n\nWhat drives evolution\u2019s spontaneous and progressive complexity is the interplay of insufficient negative feedback and strong positive feedback; or in other words what drives evolution is The Interplay of Random Innovation and Natural Reinforcement.\n\nNegative feedback here are the natural tendency that exists in the Second Law of Thermodynamics (which really is the law of large numbers). That is, systems tend towards maximum entropy. The positive feedback, however, is a mechanism that can lead to chaos. But at the upper right quadrant, we discover emergent complexity. In other words, one has to embrace the existence of mutual feedback as well as randomness. Unfortunately, our mathematical legacy, that of assuming nice independent Gaussian distributions and favoring sparsity (or parsimony) over randomness is demanding an unnatural constraint on the system.\n\nAn assumption of IID (i.e. Independent Identical Distributed) features and an assumption that sparsity is the favored solution is walking every researcher towards an entirely wrong direction! These assumptions are the equivalent of physicists making their equations linear. It is all so that our mathematics become convenient. Unfortunately, God did not mandate that reality be conveniently expressed in mathematics. We are pushing our researchers to buy into religion and not reality.\n\nNow, before I completely forget, let me explain how chaos and complexity relate to explaining Deep Learning. Let\u2019s start with randomness or entropy, I wrote about this in \u201cThe Unreasonable Effectiveness of Randomness\u201d. When we study Deep Learning, we simply can\u2019t ignore the presence of randomness. It just seems to be an intrinsic feature of these systems. The most simple intuition I can think of here is that diversity leads to survivability. Monocultures tends to less adaptability and possible extinction. In fact, the most counter-intuitive notion, randomness leads to information preservation. As an example of this in computer science, this is used in \u201cInformation Dispersal Algorithms\u201d. That is, you take information and scatter it among different storage nodes and in a massive scale you do it randomly. You basically build storage that is highly redundant. This is the same mechanism as you find in holographic memories. So here, we establish the value of high entropy.\n\nLet\u2019s examine the other axis, that of high mutual information that can lead to unstable feedback and thus chaos. Mutual Information is the antithesis of many probabilistic methods. That\u2019s because the math simply can\u2019t handle it. But should we shoehorn reality to fit the math? I think not. One of the better characterization of how Deep Learning is able to work well in domains of higher mutual information is this paper \u201cCritical Behavior from Deep Dynamics: A Hidden Dimension in Natural Language\u201d:\n\nHow can we know when machines are bad or good? The old answer is to compute the loss function. The new answer is to also compute the mutual information as a function of separation, which can immediately show how well the model is doing at capturing correlations on different scales.\n\nDeep Learning must be able to learn correlations at multiple scales to be of any use. Actually, to phrase it in a different way that does make sense is, Deep Learning must be able to understand the composition of language, from letters to word, to sentences and eventually to complete texts. Deep learning works because it captures language.\n\nAnd the learning mechanism for this is what exactly? Jeremy England actually has very compelling argument as to how life self organizes. You can read it at Quanta: \u201cA New Physics Theory of Life\u201d [ENG]. We can take this idea and use it to explain how learning works in Deep Learning. I\u2019ve written early about the 3 Ilities. Explanations of \u201cTrainability\u201d is extremely important. A layered DL system actually builds a representation of language from the lower layers up to the more abstract higher layers. Each layer has its own mutual entanglement that is actually discovered through training. Over time, the entanglement get reinforced such that the breaking of the entanglement becomes less likely. So, for example, if the network only sees Latin characters then it never develops the ability to understand Arabic characters. Layers are also interconnected, so there is a constraint at the bottom ( more fundamental concepts ) and at the top ( minimizing relative entropy ). So eventually, a language hierarchy is built.\n\nThe objection here though is that it should take an infinite amount of time to arrive at a proper representation. That\u2019s where the interplay of entropy comes into the picture. The basic theory is not unlike that of the holographic principle. Randomness begets robustness while mutual information begets self-organization and compression. What begets generalization? Not sure, but something seems to emerge at the upper right-hand quadrant!\n\nTo understand more, either keep reading this blog or head over and talk to us at \u201cIntuition Machine\u201d. Also, make sure you don\u2019t miss any Deep Learning developments. Subscribe to our newsletter: https://www.getrevue.co/profile/intuitionmachine.\n\nExplore more in this new book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/pytorch-dynamic-computational-graphs-and-modular-deep-learning-7e7f89f18d1",
        "title": "PyTorch, Dynamic Computational Graphs and Modular Deep Learning",
        "text": "Deep Learning frameworks such as Theano, Caffe, TensorFlow, Torch, MXNet and CNTK are the work horses of Deep Learning work. These frameworks as well as the GPU (predominantly Nvidia) are the what enables the rapid growth of Deep Learning. It was refreshing to hear Nando de Freitas acknowledge their work in the recently concluded NIPS 2016 conference. Infrastructure does not get enough of the recognition it deserves in the academic community. Yet, programmers toil on to continually tweak and improve their frameworks.\n\nYesterday, a new framework was revealed by Facebook and a bunch of other partners (Twitter * NVIDIA * SalesForce * ParisTech * CMU * Digital Reasoning * INRIA * ENS). PyTorch came out of stealth development. PyTorch is an improvement over the popular Torch framework (Torch was a favorite at DeepMind until TensorFlow came along). The obvious change is the support of Python over the less often used Lua language. Almost all of the more popular frameworks use Python, so it is a relief that Torch has finally joined the club.\n\nThere are many improvements in the new PyTorch framework, however the most notable change is the adoption of a Dynamic Computational Graph. There are some lesser known frameworks that have this capability (i.e. Chainer and Dynet), in fact PyTorch borrowed a lot of ideas from Chainer. This capability is also referred to as \u201cDefine by Run\u201d as opposed to the more conventional \u201cDefine and Run\u201d:\n\nBasically, DL frameworks maintain a computational graph that defines the order of computations that are required to be performed. For people new to DL frameworks, it does seem unnatural that one finds two \u201cinterpreters\u201d in the framework. One interpreter is the host language (i.e. Python) and a second one is the computational graph.\n\nSo what you typically have in these frameworks is a language that sets up the computational graph and an execution mechanism that is different from the host language. This kind of strange setup is primarily motivated for efficiency and optimization reasons. A computational graph can be optimized and run in parallel in the target GPU. This cumbersome setup has made it difficult for researchers to try out more novel approaches.\n\nOne analogy to make is that it\u2019s like Fortran. Fortran, despite is age, is still used in a lot of computational intensive problems. Fortran however has static allocation of memory. This has its pros and cons, but the main benefit is that it can optimize computation. So static computational graphs are kind of like Fortran. Now dynamic computational graphs are like dynamic memory, that is memory that is allocated on the heap. This is valuable for situations where you cannot determine before hand how much memory is required. Similarly, dynamic computational graphs are valuable for situations where you cannot determine the computation. One clear example of this are recursive computations that are based on variable data.\n\nIn the space of NLP where language can come in various expression lengths, dynamic computational graphs are essential. One can just imagine how a grammar is parsed to realize the need for a stack and therefore dynamic memory and thus dynamic computation. Speaking of a stack, there are new DL architectures that make use of a stack!\n\nNow you can always shoehorn this into a static computational graph, but its as inconvenient as programming a parser without using a stack. The folks at Google, makers of Tensorflow, have a paper out that shoe horns TensorFlow to give in dynamic capabilities (see: https://research.googleblog.com/2017/02/announcing-tensorflow-fold-deep.html ). Now you don\u2019t always need this kind of flexibility, but if you are in an exploratory environment, any kind of additional convenience will help speed up the process. Another feature about PyTorch is that it works just like Python. So there no split-brain experience that there\u2019s another execution engine that running the computation. Because of this, it\u2019s much easier to debug and much easier to create unique extensions.\n\nWith this development, it would not be unreasonable to expect that Deep Learning architectures will traverse the same evolutionary path as traditional computation. That is from monolithic stand-alone programs, to more modular programs. Introducing dynamic computational graphs are like introducing the concept of procedure when all one previously had was \u201cgoto\u201d statements. It is exactly the concept of procedure that we can write our programs in a composable manner. One of course can argue that DL architectures have no need for a stack, however one only needs to see recent research on HyperNetworks and Stretcher networks. There are networks in research were context switching like a stack appears to be effective.\n\nThere is also another concept that is related to this, this is called Modular Deep Learning. I predicted that in 2017:\n\nThis is an even richer kind of modularity. That is, what we are seeing is something akin to information encapsulation (i.e. that feature found in Object Oriented Programming). What you see in GANs are cooperating networks that are actually encapsulated away from each other. We\u2019ve actually also seen this kind of research from Maluuba (recently acquired by Microsoft). So its, absolutely astonishing the pace of development that just into the new year (it\u2019s just January right now) we are quickly building a new kind of infrastructure to support even more advanced forms of Deep Learning.\n\nTo keep up with the pace, signup to Design Patterns for Deep Learning. Also, make sure you don\u2019t miss any Deep Learning developments. Subscribe to our newsletter: https://www.getrevue.co/profile/intuitionmachine."
    },
    {
        "url": "https://medium.com/intuitionmachine/agi-checklist-30297a4f5c1f",
        "title": "AGI Checklist \u2013 Intuition Machine \u2013",
        "text": "Salience \u2014 selecting what is relevant and important to a given context and goal \u2014 is an important aspect of intelligent systems.\n\nThis comes into play at different levels of cognition:\n\nFirstly, in autonomous data selection on input \u2014 what senses and features to process and /or ignore, and what level of importance to assign to them for processing. For example, most animals are wired to pay extra attention to fast moving items in their visual field, and to loud sounds. For AGI we have to assume that much more sensory input will be available than can (or should) reasonably be processed. We must also assume that relevant feature extractors such as edge or shape detection must be prioritized. It seems that some semi-automatic mechanism needs to do this pre-selection. This mechanism should be under overall high-level cognitive control to preset parameters; for example to, say, bias it to focus on changes in color or pitch.\n\nOnce input has been appropriately selected and prioritized, pattern matching, categorization, and conceptualization mechanisms need to be selected according to contextual requirements. What matters currently? For example, are we trying to match incoming patterns against each other, or against some internal reference; are we interested in shape or texture patterns; or are we just interested in object collisions?\n\nHigher level goals also need to be selected and prioritized according to salience. What are we trying to achieve right now? What dependencies are there? What is most important in the current context?\n\nFinally, the overall architecture has to allow for consolidation and forgetting. What information or experience should be consolidated? What should be forgotten (or archived)?\n\nAGI need to have mechanisms in place at each of these levels (and probably some others) to evaluate salience and to adjust cognition accordingly."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-and-platforms-of-disruption-66e2216c3cc6",
        "title": "Deep Learning, Disruption and the Platformization of Business",
        "text": "The business world has evolved into a much more difficult and competitive environment. This situation has been exacerbated because of disruptive changes in the global economy. The potential of more nimble competitors to disrupt the businesses of incumbents have never been more likely. Peter Diamandis describes the Six D\u2019s of Exponentials [DIA] as consisting of the following:\n\nDigitization \u2014 Anything that can be digitized can lead to the same exponential growth we find in computation. Anything that is digitized, or alternatively virtualized, is unencumbered by physical law and thus cost less to mass produce and moves faster in dissemination.\n\nDeception \u2014 Once digitized or virtualized, initial growth deceptively appears linear however given time exponential growth becomes obvious. For many, it is too late to react once growth of a competitor hits this transition.\n\nDisruption- New markets are created that are more effective and less costly. Existing markets that are tied to the physical world become extinct. We\u2019ve seen this in music, photography and many other areas.\n\nDemonetization- As costs head towards zero, so does the ability to solicit a payment for it. Thus a business has to reinvent its revenue model or come up with new ways of monetization.\n\nDematerialization \u2014 Physical products disappear and are replaced by a more convenient and accessible alternative. This is the stuff that used to be on your desk that have been replaced entirely by your smartphone:\n\nDemocratization \u2014 More people now have access to technology at a lower cost. The means of production have become more accessible to everyone. This access is no longer confined to the big corporation or the wealthy. We see this fragmentation everywhere where producers are publishing their own books, music and videos. This thus feeds back into itself, where smaller players are able to come into competition.\n\nObviously, there is an ever pressing need for enterprises to take drastic action by re-engineering how they run their businesses to survive this disruption. John Hagel proposes four kinds of platforms [HAG]that leverage networking effects as an organizational mechanism to combat disruptive businesses. Here is a video of John Hagel explaining the platforms:\n\nThe four platforms that John Hagel proposes are as follows:\n\nAggregation Platforms \u2014 These are essentially marketplaces that facilitate transactions among participants. Think eBay as an example or Kaggle in the ML space.\n\nSocial Platforms \u2014 These platforms encourage long relationships among participants and also lead to the formation of cliques of like minds rather that a hub and spoke model. Facebook and Twitter are examples of these.\n\nMobilization Platforms \u2014 These are platforms that facilitate the coordinated action of a group of people in a task that takes considerable time to complete. Hagel uses the term \u2018process networks\u2019 where these kinds of platforms goes beyond a single transactions or conversations. There are platforms that coordinate supply chains or distribution operations. Hagel proposes Open source is an example of this kind of platform where many participants contribute together in complex ways to build and maintain a product.\n\nLearning Platforms- These are a more dynamic and adaptive environment where a group of people comes together to collectively learn how to address a more complex problem. This is a place where participants can connect to each other ask for a question, share experiences and provide advice. Open source projects that are actively managed with distributed source control, test driven development, issue tracking, and continuous integration is a good example of a learning platform. The key ingredient here is that there is a learning mechanism that gets codified continuously. The reason we find this in software development should not come as a surprise since software development is intrinsically a learning process.\n\nEffective Business Process Re-engineering requires that we go beyond just seeking out optimization opportunities through automation. The days of corporations that are structured like machines are numbered. One should instead seek out agile processes. Leading companies today have adaptive and nimble processes, and it is from this vantage point strive to discover opportunities that can lead to networking effects. So instead of optimizing processes, reimagine them as platforms [SCH]:\n\nThe most disruptive technology that is emerging today is called Deep Learning. It is an Artificial Intelligence technology. Unfortunately, most companies are blind to the existence of Deep Learning. Even the companies that do have an awareness, there is very little understanding as to how to take advantage of this technology. Businesses need a guide, a playbook that gives details about the methodology and strategy to move forward.\n\nThe effective way to think about Deep Learning adoption is to see how it where it can enhance a platform strategy. This is because we want to leverage the networking effects as best described by this picture:\n\nThat is, more users lead to more data. This leads to smarter Deep Learning algorithms and therefore better products. The cycle then feeds into itself. In a world of constant disruption, networking effects are essential for any defensible business. However, it takes more than understanding why this is important. It requires an understanding of platforms that enable it as well as the kinds of Deep Learning algorithms that enhance these platforms.\n\nOne of the most intriguing of platforms is the Learning platform. John Hagel says it best:\n\nWhat Hagel is saying is that the participants of the network adaptively become more effective and capable as a participant in the learning network. In other words, not only is there the conventional networking effect, but another one that kicks it into overdrive.\n\nSo how does Deep Learning play into enhancing a Learning platform? The idea at its most simple incarnation is that Deep Learning technology can be employed to augment many tasks. One task is to speed up digesting of information by a worker. In today\u2019s information-rich environments, we are constantly inundated by more and more information. Deep Learning technology can help parse, digest, curate and present that information such that we can focus on the most value-added activity. The more information we can digest, the quicker we learn. This can be further improved by tightening the feedback loop through the augmentation of agile processes.\n\nOne concrete example of this is in the context of the mining industry. One of the big problems with mining is that the sequence of equipment are daisy chained like Christmas lights. If in the event of failure of one piece, the entire production grinds to a very expensive halt. We can certainly place Deep Learning monitoring devices on the equipment to be able to predict future failure, however, to do so, requires data of different kinds of failures across different kinds of devices. This problem of lack of data can be addressed by having a learning platform where multiple mining companies come together to share their data from the field. As a result, companies that aren\u2019t sharing their data and aren\u2019t sharing their learning experience are at a disadvantage.\n\nThere has in fact been some research that studies the mathematics of a learning organization and how it relates to innovation. Technology Review describes this research [EmTech]:\n\nAs more data is shared, a language is developed (i.e. the collective vocabulary) and expands and as a consequence new vocabulary, a new way of expression is created and this leads to greater innovation. This is based on the obvious realization that almost all of human knowledge is captured in language. In the grand scheme of things, intelligence is ultimately all about language. This encompasses languages that humans use today, complex mathematical language and all the way to machine designed languages. Deep Learning is all about using machine assisted language creation.\n\nI end with a quite wild prediction from the World Economic Forum that predicts \u201cThe Largest Internet Company in 2030\u201d[WEL]:\n\nIf you want to discuss more about this subject, let us know at \u201cIntuition Machine\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-many-tribes-problem-of-artificial-intelligence-ai-1300faba5b60",
        "title": "The Many Tribes of Artificial Intelligence \u2013 Intuition Machine \u2013",
        "text": "One of the biggest confusions about \u201cArtificial Intelligence\u201d is that it is a very vague term. That\u2019s because Artificial Intelligence or AI is a term that was coined way back in 1955 with extreme hubris:\n\nWe propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire.\n\nThe study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions, and concepts, solve kinds of problems now reserved for humans, and improve themselves.\n\nAI is over half a century old and carries with it too much baggage. For a very long time, AI was dominated by Symbolists, that is rule-based systems that had \u201cZero Learning\u201d. In the 1980\u2019s a new kind of AI began to emerge, we termed this Machine Learning. Finally, we had at least \u201cSimple Learning\u201d. The big disruption, however, occurred this decade, when we stumbled upon \u201cDeep Learning\u201d and ever since it has been taking no prisoners.\n\nThis is, of course, a grossly simplified history of AI. There are actually many different approaches or tribes in AI. Pedro Domingo\u2019s in his book, the Master Algorithm, talks about five different \u201ctribes\u201d. Not to be outdone, A YCombinator user \u201csolidrocketfuel\u201d posts about at least \u201c21 different cultures\u201d.\n\nIt is important for anyone that plans on doing AI to understand that there are differences in the approaches of the different tribes of AI. AI is not a homogenous field, but rather a field in constant tribal warfare. Here\u2019s an overview:\n\nSymbolists \u2014 Folks who used symbolic rule-based systems to make inferences. Most of AI has revolved around this approach. The approaches that used Lisp and Prolog are in this group, as well as the SemanticWeb, RDF, and OWL. One of the most ambitious attempts at this is Doug Lenat\u2019s Cyc that he started back in the 80\u2019s, where he has attempted to encode in logic rules all that we understand about this world. The major flaw is the brittleness of this approach, one always seems to find edge cases where one\u2019s rigid knowledge base doesn\u2019t seem to apply. Reality just seems to have this kind of fuzziness and uncertainty that is inescapable. It is like playing an endless game of Whack-a-mole.\n\nEvolutionists- Folks who apply evolutionary processes like crossover and mutation to arrive at emergent intelligent behavior. This approach is typically known as Genetic Algorithms. We do see GA techniques used in replacement of a gradient descent approach in Deep Learning, so it\u2019s not a approach that lives in isolation. Folks in this tribe also study cellular automata such as Conway\u2019s Game of Life [CON] and Complex Adaptive Systems (CAS).\n\nBayesians \u2014 Folks who use probabilistic rules and their dependencies to make inferences. Probabilistic Graph Models (PGMs) are a generalization of this approach and the primary computational mechanism is the Monte-Carlo method for sampling distributions. The approach has some similarity with the Symbolist approach in that there is a way to arrive at an explanation of the results. One other advantage of this approach is that there is a measure of uncertainty that can be expressed in the results. Edward is one library that mixes this approach with Deep Learning.\n\nKernel Conservatives \u2014 One of the most successful methods prior to the dominance of Deep Learning was SVM. Yann LeCun calls this glorified template matching. There is what is called a kernel trick that makes an otherwise non-linear separation problem into one that is linear. Practitioners in this field live in delight over the mathematical elegance of their approach. They believe the Deep Learners are nothing but alchemists conjuring up spells without the vaguest of understanding of the consequences.\n\nTree Huggers \u2014 Folks who use tree-based models such as Random Forests and Gradient Boosted Decision Trees. These are essentially a tree of logic rules that slice up the domain recursively to build a classifier. This approach has actually been pretty effective in many Kaggle competitions. Microsoft has an approach that melds the tree based models with Deep Learning.\n\nConnectionists \u2014 Folks who believe that intelligent behavior arises from simple mechanisms that are highly interconnected. The first manifestation of this were Perceptrons back in 1959. This approach died and resurrected a few times since then. The latest incarnation is Deep Learning.\n\nThere are many sub-approaches under Deep Learning. This includes:\n\nThe Canadian Conspirators \u2014 Hinton, LeCun, Bengio et al. End-to-end deep learning without manual feature engineering.\n\nSwiss Posse \u2014 Basically LSTM and that consciousness has been solved by two cooperating RNNs. This posse will have you lynched if you ever claim that you invented something before they did. GANs, the \u201ccoolest thing in the last 20 years\u201d according to LeCun are also claimed to be invented by the posse.\n\nBritish AlphaGoist \u2014 Conjecture that AI = Deep Learning + Reinforcement Learning, despite LeCun\u2019s claim that it is just the cherry on the cake. DeepMind is one of the major proponents in this area.\n\nPredictive Learners \u2014 I\u2019m using the term Yann LeCun conjured up to describe unsupervised learning. The cake of AI or the dark matter of AI. This is a major unsolved area of AI. I, however, tend to believe that the solution is in \u201cMeta-Learning\u201d.\n\nIn addition to the above mainstream approaches, we also have:\n\nCompressionists \u2014 Cognition and learning are compression (Actually an idea that is shared by other tribes). The origins of Information theory derives from an argument about compression. This is a universal concept that it is more powerful than the all too often abused tool of aggregate statistics.\n\nComplexity Theorists- Employ methods coming from physics, energy-based models, complexity theory, chaos theory and statistical mechanics. Swarm AI likely fits into this category. If there\u2019s any group that has a chance at coming up with a good explanation why Deep Learning works, then it is likely this group.\n\nFuzzy Logicians \u2014 This approach was once quite popular, but for some reason, I haven\u2019t heard much about it as of late. One would think that there would be a little more interest here considering the success of the also \u2018fuzzy\u2019 approach of Deep Learning. There was a recently published result that showed the use of Fuzzy rules defeating a fighter pilot in a mock dogfight.\n\nBiological Inspirationalists \u2014 Folks who create models that are closer to what neurons appear in biology. Examples are the Numenta folks and the Spike-and-Integrate folks like IBM\u2019s TrueNorth chip.\n\nConnectomeist \u2014 Folks who believe that the interconnection of the brain (i.e. Connectome) is where intelligence comes from. There\u2019s a project that is trying to replicate a virtual worm and there is some ambitious heavily funded research [HCP] that is trying to map the brain in this way.\n\nInformation Integration Theorists \u2014 Argue that consciousness emerges from some internal imagination of machines that mirrors the causality of reality. The motivation of this group is that if we are ever to understand consciousness then we have to at least start thinking about it! I, however, can\u2019t see the relationship of learning and consciousness in their approach. It is possible that they aren\u2019t related at all! That\u2019s maybe why we need sleep.\n\nPAC Theorists \u2014 Are folks that don\u2019t really want to discuss Artificial Intelligence, rather prefer just studying intelligence because at least they know it exists! Their whole idea is that adaptive systems perform computation expediently such that they are all probably approximately correct. In short, intelligence does not have the luxury of massive computation.\n\nIn summary, there really is a bewildering array of alternative approaches to AI. I am certain that there are other approaches that I have missed. Some approaches are in opposition to each other, while other can be used together synergistically. However, what I want to point out is that a bit of an understanding of what is out there can help you navigate this space.\n\nIf we do take a look at Shivon Zillis\u2019 Machine Intelligence Landscape:\n\nThere are a ton of companies that all claim to be doing AI. You have to ask a more pointed question. What sort of AI is each of these firms using? Because the stark reality here is that not all AI are the same. Said differently, \u201cSome AI are more equal than other AI\u201d. Our opinion is that Deep Learning related approaches have a disproportionate monopoly on the upside. The simple reason is: \u201cIt is the learning, stupid!\u201d If your AI approach does not have a strong mechanism for learning, then you will forever be doomed to Doug Lenat\u2019s fate. That is, having to write all the rules by hand (for 30 years)! The other approaches tend to be pretty much dead end approaches. It is critical that the AI approach has a way to learn or alternative mechanically bootstrap internal rules.\n\nOne of the most effective approaches has been to use Deep Learning in combination with other algorithms. We have seen this in the AlphaGo implementation that used a Monte-Carlo Tree Search technique in combination with Deep Learning. The integration of a symbolic approach with Deep Learning is also very promising considering that they have complementary strengths and weaknesses. Looking forward into the future, it\u2019s all going to be Deep Learning, one AI to rule them all. However, more likely, Deep Learning combined with some other AI approach will be just as promising. Ignoring this reality is a surefire way to ensure one\u2019s own extinction.\n\nAn update from Davos. In an interview [WEF] with Sergey Brin of Google, Brin remarks that despite being a few paper clips away from Google Brain\u2019s Jeff Dean, he did not see Deep Learning coming. If Brin himself who created Google could not have predicted the advance of Deep Learning, then what can we say about the many other practitioners from other competing AI tribes? That is people with a vested interest? To conclude, do not trust your resident AI expert, he just might belong to a different tribe.\n\nI found this: A Review of 40 Years of Cognitive Architecture Research: Focus on Perception, Attention, Learning and Applications.\n\nUpdate: Wired magazine has an articles about the \u201cAI Factions get Feisty.\u201d"
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-to-predict-human-behavior-a2cd2ce14132",
        "title": "The Uncanny Intuition of Deep Learning to Predict Human Behavior",
        "text": "One of the most promising applications of Deep Learning (DL) is in its use to enhance interaction with computers. DL is particularly suited for this since it has an intuitive capability that is similar to biological brains. It is able to handle the inherent unpredictability and fuzziness of the natural world.\n\nThis however does imply that improving AI to human interaction will require some level of capability of an AI to predict human behavior. This is a requisite capability and interesting enough there has been some recent research on this topic.\n\nThe conventional behavioral model approach is to have social scientists build analytic models (statistical models in most cases) of human behavior and to employ a distribution fitting exercise to verify validity of the model. The DL approach however is different since a prior model is not required to begin with. Rather, the DL system learns to predict by observing the behavior of human participants. We can get a glimpse of this ground breaking technique by examining some recent research work that uses this approach.\n\nMIT trained a Predictive Vision system on YouTube videos from shows like \u201cThe Office\u201d and \u201cDesperate Housewives\u201d to predict whether two persons will hug, kiss, shake hands or slap a five. The trained the Deep Learning system on 600 hours of video. The system was able to predict an action 43 percent of the time. This compares to previous algorithms that could only predict 36 percent of the time.\n\nOne pragmatic use of predict human behavior is to do so in the confines of a car. Brains4Cars uses a sensor fusion deep learning system based on LSTMs to anticipate driver behavior 3.5 seconds before it happens. It uses a collection of sensors such as cameras, tactile sensors and wearable devices to make its predictions:\n\nA recent NIPS 2016 paper uses Deep Learning to predict strategic behavior. In most systems the assumption is that the participants perform in a perfectly rational manner and are based from insights from cognitive psychology and experimental economics. However in this system, one that is based on Deep Learning, the system learns a cognitive model without the need for expert knowledge. This system is able to outperform system that are built from expertly constructed features:\n\nExtending beyond just making predictions, Deep Learning systems have been used to assist in contexts with human negotiation. In a paper, \u201cReinforcement Learning of Multi-Issue Negotiation Dialogue Policies\u201d the authors used Reinforcement Learning and a hand-crafted agenda based policy and evaluated them by having each negotiate against each other in different settings. What was discovered was the RL model consistently outperformed the hand crafted agenda based model. In addition, human\u2019s were asked to rate both systems and the result was the RL based approach was rated to be more \u201crational\u201d.\n\nFinally, in an impressive act of engineering, a team from the Czech Republic and Canada created a Poker playing system that played 33 professional pokers players from 17 countries and gained a win rate that was an order of magnitude better than a good player rating. The team coined their creation DeepStack:\n\nIn this context, not only is DeepStack able to perform accurate predictions of the behaviors of its opponents, it does so in a way that its own behavior is not predictable!\n\nTo summarize, Deep Learning is able to make prediction on tacit behavior of humans as well as rational behavior. The ability to anticipate behavior, predict behavior or win in games of bluffing are extremely advantageous tools to have in one\u2019s business arsenal. However, if you still aren\u2019t convinced that Deep Learning can predict human behavior, then you might want wonder why Facebook has a job opening in this kind of work: Facebook\u2019s Mysterious Job Listing Sounds Like It\u2019s Working on How to Read Your Mind\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/ai-as-an-existential-threat-to-humanity-a45dc2e0db76",
        "title": "AI as an existential threat to humanity? \u2013 Intuition Machine \u2013",
        "text": "Nick Bostrom, a Swedish philosopher turned futurist, argues that \u201cAI\u201d will become \u201csuperintelligent\u201d and this is an existential threat to humanity. This has some people worried and has spurred a lot of discussion, is this realistic or science fiction?\n\nI\u2019m sure Professor Bostrom is a good guy, and I\u2019m fond of philosophers (although not necessarily of the academic kind).\n\nTo be fair he\u2019s not the only pessimist forecasting the end of humanity at the hands of AI.\n\nSeveral others have predicted the rise of super-intelligent machines at the peril of humanity, including Elon Musk, Ray Kurzweil and Stephen Hawking. Now these are fairly intelligent people, but what is their futuristic prediction based on exactly? It\u2019s tempting to draw attention to a doomsday scenario associated with something the vast majority of people don\u2019t comprehend. The news is full of existential threats, why not add to that?\n\nAfter studying Bostrom\u2019s book, it\u2019s clear that the entire thesis hinges on one relatively straight-forward assumption: that software will be able to evolve itself.\n\nThis is quite simple actually: a computer program that is able to make itself more advanced becomes out of control and \u201ctakes over.\u201d It makes a lot of copies of itself, propagates across the Internet, and so on. The philosophical conundrum posed by Bostrom and others has to do with the ways in which humanity might be able to \u201ccontrol\u201d such a thing, while that\u2019s still possible.\n\nIndeed this cornerstone idea makes logical sense, even to non-coders. So long as humans are the only ones creating code then they have control. Code that doesn\u2019t create itself cannot evolve without human involvement.\n\nIf you prefer to not read the entire book, watch here beginning at 43:25 (below is the verbatim)\n\n\u201cAnd at some point, presumably in this whole-brain emulation, at some point probably fairly soon after that point, you will have synthetic AIs that are more optimized than whatever sort of structures biology came up with. So there\u2019s a chapter in the book about that. But the bulk of the book is \u2014 so all the stuff that I talked about, like how far we are from it and stuff like that, there\u2019s one chapter about that in the beginning. Maybe the second chapter has something about different pathways.\u201d\n\n\u201cBut the bulk of the book is really about the question of, if and when we do reach the ability to create human-level machine intelligence \u2014 so machines that are as good as we are in computer science, so they can start to improve themselves \u2014 what happens then?\u201d\n\nThe bulk of the book, he says, and the entire premise of AI as an existential threat to humanity is \u2014 code evolving itself.\n\nThe vast majority of the software carrying the label \u201cAI\u201d is simply automated knowledge work. The scientific work referred to as \u201cAGI\u201d (artificial general intelligence) has relatively few resources applied to it, particularly with the commercial attention increasingly paid to so-called \u201cAI\u201d. The knowledge work misleadingly labeled \u201cAI\u201d doesn\u2019t lead to this \u201csingularity\u201d event futurists proclaim, rather it leads to more knowledge work being done by software.\n\nA human toddler can acquire knowledge, make choices through reason, it can think and conceive the world in adaptive ways. No software has this cognitive capacity today.\n\nSoftware is ultimately a set of equations, and we are very far from having equations for human thought, in fact we don\u2019t have equations for the thought of insects, including ones that are known to have high degrees of social intelligence.\n\nA bee has roughly 800k neurons, it acquires information, communicates position of food, navigates 3-dimensional space, manufactures honey, collaborates in a social hierarchy, etc. If we had software as \u201csmart\u201d as a bee that would be significant. We\u2019re nowhere near that today, and most of the attention is elsewhere, unfortunately.\n\nNoam Chomsky and others have argued that the scope and range of human intellectual capacity could easily be insufficient to produce comprehensive theories of thought. Humans are organic creatures with limited capacity. Why would we assume the opposite: that human mental capacity is so expansive as to thoroughly understand the mind and cognition?\n\nWe have formulas for accounting, therefore we can program computers to perform knowledge work in accounting. We have patterns for winning chess moves, therefore we program computers to win at the game. We simply do not have a theory of thought, even for simple organisms, therefore creating software that thinks is still beyond our capacity.\n\nOne thing is certain: to create code that evolves itself the code would need cognitive capacity far beyond that of an insect or a toddler.\n\nWill we ever be able to achieve this? We cannot know, however it seems that other existential threats to humanity are far more deserving of attention."
    },
    {
        "url": "https://medium.com/intuitionmachine/teasing-out-tensorflow-graph-mess-64cf5ece4b00",
        "title": "Teasing out the TensorFlow Graph Mess \u2013 Intuition Machine \u2013",
        "text": "Then you can run the embedding TensorBoard visualizer code and get:\n\nLet\u2019s use it with code out of Udacity course (Assignment 2: SGD). No worries guys, there\u2019s no spoiler of the solutions. Defining the following graph:\n\nThere\u2019s a way to insert a Jupyter cell with an iframe containing a TensorBoard view who draws the graph you are playing with. It was published by Google DeepDream team here . Pretty cool indeed, but.. (there\u2019s always a but) TensorBoard yields crap (internal TensorFlow crap to be fair).\n\nFirst of all, I assume you\u2019re familiar with TensorFlow, Python and Jupyter. Maybe you\u2019re learning some MOOC like Udacity Deep Learning , and you can glimpse the extreme power of TensorFlow, but you cannot actually see it . I\u2019ll teach you how to.\n\nHow can mathematicians think in 21 dimensions? Easy, they think in N dimensions and make N=21. For those who can\u2019t do that, there\u2019s a way to visualize TensorFlow graphs without getting lost in the TensorBoard automatically generated crap.\n\nBut there\u2019s a way to make it useful, you just have to name quite a few things in the code to display a better graph. To do so, use name=\u2019name_string\u2019 parameter in some invocations and group some related code lines under kind of \u201cnamespace\u201d declaration in TensorFlow using with tf.variable_scope(\u201cnamespace_string\u201d):\n\nThis last trick will let graph know about what to draw in nested groups, just what we was waiting for. You can group nodes as you want, but in order to enhance encapsulation I did some code refactor (no functional change at all). Let\u2019s see how named code looks like:\n\nYou can run again the embedding TensorBoard visualizer code and get:\n\nIf you drill down double clicking in the rounded boxes there appears the magic we were expecting for:\n\nAnd so on..\n\nVariables has been defined in a separate namespace to be reused in training, validation and testing. They could be considered part of the training process, but they are part of the model bounded context itself.\n\nTraining block includes loss calculation and the optimizer because they belong to the bounded context of training process. Taking them in a separate namespace makes it graphically messier as far as logits is part of the training internals.\n\nSoftmax normalization in training, validation and testing blocks has been intentionally left out of the scope and named respectively as train_prediction_output, valid_prediction_output and test_prediction_output for legibility shake. Letting them inside the scope, the model would end in Softmax normalization, without any clue this is the output Op node it should be run in the Session to get the actual train result and measure the accuracy.\n\nNotice in Training block detail image the block is considering SGD_model/variables as an input, instead of detailing about weights and biases. In next image, MatMul input detail, you can appreciate those details appears when needed.\n\nAnd what happened to all those crap extra nodes that were drawn in the first attempt? Well, they are still there, within the internals it creates to handle the loss and the optimization. They look even prettier isolated in their own scope.\n\nWell, it wasn\u2019t that bad, it was just a naming problem. I hope it will help you to develop a better TensorFlow code, or at least to look through it when you get lost."
    },
    {
        "url": "https://medium.com/intuitionmachine/data-science-is-informative-but-deep-learning-is-transformative-316e61871dd8",
        "title": "Deep Learning is Transformative, Data Science is Just Informative",
        "text": "One of the biggest misconceptions in the market place is that a Data Science or Data Analytics practice is the same thing as a Deep Learning practice. Let me explain to you why it indeed is radically different.\n\nThe difference is most easily explained by the difference in business intelligence and business process reengineering. The latter analyzes business processes to pinpoint areas of restructuring and improvement. The former analyzes business data with the intent of informing stakeholders and executives. The former is informational and the latter is transformative.\n\nDeep Learning as Mark Cuban has quipped is \u201cAutomating Automation\u201d (See: Mark Cuban points to Deep Learning as the next \u2018grand slam\u2019 in technology start at 6:30 mark ):\n\nThink about for example previous technologies like the web and mobile. Now think of how much changes to our business processes were introduced by these two innovations. With the web, customer service became self-serviced. With mobile, customer service could become always a few swipes away. Deep Learning is disruptive technology at equal or even greater level. That\u2019s because it is not just a customer facing technology, but rather one that affects backend activities is so many ways.\n\nThe confusion that most organizations have is that data is critical in both practices and therefore make the incorrect conclusion that they are the same activity. This is a fatal mistake because it blinds decision makers from seeing the disruptive potential of Deep Learning. Deep Learning is about leveraging narrow intelligence to augment complex workflow tasks. It is an ever present , ambient technology that exists to help run daily businesses activities. Data science and analytics are important, but aren\u2019t able to effectively deliver insight into beneficial action. The other disconnect is that the data logistics practice for Deep Learning differs from that of Data Science. We\u2019ll discuss this is a subsequent article.\n\nDeep Learning is an entirely new field that has its genesis from Machine Learning. This is also the root of confusion since it is likely the only people exposed to machine learning in an organization are data scientists. This however should not imply that a group that saw it first is the same group that should be responsible for its deployment. The skill sets are entirely different. The skills required for business process re-engineering are entirely different from that of business intelligence. So why would one assume that data scientists would have the right skill set to work on a deep learning deployment?\n\nThe learning from data aspect also gets many confused. Deep Learning learns from data just as Machine Learning learns from data. How it learns of course is entirely different, but for argument\u2019s sake, let\u2019s not bother with the subtle scientific differences. The key difference is Machine Learning only digests data, while Deep Learning can generate and enhance data. It is not only predictive but also generative. One example of this is the business context is that it is able to generate designs:\n\nDesigns that a knowledge worker can leverage to iteratively explore and select the best options. The same idea can be generalized to many other kinds of activities that a business is involved in. Think for example the notion of generating plans (i.e. sets of activities). A planner can ask a Deep Learning system to provide a recommendation of various plans. A planner can then take those recommendations and iteratively perform improvements. This kind of capability can be very valuable in fast moving reactive environments."
    },
    {
        "url": "https://medium.com/intuitionmachine/is-deep-learning-an-alien-intelligence-c7a6a2bc667a",
        "title": "Is Deep Learning an Alien Intelligence? \u2013 Intuition Machine \u2013",
        "text": "Apologize before hand for the \u2018click-bait\u2019 like headline. I however like to think that my posts are of the pragmatic kind and that you\u2019ll learn something here which you can put into real practice.\n\nI\u2019m not sure exactly were the notion of \u201cAlien Intelligence\u201d came from in the context of AI. There are some references to Stephen Wolfram in this regard (see: \u201cAI and the Future of Intelligence\u201d) however I haven\u2019t found it. Nevertheless, \u201cIs Deep Learning an Alien Intelligence?\u201d is a very thought provoking question.\n\nWe\u2019ve previously read some remarks the the Octopus has an Alien DNA (see: \u201cScientists Declare that Octopuses are Basically Aliens\u201c which actually is a blick-bait headline of a legitimate article on Nature). The use of the adjective here \u201cAlien\u201d is to mean that it is significantly different and not meaning that it is coming from some out-worldly source. The first question to address then is \u201chow similar is Deep Learning to biological brains?\u201d After we answer that, we explore the question \u201chow different is Deep Learning from biological brains?\u201d\n\nDeep Learning has a lot of similarities with biological brains in what they can predict. Biological brains for example are tuned towards face recognition, this is also an area where DL performs well. Brains are able to pick up conversation well in a noisy room, DL is able to do well here too. In speech recognition, DL does well too. Creating music, gesture recognition, style transfer, speech synthesis and many more applications are domains where humans have traditionally excelled over computers are domains that DL has done well too. It may also surprise you, that just like humans, DL systems don\u2019t do very well with simple arithmetic. It is as if, DL systems work at a more primitive cognitive level.\n\nBiological brains differ from Deep Learning not in their predictive capabilities but rather in their form. Here are some of the main differences of brains:\n\n2. There\u2019s no evidence that brains use back-propagation to learn. Furthermore, brains are actively learning all the time. DL has distinct stages like training and inference.\n\n3. The Brain\u2019s neurons uses spiking signalling to asynchronously communicate. DL systems work very different, they are synchronous and maintain continuous signal levels. DL systems are basically massive digital computational systems.\n\nWhat is apparent is that brains are more adaptive and consume less resources to function. The brain evolved to survive, DL systems however are designed to make predictions employing what one could think of as brute-force methods. DL systems use massive computation to arrive at primitive cognitive capabilities. It is still not clear yet why we can\u2019t devise artificial systems that work more like brains. Nevertheless, whatever DL systems are doing, that is narrow artificial intelligence, is unreasonably effective.\n\nSo effective that it is fair to say that it is some kind of alien intelligence. That is, DL is an intelligence that did not evolve out of earth biology. As a matter of fact, many of the latest innovations in DL are so far removed from the original concept of an artificial neural network (ANN) that the use of the term makes zero sense. The one commonality of the latest DL architectures is the notion of differentiable layers. A layer can be any kind of transformational function, the only requirement is that it is differentiable (i.e. analytically continuous). I seriously doubt that biological brains are analytically continuous.\n\nThe 3 take aways that I leave you with, are (1) DL are not like biological brains (2) they may make predictions like biological brains and (3) don\u2019t function like Von Neumann computers. I\u2019m not sure if that got you more confused, but maybe calling them an \u201calien narrow intelligence\u201d may be clearer for all!"
    },
    {
        "url": "https://medium.com/intuitionmachine/modular-deep-learning-and-consciousness-c284ac3aeda3",
        "title": "Modular Deep Learning could be the Penultimate Step to Consciousness",
        "text": "We are all aware that consciousness exists, yet we don\u2019t have an adequate explanation for its emergence. I\u2019ve written down previously about what I see as five capability levels of Deep Learning:\n\nI purposely left out a the penultimate level, that of achieving self-awareness and consciousness. I did so because it is such a wide chasm that will eventually need to be crossed, yet we know so little about. I would like to think that my investigations are evidenced based rather than ones based on thought experiments.\n\nThere are however some theories out there that I think it is worth studying. Not because there is any solid evidence, but rather because the thinking behind them seems to make sense. We have no verification if any of these theories are true, but we can still use the attributes that are predicted. Possibly employing them as that as a guide in our Deep Learning research.\n\nBefore we begin exploring consciousness, I would to point out the kind of architectures that we are using as a starting point. It is my conjecture that to achieve artificial consciousness, you have to have as your base an architecture of independent DL agents that are interacting in a decoupled manner. We discussed this briefly in my post \u201cThe End of Monolithic Deep Learning\u201d as well as an approach for coordinating agents in \u201cMarket Driven Coordination\u201d call this Modular Deep Learning.\n\nI would like to discuss two theories that I find promising. The first is Integrated Information Theory (IIT) by Giulio Tononi and the second is the theories from Jurgen Schmidhuber.\n\n\u201cFrom the Phenomenology to the Mechanisms of Consciousness: Integrated Information Theory 3.0\" or IIT 3.0 describes the theory (Here is a TED talk about this):\n\nIIT begins with several axioms, translates these axioms into postulates that are conditions that must be satisfied to achieve consciousness. The postulates are as follows:\n\nThe theory expands also to a set of agent mechanisms \u201cSystems of mechanisms\u201d :\n\nThe following graphic captures these axioms and postulates in even greater detail:\n\nIIT proposes that consciousness is a matter of degree and proposes a measure of consciousness. In other words, many systems are already conscious, but with varying degrees of consciousness. The theory is quite elaborate, the key take away though is the emphasis on information structure that captures causality and that the richness of that causality structure indicates a measure of consciousness. Note that causality, cause and effect, Bayes rule are all related to mutual information.\n\nSchmidhuber is at interesting character because he is pretty sure that the nature of consciousness has been solved. His theory combines elements that are more familiar to DL practioners, however he discussion consciousness with the context of what he labels as G\u00f6del machines.\u201d His claim is that AI gained consciousness way back in 1991:\n\nJurgen Schmidhuber\u2019s conjecture is that two recurrent networks, one responsible for actions and a second one responsible for predicting the world are the basic ingredients to achieving consciousness. That is CCIK in my classification scheme leads to consciousness. It is indeed extremely intriguing, in fact the second controller is actually performing a kind of meta-learning (see: \u201cMeta Meta-Model\u201d). Here is Schmidhuber talking about this:\n\nWhat is striking about both models of consciousness is that they have similar claims. That is, consciousness already exists in simple mechanisms and that human level consciousness is just a matter of degree. Both theories claim that there is no need for a new kind of mechanism to achieve consciousness. The conceptual missing link to explain consciousness is already known.\n\nAn additional similarity is that there is mechanism that handles temporal causality. IIT revolves around being able to create internal models that capture the causality between concepts. Schmidhuber\u2019s approach employ RNNs that are able to recognize patterns in time.\n\nI am uncertain if the mechanism proposed in IIT are in advanced modular DL systems, we are in the early stages of discovering this. On the other hand, Schmidhuber is saying we don\u2019t need to look far. Schmidhuber in the most recent NIPS conference argued that GANs (\u201cThe coolest thing in the last 20 years\u201d) were identical to his paper in 1992, \u201cLearning Factorial Codes by Predicability Minimization\u201d. This paper describes a system that involves \u201ctwo opposing forces\u201d. I can\u2019t tell if this paper is about the same thing that he claims has achieved consciousness. I don\u2019t know if Schmidhuber is claiming that GANs are conscious!\n\nWhat bothers me about all this is that we can be very near to Artificial General Intelligence (AGI) if these two theories are correct. Alternatively, we can be very far away, not knowing what that missing link may be. In both alternatives, one can\u2019t predict when we arrive at AGI and that is very disconcerting.\n\n\u201cWhile neuroscience might shed light on the input and output functions of the brain, the quantifi- cation for integrated information we have presented here implies that it will be unable to shed light on the complex tangle that is core consciousness.\u201d"
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-enables-agile-processes-6cfc353fa9b2",
        "title": "Turbo Charge Agile Processes with Deep Learning \u2013 Intuition Machine \u2013",
        "text": "There is an inordinate amount of contribution from the software development field to the way we do work and the way we run businesses.\n\nIn the early 2000\u2019s the \u201cAgile Manifesto\u201d was created in response to our collective inability to effectively manage complex software development. The tenets of the manifesto are as follows:\n\nSoftware development is fundamentally a knowledge creation activity. The factory floor processes that are so effective in the physical world simply don\u2019t apply in the virtual world. The agile manifesto was created to challenge our conventionally wisdom of processes that was prevalent at that time.\n\nAgile development has evolved since then and what we have seen is practice of employing automation where needed. The key advantage of agile is its ability to respond quickly to change. If you can automate sections of that feedback loop without compromising, then you have a tighter loop and therefore a more nimble process.\n\nThe concepts of \u201ccontinuous integration\u201d and \u201cdevops\u201d are enabled because of automation. In other words, agile processes are effective because there is a clear understanding of where automation can be best leveraged to enhance the human workflow. The correct prescription for introducing automation in one\u2019s processes is to begin with a process that is centered around humans and then to augment that with automation.\n\nIf this is done in the other direction. That is beginning with a factory floor model (i.e. waterfall) and then automating the parts, then you are left with an even more inflexible processes and a lot of unhappy workers. The mechanization of work may have worked in an earlier era that lacked the coordination and communication capabilities of computers. In a world were computation and devices are abundant and pervasive, the only efficient workflow model that one should consider is that one based on adaptive principles.\n\nIdeas coming from the agile development have lead to further usage in the world of startups. The startup world is a world were agility is a critical capability for survival. Startups are buying into the agile philosophy of prioritizing feedback with potential customers. The purpose is to create a learning organization by effectively iterating through many business plans. For startups with a lack of vision, this kind of \u201cThe Lean Startup\u201d process is better than having no process at all. How can we learn more quickly what works, and discard what doesn\u2019t? That\u2019s the essence of the approach.\n\nThe key to leveraging Deep Learning, or more broadly AI, in the workplace is to understand where it fits within an agile development environment. ( It is of course obvious that Deep Learning development and operations should work within a continuous integration and devops environment. We will discuss this issue in another post.) The application of Deep Learning to one\u2019s own processes requires a keen understanding of our human workflow processes and seeking opportunities to enhance, not a rigid process but one that is adaptive, through automation. The very notion of a complete replacement of a human worker through automation only makes sense when the original business process has always been mechanized.\n\nWhat are the strengths of DL relative to classical AI technologies? The key strength of DL is that is that it is able to function in an almost biological and adaptive manner. That plays very well in the application of DL as a conduit to interaction with humans. We already see this DL applications in speech recognition and in gesture understanding. DL is essentially the new UI. A UI that is ambient, allowing its users to easily summon its capabilities for the task at hand.\n\nMost business perceive DL or more broadly Machine Learning or AI solely as a prediction tool. That is a tool employed by Data Scientists to gather insight about the business. However, prediction is just part of the solution. Prediction requires decision making and subsequent action. Paco Nathan explains this most eloquently:\n\nWe can classify DL capabilities into 2 kinds: Assistive and Generative. These kinds are differentiated by how a human interacts with the DL system. In both kinds, the human is rarely out of the loop, rather the human is in constant collaboration with the AI in their work.\n\nWe are very familiar with automation with assistive capabilities. We experience it every day when we type message on our cell phone. The phone is able to spell correct and provide suggestion to words that we type. We also find this capability in the cameras we use. We now take for granted the auto-focus capabilities of our cameras. Newer DL driven cameras like that found in the Google\u2019s Pixel are even able to blur out image backgrounds. A capability only available on DSLRs with much larger lenses. Assistive capabilities allows us to work at a quicker pace, enhancing on the fly our content creation towards even greater quality.\n\nWe see many of these assistive technologies deployed in the software development space. IDEs are enable with many capabilities that assist and correct developers on the fly. We have style checkers, bug checkers, security vulnerability checkers, code metrics, code coverage etc. applied to source code as we write code. This kind of automated feedback via continuous integration is a best-practice in software development. There are tools that not only correct, but perform tedious tasks that require precision like refactoring.\n\nAs we can see assistive capabilities can happen in real time as well as in the backend. DL allows us to perform repetitive and time consuming tasks such as sorting and categorizing our photo collections. We are also continually overloading with information. There are certain professions where the ability to curate and analyze information is extremely valuable. We can enhance these curation capabilities and analysis capabilities by reducing the deluge of information into smaller chunks that are more quickly digestable.\n\nGenerative capabilities are a new kind of capability that we is just become more pervasive. By now, we\u2019ve all experienced the capabilities of mobile app Prisma, that is able to re-render photographs into the style of different artists:\n\nThis is an example of a generative application. We can use DL to exhaustive explore a design space to serve as a \u201cbrainstorm\u201d of ideas. This mode also works in the realm of planning and execution. DL can quickly explore many scenarios and present workers with a Chinese menu of promising options. We\u2019ve written previously about some interesting examples of generative design: \u201cThe Alien Style of Deep Learning Generative Design\u201d. We have only seen the tip of the iceberg here, there is so much more to explore in this space. One application of DL in the space of \u201cGame Theoretic\u201d design is enough to scare the bejeezus out of anyone."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-deep-learning-can-be-applied-to-natural-languages-46c74a6f861f",
        "title": "Deep Learning Can be Applied to NLP \u2013 Intuition Machine \u2013",
        "text": "There is an article going around the rounds at LinkedIn that attempts to make an argument against the use of Deep Learning in the domain of NLP. The article written by Riza Berkan \u201cIs Google Hyping it? Why Deep Learning cannot be Applied to Natural Languages Easily\u201d has several arguments about DL cannot possibly work and that Google is exaggerating its claims. The latter argument is of course borderline conspiracy theory.\n\nYannick Vesley has written a rebuttal \u201cNeural Networks are Quite Neat: a Reply to Riza Berkan\u201d where he makes his arguments on each point that Berkan makes. Vesley\u2019s points are on the mark, however one can not ignore the feeling that DL theory has a few unexplained parts in it.\n\nHowever, before I do get into that, I think it is very important for readers to understand that DL currently is an experimental science. That is, DL capabilities are actually discovered by researchers by surprise. There are certainly a lot of engineering that goes into the optimization and improvement of these machines. However, its capabilities are \u2018unreasonably effective\u2019, in short, we don\u2019t have very good theories to explain its capabilities.\n\nIt is clear that there are gaps in understanding are in at least 3 open questions:\n\n(1) How is DL able to search high dimensional discrete spaces?\n\n(2) How is DL able to perform generalization if it appears to be performing rote memorization?\n\n(3) How does (1) and (2) arise from simple components?\n\nBerkan\u2019s arguments exploit our current lack of a solid explanation with his own alternative approach. He is arguing that a symbolicist approach is the road to salvation. Unfortunately, no where in his arguments does he reveal the brittleness of the symbolicist approach, the lack of generalization and the lack of scalability. Has anyone created a rule based system that is able to classify images based on low level features that rivals DL? I don\u2019t think so.\n\nDL practitioners, however, aren\u2019t stopping their work just because they don\u2019t have air tight theoretical foundations. DL works and works surprisingly well. DL at is present state is an experimental science and it is absolutely clear that there is something going on underneath the covers that we don\u2019t fully understand. A lack of understanding however does not invalidate the approach.\n\nTo understand the issues better, I wrote in an earlier article about \u201cArchitecture Ilities found in Deep Learning Systems\u201d. I basically spell out the 3 capabilities in DL:\n\nExpressibility \u2014 This quality describes how well a machine can approximate universal functions.\n\nTrainability \u2014 How well and quickly a DL system can learn its problem.\n\nGeneralizability \u2014 How well machine can perform predictions on data that it has not been trained on.\n\nThere are of course other capabilities that also need to be considered in DL: Interpretability, modularity, transferability, latency, adversarial stability and security. But these are the main ones.\n\nTo get our bearing right about explaining all of these, we have to consider the latest experimental evidences. I\u2019ve written about this here \u201cRethinking Generalization\u201d which I summarize again:\n\nThe ICLR 2017 submission \u201cUnderstanding Deep Learning required Rethinking Generalization\u201c is certainly going to disrupt our understanding of Deep Learning . Here is a summary of what the had discovered through experiments:\n\nThe point here that surprises most Machine Learning practitioners is the \u2018brute-force memorization\u2019. See, ML has always been about curve fitting. In curve fitting you find a sparse set of parameters that describe your curve and you use that to fit the data. The generalization that comes into play relates to the ability to interpolate between points. The major disconnect here is that DL have exhibited impressive generalization, yet it cannot possibly work if we consider them as just memory stores.\n\nHowever, if we consider them as holographic memory stores, then that problem of generalization has a decent explanation. In \u201cDeep Learning are Holographic Memories\u201d I point out the experimental evidence that:\n\nAs it turns out, the universe itself is driven by a similar theory called the Holographic Principle. In fact, this serves as a very good base camp to begin a more solid explanation of the capabilities of Deep Learning. I introduce the \u201cThe Holographic Principle: Why Deep Learning Works\u201d where I introduce a technical approach of using Tensor Networks that performs a reduction of the high dimensional problem space into a space that is computable within acceptable response times.\n\nSo going back again to the question about wether NLP can be handled by Deep Learning approaches. We certainly know that it can work, afterall, are you not reading and comprehending this text?\n\nThere certainly is a lot of confusion in the ranks of expert data scientists and ML practitioners. I was aware of the existence of this \u201cpush back\u201d when I wrote: \u201c11 Arguments that Experts get Wrong about Deep Learning\u201d. However, Deep Learning likely can be best explained by a simple intuition that can be explained to a five year old:\n\nA symbolic system cannot read this, however a human can.\n\nIn 2015, Chris Manning, an NLP practitioner wrote about the concerns of the field regarding Deep Learning (see: Computational Linguistics and Deep Learning ). It is very important to take note of his arguments since his arguments are not in conflict with the capabilities of Deep Learning. His two arguments why NLP experts need not worry are as follows:\n\nThe first argument isn\u2019t a criticism of Deep Learning. The second argument explains that he doesn\u2019t believe in one-size-fits-all generic machine learning that works for all domains. That is not in conflict with the above Holographic Principle approach that indicates the importance of the network structure.\n\nTo conclude, I hope this article puts an end to the discussion that DL is not applicable to NLP.\n\nIf perhaps you still aren\u2019t convinced, then maybe Chris Manning himself should convince you himself:"
    },
    {
        "url": "https://medium.com/intuitionmachine/most-enterprises-dont-have-a-deep-learning-strategy-1fc891e83f6a",
        "title": "Most Enterprises Don\u2019t Have a Deep Learning Strategy",
        "text": "Deep Learning is a technology that is as disruptive as mobile computing or the world wide web that came before it. Yet, most enterprises have no strategy on what to do. This is perplexing given that Deep Learning most hyped up slogan is \u201cThe Last Invention of Man\u201d [KHAT]. Why is this so?\n\nIt boils down to one simple fact: enterprises don\u2019t understand Deep Learning. To make it even worse, they can\u2019t possibly understand the current wave of Artificial Intelligence (AI) developments if they don\u2019t understand Deep Learning. Let\u2019s be perfectly clear, the recent spike interest in AI is due primarily to Deep Learning technology. It is not due to other AI technologies like expert systems, semantic knowledge bases, logic programming or bayesian systems. These other AI technologies have not changed much in the last 5 years. The only quantum leap that we\u2019ve seen in the past 5 years has been Deep Learning.\n\nAI has been talked about since the 1950s. The term carries over half a century of extra baggage. Over the decades AI has become too nebulous and thus in its vagueness, isn\u2019t concrete enough to be used as a driver or goal for an enterprise. It makes better sense to focus on technology that is more concrete and real. Claiming to have an AI strategy without a corresponding Deep Learning strategy is akin to admitting that there is no strategy at all!\n\nTo understand how and where your business can apply AI, businesses have to understand what Deep Learning can or cannot do. An effective AI strategy is to recognize the benefits of Deep Learning. It is like any other tool that is used in a business, understand first what the tools are capable of and then planning from there. That, however, may sound simple unfortunately, it is not. Knowledge about the capabilities of Deep Learning is very scarce. There are many Machine Learning and Data Science experts that do not understand this technology. After all, it did not exist when they were learning their trade in graduate school.\n\nJust to give you some perspective as to the extent of Deep Learning developments, look at this graph from Google showing Deep Learning use in their apps:\n\nand this quote from the NY times article \u201cThe Great AI Awakening\u201d [KRAU]:\n\nThe neural system, on the English-French language pair, showed an improvement over the old system of seven points.\n\nHughes told Schuster\u2019s team they hadn\u2019t had even half as strong an improvement in their own system in the last four years.\n\nLet\u2019s recognize what has happened at Google. Google has been since the very beginning been using all kinds of \u201cAI\u201d or machine learning technologies. Their average gains for improvement per year was 0.4. In Google\u2019s first implementation, Deep Learning improvement was 7 points, more improvement that the entire lifetime of improvements. If you are already doing AI, then Deep Learning will accelerate your improvements. If you are not doing AI, then does it make sense to do AI instead of Deep Learning? Deep Learning is real technology and it is taking over operations of the most advanced technology companies in the world. Possibly I should use a different word than real, maybe disruptive is more appropriate.\n\nOne reason why knowledge and talent are so very scarce is because of the extremely rapid development of the Deep Learning field. Just to give you an idea, a month ago the ICLR 2017 conference had around 500 papers submitted for consideration. ICLR 2017 is to be held April 24th of 2017. That will be at least months from now and most people will learn about it only then. We already saw this with the NIPS 2016 conference held last month. Almost all the material in that conference was 6 months old, yet lots of folks were broadcasting about the newness of it all. 6 months is a very long time in the Deep Learning community, yet have folks digging up old lectures from 2014 and 2015 and claiming them to be enlightening.\n\nAlso, it is important to remember that what the big firms chose to publish are likely information that for them is maybe 6 months to a year old. Just witness Google\u2019s revelation of the Tensor Processing Unit (TPU) ASIC. When it was revealed, Google had been using it for at least a year:\n\nSo even if we dutifully read every Arxiv publication on the day it gets published, we are at best maybe 6\u201312 months behind the giants of innovation!\n\nThe key to understanding all these developments is as what Andrew Ng has advised [NG]:\n\nThat doesn\u2019t mean however that developments that are 6 months old are completely useless. Google and the other masters of the universe are themselves resource constrained. Furthermore, these companies will stick to their core competencies and will rarely venture out to other areas. This provides the opportunity for the rest of us. There are plenty of opportunities in domains small and narrow enough that the giants will either ignore or be unaware of its existence.\n\nDeep Learning technology also continues to improve at a torrid pace. So if you start leveraging today\u2019s Deep Learning technologies, you will be able to take advantages of the improvements in algorithms a few months later. But just passively waiting for the technology to improve to the point where it exactly addresses your specific business requirements may put your firm seriously running on the outside curve, possibly forever to be left behind. It therefore is more prudent and pragmatic to begin getting your enterprise\u2019s feet wet with the technology and thus maximizing opportunities to leverage future technological improvements.\n\nThis is the main motivation as to why we have worked on the Design Patterns for Deep Learning. A consistent conceptual model is absolutely essential in our ability to digest the latest research developments. If you curate enough design patterns, you begin to have an intuition of how it all begins to fit together. This intuition allows you to get ahead of everyone else and predict where the field is heading. That\u2019s why we gave a research focus on the Holographic Principle as well as Modular Deep Learning. These are concepts that you likely will not have heard elsewhere, but these are critical as we make progress.\n\nAt Intuition Machine we are working on a Deep Learning Playbook for Enterprise. This playbook provides a starting point for a company to start thinking about a Deep Learning strategy. At a 35,000 foot level, we recommend the following for enterprises:\n\nUnderstand the limitations of the technology. Which problems can be solved and which cannot.\n\nInvest and develop a process that treats data as important assets to be managed, leveraged and enhanced.\n\nBring someone in that understands Deep Learning and data infrastructure.\n\nEvaluate your own business processes to understand which ones can benefit the most from DL. Select the processes that can have the greatest impact to the business.\n\nThere are many software development methodologies, however, Deep Learning has its own unique set of capabilities and thus a unique way of doing things. Learn to fuse this new kind of methodology with your existing software development life-cycle.\n\nThese are some general guidelines that we hope you can jump start your thinking. The key is to begin preparing your company for the inevitable transition to the AI driven economy. Always best to be prepared.\n\nHave a conversation with Intuition Machine to jump start your strategy."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-ai-label-is-bullshit-559b171867ff",
        "title": "The \u201cAI\u201d Label is Bullsh*t \u2013 Intuition Machine \u2013",
        "text": "The term \u201cAI\u201d has become overused, overextended, and marketed to oblivion like \u201cHD\u201d or \u201c3D.\u201d A new product with \u201cAI\u201d in the headline of its press release is thought to be more advanced. The time has come for us to speak clearly about \u201cartificial intelligence\u201d (\u201cAI\u201d) and arrive at a new, clean starting point from which to discuss things productively.\n\nLet\u2019s begin with the words themselves, because if they are vague then we are already obscuring things. Let\u2019s accept \u201cartificial\u201d at face value: it implies something synthetic, inorganic, not from nature, as in \u201cartificial sweetener\u201d or \u201cartificial turf\u201d. So be it.\n\nThe painfully overcharged word here when paired with the word artificial is \u201cintelligence\u201d.\n\nIn thinking about artificial intelligence, I won\u2019t refer to Alan Turing or his famous \u201ctest,\u201d for he himself pointed out (correctly) that this was meaningless. Nor will I quote Marvin Minsky, who passed away recently concerned we were repeating a so-called \u201cAI Winter\u201d: in that lofty expectations would lead to disappointment and an under-investment in the science for years. His worries are well founded, and that\u2019s a different discussion. Another separate discussion is AI\u2019s existential threat to humanity, which Bostrom, Musk, Kurzweil and others have pondered.\n\nInstead let\u2019s look at what nearly all of the software carrying the label \u201cAI\u201d is doing and how it relates to working with information.\n\nI\u2019ve been among the many who have long admired the thinking and writing of Peter Drucker. His clarity of thought is reminiscent of prior generation Austrian writers, including Zweig and Rilke.\n\nSo what would Drucker say about artificial intelligence?\n\nDrucker would say that we are mostly talking about machines performing knowledge work. He would view the \u201cintelligence\u201d label as chimerical.\n\nThe term \u201cknowledge worker\u201d was coined by Drucker, who said in 1957 that \u201cthe most valuable asset of a 21st-century institution, whether business or non-business, will be its knowledge workers and their productivity.\u201d\n\nThe utility of this term was to distinguish laborers (farmers, machinists, construction workers, etc.) in the workforce from a new emerging type of worker (accountants, architects, lawyers, etc.) who worked primarily with information.\n\nHere\u2019s where it gets interesting. This new frontier of work \u201cby thinking\u201d certainly did not exclude machines \u2014 more accurately: computers \u2014 more specifically: software. That\u2019s because the new knowledge-working genre that Drucker perceived in the 1950s was just beginning to interact with computers. Now of course, software has increasingly augmented and replaced human work as it relates to information, and today it is a pervasive phenomenon.\n\nIn fact, a software spreadsheet (one of the most useful and common pieces of software ever created) is capable of knowledge work. It\u2019s doing a fair amount of the work that was done previously by calculators, and prior to that \u2014 a mano by number crunching humans. The spreadsheet is performing tasks that were once performed by a human knowledge worker. That is what it does.\n\nWe don\u2019t refer to the work of an accounting package, a travel booking server, a payroll processor, CAD (computer aided design), and countless other software systems as \u201cAI\u201d.\n\nSoftware has for a long time performed knowledge work and this work has evolved in complexity for decades. It has done so in narrowly defined tasks, always with a specific goal in mind. This is still true today.\n\nWhat about the \u201cAI\u201d that recognizes patterns in stock market data, translates writing from one language to another, transcribes audio or recognizes image patterns? This is also software applied to knowledge work.\n\nWe\u2019re referring to a set of instructions applied to a computer system (CPU, memory, etc.) to move data around, calculate and output values. Today we have have a lot more \u201csystem\u201d today than ever before, and we have a lot more data as well.\n\nThe fact that software is now doing its thing increasingly everywhere is a big thing. It is able to perform \u201cknowledge work\u201d in your car, in your hands, and in the world. It is able to do this kind of work while being connected to informational resources. Knowledge work, as Drucker pointed out, is intrinsically about information.\n\nThe primary difference between today\u2019s knowledge worker and yesterday\u2019s is the amount of processing and information at hand. What is deceivingly branded \u201cAI\u201d today is based on old algorithms (eg. neural networks, invented decades ago) applied to larger computing and datasets.\n\nHerein lies the significance of the term \u201cintelligence\u201d. A laborer is undoubtedly intelligent, a farmer deals with extraordinary amounts of information about crops, soil, weather, etc. But farmers are not knowledge workers, because their craft is not predominantly working with information, that\u2019s secondary to the actual task at hand. The accountant, also intelligent, is on the other hand primarily working with information.\n\nThis is not about \u201cintelligence\u201d, but rather what they are working on, the nature of the work.\n\nJust as knowledge work can be the job of a person, artificial knowledge work can be the job of a software application. This is what the vast majority of software with the \u201cAI\u201d branding is actually doing. But just because it\u2019s called Artificial Intelligence doesn\u2019t mean the software has any intelligence. But that too may be changing.\n\nA field known as \u201cArtificial General Intelligence\u201d, or \u201cAGI\u201d is examining the possibility of software that can \u201cthink\u201d in the pure sense of the word. What is referred to as \u201cAGI\u201d should simply and properly be called \u201cAI\u201d because once machines can acquire knowledge, learn adaptively and make rational choices, they become not just knowledge workers, but truly intelligent.\n\nIn conclusion, software continues to evolve in its capacity to perform knowledge work: narrowly defined information-driven tasks with specific objectives. The label \u201cintelligence\u201d has to do with something much more fundamental and elusive.\n\nThe human accountant has the ability to learn to be an architect (a different type of knowledge worker) but today\u2019s artificial knowledge worker cannot adapt this way. Software code can \u201clearn\u201d but thus far only within a specific type of task. DeepMind\u2019s \u201cAlphaGo\u201d defeated a Go professional player but it cannot play checkers, tic-tac-toe, or any other game. The \u201csmartest\u201d software applied in consumer and business settings today lacks the capacity to adapt itself outside of its intended purpose. It is utilitarian.\n\nThe scientific pursuit of artificial intelligence aims to change this. Will we see real advancements on this front? Are you ready?"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-holographic-principle-and-deep-learning-52c2d6da8d9",
        "title": "The Holographic Principle: Why Deep Learning Works \u2013 Intuition Machine \u2013",
        "text": "What I want to talk to you about today is the Holographic Principle and how it provides an explanation to Deep Learning. The Holographic Principle is a theory (see: Thin Sheet of Reality) that explains how quantum theory and gravity interact to construct the reality that we are in. The motivations for this theory comes from the paradox that Hawking created when he theorized that black holes would emanate energy. The fundamental concept that had been violated by Hawking\u2019s theory was that information was destroyed. As a consequence of this paradox, through several decades of research and experimentation, physicists have brought forth a unified theory of the universe that is based on information theoretic principles. The entire universe is a projection of a hologram. It is entirely fascinating that the arrow of time and the existence gravity are but mere manifestations of information entanglement!\n\nNow, you may be mistaken to think that this Holographic Principle is just some fringe idea from physics. It appears at first read to be quite a wild idea! Apparently though, the theory rests on very solid experimental and theoretical underpinnings. Let\u2019s just say that Stephen Hawking who first remarked that is was \u2018rubbish\u2019 has finally agreed to its conclusions. So at this time, it should be relatively safe to start deriving some additional theories of this principle.\n\nOne surprising consequence of this theory is that the hologram is able to capture the dynamics of the universe that has of the order of d^N degrees of freedom (where d is the dimension and N is the number of particles). One would think that the hologram would be of equal size, but it is not. It is a surface area and is proportional only to N\u00b2. This begs the question, how is an structure of order N\u00b2 able to capture the dynamics of a system in d^N?\n\nIn the meantime, Deep Learning (DL) coincidentally has a similar mapping problem. Researchers don\u2019t know how it is possible for DL to perform so impressively well considering the problem domain\u2019s search space has an exceedingly high dimension. So, Max Tegmark and Henry Lin of Harvard, have volunteered their own explanation \u201cWhy does deep and cheap learning work so well?\u201d In their paper they argue the following:\n\nThe authors bring up several promising ideas like the \u201cno-flattening theorems\u201d as well as the use of information theory and the renormalization group as explanations for their conjecture. I however was not sufficiently convinced by their argument. The argument assumes that all problem data follows \u2018natural laws\u2019, but as we all know that DL can be effective in unnatural domains. See, Identifying cars, driving, creating music and playing Go as trivial examples of clearly an unnatural domain. To be fair, I think that they were definitely on to something, and that something I discuss in more detail .\n\nIn this article, I make a bold proposal with an argument that is somewhat analogous to what Tegmark and Lin proposed. Deep Learning works so well because of physics. However, the genesis of my idea is that DL works because it uses the leverages the same computational mechanisms underlying the Holographic Principle. Specifically, the capability of representing an extremely high dimensional space (i.e. d^N) with a paltry number of parameters of the order N\u00b2.\n\nThe computational mechanism underpinning the Holographic Principle can be most easily depicted through the use of Tensor Networks (note: These are somewhat different from the TensorFlow or the Neural Tensor Network). Tensor network notation is as follows:\n\nThe value of tensor networks in physics is that they are used to drastically reduce the state space into a network that focuses only on the relevant physics. The primary motivation behind the use of Tensor Networks is to reduce computation. A tensor network is a way to perform computation in a high dimensional space by decomposing a large tensor into smaller more manageable parts. The computation can then be performed with smaller parts at a time. By optimizing each part one effectively optimizes the full larger tensor.\n\nIn the context of the holographic principle, the MERA tensor is used and it is depicted as follows:\n\nIn above the circles depict \u201cdisentanglers\u201d and the triangles \u201cisometries\u201d. One can look at the nodes from the perspective of a mapping. That is the circles map matrices to other matrices. The triangles take a matrix and map it to a vector. The key though here is to realize that the \u2018compression\u2019 capability arises from the hierarchy and the entanglement. As a matter of fact, this network embodies the mutual information chain rule:\n\nIn other words, as you move from the bottom to the top of the network, the information entanglement increases.\n\nI\u2019ve written earlier about the similarities of Deep Learning with \u2018Holographic Memories\u2019 however here I\u2019m going to make one step further. Deep Learning networks are also tensor networks. Deep Learning networks however are not as uniform as a MERA network, however they exhibit similar entanglements. As information flows from input to output in either a fully connected network or a convolution network, the information are similarly entangled.\n\nThe use of tensor networks has been studied recently by several researchers. Miles Stoudenmire wrote a blog post: \u201cTensor Networks: Putting Quantum Wavefunctions into Machine Learning\u201d where he describes his method applied to MNIST and CIFAR-10. He writes about one key idea about this approach:\n\nAmnon Shashua et al. have also done work in this space. Their latest paper (Oct 2016) \u201cTensorial Mixture Models\u201d proposes a novel kind of convolution network.\n\nIn conclusion, the Holographic Principle, although driven by quantum computation, reveals to us the existence of a universal computational mechanism that is capable of representing high dimensional problems using a relatively low number of model parameters. My conjecture here is that this is the same mechanism that permits Deep Learning to perform surprisingly well.\n\nMost explanations about Deep Learning revolve around the 3 Ilities that I described here. These are expressibility, trainability and generalization. There is definitely consensus in \u201cexpressibility\u201d, that is of a hierarchical network requiring less parameters that a shallow network. The open questions however are that of trainability and generalization. The big difficulty in explaining away these two is that they don\u2019t fit with any conventional machine learning notion. Trainability should be impossible in a high-dimensional non-convex space, however simple SGD seems to work exceedingly well. Generalization does not make any sense without a continuous manifold, yet GANs show quite impressive generalizations:\n\nThe above figure shows the StackGAN generating, given text descriptions , output images in two stages. For the StackGAN there are two generative networks and it is difficult to comprehend how the second generator captures only image refinements. There are plenty of unexplained phenomena like this. The Holographic Principle provides a base camp to a plausible explanation.\n\nThe current mainstream intuition of why Deep Learning works so well is that there exists a very thin manifold in high-dimensional space that can represent the natural phenomena that it is trained on. Learning proceeds through the discover of this \u2018thin manifold\u2019. This intuition however breaks apart considering the recent experimental data (see: \u201cRethinking Generalization\u201d). The authors of the \u2018Rethinking Generalization) paper write:\n\nBoth the Tegmark argument and the \u2018Thin Manifold\u2019 argument cannot possibly work with random data. This thus lead to the hypothesis that there should exist an entirely different mechanism that is reducing the degrees of freedom (or problem dimension) so that computation is feasible. This compression mechanism exists can be found in the structure of the DL network, just like it exists in the MERA tensor network.\n\nConventional Machine Learning thinking is that it is the intrinsic manifold structure of the data that needs to be discovered via optimization. In contrast, my conjecture claims that the data is less important, rather it is the topology of the DL network that is able to capture the essence of the data. That is, even if the bottom layers have random initializations, it is likely that the network should work well enough subject to a learned mapping at the top layer.\n\nIn fact, I would even make a bigger leap in that in our quest for unsupervised learning, we may have already overlooked the fact that a neural network has already created its own representation of the data at onset of random initialization. It is just our inability to interpret that representation that is problematic. A random representation that preserves invariances (i.e. locality, symmetry etc.) may just be a good as any other representation. Yann LeCun\u2019s cake might already be present and that it is just the icing and cherry that needs to explain what the cake represents.\n\nNote to reader: In 1991, psychologist Karl Pribham with physicist David Bohm had speculated about Holonomic Brain Theory. I don\u2019t know the concrete relationship between the brain and deep learning. So I can\u2019t make the same conclusion that they made in 1991.\n\nI leave you with one quote from Demis Hassabis of DeepMind:\n\n\u2661 Heart if you think this idea has legs!"
    },
    {
        "url": "https://medium.com/intuitionmachine/is-conditional-logic-the-new-deep-learning-hotness-96832774907b",
        "title": "Is Conditional Logic the New Deep Learning Hotness?",
        "text": "One clear trend is the pervasive use of conditional logic in state-of-the-art Deep Learning (DL) architectures. An even bigger trend to this is that DL architectures are beginning to look more like conventional computers (see: DeepMind\u2019s Differentiable Neural Computer). In fact, you can follow the progress of DL research by recognizing how classical computational constructs are retrofitted into this new architecture.\n\nAnyone who has an exposure to programming (or alternatively flowcharting) is aware that its composed of 3 kinds of things. There are 3 kinds of things that are explicitly obvious: computation, conditional logic and iteration (or recursion). In the beginning there was just the neuron, it had a computational unit (i.e. sum of products) and a conditional unit (i.e. activation function). Layers were added, to give it a way to be composed in an many different ways, this begat the Deep Learning revolution (this is the overly simplified version of the genesis story of DL).\n\nFolks then said, let\u2019s add loops into the network. That begat the RNN. The RNN was quite chaotic so a solution that included memory (or a buffer) that led to the more usable LSTM. The computational unit was further enhanced through a more powerful matching engine known as the convolution, this begat the Convolution Network (ConvNet). The ConvNet also had a more general activation function, the pooling layer. The work horses of classical DL are ConvNets and LSTMs.\n\nThere is plenty of research work with the areas of improving the computational unit as well as in introducing memory. However, I have found the developments in conditional logic quite fascinating and illuminating. This is because a conditional unit is so simple that it can\u2019t possibly be very interesting. In this article, I will show you why the neglected conditional logic unit is the hottest thing since the \u201cResidual network\u201d.\n\nLet\u2019s start then with the Residual network. The Residual network surprised the DL community by besting the 2015 ImageNet benchmark with a record breaking number 152 layers. 8 times deeper than previously seen. There have been several research papers that analyze the behavior of the Residual network. The latest one comes from Schmidhuber et. al. \u201cHighway and Residual Networks learn Unrolled Iterative Estimation\u201d. Their conclusions are important:\n\nThis confirms other research on this subject. In addition though they discovered the following:\n\nResidual networks since 2015 are all the rage everywhere. Systems that have used Residual connections (aka skip, shortcut, passthrough or identity parameterization) have been shown to best the state-of-the-art in all too many occasions. Residual connections have become a mandatory feature for any state-of-the-art architecture. By throwing conditional logic into the concoction we arrive at an even more potent potion.\n\nMicrosoft has some very interesting stuff that is in fact related to this called Conditional networks \u201cDecision Forests, Convolutional Networks and the Models in-Between\u201d:\n\nWith some very impressive results:\n\nThe Microsoft solution uses conditional logic as network routing parameters. It is similar to a gated residual, but one that does not skip layers.\n\nActivation functions and pooling layers have limitations. They are fixed and not modified during training. Furthermore, they are not information preserving in that they always destroy information between layers. Residuals and conditional networks are selective in how information traverses through layers. This kind of routing capability is not entirely new in that we\u2019ve seen it in LSTMs. What is new though is that the routing happens not within a single LSTM node but rather across layers.\n\nWhenever we talk about conditional logic we are faced with the problem of how to handle discrete values. DeepMind appears to have avery interesting ICLR 2017 paper that they call the Concrete distribution ( \u201cThe Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\u201d). The distribution provides a method to backpropagate through a discrete, rather than the typical continuous, variable. These can lead to further refinement on how we train for conditional logic.\n\nNevertheless, when we look at the success of this new kind of architecture, we are left being perplexed in that it is difficult to image how SGD can reach convergence. Conditional logic will imply that entire sections of networks become inactive in certain contexts. I suspect that are certain constraints exists that guide the effective use of conditional logic. We\u2019ll explore this in more detail later.\n\nRecursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-alien-look-of-deep-learning-generative-design-5c5f871f7d10",
        "title": "The Alien Style of Deep Learning Generative Design \u2013 Intuition Machine \u2013",
        "text": "What happens when you have Deep Learning begin to generate your designs? The commons misconception would be that a machine\u2019s design would look \u2018mechanical\u2019 or \u2018logical\u2019. However, what we seem to be finding is that they look very organic, in fact they look organic or like an alien biology. Take a look at some of these fascinating designs.\n\nThe photo above design is described as follows:\n\nThis is a car frame that is designed by a generative algorithm:\n\nThe design on the right of an antenna is twice more effective that that on the left:\n\nMany of these designs come from Autodesk\u2019s DreamCatcher research.\n\nGenerative designs also exist not only in the physical world but also in the design of neural networks themselves:\n\nThese Long Short Term Memory (LSTM) are designed by an algorithm and shown to be more effective than the conventional LSTM (Note: These are Neural Networks designed with memory elements). These are generative neural architectures, machines that learn to learn, more like meta meta-models. Learning apparently is not uniform and I highly suspect that meta-level reasoning is a primary mechanism in learning and that is reflected by its biological manifestation. After all, isn\u2019t learning enhanced by diversity as well as adaptability? The same ingredients to biological survival?\n\nWhat is surprising is that these designs do not exist for the sake of style. Rather, these designs are actually the optimal solutions to multiple competing design requirements. Why do they look organic or biological? Is there some underlying fundamental principle that exists in biological systems that leads to this? Why aren\u2019t the solutions sparse, but rather complex?\n\nEven a more deeper question is, if these were the optimal designs, then why don\u2019t inanimate objects look like this? Inanimate objects that are complex tend to have a fractal style:\n\nThe self-similar repeating patterns that we see in crystals and coastlines ,despite looking complex, certainly have a style that is different from organic or biological styles. Deep Learning clearly has similar capabilities as biological systems. I suspect that this difference originates from the difference in computational machinery that generates these. It indeed is fascinating that the style of these generated objects are a reflection of the process of its originator.\n\nSelf-similarity are created through uniform processes that occur in multiple scales. In contrast, biological processes also do occur in multiple scales, diversity rather uniformity is encouraged. Humans for example, exhibit a five fold symmetry. We see similarity occurring at multiple scales, meaning we see interesting features different coarse levels of granularity, however we also see diversity. This is why designs by Deep Learning systems look biological, the construction process use similar mechanisms.\n\nA recent study \u201cInside an AI brain \u2014 What does machine look like?\u201d Reveals similar biological like complexity in Deep Learning systems. Here is a visualization of a ResNet-50 Deep Learning network:\n\nIt indeed is fascinating that the complexity if very different from the complexity we find in chaos theory.\n\nOne misconception is that Artificial Neural Networks or Deep Learning are biologically inspired. This is NOT true. It may have thought as being true back in the 1950\u2019s when the Perceptron was first introduced. However, at best, Deep Learning employs a cartoonish version of a neuron and the newer architectures consist of computational elements that are very different from how a biological neuron would function. Nevertheless, it is indeed surprising though, that despite being very different from biology, that the resulting observable behavior appear to be very similar between both. Clearly, there is a set of fundamental processes underlying all of this that is both shared by biology and Deep Learning. That fundamental processes is what we are all seeking to discover.\n\nOne final thing though, just because DL exhibits some behavior that appears to be biological, it still is a far cry away from something that is intelligent. However, we at least know of one instance of an intelligence, and that happens to be biological.\n\nKnow more about this at Intuition Machine, read the publication: https://medium.com/intuitionmachine or join the FaceBook group: https://www.facebook.com/groups/deeplearningpatterns/\n\n\u2661 Heart if you find this discovery fascinating!"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-meta-model-and-meta-meta-model-of-deep-learning-10062f0bf74c",
        "title": "The Meta Model and Meta Meta-Model of Deep Learning",
        "text": "The model for deep learning consists of a computational graph that are most conveniently constructed by composing layers with other layers. Most introductory texts emphasize the individual neuron, but in practice it is the collective behavior of a layer of neurons that is important. So from an abstraction perspective, the layer is the right level to think about.\n\nUnderneath these layers are the computational graph, it\u2019s main purpose is to orchestrate the computation of the forward and backward phases of the network. From the perspective of optimizing the performance, this is an important abstraction to have. However, it is not at the ideal level to reason how it all should work.\n\nDeep Learning frameworks have evolved to develop models that ease construction of DL architectures. Theano has Blocks, Lasagne and Keras. Tensorflow has Keras and TF-Slim. Keras is based on Torch, so by default has a high-level modular API. Many other less popular frameworks like Nervana, CNTK, MXNet and Chainer do have high level model APIs. All these APIs however describe models. What then is a Deep Learning meta-model? Is there even a meta meta-model?\n\nLet\u2019s explore first how a meta-model looks like. A good example is in the UML domain of Object Oriented Design. This is the UML metal model:\n\nThis makes it clear that Layers, Objectives, Activations, Optimizers, Metrics in the Keras APIs are the meta-models for Deep Learning. That\u2019s not too difficult a concept to understand.\n\nConventionally, an Objective is a function and an Optimizer is an algorithm. However, what if we think of them instead as also being models. In that case we have the following:\n\nThis definitely is getting a whole lot more complicated. The objective function has become a neural network and the optimizer has also become a neural network. The first reaction to this is, has this kind of architecture been tested before? It\u2019s possible someone is already writing this paper. That\u2019s because an objective function that is a neural network is equivalent to the Discriminator in a Generative Adversarial Network (GAN) and an Optimizer being a neural network is precisely what a meta-learner is about. So this idea is not fantastically out of mainstream research.\n\nThe second reaction to this is, shouldn\u2019t we make everything neural networks and be done? There are still boxes in the diagram that are still functions and algorithms. The Objective\u2019s optimizer is one and there are 3 others. Once you do that, there\u2019s nothing else left that a designer needs to define! There are no functions, everything is learned from scratch!!\n\nSo a meta-model where everything is a neural network looks this:\n\nWhere the mode is broken apart into 3 parts just for clarity. Alternatively, it looks like this:\n\nWhat this makes abundantly clear however is that the kinds of layers that are available come from a fixed set (i.e. fully connected, convolution, LSTM etc.). There are in fact research papers that exploit this notion of selecting different kinds of layers to generate DL architectures( see: \u201cThe Unreasonable Effectiveness of Randomness\u201d ). A DL meta-model language serves as the lego blocks of an exploratory RL based system. This can generate multiple DL meta-model instances to optimize for the best architecture. That is a reflection of the importance of Deep Learning Patterns. Before you can generate architectures, you have to know what building blocks are available for exploitation.\n\nNow, if we make a quantum leap into meta meta-model of Deep Learning. What should that look like?\n\nLet\u2019s look at how OMG\u2019s UML specification describes the meta meta-model level (i.e. M3):\n\nThe M3 level has a simplified structure that only includes the class. Following an analogous prescription, we thus have the meta meta-model of Deep Learning defined by the following:\n\nDespite the simpleness of the depiction, the interpretation of this is quite interesting. You see, this is a meta object that an instance of which is the conventional DL meta-model. These are the abstract concepts that define how to generate new DL architectures. More specifically, it is the language that defines the creation of new DL models such as a convolution network or a autoregressive network. When you work at this level, you essentially generate new kinds of DL architectures. This is what many DL researchers actually do for a living, designing new novel models.\n\nThere is one important concept to remember here though, the instance, model, meta-model and meta meta-model distinction are concepts that we\u2019ve invented to better understand the nature of language and specification. This concept that is not essentially and likely does not exists in separate form in reality. As an example, there are many programming languages that do not have a distinction between instance data and model data. Languages like Lisp are like this, where everything is just data, there is not distinction between code and data.\n\nThe idea of \u201ccode is data\u201d applied to DL is equivalent to saying that the DL architecture are representations that can be learned. We as humans require the concept of a meta meta-model to get a better handle of the complex recursive self-describing nature of DL systems. It would be interesting know what the language of the meta meta-model should look like. Unfortunately, if this language is one that is learned by a machine, then it may likely be as inscrutable as any other learned representation. See: \u201cThe Only Way to Make DL Interpretable\u201d.\n\nIt is my suspicion though that this meta meta-model approach if pursued in greater detail may the key in locking \u201cUnsupervised learning\u201d or alternatively \u201cPredictive learning\u201d. Perhaps our limited human brains cannot figure this out. However armed with meta-learning capabilities, it may be possible for machines to continually self improve upon themselves. See \u201c Meta-Unsupervised-Learning: A supervised approach to unsupervised learning\u201d for an early take on this approach.\n\nThe one reason that this may not work however is that the vocabulary or language that is the is limited (see: Canonical Patterns) and therefore \u201cpredictive learning\u201d is not derivable from this bootstrapping method. Meta-learners today discover can only the weights and the weights are just parameters of a fixed DL model. A discovery, even through evolutionary methods, can only happen if the genesis vocabulary is at the correct level. Evolution appears to be a Meta Metal-Model process.\n\nThere is plenty that is missing in our understanding of the language for the meta meta-model of DL. Perhaps we can discover this only if we work up the Capability levels of Deep Learning intelligence. DARPA has a program that is researching this topic \u201cDARPA goes \u2018Meta\u2019 with Machine Learning for Machine Learning\u201d. I hope to refine this idea over time.\n\nSee Deep Learning Design Patterns for more details or visit \u201cIntuition Machine\u201d to keep abreast about the latest developments."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-3-essential-ingredients-of-intelligence-where-mathematics-breaks-down-be636eae2607",
        "title": "The 3 Traits of AI where Math Hits its Limits \u2013 Intuition Machine \u2013",
        "text": "There are 3 essential ingredients that are needed to understand intelligence and unfortunately present day mathematics has trouble tackling. Mathematics are tools that enhance our reasoning processes. Mathematics is a human language that we employ to derive understanding of reality. However, this language are not all powerful and does have limitations. We explore some of these limitations here with respect to areas important to AI.\n\nAlthough mathematics tends to get developed way ahead of its time, there are many times that the application of a different kind of mathematics to a new domain leads to breakthroughs. Richard Feynman, for example, employed century-old path integrals mathematics to gain new insight on developing Quantum Electrodynamics. There are however plenty of limitations in mathematics and this article addresses those limitations with respect to our ability to comprehend essential ingredients of cognition.\n\nThe \u201cQuasi-empiricism\u201d of math is not a new idea. Mathematics is a human language that we employ to describe our reality. Quoted from the Wikipedia article [WIKI-1]:\n\nThe first ingredient is the notion of time.\n\nTime is a difficult concept to grasp. I guess the easiest way to handle it is just do what Einstein did. Just treat it as another dimension.\n\nMost physics is invariant in time. Meaning, you can move forward or backward in time and the physics are identical. However, at the macro-world we don\u2019t see it that way, time exists because entropy exists. The arrow of time follows that of increasing entropy.\n\nIn fact, there\u2019s really no notion of memory without having to consider the existence of time.\n\nMost mathematics don\u2019t have a concept of memory. Memory is the equivalent of having state and almost all mathematics involves functional constructs that are stateless. Functional programming follows a single assignment rule where once any variable is set, it remains set to that state, never changing. It is this constraint that makes the use of functional programming something that is easily parallelizable. It is a convenient constraint that allows our mathematics to be analyzable.\n\nWe cannot, however, avoid time, because that\u2019s where the dynamics come from. The only context that mathematics is helpful in dynamics is in the context where there is no memory exits. Introduce memory or introduce state, then all bets are off! The best that mathematics can do is to quantify the boundaries of computation and not predict its final behavior (see: https://en.wikipedia.org/wiki/Halting_problem) [WIKI-2].\n\nThe only dynamics that is analyzable by math are equilibrium states. We can only make statements about states that are in equilibrium. What happens in between, that is computation, can only be, at best, be simulated. Equilibrium is the state when we assume that time is at infinity. An unrealistic assumption, but an assumption that is brought about by convenience.\n\nThere is also the notion of asynchrony that is such a beast in complexity. That is, when different parallel processes are not in lockstep synchrony. All our digital circuitry require lockstep synchrony in the form of a common clock that drives behavior. The biological brain does not have a common clock, it works in a regime of asynchrony.\n\nThe second ingredient is the notion of collective emergent behavior.\n\nRobert Robert Sapolsky has a short lecture on Youtube [SAP] (\u201cThinking about emergence and chaos\u201d) that brings about the point about bottom-up behavior (Special thanks to Felix Hovsepian for pointing put this video). He says that \u201cmost of the stuff that he and his peers do is reductive stuff that is very limited.\u201d\n\nIntelligence comes from the emergent behavior that arises from the collective behavior of millions or billions of interacting components. This is the very essence of the concept of Connectionist AI. The components themselves do not have to be constructed in a complex manner and can be very simple and in fact be all uniform. Artificial Neural Networks and Deep Learning spring from this very idea of deriving intelligence from simple components called \u2018neurons\u2019. It is important to remind oneself that the neurons in ANN are a cartoonish version of a biological neuron. However, it is not the precise construction of the neuron that is important, but rather it is the collective behavior that is important.\n\nThat is why the reasoning that ANN and DL should be rejected because they are not biologically plausible is a very bad argument. It is entirely conceivable that intelligence can be arrived with very different kinds of \u2018neurons\u2019. That\u2019s because, there\u2019s some fundamental capability that a neuron performs (i.e. information dynamics, meaning computation, memory, and signaling) that is all that is needed, however, the connectivity is where intelligence emerges.\n\nThe third ingredient is the notion of meta-level reasoning.\n\nThis is the most difficult to grasp idea and it may, in fact, be the reason why \u2018consciousness\u2019 exists. We can understand the idea of building up ideas by the composition of more primitive ideas. We can understand this because that is how language is constructed. That is, from letters to syllables to words to sentences to paragraphs etc.\n\nWe also know of meta-level reasoning. It\u2019s one of those ideas that\u2019s hard to explain to novice programmers, but it exists in many programming languages. That is, you have programs that operate on the building blocks of the language itself. It leads to very expressive and short source code. Experienced programmers have no difficulty working at the meta-level. However, these kinds of system are extremely difficult to debug.\n\nHowever, it doesn\u2019t stop with just one level of meta-reasoning. You could have meta-meta level constructs ad infinitum. I\u2019ve encountered this idea in the wild in the modeling language UML. There\u2019s a concept of meta-metamodels, here\u2019s the definition:\n\nWhich, it just occurs to me, is the most universal definition of \u201cGeneralization\u201d.\n\nThis lecture by James Crutchfield on \u201cThe Complexity of Simplicity\u201d gives a very good sense of the enormous gap that we have between our math its ability to analyze complex systems:\n\nI\u2019ve received the wrong impression with this article that I\u2019m implying that mathematics is not needed. On the contrary, it is absolutely necessary. However, I am also banging the table for those who can\u2019t see that present day mathematics has its limitations. There are many who continue to stick to 18th-century Bayesian logic and corresponding mathematics and have an unsubstantiated belief that it is actually even going to work in this new domain.\n\nThere are plenty of times where I see researchers attempt to cast DL systems in terms of \u2018equivalent\u2019 Bayesian networks in the hope that placing a round peg into a square hole will actually work. Well, it\u2019ll work if the round peg\u2019s diameter is smaller than the whole width of the square. But it is obvious that it wouldn\u2019t be a great fit. There is absolutely no evidence that the reductionist logic is going to work in a domain of collective emergent behavior. If you approached a room of statistical physicists about using Bayesian inference, then you likely will be thrown out of the room in ridicule. Let\u2019s all get real folks!"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-end-of-monolithic-deep-learning-86937c86bc1f",
        "title": "The Emergence of Modular Deep Learning \u2013 Intuition Machine \u2013",
        "text": "Deep Learning compared to other Machine Learning methods is remarkably modular. This modularity gives it unprecedented capabilities that places Deep Learning head and shoulders above any other conventional Machine Learning approach. Recent research however is pointing to even greater modularity than previously. It is likely that quite soon, monolithic Deep Learning systems will become a thing of the past.\n\nBefore I discuss what is coming in the future, let me first discuss the concept of modularity. It is a concept that software engineering is familiar with, but the idea is not as commonly found in machine learning.\n\nIn computer science, we build up complex systems from modules. One module built from more simple modules. It is what enables us to build our digital world based on just NAND or NOR gates. Universal Boolean operators are a necessity, but they are not sufficient enough to build complex system. Complex computing systems require modularity so that we have a tractable way of managing complexity.\n\nHere are six core design operators that are required to be supported in a modular systems:\n\nThese operators are of a general nature and inherent in any modular design. They allow the modification of existing structure into new structures in well-defined ways. In the context applied to software this can mean refactoring operators at the source code level, language constructs at specification time or can mean component models at configuration time. These operators are complete in that they are capable of generating any structure in computer design.\n\nThe six operator definition focuses on functional invariance in the presence of design transformations. Said more clearly, we can apply these operators and not affect the function of the whole.\n\nIn the context of deep learning, the modularity operators are enabled as follows:\n\nThe two remaining modularity operators are not available to current monolithic DL systems.\n\nNevertheless, despite these two short comings, DL systems are unmatched advantage over competing machine learning techniques.\n\nOne of the reasons for the tight coupling of layers in a monolithic DL system can be traced back to Stochastic Gradient Descent (SGD). SGD works in lock-step mode with training. It is a very highly synchronized mechanism that requires behavioral coordination across all layers.\n\nThis monolithic construction is however being replaced by an even more modular system. Here are two interesting developments related to this.\n\nDeepMind has researched a method called \u201cSynthetic Gradients\u201d that shows a way towards more loosely coupled layers. The method essentially inserts a proxy neural network in between layers to approximate the gradient descent:\n\nThe second development that can lead to greater modularity is the concept of generative adversarial networks (GANs). Typical GANs have two competing neural networks that are essentially decoupled, but contribute to the global objective function. However, we are now seeing in research the emergence of more complicated configurations like this:\n\nWhere you have a ladder like network of decoupled encoder, generators and discriminators. The prevalent pattern here is that every conventional function in a neural network has been also replaced by a neural network. More specifically, the SGD algorithm and the objective function have been themselves been replaced by neural networks. Gone are any analytic functions! This is what happens when you have Deep Meta-Learning.\n\nAnother very impressive result that was recently published with also the same name (i.e. StackGAN), shows how effective multiple decoupled GANs can be:\n\nThe task here is to take as input a text description and generate an image corresponding to the description. Here we have two GANs staged one after the other. The second GAN is able to refine the fuzzy image into one of a higher resolution. Modular networks have the capability of factorizing capabilities that would otherwise be entangled in an end-to-end network.\n\nIn software engineering we have the concept of APIs. That is, a restrictive language that communicates between different modules. In the scenario above neural networks that \u201clearning to communicate\u201d acts as the bridge APIs between the modules. More generally, we have networks that \u201clearn how to interface\u201d. From \u201cLearning to Communicate with Deep Multi-Agent Reinforcement Learning\u201d:\n\nAnother recent paper titled \u201cGenerative Adversarial Parallelism\u201d explores this further in relationship to GANs. . In this work, the authors attempt to address the difficulty in training GANs by extending the usual two player generative adversarial games into a multi-player game. They train many GAN-like variants in parallel and while doing so, they periodically swap around the discriminator and generator pairs. The motivation here is to achieve a better decoupling between the pairs. There is still much work to be done to determine if if decoupled interfaces between networks lead to be better generalization.\n\nThere\u2019s still a ton of research to be done to begin to understand how to build the APIs for DL modules. Right now however, we have the benefit of meta-learning techniques that will automatically learn the interface specification. This indeed is a very interesting research topic. How do decoupled networks learn how to interface?"
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-could-be-market-driven-de770aeebd3",
        "title": "Equilibrium Discovery in Modular Deep Learning Architectures",
        "text": "Deep Learning (DL) systems will evolve from the current monolithic systems, to more modular systems. The traditional DL system is trained end-to-end with a single objective function and optimization algorithm. We however are already seeing newer systems like GANs, that involve more than one DL system. GANs employ a generator and a discriminator, that are in an adversarial relationship, competing against each other. The main difficulty of training GANs is that finding an equilibrium is difficult.\n\nAs we progress to even more complex systems, that involve more than two participating DL systems, we need to have some guidance as how to coordinate multiple parties. Here is an example of one of the newer networks that reveal 6 different DL systems working in an adversarial context:\n\nMore advanced DL systems will require even more complex coordination mechanisms.\n\nCivilization actually has a mechanism that is very efficient in coordinating multiple parties. We use currency (i.e. money) to coordinate multiple parties with competing agendas. Perhaps, through the use of market driven dynamics, we can invent a better mechanism for handling coordination.\n\nIt is indeed interesting that in Physics, there are many conservation laws. These laws such as the conservation of momentum or the conservation of energy govern the physical world and give order to chaos. Without conservation laws, strange things would happen in our world all the time. The current understanding of the Higgs boson (aka The God particle) is that its mass is at a certain level that if it were any less or any more, a universe like ours would not exist. Our universe is in exists, because some God particle is of a certain specific mass. The ultimate hyper-parameter!\n\nMarket driven systems have money as the conserved quantity (with the exception of Quantitative Easing). The basic conceptual idea here is to have the market of networks coordinate themselves with money. A virtual currency so to speak. To drive the system towards objects, one would institute monetary incentives or disincentives. There isn\u2019t any explicit objective function here other than having mechanism that would \u201cencourage\u201d certain behaviors.\n\nMarket driven machine learning has in fact been written about previously. In a paper entitled \u201cAn Introduction to Artificial Prediction Markets for Classification\u201d the authors describe a framework for fusing multiple classifiers:\n\nAnother paper entitled \u201cMulti-period Trading Prediction Markets with Connections to Machine Learning\u201d goes one step further by introducing market makers into the mix:\n\nFinally, David Balduzzi has a paper \u201cCortical Prediction Markets\u201d where he poses the argument that the spiking neural networks are driven by market forces:\n\nA recent DeepMind paper hidden in the ICLR 2017 haystack entitled \u201cMetacontrol for Adaptive Imagination-Based Optimization\u201d discusses a particular kind of RL algorithm that coordinates the behavior of multiple experts through a market driven approach:\n\nIn short, you have here systems that pursue solutions via the allocation of a scarce commodity (i.e. currency). Complex organizations require coordination and a distributed mechanism to perform that coordination is through a currency. That\u2019s the beauty of markets, coordination is distributed and that\u2019s a key idea to take away from this.\n\nThis indeed is an extremely promising principled approach (rather than the ad-hoc way we have today) to the problem of discovering an equilibrium in a the Collaborative Classification with Imperfect Knowledge (CCIK) level of DL intelligence. See: \u201cThe Five Capability Levels of Deep Learning Intelligence\u201d.\n\nJohn Holland in 2010 had a TEDx talk about \u201cBuilding Blocks and Innovation\u201d where he describes 2 big problems about Complex Adaptive Systems (CAS):\n\nThese two problems are features of market driven systems.\n\nhttps://arxiv.org/pdf/1612.05159v1.pdf Improving Scalability of Reinforcement Learning by Separation of Concerns"
    },
    {
        "url": "https://medium.com/intuitionmachine/what-the-skeptics-say-about-deep-learning-4e1571638763",
        "title": "What Skeptics Say About Deep Learning \u2013 Intuition Machine \u2013",
        "text": "Every hyped up technology needs a healthy dose of skeptics. Deep Learning is not an exception. To keep us all level set, let\u2019s see what a few of these skeptics are saying. Here are three recent articles from three different data science/machine learning experts.\n\nThis AI Boom Will Also Bust\n\nAll these criticism have come in the past month. They are all pretty tame as compared to this blog post in 2014 \u201cGet off the deep learning bandwagon and get some perspective\u201d:\n\nonly to come back later and write in his disclaimer:\n\nInteresting arguments all of them, but are any of them valid?\n\nTo understand Deep Learning better, please see my 2017 predictions for Deep Learning or sign up with Intuition Machine to get the conversation going."
    },
    {
        "url": "https://medium.com/intuitionmachine/nips-2016-cake-rocket-ai-gans-and-the-style-transfer-debate-708c46438053",
        "title": "NIPS 2016: Cake, Rocket AI, GANs and the Style Transfer Debate",
        "text": "Or, if you put them all together, my experience of the NIPS conference can be summarized as the image below. Read along if you prefer words, details and creative applications of AI. Thanks Prisma for the style transfer and my stepdad for the somewhat GANy cake Mentioned in Yann LeCun\u2019s invited talk on Monday, the cake became the meme of NIPS 2016, appearing in presentations throughout the conference and workshops. Turns out, machine learning researchers must really like cake, sometimes with extra cherries (oh RL!).\n\nThe real cherry on the NIPS cake was of course the launch of Rocket AI. I was there, still not sure what they do, but impressive team, private residence in the embassy district and an on-call police force for when the parties get too rowdy (the team need their beauty sleep!). Yup, next year I\u2019m betting on Temporally Recurrent Optimal Learning becoming the new GAN. And now more seriously\u2026 This was my first NIPS and it was probably the best conference I had ever been to. So much inspiration, energy and positivity from the AI community amongst the chaotic frenzy of invited talks from industry titans, hundreds of posters, artistic demos, company parties, workshops and satellite events for the almost 6,000 machine learning researchers assembled from all over the world. Read on for my NIPS week highlights. Organised by Xavier Serra at Universitat Pompeu Fabra on the Sunday before NIPS, the seminar was a great music-focussed start to the week. I only managed to catch the final few presentations, but it was a delight to hear what was happening industryside with presentations from Aaron van den Oord on the Wavenet audio generation model, Oriel Nieto on music recommendation models at Pandora and Colin Raffel on using the Lakh MIDI dataset. Lines, grids and reflections at the CCIB conference venue In its 11th year, the WIML workshop welcomed 570 participants to its event on Monday, featuring invited speakers, contributed talks and mentorship roundtables on academic and career topics. I was invited to mentor on music applications and building your professional brand; we had some fascinating discussions on separating professional and personal lives online as well as figuring out what our dream applications of AI in music would be (music tutors, music-to-image and improved recommendation systems were all mentioned). Despite the fact that WIML numbers doubled from the previous year, women still made up only 15% of the almost 6,000 attendees. Judging by the number of women-specific sponsor events hosted on Sunday before the workshop, it is clear that companies are trying to address the gender balance. I do wonder if they\u2019re going about it in the right way, but that is a topic for another article. Real life sea demo found just outside the conference centre Monday, Tuesday and Wednesday evenings are reserved for posters with 200 on display every day and only 3.5 hours to see them all. Alongside the posters, there were 10 demos each on Tuesday and Wednesday allowing you to play music, figure out your personality, generate text interactively and turn frowns into smiles. They brought the research to life and were great fun to play around with. Some favourites are listed below. Memo Akten\u2019s Real-time interactive sequence generation and control with Recurrent Neural Network ensembles: a system to gesturally \u2018conduct\u2019 the generation of text. What was particularly entertaining here is watching the generated text change as you moved your hand across the screen to increase the presence of the style of the Bible, Trump or the Linux source code. Tom White\u2019s Neural Puppet: How can we understand and use the structured latent space of generative networks? This uses a neural network to remove or add smiles to a photograph.\n\nMagenta, the music generation project from Google Brain, Interactive Musical Improvisation with Magenta. The short video is of Magenta playing the bassline with Sageev Oore playing over it. Winner of the best demo award. Anh Nguyen et al\u2019s Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space. Like a never-ending film , generating image after image in response to a word prompt. Generative Adversarial Networks (GANs) were one of the hottest trends this year, with a dedicated tutorial from Ian Goodfellow on Monday, a workshop on Adversarial Training on Friday and pop-up appearances pretty much anywhere and everywhere else. I\u2019m still in awe of the artistic potential of these models. Below are some slides from Ian Goodfellow\u2019s tutorial, with some results described as \u201cproblems\u201d because of incorrect perspective, global structure, and counting. What is a bug in the quest for realistic image generation can be a feature in creative explorations. With the amount of interest in the field, I can\u2019t wait to see how the technical models will develop and how they will be used creatively. Get in touch if you spot some cool creative uses of GANs.\n\nDuring Friday lunch, I was pleasantly surprised to find a workshop on public engagement. Titled \u2018 People and machines: Public views on machine learning, and what this means for machine learning researchers\u2019, it featured presentations from Sabine Hauert, Zoubin Ghahramani and Katherine Gorman. Sabine Hauert gave an overview of the public opinion insights gained from the survey conducted by The Royal Society earlier this year. Turns out that only 9% of people know the term ML, but 76% know NLP and 75% driverless cars as possible applications. Meanwhile, Zoubin Ghahramani presented an overview of the main research and public opinion challenges facing the machine learning community.\n\nRoss Goodwin was one of the invited speakers and I can\u2019t help but include this definition of love generated by his lexiconjure bot which was trained on the OED. Ross is also the writer of writer (best job title ever?) for the science fiction short film Sunspring, which was screened at the workshop. Style transfer is only cool to computer scientists?! Ever since the Leon Gatys et al paper on A Neural Algorithm of Artistic Style in August 2015, the so-called neural style transfer has been growing in popularity and exploded this summer with the launch of the mobile app Prisma. Here is Magenta\u2019s version of live video style transfer with the option of combining multiple styles and tweaking their individual strength. Pretty cool, right? Turns out, only if you\u2019re a computer scientist (or journalist!). Simon Colton from Goldsmiths delivered a controversial talk on computational creativity with heavy criticism of the current experiments in style transfer carried out by the computer science community due to their excessive focus on the so-called \u201cpastiche generation\u201d. He also gave some additional uses of style transfer \u2014 more acceptable to the art world in his view - including style exploration and style invention. The discussion continued on twitter. With that, I sign off. What was your experience of NIPS? What are the most creative and unusual applications, papers and projects that you saw? Anything major I missed? Tweet me @elluba or comment below."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-explained-to-a-five-year-old-25919b0bf889",
        "title": "Explaining Deep Learning to a Five Year Old \u2013 Intuition Machine \u2013",
        "text": "Richard Feynman had a method of learning complex subjects. The method is simple, try to explain the complex subject to a five year old. If you can\u2019t do it, go back, refine you language and try again.\n\nDeep Learning is one of those complex subjects that continues to perplex. Here I am going to attempt an explanation that hopefully could be understood by a five year old.\n\nJust read the following:\n\nTehse mahcnies wrok by s33nig f22Uy pa773rns and cnonc3t1ng t3Hm t0 fU22y cnoc3tps. T3hy wRok l4y3r by ly43r, j5ut lK1e A f1l73r, t4k1NG cmopl3x sc3n3s aNd br3k41ng tH3m dwon itno s1pmLe iD34s.\n\nI hope this explanation was simple and intuitive enough for you to understand."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-machines-are-holographic-memories-258272422995",
        "title": "Deep Learning Machines are Holographic Memories \u2013 Intuition Machine \u2013",
        "text": "My favorite the paper in the 500 plus papers submitted to ICLR 2017 is this one done by a group at Google: \u201cUnderstanding Deep Learning required Rethinking Generalization\u201c. I\u2019ve thought about Generalization a lot, and I\u2019ve posted out some queries in Quora about Generalization and also about Randomness in the hope that someone could give some good insight. Unfortunately, nobody had enough of an answer or understood the significance of the question until the folks who wrote the above paper performed some interesting experiments. Here is a snippet of what they had found:\n\nDeep Learning networks are just massive associative memory stores! Deep Learning networks are capable of good generalization even when fitting random data. This is indeed strange in that many arguments for the validity of Deep Learning is on the conjecture that \u2018natural\u2019 data tends to exists in a very narrow manifold in multi-dimensional space. Random data however does not have that sort of tendency.\n\nJohn Hopcroft on a paper the examines the effects of random weights:\n\nTo understand Deep Learning, we must embrace randomness. Randomness arises from maximum entropy, which interestingly enough is not without its own structure! The memory capacity of a neural network seems to be highest the closer to random the weights are. The strangeness here is that Randomness is ubiquitous in the universe. The arrow of time is reflected by the direction towards greater entropy. How then is it that this property is also the basis of learning machines?\n\nIf we were to assume that the reasoning (or the intuition) behind hierarchical layers in DL is that the bottom layers consist of the primitive recognition components that are built up, layer by layer, into more complex recognition components.\n\nWhat this implies then is that the bottom components during training should be \u2018searched\u2019 more thoroughly than the top most components. But the way SGD works is that the search is driven from the top and not from the bottom. So the top is searched more thoroughly that the bottom layers.\n\nWhich tells you the bottom layers (the ones closest to inputs) are not optimal in their representation. In fact, they are the kind of a representation that likely will be of the most generalized form. The kind that will have recognizers that will have equal probability of matching anything, in short, completely random!\n\nAs you move up the layers, the specialization happens because it is actually driven from the top which is designed to fit the data. Fine tuning happens at the top.\n\nLet\u2019s make the analogy of this process with languages. The bottom components of a language are letters and the top parts are sentences. In between you have syllables, words, parts of speech etc. However from a Deep Learning perspective, it is as if there are no letters! But rather fuzzy forms of letters. Which builds up into other fuzzy forms of words and so forth. The final layers is like some projection (some wave collapse) into interpretation.\n\nThis notion of fuzzy letters and fuzzy words can intuitively be explained to you by having you read the following:\n\nOur brains work similarly to Deep Learning in that we work with primarily fuzzy concepts.\n\nThere is a recent paper that discusses a method Swapout that is a generalized form of Dropout that works across many layers:\n\nThe Swapout learning procedure which tells us that if you sample any subnetwork of the entire network the resulting prediction will be the similar to any other subnetwork you look sample. Just like holographic memory where you can slice of pieces and still recreate the whole. The procedure seems to be that the more random we try to make it to be, the better our learning. That is definitely counter-intuitive!\n\nExplore more in this new book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/revue-issue-1-69a64d08418f",
        "title": "Revue issue #1 \u2013 Intuition Machine \u2013",
        "text": "Artificial Intelligence Just Broke Steve Jobs\u2019 Wall of Secrecy | WIRED \u2014 www.wired.com \n\n Secrecy doesn\u2019t play in AI research. And as it happens, AI is more important to the future of tech giants like Apple than any other.\n\n10 Hot Consumer Trends 2017 \u2014 Ericsson ConsumerLab \u2014 www.ericsson.com \n\n Consumer trends come to light due to digital emmersion. The Ericsson ConsumerLab research gives you an insight in 10 hot consumer trends for 2017.\n\nNIPS 2016 Review, Days 0 & 1 \u2014 Gab41 \u2014 gab41.lab41.org \n\n Good morning, fellow machine learners. A few of us from Lab41 recently jumped the pond over to Barcelona, Spain, to see what machine learning and artificial intelligence stuffs we could glean from\u2026\n\n10 Exponential Growth Trends in Deep Learning \u2014 medium.com \n\n Some people just don\u2019t grok it, no matter how hard you bang the table.Continue reading on Intuition Machine \u00bb\n\n9 Misconceptions About Deep Learning \u2014 medium.com \n\n We hear and read in the popular media about Artificial Intelligence (AI) all the time. We have movies about them. We hear about Elon Musk\u2026Continue reading on Intuition Machine \u00bb\n\nElon Musk and DeepMind\u2019s AI training ground are being released to researchers | WIRED UK \u2014 www.wired.co.uk \n\n The code and levels from DeepMind Lab are being made available on GitHub\n\nDo machines actually beat doctors? \u2014 Dr Luke Oakden-Rayner \u2014 lukeoakdenrayner.wordpress.com \n\n Spoiler: You know what they say about headlines that end with a question mark, right? If you ask academic machine learning experts about the things that annoy them, high up the list is going to be overblown headlines about how machines are beating humans at some task where that is completely untrue. This is partially because\u2026\n\nTop 10 AI failures of 2016 \u2014 TechRepublic \u2014 www.techrepublic.com \n\n Recent developments in driverless cars, voice recognition, and deep learning show how much machines can do. But, AI also failed us in 2016, and here are some of the biggest examples.\n\nThe Only Way to make Deep Learning Interpretable is to Have it Explain Itself \u2014 medium.com \n\n One of the great biases that Machine Learning practitioners and Statisticians have is that our models and explanations of the world should\u2026Continue reading on Intuition Machine \u00bb\n\nCrafting Deep Learning Objective Functions now Obsolete \u2014 medium.com \n\n Classical Machine Learning (ML) is based on setting a system with an objective function and finding a minimal (or maximal, depending on\u2026Continue reading on Intuition Machine \u00bb\n\nDeep Learning is Non-Equilibrium Information Dynamics \u2014 medium.com \n\n There are basically several camps studying neural like systems. There are the folks who insist on a biologically inspired approach. These\u2026Continue reading on Intuition Machine \u00bb\n\nGitHub \u2014 ajarai/fast-weights: Implementation of Using Fast Weights to Attend to the Recent Past \u2014 github.com \n\n fast-weights \u2014 Implementation of Using Fast Weights to Attend to the Recent Past\n\nGitHub \u2014 deepmind/learning-to-learn: Learning to Learn in TensorFlow \u2014 github.com \n\n learning-to-learn \u2014 Learning to Learn in TensorFlow\n\nGitHub \u2014 jimfleming/recurrent-entity-networks: An implementation of \u201cTracking the World State with Recurrent Entity Networks\u201d. \u2014 github.com \n\n recurrent-entity-networks \u2014 An implementation of \u201cTracking the World State with Recurrent Entity Networks\u201d."
    },
    {
        "url": "https://medium.com/intuitionmachine/design-patterns-for-self-driving-automation-81c2eebbd6ad",
        "title": "Design Patterns for Self Driving Automation \u2013 Intuition Machine \u2013",
        "text": "Self Driving Cars are all the rage these days. When Udacity announced a \u201cself driving car engineering degree\u201d a few months ago, they were swamped with 11,000 hopefuls:\n\nI just happened to also be interested in this course. However, not necessarily from a self-driving car perspective, but rather from a more general perspective of self-driving \u201canything\u201d. So as my first steps at learning this vast emerging field, I am curating a set of design patterns in the field of what I call \u201cself driving automation\u201d. See: \u201cDesign Patterns for Self Driving Automation\u201d.\n\nI am hoping that this becomes a collaborative endeavor. The development of Design Patterns has historically been intertwined with the employment of a Wiki. In fact, one of the earliest Wikis, which likely pre-dates Wikipedia, was invented for the sole purpose of documenting Design Patterns.\n\nThe idea is that Design Pattern development is always a collaborative endeavor.\n\nSelf-Driving Automation is an entirely new field. It is usually described as Self-Driving Cars. Knowledge in this space is in its infancy and what better opportunity to start building a Design Pattern repository than in an emerging field. It is not only emerging, but also a complex field that involves the integration of a lot of different technologies and the real-time orchestration of these integrations. I hope in the next several months to be able to capture the knowledge into a form that is digestable by future practitioners.\n\nThe motivation as to why I use the word \u201cAutomation\u201d rather than \u201cCars\u201d is that I am seeking a more general application of this technology. A very glimpse of this idea of an automation that employs Deep Learning, Vision, Sensor Fusion and a whole lots of other technologies can be found in Amazon Go. Amazon Go isn\u2019t a car, it is a self-service retail store!\n\nThe concepts found in self driving cars, I believe is also transferrable to many other fields that have complex sensory environments and require realtime decision making. Design Patterns is the plan to disseminate knowledge in this exciting an emerging field of expertise."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-is-non-equilibrium-information-dynamics-b00baa16b135",
        "title": "Deep Learning is Non-Equilibrium Information Dynamics",
        "text": "There are basically several camps studying neural like systems. There are the folks who insist on a biologically inspired approach. These include firms like Numenta, Vicarious and researchers in the Connectome field. The other camp consists of people of the Bayesian religion. People who believe that some theorem, that was invented in the 18th century, would be the key to unlock our understanding of intelligence. There are also the alchemists who don\u2019t really care about theory and are more than happy to conjure out the latest Residual or Attention model. If the results show \u201cstate-of-the-art\u201d then that concoction must be the right approach.\n\nThe present reality of Deep Learning research is that the alchemists are winning and it\u2019s not even a close contest!\n\nWhy is this so? Why are we at such a poor state of comprehension of Deep Learning? Could it be that the biological theorists or the Bayesian zealots are using the wrong toolbox?\n\nOne major shortcoming of our present day mathematical toolbox is that it is relevant only in conditions that are in equilibrium.\n\nUnfortunately, the conditions for learning do not happen in an equilibrium state. Rather they happen at a state of non-equilibrium. It is like trying to take measurements after the fact rather than when it is happening. To measure only when a system is in equilibrium (or assume the central limit theorem) is to make observations only after the entire play is over. To understand Deep Learning, one needs to have a grasp as what happens in non-equilibrium at the transition between order and chaos.\n\nDeep Learning are not biological systems nor are they physical systems. Many researchers derive their intuition from either contexts. However if you have grounded yourself in Newton\u2019s classical mechanics, then the likelihood of you ever discovering Quantum mechanics is next to nil. Unless, you take a close look at the experimental data and realize that your world view is actually flawed. Deep Learning are information systems, not biological and not physical and therefore should be studied as such. That\u2019s why the understanding the dynamics of information is of high importance.\n\nInformation systems (alternatively computational systems) consist of 3 fundamental capabilities. These are:\n\nIt is that simple. The Cellular Automata Rule 110 that I describe in a previous post has all 3 of these capabilities. Universal Machines emerges from these 3 operators.\n\nNow you may be asking yourself that it can\u2019t be this simple! The notion that a complex system requires complex constituents is an entirely false assumption. The key to understanding the capabilities of complex systems in in the 3 operators. In fact, in my previous post about \u201c5 Capability Level of Deep Learning Intelligence\u201d, the levels are just different combinations of these 3 operators and of different levels of sophistication.\n\nDeep Learning systems are of course much more capable that being able to perform universal computation. They are capable of not only learning, but also meta-learning. The two core computational (information modification), capabilities are matching and selection. Deep Learning systems consists of ensembles of self-similar matching and selection units. They consist of multiple layers of this and are routed via signaling (information transfer). To make an analogy with another AI technique, its just like a swarm of simple matching and selection machines.\n\nThe key question however is how do these systems learn? This is a complex research subject, but we certainly know one thing, these systems aren\u2019t learning when they are in equilibrium. In fact if we study biological systems, we know that in the non-equilibrium state that the evolution of a system tends towards minimizing relative entropy. That is, the same optimization direction of minimizing the KL divergence (i.e. a measure of difference between two distributions). Furthermore, we know that phase transitions near high mutual information in models. This implies that all too convenient assumption of i.i.d. needs to be thrown in the dustbin. The study of DL must be in the regime of non-equilibrium states and not in the mathematically convenient regime of equilibrium.\n\nOne final thought, you may be also wondering if physics can be captured in a information dynamics (aka computational mechanics) framework. There actually have been several papers that cover that area, specifically in information theoretic terms. This is possibly where that entire notion of reality being in a simulation comes about. One of those topics that I, like Elon Musk, would also like to avoid!\n\nBTW, the image above is an image of the surface of a liquid in a non-equilibrium state. What does it remind of us that we find in biology?\n\nhttps://arxiv.org/pdf/1503.03585.pdf Deep Unsupervised Learning using Nonequilibrium Thermodynamics \u2014 Provides a sample technique inspired by non-equilibrium statistical mechanics.\n\nhttps://arxiv.org/abs/1610.08192 Transfer entropy in continuous time, with applications to jump and neural spiking processes\n\nIt occurs to me that many readers, with an interest in AI, don\u2019t seem don\u2019t seem to understand how mathematics is used to model reality. Math doesn\u2019t model the world, you fit math so that it looks like the world. It is the same idea as curve fitting, you hypothesize that a certain formula fits with the world and if it does then you are lucky.\n\nSo as a matter of convenience though, the math formulas that are easy to work with are the ones that are used. Furthermore, because of the limitation of mathematics, simplified systems are used for analysis. The universe doesn\u2019t have a requirement that a closed form equation exists to model its behavior.\n\nThermodynamics equations are based on empirical observations in that unlike other branches of physics, are not derived from first principles. They are about systems in equilibrium and the variables are aggregate measures of a system. Statistical mechanics is a branch of physics that has techniques to study behavior of large collections of interacting particles. If you think it uses statistics because of its name then that\u2019s also a misconception. Under Statistical Mechanics there is Non-Equilbrium Statistical Mechanics which studies systems outside of equilibrium. This is the regime where Nobel prize winner Prigogine did his work. When you get into this \u2018regime\u2019 then that\u2019s where you biological processes and physics meet.\n\nDL systems however are not biological systems and DL systems are also not physical systems. So the closest thing that can model its behavior and have the properties similar to biology is Information Dynamics in the state of Non-Equilibrium.\n\nExplore more in this new book:"
    },
    {
        "url": "https://medium.com/intuitionmachine/8-exponential-hockey-stick-charts-for-deep-learning-74bba7a0284c",
        "title": "Deep Learning Exponential Growth Trends \u2013 Intuition Machine \u2013",
        "text": "Some people just don\u2019t grok it, no matter how hard you bang the table.\n\nBut maybe some hockey stick graphs will help.\n\nIt all begins with this chart:\n\nIn 2010 image recognition would fail at least on fourth of the time, but by 2015, image recognition surpassed human recognition! What is driving this? Something definitely is brewing.\n\nSomething called \u201cDeep Learning\u201d began emerging in 2012. So let\u2019s check Google search trends that track interest:\n\nIt turns out that the academic community has something really brewing.\n\nhere is another conference but with some commentary:\n\nBut is this all theory? Let\u2019s take a peak inside Google:\n\nWhat\u2019s the demand for talent like?\n\nWhat about what the 1%ers are investing in?\n\nWhat are the companies acquiring?\n\nIs Deep Learning the main driver of Artificial Intelligence?\n\nNow for the most important trend:\n\nNow, the fact that you are reading this tells me that you\u2019ve heard of \u201cDeep Learning\u201d. It is likely 99.9% of the population have no clue as to what it is. This gives you a head start and I do recommend that you take action on this knowledge advantage.\n\nSign up at Intuition Machine to get the conversation going or discuss at FaceBook or LinkedIn."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-have-we-lost-control-of-the-objective-function-fda51b075350",
        "title": "Crafting Deep Learning Objective Functions now Obsolete",
        "text": "Classical Machine Learning (ML) is based on setting a system with an objective function and finding a minimal (or maximal, depending on which direction you are lookin) solution to this objective function. A set of data is not included in the search (or optimization ), with the purpose of verifying generalization or avoidance of overfitting. Ideally, we would like this objective function to be in analytic form (meaning you can write down the math!).\n\nWhat if the objective function was implicit, in other words, you did not know how to calculate the gradient? You can still get away with this using a finite difference equivalent or some other surrogate approximation. However, at least we still know in which direction the optimization is supposed to proceed (i.e. minimization or maximization).\n\nWhat if however, that we did not even know the direction of the optimization?\n\nIf you read my earlier piece on \u201cGame Theory and Deep Learning\u201d, you\u2019ll get a sense of where Deep Learning is evolving towards. In this new world of multiple agent neural networks and multiple objectives (also see: \u201cThe 5 Capability Levels of Deep Learning Intelligence\u201d) you find yourself in the twilight zone where the machine itself is tasked with not only finding an equilibrium state but even learning an objective function!\n\nHere are two recent research papers with some intriguing ideas. The first one is \u201cImproved Techniques for Training GANs\u201d the authors (Goodfellow et al.) discuss various heuristics. The write:\n\nThe first quote implies the need to understand the appropriate direction of an optimization to achieve equilibrium. The second alludes to the missing objective function.\n\nAnother paper, \u201cImage-to-Image Translation with Conditional Adversarial Networks\u201d, interprets the implicit objective function in terms of \u201cLearning a Objective Function\u201d:\n\nA picture is worth a thousand words, so here are their surprising results. This is not for many neural networks, it is just one network that seems to be able to selectively determine its own objective function:\n\nWelcome to the twilight zone."
    },
    {
        "url": "https://medium.com/intuitionmachine/deconfusing-ai-and-deep-learning-20473d7578c0",
        "title": "9 Misconceptions About Deep Learning \u2013 Intuition Machine \u2013",
        "text": "We hear and read in the popular media about Artificial Intelligence (AI) all the time. We have movies about them. We hear about Elon Musk and Stephen Hawking warning us about AI\u2019s apocalyptic consequences. We hear from the World Economics Forum about AI\u2019s effect on taking away our jobs. We hear about how disruptive AI will be for businesses. However, when we listen to experts speak about this, they bring about an entirely different phrase: \u201cDeep Learning\u201d.\n\nDeep Learning, two simple words that we all can understand, but yet in the context of Artificial Intelligence, when these words are combined they become inscrutable to the uninformed. In fact, even for the informed it is inscrutable. That\u2019s because decades worth of statistical training have become a liability in understanding what it means.\n\nHere are 10 points that you need to understand Deep Learning. It is simple, you just need to understand what it is not, and understand what it is.\n\nExpert systems, semantic web and deductive logic systems are examples of systems that are based on symbolic logic. These systems are typically associated with AI. They all do work, however they have one shortcoming: They are unable to effectively learn from the data.\n\n2. Deep Learning is Radically Different from Machine Learning\n\nMachine Learning in its most basic distillation is \u201ccurve fitting\u201d. That is, if you have an algorithm that is able to find the best fit of your mathematical model with observed data, then that\u2019s Machine Learning. DL at its earlier incarnation was about \u201ccurve fitting\u201d, however it has progressed beyond that in recent years. Deep Learning Meta-Learning should be a big indicator to anyone that this is indeed very different.\n\nThe architecture of DL have are nowhere close to a biological neuron in structure. Even in behavior they are different. Biological neurons work on spiking behavior, DL system work in a continuous dynamical system. Some DL systems use Artificial Neural Networks, but that is just historic terminology that exists to this day. Anyone explaining DL in terms of biological neurons really doesn\u2019t know what they are talking about. DL isn\u2019t designed to \u2018mimic\u2019 biology, DL just happens to be a computational architecture that learns surprisingly well.\n\nDL can do some fantastic things like cross translate between different human languages and read out captions from images. However, the intelligence is really specialized and narrow. Sure DL can drive cars, but that\u2019s nowhere near the capability of AGI.\n\nThere was a Wired article titled \u201cDeep Learning isn\u2019t a Dangerous Genie, it is Just Math\u201d. This is really the most vacuous statement I\u2019ve heard! It is like saying that computers are just boolean circuits or brains are just made up of neurons or DL are made up of layers that are described using mathematical functions. It doesn\u2019t explain the emergent complex behavior you find in computers, brains and DL systems.\n\nClassical statistics is about analyzing data using aggregate measures. DL systems however work in a domain that statistical methods do not apply. That is high-dimensional data with high mutual information among the variables. Simplifying i.i.d. (i.e. Independent and identically distributed) assumptions are simply not applicable.\n\nBig Data is a technology that is based on the idea that if you are able to store and compute through a massive amount of data, typically hosted in hundreds or thousands of off-the-shelf computers, then you can gain insight. DL is an algorithm that can sit on a single machine and can incrementally, special emphasis on incrementally, process your data to learn from it. Big Data can crunch massive amounts of data, but just because you can process a lot of data doesn\u2019t mean you can derive insight or learn from the data. One last point, unlike Big Data, DL doesn\u2019t need a lot of data to be useful.\n\n8. Deep Learning is not understood by Data Scientists\n\nData Scientists are trained to do modeling of data, feature engineering and data analysis. DL just does what a Data Scientist does but without a human in the loop. This is actually a bit of an exaggeration. The reality is that most Data Scientists trained in other methods have not come up to speed with DL techniques.\n\n9. Deep Learning is not just Artificial Neural Networks or Multi-Level Perceptrons\n\nANN or MLPs were developed way back in the 1950s. DL systems originate from this earlier work, however in recent years they\u2019ve evolved to new kinds of models like Convolution networks, Long Short Term Memory, Residual Networks etc. The field has a much richer collection of concepts than existed when you studied it in graduate school.\n\n10. Deep Learning is the reason for the current AI hype\n\nFinally, this is where the greatest confusion exists. On a daily basis, the press continues to report the amazing progress of AI. Furthermore, you hear about firms like Google and Microsoft changing their entire software DNA to move into AI. The reason for this massive migration is because of Deep Learning. The big problem for the majority of the readers is that, the phrase itself \u201cDeep Learning\u201d is just too difficult to comprehend.\n\nI hope this gives you a frame of reference, a lay of the land, a sketch of where exactly Deep Learning does not fit. You might still be perplexed about reading this considering I haven\u2019t defined Deep Learning it all! My apologies, but unfortunately it\u2019s a complex subject, however more detail can be found at Design Patterns for Deep Learning or start a conversation at FaceBook or LinkedIn.\n\nHowever, if you are pressed for time and need one sentence to describe Deep Learning, it is just \u201cNon-Equilibrium Information Dynamics\u201d"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-only-way-to-make-deep-learning-interpretable-is-to-have-it-explain-itself-1e874a73108f",
        "title": "The Only Way to make Deep Learning Interpretable is to Have it Explain Itself",
        "text": "One of the great biases that Machine Learning practitioners and Statisticians have is that our models and explanations of the world should be parsimonious. We\u2019ve all bought into Occam\u2019s Razor:\n\nHowever, does that mean that our machine learning model\u2019s need to be sparse? Does that mean that true understanding can only come from closed form analytic solutions? Do our theories have to be elegant and simple?\n\nYann LeCun in a recent FaceBook post commenting about a thesis on \u201cDeep Learning and Uncertainty\u201d points out to a 1987 paper by his colleagues at Bell Labs titled \u201cLarge Automatic Learning, Rule Extraction, and Generalization\u201d. This paper emphasizes the problem:\n\nOne of the probable reasons why Deep Learning requires an inordinate amount of iterations and training data is because we seek Occan\u2019s Razor, that sparse solution. What if however, the solution to unsupervised learning (aka Predictive Learning) is in embracing randomness?\n\nLet\u2019s table the proof of this for a later time, and assume its validity for argument\u2019s sake. That is, randomness is the natural equilibrium state (is it not obvious?). What this implies is that the model parameters will be completely random and interpretability will be completely hopeless. Unless of course, we can ask the machine to explain itself!\n\nI was about to end this post with the last paragraph, but I thought that some examples may help explore this idea much more thoroughly.\n\nStephen Merity (MetaMind) has a detailed examination of Google\u2019 Neural Machine Translator (GNMT) that is worth a read. The interesting thing about GNMT is that Google headlines this as \u201cZero-Shot Translation\u201d:\n\nThis zero-shot capability here refers to the capability of this machine to learn for example a Japanese to English translation even if it was never trained with this particular translation pair! To quote them:\n\nWill we perhaps be able to decipher this new \u201cinterlingua\u201d or \u201cesperanto\u201d that this machine created? Do we have a priori ideas as how this interligua is supposed to look like and perhaps performing a kind of regularization to make it more interpretable for humans? Will the act of insisting on interpretability lead to a less capable translator? Are Vulcan Mind-Melds necessary?\n\nIt just seems that we should leave the representation as it is and use the machine to perform the translation into English. In fact, that is already what it currently does. We don\u2019t need some new kind of method to interpret the representation. The capability is already baked in there.\n\nThis is in fact what the folks at MIT, who have researched about \u201cMaking computers explain themselves\u201d, have done:\n\nThey\u2019ve trained their network to learn how to explain itself.\n\nUpdate: Here are some slides from DARPA project XAI exploring explainability.\n\nIf you were able to grok this article, then feel free to join the conversation at this LinkedIn group: https://www.linkedin.com/groups/8584076"
    },
    {
        "url": "https://medium.com/intuitionmachine/10-deep-learning-trends-and-predictions-for-2017-f28ca0666669",
        "title": "10 Deep Learning Trends and Predictions for 2017 \u2013 Intuition Machine \u2013",
        "text": "I used to write predictions for the upcoming year in my previous blog. The last one I recall writing was \u201cSoftware Development Trends and Predictions for 2011\u201d. That\u2019s quite a long time ago. Just to recap, out of 10 predictions, I gather that I got 6 accurately (i.e. Javascript VM, NoSQL, Big Data Analytics, Private Clouds, Inversion of Desktop Services, Scala), however the remaining 4 have not gained enough traction (i.e. Enterprise AppStores, Semantic Indexing, OAuth in the Enterprise, Proactive Agents). Actually, AppStores and OAuth doesn\u2019t happen in big enterprises, however, small companies have adopted this SaaS model in full force. I\u2019ll chalk the prediction failure to not being able to predict how slow enterprises actually change! The remain two predictions, that of Semantic Indexing and Proactive Agents, have unfortunately not progressed as I had originally projected. I may have overly estimated the AI technology at that time. Deep Learning had not been invented back then.\n\nMy Deep Learning predictions will not be at the same conceptual level as my previous predictions. I\u2019m not going to predict enterprise adoption but I rather am going to focus on research trends and predictions. Without a doubt, Deep Learning will drive AI adoption into the enterprise. For those still living underneath a rock, it is a fact that Deep Learning is the primary driver and the most important approach to AI. However, what is not so obvious is what kind of new capabilities will arise in 2017 that will lead to exponential adoption.\n\nSo here come my fearless predictions for 2017.\n\nThis, of course, is entirely obvious if you track developments at Nvidia and Intel. Nvidia will dominate the space throughout the entire 2017 simply because they have the richest Deep Learning ecosystem. Nobody in their right mind will jump to another platform until there is enough of an ecosystem developed for DL. Intel Xeon Phi solutions are dead on arrival with respect to DL. At best they may catch up in performance with Nvidia by mid-2017 when the Nervana derived chips come to market.\n\nIntel\u2019s FPGA solutions may see adoption by cloud providers simply because of economics. Power consumption is the number one variable that needs to be reduced. Intel\u2019s Nervana based chip will likely clock in at 30 teraflops by mid-2017. That\u2019s my guesstimate, but given that Nvidia is already at 20 teraflops today, I wouldn\u2019t bet on Intel having a major impact until 2018. The only big ace that Intel may have is in 3D XPoint technology. This will help improve the entire hardware stack but not necessarily the core accelerator capabilities considering that GPUs use HBM2 that\u2019s stacked on top of the chip for performance reasons.\n\nAmazon has announced its FPGA based cloud instance. This is based on Xilinx UltraScale+ technology and are offering 6,800 DSP slices and 64 GB of memory on a single instance. That\u2019s impressive capability however, the offering may be I/O bound by not offering the HBM version of UltraScale+. The lower memory bandwidth solution as compared with Nvidia, Intel, and even AMD may give developers pause as to whether to invest in a more complicated development process (i.e. VHDL, Verilog etc).\n\nIn late breaking news, AMD has revealed its new AMD Instinct line of Deep Learning accelerators. The specifications of these are extremely competitive versus Nvidia hardware. This offering is scheduled to be available early 2017. This is probably should be enough time for AMDs ROCm software to mature.\n\nCNNs will be the prevalent bread-and-butter model for DL systems. RNNs and LSTMs with its recurrent configuration and embedded memory nodes are going to be used less simply because they would not be competitive to a CNN based solution. Just like GOTO disappeared in the world of programming, I expect the same for RNNs/LSTMs. Actually, parallel architectures trump sequential architectures in performance.\n\nDifferentiable Memory networks will be more Common. This is just a natural consequence of architecture where memory will be refactored out of the core nodes and just reside as a separate component from the computational components. I don\u2019t see the need for forget, input and output gates for LSTM that can be replaced by auxiliary differentiable memory. We already see conversation about refactoring the LSTM to decouple memory (see Augmented Memory RNN).\n\n3. Designers will rely more on Meta-Learning\n\nWhen I began my Deep Learning journey, I had thought that optimization algorithms, particularly ones that were second-order would lead to massive improvements. Today, the writing is on the wall, DL can now learn the optimization algorithm for you. It is the end of the line for anybody contemplating a better version of SGD. The better version of SGD is the one that is learned by a machine and is the one that is specific to the problem at hand. Meta-learning is able to adaptively optimize its learning based on its domain. Further related to this is whether alternative algorithms to backpropagation will begin to emerge in practice. There is a real possibility that hand tweaked SGD algorithm may be in its last legs in 2017.\n\n4. Reinforcement Learning will only become more creative\n\nObservations about reality will always remain imperfect. There are plenty of problems where SGD is not applicable. This just makes it essential that any practical deployment of DL systems will require some form of RL. In addition to this, we will see RL used in many places in DL training. Meta-Learning, for example, is greatly enabled by RL. In fact, we\u2019ve seen RL used to find different kinds of neural network architectures. This is like Hyper-parameter optimization on steroids. If you happen to be in the Gaussian Process business then your lunch has just been eaten.\n\n5. Adversarial and Cooperative Learning will be King\n\nIn the old days, we had monolithic DL systems with single analytic objective functions. In the new world, I expect to see systems with two or more networks cooperation or competing to arrive at an optimal solution that likely will not be in analytic form. See \u201cGame Theory reveals the future of Deep Learning\u201d. There will be a lot of research in 2017 in trying to manage non-equilibrium contexts. We already see this now where researchers are trying to find ways to handle the non-equilibrium situation with GANs.\n\n6. Predictive Learning or Unsupervised Learning will not progress much\n\n\u201cPredictive Learning\u201d is the new buzzword that Yann LeCun in pitching in replacement to the more common term \u201cUnsupervised Learning\u201d. It is unclear whether this new terminology will gain adoption. The question though of whether Unsupervised or Predictive Learning will make great strides in 2017. My current sense is that it simply will not because there seems to be a massive conceptual disconnect as to how exactly it should could work.\n\nIf you read my previous post about \u201c5 Capabilities of Deep Learning Intelligence\u201d, you get the feeling that Predictive Learning is some completely unknown capability that needs to be shoehorned into the model that I propose. Predictive Learning is like the cosmologists Dark Matter. We know it is there, but we just don\u2019t know how to see it. My hunch is that it has something to do with high entropy or otherwise randomness.\n\nAndrew Ng thinks this is important, I think so too!\n\n8. More Applications will use Deep Learning as a component\n\nWe saw this already in 2016 where we see Deep Learning used as a function evaluation component in a much larger search algorithm. AlphaGo employed Deep Learning in its value and policy evaluations. Google\u2019s Gmail auto-reply system used DL in combination with beam searching. I expect to see a lot more of these hybrid algorithms rather than new end-to-end trained DL systems. End-to-end Deep Learning is a fascinating area of research, but for now hybrid systems are going to be more effective in application domains.\n\nDeep Learning is just one of those complex fields that need a conceptual structure. Despite all the advanced mathematics involved, there\u2019s a lot of hand waving and fuzzy concepts that can best be captured not by formal rigor but rather with a method that has been proven to be effective in other complex domains like software development. I predict practitioners will finally \u201cget it\u201d with regards to Deep Learning and Design Patterns. This will be further motivated by the fact that Deep Learning architectures are becoming more modular rather than monolithic.\n\nThe background of researchers and the mathematical tools that they employ are a breeding ground for a kind of bias in their research approach. Deep Learning systems and Unsupervised Learning systems are likely these new kinds of things that we have never encountered before. Therefore, there is no evidence that our traditional analytic tools are going to be any help in unraveling the mystery as to how DL actually works. There are plenty of dynamical systems in physics that have remain perplexed about for decades, I see the same situation with regard to dynamical learning systems.\n\nThis situation, however, will not prevent the engineering of even more advanced applications despite our lack of understanding of the fundamentals. Deep Learning is almost like biotechnology or genetic engineering. We have created simulated learning machines, we don\u2019t know precisely how they work, however that\u2019s not preventing anyone from innovating.\n\nI\u2019ll come back to these predictions in a year from now. Wish me luck!"
    },
    {
        "url": "https://medium.com/intuitionmachine/predictive-learning-is-the-key-to-deep-learning-acceleration-93e063195fd0",
        "title": "\u201cPredictive Learning\u201d is the New Buzzword in Deep Learning",
        "text": "Yann LeCun in his many talks this year has repeatedly hammered away at this analogy:\n\nLeCun at NIPS 2016, has now started using the phrase \u201cpredictive learning\u201d in substitution of \u201cunsupervised learning\u201d. LeCun says:\n\nThis is an interesting change and indicates a subtle change in his perspective as to what he believes is required to build up the \u201ccake\u201d. In LeCun\u2019s view, the foundation needs to be built before we can make accelerated progress in AI. In other words, building off current supervised learning by adding more capabilities like memory, knowledge bases and cooperating agents will be a slog until we are all able to build that \u201cpredictive foundational layer\u201d (see: \u201cFive Capability Levels of Deep Learning\u201d ). At the conference he posted this slide:\n\nWhich emphasizes the formidable task that is ahead of us. Predictive learning is clearly requires machines to be able to not just learn without human supervision but learn a predictive model of the world. It is very important to emphasize this point and why LeCun is attempting to change our perspective of the canonical taxonomy of AI ( i.e. unsupervised, supervised and reinforcement learning).\n\nRuslan Salakhudinov, recently hired by Apple to lead their AI research, has a good survey talk of Unsupervised Learning (now to be rechristened as Predictive Learning) where he provides this taxonomy:\n\nAt the right corner of the slide he mentions Generative Adversarial Networks (GANs). GANs consists of competing neural networks, a generator and discriminator, the former tries to generate fake images while the later tries to identify real images. The interesting feature of these systems is that a closed form loss function is not required. In fact, some systems have the surprising capability of discovering its own loss function! A disadvantage of adversarial networks are they are difficult to train. Adversarial learning consists in finding a Nash equilibrium to a two-player non-cooperative game. Yann Lecun, in a recent lecture on unsupervised learning, calls adversarial networks the \u201cthe coolest idea in machine learning in the last twenty years\u201d.\n\nElon Musk\u2019s OpenAI research has a big focus on Generative Models, their motivation can be summarized by Richard Feynman\u2019s quote \u201cWhat I cannot create, I do not understand.\u201d Feynman is here alluding to his \u201cFirst Principles\u201d method of thought were he needs to be able to build up understanding by composing proven concepts. The basic idea here is that perhaps if a machine is able to generate models with high realism then perhaps ( a big leap here ) it develops an understanding of the predictive model. Here are some images of the state-of-the-art of this technique:\n\nThese are images generated by the DL system given the word provided. This indeed in quite impressive. I wouldn\u2019t expect many humans to be able to draw this well! Now, this system is not perfect, as evidence by this failure set:\n\nBut, hey, I\u2019ve seen many people do much worse while playing Pictionary!\n\nThe current consensus though are that these generative models aren\u2019t able to capture the semantics of the the task. They don\u2019t understand the meaning of an ant, volcano or redshank. They are however very good a mimicry and in fact prediction. These images are not recreations of images that the machine was previously trained on. Rather, the machine has come up with some generalized model that allows it to extrapolate a very realistic result.\n\nThis approach of using adversarial networks is different from the more classical approach of machine learning. Here we have two competing neural networks (i.e. discriminator and generator) that seem to work synergistically to accomplish a kind of generalization (see: \u201cRethinking Generalization\u201d). In the classical ML world one would define a objective function that one would fire up one\u2019s favorite optimization algorithm. However, in this research area, the correct objective function is quite unclear. Even more surprising is that these systems are able to learn their own objective function!\n\nThe fascinating realization here is that DL systems are extremely malleable. The classic ML notions that the objective function and constraints are fixed or the notion that the optimization algorithm is fixed do not apply in DL. Even more surprising is that a meta-level approach can be used. That is, DL systems can learn how to learn (see: \u201cMeta-learning\u201d)."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-different-ways-that-internet-giants-approach-deep-learning-research-753c9f99d9f1",
        "title": "Deep Learning Race: A Survey of Industry Players\u2019 Strategies",
        "text": "I\u2019ve been working for quite a while now in trying to make sense of the research developments in Deep Learning. The methodology I\u2019ve employed is through the cataloging of Design Patterns. It\u2019s been quite effective in disentangling this ever growing complex field. In fact, as new surprising research is published by the giants in this field, my own conceptual understanding of how it all fits together continues to be tweaked.\n\nThere are, however, certain patterns that I have observed that is actually outside that of a general understanding of Deep Learning. What I\u2019ve observed is that the different leading research groups seem to emphasize different kinds of approaches in solving the riddle of artificial intelligence. Now, a bit of a caveat here, I\u2019m not privy to the internal machinations inside these organizations. So if there\u2019s some secret sauce that an organization is brewing, then obviously I wouldn\u2019t know about it.\n\nHowever, by just reading the research publications, that fortunately come out quite occasionally, one gets a sense that organizations favor one approach from the other. (Note: The approaches are not mutually exclusive) So, with that out of the way, let me give you my intuition on the biases (or rather preferences) that each of the big players in the field approach Deep Learning research.\n\nEver since Google bought out DeepMind after seeing their Atari game playing AI, DeepMind has always had the preference of using Reinforcement Learning in their approach. They certainly use Deep Learning as a component in most of their research but always seems to emphasize its combination with RL. The DL research tends to focus on using variational methods that are embedded as non-parametric layers in their models. They also have a focus on attention mechanisms and memory augmented networks. In terms of breadth of research, I don\u2019t think there is any organization that is remotely close to DeepMind. DeepMind research seems to be driven by the need to discover the nature of intelligence. You can find more of their work here: https://deepmind.com/research/publications/ [DMT]\n\nGoogle has a distinct pragmatic and engineering approach in how they approach their research. You can see how they make endless tweaks on the inception architecture. They have detailed work on how they arrange their DL architectures around the compute resources that they have available. Google also combines other traditional algorithms like beam search, graph traversals and generalized linear models with DL . The approach seems to also emphasize the need for scalable solutions. Google is able to achieve impressive results as a consequence of their massive computational and data resources. You can find their research here: https://research.googleblog.com/ [GOO]\n\nThis is headed by Yann LeCun. It is unclear how strong this group is since it seems that most innovative research comes from LeCun\u2019s group in NYU. LeCun\u2019s group does an experimentation that explores the fundamentals, this is a sorely missing aspect that groups fail to perform enough. FAIR has published a couple of good open source implementations in Torch. They\u2019ve done some good work applying DL in certain problems. It is, however, hard to see if there is any particular research preference. I find it difficult to discern an overarching theme of their research. Maybe you might have better luck: https://research.fb.com/publications/?cat=2 [FAC]\n\nIs similar to Google in that their approach is very pragmatic and engineering oriented. Microsoft surprisingly has top notch computer science talent that had led to the discovery of Residual networks. They have other novel approaches like the Decision Forrest that indicates that the company clearly has thought leadership in this space and is not a company that is just a follower. Their Cognitive toolkit, despite coming late in the game, is a high- quality piece of engineering. It likely is one of the better frameworks out there with respect to learning using distributed computers. I would say that Microsoft is likely second to Google in their research contributions. That\u2019s a big statement to make considering none of the original DL researchers have joined their team. See: https://www.microsoft.com/en-us/research/research-area/artificial-intelligence/ [MIC]\n\nOpenAI was founded by Elon Musk (and some other lesser dudes) driven by the fear of seeing how quickly other firms had acquired Deep Learning talent. If you can\u2019t compete on financial terms, then offer academic freedom instead. OpenAI tends to favor the approach of using generative models, more specifically generative adversarial networks (GANs). They\u2019ve also made a serious effort towards the Reinforcement Learning space. What\u2019s curious though is that despite the impressive results of GANs, is that DeepMind seems to have a preference towards variational models instead. A glimpse of their research priorities can be found here: https://openai.com/requests-for-research/ [OPEN]. Microsoft has offered its Azure service to OpenAI thus potentially assuring OpenAI\u2019s allegiance.\n\nThis is Yoshua Bengio\u2019s group that is never really short on publications. Bengio is one of the few brave researchers who has not succumbed to joining a commercial entity. Similar to Lecun\u2019s NYU group, the approach focuses on trying to understand why DL works. The group also has plentiful work on tweaking DL with new models and learning algorithms. MILA is arguably the best academic DL research group on the planet (see: https://mila.umontreal.ca/en/ ) [MILA]. Google was pretty savvy by helping the group to some additional spare change. This move encouraged Hugo Larochelle to exit from Twitter ( quickly becoming a non-player ) and join the newly founded organization.\n\nHeaded by Richard Socher, most of the work coming from this group has an obvious bent towards solving NLP problems using RNN based approaches. These solutions will tend towards networks that employ memory in their construction. The output of these folks are quite impressive and therefore should be taken very seriously. Here\u2019s their work: http://metamind.io/research-list.html [META]\n\nAndrew Ng\u2019s group that likely was one of the first organizations to really create massive GPU systems with Infiniband networks. They\u2019ve done a lot of work emphasizing infrastructure and have open source some of their solutions. Their research focus has been in image and speech processing. In the latter, they\u2019ve done extremely well. Their research can be found here: http://research.baidu.com/institute-of-deep-learning/ [BAI]. Baidu was also originally involved with BMW with regards to self-driving cars.\n\nNvidia is betting the bank on Deep Learning solutions. They have the best engineering team that is out there tweaking their GPUs to get maximum performance. They are completely absent in terms of innovative DL research, but you can trust them to have spent serious effort in building the computation resources required for DL. They, however, have likely one of the leading implementations on self-driving cars. Their end-to-end DL solution for self-driving cars is one of those DL research papers that I remember that\u2019s notable.\n\nIntel was in deep trouble prior to their acquisition of the Nervana startup. Their hardware solutions were nowhere near competitive to Nvidia\u2019s. Nervana is very good at engineering the computational infrastructure used by Deep Learning. Some of the fastest implementations on GPU hardware come from Nervana. The speculation here is that Nervana was acquired for its software chops and not necessarily for the hardware they were designing. It is curious that the Nervana hardware solution ETA is mid-2017, that in the DL field is a very long time. You won\u2019t find a lot of research publications coming out of Nervana, I don\u2019t think they were able to amass enough DL talent while they were still a startup. However Intel is a player you cannot write off, they have technology shared with Micron ($MU) that may give them an insurmountable advantage in this space.\n\nIBM got this entire AI craze going wheb Watson destroyed its human competitors in Jeopardy. They were perceived to have been way ahead of everyone back in 2011. Unfortunately, Deep Learning came along and IBM has been slow to react in this space. Watson certainly uses DL in some of their solutions, however, they are almost non-existent in terms of DL research. They have this TrueNorth neuromorphic computing thing going, but that\u2019s mostly more hype than actual substance. IBM is not as prominent a researcher in the DL space, but if you look hard enough you\u2019ll find some: https://arxiv.org/abs/1604.08242 [IBM].\n\nNot really much to say here other than the hiring of Russ Salakhutdinov from CMU tells you that future research would emphasize his bent toward unsupervised learning approaches. Apple is busy acqui-hiring companies. Some of the bigger hires (i.e. Dato) were in the ML space. Unfortunately, their acqui-hires were mostly in the data science and big data space. I think when Apple realized that DL was different from ML, they rushed out and bought out Salakhutdinov. Apple\u2019s culture is extremely secretive, so I\u2019m not sure if their lack of research publication is an indicator of lack of expertise or towing the corporate line. Update: BusinessInsider reports Apple will start publishing their work.\n\nNot much to say other than the support of MXNet was a good thing. They had originally open sourced a framework called DSSTNE, it was unfortunately destined to receive a lot of neglect by the community. It\u2019s hard to get attention when there are so many competing frameworks that are out there. MXNet is actually impressively good considering that it did not have a big corporate backer.\n\nUber has made recent acquisitions of Otto (Self-Driving Trucks) and just recently started their AI Lab through the acquisition of Geometric Intelligence. Gary Marcus, the founder, was featured previously in other publications, he was unimpressed at that time with Deep Learning capabilities. It is important to note that Gary Marcus is a cognitive psychologist and not a computer scientist. Geometric Intelligence, however, was able to hire a basket load of ML practitioners. Uber\u2019s acquisition by all likelihood was an acqui-hire considering that is unlikely that Geometric Intelligence had any revenue, much less any product to speak of.\n\nJust another caveat, these are just my impressions and I am certain, given the deluge of information in the DL space, that I may have missed some publications that these organizations may have put out that should have been recognized. So my apologies if you feel my review was unfair.\n\nMy point though of this article is just to point out that AI research landscape is not homogeneous. Different organizations have different priorities in what they believe as important research areas. There is definitely an AI race going on and the likely winner is the company that actually was the one lucky enough to have put resources in the winning research approach! It is indeed very odd, that given the high stakes involved, that there\u2019s an implicit gamble that every firm seems to be making.\n\nAn early overview of ICLR 2017 gives an idea of the DL research activity of the above organizations.\n\nI am certainly surprised how low Baidu and Salesforce.com are on this list. Nevertheless, quantity does not necessarily always reflect quality. Let\u2019s wait till the submissions get reviewed. Also worth pointing out are the universities without a corporate affiliation, you will find that some of them have startup spinoffs that are ripe for corporate acquisition (just like Geometric.AI)."
    },
    {
        "url": "https://medium.com/intuitionmachine/game-theory-maps-the-future-of-deep-learning-21e193b0e33a",
        "title": "Game Theory reveals the Future of Deep Learning \u2013 Intuition Machine \u2013",
        "text": "If you\u2019ve been following my articles up to now, you\u2019ll begin to perceive, what\u2019s apparent to many advanced practitioners of Deep Learning (DL), is the emergence of Game Theoretic concepts in the design of newer architectures.\n\nThis makes intuitive sense for two reasons. The first intuition is that DL systems will eventually need to tackle situations with imperfect knowledge. In fact, we\u2019ve already seen this in DeepMind\u2019s AlphaGo that uses partial knowledge to tactically and strategically best the world-best human in the game of Go.\n\nThe second intuition is that systems will not remain monolithic as they are now, but rather would involve multiple coordinating (or competing) cliques of DL systems. We actually already do see this now in the construction of adversarial networks. Adversarial networks consists of competing neural networks, a generator, and discriminator, the former tries to generate fake images while the later tries to identify real images. The interesting feature of these systems is that a closed form loss function is not required. In fact, some systems have the surprising capability of discovering its own loss function! A disadvantage of adversarial networks are they are difficult to train. Adversarial learning consists in finding a Nash equilibrium to a two-player non-cooperative game. Yann LeCun, in a recent lecture on unsupervised learning, calls adversarial networks the \u201cthe coolest idea in machine learning in the last twenty years\u201d[LeC].\n\nWe are still in the early stages here of leveraging game theory, but I will point out some papers that have a game theoretic bent in them. David Balduzzi has a framework for deep learning that takes a game theoretic approach. In his paper \u201cSemantics, Representations and Grammars of Deep Learning\u201d[BAL] he writes:\n\nIt is a very elegant approach to covering an otherwise bewildering subject. He has these nice graphs (on adversarial networks) that highlight the strength of his approach:\n\nI would really love to see an entire textbook written with this approach!\n\nDavid Silver and Johannes Heinrich have a paper titled \u201cDeep Reinforcement Learning from Self-Play in Imperfect-Information Games\u201d[SILHEI]. They write:\n\nJason Hartford et al employs Deep Learning to predict human behavior. They write in \u201cDeep Learning for Predicting Human Strategic Behavior\u201d[HAR]:\n\nWhat we see in these 3 players are 3 different ways game theory plays in Deep Learning. (1) As a means of describing and analyzing new DL architectures. (2) As a way to construct a learning strategy and (3) A way to predict behavior of human participants. The last application can make your skin crawl!\n\nMathematics provides us with abstractions that aid us in our understanding of complex systems. However, every form of abstraction has its limitations in that there are certain details that are glossed over. We can sketch out some intuition with geometry, dynamics, and logic as to how these kinds of systems will tend to behave. What we begin to glean from this is that these systems consist of classifiers built from other classifiers. They are a self similar system that should be treated as a collective of many interacting machines. Furthermore, these machines are designed to make predictions out of the future. These predictions need to performed using incomplete and imperfect data. Therefore we need a mathematical framework that studies the behavior of many interacting parties that have different sets of information.\n\nThe classical view of machine learning is that the problem can be cast as an optimization problem where all that is needed are algorithms that are able to search for an optimal solution. However, with machine learning, we want to build machines that don\u2019t overfit the data but rather is able to perform well on data that it has yet to encounter. We want these machines to make predictions about the unknown. This requirement, which is called generalization, is very different from the classical optimization problem. It is very different from the classical dynamics problem where all information is expected to be available. That is why a lot of the engineering in deep learning requires additional constraints on the optimization problem. These, to my disliking, are called \u2018priors\u2019 in some texts and also called regularizations in an optimization problem.\n\nWhere do these regularizations come from and how can we select a good regularization? How do we handle impartial information? This is where a game theoretic viewpoint becomes important. Generalization is sometimes referred to as \u2018structural risk minimization\u2019. In other words, we build mechanisms to handle generalization using strategies similar to how parties mitigate risk. So we have actually returned full circle. Game theory is described as \u201cthe study of mathematical models of conflict and cooperation between intelligent rational decision-makers.\u201d In our quest of understanding learning machines, we end up with mathematics that was meant for the study of the interactions of intelligent beings.\n\nUpdate: New paper on Counter-Factual Reasoning and DL: https://arxiv.org/pdf/1701.01724v1.pdf DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker\n\nFeel to jump into the conversation by requesting an invite at LinkedIn: https://www.linkedin.com/groups/8584076 , or if you are non-techinical then FB: https://www.facebook.com/groups/deeplearningpatterns or visit us at Intuition Machine.\n\nUpdate: Google/DeepMind releases their new paper \u201cA Unified Game Theoretic Model to Multi-Agent Learning\u201d"
    },
    {
        "url": "https://medium.com/intuitionmachine/architecture-ilities-for-deep-learning-12a3ff9bec4e",
        "title": "3 Essential Deep Learning Architecture Abilities (i.e. \u201cIlities\u201d)",
        "text": "People in software development are familiar with the phrase \u201c-ilities\u201d. It is actually not a word, but you can google it:\n\nYou may think because it is not a real world, that it is some informal convention, some kind of loose jargon. This is actually not the case, software quality attributes have in fact been formalized in ISO 9126:\n\nQuality attributes are realized non-functional requirements used to evaluate the performance of a system. These are informally called \u201cilities\u201d after the suffix that of many of the words share. In software architecture, there is a notion of \u201cilities\u201d that are qualities that are important in evaluating our solutions. Lacking in DL literature is enough of an understanding of how to evaluate quality of a Deep Learning architecture. What then are the \u201cilities\u201d that are specific to evaluating Deep Learning systems?\n\nDespite the newness of the field, there are 3 main \u201cilities\u201d that a practitioner should know of:\n\nExpressibility \u2014 This quality describes how well a machine can approximate functions. One of first questions that many research papers have tried to answer is \u201cWhy does a Deep Learning system need to be Deep?\u201d Another way of saying this is, what is the importance of having multiple layers or a hierarchy of layers. There is some consensus in the literature that deeper networks require less parameters than shallow, wider networks to express the same function. You can find more detail of the various explanations here: http://www.deeplearningpatterns.com/doku.php/hierarchical_abstraction. The measure here appears to be, how few parameters (i.e. weights) do we need to effectively create a function approximator. A related research are here is weight quantization, how few bits does one need and not lose precision.\n\nTrainability \u2014 The other kind of research that gets published is on how well can a machine learn. You will find hundreds of papers that all try to out do each other by showing how trainable their system is as compared to the \u2018state-of-the-art\u2019. The open theoretical question here is why do these systems even learn at all? The reason this is not obvious is because the work horse of Deep Learning, the stochastic gradient descent (SGD) algorithm, appears absurdly too simplistic to even possibly work! There is a conceptual missing link here that researchers have yet to identify.\n\nGeneralizability \u2014 This is a quality that describes how well a trained machine can perform predictions on data that it has not seen before. I\u2019ve written about this in more detail in \u201cRethinking Generalization\u201d where I do describe 5 ways to measure generalization. I think that everyone seems talks about generalization, unfortunately few have a good handle on how to measure it.\n\nIn computer science, we do understand expressibility. This is its most general from is the notion of \u201cTuring Completeness\u201d or \u201cUniversal Computation\u201d (see: \u201cSimplicity of Universal Machines\u201d. Feed-forward networks and Convolution Networks are for example not turing complete simply because the don\u2019t have memory. What Deep Learning brings to the table that is wildly radical from conventional computer science is the latter two capabilities.\n\nTrainability, the ability to train a computer, rather than program a computer is a major capability. This is \u201cautomating automation\u201d. In other words, you don\u2019t need to provide specific detailed instructions, but rather you just need to provide the machine examples of what it needs to do. We\u2019ve actually seen this before in the difference between imperative versus declarative programming. The difference however in Deep Learning (or Machine Learning), we don\u2019t need to define the rules. The machine is able to discover the rules for itself.\n\nEven better, Generalization implies that if the machine, once trained, encounters situations where it has not been shown an example before, is able to figure out how to make the correct prediction. Generalization implies that even after discovering the rules after training, it is now able to create new rules on its own for unexpected situations. The machine has become more adaptable.\n\nThese ilities tie in with the \u201c5 Capability Level of Deep Learning\u201d. At each level we can explore the nature of expressibility, trainability and generalizability we require to achieve that level. So as an example, we can look at machines with the Classification with Memory. What does the additional memory component add to expressibility, trainability and generalizability. In the case for expressibility, we can see that memory permits a machine to perform translation instead of just classification. In terms of trainability, we had to come with additional mechanism to learn how to update memory. Finally, for generalizability we need to use other kind of benchmarks (i.e. BLEU, bAbl) to perform evaluations on this kind of system. At every capability level, we need to re-explore how we achieve each of these 3 ilities.\n\nIdeally we would like to see a framework where one understands how to compose various building blocks driven by an understanding as to how each block contributes to trainability, expressivity or generalization. Deep Learning is still very young in that we have few tools to evaluate the effectiveness of our solutions. Additionally, other ilities such as interpretability, transferability, latency, adversarial stability and security are worth exploring."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-turing-error-on-computation-energy-and-matter-b89a6a56fbf0",
        "title": "The Turing Error: On Computation, Energy and Matter",
        "text": "\u201cComputers? They are completely useless. They only give answers\u201d, Pablo Picasso.\n\nMany people, especially academics, believe that computation is inherently linked to the concept of Turing machines, or expressed through a collection of mathematically equivalent concepts: recursive functions, logical systems, states, inputs and outputs. It is also often assumed that the concepts behind a Turing-machine are crucial to the role of computers in AI and Cognitive Science.\n\nThese concepts are quite widespread and \u2026 wrong. Turing machines, recursive functions and logic are relevant concepts for computation. Turing machines, however, have little to do with computation machines, even with the most common modern form of it \u2014 the PC. If computation is defined as \u201cwhat computers do\u201d, Turing machines are irrelevant. All recent development of AI did not even depends on Turing machines.\n\nThe Turing machine is not only irrelevant for AI, is it dangerous as it directs the analyst towards a problem that do not exist. Why? Because the Turing machine is the perpetum mobile \u2014 an impossible machine that can only exist in imagination. Is a very simple machine that can perform any imaginable computation, but the creator forgot one important detail: it did not put any restrictions, either in time or energy. So, you may well get your result but you better seat down for \u2026 million of years before you see the result. Not very useful!\n\nMy point is that computation can not be separated from the substrate where it occurs, being atoms, transistors or neurons. Computation it\u2019s not possible without playing by the laws of physics. And there is where interesting things happen, because in order to do some computation you need 2 things: a dissipative system (that consumes energy) and time (a limited amount of it). This leads us to entire new landscape, as questions like \u201cis A computable\u201d become completely irrelevant. The right question to ask is \u201cwhat process is possible to computable in a given amount of time and energy\u201d? Or even more generally, \u201cwhat strategy should I use to make more efficient use of the environment that I interact with?\u201d.\n\nThose are practical and interesting question to ask. Nature has been solving them since million of years, apparently with great success. If we, humans, are something, is a tremendous computational device crafted over hundreds of millions of years. We follow only one rule: optimize the use of time and energy available to survive in our environment.\n\nIs it really important? For the mathematicians no so much, as their job is create abstract entities, but for the physicist yes \u2014 as they have to give meaning to these creations. In Turing-machines there are no time constraints, and where adding time constraints would produce no interesting differences, since only qualitative features of the behaviour were of interest.\n\nHuman intelligence, however, is precisely concerned with finding good solutions to problems quickly, and speed is central to the success of control systems managing physical systems embedded in physical environments. Aptness for their biological purpose, and not theoretical universality, is the important characteristic of animal brains, including human brains. What those purposes are, and what sorts of machine architectures can serve those purposes, are still open research problems (which I have discussed elsewhere), but it is clear that time constraints are very relevant to biological designs: speed is more biologically important than theoretical universality.\n\nNext I will explain why energy and time constraints are critical to the emergence of truly AGI."
    },
    {
        "url": "https://medium.com/intuitionmachine/five-levels-of-capability-of-deep-learning-ai-4ac1d4a9f2be",
        "title": "The Five Capability Levels of Deep Learning Intelligence",
        "text": "Arend Hintze has a good short article on \u201cUnderstanding the four types of AI, from reactive robots to self-aware beings\u201d where he outlines the following types:\n\nReactive Machine \u2014 The most basic type that is unable to form memories and use past experiences to inform decisions. They can\u2019t function outside the the specific tasks that they were designed for.\n\nLimited Memory \u2014 Are able to look into the past to inform current decisions. The memory however is transient and aren\u2019t used for future experiences.\n\nTheory of Mind \u2014 These systems are able to form representations of the world as well as other agents that it interacts with.\n\nI like his classification much better than the \u201cNarrow AI\u201d and \u201cGeneral AI\u201d dichotomy. This classification makes an attempt to break down Narrow AI into 3 categories. This gives us more concepts to differentiate different AI implementations. My reservation though of the definition is that they appear to come from a GOFAI mindset. Furthermore, the leap from limited memory able to employ the past to theory of mind seems to be an extremely vast leap.\n\nI however would like to take this opportunity to come up with my own classification, more targeted towards the field of Deep Learning. I hope my classification is a bit more concrete and helpful for practitioners. This classification gives us a sense of where we currently are and where we might be heading.\n\nWe are inundated with all the time with AI hype that we fail to good conceptual framework for making a precise assessment of the current situation. This may simply be due to the fact that many writers have trouble keeping up with the latest development in Deep Learning research. There\u2019s too much to read to keep up and the latest discoveries continue to change our current understanding. See \u201cRethinking Generalization\u201d as one of those surprising discoveries.\n\nThis level includes the fully connected neural network (FCN) and the convolution network (CNN) and various combinations of them. These system take a high dimensional vector as input and arrive at a single result, typically a classification of the input vector. You can consider these systems as being stateless functions, meaning that their behavior is only a function of the current input. Generative models are one of those hotly researched areas and these also belong to this category. In short, these systems are quite capable by themselves.\n\nThis level includes memory elements incorporated with the C level networks. LSTMs are example of these with the memory units are embedded inside the LSTM node. Other variants of these are the Neural Turing Machine (NMT) and the Differentiable Neural Computer (DNC) from DeepMind. These systems maintain state as they compute their behavior.\n\nThis level is somewhat similar to the CM level, however rather than raw memory, the information that the C level network is able to access is a symbolic knowledge base. There are actually three kinds of symbolic integration that I have found, a transfer learning approach, a top-down approach, a bottom up approach. The first approach uses a symbolic system that acts as a regularizer. The second approach has the symbolic elements at the top of the hierarchy that are composed at the bottom by neural representations. The last approach has it reversed, where a C level network is actually attached to a symbolic knowledge base.\n\nAt this level, we have a system that is built on top of CK, however is able to reason with imperfect information. An example of this kind of system would be AlphaGo and Poker playing systems. AlphaGo however does not employ CK but rather CM level capability. Like AlphaGo, these kind of systems can train itself by running simulation of it against itself.\n\nThis level is very similar to the \u201ctheory of mind\u201d where we actually have multiple agent neural networks combining to solve problems. Theses systems are designed to solve multiple objectives. We actually do se primitive versions of this in adversarial networks, that learn to perform generalization with competing discriminator and generative networks Expand that concept further into game-theoretic driven networks that are able to perform strategically and tactically solving multiple objectives and you have the making of these kind of extremely adaptive systems. We aren\u2019t at this level yet and there\u2019s still plenty of research to be done in the previous levels.\n\nDifferent level bring about capabilities that don\u2019t exist in the previous level. C level systems for example are only capable of predicting anti-causal relationships. CM level systems are capable of very good translation. CIK level systems are capable of strategic game play.\n\nWe can see how this classification somewhat aligns with Hinzte classification, with the exception of course of self-awareness. That\u2019s a capability that I really have not explored and don\u2019t intend to until the pre-requisite capabilities have been addressed. I\u2019ve also not addressed zero-shot or one-shot learning or unsupervised learning. This is still one of the fundamental problems, as Yann LeCun has said:\n\nLeCun has also recently started using the phrase \u201cpredictive learning\u201d in substitution of \u201cunsupervised learning\u201d. This is an interesting change and indicates a subtle change in his perspective as to what he believes is required to implement the \u201ccake\u201d. In LeCun\u2019s view, the foundation needs to be built before we can make substantial progress in AI. In other words, building off current supervised learning by adding more capabilities like memory, knowledge bases and cooperating agents will be a slog until we are all able to build that \u201cpredictive foundational layer\u201d. In the most recent NIPS 2016 conference he posted this slide:\n\nOne accelerator technology in all of this however is that when the capabilities are used in a feedback loop. We actually have seen instance of this kind of \u2018meta-learning\u2019 or \u2018learning to optimize\u2019 in current research. I cover these developments in another article \u201cDeep Learning can Now Design Itself!\u201d The key take away with meta-methods is that our own research methods become much more powerful when we can train machines to actually discover better solutions that we otherwise could find.\n\nThis is why, despite formidable problems in Deep Learning research, we can\u2019t really be sure how rapid progress may proceed.\n\nTo understand better how Deep Learning capabilities fit with your enterprise, visit Intuition Machine or discuss on the FaceBook Group on Deep Learning.\n\nhttp://csc.ucdavis.edu/~cmg/papers/et1.pdf Computational Mechanics of Input-Output Processes: Structured transformations and the e-transducer"
    },
    {
        "url": "https://medium.com/intuitionmachine/why-its-difficult-to-begin-using-deep-learning-in-investment-research-ea89e7a5130f",
        "title": "Deep Learning Playbook for Investment Research \u2013 Intuition Machine \u2013",
        "text": "Much ink has been used on the importance of AI, and more specifically, Deep Learning in the lives of our businesses. That it is the new electricity.\n\nThe business of Research is one of the most at-risk to dis-intermediation by Deep Learning (for the purposes of this write-up, I\u2019ll use AI and Deep Learning interchangeably, even though they are not. It\u2019s just that AI is more widely recognized by the populace).\n\nThe domain of research is much more than the be-speckled grad student toiling away beside glass beakers in a science lab.\n\nFor example, one off the most competitive fields to enter is being an equity research analyst. Equity research analysts on Wall Street can earn as much as $115k+ their first year after college. A sexy job and salary. There is much incentive to becoming a research analyst.\n\nThis $115k (going up to the millions, if wall street bonuses are included) is also a cost to the bank. The bank has an incentive to lower costs, say by hiring one less analyst but augmenting the remaining ones with research pre-processed with AI.\n\nIn fact, this is what both the equity research and the investment banking departments are focusing on at this moment.\n\nNow, \u201cresearch\u2019\u201d can be broadly defined as ingesting a lot of data, indexing/arranging the data into a coherent pile, and ultimately triggering some sort of recommendation or conclusion from the research.\n\nThis type of research is certainly done by any investment organization (and as we have seen, spending a lot of money on the process). Additionally, within the capital allocation process, the types of data being ingested and researched are varied, as are the industry \u2018verticals:\u2019 oil/gas, shipping, retail, tech, you name it.\n\nAll along the investment process \u2014 from deal origination (\u2018I\u2019d like to help you raise money, but I need to study your business\u2019) to bankruptcy (unearthing value in bankruptcy document papers \u2014 \u2018vulture investing\u2019 ) and beyond, data and content are manipulated day in and day out.\n\nAll corporations, large and small, do all sorts of research: a Japanese telecom company needing research into corporate venture capital targets in the US, for example. Or a Brazilian beverage company seeking more investment opportunities in Europe.\n\nThere\u2019s clearly a need to get ahead of the growth of data sources and the speed at which these data are growing. If the decision is made to get started on AI for research, the next obvious question is, how?\n\nWhile there has been a flurry of articles from academia about the latest breakthroughs, and some surfacing about other companies beginning to implement their Deep Learning projects, there are very few examples of how to actually get started on a Deep Learning project for research.\n\nThere are a few reasons for this.\n\n2. Where do I find the AI experts? You might have heard there is a 2 million shortage of qualified computer programmers. There is an even bigger demand for data scientists. We would suspect there is a more acute demand for AI/Deep Learning professionals. Cade Metz of Wired writes:\n\n3. Eleven other thoughts. We published eleven other thoughts on why there might be a cognitive gap between the worlds of business and AI, in our piece \u201cEleven Biases why Experts are Missing the Train on Deep Learning\u201d.\n\nDo you know what? You should get started anyway.\n\nGet started because, like electricity was to businesses in a previous era, AI will change the business landscape for this era.\n\nThere are relatively risk-free and non-destructive ways to get started in adding Deep Learning to your research initiatives.\n\nMeasure (many times) cut once. The old adage still works in the digital era.\n\nBefore you back up the truck at Home Depot and start buying planks of wood, windows and light fixtures, you spend the time and effort to first create your project on paper.\n\nIn the building example, you get an architect to design what your building would look like. In the world of AI entry points, this is working to define a Technical Roadmap to building your AI project.\n\nI call it relatively risk-free and non-destructive because no permanent, large-expenses need to be made. No permanent hires, no expensive hardware and software teams. In the era of the Gig Economy, a Technical Roadmap is the perfect entry point to:\n\na) get you working on the thought processes needed to roll-out a Research Proof-of-Concept\n\nb) make you aware of which of your research workflows can be augmented by AI\n\nc) give you options to pinpoint cost savings afforded by AI-enhanced Research\n\nd) get you to put down your AI books and journals and start working on doing AI\n\nWe\u2019ll work on fleshing out your options for getting started on your path to implementing a Deep Learning project, from Roadmap to Proof-of-Concept to Pilot Project and finally Production Environment. All in a relatively risk-free, and non-destructive way.\n\nFor more information on Research Machine visit: http://www.intuitionmachine.com/research-machine/"
    },
    {
        "url": "https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d",
        "title": "40 Ways Deep Learning is Eating the World \u2013 Intuition Machine \u2013",
        "text": "Deep Learning is eating the world. Here are a few applications that it is digesting.\n\nMicrosoft Skype is able to translate voice into different languages in realtime. Something straight out of the universal translator in Star Trek.\n\nGoogle Photos is able to automatically organize your photographs into collections with common shared themes.\n\nA hobbyist is able to teach his car to self-drive in a few hours.\n\nSketch an image as a query to a visual search."
    },
    {
        "url": "https://medium.com/intuitionmachine/persuasive-machines-weapons-of-mass-disinformation-a53787c12dbc",
        "title": "Persuasive Machines: Weapons of Mass Disinformation",
        "text": "On July 28th, 2015 a group of AI scientists published an open letter concerning the creation of weaponized AI.\n\nThey write the following:\n\nWhich lead to some research:\n\nUnfortunately, by November 2016 a different kind of weaponized AI had arrived with devastating effects:\n\nThese \u201cfilter bubbles\u201d or \u201cecho chambers\u201d are primed staging areas for further exploitation. Persuasive Machines are AI driven automation, leveraging knowledge of our cognitive biases:\n\nto hack into our thought processes, leading to persuasion (or alternatively mind control). A example of this exploit is the rampant rise of fake news and events on Facebook. Mike Caulfied write \u201cDespite Zuckerberg\u2019s Protests, Fake News Does Better on Facebook Than Real News. Here\u2019s Data to Prove It.\u201d:\n\nFake news is orders of magnitude more popular than real news. It is not just fake news that\u2019s a problem, security breaches are another. Maria Krolov writes:\n\nAutomation in the form of Twitter bots have amplified messaging to bring about a \u2018bandwagon effect\u2019 to influence the masses:\n\nThese automaton, in combination with massive hacking and phishing attacks by a nation state were part of a coordinate effort to undermine the decision making of the population. Esquire magazine has an even more detailed account of this in \u201cHow Russian Pulled the Greatest Election Hack in History\u201d.\n\nJonathan Albright has an even more detail analysis of what he calls: \u201cMicro Propaganda Machines\u201d:\n\nScott Adams (of Dilbert fame) understands the power of persuasion quite well:\n\nDylan Love writes about solutions to solving the AI arms race in \u201cThe Next Global Arms Race Aims to Perfect AI\u201d:\n\nNot a very promising list of options.\n\nhttps://www.wired.com/2016/09/inside-googles-internet-justice-league-ai-powered-war-trolls Inside Google\u2019s Internet Justice League and its AI Powered War on Trolls\n\nhttp://kioski.yle.fi/omat/at-the-origins-of-russian-propaganda Yle Kioski Traces the Origins of Russian Social Media Propaganda \u2014 Never-before-seen Material from the Troll Factory"
    },
    {
        "url": "https://medium.com/intuitionmachine/rethinking-generalization-in-deep-learning-ec66ed684ace",
        "title": "Rethinking Generalization in Deep Learning \u2013 Intuition Machine \u2013",
        "text": "The ICLR 2017 submission \u201cUnderstanding Deep Learning required Rethinking Generalization\u201d [ICLR-1] is certainly going to disrupt our understanding of Deep Learning. Here is a summary of what they had discovered through experiments:\n\nThe authors actually introduce two new definitions to express what they are observing. The talk about \u201cexplicit\u201d and \u201cimplicit\u201d regularization. Drop out, data augmentation, weight sharing, conventional regularization are all explicit regularization. Implicit regularization is early stopping, batch norm, and SGD. It is an extremely odd definition that we\u2019ll discuss.\n\nI understand regularization ( see: http://www.deeplearningpatterns.com/doku.php/regularization ) as being of two types. I use the terms \u201cRegularization by Construction\u201d and \u201cRegularization by Training\u201d. There is the Regularization by Training that is the conventional use of the term. There is also the Regularization by Construction which is a consequence of the Model choices we select as we construct the elements of our network. The reason why there is a distinction, when mathematically they do appear equivalently as constraint terms, is that Regularization conventionally is not present after training, that is in the inference path. Regularization by Construction is always present, both in the training and the inference stages.\n\nNow the paper has a distinction between explicit and implicit regularization and that is when the main intent of the method is to regularize. One does dropout to regularize, so it is explicit. One does batch normalization (BN) for normalizing the activations of the different input samples but it happens to also regularize, so it is implicit regularization. The distinction between the two is the purpose of regularization or not. The later being implicit generalization. The meaning is that the unintended consequence of the technique is regularization. So when a researcher does not think that a method would lead to regularization and to his surprise it does, then that is what they call \u2018implicit\u2019 regularization. I don\u2019t think however Hinton expected Drop Out to lead to regularization. This is why I think the definition is extremely fuzzy, however, I understand why they introduced the idea.\n\nThe goal of regularization, however, is to improve generalization. That is also what BN does. In fact, for inception architectures, BN is favored over drop out. Speaking about normalization, there are several kinds, Batch and Layer normalization are the two popular versions. The motivation for BN is supposed to be Domain Adaptation. Is Domain Adaptation different from Generalization? Is not just a specific kind of generalization? Are there other kinds of generalization? If so, what are they?\n\nThe authors have made the surprising discovery that methods that don\u2019t seem to generalization, more specifically SGD, in fact, does. Another ICLR 2017 paper An Empirical Analysis of Deep Network Loss Surfaces [ICLR-2] adds added confirmation to this SGD property. This paper shows empirically that the loss surfaces for different SGD methods differ from each other. This tells you that what is happening is very different from traditional optimization.\n\nIt reminds one of quantum mechanics, where probes affect observation. Here learning method affects what is learned. In this new perspective of neural networks, that of brute force memorization or alternatively holographic machines, then perhaps ideas of quantum mechanics may need to come in play. Quantum mechanics emerges because of the non-commutability of poisson brackets in classical dynamics. We have two variables, position, and momentum, that are inextricably tied together. In Deep Learning I have a hunch that there are more than two variables that are tied together that lead to regularization. We at least have 3 variables: learning method, network model and generative model that all seem to have an effect on generalization. The troubling discovery, however, is how ineffective conventional regularization appears to be. \u201cExplicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error.\u201d\n\nI think right now we have a very blunt instrument when it comes to our definition of Generalization. I wrote here that there are at least 5 different notions of generalization ( http://www.deeplearningpatterns.com/doku.php/risk_minimization ).\n\nWe can define it as the behavior of our system in response to validation data. That is against data that we have not included as part of the training set. We be a bit more ambitious and define it as behavior when the system is deployed to analyze real world data. We essentially would like to see our trained system perform accurately in the context of data it has never seen.\n\nA second definition is based on the idea of Occam\u2019s Razor. That is, the simplest of explanations is the best explanation. Here we make certain assumptions about the form of the data and we drive our regularization to constrain the solution toward our assumptions. So for example in the field of compressive sensing, we assume that a sparse basis exists. From there we can drive an optimization problem that searches solutions that have a sparse basis.\n\nA third definition is based on the systems ability to recreate or reconstruct the features. This is the approach taken by generative models. If a neural network is able to accurately generate realistic images, then it able to capture the concept of images in its entirety. We see this approach taken by researchers working on generative methods.\n\nA fourth definition involves the notion of ignoring invariant features or nuisance variables. That is, a system is able to generalize well if it is able to ignore invariant features for its tasks. Remove away as many features as possible until you can\u2019t remove any more. This is somewhat similar to the third definition however it tackles the problem from another perspective.\n\nA fifth generalization definition revolves around the idea of minimizing risk. When we train our system, there is an uncertainty in the context in which it will be deployed. So we train our models with mechanisms to anticipate unpredictable situations. The hope is that the system is robust to contexts that have not been previously predicted. This is kind of a game theoretic definition. We can envision an environment where information will always remain imperfect and generalization effectively means executing a particular strategy within the environment. This may be the most abstract definition of generalization that we have.\n\nI\u2019m sure there are many more as we move to a more game theoretic framework of learning. In fact, one effective strategy for learning is driven by the notion of \u201cCuriosity\u201d.\n\nUpdate: December 20, 2016: According to An early overview of ICLR 2017, Understanding deep learning requires rethinking generalization is the highest rated submission.\n\nI would like to dissect and discuss this further, link up to me at http://www.linkedin.com/in/ceperez or send me an email at ceperez AT intuitonmachine.com"
    },
    {
        "url": "https://medium.com/intuitionmachine/11-biases-why-experts-are-missing-the-train-in-deep-learning-1c092fa9cace",
        "title": "11 Arguments Experts get Wrong about Deep Learning \u2013 Intuition Machine \u2013",
        "text": "I spend most of my waking time ( and likely my subconscious works overtime while I sleep ) studying Deep Learning. Peter Thiel has a phrase, \u201cThe Last Company Advantage\u201d[THI.] Basically you don\u2019t necessarily need to have the \u201cFirst Mover Advantage\u201d however you absolutely want to be the last company standing in your kind of business. So Google may be the last Search company, Amazon may be the last E-Commerce company and Facebook hopefully will not be the last Social Networking company. What keeps me awake at night though is that Deep Learning could in fact be the \u201cLast Invention of Man\u201d!\n\nHowever, let\u2019s ratchet it down a little bit here. After all, Kurzweil\u2019s Singularity (estimate is 2045) is still 3 decades away. That\u2019s still plenty of time for us humans to scheme on our little monopolies. Your objective in the next 30 years of humankind is to figure out if you are going to be living in Elysium or in some unnamed decaying backwater:\n\nCredit: Elysium the movie, not the life-extension supplement.\n\nTo aid you in your decision making, here are 11 reasons why your \u201cexperts\u201d will lead you to miss the all important Deep Learning revolution:\n\nPractitioner\u2019s introduction to neural networks are almost always via the introduction of linear regression and then to logistic regression. That\u2019s because the mathematical equations for an artificial neural network (ANN) are identical. So there immediately is a bias here that the characteristics of these classical ML methods would also convey into the world of DL. After all, DL in its most naive explanation is nothing more than multiple layers of ANN.\n\nThere are also other kinds of ML methods that have equations that are different from DL. The basic objective however for all ML methods is a general notion of curve fitting. That is if you can have a good fit of a model with the data then that perhaps is a good solution. Unfortunately with DL systems, due to the fact that the number of parameters in the model are so large, these systems by default will over-fit any data. This is enough of a tell that a DL is an entirely different kind of animal from an ML system.\n\nDL systems have a loss function that is a measure of how well its predictions match its input data. Classic optimization problems also have loss functions (also known as objective functions). In both systems, different kinds of heuristics are used to discover an optimal point in a large configuration space. It was once thought that the solution surface of a DL system was sufficiently complex enough that it would be impossible to arrive at a solution. However, curiously enough, one of the most simple methods of optimization, the Stochastic Gradient Descent algorithm, is all that is need to arrive at surprising results.\n\nWhat this tells you is that is something else going on here that is actually very different from what optimization folks are used to.\n\nA lot of Data Scientists have an aversion for DL because of the lack of interpretability of its predictions. This is a characteristic of not only DL methods but classical ML methods as well. Data Scientists would rather use Probabilistic methods where they can have better control of the models or priors. As a result have systems that are able to make predictions with the least number of parameters. All driven by the belief that parsimony or Occam\u2019s razor is the optimal explanation for everything.\n\nUnfortunately, probabilistic methods are not competitive in classifying images, speech or even text. That\u2019s because DL methods are superior in discovering models than human beings. Brute force just happens to trump wetware. No Data Scientist has ever been able to find the \u2018principal components\u2019 that will do image classification well. Furthermore, there\u2019s no experimental evidence in the DL space that parsimonious models work any better than entangled models. For those cases where it is an absolute requirement to have some kind of explanation, there are now newer methods in DL that provide aid to interpretability as well as uncertainty. If a DL system can generate the captions in an image, then there is a good chance that it can be trained to generate an explanation of a prediction.\n\nThis is a natural bias that something that is around 5 years old and rapidly evolving is too new and volatile a technology to trust. I think we all said the same thing when the microprocessor, internet, web, mobile technologies came along. Wait and see was the safe approach for most everyone. This is certainly a reasonable approach for anyone who has not really spent the time investigating the details. However, it is a very risky strategy, ignorance may be bliss but another company eating your lunch can mean extinction.\n\nThere are a lot of things that DL can do that were deemed inconceivable just a couple years ago. Nobody expected a computer to beat the best human player in Go. Nobody expected self-driving cars to exist today. Nobody expected to see Star Trek universal translator like capabilities. It is so unbelievable that it must likely be an exaggeration than something that may be real. I hate however to burst your bubble of ignorance, DL is in fact very real and you experience it yourself with every smartphone.\n\nWe\u2019ve had so many times where the promise of AI had lead to disappointing results. The argument goes further that because it has happened so often before, that it is also bound to happen again. The problem with this argument is that despite the disappointment, AI research has led to many software capabilities that we do take for granted today and thus never notice its existence. Good old fashioned AI (GOFAI) are embedded in many systems today.\n\nThe current pace of DL development is accelerating and there are certainly certain big problems that need to be solved. The need for a lot of training data and the lack of unsupervised training are two problems. This however doesn\u2019t mean that what we have today has no value. DL can already drive cars, that in itself tells you that even if another AI winter arrives, we would have achieved a state of development that is still quite useful. Andrew Ng has more about this [NG].\n\nThe research community does not have a solid theoretical understanding as to why DL works so effectively. We have some idea as to why a multi-layer neural network is more efficient in fitting functions than one with fewer layers. We, however, don\u2019t have an understanding as to why convergence even occurs or why good generalization happens. DL at this time is very experimental and we are just learning to characterize these kinds of systems. Meanwhile, despite not having a good theoretical understanding, the engineering barrels forward. Researchers, using their intuition and educated guesses are able to build exceedingly better models. In other words, nobody is stopping their work to wait for a better theory. It is almost analogous with what happens in biotechnology research. People are experimenting with many different combinations and arriving at new discoveries that they have yet to explain. Scientific and technological progress is very messy and one shouldn\u2019t shy away from the benefits because of the chaos.\n\nDL system are very unlike the neurons in our brain. The mechanism of how DL learns (i.e. SGD) is not something we can explain happening in our brain. The argument here though is that if it doesn\u2019t resemble the brain then it is unlike to be able to perform the kind of inference and learning of a brain. This, of course, is an extremely weak argument. After all, planes don\u2019t look like birds, but they certainly can fly.\n\nNot having expertise in-house shouldn\u2019t be an excuse for avoiding finding expertise outside. Furthermore, should prevent you from having your experts learn this new technology. However, if these experts are of the dogmatic persuasion, then that should be a tell for you to get a second and unbiased opinion.\n\nBusinesses are composed of many business processes. Unless you have not gone through the exercise of examining which processes can be automated with current DL technologies, then you are not in a position to make the statement that DL does not apply to you. Furthermore, you may discover new processes and business opportunities may not exist today but are possible with the exploitation of DL technology. You cannot really answer this question until you invested in some due diligence work.\n\nThe large internet companies like Google and Facebook have gobbled up a lot of the Deep Learning talent out there. These companies have very little interest in working with a small business to identify their specific needs and opportunities. However, fortunately, these big companies have been gracious enough to allow their researchers to publish their work. We, therefore, do have a view into their latest developments and thus are able to take what they\u2019ve learned and apply it to your context. There are companies like Intuition Machine that do have an on-boarding process for you to get a competitive head start in DL technologies.\n\nIf you want a first mover advantage, then urgently reach out to Intuition Machine. They\u2019ve got a Deep Learning guide to turn your business into one that can be potentially disruptive."
    },
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-the-unreasonable-effectiveness-of-randomness-14d5aef13f87",
        "title": "Deep Learning: The Unreasonable Effectiveness of Randomness",
        "text": "The paper submissions for ICLR 2017 in Toulon France deadline has arrived and instead of a trickle of new knowledge about Deep Learning we get a massive deluge. This is a gold mine of research that\u2019s hot off the presses. Many papers are incremental improvements of algorithms of the state of the art. I had hoped to find more fundamental theoretical and experimental results of the nature of Deep Learning, unfortunately there were just a few. There was however 2 developments that were mind boggling and one paper that is something I\u2019ve been suspecting for a while now and has finally been confirm to shocking results. It really is a good news, bad news story.\n\nFirst let\u2019s talk about the good news. The first is the mind boggling discovery that you can train a neural network to learn to learn (i.e. meta-learning). More specifically, several research groups have trained neural networks to perform stochastic gradient descent (SGD). Not only have they been able to demonstrate neural networks that have learned SGD, the networks have performed better than any hand tuned human method! The two papers that were submitted were\u201dDeep Reinforcement Learning for Accelerating the Convergence Rate\u201d and \u201cOptimization as a Model for Few-Shot Learning\u201d . Unfortunately though, these two groups have been previously scooped by Deep Mind, who showed that you could do this in this paper \u201cLearning to Learn by gradient descent by gradient descent\u201c. The two latter papers trained an LSTM, while the first one trained via RL. I had thought that it would take a bit longer to implement meta-learning, but it has arrived much sooner than I had expected!\n\nNot to be out-done, two other groups created machines that could design new Deep Learning networks and do it in such a way as to improve on the state-of-the-art! This is learning to design neural networks. The two papers that were submitted are \u201cDesigning Neural Network Architectures using Reinforcement Learning\u201d and \u201cNeural Architecture Search with Reinforcement Learning\u201d. The former paper describes the use of Reinforcment Q-Learning to discover CNN architectures. You can find some of their generated CNNs in Caffe here: https://bowenbaker.github.io/metaqnn/ . The latter paper is truly astounding (you can\u2019t do this without Google\u2019s compute resources). Not only did they show state-of-the-art CNN networks, the machine actually learned a few more variants of the LSTM node! Here are the LSTM nodes the machine created (left and bottom):\n\nSo not only are researcher who hand optimize gradient descent solutions out of business, so are folks who make a living designing neural architectures! This is actually just the beginning of Deep Learning systems just bootstrapping themselves. So I must now share Schmidhuber\u2019s cartoon that aptly describes what is happening:\n\nThis is absolutely shocking and there\u2019s really no end in sight as to how quickly Deep Learning algorithms are going to improve. This meta capability allows you to apply it on itself, recursively creating better and better systems.\n\nPermit me now to deal you the bad news. Here is the paper that is the bearer of that news: \u201cUnderstanding Deep Learning required Rethinking Generalization\u201c. I\u2019ve thought about Generalization a lot, and I\u2019ve posted out some queries in Quora about Generalization and also about Randomness in the hope that someone could give some good insight. Unfortunately, nobody had enough of an answer or understood the significance of the question until the folks who wrote the above paper performed some interesting experiments. Here is a snippet of what they had found:\n\nThe shocking truth revealed. Deep Learning networks are just massive associative memory stores! Deep Learning networks are capable of good generalization even when fitting random data. This is indeed strange in that many arguments for the validity of Deep Learning is on the conjecture that \u2018natural\u2019 data tends to exists in a very narrow manifold in multi-dimensional space. Random data however does not have that sort of tendency.\n\nJohn Hopfield wrote a paper early this year examining the duality of Neural Networks and Associative Memory. Here\u2019s a figure from his paper:\n\nThe \u201cRethinking Generalization\u201d paper goes even further by examining our tried and true tool for achieving Generalization (i.e. Regularization) and finds that:\n\nIn other words, all our regularization tools may be less effective than what we believe! Furthermore, even more shocking, the unreasonable effectiveness of SGD turns out to be:\n\njust a different kind of regularization that just happens to work!\n\nIn fact, a paper submitted for ICLR2017 by another group titled \u201cAn Empirical Analysis of Deep Network Loss Surfaces\u201d confirms that the local minima of these networks are different:\n\nWhich tells you that your choice of learning algorithm \u201crigs\u201d how it arrives at a solution. Randomness is ubiquitous and it does not matter how you regularize your network or what the SGD variant that you employ, the network just seems to evolve (if you set the right random conditions) towards convergence! What are the properties of SGD that leads to machines that can learn? Are the properties tied to differentiation or is it something more general? If we can teach a network to perform SGD, can we teach it to perform this unknown generalized learning method?\n\nThe effectiveness of this randomness was in fact demonstrated earlier this year in a paper: \u201cA Powerful Generative Model Using Random Weights for the Deep Image Representation\u201d also co-authored by John Hopcroft that showed that you could generate realistic imagery using randomly initialized networks without any training! How could this be possible? (Editor\u2019s Note: Initialization with random weights is certainly better than non-random weights, unless those weights are from a pre-trained network)\n\nTherefore to understand Deep Learning, we must embrace randomness. Randomness arises from maximum entropy, which interestingly enough is not without its own structure! The memory capacity of a neural network seems to be highest the closer to random the weights are. The strangeness here is that Randomness is ubiquitous in the universe. The arrow of time is reflected by the direction towards greater entropy. How then is it that this property is also the basis of learning machines?\n\nIf we were to assume that the reasoning (or the intuition) behind hierarchical layers in DL is that the bottom layers consist of the primitive recognition components that are built up, layer by layer, into more complex recognition components.\n\nWhat this implies then is that the bottom components during training should be \u2018searched\u2019 more thoroughly than the top most components. But the way SGD works is that the search is driven from the top and not from the bottom. So the top is searched more thoroughly that the bottom layers.\n\nWhich tells you the bottom layers (the ones closest to inputs) are not optimal in their representation. In fact, they are the kind of a representation that likely will be of the most generalized form. The kind that will have recognizers that will have equal probability of matching anything, in short, completely random!\n\nAs you move up the layers, the specialization happens because it is actually driven from the top which is designed to fit the data. Fine tuning happens at the top.\n\nLet\u2019s make the analogy of this process with languages. The bottom components of a language are letters and the top parts are sentences. In between you have syllables, words, parts of speech etc. However from a Deep Learning perspective, it is as if there are no letters! But rather fuzzy forms of letters. Which builds up into other fuzzy forms of words and so forth. The final layers is like some projection (some wave collapse) into interpretation.\n\nNow let\u2019s throw in the Swap Out learning procedure which tells us that if you sample any subnetwork of the entire network the resulting prediction will be the similar to any other subnetwork you look sample. Just like holographic memory where you can slice of pieces and still recreate the whole. The procedure seems to be that the more random we try to make it to be, the better our learning. That is definitely counter-intuitive!\n\nThe following paper published last week \u201cLearning in the Machine: Random Backpropagation and the Learning Channel\u201d explores the robustness of using random matrices instead of gradients.\n\nPlease see Design Patterns for Deep Learning: Canonical Patterns for additional insight on this intriguing subject."
    },
    {
        "url": "https://medium.com/intuitionmachine/why-deep-learning-is-radically-different-from-machine-learning-945a4a65da4d",
        "title": "Why Deep Learning is Radically Different from Machine Learning",
        "text": "There is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL). There certainly is a massive uptick of articles about AI being a competitive game changer and that enterprises should begin to seriously explore the opportunities. The distinction between AI, ML and DL are very clear to practitioners in these fields. AI is the all encompassing umbrella that covers everything from Good Old Fashion AI (GOFAI) all the way to connectionist architectures like Deep Learning. ML is a sub-field of AI that covers anything that has to do with the study of learning algorithms by training with data. There are whole swaths (not swatches) of techniques that have been developed over the years like Linear Regression, K-means, Decision Trees, Random Forest, PCA, SVM and finally Artificial Neural Networks (ANN). Artificial Neural Networks is where the field of Deep Learning had its genesis from.\n\nSome ML practitioners who have had previous exposure to Neural Networks (ANN), after all it was invented in the early 60\u2019s, would have the first impression that Deep Learning is nothing more than ANN with multiple layers. Furthermore, the success of DL is more due to the availability of more data and the availability of more powerful computational engines like Graphic Processing Units (GPU). This of course is true, the emergence of DL is essentially due to these two advances, however the conclusion that DL is just a better algorithm than SVM or Decision Trees is akin to focusing only on the trees and not seeing the forest.\n\nTo coin Andreesen who said \u201cSoftware is eating the world\u201d, \u201cDeep Learning is eating ML\u201d. Two publications by practitioners of different machine learning fields have summarized it best as to why DL is taking over the world. Chris Manning an expert in NLP writes about the \u201cDeep Learning Tsunami\u201c:\n\nNicholas Paragios writes about the \u201cComputer Vision Research: the Deep Depression\u201c:\n\nThese two articles do highlight how the field of Deep Learning are fundamentally disruptive to conventional ML practices. Certainly it should be equally disruptive in the business world. I am however stunned and perplexed that even Gartner fails to recognize the difference between ML and DL. Here is Gartner\u2019s August 2016 Hype Cycle and Deep Learning isn\u2019t even mentioned on the slide:\n\nWhat a travesty! It\u2019s bordering on criminal that they their customer\u2019s have a myopic notion of ML and are going to be blind sided by Deep Learning.\n\nAnyway, despite being ignored, DL continues to by hyped. The current DL hype tends to be that we have these commoditized machinery, that given enough data and enough training time, is able to learn on its own. This of course either an exaggeration of what the state-of-the-art is capable of or an over simplification of the actual practice of DL. DL has over the past few years given rise to a massive collection of ideas and techniques that were previously either unknown or known to be untenable. At first this collection of concepts, seems to be fragmented and disparate. However over time patterns and methodologies begin to emerge and we are frantically attempting to cover this space in \u201cDesign Patterns of Deep Learning\u201c.\n\nDeep Learning today goes beyond just multi-level perceptrons but instead is a collection of techniques and methods that are used to building composable differentiable architectures. These are extremely capable machine learning systems that we are only right now seeing just the tip of the iceberg. The key take away from this is that, Deep Learning may look like alchemy today, but we eventually will learn to practice it like chemistry. That is, we would have a more solid foundation so as to be able to build our learning machines with greater predictability of its capabilities.\n\nAlso read: 11 Biases Why Experts will miss the Deep Learning revolution."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-unreasonable-simplicity-of-universal-machines-ac6a1baf38a8",
        "title": "The Unreasonable Simplicity of Universal Machines \u2013 Intuition Machine \u2013",
        "text": "Rule 110 cellular automata, or more specifically the one dimensional cellular automata (you can explore those here http://atlas.wolfram.com/01/01/ ) that has the following rule:\n\nis all the complexity that one needs to create a machine that has all the computational capability of a Turing Machine, hence any computer system.\n\nis all the logic one needs to compose any boolean equation.\n\nHow does a Rule 110 automata differ from a NAND gate? The NAND gate has 4 rules, the automata has however 8 rules. If we look closely, we see that the Rule 110 automata contains all the rules of the NAND gate. Specifically, 010 -> 1, 011 0 ->, 110 -> 1 and 111 -> 1. In other words, if the center cell is set to 1, then Rule 110 acts just like a NAND gate. However, there are 14 other cellular automata that captures NAND logic but are not universal.\n\nThe cellular automata state of 0 for Rule 110 automata apparently has some additional capability that leads to universal behavior. Let\u2019s examine these, for when the center cell is 0, the behavior becomes:\n\nor if we ignore the center cell:\n\nThe middle two rules appear to break symmetry in that there\u2019s a clear distinction as to which neighbor cell is on.\n\nLet\u2019s examine another automata, Rule 30 that is known to be chaotic:\n\nFor when the center cell is 0:\n\nand when the center cell is 1:\n\nwith that symmetry breaking that we see in Rule 110.\n\nThe complement of Rule 110 is Rule 137:\n\nWhich is the same as 110, but instead with a universal NOR gate.\n\nWhich is the same behavior as Rule 110 but with the center state now 1 instead of 0.\n\nIf we replace the rule for 111 and 010 to 111 -> 0 and 010 -> 0 we have\n\nwhich is Rule 13 and not universal.\n\nSo its not just the symmetry breaking that\u2019s important, but the fact that 11->1 and 00->0 are important. Note: Flipping the rule for 10 and 01 are also universal.\n\nSo, what perhaps is the significance of this circuitry\u2026\n\nWhat we see with these rules is that the value on the right neighbor cell becomes the center cell. The mirror rule 124 shifts from the left and the complement rule shifts also from the right. So to achieve a universal machine one just needs two rules. A NAND or NOR operator and a shift operator. The center cell determines which operator is active at the time. The simplest universal cellular automata has a computational element and a memory element.\n\nNow that we have found the simplest machine possible, can we now attempt to identify the simplest machine that can learn? If we are able to do this, we can then show that a majority of systems in nature are in fact learning machines!"
    },
    {
        "url": "https://medium.com/intuitionmachine/10-lessons-learned-from-building-deep-learning-systems-d611ab16ef66",
        "title": "10 Lessons Learned from Building Deep Learning Systems",
        "text": "Deep Learning is a sub-field of Machine Learning that has its own peculiar ways of doing things. Here are 10 lessons that we\u2019ve uncovered while building Deep Learning systems. These lessons are a bit general, although they do focus on applying Deep Learning in a area that involves structured and unstructured data.\n\nThe one tried and true way to improve accuracy is to have more networks perform the inferencing and combining the results. In fact, techniques like DropOut is a means of creating \u201cImplicit Ensembles\u201d were multiple subsets of superimposed networks cooperate using shared weights.\n\nThe current state of Deep Learning is that it works well only in a supervised context. The rule of thumb is around 1,000 samples per rule. So if you are given a problem where you don\u2019t have enough data to train with, try considering an intermediate problem that does have more data and then run a simpler algorithm with the results from the intermediate problem.\n\nNot all data is nicely curated and labeled for machine learning. Many times you have data that are weakly tagged. If you can join data from disparate sources to achieve a weakly labeled set, then this approach works surprisingly well. The most well known example is Word2Vec where you train for word understanding based on the words that happen to be in proximity with other words.\n\nOne of the spectacular capabilities of Deep Learning networks is that bootstrapping from an existing pre-trained network and using it to train into a new domain works surprisingly well.\n\nData usually have meaning that a human may be aware of that a machine can likely never discover. One simple example is a time feature. From the perspective of a human the day of the week, whether this is a holiday or not or the time of the day may be important attributes, however a Deep Learning system may never be able to surface that if all its given are seconds since Unix epoch.\n\nL1 and L2 regularizations are not the only regularizations that are out there. Explore the different kinds and perhaps look at different regularizations per layer.\n\nThere are multiple techniques to initialize your network prior to training. In fact, you can get very far just training the last layer of a network with the previous layers being mostly random. Consider using this technique to speed up you Hyper-tuning explorations.\n\nA lot of researchers love to explore end-to-end deep learning research. Unfortunately, the most effective use of Deep Learning has been to couple it with out techniques. AlphaGo would not have been successful if Monte Carlo Tree Search was not employed. If you want to make an impact in the Academic community then End-to-end Deep Learning might be your gamble. However in a time constrained industrial environment that demands predictable results, then you best be more pragmatic.\n\nIf you can, try to avoid using multiple machines (with the exception of hyper-parameter tuning). Training on a single machine is the most cost effective way to proceed.\n\n10. Convolution Networks work pretty well even beyond Images\n\nConvolution Networks are clearly the most successful kind of network in the Deep Learning space. However, ConvNets are not only for Images, you can use them for other kinds of features (i.e. Voice, time series, text).\n\nThat\u2019s all I have for now. There certainly a lot more other lessons. Let me know if you stumble on others.\n\nYou can find more details of these individual lessons at http://www.deeplearningpatterns.com and Intuition Machine."
    },
    {
        "url": "https://medium.com/intuitionmachine/a-pattern-language-for-deep-learning-30de291434e1",
        "title": "Design Patterns for Deep Learning \u2013 Intuition Machine \u2013",
        "text": "Pattern Languages are languages derived from entities called patterns that when combined form solutions to complex problems. Each pattern describes a problem and offers solutions. Pattern languages are a way of expressing complex solutions that were derived from experience such that others can gain a better understanding of the solution.\n\nPattern Languages were originally promoted by Christopher Alexander to describe the architecture of businesses and towns. These ideas where later adopted by Object Oriented Programming (OOP)practitioners to describe the design of OOP programs, these were named Design Patterns. These were extended further into other domains like SOA (http://www.manageability.org/blog/stuff/pattern-language-interoperability/view) and High Scalability (http://www.manageability.org/blog/stuff/patterns-for-infinite-scalability/view).\n\nIn the domain of Machine Learning (ML) there is an emerging practice called \u201cDeep Learning\u201d. In ML there are many new terms that one encounters such as Artificial Neural Networks, Random Forests, Support Vector Machines and Non-negative Matrix Factorization. These however usually refer to a specific kind of algorithm. Deep Learning (DL) however is not really one kind of algorithm, rather it is a whole class of algorithms that tend to exhibit similar \u2018patterns\u2019. DL systems are Artificial Neural Networks (ANN) that are constructed with multiple layers (sometimes called Multi-level Perceptrons). The idea is not entirely new, since it was first proposed back in the 1960s.. However, interest in the domain has exploded with the help of advancing hardware technology (i.e. GPU). Since 2011, DL systems have been exhibiting impressive results in the field.\n\nThe confusion with DL arises when one realizes that there actually many implementations and it is not just a single kind of algorithm. There are the conventional Feed forward Networks (aka. Fully Connected Networks), Convolution Networks (ConvNet), Recurrent Neural Networks (RNN) and less used Restricted Boltzmann Machines (RBM). They all share a common trait in that these networks are constructed using a hierarchy of layers. One common pattern for example is the employment of differentiable layers, this constraint on the construction of DL systems leads to an incremental way to evolve the network into something that learns classification. There are many such patterns that have been discovered recently and it would be very useful for practitioners to have at their disposal a compilation of these patterns. In the next few weeks we will be sharing more details of this Pattern Language.\n\nPattern languages are an ideal vehicle for describing and understanding Deep Learning. One would like to believe the Deep Learning has a solid fundamental foundation based on advanced mathematics. Most academic research papers will conjure up high-falutin math such as path integrals, tensors, Hilbert spaces, measure theory etc. but don\u2019t let the math distract oneself from the reality that our understanding is minimal. Mathematics you see has its inherent limitations. Physical scientists have known this for centuries. We formulate theories in such a way that the structures are mathematically convenient. The Gaussian distribution for example is prevalent not because its some magical construct that reality has gifted to us. It is prevalent because it is mathematically convenient.\n\nPattern languages have been leveraged in many fuzzy domains. The original pattern language revolved around the discussion of architecture (i.e. buildings and towns). There are pattern languages that focus on user interfaces, on usability, on interaction design and on software process. These all don\u2019t have concise mathematical underpinnings yet we do extract real value from these pattern languages. In fact, the specification of a pattern language is not too far off from the creation of a new algebra in mathematics. Algebras are strictly consistent but they are purely abstract and may not need to have any connection with reality. Pattern languages are however connected with reality, however consistency rules are more relaxed. In our attempt to understand the complex world of machine learning (or learning in general) we cannot always leap frog into mathematics. The reality may be such that our current mathematics are woefully incapable of describing what is happening."
    },
    {
        "url": "https://medium.com/intuitionmachine/a-development-methodology-for-deep-learning-2ce515158bb7",
        "title": "A Development Methodology for Deep Learning \u2013 Intuition Machine \u2013",
        "text": "The practice of software development has created development methodologies such agile development and lean methodology to tackle the complexity of development with the objective of improving the quality and efficiency of software creation. Although Deep Learning is built from software it is a different kind of software and therefore a different kind of methodology is needed. Deep Learning differs most from traditional software development in that a substantial portion of the process involves the machine learning how to achieve objectives. The developer is not completely out of the equation, but is working in concert to tweak the Deep Learning algorithm.\n\nDeep Learning is sufficiently rich and complex a subject that a process model or methodology is required to guide a developer. The methodology addresses the necessary interplay of the need for more training data and the exploration of alternative Deep Learning patterns that drive the discovery of an effective architecture. The methodology depicted as follows:\n\nWe begin first we some initial definition of the kind of architecture we wish to train. This will of course be driven by the nature of the data that we a training from and the kind of prediction we seek. The latter is guided by Explanatory Patterns and the former by Feature Patterns. There are a variety ways to optimize our training process, this is guided by the Learning Patterns.\n\nAfter the selection of our network model and the data we plan on training on, the developer is then tasked with answering the question as to whether adequate labeled training set is available. This process goes beyond conventional machine learning process that divides the dataset into three sets. The machine learning convention has been to create a training set, a validation set and a test set. In the first step of the process, if the training remains high there are several options that can be pursued. The first is to try to increase the size of the model, a second option is perhaps train a bit long (alternatively perform hyper-parameter tuning) and if all fails then the developer tweaks the architecture or attempts a new architecture. In the second step of the process, a develop validates the training against a validation set, if the error rate is high indicating overfitting then the options are to find more data, apply different regularizations and if all fails attempt another architecture. The observations here that differs from conventional machine learning is that Deep Learning has more flexibility in that a developer has the additional options of employing either a bigger model or using more data. One of the hallmarks of deep learning is its scalability in performing well when trained with large data sets.\n\nTrying a larger model is something that a developer has control over, unfortunately finding more data poses a more difficult problem. To satisfy this need for more data one can leverage data from different contexts. In addition one can employing data synthesis and data augmentation to increase the size of training data. These approaches however lead to domain adaptation issues, so a slight change in the traditional machine learning development model is called for. In this extended approach, the validation and training sets are required to belong to the same context. Furthermore, to validate training from this heterogeneous set, another set called the training-validation set is set aside to act as additional validation. This basic process model, inspired by a talk by Andrew Ng, serves as a good scaffolding to hang off the many different patterns that we find in Deep Learning.\n\nAs you can see, there are many paths of exploration and many alternatives models that may be explored to reach to a solution. Furthermore, their is sufficient Modularity in Deep Learning that we may compose solutions from other previously developed solutions. Autoencoders, Neural Embedding, Transfer Learning and bootstrapping with pre-trained networks are some of the tools that do provide potential shortcuts that reduce the need to train from scratch.\n\nThis is no means complete, but it is definitely a good starting point to guide the development of this entirely new kind of architecture."
    },
    {
        "url": "https://medium.com/intuitionmachine/how-blockchain-can-save-the-free-market-80b8800931f9",
        "title": "How Blockchain Can Save the Free Market \u2013 Intuition Machine \u2013",
        "text": "Markets and governments are often placed at opposite ends of a spectrum: more government intervention equals less competition, while deregulation and liberalisation always leads to more competition, innovation and growth. In reality, things are a lot more nuanced than that. Due to the existence of economies of scale in many industries, unfettered competition results in a high level of concentration and even, in some cases, in monopolies. In these cases, government intervention in the form of antitrust policies, are needed to retain competition.\n\nOnline industries like Facebook and Uber show huge economies of scale, in the specific form of platform and network effects. And due to their borderless nature, antitrust measures are very difficult to implement and, more important perhaps, would destroy real consumer value. Blockchain technology could offer a way out of this dilemma.\n\n1.\n\nCapitalism \u2014 free enterprise \u2014 is a beautiful system. It was an enormous improvement over the medieval system in which enterprise was not free, but needed a license to operate \u2014 a license from a king, a guild or the church. In that system, competition was limited and the licensed craftsmen and chartered companies made excessive profits. This gave us, in the former Hansa cities, nice guild houses and lofty canal dwellings, but it did not bring a lot of other positive effects. A permit based system \u2014 artificial monopolies \u2014 is, by its very nature, conservative and not very innovative.\n\nProgress requires competition. Whoever thinks he can do better, has to have the opportunity to do so. Inefficient producers are defeated by smarter challengers \u2014 that is progress in a capitalist system. Joseph Schumpeter called it creative destruction , we now call it disruption. Competition, or even the threat of competition, makes sure that companies stay efficient and innovative, and that brings progress.\n\nSome believe that free competition is somehow a natural state; as long as government stays out of the economy as much as possible, competition, innovation and progress magically emerge. That is evidently not true. What happens when a government doesn\u2019t regulate markets, can be witnessed in countries that have weak or corrupt governments, and in markets that officially do not exist and therefore cannot be regulated. In those circumstances, the most effective way to make money is not innovation, but elimination of the (potential) competition by all means available.\n\nGovernment\u2019s responsibilities for a decent functioning of markets are not limited to maintaining law and order. Even without corruption, violence or other forms of foul play, most markets have a tendency towards concentration. This is because economies of scale exist. Economies of scale make that bigger companies produce at lower costs than smaller companies, so that big companies keep getting bigger and smaller competitors disappear. In some industries, like oil, pharma, aircraft and accounting, economies of scale are so large that less than ten corporations dominate the global market. This is a concentration of money and power that can easily lead to abuse. That is why governments, especially in the US, used to actively break up monopolies, with Standard Oil and Bell Telephone as the most famous cases.\n\nOn the other hand: economies of scale are real, and forcing companies to break up is forcing them to produce at higher cost. This is a destruction of wealth, at least in the short term. Behold, in a nutshell, why economies of scale were one of the most challenging issues for regulators in the industrial age, in which size clearly mattered.\n\n2.\n\nThe Big Promise of first the world wide web and later social media was to end this. The triangle of mass production, mass media and mass consumption could be broken. Someone, I believe it was Hugh MacLeod, coined the phrase \u2018global microbrand\u2019: even a small company could build a worldwide client base at low cost, provided it made something special. Small would become the new Big.\n\nUnfortunately, for now not much has materialised from that promise. Only a very few multinationals have actually been disrupted. Even newspaper and music publishers still exist (and make money). Small has not become the new normal. On the contrary: we have seen the rise of some extremely big and powerful companies. Facebook, Google and Apple are the most powerful companies ever, not only in terms of their war chests, but more importantly to the extend of what they know about you. AirBnB, Uber, eBay, Amazon and a few other so-called unicorns have monopolized entire industries. All because of economies of scale or, more precisely, because of a particular sub-set of economies of scale that are known as network- and platform effects.\n\nThe network effect is also known as Metcalfe\u2019s Law. It states that the value of a network for its existing users increases with each new user \u2014 making the value of the network as a whole increase exponentially with every new user. Network effects lead to winner-takes-all situations: whoever manages to be the first to surpass a certain threshold of users, will be dominating the industry for a long time, even as better alternatives become available. WhatsApp is great example of this dynamic.\n\nThe platform effect appears everywhere where supply and demand are coupled. The buyer will go to the place where most sellers are, and vice versa. It\u2019s a difficult place to conquer \u2014 the famous chicken-and-egg problem \u2014 but an easy place to defend. This is how, in The Netherlands, Marktplaats remains the biggest second hand market place of the country, without having to make much of an effort. (eBay acquired Marktplaats in 2004 instead of trying to outcompete it.)\n\nAlmost all so-called unicorns are found in industries where network and platform effects are huge. The \u201dwinner takes all\u201d logic goes a long way in explaining the typical start-up dynamics and the huge investments: whoever is first will be able to basically block all competition and therefore have all the possibilities to recoup the investments, and then some more.\n\nSo now we have the paradoxical situation that our economies have been massively democratised on one end, but that the power of a handful of private companies has become bigger than ever at the same time. Everyone can make money writing songs, renting out their attic, driving people from A to B and selling home-made beanie caps without the permission of a government or publisher, and without needing lots of capital. Just as long as you comply to the user agreement of the dominant platform in your industry, including handing over five to thirty percent of your earnings.\n\nEconomies of scale were a difficult enough phenomenon to cope with for regulators in the old economy, but they have proven to be even more challenging in the online economy. It is obviously not in the short term interest of users to break up Facebook (or Uber, or any other similar company) \u2014 even if we knew how to do that, or who should do it. But it has to be done, these monopolies need to be be broken down. Because eventually monopolies lead to waste, corruption and crony capitalism, while at the same time slowing down innovation and progress.\n\nTraditionally, we know two ways to remedy these \u201cefficient\u201d or \u201cnatural\u201d monopolies: nationalisation (or more broadly: collectivisation) and tendering. Because we are talking about global, worldwide platforms, nationalising them seems to be out of the question. Tendering, as we know it from public transportation and ether frequencies (competition for the market instead of competition in the market) might work for a few of the mentioned platforms. Uber and her competitors could compete for the exclusive right to offer ride-hailing services in, say, the city of Amsterdam for a number of years. Same for AirBnB and other platforms that offer services that can be geographically fenced off. But \u2014 not even considering that most platforms fall outside this category \u2014 there are significant differences between Uber or AirBnB and similar platforms, and ether frequencies for instance. Uber and their kind are online services, meaning that they collect significantly more data on the markets they operate in. If Uber were to be awarded the exclusive right to offer their services in Amsterdam for a couple of years, they would collect so much data that they would have a decisive advantage over their competitors in any next tender or auction.\n\nIt seems that, in the case of global, online platforms, we lack the tools to regulate monopolies and safeguard competition. Even adequately taxing them fails most of the time.\n\n3.\n\nBlockchain has gained a reputation as \u201cthe technology behind Bitcoin\u201d. For those not familiar with the concept, this video is a great way to get up to speed in six minutes. The Bitcoin blockchain, as discussed in the video, is especially interesting as a mental model. The Bitcoin transactions are stored on the blockchain: a publicly accessible system, that is owned by no-one and that cannot be closed down or censored by anyone. On top of this, services are offered, such as exchanges, but it is easy for users to switch from one service to a competing one, because all actions one takes are recorded in the public ledger, instead of the \u201cclosed vault\u201d owned by the service provider. This means it is a lot easier for a customer to change providers for a certain service.\n\nNow imagine that Facebook would work this way. Your data \u2014 your profile, your relations, your messages, your privacy settings \u2014 are all (safely encrypted) stored in a publicly accessible system and Facebook only offers you their user experience: the ease of use, the relevancy of the ads, the speed with which pages load, that sort of things. In this scenario, you could easily switch to a competitor, without losing any of your old data. Facebook would no longer be the platform; the blockchain would be the platform. And there could be \u201cnormal\u201d competition on the platform, instead of winner-takes-all competition for the platform. In other words: a meaningful development of blockchain technology could provide a good (and possibly the only) way to get a grip on those monopolies that are a result from network- and platform effects.\n\nIf we will see such a development is a big if. The good news is that there are a multitude of initiatives to develop and add functional layers on top of the (bitcoin-)blockchain stack. Some of it is open source and/or not-for profit, but there are also venture capitalists that understand what is at stake and have even found a business model in it. The not-so-good news is that the foundation, Bitcoin itself, is slow to evolve. Changes and improvements to the code are the result of a long winded and tedious process that at times looks like a democracy, at other times appears to be a consensus-driven effort and every now and then straight up resembles an episode of Game of Thrones. Decentralized systems are not always very efficient.\n\nSimultaneously, banks and other big companies are working on their own \u201cprivate blockchains\u201d. A private blockchain essentially is a cartel: a closed group of companies sharing information with each other, but not with outsiders. A proven method to defend existing market positions and demoralise newcomers and innovators.\n\nIn the meantime, our policy makers are still contemplating whether or not the taxi industry needs to be protected from \u201cchallenger\u201d Uber. They\u2019d better focus on investing in an infrastructure that can save the free market from that sort of platform monopolists.\n\nAn earlier version of this post, in Dutch, was published here on Medium and on The Bitcoin Report."
    },
    {
        "url": "https://medium.com/intuitionmachine/microglia-a-biologically-plausible-basis-for-back-propagation-278223102aeb",
        "title": "Microglia: A Biologically Plausible Basis for Back-Propagation",
        "text": "The Perceptron, the basis of Artificial Neural Networks (ANN), was conceived in 1957:\n\nIt is of course an outdated model of how the neurons actually work. The current neural network research and development is more driven by mathematically techniques that ensure continuity and convergence rather than anything biological inspired.\n\nHowever there are other research groups like Numenta and IBM TrueNorth that are investigating more biologically inspired systems. These systems are referred to as \u201cSpiking Neural Networks (SNN)\u201d (see: https://en.wikipedia.org/wiki/Spiking_neural_network ) or alternatively \u201cNeuromorphic computing\u201d.\n\nThese SNN have unfortunately not proven to be as effective as their less biologically inspired cousins (ANN). In a recent paper (see: http://arxiv.org/abs/1603.08270 ) however, IBM TrueNorth has shown competitive results simulating a Convolution Network. This is direct evidence that an \u201cintegrate-and-spike\u201d mechanism has the similar computational capability as the more proven ANNs. The IBM paper however highlighted one major weakness of SNN. That is, training of the TrueNorth system required simulation of back-propagation using another conventional GPU:\n\nSaid in a different way, using \u201cback-propagation\u201d. Biological inspired SNNs seem to lack a mechanism to receive feedback. Although it had been previously conjectured that such a mechanism was not necessary.\n\nPrior to the invention of machine powered flight, many people could observe what a bird does to fly. They can point out the flapping of wings, the large ratio of the size of the wings to the body, to the weight of the bird or the presence of feathers. However, none of these features leads one to the actual mechanics of flight. This is one of the arguments against biologically inspired research. However birds and planes are able to fly because the dynamics are the same. The airflow under a wing has a higher density than above the wing and thus creating an upward pressure.\n\nThere is a commonality with the brain and neural networks is the fact that they are both dynamical systems. ANN researchers have observed that if we assign weights to input signals, multiply the signals with the weights and sum up the results then we have a NN that can perform pretty impressive pattern classification. The discovery of the weights is done through what is called \u201ctraining\u201d and this is done by adjusting all the weights slightly in a way that reduces the observed error in the pattern classification. Learning is achieved when the observed error settles to one that is consistently acceptable. This training mechanism is what is called \u201cBack-propagation\u201d.\n\nThere are many variants of \u201cback-propagation\u201d, the most common is gradient descent with a variant called RProp (see: https://en.wikipedia.org/wiki/Rprop ) which is an extreme simplification that uses only the sign of the gradient to perform its update. Natural Gradient based methods that are second order update mechanism an interesting variant called NES (https://en.wikipedia.org/wiki/Natural_evolution_strategy) employs genetic evolution methods. Field Alignment is another simplistic method that is extremely efficient ( see: http://arxiv.org/pdf/1411.0247.pdf ). In general, back-propagation does not necessarily require that the implementation is performed by a strict application of an analytic gradient calculation. What is essential is that there is some approximation of an appropriate weight change update and a corresponding structure to propagate the updates. Incidentally, recent research (see: http://cbmm.mit.edu/publications/how-important-weight-symmetry-backpropagation-0 ) appears to conclude that the magnitude of gradient update isn\u2019t as important as the sign of the update.\n\nThere however has been no biological evidence of a structural mechanism of \u201cback-propagation\u201d in biological brains. Yoshua Bengio published a paper in 2015 (see: http://arxiv.org/abs/1502.04156 ) \u201cTowards Biologically Plausible Deep Learning\u201d. The investigation attempts to explain a mechanism for back-propagation exists in Spike-Timing-Dependent Plasticity (STDP) of biological neurons. It is however questionable whether neurons are able to learn by themselves without the need of an external feedback pathway that spans multiple layers.\n\nThere is however an alternative mechanism that recently has been discovered that may be a more convincing argument that is based on a structure that is independent of the brain\u2019s neurons. There is a large class of cells in the Brain called Microglia ( see: https://www.technologyreview.com/s/601137/the-rogue-immune-cells-that-wreck-the-brain ) that are responsible for regulating the neurons and their connectivity.\n\nResearch on the nature of the Microglia has been ignored until very recently:\n\nIn fact, conventional wisdom was that the Lymphatic System did not interface with the brain (see: https://cosmosmagazine.com/life-sciences/lymphatic-drain-inside-your-brain )\n\nHowever, recent research ( see: http://www.nih.gov/news-events/nih-research-matters/lymphatic-vessels-discovered-central-nervous-system ) has debunked that conventional understanding.\n\n50% of the brain\u2019s volume consists of glia (see: http://brainblogger.com/2015/03/16/what-have-your-glia-done-for-you-today/ ). This new model of the brain does provide a more convincing pathway to explaining the notion of \u201cBack-propagation\u201d and does hint at explaining the lack of convincing results of SNN based systems. SNN have been formulated with unfortunately only a partial understanding of how the brain works and thus an incomplete model that is missing a regulatory mechanism.\n\nIn addition, learning does appear to be to be influenced by the microglia in the brain (see: http://www.cell.com/trends/immunology/abstract/S1471-4906(15)00200-8 ).\n\nThis is further validated in a more recent comprehensive study (see: http://journal.frontiersin.org/article/10.3389/fnint.2015.00073/full ).\n\nMicroglia are even able to communicate with neurons by neurotransmitters (see: http://www.ncbi.nlm.nih.gov/pubmed/25451814 ) :\n\nThere are in additional experimental evidence that sleep, also vital in learning, involves the Glial cells ( see: http://www.bbc.com/news/health-24567412 ):\n\nBack-propagation, perhaps at work, while we sleep?\n\nIn summary, biological brains have a regulatory mechanism in the form of microglia that are highly dynamic in regulating synapse connectivity and pruning neural growth. The activity is most pronounced during sleep. SNNs have been shown to have inference capabilities equivalent to Convolution Networks. SNNs however have not shown to effectively learn on their own without a \u2018back-propagation\u2019 mechanism. This mechanism is most plausibly provided by the microglia."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-emerging-information-geometric-approach-to-deep-learning-a39235716a95",
        "title": "The Emerging Information Geometric Approach to Deep Learning",
        "text": "Classical Statistics addresses systems involving large numbers, however Statistics breaks down in a domain of high dimensional inputs and models with a high number of parameters. In this domain, new theories and methods are being developed using new insights discovered though the use of massive computational systems. The field of Deep Learning is spearheading these discoveries, however there is a pressing need to have an overarching framework. Such a framework is at the core of our development efforts at Alluviate.\n\nThe study of Deep Learning at its foundations is based on Probability Theory and Information Theory. For a probabilistic treatment, the book \u201cThe Elements of Statistical Learning\u201d is suggested. From a Information Theoretic viewpoint, David MacKay\u2019s book and his video lectures are a great place to start (see: Information Theory, Inference, and Learning Algorithms). Joshua Bengio\u2019s upcoming book on Deep Learning also has a dedicated a chapter to cover two fields.\n\nThe Count Bayesie blog has a very intuitive tutorial that is worth a quick read. It introduces probability theory and provides a generalization of the equation for expectation :\n\nwhere the author employs the Lebesque Integral that defines probability in a space that could otherwise be non-Euclidean. This is a hint to the realization that probability may not need to defined a non-Euclidean space. If Non-Euclidean then perhaps there may be other Non-Euclidean metrics that could be employed in the study of Deep Learning?\n\nThe dynamics of a Neural Network is usually framed in the context of optimizing a convex or non-convex non linear problem. This involves the minimization/maximization of an objective function. The formulation of the objective function is a bit arbitrary but it is typically the squared error between the actual and estimated values:\n\nThe solution to the optimization problem is typically a simple gradient descent approach. What is surprising here is that Deep Learning systems are highly successful despite such a simple approach. One would have thought that gradient descent would be all too often stuck often many local minima one would expect in a non-convex space. However, the intuition of low dimensions does not convey to higher dimensions, where local minima are actually saddle points and a simple gradient descent can escape given enough patience!\n\nHowever, without a overarching theory or framework, a lot of the techniques employed in Deep Learning (i.e. SGD, Dropout, Normalization, hyper-parameter search etc) all seem to be arbitrary techniques (see: http://arxiv.org/abs/1206.5533 ).\n\nAt Alluviate we build of a Information Theoretic approach with the primary notion of employing metrics that distinguish between an estimated distribution and an actual distribution. We use this knowledge to drive more efficient training.\n\nIn Information Theory there is the Kullback-Leibler Divergence $ D_{KL}(p||q) = \\sum^x p(x) log \\left( \\frac {p(x)}{q(x)} \\right) $ which is a measure of the difference between two probability distributions. (Note: Shannon\u2019s Entropy is a special case of the KL divergence where q is constant). If one takes a distribution and its infinitesimal difference, one arrives as the following equation:\n\nwhere $ g_{ij} $ is the Fisher Information Matrix (FIM):\n\nThe Cram\u00e9r\u2013Rao lower bound is an estimate of the lower bound of the variance of an estimator. It is related to the FIM $ I(\\theta) $ in scalar form:\n\nSo the above equation that the FIM has an effect on minimizing the variance between estimated and actual values.\n\nThere exists a formulation by Sun-Ichi Amari in a field called Information Geometry that casts the FIM as a metric. Amari shows in his paper \u201cNatural Gradient works Efficiently in Learning\u201d, and speculates that natural gradient may more effectively navigate out of plateaus than conventional stochastic gradient descent. The FIM Information Geometry shares some similarity with Einstein\u2019s General Theory of Relativity in that the dynamics of a system follows a non-Euclidean space. So rather than observing the curvature of light as a consequence of gravity, one would find a curvature of information in the presence of knowledge.\n\nAlthough the Information Geometry theory is extremely elegant, the general difficulty with the FIM is that is is expensive to calculate. However recent developments (all in 2015) have shown various approaches to calculating an approximation that leads to very encouraging results.\n\nParallel training of DNNs with Natural Gradient and Parameter Averaging from the folks developing the Speech Recognizer Kaldi have developed a stochastic gradient technique that employs an approximation of the FIM. Their technique not only improves over standard SGD, but allows for parallelization.\n\nYoushua Bengio and his team at the University of Montreal have a paper Topmoumoute online natural gradient algorithm TONGA have developed a low-rank approximation of FIM with an implementation that beats stochastic gradient in speed and generalization.\n\nFinally Google\u2019s DeepMind team have published a paper \u201cNatural Neural Networks\u201d. In this paper they describe a technique that reparameterizes the neural network layers so that the FIM is effectively the identity matrix. It is a novel technique that has similarities to the Batch Normalization that was previously proposed.\n\nWe still are in an early stage for a theory of Deep Learning using Information Geometry, however recent developments seem show the promise of employing a more abstract theoretical approach. Abstract mathematics like Information Geometry should not be dismissed as impractical to implement but rather used as a guide towards building better algorithms and analytic techniques. As in High Energy physics research, there is undeniable value in the interplay between the theoretical and the experimental physicists.\n\nFor more information on this approach please see: A Pattern Language for Deep Learning."
    },
    {
        "url": "https://medium.com/intuitionmachine/the-seven-is-of-big-data-science-methodology-711af03ef5b",
        "title": "The Seven I\u2019s of Big Data Science Methodology \u2013 Intuition Machine \u2013",
        "text": "Industry press is enamored by the 4 V\u2019s of Big Data. These are Volume, Velocity, Variety and Veracity. Volume referring to the size of the data. Velocity referring to the speed of how data is collected and consumed. Variety referring to the different kinds of data consumed, from structured data, unstructured data and sensor data. Veracity referring to the trustworthiness of the data.\n\nIBM (source) has a nice infographic that highlights the problem space of Big Data:\n\nReaders however are left perplexed as how best to discover value given these tremendous data complexities. Alluviate is focused on formulating a Big Data Methodology (specifically we call it The Data Lake Methodology) for creating valuable and actionable insight. There certainly can be other Big Data methodologies, but we believe that the Data Lake approach leads to a more agile and lean process.\n\nThe current standard process for Data Mining is CRISP-DM. It involves 5 major phases:\n\nThe core value of Big Data is not just the ability to store lots of data in cheap commodity hardware. The real value, which many vendors seem to have missed the point entirely, is the ability to process that data to gain insight. This is Data Mining in the traditional sense and it is Machine Learning in the more advanced sense. Therefore, as a starting CRISP-DM is a good starting point for defining a new process. This new process however needs to get an upgrade. That is, we can\u2019t ignore advanced in technology since 1999 when CRISP-DM was originally defined.\n\nA Data Lake approach to Big Data has the following features:\n\nGiven this context of a new kind of way of doing data integration, I propose the Seven I\u2019s of the Data Lake Methodology:\n\nI hope that the Big Data industry will move beyond discussing the problem of the Four V\u2019s of Big Data. The Seven I\u2019s of Big Data Science instead focuses on the more valuable process of discovering actionable insight. This is a better place to revolve the discussion around."
    },
    {
        "url": "https://medium.com/intuitionmachine/data-lakes-and-the-responsive-21st-century-corporation-cedb9cfef9b5",
        "title": "Data Lakes and the Responsive 21st Century Corporation",
        "text": "Yammer\u2019s founder is introducing a new way of organizing the corporation. His ideas originate from earlier ideas from the Lean and Agile methodologies of software development. In his Responsive Manifesto he drives a case for a new kind of efficiency that will drive the successful workplaces of the future.\n\nThe Responsive Manifesto\n\ndeclared the following principles:\n\nSo let me explain the value of a Data Lake strategy in the context of enabling a more responsive organization:\n\nThe legacy Database administration organization limited the control and understanding of data to a few experts. This lead to an extremely time consuming and rigid process that required the continuous participation of the gatekeepers of the data.\n\nToday, circumstances and markets change rapidly as information flows faster. A Data Lake\u2019s self-service capability enables employees with the best insight and decision-making ability to easily access data of the company to gain better insight. Rather than controlling data through process and hierarchy, you achieve better results by empowering people at the edges.\n\nIn a highly unpredictable environment, plans start losing value the moment they\u2019re finished. Embracing agile methods that encourage experimentation and fuel rapid learning is a much better investment that spending too much time upfront planning.\n\nIn Data Lake\u2019s the upfront investment typically found in Data Warehouse deployments of designing a universal canonical schema is done away with. The costs to continuously update this schema and corresponding ETL scripts in the an rapidly changing environment is removed. Data is ingested in a Data Lake in the most automated way possible. This empowers people at the edge to rapidly gain insight on new data.\n\nData Lakes provide technology and connectivity to increase the ability to self-organize, collaborating more easily across internal and external organizational boundaries. Typical enterprise \u201cData Silos\u201d are demolished as all data is made available in a single BigData store.\n\nA Data Lake is designed for change and continuous learning. Rather than seeking consistency, adaptive systems increase learning and experimentation, in the hopes that one novel idea, product, or method will be the one we need in the new world.\n\nAn enterprise has its data guarded by many different organizations. Data is hard to come by and hard to disseminate across the organization. A Data Lake provides access to data across silos because it is impossible to predict which data might be useful."
    },
    {
        "url": "https://medium.com/intuitionmachine/data-lakes-salvation-from-data-integration-complexity-dc23c5c2ef04",
        "title": "Data Lakes \u2014 Salvation from Data Integration Complexity?",
        "text": "I\u2019ve been in the IT industry for 20 years and have encountered all too many proposals to solve the enterprise\u2019s data integration nightmare.\n\nIn the early 90\u2019s there was an book with an intriguing title. Orfali and Harkey had been publishing a series of books that had cartoonish aliens on the cover. They had a book called \u201cThe Essential Distributed Objects Survival Guide\u201d, which apparently won the 1996 software productivity award. In this book, you will find wild claims about COBRA 2.0 coined \u201cThe Intergalactic Object Bus\u201d. This was Client Server technology improved by Object Oriented technology. It was expected that it would untangle the Gordian knot of enterprise integration.\n\nBy the 2000\u2019s CORBA was all but forgotten. The World Wide Web had taken over and a new kind of web technology, the concept of Web Services were created and an entire consortium began conjuring up WS-I standards to support the new paradigm dubbed Service Oriented Architecture (SOA). That set of technologies was subsequently replaced by a much simpler technology called ReST. Developers realized that the additional complexity of WS-I was not worth the trouble and reverted back to the native web.\n\nCORBA and WS-I SOA are both solutions to the data integration problem via synchronous remote procedure technology. However, in parallel to these developments, there existed an alternative solution based on asynchronous message passing. These were based on message queueing (MQ), that lead to Message Oriented Middleware (MOM) that eventually led to the notion of the Enterprise Service Bus (ESB).\n\nIn parallel to all this were developments in the database realm like Data Warehousing and Virtual/Federated Databases. Over the years, many ways to skin the cat were created, all beginning with point to point solutions, only to realize the benefit of a common canonical/universal mechanism then finally only to fall into the trap of unmanageable complexity. Going from a N\u00b2 implementation to N implementation apparently requires just too much work. Strategies to boil the ocean don\u2019t really work unless that ocean happens to be a pond.\n\nThe last 3 years (Hadoop initial release was on December 2011 ) have seen the emergence of a new kind of IT system suited for \u2018Big Data\u2019. These are massively parallel compute and storage systems designed to tackle the extremely large data problems. The kind of large data problems that made companies like Google and Facebook possible. Could this new kind of technology the antidote to the ills of enterprise data integration? Big Data systems were not originally designed to tackle the problem of integrating heterogenous IT systems. The IT world continues to marvel at the technology and prowess of web scale companies like Google. Furthermore, the Data Lake approach provides promising hints that Big Data may in fact be the salvation from unmanaged complexity."
    }
]