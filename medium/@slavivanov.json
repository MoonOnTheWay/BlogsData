[
    {
        "url": "https://blog.slavv.com/identifying-churn-drivers-with-random-forests-65bad0193e6b?source=user_profile---------1----------------",
        "title": "Identifying churn drivers with Random Forests \u2013",
        "text": "At RetainKit, we aim to tackle the challenging problem of churn at SaaS companies by using AI and machine learning. If you run a SaaS company and you have churn issues, we\u2019d be happy to talk to you and see if our product could help. You can also follow us on Product Hunt Upcoming.\n\nIn the early days of Post Planner (my previous startup), everything was going fine, except that it wasn\u2019t. We had built a product that solved a problem, or so we thought. Customers were signing up in numbers. We had even reached a break-even point, something crucial for a bootstrapped company. And yet we weren\u2019t growing.\n\nAs most companies focused on growth, growth, growth, we had made a rookie mistake. We only cared about the number of new customers coming in (acquisition) but didn\u2019t pay much attention to customers leaving (churn).\n\nIt was a problem because we didn\u2019t know why people were leaving \u2014 was Post Planner too expensive for them? Did we truly solve their problem? Or was it something else we were unaware of? Also, the more customers we got through the front door, the more people left. It felt like we had a very leaky bucket.\n\nAt Post Planner, eventually our focus shifted to balance acquisition and churn reduction. And with great effort, we managed to get out of this particular hole.\n\nHowever, the importance of customer retention and the cost of ignoring it stuck with me. Now, years later, I\u2019m working to help SaaS companies minimize churn using the power of Machine Learning.\n\nMy new project, called RetainKit, utilizes product usage and customer data to predict which customers might be leaving soon, and why.\n\nIn this blog post, I\u2019m going to outline some of the techniques we use in RetainKit to analyze the \u201cWhy\u201d of churn."
    },
    {
        "url": "https://blog.slavv.com/dense-sparse-dense-training-fe0eec0e20de?source=user_profile---------2----------------",
        "title": "Dense-Sparse-Dense Training \u2013",
        "text": "In this post, I\u2019m going to share my notes on a fascinating technique for training neural networks. It describes a method that increases accuracy on already state-of-the-art models.\n\nThe research was published more than a year ago by researchers from Stanford, Nvidia, Baidu, and Facebook. It is rare to see people from many institutions working together on a single paper. I must say the results we will discuss below speak for themselves.\n\nIt\u2019s also worth noting that Song Han, one of the leading DSD authors, maintains a repository of pretrained DSD Caffe models.\n\nAt the same time, independent researchers from Singapore and China, discovered and published the same findings as in the DSD paper. This goes to show how quickly the field of Deep Learning is moving nowadays. I\u2019d recommend you read both if you are interested in the details.\n\nIn this article, I\u2019m going to refer to both Dense-Sparse-Dense training and Skinny Deep Neural Networks as DSD."
    },
    {
        "url": "https://blog.slavv.com/picking-a-gpu-for-deep-learning-3d4795c273b9?source=user_profile---------3----------------",
        "title": "Picking a GPU for Deep Learning \u2013",
        "text": "Deep Learning (DL) is part of the field of Machine Learning (ML). DL works by approximating a solution to a problem using neural networks. One of the nice properties of about neural networks is that they find patterns in the data (features) by themselves. This is opposed to having to tell your algorithm what to look for, as in the olde times. However, often this means the model starts with a blank state (unless we are transfer learning). To capture the nature of the data from scratch the neural net needs to process a lot of information. There are two ways to do so \u2014 with a CPU or a GPU.\n\nThe main computational module in a computer is the Central Processing Unit (better known as CPU). It is designed to do computation rapidly on a small amount of data. For example, multiplying a few numbers on a CPU is blazingly fast. But it struggles when operating on a large amount of data. E.g., multiplying matrices of tens or hundreds thousand numbers. Behind the scenes, DL is mostly comprised of operations like matrix multiplication.\n\nAmusingly, 3D computer games rely on these same operations to render that beautiful landscape you see in Rise of the Tomb Raider. Thus, GPUs were developed to handle lots of parallel computations using thousands of cores. Also, they have a large memory bandwidth to deal with the data for these computations. This makes them the ideal commodity hardware to do DL on. Or at least, until ASICs for Machine Learning like Google\u2019s TPU make their way to market.\n\nFor me, the most important reason for picking a powerful graphics processor is saving time while prototyping models. If the networks train faster the feedback time will be shorter. Thus, it would be easier for my brain to connect the dots between the assumptions I had for the model and its results.\n\nSee Tim Dettmers\u2019 answer to \u201cWhy are GPUs well-suited to deep learning?\u201d on Quora for a better explanation. Also for an in-depth, albeit slightly outdated GPUs comparison see his article \u201cWhich GPU(s) to Get for Deep Learning\u201d.\n\nThere are main characteristics of a GPU related to DL are:\n\nThere are two reasons for having multiple GPUs: you want to train several models at once, or you want to do distributed training of a single model. We\u2019ll go over each one.\n\nTraining several models at once is a great technique to test different prototypes and hyperparameters. It also shortens your feedback cycle and lets you try out many things at once.\n\nDistributed training, or training a single network on several video cards is slowly but surely gaining traction. Nowadays, there are easy to use approaches to this for Tensorflow and Keras (via Horovod), CNTK and PyTorch. The distributed training libraries offer almost linear speed-ups to the number of cards. For example, with 2 GPUs you get 1.8x faster training.\n\nPCIe Lanes (Updated): The caveat to using multiple video cards is that you need to be able to feed them with data. For this purpose, each GPU should have 16 PCIe lanes available for data transfer. Tim Dettmers points out that having 8 PCIe lanes per card should only decrease performance by \u201c0\u201310%\u201d for two GPUs.\n\nFor a single card, any desktop processor and chipset like Intel i5 7500 and Asus TUF Z270 will use 16 lanes.\n\nHowever, for two GPUs, you can go 8x/8x lanes or get a processor AND a motherboard that support 32 PCIe lanes. 32 lanes are outside the realm of desktop CPUs. An Intel Xeon with a MSI \u2014 X99A SLI PLUS will do the job.\n\nFor 3 or 4 GPUs, go with 8x lanes per card with a Xeon with 24 to 32 PCIe lanes.\n\nTo have 16 PCIe lanes available for 3 or 4 GPUs, you need a monstrous processor. Something in the class of or AMD ThreadRipper (64 lanes) with a corresponding motherboard.\n\nAlso, for more GPUs you need a faster processor and hard disk to be able to feed them data quickly enough, so they don\u2019t sit idle.\n\nNvidia has been focusing on Deep Learning for a while now, and the head start is paying off. Their CUDA toolkit is deeply entrenched. It works with all major DL frameworks \u2014 Tensoflow, Pytorch, Caffe, CNTK, etc. As of now, none of these work out of the box with OpenCL (CUDA alternative), which runs on AMD GPUs. I hope support for OpenCL comes soon as there are great inexpensive GPUs from AMD on the market. Also, some AMD cards support half-precision computation which doubles their performance and VRAM size.\n\nCurrently, if you want to do DL and want to avoid major headaches, choose Nvidia.\n\nYour GPU needs a computer around it:\n\nHard Disk: First, you need to read the data off the disk. An SSD is recommended here, but an HDD can work as well.\n\nCPU: That data might have to be decoded by the CPU (e.g. jpegs). Fortunately, any mid-range modern processor will do just fine.\n\nMotherboard: The data passes via the motherboard to reach the GPU. For a single video card, almost any chipset will work. If you are planning on working with multiple graphic cards, read this section.\n\nRAM: It is recommended to have 2 gigabytes of memory for every gigabyte of video card RAM. Having more certainly helps in some situations, like when you want to keep an entire dataset in memory.\n\nPower supply: It should provide enough power for the CPU and the GPUs, plus 100 watts extra.\n\nYou can get all of this for $500 to $1000. Or even less if you buy a used workstation.\n\nHere is performance comparison between all cards. Check the individual card profiles below. Notably, the performance of Titan XP and GTX 1080 Ti is very close despite the huge price gap between them."
    },
    {
        "url": "https://blog.slavv.com/differential-learning-rates-59eff5209a4f?source=user_profile---------4----------------",
        "title": "Differential Learning Rates \u2013",
        "text": "Differential Learning Rates (LR) is a proposed technique for faster, more efficient transfer learning. Below, its effectiveness is tested along with other LR strategies.\n\nIf you know your Deep Learning: the general idea is to use a lower Learning Rate for the earlier layers, and gradually increase it in the latter layers. However, this does not always work (see below for experiments), and mostly achieves the same loss as a single fixed learning rate.\n\nIf you don\u2019t fully understand what this means, don\u2019t worry, we will go into detail and explain it all below. We will also run benchmarks on some Kaggle datasets.\n\nThe idea comes from Jeremy Howard\u2019s course \u201cPractical Deep Learning for coders, part 1, version 2\u201d. Try saying this 3 times in a row.\n\nThe course is taught at the Data Institute at the University of San Francisco. I\u2019m attending the lectures online. Recorded videos are not available yet, but might be by the time you are reading this. So check Fast.AI for availability.\n\nJeremy introduced the idea of having different learning rates (LR) for different parts of the model. I haven\u2019t seen this mentioned elsewhere, although I haven\u2019t done a proper review of the literature. If you know a paper which elaborates on LR for transfer learning, let me know in the comments below.\n\nTransfer Learning is the process of applying a trained Machine Learning model to a different, but related task. This works well in Deep Learning, which uses Neural Networks consisting of layers. Especially in computer vision, earlier layers in these networks tend to learn to recognize more general features. For example, they detect things like edges, gradients, etc.\n\nThese learned feature recognizers are called filters or kernels. The more similar is the new task to the original one, the more filters we should be able to reuse.\n\nEven for completely unrelated datasets, Stanford\u2019s CS231 points out that it\u2019s beneficial to start with a pretrained model.\n\nCheck my previous post for a better Transfer Learning explanation.\n\nThe existing model then should be fine-tuned. This is the process of training the network to better fit the new dataset. It\u2019s achieved by freezing (i.e., not training) earlier layers and using a smallish Learning Rate (LR) for the trainable layers.\n\nIn the pretrained model, the layers closer to the input are more likely to have learned more general features. Thus, we don\u2019t want to change them much. For these early layers, we set the LR to be very low. We increase the LR per layer gradually as we move deeper into the model.\n\nWe will apply differential LR to 3 classification tasks \u2014 dog breeds, satellite photos and medical images.\n\n20% of each dataset is set aside for validation.\n\nWe will use the ResNet 50 pretrained on ImageNet 1000, as our base model. I chose it because it trains fast. Also, the learned filters work great for Transfer Learning.\n\nThe network will have the last layer replaced to account for the different number of categories for each dataset. Then, only this layer is trained for 1 epoch. From there, each experiment described below is performed for 30 epochs.\n\nWe will perform Differential Learning Rates in 2 ways:\n\nThere are 110 Trainable layers in ResNet 50. These are Convolution, Batch Normalization, and Fully Connected layers.\n\nWe will also compare these more traditional LR approaches:\n\nWe will run experiments on 3 datasets coming from Kaggle competitions.\n\nThis dataset comes from a playground competition to classify dogs into 120 breeds. Each category has about 100 images. This dataset is really fun to work with. I mean, who doesn\u2019t like to look at pictures of dogs all day long. Of all three datasets, this one is the most similar to ImageNet. Therefore, we should change the earlier convolutional layers in the base model very little, if at all.\n\nAugmentations applied to this dataset are: random rotate (up to 10 degrees); random brightness adjustment of (-0.05, +0.05); random left/right flip.\n\nThe Planet company has a constellation of satellites in orbit. Daily they take pictures of the entire earth surface of the earth, at 3\u20135 meters per pixel resolution. The dataset focused on understanding where deforestation and illegal logging happened. It contains 40,479 images from the Amazon region, taken from space.\n\nThe nature of satellite imagery is different than ImageNet. But we might still find useful features in the pretrained model, especially in the early layers.\n\nAugmentations applied are random rotate (up to 9 degrees); random zoom; random brightness adjustment of (-0.05, +0.1), random Dihedral transforms (left/right and up/down flips, 45-degree rotation).\n\nThis interesting dataset focuses on detecting signs of diabetic retinopathy in retina scans. There are 35,126 images in 5 classes. Images of the retina are quite unlike ImageNet.\n\nThe same augmentations as in the Planet dataset are applied."
    },
    {
        "url": "https://blog.slavv.com/a-gentle-intro-to-transfer-learning-2c0b674375a0?source=user_profile---------5----------------",
        "title": "A Gentle Intro to Transfer Learning \u2013",
        "text": "Nowadays most applications of Deep Learning rely on Transfer Learning. This is especially true in the domain of Computer Vision. We will explore what Transfer Learning is, how to do it and what the potential pitfalls are. To do this, we\u2019ll go on a little startup quest.\n\nIn a otherwise uneventful day, a friend approaches us with a revolutionary idea. He wants to build the next big thing in social media: Facebook for pets. There people would upload pictures of their pets and enjoy others\u2019 pet pictures. Naturally, we jump onboard.\n\nWe quickly raise funding and recruit people to help us to build it. After a round of customer interviews, we realize that cat people don\u2019t really enjoy looking at dog photos. Also, it turns out parrot owners plainly hate cats and folks who have a goldfish only like to look at other fish.\n\nSince this is 2017, we don\u2019t want people to tell us manually what is in the picture they are uploading. Instead, we will use the power of Machine Learning to recognize the animal in each image. So we\u2019d be able to group them into separate sections of our app."
    },
    {
        "url": "https://blog.slavv.com/embrace-the-robot-apocalypse-d430863b0a91?source=user_profile---------6----------------",
        "title": "Embrace the Robot Apocalypse \u2013",
        "text": "I recently gave a talk on the threats of Artificial intelligence, Machine Learning and Deep Learning. It was presented on the annual conference of the Bulgarian Project Managers Association.\n\nIt\u2019s titled \u201cEmbrace the Robot Apocalypse\u201d. I talk about Elon Musk, media hype, machines making paperclips, hype cycles, AI winters and a lot more. Of course, the great threats of AI are discussed:\n\nI also share many impressive Machine Learning demos, mostly in the field of computer vision.\n\nIt might be curious for someone, so I thought I\u2019ll share it. It can be viewed online (or downloaded) on OneDisk (it takes a bit to load because of the videos). Make sure to check out the notes for each slide to get the context.\n\nDo you agree with what I shared in the presentation? Do you think I\u2019m wrong? Let me know by leaving a reply below.\n\nIf you liked this article, please help others find it by holding that clap icon for a while. Thank you!"
    },
    {
        "url": "https://blog.slavv.com/creating-datasets-from-google-images-206936af44a9?source=user_profile---------7----------------",
        "title": "Creating datasets from Google Images \u2013",
        "text": "Building a classifier that knows whether something is an art object (Art) or not (Fart) requires the having images in 2 categories:\n\nA quick search turned up lists of various types of art, which became the queries to be scraped off a search engine. Here are some of them:\n\nThe list contained about 120 categories of art. But it was by no means complete and exhaustive, as you will later see.\n\nDeploying the Serverless app does a lot of things (creates the Lambda functions, uploads their code, sets permissions, creates the Dynamo DB tables, sets the tables to autoscale, and connects the functions triggers). All with a simple command:\n\nThe art categories are then loaded into a Dynamo DB table (using this simple script). Each time an item is inserted (or removed or updated) into this table, a script will start and collect the image URLs for this category. We will call it the \u201cQuery Scraper\u201d.\n\nSo then, you have the Query Scraper feeding these queries into a search engine (Google Photos or Bing Image Search API) using Phantom.js and Selenium. Then the Lambda script navigates to the end of the page to trigger new pictures to load. Also, the scraper clicks the \u201cMore results\u201d button when needed until there are no more new images. Then the script collects the links to all result images and stores them.\n\nA particular aspect of Lambda is that each script can run for a maximum of 5 minutes. Since we want to download as many pictures as we can for each category, getting the image URLs and downloading them doesn\u2019t happen in a single Lambda run.\n\nInstead, a second Dynamo DB table is used, where the Query Scraper will store the URLs of images that it finds. Inserting in this table will kickoff another script, again being run on Lambda. It will download the picture and store it on S3 for later retrieval.\n\nAfter the Lambda functions have done their magic, we have about 1000 images per category or 120k pictures total.\n\nIf you want to train locally you can download the dataset (using this script), or leave it in S3 if you\u2019ll be training with Amazon\u2019s cloud.\n\nHere is sample of the images scraped. We see that most of the images do indeed represent art."
    },
    {
        "url": "https://blog.slavv.com/37-reasons-why-your-neural-network-is-not-working-4020854bd607?source=user_profile---------8----------------",
        "title": "37 Reasons why your Neural Network is not working \u2013",
        "text": "Check if the input data you are feeding the network makes sense. For example, I\u2019ve more than once mixed the width and the height of an image. Sometimes, I would feed all zeroes by mistake. Or I would use the same batch over and over. So print/display a couple of batches of input and target output and make sure they are OK.\n\nTry passing random numbers instead of actual data and see if the error behaves the same way. If it does, it\u2019s a sure sign that your net is turning data into garbage at some point. Try debugging layer by layer /op by op/ and see where things go wrong.\n\nYour data might be fine but the code that passes the input to the net might be broken. Print the input of the first layer before any operations and check it.\n\nCheck if a few input samples have the correct labels. Also make sure shuffling input samples works the same way for output labels.\n\nMaybe the non-random part of the relationship between the input and output is too small compared to the random part (one could argue that stock prices are like this). I.e. the input are not sufficiently related to the output. There isn\u2019t an universal way to detect this as it depends on the nature of the data.\n\nThis happened to me once when I scraped an image dataset off a food site. There were so many bad labels that the network couldn\u2019t learn. Check a bunch of input samples manually and see if labels seem off.\n\nThe cutoff point is up for debate, as this paper got above 50% accuracy on MNIST using 50% corrupted labels.\n\nIf your dataset hasn\u2019t been shuffled and has a particular order to it (ordered by label) this could negatively impact the learning. Shuffle your dataset to avoid this. Make sure you are shuffling input and labels together.\n\nAre there a 1000 class A images for every class B image? Then you might need to balance your loss function or try other class imbalance approaches.\n\nIf you are training a net from scratch (i.e. not finetuning), you probably need lots of data. For image classification, people say you need a 1000 images per class or more.\n\nThis can happen in a sorted dataset (i.e. the first 10k samples contain the same class). Easily fixable by shuffling the dataset.\n\nThis paper points out that having a very large batch can reduce the generalization ability of the model.\n\nThanks to @hengcherkeng for this one:"
    },
    {
        "url": "https://blog.slavv.com/the-1700-great-deep-learning-box-assembly-setup-and-benchmarks-148c5ebe6415?source=user_profile---------9----------------",
        "title": "The $1700 great Deep Learning box: Assembly, setup and benchmarks",
        "text": "A sensible budget for me would be about 2 years worth of my current compute spending. At $70/month for AWS, this put it at around $1700 for the whole thing.\n\nYou can check out all the components used. The PC Part Picker site is also really helpful in detecting if some of the components don\u2019t play well together.\n\nThe GPU is the most crucial component in the box. It will train these deep networks fast, shortening the feedback cycle.\n\nDisclosure: The following are affiliate links, to help me pay for, well, more GPUs.\n\nThe choice is between a few of Nvidia\u2019s cards: GTX 1070, GTX 1070 Ti, GTX 1080, GTX 1080 Ti and finally the Titan X. The prices might fluctuate, especially because some GPUs are great for cryptocurrency mining (wink, 1070, wink).\n\nOn performance side: GTX 1080 Ti and Titan X are similar. Roughly speaking the GTX 1080 is about 25% faster than GTX 1070. And GTX 1080 Ti is about 30% faster than GTX 1080. The new GTX 1070 Ti is very close in performance to GTX 1080.\n\nTim Dettmers has a great article on picking a GPU for Deep Learning, which he regularly updates as new cards come on the market.\n\nHere are the things to consider when picking a GPU:\n\nConsidering all of this, I picked the GTX 1080 Ti, mainly for the training speed boost. I plan to add a second 1080 Ti soonish.\n\nEven though the GPU is the MVP in deep learning, the CPU still matters. For example, data preparation is usually done on the CPU. The number of cores and threads per core is important if we want to parallelize all that data prep.\n\nTo stay on budget, I picked a mid-range CPU, the Intel i5 7500. It\u2019s relatively cheap but good enough to not slow things down.\n\nEdit: As a few people have pointed out: \u201cprobably the biggest gotcha that is unique to DL/multi-GPU is to pay attention to the PCIe lanes supported by the CPU/motherboard\u201d (by Andrej Karpathy). We want to have each GPU have 16 PCIe lanes so it eats data as fast as possible (16 GB/s for PCIe 3.0). This means that for two cards we need 32 PCIe lanes. However, the CPU I have picked has only 16 lanes. So 2 GPUs would run in 2x8 mode (instead of 2x16). This might be a bottleneck, leading to less than ideal utilization of the graphics cards. Thus a CPU with 40 lines is recommended.\n\nEdit 2: However, Tim Dettmers points out that having 8 lanes per card should only decrease performance by \u201c0\u201310%\u201d for two GPUs. So currently, my recommendation is: Go with 16 PCIe lanes per video card unless it gets too expensive for you. Otherwise, 8 lanes should do as well.\n\nA good solution with to have for a double GPU machine would be an Intel Xeon processor like the E5\u20131620 v4 (40 PCIe lanes). Or if you want to splurge go for a higher end processor like the desktop i7\u20136850K.\n\nIt\u2019s nice to have a lot of memory if we are to be working with rather big datasets. I got 2 sticks of 16 GB, for a total of 32 GB of RAM, and plan to buy another 32 GB later.\n\nFollowing Jeremy Howard\u2019s advice, I got a fast SSD disk to keep my OS and current data on, and then a slow spinning HDD for those huge datasets (like ImageNet).\n\nSSD: I remember when I got my first Macbook Air years ago, how blown away was I by the SSD speed. To my delight, a new generation of SSD called NVMe has made its way to market in the meantime. A 480 GB MyDigitalSSD NVMe drive was a great deal. This baby copies files at gigabytes per second. \n\nHDD: 2 TB Seagate. While SSDs have been getting fast, HDD have been getting cheap. To somebody who has used Macbooks with 128 GB disk for the last 7 years, having this much space feels almost obscene.\n\nThe one thing that I kept in mind when picking a motherboard was the ability to support two GTX 1080 Ti, both in the number of PCI Express Lanes (the minimum is 2x8) and the physical size of 2 cards. Also, make sure it\u2019s compatible with the chosen CPU. An Asus TUF Z270 did it for me.\n\nMSI \u2014 X99A SLI PLUS should work great if you got an Intel Xeon CPU.\n\nRule of thumb: Power supply should provide enough juice for the CPU and the GPUs, plus 100 watts extra. \n\nThe Intel i5 7500 processor uses 65W, and the GPUs (1080 Ti) need 250W each, so I got a Deepcool 750W Gold PSU (currently unavailable, EVGA 750 GQ is similar). The \u201cGold\u201d here refers to the power efficiency, i.e how much of the power consumed is wasted as heat.\n\nThe case should be the same form factor as the motherboard. Also having enough LEDs to embarrass a Burner is a bonus.\n\nA friend recommended the Thermaltake N23 case, which I promptly got. No LEDs sadly.\n\nHere is how much I spent on all the components (your costs may vary):\n\nAdding tax and fees, this nicely matches my preset budget of $1700."
    },
    {
        "url": "https://blog.slavv.com/learning-machine-learning-on-the-cheap-persistent-aws-spot-instances-668e7294b6d8?source=user_profile---------10----------------",
        "title": "Learning Machine Learning on the cheap: Persistent AWS Spot Instances",
        "text": "First things first. Let\u2019s learn how to create a spot instance where we will be able to develop and run ML models. We want to use P2 instances. They come with one or more powerful NVIDIA K80 GPUs with lots of memory (11 GB) to test and train your models on. P2 comes in 3 sizes:\n\nLet\u2019s see how we can actually get one ourselves.\n\nBefore we can start any P2 instances, we need to setup a Virtual Private Cloud (VPC). Which is just a fancy virtual network to launch your virtual machine in. Setting up a VPC can be a little intimidating. It certainly was for me when I first did it, and the details are still a bit fuzzy. Good news is it has to be done only once. One way to approach this is to follow Amazon\u2019s guide.\n\nA better approach would be to use scripts adapted from Fast.ai\u2019s course Deep Learning For Coders. If you got the helper scripts from Needed Tools above, simply run the following:\n\nThis will create a VPC, Internet Gateway, Subnet, Route Table, Security Group and most importantly a Key Pair. We will use the newly created key (located at )to connect to the instance we are about to create. It will also print the ID of our newly created Subnet and Security group. We\u2019ll need these for the next step.\n\nWe can follow Amazon\u2019s instructions for launching a spot instance. But because we are cooler than that, we could use a little helper script named to launch the instance.\n\nWe need to pass it the following arguments:\n\nThe script will then print the IP of our new Spot instance.\n\nIf we want, we might also pass the following: volume_size (size of the root volume, in GB. Default 128), key_name (name of the key file we\u2019ll use to log into the instance. Default: aws-key-fast-ai), ec2spotter_instance_type (type of instance to launch. Default p2.xlarge), bid_price (The maximum price we are willing to pay (USD). Default 0.5).\n\nUsing the IP of the Spot instance from the previous step, we can connect via ssh:\n\nNow we can develop and test ML models to our hearts delight. For example, let\u2019s test it with Tensorflow\u2019s tutorial on MNIST:\n\nWe have a Virtual Private Network, a Spot instance instance running in it for a fraction of the price, and even a model training. But all is not roses!\n\nWhat if with hard work and wit, we manage to get the above MNIST script to achieve above state-of-art accuracy. And then we shut our instance for the night. Our great model would be lost. We need to find a way to persist the data on our Spot instances. Luckily, we found two."
    },
    {
        "url": "https://blog.slavv.com/picking-an-optimizer-for-style-transfer-86e7b8cba84b?source=user_profile---------11----------------",
        "title": "Picking an optimizer for Style Transfer \u2013",
        "text": "Have you ever woken up in the middle of the night and wondered whether Gradient Descent, Adam or Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno will optimize your style transfer neural network faster and better?\n\nMe neither, and I didn\u2019t know what half of these words meant until last week. But as I\u2019m participating in part 2 of the excellent Practical Deep Learning For Coders, and nudged by the the man behind the course Jeremy Howard, I figured I\u2019d explore which optimizer is best for this task.\n\nNB: Some knowledge of Convolutional Neural Networks is assumed. I highly recommend part 1 of the course which is the available online for free. It\u2019s the best way to get your feet wet in machine learning (ML).\n\nLet\u2019s start with some of the basics, partly because I was a little unclear of those prior to writing this. If you are familiar with style transfer, you might skim/skip this section.\n\nA: It\u2019s what apps like Prisma and Lucid are doing. Basically, it extracts the style of an image (usually a famous painting) and applies it to the contents of another image (usually supplied by the user).\n\nThe style of an painting is: the way the painter used brush strokes; how these strokes form objects; texture of objects; color palette used.\n\nThe content of the image is what objects are present in this image (person, face, dog, eyes, etc.) and their relationships in space.\n\nHere is an example of style transfer:\n\nA: Using convolutional neural networks(CNNs). Since AlexNet successfully applied CNNs to object recognition (figuring out what object is in an image), and dominated the most popular computer vision competition in 2012, CNNs have been the most popular and effective method for object recognition. They recognize objects by learning layers of filters that build on previous layers. The first layers learn to recognize simple patterns, for example an edge or a corner. Intermediate layers might recognize more complex patterns like an eye or a car tire, and so on. Jason Yosinski shows CNNs in action in this fun video.\n\nIt turns out that the filters in the first layers in CNNs correspond to the style of the painter \u2014 brush strokes, textures, etc. Filters in later layers happen to locate and recognize major objects in the image, such as a dog, a building or a mountain.\n\nBy passing a Picasso painting through a CNN, and noticing how much filters in the first layers (style layers) are activated, we can obtain a representation of the style Picasso used in it. Same thing for the content image, but this time with the filters in the last layers (content layers).\n\nA: Now it gets interesting. We can compute the difference between the styles of two images (style loss) as the difference between the activations of the style filters for each image. Same thing for the difference between content of two images (content loss): it\u2019s the difference between the activations of the filters in the content layers of each image.\n\nLet\u2019s say we want to combine a the style of Picasso painting with a picture of me. The combination image starts off as random noise. As the combination image goes through the CNN, it excites some filters in the style layers and some in the content layers. By summing the style loss between the combination image and the Picasso painting, and the content loss between the combination image and my picture, we get the total loss.\n\nIf we could change the combination image as to minimize the total loss, we would have an image that is as close to both the Picasso painting and that picture of me. We can do this with an optimization algorithm.\n\nA: It\u2019s a way to minimize (or maximize) a function. Since we have a total loss function that is dependent on the combination image, the optimization algorithm will tell us how to change the combination image to make the loss a bit smaller.\n\nThe ones I\u2019ve encountered so far fall in two camps: first- and second-order methods.\n\nFirst-order methods minimize or maximize the function (in our case the loss function) using its gradient. Most widely used first-order method is Gradient Descent and its variants, as illustrated here and explained in Excel(!).\n\nSecond-order method use the second derivative (Hessian) to minimize or maximize the function. Since the second derivative is costly to compute, the second-order method in question, L-BFGS (Limited-memory Broyden\u2013Fletcher\u2013Goldfarb\u2013Shanno) uses an approximation of the Hessian.\n\nIt depends on the task, but I mostly use Adam. Let\u2019s see which one is fastest in style transfer.\n\nSetup: The learning rate in the following experiments is set to 10 which might seem high but works out fine because we are dealing with color intensities between 0 and 255. The rest of the hyperparameters of the optimizers are left at their default values. The tests were performed on a single K80 GPU on Amazon P2 Instance.\n\nWe\u2019ll start off with Picasso and a picture of a beautiful girl. They are both sized 300 by 300 pixels.\n\nWe\u2019ll run the optimizer for a 100 steps. It\u2019s not sufficient to get a good combination image, but will allow us to see which optimizer minimizes the error faster."
    }
]