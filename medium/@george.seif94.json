[
    {
        "url": "https://towardsdatascience.com/data-science-and-machine-learning-interview-questions-3f6207cf040b?source=user_profile---------1----------------",
        "title": "Data Science and Machine Learning Interview Questions",
        "text": "Ah the dreaded machine learning interview. You feel like you know everything\u2026 until you\u2019re tested on it! But it doesn\u2019t have to be this way.\n\nOver the past few months I\u2019ve interviewed with many companies for entry-level roles involving Data Science and Machine Learning. To give you a bit of perspective, I was in graduate school in the last few months of my masters in machine learning and computer vision with most of my previous experience being research/academic, but with 8 months at an early stage startup (unrelated to ML). The roles included work in Data Science, general Machine Learning, and specializations in Natural Language Processing or Computer Vision. I interviewed with big companies like Amazon, Tesla, Samsung, Uber, Huawei, but also with many startups ranging from early-stage to well established and funded.\n\nToday I\u2019m going to share with you all of the interview questions I was asked and how to approach them. Many of the questions were quite common and expected theory, but many others were quite creative and curious. I\u2019m going to simply list the most common ones since there\u2019s many resources about them online and go more in depth into some of the less common and trickier ones. I hope in reading this post that you can get great at Machine Learning interviews and land your dream job!\n\nThere you have it! All of the interview questions I got when apply for roles in Data Science and Machine Learning. I hope you enjoyed this post and learned something new and useful! If you did, feel free to hit the clap button."
    },
    {
        "url": "https://hackernoon.com/a-comprehensive-design-guide-for-image-classification-cnns-46091260fb92?source=user_profile---------2----------------",
        "title": "A Comprehensive Design Guide for Image Classification CNNs",
        "text": "This one\u2019s easy; of course adding more layers will usually increase accuracy at the expense of speed and memory. However, something to be aware of is that this tradeoff is governed by the law of diminishing returns i.e the more layers we add, the less increase in accuracy each layer gives us individually.\n\nRecent advances in the general design of CNNs have presented some awesome alternatives that can speed up CNN run time and reduce memory consumption without too much loss in accuracy. All of these can be integrated into any of the above CNN network types quite easily:\n\nThere\u2019s a very clear tradeoff between the network type and the three metrics. First off, you\u2019re definitely want to go with either Inception or ResNet style design. They\u2019re newer than VGGNet and AlexNet and simply offer a much more relaxed tradeoff between speed and accuracy (as you can see in the graph above). Justin Johnson from Stanford provides some great benchmarks for a few of these CNNs.\n\nThis post will be your design guide to properly custom designing a CNN for your specific classification task. In particular, we\u2019re going to focus on the three main metrics of accuracy, speed, and memory consumption. We\u2019ll look at the many different classification CNNs and explore their properties with regards to those three metrics. We\u2019ll also look at the various modifications we could possibly make to those base CNNs and how they effect our metrics. In the end, you\u2019ll learn how to optimally design a CNN for your specific image classification task!\n\nWhen selecting a CNN for your image classification task, there are really 3 main metrics that you are trying to optimize: accuracy, speed, and memory consumption. The performance on these metrics are going to depend on which classification CNN you select and any modifications you make to it. The different networks such as VGG, Inception, and ResNets all come with their own tradeoffs on these metrics. In addition, you could modify those architectures, say by pruning some layers, adding more layers, using dilation within the network, or different network training techniques.\n\nSo you want to do an image classification but don\u2019t know where to start. Which pre-trained network do you use? How do you modify it to suit your needs? Should your network have 20 layers or 100? Which ones are the fastest? Most accurate? These are all questions that come up when trying to choose the best CNN for your image classification task.\n\nThere\u2019s been a lot of debate lately about this one. However, a great rule of thumb to follow is to start off with ReLU. Using ReLU will often get you some good results right off the bet without any tedious tuning like you would need with ELU, PReLU, or LeakyReLU. Once you\u2019ve determined that your design is working pretty well with ReLU, then you can play around with the others and tune their parameters to try and crank out that last bit of accuracy.\n\nOne might think that using larger convolution kernels will always yield the highest accuracy while losing out on speed and memory. This however, is not the case as it has been repeatedly found that using larger kernels makes it difficult for the network to diverge. It\u2019s more optimal to stick with many smaller kernels like 3x3. Both ResNet and VGGNet explain and demonstrate this quite thoroughly. You can also use 1x1 kernels as bottleneck layers to reduce the number of feature maps, again as those two papers show.\n\nDilated convolutions use spacing in-between the weights of the convolution kernels to be able to use pixels that are far away from the center. This allows for exponential expansion of the network receptive field without increasing the parameter count i.e without increasing the memory consumption at all. It has been shown that using dilated convolutions increases the network accuracy with a minor speed trade-off.\n\nYou should pretty much always be doing data augmentation. Using more data has been shown to consistently increase performance, even up to an extreme amount. With augmentation, you\u2019re getting more data for free. Now the type of augmentation you use will depend on your application. For example, if you\u2019re doing a self driving car application, you\u2019re probably not going to have upside trees, cars, and buildings, so it doesn\u2019t make sense to flip your images vertically. However, you\u2019ll definitely be encountering changes in lighting from the weather and some changes across the scene, so it makes sense to augment your data with lighting changes and horizontal flips. Check out this awesome data augmentation library.\n\nWhen you finally want to train your network, there are several optimization algorithms to choose from. Many people say that SGD gets you the best results with regards to accuracy, which in my experience is true. However, tuning the learning rate schedule and parameters can be challenging and tedious. On the other hand, using and adaptive learning rate such as Adam, Adagrad, or Adadelta is quick and easy, but you might not get that optimal accuracy of SGD.\n\nThe best thing here is to follow the same \u201cstyle\u201d as the activation functions: go with the easy one first to see if your design works well, then tune and optimize using something more complex. I would personally recommend starting off with Adam, as in my experience it\u2019s super easy to use: just set a learning rate that\u2019s not super high, commonly default at 0.0001 and you\u2019ll usually get some very good results! Later on you can use SGD from scratch or even start with Adam, then fine tune with SGD. In fact, this paper found that switching from Adam to SGD mid-training achieves the best accuracy in the easiest way! Check out the figure below from the paper:\n\nThere are many cases where you will be dealing with imbalanced data, especially in real-world applications. Take a simple but real-world example: You are training your deep network to predict whether someone in a video feed is holding a lethal weapon or not, for security reasons. BUT in your training data, you only have 50 videos of people holding weapons, and 1000 videos of people without weapons! If you just train your network right away with this data, your model will definitely be highly biased towards predicting that no one ever has a weapon!\n\nThere are a few things you can do to combat this:\n\nFor most applications, it is suitable and really quite optimal to use transfer learning rather than training your network from scratch. However, a choice still remains about which network layers you will keep and which you will retrain. This will really depend on what exactly your data looks like. The more similar your data is to that of the pre-trained network (which is usually trained on ImageNet), the less layers you should be re-training, and vice versa. For example, lets say you\u2019re trying to classify if an image contains grapes or not, so you have a bunch of images with grapes and a bunch without. Those kinds of images are quite similar to those in ImageNet and so you\u2019ll only need to re-train the last couple of layers, maybe just the fully-connected layers. However, lets say you\u2019re trying to classify whether or not an image of outer space contains a planet in it. Well such data is much different than that of ImageNet, so you\u2019ll want to re-train some of the later convolutional layers as well. In a nutshell, go with the following rule:\n\nThere you have it! Your comprehensive guide to designing a CNN for your image classification application. I hope you enjoyed this post and learned something new and useful. If you did, feel free to hit the clap button."
    },
    {
        "url": "https://medium.com/swlh/henry-ford-my-life-and-work-lessons-from-one-of-americas-all-time-greatest-entrepreneurs-f683941b7575?source=user_profile---------3----------------",
        "title": "Henry Ford: My Life and Work \u2014 Lessons from one of America\u2019s all time greatest entrepreneurs",
        "text": "Henry Ford: My Life and Work is an autobiography by Henry Ford, one of America\u2019s all time greatest entrepreneurs. Henry Ford was the founder of Ford Motor Company. He was largely responsible for the development of the assembly line technique of mass production, and as a result Ford Motor Company was the first to bring a revolutionary yet affordable car to the masses.\n\nThe autobiography goes through Henry Ford\u2019s career, beginning with the initial inception of his idea for building a motor car. Throughout the book, Ford gives a very raw, real, and transparent view of his life, and in particular his thought processes when making major decisions. You can really see how Ford was not only a successful businessman, but also an excellent engineer, tactician, and strategist. Combined with these technical knowledge he was also skilled with handling and connecting with people: everyday communicating, marketing, sales, and everything in between.\n\nIn this post, I\u2019m going to share with you some of the amazing life and work lessons you can learn from reading Ford\u2019s autobiography. We\u2019ll take a look at these lessons in detail by drawing connections to examples in the book, examples that display Henry Ford\u2019s brilliance in living by these lessons\u2019 principles. What you may find is that Ford was no different than any other ordinary man in the outset, but had the utmost dedication, commitment, and passion throughout his life and work. Let\u2019s dive in and learn from Henry Ford!\n\nFord was always interested in machinery and automation. Originally, his father, William Ford, wanted him to take over the farm. Ford disliked farm work and went on to work at Edison Illuminating Company as an engineer. To Ford, such a job involved much more intelligent thinking than the repetitive and mechanical farm work.\n\nFord\u2019s first idea for automation actually goes back to solving his own original problem of having to do inefficient farm work. It didn\u2019t make sense to him to be doing work in such a repetitive and manual manner as he did on the farm. He thought there must be some way to make this more efficient, to not require such extensive and inefficient manual labor. My guess is that part of these thoughts come from his experience as an engineer where it was really his job to make things more efficient.\n\nFord drew inspiration and ideas from his own problems. He saw that his own farm work was inefficient, and so must everyone else\u2019s be since they\u2019re doing the exact same work he was. He knew that it wasn\u2019t such a crazy idea to try and build such a machine when after all it was really part of his job as an engineer to make things more efficient. He knew he was doing something of value to people. He knew that a machine like that would benefit not only himself, but millions of people across America. It\u2019s what kept him going even when he first failed, because he knew his workers and his customers were relying on him.\n\nAt the same time, all of that passion was real and genuine. When people hear the word passion many of think they don\u2019t really have one, or that their passion simply won\u2019t make them a living\u2026. That\u2019s simply not true. Think about what you love doing and see how you can use your skills and passion for your craft to help people and add value to the world. I love reading and learning in general, so I share as much as I can of what I\u2019ve learned by writing here and speaking with people because I think they can also draw some value from these books. You can do the same with your passion, you just have to have the courage to commit to it.\n\nWhen Ford was designing his first motorized vehicle in his own home workshop, he realized that at some point he would have to show it to people to get some feed back on it. He knew that making a motorized machine that made work and transportation more efficient would be hugely beneficial, but he still had to get people to believe in his idea. The first model he showed people was largely rejected; they just really didn\u2019t see the value in it. They said \u201cwhy would I need such a machine when I can just do the work fast enough myself.\u201d\n\nAs much as technically minded people (including myself sometimes) like to think, products unfortunately do not just sell themselves. Sometimes, you are pointing directly to the value your product gives with flashing red lights and sound effects around it, yet customers still won\u2019t see it. So Ford put himself in those people\u2019s shoes: they were so used to farm life, that when presented with a solely technical alternative, they could not see the benefit. His idea was totally new and as such he needed a different way of selling it.\n\nBut, Henry Ford knew the key, that the one thing that doesn\u2019t change is human nature. He knew that people always bought in to things that were trendy and sexy, things that they could show off to their friends and family or even competitors. As Ford famously said: \u201cIf I had asked people what they wanted, they would have said faster horses.\u201d He knew what they wanted was something cool and trendy, but what they needed was something functional and efficient which he wanted to deliver through his product.\n\nSo Ford\u2019s new car became a racing car, one that had a more sporty and sexy look. When brought out to the race track, people were amazed at how great and fast it looked, even though Ford knew it was less than ideally efficient. As a result, Ford\u2019s work had now gained traction, people payed attention to him, and he could starting bringing in partners and funding.\n\nFord first sold what people wanted, even though he knew what they needed was different. He understood human nature and used that to gain attention for his work and further build up the value of his product. There\u2019s always an underlying human aspect to every product creation and sale. Understanding this concept will help in connecting your product with the market.\n\nPerhaps Ford\u2019s most famous accomplishment is the assembly line mass production of the Model T. This is the part of the book where you see Ford\u2019s true sharpness and technical skill. His main mission was to bring an affordable automobile to the masses, one that even his workers could afford. To do so, he knew he had to create the most efficient work place in the country.\n\nHe started by recording the assembly time of every step in assembling a single car and optimized each step. He set the height of the work benches to the optimal height, the speed of the conveyor belt to the optimal speed, and every step in between. Ford tested out several different price points to find the one that generated the most sales. He raised his worker\u2019s wage to attract as many of the best as possible from competitors and increase production. Ford Motor Company iterated until they found the best possible way of conducting their business. They had such high demand that they were panicking to find thousands more workers and more factory; definitely a good problem to have!\n\nTake care of the little things and the big things will follow. As Ford says: \u201cNothing is particularly hard if you divide it into small jobs.\u201d\n\nOver the years, Ford\u2019s consistency and unwavering work ethic was unmatched. He always strongly advocated the value of having a good work ethic and that talent wasn\u2019t what it seemed: \u201cGenius is seldom recognized for what it is, a great capacity for hard work.\u201d\n\nWhen Ford first began experimenting with building an automobile, he was still working at Edison Illuminating Company. Yet, in his own spare time and home workshop, he worked, testing out his new ideas and building new things. He was never even sure that things would end up working out, but he had the courage to try. When the Ford Motor Company\u2019s business was flourishing, still he sought to make it even more efficient and better as a company, striving to have Ford Motor Company become a global example of what a good business looks like. He was dedicated to his mission and his craft. He thought of his business as being part of him. He felt that his business and everything that made it: his investors, customers, and workers relied on his work. He needed to set a good leading example for those around him.\n\nDon\u2019t be afraid of hard work. Your work ethic not only effects your success, but the success of those around you. Set a good example like Ford did by always being on the working floor. Lead by example. Be the hardest worker at your company, both for your own sake and for those around you. Commit to your craft and mission, even if it means going the extra mile and doing that extra bit of work after your duties are done. Work hard at adding value and the results will follow.\n\nThere you have it! Life and Work lessons learned from the great Henry Ford. I hope you enjoyed this post and learned something new and useful. If you did, feel free to hit the clap button!"
    },
    {
        "url": "https://medium.com/swlh/founders-at-work-what-we-can-learn-from-observing-the-paths-of-startup-founders-32ed17766834?source=user_profile---------4----------------",
        "title": "Founders at Work: What we can learn from observing the paths of startup founders",
        "text": "Founders at work is a book containing interviews with founders of famous technology companies about what happened in the very earliest days. The author interviews the best of the best: Steve Wozniak (Apple), Caterina Fake (Flickr), Mitch Kapor (Lotus), Max Levchin (PayPal), and Sabeer Bhatia (Hotmail), and many more! The great thing about this book is that the interviews aren\u2019t canned and standard, they\u2019re authentic and natural. The author has a really free-flowing conversation with the founders. It is these open and true conversations that we can really learn the most from because we can see past the picture the media paints of these people and see these founders and their experiences in their true colors.\n\nThroughout the book you can really see how the founders are all quite similar and yet very different at the same time! They have some overlapping qualities that helped them succeed such as grit, vision, and courage, but they also all had their own strengths and weakness and unique startup paths. Today, I\u2019m going to share with you some of the key things I learned about startups from reading this book. Some of the lessons were repeated, coming from many of the founder interviews while others were specific to a particular one because of their specific path. In either case there\u2019s a lot to learn from these authentic founder stories. Let\u2019s dive in!\n\nSolve a problem: Good businesses and their products add value to peoples\u2019 lives. The most direct way to give people good value is by solving some problem(s) they have. Paypal made online payments a lot easier for online sellers and hotmail gave the public easy access to email. A lot of the time, startups are formed by creating a solution to the founders\u2019 own problems! So the problem your business solves can be your own at first, but you\u2019ll soon find many others had the same problem, or at least find your solution useful.\n\nJust start and try something: A lot of the time we like to plan, strategize, and \u201csurvey the field\u201d. We spend so much time making this perfect plan and then never actually end up getting started! A finished project is better than a perfect project you never actually end up starting, so\u2026.. start! Create that first product and then see where to go from there based on the outcome of it and user feedback. Your users may want something different than you originally had in mind so you may end up just throwing your perfect plan out of the window. Sometimes, the best way to learn is through trial and error because then you get the real, raw experience.\n\nTalk to everyone: Everyone knows that networking is huge right. The whole \u201cyour network is your network thing\u201d, but some of the startup founders went beyond that. They asked people what their problems and pains were so they could gain information and solve them. They talked to everyone at work, friends, friends of friends, and acquaintances, just to get to know people. You never know when you\u2019ll need that a co-founder, someone to test out your product and give feedback, that key connection, or just a friend to talk to and hang out with to have a balanced life. Being connected with people is valuable both from a \u201cbusiness\u201d standpoint but more importantly, for enriching and bringing more happiness and balance into your life.\n\nInspiration for a business can come from anywhere: Pay attention to technology trends, especially the future potential of that technology and how it can solve people\u2019s pains and problems. Don\u2019t be afraid to do side projects. It can start off as just something out of curiosity, to learn, and eventually you see how valuable it is to other people. Worst case scenario you learn a lot, best scale you scale it up and sell it! Try hacking at your own problems and see what solutions you come up with, you might be surprised what comes of it all.\n\nKISS \u2192 Keep it simple, stupid: KISS is an acronym for \u201cKeep it simple, stupid\u201d as a design principle. The KISS principle states that most systems work best if they are kept simple rather than made complicated; therefore simplicity should be a key goal in design and unnecessary complexity should be avoided. From an engineering standpoint, the less moving parts and components that your system has, the easier it is to design, debug, and scale. First off, don\u2019t worry about those complex and fancy features of your product, build the foundation first! When you actually do build your product, make it as easy and simple to use as possible. Users hate it when things are complicated, you want your product to be dummy proof!\n\nMake it sexy: As much as we\u2019d all like our product\u2019s value to speak for itself, things often don\u2019t work like that. Remember, you\u2019re selling your product to humans; we\u2019re very emotional creatures with complex psychology. We\u2019re naturally drawn to things that look good, that are aesthetic, trendy, really popular etc. If you\u2019re product isn\u2019t sexy it\u2019ll be a hard sell. When Tesla first released their first car, it wasn\u2019t the super efficient yet cheap Model 3. It was the fast and sexy Roadster and they made sure that popular people were seen with it like George Clooney and Leonardo DiCaprio. The point is, in addition to your product giving lots of value, it also needs to be sexy to sell.\n\nThere you have it! Lessons learned from founders and their startups. I hope you enjoyed this post and learned something new and useful. If you did, feel free to give it some claps."
    },
    {
        "url": "https://towardsdatascience.com/deep-learning-vs-classical-machine-learning-9a42c6d48aa?source=user_profile---------5----------------",
        "title": "Deep Learning vs Classical Machine Learning \u2013",
        "text": "Over the past several years, deep learning has become the go-to technique for most AI type problems, overshadowing classical machine learning. The clear reason for this is that deep learning has repeatedly demonstrated its superior performance on a wide variety of tasks including speech, natural language, vision, and playing games. Yet although deep learning has such high performance, there are still a few advantages to using classical machine learning and a number of specific situations where you\u2019d be much better off using something like a linear regression or decision tree rather than a big deep network.\n\nIn this post we\u2019re going to compare and contrast deep learning vs classical machine learning techniques. In doing so we\u2019ll identify the pros and cons of both techniques and where/how they are best used.\n\nThere you have it! Your comparison of Classic Machine Learning and Deep Learning. I hope you enjoyed this post and learned something new and useful. If you did, feel free to give it some claps."
    },
    {
        "url": "https://towardsdatascience.com/5-types-of-regression-and-their-properties-c5e1fa12d55e?source=user_profile---------6----------------",
        "title": "5 Types of Regression and their properties \u2013",
        "text": "Linear and Logistic regressions are usually the first modelling algorithms that people learn for Machine Learning and Data Science. Both are great since they\u2019re easy to use and interpret. However, their inherent simplicity also comes with a few drawbacks and in many cases they\u2019re not really the best choice of regression model. There are in fact several different types of regressions, each with their own strengths and weaknesses.\n\nIn this post, we\u2019re going to look at 7 of the most common types of regression algorithms and their properties. We\u2019ll soon find that many of them are biased to working well in certain types of situations and with certain types of data. In the end, his post will give you a few more tools in your regression tool box and give greater insight into regression models as a whole!\n\nRegression is a technique used to model and analyze the relationships between variables and often times how they contribute and are related to producing a particular outcome together. A linear regression refers to a regression model that is completely made up of linear variables. Beginning with the simple case, Single Variable Linear Regression is a technique used to model the relationship between a single input independent variable (feature variable) and an output dependent variable using a linear model i.e a line.\n\nThe more general case is Multi Variable Linear Regression where a model is created for the relationship between multiple independent input variables (feature variables) and an output dependent variable. The model remains linear in that the output is a linear combination of the input variables. We can model a multi-variable linear regression as the following:\n\nWhere a_n are the coefficients, X_n are the variables and b is the bias. As we can see, this function does not include any non-linearities and so is only suited for modelling linearly separable data. It is quite easy to understand as we are simply weighting the importance of each feature variable X_n using the coefficient weights a_n. We determine these weights a_n and the bias b using a Stochastic Gradient Descent (SGD). Check out the illustration below for a more visual picture!\n\nWhen we want to create a model that is suitable for handling non-linearly separable data, we will need to use a polynomial regression. In this regression technique, the best fit line is not a straight line. It is rather a curve that fits into the data points. For a polynomial regression, the power of some independent variables is more than 1. For example, we can have something like:\n\nWe can have some variables have exponents, others without, and also select the exact exponent we want for each variable. However, selecting the exact exponent of each variable naturally requires some knowledge of how the data relates to the output. See the illustration below for a visual comparison of linear vs polynomial regression."
    },
    {
        "url": "https://medium.com/swlh/marcus-aureliuss-meditations-5-stoic-lessons-for-mental-strength-f81a5cdac7fd?source=user_profile---------7----------------",
        "title": "Marcus Aurelius\u2019s Meditations: 5 Stoic Lessons for Mental Strength",
        "text": "Stoicism is an ancient Greek philosophy that was founded at around 300 B.C. Stoicism\u2019s core assertion is that true wisdom and happiness comes from self-control and fortitude. Stoic philosophy encourages one to see the world for what it is from a rational and realistic perspective, rather than get caught up in our emotions and how we feel in the moment. The ultimate goal of stoicism is to develop clear judgment, inner calm, and freedom from suffering.\n\nMarcus Aurelius is perhaps the most well-known stoic. He was the emperor of Rome from 161\u2013180 A.D. Aurelius wrote his Meditations book for his own use as a guide for self-improvement. It is a very raw collection of notes reflecting his true thoughts and feeling, written in a very simple and straightforward manner, perhaps reflecting his stoic philosophy. I\u2019m going to share with you here 5 life-long lessons on mental strength I learned from this timeless classic. There are many quotes from this book that one will find useful; I\u2019ve chosen the most thought-provoking ones to share with you today.\n\nThis one is all mental. How many times in our lives has there been a small thing that ticked us off far more than it should have? We can get so angry with trivial things like being stuck in traffic, losing our cell phone, or just slow internet. These things get us frustrated, totally throwing off our day and slowing us down over something so small and really insignificant! Is being stuck in traffic really going to be the end of the world? (p.s I doubt it).\n\nThere might even be a bigger problem. Maybe you have a terrible coworker? Maybe you\u2019re being bullied? Getting over a bad breakup? All in all these things always seem to effect us, triggering our emotions and getting us stressed out\u2026.\n\nBut it doesn\u2019t have to be this way. The only reason you you feel pain, stress, or suffering is because you think you do. You can choose not to be in pain. choose not to care about that lost cell phone or bad breakup. You still have a lot going for you. You still have friends and family. You still have food. You can still go out and live your life how you want to live it. The main point is: you can choose to be happy.\n\nHumans can take things for granted; a lot. The thing we probably take for granted the most is our time. It\u2019s actually quite mind boggling how much time we waste. Try tracking how many minutes or hours you spend on Facebook, Twitter, YouTube, Netflix, or other sources of entertainment. Imagine if instead you used that time to bond with your family and friends; your relationship with them would improve a lot. What about using some of that time to work on a passion project after work? It might just turn into your greatest joy, maybe even a business one day.\n\nBeyond that, what about the little things we do every day. When you see someone, do you smile or look away? Do you treat people with kindness or aggression? We can all do our part in making this world a happier one to be in and part of that is doing the little things to spread some joy in the world.\n\nAs Aurelius said, while you still can, use your time wisely and be good.\n\nSo often we follow the opinions of others. We went through elementary schools and high schools, taught the same things as everyone else and in the same ways. Everyone values fitting in with the crowd, being cool and popular. But what unique value does that bring into the world? Are you truly happy not being yourself and acting like others?\n\nWhat Aurelius is suggesting is that life is a constant battle of trying to be unique and thinking for yourself. People who follow the crowd become average and ordinary. But people whom are themselves, who love their own uniqueness and share that with the world are far better off. They are happy and truly at peace with themselves, and as a result they offer the world their own unique value that can\u2019t come from being like everyone else.\n\nNever complain about circumstance. Never complain about back luck. Never complain about disadvantages. Yes those things are real, but does it really help to dwell on them? Having negative thoughts about these things will never actually move you forward. You must have a constant sense of positivism and opportunity.\n\nThink of everything that comes your way as something positive. Lost your job? Maybe it was a blessing in disguise, a wake up call to work harder or pursue something different. Maybe your \u201cbad luck\u201d is just a lot of mistakes on your part and you should be more careful. In everything that happens to us there is both a lesson and an opportunity for self-improvement. If we approach everything that comes our way with a positive attitude and a fiery boldness, we undoubtedly turn all situations into favorable ones in one way or another.\n\nPeople are inherently quite judgmental. We\u2019re always gossiping about others. Oh so and so is like this, Mr. Doe does this and that. We spend so much time judging others that we fail to look at ourselves. Are we so good and noble?\n\nWe should focus on ourselves and our own self-improvement. Don\u2019t worry about if your neighbor is good or bad. You should be concerned if you yourself are making the right decisions. Be good, do what you think is right, and do what is good. If you truly believe in yourself and that you are making the good and right choices, then there is no need to worry about the opinions of others.\n\nHere I chose 5 quotes from the book that I thought were the most thought provoking, but of course there are many more! Even the 5 quotes above are open to interpretation. If you have a different interpretation or another quote you would like to share, leave a response below! If you enjoyed this article, feel free to clap."
    },
    {
        "url": "https://towardsdatascience.com/selecting-the-best-machine-learning-algorithm-for-your-regression-problem-20c330bad4ef?source=user_profile---------8----------------",
        "title": "Selecting the best Machine Learning algorithm for your regression problem",
        "text": "When approaching any type of Machine Learning (ML) problem there are many different algorithms to choose from. In machine learning, there\u2019s something called the \u201cNo Free Lunch\u201d theorem which basically states that no one ML algorithm is best for all problems. The performance of different ML algorithms strongly depends on the size and structure of your data. Thus, the correct choice of algorithm often remains unclear unless we test out our algorithms directly through plain old trial and error.\n\nBut, there are some pros and cons to each ML algorithm that we can use as guidance. Although one algorithm won\u2019t always be better than another, there are some properties of each algorithm that we can use as a guide in selecting the correct one quickly and tuning hyper parameters. We\u2019re going to take a look at a few prominent ML algorithms for regression problems and set guidelines for when to use them based on their strengths and weaknesses. This post should then serve as a great aid in selecting the best ML algorithm for you regression problem!\n\nBeginning with the simple case, Single Variable Linear Regression is a technique used to model the relationship between a single input independent variable (feature variable) and an output dependent variable using a linear model i.e a line. The more general case is Multi Variable Linear Regression where a model is created for the relationship between multiple independent input variables (feature variables) and an output dependent variable. The model remains linear in that the output is a linear combination of the input variables.\n\nThere is a third most general case called Polynomial Regression where the model now becomes a non-linear combination of the feature variables i.e there can be exponential variables, sine and cosine, etc. This however requires knowledge of how the data relates to the output. Regression models can be trained using Stochastic Gradient Descent (SGD).\n\nA Neural Network consists of an interconnected group of nodes called neurons. The input feature variables from the data are passed to these neurons as a multi-variable linear combination, where the values multiplied by each feature variable are known as weights. A non-linearity is then applied to this linear combination which gives the neural network the ability to model complex non-linear relationships. A neural network can have multiple layers where the output of one layer is passed to the next one in the same way. At the output, there is generally no non-linearity applied. Neural Networks are trained using Stochastic Gradient Descent (SGD) and the backpropagation algorithm (both displayed in the GIF above).\n\nBeginning with the base case, a Decision Tree is an intuitive model where by one traverses down the branches of the tree and selects the next branch to go down based on a decision at a node. Tree induction is the task of taking a set of training instances as input, deciding which attributes are best to split on, splitting the dataset, and recurring on the resulting split datasets until all training instances are categorized. While building the tree, the goal is to split on the attributes which create the purest child nodes possible, which would keep to a minimum the number of splits that would need to be made in order to classify all instances in our dataset. Purity is measured by the concept of information gain, which relates to how much would need to be known about a previously-unseen instance in order for it to be properly classified. In practice, this is measured by comparing entropy, or the amount of information needed to classify a single instance of a current dataset partition, to the amount of information to classify a single instance if the current dataset partition were to be further partitioned on a given attribute.\n\nRandom Forests are simply an ensemble of decision trees. The input vector is run through multiple decision trees. For regression, the output value of all the trees is averaged; for classification a voting scheme is used to determine the final class.\n\nBoom! There\u2019s your pros and cons! In the next post we\u2019ll take a look at the pros and cons of different classification models. I hope you enjoyed this post and learned something new and useful. If you did, feel free to give it some claps."
    },
    {
        "url": "https://towardsdatascience.com/5-quick-and-easy-data-visualizations-in-python-with-code-a2284bae952f?source=user_profile---------9----------------",
        "title": "5 Quick and Easy Data Visualizations in Python with Code",
        "text": "Data Visualization is a big part of a data scientist\u2019s jobs. In the early stages of a project, you\u2019ll often be doing an Exploratory Data Analysis (EDA) to gain some insights into your data. Creating visualizations really helps make things clearer and easier to understand, especially with larger, high dimensional datasets. Towards the end of your project, it\u2019s important to be able to present your final results in a clear, concise, and compelling manner that your audience, whom are often non-technical clients, can understand.\n\nMatplotlib is a popular Python library that can be used to create your Data Visualizations quite easily. However, setting up the data, parameters, figures, and plotting can get quite messy and tedious to do every time you do a new project. In this blog post, we\u2019re going to look at 6 data visualizations and write some quick and easy functions for them with Python\u2019s Matplotlib. In the meantime, here\u2019s a great chart for selecting the right visualization for the job!\n\nScatter plots are great for showing the relationship between two variables since you can directly see the raw distribution of the data. You can also view this relationship for different groups of data simple by colour coding the groups as seen in the first figure below. Want to visualize the relationship between three variables? No problemo! Just use another parameters, like point size, to encode that third variable as we can see in the second figure below.\n\nNow for the code. We first import Matplotlib\u2019s pyplot with the alias \u201cplt\u201d. To create a new plot figure we call . We pass the x-axis and y-axis data to the function and then pass those to to plot the scatter plot. We can also set the point size, point color, and alpha transparency. You can even set the y-axis to have a logarithmic scale. The title and axis labels are then set specifically for the figure. That\u2019s an easy to use function that creates a scatter plot end to end!\n\nLine plots are best used when you can clearly see that one variable varies greatly with another i.e they have a high covariance. Lets take a look at the figure below to illustrate. We can clearly see that there is a large amount of variation in the percentages over time for all majors. Plotting these with a scatter plot would be extremely cluttered and quite messy, making it hard to really understand and see what\u2019s going on. Line plots are perfect for this situation because they basically give us a quick summary of the covariance of the two variables (percentage and time). Again, we can also use grouping by colour encoding.\n\nHere\u2019s the code for the line plot. It\u2019s quite similar to the scatter above. with just some minor variations in variables.\n\nHistograms are useful for viewing (or really discovering)the distribution of data points. Check out the histogram below where we plot the frequency vs IQ histogram. We can clearly see the concentration towards the center and what the median is. We can also see that it follows a Gaussian distribution. Using the bars (rather than scatter points, for example) really gives us a clearly visualization of the relative difference between the frequency of each bin. The use of bins (discretization) really helps us see the \u201cbigger picture\u201d where as if we use all of the data points without discrete bins, there would probably be a lot of noise in the visualization, making it hard to see what is really going on.\n\nThe code for the histogram in Matplotlib is shown below. There are two parameters to take note of. Firstly, the parameters controls how many discrete bins we want for our histogram. More bins will give us finer information but may also introduce noise and take us away from the bigger picture; on the other hand, less bins gives us a more \u201cbirds eye view\u201d and a bigger picture of what\u2019s going on without the finer details. Secondly, the parameter is a boolean which allows us to select whether our histogram is cumulative or not. This is basically selecting either the Probability Density Function (PDF) or the Cumulative Density Function (CDF).\n\nImagine we want to compare the distribution of two variables in our data. One might think that you\u2019d have to make two separate histograms and put them side-by-side to compare them. But, there\u2019s actually a better way: we can overlay the histograms with varying transparency. Check out the figure below. The Uniform distribution is set to have a transparency of 0.5 so that we can see what\u2019s behind it. This allows use to directly view the two distributions on the same figure.\n\nThere are a few things to set up in code for the overlaid histograms. First, we set the horizontal range to accommodate both variable distributions. According to this range and the desired number of bins we can actually computer the width of each bin. Finally, we plot the two histograms on the same plot, with one of them being slightly more transparent.\n\nBar plots are most effective when you are trying to visualize categorical data that has few (probably < 10) categories. If we have too many categories then the bars will be very cluttered in the figure and hard to understand. They\u2019re nice for categorical data because you can easily see the difference between the categories based on the size of the bar (i.e magnitude); categories are also easily divided and colour coded too. There are 3 different types of bar plots we\u2019re going to look at: regular, grouped, and stacked. Check out the code below the figures as we go along.\n\nThe regular barplot is in the first figure below. In the function, represents the tickers on the x-axis and represents the bar height on the y-axis. The error bar is an extra line centered on each bar that can be drawn to show the standard deviation.\n\nGrouped bar plots allow us to compare multiple categorical variables. Check out the second bar plot below. The first variable we are comparing is how the scores vary by group (groups G1, G2, ... etc). We are also comparing the genders themselves with the colour codes. Taking a look at the code, the variable is now actually a list of lists, where each sublist represents a different group. We then loop through each group, and for each group we draw the bar for each tick on the x-axis; each group is also colour coded.\n\nStacked bar plots are great for visualizing the categorical make-up of different variables. In the stacked bar plot figure below we are comparing the server load from day-to-day. With the colour coded stacks, we can easily see and understand which servers are worked the most on each day and how the loads compare to the other servers on all days. The code for this follows the same style as the grouped bar plot. We loop through each group, except this time we draw the new bars on top of the old ones rather than beside them.\n\nWe previously looked at histograms which were great for visualizing the distribution of variables. But what if we need more information than that? Perhaps we want a clearer view of the standard deviation? Perhaps the median is quite different from the mean and thus we have many outliers? What if there is so skew and many of the values are concentrated to one side?\n\nThat\u2019s where boxplots come in. Box plots give us all of the information above. The bottom and top of the solid-lined box are always the first and third quartiles (i.e 25% and 75% of the data), and the band inside the box is always the second quartile (the median). The whiskers (i.e the dashed lines with the bars on the end) extend from the box to show the range of the data.\n\nSince the box plot is drawn for each group/variable it\u2019s quite easy to set up. The is a list of the groups/variables. The Matplotlib function makes a box plot for each column of the or each vector in sequence ; thus each value in corresponds to a column/vector in . All we have to set then are the aesthetics of the plot.\n\nThere are your 5 quick and easy data visualizations using Matplotlib. Abstracting things into functions always makes your code easier to read and use! I hope you enjoyed this post and learned something new and useful. If you did, feel free to give it some claps."
    },
    {
        "url": "https://medium.com/swlh/never-eat-alone-5-key-lessons-from-the-most-famous-book-on-networking-ae70ec6c9bd5?source=user_profile---------10----------------",
        "title": "Never Eat Alone: 5 Key lessons from the most famous book on networking",
        "text": "It\u2019s not what you know, it\u2019s who you know.\n\nYour network is your net worth.\n\nMost people have heard the above quotes at some time in their career. If you went to college or university, they preached this stuff whenever they had a networking event going on. To move up the corporate ladder, you need to be connected to the right people. To scale your business it helps a lot to know the best sales people, investors, and those who can advise you. To find the right job and grow in your career, it helps to have the right mentor. In general, you can learn so much more in life just by getting to know more people and exchanging value with one another.\n\n\u2026.. So how do you network? How do you get to know people whom you can learn from and exchange value?\n\nNever Eat Alone is a New York Times best selling book about networking. Even beyond that, it\u2019s about building long-lasting personal relationships in life where everybody gets value and everybody wins. It\u2019s about establishing the right mindset and taking the real steps towards building a strong network that creates success. Here we\u2019re going to look at 5 key lessons from the book.\n\nPeople love passion. They\u2019re drawn to it because it\u2019s so incredibly exciting and exhilarating! When you do something you\u2019re passionate about, work becomes play. You\u2019ll love to work, produce, and add value to the world. But passion doesn\u2019t just come out of thin air; it\u2019s not this random light bulb that dings above our head. Passion comes from having a mission.\n\nHave you ever listened to a presentation from Steve Jobs? He loved giving Apple products to the world. He got people excited and engaged. People loved him without having ever met him. Having a mission inspires people and when you inspire someone, they\u2019ll get just as excited to be a part of your mission as you are.\n\nConnect with people through the passions that drive your mission.\n\nHuman psychology is perhaps one of the most powerful and rewarding areas of knowledge that one can possess. Every decision and action that people take is driven by human psychology. It\u2019s so so important to know how people\u2019s brains and emotions work so that you can truly understand why they make certain choices or do certain things.\n\nWhen looking to hire someone for a job or to partake in a business venture, people will hire whom they like first and who\u2019s most qualified second. They go with someone whom they can trust. This is because although we try our best to make rational and logical decisions, humans are still naturally emotionally creatures. We\u2019re still swayed by love and fear. Think about who your current friends are: they\u2019re probably people you like because you have common interests and values.\n\nSo when you want to connect with someone new, put yourself in their shoes. What are their likes and dislikes? What passions drive them to do what they do? What do they want? By reaching out to people through their emotional drivers we establish deeper, more personal connections that build stronger relationships for everyone.\n\nEveryone wants to do well and to be successful. When you\u2019re connecting with someone new, their first thought isn\u2019t how they can help you, but how you can help them. When a potential employer is interviewing you, they\u2019re not thinking \u201cHey I\u2019d love to pay this person a full time salary so they can buy new shoes\u201d; they\u2019re thinking \u201cIf I hire this person, will they provide me with enough value to make a profit?\u201d\n\nThey key to building strong relationships is to give give give. When you give people value they see that being connected with you is good for them and their success. And it\u2019s not you doing all the work. Going back to human psychology, we naturally have a sense of fairness and reciprocation. When someone does us a favor, we feel obligated to be fair and do them a favor in return. By doing that everyone is giving each other value. Nobody is really losing anything because everyone is helping each other; when everyone gives, everyone wins. We as a network all grow successful together.\n\nThe takeaway here is to find out what the person you\u2019re trying to connect with values and then try and give them that value in the best way you can.\n\nWho do you notice has the biggest online following? Who knows the most people? Who is the networking guru? It\u2019s the person that always puts in the effort and shows up.\n\nGo to meetups, conferences, and events. You\u2019ll meet a ton of new people and learn a lot. It really gets your name out there too.\n\nGive a talk at one of those events! That\u2019s a supercharged way to get noticed and get some feedback on what you had to say so you can learn.\n\nLike and comment on people\u2019s LinkedIn posts, if you liked them! People love hearing feedback and in general like to actually see that you listened to what they had to say. Going back again to human psychology, people like to feel important and naturally people feel connected to and like whomever makes them feel good like that.\n\nBe that person that everyone wants to be connected to.\n\nThere are many \u201ccloset experts\u201d in the world. People whom are really good and doing awesome things, but nobody knows about them! If you want to connect with people and give the best possible value, people need to first know who you are! Help people get to know you by building up your personal brand.\n\nShow the world what you\u2019re all about. Give the world value through your passions, skills, and personal brand. Maybe you\u2019re awesome at coding? Great! post that code online and demonstrate to people that it works. Do you love to write? Prove it through blogging or publishing an e-book. If no one knows about you, they\u2019ll never really know if you\u2019re good, or if you can add any value.\n\nYou\u2019ve got to build your personal brand and then broadcast it. People should know that you\u2019re offering them value right away. Use Quora, blogs, LinkedIn, Medium, Twitter, YouTube whatever. Just get your name out there so people can be aware of the value you have to offer. You can have all the skills in the world and if no one knows about it, it doesn\u2019t matter!\n\nYou\u2019ll also give yourself some amazing credibility this way too. Back to our human psychology, people will listen to what you have to say more if you have social proof i.e the credibility of the crowd. Why do people buy things from the best sellers list on Amazon? Well because other people bought it, so it must be at least pretty good if other people have bought it and given good reviews. If you have a big following online or have a large network, people will pay attention to you because they have the social proof from others that says hey, this person\u2019s good. That\u2019s powerful.\n\nThere you have it! 5 Key lessons from the #1 book on networking Never Eat Alone. I\u2019d highly recommend you give it a read! It\u2019ll help you build deep and rewarding personal relationships, all along your journey to career success. Check out this link to the book on Amazon."
    },
    {
        "url": "https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68?source=user_profile---------11----------------",
        "title": "The 5 Clustering Algorithms Data Scientists Need to Know",
        "text": "Clustering is a Machine Learning technique that involves the grouping of data points. Given a set of data points, we can use a clustering algorithm to classify each data point into a specific group. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features. Clustering is a method of unsupervised learning and is a common technique for statistical data analysis used in many fields.\n\nIn Data Science, we can use clustering analysis to gain some valuable insights from our data by seeing what groups the data points fall into when we apply a clustering algorithm. Today, we\u2019re going to look at 5 popular clustering algorithms that data scientists need to know and their pros and cons!\n\nK-Means is probably the most well know clustering algorithm. It\u2019s taught in a lot of introductory data science and machine learning classes. It\u2019s easy to understand and implement in code! Check out the graphic below for an illustration.\n\nK-Means has the advantage that it\u2019s pretty fast, as all we\u2019re really doing is computing the distances between points and group centers; very few computations! It thus has a linear complexity O(n).\n\nOn the other hand, K-Means has a couple of disadvantages. Firstly, you have to select how many groups/classes there are. This isn\u2019t always trivial and ideally with a clustering algorithm we\u2019d want it to figure those out for us because the point of it is to gain some insight from the data. K-means also starts with a random choice of cluster centers and therefore it may yield different clustering results on different runs of the algorithm. Thus, the results may not be repeatable and lack consistency. Other cluster methods are more consistent.\n\nK-Medians is another clustering algorithm related to K-Means, except instead of recomputing the group center points using the mean we use the median vector of the group. This method is less sensitive to outliers (because of using the Median) but is much slower for larger datasets as sorting is required on each iteration when computing the Median vector.\n\nMean shift clustering is a sliding-window-based algorithm that attempts to find dense areas of data points. It is a centroid-based algorithm meaning that the goal is to locate the center points of each group/class, which works by updating candidates for center points to be the mean of the points within the sliding-window. These candidate windows are then filtered in a post-processing stage to eliminate near-duplicates, forming the final set of center points and their corresponding groups. Check out the graphic below for an illustration.\n\nAn illustration of the entire process from end-to-end with all of the sliding windows is show below. Each black dot represents the centroid of a sliding window and each gray dot is a data point.\n\nIn contrast to K-means clustering there is no need to select the number of clusters as mean-shift automatically discovers this. That\u2019s a massive advantage. The fact that the cluster centers converge towards the points of maximum density is also quite desirable as it is quite intuitive to understand and fits well in a naturally data-driven sense. The drawback is that the selection of the window size/radius \u201cr\u201d can be non-trivial.\n\nDBSCAN is a density based clustered algorithm similar to mean-shift, but with a couple of notable advantages. Check out another fancy graphic below and let\u2019s get started!\n\nDBSCAN poses some great advantages over other clustering algorithms. Firstly, it does not require a pe-set number of clusters at all. It also identifies outliers as noises unlike mean-shift which simply throws them into a cluster even if the data point is very different. Additionally, it is able to find arbitrarily sized and arbitrarily shaped clusters quite well.\n\nThe main drawback of DBSCAN is that it doesn\u2019t perform as well as others when the clusters are of varying density. This is because the setting of the distance threshold \u03b5 and minPoints for identifying the neighborhood points will vary from cluster to cluster when the density varies. This drawback also occurs with very high-dimensional data since again the distance threshold \u03b5 becomes challenging to estimate.\n\nOne of the major drawbacks of K-Means is its naive use of the mean value for the cluster center. We can see why this isn\u2019t the best way of doing things by looking at the image below. On the left hand side it looks quite obvious to the human eye that there are two circular clusters with different radius\u2019 centered at the same mean. K-Means can\u2019t handle this because the mean values of the clusters are a very close together. K-Means also fails in cases where the clusters are not circular, again as a result of using the mean as cluster center.\n\nGaussian Mixture Models (GMMs) give us more flexibility than K-Means. With GMMs we assume that the data points are Gaussian distributed; this is a less restrictive assumption than saying they are circular by using the mean. That way, we have two parameters to describe the shape of the clusters: the mean and the standard deviation! Taking an example in two dimensions, this means that the clusters can take any kind of elliptical shape (since we have standard deviation in both the x and y directions). Thus, each Gaussian distribution is assigned to a single cluster.\n\nIn order to find the parameters of the Gaussian for each cluster (e.g the mean and standard deviation) we will use an optimization algorithm called Expectation\u2013Maximization (EM). Take a look at the graphic below as an illustration of the Gaussians being fitted to the clusters. Then we can proceed on to the process of Expectation\u2013Maximization clustering using GMMs.\n\nThere are really 2 key advantages to using GMMs. Firstly GMMs are a lot more flexible in terms of cluster covariance than K-Means; due to the standard deviation parameter, the clusters can take on any ellipse shape, rather than being restricted to circles. K-Means is actually a special case of GMM in which each cluster\u2019s covariance along all dimensions approaches 0. Secondly, since GMMs use probabilities, they can have multiple clusters per data point. So if a data point is in the middle of two overlapping clusters, we can simply define its class by saying it belongs X-percent to class 1 and Y-percent to class 2. I.e GMMs support mixed membership.\n\nHierarchical clustering algorithms actually fall into 2 categories: top-down or bottom-up. Bottom-up algorithms treat each data point as a single cluster at the outset and then successively merge (or agglomerate) pairs of clusters until all clusters have been merged into a single cluster that contains all data points. Bottom-up hierarchical clustering is therefore called hierarchical agglomerative clustering or HAC. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. Check out the graphic below for an illustration before moving on to the algorithm steps\n\nHierarchical clustering does not require us to specify the number of clusters and we can even select which number of clusters looks best since we are building a tree. Additionally, the algorithm is not sensitive to the choice of distance metric; all of them tend to work equally well whereas with other clustering algorithms, the choice of distance metric is critical. A particularly good use case of hierarchical clustering methods is when the underlying data has a hierarchical structure and you want to recover the hierarchy; other clustering algorithms can\u2019t do this. These advantages of hierarchical clustering come at the cost of lower efficiency, as it has a time complexity of O(n\u00b3), unlike the linear complexity of K-Means and GMM.\n\nThere are your top 5 clustering algorithms that a data scientist should know! We\u2019ll end off with an awesome visualization of how well these algorithms and a few others perform, courtesy of Scikit Learn!"
    },
    {
        "url": "https://towardsdatascience.com/7-practical-deep-learning-tips-97a9f514100e?source=user_profile---------12----------------",
        "title": "7 Practical Deep Learning Tips \u2013",
        "text": "Deep Learning has become the go-to method for solving many challenging real-world problems. It\u2019s by far the best performing method for things like object detection, speech recognition, and language translation. Many people see Deep Neural Networks (DNNs) as magical black boxes where we shove in a bunch of data and out comes our solution! In practice, things actually get a lot more complicated\u2026\n\nThere can be a whole host of challenges in designing and applying a DNN to a specific problem. To achieve the performance standards required for real-world application, proper design and execution of all stages in the pipeline are crucial including data preparation, network design, training, and inference. Here I\u2019m going to share with you 7 practical tips for getting the most out of your Deep Neural Net.\n\nThis one\u2019s no big secret. The deep learning machines that have been working so well need fuel \u2014 lots of fuel; that fuel is data. The more labelled data we have, the better our model performs. The idea of more data leading to better performance has even been explored at a large-scale by Google with a dataset of 300 Million images!\n\nWhen deploying your Deep Learning model in a real-world application, you should really be constantly feeding it more data and fine tuning to continue improving its performance. Feed the beast: if you want to improve your model\u2019s performance, get some more data!\n\nOver the years, many gradient descent optimization algorithms have been developed and each have their pros and cons. A few of the most popular ones include:\n\nRMSprop, Adadelta, and Adam are considered to be adaptive optimization algorithms, since they automatically update the learning rate. With SGD you have to manually select the learning rate and momentum parameter, usually decaying the learning rate over time.\n\nIn practice, the adaptive optimizers tend to converge way faster than SGD; however, their final performance is usually slightly worse. SGD usually achieves a better minimum and thus better final accuracy, but it might take significantly longer than with some of the optimizers. It\u2019s also much more reliant on a robust initialization and learning rate decay schedule, which can be quite challenging to tune in practice.\n\nThus, if you\u2019re in need of some quick results or just want to test out a new technique, go with one of the adaptive optimizers. I\u2019ve found Adam to be very easy to use as it\u2019s not very sensitive to you selecting the perfect learning rate. If you want the absolute best final performance, go with SGD + Momentum and work with the learning rate, decay, and momentum value to maximize the performance.\n\nGetting the best of both worlds\n\nIt\u2019s recently been shown that you can get the best of both worlds: high-speed training with top notch performance by switching from Adam to SGD! The idea is that the early stages of the training is really the time when SGD is very sensitive to parameter tuning and initialization. Thus, we can start off our training by using Adam, which will get you pretty far while not having to worry about initialization and parameter tuning. Then, once Adam has got us rolling, we can switch to SGD + Momentum optimization to achieve peak performance!\n\nThere are many cases where you will be dealing with imbalanced data, especially in real-world applications. Take a simple but real-world example: You are training your deep network to predict whether someone in a video feed is holding a lethal weapon or not, for security reasons. BUT in your training data, you only have 50 videos of people holding weapons, and 1000 videos of people without weapons! If you just train your network right away with this data, your model will definitely be highly biased towards predicting that no one ever has a weapon!\n\nThere are a few things you can do to combat this:\n\nAs we looked at in the first tip, deep networks need lots of data. Unfortunately, for many new applications, this data can be difficult and expensive to acquire. We might need tens or hundreds of thousands of new training examples to train on if we want our model to perform well. If a data set is not readily available, it must all be collected and labelled manually.\n\nThat\u2019s where transfer learning comes into play. With transfer learning, we don\u2019t need a lot of data! The idea is to start off with a network that was previously trained on millions of images, such as ResNet pre-trained on ImageNet. Then we will fine-tune the ResNet model by only re-training the last few layers and leaving the others alone. That way, we are taking the information (image features) that ResNet learned from millions of images and fine-tuning it such that we can apply it to a different task. This is possible because the feature information of images across domains is often quite similar, but the analysis of these features can be different depending on the application.\n\nWe\u2019ve said it a few times now: more data = better performance. Aside from transfer learning, another quick and easy way to increase your model\u2019s performance is data augmentation. Data augmentation involves generating synthetic training examples by altering some of the original images from the data set, while using the original class label. For example, common ways of data augmentation for images include:\n\nBasically, you can perform any alteration that would change the look of the image, but not the overall content i.e you can make a picture of a dog blue, but you should still be able to clearly see that it\u2019s a picture of a dog.\n\nIn machine learning, ensembles train multiple models and then combine them together to achieve higher performance. Thus, the idea is to train multiple deep network models on the same task on the same data set. The results of the models can then be combined via a voting scheme i.e the class with the highest number of votes wins.\n\nTo insure that all of the models are different, random weight initializations and random data augmentation can be used. It is well known that an ensemble is usually significantly more accurate than a single model, due to the use of multiple models and thus approaching the task from different angles. In real-world applications, especially challenges or competitions, almost all the top models use ensembles.\n\nWe know that model accuracy increases with depth, but what about speed? More layers means more parameters, and more parameters means more computation, more memory consumption, and less speed. Ideally, we would like to maintain high accuracy while increasing our speed. We can do this with pruning.\n\nThe idea is that among the many parameters in the network, some are redundant and don\u2019t contribute a lot to the output. If you could rank the neurons in the network according to how much they contribute, you could then remove the low ranking neurons from the network, resulting in a smaller and faster network. The ranking can be done according to the L1/L2 mean of neuron weights, their mean activation, the number of times a neuron wasn\u2019t zero on some validation set, and other creative methods. Getting faster/smaller networks is important for running deep learning networks on mobile devices.\n\nThe most basic way of pruning networks is to simply drop certain convolutional filters. This was done fairly successfully in this recent paper. The neuron ranking in this work is fairly simple: it\u2019s the L1 norm of the weights of each filter. On each pruning iteration they rank all the filters, prune the m lowest ranking filters globally among all the layers, retrain and repeat!\n\nA key insight to pruning filters was presented in another recent paper that analysed the structure of Residual Networks. The authors showed that when removing layers, networks with residual shortcut connections (such as ResNets) were far more robust in maintaining good accuracy than networks that did not use any shortcut connections (such as VGG or AlexNet). This interesting discovery is of great practical importance because it tells us that when pruning a network for deployment and application, the network design is of critical importance (go with ResNets!). So it\u2019s always good to use the latest and greatest methods!\n\nThere you have it, your 7 practical tips for Deep Learning!"
    },
    {
        "url": "https://towardsdatascience.com/deep-learning-for-image-classification-why-its-challenging-where-we-ve-been-and-what-s-next-93b56948fcef?source=user_profile---------13----------------",
        "title": "Deep Learning for Image Recognition: why it\u2019s challenging, where we\u2019ve been, and what\u2019s next",
        "text": "Deep learning has absolutely dominated computer vision over the last few years, achieving top scores on many tasks and their related competitions. The most popular and well known of these computer vision competitions is ImageNet. The ImageNet competition tasks researchers with creating a model that most accurately classifies the given images in the dataset.\n\nOver the past few years, deep learning techniques have enabled rapid progress in this competition, even surpassing human performance. Today we\u2019re going to review that progress to gain insight into how these advances came about with deep learning, what we can learn from them, and where we can go from here.\n\nSo what\u2019s so hard about the ImageNet challenge? Lets start by taking a look at the data. The data for the ImageNet classification task was collected from Flickr and other search engines, manually labeled by humans with each image belonging to one of 1000 object categories/classes. The distribution of the data set is shown below in the table.\n\nBy 2012, ImageNet had nearly 1.3 million training images. The main challenge with such a large scale image classification task is the diversity of the images. Here we can take a look at a couple of examples of that.\n\nCheck out the image below. On the left we see some example images from another image classification challange: PASCAL. In the PASCAL challenge, there were only about 20,000 training images and 20 object categories. That challenge had quite generic class categories like \u201cbird\u201d, \u201cdog\u201d, and \u201ccat\u201d as depicted below. Shift over to the ImageNet challenge and it\u2019s a whole new ball game. Instead of having a general class called \u201cdog\u201d that encompasses all kinds of dog, ImageNet has classes for each dog species. In fact, instead of the PASCAL \u201cdog\u201d category, ImageNet has 120 categories for the different breeds of dogs! Thus, any model/algorithm that we use for this task must be able to handle these very fine-grained and specific classes, even though they may look very similar and are hard to distinguish.\n\nIn more technical terms, we want to maximize the inter-class variability. This means that we want two images each containing a different kind of bird to look very different to our model, since even though they are both birds, in our data set they are in different categories.\n\nHere\u2019s another challenging feature of ImageNet: objects of the same class can look vastly different. Lets check out the images below. The two on the left are both from the class \u201corange\u201d and the two on the right are both from the class \u201cpool table\u201d. Yet, each pair of images looks very different! As humans we can see that one of the oranges is cut and the other is not; we can also see that one picture of the pool table is zoomed in, the other isn\u2019t. This is called intra-class variability. We want to minimize this variability since we want two images of the same class to look pretty similar to our deep learning model, quantitatively that is.\n\nWith these image classification challenges known, lets review how deep learning was able to make great strides on this task.\n\nNearly every year since 2012 has given us big breakthroughs in developing deep learning models for the task of image classification. Due to it\u2019s large scale and challenging data, the ImageNet challenge has been the main benchmark for measuring progress. Here we\u2019re going to take a look at the progress of deep learning on this task and some of the major architectures that made that progress possible.\n\nBack in 2012, a paper from the University of Toronto was published at NIPS and boy was it ever a shocker. That paper was ImageNet Classification with Deep Convolutional Networks. It would go on to become one of the most influential papers in the field after achieving a nearly 50% reduction in the error rate in the ImageNet challenge, which was unprecedented progress at the time.\n\nThe paper proposed to use a deep Convolutional Neural Network (CNN)for the task of image classification. It was relatively simple compared to those that are being used today. The main contributions that came from this paper were:\n\nBasically, AlexNet set the bar, providing the baseline and default techniques of using CNNs for computer vision tasks!\n\nThe VGGNet paper \u201cVery Deep Convolutional Neural Networks for Large-Scale Image Recognition\u201d came out in 2014, further extending the ideas of using a deep networking with many convolutions and ReLUs. Their main idea was that you didn\u2019t really need any fancy tricks to get high accuracy. Just a deep network with lots of small 3x3 convolutions and non-linearities will do the trick! The main contributions of VGGNets are:\n\nThe GoogLeNet architecture was the first to really address the issue of computational resources along with multi-scale processing in the paper \u201cGoing Deeper with Convolutions\u201d. As we keep making our classification networks deeper and deeper, we get to a point where we\u2019re using up a lot of memory. Additionally, different computational filter sizes have been proposed in the past: from 1x1 to 11x11; how do you decide which one? The inception module and GoogLeNet tackles all of these problems with the following contributions:\n\nSince it\u2019s initial publication in 2015 with the paper \u201cDeep Residual Learning for Image Recognition\u201d, ResNets have created major improvements in accuracy in many computer vision tasks. The ResNet architecture was the first to pass human level performance on ImageNet, and their main contribution of residual learning is often used by default in many state-of-the-art networks today:\n\nShortcut connections were taken to the extreme with the introduction of DenseNets from the paper \u201cDensely Connected Convolutional Networks\u201d. DenseNets extend the idea of shortcut connections but having much more dense connectivity than ResNet:\n\nThose are the major architectures that have formed the backbone of progress in image classification over the last few years. Great progress has been made and it\u2019s exciting to see since it allows use to solve many real world problems with this new technology. Only one question remains\u2026..\n\nAs we just reviewed, research in deep learning for image classification has been booming! We\u2019ve taken huge steps in improving methods for this task, even surpassing human level performance. Deep neural networks are now widely used in many businesses for classifying images, even being the basis for many new start-up technologies.\n\nIt\u2019s great to see all of this progress, but we must always strive to improve. There are still a number of challenges with deep learning models in image classification. These are challenges that are critical to address if we want to move forward. Here I\u2019ll go over some of them that I consider important and that researchers are actively trying to address:\n\nCurrently, most deep learning methods being applied to computer vision tasks are supervised. This means that we need large amounts of labelled training data. This data is both tedious and costly to obtain. Think about it: the ImageNet challenge had 1.3 million training examples and that was only for 1000 different categories! A human needs to get all of the data, go through each image, and label it; that\u2019s a lot of manual work!\n\nMost of the time, when a business wants to apply an image classification network for their own specific application, they have to use transfer learning to fine tune a pre-trained ImageNet network. To do this fine tuning they still have to collect a lot of their own data and label it; tedious and costly to say the least.\n\nResearchers are actively putting effort and making progress in addressing this problem. There\u2019s more and more work being done on things likes fast and effective transfer learning, semi-supervised learning, and one-shot learning. We probably won\u2019t jump straight to unsupervised learning, but research in these methods is a strong step in the right direction\n\nThe rising popularity of using Generative Adversarial Networks (GANs) has revealed a new challenge for image classification: Adversarial Images. Adversarial images are in a nutshell images whose class category looks obvious to a human, but causes massive failures in a deep network. Check out the image above. With only a minor distortion (seemingly), a deep network\u2019s classification of the image goes from a panda to a gibbon!\n\nTo us humans it looks obvious that the image is still a panda, but for some reason it causes the deep network to fail in its task. This can be very dangerous in real-world applications: what if your self-driving car doesn\u2019t recognize a pedestrian and instead runs them over? Part of the problem may be stemming from the idea that we don\u2019t have a full understanding of what\u2019s going on inside our networks. In any case researchers are actively working on this challenging problem.\n\nMuch of the progress in deep learning has been driven by improvements in hardware, specifically GPUs. GPUs allow for high-speed processing of computations that can be done in parallel. Deep networks require a ton of multiply-add operations due to matrix operations; GPUs excel at performing these operations. This has been fantastic for progress, but we don\u2019t have GPUs everywhere!\n\nMany state-of-the-art networks, including those that have been discussed above, only run in inference at a reasonable speed on a high-end GPU. Mobile devices are a massive market and it\u2019s important that steps are taken towards serving that market. Plus, as networks get deeper and deeper they tend to require more memory, limiting even more devices from being able to run the networks!\n\nResearch in this area has actually picked up quite a bit recently. MobileNets is a family of architectures that has become popular for running deep networks directly on mobile devices. They use a different style of convolutions to reduce both memory consumption and inference time.\n\nThat\u2019s a wrap! We saw what\u2019s so hard about classifying images, and reviewed the amazing progress that\u2019s been made in the field using deep learning. We also saw some of the challenges that lie ahead. But tackling those challenges with new science and engineering is what\u2019s so exciting about technology."
    },
    {
        "url": "https://towardsdatascience.com/heres-how-to-leverage-deep-learning-in-your-startup-9204666a3272?source=user_profile---------14----------------",
        "title": "Here\u2019s how you can leverage Deep Learning in your business",
        "text": "Deep learning is the talk of the town and rightfully so! It\u2019s made many new innovations possible and it gives us the power to solve many real-world problems. Solving people\u2019s problems provides them with value and delivering good value to customers is exactly what strong, profitable businesses are built on.\n\nWe know deep learning works super well for a variety of tasks: speech recognition, image classification, chat bots, and many others! But how can we leverage that power? How can you use it in your business? As I guide you through how you can do that, we\u2019ll use the following flow chart to visualize the process.\n\nThe first step in integrating deep learning into your business is getting that ever so important data! The main source of deep learning\u2019s power that enables it to address valuable problems so well is data. The best deep learning models are based on supervised learning i.e they require large amounts of labelled data for them to achieve such high performance. In a nutshell: If we want our deep learning machine to work, we need to give it lots of fuel; that fuel is data.\n\nWell I\u2019ve got some great news for you: most of the time, that data is readily available! There are already many publicly available labelled data sets, collected for the training of deep learning models to apply to common applications. There\u2019s tons of data for classifying images, translating languages, chat bots, self-driving cars, and many more that you can use for your application! Here\u2019s a quick resource on some popular deep learning datasets: http://bit.do/Deep-Learning-Datasets\n\nNow what if you\u2019re in the rare case where you have a really specific, custom application for which there\u2019s no big public data set?\u2026 Well we\u2019ll just make our own! Data collection isn\u2019t as challenging as it was before. Web-scraping tools can be excellent to use in this case. Adrian Rosebrock has a great tutorial on how to automatically scrape Google Images to build your own data set http://bit.do/Scrape-Your-Dataset. To label large amounts of data, you can use crowd sourcing like Amazon\u2019s Mechanical Turk! The goal here is to get enough data to build your Minimum Viable Product (MVP).\n\nYou may be wondering how much data you need. A good way to gauge how much you need is to look a similar tasks, and see how much data others used to tackle that problem. That\u2019ll give you a nice ball-park number. Beyond that, more is usually better (as long as you\u2019re not getting diminishing returns!)!\n\nNow that you have your labelled data, you can fully build your product. Using the data, you train your deep learning model to perform a task, one that eventually provides value to your customers. Deep learning models work best on repetitive tasks that tend to exhibit a lot patterns. So focus on automating tasks that are quite repetitive in both required knowledge and execution, but also normally require lots of human effort or specific skills. That way, you\u2019re maximizing the value you are giving your customers by giving them something they otherwise wouldn\u2019t have had without your deep learning powered product.\n\nAt this stage you\u2019ll have gone through a couple of key steps in leveraging deep learning in your business. You\u2019ve collected your data and used it to train a deep learning model. Then you\u2019ve used that model to power your product, achieving great performance and providing value. Now onto the most powerful part of the process: the positive feedback loop.\n\nYou start delivering your product to customers. Some love it, some hate it, it\u2019s all a learning process! Very importantly, you\u2019re new customers are now also a new source of data to further improve your deep learning models and the products they power! The key here is the efficient acquisition of new data.\n\nWhen you were first building your MVP, you may have used a publicly available data set or collected your own for your custom product. In both cases, your data was good enough to build an MVP, but it likely wasn\u2019t enough to optimize your product. If you used a public data set, that data might not have been specific enough. Perhaps you\u2019re building a face recognition security system for your homes which only unlocks the door if the system recognizes the person\u2019s face. If the data set you used only contains pictures of clear and crisp faces, then it might fail in a rainy condition or at night time; you\u2019ve left your customer out stuck in the rain! In the other case where you have collected your own data set, your goal was to get enough data to build an MVP. In both cases it is likely that your model could benefit from having far more data; research has proven time and time again that more data helps improve deep learning models: https://arxiv.org/abs/1707.02968v2.\n\nNow that your product is in the hands of paying customers, they\u2019re your best source of new data! Your customers will be using your product on a regular basis and through that usage you can acquire even more data. Remember that face recognition that failed in the rain? Well why not use those failures to help train your deep learning model to perform even better! You can collect those failure cases like in the rain and night time, label them, and then fine tune your model using that new data! Every time your system makes a mistake it provides you with a great opportunity: get that new data, label it, and train your model to learn from that mistake. You can even do this with the successful cases, collecting data and reinforcing the model\u2019s performance for those situations.\n\nThis whole process creates a positive feedback loop where your customers are the ones driving it. More data gets you a better product, a better product gets you more customers, and those customers get you that ever so valuable data to fuel your deep learning powered product!\n\nSo there you have it! Through this process you can build a product that supports a defensible business. A product that\u2019s fueled by a continuous cycle of data. A product powered by deep learning that provides massive value to your customers."
    },
    {
        "url": "https://medium.com/swlh/ill-tell-you-why-deep-learning-is-so-popular-and-in-demand-5aca72628780?source=user_profile---------15----------------",
        "title": "I\u2019ll tell you why Deep Learning is so popular and in demand",
        "text": "So you\u2019re browsing through your News Feed and once again there\u2019s an article on Deep Learning. You keep seeing it everywhere along with all of the usual attached buzzwords. Recent grads being hired by big tech companies for well into the six-figures, working on the next big thing. There\u2019s all kinds of new software and apps being \u201cpowered by AI\u201d.\n\nDeep Learning has become the main driver of many new applications and it\u2019s time to really look at why this is the case. With so many other options that we\u2019ve been using for so long, why Deep Learning?\n\nDeep Learning is popular right now because it\u2019s easy and it works.\n\nLet me explain more!\n\nTraditional Machine Learning approaches worked like the top half of the picture above. You would have to design a feature extraction algorithm which generally involved a lot of heavy mathematics (complex design), wasn\u2019t very efficient, and didn\u2019t perform too well at all (accuracy level just wasn\u2019t suitable for real-world applications). After doing all of that you would also have to design a whole classification model to classify your input given the extracted features.\n\nDeep Learning has really made many new applications practically feasibile. We wouldn\u2019t have been able to make good language translators pre-deep learning, because we simply had no technique at the time that would perform well enough or at a high enough speed for a real-world application. The translator programs of the past messed up a lot. Check out this funny fail by Google translate!\n\nNow with deep learning and GPUs we can achieve higher accuracy at a practical speed! Deep learning is also much more accessible in terms of the learning curve. Much of the open source software is very easy to use and getting a simple language translator, chat bot, or image recognizer isn\u2019t too challenging. Less complex math and coding, more making cool stuff!"
    },
    {
        "url": "https://medium.com/@george.seif94/solving-sudoku-using-a-simple-search-algorithm-3ac44857fee8?source=user_profile---------16----------------",
        "title": "Solving Sudoku using a simple search algorithm \u2013 George Seif \u2013",
        "text": "Today, we\u2019re going to learn how to code a Sudoku puzzle solving algorithm in C++! It\u2019s also easy enough to extend to any other program language, so feel free to stick around if Python, Java, or something else is more of your forte!\n\nThe word Sudoku is Japanese and is composed of two parts: Su- meaning \u2018number\u2019, and Doku- meaning \u2018single\u2019. Rightfully so, as Sudoku is a puzzle where the objective is to fill a 9\u00d79 square grid with digits numbered 1 to 9, so that each column, each row, and each of the nine 3\u00d73 sub-grids contains all of the digits, but absolutely NO duplicates. The puzzle starts out partially filled in i.e some of the numbers are given to you as a starting point as shown for example above. It\u2019s quite the mentally stimulating game and is often featured in newspapers, right beside the crossword.\n\nAs with any puzzle game, we want to know the absolute best way to play so we can win every time! With Sudoku, there can sometimes be some really tough puzzles that don\u2019t give you many starting numbers, forcing you to think far ahead before filling in any of the grid cells. This is where computers can really come in handy, since computers can perform many logical operations very, very fast. This is exactly what we want when we\u2019re trying to think ahead with many different combinations of numbers.\n\nEnter Brute Force Search (BFS). BFS is one of the most general and basic techniques for searching for possible solutions to a problem. The idea is that we want to generate every possible move within the game and then test to see whether it solves our problem. The step-by-step process is the following:\n\nWell that seems straight forward enough! Thus with Sudoku, the BFS algorithm visits the empty cells in some order, filling in digits sequentially (from 1 to 9). The program would solve a puzzle by placing the digit \u201c1\u201d in the first cell and checking if it is allowed to be there. If there are no violations (checking row, column, and box constraints) then the algorithm advances to the next cell, and places a \u201c1\u201d in that cell. When checking for violations, if it is discovered that the \u201c1\u201d is not allowed, the value is advanced to \u201c2\u201d.\n\nHOLD ON; what if a cell is discovered where none of the 9 digits is allowed?! \u2026 Well, then there\u2019s a quick and easy solution to this call Backtracking. If we found a cell where no possible solution exists, then obviously we must have done something wrong with the previous cell. We should go back and change something in one of the previous cells and see if that works. Thus the algorithm will leave this unsolvable cell blank for now and move back i.e backtrack to the previous cell. The value in that cell is then incremented by one. This is repeated until the allowed value in the last (81st) cell is discovered. Backtracking is a depth-first search (in contrast to a breadth-first search), because it will completely explore one branch to a possible solution before moving to another branch. Check out the fun GIF below for an illustration:\n\nNow everyone's favorite part\u2026.. Lets code it and see our algorithm in action! We\u2019ll do this in C++.\n\nThe first thing to do when programming any game solving algorithm is to define the game itself. First off, we\u2019ll set up the Sudoku grid and a basic function to print it out. We need basic global variables to define the actual grid and how it will be printed. We do this in the code snippet below. Naturally, we define the Sudoku grid as a 2D array of integer type that has size 9x9. Also note that we define BLANK cells with the integer value 0. Thus when we create our initial Sudoku grid, the cells that start out as blank in the grid will be filled with 0s.\n\nNext off we want to define the rules of the game. The computer needs to know to play by the rules just like we do if we want it to solve the puzzle properly. It should know which moves are allowed and which are not, as well as what constitutes as a win! The code below helps us define to the computer which moves are allow and which are not. The first three functions check to see if a given number is already being used in a particular row, column, or box. The \u201cget_unassigned_location\u201d function is the one the loops through the grid to fill in each cell one-by-one; it is the mover of the BFS algorithm. The \u201cis_safe\u201d function checks to see if placing a given number in a particular cell is legal i.e it doesn\u2019t violate the rules of the game that are defined by the first three functions.\n\nFinally, we can get to the meat of the code. The function below is the BFS algorithm with backtracking and is responsible for solving the Sudoku puzzle perfectly. As can be seen below, the function is recursive; each time it is called, it is handling a single cell at a time; whichever one has not been assigned yet. For that particular cell, we loop through all possible digits from 1 to 9 and try them out. If a digit is valid for that cell, we move on to the next unassigned cell. If we find that for a particular cell that there are no valid digits, then we return false, then backtrack, incriminating the digit in the previous cell and trying again. If we are able to assign a valid number to every cell in the grid i.e the array is totally full, then the recursive algorithm will return true, jumping out of all the recursive loops having filled in the whole grid with valid numbers.\n\nAwesome! We\u2019re all done! All we have to do is define the initial Sudoku grid, pass to our solver function like below, and BAM we\u2019re done and have solved Sudoku!\n\nIf you would like to see the full code, check out my GitHub repository. https://github.com/GeorgeSeif/Sudoku-Solver. Any feedback is very much appreciated! Follow me if you learned something new today!"
    }
]