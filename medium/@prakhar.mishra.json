[
    {
        "url": "https://medium.com/@prakhar.mishra/reinforcement-learning-part-6-b3eaee529fe3?source=user_profile---------1----------------",
        "title": "Reinforcement Learning \u2014 Part 6 \u2013 Prakhar Mishra \u2013",
        "text": "Let\u2019s see how we can train a bot to evacuate the building in minimum time\n\nI found Painless Q-Learning as one of the best sources over the internet to get started with Q-Learning. Apart from this Basic Reinforcement Learning also helps me to take an informed step in this field. This blog is highly inspired from Painless Q-Learning, I would be coding out the same example that has been mentioned there.\n\nThis blog assumes the reader to have an understanding of about Q-Learning. If not, do read this Painless Q-Learning and then come back to see the implementation of the same example. I will be making a dedicated blog on explaining Q-Learning in near future.\n\nI made a class to handle all the functions required in the learning process, also I have initialized the environment here (should have done under Environment class), but it\u2019s ok for now. Here is some specification about the variables from the initialization function.\n\nYou can see the pictorial view of the environment below:\n\nHere each node in the graph is a room in the building. The arrows show the connectivity from one to other. We define each room as a state and each arrow as the action.\n\nThe above snippet is an iterative process over episodes that we do to make our bot learn optimal path over time. We start by initializing q_matrix to Zeros, considering that bot has no memory when it starts learning. We start with certain random state and take an informed decision of transition to go to another state by choosing a state which has maximum q-value. Now, considering this as a new state, we look for the maximum q value that we can get from transitioning to any other possible state. Next, we calculate the reward of going from current state to next and put that all in the Q-Learning formulae\n\nwhich updates the brain of our bot with some knowledge of the environment. At last, we set next state to current state showing the transition till goal state is reached for this episode. So, iterating over multiple episodes our bot figures out the optimal path from any room to exit. I have not described certain methods here. So, you can find full code here."
    },
    {
        "url": "https://hackernoon.com/reinforcement-learning-part-5-9dba9ac44369?source=user_profile---------2----------------",
        "title": "Reinforcement Learning \u2014 Part 5 \u2013",
        "text": "We will be talking on how to train your player go up the victory hill without prior rules using OpenAI Gym\n\nIf you are wondering what is OpenAI Gym, please refer to Reinforcement Learning \u2014 Part 4. Today we will be seeing some reinforcement learning in practice, wherein we will try to teach our agent (car in this case) to climb up to the right hill where the goal point is situated in minimum steps (a.k.a minimum time). Officially, you can read Mountain-v0 Environment Gym for more clarity on the problem statement that we are trying to solve.\n\nWe will be using good old school technique epsilon-greedy approach to this problem. Just to make sure we are on the same page, please go through basics of Gym and then come back here.\n\nIn the above snippet, we are instantiating our environment and by calling make method by passing in the environment name as an argument. You can find the full list of possible gym environments here. Next, we would like to see possible actions that our agent can take in this particular environment. This action set is needed to make an informed decision at any time-step during the game-play.\n\nHere, we define a class EpsilonGreedy with instance initializations as greedy , epsilon, values, counts because these are the only ones needed, you can think of episodes as player environment interaction when the game starts and ends (maybe because player died or time-up state was reached) and epsilons as the probability of taking random action, here our agent would explore the environment(take random action) with a probability of 20% and exploit (take highly weighted actions) with 80% probability at every time-step. Certain level of exploration is required for model to find the best possible action in a given situation and avoid trap of local minima. Also values and counts as the weighted average of the previously estimated value and the reward we just received and action taken count respectively. Also, if you see we define six methods \u2014 explore, exploit, select_option, update_all, update_values, update_counts. select_option is a method that is responsible for triggering an action of either to explore or exploit at any time-step in the game and update_all is responsible for updating action counts and values dict. If you notice carefully, then you will see that we have specified a value of 0(Zero) to possible actions in count and values dictionary initially.. just because we need to start somewhere with the memory.\n\nAbove snippet is the final piece of code that we will be writing for this task. We first get the instance/object of the class EpsilonGreedy (we defined this in one of the above snippets). Next, we iterate through all the episodes (possible lives in a game) and for each of it we reset the environment properties. We then run a (till goal achieved) loop in which we select an action for every time-step and use the reward and next state observation to choose consecutive action wisely (in-order to maximize our total rewards). At last, on completion of the game, we destroy the environment and restart with the next episode.\n\nThat\u2019s what it took to create an agent that can now learn it\u2019s environment and educated actions on to be taken at each time-step in-order to maximize the total reward."
    },
    {
        "url": "https://becominghuman.ai/heroku-for-deep-learning-floydhub-6bbc0fb6a77e?source=user_profile---------3----------------",
        "title": "Heroku for Deep Learning \u2014 FloydHub \u2013",
        "text": "That\u2019s what they say!\n\nFor all those like me, who are looking for an alternate service like Amazon EC2/Azure/Google Cloud but with minimum learning curve. Here\u2019s what I found couple of days ago, FloydHub i.e. Platform-as-a-Service for training and deploying your DL models in the cloud. Setting it up is super easy. It takes the pain of setting up libraries and environment for deep learning like CUDA, cuDNN, etc along with popular deep learning frameworks like Keras, TensorFlow and PyTorch already in-place, that\u2019s what impressed me. Apart from that, they provide almost all popular datasets for the user to get started with. FloydHub offers 100 hours of free GPU power and also No credit card needed for sign-up. Under the hood they use AWS reserve instances but are providing the simplicity as a service model, now you get why it\u2019s called Heroku for DL.\n\nSo, today we will see how you can setup, deploy and train your model on FloydHub with blink of an eye.\n\nRunning out your code over the Floyd services requires you to follow 4\u20135 steps in order.\n\n3. Login to Floyd through the CLI.\n\nYou will be prompted to enter the password after this, thereafter, you should successfully login into your user account.\n\nP.S. In case, you get \u201cAttributeError: \u2018module\u2019 object has no attribute \u2018OP_NO_TLSv1_1\u2019\u201d error while trying to login. Install the below version of Twisted."
    },
    {
        "url": "https://hackernoon.com/a-laymans-introduction-to-principal-components-2fca55c19fa0?source=user_profile---------4----------------",
        "title": "A Layman\u2019s Introduction to Principal Components \u2013",
        "text": "Principal Component Analysis is one of the techniques used for dimensionality reduction\n\nIn the last blog, I had talked about how you can use Autoencoders to represent the given input to dense latent space. Here, we will see one of the classic algorithms that is being practiced since very long and continues to deliver desirable results.\n\nIn machine learning, dimensionality simply refers to the number of features (i.e. input variables) in your dataset. Consider an example \u2014\n\nLet\u2019s say you want to predict the price of the house. Then what all parameters/features will you consider?\n\nJust now itself we are dealing with 6\u20137 dimensional data for just predicting the house price. So, these things that matter while making any decisions are called dimensions.\n\nFor the purpose of visualization high dimension can be any number of dimension above 3 to 4. Whereas, in general, I personally have found reduction working really well when visualizing the word embeddings which are usually in the order of few 100s.\n\nAs a thumb rule, you should always do feature standardization before applying PCA to any dataset. Standardizing the features so that they are centered around 0 with a standard deviation of 1 is not only important if we are comparing measurements that have different units, but it is also a general requirement for many machine learning algorithms. All this because, we would want all the scale of measurements to be treated on the same scale.\n\nPCA is a variance maximizer. It projects the original data onto the directions where variance is maximum.\n\nX(i) where i in [1,2,3,4,5] are the original data points in a 2-D space. Then Z(i) where i in [1,2,3,4,5] are the projected points on a 1-D space (Line). We chose the line going from -xy to +xy (dotted one) because the data is most spread in this direction. Now, for all the points in 2-D space X(i) we map them to 1-D space/component Z(i).\n\nWe will be using scikit-learn for this experiment. People who are wondering what scikit-learn it read this.\n\nIt is very easy to apply this statistical technique in python.Thanks to the community! \u270c\ufe0f We will be dealing with a curated dataset just for the purpose of this snippet.\n\nIt seems that we are successful in preserving the semantic properties of words as and when they are used even after reducing the dimension space from 300 to 2.\n\nI have tried to keep this blog simple and intuitive as much possible. For in-depth details of the algorithms see this"
    },
    {
        "url": "https://becominghuman.ai/understanding-autoencoders-unsupervised-learning-technique-82fb3fbaec2?source=user_profile---------5----------------",
        "title": "Understanding Autoencoders \u2014 Unsupervised Learning Technique",
        "text": "Recently, I came across a very good article How to autoencode a pokemon? Believe me or not, this article motivated me to explore this topic. So here I am, today we will see how we can use this technique in the field of Natural Language Processing. If you would have noticed the thumbnail image properly, then you must have noticed that the input to the network is same as the output from the network. That\u2019s exactly are Autoencoders.\n\nNow, you might be thinking where are such networks used in practice? Mostly, these are used in data compression, dimensionality reduction, text generation, image generation, etc. For all of this to achieve, we are interested in the Latent Space Representation i.e. the layer about which the network is symmetric, for which, we train our model on a data for certain use-case and then chop off the decoder part of it and use the encoder for other likely datasets. You can think of it as we could use the encoder as a feature detector that generates a compact, semantically rich representation of our input.\n\nBuilding Autoencoders in Keras has great examples of building autoencoders that reconstructs MNIST digit images using fully connected and convolutional neural networks.\n\nAutoencoders in general consists of two parts, an encoder and a decoder. The encoder will read the input and compress it to a compact/compressed representation, and the decoder will read this representation and recreate the input. The whole process is about trying to learn the identity function with the minimum reconstruction error.\n\nAnyone, who has tasted the flavor of working with NLP knows the power and usefulness of word embeddings. Almost all the solutions to the use-cases in NLP are built on top of it.\n\nLet\u2019s consider an example \u2014 The cat is chased by the dog.\n\nBy using Google Vectors or GloVe Vectors, we can represent this sentence as the sequence of each word as 300 dimension vector and a common strategy to get sentence vector is to average out the word vectors over the sentence length. But, if you analyze this approach, it\u2019s more of a bag of words approach that capture the semantics of the sentence and at the same time discards the syntactic structure of the sentence resulting in \u201cThe cat is chased by the dog\u201d and \u201cThe dog is chased by the cat\u201d as the same thing.\n\nThere are a couple of other strategies that are used in practice apart from averaging technique, like, traversing the syntax tree in DFS fashion and averaging out the phrases and eventually averaging phrases to reach the sentence representation. Today, we will see how we can use RNN Autoencoders to get sentence vector representation.\n\nBelow is the overall architecture that we will be building.\n\nOur autoencoder model takes a sequence of GloVe word vectors and learns to produce another sequence that is similar to the input sequence. The encoder LSTM compresses the sequence into a fixed size context vector, which the decoder LSTM uses to reconstruct the original sequence.\n\nAbove snippet is the main boilerplate for building any Autoencoder that deals with textual data. Since the objective of the autoencoder is to produce a good latent representation, we compare the latent vectors produced from the encoder using the original input versus the output of the autoencoder using cosine similarity between original sentence vector and generated one. If you are lost in the syntax anywhere read this."
    },
    {
        "url": "https://hackernoon.com/alexa-i-am-your-boss-e4387f105697?source=user_profile---------6----------------",
        "title": "Alexa, I am your boss! \u2013",
        "text": "Configuring Alexa to work for you\n\nThis blog is one of two blogs on the same topic wherein, I will show you how easy it is to make Alexa work for you. In this blog we will see how to use Alexa developer\u2019s portal to configure any new skill that you might want to add to Alexa.\n\nAlexa is Amazon\u2019s cloud-based voice service available on tens of millions of devices from Amazon and third-party device manufacturers. With Alexa, you can build natural voice experiences that offer customers a more intuitive way to interact with the technology they use every day.\n\nThe skill that we want our Alexa to learn is a basic thesaurus skill of finding definition, antonyms and synonym of any word. Pretty useful and easy skill to start with.\n\nWe will be using Python 2.x as the programming language of our choice for this project. The approach is generic enough to work with any programming language. Right now, we will be using our laptop as the server that will serve request to user\u2019s query.\n\nBefore we code anything, let\u2019s understand the jargons and concepts on how does Alexa understand your speech.\n\nBelow are the 3 possible search queries that Alexa should understand if configured properly.\n\nIn the above-mentioned examples, Alexa first tries to understand the intent of what user is trying to say, thereafter locating the entities that fulfill that intent.\n\nAfter the extraction of intent and slot entities from the user\u2019s voice query is done, those values hit our server which are conditioned to handle those variables in a specific way and thereafter accordingly returning the desired output back to the user with Alexa\u2019s voice output feature.\n\nSo, we will be using flask-ask, a wrapper over flask for building Alexa skills easier and much more fun.\n\nTo start with, visit Alexa Developer\u2019s Portal. The learn section is some pretty extensive resource for learning Alexa development. Since in the above example we talked about intent and entity slots, we will now see on how to configure Alexa so that it knows, what are the entities and intent possible with this specific skill. Click on Start Skill and you will be redirected to Alexa Skills Page. Now click on Create a Skill and after you give a name to your skill, select custom model from the screen following it thereafter, you will be redirected to model creation skill builder page which looks similar to as shown below."
    },
    {
        "url": "https://medium.com/formcept/omniscient-userbot-slack-4f852d22e921?source=user_profile---------7----------------",
        "title": "Omniscient @ UserBot \u2014 Slack \u2013 FORMCEPT \u2013",
        "text": "Chat Bots are the software programs that are trained to have the conversation with humans, it can be of automating some particular task or a general chit-chat in the form of both text and voice.\n\nThere are many applications of chat bots, some of them are:\n\nThere exist many platforms where you can deploy your bot and make it available for the public to use it. Some of the prominent ones are Facebook Messenger, Slack, KiK, etc. Each of the above-mentioned platforms have their own limitations and advantages. We will be using Slack as our preferred platform for deployment, because of more interactive options.\n\nIn this blog, we will see how to build your own bot over Slack that tries to predict the class of the image that is given to it.\n\nP.S: This blog assumes that user knows how to setup your bot initials on Slack.\n\nSlack bots are majorly of two types:\n\nIn this blog, we will be talking about UserBots as they model our problem statement perfectly.\n\nThat\u2019s what happens at the high level, i.e. we feed it image to our bot and it tries to predict the class of that image.\n\nLet\u2019s deep dive into more details and see how exactly is Omniscient connecting Slack and the Classification Model. You can read What\u2019s that Image? to know more about how to load and predict the pre-trained VGG model.\n\nAbove diagram shows and end-to-end flow on how an image is predicted after uploading it to slack interface. To know more about the \u201cModel\u201d, read this interesting paper.\n\nSo, can Omniscient predict anything? Obviously, NO! Possible classes from which it can predict is restricted to 1000 i.e. ImageNet Database\n\nTo play and setup this for your own environment fetch source code. \u270c\ufe0f"
    },
    {
        "url": "https://medium.com/formcept/word-clouds-old-but-insightful-12c30cd4652d?source=user_profile---------8----------------",
        "title": "Word Clouds \u2014 Old but Insightful \u2013 FORMCEPT \u2013",
        "text": "Word clouds are fun ways to think creatively about any topic. It gives user a quick and dense view of the frequency distribution of words in the text. They relatively are better visualization than bar charts, when visualizing frequency of the words. As, the number of unique words in the text can be relatively huge.\n\nIn this blog, we will see how we can add a bit of visual context to our old school word clouds to make them visually attractive and contextually relevant.\n\nWe will be working with Sentiment Dataset for our purpose that to specifically with yelp reviews.\n\nWe will first load the dataset and apply basic preprocessing to the textual content despite of reviews being positive or negative.\n\nThe above snippet is responsible for grouping all the positive and negative reviews under the same bucket.\n\nThe above snippet holds all the necessary functions that are responsible for removing noise from our dataset. I have written descriptive comments for reader to figure out the specific functions.\n\nWe are now ready to render things visually. But wait a minute! What\u2019s new in this? Don\u2019t hustle, see \u261f\n\nWe have been seeing word clouds for long time now. Let\u2019s consider 2 situations.\n\nSituation-1: Consider of a scenario wherein, you plot word cloud for +ve and -ve comments on Youtube. It will take you 3\u20135 seconds to figure out which cloud holds respective words.\n\nSituation-2: Consider of a scenario wherein, you need to mine the stock market comments over some company stock. You need to see the prominent words that are leading to the increase in the stock price over time. (i.e mind that value of stock can increase, decrease overtime) \u2014 it\u2019s an useful information for the data.\n\nTraditional simple word clouds will fail to capture all the information. Is it? Yes, I guess.\n\nWhat if we overlay it on context driven mask?\n\nWill that help? YES! \ud83d\ude0b\n\nLet\u2019s do it then \u2026 \ud83d\ude0e\n\nAbove snippet generates the word cloud with overlaying the context mask over words and save it to the disk.\n\nBelow are the resulting wordcloud that got rendered for positive and negative reviews on yelp dataset."
    },
    {
        "url": "https://hackernoon.com/beam-search-a-search-strategy-5d92fb7817f?source=user_profile---------9----------------",
        "title": "Beam Search \u2014 A Search Strategy \u2013",
        "text": "One of the use-cases wherein, beam search is applied to get relevant results is Machine Translation. For those who are not familiar with machine translation, you surely must be knowing what Google Translate is, right ? That\u2019s what I am talking about. System like those use Beam Search technique to figure out most relevant translation from set of possible candidates. Read this Wiki to know literature definition of the same.\n\nLet\u2019s discuss this strategy apropos of Machine Translation use-case. If you are someone who loves to scratch out behind the scene of any fascinating research, then you must have read somewhere, somehow that Google\u2019s Machine Translation is built upon the encoder-decoder architecture of Neural Networks. I won\u2019t be explaining that right now but might do it in future blogs. In case, you are not aware of this architecture then hop in here first to get an idea and then comeback to read the remaining.\n\nMachine translation model can be thought of as a \u201cConditional Language Model\u201d, for a system that translates French to English, the model can be thought of probability of English sentence conditioned on French sentence.\n\nDue to the probabilistic behavior of our model, there can be more than one translation to input French sentence. Should we pick any one at random? Absolutely, NO. We aim to find the most suitable translation (i.e maximize the join probability distribution over output words given the input sentence.)\n\nI won\u2019t say YES or NO but, won\u2019t recommend being greedy. Because it may get you sub-optimal translations. Until and unless you are ready to compromise on accuracy, you should not go for this approach.\n\nFor a given French sentence, consider a English Translation as \u2014\n\nTranslation 2: I am going to be visiting NY this year end.\n\nIf you go with the greedy approach, model is likely to choose Translation 2 the output instead of 1. Because the probability of \u201cgoing\u201d after \u201ci am\u201d should be maximum over the distribution of possible words in the vocabulary compared to \u201cvisiting\u201d after \u201ci am\u201d.\n\nI think you got me. \u270c\ufe0f\n\nBeam Search is an approximate search strategy that tries to solve this in a efficient way. In it\u2019s simplest representation, B (Beam width) is the only tunable hyper-parameter for tweaking translation results. B in-general decides the number of words to keep in-memory at each step to permute the possibilities.\n\nWe will illustrate this with an example at each step level.\n\nAs a part of Step-1 to Beam Search the decoder network outputs the Softmax probabilities of the top B probabilities and keep them in-memory. Let\u2019s say am, going, visiting are the top 3 probable words.\n\nAs a part of Step-2 to Beam Search we hardwire the Y1 in the decoder network and try to predict the next probable word if Y1 has already occurred.\n\nOur aim is to maximize the probability of Y1 and Y2 occurring together.\n\nHere X = x1, x2\u2026. xn (all words in the input)\n\nAgain to work in step-3, we take top B probable (Y1, Y2) pairs from step-2 and hardwire them in the decoder network and try to find conditional probability of Y3. i.e. P(Y3 | X, Y1, Y2)\n\nSimilar to previous 2 steps we again find top B probable 3 word sequences and so on. We keep this process till <EOS> is reached."
    },
    {
        "url": "https://towardsdatascience.com/training-recurrent-networks-523d3b3bad3c?source=user_profile---------10----------------",
        "title": "Training Recurrent Networks \u2013",
        "text": "Training a Recurrent Neural Networks solves our problem of learning patterns from the sequential data to some extent. The architectural design helps it achieves the same but with a downside, as it suffers from the vanishing gradient problem. We will be discussing this problem in detail in the later part of this blog.\n\nRecurrent Nets were designed to model sequential data and capture the temporal patterns in the data fed into it.\n\nThe below diagram shows RNN cell at two timestamps t=0 and t=1.\n\nThe below diagram shows Unfolded RNN at multiple timestamps. At any time stamp (t) the state (S) of the cell is\n\nIf you recall, U and W parameters are shared across the sequence over time, so as to capture sentences with same semantics and different word ordering. Otherwise, it would be hard for the network to learn these representations of sentences, as it will have to relearn every time when sentences are fed in.\n\nLet\u2019s demonstrate the process by considering error at t=3.\n\nWe sum up the contributions of each time-step to the gradients. Since, W, is used in every step till the output, we need to back-propagate error from t=3 to t=0.\n\nBefore that, let\u2019s define few variables in-hand:\n\ncan\u2019t be seen as constant because S3 depends on previous states recursively. Also, the\n\nSimilarly, the equation will be expanded for k=[0, 1, 2, 3, 4]\n\nIf you understand the derivative in the above section then you can think of the chain rule expanding to extremely long for longer sentences. Activation Functions (Tanh or Sigmoid) tend to go flat (i.e. derivate equals to Zero) for very large and very small values of linear summation at every node. Multiplying small values over long sequences makes them vanish (or what we call as neurons going dead)."
    },
    {
        "url": "https://becominghuman.ai/evaluating-the-quality-of-translation-5b6708d4838a?source=user_profile---------11----------------",
        "title": "Evaluating the Quality of Translation \u2013",
        "text": "I found a very good article on slator, where it was mentioned that\n\nBLEU score gives you the sense of the particular machine translation that you have build. BLEU was one of the first metrics to claim a high correlation with human judgments of quality. It is fast/cheap, Language agnostic, high correlation with human evaluation because of which it is widely adopted strategy for evaluating translation.\n\nBLEU Score is typically measured on the scale of 0\u20131. So, it can be thought of as the probability that is assigned to the output. Here, a score of 0 would mean that machine translated text is completely different from the human reference text whereas, a score of 1 would mean that both the outputs are a perfect match. Better score means less post-editing will be required to achieve publishable translation quality. The basic notion of automatic scoring is to have a reference that you actually compare against and then we generate a score.\n\nBLEU is highly sensitive to words that are being used in the output of the translated text. BLEU is a generic methodology that works with any language, but highly depends on the accuracy of the word tokenizer used for that specific language i.e. languages such as Chinese and Japanese where word segmentation is not as trivial as in English where the notion of word is pretty clear, a poor performing word breaking rules/model would effect the BLEU score a lot.\n\nI could have written it from scratch, but what\u2019s the need in re-inventing the wheel. We will be using NLTK for this purpose.\n\nAbove examples, clearly show best and worst cases in translation.\n\nBy default, NLTK calculates BELU-4 (cumulative 4-gram BLEU score) on sentence_bleu or corpus_bleu.\n\nI would encourage the reader to go through official docs and code for detailed understanding. Above code can be found here.\n\nFeel free to comment down your thoughts and don\u2019t forget to spread the word if you \u2764\ufe0f\u200d it."
    },
    {
        "url": "https://towardsdatascience.com/whats-that-image-fb6ab703c4a5?source=user_profile---------12----------------",
        "title": "What\u2019s that Image ? \u2013",
        "text": "This blog is my first ever step towards applying deep learning techniques to Image data. This will be more of a practical blog wherein, I will be discussing how you can do a task like image classification without having much theoretical knowledge of mathematical concepts that lay the foundation of the deep learning models.\n\nI have been listening to all the amazing results (better than humans in some cases) that people have been producing for this task. I have been reading many blogs regarding VGG (Visual Geometry Group from Oxford) Model on how using this model people are making state-of-the-art models for image classification.\n\nIn 2014, researchers from Oxford Visual Geometry Group(VGG) developed a CNN model for ILSVRC challenge. The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. VGG published a paper Deep Convolutional Networks for Large-Scale Image Recognition just after the competition. They released 16 and 19 layers DNN to the community for further development and extension on the same. To know more about what those 16/19 layers are, read through the paper.\n\nRead about ImageNet from here and here.\n\nI will be using Keras as my deep learning wand for loading the pre-trained VGG model and predicting the class of some random image passed to the model.\n\nThis is how you load the model weights using Keras.\n\nSince, the VGG model is trained on all the image resized to 224x224 pixels, so for any new image that the model will make predictions upon has to be resized to these pixel values. Below snippet shows the same using Keras.\n\nNext step requires you to convert the image to numbers. These are the pixel values of the image stored in a 2D matrix. Below snippet show the same using Keras.\n\nBy now, we have the number representation of our image. Next, step is to pre-process the image as per the same standards that were used while training of the model. As said in the paper,\n\nKeras provides an inbuilt function for the same.\n\nWe are now good to predict the class of this image \u270c\ufe0f. The predict method does the magic for you. It generates the prediction of the probability of the image belonging to each of the 1,000 known object types.\n\nKeras also provides an inbuilt function for decoding the probabilities and giving class to them. It returns classes corresponding to top 3 probabilities.\n\nIn future blogs, I will try to relate the concepts learned in this to more sophisticated AI problems like Image Captioning.\n\nSee Make your own Slack Bot on how to deploy the above application.\n\nYou can find the full code on vgg gist. Feel free to appreciate or correct at any point by commenting down below \u2935\ufe0f. Spread the word if you \u2764\ufe0f\u200d it."
    },
    {
        "url": "https://hackernoon.com/whats-the-next-word-69375dab7d35?source=user_profile---------13----------------",
        "title": "What\u2019s the next word ? \u2013",
        "text": "A Language model is a probability distribution over sequences of words. It is one of the backbones in many NLP applications such as speech recognition, machine translation, part-of-speech tagging, parsing, handwriting recognition, etc. The use of neural networks in language modeling is often called Neural Language Modeling (NLM).\n\nIn this writeup, we will be using Keras to make a NLM that will try to learn the writing style of any text and predict a follow-up word with certain probability given a seed word. Considering the problem, there is no full-proof modeling technique to this but, we will have a high-level discussion on few data modeling technique for this problem which I personally use.\n\nThis model works by taking a word as an input and predicts the probability of the follow-up word. Consider the below snippet as an example.\n\nSince computers can\u2019t understand the text directly, so we convert them into the sequence of numbers i.e. Each unique word in the corpus is given a unique number. Keras eases this part for you by providing Tokenizer Class.\n\nSince, we are modeling our problem statement as one-word input and one-word output. We will break out sequence into one-one i/o pairs as shown below:\n\nBelow code let\u2019s you achieve the same\n\nAs already discussed that, our model works by taking in a word as an input and outputs a probability distribution across all the words in the vocabulary. For our model to train in such fashion we need to convert our output sequence into the one-hot vector as this gives the network a ground truth to aim for from which we can calculate the error and update the model. We can do this in a single line using Keras as shown below:\n\nThe last segment of this code is about training a neural network model which is straight forward other than tuning the model parameters. We will be training our word associations on the fly, because of data scarcity. You can train them in advance if you have huge data. See this gist for full code.\n\nWe tested our model with some seed examples:\n\nOur model seems to perform pretty well for the given seeds. Though this model has it\u2019s limitations of capturing just one-word context window which doesn\u2019t help much in case of an ambiguous prediction."
    },
    {
        "url": "https://becominghuman.ai/word-embeddings-with-gensim-68e6322afdca?source=user_profile---------14----------------",
        "title": "Word Embeddings with Gensim \u2013",
        "text": "It is an approach that provides dense vector representation of words that try to capture the meaning of that word. It is an improvement over older Bag of Words representation in terms of accuracy and computational time. The vector length learned for each word in the text is of fixed length usually several hundred. The vector space representation of the words provides a projection where words with similar meanings are locally clustered within the space.\n\nThe use of embeddings over traditional Bag of Words is one of the key success ingredients in NLP use-cases, especially in Machine Translation.\n\nThere are mainly 2 Algorithms that are used for training such vectors.\n\nIn this blog, we will be seeing how we can train such vectors using Gensim package.\n\nNO! \ud83d\ude09 Gensim is not the only solution to get this task done, but surely one of the easiest to start with. In future blogs, I will be discussing how the same can be achieved using Keras as well.\n\nGensim is an open source Python library for Natural Language Processing. It provides an easy to load functions for pre-trained embeddings in a few formats and support of querying and creating embeddings on a custom corpus. Before feeding the raw data to your training algorithm, you might want to do some basic preprocessing on the text. You can find basic preprocessing techniques here.\n\nGensim expects input of tokenized sentences as shown below\n\nThe dataset we will be using for this task is the famous Brown Corpus.\n\nCool! \u270c\ufe0f We have our environment setup and data ready to train on. There are few parameters that user can tune for better performance though default parameters give decent results.\n\nVisualization always helps in gaining the better understanding of the model. But the genuine question that might come to your mind is that how do we visualize hundreds of dimensions? No, we need not do that. We can apply dimensionality reduction techniques like PCA to solve this issue which can help us project these hundreds to 2\u20133 dimensions. Are we losing information by this? Unfortunately Yes \ud83d\ude10, it is a lossy transformation but it OK, since we are reducing the dimensions using just to visualize the vector clusters.\n\nIt is not always feasible to have large text corpus. There exist pre-trained word vectors available openly by Google and Stanford. Thanks to the research community.\n\nGensim rescues you from this as well, as it has functions to read in pre-trained vectors.\n\nYou are now good to roll and build some state-of-the-art NLP models.\n\nJust a quick suggestion, the vectors will easily consume 2\u20133GB of your RAM, so I would suggest you using a minimum of 8GB machine or some fast lookup key value database as described in this blog.\n\nWhatever I had discussed is based on my understanding of the above topic. Please feel free to comment down below for any suggestions."
    },
    {
        "url": "https://hackernoon.com/how-readable-is-your-text-4b066aa6aa9c?source=user_profile---------15----------------",
        "title": "How Readable is your Text ? \u2013",
        "text": "How Readable is your Text ?\n\nA Language is a tool which people have been using since ages to express their thoughts and emotions. Some people use heavy vocabulary whereas some use common words to express the same. Today, we will be discussing that how we can help our computer differentiate between the two writing styles \u270c\ufe0f. There exist quiet a few statistical techniques based on language rules that can help us achieve this task. Almost all the below mentioned formulae are based on syllable or polysyllable word fraction the text with few weight tweaks here and there. In all the methods there is nothing to explain as they are self- explanatory. So, I would be just writing down the formula.\n\nWe finally take the average of all the four scores from above and finalize it as the final readability score. You can just use any one of the methods also, I used an average to normalize the advantages and disadvantages of any one of the methods.\n\nI had tested it on few of the datasets and the approach seems to be working decent. For eg. I had tested it on Harry Potter books, there the most difficult book to read came out to be Harry Potter and the Order of the Phoenix \ud83d\ude31 and the easiest was Harry Potter and the Philosopher\u2019s Stone \ud83d\ude0b which is actually correctly deduced.\n\nPlease read out my previous blog where I had discussed Natural Language Preprocessing. Do spread the words and clap if you \u2764\ufe0f\u200d it.\n\nI am not aware of any other advanced techniques to do this task in a much efficient way. Do let me know by commenting down below. \u2935\ufe0f"
    },
    {
        "url": "https://hackernoon.com/natural-language-preprocessing-630c28832fd1?source=user_profile---------16----------------",
        "title": "Natural Language PREprocessing \u2013",
        "text": "Preprocessing a text corpus is one of the mandatory things that has to be done for any NLP application. There are some standard steps that go along with most of the applications, whereas sometimes you need to do some customized preprocessing. In this blog, I will be discussing few data transformation steps that I personally use while working with textual data. We will also talk about the limitations of such transformations.\n\nP.S. The steps taken by me for data preprocessing are not mandatory for all applications, these ones are what I found useful over the period of time.\n\nBelow mentioned are few transformations that I usually prefer doing depending on how problem statement is defined.\n\nApart from the transformations, one also needs to smartly stack them in the pipeline. If there is an application that requires both punctuation removal and contraction expansion then punctuation removal should be done after contraction expansion else it will not make sense.\n\nHaving said that, feel free to correct me wherever you feel by commenting down below."
    },
    {
        "url": "https://hackernoon.com/reinforcement-learning-part-4-5cb408a4b368?source=user_profile---------17----------------",
        "title": "Reinforcement Learning \u2014 Part 4 \u2013",
        "text": "OpenAI is a non-profit organization founded in 2015. One major contribution that OpenAI made to the machine learning world was developing both the Gym and Universe software platforms \u2764\ufe0f. I will not be talking about OpenAI rather would be discussing their immense useful contribution Gym. If you have any idea of reinforcement learning then you should be knowing that this kind of learning doesn\u2019t just need the data but they also need an environment with which the agent interacts.\n\nGym at rescue \u270c\ufe0f . Gym is a collection of environments designed for testing and developing reinforcement learning algorithms. Gym is in Python and supports a collection of environments.\n\nInstalling it in your system is as simple as\n\nelse you can follow the full procedure here.\n\nTo start with anything else, we first need an environment where our agent will play. It is as simple as invoking make method with environment name.\n\nAbove command returns the list of all the environment supported by Gym.\n\nNow that we have our environment loaded, we would like to know the possible actions that our agent can perform.\n\nThis shows us there are a total of two actions available i.e. Left and Right.\n\nWe can make our agent take action at every step by calling the step function.\n\nGym environment will always return these four variables after any action.\n\nIn this loop, agent takes some action on the environment and discovers a new state and receives a reward."
    },
    {
        "url": "https://hackernoon.com/reinforcement-learning-part-3-38ca74a0bde4?source=user_profile---------18----------------",
        "title": "Reinforcement Learning \u2014 Part 3 \u2013",
        "text": "In this writeup we will be discussing about methods that can solve the explore exploit dilemma. It\u2019s going to be little bit more technical compared to last two blogs.\n\nThis is one of the simplest possible algorithms for trading off exploration and exploitation, where you make the decision at each step of the way; you can either choose to take a random action or choose the agent\u2019s trusted action. This algorithm more or less mimics greedy algorithm as it generally exploits the best available option, but every once in a while the Epsilon-Greedy algorithm explores the other available options.\n\nThe probability of taking random action is known as Epsilon in this algorithm and 1-Epsilon as the probability of exploiting the recommended action.\n\nIn the above figure, it is clear that algorithm at any moment exploits/chooses the best with the probability of 1-Epsilon whereas, explores the best with the probability of Epsilon/n.\n\nThere is one drawback to this algorithm, i.e when it explores it chooses equally among the all possible actions which mean it is equally likely to choose the worst action and best action. There are better approaches than this like Softmax-Action selection, which we will be discussing in the future blogs.\n\nThat\u2019s it. This is the high-level details of Epsilon-Greedy Algorithm. We will now discuss it\u2019s implementation in Python.\n\nIf we notice carefully, then we would like to initialize and update certain parameters as the code executes.\n\nWe initialize counts and values to 0 and 0.0 respectively for available actions.\n\nWe now need a function that does an update to counts and values array that we had initialized.\n\nThis completes all the important functions required to head start a cool project."
    },
    {
        "url": "https://hackernoon.com/reinforcement-learning-part-2-152fb510cc54?source=user_profile---------19----------------",
        "title": "Reinforcement Learning \u2014 Part 2 \u2013",
        "text": "In a learning process that is of trial and error type, an agent that is afraid of making mistakes can be problematic to us.\n\nLearning to balance explore vs exploit is extremely important in order to learn a successful policy.\n\n2. Multi-Arm Bandit: Slot machines are called bandits because they\u2019re taking your money. Suppose, you go to a casino and you have a choice between 3 slot machines. Each slot machine can give you a reward of 0 or 1 (trivializing). Your goal will be obvious to maximize your winning chances. Since, you don\u2019t know the win rate of each slot machine, so you will have to play/explore each of them significant times to have get an idea of which machine has higher win rate. Once, you get that higher reward machine then you can exploit it. But, you will have to spend lots of time playing suboptimal bandit arms. Earlier people used to solve this problem by A/B testing but it had it\u2019s dis-advantages like you have to predetermine ahead of time the number of times you need to play each of the bandit to get statistical significance. It also doesn\u2019t allow to stop the tests early even if you have calculated 90% success rate for bandit 1 after certain episodes."
    },
    {
        "url": "https://hackernoon.com/reinforcement-learning-part-1-9b20c4f24901?source=user_profile---------20----------------",
        "title": "Reinforcement Learning \u2014 Part 1 \u2013",
        "text": "The term reinforcement learning was first heard by the world in 1950s. It was figured out that by mimicking the behavior of humans, dogs, and other biological entities, machines can actually do some incredible things.\n\nIn 2013, Reinforcement Learning, as Machine Learning methodology started it\u2019s revolution when couple of researchers called DeepMind made software to learn Atari Games without any prior knowledge about game rules.\n\nIn 2016, Deep Mind\u2019s AlphaGo beat the world champion in the game of Go. Read the Google DeepMind\u2019s AlphaGo: How it works to know more.\n\nAn example of this is training your dog. How do you teach your dog to sit down whenever you give the command ? You tell him the command million times, and when it finally sits down, you reward him with the treat as reward."
    }
]