[
    {
        "url": "https://ayearofai.com/rohan-lenny-3-recurrent-neural-networks-10300100899b?source=---------0",
        "title": "Rohan & Lenny #3: Recurrent Neural Networks & LSTMs",
        "text": "It seems like most of our posts on this blog start with \u201cWe\u2019re back!\u201d, so\u2026 you know the drill. It\u2019s been a while since our last post \u2014 just over 5 months \u2014 but it certainly doesn\u2019t feel that way. Whether our articles are more spaced out than we\u2019d like them to be, well, we haven\u2019t actually discussed that yet. But I, Rohan, would definitely like to get into a more frequent routine. Since November, we\u2019ve been grinding on school (basically, getting it over and done with), banging out Contra v2, and lazing around more than we should. End of senior year is a fun time.\n\nIt\u2019s 2017. We started A Year Of AI in 2016. Last year. Don\u2019t panic, though. If you\u2019ve read our letter, you\u2019ll know that, despite our name and inception date, we\u2019re not going anywhere anytime soon. There\u2019s a good chance we\u2019ll move off Medium, but we\u2019re still both obsessed with AI and writing these posts to hopefully make other people obsessed, as well.\n\nI wrote the first article on this blog just over a year ago, and mentioned that my goal for the year was to be accepted into Stanford University as an undergrad student. A few months ago, I achieved this goal. At Stanford, I\u2019ll probably be studying Symbolic Systems, which is a program that explores both the humanities and STEM to inform an understanding of artificial intelligence and the nature of minds. Needless to say, A Year of AI will continue to document the new things I learn \ud83d\ude00.\n\nAnyways, you can find plenty of articles on recurrent neural networks (RNNs) online. My favorite one, personally, is from Andrej Karpathy\u2019s blog. I read it about 1.5 years ago when I was learning about RNNs. We definitely think there\u2019s space to simplify the topic even more, though. As usual, that\u2019s our aim for the article \u2014 to teach you RNNs in a fun, simple manner. We\u2019re also importantly doing this for completion purposes; we want people to hop onto A Year of AI and be able to work their way up all the way from logistic regression to neural machine translation (don\u2019t worry, you\u2019ll find out what means soon enough), and thus recurrent neural networks is a vital addition. After this, we want to look at and summarize/simplify a bunch of new super interesting research papers, and for most of them RNNs are a key ingredient. Finally, we think this article contains so much meat and ties together content unlike any other RNN tutorial on the interwebs.\n\nBefore we get started, you should try to familiarize yourself with \u201cvanilla\u201d neural networks. If you need a refresher, check out our neural networks and backpropogation mega-post from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc. Reading our article on convolutional neural networks may also make you more comfortable entering this post, especially because we often reference CNNs. Checking out this article I wrote on vanishing gradients will help later on, as well.\n\nRule of thumb: the more you know, the better!\n\nI can\u2019t link to each section, but here\u2019s what we cover in this article (save the intro and conclusion):\n\nThere are a number of very important tasks that ANNs and CNNs cannot solve, that RNNs are used for instead. Tasks like: image captioning, language translation, sentiment classification, predictive typing, video classification, natural language processing, speech recognition, and a lot more interesting things that have been presented in recent research papers (for example\u2026 learning to learn by gradient descent by gradient descent!).\n\nRNNs are very powerful. Y\u2019know how regular neural networks have been proved to be \u201cuniversal function approximators\u201d ? If you didn\u2019t:\n\nThat\u2019s pretty confusing. Basically, what this states is that an artificial neural network can compute any function. Even if someone gives you an extremely wiggly, complex looking function, it\u2019s guaranteed that there exists a neural network that can produce (or at least extremely closely approximate) it. The proof itself is very complex, but this is a brilliant article offering a visual approach as to why it\u2019s true.\n\nSo, that\u2019s great. ANNs are universal function approximators. RNNs take it a step further, though; they can compute/describe programs. In fact, some RNNs with proper weights and architecture qualify as Turing Complete:\n\nThat\u2019s cool, isn\u2019t it? Now, this is all theoretical, and in practice means less than you think, so don\u2019t get too hyped. Hopefully, though, this gives some more insight into why RNNs are super important for future developments in machine learning \u2014 and why you should read on.\n\nAt this point, if you weren\u2019t previously hooked on learning what the heck these things are, you should be now. (If you still aren\u2019t, just bare with me. Things will get spicy soon.) So, let\u2019s dive in.\n\nWe took a bit of a detour to talk about how great RNNs are, but haven\u2019t focused on why ANNs can\u2019t perform well in the tasks that RNNs can.\n\nIt boils down to a few things:\n\nLet\u2019s address the first three points individually. The first issue refers to the fact that ANNs have a fixed input size and a fixed output size. ANNs have an elaborate list of hyperparameters, and this notably includes the number of neurons in the input layer and output layer. But what if we wanted input data and/or output data of variable size, instead of something that needs to have its size as a preset constant? RNNs allow us to do that. In this aspect, they offer more flexibility than ANNs.\n\nI\u2019ll give you a couple examples of why this matters.\n\nIt\u2019s unclear how we could use an ANN by itself to perform a task like image captioning, because the network would need to output a sentence \u2014 a list of words in a specific order \u2014 which is a sequence. It would be a sequence of vectors, because each word would need to be represented numerically. In machine learning and data science, we represent words numerically as vectors; these are called word embeddings. An ANN can only output a single word/label, like in image classification where we treat the output as the label with the highest value in the final vector that is a softmax probability distribution over all classes. The only way to make sentences work with ANNs would be to have billions of output neurons that each map to a single possible sentence in the permutation of all [sensible] sentences that can be formed by the vocabulary we have. And that doesn\u2019t sound like a good idea.\n\nWow, that was a lot of words. Nevertheless, I hope it\u2019s clear that, with ANNs, there\u2019s no feasible way to output a sequence.\n\nNow, what about inputting a sequence into an ANN? In other words, \u201ctemporal\u201d data: data that varies over time, and is thus a sequence. Take the example of sentiment classification where we input a sentence (sequence of words = sequence of vectors = sequence of set of values where each value goes into an individual neuron) and want to output its sentiment: positive or negative. The output part seems easy, because it\u2019s just one neuron that\u2019s either rounded to 1 (positive) or 0 (negative). And, for the input, you might be thinking: couldn\u2019t we input each \u201cset of values\u201d separately? Input the first word, wait for the neural net to fully feed forward and produce an output, then input the next word, etc. etc.\n\nLet\u2019s take the case of this utterly false, and most certainly negative sentence, to evaluate:\n\nWe\u2019d input \u201cLenny\u201d first, then \u201cKhazan\u201d, then \u201cis\u201d, etc. But, at each feedforward iteration, the output would be completely useless. Why? Because the output would be dependent on only that word. We\u2019d be finding the sentiment of a single word, which is useless, because we want the sentiment of the entire sentence. Sentiment analysis only makes sense when all the words come together, dependent on each other, to form a sentence.\n\nThink of it this way \u2014 this means you\u2019re essentially running a neural network a bunch of times, just with new data at each separate iteration. Those run-throughs aren\u2019t linked in any way; they\u2019re independent. Once you feedforward and fully run the neural network, it forgets everything it just did. This sentence only makes sense and can only be interpretable because it\u2019s a collection of words put together in a specific order to form meaning. The relevance of each word is dependent on the words that precede it: the context. This is why RNNs are being used heavily in NLP; they retain context by having memory. ANNs have no memory.\n\nI like this quote from another article on RNNs:\n\nRNNs don\u2019t just need memory; they need long term memory. Let\u2019s take the example of predictive typing. Let\u2019s say we typed the following sentence in an SMS message to 911, and the operating system needs to fill in the blank:\n\nHere, if the RNN wasn\u2019t able to look back much (ie. before \u201cshould\u201d), then many different options could arise:\n\nThe word \u201csent\u201d would indicate to the RNN that a location needs to be outputted. However, if the RNN was able to retain information from all the way back, such as the word \u201ccriminal\u201d, then it would be much more confident that:\n\nThe probability of outputting \u201cjail\u201d drastically increases when it sees the word \u201ccriminal\u201d is present. That\u2019s why context matters, be it predictive typing, image captioning, machine translation, etc. The output or outputs of a recurrent neural network will always be functionally dependent on (meaning, a function of) information from the very beginning, but how much it chooses to \u201cforget\u201d or \u201cretain\u201d (that is, varying degrees of influence from earlier information) depends on the weights that it learns from the training data.\n\nAs it turns out, RNNs \u2014 especially deep ones \u2014 are rarely good at retaining much information, due to an issue called the vanishing gradient problem. That\u2019s where we turn to other variants of RNNs such as LSTMs and GRUs. But, more on that later.\n\nTo address the third point, one more constraint with ANNs is that they have a fixed number of computation/processing steps (because, once again, the number of hidden layers is a hyperparameter). With RNNs, we can have much more dynamic processing since we operate over vectors. Each neuron in an RNN is almost like an entire layer in an ANN; this will make more sense as we bring up an illustration for you. Exciting stuff.\n\nOK, that\u2019s enough teasing. Three sections into the article, and you\u2019re yet to see what an RNN looks like, or appreciate how it really works. Everything comes in due time, though!\n\nThe first thing I\u2019m going to do is show you what a normal ANN diagram looks like:\n\nEach neuron stores a single scalar value. Thus, each layer can be considered a vector.\n\nNow I\u2019m going to show you what this ANN looks like in our RNN visual notation:\n\nThe two diagrams above represent the same thing. The latter, obviously, looks more succinct than the former. That\u2019s because, with our RNN visual notation, each neuron (inputs, hidden(s), and outputs) contains a vector of information. The term \u201ccell\u201d is also used, and is interchangeable with neuron. (I\u2019ll use the latter instead of the former.) Red is the input neuron, blue is the hidden neuron, and green is the output neuron. Therefore, an entire ANN layer is encapsulated into one neuron with our RNN illustration. All operations in RNNs, like the mapping from one neuron\u2019s state to another, are over entire vectors, compared to individual scalars that are summed up with ANNs.\n\nLet\u2019s flip it the other way:\n\nThis is in fact a type of recurrent neural network \u2014 a one to one recurrent net, because it maps one input to one output. A one to one recurrent net is equivalent to an artificial neural net.\n\nWe can have a one to many recurrent net, where one input is mapped to multiple outputs. An example of this would be image captioning \u2014 the input would be the image in some processed form (usually the result of a CNN analyzing the image), and the output would be a sequence of words. Such an RNN may look like this:\n\nThis may be confusing at first, so I\u2019m going to make sure I walk slowly through it. On the x-axis we have time, and on the y-axis we have depth/layers:\n\nWhen I refer to \u201ctime\u201d on the x-axis, I\u2019m referring to the order at which these operations occur. Time could also be literal for temporal data, where the input is a sequence. When I say \u201cdepth\u201d on the y-axis, I\u2019m referring to the mapping from the input layer, to the hidden layer(s), to the output layer, where layer number and thus depth increases.\n\nIt may look like we have seven neurons now, but we still have three: one input neuron, one hidden neuron, and one output neuron. The difference is that these neurons now experience multiple \u201ctimesteps\u201d where they take on different values, which are, again, vectors. The input neuron in our example above doesn\u2019t, because it\u2019s not representing sequential data (one to many), but for other architectures it could.\n\nThe hidden neuron will take on the vector value h_1 first, then h_2, and finally h_3. At each timestep, the hidden neuron\u2019s vector h_t is a function of the vector at the previous timestep h_t-1, except for h_1 which is dependent only on the input x_1. In the diagram above, each hidden vector then gives rise to an output y_t, and this is how we map one input to multiple outputs. You can visualize these functional dependencies with the arrows, which illustrates flow of information in the network.\n\nAs we progress on the x-axis, the current timestep increases. As we progress on the y-axis, the neuron in question changes. Each point on this graph thus represents one neuron \u2014 be it input, hidden, or output \u2014 at some timestep, being fed information from a neuron (be it itself or another) at the previous timestep.\n\nThe RNN would execute like so:\n\nYou could compute y_t either immediately after h_t has been computed, or, like above, compute all outputs once all hidden states have been computed. I\u2019m not entirely sure which is more common in practice.\n\nThis allows for more complex and interesting networks than ANNs because we can have as many timesteps as we want.\n\nThe value of the output neuron at each timestep represents a word in the sentence, in the order the sentence will be constructed. The caption this RNN produces is hence 3 words long. (It\u2019s actually 2, because the RNN would need to output a period or <END> marker at the final timestep, but we\u2019ll get into that later.)\n\nIn case you don\u2019t understand yet exactly why RNNs work, I\u2019ll walk through how these functional dependencies come to fruition when you apply it to a one to many scenario such as image captioning.\n\nWhen you combine an RNN and CNN, you \u2014 in practice \u2014 get an \u201cLCRN\u201d. The architecture for LCRNs are more complex than what I\u2019m going to present in the next paragraph; rather, I\u2019m going to simplify it to convey my point. We\u2019ll actually get fully into how they work later.\n\nImagine an RNN tries to caption this image. An accurate result might be:\n\nThe input to the RNN would be the output of a CNN that processes this image. (However, to be pedantic, it would be the output of the CNN without a classification/softmax layer \u2014 that is, pulled from the final fully connected layer.) The CNN might pick up on the fact that there are two primary human face-like objects present in the image, which, paired with what the RNN has learned via training, may induce the first hidden state\u00b9 of the recurrent neural network to be one where the most likely candidate word is \u201ctwo\u201d.\n\nPro-tip\u00b9: The term \u201chidden state\u201d refers to the vector of a hidden neuron at a given timestep. \u201cFirst hidden state\u201d refers to the hidden state at timestep 1.\n\nThe first output, which represents the word \u201ctwo\u201d, was functionally dependent on the first hidden state, which in itself was a function of the input to the RNN. Thus, \u201ctwo\u201d was ultimately determined from the information that the CNN gave us and the experience/weights of the RNN. Now, the second word, \u201cpeople\u201d, is functionally dependent on the second hidden state. However, note that the second hidden state is just a function of the first hidden state. This means that the word \u201cpeople\u201d was the most likely candidate given the hidden state where \u201ctwo\u201d was likely. In other words, the RNN recognized that, given the word \u201ctwo\u201d, the word \u201cpeople\u201d should be next, based on the RNN\u2019s experience from training and the initial image [analysis] we inputted.\n\nThe same will occur for every following word; the nth word will be based on the nth hidden state, which, ultimately, is a function of every hidden state before it, and thus could be interpreted purely as an extremely complex and layered function of the input. The weights do the heavy lifting by making sense of all this information and deducing an output from it.\n\nTo put it bluntly, you can boil down what the RNN is \u201cthinking\u201d to this:\n\nThus, each outputted word is dependent on the words before it, all the way back to the input image data. However, this relationship is indirect. It\u2019s indirect because the outputs are only dependent on the hidden states, not on each other (ie. the RNN doesn\u2019t deduce \u201cpeople\u201d from \u201ctwo\u201d, it deduces \u201cpeople\u201d, partly, from the information \u2014 the hidden state \u2014 that gave rise to \u201ctwo\u201d). In LCRNs, though, this is explicit instead of implicit; we \u201csample\u201d the output of one timestep by taking it and literally feeding it back as input into the next timestep. In a sense, LCRNs can hence be interpreted as having many to many architecture.\n\nThe exact quantitative relationships depend on the RNN\u2019s weights. But, generally, this is the concept of memory in play. Creating a coherent sentence as we go along is only really possible if we can recall what we said before. And RNNs are able to do exactly that; they remember what they said before and figure out, based on their image captioning expertise, what from this is useful to continue accurately speaking.\n\nObviously, an RNN needs to be trained and have proper weights for this to all function properly. RNNs aren\u2019t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.\n\nPerhaps this was a bit over-explaining on my part, but hopefully I nailed down some important and core ideas about how RNNs function.\n\nSo far we\u2019ve looked at one to one and one to many recurrent networks. We can also have many to one:\n\nWith many to one (and many to many), the input is in the form of a sequence, and so the hidden states are functionally dependent on both the input at that timestep and the previous hidden state. This is different to one to many, where the hidden state after h_1 is only dependent on the previous hidden state. That\u2019s why, in the image above, the second hidden state has two arrows directed at it.\n\nOnly one output exists in many to one architecture. An example application is sentiment classification, where the input is a sentence (sequence of words) and the output is a probability indicating that the inputted sentence was positive.\n\nThe final type of recurrent net is many to many, where both the input and output are sequential:\n\nA use case would be machine translation where a sequence of words in one language needs to be translated to a sequence of words in another.\n\nWe can also go deeper and have multiple hidden layers, and/or a greater number of timesteps:\n\nReally, this could be considered as multiple RNNs. Technically, you can consider each \u201chidden layer\u201d as an RNN itself, given each neuron operates on vectors and updates through time; in ANN context, that volume of operations would be considered an entire network. So this is like stacking RNNs on top of each other. However, in this article I\u2019ll refer to it as multiple hidden layers; different papers and lecturers may take different approaches.\n\nWhen we have many timesteps (usually hundreds) and multiple hidden layers, the architecture of the network becomes much more complex and interesting. One feature of this RNN, in particular, is that all the outputs, including the first, depend on not just the input up to that timestep, but all of the inputs. (You can see this because the green neuron is only introduced after the final input timestep.) If this RNN was to translate English to Chinese, the first word of translated Chinese isn\u2019t just dependent on the first word of the inputted English; it\u2019s dependent on the entire sentence.\n\nOne way to demonstrate why this matters is to use Google Translate:\n\nNow I\u2019ll input \u201cHe\u2019s a victim\u201d and \u201cof his own time\u201d separately. You\u2019ll notice that when you join the two translated outputs, this won\u2019t be equal to the corresponding phrase in the first translation:\n\nWhat gives? Well, the way sentences are constructed in languages can differ in varying scenarios. Some words in English may also map to multiple different words in Chinese, depending on how it\u2019s used. It all depends on the context and the entire sentence as a whole \u2014 the meaning you\u2019re trying to convey. This is the exact approach a human translator would take.\n\nAnother type of many to many architecture exists where each neuron has a state at every timestep, in a \u201csynchronized\u201d fashion. Here, each output is only dependent on the inputs that were fed in during or before it. Because of this, synchronized many to many probably wouldn\u2019t be suitable for translation.\n\nAn application for this could be video classification where each frame needs to be mapped to some sort of class or label. Interesting note \u2014 an RNN is better at this task than CNNs are because what\u2019s going on in a scene is much easier to understand if you\u2019ve watched the video up to that point and thus can contextualize it. That\u2019s what humans do!\n\nQuick note: we can \u201cwrap\u201d the RNN into a much more succinct form, where we collapse the depth and time properties, like so:\n\nThis notation demonstrates that RNNs take input, process that input through multiple timesteps and hidden layers, and produce output. The arrow both leaving and entering the RNN conveys that an RNN hidden state is functionally dependent on the hidden state at the preceding timestep; it\u2019s sort of like a loop that feeds itself.\n\nWhen you ever read about \u201cunrolling\u201d an RNN into a feedforward network that looks like it\u2019s in the same collapsed format as the diagram above, this means we expand it to show all timesteps and hidden layers like we did before.\n\nAnother quick note: when somebody or a research paper mentions that they are using \u201c512 RNN units\u201d, this translates to: \u201c1 RNN neuron that outputs a 512-wide vector\u201d; that is, a vector with 512 values. At first, I thought this meant that maybe at each timestep there were 512 separate neurons somehow working in conjunction, but nope, it\u2019s luckily much simpler than that\u2026 albeit strangely worded.\n\nFurthermore, one \u201cRNN unit\u201d usually refers to an RNN with one hidden layer; thus, instead of defining RNN as something that is multilayer inherently, we often see people use the phrase like: \u201cstacking RNNs on top of each other\u201d. Each RNN will have its on weights, but connecting them gives rise to an overarching multilayer RNN. In this article, we treat recurrent neural networks as a model that can have variable timesteps t and fixed layers \u2113, just make sure you understand that this is not always the case. Our formalism, especially for weights, will slightly differ.\n\nSo, now, let\u2019s walk through the formal mathematical notation involved in RNNs.\n\nIf an input or output neuron has a value at timestep t, we denote the vector as:\n\nFor the hidden neurons it\u2019s a bit different; since we can have multiple hidden layers, we denote the hidden state vector at timestep t and hidden layer \u2113 as:\n\nThe input is obviously some preset values that we know. The outputs and hidden states are not; they are calculated.\n\nLet\u2019s start with hidden states. First, we\u2019ll revisit the most complex recurrent net we came across earlier \u2014 the many to many architecture:\n\nThis RNN has: sequential input, sequential output, multiple timesteps, and multiple hidden layers. The formula we derive for this RNN should generalize for all others.\n\nFirst, let\u2019s list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:\n\nA hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists.\n\nIf this is all difficult to follow, make sure once again to look at and trace back the arrows in the RNN that illustrate flow of information throughout the network.\n\nBecause of the impossible combination, we define two separate equations: an equation for the hidden state at hidden layer 1, and for layers after 1.\n\nThis probably looks a bit confusing; let me break it down for you. The function \u0192w computes the numeric hidden state vector for timestep t and layer \u2113; it contains the \u201cactivation function\u201d you\u2019re used to hearing about with ANNs. W are the weights of the recurrent net, and thus \u0192 is conditioned on W. We haven\u2019t exactly defined \u0192 just yet, but what\u2019s important to note is the two parameters it takes. Once you do, this notation simply states what we have stated before in plain English:\n\nYou might notice that we have a couple issues:\n\nIf these are 0, then the invalid functional dependency stops existing, and our formal notation still holds up.\n\nWe actually have five different types of weight matrices:\n\nPro-tip: The indices for each weight matrix tell you what they are used for in the recurrent net. W_xh maps an input vector x to a hidden state vector h. W_hht maps a hidden state vector h to another hidden state vector h along the time axis, ie. from h_t-1 to h_t. On the other hand, W_hhd maps a hidden state vector h to another hidden state vector h along the depth axis, ie. from h^(\u2113-1)_t to h^\u2113_t. W_hy maps a hidden state vector h to an output vector y.\n\nLike with ANNs, we also learn and add a constant bias vector, denoted b_h, that can vertically shift what we pass to the activation function. We can also shift our outputs with b_y. More about bias units here.\n\nFor both b_h and W_hht/W_hhd, we actually have multiple weight matrices depending on the value of \u2113, as indicated by the superscript. This is because each hidden layer can have a different set of weights (the network would be extremely uninteresting if this wasn\u2019t the case), including the bias vector. However, inside a single hidden layer, all timesteps share the same weight matrix. This is important because the number of timesteps is a variable; we may train on sequences with up to 20 values, but in practice output sequences with up to 30 values \u2014 10 extra timesteps. If each timestep had an independent weight to learn, those last 10 timesteps wouldn\u2019t have anything to use. Since this would also mean that the number of parameters in the neural network would grow linearly relative to the input, we would have way too many parameters very potentially causing overfitting.\n\nW_hy is just one matrix because only the final layer gives rise to the outputs denoted y. At the final hidden layer \u2113, we could suggest that W_hhd will not exist because W_hy will be in its place.\n\nThe function is very similar to the ANN hidden function you\u2019ve seen before; it applies the correct weights to the corresponding parameters, adds the bias, and passes this weighted sum through an activation or \u201csquashing\u201d function to introduce non-linearities. The key difference, though, is that this is not a weighted sum but rather a weighted sum vector; any W \u22c5 h, along with the bias, will have the dimensions of a vector. The tanh function will thus simply output a vector where each value is the tanh of what it was in the inputted vector (sort of like an element-wise tanh). Remember, this contrasts ANNs because RNNs operate over vectors versus scalars.\n\nIf you\u2019ve followed our blog so far, you most likely know about two activation functions: sigmoid and ReLU. tanh is another such function. We mostly use the tanh function with RNNs. This is, I think, mostly because of their role in LSTMs (a variant of RNNs that are used more than RNNs \u2014 more on that later), the fact that they produce gradients with a greater range, and that their second derivative don\u2019t die off as quickly.\n\nSimilar to sigmoid, the tanh function has two horizontal asymptotes and a smooth S-shape. The main difference is that the tanh function asymptotes at y = -1 instead of y = 0, intercepting the y-axis at y = 0 instead of y = 0.5. Thus, the tanh function has a greater range than the sigmoid.\n\nIf interested, the tanh equation follows (though I won\u2019t walk you through it):\n\nThe final equation is mapping a hidden state to an output.\n\nThis is one such possible equation. Depending on the context, we might also remove the bias vector, apply a non-linearity like sigmoid/softmax (for example if the output needs to be a probability distribution), etc.\n\nAnd that\u2019s how we express recurrent nets, mathematically!\n\nQuick note: Notation may and will differ between various lectures, research paper, articles, etc. For example \u2014 some research papers may start indexing at 0 instead of 1. More drastically, most RNN notation is much more general than mine to promote simplicity, ie. doesn\u2019t cover edge cases like I did or obfuscates certain indices like \u2113 with hidden to hidden weight matrices. So, just keep note that specifics don\u2019t always transfer over and avoid being confused by this. The reason I was meticulous about notation in this article is that I wanted to ensure you understood exactly how RNNs work, fueled by my frustration with the very same problem ~1.5 years ago.\n\nLet\u2019s take a look at a quick example of an RNN in action. I\u2019m going to adapt a super dumbed down one from Andrej Karpathy\u2019s Stanford CS231n RNN lecture, where a one to many \u201ccharacter level language model\u201d single layer recurrent neural network needs to output \u201chello\u201d. We\u2019ll kick it of by giving the RNN the letter \u201ch\u201d , such that it needs to complete the word by outputting the other four letters.\n\nSidenote: this model nicknamed \u201cchar-rnn\u201d \u2014 remember it for later, where we get to code our own!\n\nThe neural network has the vocabulary: h, e, l , o. That is, it only knows these four characters; exactly enough to produce the word \u201chello\u201d. We will input the first character, \u201ch\u201d, and from there expect the output at the following timesteps to be: \u201ce\u201d, \u201cl\u201d, \u201cl\u201d, and \u201co\u201d respectively, to form:\n\nWe can represent input and output via one hot encoding, where each character is a vector with a 1 at the corresponding character position and otherwise all 0s. For example, since our vocabulary is [h, e, l, o], we can represent characters using a vector with four values, where a 1 in the first, second, third, and fourth position would represent \u201ch\u201d, \u201ce\u201d, \u201cl\u201d, and \u201co\u201d respectively.\n\nThis is what we\u2019d expect with a trained RNN:\n\nAs you can see, we input the first letter and the word is completed. We don\u2019t know exactly what the hidden states will be \u2014 that\u2019s why they\u2019re hidden!\n\nOne interesting technique would be to sample the output at each timestep and feed it into the next as input:\n\nWhen we \u201csample\u201d from a distribution, we select a random character probabilistically following the distribution. For example, in the diagram above, the character with the highest likeliness is \u201ce\u201d at the first timestep\u2019s output. Let\u2019s say this likeliness is, concretely, 0.9. Now, when we sample into the next timestep\u2019s input, there\u2019s a 90% chance we select \u201ce\u201d; most of the time we will pick the most likely character, but not every time. This adds a level of randomness so you don\u2019t end up in a loop where you keep sampling the same letter or sequence of letters over and over again.\n\nAs mentioned earlier, this is used pretty heavily with LCRNs. It\u2019s even more effective than only relying on the memory of the RNN to output the correct letter at the future timesteps. In a sense, this makes the recurrent net many to many. (Though, not really, because we still only have one preset input.)\n\nHowever, to be clear, this does not mean that the RNN can only rely on these sampled inputs. For example, at timestep 3 the input is \u201cl\u201d and the expected output is also \u201cl\u201d. However, at timestep 4, the input is again \u201cl\u201d but the output is now \u201co\u201d, to complete the word. Memory is still needed to make a distinction like this.\n\nIn numerical form, it would look something like this:\n\nOf course, we won\u2019t get a one-hot vector output during prediction mode; rather, we will get a probability distribution over each letter (so we\u2019d apply softmax to the output), and will sample from this distribution to get a single character output.\n\nEach hidden state would contain a similar sort of vector, though not necessarily something we could interpret like we can for the output.\n\nThe RNN is saying: given \u201ch\u201d, \u201ce\u201d is most likely to be the next character. Given \u201che\u201d, \u201cl\u201d is the next likely character. With \u201chel\u201d, \u201cl\u201d should be next, and with \u201chell\u201d, the final character should be \u201co\u201d.\n\nBut, if the neural network wasn\u2019t trained on the word \u201chello\u201d, and thus didn\u2019t have optimal weights (ie. just randomly initialized weights), then we\u2019d have garble like \u201chleol\u201d coming out.\n\nOne more important thing to note: start and end tokens. They signify when input begins and when output ends. For example, when the final character is outputted (\u201co\u201d), we can sample this back as input and expect that the \u201c<END>\u201d token (however we choose to represent it \u2014 could also use a period) will be outputted at the next timestep; this is the RNN telling us that it has completed the word and its processing as a whole. The use case isn\u2019t as obvious in this fabricated example, because we know when \u201chello\u201d has been completed, but consider a real-life scenario where we don\u2019t: image captioning. In image captioning, the caption could be 1, 2, 3, or n words long, given a reasonable upper limit of n. The end token tells us when the caption has been completed, so we can halt the RNN and complete the prediction loop (which would keep going forever if we were using or stop after the upper limit/max possible preset constant value of n is reached).\n\nStart tokens are more used for generating content from complete scratch. For example, imagine an RNN read and learned from a bunch of Shakespeare. (This is an actual funny application of character level language models that Karpathy implemented, and we\u2019ll see it in action on a later section.) Now, based on what the RNN learned, we want it to create a brand new Shakespearean sonnet! Feeding in a \u201c<START>\u201d token enables it to kick this process off and begin writing without us giving the network some arbitrary pre-determined initial word or character.\n\nI\u2019ve also noticed that another potential use case of start tokens is when we have some other sort of initial input, like CNN produced image data with image captioning, that doesn\u2019t \u201cfit\u201d what we\u2019ll normally use for input at timesteps after t=1 (the word outputted at the previous timestep via sampling). As a result, we feed this data directly to the first hidden state and set the input as \u201c<START>\u201d instead.\n\nNow, just to be clear, the RNN doesn\u2019t magically output these end tokens and recognize the start tokens. We have to add them, along with start tokens, to the training data and vocabulary such that they can be outputted by the recurrent net during prediction time.\n\nThis is how we can get RNNs to \u201cwrite\u201d! More on some examples of text RNNs have actually generated, Shakespeare most certainly included, in a later section.\n\nFor a recurrent net to be useful, it needs to learn proper weights via training. That\u2019s no surprise.\n\nThis is, of course, because we initialize the W weights randomly at first, so random stuff will come out.\n\nBut, through multiple iterations of training with a first-order optimization algorithm like gradient descent, we perturb the weights such that the probability of each correct character being outputted at their respective timestep increases. The actual output would be \u201chello\u201d in one-hot encoding form, and we\u2019d compute the discrepancy between this output and what the recurrent net predicts (we\u2019d get the error at each timestep and then add this up) as the total error to then calculate the gradient/update value.\n\nSo, each output contributes to the error somehow. If the error is an addition of the outputs, then, if we had something like Y outputs, we\u2019d need to backpropagate them individually and add these up. This is because derivatives are distributed evenly when we\u2019re differentiating a sum:\n\nBut, you should know that, with artificial neural networks, calculating these gradients isn\u2019t that easy. We have so many weights contributing to the output, and thus need to figure out exactly how much these weights contribute, and by how much we modify them to decrease overall error. To do this, we use the backpropagation algorithm; this algorithm propagates the error between the predicted output of a recurrent net and the actual output in the dataset all the way back to the beginning of the network. Using the chain rule from differential calculus, backprop helps us calculate the gradients of the output error w.r.t. each individual weight (sort of like the error of each individual weight).\n\nOnce we have those gradients, we have to use an optimization algorithm to calculate the update values and make the updates. We can use the vanilla gradient descent algorithm to do this, but there are many other possible, better variants as well; learn about them by reading this article, if you want. (I think we\u2019re long overdue for our own mega-post on optimization!)\n\nBackpropagation with RNNs is called \u201cBackpropagation Through Time\u201d (short for BPTT), since it operates on sequences in time. But don\u2019t be fooled \u2014 there\u2019s not much difference between normal backprop and BPTT; when it comes down to it, BPTT is just backprop, but on RNNs! Remember that when you \u201cunroll\u201d an RNN, it essentially becomes a feedforward network; not an ANN, but a feedforward network in the sense that we can visualize where all the information is flowing and observe the activations at each neuron and timestep, all the way from the input to the final output. Like ANNs, RNNs have functional dependencies that link the entire network together; it\u2019s just that RNNs operate over vectors instead (yay for matrix calculus?) and extend in depth as well as time. There\u2019s more work to do to compute the gradients, but it\u2019s no surprise that backprop works pretty much the same way for recurrent nets that it would for normal ones. Because of this, I\u2019m not going to walk through all the math and show the derivatives etc. Read our backprop mega-post for all that jazz.\n\nOne thing to note is that, since we have multiple timesteps in our RNN, each timestep in a single layer will want to change the weight in a different way and have different gradients. However, remember that each hidden layer uses only one weight matrix because the number of timesteps is a variable. Thus, we just average or sum the weight updates between these timesteps and apply this as an update to the W_hh for that entire layer. Also, a general practice is to train on shorter sequences first and then gradually increase sequence size as we train on more and more data.\n\nNow, if you haven\u2019t already, make sure to read this article that I wrote on vanishing and exploding gradients before proceeding:\n\nYou may be thinking: how does this issue apply to RNNs? Well, RNNs are very deep models; on top of often having multiple hidden layers, each hidden layer in practice can have hundreds of timesteps. That\u2019s like an ANN with hundreds of entire hidden layers! That\u2019s deep. (Well, it\u2019s more long because we\u2019re dealing with the time axis here, but you know what I mean.) tanh derivatives are very similar to sigmoid derivatives in range, so the problem of vanishing gradients is thus even more drastic with RNNs than with ANNs, and training them becomes almost impossible.\n\nImagine trying to propagate the error to the 1st timestep in an RNN with k timesteps. The derivative would look something like this:\n\nWith a tanh activation function, that\u2019s freaking crazy. Then, for getting the derivative of the error with respect to a weight matrix W_hh, we\u2019d add \u2014 or, as mentioned before, we could average as well \u2014 each of these hidden state error gradients, then multiplied by the derivative of the hidden state with respect to the weight, such that we can backprop from the error to the weight:\n\nSo we\u2019d be effectively adding together a bunch of terms that have vanished \u2014 the exception being very late gradients with a small number of terms \u2014 and so dJ/dWhh would only capture gradient signals from the last few timesteps. (Or, for exploding gradients, it would become infinity).\n\nBut, you might be asking, instead of tanh \u2014 which is bounded between -1 and 1, and has a similar problem to sigmoid where the peak of the derivative is smaller than 1 \u2014 why don\u2019t we just use ReLUs? Don\u2019t ReLUs, or perhaps leaky ReLUs, solve the vanishing gradient problem?\n\nWell, not entirely; it\u2019s not enough to solve the problem. With RNNs, the problem really lies in the architecture. Even though we could use ReLU to ensure many of the values in the gradient computation are not between -1, 0, and 1 such that they vanish \u2014 or vice-versa, explode \u2014 we do still indeed have a lot of other variables other than the activation function derivative in the gradient computation such as the weights; you can revisit the mega-post on backprop we wrote to confirm this. Since weights are also normally randomly initialized in the range -1 to 1, and RNNs are like super deep ANNs, these weights keep multiplying on top of each other and potentially cause the gradients to vanish.\n\nThis is more my suspicion though \u2014 I\u2019m yet to confirm this is the case by testing. I was curious so I asked this exact question on Quora:\n\nFrom this, something interesting I learned is that: since ReLUs are unbounded (it\u2019s not restricted to be between -1 and 1 or 0 and 1) unlike sigmoid/tanh, and RNNs are very deep, the activations, especially later ones, can become too big. This is because hidden states have a multiplicative relationship; one hidden state is a multiple of the previous ones, where that multiple specifically is a weight. If we use ReLU, then the hidden state isn\u2019t limited by any range, and we could have a bunch of numbers bigger than 1 multiplying by each other.\n\nIt ends up being sort of like the exploding gradient problem, but with the values inside the neurons, not gradients. This is also what then causes the gradients to explode: large activations \u2192 large gradients \u2192 large change in weights \u2192 even bigger activations, because updating the weights in the wrong direction ever so slightly can cause the entire network to explode. This makes learning unstable:\n\nAlso well said:\n\nOther issues with ReLU functions are discussed in the article I wrote, and they similarly apply to RNNs. Generally speaking, though, they just don\u2019t work that well, especially compared to other options we have. Making RNNs perform well with ReLUs is actually a pretty hot topic of research right now, but until someone figures out something genius, RNNs are a lost cause.\n\nAnd that\u2019s why vanilla RNNs suck. Seriously. In practice, nobody uses them. Even if you didn\u2019t fully grasp this section on how the vanishing and exploding gradient/activation problem is applicable to them, it doesn\u2019t matter anyways. Because, everything you\u2019ve read up to this point so far\u2026 throw it all away. Forget about it.\n\nJust kidding. Don\u2019t do that.\n\nYou shouldn\u2019t do that because RNNs actually aren\u2019t a lost cause. They\u2019re far from it. We just need to make a few\u2026 modifications.\n\nHow about this?\n\nOK. Clearly something\u2019s not registering here. But that\u2019s fine; LSTM diagrams are frikin\u2019 difficult for beginners to grasp. I too remember when I first searched up \u201cLSTM\u201d on Google to encounter something similar to the works of art above. I reacted like this:\n\nIn this section, I\u2019m going to embark on a mission to design the first simple, comprehensible, and beautiful LSTM diagram. Wish me luck, because I\u2019ll probably fail.\n\nWith that being said, let\u2019s dive into Long Short-Term Memory networks. (Yes, that\u2019s what LSTM stands for.)"
    },
    {
        "url": "https://ayearofai.com/rohan-lenny-2-convolutional-neural-networks-5f4cd480a60b?source=---------1",
        "title": "Rohan & Lenny #2: Convolutional Neural Networks \u2013",
        "text": "We are back! College applications and senior year have drained quite a bit of our free time, but we\u2019re still alive (albeit slightly sleep deprived).\n\nLet\u2019s talk about a topic that we\u2019ve been meaning to write about for a while but haven\u2019t gotten around to doing: convolutional neural networks.\n\nBefore we get started, you should try to familiarize yourself with \u201cvanilla\u201d neural networks. If you need a refresher, check out our neural networks and backpropogation mega-post from earlier this year. This is so you know the basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax, etc.\n\nYou probably guessed it: it\u2019s computers seeing things! Duh.\n\nBut \u201cseeing\u201d is a pretty vague term. Computer vision \u2014 which is really an interdisciplinary field \u2014 is about computers being able to gain high-level understandings and make conclusions, predictions, and/or statements from some kind of visual input.\n\nLet\u2019s start talking about sub-problems in computer vision. Because, in reality, when we say \u201ccomputer vision\u201d we are not referring to one concrete task. Instead, we are referring to a group of tasks that machine learning has attempted (both successfully and less so) to solve. However, the most common task we hear about, and the one we will specifically look at in this article, is that of object classification. It\u2019s pretty much exactly what it sounds like: identifying and classifying the objects in an image.\n\nA successful object classification algorithm would look at this image and output that there is a \u201cDog\u201d present in it. An even better classification algorithm may suggest \u201cPuppy\u201d as a more accurate or precise class of the object. \u201cFlowers\u201d and \u201cGrass\u201d may also be reasonable classifications if we\u2019re trying to look for all of the objects in an image.\n\nThis is one of the hallmark problems in computer vision \u2014 being able to take an image or video and identify/classify the objects inside it. And it\u2019s not easy. Because, this is what the computer sees when it looks at that dog:\n\nSo\u2026 why do you care? And why should you care? Object classification, other than being a major step forward for understanding computer vision as a whole and furthering machine learning, has dozens of applications. Some of these applications are integral to your daily life, some are integral to the operation/growth of many companies, and some make the government or military run. Here\u2019s a small list of specific capabilities:\n\nThe same algorithm that can be used for object classification, however, can also be used to \u2014 for example \u2014 analyze an fMRI brain scan and make a predictive diagnosis on it. In this sense, the algorithm is not classifying an object but trying to look for patterns in the image that would make it similar to other fMRI scans it has seen before.\n\nComputer vision has countless other sub-problems. Object segmentation is about figuring out where exactly different objects lie in an image. Scene parsing lets us look at an image as a whole to segment it into generic categories (sky, grass, road, etc.). When dealing with video, motion tracking helps figure out how a particular object moves from frame to frame.\n\nPut simply, computer vision is a pretty big field. This article will mostly focus on classification, but CNNs are applicable to countless other problems as well.\n\nOkay, cool. So we need an algorithm to do some computer vision \u2014 why not use a good ol\u2019 neural network? Surely, deep artificial neural networks, the almighty machine learning algorithm, would be able to succeed in computer vision!\n\nWell, as it turns out, traditional neural networks don\u2019t work that well for computer vision. Actually, they really suck. Let\u2019s see why.\n\nImagine a neural network that takes an image as input and outputs a probability distribution over potential classes for the \u201cprimary\u201d object it sees in the image. Let\u2019s pretend that each of the images we are feeding into this ANN is a closeup of said object against some background, like this:\n\nLet\u2019s now say we are trying to classify cutlery only (I think you Americans call it \u201csilverware\u201d or something). Our discrete classes will be represented by the probability in the individual output nodes of the ANN. Like this:\n\nWe\u2019d expect a properly trained ANN\u2019s value in the \u201cSpoon\u201d output node to be the greatest, with \u201cSpork\u201d probably coming in second place.\n\nOk, so this all looks great. What\u2019s the issue? Well, here it is:\n\nTurning an image into a bunch of separate input neurons isn\u2019t actually the problem. That\u2019s pretty easy to do \u2014 we could just treat each neuron as each separate pixel in the image. For simplicity\u2019s sake, let\u2019s convert the picture to grayscale, such that each pixel is represented by exactly one number between 0 (black) and 1 (white). We can then \u201cflatten\u201d this 2D array of pixel values into a 1D array (that is, just concatenate each sub-array to the previous one to make a long list of all the pixels), which corresponds to the input to our neural network.\n\nHere\u2019s what that flattening process could look like visually:\n\nLet\u2019s just call this a pixel input vector. Now, let\u2019s consider \u2014 on a high level \u2014 what the ANN is doing.\n\nFirst, we\u2019ll give it a lot of spoons. So. Many. Spoons. The network will try to look at all these spoons and try to figure out what makes a spoon a spoon based on patterns in the images (e.g. the reflection from the spoon head, the convex indent that is on the spoon head, the handle, etc.) We might also add images of forks and knives to understand concretely what makes a piece of cutlery NOT a spoon.\n\nBut, notice something. While inputting these images of different spoons, we would expect them to have similar pixel input vectors because they are all spoons. When a new image of a spoon comes along the ANN should be able to (abstractly and mathematically) say \u2014 \u201cHey, this looks similar to all the other spoon pixel input vectors I\u2019ve seen\u2026so it\u2019s probably a spoon!\u201d. It should be able to find distinct patterns in these pixel input vectors that it can use to ascertain what makes a certain pixel input vector likely to be that of a spoon. That\u2019s how any supervised machine learning algorithm or ANN should work.\n\nHowever, this just isn\u2019t the case. The reality is, the pixel vectors of the following two images \u2014 even at grayscale \u2014 are completely different:\n\nYes, these are both images of a spoon, but they are different type of spoons, the indented parts have different reflections, the orientations are different, the angles are different, the backdrops are different \u2014 I could go on and on. As a result, the actual pixel input vector will not be even remotely similar. Thus, an ANN will have a hard time associating these two images.\n\nAnd this is a really really simple example. In reality our images will be larger, noisier, and the objects may be at different places in the image and of different sizes. Put simply, ANNs just won\u2019t do very well at these tasks.\n\nThe reality is that it\u2019s not the individual pixels that tell us whether an image of a spoon is an image of a spoon \u2014 it\u2019s something much more abstract than that. It\u2019s the handle and the circular indent that we identify, for example. These are the \u201ccharacteristics\u201d of a spoon. And, ultimately, it\u2019s edges, lines, and corners that superimpose on each other to build out these characteristics. Operations on individual pixels aren\u2019t enough to let us figure out if these pixels represent a spoon. Instead, we need to figure out the pixel groupings that make these edges/lines/corners and see how groupings of those edges/lines/corners then go on to form the characteristics. Furthermore, flattening images into single vectors \u2014 though retaining information of the pixels \u2014 loses information such as the structure of the image. Our neural network would not be able to exploit this structure, which is certainly important information when it comes to recognizing objects.\n\nHowever, this is not the only issue. It\u2019s also very valid to suggest that ANNs simply don\u2019t scale well to large images. If each value in the pixel input vector is fed into a separate node, we essentially are going to have a new input feature per pixel. Imagine a 200 x 200 grayscale image \u2014 that\u2019s 40,000 input features! That would mean something at least on the order of hundreds of thousands of weights per layer (if not more), which is simply infeasible. Such a large number of parameters would mean slow training and very likely overfitting as well.\n\nWhy input pixel values to the ANN? Why not choose and extract/compute our own features for classifying spoons?\n\nWell, it could work (and that\u2019s exactly what we used to do), but human-constructed features are rarely as good as \u201clearned\u201d features. Sometimes we also just don\u2019t know how to come up with good features for certain objects. On top of that, if we want to truly solve computer vision, human-constructed features simply aren\u2019t a good approach \u2014 we want to build algorithms that can see and classify anything based on prior experience/training with that thing and without human intervention.\n\nReally, this is the sort of thing that we want artificial intelligence to solve. And convolutional neural networks do it beautifully.\n\nBefore we go any further, you might be wondering: why is it so frikin\u2019 hard to get computers to see? We humans don\u2019t need to put any effort into it! When I see a computer in front of me, there is no hardcore processing or rationalization that my brain needs to undergo. I\u2019m just like: \u201cYo, whose Macbook is that?\u201d\n\nYet, why is it so hard for us to multiply 359 and 214.24 in our heads? (Try it, I dare you.) The dumbest, slowest computers can do it so easily, yet I\u2019m 100% sure the common man would give up before even starting.\n\nThis highlights moravec\u2019s paradox: the discovery by researchers in artificial intelligence/robotics that high-level reasoning requires little computation for computers but great computation for humans, whilst low-level sensorimotor (fancy word \u2014 basically sense perception like seeing) skills are the opposite. Hans Moravec puts it straightforwardly:\n\nMarvin Minsky (R.I.P.) said it best, though:\n\nThis is an interesting observation because it\u2019s obviously so relevant to computer vision. An explanation offered by Moravec, though, is that we can attribute this all to evolution. Over the course of millions of years of human evolution by natural selection, our brains have increased in complexity and improved in design. Seamless object recognition and abstract thinking are potential examples of these.\n\nReally, the oldest human skills like abstract thought, object identification through vision, and complex linguistic expression are largely unconscious processes (we don\u2019t really need to think to do them) and thus to us they seem very effortless to us. Over many many years, they have continued to evolve to make them so seamless. We should expect that these skills \u2014 being so old and so evolved/iterated upon \u2014 will take a lot of time to replicate in machines/computers. That is why this seeming \u201cparadox\u201d might not be a paradox at all.\n\nUnsurprisingly, these convolutional neural networks (and yes, we still haven\u2019t explained what those are \u2014 we\u2019re getting there, I promise) are heavily inspired by our own brains. So, it might behoove us to figure out how we humans look at stuff, and then derive a neural network architecture to mimic that. If you don\u2019t really care about any of this and just want to get to the good stuff, you should be OK to just skip this section and move on.\n\nStill here? Good. Our visual system, like the rest of our nervous system, is composed largely of neurons. Biological neurons do pretty much the same thing as artificial neurons in our ANNs; they take inputs from other neurons, and can choose to emit an output based on these inputs. Unlike ANNs, the inputs and outputs are a binary on/off signal instead of a real number, but neurons can vary the firing rate (the rate at which they send an output signal). When we say a neuron is \u201cexcited,\u201d that just means its firing signals like there\u2019s no tomorrow.\n\nOk, now let\u2019s talk about how we go from light to neurons. Humans see things through their eyes: photons shoot towards your cornea, through your pupil, and hit the retina at the back of your eyeball. Photoreceptor cells (rods, cones) in your retina get excited when they see light and color, and that eventually works its way to your retinal ganglion cells.\n\nIt\u2019s in these retinal ganglion cells where things first start to get interesting. Each retinal ganglion cell takes inputs from a cluster of photoreceptors, representing a small region of your vision. Your retinal ganglion cells divide this region into a \u201ccentral zone\u201d and a \u201cperipheral zone\u201d around the center; if the cell gets the right amount of light in each of these regions, it will start firing to let the downstream parts of your vision pathway know it sees something. Concretely, there are two types of cells: on-center and off-center. When on-center cells see light in the middle of their respective region, they get all excited and start firing. When off-center cells see light around the center, they too get excited. The takeaway here is that each cell responds to a certain pattern in its input, which we can build off later on to detect more complicated patterns. We\u2019ll see how that works in a minute.\n\nAn important thing to note here is that every cell is only affected by a small region of your vision. The firing rate might be heavily influenced by the amount of light hitting that small part of your retina, but outside of that region the firing rate of that cell is unaffected. The region of your vision that influences a particular cell is called a receptive field (RF). There was a famous experiment by Hubel & Wiesel that demonstrated this concept: by recording the signals coming out of one of these cells (converted to audible beeps in the following video), they showed that one of these cells is only responsive to a particular region of your vision.\n\n(Now comes the part where I think I can design my own diagrams.) Imagine all of your retinal ganglion cells organized into a grid based on the region of your vision that they represent, like so:\n\nLet\u2019s pretend we see something: a bright light directed at the center of our vision. The cells in the center of our field of view get all excited, but the rest still have no idea anything\u2019s changed \u2014 receptive fields at work!\n\nIf we take a step back and look at this a bit more abstractly, what we\u2019re really doing is transforming the data we\u2019re getting. Initially, our \u201cdata\u201d was the light coming into our eye; while this representation was rich with detail, it was hard to get to the \u201ccore\u201d of what we were really seeing. Using the photoreceptors and retinal ganglion cells, we can reduce this representation to something more meaningful: \u201cwhere is there light?\u201d Other cells early on in the vision pathway can pick up unique colors and basic patterns that help round out this new representation with more information that we can use to eventually pick out individual objects and shapes.\n\nThe signals from your retinal ganglion cells eventually make their way through something called the LGN and into your primary visual cortex (affectionately called \u201cV1\u201d). Like the cells in your retina, V1 cells look at just small region of their input. Building off of the work done in your retina to pick out light and color in what we\u2019re seeing, these V1 cells can start looking for things like lines and patterns; essentially building a \u201crepresentation\u201d of the data that\u2019s a little more abstract and higher-level. Once again, Hubel & Wiesel come to the rescue with a demonstration:\n\nHo-ho-ho, I think we have ourselves another transformation! Our new grid of cell activations will light up when it sees a line or border, and stay quiet otherwise. I think you can see where we are going from here \u2014 once we have \u201cmaps\u201d of different patterns and lines in the image, it becomes easier to look for shapes and borders. From there, we can begin to identify primitive objects and more complex shapes. Finally, we can look for and identify complete objects in our vision. Each transformation builds on the previous representation; it\u2019s hard to identify that we are looking at a cat from raw photoreceptor inputs, but if we are looking at borders, shapes, and textures it suddenly becomes quite clear what we\u2019re looking at.\n\nIt\u2019s this hierarchical structure of our visual system that enables us to see and identify so many different objects. As you\u2019re about to see, the same concept transfers beautifully to neural networks.\n\nSo the problem with using a normal neural network is that it\u2019s basically impossible to extract any meaningful features from a bunch of pixels directly. To solve this, a CNN uses a hierarchical approach to learn feature detectors that are increasingly abstract. For example, a CNN might start by looking for lines and borders in our image. From there, it can look for basic shapes and curves. Oh hey, those curves kind of look like an ear and a nose. A face tends to have ears and a nose. Hey, maybe we\u2019re looking at a person!\n\nLet\u2019s put this all into more concrete terms. If you read the last section, you\u2019ll recall that the cells in our retina have a receptive field within your field of view \u2014 each cell only responds to activity in a small portion of your vision. Furthermore, recall that each cell only activates for a specific pattern within that region. In a convolutional neural network, we have a very similar principle \u2014 a convolutional kernel (or filter) describes an individual pattern, which is then applied to every part of our image.\n\nWith CNNs, we talk about volumes instead of normal vectors. Instead of a 1-D vector of numbers that we pass into our network, it\u2019s conceptually easier to envision our image as a 100 x 100 x 3 volume of numbers (100 pixels wide, 100 pixels tall, and 3 channels [R, G, B] deep for the colors).\n\nWe make these transformations using convolutional layers. In a normal fully-connected layer, we would have one weight per input for each neuron; in a convolutional layer, we instead learn the weights of a filter that we apply to every part of our input volume. If our initial input volume is 100 x 100 x 3, we might learn a 5 x 5 x 3 filter that we apply to each individual part/sub-section of our image.\n\nWhat do we mean by \u201capply\u201d? Quite simply, we take the dot-product of our filter with the region of our image. We start with the top-left corner, and apply our 5 x 5 x 3 filter to get a single number (intuitively, this number represents how strongly that region \u201cmatches\u201d our filter). We then shift over our filter by 1, and take another dot-product. This gives us another value, which we pass through our non-linear activation function. We combine all of these dot-products into a brand new volume (which we call a \u201cfeature map\u201d).\n\nMake sense so far? If not, here\u2019s a GIF of a 3 x 3 filter in action (wait for it to begin animating):\n\nA few extra things to note: I just said that we shift over our filter by 1 every time we take a dot-product, but we can actually shift it over by as little or as much as we want; the number of spots we move over is called the stride. We should also discuss the size of our new volume we get; intuitively, the size of the new volume will be smaller than the input (see the animation above if you need convincing). If we don\u2019t want the volume to get smaller with every subsequent convolutional layer, we can add a layer of padding around the input before we apply our convolutions. For example, if we use a padding of two, we will add a two-pixel-wide border of zeros around our input, and then do our convolution generate our feature map. We can use this handy equation for computing the size of our new output:\n\nLet\u2019s reiterate: we have our input image, which we\u2019ll say is 100 pixels long by 100 pixels wide. Each pixel has three numbers associated with it: a red channel, a green channel, and a blue channel. We represent our image as a three-dimensional volume of numbers (100 x 100 x 3). We learn a filter (we\u2019ll explain how to learn the filter in a bit), which we apply to every part of our image, giving us a new volume (assuming we used the appropriate padding, we\u2019ll assume this new feature map has the shape 100 x 100 x 1). This process of applying the same filter all over the image is known as a \u201cconvolution\u201d (hence the term, \u201cconvolutional neural network\u201d).\n\nWhy does this work? Let\u2019s say the filter we learned was great at detecting edges in our image. This means that our new \u201cfeature map\u201d would light up wherever we have a border, essentially giving us an outline of what we\u2019re looking at. This kind of representation is far more useful to our network than the color of each pixel; it lets us extract more meaningful features from our image further down the line.\n\nThat\u2019s basically the gist of it. The last remaining detail is that each convolutional layer can actually have multiple filters in it; instead of convolving just one filter and creating just one filter map, we learn many filters (often as many as hundreds per layer) and apply each one to the input volume. This gives us as many feature maps as we have many filters, which we can stick together to create a larger volume. So if our initial input is 100 x 100 x 3, and our first convolutional layer has 192 filters, our output volume will have size 100 x 100 x 192 (again, assuming that the stride and padding are configured appropriately to prevent the output size from shrinking). Each filter map will give us a different unique perspective on the image (unique patterns, lines, borders, etc.), which when combined together give us a new representation of the image which is far more meaningful than the initial image or any individual feature map.\n\nThe next convolutional layer will take this 100 x 100 x 192 input and apply its own set of filters to the image of size n x n x 192 (notice that, while the size of the filter can be whatever we want, it extends through every feature map from our input volume). Each successive convolutional layer can build off of the work of the previous one; using the borders we found in the first layer, we can find unique shapes and curves in the second layer. The third layer can find even more abstract and meaningful features. But eventually, we need to start reducing the size of our representation as the features we learn span a larger and larger region of our image. This is where pooling layers come in.\n\nPooling layers are a way to reduce the size of our volume as it flows through our network. We\u2019ll start by looking at one particular kind of pooling layer, the max-pooling layer, but the same principle applies to related layers, like average-pooling.\n\nQuite simply, our max-pooling layer takes a small subsection (say, 2 x 2) of each filter map in its input volume, and only takes the largest value from that subsection. Perhaps this is best explained with a picture:\n\nLet\u2019s go back to our 100 x 100 x 192 output of our first convolutional layer and apply a 2 x 2 max pooling. Our new output will have size 50 x 50 x 192 (notice that it keeps the number of feature maps the same, but reduces the two other dimensions by a factor of 2). Our pooling layer has two hyperparameters: the size (2 in the previous example) and the stride (also 2 in the previous example). We can compute the size of our output using the following formula:\n\nAnd that\u2019s it. Notice that pooling layers have no parameters (only hyperparameters) \u2014 they simply apply a function to regions of its input. Intuitively, you can think about pooling layers as reducing the \u201cresolution\u201d of our input volume; if we go back to our feature map of lines, and apply a pooling operation, we will end up with basically the same picture just with fewer \u201cpixels\u201d. This is called down sampling.\n\nGenerally speaking, the stride and size of a pooling layer are the same, but there\u2019s nothing stopping you from creating an \u201coverlapping\u201d pooling layer (as we\u2019ll see in the case study that follows) where the size is greater than the stride.\n\nConvolutional and pooling layers make up the bulk of most CNNs, but we still need to somehow come up with a probability distribution over classes. We can do this with our normal fully-connected layers we are so used to seeing in regular neural networks. Our last convolutional or pooling layer gives us one last output volume, with very high-level and abstract features (ears, eyes, leaves, etc.). If we connect a fully-connected layer to each value in our volume, we can train one neuron per class (with a softmax stuck on the end to convert it into a probability distribution). If we want an extra little bit of representational power, we can stick on some extra fully-connected layers before our final output layer.\n\nSo now, how do we actually learn or define these filters and their specifically weights/biases? Our objective is to find filters that minimize error/cost and in doing so maximize the percentage of correctly classified images. We of course use the backpropagation algorithm to compute our derivatives and then apply a first-order or second-order optimization algorithm like stochastic gradient descent or L-BFGS respectively. The cost function we use can be any typical one, for example cross-entropy (or \u201clogistic\u201d) loss.\n\nI\u2019ll wrap this up with a little note on transfer learning (which applies to neural networks in general, but is especially common when training CNN models). You need a lot of data to train one of these networks successfully, and depending on the problem you\u2019re solving, there isn\u2019t always all that much data to go around. Data augmentation can help, but nothing beats a ginormous dataset with hours of GPU training time. But because of the nature of pictures, a lot of the features that CNNs are learn are fairly generic; the lines, patterns, textures, and so on that a network learns to look for can often be found in most images, even in categories it may not have been explicitly trained on. For this reason, it\u2019s common to start training with a popular architecture and an already-trained model; if you freeze the weights of the convolutional layers and just train some fully-connected layers on top of them, you can learn to recognize an entirely different distribution of objects with far fewer images.\n\nAnd that\u2019s it! Not so bad, right?\n\nLet\u2019s take a quick look at why ConvNets work and what they really do. We briefly mentioned before that they can learn weights for feature detectors that are increasingly abstract; perhaps they first start by detecting the edges, lines, and corners in parts of an image via the initial convolution layers and then proceed to, in the later convolution layers, recognize things like noses, eyes, bike handles (just a random thought) that can then be used to figure out exactly what the object is.\n\nThe fully connected layer(s) at the very end let us combine these individual features and figure out what exactly we\u2019re looking at. Another approach, as mentioned before, would have been to use a normal neural network with hand-crafted featured as inputs, instead of the convolutional layers. Obviously, finding and computing said features is extremely difficult, very time consuming, or simply not possible. CNNs fix this by finding effective feature detectors themselves. The point of using CNNs is that by training these convolution layers they can create their own feature detectors (ie. learning that seeing a bike handle might be a good way of figuring out that we\u2019re looking at a bike) that can then be applied to a fully connected ANN-like layer at the end.\n\nA really cool way to demonstrate this is to extract the feature maps from each of the hidden layers and treat them as pixels that we can visually interpret. You can\u2019t see the actual contents of the following GIF up close but this is a good visualization of said feature maps from the original image and how they lead us to the end classification:\n\nIf we look at how the feature maps change as we go deeper and deeper into the network, we see that they quickly shift from easily interpretable (picking out basic textures and borders) to much less so (where each feature map might look for the presence of eyes or ears, for example).\n\nHowever, if we look at the actual features that each feature map is looking for, we find that as we go further into our network they quickly transition from low-level borders and shapes to higher-level features and unique facial characteristics.\n\nUltimately, this is the point of the filter. Instead of looking at the image as a whole, we can look for low-level features \u2014 edges, corners, lines, borders \u2014 in a small region of the input. From these low-level features, we can \u201cbuild up\u201d and look for slightly more complex features in a slightly larger region of the image. That\u2019s why ConvNets work\u2014the spatial convolution let us pick out small features and then work our way up to bigger ones. Once we have these greater abstractions it becomes pretty easy to classify.\n\nCNNs are undoubtedly sort of automagical and black box (like a lot of deep learning) but they work really well. I highly encourage (in fact\u2014I mandate!) that you watch the following YouTube video. It should make this explanation very intuitive.\n\nAlright\u2014now, let\u2019s take a look at some of the datasets and CNNs that are used in practice!\n\nOK, so now we\u2019re gonna take a look at an example. That is, how have some of the most successful CNNs been architected? How does one even get the data for training CNNs, and what are the kind of results that modern day networks achieve?\n\nTo answer these questions, we\u2019ll take a look at ImageNet. What is ImageNet? According to the website:\n\nBasically, ImageNet is an open source (free) database of images with words/captions associated to them. Obviously, if you want to perform image classification (where you take an image and spit out the caption[s]/words associated to the main object[s] in the image), this seems like the perfect resource. In fact, as of 2016 (ImageNet began in 2010), there are over 15 million high-resolution, labeled images \u2014 meaning paired with crowd-sourced hand-annotated (yes, hand-annotated!) captions \u2014 from 22,000 different categories. Now that\u2019s a lot of training data for any sort of CNN out there!\n\nImageNet was started by students/faculty at Stanford University and the Stanford Vision Lab to collect data for computer vision algorithms \u2014 CNNs obviously included. The project continues to grow in an effort to help grow the field of computer vision and increase\n\nSince inception, ImageNet has hosted an annual competition called the ImageNet Large Scale Visual Recognition Challenge (ILSVCR for short but I\u2019m not sure who could memorize that acronym). In said competition, research teams (from institutions and companies) submit programs that perform object classification and detection on scenes/images from the ImageNet database. In 2010, a good classification error hovered around 25%. In 2012, the first year that a convolutional neural network won the competition, that almost halved to 16% (CNNs have dominated the leaderboard ever since). Now, the error rates are in the single digits, and researchers have reported that these error rates are in fact lower than human error rates! Of course, in context this means little since humans are still the best at contextual reasoning, not to mention the fact that we can recognize a lot more classes than ImageNet has. But it\u2019s still impressive.\n\nIn ILSVCR, models are trained on 1.2 million images in 1000 categories (hence the \u201cLarge Scale\u201d part of the name). The task is to make 5 guesses, sorted by confidence/probability, about the potential label of input images:\n\nYou may see two separate measures of error: \u201ctop-1 error\u201d and \u201ctop-5 error\u201d. Top-1 error occurs if the actual object in an image is not the most probable outcome class that the CNN produces. Top-5 error occurs if the actual object in an image is not in the five most probable outcome classes that the CNN produces.\n\nFun fact: in 2015 Baidu was caught cheating in the competition.\n\nAlexNet was a deep CNN model created by Alex Krizhevsky (a student at UToronto) that, as mentioned earlier, took the error percent rate from the mid 20s to the mid 10s (which is a huge deal) in 2012\u2019s ILSVCR. AlexNet, however, isn\u2019t just any vanilla CNN you can throw together in 2 hours of hacking \u2014 there are many features that distinguish it from a typical convolutional neural network and I\u2019ll talk about them in this section.\n\n*Ok but if you really forgot what they are (and haven\u2019t checked out our \u201cVanishing gradient problem\u201d article) then it\u2019s a non-saturating (that is, the gradients don\u2019t die off) and non-linear activation function. The equation is ReLU(x) = max(0, x). If nothing you just read makes sense, I guess that\u2019s a prompt to go read the other article! ReLU is helpful; a 4 layer CNN with ReLUs can converge up to six times faster than an equivalent network with logistic/tanh neurons on the CIFAR-10 dataset. ReLU layers exist independently of convolutional layers.\n\nA visual representation of the entire architecture follows:\n\nThere are 5 convolutional layers after the input, followed by 3 fully-connected layers (that is, just a regular ANN), and finally a 1000-way softmax for the output (to give us a probability for each of the 1000 classes in the 2012 challenge). If we fed in the leopard image above, we\u2019d hope that the CNN would have the \u201cLeopard\u201d class with the highest output probability.\n\nNow, let\u2019s look at each individual layer in the CNN and their features/role:\n\nThis is the first convolutional and hidden layer. In this layer, the convolutional filter convolves around the input image volume of size 227 x 227 x 3 with 96 11 x 11 x 3 filters with a stride of 4 and no padding. Using the equation we brought up earlier, we can figure out the size of our output volume: (227\u201311)/4 + 1 = 55. Because we have 96 filters, the output of the layer is ultimately 55 x 55 x 96. So, the first layer of AlexNet convolves around a 277 x 227 x 3 input image and outputs a 55 x 55 x 96 tensor as input for the next convolutional layer. Here\u2019s a nice definition of a \u201ctensor\u201d from Google, by the way:\n\nTraining for AlexNet also occurred on multiple (well, two) GPUs, but not much explanation needed here. The arrows that point to inter-GPU connections show connections that are being trained across different GPUs and intra-GPU connections show the ones being trained on the same GPU.\n\nBefore we looked at pooling layers in CNNs. AlexNet does use pooling \u2014 in particular it uses max pooling with overlapping. According to Krizhevsky, overlapping pooling slightly reduces error rates and overfitting as well (by about 0.4%.)\n\nNow let\u2019s take a look at an overview of the full architecture of AlexNet:\n\nWe start with the previously described convolutional layer. Next comes a normalization layer (known as a Local Response Normalization \u2014 this isn\u2019t used all that much anymore, so we won\u2019t go into much detail here). Next comes a 2 x 2 max pooling layer. This block is repeated. Following that, we have three convolutional layers of smaller filter size (now 3 x 3 instead of 11 x 11) and then another max pooling layer. Then, we have two fully connected ReLU layers. Lastly, we have a fully connected softmax layer that outputs 1000 values (which is the probability distribution over the 1000 classes) from the down sampled, convolved, and basically broken down representation of the image.\n\nWhew, that was a pretty long case study. AlexNet is some good stuff. There are some small extra techniques employed, mostly related to preventing overfitting, but I won\u2019t go into it \u2014 it isn\u2019t necessary at this point. Dropout, data augmentation, and modifications to the softmax algorithm are the notable mentions here. Finally, stochastic gradient descent with momentum (don\u2019t worry if you don\u2019t know what this is, but I think it\u2019s explained in our ANN article) was used as the optimization algorithm.\n\nOK, now for some fun stats:\n\nOK, now for some more fun: example classification results! This is actually really cool.\n\nThe top shows the actual image, in the middle shows the object present in the image, and the bottom shows the top-5 most probable objects that the algorithm has classified. The pink/red bar shows shows the class in the prediction/classification that corresponds with the actual label.\n\nIf you\u2019re thinking: \u201cRohan, dude, dope, CNNs sound like THE BOMB, but how do I actually make them?\u201d\n\nFirst of all, I agree with you. CNNs are the bomb. As for making them, lemme show you. We\u2019ll be, in particular, using TensorFlow, a popular \u201cOpen Source Software Library for Machine Intelligence\u201d created and maintained by Google.\n\nThe following code, in Python, demonstrates the implementation of CNNs with TensorFlow (obviously make sure to install TensorFlow first, instructions are on their site). The CNN we\u2019ll be making is specifically being applied to MNIST: a dataset of handwritten numbers and their corresponding label. Thus, instead of object classification we are classifying a number (as a result there are much fewer classes; just 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9) based on an image of said letter/number. This is useful for OCR.\n\nThe code is taken from TensorFlow\u2019s official tutorial on Deep MNIST implementation. We\u2019re going to build a CNN with the following architecture:\n\nWe also use dropout for regularization.\n\nThe following code downloads the MNIST data:\n\nThe next 2 lines of code sets up a TensorFlow session:\n\nThe first line of code is easy to understand: we just import TensorFlow as \u201ctf\u201d. Then we create a new \u201cInteractiveSession\u201d. Sounds fancy, but what is it? Well, this concept is perhaps at the core of TensorFlow. It\u2019s the idea of representing computations as graphs. Nodes in said graphs are called ops (short for operations), and each op takes zero or more Tensors, performs some computation, and produces zero or more Tensors. These \u201cTensors\u201d are typed multi-dimensional arrays\u2014or simply put, mathematical tensors\u2014the dimensionality of which we will look at soon.\n\nA lot of that description was pulled from TensorFlow\u2019s site, however I\u2019m gonna make it simpler for you here. After all, that\u2019s one of the goals of this blog.\n\nIn TensorFlow, mathematical operations that you perform on tensors aren\u2019t actually performed. Instead, they\u2019re added to a computational graph that defines what kinds of computations we are doing. So we first build our network by defining all of our operations (which are the convolutional neural network layers eg. convolutional/pooling/ReLU etc.), and that creates our graph. Then, when we\u2019re ready to use our network, we tell TensorFlow to run the graph that we\u2019ve created, giving it actual values to plug in to the tensors and letting it compute the rest. It\u2019s pretty dope, in my opinion! But, if it doesn\u2019t quite make sense yet don\u2019t worry, you\u2019ll see what I mean as we keep going.\n\nOK, so now we\u2019re going to define two functions to help us out:\n\nThe first function initializes weights based on a given shape/dimension. Obviously, to set up a CNN we\u2019re gonna need a lot of weights/biases. Generally, we don\u2019t set these values to zero but instead add a small amount of noise to prevent symmetry and premature convergence/high error on neural networks: that\u2019s exactly what the truncated_normal function does, and we specify a standard deviation of 0.1 to the normal distribution.\n\nWe put our weights into a TensorFlow variable. Remember that TensorFlow doesn\u2019t deal with values directly; it deals with \u201coperations\u201d on a \u201ccomputational graph\u201d. A Variable is just a special operation that doesn\u2019t take any inputs and returns the value that sits inside the variable. The variable won\u2019t actually have a value inside of it until we run the graph.\n\nThe second function creates a bias variable, again based on a given shape. We initialize it to a constant value of .1 (ReLU cuts off any activations <0, so giving it a slight positive bias prevents dead neurons).\n\nWe\u2019ll now write functions to create reusable layers\u2014that is, the convolutional and max pooling layers. The code follows for the convolutional layer:\n\nThis function, aptly titled \u201cconv2d\u201d, creates a 2D convolutional layer. A 2D convolutional layer uses a 2D filter, which is a filter that convolves in the horizontal and vertical direction/dimension of an image. Note that we do not convolve in the depth/z-axis of an image (the channels).\n\nThe tf.nn.conv2d function takes as input a 4-D input tensor (which corresponds to variable x, which will be the input of the layer). Wait\u2026 but why 4-D? \u201cI thought you said images were 3D volumes?\u201d, you\u2019re probably thinking. I hear you. Worry not. Basically, when we train a convolutional neural network, we want to train it on a batch of images. You don\u2019t just wanna train it on one image/corresponding label at a time, just like a (properly vectorized) linear regression algorithm wouldn\u2019t train on one data point at once. Thus, we collect a batch of 3D tensors and put it into a 4D tensor. Obviously, this means that all images must be of the same dimension. The dimension of the entire input tensor is now: NUMBER_OF_IMAGES x STANDARD_IMAGE_WIDTH x STANDARD_IMAGE_HEIGHT x STANDARD_IMAGE_CHANNEL. The 2d convolution layer still only operates/convolves on the height and width.\n\nThe W parameter corresponds to our convolving filter \u2014 these are of course, weights. They are TensorFlow Variables that can be generated using the function we wrote earlier on. Now, the strides argument (which we set explicitly to an array of four 1s) is sort of confusing: isn\u2019t there just\u2026 one stride? Maybe two? But why four? Ok, so here\u2019s what\u2019s up: since we have a four dimensional tensor input, we have a four dimensional stride input that corresponds to the stride value used for each axis. (Yes, you can technically have a different stride for each dimension, although I\u2019m not quite sure why one would want that. But hey, I\u2019m not here to judge.) Because this is a 2D convolution, TensorFlow actually requires that the first and last strides, corresponding to the batch dimension and the channel dimension, be set to 1.\n\nTherefore, strides[1] and strides[2] can just be set to the stride values for the x and y axis. Why TensorFlow has you input an array of four values, only to literally force two of those values to be 1, confuses me as well. Don\u2019t worry.\n\nWe set the stride for the x and y axis to just be one. padding=\u2019SAME\u2019 is sort of confusing as well. Basically, this is the argument we pass to specify the zero padding for the convolutional layer. With TensorFlow, you can pass one of two arguments:\n\nAgain, this confuses the hell out of me. SAME is the most common one, apparently. A more thorough exploration into the differences between these not so aptly titled arguments are available on one of their documentation pages.\n\nThe code follows for the max pooling layer:\n\nNo need to spend too much time on this. We pass the input x, and the ksize parameter (once again we set the first and fourth value to 1) is the size of each max pooling sub-region (2 x 2 here), whilst we use stride of value 2. We use the same padding as before.\n\nAt this point we\u2019ve defined all the functions we need to, and we can construct our neural network in our computation graph. But before that, we need to write the code to create placeholders for our training set data values:\n\nRemember, these are not specific values (therefore they are placeholders). We need a way to feed in our values when TensorFlow starts executing our computation graph; we do this with a placeholder. When we actually run the graph (and we\u2019ll see how in a bit), we can specify which values we want these placeholders to have.\n\nThe input x is a 2D tensor of floating point numbers, with the dimension [None, 784]. 784 is the number of pixels in a single flattened MNIST image. TensorFlow gives us these images as a 1D vector, but we will reshape them back into a square before we give them to our CNN. Another little thing to note is that MNIST images are in grayscale, so it\u2019s 2D rather than 3D because the channels dimension is just 1. None means that the first dimension can be of any size. In our case, that dimension corresponds to the batch size we use.\n\ny_, the labels or known output of the training data, is a \u201cone-hot vector\u201d of 10 values. What does a one-hot vector mean? It\u2019s actually really simple; one-hot vectors \u2014 aptly titled by the way, which y\u2019all know I really appreciate \u2014 are just vectors that contain the value of 1 in any given single position and the value of 0 in every other position. There are 10 values because there are 10 possible output classes of the CNN with MNIST: 0, 1, 2, 3, 4, 5, 6, 7, 8, and 9. When it comes to the output of the network, there will a probability/confidence distribution over each of these classes. However, the output of the input training set data is\u2026 certain, so one of the values will be 1 and the rest will be zero.\n\nLet\u2019s get on to creating our first convolutional layer! We\u2019ll first create the weights and biases (the convolutional filter) for this layer.\n\nWe use the functions we\u2019ve already defined to create these weights/the bias. The weight tensor has a shape of [5, 5, 1, 32] because our filter is of size 5 x 5 (pretty standard). We only have one channel for grayscale and it\u2019s a 2D convolution. The final value, 32, is the number of output channels that we have \u2014 the number of features or \u201cfeature maps\u201d that will be produced from this convolution. Basically, a 5 x 5 x 32 tensor will be outputted from this convolutional layer that will be inputted to the next one. We also create our bias variable with a component for each output channel.\n\nNext, we take our tensor of n x 784 flattened image vectors and reshape them back to their original 28 x 28 dimensions:\n\nThis is again important because CNNs can exploit and learn from the structure of images. The -1 here just means it does not change, because we retain the batch size.\n\nWe now convolve x_image in 2D with our weights and add our biases using the function we created earlier:\n\nFollowing this, we apply the ReLU activation function and then do a 2 x 2 max pooling operation:\n\nThat\u2019s the first layer (or first three, if you think of ReLU/pooling as separate) done! Let\u2019s take a quick look at the size of our current image. Before, it was 28 x 28. However, with our convolution and pooling, it should be down sampled. We apply the following equation that was explored before:\n\nWhere W is the dimension of the original image, F is the filter size, P is the padding size, and S is the stride value. We sub in:\n\nTo get a value of 28. That\u2019s right \u2014 the image size doesn\u2019t change. However, we\u2019re forgetting that we also need to apply the pooling equation:\n\nWhere W is the original image size, F is the pooling filter size, and S is the stride value. We sub in, as per the code in our functions:\n\nNow we get to an image dimension of 14. So, after the first convolution/pooling layer, our image has been down sampled to 14 x 14.\n\nNext we have a second convolutional layer that, instead of 32, now creates 64 features for each 5 x 5 filter and then applies ReLU/max pooling.\n\nOur image is now down sampled to 7 x 7 (if you repeat the same calculations as before). Since we have 64 output channels, we now have 7*7*64 = 3136 values. Next, we have a fully connected layer with 1024 neurons. That is, 3136 output values are flattened and then directly connected to 1024 neurons. The number 1024 is arbitrary and has been chosen based on positive empirical results.\n\nWe first create the weights/biases:\n\nThen we take the output of our previous layer (the pooling layer) and reshape (flatten here) it into a batch of vectors of 3136 values.\n\nFollowing this, we perform the fully-connected layer operation of multiplying our values by the weight matrix (\u201cmatmul\u201d stands for matrix multiplication) and then adding the biases. This is like Machine Learning 101 \ud83d\ude1b\n\nTo reduce overfitting, we apply this technique called \u201cdropout\u201d. No worries if you don\u2019t know what it is, very little code is required to implement it. On a high level, dropout randomly turns off some of the neurons while training to increase the independence of each of them individually and reduce dependence on each other and temper the influence of a given neuron. In general, this enables a neural network to generalize better because several independent representations of patterns can be learned. The variable keep_prob in the following code is an aptly-titled value that states the probability of any given neuron being kept/left on and not turned off. We\u2019ll explicitly set that value when we run the interactive session.\n\nWe\u2019re almost done! Finally, we\u2019ll add a fully-connected softmax layer that can turn the output from 1024 neurons into a probability distribution over the 10 classes of number characters.\n\nWe create the weights as per usual:\n\nNow we create y_conv, our final output that performs a fully-connected operation on the dropout and following that a softmax.\n\nCool! You\u2019ve just constructed your very first convolutional neural network. You\u2019re not just a builder \u2014 you\u2019re an architect!\n\nBut we\u2019re not done yet. Now let\u2019s go ahead and train/evaluate our model, and learn how well it performs.\n\nThe line of code above is essentially our cost function, where we aim to make y_ (our known output) and y_conv (out estimated/predicted output) as similar as possible.\n\nThis line of code defines an iteration of optimization (using the first-order optimization method called Adam) with a step size of 0.0001 by minimizing the output of the cost function we just defined.\n\nThis is a vector of booleans where 1 indicates a correct prediction (that is, for that image in the batch y_conv and y_ both chose the same class as the output number character class.) y_conv is continuous rather than discrete like y_, but the function argmax returns the index with the highest value (exactly 1 for y_ and closest to 1 for y_conv) so that\u2019s not an issue here. We don\u2019t check if the values are exactly equal.\n\nThis line of code just gets our overall accuracy by calculating the mean of our predictions.\n\nOK. You ready for this? Let\u2019s finally run our session!\n\n20,000 iterations in the for loop is basically the batch size (which we have set to 50) multiplied by the number of steps of the training/optimization method (which we have set to 400). You can change these numbers as you wish.\n\nIn each iteration, we get our next batch (remember we are training in batches) and then proceed with our training/optimization step using train_step. We pass to train_step the argument feed_dict, which contains the 50 x input images, 50 y_ known output classes, and the keep_prob for dropout which is 0.50.\n\nAlso, every 100 overall iterations (that\u2019s the i % 100 == 0 if-statement), we log the current training accuracy for the batch (which would be every 2 training/optimization steps for each individual batch since 100 / 2 = 50) and then print that out. This helps us keep track of how the accuracy is changing over time, but we don\u2019t want to spam by calculating and outputting the accuracy every iteration because it\u2019s unnecessary and would slow things down a lot.\n\nFinally, once we have finished optimizing on our training set, we go ahead and log our test set accuracy:\n\nYou should get up to 99.2% accuracy if you leave it running for a while (and if you\u2019re okay with your fans goin\u2019 crazy!). Hope you had fun with that. The theory and math behind CNNs and machine learning in general is great, but implementing and getting your OWN machine to learn is even cooler \ud83d\ude0e\n\nAlso, for this small CNN, performance is nearly identical regardless of whether dropout is used. Dropout is really good at reducing overfitting, but it\u2019s more relevant and effective with very large industrial-setting networks.\n\nTIP: if you get an error about some UTF-8 locale stuff, enter this into your command line/Terminal (not Python):\n\nHere\u2019s all the code pieced together.\n\nYea, so that was probably a long read! Hope you learned something about convolutional neural networks and that they\u2019re not magic but instead just a really cool and ingenious advancement in computer vision.\n\nTo conclude, here\u2019s an image of a cute cat that has been painted over in a really dream-like/psychedelic fashion by a CNN. It\u2019s done by Google\u2019s DeepDream program. Maybe this is something we can talk about in the future?\n\nTil\u2019 next time, when we dive head first into the recurrent neural network! \ud83c\udf7a"
    },
    {
        "url": "https://ayearofai.com/rohan-7-can-artificial-intelligence-achieve-human-intelligence-b0c95e23ca4b?source=---------2",
        "title": "Rohan #7: Can artificial intelligence possess human-like minds?",
        "text": "By exploring how we define the mind in the context of AlphaGo and this artificial intelligence case study, we in turn also question what it means to be human, and the nature of the human condition.\n\nWell, it obviously depends on how we define the \u201cmind\u201d and define its relationship to the body. That\u2019s right\u2014can you articulate what exactly that comfortable and familiar private mode of thought you have actually is and where it exists? Is it physical (just like the code of AlphaGo)? Non-physical?\n\nSo with all this being said\u2014with this level of intuition, abstract thinking, and mental capacity required\u2014it might suffice to say: one would need a mind to play the game of Go. Lee Sedol has a mind (presumably, we\u2019ll even question the validity of this claim later on.) More importantly, though, we want to know: can A.I. have minds? If they can play the game of Go, does that mean they have some sort of mental process that we can call a mind?\n\nGo is not your regular board game, though. Chess was pretty easy for A.I. to master; a simple tree search would suffice because the game has a limited number of moves. But Go? It\u2019s a bit different; think trillions of moves. And it\u2019s not just the vast number of possibilities that makes Go immensely difficult for computers, it\u2019s the level of high-order abstract thinking (and even imagination and intuition) that makes it difficult for people as well. Sometimes the most effective plays/moves in Go are the ones which require a lot of advanced planning and aren\u2019t obvious when looking at the board. Put simply, it\u2019s definitely known to be one of the hardest games to get good at. Sorry, Chess!\n\nYet, though intuitive, as a very popular belief of many historical philosophers, we can\u2019t be sure if this is true nowadays with progression in science\u2014specifically that of machine learning. Machine learning is a sub-field in artificial intelligence that deals with the number-crunching code and algorithms that make a computer learn and act on acquired knowledge. The most recent compelling example is AlphaGo, a machine learning program created by DeepMind (a startup acquired by Google/Alphabet) that won in a match against world champ Lee Sedol in Go\u2014a Chinese board game.\n\nArtificial intelligence, in its recent and historical developments, has never failed to challenge our understanding of the human condition\u2014especially the nature of our mind. The problem of the mind and body, a branch/sub-topic in philosophy, is a prime example. That is, how are our minds (y\u2019know, that thing we use to think and feel) relate to our bodies (the physical part of us). It almost intuitively seems that these are two and separate but affect each other in a causal relationship. We just seem to have this private, intangible part of us and this public, physical part.\n\nLet\u2019s start with the most intuitive (I would think) approach for defining the mind. Such an approach states that there are, in a sense, two separate dimensions that make up our person. One such dimension is physical, and the other is non-physical. The physical dimension is, of course, our body\u2014not just our hands, legs, and arms but also our brain. That\u2019s right, we assert that the brain does not influence our mental states and thought process. Very anti-science, but you may see the justification for it soon. Our non-physical or intangible dimension is where these mental states/thought process/emotions instead do occur\u2014which is the mind. We thus say that the mind and body are separate.\n\nIt is said that, though separate, the mind and body can interact with each other. We see evidence of this in Lee Sedol, in fact. When battling against an opponent, he uses his the mind to decide where the stone (game piece) should go. Said mind induces an action in the body to actually move the stone from one location extended in space-time to a different location extended in space-time. When Lee Sedol loses the match\u2014which has to first register/observe through his sensory capabilities\u2014emotions such as dismay are invoked in his mind.\n\nSo, this notion is called \u201cdualism\u201d (pretty aptly titled). Ren\u00e9 Descartes, one of the most famous philosophers in the 17th century, created his own theory of dualism\u2014a very popular one today\u2014called \u201cCartesian dualism\u201d.\n\nCartesian dualism, like regular dualism, states that the mind and body are separate substances that have a mutually causal relationship. Descartes has a more concrete definition of the two, though. He states that the mind exists on a separate plane (that is, not the same plane/realm of reality that the world is in) that is immaterial, not affected by any laws of physics, and private to the person. The body is obviously the opposite of this.\n\nDescartes was extremely religious. All of his theories really center around this, including Cartesian dualism. Descartes believed that humans were the only creatures in the universe to have minds, with other animals being confined to solely the physical realm. This is supposedly because (and this claim is clearly subject to criticism) God chose it to be that way. Do remember, however, that this was the 17th century and every philosopher must be evaluated in context of the beliefs/scientific development/thinking of their era. Don\u2019t get me wrong\u2014he was definitely a smart guy. Does \u201cCartesian\u201d ring a bell to you in any other field aside from philosophy? If you guessed mathematics, you\u2019re right! Descartes invented the Cartesian plane.\n\nDescartes would obviously ridicule the notion that AlphaGo might have a mind. To him, AlphaGo is nothing but a bunch of lines of computer programming instructions, which is really just physical (ultimately everything on your computer is stored on your memory/storage as a physical representation).\n\nI sincerely believe that Descartes\u2019 ability to prosper in philosophy was severely handicapped by these religious constraints he needed to introduce. Because, once you probe deeper, you start to notice some really gaping holes in Cartesian dualism.\n\nFirst of all, if all animals and AlphaGo alike are purely physical beings without minds awarded by some arbitrary God (apologies for the loaded rhetoric!), what stops humans from being the same? Or, what stops the others from also having minds? Perhaps our mental states are just more advanced than, for example, a gorilla\u2019s, just like a gorilla\u2019s is more advanced than an ant\u2019s. Our more powerful mental faculties cannot and do not imply that we have a completely different makeup to other creatures in the universe.\n\nThe scientific evidence is, of course, not present to support any of the claims about an intangible mind existing. With that being said, it\u2019s not like we can scientifically observe anything that lies outside of our physical realm. Either way, the claim is unsubstantiated.\n\nThis actually isn\u2019t the biggest flaw, though. Something that Descartes (and almost every dualist out there to be frank) fails to logically assert is how the mind and the body can affect each other and interact. If the mind is on a separate \u201cplane\u201d than the body\u2014one being physical and the other being non-physical\u2014how is it possible for this to occur? When Lee Sedol\u2019s mind is forming the strategy of where to move the next stone, how does it enable his hand and the atomic makeup of his hand to actually move it? Remember that, in a non-physical realm there would be: no laws of physics, no momentum, and no quantities that relate to the functioning of our world at all. So how is this possible? I have no idea. But let\u2019s see what Descartes says.\n\nDescartes points to the \u201cpineal gland\u201d. This gland is an interface for the mind to work with the body and the body to work with the mind\u2014it\u2019s a real gland in our brain! Yet, there is obviously no evidence that this interface exists. The pineal gland serves a completely separate purpose in the brain and I\u2019m not even sure, to be honest, how Descartes came to choose it for his theory.\n\nOK, let\u2019s continue attacking some of Descartes claims \ud83d\ude1b Perhaps his weakest argument is the logic he proposes for the mind and the body being two separate substances. He does this through the following deduction:\n\nThis actually commits a fallacy called the \u201cmasked man fallacy\u201d, which is an invalid use of Leibniz law. Let me demonstrate this through another example. Let\u2019s say that a masked man has recently robbed a bank, and some people are speculating that it is your friend Bob (doesn\u2019t matter if you don\u2019t have a friend named Bob, just imagine!) With the same logic Descartes uses, we can quickly \u201cconclude\u201d that Bob must not be the masked man:\n\nAnyways, time to move on from dualism. It really does seem that the main issue is this whole notion of the mind and immaterial, so why not just focus on the body and physical realm?\n\nFor that, we turn to eliminative materialism. For many of the computer scientists reading, you\u2019ll finally find some sanity in this whole philosophy thing \ud83d\ude04 Eliminative materialism is very scientific, but perhaps\u2014as you\u2019ll soon find out\u2014painstakingly so. With materialism, we introduce nothing new, unfamiliar, or unfounded (such as a mind) in terms of science. Yes, though we often throw around the term \u201cmind\u201d when referring to our internal mode of thought, we really are just referring to the brain. Because, y\u2019know, let\u2019s not forget that the brain is where all of our thoughts, emotions, beliefs, disposition, etc. is scientifically supposed to originate first.\n\nHere\u2019s the issue though\u2014in doing this, we acknowledge that what is mental is just physical. If you can concede that our mind is really just physical then you\u2019re perfectly okay with that. But are you okay with its implications? If all of our thoughts, emotions, beliefs, disposition, etc. is from the brain, then they are simply the product of a biochemical machine. That\u2019s right\u2014the brain is an organ (I think!). It is controlled, ultimately, by the atoms (and thus quarks/leptons) that make it up and any sort of reactions that occur. Because, in our physical universe, everything that occurs has to be the result of something that occurred before it (the law of causality.) When we think, is that just a bunch of chemical reactions and electrical impulses going crazy? Does that mean that our beliefs are not really ours (as in, our persona) but the cooperation of a bunch of atomic particles? Because of causality, is everything we do and stand for pre-determined?\n\nYes. If we conform to eliminative materialism, then we start to quickly realize that everything about us just reduces (thus \u201celiminates\u201d) to the thinks that make us! As a result, we lack any sort of free will or agency. Lee Sedol\u2019s mastery of Go and his emotions after a shocking loss are just the result of neurons, axons, cortices, etc. Lee Sedol, and every other human out there, is no different to AlphaGo; both of us are purely physical beings. AlphaGo has a digital machine powering it, and I have a biochemical machine powering it. In this case, it just so happens that my biochemical machine\u2014through many many years of evolution\u2014is more complex than the digital machine, which we have designed in a short amount of time. For this reason, AlphaGo seemingly has less agency than we do.\n\nThat\u2019s the kind of definition of personhood and the human condition that pure scientific reasoning gives you. It makes life meaningless, and our innovations like AlphaGo equally so. Although it really does seem like we have free will, materialists state that the brain creates a very compelling illusion through its years of evolution.\n\nMaybe they\u2019re not wrong; maybe we just have to accept this absurdity. Oh man.\n\nScience has constantly disproved folk theories over the years; one such example is the flat Earth belief (which some lunatics still think is true). Maybe the notion of the mind is just another one that will be even more concretely shut down with advancements in neuroscience.\n\nBut, maybe not. The implications of eliminative materialism cannot be considered a weaknesses\u2014they\u2019re something we just have to deal with. That doesn\u2019t stop us, however, from finding weaknesses in it!\n\nThe greatest weakness of any materialist theory is the failure to explain qualia. Qualia is the subjective experiences of every day actions and perceptions. Listening to music is equivalent to the transfer of sound waves from a wire to your eardrums (I\u2019m pretty sure). But, to me \u2014 and maybe to you as well \u2014 it\u2019s so much more than that; it\u2019s places, atmospheres, moments, and just in general the subjective experience of listening to music. If these sensory events reduce to purely mechanical, replicable actions, how can this uniquely intangible experience emerge? Materialists have not figured out a way to address qualia, with famous philosopher Paul Churchland even outright denying it. The inability to fit the qualia is perhaps one of the biggest flaws in any mind-body theory.\n\n\u201cemerge\u201d\u2026 I like that word. This might be a good segue to our next theory (and my favorite one at that): emergentism! Emergentism states that, instead of a mind and a body being distinct substances that originate separately, the mind emerges from the body.\n\nFor this emergence to occur, there are basic constraints that have to be satisfied; for example, the brain or heart is needed to facilitate this emergence, but Lee Sedol\u2019s legs aren\u2019t. That\u2019s for a human. For a computer, we could suggest that the CPU and algorithm are examples of the basic constraints.\n\nThe substance that emerges is always greater in complexity and abstraction than the collective individual parts that make it up. When we pair a biochemical machine and an organ together, we get\u2026 consciousness, speech, and a disposition! We get Lee Sedol! When we pair a computer processor and running code together, we get AlphaGo similarly. Emergent properties are the properties of the emergent substance that do not exist in the individual parts (like consciousness or free will). AlphaGo is not able to achieve either of these, but this can be attributed to its less advanced makeup. Our progression in artificial intelligence over 70+ or so years can\u2019t compensate for the billions of years of human evolution.\n\nWith this being said, Lee Sedol and AlphaGo do actually have one similar (but not equal) individual part: neural networks. Neural networks\u2014literally deep, complex networks of neurons and neurotransmitters\u2014is supposedly how our nervous system, most of our sensory perception capabilities, and even thought process (maybe?) works. A digital implementation of neural networks (called \u201cartificial neural networks\u201d) is how most advanced machine learning and artificial intelligence programs like AlphaGo work.\n\nIt may be suggested, in fact, that neural networks could be the most fundamental constraint for the emergence of minds. How? Scientific evidence!\n\nResearchers at MIT University, around 16 years ago, hypothesized that neural networks were the basis of mental faculties. People used to believe that the brain was a large volume of separate instructions that coordinated together to power us humans. However, when said researchers found that animal brains could learn to adapt to changes\u2014they could use the auditory cortex to see after signals were redirected as such\u2014neural networks were denoted as the \u201cone learning algorithm\u201d. Without neural networks it\u2019s questionable if we could still have a lot of faculties like language, thought, and autonomy. This hypothesis has continued to stay prominent with the development of programs like AlphaGo that used digital implementations of neural networks\n\nThe mind emerging from the fundamental requirement of neural networks is known as \u201cconnectionism\u201d. There\u2019s a lot that we don\u2019t know yet about connectionism, and this mostly stems from the fact that we don\u2019t have a complete comprehension of neural networks either. Also, artificial neural networks are tiny compared to real ones; think tens of thousands of neurons for something like AlphaGo compared to trillions for a human like Lee Sedol. This discrepancy could be a reason for the emergent mind and emergent properties not being equal (in terms of the actual substance and complexity) between humans/Lee Sedol and AlphaGo. Maybe, once we start to introduce much deeper and more complex artificial neural networks we could achieve mild forms of consciousness. A bit of a jump here, but a cool thought nonetheless. Of course, there\u2019s a massive difference between tens of thousands and trillions. Either way, it is strange that, though AlphaGo does not have some of the faculties we do, it is better in some areas (highlighted by beating Lee Sedol in the match) than we are. I wonder if this demonstrates some sort of misunderstanding in the development of digital neural networks.\n\nConnectionism and the whole mind body conundrum are very thought provoking. I hope that, in this article, you not just learned about the potential nature of artificial intelligence but more importantly\u2026 the nature of us! This is useful knowledge to have, not just because it makes you question what you are but more importantly because we are on the brink (perhaps within 50+ years) of developing superintelligence and after that maybe even artificial general intelligence. These philosophical issues are only going to get more relevant, and it\u2019s important that we start to dip our toes in.\n\nThe day artificial intelligence obtains consciousness, I\u2019ll be sitting here saying: \u201ctold ya so!\u201d"
    },
    {
        "url": "https://ayearofai.com/rohan-6-follow-up-statistical-interpretation-of-logistic-regression-e78de3b4d938?source=---------3",
        "title": "Rohan #6: The beautiful math behind logistic regression.",
        "text": "It\u2019s been a while since my last article (work), but with summer approaching Lenny and I are going full speed, so expect articles on interesting machine learning algorithms (eg. recurrent and convolutional neural networks), our recent work with using machine learning to analyze fMRI scans, and new research papers from institution/companies like Google\u2019s DeepMind we are going to learn about at our trip to ICML 2016 next month. Keep your eyes peeled for some more philosophical write-ups on AI, too!\n\nToday I want to build on my very first article about logistic regression. In particular, I want to discuss the statistical/probabilistic interpretation of logistic regression, which I felt was missing from explanations and lectures provided by certain online courses like Andrew Ng\u2019s Machine Learning one (which is still wonderful). I will discuss the intuition behind the logistic regression model formulated in the previous article.\n\nUPDATE: The work presented in this article was part of my submission for my school mathematics coursework. Since I submitted it, and don\u2019t want to be caught plagiarizing myself (heh), I\u2019ve replaced the rest of the article with images of each page in the PDF. You can skip the intro and the conclusion + visualization through programming.\n\nOnce I get my IB results \u2014 around July \u2014 I\u2019ll put the post back up."
    },
    {
        "url": "https://ayearofai.com/lenny-2-autoencoders-and-word-embeddings-oh-my-576403b0113a?source=---------4",
        "title": "Lenny #2: Autoencoders and Word Embeddings \u2013",
        "text": "I want to take a quick break from my reinforcement learning endeavor (more on that soon) to talk about an interesting unsupervised learning model: autoencoders. The basic idea behind autoencoders is dimensionality reduction \u2014 I have some high-dimensional representation of data, and I simply want to represent the same data with fewer numbers. Let\u2019s take the example of a picture of a face. If you look at each individual pixel, you\u2019ll quickly realize that neighboring pixels are usually highly correlated (they have a very similar color). There\u2019s a lot of redundant information going on there. What if we could take out the redundancy and express that same image in a fraction of the numbers?\n\nThat\u2019s where autoencoders come in.\n\nRemember how neural networks work? Autoencoders are a kind of neural network designed for dimensionality reduction; in other words, representing the same information with fewer numbers. The basic premise is simple \u2014 we take a neural network and train it to spit out the same information it\u2019s given. By doing so, we ensure that the activations of each layer must, by definition, be able to represent the entirety of the input data (if it is to be successfully recovered later on). If each layer is the same size as the input, this becomes trivial, and the data can simply be copied over from layer to layer to the output. But if we start changing the size of the layers, the network inherently learns a new way to represent the data. If the size of one of the hidden layers is smaller than the input data, it has no choice but to find some way to compress the data.\n\nAnd that\u2019s exactly what an autoencoder does. If you look at the diagram, the network starts out by \u201ccompressing\u201d the data into a lower-dimensional representation z, and then converts it back to a reconstruction of the original input. If the network converges properly, z will be a more compressed version of the data that encodes the same information. For example, if the input image is a face, the data can be reduced down to certain defining characteristics of a face \u2014 shape, color, pose, and so on. Representing each of those characteristics could be more effective than storing each pixel, and that\u2019s what autoencoders are really great at.\n\nIt\u2019s often helpful to think about the network as an \u201cencoder\u201d and a \u201cdecoder\u201d. The first half of the network, the encoder, takes the input and transforms it into the lower-dimensional representation. The decoder then takes that lower-dimensional representation and converts it back to the original image (or as close to it as it can get). The encoder and decoder are still trained together, but once we have the weights, we can use the two separately \u2014 maybe we use the encoder to generate a more meaningful representation of some data we\u2019re feeding into another neural network, or the decoder to let the network generate new data we haven\u2019t shown it before.\n\nThat\u2019s pretty much it for the basic concept of autoencoders, but there are a few modifications that people have made that are worth mentioning.\n\nDenoising autoencoders are a technique often used to help the network learn representations of the data that are more meaningful to the underlying data\u2019s variability. Instead of simply training a network to recall the input it was given, random noise is applied to the input before passing it to the network \u2014 the network is still expected to recall the original (uncorrupted) input, which should force the network to stop picking up on minute details while focusing on the bigger picture. The random noise essentially prevents the network from learning the specifics, ensuring that it generalizes to the important characteristics. This is especially important in the situation where the encoded vector is larger than the input \u2014 using a traditional autoencoder, the network could (and often will) simply learn to copy the input into the encoded vector to recreate the original. With a denoising autoencoder, the autoencoder can no longer do that, and it\u2019s more likely to learn a meaningful representation of the input.\n\nWe haven\u2019t covered recurrent neural networks (RNNs) directly (yet), but they\u2019ve certainly been cropping up more and more \u2014 and sure enough, they\u2019ve been applied to autoencoders. The basic premise behind an RNN is that, instead of operating on and producing a fixed-size vector, we instead start operating on and producing sequences of fixed-size vectors. Let\u2019s take the example of machine translation (here\u2019s a sentence in English, give me the same sentence in Russian). Posing this problem to a vanilla neural network is intractable (we could translate the sentence word-for-word, but we already know grammar doesn\u2019t work like that). Enter RNNs: we give the network the input sentence one word at a time, and the network magically spits out the same sentence in Spanish, one word at a time. We\u2019ll get more into the specifics of how exactly an RNN works soon enough, but the general concept is enough for the time being.\n\nSequence-to-sequence autoencoders work the same way as traditional autoencoders, except both the encoder and decoder are RNNs. So instead of converting a vector to a smaller vector, we convert an entire sequence into one vector. Then we take that one vector and expand it back into a sequence. Let\u2019s take the example of getting a fixed-length representation of a variable-length audio clip. The autoencoder takes in an audio segment and produces a vector to represent that data. That\u2019s the encoder. We can then take that vector and give it to the decoder, which gives back the original audio clip.\n\nI won\u2019t go into depth on how variational autoencoders work now (they really deserve a post of their own \u2014 maybe this summer), but the concept is still important enough that it\u2019s worth mentioning. If you\u2019re curious (and comfortable with some heavy-duty statistics), this paper is worth a read.\n\nVariational autoencoders essentially let us create a generative model for our data. A generative model learns to create brand new data that isn\u2019t from the training set, but that looks as if it is. For example, given a dataset of faces, it will learn to generate brand new faces that look real. Variational autoencoders aren\u2019t the only generative model \u2014 generative adversarial networks are another kind of generative model that we\u2019ll take a look at some other day. (Forgive me for the handy-wavy explanation, but specifics are hard without going into the statistics. Rest assured I\u2019ll give variational autoencoders the explanation they deserve soon enough.)\n\nNow I want to talk about a different (but somewhat related) type of model known as a word embedding. Like an autoencoder, this type of model learns a vector space embedding for some data. I kind of skipped over this point earlier on, so let me take a minute to address this.\n\nLet\u2019s go back to the example of faces one last time. We started out by representing faces as a series of pixel values (let\u2019s say 10,000 pixels per face) \u2014 for simplicity\u2019s sake, we\u2019ll assume that the image is grayscale and each pixel is represented by a single intensity value, but know that the same principles carry over for RGB data (and, of course, non-image data too). Now, let\u2019s do something a bit interesting, and maybe unnatural at first, with these pixels. Let\u2019s use each pixel value as a coordinate in a 10,000-dimensional space. (Sidenote: if you\u2019re having trouble visualizing this, it\u2019s because we mortal humans can\u2019t think in more than 3 dimensions \u2014 just think of everything in terms of 2 or 3 dimensions, but realize that the exact same principles transfer over to as many dimensions as you want). So we have a 10,000-dimensional space, and and the 10,000 pixel values are the coordinates into this space. Each image in our dataset has a distinct location in this space. If you were to look at this impossible-to-visualize vector space, you\u2019ll notice that pictures that have very similar pixel values are very close to each other, while very different images are very far away from each other.\n\nThis relationship between images is critical, and it\u2019s what makes it useful to think about data in vector form. But there are flaws in the coordinate system we\u2019re using now \u2014 for example, if you have an identical image but shifted over one pixel, it could be nowhere near the original image in this vector space because each dimension will have a wildly different coordinate. If we can find some coordinate system that gives us more meaningful relationships between points in our vector space, we can put our images into a space that tells us even more about the relationships between these images.\n\nAnd if you think about it, that\u2019s exactly what neural networks do! We take an image, which is represented by very low-level pixel data, and convert it into a higher-level representation that might encode features like pose or facial expression. If we train our neural network on, for example, a classification task, we\u2019ll get a new representation that best encompasses the differences between the various classes. If we\u2019re using an autoencoder, the new representation will instead tell us more about the factors that best define what is unique about a particular face. If we use these new encoded vectors as coordinates into a new coordinate system, we\u2019ll see that the relationship between images suddenly becomes even more meaningful \u2014 images that are close together are similar not because of pixel-by-pixel similarity, but because they have similar characteristics. Instead of having a dimension per pixel, we have a dimension for every feature of the face (pose, color, etc.). As a result, as we move along a certain axis, we notice that an individual feature of the image changes while the rest remains the same (for example, we might notice that a smile gradually becomes a frown, while skin color and pose remain the same). If we were to try the same thing on our original pixel-based coordinate system, we\u2019ll just see the intensity of one individual pixel change, which is much less meaningful.\n\nSo this process of taking a point in one vector space (the pixel-based one) and putting it into a different vector space (the autoencoder-based one) is a vector space embedding. We are embedding the points of one vector space into another, which could tell us something new about our data that we didn\u2019t quite realize before.\n\nThe process of creating a meaningful vector space embedding is usually relatively straightforward for something like faces. If two faces have similar appearance, they should appear next each other in the vector space. Traditional autoencoders are pretty good at this. But the relationships aren\u2019t always quite so obvious \u2014 let\u2019s look at the example of words. If I look at the letters that make up a word, it\u2019s darn near impossible for me to make sense of what words should be near each other. I might notice that \u201cpresentation\u201d and \u201cpresent\u201d have similar letters, so they should be next to each other; but on the other hand, \u201cprequel\u201d and \u201csequel\u201d also have similar letters, yet they have a very different relationship. Similarly, \u201cyacht\u201d and \u201cboat\u201d are quite related, but don\u2019t have much in common letter-wise. As a result, words are usually encoded in vector form using one-of-k encoding (also known as one-hot encoding). If we have a total of N possible words, then each vector will have N numbers, and all but one of the numbers will be a 0. Each word will have its own index into the vector, and the value at that position will be a 1. While this does let us differentiate between words, it gives us no information about how words are related or linked.\n\nWe need a better system. Instead of using one-hot encoding, what if we can use something like an autoencoder to reduce our words to a lower-dimensional vector that gives us more information about the meaning and characteristics of a word itself? Sounds like a plan! But, as we already established, autoencoders can\u2019t learn much meaning from the letters that make up a word. We\u2019ll need to set up a new model to learn the meanings of and relationships between words.\n\nAnd that is exactly what a word embedding is. We\u2019ll take a look at a popular word embedding model shortly, but first let\u2019s explore some of the fascinating properties a word embedding vector space can have. As it turns out, and this shouldn\u2019t come as much of a surprise, words have much more complicated relationships than pictures of faces. For example, take \u201cWashington D.C.\u201d and \u201cUnited States\u201d. The two are obviously related, but the relationship is very specific. It\u2019s also the same relationship that \u201cOttawa\u201d and \u201cCanada\u201d have. And both of those relationships are very different than the relationship between \u201cboy\u201d and \u201cgirl\u201d. Ideally, a word embedding model will be able to express each of these relationships distinctly.\n\nWhen you have a vector space, you can express relationships as the vector between two points. So, if you take the distance from \u201cboy\u201d to \u201cgirl\u201d, traveling the same distance (in the same direction) should get you from \u201cman\u201d to \u201cwoman\u201d, because they two have the same relationship. But the direction you take to get from \u201cOttawa\u201d to \u201cCanada\u201d should be completely different. If we can create a word embedding space that captures each of these relationships, that means that we\u2019ll now have an incredibly useful and meaningful way to represent words. Once we can get words into a space like this, neural networks (and of course other ML models) will be able to learn much more effectively \u2014 while neural networks would typically have to inherently learn a simplified version of these relationships on their own, the ability to give them this knowledge directly means that it\u2019s easier to perform natural language processing tasks (like, for example, machine translation).\n\nNow, finally, we can talk about how we get these word embeddings. I\u2019m going to talk about one particularly impressive model called word2vec. At a very high level, it learns its word embedding through context. This actually makes lots of sense \u2014 when you see a word you don\u2019t know, you look at the words around it to figure out what it means. As it turns out, word2vec does the exact same thing.\n\nGenerally, word2vec is trained using something called a skip-gram model. The skip-gram model, pictures above, attempts to use the vector representation that it learns to predict the words that appear around a given word in the corpus. Essentially, it uses the context of the word as it is used in a variety of books and other literature to derive a meaningful set of numbers. If the \u201ccontext\u201d of two words is similar, they will have similar vector representations \u2014 that already makes this new embedding more useful than one-hot encoding. But, as it turns out, the relationships go deeper.\n\nIf you look at the relationship between a country and its capital, you\u2019ll notice that the vector taking you from one to the other is almost identical in every instance. That is one of the other critical characteristics we defined earlier for a word embedding \u2014 representing the unique relationships that words have with each other.\n\nLet\u2019s look at a neural network trained for machine translation to get a better idea of how this new word embedding helps us. Giving it words as one-hot vectors is not super useful \u2014 it has to learn on its own that the vector for \u201cscreen\u201d and \u201cdisplay\u201d have the same meaning, even though there is no correlation whatsoever between the two vectors. But, using our fancy new word2vec embedding, \u201cscreen\u201d and \u201cdisplay\u201d should have a nearly identical vector representations \u2014 so this correlation comes very naturally. In fact, even if the network has never seen a training sample with the word \u201cdisplay\u201d in it, it will likely be able to generalize based purely on the examples it has seen with \u201cscreen\u201d because the two have a similar representation.\n\nThere\u2019s lots more I didn\u2019t get to cover about autoencoders (most notably pre-training comes to mind, but that\u2019s for another day), but hopefully you learned enough to understand where they might come in handy and a few ways they can be used in practice. We also talked about embeddings and word2vec, a word embedding model that gives us meaningful word vectors to make our other models more effective. But the utility of word2vec extends far beyond NLP \u2014 just last year, a word2vec-based system of creating an embedding space for protein sequences was proposed for improved accuracy on classification tasks.\n\nThe applications for these unsupervised learning models are practically endless, and they generally serve to improve the accuracy of more traditional supervised learning problems. That being said, I\u2019m looking forward to seeing what else comes out of the field in the coming years \u2014 the most exciting advances are still to come."
    },
    {
        "url": "https://ayearofai.com/rohan-5-what-are-bias-units-828d942b4f52?source=---------5",
        "title": "Rohan #5: What are bias units? \u2013",
        "text": "In a typical artificial neural network, each neuron/activity in one \u201clayer\u201d is connected \u2014 via a weigh \u2014 to each neuron in the next activity. Each of these activities stores some sort of computation, normally a composite of the weighted activities in previous layers.\n\nA bias unit is an \u201cextra\u201d neuron added to each pre-output layer that stores the value of 1. Bias units aren\u2019t connected to any previous layer and in this sense don\u2019t represent a true \u201cactivity\u201d.\n\nTake a look at the following illustration:\n\nThe bias units are characterized by the text \u201c+1\u201d. As you can see, a bias unit is just appended to the start/end of the input and each hidden layer, and isn\u2019t influenced by the values in the previous layer. In other words, these neurons don\u2019t have any incoming connections.\n\nSo why do we have bias units? Well, bias units still have outgoing connections and they can contribute to the output of the ANN. Let\u2019s call the outgoing weights of the bias units w_b. Now, let\u2019s look at a really simple neural network that just has one input and one connection:\n\nLet\u2019s say act() \u2014 our activation function \u2014 is just f(x) = x, or the identity function. In such case, our ANN would represent a line because the output is just the weight (m) times the input (x).\n\nWhen we change our weight w1, we will change the gradient of the function to make it steeper or flatter. But what about shifting the function vertically? In other words, what about setting the y-intercept. This is crucial for many modelling problems! Our optimal models may not pass through the origin.\n\nSo, we know that our function output = w \u00b7 input (y = mx) needs to have this constant term added to it. In other words, we can say output = w \u00b7 input + w_b, where w_b is our constant term c. When we use neural networks, though, or do any multi-variable learning, our computations will be done through Linear Algebra and matrix arithmetic eg. dot-product, multiplication. This can also be seen graphically in the ANN. There should be a matching number of weights and activities for a weighted sum to occur. Because of this, we need to \u201cadd\u201d an extra input term so that we can add a constant term with it. Since, one multiplied by any value is that value, we just \u201cinsert\u201d an extra value of 1 at every layer. This is called the bias unit.\n\nFrom this diagram, you can see that we\u2019ve now added the bias term and hence the weight w_b will be added to the weighted sum, and fed through activation function as a constant value. This constant term, also called the \u201cintercept term\u201d (as demonstrated by the linear example), shifts the activation function to the left or to the right. It will also be the output when the input is zero.\n\nHere is a diagram of how different weights will transform the activation function (sigmoid in this case) by scaling it up/down:\n\nBut now, by adding the bias unit, we the possibility of translating the activation function exists:\n\nGoing back to the linear regression example, if w_b = 1, then we will add bias \u00b7 w_b= 1 \u00b7 w_b = w_b to the activation function. In the example with the line, we can create a non-zero y-intercept:\n\nI\u2019m sure you can imagine infinite scenarios where the line of best fit does not go through the origin or even come near it. Bias units are important with neural networks in the same way."
    },
    {
        "url": "https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b?source=---------6",
        "title": "Rohan #4: The vanishing gradient problem \u2013",
        "text": "Let\u2019s take a look at the derivative of the sigmoid function. I\u2019ve precomputed it for you:\n\nThe sigmoid function is useful because it \u201csqueezes\u201d any input value into an output range of (0, 1) (where it asymptotes). This is perfect for representations of probabilities and classification. The sigmoid function, along with the tanh function, though, have lost popularity in recent years. Why? Because they suffer from the vanishing gradient problem!\n\nRecall the sigmoid function, one that was almost always used as an activation function for ANNs in a classification context:\n\nOkay. Small gradients = bad news, got it. The question, then, is: does this problem exist? In many cases it indeed does, and we call it the vanishing gradient problem .\n\nThis isn\u2019t the only issue. When gradients become too small, it also becomes difficult to represent the numerical value in computers. We call this underflow \u2014 the opposite of overflow.\n\nSo, why is this the case? Well, put simply, if the gradient at each step is too small, then greater repetitions will be needed until convergence, because the weight is not changing enough at each iteration. Or, the weights will not move as close to the minimum (versus greater gradients) in the set number of iterations. And with really really small gradients, this becomes a problem. It becomes infeasible to train neural networks, and they start predicting poorly.\n\nAs you can see, we are to \u201crepeat\u201d until convergence. In reality, though, we actually set a hyper-parameter for the number of max iterations. If the number of iterations is too small for certain deep neural nets, we will have inaccurate results. If the number is too large, the training duration will become infeasibly long. It\u2019s an unsettling tradeoff between training time and accuracy.\n\nHere\u2019s what we know about iterative optimization algorithms: they slowly make their way to the local optima by perturbing weights in a direction inferred from the gradient such that the cost function\u2019s output is decreased. The gradient descent algorithm, in specific, updates the weights by the negative of the gradient multiplied by some small (between 0 and 1) scalar value.\n\nIt\u2019s summer time, and you recently read my Medium post on the backpropagation algorithm. Excited to hack away at your own implementation, you create a deep, multi-layer neural network and begin running the program. But, to your avail, it\u2019s either taking forever to train or not performing accurately. Why\u203d You thought neural networks were state of the art, didn\u2019t you!\n\nLooks decent, you say. Look closer. The maximum point of the function is 1/4, and the function horizontally asymptotes at 0. In other words, the output of the derivative of the cost function is always between 0 and 1/4. In mathematical terms, the range is (0, 1/4]. Keep that in mind.\n\nNow, let\u2019s move on to the structure of a neural network and backprop and their implications on the size of gradients.\n\nRecall this general structure for a simple, univariate neural network. Each neuron or \u201cactivity\u201d is derived from the previous: it is the previous activity multiplied by some weight and then fed through an activation function. The input, of course, is the notable exception. The error box J at the end returns the aggregate error of our system. We then perform backpropagation to modify the weights through gradient descent such that the output of J is minimized.\n\nTo calculate the derivative to the first weight, we used the chain rule to \u201cbackpropagate\u201d like so:\n\nWith regards to the first derivative \u2014 since the output is the activation of the 2nd hidden unit, and we are using the sigmoid function as our activation function, then the derivative of the output is going to contain the derivative of the sigmoid function. In specific, the resulting expression will be:\n\nThe same applies for the second:\n\nIn both cases, the derivative contains the derivative of the sigmoid function. Now, let\u2019s put those together:\n\nRecall that the derivative of the sigmoid function outputs values between 0 and 1/4. By multiplying these two derivatives together, we are multiplying two values in the range (0, 1/4]. Any two numbers between 0 and 1 multiplied with each other will simply result in a smaller value. For example, 1/3 \u00d7 1/3 is 1/9.\n\nThe standard approach to weight initialization in a typical neural network is to choose weights using a Gaussian with mean 0 and standard deviation 1. Hence, the weights in a neural network will also usually be between -1 and 1.\n\nNow, look at the magnitude of the terms in our expression:\n\nAt this point, we are multiplying four values which are between 0 and 1. That will become small very fast. And even if this weight initialization technique is not employed, the vanishing gradient problem will most likely still occur. Many of these sigmoid derivatives multiplied together would be small enough to compensate for the other weights, and the other weights may want to shift into a range below 1.\n\nThis neural network isn\u2019t that deep. But imagine a deeper one used in an industrial application. As we backpropagate further back, we\u2019d have many more small numbers partaking in a product, creating an even tinier gradient! Thus, with deep neural nets, the vanishing gradient problem becomes a major concern. Sidenote: we can even have exploding gradients, if the gradients were bigger than 1 (a bunch of numbers bigger than 1 multiplied together is going to give a huge result!) These aren\u2019t good either \u2014 they wildly overshoot.\n\nNow, let\u2019s look at a typical ANN:\n\nAs you can see, the first layer is the furthest back from the error, so the derivative (using the chain rule) will be a longer expression and hence will contain more sigmoid derivatives, ending up smaller. Because of this, the first layers are the slowest to train. But there\u2019s another issue with this: since the latter layers (and most notably the output) is functionally dependent on the earlier layers, inaccurate early layers will cause the latter layers to simply build on this inaccuracy, corrupting the entire neural net. Take convolutional neural nets as an example; their early layers perform more high-level feature detection such that the latter layers can analyze them further to make a choice. Also, because of the small steps, gradient descent may converge at a local minimum.\n\nThis is why neural networks (especially deep ones), at first, failed to become popular. Training the earlier layers correctly was the basis for the entire network, but that proved too difficult and infeasible because of the commonly used activation functions and available hardware.\n\nHow do we solve this? Well, it\u2019s pretty clear that the root cause is the nature of the sigmoid activation function derivative. This also happened in the most popular alternative, the tanh function. Until recently, not many other activation functions were thought of / used. But now, the sigmoid and tanh functions have been declining in popularity in the light of the ReLU activation function.\n\nWhat is the ReLU \u2014 Rectified Linear Unit \u2014 function anyways? It is a piecewise function that corresponds to:\n\nAnother way of writing the ReLU function is like so:\n\nIn other words, when the input is smaller than zero, the function will output zero. Else, the function will mimic the identity function. It\u2019s very fast to compute the ReLU function.\n\nIt doesn\u2019t take a genius to calculate the derivative of this function. When the input is smaller than zero, the output is always equal to zero, and so the rate of change of the function is zero. When the input is greater or equal to zero, the output is simply the input, and hence the derivative is equal to one:\n\nIf we were to graph this derivative, it would look exactly like a typical step function:\n\nSo, it\u2019s solved! Our derivatives will no longer vanish, because the activation function\u2019s derivative isn\u2019t bounded by the range (0, 1).\n\nReLUs have one caveat though: they \u201cdie\u201d (output zero) when the input to it is negative. This can, in many cases, completely block backpropagation because the gradients will just be zero after one negative value has been inputted to the ReLU function. This would also be an issue if a large negative bias term / constant term is learned \u2014 the weighted sum fed into neurons may end up being negative because the positive weights cannot compensate for the significance of the bias term. Negative weights also come to mind, or negative input (or some combination that gives a negative weighted sum). The dead ReLU will hence output the same value for almost all of your activities \u2014 zero. ReLUs cannot \u201crecover\u201d from this problem because they will not modify the weights in anyway, since not only is the output for any negative input zero, the derivative is too. No updates will be made to modify the (for example) bias term to be a lesser magnitude of negative such that the neural net can escape from corruption of the entire network. It doesn\u2019t happen all that often that the weighted sum ends up negative, though; and we can indeed initialize weights to be only positive and/or normalize input between 0 and 1 if we are concerned about the chance of an issue like this occurring.\n\nEDIT: As it turns out, there are some extremely useful properties of gradients dying off. In particular, the idea of \u201csparsity\u201d. So, many times, backprop being blocked can actually be an advantage. More on that in this StackOverflow answer:\n\nA \u201cleaky\u201d ReLU solves this problem. Leaky Rectified Linear Units are ones that have a very small gradient instead of a zero gradient when the input is negative, giving the chance for the net to continue its learning.\n\nEDIT: Looks like values in the range of 0.2\u20130.3 are more common than something like 0.01.\n\nInstead of outputting zero when the input is negative, the function will output a very flat line, using gradient \u03b5. A common value to use for \u03b5 is 0.01. The resulting function is represented in following diagram:\n\nAs you can see, the learning will be small with negative inputs, but will exist nonetheless. In this sense, leaky ReLUs do not die.\n\nHowever, ReLUs/leaky ReLUs aren\u2019t necessarily always optimal \u2014 results with them have been inconsistent (maybe because, due to the small constant, in certain cases this could cause vanishing gradients again? \u2014 that being said, dead units again don\u2019t happen all that often). Another notable issue is that, because the output of ReLU isn\u2019t bounded between 0 and 1 or -1 and 1 like tanh/sigmoid are, the activations (values in the neurons in the network, not the gradients) can in fact explode with extremely deep neural networks like recurrent neural networks. During training, the whole network becomes fragile and unstable in that, if you update weights in the wrong direction even the slightest, the activations can blow up. Finally, even though the ReLU derivatives are either 0 or 1, our overall derivative expression contains the weights multiplied in. Since the weights are generally initialized to be < 1, this could contribute to vanishing gradients.\n\nSo, overall, it\u2019s not a black and white problem. ReLUs still face the vanishing gradient problem, it\u2019s just that they often face it to a lesser degree. In my opinion, the vanishing gradient problem isn\u2019t binary; you can\u2019t solve it with one technique, but you can delay (in terms of how deep we can go before we start suffering again) its impact.\n\nYou may also be wondering: by the way, ReLUs don\u2019t squeeze values into a probability, so why are they so commonly used? One can easily stick a sigmoid/logistic activity to the end of a neural net for a binary classification scenario. For multiple outputs, one could use the softmax function to create a probability distribution that adds to 1.\n\nThis article was a joy to write because it was my first step into the intricacies and practice of Machine Learning, rather than just looking at theoretical algorithms and calculations. The vanishing gradient problem was a major obstacle for the success of deep learning, but now that we\u2019ve overcome it through multiple different techniques in weight initialization (which I talked less about today), feature preparation (through batch normalization \u2014 centering all input feature values to zero), and activation functions, the field is going to thrive \u2014 and we\u2019ve already seen it with recent innovations in game AI (AlphaGo, of course!). Here\u2019s to the future of multi-layered neural networks! \ud83c\udf7a"
    },
    {
        "url": "https://ayearofai.com/rohan-lenny-1-neural-networks-the-backpropagation-algorithm-explained-abf4609d4f9d?source=---------7",
        "title": "Rohan & Lenny #1: Neural Networks & The Backpropagation Algorithm, Explained",
        "text": "In Rohan\u2019s last post, he talked about evaluating and plugging holes in his knowledge of machine learning thus far. The backpropagation algorithm \u2014 the process of training a neural network \u2014 was a glaring one for both of us in particular. Together, we embarked on mastering backprop through some great online lectures from professors at MIT & Stanford. After attempting a few programming implementations and hand solutions, we felt equipped to write an article for AYOAI \u2014 together.\n\nToday, we\u2019ll do our best to explain backpropagation and neural networks from the beginning. If you have an elementary understanding of differential calculus and perhaps an intuition of what machine learning is, we hope you come out of this blog post with an (acute, but existent nonetheless) understanding of neural networks and how to train them. Let us know if we succeeded!\n\nLet\u2019s start off with a quick introduction to the concept of neural networks. Fundamentally, neural networks are nothing more than really good function approximators \u2014 you give a trained network an input vector, it performs a series of operations, and it produces an output vector. To train our network to estimate an unknown function, we give it a collection of data points \u2014 which we denote the \u201ctraining set\u201d \u2014 that the network will learn from and generalize on to make future inferences.\n\nNeural networks are structured as a series of layers, each composed of one or more neurons (as depicted above). Each neuron produces an output, or activation, based on the outputs of the previous layer and a set of weights.\n\nWhen using a neural network to approximate a function, the data is forwarded through the network layer-by-layer until it reaches the final layer. The final layer\u2019s activations are the predictions that the network actually makes.\n\nAll this probably seems kind of magical, but it actually works. The key is finding the right set of weights for all of the connections to make the right decisions (this happens in a process known as training) \u2014 and that\u2019s what most of this post is going to be about.\n\nWhen we\u2019re training the network, it\u2019s often convenient to have some metric of how good or bad we\u2019re doing; we call this metric the cost function. Generally speaking, the cost function looks at the function the network has inferred and uses it to estimate values for the data points in our training set. The discrepancies between the outputs in the estimations and the training set data points are the principle values for our cost function. When training our network, the goal will be to get the value of this cost function as low as possible (we\u2019ll see how to do that in just a bit, but for now, just focus on the intuition of what a cost function is and what it\u2019s good for). Generally speaking, the cost function should be more or less convex, like so:\n\nIn reality, it\u2019s impossible for any network or cost function to be truly convex. However, as we\u2019ll soon see, local minima may not be a big deal, as long as there is still a general trend for us to follow to get to the bottom. Also, notice that the cost function is parameterized by our network\u2019s weights \u2014 we control our loss function by changing the weights.\n\nOne last thing to keep in mind about the loss function is that it doesn\u2019t just have to capture how correctly your network estimates \u2014 it can specify any objective that needs to be optimized. For example, you generally want to penalize larger weights, as they could lead to overfitting. If this is the case, simply adding a regularization term to your cost function that expresses how big your weights will mean that, in the process of training your network, it will look for a solution that has the best estimates possible while preventing overfitting.\n\nNow, let\u2019s take a look at how we can actually minimize the cost function during the training process to find a set of weights that work the best for our objective.\n\nNow that we\u2019ve developed a metric for \u201cscoring\u201d our network (which we\u2019ll denote as J(W)), we need to find the weights that will make that score as low as possible. If you think back to your pre-calculus days, your first instinct might be to set the derivative of the cost function to zero and solve, which would give us the locations of every minimum/maximum in the function. Unfortunately, there are a few problems with this approach:\n\nEspecially as the size of networks begins to scale up, solving for the weights directly becomes increasingly infeasible. Instead, we look at a different class of algorithms, called iterative optimization algorithms, that progressively work their way towards the optimal solution.\n\nThe most basic of these algorithms is gradient descent. Recall that our cost function will be essentially convex, and we want to get as close as possible to the global minimum. Instead of solving for it analytically, gradient descent follows the derivatives to essentially \u201croll\u201d down the slope until it finds its way to the center.\n\nLet\u2019s take the example of a single-weight neural network, whose cost function is depicted below.\n\nWe start off by initializing our weight randomly, which puts us at the red dot on the diagram above. Taking the derivative, we see the slope at this point is a pretty big positive number. We want to move closer to the center \u2014 so naturally, we should take a pretty big step in the opposite direction of the slope.\n\nIf we repeat the process enough, we soon find ourselves nearly at the bottom of our curve and much closer to the optimal weight configuration for our network.\n\nMore formally, gradient descent looks something like this:\n\nLet\u2019s dissect. Every time we want to update our weights, we subtract the derivative of the cost function w.r.t. the weight itself, scaled by a learning rate , and \u2014 that\u2019s it! You\u2019ll see that as it gets closer and closer to the center, the derivative term gets smaller and smaller, converging to zero as it approaches the solution. The same process applies with networks that have tens, hundreds, thousands, or more parameters \u2014 compute the gradient of the cost function w.r.t. each of the weights, and update each of your weights accordingly.\n\nI do want to say a few more words on the learning rate, because it\u2019s one of the more important hyperparameters (\u201csettings\u201d for your neural network) that you have control over. If the learning rate is too high, it could jump too far in the other direction, and you never get to the minimum you\u2019re searching for. Set it too low, and your network will take ages to find the right weights, or it will get stuck in a local minimum. There\u2019s no \u201cmagic number\u201d to use when it comes to a learning rate, and it\u2019s usually best to try several and pick the one that works the best for your individual network and dataset. In practice, many choose to anneal the learning rate over time \u2014 it starts out high, because it\u2019s furthest from the solution, and decays as it gets closer.\n\nBut as it turns out, gradient descent is kind of slow. Really slow, actually. Earlier I used the analogy of the weights \u201crolling\u201d down the gradient to get to the bottom, but that doesn\u2019t actually make any sense \u2014 it should pick up speed as it gets to the bottom, not slow down! Another iterative optimization algorithm, known as momentum, does just that. As the weights begin to \u201croll\u201d down the slope, they pick up speed. When they get closer to the solution, the momentum that they picked up carries them closer to the optima while gradient descent would simply stop. As a result, training with momentum updates is both faster and can provide better results.\n\nHere\u2019s what the update rule looks like for momentum:\n\nAs we train, we accumulate a \u201cvelocity\u201d value V. At each training step, we update V with the gradient at the current position (once again scaled by the learning rate). Also notice that, with each time step, we decay velocity V by a factor mu (usually somewhere around .9), so that over time we lose momentum instead of bouncing around by the minimum forever. We then update our weight in the direction of the velocity, and repeat the process again. Over the first few training iterations, V will grow as our weights \u201cpick up speed\u201d and take successively bigger leaps. As we approach the minimum, our velocity stops accumulating as quickly, and eventually begins to decay, until we\u2019ve essentially reached the minimum. An important thing to note is that we accumulate a velocity independently for each weight \u2014 just because one weight is changing particularly clearly doesn\u2019t mean any of the other weights need to be.\n\nThere are lots of other iterative optimization algorithms that are commonly used with neural networks, but I won\u2019t go into all of them here (if you\u2019re curious, some of the more popular ones include Adagrad and Adam). The basic principle remains the same throughout \u2014 gradually update the weights to get them closer to the minimum. But regardless of which optimization algorithm you use, we still need to be able to compute the gradient of the cost function w.r.t. each weight. But our cost function isn\u2019t a simple parabola anymore \u2014 it\u2019s a complicated, many-dimensional function with countless local optima that we need to watch out for. That\u2019s where backpropagation comes in.\n\nThe backpropagation algorithm was a major milestone in machine learning because, before it was discovered, optimization methods were extremely unsatisfactory. One popular method was to perturb (adjust) the weights in a random, uninformed direction (ie. increase or decrease) and see if the performance of the ANN increased. If it did not, one would attempt to either a) go in the other direction b) reduce the perturbation size or c) a combination of both. Another attempt was to use Genetic Algorithms (which became popular in AI at the same time) to evolve a high-performance neural network. In both cases, without (analytically) being informed on the correct direction, results and efficiency were suboptimal. This is where the backpropagation algorithm comes into play.\n\nRecall that, for any given supervised machine learning problem, we (aim to) select weights that provide the optimal estimation of a function that models our training data. In other words, we want to find a set of weights W that minimizes on the output of J(W). We discussed the gradient descent algorithm \u2014 one where we update each weight by some negative, scalar reduction of the error derivative with respect to that weight. If we do choose to use gradient descent (or almost any other convex optimization algorithm), we need to find said derivatives in numerical form.\n\nFor other machine learning algorithms like logistic regression or linear regression, computing the derivatives is an elementary application of differentiation. This is because the outputs of these models are just the inputs multiplied by some chosen weights, and at most fed through a single activation function (the sigmoid function in logistic regression). The same, however, cannot be said for neural networks. To demonstrate this, here is a diagram of a double-layered neural network:\n\nAs you can see, each neuron is a function of the previous one connected to it. In other words, if one were to change the value of w1, both \u201chidden 1\u201d and \u201chidden 2\u201d (and ultimately the output) neurons would change. Because of this notion of functional dependencies, we can mathematically formulate the output as an extensive composite function:\n\nHere, the output is a composite function of the weights, inputs, and activation function(s). It is important to realize that the hidden units/nodes are simply intermediary computations that, in actuality, can be reduced down to computations of the input layer.\n\nIf we were to then take the derivative of said function with respect to some arbitrary weight (for example w1), we would iteratively apply the chain rule (which I\u2019m sure you all remember from your calculus classes). The result would look similar to the following:\n\nNow, let\u2019s attach a black box to the tail of our neural network. This black box will compute and return the error \u2014 using the cost function \u2014 from our output:\n\nAll we\u2019ve done is add another functional dependency; our error is now a function of the output and hence a function of the input, weights, and activation function. If we were to compute the derivative of the error with any arbitrary weight (again, we\u2019ll choose w1), the result would be:\n\nEach of these derivatives can be simplified once we choose an activation and error function, such that the entire result would represent a numerical value. At that point, any abstraction has been removed, and the error derivative can be used in gradient descent (as discussed earlier) to iteratively improve upon the weight. We compute the error derivatives w.r.t. every other weight in the network and apply gradient descent in the same way. This is backpropagation \u2014 simply the computation of derivatives that are fed to a convex optimization algorithm. We call it \u201cbackpropagation\u201d because it almost seems as if we are traversing from the output error to the weights, taking iterative steps using chain the rule until we \u201creach\u201d our weight.\n\nWhen I first truly understood the backprop algorithm (just a couple of weeks ago), I was taken aback by how simple it was. Sure, the actual arithmetic/computations can be difficult, but this process is handled by our computers. In reality, backpropagation is just a rather tedious (but again, for a generalized implementation, computers will handle this) application of the chain rule. Since neural networks are convoluted multilayer machine learning model structures (at least relative to other ones), each weight \u201ccontributes\u201d to the overall error in a more complex manner, and hence the actual derivatives require a lot of effort to produce. However, once we get past the calculus, backpropagation of neural nets is equivalent to typical gradient descent for logistic/linear regression.\n\nThus far, I\u2019ve walked through a very abstract form of backprop for a simple neural network. However, it is unlikely that you will ever use a single-layered ANN in applications. So, now, let\u2019s make our black boxes \u2014 the activation and error functions \u2014 more concrete such that we can perform backprop on a multilayer neural net.\n\nRecall that our error function J(W) will compute the \u201cerror\u201d of our neural network based on the output predictions it produces vs. the correct a priori outputs we know in our training set. More formally, if we denote our predicted output estimations as vector p, and our actual output as vector a, then we can use:\n\nThis is just one example of a possible cost function (the log-likelihood is also a popular one), and we use it because of its mathematical convenience (this is a notion one will frequently encounter in machine learning): the squared expression exaggerates poor solutions and ensures each discrepancy is positive. It will soon become clear why we multiply the expression by half.\n\nThe derivative of the error w.r.t. the output was the first term in the error w.r.t. weight derivative expression we formulated earlier. Let\u2019s now compute it!\n\nOur result is simply our predictions take away our actual outputs.\n\nNow, let\u2019s move on to the activation function. The activation function used depends on the context of the neural network. If we aren\u2019t in a classification context, ReLU (Rectified Linear Unit, which is zero if input is negative, and the identity function when the input is positive) is commonly used today.\n\nIf we\u2019re in a classification context (that is, predicting on a discrete state with a probability ie. if an email is spam), we can use the sigmoid or tanh (hyperbolic tangent) function such that we can \u201csqueeze\u201d any value into the range 0 to 1. These are used instead of a typical step function because their \u201csmoothness\u201d properties allows for the derivatives to be non-zero. The derivative of the step function before and after the origin is zero. This will pose issues when we try to update our weights (nothing much will happen!).\n\nNow, let\u2019s say we\u2019re in a classification context and we choose to use the sigmoid function, which is of the following equation:\n\nAs per usual, we\u2019ll compute the derivative using differentiation rules as:\n\nEDIT: On the 2nd line, the denominator should be raised to +2, not -2. Thanks to a reader for pointing this out.\n\nSidenote: ReLU activation functions are also commonly used in classification contexts. There are downsides to using the sigmoid function \u2014 particularly the \u201cvanishing gradient\u201d problem \u2014 which you can read more about here.\n\nThe sigmoid function is mathematically convenient (there it is again!) because we can represent its derivative in terms of the output of the function. Isn\u2019t that cool\u203d\n\nWe are now in a good place to perform backpropagation on a multilayer neural network. Let me introduce you to the net we are going to work with:\n\nThis net is still not as complex as one you may use in your programming, but its architecture allows us to nevertheless get a good grasp on backprop. In this net, we have 3 input neurons and one output neuron. There are four layers in total: one input, one output, and two hidden layers. There are 3 neurons in each hidden layer, too (which, by the way, need not be the case). The network is fully connected; there are no missing connections. Each neuron/node (save the inputs, which are usually pre-processed anyways) is an activity; it is the weighted sum of the previous neurons\u2019 activities applied to the sigmoid activation function.\n\nTo perform backprop by hand, we need to introduce the different variables/states at each point (layer-wise) in the neural network:\n\nIt is important to note that every variable you see here is a generalization on the entire layer at that point. For example, when I say x_i, I am referring to the input to any input neuron (arbitrary value of i). I chose to place it in the middle of the layer for visibility purposes, but that does not mean that x_i refers to the middle neuron. I\u2019ll demonstrate and discuss the implications of this later on.\n\nx refers to the input layer, y refers to hidden layer 1, z refers to hidden layer 2, and p refers to the prediction/output layer (which fits in nicely with the notation used in our cost function). If a variable has the subscript i, it means that the variable is the input to the relevant neuron at that layer. If a variable has the subscript j, it means that the variable is the output of the relevant neuron at that layer. For example, x_i refers to any input value we enter into the network. x_j is actually equal to x_i, but this is only because we choose not to use an activation function \u2014 or rather, we use the identity activation function \u2014 in the input layer\u2019s activities. We only include these two separate variables to retain consistency. y_i is the input to any neuron in the first hidden layer; it is the weighted sum of all previous neurons (each neuron in the input layer multiplied by the corresponding connecting weights). y_j is the output of any neuron at the hidden layer, so it is equal to activation_function(y_i) = sigmoid(y_i) = sigmoid(weighted_sum_of_x_j). We can apply the same logic for z and p. Ultimately, p_j is the sigmoid output of p_i and hence is the output of the entire neural network that we pass to the error/cost function.\n\nThe weights are organized into three separate variables: W1, W2, and W3. Each W is a matrix (if you are not comfortable with Linear Algebra, think of a 2D array) of all the weights at the given layer. For example, W1 are the weights that connect the input layer to the hidden layer 1. Wlayer_ij refers to any arbitrary, single weight at a given layer. To get an intuition of ij (which is really i, j), Wlayer_i are all the weights that connect arbitrary neuron i at a given layer to the next layer. Wlayer_ij (adding the j component) is the weight that connects arbitrary neuron i at a given layer to an arbitrary neuron j at the next layer. Essentially, Wlayer is a vector of Wlayer_is, which is a vector of real-valued Wlayer_ijs.\n\nNOTE: Please note that the i\u2019s and j\u2019s in the weights and other variables are completely different. These indices do not correspond in any way. In fact, for x/y/z/p, i and j do not represent tensor indices at all, they simply represent the input and output of a neuron. Wlayer_ij represents an arbitrary weight at an index in a weight matrix, and x_j/y_j/z_j/p_j represent an arbitrary input/output point of a neuron unit.\n\nThat last part about weights was tedious! It\u2019s crucial to understand how we\u2019re separating the neural network here, especially the notion of generalizing on an entire layer, before moving forward.\n\nTo acquire a comprehensive intuition of backpropagation, we\u2019re going to backprop this neural net as discussed before. More specifically, we\u2019re going to find the derivative of the error w.r.t. an arbitrary weight in the input layer (W1_ij). We could find the derivative of the error w.r.t. an arbitrary weight in the first or second hidden layer, but let\u2019s go as far back as we can; the more backprop, the better!\n\nSo, mathematically, we are trying to obtain (to perform our iterative optimization algorithm with):\n\nWe can express this graphically/visually, using the same principles as earlier (chain rule), like so:\n\nIn two layers, we have three red lines pointing in three different directions, instead of just one. This is a reinforcement of (and why it is important to understand) the fact that variable j is just a generalization/represents any point in the layer. So, when we differentiate p_i with respect to the layer before that, there are three different weights, as I hope you can see, in W3_ij that contribute to the value p_i. There also happen to be three weights in W3 in total, but this isn\u2019t the case for the layers before; it is only the case because layer p has one neuron \u2014 the output \u2014 in it. We stop backprop at the input layer and so we just point to the single weight we are looking for.\n\nWonderful! Now let\u2019s work out all this great stuff mathematically. Immediately, we know:\n\nWe have already established the left hand side, so now we just need to use the chain rule to simplify it further. The derivative of the error w.r.t. the weight can be written as the derivative of the error w.r.t. the output prediction multiplied by the derivative of the output prediction w.r.t. the weight. At this point, we\u2019ve traversed one red line back. We know this because\n\nis reducible to a numerical value. Specifically, the derivative of the error w.r.t. the output prediction is:\n\nGoing one more layer backwards, we can determine that:\n\nIn other words, the derivative of the output prediction w.r.t. the weight is the derivative of the output w.r.t. the input to the output layer (p_i) multiplied by the derivative of that value w.r.t. the weight. This represents our second red line. We can solve the first term like so:\n\nThis corresponds with the derivative of the sigmoid function we solved earlier, which was equal to the output multiplied by one minus the output. In this case, p_j is the output of the sigmoid function. Now, we have:\n\nLet\u2019s move on to the third red line(s). This one is interesting because we begin to \u201cspread\u201d out. Since there are multiple different weights that contribute to the value of p_i, we need to take into account their individual \u201cpull\u201d factors into our derivative:\n\nIf you\u2019re a mathematician, this notation may irk you slightly; sorry if that\u2019s the case! In computer science, we tend to stray from the notion of completely legal mathematical expressions. This is yet again again another reason why it\u2019s key to understand the role of layer generalization; z_j here is not just referring to the middle neuron, it\u2019s referring to an arbitrary neuron. The actual value of j in the summation is not changing (it\u2019s not even an index or a value in the first place), and we don\u2019t really consider it. It\u2019s less of a mathematical expression and more of a statement that we will iterate through each generalized neuron z_j and use it. In other words, we iterate over the derivative terms and sum them up using z_1, z_2, and z_3. Before, we could write p_j as any single value because the output layer just contains one node; there is just one p_j. But we see here that this is no longer the case. We have multiple z_j values, and p_i is functionally dependent on each of these z_j values. So, when we traverse from p_j to the preceding layer, we need to consider each contribution from layer z to p_j separately and add them up to create one total contribution. There\u2019s no upper bound to the summation; we just assume that we start at zero and end at our maximum value for the number of neurons in the layer. Please again note that the same changes are not reflected in W1_ij, where j refers to an entirely different thing. Instead, we\u2019re just stating that we will use the different z_j neurons in layer z.\n\nSince p_i is a summation of each weight multiplied by each z_j (weighted sum), if we were to take the derivative of p_i with any arbitrary z_j, the result would be the connecting weight since said weight would be the coefficient of the term (derivative of m*x w.r.t. x is just m):\n\nW3_ij is loosely defined here. ij still refers to any arbitrary weight \u2014 where ij are still separate from the j used in p_i/z_j \u2014 but again, as computer scientists and not mathematicians, we need not be pedantic about the legality and intricacy of expressions; we just need an intuition of what the expressions imply/mean. It\u2019s almost a succinct form of psuedo-code! So, even though this defines an arbitrary weight, we know it means the connecting weight. We can also see this from the diagram: when we walk from p_j to an arbitrary z_j, we walk along the connecting weight. So now, we have:\n\nAt this point, I like to continue playing the \u201creduction test\u201d. The reduction test states that, if we can further simplify a derivative term, we still have more backprop to do. Since we can\u2019t yet quite put the derivative of z_j w.r.t. W1_ij into a numerical term, let\u2019s keep going (and fast-forward a bit). Using chain rule, we follow the fourth line back to determine that:\n\nSince z_j is the sigmoid of z_i, we use the same logic as the previous layer and apply the sigmoid derivative. The derivative of z_i w.r.t. W1_ij, demonstrated by the fifth line(s) back, requires the same idea of \u201cspreading out\u201d and summation of contributions:\n\nBriefly, since z_i is the weighted sum of each y_j in y, we sum over the derivatives which, similar to before, simplifies to the relevant connecting weights in the preceding layer (W2 in this case).\n\nWe\u2019re almost there, let\u2019s go further; there\u2019s still more reduction to do:\n\nWe have, of course, another sigmoid activation function to deal with. This is the sixth red line. Notice, now, that we have just one line remaining. In fact, our last derivative term here passes (or rather, fails) the reduction test! The last line traverses from the input at y_i to x_j, walking along W1_ij. Wait a second \u2014 is this not what we are attempting to backprop to? Yes, it is! Since we are, for the first time, directly deriving y_i w.r.t. the weight W1_ij, we can think of the coefficient of W1_ij as being x_j in our weighted sum (instead of the vice versa as used previously). Hence, the simplification follows:\n\nOf course, since each x_j in layer x contributes to the weighted sum y_i, we sum over the effects. And that\u2019s it! We can\u2019t reduce any further from here. Now, let\u2019s tie all these individual expressions together:\n\nEDIT: The denominator on the left hand side should say dW\u00b9ij instead of \u201clayer\u201d.\n\nWith no more partial derivative terms left, our work is complete! This gives us the derivative of the error w.r.t. any arbitrary weight in the input layer/W1. That was a lot of work \u2014 maybe now we can sympathize with the poor computers!\n\nSomething you should notice is that values such as p_j, a, z_j, y_j, x_j etc. are the values of the network at the different points. But where do they come from? Actually, we would need to perform a feed-forward of the neural network first and then capture these variables.\n\nOur task is to now perform Gradient Descent to train the neural net:\n\nWe perform gradient descent on each weight in each layer. Notice that the resulting gradient should change each time because the weight itself changes, (and as a result, the performance and output of the entire net should change) even if it\u2019s a small perturbation. This means that, at each update, we need to do a feed-forward of the neural net. Not just once before, but once each iteration.\n\nThese are then the steps to train an entire neural network:\n\nIt\u2019s important to note that one must not initialize the weights to zero, similar to what may be done in other machine learning algorithms. If weights are initialized to zero, after each update, the outgoing weights of each neuron will be identical, because the gradients will be identical (which can be proved). Because of this, the proceeding hidden units will remain the same value and will continue to follow each other. Ultimately, this means that our training will become extremely constrained (due to the \u201csymmetry\u201d), and we won\u2019t be able to build interesting functions. Also, neural networks may get stuck at local optima (places where the gradient is zero but are not the global minima), so random weight initialization allows one to hopefully have a chance of circumventing that by starting at many different random values.\n\n3. Perform one feed-forward using the training data\n\n4. Perform backpropagation to get the error derivatives w.r.t. each and every weight in the neural network\n\n5. Perform gradient descent to update each weight by the negative scalar reduction (w.r.t. some learning rate alpha) of the respective error derivative. Increment the number of iterations.\n\n6. If we have converged (in reality, though, we just stop when we have reached the number of maximum iterations) training is complete. Else, repeat starting at step 3.\n\nIf we initialize our weights randomly (and not to zero) and then perform gradient descent with derivatives computed from backpropagation, we should expect to train a neural network in no time! I hope this example brought clarity to how backprop works and the intuition behind it. If you didn\u2019t understand the intricacies of the example but understand and appreciate the concept of backprop as a whole, you\u2019re still in a great place! Next we\u2019ll go ahead and explain backprop code that works on any generalized architecture of a neural network using the ReLU activation function.\n\nNow that we\u2019ve developed the math and intuition behind backpropagation, let\u2019s try to implement it. We\u2019ll divide our implementation into three distinct steps:\n\nLet\u2019s start off by defining what the API we\u2019re implementing looks like. We\u2019ll define our network as a series of Layer instances that our data passes through \u2014 this means that instead of modeling each individual neuron, we group neurons from a single layer together. This makes it a bit easier to reason about larger networks, but also makes the actual computations faster (as we\u2019ll see shortly). Also \u2014 we\u2019re going to write the code in Python.\n\nEach layer will have the following API:\n\nWe can start by implementing the weight initialization. As it turns out, how you initialize your weights is actually kind of a big deal for both network performance and convergence rates. Here\u2019s how we\u2019ll initialize our weights:\n\nThis initializes a weight matrix of the appropriate dimensions with random values sampled from a normal distribution. We then scale it rad(2/self.size_in), giving us a variance of 2/self.size_in (derivation here).\n\nAnd that\u2019s all we need for layer initialization! Let\u2019s move on to implementing our first objective \u2014 feed-forward. This is actually pretty simple \u2014 a dot product of our input activations with the weight matrix, followed by our activation function, will give us the activations we need. The dot product part should make intuitive sense; if it doesn\u2019t, you should sit down and try to work through it on a piece of paper. This is where the performance gains of grouping neurons into layers comes from: instead of keeping an individual weight vector for each neuron, and performing a series of vector dot products, we can just do a single matrix operation (which, thanks to the wonders of modern processors, is significantly faster). In fact, we can compute all of the activations from a layer in just two lines:\n\nSimple enough. Let\u2019s move on to backpropagation.\n\nThis one\u2019s a bit more involved. First, we compute the derivative of the output w.r.t. the weights, then the derivative of the cost w.r.t. the output, followed by chain rule to get the derivative of the cost w.r.t. the weights.\n\nLet\u2019s start with the first part \u2014 the derivative of the output w.r.t. the weights. That should be simple enough; because you\u2019re multiplying the weight by the corresponding input activation, the derivative will just be the corresponding input activation.\n\nExcept, because we\u2019re using the ReLU activation function, the weights have no effect if the corresponding output is < 0 (because it gets capped anyway). This should take care of that hiccup:\n\nLet\u2019s take a brief detour to talk about the out_grad parameter that our backward method gets. Let\u2019s say we have a network with two layers: the first has m neurons, and the second has n. Each of the m neurons produces an activation, and each of the n neurons looks at each of the m activations. The out_grad parameter is an m x n matrix of how each m affects each of the n neurons it feeds into.\n\nNow, we need the derivative of the cost w.r.t. each of the outputs \u2014 which is essentially the out_grad parameter we\u2019re given! We just need to sum up each row of the matrix we\u2019re given, as per the backpropagation formula.\n\nFinally, we end up with something like this:\n\nNow, we need to compute the derivative of our inputs to pass along to the next layer. We can perform a similar chain rule \u2014 derivative of the output w.r.t. the inputs times the derivative of the cost w.r.t. the outputs.\n\nAnd that\u2019s it for the backpropagation step.\n\nThe final step is the weight update. Assuming we\u2019re sticking with gradient descent for this example, this can be a simple one-liner:\n\nTo actually train our network, we take one of our training samples and call forward on each layer consecutively, passing the output of the previous layer as the input of the following layer. We compute dJ, passing that as the out_grad parameter to the last layer\u2019s backward method. We call backward on each of the layers in reverse order, this time passing the output of the further layer as out_grad to the previous layer. Finally, we call update on each of our layers and repeat.\n\nThere\u2019s one last detail that we should include, which is the concept of a bias (akin to that of a constant term in any given equation). Notice that, with our current implementation, the activation of a neuron is determined solely based on the activations of the previous layer. There\u2019s no bias term that can shift the activation up or down independent of the inputs. A bias term isn\u2019t strictly necessary \u2014 in fact, if you train your network as-is, it would probably still work fine. But if you do need a bias term, the code stays almost the same \u2014 the only difference is that you need to add a column of 1s to the incoming activations, and update your weight matrix accordingly, so one of your weights gets treated as a bias term. The only other difference is that, when returning cost_wrt_inputs, you can cut out the first row \u2014 nobody cares about the gradients associated with the bias term because the previous layer has no say in the activation of the bias neuron.\n\nImplementing backpropagation can be kind of tricky, so it\u2019s often a good idea to check your implementation. You can do so by computing the gradient numerically (by literally perturbing the weight and calculating the difference in your cost function) and comparing it to your backpropagation-computed gradient. This gradient check doesn\u2019t need to be run once you\u2019ve verified your implementation, but it could save a lot of time tracking down potential problems with your network.\n\nNowadays, you often don\u2019t even need to implement a neural network on your own, as libraries such as Caffe, Torch, or TensorFlow will have implementations ready to go. That being said, it\u2019s often a good idea to try implementing it on your own to get a better grasp of how everything works under the hood.\n\nIntrigued? Looking to learn more about neural networks? Here are some great online classes to get you started:\n\nStanford\u2019s CS231n. Although it\u2019s technically about convolutional neural networks, the class provides an excellent introduction to and survey of neural networks in general. Class videos, notes, and assignments are all posted here, and if you have the patience for it I would strongly recommend walking through the assignments so you can really get to know what you\u2019re learning.\n\nMIT 6.034. This class, taught by Prof. Patrick Henry Winston, explores many different algorithms and disciplines in Artificial Intelligence. There\u2019s a great lecture on backprop that I actually used as a stepping stone to getting setup writing this article. I also learned genetic algorithms from Prof. Winston \u2014 he\u2019s a great teacher!"
    },
    {
        "url": "https://ayearofai.com/rohan-3-deriving-the-normal-equation-using-matrix-calculus-1a1b16f65dda?source=---------8",
        "title": "Rohan #3: Deriving the Normal Equation using matrix calculus",
        "text": "Before I start exploring new algorithms/practices in Machine Learning & Co., I want to first refine my current knowledge. Part of this involves plugging gaps in content I either failed to understand or research during my prior endeavors in Artificial Intelligence thus far. Today, I will be addressing the Normal Equation in a regression context. I am not going to explore Machine Learning in depth (but you can learn more about it in my previous two posts) so this article assumes basic comprehension of supervised Machine Learning regression algorithms. It also assumes some background to matrix calculus, but an intuition of both calculus and Linear Algebra separately will suffice.\n\nToday, we try to derive and understand this identity/equation:\n\nRecall that a supervised Machine Learning regression algorithm examines a set of data points in a Cartesian instance space and tries to define a model that \u201cfits\u201d said data set with high accuracy. In this context, a model is any linear or non-linear function.\n\nAs you can see, the blue line captures the trend (that is, how the data points move across across the instance space) in this two dimensional, noisy example. The term \u201cresiduals\u201d, primarily used in Statistics and rarely in Machine Learning, may be new for many of you. Residuals, the vertical lines in gray, are the distances between each of the y-coordinates of the data points and their respective predictions on the model. At TV = 0, our model predicted a value of Sales = 6.5 even though the actual value was Sales = 1 (I\u2019m eye-balling this!). Hence, the residual for this instance is 5.5.\n\nWe would intuitively suggest that a smaller residual is preferred over a greater residual, however a model may not (and again, we may not want it to \u2014 overfitting) be able to accommodate a change in its output for one data point and keep its output constant for all other data points. So, instead, we say that we want to calculate the minimum average residual. That is, we want to minimize the value of the summation of each data point\u2019s residual divided by the number of data points. We can denote this as the \u201ccost function\u201d for our model, and the output of said function as the \u201ccost\u201d.\n\nThe final step is to evaluate how we use a residual in the cost function (the principle unit of the cost function). Simply summing over the residuals (which are mere differences/subtractions) is actually not suitable because it may lead to negative values and fails to capture the idea of Cartesian \u201cdistance\u201d (which would cause our final cost to be inaccurate). A valid solution would be wrapping an absolute value over our subtraction expression. Another approach is to square the expression. The latter is generally preferred because of its Mathematical convenience when performing differential Calculus.\n\nOK. Let\u2019s approach this Mathematically. First, we will define our model as:\n\nThis is pretty much y = mx + c but in a high-dimensional space such that x is vector of values. The TV sales scatter plot diagram above is two-dimensional and hence has a single x: TV, however this need not always be the case.\n\nNotice that this is the \u201cdot-product\u201d between the vectors \u0398 and x. We can rewrite this using the conveniences of Linear Algebra notation:\n\n\u0398 stores the real coefficients (or \u201cweights\u201d) of the input features x and is hence of the exact same dimensionality as x. It is important to note that we prefix the vector x with 1 so that we can add support for a constant term in our model.\n\nNow, we can expect to plug in any value x for TV and expect to get an accurate projection for the number of Sales we plan to make. The only issue is, well, we have to figure out the value(s) of \u0398 that form an accurate model. As humans, we may be able to guess a few values but a) this will not apply in higher-dimensional applications b) a computer cannot do \u201cguess-work\u201d c) we want to find the most accurate \u0398.\n\nx represents any single data point we enter to receive a projection. We can now define X as a vector of all the inputs for the the A-Priori data points that we use to shape our model. Note that each data point is a vector in itself, so X is a large matrix (denoted the \u201cdesign matrix\u201d) whereby each row (we use the notation X superscript arbitrary index i) is one vector x. y is, conversely, the outputs for these A-Priori data points. We define m as the number of rows in X, or otherwise the number of A-Priori data points. In Machine Learning this is called a \u201ctraining set\u201d.\n\nI hope it is clear that, here, \u0398 essentially defines the model. Everything else stays constant. So, to find the most accurate model, we need to find the \u0398 with the lowest cost (lowest average residual \u2014 on top of a function \u2014 as stated earlier). Let\u2019s put our cost function in Mathematical notation:\n\nThe residual is simply the difference between the actual value and the predicted value (the input values fed through the model):\n\nEarlier we discussed different ways we could interact with the residual to create a principle cost unit. Squared difference is the preferred one:\n\nInserting this expression in place with the cost function leaves us with:\n\nNotice how we\u2019ve multiplied the overall expression by a half. We do this, as per usual, for Mathematical convenience.\n\nUnfortunately, before we can proceed, we will need to represent the cost function in vectorized (using Linear Algebra) format. Instead of explicitly summing over the square residuals, we can calculate the square residuals with matrix operations.\n\nIf we were to collect all the residuals in one vector, it would look somewhat like this:\n\nWe can further split this up:\n\nAnd bam, we can clearly see that this simplifies to:\n\nBut we\u2019re forgetting something important; each residual is squared. This poses a problem because we cannot simply square the overall expression X\u0398 \u2212 y. Why? Put simply, the square of a vector/matrix is not equal to the square of each of its inner values. So \u2014 what do we actually want to do? We want to take each value in the vector X\u0398 \u2212 y and multiply it by itself. This is what we can formulate:\n\nIf we want to get each value in X\u0398 \u2212 y to multiply by itself, we can use a second X\u0398 \u2212 y and have each value in the first vector multiply by the value at the same index in the second vector. This is a dot-product. Recall that when we formulated our hypothesis/model function, we took the dot-product of two vectors as the multiplication between one of the vectors transposed and the other vector. Hence, to achieve square residuals, we end up with the following expression:\n\nBy adding the division term to the front of the expression, we should have the final Linear Algebra/vectorized expression for our cost function:\n\nSo, now that we\u2019re able to \u201cscore\u201d \u0398, the end goal is simple:\n\nIn other words, find the \u0398 that minimizes on the output of the cost function and hence represents the most accurate model with lowest average residual.\n\nSo, how shall we approach this? I hope it will become clear through this graph:\n\nThat\u2019s right \u2014 in Linear Regression one can prove that Cost(\u0398) is a convex function! So, at this point, it seems pretty simple: apply differential Calculus to solve for the least cost. Let\u2019s get started!\n\nWe are going to differentiate with respect to, as expected, \u0398. At this point, we can drop the division term because when we equate the derivative to zero it will nulled out.\n\nWe are yet again going to be simplifying the cost function (RHS), this time using matrix transpose distribution identity specifically (AB)\u2019 \u2261 B\u2019A\u2019:\n\nEDIT: I make a mistake here and do (AB)\u2019 = A\u2019B\u2019, so I end up having to rearrange (which would otherwise be illegal due to non-commutative nature of matrix multiplication). This does not matter with respect to determining the final derivative.\n\nAt this point, we can expand the two brackets:\n\nSince vectors y and X\u0398 are of the same dimensionality and are both vectors, they (or rather the transpose of one with the other) satisfy the commutative property (which states that a * b = b * a \u2014 if you weren\u2019t aware, this property is not the case for higher-dimensional matrices). Hence, the two middle terms can be collected into one term:\n\nLet\u2019s try to remove all the instances of transpose (again, using the distribution identities):\n\nWe can remove the last term because, when deriving with respect to \u0398, it has no effect and the derivative is zero (since the term \u0398 is not involved):\n\nLet us evaluate the left term first:\n\nNote that the X terms are simply constants/scalars in this partial derivative, so we can \u201ctake it out\u201d (using differentiation rules) like so:\n\nUsing the following matrix calculus scalar-by-vector identity:\n\nEDIT: I want to add intuition for the rule here. Recall that a vector multiplied by its transpose results in a vector where each value is squared \u2014 but this is not the same as the square of the vector itself. If we were to take the derivative of this vector with respect to the original vector, we apply the basic principle which states that we derive by each value in the vector separately:\n\nSo now, using this identity, we can solve the derivative of the left term to be:\n\nLet\u2019s move on. The right term is much simpler to solve:\n\nEDIT: I hope the intuition behind this identity becomes clear using the same principles in the previous explanation.\n\nWe can see that:\n\nNow we can combine the two terms together to formulate the final derivative for the cost function:\n\nRecall that the only turning point \u2014 a point where the instantaneous gradient is zero \u2014 for a convex function will be the global minimum/maximum of the function. So, we set the derivative to zero and hence find said minimum point:\n\nNow, we sub in our computed derivative:\n\nWith some basic algebra, we\u2019re on our way!:\n\nSlow down, ! Since matrices are not commutative, we can\u2019t isolate \u0398 by simply \u201cbringing down\u201d the X terms. Instead, we multiply both sides by the inverse of said terms (remember that division is the same as multiplication of the inverse):\n\nThe inverse of any value multiplied by the value itself is just 1, so\u2026:\n\nHere we have it \u2014 finally! This is our solution for \u0398 where the cost is at its global minimum."
    },
    {
        "url": "https://ayearofai.com/lenny-1-robots-reinforcement-learning-ae6e69ff4cf0?source=---------9",
        "title": "Lenny #1: Robots + Reinforcement Learning \u2013",
        "text": "Before we dive in, let\u2019s get a brief overview of what reinforcement learning is all about. The fundamental goal of a reinforcement learning algorithm is to train some agent (let\u2019s take the example of an AI for some board game) to make the best possible long-term decisions; in this case, what moves do I make that will ultimately result in a victory, even if that means some short-term sacrifices. Every time the agent takes some action, the environment takes it and gives back a new state. So after I make a move, the environment will tell me what the board looks like the next time it\u2019s my turn. The agent then picks the next move, and the cycle repeats until the end of the episode (when one of the players wins). The policy that it follows determines what moves it will make in any given state.\n\nThe ultimate goal is to learn an optimal policy capable of picking the move that is most likely to end in a victory for itself. There\u2019s one more important concept that we left out, and that\u2019s the idea of a reward. Every time the agent takes some action and the environment generates a new state, it also has the option of giving the agent some positive or negative feedback \u2014 in this case, maybe a positive reward if I win and a negative one if I lose. With this, we can give a more formal definition for the objective of an RL algorithm: we want to train a policy such that it makes decisions that maximize the total expected reward. In other words, teach it to take the action that will eventually result in the greatest gain.\n\nI\u2019ll also briefly mention the concept of a value function V(s). The value function answers this simple question: \u201cgiven my current state, if I follow some given policy, how much reward am I expected to receive?\u201d Similarly, the action-value function Q(s, a) answers the question \u201cgiven my current state, if I take the given action and follow some given policy from then on, how much reward am I expected to receive?\u201d If you\u2019re looking for a more intuitive interpretation, you can think of these functions as merely a representation of how \u201cgood\u201d it is to be in a particular state (or to take a particular action in a particular state). We won\u2019t talk about how we actually get these functions just yet, but for now it\u2019s okay to think about it as a black box function that magically gives you the right values.\n\nReinforcement learning has been around for a while, and is behind several of machine learning\u2019s success stories over the years. Back in 1992, TD-Gammon learned to play Backgammon at nearly the level of top human players using RL techniques. More recently, the combination of Monte Carlo tree search, convolutional neural networks, and reinforcement learning achieved unprecedented performance in the game of Go. The same team also developed a single algorithm capable of learning to play dozens of Atari games, a major breakthrough in the development of \u201cuniversal\u201d algorithms that can be applied to a number of different challenges."
    },
    {
        "url": "https://ayearofai.com/rohan-2-time-progress-ae804fa86cca",
        "title": "Rohan #2: Artificial intelligence, \u2202Progress/\u2202Time \u2013",
        "text": "This is sort-of my first blog post into A.Y.O.A.I. even though I posted a lengthy (30 minute read) exploration of applied Logistic Regression a couple of weeks ago. I may do more of these explorations, now that I think about it; they\u2019re a nice documentation of in-depth & holistic comprehension, and they only take a week or two to write. Interestingly enough, my zero\u2019th blog post garnered enough attention to (partly, at least) land me an internship interview at a high profile fin-tech startup in the Bay Area. Pretty cool, right? Unfortunately, since I had to prepare for said interview, I\u2019ve had less time to submerge myself into Artificial Intelligence than expected. Instead, I had to submerge myself into data structures and algorithms \u2014 boring (admittedly, though, some questions were very fun)! Today we look at the history of Artificial Intelligence, where we are at this point in time, and predictions for the long term future. The discussion will not exclusively feature a timeline of events; I\u2019ll also look into the technical details of modern day AI and future strategies/paths/dangers. I feel compelled to write this because, before I delve into such a complex field, I need to understand the history behind it: the problems people faced and why we are where we are now. The content in this blog post is an accumulation of what I\u2019ve learned in the Superintelligence book, Machine Learning book, and Wikipedia entries I\u2019ve been reading. To most (I would think) people, the concept of Artificial Intelligence is most synonymous with representations in popular fiction like Terminator or Robocop. That is, intricate robots that can become sentient and as a result relentlessly pursue existentialism objectives with regards to some (evil, more often than not) selected moral standard. As [aspiring] Computer Scientists and Mathematicians, it is easy to diminish these views and take pride in our more educated approaches like convex optimization. But let\u2019s not forget that AI grew out of myth and speculation; in fact, mechanical/artificial men began appearing from Greek mythology and continued to develop through the Middle Ages until its introduction to formal fiction in the 1800s. Philosophers began to introduce and discuss the concept and implications of Artificial Intelligence in formal reasoning because it imposed interesting conflicts in their theories of topics such as the relationship between mind and body, ethics, personal identity, etc. I recently learned about the ideology of \u201cFunctionalism\u201d \u2014 which states that mental states are defined solely by their function \u2014 in my IB High School Philosophy course. Functionalism is fascinating because it implies that Artificial Intelligence agents legally have minds just like us humans. Many also thought that all human reason could be reduced to some systematic representation. Development of this notion, associated with Mathematical Logic (eg. Lambda Calculus), by Philosophers such as Hobbes and Descartes was the basis for the breakthrough that resulted in many scientists beginning to delve deep into the idea of machines that could think. The first modern, digital computers (such as ENIAC and Colossus) were invented during World War II (for encryption breaking purposes). These computers were developed on the theoretical basis laid by Alan Turing in World War I and John Von Neumann. With this new technology in hand, Artificial Intelligence was officially founded as an academic discipline in 1956 by domain experts in a plethora of fields such as Math, Psychology, Economics, Philosophy, etc. Algorithms modelled after biological systems were, interestingly enough, one of the first distinct groups of AI to be introduced in the field. After extensive research in neurology demonstrated our brains were an electrical network of neurons firing signals, many hypothesized that, according to Alan Turing\u2019s theory of computation, such a data structure and system could be represented digitally. Researchers Walter Pitts and Warren McCulloch explored how artificial neurons could work together to perform simple logical functions such as OR, AND, etc. Mimicking the output of the logic gate XOR (logical complement of OR) posed challenges with single layers of neurons, and researchers would much later pioneer a data structure with multiple layers neurons \u2014 coined \u201cNeural Networks\u201d (which I\u2019m sure many of you have heard of). The first neural network machine, the SNARC, was designed and built by Marvin Minsky, who is now recognized to be a leader in the field of AI. Of course, this invention was not as revolutionary as contemporary Neural Networks because a) the effective backpropagation training algorithm had not been discovered yet and b) because there was a lack of computation power to do something meaningful like image recognition. Sidenote: Was going over my draft and just found out that Marvin Minsky passed away last month on January 24 at 88 years old. R.I.P. Marvin Minsky and thank you for your contribution to this wonderful discipline.\n\nMeanwhile, Alan Turing devised a Philosophic thought experiment famously denoted as the \u201cTuring Test\u201d. As progress in AI was accelerating, Turing suggested that a \u201cthinking machine\u201d could be determined if it convinced another subject, through an online chatting mechanism, it was human. Throughout history, game AI has been an underlying indicator of progression in AI. The first AI to beat a human in checkers (by Christopher Strachey in 1951 \u2014 in this timeline) was perhaps one of the first tangible feats of achievement in the discipline. The most recent, tangible feat in AI was Google\u2019s Deep Learning (deep neural networks) project that won against a Chinese professional player in the infamously complex game of Go. If one were to ask the question \u2014 which institution made the greatest innovation in early Artificial Intelligence? The typical response would most likely be MIT or Stanford. Perhaps those are true of the modern day, but for early AI, the answer is undeniably Dartmouth. In the summer of 1956, a group of scientists sharing an interest in the study of Artificial Intelligence through concepts like biological representations of intelligent algorithms and Automata Theory joined together in a six-week workshop/conference at Dartmouth College (formally known as the Dartmouth Summer Research Project on Artificial Intelligence). The conference is renown (perhaps also due to its convenient timing) to be the birth of modern day AI research. Many of the participants (most notably including Marvin Minsky) would later be recognized as the founding fathers of the field. The following is the proposal of the event by organizer John McCarthy: \u201cWe propose that a 2 month, 10 man study of artificial intelligence be carried out during the summer of 1956 at Dartmouth College in Hanover, New Hampshire. The study is to proceed on the basis of the conjecture that every aspect of learning or any other feature of intelligence can in principle be so precisely described that a machine can be made to simulate it. An attempt will be made to find how to make machines use language, form abstractions and concepts, solve kinds of problems now reserved for humans, and improve themselves. We think that a significant advance can be made in one or more of these problems if a carefully selected group of scientists work on it together for a summer.\u201d \u2014 McCarthy et al. 1955 Advancements in Natural Language Processing, Neural Networks, Abstraction, and/or Self-Improvement were either made or inspired due to the conference. The work was mainly focused on proving skeptics incorrect (perhaps this was the most effective way of fostering excitement in the field) with regards to claims that machines were unable to perform various arbitrary tasks such as the ability to apply logical deduction to abstract problems. The Logic Theorist \u2014 a computer program that formed elegant proofs for many theorems presented in the book Principia Mathematica (the foundation of Mathematics) \u2014 did exactly that. Five of the Dartmouth conference members, reunited in 2006 \u2014 http://www.dartmouth.edu/~vox/0607/0724/images/ai50.jpg It was a bullish era for AI \u2014 http://i.telegraph.co.uk/multimedia/archive/02423/bull_2423808b.jpg The years from 1956 to 1974 were largely considered to be the \u201cgolden years\u201d of AI. After the Dartmouth conference, many new inventions and discoveries were made such as: the General Problem Solver (which could solve a wide range of formal Mathematical problems), Calculus problem solvers, Algebra problem solvers, etc. The ELIZA program made advancements in (and was one of the first implementations of) Natural Language Processing that enabled a user to communicate and obtain advice from an AI psychotherapist using semantic nets. In the year 1968, the SHRDLU program made further developments in NLP as it displayed a simulated robotic arm in a virtual, 3D, micro-world (powered by Physics engines) that was able to decode and follow instructions typed in by users in natural language such as moving and stacking blocks. SHRDLU. Do the graphics not look as cool as you had envisioned? Well it was the 1970s! \u2014 http://hci.stanford.edu/winograd/shrdlu/shrdlu-original.gif In the coming decades, accomplishments in the industry only heightened. Programs that could compose music, create art, perform clinical tasks, drive cars autonomously, and even make original jokes! \u201cWhat do you get when you cross an optic with a mental object? An eye-dea!\u201d Many more innovations in NLP, logical reasoning, and micro-worlds were conceived in the era. Optimism was abundant; Marvin Minsky and many other leaders conjectured that machines with general intelligence of an average human being would be successfully developed by 1978. From the 1960s-19670s, a public sector organization called DARPA awarded MIT a $5 million dollars in grants to fund many more of these scientific breakthroughs. Similar grants were awarded to colleges like CMU and Stanford.\n\nProving a theorem that has a prospective 10 line proof? Simple \u2014 it takes some negligible time \u03b1. Proving a theorem that has a prospective 50 line proof? Would it take equally negligible time 5\u03b1? No; it would take 5\u2075\u2070 possible sequences (thank you Nick Bostrom for the reference). This is because many of these \u201cimpressive\u201d AI programs used the same basic algorithm \u2014 that is a simple search algorithm (which was by no means simple back then) which iterated through all possible solutions until a successful one was found. As the search space increased, the number of possible paths astronomically increased \u2014 this is known as a combinatorial explosion. Combinatorial explosion and poor scalability of AI tasks in general was a key contributing factor to a downfall of the field of AI in general from 1974\u20131980. These \u201ctoy\u201d solutions grouped with: Limited computational power. This was especially a problem with neural nets as mentioned before \u2014 there was simply not enough memory and/or processing power to achieve meaningful results in a reasonable time. Single-layered perceptrons (a type of neural network) were too rudimentary to perform logical tasks, but the answer \u2014 multi-layered perceptrons \u2014 was too computationally expensive. In addition, there was no effective way to train a neural network \u2014 this would come later. A lack of growth in Computer Vision and robotics research. This is correlated with limited computational power, but dead-end approaches in general were common too. It was almost a paradox \u2014 highly complex tasks like formulating proofs to Mathematical theorems was easy for a computer but difficult for a human. Yet something so seamless for us such as recognizing objects and patterns was near impossible for machines. This sparked an underlying belief that perhaps the \u201cArtificial Intelligence\u201d that had been developed was not closely modelled to our human intelligence enough and was really just taking advantage of the greater computational and numerical arithmetic abilities (which were proving eventual limiting factors in any case). Scarcity of centralized/collective data and information to train or supply algorithms eg. NLP with. Remember Google let alone the Internet was neither mainstream nor available to the general public at the time. Poor abstraction. Many AI specialists, when creating solutions to problems, discovered that their solutions were did not generalize on the program domain \u2014 making small changes involved drastically changing the logic used. If one were to differentiate an arbitrary function of AI progress with respect to time (the inspiration for this title), one were to find that at the beginning of the winter, we had reached a gradient of 0. We had hit a local maximum, and research was beginning to decline. DARPA became frustrated with the lack of innovation and cut off funding/ended grants from almost all of their AI research investments; DARPA rescinded $3 million dollars from the Speech Understanding Research program at CMU. By the mid 1970s, obtaining funding for AI research projects was becoming increasingly difficult, and eventually near impossible. Skepticism for the field increased and AI fell out of fashion. Many contemporary Philosophers refuted the possibility of machine intelligence (in terms of semantics). John Searle most notably did so in 1980 with his \u201cChinese Room Experiment\u201d. The Chinese Room Experiment demonstrates that, even though a computer can convey intelligence (or potential sentience in the future), it actually possesses none. Suppose a room with two holes on either end \u2014 one which allows the input of English messages, and one which outputs the Chinese translation of said message. In the middle is a room where the translation, achieved by a man (or intelligent agent), occurs. If you haven\u2019t noticed already, this ties in nicely with the traditional computer architecture of (Input \u2192 Processing \u2192 Output). Although the man comprehends neither English nor Chinese, he has a book that allows him to \u201cmap\u201d each character/word of English to each respective character of Chinese. Thus, on the outside, it seems as if the man is fluent in both Chinese and English. Yet he is not. This is Searle\u2019s argument against Artificial Intelligence; although computers can portray signs of intelligence, they do not have an inherent understanding of\u2026 really anything. They simply follow instructions, and can be reduced to a series of states of the presence or absence of current. Sidenote: Another compelling theory to look at is Godel\u2019s incompleteness theorem. Philosopher John Lucas argued that this theory demonstrated a formal system could never evaluate the truth of certain statements that humans can.\n\nAt this point I feel mentally exhausted knowing I\u2019ve spent 5 hours writing what is at most a third of my entire article. So here\u2019s what you need to know: from 1980\u20131987 another boom occured \u2014 just like that, it was summer time again! Japan launched its aggressively (just shy of $1 billion USD) funded Fifth-Generation Computer Systems project, a public-private partnership that focused on advancing computer architecture to, as a result, enable innovation in AI algorithms of high space and time complexity. Many other countries such in the Europe and the U.S. followed in hopes to simulate the economic success Japan experienced throughout the years. Following the failures from the previous AI winter, many researchers decided to focus on centralizing knowledge/information so that these intelligent agents could become informed in their reasoning. Again, remember, the mainstream Internet and online search succeeded this era by over 10 years. And as a result, the new direction for AI was centralized around knowledge: expert systems. An expert system is a series/combination of logical rules layered on top of each other to determine some output based on an input. These rules were usually devised by domain experts. A common use case for expert systems were to determine the possible illnesses based on a collection of symptoms the user inputs. A diagram representation of expert systems. It really is surprising how this was ever considered AI! \u2014 http://www.igcseict.info/theory/7_2/expert/files/stacks_image_5738.png Expert systems, however, were really the epitome of \u201cdumb\u201d AI \u2014 each expert system was effectively a database that had to be coded by hand and was not self-adaptable to any extent. It was time consuming and expensive to develop, validate, and maintain \u2014 not to mention the impracticality of requiring a standalone computer to just run a single program. By the late 1980s, this growth cycle had too run its tiring course. It was clear that expert systems were a dead-end \u2014 how could these\u2026 databases ever be further developed to create machines which could think on their own? Fortunately, in 1982, physicist John Hopfield devised a form of neural network (denoted as the \u201cHopfield net\u201d) that pushed the field forward. Hopfield nets could learn and process information in an entirely new way \u2014 and they contributed to modern day Artificial Neural Networks. Their outputs could be rewired into their inputs so they are a form of Recurrent Neural Networks (RNNs). At the same time, David Rumelhart evangelized the \u201cbackpropagation\u201d algorithm discovered by Paul Werbos two years prior. The backpropagation algorithm exploited differential Calculus to identify how much each weight in a neural network contributes to the overall error. With the ability to pinpoint the accuracy of each individual weight, modifying the entire system to move in a more optimal direction could be immediately clear. A Hopfield net with four nodes. Does it look familiar? \u2014https://upload.wikimedia.org/wikipedia/commons/9/95/Hopfield-net.png Sidenote: The backpropagation algorithm is awesome, and I\u2019m really excited to delve into the Mathematics behind it in a couple of days! Connectionism and algorithms modelled after the brain experienced a surge in popularity. They experienced commercial success in the 1990s where they were used for programs like optical character recognition and speech recognition. If all these terms (ANNs, RNNs, Hopfield nets, backpropagation, etc.) seem similar to you, it\u2019s because these inventions are still widely prominent in AI today, classed under the umbrella of \u201cMachine Learning\u201d. Deep Learning (which, as previously mentioned, is just extremely deep layered neural networks) is perhaps the AI-craze of the 21st century. So it should be surprising to find out that AI experienced a second winter from 1987\u20131993. Advances in Connectionism are still appreciated today, but it was time for expert systems to retire. Ultimately, The Fifth-Generation Computer Systems (alongside the Western counterparts) failed to meet its objectives (which was actually primarily centralized around NLP). As history repeated itself a second time, skepticism in AI was perhaps at its global maximum. Artificial Intelligence, the field where \u201cultimate failure almost always followed ultimate success\u201d, became shunned by investors and academics. It is important to note, however, that the field continued to make advances despite heavy criticism. That is why, when the winter inevitably thawed in 1993, it has remained in a state of California-esque summer until today, February 2016 and beyond. Sidenote: At the end of the 1980s, Nouvelle AI \u2014 the idea that Artificial Intelligence should possess a physical body, achieved through robotics \u2014 became increasingly popular amongst researchers. I am unsure the extent to which this has developed in the present day, but I would love to hear from anyone who is knowledgeable on this topic. And so, from 1993 onwards, the field has thrived in overcoming G.O.F.A.I. (Good Old Fashioned Artificial Intelligence) \u2014 or the \u201csymbolic Artificial Intelligence\u201d that we have described previously. Examples of where the field has experienced great success include neural networks, statistical modelling, probabilistic modelling, clustering, and genetic algorithms (those which can evolve a set of candidate solutions based on a digital implementation of biological natural selection). These alternative, more Mathematical paradigms rekindled optimism and excitement in the early 1990s. Century old Artificial Intelligence goals were finally achieved. There definitely seemed to be a greater focus on creating models for data through systems using geometric model related approaches. The different ways of achieving these optimal models include convex optimization, evolutionary algorithms, pattern/clustering recognition, etc. These algorithms tie together a plethora of different disciplines, most notably including Mathematics, Economics, and Psychology to make it a much more \u201cscientific\u201d discipline. Other practices such as NLP are also being worked on by many, but experience less innovation (perhaps because perfect natural language is considered to be an \u201cAI-Complete\u201d problem \u2014 that is a problem whose solution is essentially the equivalent of a human-level general intelligence / Strong AI). Companies like Google, Baidu, and Facebook have opened Artificial Intelligence research centers just for the purpose of pushing the field forward \u2014 they open source and publish (presumably) all of their findings. Artificial Intelligence is now recognized to be one of the \u201chottest\u201d fields of Computer Science to be in during college and in career. In this way, there is great emphasis on machines that can \u201clearn\u201d from past experiences and data it is provided (and, on that note, there is a great volume of information due to the Internet). This training, especially in high dimensions, is very computationally and memory expensive but, due to advancements in hardware, we are at a point where it no longer matters. We have Moore\u2019s Law to thank for this. Moore\u2019s Law is the observation that the number of transistors (and hence computational power) that can fit into a dense, integrated circuit has doubled approximately every two years. The observation was made by Gordon Moore, the co-founder of Intel. We can postulate that Computational Power = a \u22c5 2^(t/2) where a is the initial computational power and t is the number of years after the first existing transistor circuit. Compared to the mid 1970s, we in 2016 have at best a greater computational power of 2\u00b2\u2070! Thus, availability of data/information and limited computer power was no longer an issue. On top of that, heuristic-based search that exploited prior knowledge and the target domain solved the combinatorial explosion that search algorithms faced (this solution became more prevalent as search spaces increased eg. with mapping software). In addition, with research ongoing not only in academia but also at top Silicon Valley & tech companies, Computer Vision (eg. with Google\u2019s self driving cars) experienced rapid development, especially through exploitation of the clustering, segmentation, and neural network algorithms that had been unlocked as the other limiting factors diminished. It\u2019s remarkably difficult to name and describe examples of breakthrough Artificial Intelligence projects in the last 20 years because\u2026 there\u2019s so damn much! In this decade, there appears to be a new revolution every week. But, as mentioned before, advancements in game AI is a decent, general indicator on the progress of AI, and so I will discuss the new inventions with regards to this scope. In 1997, Deep Blue became the first AI to beat the world champion (Garry Kasparov) in a game of Chess. This is a classic example of \u201cWeak AI\u201d \u2014 that is AI that matches or exceeds human level intelligence but only in one (or one in the potential millions) domain. By contrast, Strong AI matches or exceeds human intelligence in general (in majority of domains). In 2005, a Stanford robot won the DARPA Grand Challenge by driving autonomously for 131 miles on a desert track without any prior rehearsal on said track. Just a couple years later, CMU won a similar challenge by successfully driving an autonomous car through an urban city environment without any crashes or accidents. These efforts inspired companies like Google, Tesla, and perhaps Apple to develop commercial self-driving cars. In 2011, IBM challenged two Jeapordy! (a popular trivia/quiz game show) champions to its universal AI system Watson and successfully won by a significant margin. Such defeats (or ties) against top contenders extended to games such as Checkers, Backgammon, Othello, Crosswords, Scrabble, Bridge, Poker, FreeCell, etc. In March of this year (2016), Google\u2019s Deep Learning system will challenge the world\u2019s #1 champion in a game of Go (which, as mentioned previously, is remarkably complex). These feats were not due to abrupt revolutions or constant paradigm shifts, but rather involved taking some existing algorithm eg. Neural Networks and applying tedious application of engineering skill by iteratively improving on both the efficiency and accuracy through new techniques (like Dropout, Convolutions, or Simulated Annealing) or ruthless feature tuning. Many of these new applications are published and digested/built-on by the AI community. This is Google\u2019s paper on using Convolutional Neural Networks to power their Go AI. Sidenote: I know very little about Convolutional Neural Networks but I think now\u2019s the time to delve in deep :) Thanks Lenny Khazan for introducing me to Stanford\u2019s course on the topic.\n\nA major field that was introduced only in the present era is Reinforcement Learning; Reinforcement Learning only became widely accepted during the 1990s. RL borrows theory from Economics and heuristic-based search to introduce \u201cintelligent agents\u201d into a universe to independently learn and perform some action. An intelligent agent is a system that can perceive its surrounding environment as a quantitative/symbolic state and take some action to maximize its likelihood of future success on some arbitrary goal. For taking a step in the optimal direction of the goal, these agents receive artificial rewards. In some cases these intelligent agents need to tradeoff short term rewards for long term utility (where Economics plays a role). Over time, these intelligent agents mold a certain optimal behavior that they can employ in the future. Although Reinforcement Learning is used heavily in games and robotics, the study of intelligent agents is very meta; it is linked to us humans (and all other animals) in our sandbox (of the Earth or perhaps the physical universe). It really can describe all kinds of intelligence. The following is an example of an intelligent Mario avatar agent that, through reinforcement learning, learned how to interact with its environment (ground, obstacles, coins, etc.) with regards to its actions (jump, move forwards, move backwards) and some goal state (completing the level / completing as much of the map as possible). Perhaps the most important thing to understand about modern day Artificial Intelligence, however, is that AI development is no longer just for the sake of AI development (or some future goal ie. superintelligence), but instead is also for implementation in industry. That is not to say the two are mutually exclusive, but the latter is definitely more prominent than the former. In fact, this is exactly why Google, Baidu, Facebook, etc. have made efforts in AI research as mentioned previously. Not only do they want to pioneer the future (many argue that machine intelligence is the next \u201cbig thing\u201d on an existential level) but they also, importantly, want to employ these innovations into their consumer and enterprise offerings for competitive advantage. Facebook uses Artificial Intelligence algorithms to understand what content you like the most and hence would like to see on the top of your newsfeed. Google uses Artificial Intelligence for a personalized search experience, and also most notably for their self-driving cars which, alike Tesla, are speculated to be mainstream in just 2\u20135 years! Big data analysis, speech recognition, banking software, medical diagnosis, facial recognition, etc. are all being executed on the consumer and enterprise level to make products more intelligent (hence convenient \u2014 for example Siri) and individualized (for example online advertisements). Artificial Intelligence is also used for safety on a military level, fraud prevention level, extracting some qualitative intent from messages/phone calls/search logs, etc. Powerful tools like TensorFlow are freely available to any developer wanting to introduce sophisticated AI into their own products. A lot of cutting edge Artificial Intelligence is no longer labelled as AI because it has become so useful and common in general applications. In other words \u2014 we take this technology for granted! And perhaps this is why Artificial Intelligence is operating at such a rapid pace. The volume of money, engineering talent, resources/information/data (this one is key), and public support going into research projects is much greater in the commercial sector vs. in academia. Add the incentive of many hundred billion dollar companies and such results are only inevitable. Just approximately 15 lines of code to implement a state of the art AI algorithm. Woah!\u2014 https://www.tensorflow.org/versions/0.6.0/get_started/index.html Let\u2019s switch gears for a minute and talk about Machine Learning (and as a result, Deep Learning). Machine Learning is perhaps the most popular modern day subset of Artificial Intelligence, and the one most commonly found in industrial applications. There is no set definition for ML, but Tom Mitchell from CMU defines it very rigorously as: A computer program that is said to learn from experience E, with respect to some task T, and some performance measure P, if its performance on T as measured by P improves with experience E. Let\u2019s take an example where task T is to classify an email as spam or not spam. Imagine a \u201ctraining set\u201d where you have a collection of emails which are known and labelled as either spam or not spam, and you then ask the machine to guess (using a model it has devised through its training \u2014 more on this later) the state of each email. For each email it classifies incorrectly we decrement the performance measure or increment the error measure. Using this knowledge regarding its current performance, the machine can make iterative adjustments to its model (in some direction that can be solved using Calculus) and re-evaluate itself \u2014 we call this the experience E. The cycle repeats itself until the Machine Learning program can correctly classify almost all emails. Machine Learning is very interesting because it explores algorithms that continuously learn from data to build complex functions that model data. It is a field of study that gives computers the ability to learn without being explicitly programmed. In many ways, it is the opposite of expert systems. Instead of having to hard-code logical rules into a system, Machine Learning programs can examine data and (indirectly) devise its own logical rules through a Mathematical function usually in some geometric space. This performance measure P and experience E are both Mathematical concepts (average squared error summation and partial differentiation respectively), most specifically associated with statistics and convex optimization. Machine Learning was really the landmark of the 1990s boom because it demonstrated a new kind of intelligence \u2014 an adaptive intelligence \u2014 that was closer and closer to emulating humans. It took concepts in Connectionism too and further developed on them. It was clear that Machine Learning could automate (with high efficiency, high accuracy, and with novel solutions) many mundane tasks and in the process both introduce a new era of convenience and emphasis on speciality occupations whilst displacing many blue-collar jobs. Some of the most popular Machine Learning algorithms include Bayesian networks, Artificial Neural Networks, Linear Regression, Logistic Regression, Reinforcement Learning (which we looked at earlier in this section), Support Vector Machines, Clustering, Genetic Algorithms, Decision Tree Learning, etc. You\u2019ll notice that these are almost identical to those algorithms I introduced in the beginning of this section. This is because Machine Learning really is the focal point of Artificial Intelligence. Machines that can study data and learn from it \u2014 http://assets.toptal.io/uploads/blog/image/443/toptal-blog-image-1407508081138.png Let\u2019s talk about Machine Learning in great depth. I particularly want to talk about: Features: these are the domains of the data that you are trying to model and eventually use to make a distinct prediction. For example, in a program that will predict the price of a house based on its different factors, the features could be: size of the house, location of the house, age of the house, etc. n corresponds to the number of features we are using. If n > 1, we are in a multi-variable setting (which is almost always for industrial applications). Output: the output is the established (or predicted) contextual result that is correlated to the features. For example \u2014 the price of the house. This purely depends upon its features (considering the features selected are relevant). The output is generally denoted as y. Training example: A training example is one single record of data (group of features + output) that has been labelled by humans and hence is \u201ccorrect\u201d. For example, if we went online and found a house that was $5 million dollars to purchase with a size of 4000 square feet, 20 years old, and located in Palo Alto, CA this would be a single training example. Feature vector: each training example has a vector of the values for the corresponding features, along with its correct/established output (price). Training set: a training set, as discussed before, is then a collection of training examples which have been selected for the Machine Learning algorithm to learn from. Generally, a larger training set will translate to greater results (typically millions of training examples). This data can be found online or collected by academic institutions/corporations. Our training set is usually denoted as X, with each example denoted as x. The number of training examples in our training set is denoted as m. Trend: the trend of the data is the general correlation between the input and the output. One must look at the overarching \u201cdirection\u201d the data points move in and not scrutinize the individual, minute, and intricate offsets. Noise: noise is either a) any data that falls out of the traditional \u201ctrend\u201d or, more leniently, b) the small deviation (offset) that each data point has from the trend (note: almost every point will have this). In a large training set, you are guaranteed to have noise, but it is important to understand that noise is stochastic due to the chaotic, physical universe we live in. We do not want to accustom for all the noise, because this prevents us from extrapolating the trend. This is an example of noisy data (almost each data point has some deviation from the fitted line) that still has a clear trend \u2014 https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Linear_regression.svg/400px-Linear_regression.svg.png Model I: a model is something that takes data points in some (multi-dimensional) input space and generally maps them to a single real value. Neural networks are different here in that they can output multiple values representing the probabilities of different \u201cevents\u201d/\u201cstates\u201d. So, in this case, each of the data points in the input space corresponds to the feature vector, and the output is the price. A training set without a model (though it looks like we want a polynomial here); the x-axis represents the input/features: size of the house, y-axis represents the output: price \u2014 class.coursera.org/ml-006 Model II: as you can see, we need \u201ccarve\u201d our model into fitting the data/training set above \u2014 or quantitatively find/establish the trend. In other words, the model is the trend. But what defines a model from its standard nature (ie. y = 0, y = sin(x), y = log(x))? The coefficients do. Hence, one must select optimal coefficients that makes an accurate model. Assuming we\u2019ve decided to opt for a linear fit: y = mx + c, we need to select these parameters m and c suitably (x in this case represents the value for a single feature, and y hence represents the output/price). Imagine \u201crebranding\u201d these parameters to \u03f4o and \u03f41 \u2014 we can now say, with brevity, that we want to select the optimum \u03f4 parameter/weight vector. An example of the different models (polynomial vs. linear) and the selected coefficients that make this model accurate \u2014 class.coursera.org/ml-006 We can now predict the price of the house if the size is 1925 square meters. This isn\u2019t in our dataset but our informed model can now tell us the predicted value. Weight/Parameters: these are equivalent to coefficients in the model. But they say something interesting; they tell us how \u201cimportant\u201d each feature is. If the weight of a feature is zero, said feature clearly does not contribute to the output at all. Cost Function: The cost function is the same as the performance measure (or, perhaps, is the inverse of it). The cost function measures the magnitude of error the model \u2014 or the parameters (because these define the model) for that matter \u2014 makes on the training set. Generally, it is the summation of the differences between each prediction on some input and each established output on the same input. We can square each of these individual/principle errors to achieve an absolute value, a convenient differentiated expression, an exaggeration of very poor solutions (so we can skip past them quickly), etc. Generally, we take the average over the entire training set so one anomaly does not skew the results. The following is an example of a Linear Regression cost function denoted as J(\u03f4): Optimization: Now that we have a function that can \u201cscore\u201d each set of parameters, we want to find the parameters with the \u201cbest score\u201d or otherwise the lowest cost. If we were to plot how the overall cost changes with respect to changes in each individual weight, you will find that it is a convex function! There are no local minimas in the case of linear regression, but there may very well be in Neural Networks (which, I\u2019ll soon explain, may be an issue). Our goal, then, is to find that deep blue point which has the lowest y-value/cost. To do this, there are two distinct methods: a) Analytical solution: we can simply take the partial derivative of the cost function with respect to each individual weight and set this expression to 0. If there are local minimas we need to iterate through the solutions of this equation to extract the global minimum. We take the tangent to said point and find the normal to it to get the optimum value for each parameter. Set the partial derivative to zero and hence find the \u201cflat\u201d points \u2014 the global minimum will be one of them If we solve this (considering we are in a multi-variable setting) and then get the normal, we actually end up with this general equation (denoted the \u201cNormal Equation\u201d): In this case beta correspods to our theta \u2014 https://upload.wikimedia.org/math/2/c/e/2ce21b8e24ea7509a3295c3acd2ae0ea.png Ew! Is that a matrix inversion\u203d Taking the inversion of such a large matrix (remember \u2014 millions of training examples) requires a colossal volume of operations. In specific, the most efficient matrix inversion program known runs in O(n^3) time, meaning that as the number and size (this one especially) of training examples increases, the time taken to execute grows in cubic time. b) Iterative solution: performing such complex calculations on a large scale is simply infeasible. So, we turn to Gradient Descent. Gradient Descent simply initializes the weights to some random value, takes the partial derivative with respect to each weight, and updates each weight by a scalar reduction of this value. After doing this hundreds or hundreds of thousands of times, the program will \u201cconverge\u201d at a flat point. Let\u2019s just hope it isn\u2019t a local minima! Generalization: this is perhaps the most fundamental concept in Machine Learning. We ideally want a Machine Learning program to examine the data and create a model that generalizes on said data (or, otherwise, find the trend). The better we generalize, the better predictions we will make in the future. We may fail to generalize on a dataset if we choose to factor in all the noise: The model on the right attempts to intersect each individual point, and so attains a high accuracy measure, but fails to actually understand extrapolate a trend. Future predictions will be poor. \u2014 http://www.experian.com/blogs/marketing-forward/wp-content/uploads/2013/05/model_overfitting_image2.jpg \u201cOverfitted\u201d models will attain extremely high accuracy measures and so a Machine Learning model with eg. access to high order polynomials may objectively decide to overfit. There are many different ways to prevent this, most notably including regularization (where we penalize complex models in our cost function) and cross-validation (where we simulate real world predictions and evaluate their accuracy). Almost everything I\u2019ve mentioned here is inherently statistical/Mathematical. This is the nature of Machine Learning."
    },
    {
        "url": "https://ayearofai.com/rohan-1-when-would-i-even-use-a-quadratic-equation-in-the-real-world-13f379edab3b",
        "title": "Rohan #1: Logistic regression case study \u2014 diagnosing cancer",
        "text": "My inspiration for this investigation evolved out of an avid passion for Computer Science. As a programmer for over 5 years, I was eager to explore the workings of modern Artificial Intelligence used in contexts ranging from handwriting recognition to self driving cars. After successfully completing an online course offered by Stanford on the topic, I was delighted by the sophisticated Mathematical content involved and wanted to expand on these findings in my HL Math IA.\n\nMachine Learning, a specific subset of Artificial Intelligence, is a stellar exemplar of Applied Mathematics in the every day world. Machine Learning algorithms (a set of instructions executed by a computer) are able to learn from large sets of data and generalize on new examples; when we log into social media sites such as Facebook, Machine Learning finds the content it thinks will interest us the most based on previous activity.. These very algorithms are also responsible for keeping our emails\u2019 inbox spam-free. The use cases extend to credit card fraud detection, character recognition, product recommendations, etc. The real-world nature of these problems are captivating to me since it not only enables me to appreciate Math\u2019s link to Computer Science, but also the prominence of Math in our daily lives.\n\nCancer is one of the leading deaths in the world, claiming 600,000 lives in the US alone just last year. My main goal for this investigation is to explore the Math behind Machine Learning with respect to classifying breast cancer tumors as either malignant (being cancerous) or benign (being non-cancerous) based on a given patient\u2019s symptoms and attributes. Not only is this a classic problem used when teaching Artificial Intelligence, but its altruistic sentiment in contributing to a tragic issue that impacts so many lives leads me to select it. I will code a program in Octave (a coding tool) to visualize the results of the algorithm.\n\nII. What is Machine Learning and Artificial Intelligence?\n\nIn general, Artificial Intelligence is any form of artificial system, i.e. computer system, that can mimic human intelligence in some way. Machine Learning is a type of Artificial Intelligence that provides computers with the ability to learn without being explicitly programmed, allowing them to predict on new data. Relating back to the cancer context, a computer program can examine the different existing medical data of patients\u2019 attributes, eg. their tumor size, tumor radius, age, etc. paired with the status of having a malignant or benign tumor. The algorithm will then discover patterns in the data, ie. identifying unimportant attributes, important attributes, and thresholds and combinations of attributes that influence outcomes. This will enable the computer to accurately predict whether a new patient has a malignant or benign tumor based on their attributes.\n\nMachine Learning algorithms work by learning from a training set, which consists of a matrix X and vector y. X is a collection of the all the numeric attributes or \u201cfeatures\u201d eg. the tumor size and radius. Each column in X represents a single feature, and each row in that column is the value of that feature for a specific instance ie. patient. Each row\u2019s/instance\u2019s features put together can be called a \u201cfeature vector\u201d. y is the class of the data points, either 0 or 1 (false/true) or benign vs. malignant otherwise. Each feature vector paired with its class is known as an instance or training example.\n\nThe above is an example of a small training set, where patient 0 with a tumor size of tumorSize1 and radius of tumorRadius1 has a malignant tumor, whilst patient 1 with a size of tumorSize1 and radius tumorRadius 1 has a benign tumor. The feature vector x shows how we can represent only patient 0\u2019s data. We can denote tumorSize_0 and tumorRadius_0 or any values inside a feature vector with a subscript on x, i.e. x_0, x_1, x_2\u2026. x_n. This notation will provide a backing for an algorithm that I will introduce later on.\n\nFigure 1. shows how two features, a patient\u2019s tumor size and tumor radius, may hypothetically relate to the status of a tumor. A cross represents a malignant tumor \u2014 1 \u2014 and a circle represents a benign tumor \u2014 0 \u2014 . The data looks to be separated into two distinct groups. Now, the task for Machine Learning is to identify patterns in the data, and Figure 2. shows what a typical \u201cclassification\u201d algorithm may do; it may formulate a line that separates the malignant from benign tumors, referred to as a \u201cdecision boundary\u201d. Now, any point plotted to the right of the decision boundary will be considered malignant, and to the left benign.\n\nFigure 3. is a different type of Machine Learning problem where we plot X against y and want to predict any real value of y in terms of X. In this case, X is one-dimensional/one feature: the size of a house. y, then, is the price of the house. From this, we want to formulate a model (function) for the data that allows us to accurately find the price of a house from its size. We can use this to make predictions in the future. Figure 4. shows how a Machine Learning algorithm may fit a polynomial to the data, allowing it to make a generalization. We call this a \u201cregression\u201d; the primary differentiator between classification and regression is that a regression algorithm outputs a real continuous value, whilst the classification algorithm outputs a discrete category/class, normally a binary state; 0 or 1.\n\nWe denote m as the number of training examples, instances, or data points in the training set, which is equal to 8 in Figure 1 and 11 in Figure 3. Then, we define n is the number of features or the dimensionality of the dataset, which is equal to 2 (tumor size and radius) in Figure 1 and 1 (house size) in Figure 3. So, mathematically, X \u2208 \u211d^(m x n), feature vector x or X_i \u2208 \u211d^n and y \u2208 {0, 1}^m for classification problems or y \u2208 \u211d^m for regression problems. Now, we define a \u201cparameter vector\u201d denoted as \u0398, where \u0398 \u2208 \u211d^n. The goal, then, for any Machine Learning algorithm, is to map each value of X to its respective y value in the training set with high accuracy using this \u0398 and a hypothesis function. These notations are important because we will use them to formulate equations in the future.\n\nA hypothesis function is a function we use to model the training set and to \u201chypothesize\u201d on new data. Like a normal function, it takes a parameter x (a vector which corresponds to one feature vector from matrix X), but also takes the parameter vector \u0398. \u0398 is used to store the coefficients or \u201cweights\u201d for each feature in x in the function. Hence we denote the hypothesis function as h(x). For a regression such as the housing prices example, we may use h(x) = \u0398_0\u22c5x_0 \u2014 a linear function where x_0 is the size of the house and hence \u0398_0 is the gradient (the constant in this case is zero). On the other hand, we may use a quadratic function instead. When classifying, we actually use a continuous function and then round the values up/down to achieve 0 or 1. This function is known as the \u201csigmoid\u201d or \u201clogistic\u201d function, and it asymptotes at 0 and 1. The idea is that this hypothesis function, paired with some value of \u0398, can fit our training set well. Ultimately, we want h(x) \u2248 y for each training example.\n\nIn both cases, our goal is to try to formulate a model that maps X from y accurately with h(x). We must choose and experiment with the hypothesis function to use, i.e. linear, polynomial, logistic, logarithmic, etc. The Machine Learning algorithm will find coefficients for the function that enable the mapping to be as accurate as possible across all training examples. In other words, the algorithm\u2019s task is to find an optimum \u0398 for h(x). To find this optimum \u0398, the algorithm needs a way to evaluate the performance of any given \u0398. This is where the cost function J(\u0398) comes in; J(\u0398) is a performance measure that iterates through each training example and, for any given \u0398, compares h(x) to y and treats this discrepancy as the \u201cerror\u201d. Different cost functions may treat these errors different, i.e. sum them, average them, sum the squares, take their logarithm, etc. We will formally notate and use a popular cost function later on.\n\nNow, the goal of the algorithm is, qualitatively, to find a \u0398 that minimizes the output of the cost function. This is known as an optimization problem and is colloquially referred to as \u201ctraining\u201d the algorithm. We can achieve this with Calculus by either solving the derivative analytically to find the minimum point of \u0398 plotted against J(\u0398) (in some high dimensional space) or by using a popular, efficient method called \u201cGradient Descent\u201d. Once we have found an optimum \u0398, we have built a model that not only performs well on the training set but can be used to confidently predict the output of new features.\n\nError will always be present because it is highly unlikely that one continuous function can perfectly fit many noisy data points, unless the function is of a very high order \u2014 but in that case, the model may be fitting the data too much so that it is disadvantageous when used for prediction.\n\nNow, we will look at \u201cLogistic Regression\u201d. Although it has \u201cregression\u201d in the name, Logistic Regression is actually a popular classification algorithm. Many have hypothesized that the roots of the name arose from the fact that Logistic Regression draws a \u201cdecision boundary\u201d (a line to separate the two classes as shown in Figure 2.) in the form of a line/polynomial, much like a regression. Figure 2. shows exactly what a Logistic Regression algorithm may do \u2014 figure out how to separate the data into two distinct groups that differentiates malignant from benign.\n\nTo refresh our memory, we asserted that, for classification problems, y \u2208 {0, 1}^m, where 0 is the \u201cnegative class\u201d (benign) and 1 is the \u201cpositive class\u201d (malignant). Then, we ensure that 0 \u2264 h(x) \u2264 1, to resemble a probability, where h(x) = P(y=1|x;\u0398) or the probability that the tumor is malignant, given x and parameterized by \u0398. A h(x) = 0.4 implies that a person has 0.4 chance of having a malignant tumor, and hence we predict that they do not. As an additional note, P(y=1|x;\u0398) + P(y=0|x;\u0398) must equal to 1.\n\nWe want a continuous function h(x) where 0 \u2264 h(x) \u2264 1. For this, we turn to the \u201csigmoid\u201d or \u201clogistic\u201d function which satisfies this inequality, notable for its smooth \u201cS\u201d shape:\n\nWe can see that the function has horizontal asymptotes at g(z) = 1 and g(z) = 0. As z \u2192 \u221e, e-\u221e \u2192 0, and so g(e-\u221e) \u2192 1. On the other hand, as z \u2192 -\u221e, e\u221e \u2192 \u221e, and so g(e\u221e) \u2192 0. We can show that the y-intercept is 0.5; z = 0, e0 = 1 and so g(0) = 1/2 = 0.5. Hence, the range of this function is y = {y | y \u2208 \u211d, 0 \u2264 g(z) \u2264 1}. Technically, it is not possible to attain an output of 0 or 1 from this function due to the asymptotes, but a good argument for this is that, even with 100% accuracy, we can almost never be certain that an outcome absolutely will occur.\n\nSo now, our hypothesis function will be g(z). But where do the weights \u0398 and the features X come in? Recall that X is a matrix where each row in X is a single feature vector x of length n. We use \u0398 to weight our values like so: \u0398_0\u22c5x_0 + \u0398_1\u22c5x_1 + \u2026\u2026. + \u0398_(n-1)\u22c5x_(n-1). Since both \u0398 and x are vectors, we can represent this as a dot-wise product:\n\nHowever, we can write this in an even shorter way. If we recognize that both \u0398 and x have the same dimensions, then \u0398 transposed, a Mathematical operation where we switch over the columns and rows of a vector, can be directly multiplied with x since the dimensions (1 x n) and (n x 1) can be legally multiplied. The figures below make it apparent why:\n\nNow, we can see that our weighted feature vector is equal to the value \u0398^Transpose\u22c5x. We will now use this product as the parameter z in our sigmoid function g(z). Hence, our final expression for h(x) is g(\u0398^Transpose\u22c5x) or:\n\nBut why? By changing \u0398 we essentially apply transformations to g(z)/h(x), altering the decision boundary until we reach a point where \u0398 is optimum and our transformed h(x) models our data not just nicely, but with the highest accuracy. We use the sigmoid function because, again, it bounds values from 0 to 1 for any real input and provides us with a valid probability.\n\nClassification algorithms, however, must eventually classify between multiple categories/states. This becomes more apparent when we look at a use case such as spam filtration; we have to make a choice to send to the spam folder or the inbox. Suppose we predict y = 1 when h(x) \u2265 0.5 and y = 0 when h(x) < 0.5.\n\nRecall that g(z) = 0.5 only when z = 0. Since z = \u0398^Transpose\u22c5x, then \u0398^Transpose\u22c5x = 0 when g(z) = 0.5. Therefore, we predict y = 1 when \u0398^Transpose\u22c5x \u2265 0. Now, if we graph the line \u0398^Transpose\u22c5x = 0 or \u0398_0\u22c5x_0 + \u0398_1\u22c5x_1 + \u2026\u2026. + \u0398_(n-1)\u22c5x_(n-1) = 0 on our training set, this will act as a \u201cdecision boundary\u201d, where data points on one side of the line would be labelled as y = 0 and data points on another side would be labelled y = 1. Figure 5. illustrates this.\n\nFigure 5. shows how a decision boundary might be formulated for some arbitrary \u0398 and training set. If we train Logistic Regression correctly and reach an optimum \u0398, then we will, in turn, create a decision boundary that will nicely separate the data (unless, of course, the data is very noisy). Initially, the decision boundary will be inaccurate (because the convention is to first set \u0398 to a series of 0s). Decision boundaries are significant because they provide us with a visual representation of what Logistic Regression does, but it is important to note that decision boundaries are simply an implication/product of the Mathematics behind Logistic Regression. In reality, the algorithm is trying to minimize the error on the output of the sigmoid-hypothesis function with respect to our training set, not directly build a decision boundary. This visual element becomes irrelevant when n > 3 and we start working with four plus dimensions (which is more reflective of industrial applications).\n\nAs discussed in the general overview of Machine Learning, we need a performance measure a.k.a \u201ccost function\u201d J(\u0398) to score the accuracy of a given \u0398 on a training set. J(\u0398) will output the cost or error of the parameters, and thus the goal of the algorithm is to select such parameters that minimize J(\u0398). In this section we will look at a popular cost function that is used in Logistic Regression.\n\nThe basis of our error is the discrepancy between the actual output y and the output of our hypothesis h(x) for our individual training examples. Normally, a cost function will get the average error in a system (so that an outlier does not affect us too much). So, one may suggest a cost function that simply sums over all the discrepancies (h(x) \u2212 y) in our training set and divides it by the number of training examples to get the average figure, like so:\n\nAn example would be if y = 1 and h(x) = 0.4, we would add 0.6 (divided by m) to the cost. But this actually is not the best formulation. The reason becomes clear when we look at the optimization section, but in short, each individual error can be a maximum of 1. This means that the maximum cost for any given training set is m. This is a fairly small number and, relatively, would be fairly close to a minimized J(\u0398) on the number scale. Ideally, we want the cost of poor fit parameters \u0398 to grow fast (faster than linearly), so that when we are optimizing we can quickly skip past the poor parameters which have very high costs. If the outputs of the cost functions are very close together, we have to take many more \u201csmall steps\u201d to reach our solution. In addition, what if our h(x) = 0 and y = 1? If this error arises during prediction, we may be telling patients with certainty that they do not have a malignant tumor when in fact they, tragically, do! Clearly, we need to assign a cost greater than merely 1 to this.\n\nSo instead, we use a different cost function. But first, we need to define our core discrepancies using a bipartite piecewise function:\n\nIf y = 1, then we will take the negative log of h(x). Since 0 < h(x) < 1, the output of the log function will always be negative, so negating this expression will give us a positive value. If y = 0, we take the log of 1 \u2212 h(x). Figure 7 and Figure 8 make it clear why this is so.\n\nIf y = 1 then we use the left hand side function to give us the discrepancy. The function has an x-intercept at x = 1 because log(1) for any base is 0. This captures the logic that, if y = 1 and h(x) = 1, then the discrepancy is 0. Due to the the vertical asymptote at 0 (log(0) is always undefined), it also captures the intuition that if y = 1 and h(x) = 0 then the discrepancy should tend to infinity. The same behavior occurs when y = 0 with the function on the right hand side. Of course, due to the nature of the sigmoid function, we cannot actually achieve h(x) = 1 or h(x) = 0 due to the horizontal asymptotes nor can we really penalize our parameters by an infinite value, but the behavior we are describing in general occurs as these outputs tend towards 0 and 1.\n\nNow, the final cost can be written as:\n\nWe can further collapse it from a piecewise function into a single function like so:\n\nIn the case of y = 0, then the term yi will be zero, and the entire left hand side product will evaluate to zero. (1\u2212y_i) will evaluate to 1, hence for this iteration we will add on a discrepancy term in the form log(h(x)) as discussed in the paragraph above. When y = 1, the right hand side product will be zero since (1\u2212y_i) will be 0 too, and so we will treat the discrepancy term as log(1\u2212h(x)), again, as we did for our piecewise function. Notice that the negative signs are missing from the log functions. Instead, we have factored it out of the summation expression for brevity.\n\nNow, our task is to min J(\u0398)\u2014to find/select a \u0398 parameter vector that minimizes the output of the cost function, and hence fits our model the best it can. In this section we will look at employing a popular method called Gradient Descent that uses Calculus to find this optimum value. Gradient Descent is one of multiple optimization methods that are used in Machine Learning.\n\nOne may ask why we cannot simply find the minimum point analytically by setting the derivates to zero akin to a textbook Calculus problem. Ultimately, analytical solutions (called the Normal Equations) and Gradient Descent differ in many ways, with Gradient Descent being the preferred industry method. Why? Well, for starters, the Normal Equation is computationally expensive, meaning it takes a lot of time to execute. The reason for this is simple: the equation includes (X^Transpose\u22c5X)^-1, as in taking the \u201cinverse\u201d of an n x n matrix. The issue with this is that it runs in O(n^3) time complexity, meaning that, as the number of training examples in the training set X/y increase, the time it takes to compute the inverse grows by that value to the power of 3. This is simply because programmers haven\u2019t found a truly efficient way to calculate the inverse of a square matrix yet. This is a good article that goes into further depth on the reasons we use GD vs. Normal Equation(s).\n\nFirstly, imagine a hypothetical scenario where n = 2 so \u0398 only stores two weights: \u0398_0 and \u0398_1. The first thing we will do is set \u0398 to some randomly generated parameter vector, or simply zeros. Then, we will create a graph where \u0398_0 and \u0398_1 are on the x and z axes, and J(\u0398) is on the y-axis.\n\nFigure 9. shows how, on some arbitrary scale, the weights in \u0398 affect the cost J(\u0398). The graph shows that at roughly \u0398 = (-8, -8) our cost J(\u0398) is at a minimum point. There is one global minimum (or optimum) and no local optima. It is also convex. In fact, we can make a generalization (which can be proved) that any arbitrary weight \u0398_j graphed against cost J(\u0398_j) will be of a convex shape. Figure 10. demonstrates this visually.\n\nYou can check out this article for a formal proof on the convexity of Logistic Regression\u2019s cost function.\n\nThis minimum point signifies the value of \u0398_j that occurs when the output of J(\u0398) is at its lowest, and hence it is \u0398_j at its optimum. Now we are tasked with finding this minimum point/global optimum for each \u0398_j, ultimately using these results to achieve our optimum \u0398. The gist of Gradient Descent is that we can take small \u201csteps\u201d from some arbitrary point on the graph to the optimum by updating the current \u201cposition\u201d in the direction of the derivative. Firstly, we need to \u201cplace\u201d ourselves somewhere on the graph in Figure 10. by computing any coordinate \u2014 i.e. by setting \u0398_js to zero and calculating the resulting cost. Figures 11\u201313. demonstrate this visually.\n\nWe start at Figure 11. at iteration (or step) 1 with a very high cost. Then, after just 100 iterations (compared to our total number of iterations which can range from 1000 to 100,000 and beyond) our cost has drastically decreased, demonstrated by a much lower y coordinate in Figure 12. compared to Figure 11. Then, after another 900 iterations we are at our optimum value for \u0398_j \u2014 Figure 13. shows how we are now placed at the minimum point. Here we have \u201cconverged\u201d and no longer move, so we terminate Gradient Descent.\n\nIf you were on the right hand side of the minimum point, you would move in the left direction until convergence. We can translate \u201cleft\u201d to \u201cin the negative direction\u201d, and \u201cright\u201d to \u201cin the positive direction\u201d. We can use the gradient to tell us this \u2014 a negative gradient indicates that we need to move in the positive direction (as we would be left of the min. point) and a positive gradient indicates that we need to move in the negative direction. We repeat this process many times and hence keep descending. If our cost is very high, our gradient would be steep and so our steps would be larger. This allows us to quickly skip past poor \u0398_j candidates. Then, as we get closer to the optimum, the gradient becomes much flatter, allowing us to take finer steps to prevent any divergence from the minimum point. Hence the name \u201cGradient Descent\u201d.\n\nWe will update our current position (which is our current \u0398_j) by subtracting it from the gradient (subtracting by a negative is an addition and vice versa). To extract the gradient at any given point, we need to differentiate to compute the derivative. Since we are finding the derivative with respect to \u0398_j only, we will solve it with partial differentiation. We will then multiply this gradient by some (adjustable & custom) constant \u03b1 which we call the \u201clearning rate\u201d where 0 < \u03b1 < 1 (usually 0.03). \u03b1 controls how fast we move towards the optimum by being multiplied with the gradient value and hence minimizing the expression. If we update by just the gradient value (or if \u03b1 is too large), we will take steps much too large and move past the optimum, which would render us in a viscous cycle of divergence, moving further and further from the global optimum. If \u03b1 is too small, then we will take a long time to converge.\n\nWe need to perform Gradient Descent for each weight \u0398_j in \u0398. The following equation is the formal notation for Gradient Descent:\n\nThe Assignment Operator := is the notation we use to assign a value to a variable and hence update \u0398_j. We know when we have converged because, at any optima, the derivative will be zero. Hence, the Update Term will be equal to zero, and we assign \u0398_j to itself \u2014 so we get \u201cstuck\u201d at the optimum. This is why the convexity of our cost function allows us to always find the best solution; if we had any local minimum then we may get stuck at these points too, and Gradient Descent would terminate at a non-optimal solution. Sometimes Gradient Descent may take 1000 steps/iteration, and sometimes it may take 100,000. It depends on the nature of your training set and learning rate.\n\nNow, we can solve the partial derivative. I\u2019m not going to include the derivation here, but if you do manually solve it, it should look like what follows in teh final expression for the Gradient Descent algorithm:\n\nAfter a successful execution of Gradient Descent, we can evaluate the cost of our trained model using the cost function J(\u0398), however this outputs a real number that may be difficult to interpret (is it high or low \u2014 and on what scale?). Instead, we can iterate through our entire training set and \u201ctally\u201d the number of times that h(X_i) = y^i. We divide this tally value by the total number of training examples and multiply by 100 to retrieve the training accuracy percentage. If it is above at least 95% (a numerical threshold used by many Computer Scientists), we can confidently use it to make new predictions by inputting any new feature vector x into our hypothesis h(x). If our accuracy is below 95%, we may need to do some further optimization, and we will look into this in a later section.\n\nFortunately, many institutions have published medical breast cancer tumor data online that can be used to apply Logistic Regression to. The University of Wisconsin, Madison in the state of Wisconsin, USA is most notable for this effort. They offer a sample code number for each patient, followed by a series of tumor features, and finally a field indicating whether the tumor is malignant or benign. I downloaded the dataset and imported it into Excel. Figure 14. displays an excerpt (6 out of 699 instances) of the data.\n\nColumn A, the sample code number, can be thought of as simply a reference to the instance and is not a feature in itself because it does not contribute to the outcome. Columns B-J for a single row, however, are features and could be thought of as inside a feature vector x. All the rows in these columns put together would make matrix X. Column L is the category/class (malignant or benign) that can be thought of as a value in the output vector y. Thus, this is our training set. 458 of the instances are benign and 241 are malignant \u2014 a 66% to 34% ratio.\n\nI have provided my Logistic Regression algorithm (written in Octave) with the data. I will perform Logistic Regression with regards to just two features in two dimensions for visualization purposes. There are 9 permutation 2 = 72 combinations of feature pairs, and Figure 15. shows a good correlation I found between Single Epithelial Cell Size and Uniformity of Cell Size, plotted with my code. A plus represents a malignant tumor, and a circle represents a benign tumor.\n\nThe data is noisy (a few anomalies), but this is to be expected from any real-world/industrial application dataset; we don\u2019t need 100% accuracy, we just need to find the general pattern presented since noise is stochastic. More importantly, however, this data is just taking into account two features; later we will look at combining all features. This is also why there seem to be very few data points; many of them overlap/are duplicates. I ran Gradient Descent for 1000 iterations to calculate our optimum weights and decision boundary. Figure 16. shows the decision boundary that was formulated.\n\nUnfortunately, the decision boundary does not seem to accurately separate our two classes. Figure 17. displays more information about our algorithms\u2019 results before and after training.\n\nWe set our initial theta to zeros, with a cost of 0.69. Although this seems small, it is relatively large because we only have 35% accuracy \u2014 that is we only correctly predict malignant or benign on 35% of our training examples in our training set. In addition, our gradients in Gradient Descent are non-zero, indicating that we have to still perform iterations of Gradient Descent to reach our optimum. After we have trained, our new theta is [-0.75964, 0.91470] \u2014 much different to our initial theta. Our final cost is 0.53, smaller than our initial cost. Intuitively, this does not seem much smaller. And this intuition plays out; our final percent accuracy is only 79% (again, remember that many modern Machine Learning algorithms can achieve 95% fairly easily). We know that this is our available optimum because our gradients are almost zero: 1.1593e-05 and 1.3926e-05 where e-05 is a shorter notation for \u22c510\u20135, not to be confused with the natural constant e. Hence, our final gradients are 0.000011593 and 0.000013926. Although this isn\u2019t 0, we still say we have converged. Practically speaking, Gradient Descent will never converge because the gradient that we subtract \u0398_j by will keep getting smaller each iteration. So we progress further on the x-axis in a fashion that tends towards the optimum, but won\u2019t reach the optimum. The gradient of our convex function will hence become flatter and flatter, and so the gradient will tend to zero. This doesn\u2019t matter; there is no practical difference between a gradient of 0.000011593 and 0. This is why we terminate Gradient Descent after a constant number of iterations.\n\nOur results, statistically and visually, clearly show that our current implementation of Logistic Regression was not able to find an accurate decision boundary to fit the data. We will now look at how we can increase the accuracy from its current 79% to at least 95%, and then beyond.\n\nA major oversight of mine while looking at Logistic Regression (and one that was clear in our data testing) was forgetting that many functions have a \u201cconstant\u201d term that isn\u2019t attached to any feature. We can see this in Figure 16. because the decision boundary intercepts the origin. This constant term allows us to vertically translate the decision boundary so that it does not have to intercept the origin, enabling us to fit our data much more optimally.\n\nWe will denote our constant term as \u0398_0, the first value in \u0398, and hence extend the dimensionality of \u0398. Note that this does not change the value of n, our dimensionality of the feature vector, as our new hypothesis will be: h(x) = \u0398_0 + \u0398_1\u22c5x_1 + \u2026\u2026. + \u0398_n\u22c5x_n. So, instead of referring to features and weights at index 0 (\u0398_0/x_0), we start at index 1 (\u0398_1/x_1). No feature is attached to \u0398_0, hence it weights nothing and acts to vertically translate the decision boundary.\n\nHowever, now our equation \u0398^Transpose\u22c5x is invalid, as the dimensions of \u0398 and x mismatch and every value in \u0398 will need to multiply with some value in x. But, instead of thinking that \u0398_0 weights/is a coefficient of no value, we can think that it weights 1. This is because 1\u22c5\u0398_0 = \u0398_0 for any possible value of \u0398_0. We hence append the value 1 to the start of every feature vector x and call it a \u201cbias term\u201d, such that x_0 = 1. This bias term isn\u2019t really a feature, and so we still consider n to be the same. Now, we say that matrix X \u2208 \u211d^(m x [n + 1]) , feature vector x or X_i \u2208 \u211d^(n+1), and parameter vector \u0398 \u2208 \u211d^(n+1). Figure 18. shows our decision boundary with bias terms included.\n\nOur decision boundary separates our classes much better now. Figure 19. shows statistics regarding the improvement.\n\nNow we have three weights, with the first being our new constant term. The algorithm decided that a y-intercept of -5.9 is suitable. Notice that our other weights have also completely changed as a result of this addition. Our final percent accuracy is an impressive 94%! Our cost is much lower at just 0.18. This is almost ready for industry usage; we could employ a few more techniques to increase the accuracy even more. Based on Figure 18., it seems as if a line may not be the best decision boundary for our data. It instead looks like it could be better separated in a curve-like fashion. Next, we will begin to change our decision boundary from a simple linear function to a model of higher complexity.\n\nPerhaps a linear function isn\u2019t ideal for our decision boundary because it \u201cunderfits\u201d our data. A Machine Learning hypothesis/parameter vector is said to underfit when it fails to generalize well on the training set, and is too simple with regards to the data it is trying to model, resulting in a high cost. Figure 20. demonstrates a data set where a polynomial decision boundary is clearly suitable, but we have fitted a linear function.\n\nFigure 20. is an example of extreme underfitting, but our own results with Figure 18. are much milder. Still, there is room for improvement. And with something as serious as classifying cancer tumors, effort to reduce underfitting should be taken seriously.\n\nDiagnosing the problem of underfitting is very easy in a two or three-dimensional space like Figure 20.; we can simply plot the data and make a judgement call. But when we want to include 4 or more features, this benefit is no longer available to us, so things get a bit more complex. To explore how we detect underfitting in 4+ dimensions, we first need to define some more terminology. We will still denote our training set as the one our Machine Learning algorithm will learn from, but we will now define our cross-validation set. Our cross-validation set is actually inherited from our training set \u2014 we will choose some small percentage (eg. 30%) and select and remove that percentage of instances from the initial training set, placing it into our cross-validation set. Our cross-validation set is hence of the same data reliability and accuracy as the training set is. We can then use our cross-validation set to estimate how well the model would perform on data out of its training set \u2014 when making predictions.\n\nMore concretely then, we can detect underfitting when our model, at its optimum, performs poorly (with a high cost) on both the training set and the cross-validation set. This is called high-bias. Imagine you were to make future predictions with data similar to that shown in Figure 20.; it would be difficult to be confident about its accuracy not just because the model does not fit the training set well, but more implicitly because we have failed to extrapolate the clear trend presented.\n\nA linear function has less distinct terms than a quadratic function. To solve underfitting, hence, we increase the complexity of the hypothesis function. Figure 21. displays the kind of model we may opt for.\n\nTo achieve a quadratic decision boundary like the one in Figure 21., we don\u2019t actually need to modify our hypothesis function h(x). \u0398^Transpose\u22c5x is very convenient for us mathematically, so we can just include new \u201cderived features\u201d into x and expand the dimensions of \u0398, increasing n in the same fashion. These derived features will be the existing features with exponents raised to them. For example, in the case of a quadratic decision boundary, we will add in the value x_i^2 for each feature x_i into each of our feature vectors x in matrix X. If we want a decision boundary of any polynomial with degree d, then we would add the value(s) x_i^d, x_i^(d-1), \u2026 , x_i^2. However, the different feature values in x must also multiply with each other in varying degrees. Figure 22. demonstrates a select few of features that may be derived from the features in x.\n\nAs the degree increases, many more features are created and the complexity of the model also increases. Then, the Machine Learning algorithm will be able to interact with these new features and fit optimum weights in \u0398 to build a suitable polynomial decision boundary.\n\nWith all this being said, it is important to not overfit. Overfitting is the opposite of underfitting; it is when you build a model that is so complex it looks as if it is suitable but in reality it is not. Figure 23. illustrates an example of this.\n\nNotice how this vastly complex decision boundary in Figure 23. separates our two classes with higher accuracy than the one in Figure 21. This is the idea (and how we diagnose) of overfitting; our model performs very well (sometimes even with 100% accuracy) on our training set, but extremely poorly in our cross-validation set. We call this high-variance. This is because the Machine Learning algorithm has failed to generalize on the data and find the pattern. Instead, it has factored in all the noise in an attempt to reduce the cost as much as possible with access to a high-degree, flexible polynomial model. So when we make new predictions based on new data that will follow this real-life trend, our predictions won\u2019t be accurate. It is very easy to dismiss the potential of overfitting because, unlike underfitting, it does not immediately raise any red flags eg. a high cost.\n\nThere are two ways to reduce overfitting: implicitly increasing model complexity and introducing regularization. By reducing the degree of the model, the curvature/shape of the model can be less versatile, and hence there is less opportunity to build a decision boundary that can overfit. However, sometimes the trend is of high degree in nature like shown in Figure 21., so we choose a complex hypothesis function. But, because the goal of a Machine Learning algorithm is to find the least possible cost, we will just end up overfitting instead of finding this trend. Herein lies a problem with the cost function; we need to inform the it that it needs to not overfit. This is in fact possible, and it\u2019s a common technique called regularization.\n\nThe gist of regularization is that we penalize model complexity along with training set error to reach a middle ground of simplicity and accuracy. We do this concretely by penalizing the size of the weights in \u0398. This is because smaller weights results in simpler and smoother models as each feature is \u201cweighted\u201d less (less importance) \u2014 as these weights tend to 0, the features have little to no contribution on the output. The following equation is the new cost function that will be used.\n\nAs you can see, we have introduced a new term into the cost function which adds on the squared size of each of the weights to the numeric output. This will determine how complex the model is; greater size weights increase the complexity factor as discussed before. Now, recall that the aim of Machine Learning is to minimize the output of this function; to minimize both training error and complexity. An optimum training error is created by very complex, overfitted models as shown in Figure 23. But this will cause a high complexity factor and hence the cost will go back up. With very simple models, though, the training set error will be high so the cost will also go back up. An absurdly low error is compensated by a high complexity factor and vice versa; an inversely proportional relationship. Ultimately, Gradient Descent will need to find a combination of error and complexity that will be minimum; this is our balance/middle ground, and it will allow us to prevent overfitting.\n\nWe multiply this complexity term by constant \u03bb which is like the \u03b1 term in Gradient Descent in that it is an adjustable constant which controls how much we emphasis we put on regularization. If we have a high value for \u03bb, we penalize the complexity factor by a greater degree and so our final model will be much simpler. Generally, we use \u03bb = 1, but also experiment with values like 10 and 100 to see the effect on our decision boundary. Notice that we do not regularize the bias term weights/constant \u0398_0 (we start at j=1) because it merely offsets the relationship and is not a coefficient of a real-life feature. Adding regularization to the constant term just implies that we should use a small constant term, which perhaps isn\u2019t correct and would just result in a grossly poor model. We divide the entire term by 2m as an average to keep consistent with our training set error which is also an average term (for obvious reasons). We take the square of \u0398_j to penalize extremely large weights / extremely complex models even further. There are other reasons we do this too; \u0398_j^2 will give us a positive value (negative values for our weights are common) which we need, and also when we differentiate this new function for Gradient Descent, the exponent will conveniently lower and cancel out with the 2 in 2m\u22121. The following equation is our new expression for each update in Gradient Descent after applying the differentiation rules.\n\nI ran regularized Logistic Regression with varying polynomial model degrees. Figure 24. shows a few different decision boundaries that were formulated, along with their corresponding degrees.\n\nDegrees 3 and 4 achieve a 95% accuracy! A degree of 2 fits the data with 94% accuracy \u2014 the same as our linear decision boundary. A degree of 4 extends/wraps around vertically in a fashion that seems obviously grossly inaccurate for predicting on new data. I would use a degree of 3 for our decision boundary.\n\nSo far, we have formulated a model that achieved a compelling, industry level accuracy of 95%. However, we have only been working with two features (Single Epithelial Cell Size and Uniformity of Cell Size) for visualization purposes. Remember that in Figure 14. we have 9 features for each patient, including:\n\nAlong with the benign/malignant category. Now, we need to train our algorithm to create a model that classifies the tumor based on all these 9 features, not just two! Can you imagine a decision boundary like those shown in Figure 24. but in 9 dimensions instead of 2 dimensions? No! It is impossible for us, but machines can achieve this purely by using Linear Algebra.\n\nFigure 25. shows varying values for the degree of the model and \u03bb with their corresponding accuracies and results.\n\nWe have great results all around \u2014 each much beyond 95%. A linear fit has an accuracy of 97%, but a model of degree 2 (with regularization applied) achieves an astounding 98%, and we will proceed with this. A degree of 3 dips in accuracy, and will be more complex than a degree of 2, so we do not use it.\n\nNow it is time to make predictions! I used our \u0398 from the model we decided on. To make predictions, we perform the following procedure:\n\nLet us take a patient from the data presented on Figure 14., row 7, sample code number 1017122. This patient\u2019s class is M, hence we expect our Machine Learning algorithm to predict a malignant tumor. Figure 26. shows what my program actually predicted.\n\nMy program makes an accurate prediction. Interestingly, our prediction probability is almost exactly 1 (computer does rounding), even though any value of 0.5 or greater would be considered malignant. Figure 27. shows an example where this is not the case.\n\nOur probability is still very high \u2014 0.999! This is one of the lowest (but accurate) probabilities I could find.\n\nRemember that our accuracy was approximately 98%. This means that we predicted inaccurately for 2% of our training set. Figure 28. shows a cell in our data spreadsheet that is part of this 2%.\n\nAn anomaly in our data, we were unable to predict the status of this patient correctly. However, do note that the output probability was roughly 0.33 \u2014 quite high to simply disregard. For a doctor, this may be a sign to take precautions and undergo a more thorough examination. If we tell a patient they do not have cancer when in fact they do, this is called a \u201cfalse negative\u201d (a negative class \u2014 benign \u2014 which we have predicted incorrectly). We want to minimize on false negatives because if we do not treat a patient with any medication, they may pass away from their cancer unknowingly. On the other hand, false positives (we predict that the patient has a malignant tumor when in fact they don\u2019t) are less serious because it is better to be safe than sorry \u2014 that is we do not actually know what the patient has and so treating them with medication will guarantee their health either way. Figure 29. displays statistics about the false negatives and positives in our data.\n\n67% of our errors are false negatives, which is bad news. One thing we could do is, instead of rounding the probability, setting a \u201cdecision threshold\u201d to something lower eg. 0.25. This means that any probability bigger or equal to 0.25 will be considered as malignant, and anything lower benign. Figure 30. shows how this changes our results.\n\nNote that even though we have significantly more errors, we only have half of the number of false negatives as before. And they make up only 24% of all errors. This is much better.\n\nLet\u2019s say that a patient arrives to the doctor\u2019s office and the doctor records the following data for this person:\n\nWe will tell them that they, tragically, have a malignant tumor, and will proceed with medication from there."
    },
    {
        "url": "https://ayearofai.com/0-2016-is-the-year-i-venture-into-artificial-intelligence-d702d65eb919",
        "title": "#0: 2016 is the year I venture into Artificial Intelligence.",
        "text": "As I find myself increasingly interested in Mathematical/Scientific Computer Science and less so in generic application development *, a yearning to extend my side-side hobby of Machine Learning and Artificial Intelligence into a pursuit of virtuosity overwhelms me.\n\nEarlier this month, I decided on my New Year\u2019s goal \u2014 to be accepted into Stanford\u2019s undergraduate class of 2021. Although an overarching dream of mine (and almost the culmination of my education thus far), I know one thing for sure: I will study at a great college and make the most out of it.\n\nBut what after that? Start a startup, enter academia, join a VC, work at a tech company? It\u2019s hard to say. Perhaps all four (denoted a \u201cSilicon Valley pundit\u201d) \u2014 think Balaji Srinivasan. But this is what I do know: I want to eventually innovate and make major research contributions to Artificial Intelligence. And I want to do it sooner rather than later; it\u2019s a field that stimulates not only my STEM roots, but my Humanities (Philosophy, History, Langauge) roots too. My love of making iOS apps in 2011 has matured to a love of AI (and its real world applications) in 2015 and beyond.\n\nIf this is who I feel I am, then per existentialism (and me, personally) it\u2019s clear: I need to double-down. With Contra currently being outsourced to developers and designers after an investment (in my opinion, content-driven sites need long-term oriented founders), I do find irregular pockets of free time during my weekly schedule when the IB runs out of homework, assignments, and tests to shove my way. After being influenced by my English teacher\u2019s prompt to develop a greater reading portfolio, echoed by many successful people such as Mark Zuckerberg, Bill Gates, and Andrew Ng, I have decided that it is time to make learning about Artificial Intelligence a project (via books but video lectures as well, of course).\n\nMy current knowledge and work in Artificial Intelligence consists of:\n\nI am hoping to expand this list to include:\n\nI would like to continue headway with my Extended Essay/Research Paper on using evolutionary algorithms to generate new code, and am eager to expand this into other fields such as NLP and Recurrent Neural Networks. But more importantly, I want to look at how we can tie all these together. I\u2019m not trying to (and don\u2019t want to) quantify what I\u2019m attempting to embark on\u2026 I want to leave much of it abstract and let each endeavor take me to the next in its own respect. By December of this year, I would like to momentarily forget my application to Stanford and other colleges, and reflect on how much I\u2019ve learned. Not if I\u2019ve checked each on the list above (and maybe I\u2019ve added a completely different route!), but if my gross knowledge of Artificial Intelligence has greatly netted to a level where I feel confident that I have made at least some progress to contributing to this beautiful discipline governed by professors and researchers at top universities.\n\nSo how will this blog be structured? I am going to start by studying one chapter from my Machine Learning textbook and one lecture from a Reinforcement Learning online course each week, both whilst reading a (highly endorsed) book on the implications of Strong, General Artificial Intelligence. Once I feel like I have completed one significant learning phase (be it technical or not), I will attempt a small project to demonstrate my skills. This could be a hack on Github or an argumentative discussion on this very blog. I\u2019ll post updates when these happen (and when I feel like it)\u2026 minimum once a month, hopefully twice.\n\nAnd then I\u2019ll continue from there. Perhaps I\u2019ll look at Deep Learning algorithms, or perhaps I\u2019ll look at refining my understanding of Epistemology."
    }
]