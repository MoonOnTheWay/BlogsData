[
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-attention-model-and-speech-recognition-deeb50632152?source=---------0",
        "title": "DeepLearning series: Attention Model and Speech Recognition",
        "text": "This model is an alternative to the encoder-decoder RNN architecture (see previous blog), and acts similar to how humans translate. Therefore, not waiting for the whole input sentence before translating, but starting doing it on the go, looking at one part of the original sentence at a time.\n\nTo architect a model in such a way, we need to have a context of words that the network pays attention to, in order to generate the subsequent words. The \u201cattention weights\u201d (\u03b1) denote how much attention should be paid to each piece of the original sentence.\n\nThe model is composed of two RNN networks. A bidirectional RNN to compute set of features for each of the input words and another RNN to generate the English translation. The input to the second RNN will be governed by the attention weights, which denote the context that is considered by every single word to compute the translation.\n\nThe alpha parameters tell us how much the context would depend on the features we are getting, so the activations we are receiving from the different time steps.\n\nThe \u201cc\u201d context is the weighted sum of the attention weights.\n\nTherefore alpha<t,t\u2019> is the amount of \u201cattention\u201d that y<t> should pay to a<t\u2019>.\n\nNow we need to know how to compute these parameters alpha<t,t\u2019>.\n\nOne way to do it is to train a small neural network where the inputs are represented by the network state in the previous time step (s<t-1>) and the features from the time step t\u2019 (a<t\u2019>).\n\nThe downside of this algorithm is the time cost to run it. In fact, if we have Tx words as input and Ty for the output, the total number of attention parameters is Tx* Ty.\n\nThe attention model is used for applications related to speech recognition, where the input is an audio clip and the output is its transcript. Before inputting the audio clip, there is one pre-processing step that is in charge of generating a spectrogram out of the raw audio clip.\n\nIn the past, speech recognition used to be built using an additional step, which computes phonemes before entering a neural network. Nowadays, we use end-to-end deep learning, which makes this step unnecessary. We take different time frames of the audio input and then the attention model outputs the transcript.\n\nOne successful model that is used is the CTC, Cost Per Speech Recognition.\n\nNormally the input data Tx is much larger than the output Ty since an audio clip for example of 10 seconds at 100 Hz results in a 1,000 input sequence. While, instead, the output, the transcript, is not composed of 1,000 characters.\n\nThis model uses a simple RNN network where Tx = Ty.\n\nThe function of the model generates an output composed of repeated characters separated by blanks. (Like: \u201cttttttt__h_eeeee____bbb_rrrr_oo_w_nnnn\u201d). Finally, it collapses repeated characters not separated by blanks to obtain the output sequence. (\u201cthe brown\u201d).\n\nYou might be using one of those speakers in the house that respond to you when you call them by \u201cname\u201d and perform a particular action required at your request.\n\nYou know what I\u2019m talking about, right? The exotic-named assistants? I just don\u2019t want to put names out there and discriminate against a single company\n\nAnyway, those systems react to you when you say a \u201ctrigger\u201d word. For example let\u2019s use the word espresso as our trigger word\u2026. When you say \u201cespresso\u201d the voice assistant \u201cwakes up\u201d and listens to what you ask and does something for you.\n\nEver wondered how does that work? Well, the wait is over.\n\nHere is how you use an RNN model and train it to learn a trigger word.\n\nArchitecturally we have the audio clip transformed into spectrogram features that pass through an RNN. We define the target labels y as 0 or 1. In the training dataset, we put the target labels to be 0 for everything before the trigger word and 1 right after.\n\nIf we just put 1 after the end of the trigger word and 0 everywhere else it will cause the training set to be unbalanced, with a lot more of zeros than ones. A way to overcome this is to output 1 not just for a single time step, but for several times before reverting to 0.\n\nNow the model is trained to \u201cwake up\u201d to a trigger word and be ready to listen to what you say!"
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-sequence-to-sequence-architectures-4c4ca89e5654?source=---------1",
        "title": "DeepLearning series: Sequence-to-Sequence Architectures",
        "text": "This model architecture is useful for machine translation and speech recognition.\n\nLet\u2019s use the first application in an example. We want to translate an input text in French (x<1>, x<2>, \u2026, x<Tx>) to an output in English (y<1>, y<2>, \u2026, y<Tx>).\n\nThe RNN network will be built in two blocks: an encoder and a decoder.\n\nThe encoder network is built as an RNN (which could be GRU, LSTM) where we feed the French words, one word at a time. After ingesting the input sequence, the RNN then offers a vector that represents the input sequence. After that, there is a decoder network, which takes as input the encoding output, which is spit out by the encoder network, and then can be trained to output the translation one word at a time until it outputs the end of the sequence.\n\nThe decoder here acts mainly as a language model, where it estimates the probability of a sentence. Instead of having an input of all zeros as in the language model though, here the input is represented by the output vector of the encoder network.\n\nIt is called \u201cconditional language model\u201d, as instead of modeling the probability of any sentence, it is now shaping the likelihood of the output (English translation) conditioned on some input sentence (French text).\n\nThe model gives the probability of different English translations, so it doesn\u2019t sample the outputs at random, but it finds the English sentence that maximizes that conditional probability.\n\nSo what we need now is an algorithm that maximizes this. The most common is the Beam Search algorithm.\n\nThe Greedy Search algorithm wouldn\u2019t be a good choice here. To generate the first translated word (y<1>), it will pick the most likely first word according to the conditional language model and so on for the following words.\n\nWhat we want, instead, is to pick the entire sequence of words (y<1>, y<2>, \u2026, y<Tx> ) that maximizes the joint probability.\n\nAs a first step, we define a parameter B (beam width), which represents the number of words we want the algorithm to consider as the first word.\n\nThen, having picked the B number of choices for the first word, for each of these choices the algorithm considers what could be the second word. The goal is to find the most likely pair of the first and second word, considering B*vocabulary_length possibilities.\n\nThen it continues with the same approach for every subsequent word in the translation output. If B is equal to 1, this algorithm becomes the \u201cgreedy search\u201d.\n\nHow do we choose the parameter B?\n\nWell, the larger B is, the more possibilities the algorithm is considering and therefore the better the sentence it finds. But obviously, this is done at the cost of computations. So, a large B yields to better results, but with a slower implementation. On the other hand, a small B gives worse results, but it runs faster.\n\nFor an application that we run in production a good choice of B is around 10. There is one change we can implement to help optimize the Beam Search algorithm, the \u201clength normalization\u201d.\n\nThe Beam Search algorithm is trying to maximize the product of the probabilities as we saw earlier. To generalize, the formula is:\n\nAs all the probabilities are less than one, the above formula leads to a very small number. This can cause problems for the computer to store this value accurately, so in practice instead of maximizing this product we take the log of it:\n\nThe result (selecting the most likely sentence) is the same as before. However, by taking the log, we end up with a more numerically stable algorithm, which is less prone to rounding errors.\n\nFinally, let\u2019s focus on one aspect that affects the results even more. If we have a very long sentence, the probability of that sentence is going to be low because we have many multiplications to compute, so the final number is smaller than for a shorter sentence.\n\nSo, we end up with the unintended effect that the algorithm prefers short sentences translations since the multiplications are fewer, and the final probability is higher.\n\nWell, that\u2019s not our objective, though. We want accurate translations, independent of the length of the output sentence.\n\nThe way to overcome this is to normalize the formula above by the number of words in the output. Therefore, the algorithm takes the average of the log of the probability of each word, and significantly reduces the penalty for outputting longer sentences.\n\nIn practice, though, we take a softer approach and instead of dividing the log by the number of words in the output sentence (Ty) we divide by Ty to the power of \u03b1, where this parameter is set to 0.7.\n\nUnlike exact search algorithms, such as the breadth-first search (BFS) or depth-first search (DFS), the beam search algorithm is an approximate search model, and doesn\u2019t always find the exact maximum. Thus, it doesn\u2019t always output the most likely sentence.\n\nA few things play a role in that. Two in particular: the value of B that we choose and the type of RNN network.\n\nIf we were trying to optimize the results, we should discern the two and find out the cause of the error.\n\nUsing the RNN model we compute the probabilities P(Y*|X) and P(\u0176 |X) and compare the two.\n\nIf P(Y*|X) > P(\u0176 |X), the translation is not the best one out there and, therefore, the beam search algorithm is at fault and we should increase B, so we have more opportunities for the algorithm to choose a better output.\n\nOn the other hand, if P(Y*|X) <= P(\u0176 |X), then despite the fact that Y* is the better translation than \u0176, the RNN wrongly predicted a higher probability for \u0176. So the RNN model is not performing as it should. We can try to use regularization, change the network architecture, etc. to fix the model.\n\nIn language translation, differently than image recognition, there is not one single right answer. Meaning, for a given French sentence, for example, there could be multiple good English translations, which are comparable to the human translation.\n\nThat happens for us too; different humans translate correctly in different ways. So, let\u2019s cut the machine some slack!\n\nThere is a tool, called Bleu Score that helps us measure the accuracy of multiple results and evaluate the different outputs.\n\nGiven a machine-generated translation, the Bleu Score allows us to automatically compute a score that measures how good that machine translation is.\n\nThe intuition behind this method is to measure accuracy against the human translation and see if the words generated by the machine appear in at least one of the human-generated references.\n\nHowever, there is a flaw in this score. If a word is repeated many times, then it will be counted as much, which is not a good measure of precision.\n\nTherefore the tweak is to introduce the \u201cmodified precision\u201d, which gives each word credit only up to the maximum number of times it appears in the reference sentences.\n\nOne additional modification would be to use this not just for single words in isolation, but also for pairs of words. This method is called \u201cBleu Score on bigrams\u201d."
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-sentiment-classification-d6fb07b0da43?source=---------2",
        "title": "DeepLearning series: Sentiment Classification \u2013 Machine Learning bites \u2013",
        "text": "Great use of word embeddings (see previous blog) is done by a sentiment classification application.\n\nIt entails the machine to \u201cread\u201d a piece of text and deduct the intention of the writer; if he/she likes or not what he/she is talking about.\n\nOne challenge is the fact that we might not have a massive training set, but luckily, with word embeddings, we can build a good sentiment classifier even with a small size labeled training set.\n\nOne implementation can be done through a simple model. We take each word and, as usual, we create a one-hot representation, multiply it by the embedding matrix E, which has learned from a large text corpus, and used that to extract the embedding vector for each word.\n\nWe then average out (or sum) all the embedding vectors and run that through a Softmax which spits out the probability of the outcomes, which, for example, could be the star ratings from 1 to 5.\n\nThe problem with this simple model is that it ignores the word order. While it considers the sum or average, it will fail to recognize when a review is negative.\n\nAn example could be:\n\n\u201cThe restaurant was lacking in good service, good ambiance, good food and good price\u201d. The above model counts the number of times the word \u201cgood\u201d appears and it translates that sentence as a positive review. We know better, though!\n\nInstead of using the average or sum vector, we can use an RNN.\n\nThis will have the many-to-many architecture, and it will be able to \u201clisten\u201d to the whole sentence before spitting the output.\n\nWe have seen how the embedding matrix is trained on a big text corpus, which ultimately defines the features for each word. The way to do this is to run a lot of text taken from the internet.\n\nWe can predict how the machine, like a toddler, might learn from this big corpus, the \u201cadults\u201d, many things, together with some bias.\n\nLuckily researchers have been working on fixing these biases in word embeddings, with some success.\n\nLet\u2019s take for example the gender bias and see what tools we have to de-bias this from words.\n\nFor the gender bias, we take the embeddings of the vector for \u201che\u201d and subtract the embeddings vector for \u201cshe\u201d, because the only difference between the two words is the gender.\n\nWe do this for a few more pairs of words where the only discriminant is the gender (i.e., \u201cmale\u201d-\u201cfemale\u201d, \u201cboy\u201d-\u201cgirl\u201d\u2026) and average them out.\n\nThis will help us in figuring out the direction of bias, such as:\n\nTherefore, the bias direction will be a 1-dimensional subspace, while the non-bias one will be a 299D space.\n\nThere is another, more complicated way, to find the direction (higher than 1-dimensional) and it involves the use of the SVU (singular value decomposition) algorithm, which uses an idea similar to the PCA (principal component analysis) used in machine learning for feature selection. (link here)\n\nBut let\u2019s keep things simple, and move on to the next step.\n\nAs we can see from the image above, in some words, called \u201cdefinitional words\u201d, the gender is intrinsic (i.e., girl, boy, \u2026.), while others, instead, we want them to be neutral (i.e., doctor, babysitter, \u2026)\n\nTo neutralize the gender from those we have to reduce the component in the bias direction and therefore project the vector to the non-bias axis.\n\nFor pairs of definitional words such as boy/girl, grandmother/grandfather, we want their difference in their embeddings to be just the gender. Therefore the distance between them and other words should be the same.\n\nFinally, how do we know which words to neutralize? Some researchers (paper: https://arxiv.org/abs/1607.06520) trained a classifier to try to figure out the words that are definitional and the ones that are not. In the English dictionary most of the words are not definitional, so the word pairs we need to equalize is a relatively small subset, so it\u2019s feasible to hand-pick and to adjust them."
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-natural-language-processing-and-word-embeddings-70599080efc9?source=---------3",
        "title": "DeepLearning series: Natural Language Processing and Word Embeddings",
        "text": "In the previous blog related to Sequence Models and Language model generation, we have used a dictionary of words, and each word was represented by a one-hot representation. For example, the word \u201cwoman\u201d was word number 5678 in the dictionary, and the one-hot representation was a vector of zeros with a 1 in the 5678th position.\n\nThe most significant weakness of this representation is that each word is treated as a thing by itself and there is nothing that can help generalize and create an analogy reasoning.\n\nFor example, if we tell the machine \u201cmen is to woman as boy is to ___?\u201d it won\u2019t be able to deduct that analogy and quickly say \u201cgirl\u201d. For the machine, girl is a number by itself, with no correlation whatsoever with anything else. This is undoubtedly a huge drawback because in natural language processing making analogies to predict a word is crucial.\n\nWord embeddings are what creates a feature representation of every word. Therefore it allows a correlation among words. Each word is a vector of a certain dimension, which represents some features.\n\n(I will refer to 300 features throughout the rest of this blog, which is a sort of a standard). Therefore, now similar words will have same values of features.\n\nRegarding features think about characteristics you would give to a word. For example age, size, gender, height. The algorithm to learn word embeddings examines very large text corpora, so big datasets (of around 1\u2013100 billion words) of unlabeled texts.\n\nIn case you have a small training dataset, you can take a pre-trained embeddings and use transfer learning to transfer the embeddings to the new task. Finally, you can fine-tune the word embeddings using the small dataset you have at hands.\n\nIn a way, word embeddings is similar to face encoding that we saw in a previous blog (link here), where a face was \u201ctranslated\u201d into a vector of numbers.\n\nOf course, a 300-feature representation of a word is a non-intuitive representation of a word. Luckily, there is an algorithm (t-SNE algorithm) that maps and visualizes this 300 data vector in a non-linear way to 2-dimensional space, allowing us to see clusters of words related to each other. Such as:\n\nThe most commonly used similarity function for inferring analogy reasoning is the \u201ccosine similarity\u201d, which measures the cosine between two vectors u and v:\n\nWhen we implement an algorithm to learn word embeddings, what we end up learning is an embedding matrix.\n\nFor a 300-feature embedding and a 10,000-word vocabulary, the matrix E will be (300x10,000). To learn the Matrix E, we initialize it randomly and then use gradient descent to learn all of its parameters.\n\nEach word will be represented by the embeddings vector \u201ce\u201d such as:\n\nFor example, if woman is the 5678th word within the dictionary, its embeddings vector is the multiplication of the embedding matrix by the one-hot representation of the word:\n\nIn practice, performing this multiplication, which is very inefficient since there are many zeros within the one-hot vector, we use a specialized function that just looks up a column of the matrix E.\n\nLet\u2019s see more in detail different algorithms we can implement to learn the matrix E parameters.\n\nFor each training sentence we construct the one-hot vector for each word and multiply that by the matrix E to get the embeddings vector \u201ce\u201d.\n\nNow we feed all these 300-dimensional vectors, representing each word within the sentence, to a neural network, which feeds into a softmax. The softmax classifies among all the 10,000 (dictionary size) possible outputs.\n\nThe parameters of the algorithms are the matrix E, the weights, the bias of the layer of the network and the softmax. We then use back propagation with gradient descent to maximize the likelihood of the training set, given the input words, what the next word would be.\n\nIf our goal is to predict the word embeddings, then you normally feed the entire sentence. If instead, the purpose is to build a language model, then we use the previous few words (i.e., four) from the target word. See below:\n\nThis is a simpler and computationally more efficient algorithm to learn word embeddings rather than the neural language model. Word2Vec is a group of models that try to represent each word as a vector, making similar words also be close to each other. One of these models is the Skip-Gram model.\n\nThe setup is quite simple. We have a \u201ccontext\u201d, which is composed of a randomly picked word. Then a \u201ctarget\u201d, which is a word picked within a defined window from the context.\n\nThis creates a supervised learning problem where, given the context word, you are asked to predict what is a randomly chosen word within a certain window.\n\nWe are going to train a simple neural network with a single hidden layer, to perform this supervised learning problem. We are not going to use the network for the task we trained on, but instead to learn the weights of the hidden layer, which are the word embeddings we are interested in gathering.\n\nSo, given a specific word in the middle of the sentence (the \u201ccontext\u201d word), we look at the words nearby and pick one at random (the \u201ctarget\u201d word). The network is going to tell us the probability for every word in the vocabulary of being the \u201ctarget\u201d word that we chose.\n\nThe output of the softmax in indeed:\n\ntheta_t is the parameter associated with the output t (a chance of the particular word \u201ct\u201d of being the correct one).\n\nec is the embedding vector of the context word.\n\nThe network is, therefore, trained by giving it word pairs taken from the sentence with the context-target method described above and represented below:\n\nAs mentioned, the input to the network is the context word, obviously the one-hot representation of it. So the context word is a vector with 10,000 positions since our vocabulary is composed of 10,000 words.\n\nFor training, the input is a one-hot vector representing the input word, and the training output is also a one-hot vector representing the output word (the target word described above,) When you evaluate the trained network on an input word, the output vector will be a probability that a randomly selected nearby word is that vocabulary word.\n\nIf we use our \u201cstandard\u201d representation of 300 features for each word, then the hidden layer of the network will be represented by a weight matrix of dimension (10,000 x 300). These are the weights we are interested in learning. We don\u2019t care about the output of the network.\n\nIt\u2019s sort of a trick: we train a network for doing something, but we actually don\u2019t care about the output of this supervised learning problem. Instead, we use it to learn the parameters (the weights) within the network.\n\nThe intuition behind all this is that if two words are appearing around each other often, then the model will output similar results for these words. One way for the network to output similar predictions for these words is if the vectors are similar.\n\nSo think about the network learning that when it sees the word \u201corange\u201d then the word \u201cjuice\u201d is also \u201cin the neighborhood\u201d. One way to predict that also with the word \u201capple\u201d the word \u201cjuice\u201d is close by, is when \u201capple\u201d and \u201corange\u201d are similar.\n\nThen, finally, the network learns that \u201corange\u201d and \u201capple\u201d are similar vectors and predicts \u201cjuice\u201d when it sees either of them.\n\nI should mention a couple of problems when using the \u201cskip gram\u201d model. First of all, the computational speed. For the softmax to evaluate the probability, it consists in carrying out the sum over all the words within the vocabulary.\n\nOne solution is to use a \u201chierarchical softmax classifier\u201d. Instead of trying to categorize something into all the words within the vocabulary (i.e., 10,000 words) on one go, there is a tree classifier that tells if the target word is within the first 5,000, and so on.\n\nOne other issue with this model is related to how to choose the context word. In fact, if we pick it randomly we might end up picking the most frequent words (i.e., \u201cthe\u201d, \u201ca\u201d, \u201cof\u201d, \u2026). In this case, the network doesn\u2019t learn that much, as the training dataset doesn\u2019t contain \u201cinteresting\u201d word to learn.\n\nTherefore, instead of a random selection, there are different heuristics that you can use to balance out \u201ccommon\u201d words with more \u201cinteresting\u201d ones.\n\nThis model is quite similar to the skip-gram, but it\u2019s a more efficient learning algorithm. It still is set as a supervised learning problem like before, but with a little tweak.\n\nGiven a pair of words like \u201corange\u201d and \u201cjuice\u201d, we are going to predict if this is a context-target pair. We do so by generating a set of a positive and several (k) negative examples. So now our target will be a binary result (1 if the pair of words is a positive example, 0 otherwise).\n\nTo generate a positive example, we pick a context word, look around a window and pick the correct target word to go with it. To generate a negative example, instead, we take the context word and then a word at random from the dictionary. For a small dataset, the number of negative examples k is within a range of 5 to 20, while 2 to 5 for a large dataset.\n\nTherefore, we now have a logistic regression model:\n\nSimilar to before, the network is trained inputting the one-hot representation of the word and the output this time is 10,000 possible logistic regression classification problems. Instead of training all 10,000 of them on every iteration, we are only going to use (k+1) of them. (k total negative examples, one positive example).\n\nTherefore, instead of having a 10,000 positions softmax, we have 10,000 binary classification problems, which are much less expensive, computation-wise.\n\nFurthermore, the training is done on every iteration, on a small set of (k+1) binary classification problems.\n\nWhen sampling the words for negative examples, it is recommended to do it proportionally to the frequency of the word to the power of \u00be. Therefore, we avoid sampling the most frequent words in the sentence.\n\nIn the previous models, we were sampling pairs of words (context and target words) within the text, driven by a certain set window. The GloVe algorithm does this right off the bat; by counting the number of times, a word \u201ci\u201d appears in the context of \u201cj\u201d.\n\nNow we have seen how to learn word embeddings. In the following blog I will go over some interesting applications that use this feature."
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-sequence-models-7855babeb586?source=---------4",
        "title": "DeepLearning series: Sequence Models \u2013 Machine Learning bites \u2013",
        "text": "This blog will cover the different architectures for Recurrent Neural Networks, language models, and sequence generation. I will go over the details of Gated Recurrent Units (GRU) and Long-Short Term Memory Units (LSTM), which are part of a sequence model architecture.\n\nThere are various sequence models in the deep learning domain. Before we jump into those, let\u2019s see the applications we can apply these models to.\n\nLet\u2019s start with some notations that will help us throughout this blog. I\u2019ll use an example of an application for name identity recognition.\n\nTo represent words in a sentence, we come up with a vocabulary (dictionary) that lists all the words and assigns a sequential number to each one. You can find online dictionaries already prepared for you, which contain 100,000 words. If a word is not in the vocabulary, you can assign it to the <UNK> (\u201cunknown\u201d) token. Finally, we use a one-hot representation for each word as a vector of zeros and a one corresponding to the position of the word in the vocabulary list.\n\nTo learn the mapping from X to Y, we might use a standard neural network, where we feed the x<1>, x<2>, \u2026 x<t> to obtain y<1>, y<2>, \u2026 y<t>.\n\nThis doesn\u2019t work well. A couple of problems are present:\n\nWell, then I guess it\u2019s time to introduce the RNN and explain why it works best for applications dealing with sequence models. Here\u2019s the architecture:\n\nThe RNN scans through the data from left to right and the parameters used for each time step (Wax) are shared. The horizontal connections are governed by Waa parameters, which are the same for every time step. The Way\u2019s are the parameters that govern the output predictions.\n\nOne very significant characteristic that we notice is that when making a prediction y, the network uses information not only from the corresponding input x, but also all from the previous ones. For example, for prediction \u0177<3> it gets information not only from x<3>, but also from x<1> and x<2>.\n\nThe main weakness of this architecture is that it only uses information coming from earlier in the sequence and not anything that comes after. We\u2019ll soon see that there\u2019s a network for that: bidirectional RNN (BRNN).\n\nLet\u2019s see how the forward propagation looks for this network, so that we can familiarize with the architecture.\n\nWe can simplify the last two equations as such:\n\nFor each layer of the network, we will calculate the loss, and then sum the losses up to obtain the entire loss for the sequence.\n\nAfter that, we can compute the backpropagation, which in this network is called \u201cbackpropagation through time\u201d as we run back through the sequence. Yup, that\u2019s right, like Martin McFly in \u201cBack to the Future\u201d! Sorry I got too excited.\n\nI mentioned earlier that there are different types of RNNs. What we\u2019ve have seen so far is the \u201cmany-to-many\u201d architecture where Tx = Ty.\n\nFor an application such as sentiment classification we end up in a situation where Ty=1, so our RNN is of the \u201cone-to-one\u201d type, and the architecture is:\n\nOn the other hand, for an application as music generation, we have a \u201cone-to-many\u201d architecture, as the input can be an integer related to a genre, while the output is a piece of music:\n\nIn other applications, instead, when the input sequence length is different than the output sequence (Tx \u2260 Ty), the architecture of the RNN reflects a \u201cmany-to-many\u201d relationship. Think about a machine translation.\n\nAll right, now that we have seen the RNN models, it\u2019s time to put them into practice!\n\nOne of the most essential tasks in Natural Language Processing is \u201cLanguage modeling\u201d. Let\u2019s see how this is built.\n\nThis model is also used in speech recognition, where the machine listens to what a human says and predicts the correct sentence, based on the probability of one sentence versus the other.\n\nSo, I gave it away already\u2026. what a language model does is to estimate the probability of the particular sequence of words that it will output.\n\nThese are the steps we take to build the model:\n\n- Take a training set: a large corpus of English (or whichever language suits you) text.\n\n- Tokenize the input sentences. (This is what we did before when we assigned a token (a number) to each word from the vocabulary). Remember, if a word does not exist within the dictionary we can always replace it with the <UNK> token.\n\nThese steps should be straightforward, except for the last bullet point, which I am going to explain in detail. Let\u2019s start with the model architecture:\n\nAs you see a<1> makes a softmax prediction to try to figure out what is the probability of the first word \u0177<1>. Then in the second step, a<2> makes a softmax prediction given the correct first word (y<1>). All of the following steps make a prediction based on the correct words that come before them.\n\nAfter training, it can predict, given an initial set of words, what\u2019 s the chance of the next word. So, given a new sentence (y<1>, y<2>, y<3>) it can tell the probability of this sentence:\n\nNow that we have trained our RNN we can use it to generate novel sequences.\n\nThis generation is done with a little variant from the model above, allowing sampling of each word to generate noble sequences. Essentially, the input of each step, instead of being the y from the previous step will be a random sample from the previous step distribution. You see what I mean looking at the architecture below:\n\nA fun thing to do is, for example, to train a network on a Shakespearean text and then use sampling to generate a novel sentence \u201cinspired\u201d by Shakespeare. I know Shakespeare would be proud!\n\nSo far we have built RNN on a word level, meaning the vocabulary is composed of words. We can also build a character level RNN, where the vocabulary is comprised of the individual character of the alphabet.\n\nOne of the advantages of this method is that we never encounter an unknown word. On the other hand, a disadvantage constitutes the computational cost to train such a network as they deal with much longer sequences. Furthermore, character level models are not so good at capturing long-range dependencies, meaning how the earlier part of the sentence affects the later part.\n\nThroughout the previous examples, you might have noticed that the output \u0177 was mainly influenced by the values in the sequence close to it. On the other hand, there are situations when some sentences have long dependencies, meaning some words within the sentence are related to other ones much earlier in the sequence.\n\nThink about a sentence where you have a subject, followed by many words, and then finally we have the verb, which is depending on the earlier subject.\n\nBasic RNNs are not good at capturing these long-term dependencies.\n\nIt\u2019s like what we have seen in a deep neural network, where the network has a difficult time propagating back to affect the weights of earlier layers.\n\nExploding gradients in an RNN are rare, but when they happen, they can be catastrophic, as parameters get very large. So, in a way, it\u2019s kind of easy to spot when this is happening and fixing the situation.\n\nThe solution is to apply \u201cgradient clipping\u201d.\n\nTherefore, take a look at the gradient vectors, and if it\u2019s getting bigger than a set threshold, you can rescale the gradients.\n\nLet\u2019s focus on the most difficult problem: vanishing gradients. When the network \u201cforgets\u201d what happened earlier and does not propagate dependencies across the whole sentence.\n\nHere we are. GRU units have \u201cmemory cells\u201d that allow an RNN to capture much longer range dependencies.\n\nLet\u2019s see the difference of a GRU unit compared to the regular RNN unit below:\n\nSo the GRUn unit has a new variable called c, which is a \u201dmemory cell\u201d that provides a bit of memory to remember words even further along the sentence.\n\nAt every step, this memory cell c is overwritten by \u010d, computed using the activation function tanh of Wc.\n\nThe purpose of \u010d is to replace c through the use of a gate \u0393u, which takes a value between 0 and 1, and it basically decides whether or not we update c with \u010d.\n\nWhen \u0393u = 0 then c<t> = c<t-1> . Therefore the value of c<t> is maintained across many time steps. This allows overcoming the problem of vanishing gradients.\n\nOn the other hand, when \u0393u = 1 then \u010d <t> = c<t>.\n\nThe equations that govern this unit are:\n\nLSTM is another, even more powerful, unit to learn very long-range connections in a sequence. The unit consists of three gates: the \u201cforget\u201d, \u201cupdate\u201d and \u201coutput\u201d gate.\n\nThe forget gate plays the role of (1- \u0393u) that we saw in the GRU unit.\n\nAdditionally, this time, c<t> is different than a<t>.\n\nThese are the equations that govern the unit, followed by the visual architecture:\n\nAn LSTM unit is more powerful and flexible than a GRU unit, and it\u2019s the more proven choice. GRU units, instead, are more recent and more straightforward, so it\u2019s easier to build bigger models with them.\n\nThese networks take not only information from earlier in the sequence, but also from later on. They are basically like a patient listener. They \u201clisten\u201d to the whole sentence before making a prediction, which is nice, generally speaking. Not a lot of humans can do that!\n\nTheir disadvantage comes precisely from that characteristic, though, as they need the entire sequence of data before predicting. And we are impatient, as we want machines to respond quickly\u2026 we can\u2019t help ourselves.\n\nThe graph is acyclic, and it is represented this way:\n\nAs you might have predicted, like in a CNN, we can have multiple hidden layers throughout the network. Here we have a stack of multiple layers of RNNs.\n\nThese networks can learn complex functions. We generally don\u2019t have a lot of layers (three is a lot), because of the temporal dimension."
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-face-recognition-383b1558a99e?source=---------5",
        "title": "DeepLearning series: Face Recognition \u2013 Machine Learning bites \u2013",
        "text": "In this write-up, I will describe the techniques to build a face recognition system, through the use of neural networks.\n\nThere are two aspects in the field of face recognition that we should separate:\n\nIn terms of \u201cface verification\u201d we are dealing with a 1-to-1 problem where we have:\n\n\u201cFace recognition\u201d, instead, is a 1-to-k problem where we have:\n\nImplementing a \u201cface verification\u201d system is solving a \u201cone-shot learning problem\u201d. Let\u2019s see different ways to do that:\n\nOne approach is to implement a ConvNet where we have input images and the output unit is reflecting the number of people we are verifying.\n\nThis doesn\u2019t work well, as for every new person we need to change the output of the network and re-train the CNN from scratch.\n\nAnother approach is to learn a \u201csimilarity\u201d function, which depicts the degree of difference between two images:\n\nThe value of this function will be small if img1 and img2 are of the same person, and bigger otherwise. This is a good way of thinking\u2026but how do we implement this d(img1, img2) ?\n\nAnswer: with the Siamese network, of course!\n\nWe create a ConvNet that outputs a feature vector as shown below:\n\nwhere f(x(1)) is the \u201cencoding\u201d of x(1) and it represents the picture as a vector of numbers (which, in this case, is 128).\n\nNow we feed a second picture to a network with the same parameters and we get a vector that identifies that image.\n\nWe define the similarity function as:\n\nOk, now we got to find out the way to create our similarity function, but we created a new hurdle. How do we get a good encoding?\n\nThis is a good objective function to learn the parameters of the Neural Network so that it gives us a good encoding for the pictures of the faces.\n\nWhat we do is to always look at three images at a time.\n\nSo now we have an \u201canchor\u201d image (A), a positive picture (P), which is the same picture of the person we want to represent, and finally a negative picture (N) of a completely different person.\n\nWhat we want to achieve is:\n\n(\u03b1 is a margin that we use to avoid a zero result, which would actually be satisfied otherwise by even an empty image. So, this avoids the network to output a trivial solution).\n\nThe loss function for a single triplet is then computed as:\n\nand the overall cost function over a training set is:\n\nFinally we train the neural network using gradient descent on this cost function.\n\nTo get better training results it\u2019s important to choose triplets that are \u201chard\u201d to train on. So where d(A,P) is close to d(A,N).\n\nAfter having trained the system (through the use of multiple pictures), we can apply it to a one-shot learning problem where we can have only one single picture of the person we want to recognize.\n\nThere is also another way, besides the \u201ctriplet loss\u201d, to learn the parameters of the neural network. It is done by looking at face verification as a binary classification problem.\n\nWe create two Siamese networks that have the same parameters. Both of them output the encoding for the image and we use those as inputs to a logistic regression unit. The output will be 1 if the input images are the same person or 0 otherwise."
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-objection-detection-and-localization-yolo-algorithm-r-cnn-71d4dfd07d5f?source=---------6",
        "title": "DeepLearning series: Objection detection and localization \u2014 YOLO algorithm, R-CNN",
        "text": "In the previous blog I explained the theory behind and how a Convolutional Neural Network works for a classification task. Here I will go a step further and touch on techniques used for object detection and localization, such as the YOLO algorithm and Regional Convolutional Neural Networks.\n\nWith object localization the network identifies where the object is, putting a bounding box around it.\n\nThis is what is called \u201cclassification with localization\u201d. Later on, we\u2019ll see the \u201cdetection\u201d problem, which takes care of detecting and localizing multiple objects within the image.\n\nBut first things first.\n\nFor an object localization problem, we start off using the same network we saw in image classification. So, we have an image as an input, which goes through a ConvNet that results in a vector of features fed to a softmax to classify the object (for example with 4 classes for pedestrians/cars/bike/background). Now, if we want to localize those objects in the image as well, we change the neural network to have a few more output units that encompass a bounding box. In particular, we add four more numbers, which identify the x and y coordinates of the upper left corner and the height and width of the box (bx, by, bh, bw).\n\nThe neural network now will output the above four numbers, plus the probability of class labels (also four in our case). Therefore, the target label will be:\n\nWhere pc is the confidence of an object to be in the image. It responds to the question \u201cis there an object?\u201d Instead, c1,c2,c3, in case there is an object, tell if the object is part of class 1, 2 or 3. So, it tells us which object it is. Finally, bx, by, bh, bw identify the coordinates related to the bounding box around the detected object.\n\nFor example, if an image has a car, the target label will be:\n\nIn case the network doesn\u2019t detect an object, the output is simply:\n\nWhere the question marks are placed in the rest of the positions that don\u2019t provide any meaning in this case. Technically the network will output big numbers or NaN in these positions.\n\nThis technique is also used for \u201cLandmarks detection\u201d. In this case, the output will be even bigger since we ask the network to output the x and y coordinates of important points within an image. For example, think about an application for detecting key landmarks of a face. In this situation, we could identify points along the face that denote, for example, the corners of the eyes, the mouth, etc.\n\nObject detection can be performed using a technique called \u201csliding window detection\u201d. We train a ConvNet to detect objects within an image and use windows of different sizes that we slide on top of it. For each window, we perform a prediction.\n\nThis method gives pretty good results (I have used it for a project related to self-driving cars, and it achieved great outcomes. Check it out: here).\n\nThe big downside of it is the computational cost, which is very extensive since we can have a lot of windows. The solution to that is the sliding window detection computed convolutionally.\n\nInstead of sliding a small squeegee to clean a window, we now have a big one that fits the entire window and magically cleans it completely without any movement.\n\nThe first step to build up towards the convolutional implementation of sliding windows is to turn the Fully Connected layers in a neural network into convolutional layers. See example below:\n\nGreat, now to simplify the representation, let\u2019s re-sketch the final network in 2D:\n\nIf our test image is of dimension 16x16x3 and we had to perform the \u201cregular\u201d sliding window we would have to create 4 different windows of size 14x14x3 out of the original test image and run each one through the ConvNet.\n\nThis is computationally expensive and a lot of this computation is duplicative. We would like, instead, to have these four passes to share computation.\n\nSo, with the convolutional implementation of sliding windows we run the ConvNet, with the same parameters and same filters on the test image and this is what we get:\n\nEach of the 4 subsets of the output unit is essentially the result of running the ConvNet with a 14x14x3 region in the four positions on the initial 16x16x3 image.\n\nYou might be wondering if this works on other examples too, and it does.\n\nThink about an input image of 28x28x3. Going through the network, we arrive at the final output of 8x8x4. In this one, each of the 8 subsets corresponds to running the 14x14x3 region 8 times with a slide of 2 in the original image.\n\nOne of the weaknesses of this implementation is that the position of the bounding box we get around the detected object is not overly accurate.\n\nWe will soon see that the YOLO algorithm is the solution to that.\n\nWe start with placing a grid on top of the input image. Then, for each of the grid cells, we run the classification and localization algorithm we saw at the beginning of the blog. The labels for training, for each grid cell, will be similar to what we saw earlier, with an 8-dimensional output vector:\n\nFor each cell, we will get a result whether there is an object or not. For example:\n\nThe object is \u201cassigned\u201d to the specific cell looking to where the center falls.\n\nIf we have a 3x3 grid cell, then the target output volume will have a dimension of 3x3x8 (where 8 is the number of labels in y). So, in this case, we will run the input image through a ConvNet to map to an output of 3x3x8 volume.\n\nSo we have a convolutional implementation for the entire grid cells (not 9 individual ones), as we saw earlier. We, therefore, combine what we saw in the localization classification algorithm with the convolutional implementation.\n\nThe advantage of this algorithm is that it outputs precise positions of bounding boxes, as the values bx, by, bh, bw are computed relative to the cell. So, the finer grid we have the more precision we can obtain and also we have fewer chances of having multiple objects within a cell.\n\nThis is a way of measuring if the object detection algorithm is working well.\n\nIt computes the intersection over the union of the detected bounding box and the correct one.\n\nWe identify a benchmark and consider an accurate object detection if the result of IoU is above that specific value. (i.e. IoU <= 0.5)\n\nClearly, the higher the IoU value, the more accurate results we have.\n\nThis technique is used to make our YOLO algorithm perform better.\n\nIn fact, YOLO could detect an object multiple times, since it\u2019s possible that many grid cells detect the object. To avoid that, we take the following steps:\n\nFirst, we assign a probability on each detection, then we take the \u201clargest probability\u201d box. We now look at the boxes that overlap the most with the \u201clargest probability\u201d box and remove the ones that have high IoU (so the ones that have a big area of intersection). Finally, the remaining box is the correct detection.\n\nRemember that each prediction comes with a value pc, which identifies the prediction probability. We now discard, for example, all the boxes with pc <= 0.6.\n\nWhile there are any remaining boxes then we do:\n\nIf we have multiple classes (objects), then we implement non-max suppression independently for each one.\n\nOne of the problems with object detections as we have seen so far is the fact that each grid cell can only detect one object. If we have instead multiple objects in the same cell, the techniques we have used so far won\u2019t help to discern them. Anchor boxes will help us overcome this issue.\n\nThe idea here is to predefine different shapes (called anchor boxes) for each object and associate predictions to each one of them. Our output label now will contain 8 dimensions for each of the anchor boxes we predefined.\n\nIf we chose two anchor boxes, then the class label will be:\n\nSo each object in the training image was assigned to the grid cell that contained that object\u2019s midpoint (for a 3x3 grid, the output was 3x3x8). Now, each object in the training image is assigned to the grid cell that contains that object\u2019s midpoint and the anchor box for the grid cell with highest IoU.\n\n(for a 3x3 grid and 2 anchor boxes, the output is 3x3x16).\n\nThe only thing it can not handle well is in case two objects in the same cell have the same anchor box. Additionally, we get to choose and redefine the shape of the anchor boxes.\n\nPutting it all together for YOLO\n\nThis algorithm tries to pick few regions within the image, which make sense to run the classifier. As for some regions of the image that contain no objects, it makes no sense to run the ConvNet classifier.\n\nSo first we need to find a way to find out where the objects are. We can do so by running a segmentation algorithm, which identifies blobs around objects. Then, we place a bounding box around each blob and run the classifier for each of these bounding boxes. It is a pretty slow algorithm as it proposes some regions and it classifies them one at a time.\n\nTo speed it out there, has been proposed the \u201cfast R-CNN\u201d algorithm. For this one, we still have the first step, which proposes the regions, but then it uses the convolution implementation of sliding windows to classify all the proposed regions.\n\nWell, the first step is still a bit annoyingly slow, right?\n\nWhy not a \u201cfaster R-CNN\u201d? Yes, it exists.\n\nThis one replaces the first step with the use of a convolutional network to propose regions."
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-convolutional-neural-networks-a9c2f2ee1524?source=---------7",
        "title": "DeepLearning series: Convolutional Neural Networks \u2013 Machine Learning bites \u2013",
        "text": "In this blog, I will explain the details of Convolutional Neural Networks (CNNs or ConvNets), which have proven to be very effective in areas such as image recognition and classification.\n\nFinally, I will move on to Residual and Inception networks, which help overcome issues related to training very deep networks.\n\nComputer vision is an exciting field, which has evolved quickly thanks to deep learning. Researchers in this area have been experimenting many neural-network architectures and algorithms, which have influenced other fields as well.\n\nIn computer vision, images are the training data of a network, and the input features are the pixels of an image. These features can get really big. For example, when dealing with a 1megapixel image, the total number of features in that picture is 3 million (=1,000 x 1,000 x 3 color channels). Then imagine passing this through a neural network with just 1,000 hidden units, and we end up with some weights of 3 billion parameters!\n\nThese numbers are too big to be managed, but, luckily, we have the perfect: Convolutional neural networks (ConvNets).\n\nThere are 3 types of layers in a convolutional network:\n\nA \u201cconvolution\u201d is one of the building blocks of the Convolutional network. The primary purpose of a \u201cconvolution\u201d in the case of a ConvNet is to extract features from the input image.\n\nEvery image can be represented as a matrix of pixel values. An image from a standard digital camera will have three channels \u2014 red, green and blue. You can imagine those as three 2d-matrices stacked over each other (one for each color), each having pixel values in the range 0 to 255.\n\nApplying a convolution to an image is like running a filter of a certain dimension and sliding it on top of the image. That operation is translated into an element-wise multiplication between the two matrices and finally an addition of the multiplication outputs. The final integer of this computation forms a single element of the output matrix.\n\nLet\u2019s review this via an example, where we want to apply a filter (kernel) to detect vertical edges from a 2D original image.\n\nThe value 1 on the kernel allows filtering brightness, while -1 highlights the darkness and 0 the grey from the original image when the filter slides on top.\n\nIn the above example, I used a value of a stride equal to 1, meaning the filter moves horizontally and vertically by one pixel.\n\nIn this example the values of the filter were already decided in the convolution. The goal of a convolutional neural network is to learn the number of filters. We treat them as parameters, which the network learns using backpropagation.\n\nYou might be wondering how to calculate the output size, based on the filter dimensions and the way we slide it though the image. I will get to the formula, but first I want to introduce a bit of terminology.\n\nYou saw in the earlier example how the filter moved with a stride of 1 and covered the whole image from edge to edge. This is what it\u2019s called a \u201cvalid\u201d convolution since the filter stays within the borders of the image. However, one problem quickly arises. When moving the filter this way we see that the pixels on the edges are \u201ctouched\u201d less by the filter than the pixels within the image. That means we are throwing away some information related to those positions. Furthermore, the output image is shrinking on every convolution, which could be intentional, but if the input image is small, we quickly shrink it too fast.\n\nA solution to those setbacks is the use of \u201cpadding\u201d. Before we apply a convolution, we pad the image with zeros all around its border to allow the filter to slide on top and maintain the output size equal to the input. The result of padding in the previous example will be:\n\nI talked about \u201cstride\u201d, which is essentially how many pixels the filter shifts over the original image. Great, so now I can introduce the formula to quickly calculate the output size, knowing the filter size (f), stride (s), pad (p), and input size (n):\n\nKeep in mind that the filter size is usually an odd value, and if the fraction above is not an integer you should round it down.\n\nThe previous example was on a 2D matrix, but I mentioned earlier that images are composed of three channels (R-red, G-green, B-blue). Therefore the input is a volume, a stack of three matrices, which forms a depth identified by the number of channels.\n\nIf we apply only one filter the result would be:\n\nwhere the cube filter of 27 parameters now slides on top of the cube of the input image.\n\nSo far we have only applied one filter at a time, but we can apply multiple filters to detect several different features. This is what brings us to the crucial concept for building convolutional neural networks. Now each filter brings us its own output We can stack them all together and create an output volume, such as:\n\nTherefore, in general terms we have:\n\nThe final step that takes us to a convolutional neural layer is to add the bias and a non-linear function.\n\nRemember that the parameters involved in one layer are independent of the input size image.\n\nSo let\u2019s consider, for example, that we have 10 filters that are of size 3x3x3 in one layer of a neural network. Each filter has 27 (3x3x3) + 1 bias => 28 parameters.\n\nTherefore, the total amount of parameters in the layer is 280 (10x28).\n\nWe are now ready to build a complete deep convolutional neural network.\n\nThe following architecture depicts a simple example of that:\n\nThere are two types of pooling layers: max and average pooling.\n\nWe define a spatial neighborhood (a filter), and as we slide it through the input, we take the largest element within the region covered by the filter.\n\nAs the name suggests, it retains the average of the values encountered within the filter.\n\nOne thing worth noting is the fact that a pooling layer does not have any parameters to learn. Of course, we have hyper-parameters to select, the filter size and the stride (it\u2019s common not to use any padding).\n\nA fully connected layer acts like a \u201cstandard\u201d single neural network layer, where you have a weight matrix W and bias b.\n\nWe can see its application in the following example of a Convolutional Neural Network. This network is inspired by the LeNet-5 network:\n\nIt\u2019s common that, as we go deeper into the network, the sizes (nh, nw) decrease, while the number of channels (nc) increases.\n\nAnother common pattern you can see in neural networks is to have CONV layers, one or more, followed by a POOL layer, and then again one or more CONV layers followed by a POOL layer and, at the end, a few FC layers followed by a Softmax.\n\nWhen choosing the right hyper-parameters (f, s, p, ..), look at the literature and choose an architecture that was successfully used and that can apply to your application. There are several \u201cclassic\u201d networks, such as LeNet, AlexNet, VGG, \u2026\n\nI won\u2019t go into details for each one, but you can easily find them online.\n\nThese networks are normally used in transfer learning, where we can use the weights coming from the existing trained network and then replace the output unit, since training such a big network from scratch would require a long time otherwise.\n\nVery deep neural networks are difficult to train because of vanishing and exploding gradients. A solution to that is the use of \u201cskip connections\u201d, which allow you to take the activation function from one layer and feed it to another layer even much deeper in the network. Skip connections are the building blocks of a Residual Network (ResNet).\n\nwhich is computed as:\n\nTherefore, with a residual block, instead of taking the regular, \u201cmain\u201d path, we take a[l] and add it to a later layer before applying the non-linearity ReLu.\n\nA Residual Network is composed of these residual blocks stacked together to form a deep network. In reality, a \u201cplain\u201d, regular network has a harder time to train, as the network gets deeper, so the training error increases. The advantage of using a Residual Network, instead, is to train a very deep network and keep decreasing the training error.\n\nWe saw on the previous example of convolutional neural networks that applying a pooling layer we essentially shrink the dimension nh and nw. By applying, instead, a 1x1 convolution, we can shrink the number of channels, therefore save on computation. Furthermore, it adds non-linearity to the network.\n\nLet\u2019s see the following example:\n\nHere, the 1x1 convolution looks at each 36 (6x6) different positions, and takes the element-wise product between 32 numbers on the left and 32 number in the filter, and then it applies a ReLU non-linearity to it. If we apply \u201cnc\u201c number of 1x1 convolutional filters, then the output will be, in our example, 6x6xnc.\n\nWhen designing a layer for a ConvNet we need to pick many things: the number of filters, the type of layer (pooling, conv, etc..),\u2026 what if we didn\u2019t have to choose, but get them all?\n\nThat\u2019s what the Inception network does. It uses all these options and stacks them up:\n\nThe problem of applying the above architecture is the computational cost.\n\nFor example, looking at the computational cost of the 5x5 filter we have:\n\n32 filters and each filter is going to be 5x5x192. The output size is 28x28x32.\n\nSo, the total number of multiplications it computes is:\n\nFortunately, there is a way to reduce the above number of computations. That comes with some little help from our friend described above, the 1x1 convolution.\n\nThe interposed 1x1 convolution reduces by 10 times the total computational cost since we have:\n\nTherefore, an inception network is built interposing 1x1 convolutions on the filters and stacking these modules together."
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-how-to-structure-machine-learning-projects-ae484c0919c3?source=---------8",
        "title": "DeepLearning series: How to structure machine learning projects",
        "text": "In this blog, I will explain how to structure a machine learning project and some useful techniques for deep learning, such as transfer learning, multi-task, and end-to-end learning.\n\nIn a previous blog I mentioned how many strategies and parameters are involved in a machine learning project. In particular, when we want to optimize the results of our algorithm we have several options such as:\n\nThis can be daunting especially because taking one step could mean spending time only to realize it wasn\u2019t a good choice to begin with.\n\nWhat we want is quick advice on which idea is worth pursuing. That is, which knob we want to turn to achieve a particular effect. (This is what they call \u201corthogonalization\u201d, which sounds intimidating in itself!)\n\nWhen tackling a machine learning project, the first thing we want is good performance on the training set (for some applications, like image recognition, this might mean reaching human-level performance). Then move on to fix the dev set, the test set and then finally perform well in the real world.\n\nIn summary, these are the things we can tweak to improve each of those steps:\n\nYou might be wondering, \u201chow do I know if things are going well?\u201d\n\nThanks for asking! Yeah, that\u2019s right, we need a metric to evaluate things!\n\nYou might read in the published literature that to evaluate the performance of a classifier you look at the two evaluation metrics: precision and recall.\n\nLet\u2019s remind ourselves what they are.\n\nFor example, take an image recognition classifier for cats.\n\nPrecision, in this case, measures what percentage of the examples recognized as cats are actually cats. Recall, on the other hand, tells us what percentage of the actual cats, are correctly classified as cats.\n\nThe problem of using both of these evaluation metrics is that it is difficult to discern which classifier works better. One could have better results regarding precision and another concerning recall.\n\nWouldn\u2019t it be best if we had only one evaluation metric? Maybe one that combines the two? Since you asked\u2026 yes! F1-score does that for us! It is essentially the average of precision (P) and recall (R), and it is calculated as:\n\nSometimes we want to consider other metrics as well (i.e., running time) and therefore one evaluation metric is not sufficient. In those situations, where we have several metrics, we can identify one of them to be the optimizing metric, and the rest consider as \u201csatisficing\u201d. The latter is essentially the \u201cgood enough\u201d value that we need to satisfy.\n\nFor example, we want an algorithm to run less than 100ms. In that case, our \u201csatisficing\u201d metric (running time <=100ms) will filter out all the algorithms that are running above that time. For the ones that are, instead, running less than 100 ms. we will then consider the other (optimizing) metric to discern which algorithm fits our requirements.\n\nDefining a metric to evaluate a classifier helps us place a target. How to do well on that metric is to aim and shoot at that goal.\n\nI mentioned earlier that for some applications recognizing the good performance of an algorithm could be done comparing human level results.\n\nThat is true for natural perception applications, such as image recognition, for example, but for the ones that involve a lot of data and, additionally, more structured data, even current algorithms are performing better than humans.\n\nSo the correct benchmark should be the \u201cBayes optimal error\u201d, which is the best possible error that can be achieved. In some cases, human error can be close to that but never above it.\n\nLet\u2019s evaluate the performance of our training/dev sets related to the Bayes error and see how we can identify the bias/variance problem, through the following example:\n\nAs we can see, the difference between the Bayes error and the training set denotes a problem related to high bias, while the gap between the training error and the dev set error is related to a high variance issue. So, fitting the training set lowers the avoidable bias, and when the training set performance generalizes well to the dev set, then we avoid high variance.\n\nI have mentioned in a previous blog some of the techniques to overcome the bias/variance problem, but it\u2019s worth repeating.\n\nThe above bias/error analysis only stands if the training and dev/test sets are coming from the same distribution.\n\nOn the other hand, if we were analyzing the same error gap as above (training error 8.0% and dev set error of 12%) we could no longer say for certainty that the model was affected by high variance because two things are now in place:\n\nLuckily, we have an option to fix this!\n\nWe need to create a new subset of data (called \u201ctraining-dev\u201d set), which is a portion of the training set (therefore it has the same distribution of the training set), but is not used for training.\n\nIn this way, we can discern the problem related to the different distribution from the different data, and we can analyze the errors appropriately.\n\nI guess it\u2019s worth mentioning how to address the data mismatch.\n\nUnfortunately, there are no systematic ways to do so. However, here are a couple of recommendations:\n\nIn the following section I will touch on some aspects of learning that can be carried out when working on a deep learning project:\n\nIn a way, it\u2019s similar to how humans gather knowledge: learn from one task and apply it to others.\n\nTo be more specific, for a neural network, we delete the last output layer and its weights, and replace them with a new layer (or even several new layers) as well as a new set of randomly initialized weights for the last portion of the network.\n\nAt this point, we can retrain the new network on the new dataset.\n\nTo be more precise, we need differentiate between two cases: a new small dataset or a new big dataset. In the first situation, we only re-train the weights of the last layer and keep the rest of the parameters fixed. This is called \u201cfine-tuning\u201d.\n\nOn the other hand, when applying transfer-learning on a new, big dataset, we re-train all the parameters of the network.\n\nIf we are trying to learn from task A to B, then transfer-learning makes sense if:\n\nFor example, we might have a lot of data taken for image recognition. We can use that trained network as \u201ctransfer learning\u201d for x-rays recognition, where instead we don\u2019t have many images to be able to train a network from scratch.\n\nWe have seen how in transfer-learning we have a sequential process from A to B. In multi-task learning, instead, we start off with one neural network trying to simultaneously do several things at the same time. Each of these tasks is also helping the other tasks.\n\nOne example of multi-task learning is to train a neural network to recognize several objects from an image, such as recognizing pedestrians, cars, stop signs and traffic lights.\n\nIn this case, the output would be of 4 categories:\n\nwhere Y can be:\n\n(The question mark will be put in a position where it can\u2019t identify that specific object).\n\nThe loss (for the entire training set m) will then be calculated over the values of the outputs (4 in this example):\n\nSo for each image, the output of the network tells us if the image contains a pedestrian/car/stop sign/traffic light.\n\nYou might be asking yourself, couldn\u2019t I train four separates network?\n\nYes, you could have. But if some of the earlier features in the neural network can be shared among these different types of objects, then training one neural network to do four things, results in better performance than training four separates networks to do four separate tasks.\n\nOn the other hand, if the network isn\u2019t big enough, then having separate neural networks works better. Finally, multi-task learning is not used too often, except for applications related to object detection. Transfer learning, instead, is more widely used.\n\nSome learning systems require multiple stages of processing. End-to-end learning, instead, takes all those multiple stages and replaces them with just a single neural network.\n\nThis type of learning is one of the most recent ones, and it is sometimes identified as a \u201cblack box\u201d since we don\u2019t pre-process data but let the network figure things out all by itself.\n\nTo understand what I mean, let\u2019s see some examples.\n\nWith a multi-stages approach the process would be:\n\nAnd end-to-end method instead feeds the audio and gets the transcripts directly:\n\nWhen applying end-to-end learning, the key is to have a lot of data for the network to learn a function of the complexity needed to map X to Y.\n\nThe Pros of using this method are:"
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-deep-neural-networks-tuning-and-optimization-39250ff7786d?source=---------9",
        "title": "DeepLearning series: Deep Neural Networks tuning and optimization",
        "text": "Now that we got our neural network model set up (check blog), I want to cover three aspects that help you optimize your network:\n\nHow do we know if the network we created is reliable in predicting an outcome?\n\nAs we saw at the end of the previous blog, training a neural network is an iterative process and finding good hyper-parameters helps reach faster and better results. But what\u2019s equally important is what and how you feed your network: your data. It\u2019s like us, humans, we can satisfy our hunger and get quickly back to work, but depending on the food we eat and how we eat it, can affect our performance.\n\nDeep neural networks are very hungry, and perform best with a lot of data!\n\nYou\u2019ve learned with previous \u201ctraditional\u201d machine learning models that we need to split our data into: training, development and test sets.\n\nWhy do we need them?\n\nThink about it this way: the development (\u201cdev\u201d) set is needed to evaluate the best algorithm to use, as it compares many; the test set, instead, gives us a confident estimate, that is, an unbiased estimate of the performance of the selected network.\n\nIn the \u201ctraditional\u201d machine learning field, where algorithms could work on relatively small datasets (i.e., around 10,000 samples) the rule of thumb was to get 70% of the data as the training set, 20% to the dev set and the last 10% for the test set.\n\nWith Deep Learning, instead, where big data are involved (i.e.,> 1,000,000 samples) the rule is to split the data into 98% training, 1% dev and 1% test.\n\nThis is because, as I said, a deep neural network is starving for data, so the more data we use for training, the better the network is. Also, sometimes, it\u2019s difficult to collect a lot of data, so you want to use the most of what you have.\n\nOne note is to make sure that the dev and test sets come from the same distribution, otherwise, we would be choosing an algorithm that works well on our dev set, but in the real world (test set) we get different the results.\n\nIt\u2019s like preparing for a sprint run (your dev set) but then being tested on a marathon (your test set)!\n\nHaving correctly set the data sets, we can now measure more efficiently the algorithm\u2019s bias and variance in order to subsequently improve it.\n\nIn a previous blog I described the difference between bias and variance in a 2-dimensional space.\n\nLooking at that graph, we can quickly identify the problem, but when dealing with multi-dimensions there is not such a visual representation that can help us discern the two causes of error.\n\nFortunately, when we compare the training, dev and the Bayes errors, we get a clue.\n\n(The Bayes error, is the optimal error, which in image recognition can be set as the human error, as humans are pretty good at recognizing images. So that\u2019s our benchmark).\n\nThis is our fast rule:\n\nGreat, now let\u2019s see what solutions we have at hand to fix these errors:\n\nTo fix high bias (which relates to the training set performance) we can:\n\nTo fix high variance (which reflects the dev set performance) we can:\n\nMost of those fixes are self-explanatory, so I will only spend time on explaining the different methods regarding regularization, which helps us avoid overfitting.\n\nThere are several regularization methods that we can use in our neural network.\n\nThis method adds a regularization parameter (\u03bb) to the cost function:\n\nWhen computing the derivative of the weight during back propagation, used in calculating the gradient descent, we get:\n\nwhich explains why this regularization is also called \u201cweight decay\u201d.\n\nNow that we have the formula out of the way let\u2019s see why the L2 Regularization makes the decision boundary \u201csmoother\u201d, less fit to the training data.\n\nIf we are using a big value of \u03bb, then to offset the cost function, the weights need to get closer to 0. If the weights are almost zero, then a lot of hidden units are not active and the network becomes much simpler. Therefore it can\u2019t fit complicated decision boundaries.\n\nLike when we wake up in the morning, the brain is less \u201cactive\u201d and can\u2019t handle decisions that are too complex. But after our coffee, our neurons\u2019 network is full and ready to deal with more complex problems.\n\nLet\u2019s look at this from another perspective.\n\nTo get a visual indication, let\u2019s consider an activation represented by the tanh function, which curve is represented as:\n\nIf we have a big value of \u03bb, then the weights are closer to 0, which means that \u201cz\u201d is also small since:\n\nAs we chose the activation function to be is equal to tanh(z), for small values of z, it is basically a linear function. (to understand what I mean, look at the curve above and see the behavior of the curve in a range of x values around 0).\n\nSo if every layer is represented by a linear function, then our network is linear and it doesn\u2019t fit complicated decision boundaries.\n\nJust keep in mind, though, that if the value of \u03bb is too big, then we risk to \u201coversmooth\u201d and therefore we end up being affected by high bias.\n\nWhen we add a dropout layer in the neural network, defining a \u201ckeep_prob\u201d value, we tell the network to set a probability of eliminating a node at each training example.\n\nLet\u2019s look at the following network:\n\nIf we set the probability of keeping each node to 50% (keep_prob=0.5) in the two hidden layers, then the network randomly eliminates 50% of the nodes in each layer (two nodes per layer) at each training sample. So the two eliminated nodes, randomly chosen, are different at every training example. So the network might look like this:\n\nand we can see now that, having eliminated 50% of the nodes, has made the network much simpler.\n\nWhen we use a dropout function, the network can\u2019t rely on any single feature since at any given time this might be suppressed. So it has to spread out the weights, not putting much weight on any one feature.\n\nIt\u2019s like having a lot of friends but no one is really that reliable to completely trust (\u201cgive a lot of weight\u201d).\n\nAbove, I sketched the same probability for each layer, but we can select to vary it layer by layer. Normally, for layers with a lot of parameters we use a low value of keep_prob (so high chances of eliminating the nodes) to avoid overfitting.\n\nIn applications such as computer vision, where we normally don\u2019t have a lot of training data, the use of dropout is very common, as it makes the network less fit to the data.\n\nThis technique is used during training, and not during test time, otherwise we would only add noise to the predictions.\n\nA good practice is to first train the network without the dropout layer, so we can compute the cost function at each iteration, and see if the gradient descent is performing correctly (decreasing the cost monotonically). Then we activate the dropout.\n\nAs you know, getting more training data helps to overcome overfitting. Sometimes, however, getting more data might be difficult or expensive, so one technique is to augment the existing training examples.\n\nIn the case of images, for example, we can flip the image horizontally (or rotate it) and add this new image to the dataset, or zoom and crop part of the original image.\n\nThis is an inexpensive way to gather more data and regularize the algorithm, reducing overfitting. These new data, though, don\u2019t introduce more info than a brand new independent example would.\n\nAs we are training, and run the gradient descent, we plot the curve of the cost function with respect to the number of iterations. We do this for both the training sample and the dev set. When we see the curve of the cost on the dev set reaching the minimum, then slightly increasing, it\u2019s a sign that we are overfitting the data and we should stop iterating.\n\nUnfortunately, this method is not ideal since it is affecting two orthogonalization problems at the same time. Specifically, it tries to fix overfitting, but it also touches the cost function because stopping the iterations goes against the goal of optimizing the cost.\n\nIn machine learning, instead, we want to work on one orthogonalization problem at a time:\n\n- optimize the cost function (which we are going to talk next)\n\nWe always want to turn one \u201cknob\u201d at a time to see how to fix the \u201cmachine\u201d, where each knob? has a specific effect. Early stopping is instead a knob that affects a couple of things at the same time and that doesn\u2019t help us figure out what is going on.\n\nThere are many techniques we can use to speed up training in a deep neural network.\n\nthe mean would be:\n\nand subtracting it from each input as x = x \u2014 \u03bc would allow the data to re-adjust as:\n\n2. Then, normalize the variance, so that each feature would have variance equal to 1. (i.e. In the plot above x1 has a much larger variance than x2). Calculating the variance:\n\nand dividing it from x:\n\nwill get our data reshaped as:\n\nThe logic, therefore, is that if we don\u2019t normalize the inputs, then consequently the cost function will be very elongated, while with normalization, it will become more symmetric.\n\nYou will see what I mean (and I will explain the consequences) in a second, when I plot the cost function.\n\nAs you can see above, when I run the gradient descent on the cost function for the not-normalized inputs (elongated shape), I need to use a small value of learning rate. It might take a lot of small steps to converge.\n\nOn the other hand, for a normalized version (symmetric circles), wherever I start, I can take the same steps (even larger steps) to get to the minimum.\n\nI mentioned this topic in an earlier blog, but I want to reiterate the concept, as it is one of the optimization methods you can use to train your deep network.\n\nIf you have implemented several networks you have certainly experienced the difficulty of training it. Depending on the weights\u2019 value, the activations can decrease exponentially as a function of the number of layers (if the weights are smaller than 1), or increase exponentially if they are above 1.\n\nThis problem is called the \u201cvanishing/exploding gradients\u201d. A partial solution to that is to properly initialize the weights.\n\nthe larger the number of input features is (n), the smaller we want w_i to be.\n\nTherefore, we can set the variance of wi to be 1/n.\n\nThis means that the weights can now be initialized as:\n\nand z now will be on a similar scale.\n\nJust a quick note, if we are using \u201cReLU\u201d activation we set the variance to be 2/n, instead of 1/n, which is used when dealing with a \u201ctanh\u201d function.\n\nThis is a debugging method that helps you figure out if backpropagation is working. It numerically checks the derivatives computed by your code to make sure that your implementation is correct.\n\nSo we know that the gradient of the cost function is the derivative of it. Let\u2019s recall the mathematical computation of a derivative:\n\nThis can be numerically approximated as:\n\nGreat, so now we can take w[1], b[1], \u2026, w[L], b[L] and concatenate them and reshape into a big vector \u03b8.\n\nThe same we do for dw[1], db[1], \u2026, dw[L], db[L] which will be stack in a vector d\u03b8.\n\nWhat we simply do, for each \u201c i \u201d of \u03b8, we compute the d\u03b8approx. with the formula above. If everything is correct, this should bring to a value close to d\u03b8 calculated by the gradient descent.\n\nIf the algorithm fails gradient checking, then we can look at which component (i.e. dw[1], db[1], \u2026) within the network is failing, since our vector d\u03b8 is a vector composed of those elements.\n\nThe concept here is simple (I promise!). Instead of training the network with the entire training set, we will train it on small batches (small chunks) of it.\n\nThis gives us the advantage of seeing gradient descent make some progress even before processing the entire training set and, furthermore, helps to not saturate the memory of our CPU/GPU.\n\nWe always want to eat small bites of cake to check if we like it, instead of eating the whole thing and then realize it\u2019s not quite our taste!\n\nSide note: when we train on the entire training set then our cost function decreases at every iteration. While training on mini-batches it might not do so at every iteration since each mini-batch is composed of different sets. But overall the cost function will decrease at every epoch! (Recall, one epoch is a single pass through the whole training set).\n\nFinally, watch the size of each mini-batch. At the extremes, if mini-batch size is equal to the entire training set (m), then it takes a long time for each iteration. On the other hand, if the mini-batch size is composed of just 1 example (stochastic gradient descent), then we lose the speed gained from vectorization.\n\nA good rule of thumb is to train on the entire set if the training dataset is fairly small (< 2,000 data), while otherwise choose a mini-batch size of 64, 128, 256, 512.\n\nIt computes an exponentially weighted average of the gradients and uses that gradient to update the weights.\n\nLet\u2019s start with an example. We are trying to minimize a cost function that has an elliptical shape like the one depicted below. The gradient descent is trying to reach the minimum and it oscillates throughout its path. To prevent the oscillations, which means to prevent overshooting, we are forced to use a small learning rate. This not only certainly reduces oscillations, but also slows down the process of reaching the minimum.\n\nIdeally, our goal would be to have a slow learning process on the vertical axis (small oscillations) while a faster learning on the horizontal axis.\n\nWell, I\u2019ve got good news for you. That\u2019s exactly what \u201cgradient descent with momentum\u201d does!\n\nOn every iteration, we compute the moving average of dw and db and use that (instead of the derivatives) to update the weights and the bias. So:\n\nWe can think of the cost function represented as a bowl and we have a ball that is moving along it to reach the bottom (the minimum).\n\nIn the above equations, the derivatives (dw and db) represent the \u201cacceleration\u201d of the ball rolling down, while the term Vdw and Vdb represent the velocity of the ball. The parameter \u03b2 is representing the friction to the ball that is gaining momentum along its way to the minimum.\n\nThis is another algorithm that can speed up gradient descent. As before, it moves quickly on the horizontal axis and slows down the oscillation on the vertical one. The way it gets to that is represented by the following equations:\n\nThis algorithm combines the effect of \u201cgradient descent with momentum\u201d with RMSprop. This is one of the most effective algorithms, and one I tend to use most of the times.\n\nFor every iteration t we compute:\n\nAs we can see from the equations that govern this model, there are several parameters involved, but, fortunately, the inventors of this algorithm recommend the best values to use.\n\nSo normally you have:\n\nThe concept here is to slowly reduce the learning rate over time. Why?\n\nWell, let\u2019s think about an implementation with small mini-batch gradient descent. We know that each step will be a bit noisy and that it will get towards the minimum but never quite converge to the minimum, while instead wandering around it. The way to fix it would be to reduce the learning rate (remember, reducing the learning rate reduces the oscillations, making the steps slower and smaller) when we get closer to the minimum, to allow a tighter oscillation around the minimum.\n\nSo that sounds awesome! We can take advantage of a large learning rate at the beginning, which favorites faster learning, and then lower the value of the learning rate when we get closer to the minimum.\n\nSo this is how we can set our learning rate \u03b1:\n\nThe \u201cdecay_rate\u201d is obviously a parameter that we need to tune.\n\nAs you have seen throughout this blog, each optimization algorithm has its hyper-parameter(s) that need to be tuned to gain the best performance.\n\nThe learning rate \u03b1, \u03b2 (when dealing with gradient descent with momentum), \u03b21, \u03b22 and \u03b5 (for the Adam algorithm) and then the # of layers, the # of hidden units, the mini-batch size, the learning rate decay.\n\nYeah, there\u2019s a lot of them. You might be wondering \u2026 where should I start?\n\nAndrew Ng gives us his suggestions to answer to that question.\n\nThe most important parameter, according to Ng, is certainly the learning rate \u03b1. This drives the whole process of learning (duh!) and if we use a wrong value (i.e., too large), we can overshoot or (i.e., too small) slooowly get to a convergence.\n\nThen, we can focus on \u03b2, mini-batch size and # of hidden units.\n\nFinally, we can get our hands on the # of layers and the learning rate decay.\n\nLuckily, we already saw that Adam\u2019s parameters are good as they are set by default: \u03b21 = 0.9, \u03b22 = 0.999.\n\nBefore closing this blog (I know, I know, I\u2019ll be quick) I want to mention an algorithm, Batch Normalization, that is really helpful in making the network pretty robust and in making the hyper-parameters search problem easier since it \u201callows\u201d a bigger range of hyper-parameters that work well.\n\nI mentioned earlier about normalizing the input features, which helps to speed up learning, computing the mean and the variance. Well, the batch normalization algorithm does that for the hidden layers, before applying activation.\n\nIt also makes the weights later (or deeper) in the network more robust to changes to weights in earlier layers. Therefore, it makes each layer more \u201cindependent\u201d and ready to learn by itself, speeding up the whole learning process.\n\nFinally, it also has a regularization effect. In fact, when using mini-batch gradient descent and batch normalization, each mini-batch is scaled by the mean/variance computed on just that mini-batch. This adds some noise to the values z[l] within that mini-batch. So, similar to the dropout regularization, it adds some noise to each hidden layer\u2019s activations; it can\u2019t \u201crely\u201d on each of them as we saw for dropout.\n\nThis slight regularization effect, though, is reduced if we increase the size of the mini-batch."
    },
    {
        "url": "https://medium.com/machine-learning-bites/deeplearning-series-neural-networks-and-deep-learning-3ea0fd1aef31",
        "title": "DeepLearning series: Neural Networks and Deep Learning",
        "text": "In this blog I will start with the basic definition of a neural network, then to deep learning concepts.\n\nTo cover the basics of a neural network, I will use a logistic regression, which is an algorithm that is used for binary classification (when the output is 0 or 1), since the implementation is easy to follow.\n\nFrom there we will move to the representation of a one-hidden layer neural network, which will lay down the foundation for a more complex one, a deep neural network.\n\nOkay, let\u2019s dive deep (yeah, I know, \u2026 couldn\u2019t help it!) into this exciting material.\n\nIn a previous blog I already wrote about the neural network, but I want to add some intuition regarding the basic concepts. Additionally, I wanted to write about the amazing deep learning networks that I\u2019ve been working on thus far, so I decided to take a step back and start from the \u201cbeginning\u201d.\n\nYou might hear or read a lot about the fact that a neural network is inspired by the human brain\u2019s architecture. . . Well, that\u2019s sexy, but it sounds very complicated and really not that intuitive.\n\nSimply put, a neural network tries to learn a function \u201cf\u201d that can connect the input X to the output Y. (simpler, but not as sexy, right?) It can be represented as:\n\nThe \u201cneuron\u201d here computes this function \u201cf\u201d, which can be linear or non-linear.\n\nLet\u2019s move to the use of a neural network for a binary classification. In this case the output Y can take the value of 0 or 1.\n\nGiven an input feature vector X, I want an algorithm that can output a prediction, \u0177, which is the estimate of Y.\n\nEssentially, \u0177 is the probability that Y is equal to 1, given the input features X.\n\nHow do we generate this output \u0177 In other words, what function can we use to connect \u0177 to X through the use of some parameters w and b?\n\nWe could think of using a linear function, such as \u0177 = w*X+b (technically it should be w transpose (wT), (as X and w are dimensional vectors, but let\u2019s just skip this \u201cdetail\u201d for the sake of simplicity). This won\u2019t help us in evaluating this case scenario as \u0177 is linearly connected to X, so it can take any sort of value.\n\nInstead, to get a result that ranges from 0 to 1, which is what \u0177 is binded to, we can use a logistic (sigmoid) function applied to the linear expression of the data.\n\nThe sigmoid function is defined as:\n\nwhich is represented as:\n\nLet\u2019s call z our linear expression we mentioned before, so\n\nand therefore, applying the sigmoid function to it we get:\n\nGreat, so we got to the logistic regression model!\n\nTo recap, this is what we have defined so far:\n\nWe have an input, defined by a set of training examples X with its labels Y, and an output \u0177 as the sigmoid of a liner function. (\u0177 = \u03c3 (wT*X +b)).\n\nWe need to learn the parameters w and b so that, on the training set, the output \u0177 (the prediction on the training set) will be close to the ground truth labels Y.\n\nTo learn these parameters we need to train the network, which means we will need to define a cost function and minimize it so to obtain w and b that get the predictions as close as possible to the ground truth.\n\nLet\u2019s first define the loss function (that computes the error for one training example), which is then generalized to the cost function. The latter encompasses the whole training set.\n\nWe could use the square error between the label (y) and the prediction (\u0177), but this is not used in the logistic regression as it creates problems around finding the greatest optimum minimum.\n\nWhat is generally used for the loss function is this formula:\n\nSince we need to minimize the loss, we want log \u0177 to be large. This means that we want \u0177 to be large and as \u0177 is a sigmoid function, \u0177 will be close to 1.\n\nOn the other hand, if y=0 then\n\nTo minimize the loss we will want \u0177 to be small, therefore close to 0.\n\nFinally, the cost function on the entire training set is defined as:\n\nWhat we have done so far is compute what is called the \u201cforward propagation\u201d, which moves from the input through the neural network and identifies the loss, so the error between our prediction and the ground truth.\n\nAt this point, we want to minimize this error so we use \u201cback propagation\u201d to propagate back what we have learned to adjust the parameters.\n\nOnce we have done that, we update the parameters w and b using gradient descent, which computes the derivatives of the cost function with respect to w and b as:\n\nOkay, time for an example! Let\u2019s use a network with two inputs.\n\nOur inputs are x1, x2 with their associated parameters w1, w2 and b. We can then compute \u201cz\u201d as a linear function of those and then apply a sigmoid function to it (called \u201ca\u201d in the graph below) and finally calculate the loss function of \u201ca\u201d with respect to y.\n\nWhat we have depicted is the forward pass of the logistic regression.\n\nNow, we want to calculate the derivatives of each parameter with respect to the cost function. We do this in order to calculate how much a change of each parameter affects the final loss.\n\nFinally, we compute the gradient descent algorithm, which, at every loop, allows us to update the parameters while keeping in consideration the derivatives and the learning rate.\n\nThus, we find the parameters w1, w2 and b that minimize the error between our prediction and the ground truth.\n\nThe computational graph for forward and back propagation is depicted below.\n\nThe formula for forward propagation is:\n\nFinally, the gradient descent, which computes the updates is:\n\nIn the logistic regression we saw how this model\n\ncorresponds to the following computational graph:\n\nA neural network (with a one-hidden layer) looks like this:\n\nwhere each node (\u201cneuron\u201d) corresponds to the previous two-step calculation of \u201cz\u201d and \u201ca\u201d.\n\nSo think of something like this:\n\nFinally, the computational graph for the neural network depicted above is:\n\nThe process is the same as the one for the logistic regression, therefore, after the forward propagation (as written above in the computational graph) we will perform the derivatives at each step in order to compute the back-propagation.\n\nFinally, we apply gradient descent to acquire the parameters that minimize the loss.\n\nIn this case the parameters we are optimizing are:\n\nI won\u2019t sketch the back propagation steps on the computational graph as it will be as messy as a toddler\u2019s drawing, but you get the idea.\n\nBefore moving to a more complex neural network (the deep neural network), I want to expand on the activation function and the initialization of the parameters w and b.\n\nSo far we have been using the sigmoid function as an activation function (that\u2019s when we do a = \u03c3(z), remember?) which, as you recall, is defined by this curve:\n\nIn a binary classification, where the output is a value between 0 and 1, this curve fits pretty good. On the other hand, as the data are not \u201ccentered\u201d is not a good choice as activation function for the hidden layers.\n\nFor these ones, in fact, we have a better option: the tanh(z) function:\n\nThat is \u201ccentered\u201d around 0 and assumes values between -1 and 1:\n\nAlthough, for the hidden layers is most common to use the ReLU (rectified linear unit) function:\n\nThis follows the linear curve for the positive values and 0 everywhere else:\n\nA variant to that is the LeakyReLU function:\n\nThis allows a small, non-zero gradient when the unit is not active:\n\nOkay, so now you know that there are several activation functions that you could use. If you are confused on which one to select, the main take-away here is this:\n\nFor the hidden units, never use a linear activation, otherwise the output would be a linear function of the input, which doesn\u2019t do us any good \u2014 actually in that case it wouldn\u2019t even matter how many layers we have in the network\u2026 which defeats the purpose of having a neural network.\n\nAlways use non-linear functions. Researchers in the past have used sigmoid, but recently the ReLU function is the most common for the hidden layers.\n\nFor the output layer instead, if you are doing a binary classification, then the sigmoid function works great, otherwise for a regression problem you can use a linear activation function (since y can take any real number).\n\nYou saw in the previous computation graphs that apart of the inputs of the network, there are also the parameters w and b, and you might be wondering what values those might be initiated with.\n\nLet\u2019s get the easy one out of the way: the bias b can be initialized to zero. It wouldn\u2019t matter.\n\nFor the weights w, instead, the things get tricky (or even problematic!) if we don\u2019t initialize them correctly.\n\nIn fact, we need to initialize the weights to small random values!\n\nIf the weights are too big, indeed, we end up having a very slow learning and the gradient descent is pretty slow.\n\nTo make this more intuitive, let\u2019s look at the curve of the tanh function. A large number there (in absolute terms) would be either close to +1 or -1. Looking at the graph, you see that in that position the slope is pretty much close to zero. Remember what the derivative of a variable is? It\u2019s the slope of the tangent to the curve. And the derivative is what we use in gradient descent to update our parameters.\n\nSo, a slope ~0 means a derivative ~0, which means a very small variation update, which means slow learning!\n\nOn the other hand, if we initialize the weights to 0, all the hidden units are computing the same function during forward propagation (and the same derivatives during back propagation). Therefore, they have the same influence on the output unit. Thus, the hidden units are symmetric, and no matter how many iterations you perform, they still compute the same function, so it wouldn\u2019t matter how many hidden units you have in the network.\n\nIf you got this far \u2014 thank you!\n\nThe last piece is to build a Deep Neural Network. Finally!\n\nYou might have predicted by now that a deep neural network is a network with many hidden layers.\n\nThe logistic regression we started with was basically just a \u201cshallow\u201d network.\n\nThe following is a deep neural network with a total of 4 layers (3 hidden layers and one output layer):\n\nHere is a bit of nomenclature:\n\nWe compute as before the forward, back propagation and gradient descent update for each layer \u201cl\u201d in order to retrieve the parameters w and b (in each layer) that minimize the final cost function.\n\nIn an image recognition problem, a deep neural network works very well. Each layer captures different features; for example, in the first layers it captures simpler functions (such as lines), while the latter layers can depict more complex ones.\n\nOne final note: in neural-network literature, the term \u201cdeep\u201d refers to the depth of the network, therefore the number of hidden layers. The term \u201csmall\u201d, instead, refers to the number of units.\n\nThere is a theory, called \u201ccircuit theory\u201d, that states:\n\n\u201cThere are functions you can compute with a small deep neural network that, instead, a shallower network would require exponentially more hidden units to compute.\u201d\n\nA good heads up when you are building your own neural network model!\n\nOn the next blog, we\u2019ll discuss the many hyper-parameters that are involved in a deep neural network, such as the learning rate, the number of iterations, the number of hidden layers, units, and a choice of activation function."
    },
    {
        "url": "https://medium.com/machine-learning-bites/capsule-networks-b94a2182be56",
        "title": "Capsule Networks \u2013 Machine Learning bites \u2013",
        "text": "The recent paper published by Geoffrey Hinton has received a lot of media coverage due to the promising new advancements in the evolution of neural networks.\n\nThe advancement is called \u201cCapsule Networks\u201d and its latest implementation referred to in his paper \u201cDynamic Routing between capsules\u201d.\n\nTo be clear, capsule networks were first introduced by Hinton in 2011 but remained somehow dormant as he had hard time making them work.\n\nThe recent research paper from October 2017 seems to be paving the way for a future use of this technology.\n\nSo what is a Capsule Network?\n\nIn computer graphics, we start with an abstract representation of an object, through its \u201cinstantiation parameters\u201d and then apply a rendering function in order to obtain an image.\n\nThink about the \u201cinstantiation parameters\u201d as \u201cpose parameters\u201d for example. In this case, the object could be \u201crepresented by its x, y location, and angle value.\n\nInverse graphics, on the other hand, is precisely the opposite way. We start from an image and through inverse rendering we find what objects it contains and what their instantiation parameters are.\n\nA Capsule Network is a neural network that performs the inverse graphics mechanism.\n\nThis network is composed of many capsules, and each of them is a function that aims to predict the presence and the instantiation parameters of an object at a given location within an image.\n\nThe arrows in the representation below represent the output vector of a capsule.\n\nThe length of the activation vector is the estimated probability that the object is indeed present within the image.\n\nThe orientation of the vector encodes the object\u2019s estimated pose (instantiation) parameters.\n\nTo obtain this \u201crepresentation\u201d of the instantiation parameters, we apply several convolutional layers to the image, which output an array containing a bunch of feature maps.\n\nThen we need to reshape this array to get a set of vectors for each location.\n\nFinally, we \u201csquash\u201d the vectors to ensure there are no vectors longer than 1 (since the length of a vector represents the probability, we can not have a value more than 100%). \u201cSquashing\u201d the vectors will put their length\u2019s value between 0 and 1.\n\nOne of the main advantages of a capsule network is that it preserves the object location within an image. This feature is called \u201cequivariance\u201d.\n\nOn the other hand, a convolutional neural network loses this location information of an object, due to the pooling layers that \u201cextract\u201d only the most meaningful information.\n\nAnother important feature brought by the capsule network is its ability to analyze the hierarchy of objects and, in a way, their \u201cinteraction\u201d among them within the image.\n\nThis characteristic is obtained because every capsule in the first layer tries to predict the output of every capsule in the next layer.\n\nDuring training, the network learns the transformation matrix for each pair of capsules in the first and second layer, which allows the \u201cdiscovery\u201d of the relationships between objects.\n\nAs I mentioned at the beginning of the post, for a further and more detailed explanation I redirect to Aur\u00e9lien G\u00e9ron\u2019s video, which gives an incredible overview of the subject. (Geoffrey Hinton himself even praised it!!)"
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-reinforcement-learning-game-theory-a869232450bf",
        "title": "Machine Learning: Reinforcement Learning \u2014 Game Theory",
        "text": "So far we have been dealing with a world where, following a certain policy, an agent takes certain actions, but acting in a vacuum state, with no interaction with the outside world.\n\nIn gamy theory, agents evaluate how their decisions interact with the decisions of others, to lead to different outcomes.\n\nIn this environment we are now dealing with multiple agents, instead of a single one.\n\nGame theory can be defined as the mathematics of conflicts.\n\nIn game theory the policy is the strategy, mapping all possible states to actions, in respect to one of the players of the game.\n\nIn a 2-player (A and B) game the player A\u2019s goal is to choose a strategy that maximizes his/her rewards, while player B chooses the one that minimizes player A\u2019s outcome.\n\nHere below there are different games described by different characteristics:"
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-reinforcement-learning-markov-decision-processes-431762c7515b",
        "title": "Machine Learning: Reinforcement Learning \u2014 Markov Decision Processes",
        "text": "The goal of reinforcement learning, contrary to the previously seen methods of machine learning (supervised/unsupervised learning), is to tweak the system in order to perform right or wrong in certain ways.\n\nWe do that by attaching rewards and punishments to different outcomes, which ultimately drive the machine to find the \u201cright\u201d priorities.\n\nAn example of it is self-driving cars. There the system \u201cprefers\u201d one optimality versus another, taking into consideration rewards and punishments along the way.\n\nSupervised learning is characterized by x,y pairs and the goal is to find function approximation that defines them and that predicts future data.\n\nIn Unsupervised learning, instead, we are given a bunch of x\u2019s and the goal is to find a function that describes the unrolling clustering description.\n\nInstead, in reinforcement learning we are given a pair of data (x,z) and we need to learn the function that generates y. (I am going to better explain what these values are).\n\nThe final goal of the MDP is to find a policy that can tell us, for any state, which action to take.\n\nThe optimal policy is the one that maximizes the long-term expected reward.\n\nThere are two properties on which the Markovian process is based on:\n\nGoing back to the reinforcement learning definition above, when compared to the rest of the machine learning tools, we can see better what are the inputs and the outputs. We are given a bunch of state/action pairs and we learn a function, i.e. the policy, which maps the states into actions.\n\nTherefore in the MDP world the x is state \u201cS\u201d, z is the reward \u201cR\u201d and y is the action \u201cA\u201d and finally the policy is the function we are aiming to.\n\nAs I mentioned before, the MDP process is defined by two characteristics, one of which is stationarity. This property plays a role in terms of assumptions we take.\n\nThe actions taken during a transition between different states take us to different rewards. As I said, this mapping of states to actions is our policy.\n\nWell, this policy remains the same only if we are considering an infinite time horizon, while, instead, for a finite time horizon the policy might change, even if I\u2019m in the same state.\n\nAnother concept related to stationarity is the \u201cstationarity of sequences\u201d. This claims that if we prefer one sequence of states today over another sequence of states, then we prefer that sequence of states over the same sequence of states even tomorrow.\n\nWhat defines a sequence of states is the \u201cutility\u201d function.\n\nBasically a reward gives us immediate feedback, present gratification, while utility gives a long-term feedback, since it accounts for all the delayed rewards.\n\nHaving introduced this concept of utility now helps us understand better what a policy and, in particular, what an optimal policy is.\n\nThe optimal policy is the one that maximizes the expected rewards. The utility does indeed depend upon the policy. It is the expected set of states that we see from that point on, given that we follow a policy.\n\nThis finally takes us to the definition given by the Bellman equation, which mathematically states the following:\n\nThe \u201cdiscount\u201d mentioned above is a value between zero and one, and it allows us to treat an infinite sequence with finite value. Otherwise the utility in an infinite time horizon will always be the same, of an infinite magnitude.\n\nTo solve the Bellman equation we normally start with arbitrary utilities and then update them based on the neighbors (which are all the states that it can reach from the current state I am in). Finally, we repeat that until convergence.\n\nAs we can infer, the utility is similar to a regression, mapping through continuous values, while the policy is more like a classifier.\n\nBesides the above-mentioned value-based function (from which we observe the values as outputs, find the utility and finally infer the policy), there are two other approaches.\n\nOne is the \u201cpolicy-search\u201d algorithm, with which we observe the actions from different states and deduct the policy, which we can directly use.\n\nThe other one is a \u201cmodel-based\u201d algorithm, from which we get state/action pairs as input and we receive rewards/states as output.\n\nIt\u2019s a direct learning approach, similar to a supervised learning one, but with an indirect use. In fact, from this one we will need to solve the Bellman equation in order to deduct the utility and then finally derive the policy."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-unsupervised-learning-feature-transformation-482adc937b6c",
        "title": "Machine Learning: Unsupervised Learning \u2014 Feature Transformation",
        "text": "When talking about ICA, it\u2019s useful to refer to the comparison with the previously depicted method of feature selection, PCA.(https://medium.com/machine-learning-bites/machine-learning-unsupervised-learning-principal-component-analysis-8f7ad311027e#.cy21s6xdr).\n\nThe goal of the latter is to find correlation, maximizing variance.\n\nICA, instead, maximizes independence among features. It finds a linear transformation of the feature space into a new feature space such that each of the individual new features are mutually independent, in a statistical sense.\n\nA common example and good application of this method is the so-called \u201cblind source separation\u201d or \u201ccocktail party\u201d problem.\n\nIn this case we have sources of separate, independent noise that are mixed together and \u201ccaptured\u201d by microphones. The sources of noise are the hidden, latent variables we are trying to find, while the microphones are the observable variables.\n\nWhat we want to find are the latent features, the independent source of noise, separated from each other.\n\nIn mathematical terms, this is achieved by sampling the sound waves and assigning a sequence of numbers to them, like a different time sequence.\n\nThe observables are now a matrix of numbers in which each row is a feature on the observable space and every column is a sample (distinct from one another in time measurements).\n\nThe goal is therefore to find a projection on the new feature space where the features are the hidden variables.\n\nJust looking at this example we can extrapolate an important characteristic of ICA, which is its directionality. In fact, the matrix depicted earlier is composed of rows of features, and column of samples. If we transpose this matrix, the result is completely different. In the PCA method instead this restriction wasn\u2019t present.\n\nIt\u2019s important to notice that sometimes ICA can fail to find independent components.\n\nIt is interesting to notice how PCA and ICA behave when approaching different problems.\n\n- Let\u2019s look at \u201cimages of faces\u201d analysis and see what result each method brings to it.\n\nPCA: it first finds the brightness and separates images by it. Then it detects an \u201caverage face\u201d, which does not have a straight forward and impactful interpretation.\n\nICA: it finds different selectors, such as nose selector, mouth selector, hair selector.\n\n- In \u201cnatural scenes\u201d analysis PCA behaves as before, while ICA finds the independent components, which in this case are the edges. It\u2019s clear then, that to apply ICA in this circumstance helps quite a lot in terms of pre-processing information, as we can have a subsequent algorithm which reads edge detectors and can quickly select the different images."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-unsupervised-learning-principal-component-analysis-8f7ad311027e",
        "title": "Machine Learning: Unsupervised Learning \u2014 Principal Component Analysis",
        "text": "The Principal Component Analysis (PCA) is a method for feature selection that turns a set of correlated variables into the underlying set of orthogonal (latent) variables, that are not measured directly but are driving the phenomenon that we are measuring.\n\nIn a given dataset, the PCA method finds a new coordinate system that is obtained from the old one only by translation and rotation.\n\nIt first moves the center of the coordinate system to the center of the data, then it moves the x-axis into the principal axis of variation, which is the one where we see the most variation relative to all the data points.\n\nFinally, it moves the other axis into an orthogonal, less important, direction of variation.\n\nPCA provides vectors and not functions (contrary for example to linear regression), therefore any type of data sets can be represented by it.\n\nAs mentioned earlier, when applying PCA in the context of dimensionality reduction, we basically want to select a smaller number of features that are driving the patterns. We detect the \u201ccomposite features\u201d (which are actually called \u201cprincipal components\u201d) that more directly probe the underlying phenomenon.\n\nSo, let\u2019s introduce the concept of \u201cPrincipal Component\u201d (PC).\n\nWhile with a regression we are trying to predict an output variable with respect to the value of an input variable, with a Principal Component we try to come up with a direction in the data (not to predict anything) where we can project the data onto, while loosing the minimum amount of information.\n\nOnce again, the Principal Component of a dataset is the direction that has the largest variance, (where variance is intended as the \u201cspread\u201d of a data distribution), because it can retain the maximum amount of information in the original data, minimizing the information loss.\n\nThe amount of information loss is the sum of the distance of each point to the principal component line.\n\nApplying PCA to all the features allows us to derive the principal components involved.\n\nPCA is a systematized way to transform input features into principal components, using them as new features.\n\nThese principal components are the directions in the data that maximize variance when we project the original data to them.\n\nThe more variance of the data along a principal component, the higher the principal component is ranked. Each principal component is orthogonal to each other, without overlapping, so they are a sort of independent features.\n\nPCA helps visualize high-dimensional data, it reduces noise and finally makes other algorithms to work better because we are injecting fewer inputs.\n\nA successful use of PCA is seen in facial recognition problem.\n\nIn this case the principal components that are the result of PCA are called eigenfaces.\n\nThis method is very effective since picture of faces have high input dimensionality (many pixels) and faces have certain general patterns that can be captured in smaller number of dimensions (i.e. two eyes and the mouth in the bottom).\n\nThe dilemma is given by the selection of principal components. Unfortunately there is no written rule on how many of these should be, but a good practice is to train on different number of PCs and see how the accuracy responds. Then finally stop adding when it becomes apparent that adding more PCs doesn\u2019t buy much more discrimination.\n\nIt certainly is not good to perform feature selection before running PCA, as it defeats the purpose of having many features from which PCA combines information.\n\nHowever, in a multi-class classification problem like this one, where we have more than 2 labels, accuracy is a less intuitive metric, while instead F1-score provides better explanation for the results.\n\nThe F1-score in fact, considers both the precision and the recall of the test to compute the score. The precision is the number of correct positive results divided by the number of all positive results; the recall instead is the number of correct positive results divided by the number of positive results that should have been returned.\n\nThe F1 score can be interpreted as a weighted average of the precision and recall.\n\nFinally, one thing to note is that adding more PCs does not necessarily mean improving the F1-score, as it can deteriorate after a certain point."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-unsupervised-learning-feature-selection-a9bdccd70f95",
        "title": "Machine Learning: Unsupervised Learning \u2014 Feature selection",
        "text": "The filtering method is represented by a search algorithm that acts as a \u201cfeatures selector\u201d prior to the learning algorithm.\n\nThe advantage of this method is speed, since the learning algorithm has already fewer inputs to deal with.\n\nOn the other hand, there is no feedback from the learning algorithm, which has no inputs in the features selection. These are in fact filtered as isolated inputs. Some of them might be valuable when associated to others, but be discarded instead by the search algorithm.\n\nDecision trees and boosting algorithms are using the filtering method. The first one in particular, takes advantage of the information gain (looking at the class label) for selecting the relevant features.\n\nBesides information gain, other filtering criteria that can be used are:\n\nThis method combines the search and learning algorithms together. This means that the two communicate with each other, exchanging feedback on how well a feature does and therefore whether it is selected afterwards.\n\nThe advantage is exactly that the feedback is passed from the learning algorithm to the search one. Different from the previous method, this one takes into account the model bias. Unfortunately, this process is obviously slow.\n\nThere are several ways of performing the wrapping method, such as:\n\nA few words on how the last two methods are performed.\n\nThe forward search algorithm looks at the features independently and keeps the one that\u2019s best, evaluating the score given by the learning algorithm.\n\nOnce it selects the first feature (the one with the highest score) it adds a second one, chosen among all the remaining features, as the one that performs best in conjunction with the first feature.\n\nThen it continues on with the third feature, performing the same type of selection, and finally stopping until it doesn\u2019t see any substantial improvement from the previous step.\n\nThe backwards search is done basically the same way, but backwards. Therefore starting with all the features and seeing which one can be eliminated and continuing on until there is no substantial improvement.\n\nBefore I conclude this section, i would like to emphasize the definition of two words that are loosely used on feature section: relevance and usefulness.\n\nIn machine learning these words are crucially important.\n\nBefore describing the characteristics of both, I would like to introduce a concept called Bayes Optimal Classifier (B.O.C.) that is important to understand the concept of \u201crelevance\u201d.\n\nThe Bayes Optimal Classifier (B.O.C.) takes the weighted average of all the hypotheses, based on their probability of being the correct hypothesis given the data. It simply is the best I could possibly do on average.\n\nNow that we have this notion we can express what \u201crelevance\u201d is in feature selection.\n\n- A feature is strongly relevant if removing it degrades the B.O.C.\n\n- A feature is instead weakly relevant if:\n\nUsefulness instead measures the effect on a particular predictor.\n\nRelevance is about information, while usefulness is about minimizing the error given a model or a learner.\n\nUltimately, relevance and usefulness are criteria to be considered for feature selection."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-unsupervised-learning-feature-scaling-8e6882b5f2fe",
        "title": "Machine Learning: Unsupervised Learning \u2014 Feature scaling",
        "text": "Sometimes we are dealing with a lot of features as inputs to our problem, and these features are not necessarily scaled among each other in comparable ranges. We want, instead, to have features that are expressed in dimensions that can be compared, therefore we need to perform what we call \u201cpreprocessing\u201d in order to achieve this goal.\n\nThe benefit of this is having, as mentioned, comparable and reliable numbers.\n\nOne disadvantage, though, is the role of outliers in the input features, which could actually mess up the rescaling.\n\nMathematically the new rescaled feature X\u2019 is expressed as follows:\n\nwhere the expression on the right is based on the information taken from the old (not yet rescaled) feature.\n\nSklearn has the easy library called \u201cMinMaxScaler\u201d that can be imported from sklearn.preprocessing, which allows to rescale a list array, for example, and then \u201cfit_transform\u201d it to finally rescale it. (http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html\n\nRescaling is an important housekeeping activity to be performed prior to starting processing the features. We need to keep in mind though that there are certain algorithms that are affected by feature rescaling therefore we need to be careful in performing it correctly for all the features involved, otherwise the result can be negatively affected.\n\nIn particular, the following algorithms are impacted:\n\nIn fact, both these algorithms involve distance calculations. They trade off one dimension against the other, therefore if one dimension is twice as big as the other, then the final dimension is twice as big.\n\nAlgorithms calculating diagonal distances are clearly exposed to this.\n\nOn the other hand, the following are not affected by feature rescaling:\n\nBoth these algorithms calculate horizontal lines. Each feature has a coefficient associated with it; feature and coefficient go together and don\u2019t affect other features."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-unsupervised-learning-clustering-d686af2466e1",
        "title": "Machine Learning: Unsupervised Learning \u2014 Clustering",
        "text": "This algorithm takes a deterministic approach and it operates in two steps:\n\n1- First, we separate the initial data in different clusters, which gives a representation of how the data are distributed and therefore categorized.\n\nInitially we assign at random the cluster centers (we can decide how many cluster centers are needed). The algorithm assigns the data points that are closer to a cluster center rather than other ones.\n\nLet\u2019s imagine we have two cluster centers, then this assignment is done in the following way: we draw a line between two cluster centers and sketch the perpendicular line passing in the middle. This line divides the plane in two spaces and each data is therefore evaluated depending on which side of the space they fall in.\n\n2 \u2014 Now that the cluster centers are positioned and the data are assigned to each one of them, the algorithm will reposition the cluster centers, finding a way to minimize the total quadratic distance of the cluster center to the points assigned to it.\n\nFinally these two steps are repeated until the algorithm stabilizes and the position of the cluster center is fixed.\n\nThere are two challenges associated with this algorithm:\n\nIn sklearn (http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) there is one parameter we can play with: \u201cn_init\u201d. It allows us to define the number of initializations, so the algorithm repeats the procedure many times with different initial random states and therefore it will choose the best output among the different ones that it ran.\n\nSo these considerations lead to the limitations of this algorithm, which I have already hinted to:"
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-supervised-learning-algorithms-summary-76adc41b8ecc",
        "title": "Machine Learning \u2014 Supervised Learning Algorithms: Summary",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-ensemble-methods-df8d0745656e",
        "title": "Machine Learning \u2014 Ensemble methods \u2013 Machine Learning bites \u2013",
        "text": "Ensemble Learning is based on the concept of building simple rules and combining them together, creating a complex rule.\n\nThere are two types of ensemble methods:\n\n- averaging methods: build several estimators independently and then average out their predictions, reducing therefore the variance from a single estimator. Part of this family of appraoches are \u201cbagging\u201d, \u201crandom forests\u201d methods.\n\n- boosting methods: they involve the combination of several \u201cweak\u201d models, built sequentially. Part of this category are the \u201cAdaBoost\u201d and \u201cgradient tree boosting\u201d methods.\n\nWhile the first type of methods (the averaging methods) are pretty straight forward, since they simply combine different predictions and average them out, the former one (the boosting method) is a little bit less intuitive, but very powerful in its implementation.\n\nAt this point it is important to understand the concept of the \u201cweak learner\u201d.\n\nThis is a classifier which, no matter what distribution it is in, when learning labels on the data, it does always better than the random \u201cchance\u201d, therefore finding a hypothesis with a small error (no bigger that 49%).\n\nIn each step, for each estimator, the algorithm measures how good the weak hypothesis is and assigns the weight for that hypothesis. It therefore puts more weight on the answers that it\u2019s getting wrong and less on the ones that it\u2019s getting right.\n\nIn this way, it computes the different boundaries for the classification of the different classes of data."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-naive-bayes-7f3b52c470f5",
        "title": "Machine Learning \u2014 Naive-Bayes \u2013 Machine Learning bites \u2013",
        "text": "The Naive-Bayes method is based on the naive assumption of independent predictors, which means that all properties contribute independently to the probability. Each feature is therefore assumed to be independent to another.\n\nWith this method we can infer the posterior probability, knowing the prior probability and the test evidence.\n\nThe conditional independence assumption states that X is conditionally independent of Y, given Z, if the probability distribution governing X is independent of the value of Y given the value of Z.\n\nTherefore, if I know the probability of Z, I can figure out the probability of X without looking at Y.\n\nThe final Naive-Bayes rule is the following:\n\nwhich in plain english can be translated into:\n\nOne of the problems of this method is that if a categorical variable has a category in the test dataset which was not in the training dataset, the model assigns zero probability to this category and therefore it is unable to predict. One solution to that would be the use of a smoothing factor, such as Laplace smoothing.\n\nThis algorithm performs well in a large dataset with a lot of noise. Additionally it can be successful in multi-class predictions.\n\nIt is mainly used in text classification and where there are problems of multiple classes.\n\nOne downside is the fact that it is based on the above mentioned assumption of independent predictors which is a fairly rare situation in a real-life scenario."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-support-vector-machines-5bc28a7f5bcb",
        "title": "Machine Learning \u2014 Support Vector Machines \u2013 Machine Learning bites \u2013",
        "text": "The goal of a Support Vector Machine (SVM) is to analyze the given data and find the line that best separates them, but committing the least to the existing data.\n\nWhen looking for a line that separates the data we can encounter several ones. The best line is the one that not only classifies the data in two classes, but also maximizes the margin (the distance to the nearest point) between the line itself and the data.\n\nOne characteristic of the SVM is its robustness, in the sense that it \u201cignores\u201d the outliers.\n\nThis looks pretty easy if we think about data spread across a 2-dimensional graph and somehow already \u201cdivided\u201d into two different areas. It\u2019s just a matter of finding the line that best maximizes the margin and that\u2019s pretty much it.\n\nA different story is instead when we have data that are scattered in different \u201cshapes\u201d. Let\u2019s imagine two data classes where one class of data is scattered around the origin of the graph, in a circular shape, while the other class is more on the outbound of this circle.\n\nObviously there are no lines that can separate the two classes. A circle would be the geometry that identifies the boundary between the two classes. But how do we do that?\n\nWell, let\u2019s think about the logic of the SVM. We have inputs coming in (in our case X and Y) and labels coming out (classifying the data). What if we introduce another feature as an input, which is nothing new, but just a combination of the existing inputs X and Y? Let\u2019s call it Z, equal to the sum of X-squared and Y-squared.\n\nTherefore Z measures the distance of each point from the origin of the axes. If we now display the data on a graph where the axes are X and Z, we can see that the data (the red crosses above) which were placed in a circle around the origin (in the original graph of X, Y axes) are now laying close to the X axis. These points in fact have a short distance from the origin. On the other hand, the rest of the data (blue dots above) have higher values of Z, therefore they occupy the higher portion of the first and second quadrants (in the graph with X and Z axes). Now these data are linearly separable! We can actually draw a line that separates them.\n\nThis line, when \u201ctranslated\u201d into the original space (with X and Y axes) corresponds to a circle!\n\nThis approach is valid for any other type of data positioned on the 2-dimensional space. Meaning, we can find a feature that can be used as an additional input, which can \u201ctransform\u201d the data into linearly separable. This line, created in this new space, can then be \u201ctranslated\u201d back into the original space and provide a non-linear shape that separates the data.\n\nThis is called the \u201cKernel trick\u201d. It basically finds functions that take a low dimensional input space and map it to a high dimensional space, so that what used to be not linearly separable now becomes such.\n\nFinally we can take the solution (the linear separation) from the high dimensional space to the low dimensional space and deduct a non-linear separation.\n\nThere are three parameters in SVM:\n\n-Kernel can take different \u201cvalues\u201d such as linear, poly, ref,..\n\n-C controls the tradeoff between a smooth decision boundary and classifying the points correctly. A large value of C favors a more intricate decision boundary (getting therefore more training points correctly) versus a smoother separator.\n\n-Gamma defines how much influence a single training example can have. The larger the value of gamma, then the closer other examples must be to be affected.\n\nThe choice of these parameters affects the performance of the SVM algorithm.\n\nTo conclude, SVM performs well in complicated domains with a clear marginal separation, while it doesn\u2019t perform well with large datasets and with a lot of noise. In this last case, the Naive-Bayes approach is a better choice of algorithm."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-neural-network-e11ef86f8da4",
        "title": "Machine Learning: Neural Network \u2013 Machine Learning bites \u2013",
        "text": "Another machine learning model is the \u201cneural network\u201d. It was inspired by how the real neurons interact with each other and it simplifies their activity in this form: we have different inputs x with different weights (w); the sum of the inputs by their weights triggers a certain activation function, which gets compared to a firing threshold which defines an output y.\n\nA perceptron unit is a linear function which acts as described above and which has an output of 1 or 0. Therefore the result of a perception unit is a line that separates two different planes where on one side the output is zero and on the other it is equal to 1.\n\nFunctions like \u201cAND\u201d, \u201cOR\u201d and \u201cNOT\u201d are expressible as perceptron units.\n\nIn regards to training a perceptron when given examples it\u2019s a matter of finding the weights that map the inputs to the outputs.\n\nIn neural network there are two cases which we are going to consider:\n\nWhere we have: x as input; y as target; y^ as output; n as learning rate; w as weight.\n\nThe perceptron tries to find the delta-weight needed to adjust the weight in order to find the correct output when compared to a threshold.\n\nThat way it finds the separation line that differentiates the training examples.\n\nWhen the data sets are linearly separable, then the perceptron will always find the line that separates them.\n\nThe gradient descent is instead used when there is non-linear separability among the data set and the output is not thresholded, but continuous.\n\nIn this case we have an \u201cactivation function\u201d (\u201ca\u201d); we have an error which is defined as the sum of the squared values of the difference between the target and the activation. We then try to minimize this error by applying the derivative of it over the weights.\n\nThe main characteristics of the two rules are the following:\n\n- A perceptron guarantees to converge if the data are linearly separable.\n\n- The gradient descent is more robust and is applicable to data sets that are not linearly separable. On the other hand, it can converge to a local optimum, failing to identify the global minimum.\n\nWhile in the perceptron rule we were calculating this delta weights as a function of the difference between the target and the thresholded output, now with gradient descent instead of the thresholded output we use the activation function.\n\nThe two formulas would be the same if indeed the activation function was thresholded, therefore the same as y^.\n\nWe can apply gradient descent on the activation function since it is a continuous function, while we cant apply a gradient descent to a thresholded output (y^) since it is discontinuous, as it jumps in a step from value 0 to value 1.\n\nA discontinuous function is not differentiable. To overcome this issue we can introduce a differentiable threshold, defined as \u201csigmoid\u201d. It is a function that instead of sharply going from 0 to 1, it smoothly transitions from one to another.\n\nThe neural network is defined as a chain between input layers, hidden layers and output layers.\n\nThe units in each hidden layer are \u201csigmoid units\u201d (computed with the weighted sum, sigmoided, of the layer unit before it).\n\nUsing these sigmoid units we can construct a chain of relationship between the input layer (x) with the output layer (y).\n\nThe use of the sigmoid function is important in the \u201cback propagation\u201d where we are able to flow the error info from the output to the input, computationally defining the chain rule between inputs and outputs throughout different unit layers.\n\nRunning gradient descent allows us to find the weights that define this network.\n\nUnfortunately, though, as mentioned before, gradient descent can find many local optima.\n\nTo avoid this issue there are many advanced methods that can be used, such as momentum, higher order derivatives, random optimization and penalty for complexity.\n\nThe last one sounds familiar as it has been used also in the previous methods: regression and decision tree.\n\nIn a regression, to avoid overfitting, in fact, we were penalizing the order of the polynomial. In a decision tree, instead, the penalty was on the size of the tree.\n\nIn a neural network the penalty is applied when there are many nodes, many layers and large numbers of the weights.\n\nFinally, let\u2019s evaluate the restriction and preference bias for this neural network model.\n\nLet\u2019s recall what they are:\n\n- a restriction bias tells us what we are able to represent.\n\n- a preference bias tells us something about the algorithm we are using; given two representations it tells us why it prefers one over the other. (In the decision tree the preference bias was for shorter trees, correct trees, etc.)\n\nIf a network structure is sufficiently complex then the neural network method does not have many restriction biases. The only danger is overfitting.\n\nAlso in this case, the use of cross-validation is helpful. It can identify how many hidden layers to use, the nodes in each layer or when to stop training because the weights are getting too large.\n\nThe preference bias for the neural network is for simpler explanations, better generalizations with simpler hypothesis. It follows the \u201cOccam\u2019s Razor\u201d rule: entities should not be unnecessarily more complex.\n\nAn important step to guide us to this preference is given by the selection of the initial weights. These get chosen as small random values. The \u201csmall\u201d characteristic is chosen because of the low complexity they bring (let\u2019s remind that there is in fact a penalty for big weights). The \u201crandomness\u201d selection, instead, gives variability and it makes us avoid local minima."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-supervised-learning-classification-4f44a91d767",
        "title": "Machine Learning : Supervised Learning -classification",
        "text": "Before diving into a classification learning model, I would like to define key words used in supervised learning.\n\n- Instances: vectors of attributes that define what the input space is.\n\n- Concept: the function that maps inputs to outputs.\n\n- Target concept: the answer we are looking for; what we are trying to find.\n\n- Hypothesis class: a set of all concepts (functions) that we are willing to entertain.\n\n- Sample: the training set. The collection of all the inputs paired with a label, which is ultimately the correct output.\n\n- Candidate: the concept that might be the target concept.\n\n- Testing set: used to verify the candidate concept, looking if it does a good job on the testing set, therefore if it can generalize well.\n\nOne of the models used for classification is the decision tree. (more on this at the following blog: https://medium.com/@mikecavs/machine-learning-decision-tree-classifier-9eb67cad263e#.38pwmn9xv)\n\nTo start a decision tree we need to identify the features about our inputs and evaluate them in order to reach the output.\n\nWe start from a particular attribute of the inputs and ask a question about it, represented by a node. The response to the question is the edge of the tree, which leads to other nodes or to the actual output.\n\nThe best way to start is to first pick the attribute that best narrows down the possibilities and then move on, following the answer path, until we reach the answer.\n\nAs we start getting familiar with this model, we can soon realize that each case could be depicted by different decision trees. So it\u2019s important to find the one that starts with the best attribute and reaches the answer faster.\n\nThe best attribute can be defined as the one that provides the largest gain in amount of information we can gather.\n\nAs a matter of fact, the gain of picking a certain attribute among a collection of training examples is mathematically defined as the difference between the measure of randomness and the average entropy over each set of examples.\n\nThe best attribute is simply the one which minimizes the entropy, therefore maximizes the gain.\n\nThe algorithm that reflects this statement and therefore picks the best decision tree is the ID3 algorithm. It\u2019s defined as a \u201ctop-down\u201d learning algorithm.\n\nIt basically creates a good split at the top of the tree and finds the shortest tree."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-decision-tree-classifier-9eb67cad263e",
        "title": "Machine Learning: Decision Tree Classifier \u2013 Machine Learning bites \u2013",
        "text": "As noted in a previous blog (https://medium.com/@mikecavs/machine-learning-supervised-learning-classification-4f44a91d767#.5rqr5srd1), a decision tree classifier deals with different parameters and, depending on the response over each parameter, splits the data until a final answer is reached.\n\nWhat the machine does is to select the best attribute that can split the data and can give as much information as possible. That is how the machine selects the best decision tree among many.\n\nAlthough, it is important to understand that the more we split the data the more we risk overfitting the data.\n\nThis concept can be understood by taking a closer look at one of the parameters available in the \u201csklearn\u201d library, used to build this sort of classifier.\n\n\u201cmin_samples_split\u201d is the parameter that tells the machine the \u201cgranularity\u201d of the data we want to reach. The default value is 2, which means that the algorithm will continue to split the data until it reaches a node where there are at most 2 samples.\n\nIntuitively, we can gather that if we are in this condition, than there is a high probability of overfitting, since the algorithm will try to cover pretty much all the data. In fact, if we increase this value we can see that the complexity and the \u201cdepth\u201d of the tree diminishes and therefore it avoids overfitting our data.\n\nSurprisingly, increasing the \u201cmin_samples_split\u201d parameter increases also the accuracy of the algorithm.\n\nOne of the main, fundamental concepts to understand how a decision tree classifier works, is introduced by \u201centropy\u201d. It is a measure of the \u201cimpurity\u201d found in a bunch of examples.\n\nEntropy controls where a decision tree decides to split the data.\n\nIn a decision tree we try to find variables and split points that make the subsets as pure as possible, therefore with the lowest value of entropy.\n\nIf all the examples are of the same class then the entropy value is zero. So we want to arrive to a node that splits the data into 2 complete separate classes, meaning each side of the node has zero entropy.\n\nOn the other hand, if the examples are evenly split between different classes, then the entropy is at the maximum, 1.\n\nTo further understand this concept, it is important to reference the \u201cinformation gain\u201d which is intrinsically connected to the entropy.\n\nIn mathematical terms, the \u201cinformation gain\u201d is defined by the \u201centropy of the parent\u201d less the \u201cweighted average entropy of the children\u201d.\n\nA decision tree algorithm aims to maximize the information gain. This is how the machine chooses which feature to split on.\n\nSo we simply want to arrive to a node that splits the data \u201cclearly\u201d.\n\nLet\u2019s say we have a bunch of 12 examples containing two different classes, 6 identified as A and 6 as B. If a node further splits the data into two smaller samples, each one containing 3 units of A and 3 units of B, we can clearly see that we haven\u2019t gained much information compared to before the split. We just have divided the 12 examples into two parts of 6 but the content of each is still \u201cunclear\u201d. We still don\u2019t know which one to choose as they both contain the same amount of A and B.\n\nIf, instead, the node splits the data in two parts where on one side we have 6 units of A and on the other we have 6 units of B, then we have much clearer information about the data. On one side we have all the As and on the other all the Bs. Therefore we gained the greatest amount of information to properly answer a question (which could be \u201cwhich side should I go if I want have an A?\u201d).\n\nThe entropy on each side of the data is zero. Both are \u201cpure\u201d as they contain examples of the same class (A or B)."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-supervised-learning-regression-d0b4f3762608",
        "title": "Machine Learning: Supervised Learning - regression \u2013 Machine Learning bites \u2013",
        "text": "Supervised learning involves data with features as inputs and labels as output. (read more at: https://medium.com/@mikecavs/machine-learning-the-basics-c7487ac35c36#.o3p815h0l)\n\nWe predict future data considering the features (attributes) which identified the training data collected.\n\nThe first takes certain inputs X and maps them to some discrete labels. The latter, instead, deals with continuous values functions.\n\nLet\u2019s start digging into the details of the regression model.\n\nAs already mentioned, what differentiates it from classification is the fact that the outputs are continuous instead of discrete.\n\nIn a regression model we try to find a line, linear or polynomial, that best fits the data, meaning that it minimizes the squared errors between each data and the line itself.\n\nThe model that we need to find has to be complex enough to fit the training data, but without causing problems on the test set. Therefore without overfitting the training data and misrepresenting the future predictions.\n\nOne method to verify the above is to use cross-validation, which splits the data in different folds and takes one fold as a test set and the rest as training set, and it rotates. Finally it evaluates the average error and we pick the model that has the lowest error. This one would be the best model to be used.\n\nOne quick note on the use of the squared error, as one could argue that minimizing the sum of absolute errors could be sufficient.\n\nThe main reason we use squared error is that there can be multiple lines that minimize the sum of absolute errors, while there is only one line that minimizes the sum of squared errors.\n\nAs a matter of fact let\u2019s visualize some scattered data which can be fit by a straight line. Now, let\u2019s consider another line parallel to it. Just looking at two points and sketching the distance (error) between the points and the lines, we can see that the sum of the distances from the point tp one line or the other is the same, while the sum of the squared distances varies.\n\nFortunately we can use another value, the \u201cR-square\u201d. This answers the following question: \u201chow much of my change in the output Y is explained by the change in the input X?\u201d\n\nIt describes the relationship between input and output and it\u2019s independent of the number of training examples we use.\n\nIts value ranges from 0 to 1. Where 1 identifies the closest relationship between X and Y and zero the opposite spectrum."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-causes-of-error-87ff372cd5be",
        "title": "Machine Learning: Causes of error \u2013 Machine Learning bites \u2013",
        "text": "In machine learning, results of an algorithm implementation are affected by errors. There are two main causes of errors that we need to analyze in order to improve our algorithm:\n\nThe first one implies that our model is not complex enough to capture the underlying relationships among the data. As a consequence, it misrepresents the data, leading to low accuracy and ultimately \u201cunderfitting\u201d.\n\nOne simple, but good graphical representation of this case is depicted by scattered data and a line passing through them. The line is our model prediction. We can infer that the error between the single data and the line (represented by the distance of each data from the line, along the y-axis) can vary from point to point.\n\nThe second form of error, instead, is represented by the variance, which is a measure on how the predictions vary for any given test sample. Too much variance indicates that the model is not able to generalize its predictions to the larger population, but it only fits very well the existing training data. This leads to \u201coverfitting\u201d. The reason for this could be that either the model is too complex, as it perfectly follows the training data, or that we don\u2019t have much data to support it.\n\nA visual representation is in this case constituted by scattered data and a curved line that basically connects each data, leaving zero or minimal error between each data and the line itself.\n\nAs we can imagine this line \u201cshapes\u201d perfectly the progression of the training data, but it could fail miserably on predicting future ones.\n\nIn summary, a high bias is reached in the following circumstances: when a model that pays little attention to the training data, oversimplifies the prediction, underfits the data, leads to high errors on training set and uses few features.\n\nOn the contrary, high variance is caused by a model that pays too much attention to the training data and does not generalize well, overfits the training data, has much higher error on the test set than on the training set, uses many features.\n\nA final note about the number of features and why a high number of them causes high variance.\n\nLet\u2019s think of the number of features as dimensions. As they grow, the amount of data that we need to generalize, grows exponentially.\n\nThis is defined as the \u201ccurse of dimensionality\u201d.\n\nTo put these words into a graphical prospective, let\u2019s imagine a line and define the data as the number of points needed to split the line in equal parts. If the line is 10 meters long and we want to split it in pieces of 1 meter length, we will need a total of 10 data. Each data, 1 meter long, will cover a piece of the line.\n\nIf now, instead, we want to \u201csplit\u201d a square, a 2-dimension figure, of 10 by 10 meters sides, we will need 100 features, as big as 10 msq each.\n\nIf ultimately we have a cube, a 3-dimension object, with 10x10x10 sides, the amount of data needed to cover it all would be 1000.\n\nAs we can see, going from 1 dimension (the line) to 3 dimensions (the cube), has exponentially increased the amount of data needed."
    },
    {
        "url": "https://medium.com/machine-learning-bites/machine-learning-the-basics-c7487ac35c36",
        "title": "Machine Learning: the basics \u2013 Machine Learning bites \u2013",
        "text": "Let\u2019s start with what machine learning is. From the many different descriptions I read and heard, the one that stuck to me the most is the following.\n\nMachine Learning is \u201ccomputational statistics\u201d; it means building computational artifacts that can learn over time based on experience. In layman terms, it provides statistical data to a machine, which, through a built-in algorithm, can analyze the given data set, make assumptions, and evaluate further data provided.\n\nNow that we got the main question out of the way, let\u2019s dive deeper into the three main categories of machine learning: supervised learning, unsupervised learning and reinforcement learning.\n\nSupervised learning entails taking a labeled data set, gleaning info from it so I can label a new data set. So, the data consists of an input variable (what is technically called a feature or multiple features) and an output variable (a label, a target) connected to the variable.\n\nIn simpler terms: we, as programmers, tell the machine that a certain input triggers a certain output. For example, we can tell a computer wether a certain image can be classified as an image of a cat or not.\n\nIt\u2019s simply an induction. The machine extrapolates the general rule, by looking at many examples. (This is the opposite of deduction, which is mainly reasoning; going from a general rule to specifics).\n\nA simple examples is the following: the data set used to train the machine is composed of these inputs and outputs: 1 \u2014 >1; 2 \u2014 >4; 3 \u2014 >9; 4 \u2014 >16; \u2026 therefore the rule, the function behind it that we can deduct is that output = input\u00b2.\n\nThis is what we can evaluate looking at the training data set. Is it 100% certain that this is the rule? Not really, but it\u2019s definitely the best bet. Why not really? Because we are extrapolating a general rule given a cause and an effect just looking at a limited amount of examples. But, as I said, it\u2019s the best bet, the best function approximation.\n\nFor the unsupervised learning instead, we just have inputs without any label. Therefore just from observing these inputs themselves we need to derive some structure, some \u201cdescription\u201d of the data.\n\nIt\u2019s like having a bunch of t-shirts and without knowing anything about them (i.e. which size they are,etc..), we try to see what the \u201cstructure\u201d among them is, what are the features that characterize them. We basically classify the t-shirts among them, based on some characteristics we observe.\n\nFinally, the last section of machine learning is reinforcement learning. This is more intuitive as it basically means learning to be better by experience or as some put it (in more technical terms) \u201clearning from delayed reward\u201d.\n\nJust think about playing a game without knowing the rules, but learning them as you go. The \u201cdelayed reward\u201d is inferred by the fact that you first see the result (win or loss) and from there you infer the move that made you win or lose.\n\nHopefully this blog gave you a better understanding of the general terms and sparked some interest in machine learning. Furthermore, I believe that just this brief description of the topic can show how vast this science is and how many fields it can impact. I can barely find some examples where machine learning is not applicable. It is definitely a better way of interacting with a machine, than traditional coding. Machine learning is more like educating a child, while coding is telling a child exactly what to do."
    }
]