[
    {
        "url": "https://medium.com/emergent-future/emergent-future-ai-at-the-un-ml-production-and-rip-theano-c90fc8a218c9?source=---------0",
        "title": "Emergent Future // AI at the UN, ML Production, and RIP Theano",
        "text": "This week we check out the new AI center at the UN, what Microsoft is up to with Azure ML, and learn that Theano will be sunset. Plus, our favorite reads from the past week and some projects to try at home!\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nThe United Nations announced the establishment of an AI center in the Hague, Netherlands.\n\nThe Center for Artificial Intelligence and Robotics will work to educate and inform policy-makers about the opportunities and implications of robotics and artificial intelligence on society.\n\nThe Centre will contribute to the understanding of AI and Robotics through coordination, knowledge sharing, training, and awareness-raising and outreach activities while monitoring global developments that contribute to policymaking.\n\nThe Center for Robotics and AI launched in 2014 but just now signed a Host Country Agreement.\n\nMicrosoft launched two new machine learning services, giving developers more flexibility for experimentation and optimized model serving.\n\nAs Machine Learning edges its way into every application, a deployment layer is becoming essential to scaling. Here at Algorithmia we needed to solve this problem early to run +4,000 models for +50,000 developers.\n\nWe\u2019re glad to see a big cloud provider jumping with platform-specific solutions to popularize the need for an AI deployment layer \u2014 we\u2019ll continue to blaze the trail with Algorithmia Enterprise.\n\nThe Montreal Institute of Learning Algorithms will stop development on the Theano deep learning framework after the release of version 1.0.\n\nIn a post to the Theano user group, Yoshua Bengio said Theano would receive \u201cminimal maintenance\u201d moving forward, but in a year the would stop actively implementing new features.\n\n\u201cSupporting Theano is no longer the best way we can enable the emergence and application of novel research ideas,\u201d Bengio wrote. \u201cEven with the increasing support of external contributions from industry and academia, maintaining an older code base and keeping up with competitors has come in the way of innovation.\u201d"
    },
    {
        "url": "https://medium.com/emergent-future/microsofts-agi-lab-google-s-gradient-ventures-and-the-state-of-ai-and-ml-in-2017-548b3214b810?source=---------1",
        "title": "Microsoft\u2019s AGI Lab, Google\u2019s Gradient Ventures, and the State of AI and ML in 2017",
        "text": "Microsoft is launching an AI research lab to challenge Google and DeepMind.\n\nMore than 100 scientists from across various disciplines of artificial intelligence research will work toward building a more general artificial intelligence \u2014 a single system that can solve a range of problems compared to \u201cnarrow\u201d AI, which can only solve the specific problem it was trained to do.\n\nGoogle is attempting to tackle the same challenge of generalized AI via both its own Google Brain project and through efforts at DeepMind.\n\nThe new lab, called Microsoft Research AI, will be based in Redmond, Washington and will focus on \u201cbuilding AI advances that amplify human ingenuity, and also that reflect our shared societal values and expectations. The AI tools and services we create must assist humanity and augment our capabilities.\u201d\n\nYou might have heard\u2026 Google launched Gradient Ventures, an AI-focused venture fund to provide startups with funding and connect them with Google\u2019s AI-related resources in an effort to help founders navigate the challenges in developing AI-based products.\n\nGradient has made four investments so far, including leading Algorithmia\u2019s Series A investment round.\n\nBut did you know\u2026 Google also launched PAIR \u2014 the People + AI Research Initiative \u2014 that will focus on the relationship between users and technology. They\u2019ll be open sourcing tools and research based on learnings from AI practitioners from across Google.\n\nMcKinsey interviewed more than 3,000 senior executives on the use of AI technologies and here\u2019s what they found:"
    },
    {
        "url": "https://medium.com/emergent-future/doctor-ai-building-an-ai-os-project-to-try-at-home-what-were-reading-and-more-ab03a90dfc10?source=---------2",
        "title": "Doctor AI, Building an AI OS, Project to Try at Home, What We\u2019re Reading, and More",
        "text": "A team of researchers at Stanford University have a machine learning model that can identify heart arrhythmias from an electrocardiogram better than a doctor.\n\nResearchers collected 30,000 30-second clips from patients with different forms of arrhythmia and trained a 34-layer convolutional neural network, which maps a sequence of ECG samples to a sequence of heart rhythms.\n\nCommittees of board-certified cardiologists annotate a gold standard test set on which we compare the performance of our model\n\nCompared to six individual cardiologists, the model exceeded the average cardiologist performance in both recall and precision.\n\nBut that\u2019s not all\u2026 a collaboration between S\u00e3o Paulo State University and London\u2019s Boston Place Clinic were able to select viable embryos for in vitro fertilization.\n\nUsing just 24 key characteristics, such as morphology, texture, and the quantity and quality of the cells present, the AI was able to pick viable embryos 76% of the time.\n\nBetween 30% to 60% of seemingly viable embryos fail to implant in the uterus."
    },
    {
        "url": "https://medium.com/emergent-future/deep-learning-dot-ai-a-neural-net-learns-to-reason-and-some-company-news-c9e31e49ecea?source=---------3",
        "title": "Deep Learning Dot AI, a Neural Net Learns to Reason, and some company news",
        "text": "You might have heard: Andrew Ng announced his new ventureDeeplearning.ai a few days ago.\n\nNg left his role as chief scientist of Baidu three months ago. At the time he said he was \u201clooking into quite a few ideas in parallel, and exploring new AI businesses that I can build.\u201d\n\nThe announcement includes only a name logo, a domain name, and a note about an August launch date.\n\nBut did you know\u2026 Deep learning is impacting everything from healthcare to transportation to manufacturing, and more. Companies are turning to deep learning to solve hard problems, like speech recognition, object recognition, and machine translation. Here\u2019s a brief introduction to deep learning."
    },
    {
        "url": "https://medium.com/emergent-future/building-an-operating-system-for-ai-bd2f43361836?source=---------4",
        "title": "Building an Operating System for AI \u2013 Emergent // Future \u2013",
        "text": "This post is a summary of a talk I recently gave at the 2017 GeekWire Tech Cloud Summit, titled \u201cBuilding an Operating System for AI\u201d. You can listen to the original talk from here. For better view of the slides, you can follow along from here.\n\nThe operating system on your laptop is running tens or hundreds of processes concurrently. It gives each process just the right amount of resources that it needs (RAM, CPU, IO). It isolates them in their own virtual address space, locks them down to a set of predefined permissions, allows them to inter-communicate, and allow you, the user, to safely monitor and control them. The operating system abstracts away the hardware layer (writing to a flash drive is the same as writing to a hard drive) and it doesn\u2019t care what programming language or technology stack you used to write those apps \u2014 it just runs them, smoothly and consistently.\n\nAs machine learning penetrates the enterprise, companies will soon find themselves productionizing more and more models and at a faster clip. Deployment efficiency, resource scaling, monitoring and auditing will start to become harder and more expensive to sustain over time. Data scientists from different corners of the company will each have their own set of preferred technology stacks (R, Python, Julia, Tensorflow, Caffe, deeplearning4j, H2O, etc.) and data center strategies will shift from one cloud to hybrid. Running, scaling, and monitoring heterogeneous models in a cloud-agnostic way is a responsibility analogous to an operating system \u2014 that\u2019s what we want to talk about.\n\nAt Algorithmia.com, we run 3,500+ algorithms (each has multiple versions, taking the figure up to 40k+ unique REST API endpoints). Any API endpoint can be called anywhere from once a day to a burst of 1,000+ times a second, with a completely no-devops experience. Those algorithms are written in any of the eight programming languages we support today, can be CPU or GPU based, will run on any cloud, can read and write to any data source (S3, Dropbox, etc.) and operate with a latency of ~15ms on standard hardware.\n\nWe see Algorithmia.com as our version of an OS for AI and this post will share some of our thinking.\n\nMachine and deep learning is made up of two distinct phases: training and inference. The former is about building the model, and the latter is about running it in production.\n\nTraining a model is an iterative process that is very framework dependent. Some machine learning engineers will use Tensorflow on GPUs, others will use scikit-learn on CPUs, and every training environment is a snowflake. This is analogous to building an app, where an app developer has a carefully put-together development toolchain and libraries, constantly compiling and testing their code. Training requires a long compute cycle (hours to days), is usually fixed load input (meaning you don\u2019t need to scale from one machine to X machines in response to a trigger), and is ideally a stateful process whereby the data scientist will need to repeatedly save progress of their training, such as neural network checkpoints.\n\nInference on the other hand is about running that model at scale to multiple users. When running numerous models concurrently, each written in different frameworks and languages, that\u2019s when it becomes analogous to an operating system. The OS will be responsible for scheduling jobs, sharing resources, and monitoring of those jobs. A \u201cjob\u201d is an inference transaction, and unlike in the case of training, requires a short burst of compute cycle (similar to an SQL query), elastic load (machines need to increase/decrease in proportion to inference demand), and is stateless, where the result of a previous transaction does not impact the result of the next transaction.\n\nWe will be focusing on the Inference side of the equation.\n\nWe\u2019ll be using serverless computing for our operating system, so let\u2019s take a moment to explain why serverless architecture makes sense for AI inference.\n\nAs we explained in the previous section, machine learning inference requires a short compute burst. This means a server that is serving a model as a REST API will be idle. When it receives a request, say, to classify an image, it will burst CPU/GPU utilization for a short period of time, return the result, then resume to being idle. This process is similar a database server that is idle until it receives an SQL query.\n\nBecause of this requirement, AI inference is a perfect fit for serverless computing. Serverless architecture has an obvious scaling advantage and an economic advantage. For example, let\u2019s say you\u2019ve built an app called \u201cSeeFood\u201d which classifies images as Hotdog and Not Hotdog. Your app goes viral and is now on the top charts. Here\u2019s how your daily app usage might look like.\n\n\u201cSeeFood\u201d Daily App Usage. It\u2019s very popular during lunch time.\n\n Y axis is \u201ccalls per second\u201d. X axis is \u201chour of the day\u201d.\n\nIn this fictional scenario, if we were to use traditional (fixed scale) architecture, then we\u2019d be paying for 40 machines per day. This is the area highlighted in green below. With standard cloud pricing, this might cost us around $648 * 40 = $25,920 per month.\n\nIf instead we used a auto-scaling architecture (where we would add or remove machines every hour) then we\u2019d be paying for on average 19 machines per day. This is the area highlighted in green below. That\u2019s a total of $648 * 19 = $12,312 per month.\n\nAnd finally, if we use a serverless architecture, then we\u2019d be paying for (theoretically) exactly the amount that we use, and not paying for the idle time. This is all the blue area in the chart below. The cost in this fictional scenario is tricky to calculate \u2014 it comes down to an average of of 21 calls / sec, or equivalent of 6 machines. That\u2019s $648 * 6 = $3,888 per month.\n\nServerless Architecture \u2014 Design for Minimum\n\n Avg. of 21 calls / sec, or equivalent of 6 machines. $648 * 6 = $3,888 per month\n\nIn this (fictional) scenario, our cloud computing cost went from ~$26k to ~$4k. That\u2019s a great reason to use Serverless Computing, in addition to other advantages such as simpler development (functions are encapsulated as atomic services), lower latency (when used with edge computing), and rolling deployment capabilities.\n\nBuilding a serverless architecture from scratch is not a particularly difficult feat, especially with recent devops advancements. Projects like Kubernetes and Docker will greatly simplify the distribution, scaling, and healing requirements for a function-as-a-service architecture.\n\nExample FaaS Architecture\n\n An API call is routed to an API Server then routed to a warmed up Docker container.\n\n The container loads and executes the code just-in-time.\n\nAt this stage, we have table stakes for a serverless architecture. Building this and stopping there is the equivalent of using AWS Lambda. However for our system to be more valuable to AI workflows, we need to accommodate additional requirements such as GPU memory management, composability, cloud abstraction, instrumentation, and versioning to name a few.\n\nAnything we build on top of our function-as-a-service platform is what really defines our operating system, and that\u2019s what we\u2019ll talk about next.\n\nSimilar to how an operating system is made up of Kernel, Shell, and Services, our operating system will consist of those components as well.\n\nThe Shell is the part that the user interacts with, such as the website or API endpoint. Services are pluggable components of our operating system, such as authentication and reporting. The last layer, the Kernel, is what really defines our operating system. This is the layer we will focus on for the rest of the post.\n\nOur kernel is made up of three major components: elastic scaling, runtime abstraction, and cloud abstraction. Let\u2019s explore each of those components in detail.\n\nIf we know that Algorithm #A will always call Algorithm #B, how can we use that knowledge to our advantage? That\u2019s the intelligent part of scaling up on demand.\n\nComposability is a very common pattern in the world of machine learning and data science. A data pipeline will typically consist of pre-processing stage(s), processing, and post-processing. In this case the pipeline is the composition of different functions that make up the pipeline. Composability is also found in ensembles, where the data scientist runs the model through different models and then synthesize the final score.\n\nHere\u2019s an example: let\u2019s say our \u201cSeeFood\u201d takes a photo of a fruits or vegetables and then return the name of the fruit or the vegetable. Instead of building a single classifier for all food and vegetables, it\u2019s common to break it down to three classifiers, like so.\n\nIn this case, we know that the model on the top (\u201cFruit or Veggie Classifier\u201d) will always call either \u201cFruit Classifier\u201d or \u201cVeggie Classifier\u201d. How do we use this to our advantage? One way is to instrument all the resources, keeping track of what CPU level, memory level, and IO level each model consumes. Our orchestrator can be designed to use this information when stack those jobs, in a way that reduces network or increase server utilization (fitting more models in a single server).\n\nThe second component in our kernel is runtime abstraction. In machine learning and data science workflows, it\u2019s common that we build a classifier with a certain stack (say R, Tensorflow over GPUs) and have pre-processing or adjacent model running on a different stack (maybe Python, scikit-learn over CPUs).\n\nOur operating system must abstract away those details and enable seamless interoperability between functions and models. For this, we use REST APIs and exchange data using JSON or equivalent universal data representation. If the data was too large, we pass around the URI to the data source instead of the actual JSON blob.\n\nThese day we do not expect programmers to be devops. More so, we do not expect data scientists to understand all the intricate details of different cloud vendors. Dealing with different data sources is a great example of such abstraction.\n\nImagine a scenario where a data scientist creates a model that classifies images. Consumers of that model might be three different personas: (a) backend production engineers might be using S3, (b) fellow data scientists might be using Hadoop, and (3) BI users in a different org might be using Dropbox. Asking the author of that model to build a data connector for each of those sources (and maintain it for new data sources in the future) is a distraction from their job. Instead, our operating system can offer a Data Adapter API that reads and writes from different data sources.\n\nIn the above code shows two examples: reading data without\n\n abstraction, and reading data with abstraction.\n\nIn the first block, not having storage abstraction requires us to write a connector for each data source (in this case S3) and hard code it in our model. In the second block, we use the Data Adapter API which takes in a URI to a data source and automatically injects the right data connector. Those URIs can point to S3, Azure Blob, HDFS, Dropbox, or anything else.\n\nOur operating system so far is auto-scaling, composable, self-optimizing, and cloud-agnostic. Monitoring your hundreds or thousands of models is also a critical requirement. There is however room for one more thing: Discoverability.\n\nDiscoverability is the ability to find models and functions created by:\n\nOperating systems are not the product, they are the platform. If you examine how operating systems evolved over time, we went from punched cards, to machine language, to assemblers, and so on, slowly climbing the ladder of Abstraction. Abstraction is about looking at things as modular components, encouraging reuse, and making advanced workflows more accessible. That\u2019s why the latest wave of operating systems (the likes iOS and Android) came with built-in App Stores, to encourage this view of an OS.\n\nFor the same reasons, our operating system must place much emphasis on Discoverability and Reusability. That\u2019s why at Algorithmia we created CODEX, our on-prem operating system for AI, and Algorithmia.com, our app store for algorithms, functions and models. We help companies make their algorithms discoverable to every corner of their organization, and at the same time, we give those companies (and indie developers) access to the best third-party algorithms via Algorithmia.com."
    },
    {
        "url": "https://medium.com/emergent-future/ai-policy-and-strategy-negotiating-chatbots-top-reads-and-more-1ed386ed77b0?source=---------5",
        "title": "AI Policy and Strategy, Negotiating Chatbots, Top Reads, and more!",
        "text": "Artificial intelligence represents one of the highest-impact opportunities to improve the world.\n\nAI policies, from liability issues with driverless cars to algorithmic bias, are often neglected by most researchers, which could result in disaster.\n\n80,000 Hours and AI researcher Miles Brundage have put together a guide to working in AI policy and strategy that helps frame the short-term and long-term challenges of AI policy and how to make progress while reducing risk.\n\nThe guide provides questions policy-makers should be asking as well as providing possible career trajectories for those interested in AI policy."
    },
    {
        "url": "https://medium.com/emergent-future/apple-core-ml-kaggle-competitions-projects-to-try-at-home-3dfadb286d79?source=---------6",
        "title": "Apple Core ML, Kaggle Competitions, Projects to Try at Home",
        "text": "Apple\u2019s announced a bevy of new products and services at their annual WWDC event last week, including iMac updates and the new HomePod speaker.\n\nBut, tucked away in the flurry of news, Apple unveiled Core ML, their new programming framework designed to make it easier to run machine learning models on mobile devices.\n\nCore ML will be part of iOS 11, expected to launch later this year, and allows developers to load trained machine learning models onto iOS devices for use in apps. The new framework makes it easier for apps to process data locally using ML without sending user information to the cloud.\n\n\u201cWe want to make powerful machine learning easy for you to incorporate into your apps,\u201d software chief Craig Federighi said, announcing new computer vision and NLP APIs for developers to leverage in an effort to join Google, Facebook, Microsoft, and Amazon as a developer\u2019s best friend."
    },
    {
        "url": "https://medium.com/emergent-future/putting-alphago-in-context-building-a-deep-learning-box-and-more-d9c836dbf56e?source=---------7",
        "title": "Putting AlphaGo in context, building a deep learning box, and more",
        "text": "After AlphaGo\u2019s historic victory, the media focused on questions like, \u201cto what extent is AlphaGo a breakthrough?\u201d, \u201cHow do researchers in AI see its victories?\u201d and \u201cwhat implications do the wins have?\u201d\n\nBut, if we step back, we can see that AlphaGo is actually a narrow AI system that can only play Go and that\u2019s it. It cannot be generalized to solve any problem other than playing Go.\n\nSo, while AlphaGo doesn\u2019t represent a fundamental breakthrough in AI algorithmically, it does show how Alphabet\u2019s (Google\u2019s) AI resources can be strategically deployed to solve problems. In short, the researchers and developers behind the components can be generalized to solve problems from data center heating and cooling efficiencies to improving Google search."
    },
    {
        "url": "https://medium.com/emergent-future/gpus-tpus-and-deep-learning-whats-right-for-you-31befef028b2?source=---------8",
        "title": "GPUs, TPUs, and Deep Learning \u2014 What\u2019s Right For You?",
        "text": "You Might Have Heard\u2026 Google announced its second-generation Tensor Processing Units, which is optimized to both train and run machine learning models.\n\nEach TPU includes a custom high-speed network that allows Google to build machine learning supercomputers, called \u201cTPU pods.\u201d The pods contain 64 second-generation TPUs and provides up to 11.5 petaflops to accelerate the training of a single large machine learning model.\n\nGoogle will also provide select researchers with a cluster of 1,000 Cloud TPUs at no charge to support the next wave of breakthroughs?\n\nThe goal is to ensure that the most promising researchers have access to enough compute power train machine learning models. Google\u2019s setting up a program to accept applications and will evaluate applications on a rolling basis.\n\nBut did you know\u2026 Android app developers will soon have a specialized version of TensorFlow to work with on mobile devices. TensorFlow Lite, which will be part of the TensorFlow open source project, will let developers use machine learning for their mobile apps."
    },
    {
        "url": "https://medium.com/emergent-future/cloud-gpus-the-ai-playbook-neural-machine-translation-and-video-metadata-extraction-ee2602ebfa51?source=---------9",
        "title": "Cloud GPUs, the AI Playbook, Neural Machine Translation, and Video Metadata Extraction",
        "text": "NVIDIA launched its GPU Cloud Platform last week to simplify AI development, by combining the latest GPUs with optimized deep learning frameworks.\n\nDevelopers and data scientists have to overcome two main challenges with deep learning: The first is to get the required deep learning framework, libraries, operating system, and drivers setup. The other is access to the GPUs to train a neural network.\n\nTheir new cloud platform will make it easier for developers to train and experiment with deep learning by giving them\n\nWhen you\u2019re ready to deploy your model as a scalable service, check out what we\u2019ve learned from deploying the five most popular deep learning frameworks in the cloud."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-rethinking-human-computer-interaction-data-science-platforms-for-enterprise-2179eb4f2830",
        "title": "Emergent // Future: Rethinking Human-Computer Interaction, Data Science Platforms for Enterprise\u2026",
        "text": "Sharing, reusing, and running data science models at scale is not typically part of the data scientist\u2019s workflow and this inefficiency is amplified in a corporate environment where data scientists need to coordinate every move with IT, continuous deployment is a mess, and code reusability is low.\n\nThat\u2019s why a data science platform is a necessity for every data science team, big or small.\n\nBy centralizing everything except the training, you can focus on building a registry of your models, showing the lineage of how they progressed from one version to the next by making them available as self-contained artifacts that are ready to be plugged into any data pipeline.\n\nThis reduces duplication of efforts across teams, identifies and surfaces the best internal and external models, and creates the capability for an AI strategy.\n\nLearn more about how to collaborate with internal stakeholders to build a better data science and machine learning platform."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-amazons-echo-look-machine-learning-models-at-scale-and-more-8212a0f533a",
        "title": "Emergent // Future \u2014 Amazon\u2019s Echo Look, Machine Learning models at scale and more",
        "text": "Amazon debuted the Echo Look, an Alexa-enabled speaker that uses computer vision to tell people what clothes to wear.\n\nOf course that\u2019s just the start. Security, mood monitoring, and other visual services seem like obvious next steps.\n\nIt\u2019s a smart way for Amazon to begin collecting data about what we like to wear. By adding a camera and moving its personal voice assistant from the kitchen to the bedroom, Amazon is familiarizing us with the idea that wherever we are in our homes (or our cars), Amazon is at our beck and call."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-facebooks-ai-research-google-s-speech-recognition-and-more-39e4123cef4f",
        "title": "Emergent // Future \u2014 Facebook\u2019s AI Research, Google\u2019s speech recognition and more",
        "text": "The Facebook Artificial Intelligence Research \u2014 known as FAIR \u2014 is focused on a singular goal: to create computers with intelligence on par with humans.\n\nFAIR is improving computers\u2019 ability to see, hear, and communicate on their own, and the group\u2019s learnings are permeating all of Facebook\u2019s products, from the News Feed to cameras and photo filters. And Facebook is investing, big time.\n\nThis is all led by Yann LeCun, a 56-year-old academic who\u2019s once-rejected theories about artificial intelligence are now considered world-class.\n\nWhile still far from its finish line, the group is making the sort of progress few believed possible just 15 years ago."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-google-doodles-canadas-ai-hub-tpus-vs-gpus-and-more-44a53fe32c24",
        "title": "Emergent // Future \u2014 Google Doodles, Canada\u2019s AI Hub, TPUs vs GPUs, and more",
        "text": "Google taught an AI to draw by using doodles. Last year, Google released Quick, Draw!, a game where you had to draw an image of a word or phrase in less than 20-seconds.\n\nWell, they used some 70,000 Quick, Draw! doodles as training data for Sketch-RNN, a recurrent neural network that can draw stroke-based drawings of objects.\n\nInterestingly, Quick, Draw! recorded not only the final image, but also the order and direction of every pen stroke used to make it. The result was more data about how humans draw."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-humans-in-the-loop-ai-in-the-cloud-and-projects-at-home-5b262e81d7a0",
        "title": "Emergent // Future \u2014 Humans in the Loop, AI in the Cloud, and Projects at Home",
        "text": "Issue 47\n\n This week we check out what it means to have machine learning systems with humans in the loop, how AI in the cloud is the next frontier for Amazon, Microsoft and Google, and our favorite reads and some projects to try at home.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nThough we\u2019ve seen advances in the quality and accuracy of pure machine learning systems, the most accurate paradigms are those that use \u201chuman-in-the-loop.\u201d\n\nWe\u2019re not talking about just labeling data here.\n\nBy adding humans to correct results, we enable the machine learning system to actively learn and correct itself from classifications it initially got wrong. With every iteration (and more classified data) the classification model gets more accurate.\n\nThe next cloud battle between Amazon, Microsoft and Google is about bringing AI to all businesses.\n\nAs companies try to better analyze, optimize, and predict everything from sales cycles to product development, they are turning to AI techniques like deep learning.\n\nThe cloud infrastructure market is worth $25 billion and democratizing access to AI could decide which tech giant emerges as the ultimate winner.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-siri-ously-neural-lace-ai-fail-and-deep-learning-ddr-113817aa420f",
        "title": "Emergent // Future \u2014 Siri-ously, Neural Lace, AI #FAIL, and Deep Learning DDR",
        "text": "Issue 46\n\n This week we check out the future of work, why Elon\u2019s freaked out about AI, learn that we shouldn\u2019t let AI cook for us, and how deep learning is learning to dance. Plus our favorite reads and some projects to try at home.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nYou might have heard\u2026 Treasury Secretary Steve Mnuchin is \u201cnot worried at all\u201d about robots displacing human jobs in the near future. \ud83d\ude33\n\nMnuchin said he wasn\u2019t concerned about the effects of automation, both with physical machines and artificial intelligence, on jobs.\n\nOthers say the robots are coming whether Mnuchin says so or not.\n\nBut did you know\u2026 Elon Musk is on a billion-dollar crusade to stop the A.I. apocalypse? Musk is famous for his futuristic gambles, but Silicon Valley\u2019s latest rush to embrace artificial intelligence scares him. And he thinks you should be frightened too.\n\nThen again, Musk\u2019s latest startup wants to merge human brains with computers. He\u2019s pursuing \u201cneural lace\u201d technology, where tiny electrodes are implanting in the brain in order to (one day) upload and download thoughts.\u200b\n\nA scientist is trying to teach a neural network to cook. Janelle Shane started plugging in cookbooks to see if her neural network could learn food and cooking associations, like pasta goes in water, yogurt needs to be strained, and gelatinous dogs are gross.\n\nA machine learning algorithm watched 35 hours of Dance Dance Revolution gameplay to understand the music and corresponding 350,000 dance steps in order to learn how to create dances of its own.\n\nResearchers combined recurrent and convolutional neural networks to learn to predict steps based on the difficulty of the song.\u200b\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/technical-human-problems-with-anthropomorphism-technopomorphism-13c50e5e3f36",
        "title": "Technical & Human Problems With Anthropomorphism & Technopomorphism",
        "text": "Anthropomorphism is the attribution of human traits, emotions, and intentions to non-human entities (OED). It has been used in storytelling from Aesop to Zootopia, and people debate its impact on how we view gods in religion and animals in the wild. This is out of scope for this short piece.\n\nWhen it comes to technology, anthropomorphism is certainly more problematic than it is useful. Here are two examples:\n\nHistorically, we have used technology to achieve both selfish and altruistic goals. Overwhelmingly, however, technology has helped us reach a point in human civilization in which we are the most peaceful and healthy in history. In order to continue on this path, we must design machines to function in ways that utilize their best machine-like abilities."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-building-robot-factories-cars-driving-cars-and-more-a35f26992ffb",
        "title": "Emergent // Future \u2014 Building Robot Factories, Cars Driving Cars, and More!",
        "text": "Issue 45\n\n This week we check out Y Combinator\u2019s new track for companies applying AI to factories, take a deep dive into the lasted autonomous car news from Nvidia and Uber, and relay our favorite reads from the week.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nY Combinator will offer its first track for AI startups building \u201crobot factories.\u201d\n\nThe thinking goes that even for factories outfitted with state of the art robotics, a lot of the work revolves around setting up and fixing robots. When things break, nobody knows what to do.\n\nYC AI companies will get access to machine learning experts to help overcome technical problems and cloud compute credits for GPU instances.\n\n\u201cWe can\u2019t afford to ignore what might be the biggest technological leap since the Internet,\u201d YC partner Daniel Gross said.\n\nNvidia partners with trucking giant PACCAR to create a proof-of-concept, level 4 autonomous truck. They\u2019ll be using the Nvidia\u2019s Drive PX 2 platform with neural network training fed by data of humans driving semis-tractors.\n\nLevel 4 autonomous vehicles are considered \u201cfully autonomous,\u201d but are limited to the scope of driving scenario \u2014 long-haul trucking in this instance.\n\nUber\u2019s 43 self-driving cars crossed 20,000 mile last week. The cars drove an average of .8 miles before the human driver had to take over for one reason or another.The good news is the number of miles between these \u201ccritical\u201d interventions has improved to approximately 200 miles between incidents.\n\nSelf-driving cars have a spinning-laser problem. Lidar sensors are considered essential for building self-driving cars and pretty much everybody but Tesla is using them. But they\u2019re bulky. And expensive. Just look at how ridiculous this thing is. Experts say that the sensors need improvements to better map a vehicle\u2019s environment in 3-D.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-google-buys-kaggle-a-chatbot-for-refugees-and-more-1fe969753334",
        "title": "Emergent // Future \u2014 Google Buys Kaggle, A Chatbot for Refugees, and more!",
        "text": "Issue 44\n\n This week we take a look at why Google bought Kaggle, a chatbot that gives free legal aid to refugees seeking asylum in US and Canada, our favorite reads, and some things to try at home.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nGoogle acquired the online service that hosts data science and machine learning competitions, immediately broadening its reach within the AI community.\n\nGoogle has faced increasing competition in the AI space as verticals, like autonomous driving and deep learning, open up opportunity for startups to exploit. Buying Kaggle, and its mindshare within the community, will also probably help with recruiting.\n\nMore than 800,000 data experts use Kaggle to explore, analyze and understand the latest updates in machine learning and data analytics.\n\nKaggle and Google Cloud will continue to support machine learning training and deployment services, while offering the community the ability to store and query large datasets.\n\nThe creator of a chatbot, which overturned more than 160,000 parking fines and helped vulnerable people apply for emergency housing, is now turning the bot to helping refugees claim asylum.\n\nThe \u201cRobot lawyer\u201d DoNotPay is giving free legal aid to refugees seeking asylum in US and Canada, and asylum support in UK.\n\nThe chatbot, using Facebook Messenger, can now help refugees fill in an immigration application in the US and Canada. For those in the UK, it helps them apply for asylum support.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-cloud-gpus-deploying-deep-learning-models-applied-ml-and-more-fbd2101dd91a",
        "title": "Emergent // Future \u2014 Cloud GPUs, Deploying Deep Learning Models, Applied ML, and more",
        "text": "Issue 43\n\n This week we look at Google\u2019s new cloud GPUs, how to deploy deep learning models in the cloud, and what applied machine learning looks like at Facebook, Pinterest and others.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nYou Might Have Heard: The Google Cloud Platform got a performance boost with the public beta of NVIDIA Tesla K80 GPU virtual machines.\n\nThe new Cloud GPUs are integrated with Google\u2019s Cloud Machine Learning service and the cost per GPU is $0.70 per hour in the U.S. and $0.77 in the European and Asian data centers.\n\nSo, not cheap. At the same time a Tesla K80 with two cores and 24 GB of Ram will set you back a few thousand dollars.\n\nBut did you know\u2026 it\u2019s still not as easy as spinning up an EC2 instance or DigitalOcean Droplet. Deploying deep learning models in the cloud is still a challenge due to complex hardware requirements and software dependencies.\n\nEvery time you use Facebook, Instagram or Messenger, you\u2019re experiencing artificial intelligence at work. Some within the social network say FB is so reliant on AI that they cannot exist without it.\n\nSo, while the Facebook Artificial Intelligence Research group has been publishing breakthrough after breakthrough on ways to improve the way computers see, hear, and converse, this research doesn\u2019t just find its way into the products.\n\nFB has dispatched an Applied Machine Learning team to create \u201cmagical\u201d experiences by leveraging the machine learning algorithms to improve the results of feeds, ads and search results, and to create new text understanding algorithms that keep spam and misleading content at bay.\n\nMore importantly, however, because Facebook can\u2019t exist without AI, it needs to empower all of the company\u2019s engineers to integrate machine learning in their work. In other words, algorithm development is broken.\n\nBut, thanks to the algorithm economy, every company can now access algorithmic intelligence.\n\nFor instance, Google has been applying machine learning to everything from self-driving cards to language translation, and now to the comment sections on sites to make the internet a little less awful. While Pinterest has built a new visual search, Lens, which is basically a point and shoot search engine thanks to deep learning, lots of data, and GPUs now available on many phones.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-tensorflow-1-0-the-ultimate-breakthrough-ford-invests-1b-into-ai-bdbd0e11e5fa",
        "title": "Emergent // Future: TensorFlow 1.0, the Ultimate Breakthrough, Ford Invests $1B Into AI",
        "text": "Issue 42\n\n This week we look at Google\u2019s release of TensorFlow 1.0, what the Microsoft CEO thinks the ultimate breakthrough is, why Ford is investing $1B into AI, our top reads of the week, and things to try at home.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nGoogle released the 1.0 version of its open-source deep learning framework last week.\n\nThe new release brings more speed, flexibility, and stability to the TensorFlow platform, which features more than 6,000 open-source repositories.\n\nThe big feature is the experimental release of XLA, a domain-specific compiler for linear algebra that optimizes TensorFlow computations. This is big news because the improvements in speed and memory usage open the door to a new class of apps that can run on smartphone-grade hardware.\n\nIn addition, the Python APIs have been changed to more closely resemble NumPy. Experimental APIs for Java and Go are also available.\n\nTensorFlow has helped researchers, engineers, artists, students, and others make progress with everything from language translation to early detection of skin cancer and preventing blindness in diabetics.\n\nIt\u2019s now as simple as pip install tensorflow\n\nMicrosoft CEO Satya Nadella is stressing the immense potential of artificial intelligence lately, calling it the \u201cultimate breakthrough\u201d in technology.\n\nFor all the advances in computer interfaces, nothing has come close to beating human-level speech recognition than deep neural networks.\n\nNadella says they\u2019re \u201cfundamentally giving us human perception, whether it is speech or image recognition, and that\u2019s just magical to see.\u201d\n\nThat could explain why Ford Motor plans to invest $1 billion over the next five years in Argo AI, an artificial intelligence start-up that is focused on developing autonomous vehicle technology.\n\nThe move is Ford\u2019s biggest effort to move into self-driving car research.\n\nArgo AI will develop the technology exclusively for Ford at first, and then plans to license its technology to others.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-staying-relevant-lets-enhance-and-a-portrait-by-neural-networks-c53ccbad5510",
        "title": "Emergent // Future: Staying Relevant, Let\u2019s Enhance!, and a Portrait by Neural Networks",
        "text": "Issue 41\n\n This week we look at what it means to stay relevant in the AI age, how you can make your own image \u201cenhance\u201d button like on CSI Miami, what we\u2019re reading this week and things to try at home!\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nElon Musk says humans must merge with machines or become irrelevant in AI age.\n\n\u201cOver time I think we will probably see a closer merger of biological intelligence and digital intelligence,\u201d Musk told an audience at the World Government Summit in Dubai.\n\n\u201cIt\u2019s mostly about the bandwidth, the speed of the connection between your brain and the digital version of yourself, particularly output.\u201d\n\nMusk\u2019s point is that computers can move bits at a trillion per second, while humans can do about 10 bits per second. Basically, humans are last year\u2019s model.\n\nMeanwhile, Apple CEO Tim Cook sees augmented reality as a fundamental, revolutionary technology similar to the smartphone. He views AR like the silicon used in the iPhone\u2019s chips \u2014 it\u2019s a core technology and not a product.\n\nGoogle\u2019s new neural networks can turn pixelated faces back into real ones.\n\nIn a new paper, Google researchers describe using neural networks to not only \u201cdeblur\u201d an image, but to generate new image details that appear plausible to the human eye.\n\nThe method uses a neural network, the \u201cconditioning\u201d network, which has scanned a large number of related but higher-resolution images and downsized them to a lower resolution 8\u00d78 pixel. The second algorithm, known as the \u201cprior\u201d network, then adds details to the 8\u00d78 images based on higher-resolution images of similar photos.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-real-time-parking-predictions-improving-image-search-with-deep-learning-eea9c74ee0f8",
        "title": "Emergent // Future: Real-Time Parking Predictions, Improving Image Search With Deep Learning\u2026",
        "text": "Issue 40\n\n This week we look at Google\u2019s real-time parking predictions, the largest dataset of annotated YouTube videos, and how Facebook is improving image search using deep learning.\n\nPlus! What we\u2019re reading this week and things to try at home!\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nYou Might Have Heard: Google launched a new feature last week for Google Maps for Android that offers predictions about parking options \u2014 like Waze for parking.\n\nThe feature uses a logistic regression machine learning model with real time crowdsourcing to provide parking difficulty information at your destination.\n\nThe model takes into account circling (driving around the block several times), difference between when a user should have arrived and when they actually did, the dispersion of parking locations at the destination, time-of-day and date.\n\nBut Did You Know\u2026 One of the most challenging research areas in machine learning is enabling computers to understand what a scene is about.\n\nNow, they\u2019re releasing YouTube-BoundingBoxes, a dataset of 5 million bounding boxes that span 23 object categories from 210,000 YouTube videos.\n\nThis is the largest manually annotated video dataset with bounding boxes that track objects in temporally contiguous frames.\n\nThe dataset is designed to be big enough to train large-scale models.\n\nFacebook has long been able to recognize people in photos. But it hasn\u2019t been as precise at understanding what\u2019s actually in the photos. That\u2019s starting to change.\n\nFB built a platform for image and video understanding called Lumos, which makes it possible to search photos based on what\u2019s in them, rather than just by when it was taken, the tag, or location. It\u2019s more like describing what\u2019s in the photos by keyword.\n\nTo accomplish this, Facebook trained a deep neural network on tens of millions of photos with millions of parameters.\n\nThe model matches search descriptors to features pulled from photos and ranks its output using information from both the images and the original search. Facebook to let users search for photos using keywords to describe them, powered by its Lumos AI, which has been used to help the visually impaired\n\nFor more, check out FB\u2019s post on building scalable systems to understand content.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-ai-goes-all-in-on-humans-apple-partners-on-ai-ethics-cancer-classifying-e76a96bd34b3",
        "title": "Emergent // Future: AI Goes All-In On Humans, Apple Partners on AI Ethics, Cancer-Classifying\u2026",
        "text": "Issue 39\n\n This week we look at how he world\u2019s best poker players are getting crushed by AI, why Apple joined the Partnership on AI, and how an algorithm is diagnosing skin cancer as accurate as dermatologists.\n\nPlus! What we\u2019re reading this week and things to try at home!\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nThe world\u2019s best poker players are getting crushed by AI in a Pittsburgh casino.\n\nFour poker players are playing against DeepStack, an artificial intelligence algorithm running on a supercomputer located 15 miles away.\n\nWith just one day left in the 20-day tournament, the algorithm is up more than $1.6 million over the humans.\n\nWhile a crushing defeat for humanity, it\u2019s a major milestone for AI.\n\nMachines have already become smart enough to beat humans at other games such as chess and Go, but poker is more difficult because it\u2019s a game with imperfect information. With chess and Go, each player can see the entire board, but with poker, players don\u2019t get to see each other\u2019s hands.\n\nApple joined Google, Amazon, Microsoft, Facebook, and IBM to keep an open dialogue about the ethics of AI.\n\nThe Partnership on AI was formed in September 2016 with the goal of establishing best practices on the challenges and opportunities within the field.\n\nThink of this group as a United Nations-like forum for companies developing AI \u2014 a place for self-interested parties to seek common ground on issues that could do great good or great harm to all of humanity.\n\nThis is the second signal we\u2019ve seen from about Apple becoming less secretive with their research. The big news to spill out of NIPS 2016 was Apple allowing its artificial intelligence teams to publish research papers for the first time.\n\nStanford researchers have trained a single convolutional neural network to diagnose skin cancer as accurately as dermatologists.\n\nThe algorithm was trained from images directly, using only pixels and disease labels as inputs.\n\nIt was fed a dataset of nearly 130,000 images of skin disease labeled with 2,032 different diseases.\n\nIn three tests, the algorithm matched the performance of dermatologists, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists.\n\nThis single CNN is capable of running on smartphones, potentially providing low-cost universal access to diagnostic skin care.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\n\ud83d\ude80 Forwarded from a friend? Sign-up to Emergent // Future here.\n\nFollow @EmergentFuture for more on frontier technology\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/teslas-autopilot-saves-lives-algorithms-irl-neural-nets-in-python-and-more-758351bb50bf",
        "title": "Tesla\u2019s Autopilot Saves Lives, Algorithms IRL, Neural Nets in Python, and more",
        "text": "Issue 38\n\nThis week we look at the 40% drop in car crashes using Tesla\u2019s Autopilot, why Microsoft is investing into Montreal AI research, check out some algorithms IRL, and building neural networks in Python.\n\n\ud83d\ude80 Not a subscriber? Sign-up to Emergent // Future here.\n\nLast week: we looked at training self-driving cars using Grand Theft Auto V and the CB Insights AI 100 landscape.\n\n\ud83d\udce2 Want to contribute to Emergent // Future? Reply and let me know!\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-train-your-own-self-driving-car-and-the-ai-100-list-3a3dc624191f",
        "title": "Emergent // Future: Train Your Own Self-Driving Car, and the AI 100 List",
        "text": "Issue 37\n\n This week we look at the OpenAI + DeepDrive project that helps developers train self-driving car AI\u2019s using Grand Theft Auto V. Plus, CB Insights releases their AI 100 landscape.\n\nLast week: The year in artificial intelligence, what 2017 holds, and the shape of things to come.\n\n\ud83d\udce2 Want to contribute to Emergent // Future? Reply and let me know!\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook\n\nIt\u2019s now possible to train self-driving cars using Grand Theft Auto V thanks to the DeepDrive Project.\n\nThe work is being integrated and open sourced with OpenAI Universe, cutting the setup time to just 20-minutes. By combining a highly realistic driving simulation, DeepDrive gives developers the opportunity to build better self-driving cars.\n\nThe post goes on to say that you need to purchase a copy of GTA V in order for your Universe agent to be able to drive around the streets of the virtual world. The release includes an agent trained on 21-hours and about 600,000 images of AI driving.\n\nOne of the biggest advantages of training off of a virtual environment is that the labeled objects, whether traffic signs or cyclists, can easily be analyzed.\n\nCB Insights released their 2017 ranking of the 100 most promising private artificial intelligence companies globally.\n\nAlgorithmia was selected as one of the prestigious companies breaking ground on artificial intelligence algorithms and technology.\n\nMeanwhile, LinkedIn co-founder Reid Hoffman and Omidyar Network have formed a $27 million fund to support research into the social impacts of artificial intelligence.\n\nThe aim is to promote artificial intelligence research in the public interest.\n\nEmergent // Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.\n\nLovingly curated for you by Algorithmia"
    },
    {
        "url": "https://medium.com/emergent-future/looking-back-and-looking-forward-to-the-shape-of-things-to-come-495ba6a097f4",
        "title": "Looking Back and Looking Forward to the Shape of Things to Come",
        "text": "Issue 36\n\nAnd now back to our regularly scheduled programming\u2026 In the last issue of 2016, we looked at self-driving cars, Zuckerberg\u2019s AI assistant, and what\u2019s new with IoT, drones, and chat bots.\n\nThis week, we recap the year in artificial intelligence before looking ahead to what 2017 holds for AI, data science, and the shape of things to come.\n\nPlus, we check out the latest White House report on Artificial Intelligence, Automation, and the Economy.\n\n\ud83d\udce2 Want to contribute to Emergent // Future? I\u2019m looking for people curious about the future to contribute to both this newsletter as well as our community on Medium.\n\nI\u2019m also in need of a few smart ideas for an Emergent // Future microsite in the vein of Techmeme, Hacker News, or Slashdot.\n\n\ud83d\udc4b Spread the love of E//F on Twitter and Facebook"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-autonomous-taxis-building-ai-assistants-drones-bots-and-iot-af8e80c10bb0",
        "title": "Emergent // Future: Autonomous Taxis, Building AI Assistants, Drones, Bots, and IoT",
        "text": "Issue 35\n\n In the last issue of Emergent // Future for 2016, we look at Uber\u2019s self-driving cars in San Francisco, Google\u2019s new autonomous car company, how Mark Zuckerberg built an AI assistant for himself, what Amazon, Google, and Microsoft are up to.\n\nWe\u2019ll see you in 2017. Thanks for joining us as we explore technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\nYou Might Have Heard: Uber debuted self-driving cars in San Francisco last week.\n\nEach car is staffed with a backup driver and a data collector. The autonomous taxis are a modified Volvo XC90s with Uber\u2019s self-driving technology integrated into the sports utility vehicle.\n\nThe cars are equipped with seven cameras, as well as a fast-spinning lidar system on the top to constantly scan and analyze the moving environment.\n\nBut Did You Know\u2026 Google spun off its self-driving car unit as a new company within Alphabet called Waymo.\n\nThe new company is planning a ride-sharing service with Fiat Chrysler and will deploy a semi-autonomous version of the Chrysler Pacifica minivan as early as end of 2017.\n\nALSO: NVIDIA will start testing self-driving cars on public roads after California gave them the go-ahead.\n\nMeanwhile, Uber\u2019s refusing to apply for a $150 permit for its self-driving car testing in California, arguing that the cars are technically not autonomous vehicles under state law due to a human in the drivers seat. Similar to how Tesla\u2019s Autopilot system doesn\u2019t need a permit since a human is monitoring the ride from the drivers seat.\n\nMark Zuckerberg made good on his 2016 personal challenge to build artificial intelligence tools to help him at home and work.\n\nIn a post, Zuckerberg details how he built an AI assistant to control aspects of his home, like the lights, temperature, appliances, music and security, what he learned over the 100\u2013150 hours spent working on it, and what\u2019s next for his AI assistant.\n\nHey Zuck, I\u2019ve got news for you: we built part of your AI solution in February during a weekend hackathon.\n\nMicrosoft announced a Cortana Devices SDK and Skills Kit for developers to create and publish chat bots. The Bot Framework also allows developers to repurpose code from their existing Alexa skills as Cortana skills.\n\nGoogle debuted a platform for building IoT devices with Google services and Android APIs called Android Things.\n\nAnd, Amazon tested Prime Air drone deliveries in UK with two shoppers last week. The first order took 13 minutes to deliver. Amazon will later add drone deliver for hundreds living near their Cambridge warehouse.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.\n\nLovingly curated for you by Algorithmia\n\n Follow @EmergentFuture for more frontier technology posts"
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2",
        "title": "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)",
        "text": "In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom environment! With the holidays right around the corner, this will be my final post for the year, and I hope it will serve as a culmination of all the previous topics in the series. If you haven\u2019t yet, or are new to Deep Learning and Reinforcement Learning, I suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here. If you have been following the series: thank you! I have learned so much about RL in the past year, and am happy to have shared it with everyone through this article series.\n\nSo what is A3C? The A3C algorithm was released by Google\u2019s DeepMind group earlier this year, and it made a splash by\u2026 essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces. In fact, OpenAI just released a version of A3C as their \u201cuniversal starter agent\u201d for working with their new (and very diverse) set of Universe environments.\n\nAsynchronous Advantage Actor-Critic is quite a mouthful. Let\u2019s start by unpacking the name, and from there, begin to unpack the mechanics of the algorithm itself.\n\nAsynchronous: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it\u2019s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.\n\nActor-Critic: So far this series has focused on value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy \u03c0(s) (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.\n\nAdvantage: If we think back to our implementation of Policy Gradient, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were \u201cgood\u201d and which were \u201cbad.\u201d The network was then updated in order to encourage and discourage actions appropriately.\n\nThe insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network\u2019s predictions were lacking. If you recall from the Dueling Q-Network architecture, the advantage function is as follow:\n\nSince we won\u2019t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage.\n\nIn this tutorial, we will go even further, and utilize a slightly different version of advantage estimation with lower variance referred to as Generalized Advantage Estimation.\n\nIn the process of building this implementation of the A3C algorithm, I used as reference the quality implementations by DennyBritz and OpenAI. Both of which I highly recommend if you\u2019d like to see alternatives to my code here. Each section embedded here is taken out of context for instructional purposes, and won\u2019t run on its own. To view and run the full, functional A3C implementation, see my Github repository.\n\nThe general outline of the code architecture is:\n\nThe A3C algorithm begins by constructing the global network. This network will consist of convolutional layers to process spatial dependencies, followed by an LSTM layer to process temporal dependencies, and finally, value and policy output layers. Below is example code for establishing the network graph itself.\n\nNext, a set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more workers than there are threads on your CPU.\n\n~ From here we go asynchronous ~\n\nEach worker begins by setting its network parameters to those of the global network. We can do this by constructing a Tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network.\n\nEach worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples (observation, action, reward, done, value) that is constantly added to from interactions with the environment.\n\nOnce the worker\u2019s experience history is large enough, we use it to determine discounted return and advantage, and use those to calculate value and policy losses. We also calculate an entropy (H) of the policy. This corresponds to the spread of action probabilities. If the policy outputs actions with relatively similar probabilities, then entropy will be high, but if the policy suggests a single action with a large probability then entropy will be low. We use the entropy as a means of improving exploration, by encouraging the model to be conservative regarding its sureness of the correct action.\n\nA worker then uses these losses to obtain gradients with respect to its network parameters. Each of these gradients are typically clipped in order to prevent overly-large parameter updates which can destabilize the policy.\n\nA worker then uses the gradients to update the global network parameters. In this way, the global network is constantly being updated by each of the agents, as they interact with their environment.\n\nOnce a successful update is made to the global network, the whole process repeats! The worker then resets its own network parameters to those of the global network, and the process begins again.\n\nTo view the full and functional code, see the Github repository here.\n\nThe robustness of A3C allows us to tackle a new generation of reinforcement learning challenges, one of which is 3D environments! We have come a long way from multi-armed bandits and grid-worlds, and in this tutorial, I have set up the code to allow for playing through the first VizDoom challenge. VizDoom is a system to allow for RL research using the classic Doom game engine. The maintainers of VizDoom recently created a pip package, so installing it is as simple as:\n\nOnce it is installed, we will be using the environment, which is provided in the Github repository, and needs to be placed in the working directory.\n\nThe challenge consists of controlling an avatar from a first person perspective in a single square room. There is a single enemy on the opposite side of the room, which appears in a random location each episode. The agent can only move to the left or right, and fire a gun. The goal is to shoot the enemy as quickly as possible using as few bullets as possible. The agent has 300 time steps per episode to shoot the enemy. Shooting the enemy yields a reward of 1, and each time step as well as each shot yields a small penalty. After about 500 episodes per worker agent, the network learns a policy to quickly solve the challenge. Feel free to adjust parameters such as learning rate, clipping magnitude, update frequency, etc. to attempt to achieve ever greater performance or utilize A3C in your own RL tasks.\n\nI hope this tutorial has been helpful to those new to A3C and asynchronous reinforcement learning! Now go forth and build AIs.\n\nIf you\u2019d like to follow my writing on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjuliani.\n\nIf this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!"
    },
    {
        "url": "https://medium.com/emergent-future/amd-takes-on-nvidia-recapping-nips-and-more-9afe887d7f2b",
        "title": "AMD Takes On NVIDIA, Recapping NIPS, and More \u2013 Emergent // Future \u2013",
        "text": "AMD\u2019s finally getting into the high performance computing market with their new Radeon Instinct.\n\nThe new chip is a combined hardware and software stack aimed at taking on NVIDIA in the server market for deep learning, machine learning, and neural networking tasks.\n\nThe Radeon Instinct will be available in the first half of 2017.\n\nAMD already owns the graphics chips in PlayStation 4, Wii U, and the Xbox One, but rumors are swirling that Intel may also use AMD GPUs to challenge NVIDIA\u2019s rising power."
    },
    {
        "url": "https://medium.com/emergent-future/looking-forward-from-2016-5-technology-concepts-to-watch-351152c7e4df",
        "title": "Looking Forward From 2016: 5 Technology Concepts to Watch",
        "text": "As I read through the \u201cbest of 2016\u201d lists and predictions of what\u2019s to come in 2017, I\u2019d like to take the opportunity to reflect on how emerging technology has been developing, based on my own experiences and observations, while daring to speculate on what trends may be important to keep in mind as we progress with the evolution of and with technology, or The Convergence. Some of these concepts will gain more traction in the coming year than others, but all are vital for human/technological progression and enable greater potential for enhanced learning experiences.\n\nThe common thread between these five concepts is the blending of the virtual with the physical, the digital with the material, the simulated with the real, the abstract with the tangible. We uses many phrases to describe this duality, as if they are in contrast, but the way we use these phrases assumes a false dichotomy. We are moving towards a reality where man and machine work best together.\n\nThis first concept comes from the world of Advanced Chess. The term \u201ccentaur\u201d is used to describe a human working with the assistance of machines. It was coined by world champion Garry Kasparov after IBM\u2019s Deep Blue defeated him in 1997. Understanding that AI itself was a human creation, he helped create a new form of chess in which people can confer with gaming machines. The competition allows for teams to use human creativity and empathy as well as machine calculations. In matches that centaurs against machine-only competitors, centaurs consistently win.\n\nArtificial intelligence enables us to process more information faster than our brains can. Robotics enables us to move heavier and/or more fragile objects than our bodies can. We will come to recognize our strongest human traits as well as some of our own shortcomings and leverage the capabilities of our inventions where appropriate.\n\nRather than seeing our creations as Frankenstein\u2019s monsters and wasting our efforts in futile attempts at competing directly with them, we will work with them. In order to do this, we need to embrace technologies that enable better communication between ourselves and our machine counterparts. The purpose of communication is to create a connection or a link. Biosyncing is that link.\n\nBiosyncing refers to biomechanical symbiosis; when a human and a machine are in a reactive, performance-augmenting loop. It describes behavioral changes on the part of humans, based on mechanical feedback, and machines learning from human responses, making alterations automatically.\n\nThe technologies included in this trend include many that are often referred to in suites of technologies such as \u201cwearables\u201d or the \u201cInternet of things.\u201d Haptic feedback, heart rate monitoring, location-aware suggestions, and facial recognition are examples of technological capabilities that have become ubiquitous in everyday products. Biosyncing will facilitate the connection required to fully immerse ourselves in convergent experiences.\n\nFor the most part, augmented reality and virtual reality have thus far been developed as standalone technologies, separate and different from one another. Similarly, the experiences created for these technologies have been individual, designed to be experienced by one person at a time. Augmented reality and virtual reality are now coming together and so are the possibilities of greater social interaction. With the ability to trick our brains into believing simulated experiences are \u201creal\u201d comes the ability to create human connections through shared simulated experiences.\n\nAt another time in recent history, many thought virtual reality would go mainstream, but they mistook a fad for a trend. What was missing was the ability to have a shared human experience. Looking back a little further in human history, we created the ability to simulate two dimensional animation from photographic images. Some, like Thomas Edison, believed that people would watch these productions by themselves, such as through Edison\u2019s Kinetoscope. They were mistaken, and people flocked to movie houses to see public projections, laughing or crying with the crowds. As soon as access to the Internet became available to the public, we used it as a social tool through message boards, email, and by sharing our creations with a wider audience online. Every immersive experience, in virtual reality or augmented reality, can now be linked to and from others via the Internet. This is invaluable for the use of virtual reality for education, as learning is a naturally social act.\n\nSocial immersive experiences are necessary for more widespread adoption of convergent technologies. As we come to appreciate the opportunities these experiences allow, we will develop features to satisfy our basic human need for interpersonal connections. All of the tools to create these collaborative experiences are now at our disposal.\n\nWith our increased awareness of living simultaneously in the physical and virtual worlds, we will seek out and contribute virtual media that relates to our surroundings. Triggers for additional information will increasingly remain affixed to physical objects and locations. Think of this as a virtual layer on top of everything we can already perceive with our five senses. The amount of information that we can gather through this layer is theoretically infinite, and it can contain static or editable media.\n\nMillions of people interact with one layer of situated media daily as they play Pok\u00e9mon Go. Their phones act as a sort of portal, using screen-based augmented reality to enable them to view the virtual creatures that exist at the same latitude and longitude coordinates for every player in the world. Similarly, we have much more data about \u201creal\u201d objects and locations, but it is currently disconnected from those objects and locations and can only be accessed through a traditional Internet search. We now have the means to connect its virtual information with the matching physical counterparts.\n\nAt all times, we are immersed in a world full of information that, to date, has largely been invisible to us. It is as if we are swimming in it like fish unaware of water. We will become more aware of the potential benefits of accessing and updating additional information embedded in every place and thing that surrounds us.\n\nAccording to the United Nations, \u201cHuman rights are rights inherent to all human beings, whatever our nationality, place of residence, sex, national or ethnic origin, colour, religion, language, or any other status. We are all equally entitled to our human rights without discrimination. These rights are all interrelated, interdependent and indivisible.\u201d\n\nOn June 27th of this year, the UN passed a non-binding resolution declaring Internet access a basic human right: \u201cPromotion and protection of all human rights, civil, political, economic, social and cultural rights, including the right to development.\u201d (Emphasis added) Mark Zuckerberg, Bill & Melinda Gates, Richard Branson, and many others have pledged their support for the Global Goals, calling for Internet access for all by 2020.\n\nThe Convergence brings the Internet to us and us to the Internet. The understanding that access to opportunities that enable participation in the convergence of virtual and physical worlds is a basic human right will become mainstream; governments, companies, and individuals will increasingly support initiatives that improve access to the latest technologies for all of humanity."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-open-sourced-ai-aws-ai-tools-fpgas-and-more-4d3fe4ec0c0c",
        "title": "Emergent // Future: Open Sourced AI, AWS AI Tools, FPGA\u2019s, and more",
        "text": "Issue 33\n\n This week we look into the efforts to open source artificial intelligence by Google DeepMind and OpenAI, how Amazon AWS is re:Inventing its platform for AI with new services and FPGA\u2019s for EC2.\n\nPlus our favorite articles from the past week about AI and some of the projects making us smarter.\n\nYou might have heard: Google DeepMind is open sourcing its AI training platform and making the code available on GitHub for researchers and developers to experiment with.\n\nBy open sourcing the 3D game-like platform, DeepMind hopes the research community can make use of it and help shape the development of the platform going forward.\n\nBut did you know\u2026 The move comes after OpenAI open sourced Universe, their platform that features a thousand different environments for training AI to play games and use apps.\n\nUniverse lets AI agent \u201cuse a computer like a human does: by looking at screen pixels and operating a virtual keyboard and mouse.\u201d\n\nPlus: In keeping with our mission to democratize access to AI, we\u2019ve open sourced the AWS AMI for training your own style transfer models.\n\nAmazon AI launched with three tools for developers at re:Invent last week: image recognition, text-to-speech, and a chatbot builder.\n\nThe platform is Amazon\u2019s answer to the Google Cloud ML platform, which provides machine learning services with pre-trained models.\n\nQuick overview of the three tools:\n\nAmazon also debuted their FPGA instances for EC2, called F1. These reprogrammable chips make it easy to switch context within an application without having to switch the hardware.\n\nALSO: Amazon and Techstars are partnering on a 13-week conversational AI startup accelerator. They\u2019re offering companies $20K in initial investment and $100K convertible note option upon completion of the program.\n\nWatch: DeepMind Founder on The Future Of Artificial Intelligence\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.\n\nLovingly curated for you by Algorithmia\n\n Follow @EmergentFuture for more frontier technology posts"
    },
    {
        "url": "https://medium.com/emergent-future/the-convergence-of-physical-virtual-worlds-mixed-reality-ai-biosyncing-3670e595bbde",
        "title": "The Convergence of Physical & Virtual Worlds: Mixed Reality, AI, & Biosyncing",
        "text": "Companies that work with groundbreaking new technologies often struggle to find the most valuable uses for them. This is understandable: the foundations are still being laid, and we\u2019re still in the process of understanding what can be built on top of them. We tend to replicate uses for older technologies until we learn enough to break out of that mindset. For example, whenever we have had a new medium gain popularity, we have staged productions that leveraged the strengths of preexisting ones. Early cinema was little more than filmed theater, but that was only after we moved beyond the novelty of the technology itself.\n\nA \u201cmoving picture\u201d was enough to keep audiences entertained for a little while \u2014 the movement alone astounded them. The modern iteration of this technological fascination is the 3 dimensional heart. Nearly every company that produces augmented reality content showcases a 3D heart, seemingly to the amazement of those to whom the heart is demoed. While the exercise of creating a 3D heart may be a fantastic learning tool for the designers or developers themselves, we have reached peak heart for users of AR.\n\nIn being mindfully skeptical, we know that technology for technology\u2019s sake does not add value, but the potential uses for these features may have significant long term value. These new features will be crucial to new, larger creations once we understand what these larger creations could be. Returning to cinema: a still shot of a man sneezing (or a train pulling into a station, to use the famous example by the Lumi\u00e8re Brothers) added no value to a filmed theatrical performance. In our age of mixed reality, we have had our inventors. Our Edisons and Lumi\u00e8res. We haven\u2019t yet had creators who define a new form, as D.W. Griffith did, or artists of that form who unlock even more potential, like Welles or Renoir. In cinema, that took decades, but in the age of exponential technology, thankfully, we will not have to wait so long for advancements in this new medium.\n\nThe future of mixed reality is bright; it will help us create in ways we have not yet even considered. This is not to say that mixed reality does not yet have any useful applications \u2014 it has many already. There is, however, a risk when companies over-promise on what is feasible today. Some companies with a vested interest tend to set unrealistic expectations, especially when capitalizing on related newsworthy success. (See Pokemon Go.) However, technologies inevitably go through the hype cycle, and the higher the peak of inflated expectations, the deeper the trough of disillusionment. As a mindful skeptic, managing this is relatively simple:"
    },
    {
        "url": "https://medium.com/emergent-future/improved-realities-part-4-when-can-we-actualize-the-convergence-fd791d9858c0",
        "title": "Improved Realities Part 4: When Can We Actualize The Convergence?",
        "text": "Improved Realities Part 4: When Can We Actualize The Convergence? The Convergence refers to utilizing three suites of technologies \u2014 mixed reality, artificial intelligence, and biosyncing \u2014 to create hybrid realities in which virtual and physical components interact seamlessly. Through this series, I aim to explain this trend in greater detail as well as what it means for education, specifically. In my first post, I covered an overview of what The Convergence consists of and how the these elements may be used once combined. In my second post, I explained why we should embrace the individual elements that compose The Convergence. In my third post, I explored how we can do this, considering mankind\u2019s resistance to change and the current technological limitations. The next question is \u201cWhen?\u201d We live in the Age of Exponential Technologies.\n\nIn linear growth, change occurs in a predictable manner. Exponential technologies underperform relative to our expectations\u2026 for a little while. (A) However, exponential technology growth is composed of sigmoidal curves. This means we have moments of small plateaus before another massive, sudden spike in growth. (B) We have been on one of those plateaus for about ten years now, since 2007. (C) While I recently referred to 2007 as a year of significance at EdTech Sweden, Thomas Friedman explains this in greater detail. In addition to the introduction of the iPhone and Android, Twitter and Facebook gained widespread popularity, IBM introduced Watson, and \u201cthe cloud\u201d took off. Since then, we have made tremendous improvements to important technologies, but much of this has been out of sight to consumers or has seemed incremental. Monumental changes occur when these technologies begin to be used in tandem. We are just now figuring out how to do this productively. That is not to say that exponential technologies have not already upset global economic markets and societal norms. As adaptive as we humans are as a species, these changes are occurring faster than we can handle. Out of fear, we resist them and refuse to acknowledge inevitable progress. Ultimately, we will accept these changes. Soon after, we will expect the benefits that come from technological adoption, whatever we perceive these benefits to be. In order to thrive through these turbulent times, while we strive to understand how to use these new technologies and adjust accordingly, we need to embrace our humanity. The skills required to succeed in the near future are those that make us most human. Even the most powerful artificial intelligence is incapable of empathy, curiosity, and creativity. These \u201c21st century\u201d or \u201csoft\u201d skills enable us to work in enriching ways while machinelike work is relegated to machines. We cannot simply learn one skill or trade and rely on that for life. Lifelong learning is essential. It can also be enjoyable. We must remain flexible and open to new ideas and challenges as they present themselves while also actively seeking out new opportunities.\n\nAccess to enabling technologies is also essential for all. Recently, I was in Cartagena, Colombia, for ANDICOM, a telecommunications conference. The morning of the first day, I went down to the beach in front of my hotel for a run. Just beside these luxury hotels is the fishing village La Boquilla. Most of the fisherman were out for the day, and their small, wooden homes stood in stark contrast to the towers shimmering a few hundred yards away. On the way to the conference later that morning, I shared a cab with an organizer from CINTEL. He asked me if I had noticed the village beside the hotel. He then told me about a program CINTEL and Qualcomm have been running: Fishing with Mobile Nets. The program gives Android devices to these fisherman so that they can check market prices, plan based on weather predictions, and organize more easily with fellow villagers. Those of us who are leaders in developing and adopting these technologies need to ensure that the access gap does not become too great. It is a responsibility we must accept as part of the social contract. We must learn how to adapt, and to do that, we must adapt how we learn. The educational opportunities unlocked through The Convergence of mixed reality, artificial intelligence, and biosyncing are transformative. We can mindfully embrace these technologies for enhanced, immersive experiences."
    },
    {
        "url": "https://medium.com/emergent-future/why-deep-learning-matters-and-whats-next-for-artificial-intelligence-5c629993dc4",
        "title": "Why Deep Learning Matters and What\u2019s Next for Artificial Intelligence",
        "text": "Why Deep Learning Matters and What\u2019s Next for Artificial Intelligence\n\nIt\u2019s almost impossible to escape the impact frontier technologies are having on everyday life.\n\nAt the core of this impact are the advancements of artificial intelligence, machine learning, and deep learning.\n\nThese change agents are ushering in a revolution that will fundamentally alter the way we live, work, and communicate akin to the industrial revolution \u2014 more specifically, AI is the new industrial revolution.\n\nThe most exciting and promising of these frontier technologies is the advancements happening in the deep learning space.\n\nWhile still nascent, it\u2019s deep learning percolating into your smartphone, driving advancements in healthcare, creating efficiencies in the power grid, improving agricultural yields, and helping us find solutions to climate change.\n\nJust this year a handful of high-profile experiments came into the spotlight, including Microsoft Tay, Google\u2019s DeepMind AlphaGo, and Facebook M and highlight the versatility of deep learning and the application of AI.\n\nFor instance, Google DeepMind has been used to master the game of Go, cut their data center energy bills by reducing power consumption by 15%, and even working with NHS to fight blindness.\n\n\u201cDeep Learning is an amazing tool that is helping numerous groups create exciting AI applications,\u201d Andrew Ng says, Chief Scientist at Baidu and chairman/co-founder of Coursera. \u201cIt is helping us build self-driving cars, accurate speech recognition, computers that can understand images, and much more.\u201d\n\nThese experiments all rely on a technique known as deep learning, which attempts to mimic the layers of neurons in the brain\u2019s neocortex. This idea \u2014 to create an artificial neural network by simulating how the brain works \u2014 has been around since the 1950s in one form or another.\n\nDeep learning is a subset of a subset of artificial intelligence, which encompasses most logic and rule-based systems designed to solve problems. Within AI, you have machine learning, which uses a suite of algorithms to go through data to make and improve the decision making process. And, within machine learning you come to deep learning, which can make sense of data using multiple layers of abstraction.\n\nDuring the training process, a deep neural network learns to discover useful patterns in the digital representation of data, like sounds and images. In particular, this is why we\u2019re seeing more advancements for image recognition, machine translation, and natural language processing come from deep learning.\n\nOne example of deep learning in the wild is how Facebook can automatically organize photos, identify faces, and suggest which friends to tag. Or, how Google can programmatically translate 103 languages with extreme accuracy.\n\nIt\u2019s been more than a half-century since the science behind deep learning was discovered, but why is it just now starting to transform the world?\n\nThe answer lies in two major shifts: an abundance of digital data and access to powerful GPUs.\n\nTogether, we are now capable of teaching computers to read, see, and hear simply by throwing enough data and compute at the problem.\n\nThere\u2019s a special kind of irony reserved for all of these new breakthroughs that are really just the same breakthrough: deep neural networks.\n\nThe basic concept of deep learning reach back to the 1950s, but were largely ignored till the 1980s and 90s. What\u2019s changed, however, is the context of abundant computation and data.\n\nWe now have access to, essentially, unlimited computational power thanks to Moore\u2019s law and the cloud. On the other side, we\u2019re creating more image, video, audio, and text data everyday than before due to the proliferation of smartphones and cheap sensors.\n\n\u201cThis is deep learning\u2019s Cambrian explosion,\u201d Frank Chen says, partner at the Andreessen Horowitz.\n\nFour years ago, Google had just two deep learning projects. Today, the search giant is infusing deep learning into everything it touches: Search, Gmail, Maps, translation, YouTube, their self-driving cars, and more.\n\n\u201cWe will move from mobile first to an AI first world,\u201d Google\u2019s CEO, Sundar Pichai said earlier this year.\n\nIn a very real sense, we\u2019re teaching machines to teach themselves.\n\n\u201cAI is the new electricity,\u201d Ng says. \u201cJust as 100 years ago electricity transformed industry after industry, AI will now do the same.\u201d\n\nDespite the breakthroughs, deep learning algorithms still they can\u2019t reason the way humans do. That could change soon, though.\n\nYann LeCun, Director of AI Research at Facebook and Professor at NYU, says deep learning combined with reasoning and planning is one area of research making promising advances right now, he says. Solving this in the next five years isn\u2019t out the realm of possibilities.\n\n\u201cTo enable deep learning systems to reason, we need to modify them so that they don\u2019t produce a single output, say the interpretation of an image, the translation of a sentence, etc., but can produce a whole set of alternative outputs. e.g the various ways a sentence can be translated,\u201c LeCun says.\n\nYet, despite plentiful data, and abundant computing power, deep learning is still very hard.\n\nOne bottleneck is the lack of developers trained to use these deep learning techniques. Machine learning is already a highly specialized domain, and those with the knowledge to train deep learning models and deploy them into production are even more select.\n\nFor instance, Google can\u2019t recruit enough developers with vast deep learning experience. Their solution is to simply teach their developers to use these techniques instead.\n\nOr, when Facebook\u2019s engineers struggled to take advantage of machine learning, they created an internal tool for visualizing machine and deep learning workflows, called FBLearner Flow.\n\nBut, where does that leave the other 99% of developers that don\u2019t work at one of these top tech company?\n\nVery few people in the world know how to use these tools.\n\n\u201cMachine learning is a complicated field,\u201d S. Somasegar says, venture partner at Madrona Venture Group and the former head of Microsoft\u2019s Developer Division. \u201cIf you look up the Wikipedia page on deep learning, you\u2019ll see 18 subcategories underneath Deep Neural Network Architectures with names such as Convolutional Neural Networks, Spike-and-Slab RBMs, and LTSM-related differentiable memory structures.\u201d\n\n\u201cThese are not topics that a typical software developer will immediately understand.\u201d\n\nYet, the number of companies that want to process unstructured data, like images or text, is rapidly increasing. The trend will continue, primarily because deep learning techniques are delivering impressive results.\n\nThat\u2019s why it\u2019s important for the people capable of training neural nets are also able to share their work with as many people as possible. In essence, democratizing access to machine intelligence algorithms, tools, and techniques.\n\nGPUs on-demand and running in the cloud, eliminate the manual work required for teams and organizations to experiment with cutting-edge, deep learning algorithms and models, which allows them to get started for a fraction of the cost.\n\n\u201cDeep learning has proven to be remarkably powerful, but it is far from plug-n-play,\u201d Oren Etzioni says, CEO of the Allen Institute for Artificial Intelligence. \u201cThat\u2019s where Algorithmia\u2019s technology comes in \u2014 to accelerate and streamline the use of deep learning.\u201d\n\nWhile GPUs were originally used to accelerate graphics and video games, more recently they\u2019ve found new life powering AI and deep learning tasks, like natural language understanding, and image recognition.\n\n\u201cWe\u2019ve had to build a lot of the technology and configure all of the components required to get GPUs to work with these deep learning frameworks in the cloud,\u201d Kenny Daniel says, Algorithmia founder and CTO. \u201cThe GPU was never designed to be shared in a cloud service like this.\u201d\n\nHosting deep learning models in the cloud can be especially challenging due to complex hardware and software dependencies. While using GPUs in the cloud are still nascent, they\u2019re essential for making deep learning tasks performant.\n\n\u201cFor anybody trying to go down the road of deploying their deep learning model into a production environment, they\u2019re going to run into problems pretty quickly,\u201d Daniel says. \u201cUsing GPUs inside of containers is a challenge. There are driver issues, system dependencies, and configuration challenges. It\u2019s a new space that\u2019s not well-explored, yet. There\u2019s not a lot of people out there trying to run multiple GPU jobs inside a Docker container.\u201d\n\n\u201cWe\u2019re dealing with the coordination needed between the cloud providers, the hardware, and the dependencies to intelligently schedule work and share GPUs, so that users don\u2019t have to.\u201d\n\nMost commercial deep learning products use \u201csupervised learning\u201d to achieve their objective.\n\nFor instance, in order to recognize a cat in a photo, a neural net will need to be trained with a set of labeled data. This tells the algorithm that there is a \u201ccat\u201d represented in this image, or there is not a \u201ccat\u201d in this photo. If you throw enough images at the neural network, it will, indeed, learn to identify a \u201ccat\u201d in an image.\n\nProducing large, labelled datasets is an achilles heel for most deep learning projects, however.\n\n\u201cUnsupervised learning,\u201d on the other hand, is how deep learning works and enables us to discover new patterns and insights by approaching problems with little or no idea what our results should look like.\n\nIn 2012, Google and Stanford let a neural net loose on 10 million YouTube stills. Without any human interaction, the neural net learned to identify cat faces from the YouTube stills, effectively identifying patterns in the data and teaching itself what parts of the images might be relevant.\n\nThe important distinction between supervised and unsupervised learning is that there is no feedback loop with unsupervised learning. Meaning, there\u2019s no human there correcting mistakes or scoring the results.\n\nThere\u2019s a bit of a gotcha here: we don\u2019t really know how deep learning works. Nobody can actually program a computer to do these things specifically. We feed massive amounts of data into deep neural nets, sit back, and let the algorithms learn to recognize various patterns contained within.\n\n\u201cYou essentially have software writing software,\u201d says Jen-Hsun Huang, CEO of GPU leader NVIDIA says.\n\nWhen we master unsupervised learning, we\u2019ll have machines that will unlock aspects about our world previously out of our reach.\n\n\u201cIn computer vision, we get tantalizing glimpses of what the deep networks are actually doing,\u201d Peter Norvig, research director at Google says. \u201cWe can identify line recognizers at one level, then, say, eye and nose recognizers at a higher level, followed by face recognizers above that and finally whole person recognizers.\u201d\n\nIn other areas of research, Norvig says, it has been hard to understand what the neural networks are doing.\n\n\u201cIn speech recognition, computer vision object recognition, the game of Go, and other fields, the difference has been dramatic,\u201d Norvig says. \u201cError rates go down when you use deep learning, and both these fields have undergone a complete transformation in the last few years. Essentially all the teams have chosen deep learning, because it just works.\u201d\n\nIn 1950, Alan Turing wrote \u201cWe can only see a short distance ahead, but we can see plenty there that needs to be done.\u201d Turing\u2019s words hold true.\n\n\u201cIn the next decade, AI will transform society,\u201d Ng says. \u201cIt will change what we do vs what we get computers to do for us.\u201d\n\n\u201cDeep learning has already helped AI make tremendous progress,\u201d Ng says, \u201cbut the best is still to come!\u201d"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-ai-chips-gpus-and-machine-learning-in-the-cloud-edc5edf0d5a0",
        "title": "Emergent // Future: AI Chips, GPUs, and Machine Learning in the Cloud",
        "text": "Issue 32\n\n This week is all about AI: Intel\u2019s new AI processor, OpenAI and Microsoft team up, Google brings GPUs to the cloud and invests in a Montreal AI research group.\n\nPlus our favorite articles from the past week about AI and some of the projects making us smarter.\n\nIntel unveiled a new AI processor designed to help with both training and execution of deep neural nets. They\u2019re calling it Nervana, based on the technology originally built by the startup of the same name Intel acquired earlier this year.\n\nIntel plans to have prototypes ready by the middle of next year, with a market-ready chip by the end of 2017.\n\nNVIDIA is the primary supplier of GPUs, which Microsoft, Google, Facebook, and Baidu typically us to train their deep neural nets. Intel is pushing to be part of this potentially enormous market despite that the hardware used to train and execute deep neural networks is changing.\n\nFor instance, Google\u2019s developed their TPU. Even Microsoft is evolving, moving to FPGAs, a type of programmable chip.\n\nPlus: Intel details its vision for AI chips, but the strategy is contingent on their deal with Movidius closes\n\nMicrosoft is partnering with Elon Musk\u2019s OpenAI to help protect humanity\u2019s best interests \u2014 and to make Azure the primary cloud platform for their compute needs.\n\nOpenAI will get access to the Azure N-series virtual machines and GPUs.\n\nMicrosoft says its goal is democratize AI and make it accessible to everyone.\n\nHmm, that sounds vaguely familiar to some company building a marketplace to democratize access to algorithmic intelligence. \ud83e\udd14\n\nOpenAI won\u2019t use Azure exclusively, but it is moving a majority of its compute to their cloud due, in part, to the bet Microsoft has placed on the future of AI.\n\nGoogle announced a new Cloud Machine Learnings group headed by Fei-Fei Li (Stanford Artificial Intelligence Lab) and Jia Li (former head of research at Snapchat).\n\nThe news welcomes GPUs to the Google cloud, which will offer the AMD FirePro S9300 x2, as well as the NVIDIA Tesla P100 and K80 GPUs for deep learning, AI, and high-performance computing applications. This is AMD\u2019s first move into deep learning.\n\nGoogle\u2019s also invested invested $3.4M in the Montreal Institute for Learning Algorithms, an academic fund covering three years for seven faculty members across various Montreal academic institutions.\n\nTo top that off, they\u2019re opening a brand new deep learning and AI research group in their Montreal offices.\n\nPLUS: Lessons learned from deploying deep learning at scale\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.\n\nLovingly curated for you by Algorithmia\n\n Follow @EmergentFuture for more frontier technology posts"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-augmented-reality-face-stealing-snapchat-specs-ai-and-more-ed8e952c8cc4",
        "title": "Emergent // Future Weekly: Augmented Reality, Face Stealing, Snapchat Specs, AI, and more",
        "text": "Issue 31\n\n This week we take a deep dive into augmented reality and try on Snapchat Spectacles, we learn how easy it is to steal your face with some cardboard glasses, and why Apple\u2019s working on their own Google Glass project.\n\nPlus, we check out Facebook\u2019s new photo filters, and what AI can and cannot do right now.\n\nDon\u2019t forget our favorite articles from the past week and some of the projects making us smarter.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nSnapchat will sell its new video-capturing glasses via vending machines called Snapbots. The ephemeral company says the vending machines are only available for about a day at each location before they\u2019re removed.\n\nNew research from Carnegie Mellon University shows that facial recognition software is far from secure. The team used a special pair of eyeglass frames printed with a pattern that is perceived by the computer as facial details of another person. This pattern caused commercial-grade facial recognition software into identifying the wrong person with 100% success (failure?).\n\nWhile still in an exploration phase, the device would connect wirelessly to iPhones, show images and other information in the wearer\u2019s field of vision, and may use augmented reality. The company has ordered small quantities of near-eye displays from one supplier for testing, but hasn\u2019t ordered enough components to indicate imminent mass-production.\n\nThe company shipped an AI-based style transfer framework called Caffe2Go, which runs a neural network in real time on your mobile devices. Facebook says it can execute neural nets in less than 1/20th of a second.\n\nSo now you can transform your Facebook photos and videos into the style of a Picasso, Van Gogh, or a Warhol, just like Prisma.\n\nFacebook is currently focused on three main areas of research \u2014 connectivity, virtual reality and artificial intelligence.\n\nMeanwhile, Prisma now lets iPhone 6S and 7 owners broadcast to Facebook Live using one of eight Prisma artistic filters.\n\nAndrew Ng, Chief Scientist at Baidu, says AI will transform many industries. But it\u2019s not magic. Much has been written about AI\u2019s potential to reflect both the best and the worst of humanity. As leaders, it is incumbent on all of us to make sure we are building a world in which every individual has an opportunity to thrive. Understanding what AI can do and how it fits into your strategy is the beginning, not the end, of that process.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.\n\nLovingly curated for you by Algorithmia\n\n Follow @EmergentFuture for more frontier technology posts"
    },
    {
        "url": "https://medium.com/emergent-future/improved-realities-how-do-we-embrace-the-convergence-aa164a547a68",
        "title": "Improved Realities Part 3: How Do We Embrace The Convergence?",
        "text": "The Convergence refers to utilizing three suites of technologies \u2014 mixed reality, artificial intelligence, and biosyncing \u2014 to create hybrid realities in which virtual and physical components interact seamlessly. Through this series, I aim to explain this trend in greater detail as well as what it means for education, specifically. In my first post, I covered an overview of what The Convergence consists of and how the these elements may be used once combined. In my last post, I explained why we should embrace the individual elements that compose The Convergence. The next question is \u201cHow?\u201d\n\nAs with all new technologies and concepts, we encounter challenges and opportunities when it comes time to adopt them. In some instances, of course, the opportunities present themselves in how we overcome the challenges. As I see it, the two biggest obstacles to widespread acceptance of The Convergence are:\n\nHumans fear what they don\u2019t understand. We are descendants, mostly, of the people who decided not to stray from the cave for fear of the unknown. (Exploring, especially in early human civilization, was often deadly.) Our fear of technology seems especially acute \u2014 even Socrates warned that the written word would bring about the end of civilization. In modern times, Steven Hawking and Elon Musk have warned about the dangers of artificial intelligence. As Eric Schmidt pointed out: though they are both brilliant men, they are not computer scientists.\n\nTechnological progress is inevitable, and the best way to manage it is to understand it. Musk, for his part, has embraced OpenAI to research and share the technology. Advocates for the use of emerging technologies for societal good must remember that humans don\u2019t scale as quickly as machines. However, humans are an adaptive species. We adjust our cultural practices when necessary in order to adjust to new environments and to take advantage of resources as they become available to us. As a result, we don\u2019t merely survive, we thrive.\n\nOn an individual and professional level, the potential benefits of risk outweigh that of caution. Failures are usually forgiven, while successes are remembered and rewarded. Not all uses of these technologies are useful or beneficial, but we should be just as wary of our natural inclination for fear. By remaining informed and aware of both technology and our biological response, we can be mindfully skeptical \u2014 questioning, embracing, or rejecting when appropriate."
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf",
        "title": "Simple Reinforcement Learning with Tensorflow Part 7: Action-Selection Strategies for Exploration",
        "text": "In this entry of my RL series I would like to focus on the role that exploration plays in an agent\u2019s behavior. I will go over a few of the commonly used approaches to exploration which focus on action-selection, and show their comparative strengths and weaknesses, as well as demonstrate how to implement each using Tensorflow. The methods are discussed here in the context of a Q-Network, but can be applied to Policy Networks as well. To make things more intuitive, I also built an interactive visualization to provide a better sense of how each exploration strategy works (It uses simulated Q-values, so there is no actual neural network running in the browser \u2014 though such things do exist!). Since I can\u2019t embed it in Medium, I have linked to it here, and below. I highly recommend playing with it as you read through the post. Let\u2019s get started!\n\nThe first question one may ask is: why do we need exploration at all? The problem can be framed as one of obtaining representative training data. In order for an agent to learn how to deal optimally with all possible states in an environment, it must be exposed to as many of those states as possible. Unlike in traditional supervised learning settings however, the agent in a reinforcement learning problem only has access to the environment through its own actions. As a result, there emerges a chicken and egg problem: An agent needs the right experiences to learn a good policy, but it also needs a good policy to obtain those experiences.\n\nFrom this problem has emerged an entire subfield within reinforcement learning that has attempted to develop techniques for meaningfully balancing the exploration and exploitation tradeoff. Ideally, such an approach should encourage exploring an environment until the point that it has learned enough about it to make informed decisions about optimal actions. There are a number of frequently used approaches to encouraging exploration. In this post I want to go over some of the basic approaches related to the selection of actions. In a later post in this series I will cover more advanced methods which encourage exploration through the use of intrinsic motivation.\n\nExplanation: All reinforcement learning algorithms seek to maximize reward over time. A naive approach to ensuring the optimal action is taken at any given time is to simply choose the action which the agent expects to provide the greatest reward. This is referred to as a greedy method. Taking the action which the agent estimates to be the best at the current moment is an example of exploitation: the agent is exploiting its current knowledge about the reward structure of the environment to act. This approach can be thought of as providing little to no exploratory potential.\n\nShortcomings: The problem with a greedy approach is that it almost universally arrives at a suboptimal solution. Imagine a simple two-armed bandit problem (for an introduction to multi-armed bandits, see Part 1 of this series). If we suppose one arm gives a reward of 1 and the other arm gives a reward of 2, then if the agent\u2019s parameters are such that it chooses the former arm first, then regardless of how complex a neural network we utilize, under a greedy approach it will never learn that the latter action is more optimal.\n\nExplanation: The opposite approach to greedy selection is to simply always take a random action.\n\nShortcomings: Only in circumstances where a random policy is optimal would this approach be ideal. However it can be useful as an initial means of sampling from the state space in order to fill an experience buffer when using DQN.\n\nExplanation: A simple combination of the greedy and random approaches yields one of the most used exploration strategies: \u03f5-greedy. In this approach the agent chooses what it believes to be the optimal action most of the time, but occasionally acts randomly. This way the agent takes actions which it may not estimate to be ideal, but may provide new information to the agent. The \u03f5 in \u03f5-greedy is an adjustable parameter which determines the probability of taking a random, rather than principled, action. Due to its simplicity and surprising power, this approach has become the defacto technique for most recent reinforcement learning algorithms, including DQN and its variants.\n\nAdjusting during training: At the start of the training process the e value is often initialized to a large probability, to encourage exploration in the face of knowing little about the environment. The value is then annealed down to a small constant (often 0.1), as the agent is assumed to learn most of what it needs about the environment.\n\nShortcomings: Despite the prevalence of usage that it enjoys, this method is far from optimal, since it takes into account only whether actions are most rewarding or not.\n\nExplanation: In exploration, we would ideally like to exploit all the information present in the estimated Q-values produced by our network. Boltzmann exploration does just this. Instead of always taking the optimal action, or taking a random action, this approach involves choosing an action with weighted probabilities. To accomplish this we use a softmax over the networks estimates of value for each action. In this case the action which the agent estimates to be optimal is most likely (but is not guaranteed) to be chosen. The biggest advantage over e-greedy is that information about likely value of the other actions can also be taken into consideration. If there are 4 actions available to an agent, in e-greedy the 3 actions estimated to be non-optimal are all considered equally, but in Boltzmann exploration they are weighed by their relative value. This way the agent can ignore actions which it estimates to be largely sub-optimal and give more attention to potentially promising, but not necessarily ideal actions.\n\nAdjusting during training: In practice we utilize an additional temperature parameter (\u03c4) which is annealed over time. This parameter controls the spread of the softmax distribution, such that all actions are considered equally at the start of training, and actions are sparsely distributed by the end of training.\n\nShortcomings: The underlying assumption made in Boltzmann exploration is that the softmax over network outputs provides a measure of the agent\u2019s confidence in each action. If action 2 is 0.7 and action 1 is 0.2 the tempting interpretation is that the agent believes that action 2 is 70% likely to be optimal, whereas action 1 is 20% likely to be optimal. In reality this isn\u2019t the case. Instead what the agent is estimating is a measure of how optimal the agent thinks the action is, not how certain it is about that optimality. While this measure can be a useful proxy, it is not exactly what would best aid exploration. What we really want to understand is the agent\u2019s uncertainty about the value of different actions.\n\nExplanation: What if an agent could exploit its own uncertainty about its actions? This is exactly the ability that a class of neural network models referred to as Bayesian Neural Networks (BNNs) provide. Unlike traditional neural network which act deterministically, BNNs act probabilistically. This means that instead of having a single set of fixed weights, a BNN maintains a probability distribution over possible weights. In a reinforcement learning setting, the distribution over weight values allows us to obtain distributions over actions as well. The variance of this distribution provides us an estimate of the agent\u2019s uncertainty about each action.\n\nIn practice however it is impractical to maintain a distribution over all weights. Instead we can utilize dropout to simulate a probabilistic network. Dropout is a technique where network activations are randomly set to zero during the training process in order to act as a regularizer. By repeatedly sampling from a network with dropout, we are able to obtain a measure of uncertainty for each action. When taking a single sample from a network with Dropout, we are doing something that approximates sampling from a BNN. For more on the implications of using Dropout for BNNs, I highly recommend Yarin Gal\u2019s Phd thesis on the topic.\n\nShortcomings: In order to get true uncertainty estimates, multiple samples are required, thus increasing computational complexity. In my own experiments however I have found it sufficient to sample only once, and use the noisy estimates provided by the network. In order to reduce the noise in the estimate, the dropout keep probability is simply annealed over time from 0.1 to 1.0.\n\nI compared each of the approaches using a DQN trained on the CartPole environment available in the OpenAI gym. The Bayesian Dropout and Boltzmann methods proved most helpful, at least in my experiment. I encourage those interested to play around with the hyperparameters, as I am sure better performance can be gained from doing so. Indeed, different approaches may be best depending on what hyperparameters are used. Below is the full implementation of each method in Tensorflow:\n\nAll of the methods discussed above deal with the selection of actions. There is another approach to exploration that deals with the nature of the reward signal itself. These approaches fall under the umbrella of intrinsic motivation, and there has been a lot of great work in this area. In a future post I will be exploring these approaches in more depth, but for those interested, here is a small selection of notable recent papers on the topic:"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-the-cyber-the-zerg-the-ethics-and-more-c636590e6cfc",
        "title": "Emergent // Future Weekly: The Cyber, the Zerg, the Ethics, and more",
        "text": "Issue 30\n\n This week we check out the the government\u2019s open source projects, how Google DeepMind is learning to play StarCraft II, and what CMU is doing to improve the ethics of artificial intelligence.\n\nPlus, our top projects to try at home, and our favorite articles from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nThe goal is to improve access to the government\u2019s custom-developed software, like the We the People app, the White House Facebook chatbot, and Data.gov.\n\nThe site currently has 50 repos from 10 federal agencies. Get the Code.gov project Repo here.\n\nGoogle\u2019s DeepMind and Blizzard are working together to open up StarCraft II to AI and machine learning researchers.\n\nThe goal is to test DeepMind and see if it can learn to play the complex real-time strategy game. The game will also be modified so researchers can build artificial intelligence systems for the purpose of learning to play StarCraft 2.\n\nWhy? Games are a perfect environment to develop and test smarter, more flexible AI algorithms, because they provide instant feedback through scores.\n\nCarnegie Mellon is set to open a research center focused on the ethics of artificial intelligence after receiving a $10 million endowment. The funds will be used for undergraduate students and to host a biennial conference on AI ethics.\n\nThe IEEE recently launched an initiative to examine ethical considerations in the design of autonomous systems.\n\nThese initiatives echo the Obama administration\u2019s new report on the future of AI, which puts a strong emphasis on ethics.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.\n\nLovingly curated for you by Algorithmia\n\n Follow @EmergentFuture for more frontier technology posts"
    },
    {
        "url": "https://medium.com/emergent-future/cognitive-toolkits-artsy-ai-solar-roofs-and-autonomous-trucking-66069bfe4e77",
        "title": "Cognitive Toolkits, Artsy AI, Solar Roofs, and Autonomous Trucking",
        "text": "You Might Have Heard: Microsoft launched Cognitive Toolkit 2.0 beta last week, the next version of its deep learning framework.\n\nPreviously known as CNTK, it was initially developed by Microsoft developers who wanted a tool to do their research more quickly and effectively. The latest version of the toolkit, available on GitHub, includes new functionality and support for Python and C++.\n\nBut Did You Know: AI Pioneer Yoshua Bengio is launching Element AI, a deep learning incubator.\n\nThe incubator will help build companies from the AI research that emerges from the University of Montreal, where Bengio is also a professor, and at nearby McGill University.\n\nBengio says this is just part of his efforts to develop an ecosystem for AI in Montreal.\n\nThat\u2019s why we\u2019re working with MIT, University of Washington, Carnegie Mellon University, University of California Berkeley, Caltech, University of Texas at Austin, University of Tokyo, University of Toronto, and others to make state-of-the-art algorithms accessible and discoverable by everyone."
    },
    {
        "url": "https://medium.com/emergent-future/speech-recognition-milestone-open-sourced-ai-webcams-breaking-the-internet-73a9e0702411",
        "title": "Speech Recognition Milestone, Open Sourced AI, Webcams Breaking the Internet",
        "text": "You Might Have Heard: Microsoft AI research group says its speech recognition tech is now as proficient as humans.\n\n\u201cWe\u2019ve reached human parity,\u201d says Microsoft\u2019s chief speech scientist Xuedong Huang in a statement. \u201cThis is an historic achievement.\u201d\n\nThe 5.9% error rate is about equal to that of professional transcriptionists. Microsoft\u2019s system understands acoustics and details like a speaker\u2019s pitch and how fast or slow they speak.\n\nBut Did You Know: Amazon filed a patent for a new drone that can both see and hear you.\n\nThe voice-controlled drone is small enough to fit in your pocket, and could act like a personal assistant hovering over you.\n\nThere is a gotcha, however: Most of the use cases Amazon describes in the filing aren\u2019t legal since they require flying a drone out of line of sight and in populated areas. Oops. \ud83d\ude14\n\nAI agents like Alexa, Siri, and M will create the first trillion-dollar company."
    },
    {
        "url": "https://medium.com/emergent-future/learning-data-science-9fbf745fab13",
        "title": "Learning Data Science \u2013 Emergent // Future \u2013",
        "text": "Back in August I started learning Data Science. For a couple of months (August and September) I\u2019ve devoted a lot of time to this task. I\u2019ve chosen to study it by my self using online resources, mainly MOOCs. All the courses described here are free. Here is my experience.\n\nIn general I really like the approach of Udacity: short videos with frequent and relatively easy quizzes to test yourself, the teacher actually writes in front of you, she doesn\u2019t read slides. The student has the same point of view of the teacher. It\u2019s an amazing toolkit, BTW.\n\nI really needed a refresh of statistics and these courses are extremely easy to follow, the problem sets are hard enough to be challenging but never discouraging. The choice of the material is simply perfect.\n\nIt starts with an introduction about processing and cleaning of datasets with Python, mainly aimed at non-programmers, I think.\n\nThe best part are the second and the third lessons. They are a great introduction to NumPy and Pandas.\n\nNot as compelling as the previous courses. The lesson about data preparation is for the most part a repetition of \u201cIntro to Data Analysis\u201d.\n\nThe lesson \u201cData Analysis\u201d is a nice overview of useful algorithms in scipy and how to apply them in a very straightforward way to your dataset.\n\nThis course includes also some interviews with real data scientists. Their goal, I guess, it to provide you with useful tips for your career.\n\nAnother lesson is devoted to data visualisation. It teaches to use ggplot and its counter-intuitive declarative syntax.\n\nThe last lesson is about map reduce, but honestly I wouldn\u2019t define it so useful.\n\nI really liked this course. It\u2019s easy to follow and Sebastian Thrun is a great teacher, he is able to communicate the intuition of non-trivial concepts and algorithms in a very effective way.\n\nThe mini-projects at the end of each lesson are in general very easy and for the most part based on the Enron emails dataset.\n\nThis is a very basic introduction to Apache Spark. It clarifies the main concepts of Spark very well, e.g., what transformations and actions are. The course guides the student to configure a free account on Databricks and has two very useful exercises, in the form of IPython Notebooks, that help you to grasp the (IMHO confusing) syntax of Spark.\n\nI am still not sure about the advantages of using Spark instead of other tools. I think the next most obvious step would be Distributed Machine Learning with Apache Spark. I will give it a try.\n\nAt the end of this period I was able to read and understand (most of the) kernels of other users on Kaggle, and to publish my first one."
    },
    {
        "url": "https://medium.com/emergent-future/do-humans-dream-of-android-children-90ac3acae5a1",
        "title": "Do humans dream of android children? \u2013 Emergent // Future \u2013",
        "text": "Technology is getting to a point where it is indistinguishable from something that is intelligent, from life. We will not (haven\u2019t) even notice(d) at what point it became an inseparable part of ourselves. This will mean that we will experience all range of emotions and psychological attachments to these \u201cthings\u201d.\n\nThe reason to be kind to machines is not because the machines can or cannot become conscious. This is not a question about philosophy or definition of life. The reason to be kind to AI, is to practice and refine the best parts of ourselves.\n\nPracticing kindness as we design these new learning technologies extends the range and possibility of what we can do with our compassion."
    },
    {
        "url": "https://medium.com/emergent-future/immersive-experiences-lifelong-learning-471c43fd6f38",
        "title": "Immersive Experiences & Lifelong Learning \u2013 Emergent // Future \u2013",
        "text": "A summary of the technologies that are most integral to learning during the Age of Exponential Technologies and an overview of how the workforce is changing as we enter the Fourth Wave of The Industrial Revolution. Before we begin to focus on how to leverage new technologies for education, it will be useful to review how we arrived at this moment. Without getting lost in debates about the Prussian model or the value of the university, I think we can agree that much of education over the past 100 years has been to train people for the workforce. The liberal arts studies have endured, less so in some western countries, with the students often chided that their degree in philosophy will serve them well in their career as an actuary. From the American school system\u2019s emphasis on math in the 60s \u2014 to train scientists who will be able to compete with the Soviets \u2014 through the broader STEM movement in reaction to the explosion of the Internet, we are have spent the last 100 years fighting the last war. The Industrial Revolution brought us the division of labor, radically changing the way most of us work. In order to be most efficient, our tasks were highly specialized, and usually rote. This has lead to a general dissatisfaction with work, where people often work for the weekends. Or for holidays. Or just to make to retirement. Even Adam Smith, a father of modern capitalist economics, noted that repetitive, machinelike work dehumanizes man. From The Wealth of Nations: \u201cThe man whose whole life is spent in performing a few simple operations, of which the effects, too, are perhaps always the same, or very nearly the same, has no occasion to exert his understanding, or to exercise his invention, in finding out expedients for removing difficulties which never occur.\u201d Now we are entering the Fourth Wave of the Industrial Revolution. While the Third Wave merely digitized our economy, the Fourth relies on creatively using a combination of mature technologies. Most repetitive work has already begun leaving the realm of human labor. The skills required to thrive in the new market are those that make us most human. Even the most powerful artificial intelligence is incapable of empathy, curiosity, and creativity. We live in the age of Exponential Technologies, which means things are going to change drastically, very quickly. In linear growth, change occurs in a predictable manner, and we understand how to cope with it as it comes. With exponential growth, technologies underperform relative to our expectations\u2026 for a little while. You may be familiar with Moore\u2019s Law: \u201cthe observation that the number of transistors in a dense integrated circuit doubles every 18 months.\u201d This concept, dating from the 1960s, is often applied to all technology, not just circuitry. The growth within the Age of Exponential Technology is composed of sigmoidal curves (S-curves). This means we have moments of small plateaus before another massive, sudden spike in growth. We have been on one of those plateaus for about ten years now. How many people remember life \u2014 truly daily life \u2014 before the smartphone? The iPhone was introduced in 2007. Sometime in the following years, most of you replaced your feature phone with a device equipped with GPS, digital camera, video conferencing, music player, gaming system, video player, and more. One that you controlled with a touchscreen and, a little while later, your own voice. A device that gave you quick access to all of the world\u2019s information at any time, anywhere.\n\nIt has been said that Thomas Jefferson\u2019s library contained off of the western world\u2019s known information at the time. Jefferson obsessed with collecting books after his family home burned in 1770, destroying his collection at the time\u2026 so I hope you have a backup. Actually, you don\u2019t need to, because this information exists in the cloud. Many places at all times. We\u2019ve grown accustomed to having these devices. This is human nature. When it comes to technology: we resist, we accept, we expect. If I\u2019m right, and we have been on this plateau for nearly ten years, what\u2019s next? We have been making progress, but it has been out of sight from consumers, mostly. Siri has been getting smarter through improvements in artificial intelligence. Designers use 3D printers to rapidly build functioning prototypes. Recreational drones fill the air and may soon be delivering pizzas. But the revolutionary change comes when technologies are used together. There are many exponential technologies that will impact our daily lives, but we are here for education technology. At this moment, the biggest potential for professional development is in immersive experiences: augmented reality, virtual reality, and interactive 360\u00b0 video. Immersive experiences rely on mixed reality: the merging of real world and virtual worlds to produce a new environment where physical and digital objects can coexist and interact. Mixed reality can be viewed as existing on a spectrum, the virtuality continuum, as Paul Milgram initially defined it in 1994.\n\nOn one end of the spectrum, is the physical world with some peripherals through which we can interface with the virtual world, or TUI (Tangible User Interfaces), such as a mouse, keyboard, can touchscreen. Moving along the spectrum, our perception of the physical world decreases as our interaction with the virtual world increases. Spatial augmented reality, such as we often see in museums, projects images into our physical space. This example, from the Museum of Natural History, shows how Superstorm Sandy affected New York City. Visitors could manipulate the data and see a visual representation adjust accordingly on a topographical map. See-through AR, which we can experience through a handheld device (phone or tablet) or head-mounted device (Google Glass or Microsoft HoloLens) displays digital images to the individual viewer as if it were existing in the physical world. The most popular example of this has been Pokemon Go. Augmented virtuality incorporates more virtual elements and less of the user\u2019s physical space. This has been a small market. The image used is from the education resource Z-Space. Finally, at the far end of the spectrum, we have fully immersive virtual reality. In true VR, the user is no longer aware of their physical surroundings, which has been replaced by the virtual one. Our recent interest at Pearson has been focussing on the right side of the spectrum. Learning is best achieved through experiences. Thanks to the wonderful invention the Gutenberg Press, for hundreds of years we have been able to engage with information recorded in text and widely distributed. However, interactions are limited, even when presented in a digital format with tangible user interfaces. Through immersion, we enable learners to engage with information. These simulations trick the brain into believing that the experiences are real. This is something that the human brain is very good at and machines are not \u2014 even AI: making leaps in logic, connecting dots. For example, Pearson developed a 360\u00b0 Video VR tour for the London Underground. The narrated instruction utilizes the immersive experience to orient the students while providing additional, digital information on top of the \u201creal-world\u201d visual information. When the student is in the actual environment, the lesson will have provided valuable information not so apparent when not in the simulation, but hopefully remembered. Another key component to immersive experiences relates to digital information that \u201cremains\u201d in a physical space. The best term for this is \u201csituated media.\u201d Again, if you played Pokemon Go, think of the fact that those Pokemon existed in the real world, in the same physical space but in the form of virtual information, for everyone who had the app installed on their phone. For more on situated media, check out The Triad of Technologies for Location-Based Mixed Reality. I stated earlier that the next great leap in technological progress will occur when multiple technologies are used in unison. Virtual reality, augmented reality, and situated media constitute a relatively small piece of the Exponential Technology pie. For an idea of what could be next, let\u2019s consider adding artificial intelligence and biosyncing. Biosyncing is communication between the virtual and physical worlds. It is the symbiotic relationship between the biological and the digital, where the human receives and processes information from the machine, and the machine receives and processes information from the human. We get raw data or nudges from our machine counterpart(s); we consciously send commands through our voice or gestures and subconsciously do so through our heart beat, facial reactions, and the tone we use when giving commands. (For more on biosyncing, check out Improved Learning Through Biosyncing.) Immersed in a virtual world, every interaction is data. Elements of the immersive experience, through AI, can adjust to our behavior, both conscious and subconscious, through biosyncing. Imagine an emergency medical training simulation. You are the doctor, and the patient\u2019s response to your actions is artificially intelligent. Whether your virtual patient lives or dies, and whether or not he is traumatized depends not only on memorized information you learned from a textbook, but how your hands manipulate the tools at your disposal, and the way in which you command your surroundings just as you would have to in the real world. The difference is that this world is simulated, controlled, and safe. Your brain will hardly know the difference in the moment. This is The Convergence. What does all of this mean for lifelong learners today? 21st century skills are a must for all of us. We are figuring out how to adjust our education system so that our youth learn persistence, critical thinking, problem solving, culture awareness, and collaboration in addition to more traditional hard skills. However, those of us in the workforce need to immediately adopt and maintain two of those 21st century skills: flexibility and adaptability. Adapting to new methods of learning, especially through immersive technology, will unlock the possibility of exploring new pathways as if they were first hand experiences. However, these technologies will change quickly and often, so we need to remain flexible. Developments in technology will, in some ways, make this easier. Learning is a naturally social act, but many experiences in augmented reality and virtual reality have been developed with a singular, isolated user in mind. This is quickly changing: HoloPortation, using the HoloLens, enables two people engage in the same physical space in real time. I\u2019ll end with two quotes from a fellow American. He will be awarded the Nobel Prize for literature in Stockholm in December: \u201cYou don\u2019t need a weatherman to know which way the wind blows,\u201d which, unfortunately, make me unnecessary. So you can rely on that same 21st skill that brought you to this room: curiosity. This presentation was created and delivered on 20 October in Stockholm for EdTech Sweden."
    },
    {
        "url": "https://medium.com/emergent-future/vc-lunch-are-chat-bots-over-hyped-yes-52f01a13310b",
        "title": "Are Chat-Bots Over-Hyped? Yes. \u2013 Emergent // Future \u2013",
        "text": "There\u2019s been a tremendous amount of attention paid lately to a genre of AI that\u2019s existed since the stone of age \u2014 chat-bots. I was asked by a friend recently to elucidate my opinions on the bot hype, and I thought it might be worth sharing in my inaugural post.\n\nMy instinct is that chat-bots are over-hyped. Sure, there are revenue angles and B2B aggregation plays that are completely valid positives for the proliferation of this technology. Generally, however, there has been a massive increase in interest around AI over the last year or so, and this explosion of chat-bots we are seeing is mainly a result of that. Chat-bots are a preexisting application of AI that people are taking another look at simply because the space as a whole is hot right now. This may seem like a fairly large write-off, and of course there have been advancements in Natural Language Processing, but the bottom-line reality is that bots need to be smarter to be taken seriously. We can\u2019t expect to see wide-scale disruption from a standalone chat-bot until it can pass the Turing test in 99% of situations. This, after all, is the reason why chat-bots and assistants are not widely used to begin with. The UI/UX, and often times even the underlying AI, are just not that good at being human. No one wants to learn a new communication syntax every time they encounter a new bot. Even now in 2016, using Siri for 2 minutes will do a sufficient job of demonstrating just how far we are from a truly useful chat-bot.\n\nWhile it may seem disheartening, this doesn\u2019t mean that there are no worthwhile investments in the space today. Bots, as they are now, are actually extremely useful when it comes to automating low intelligence tasks. You don\u2019t need a chat-bot with perfect NLP to turn data fetching from 30 disparate platforms into a 20 second request rather than a 5 hour one. These applications, where the bot is responding to simple input and automating tedious business processes, are examples of the early potential for this market to develop a base for future iterations. The primary problem with this from an investment standpoint; however, is that those bots often feel more like platform features than companies. When considering a startup for investment, most VCs need to see major potential for expansion. A bot built on-top of a platform like Slack or Facebook messenger may be useful, but it needs to exist outside of that ecosystem and be able to hold its own as an independent platform to make sense as investment target.\n\nSo, is the $170MM+ poured into bots in the past year a complete waste? I don\u2019t think so. The chat-bot ecosystem as a whole may be both overcrowded and over funded, but that\u2019s not necessarily a bad thing when you consider how these types of trends generally tend to play out. We are in the arching phase of the \u201chype-cycle\u201d of this technology, and when it reaches its climax, what you hope to see is a 2nd generation of chat-bots that are beginning to figure out what works and what doesn\u2019t. Eventually the fat trimming will result in real breakthroughs. What I\u2019d look for here as the market becomes more mature is a bot that successfully differentiates itself through fundamentally superior performance as an indicator that a growth/adoption phase may be taking hold. This is how we get to a Turing-proof bot. Companies like Viv are a great example of the nascent stages of this transition, as they approach AI in a way that goes beyond traditional players like Siri and Google Assistant.\n\nI\u2019m looking forward to the chat-bot stampede dying down, mostly because I\u2019m excited to see what rises from its ashes. We may not get to see the HAL 9000 anytime soon, but if 5 years from now most people genuinely use some form of AI for meeting scheduling, I would consider it a win."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-preparing-for-our-ai-future-smarter-smartphones-self-driving-cars-12a1c0e71177",
        "title": "Emergent // Future: Preparing for Our AI Future, Smarter Smartphones, Self-Driving Cars",
        "text": "You Might Have Heard: President Obama sat down with MIT Media Lab director Joi Ito and WIRED to talk about neural nets, self-driving cars, and the future of the world.\n\nIn this must read interview Obama and Ito discuss how AI will shape our world over the next 50 years more than any other single technology.\n\nBut Did You Know? The Obama Administration releasedPreparing for the Future of Artificial Intelligence, a report focused on the opportunities, considerations, and challenges of artificial intelligence on society.\n\nThe new report coincided with the White House Frontiers Conference in Pittsburgh, where some of the world\u2019s top innovators came together to discuss how science and technology can help improve lives, such as using machine learning to build intelligent traffic systems, improve medical diagnoses, and protect wildlife.\n\nWorried about your job? Don\u2019t be. While artificial intelligence will kill some jobs, it\u2019ll create others.\n\nALSO: Obama challenged America to send humans to Mars by the 2030s, and safely return them to Earth, with the ultimate ambition to one day make human settlement of space a reality.\n\nGoogle aims to win the AI revolution by building a personal assistant into everything.\n\nTim Cook, meanwhile, says the iPhone\u2019s future is in AI, which is why Apple\u2019s hired a prominent artificial intelligence researcher from Carnegie Mellon University.\n\nApple\u2019s penchant for secrecy has burdened efforts to improve their AI offerings and hire the best talent. Researchers in the field like to publish their findings, something Apple has historically frowned upon.\n\nWalt Mossberg says Apple wasted its lead with Siri and now has an intelligent assistant that is too limited and unreliable to be an effective tool in coming AI wars.\n\nIt\u2019s a bit ironic then that Samsung recently acquired Viv, the next-gen AI assistant built by the creators of Apple\u2019s Siri. \ud83d\udd25\n\nIn the end, the only difference between the next generation of smartphones will be their AI assistants."
    },
    {
        "url": "https://medium.com/emergent-future/the-fault-in-our-approach-what-youre-doing-wrong-while-implementing-recurrent-neural-network-lstm-929fbe17723c",
        "title": "The Fault in our Approach: What you\u2019re doing wrong while implementing Recurrent Neural\u2026",
        "text": "I started to delve into the field of Machine Learning few months back and after making a few projects, I thought to myself, \u201cthis isn\u2019t really tough\u201d. That was until I encountered Deep Learning. A whole new field of study, Deep Learning requires a vast amount of mathematical as well as analytical knowledge.\n\nAs I was preparing to get hands on with Neural Nets, I realized that it is so overwhelming. There are so many complex concepts that cannot be just \u201clearnt and implemented\u201d. Recurrent Neural Networks, for example, requires a different level of understanding than Convulation Neural Networks. My learnings were marred by the struggle of not being able to implement Neural Nets.\n\nI was focusing too much on implementing RNN with LSTM that I totally overlooked the implication of the model. So here\u2019s my first advice: Do not be a robot and jump straight into coding. Modelling a RNN requires patience and understanding. How can you create a net without being able to determine the input dimensions?\n\nI was given the job to pick up a case study and apply RNN model on it. The mistake I commited here was to start coding right away without even giving a single thought to the data! I chose to model a Stock Market Prediction Engine as my case study."
    },
    {
        "url": "https://medium.com/emergent-future/the-improved-realities-part-2-why-embrace-the-convergence-a703717b31e5",
        "title": "Improved Realities Part 2: Why Embrace The Convergence?",
        "text": "The Convergence refers to utilizing three suites of technologies \u2014 mixed reality, artificial intelligence, and biosyncing \u2014 to create hybrid realities in which virtual and physical components interact seamlessly. Through this series, I aim to explain this trend in greater detail as well as what it means for education, specifically. In my previous post, I covered an overview of what The Convergence consists of and how the these elements may be used once combined. Here, I will attempt to explain why we should focus on the three suites of technologies that constitute The Convergence.\n\nWe have not yet arrived at the moment when we can leverage the full capabilities of all of these technologies working in tandem. In this time of pre-Convergence, however, we can create products using each one of these suites individually and add in elements as that becomes feasible. It is a crucial time for exploration and understanding. The more we learn about the capabilities of each, the better prepared we will be to build even greater products and services, combining the most powerful elements of the individual technologies in a way that boosts the features of the others. Think of the Three Tenors: Pl\u00e1cido Domingo, Jos\u00e9 Carreras, and Luciano Pavarotti \u2014 each fantastic on his own, but, when working together, even more powerful. Or the Marx Brothers, if that\u2019s more your style."
    },
    {
        "url": "https://medium.com/emergent-future/improved-realities-what-is-the-convergence-29d94eb8c278",
        "title": "Improved Realities Part 1: What Is The Convergence?",
        "text": "The Future Technologies team at Pearson researches emerging technologies and trends that can be used for educational purposes. The team has completed over 40 projects and prototypes, including explorations into robotics, brain-computer interfaces, gesture-recognition, and more. Many projects have included individual features or themes that came to be standard in product development, such as responsive web design and elements of gamification.\n\nOn taking a more expansive view of Future Technologies\u2019 work, we have identified one major theme that ties many of these prototypes together: an attempt to bridge the virtual and physical worlds. This Convergence of digital and tangible objects and interactions incorporates many different technologies \u2014 too many for even one suite of technologies to accurately describe. There are three components key to merging the information existing in parallel in our \u201creal-time\u201d world and in the ever-expanding digital world."
    },
    {
        "url": "https://medium.com/emergent-future/what-can-ai-actually-do-c61f4150737b",
        "title": "What can AI Actually Do? \u2013 Emergent // Future \u2013",
        "text": "What\u2019s the difference between a forest fire and a hurricane? What\u2019s the difference between bricklaying and architecture?\n\nPredictions of how quickly AI technologies will advance are as plentiful as they are diverse. Often the most extreme predictions garner the most attention, but there are also many experts who are considerably more conservative in their forecasts. Getting a handle on the direction of travel of these technologies would seem to be imperative for the making of both good business decisions and good public policy. How did we get to a position of such uncertainty? How do we figure out what AI can actually do? I\u2019m going to try and sketch some answers to these questions, without resorting to any math, so please bear with me!\n\nBefore turning to the second question I\u2019m going to take a stab at the first. Most public discussion of AI actually refers to a much narrower field: Deep learning or deep neural networks. Neural networks have been studied since the 1950s and for most of their history were considered pretty useless. It\u2019s probably fair to say that interest in them resumed significantly in 2012 in large part thanks to a paper by Krizhevsky, Sutskever and Hinton on image recognition (Hinton now moonlights at Google). Two important things happened here:\n\nSince 2012 huge progress has been made, with neural networks being applied very successfully to a vast and growing set of problems that have stumped AI researchers for decades. Suddenly engineers and researchers were able to solve, with relative ease, problems that a few years earlier were considered unassailable. I think this is source of much of the optimism about the progress of AI in the next few years.\n\nResearchers remained largely ignorant though, as to why these techniques were so effective. There were no known limits on what could be accomplished using this technology. It seemed that all you needed was a big enough neural network and you would have a program capable of learning to perform any task (strong AI). You will notice that I\u2019m using the past tense here, because this question may just have been answered.\n\nIn August two physicists from MIT and Harvard released a paper that goes a long way to explaining why neural networks perform so well at certain tasks. This work hasn\u2019t been peer reviewed yet but hopefully, given its high profile and accessible reasoning, any problems will be uncovered quickly. The main conclusions are that:\n\nSay we want an algorithm that can identify cats in photos. In this case a neural network would be a good choice because:\n\nThese properties of cats make it much easier for a neural network to learn how to recognise them in photos and the same is true for the vast majority of things we usually take photos of. We rarely think about how unusual they are. They\u2019re unusual because of all of the possible combinations of pixels that a photo could consist of, most wouldn\u2019t have these kind of patterns in them. Most possible photos contain no patterns at all, they\u2019re just noise, mess, chaos.\n\nEven of the photos that do contain patterns that can be learned, most don\u2019t have the same properties. Consider another common example of photos that are intensely analysed all of the time, stock prices. Let\u2019s say you want an algorithm that\u2019s going to tell you when the chart of a stock price is from a day when there was some exciting news about the company. Immediately you run into a few problems:\n\nAll of these problems make it difficult to train a neural network to perform this task. When human beings engage in this kind of exercise they combine different sources of knowledge in ways that are difficult to codify simply.\n\nWe\u2019re now ready to attempt an answer to the question that titles this essay. We should expect neural networks to be extraordinarily useful in the natural sciences and those disciplines related to their application (e.g. medicine and mechanical engineering). Even within these disciplines though, there are problems that don\u2019t have these kind of properties. Hurricanes, for example, are famously (and tragically) unpredictable, their future paths depending not just on where the weather system is now, but its entire history.\n\nThe upshot of the this is that neural network powered systems capable of predicting and manipulating the most aspects of physical world apparently face no significant barriers. Even very complicated physical tasks can probably be learned using neural networks small enough to be practical in terms of the computing power they need. A robot that can do the ironing is probably just around the corner!\n\nMoving onto social or intellectual pursuits the picture becomes more complicated. Predicting the monetary value of a building or the success of a military strategy is a problem in which the properties listed above clearly don\u2019t apply. Suggesting that AI would struggle with such tasks isn\u2019t new, but I think we can now begin to understand what makes these cases different.\n\nLearning to physically assemble the Sydney Opera House is a question of learning to adequately manipulate the materials the building is constructed of. It\u2019s a task that can be reduced to a (large) number of simple steps, each of which themselves can be reduced to relatively simple rules governing the behaviour of the materials involved and so on. Predicting that Utzon\u2019s design for the Sydney Opera House would be the winning bid is a qualitatively different kind of task. It requires understanding the history, politics and finance surrounding the bid process and is not reducible to a combination of simpler steps in the same way.\n\nUnderstanding that the effectiveness of neural networks derives from their ability to model the kind of systems common in physics helps us to predict the kind of tasks they will be able to learn successfully. We should expect neural networks to be able to take over a huge variety of tasks where the ability to predict the physical environment is key. Ironing, driving and construction are all fair game. Tasks such as finance, architecture and publishing are likely to be much harder to learn. I don\u2019t think we should expect the robots to be coming for the architects anytime soon."
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-6-partial-observability-and-deep-recurrent-q-68463e9aeefc",
        "title": "Simple Reinforcement Learning with Tensorflow Part 6: Partial Observability and Deep Recurrent\u2026",
        "text": "In this installment of my Simple RL series, I want to introduce the concept of Partial Observability and demonstrate how to design neural agents which can successfully deal with it. As always, in addition to a theoretical discussion of the problem, I have included a Tensorflow implementation of a neural agent which can solve this class of problems.\n\nFor us humans, having access to a limited and changing world is a universal aspect of our shared experience. Despite our partial access to the world, we are able to solve all sorts of challenging problems in the course of going about our daily lives. In contrast, the neural agents we have discussed so far in this tutorial series are ill equipped to handle partial observability.\n\nWhen we think about the kinds of environments used until this point to train our networks, the agent has had access to all the information about the environment it might need in order to take an optimal action. Take for example the Gridworld used in Tutorials 4 & 5 of this series:\n\nBecause the entire world is visible at any moment (and nothing moves aside from the agent), a single frame of this environment gives the agent all it needs to know in order to maximize its reward. Environments which follow a structure where a given state conveys everything the agent needs to act optimally are called Markov Decision Processes (MDPs).\n\nWhile MDPs provide a nice formalism, almost all real world problems fail to meet this standard. Take for example your field of view at this very moment. Can you see what is behind you? This limited perspective on the visual world is almost always the default for humans and other animals. Even if we were to have 360 degree vision, we may still not know what is on the other side of a wall just beyond us. Information outside our view is often essential to making decisions regarding the world.\n\nIn addition to being spatially limited, information available at a given moment is also often temporally limited. When looking at a photo of a ball being thrown between two people, the lack of motion may make us unable to determine the direction and speed of the ball. In games like Pong, not only the position of the ball, but also it\u2019s direction and speed are essential to making the correct decisions.\n\nEnvironments which present themselves in a limited way to the agent are referred to as Partially Observable Markov Decision Processes (POMDPs). While they are trickier to solve than their fully observable counterparts, understanding them is essential to solving most realistic tasks.\n\nHow can we build a neural agent which still functions well in a partially observable world? The key is to give the agent a capacity for temporal integration of observations. The intuition behind this is simple: if information at a single moment isn\u2019t enough to make a good decision, then enough varying information over time probably is. Revisiting the photo example of the thrown ball A single image of a ball in motion tells us nothing about its movements, but two images in sequence allows us to discern the direction of movement. A longer sequence might even allow us to make sense of the speed of the ball. The same principle can be applied to problems where there is a limited field of view. If you can\u2019t see behind you, by turning around you can integrate the forward and backward views over time and get a complete picture of the world with which to act upon.\n\nWithin the context of Reinforcement Learning, there are a number of possible ways to accomplish this temporal integration. The solution taken by DeepMind in their original paper on Deep Q-Networks was to stack the frames from the Atari simulator. Instead of feeding the network a single frame at a time, they used an external frame buffer which kept the last four frames of the game in memory and fed this to the neural network. This approach worked relatively well for the simple games they employed, but it isn\u2019t ideal for a number of reasons. The first is that it isn\u2019t necessarily biologically plausible. When light hits our retinas, it does it at a single moment. There is no way for light to be stored up and passed all at once to an eye. Secondly, by using blocks of 4 frames as their state, the experience buffer used needed to be much larger to accommodate the larger stored states. This makes the training process require a larger amount of potentially unnecessary memory. Lastly, we may simply need to keep things in mind that happened much earlier than would be feasible to capture with stacking frames. Sometimes an event hundreds of frames earlier might be essential to deciding what to do at the current moment. We need a way for our agent to keep events in mind more robustly.\n\nAll of these issues can be solved by moving the temporal integration into the agent itself. This is accomplished by utilizing a recurrent block in our neural agent. You may have heard of recurrent neural networks, and their capacity to learn temporal dependencies. This has been used popularly for the purpose of text generation, where groups have trained RNNs to reproduce everything from Barack Obama speeches to freeform poetry. Andrej Karpathy has a great post outlining RNNs and their capacities, which I highly recommend. Thanks to the high-level nature of Tensorflow, we are free to treat the RNN as somewhat of a black-box that we simply plug into our existing Deep Q-Network.\n\nBy utilizing a recurrent block in our network, we can pass the agent single frames of the environment, and the network will be able to change its output depending on the temporal pattern of observations it receives. It does this by maintaining a hidden state that it computes at every time-step. The recurrent block can feed the hidden state back into itself, thus acting as an augmentation which tells the network what has come before. The class of agents which utilize this recurrent network are referred to as Deep Recurrent Q-Networks (DRQN).\n\nIn order to implement a Deep Recurrent Q-Network (DRQN) architecture in Tensorflow, we need to make a few modifications to our DQN described in Part 4 (See below for full implementation, or follow link here).\n\nFor this tutorial however I\u2019d like to work with something a little less flashy, though hopefully more informative. Recall our Gridworld, where everything was visible to the agent at any moment. By simply limiting the agent\u2019s view of the environment, we can turn our MDP into a POMDP. In this new version of the GridWorld, the agent can only see a single block around it in any direction, whereas the environment itself contains 9x9 blocks. Additional changes are as follows: each episode is fixed at 50 steps, there are four green and two red squares, and when the agent moves to a red or green square, a new one is randomly placed in the environment to replace it. What are the consequences of this?\n\nIf we attempt to use our DQN as described in Parts 4 and 5 of this series, we find that it performs relatively poorly, never achieving more than an average of 2.3 cumulative reward after 10,000 training episodes.\n\nThe problem is that the agent has no way of remembering where it has been or what it has seen. If two areas look the same, then the agent can do nothing but react in exactly the same way to them, even if they are in different parts of the environment. Now let\u2019s look at how our DRQN does in the same limited environment over time.\n\nBy allowing for a temporal integration of information, the agent learns a sense of spatial location that is able to augment its observation at any moment, and allow the agent to receive a larger reward each episode. Below is the Ipython notebook where this DRQN agent is implemented. Feel free to replicate the results yourself, and play with the hyperparameters. Different settings for many of them may provide greater performance for your particular task.\n\nWith this code you have everything you need to train a DRQN that can go out into the messy world and solve problems with partial observability!"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-historic-ai-partnership-mission-to-mars-chatbots-aws-p2-gpus-c70bc0d56baf",
        "title": "Emergent // Future Weekly: Historic AI Partnership, Mission to Mars, Chatbots, AWS P2 GPUs",
        "text": "You Might Have Heard: Amazon, Facebook, Google, IBM, and Microsoft have formed a partnership to advance the public understanding of artificial intelligence.\n\nThe five tech giants are tasked with establishing best practices for dealing with the challenges and opportunities that AI presents. They\u2019ve released eight guiding principles, which evoke Isaac Asimov\u2019s original \u201cThree Laws of Robotics.\u201d\n\nTechCrunch points out that \u201cData is becoming an important currency for the modern world. The data\u2019s value is rooted in its applications to artificial intelligence. Whichever company owns the data, effectively owns AI. Right now that means companies like Facebook, Amazon, Alphabet, IBM and Microsoft have a ton of power.\u201d\n\nThis is why it\u2019s important to democratize access to state-of-the-art algorithmic intelligence.\n\nBut, why form an AI coalition at all? tl;dr people are afraid of a robot takeover.\n\nOh, and Apple refused to join new AI club. Go figure.\n\nBut Did You Know? Microsoft formed a new 5,000-person AI division. The combined AI and Research group and will include both the Cortana and Bing teams."
    },
    {
        "url": "https://medium.com/emergent-future/the-amazon-dash-button-another-automation-device-71568d9aca56",
        "title": "The Amazon Dash Button. Another automation device? \u2013 Emergent // Future \u2013",
        "text": "Home automation is at an all time high, with a reach into the market of even the least tech-savvy consumers. Amazon have just released a new product; but it is no Kindle. The Amazon Dash Button is a Wi-Fi-connected device that reorders a specified product with the press of a button.\n\nI recently read an article by Noah Kagan (founder of AppSumo.com) about how he has just moved into a new place in Austin, and has been \u201coptimising his place for maximum automation\u201d. Kagan goes on to talk about what devices he has been using and lists no less than 14 smart items he has implemented into his new home including Amazon Echo, TVs, Bulbs, Switches, Motion Sensors, Router, Thermostat and even his water bottle. All this working together using the power of the Wink Hub.\n\nBut what was truly intriguing is his explanation for why anyone should care!"
    },
    {
        "url": "https://medium.com/emergent-future/what-machine-learning-will-do-for-design-42661096f21",
        "title": "What Machine Learning Will Do For Design \u2013 Emergent // Future \u2013",
        "text": "Firstly, let\u2019s operating under an assumption that you are collaborating with a computer that has been trained to know about logo design. To do this, we have given it a supervised pre-tagged dataset of thousands of logos to learn with. We taught it logos from different decades, genres, and different types of companies. It learned to cluster and categorize logos, so it can differentiate industries like \u2018technology\u2019 vs \u2018entertainment\u2019 as well as styles like \u2018retro\u2019 vs \u2018clean\u2019.\n\nAgain, let\u2019s assume for now that the training set was sufficient and your computer is basically now a junior logo designer ready to assist you. Let\u2019s collaborate.\n\nYou first type in the client brief, tell it how many references you want to see, how long to spend finding each reference, and let the computer process the information. It scours it\u2019s database as well as Dribbble and Pinterest. It produces a grid of references to choose from. You capped the references to only 40, but 30 of them were not good, so you select the only 10 you like and run it again. Want to explain to the computer why you liked the 10 you chose? You have a few options, ranging from discrete to non-discrete . On the discrete side you have something like radio buttons:\n\nOn the non-discrete, conversational side, you have natural language processing. You can just talk about each one. You choose your feedback method and wait for more references. In a few minutes time, you receive 100 more references. You like 20 of them, so now you have a more specific dataset of 30 images to feed it. You can further describe why you like these images, being as verbose or curt as you want to be.\n\nThe computer shows you a few fonts, all spelling out the client\u2019s business name, and you choose the ones you like, with multiple rounds of revisions as necessary.\n\nYou\u2019re ready for the actual designs. Now since this is your first project with your computer, it does not know your style or your proclivities yet, but in future projects that will be weighted as well. What might this interface look like?\n\nImagine how a 3D designer can model their geometry from 4 perspectives at the time time:\n\nImagine something like that, but instead of spacial dimensions, it\u2019s aesthetic dimensions. The designer can slide the style parameters (in the top left) and see it update the logo (in the bottom right) in realtime. The designer can hand pick icons and fonts. If any one of these 3 categories needs to be locked, the designer can do so. Example, you have a font you like, but you change the style from \u2018masculine\u2019 to \u2018feminine\u2019 but you don\u2019t want the font to update."
    },
    {
        "url": "https://medium.com/emergent-future/good-morning-chatbot-hows-ux-today-the-impact-of-ai-in-user-experiences-7320fe35c333",
        "title": "\u201cGood morning chatbot, how\u2019s UX today?\u201d \u2014 The impact of AI in user experiences",
        "text": "Think about the very first moment of your day. It usually starts with a predefined user interface of your mobile phone: the alarm clock. It\u2019s set up in advance to do a simple task. Now what if your device would learn how to wake you up in a great mood, just like in the movie Her? It would choose the right words, the right tone of voice, and the right sounds to cheer you up. So instead of making you wish you had set up a new ringtone, it would adapt to your life in such way that no single designer in the world could have predicted while designing your app or device.\n\nThat is where the power of machine learning resides. By understanding our unique behaviors, computers will automatically build interfaces that are tailor-made to each one of us. New technologies will continue to pursue that, and the traditional UX field we have known for years may evolve into something very different. In this article, I\u2019ll attempt to cover the role of AI in this transition.\n\nEssentially, AI opens new opportunities for human-machine interactions. Interfaces will be based on more natural, humane communications, and many staples we know in electronic devices today might soon be forgotten. Think about it: we definitely did not need a mouse pointer to communicate with other people before the first personal computers were born. If computers are learning to listen to what we say and engage in real conversations, why would we need to click or touch screens in the future? As much as we\u2019ve learned how to make computer interactions a norm in our lives, they\u2019re still not natural to human beings. Humans build seamless connections among each other by using simple speech, text and visual expressions.\n\nWe\u2019ve already seen many AI consumer products make their way to the consumer market. For instance, Amazon recently launched Echo, which is a hands-free speaker fully controlled by the user\u2019s voice. Although it\u2019s still not a very advanced computer, it\u2019s potentially capable of rendering obsolete some mouse-and-keyboard interactions we tend to have in our daily lives. Take, for instance, a simple weather search:\n\nWe\u2019ve also seen samples of AI-powered interfaces in more business-driven contexts. The startup saas.do is preparing to launch a tool to help businessmen with software development. The service would feature a chatbot allowing users to build applications without any programming skills \u2014 that is, using simple conversational language such as \u201cCreate a to-do app\u201d.\n\nWith machine learning, every single user interaction could be processed and learned by computers. With progressive use, we may have interfaces that change and automatically adapt to each user\u2019s profile. So it\u2019s not too crazy to say we will never again have usability issues. However, we know usability is not everything in user experience. AI and bots, as much futuristic as they may sound, will allow designers to step back in time and focus on what really matters: the user\u2019s wants and needs. That means they will spend less time creating interfaces to solve user problems and will rather imagine the macro structures behind all those interactions.\n\nUX will not lose any of its relevance. But with the advent of digital products that are controlled purely by voice or conversational interactions, UX roles will undertake a major shift. User interfaces may become less visual and more humane experiences will begin to take over.\n\nWhat designers should keep in mind is that the means are always evolving. The PC completely changed the game. Mobile completely changed the game. Screen sizes keep on changing almost on a daily basis. But again, why should we focus on screens? They are all just means to an end. As we\u2019ve seen before, we don\u2019t really need a screen to get to know the weather forecast.\n\nMy point is, when it comes to technology, nothing is change-proof. And it will never be easy to predict every single trend. But what we can do is focus on the people behind the machines. What are they feeling or thinking when they click with a mouse, touch a screen, or speak to their bot?\n\nNow, we can focus on some devices that should always remain change-proof: sight, hearing, voice, taste, smell, touch, and most importantly, our minds and emotions. These are all intrinsic to human beings and aren\u2019t just trends. They happen every day, whether your phone screen is ready for it or not. And if you wonder \u201cthis is madness, we\u2019ll never be able to control machines with our minds\u201d, think again:\n\nThere is no one right interface for everyone. And there is no one technology to cover all of them. What AI and bots have taught us is that if we give users the right tools, at the right time, they will create their own experiences."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-making-a-spectacle-with-snapchat-ddos-attacks-notes-from-the-frontier-2be351009a75",
        "title": "Emergent // Future Weekly : Making a Spectacle with Snapchat, DDoS Attacks, Notes from the\u2026",
        "text": "You Might Have Heard: Snapchat announced Spectacles, glasses that record up to 10 seconds of video with a tap of a button.\n\nThe connected sunglasses are Snapchat\u2019s first hardware product and go on sale this fall for $130.\n\nWearers tap the button on the top left-hand corner of the glasses to begin recording a snap. An inward-facing light turns on when you\u2019re snapping, with an outward-facing light to alert anyone in your field of vision.\n\nThe Spectacles camera uses a 115-degree-angle lens, wider than a typical smartphone\u2019s, and records circular video, similar to human vision.\n\nBut Did You Know: Framing Spectacles as a toy is smart.\n\nThey\u2019re more than just specs, which is why they won\u2019t share the same fate of Google Glass.\n\nAnd, it\u2019s worth remembering: the next big thing usually starts out looking like a toy.\n\nWATCH: Check out the promotional video for Spectacles\n\nPLUS: Google launches VR SDK 1.0, with support for Daydream"
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-1-5-contextual-bandits-bff01d1aad9c",
        "title": "Simple Reinforcement Learning with Tensorflow Part 1.5: Contextual Bandits",
        "text": "In Part 1 of my Simple RL series, we introduced the field of Reinforcement Learning, and I demonstrated how to build an agent which can solve the multi-armed bandit problem. In that situation, there are no environmental states, and the agent must simply learn to choose which action is best to take. Without a given state state, the best action at any moment is also the best action always. Part 2 establishes the full Reinforcement Learning problem in which there are environmental states, new states depend on previous actions, and rewards can be delayed over time.\n\nThere is actually a set of problems in-between the stateless situation and the full RL problem. I want to provide an example of such a problem, and show how to solve it. My hope is that those entirely new to RL can benefit from being introduced to each element of the full formulation step by step. Specifically, in this post I want to show how to solve problems in which there are states, but they aren\u2019t determined by the previous states or actions. Additionally, we won\u2019t be considering delayed rewards. All of that comes in Part 2. This simplified way of posing the RL problem is referred to as the Contextual Bandit.\n\nIn the original multi-armed bandit problem discussed in Part 1, there is only a single bandit, which can be thought of as like a slot-machine. The range of actions available to the agent consist of pulling one of multiple arms of the bandit. By doing so, a reward of +1 or -1 is received at different rates. The problem is considered solved if the agent learns to always choose the arm that most often provides a positive reward. In such a case, we can design an agent that completely ignores the state of the environment, since for all intents and purposes, there is only ever a single, unchanging state.\n\nContextual Bandits introduce the concept of the state. The state consists of a description of the environment that the agent can use to take more informed actions. In our problem, instead of a single bandit, there can now be multiple bandits. The state of the environment tells us which bandit we are dealing with, and the goal of the agent is to learn the best action not just for a single bandit, but for any number of them. Since each bandit will have different reward probabilities for each arm, our agent will need to learn to condition its action on the state of the environment. Unless it does this, it won\u2019t achieve the maximum reward possible over time. In order to accomplish this, we will be building a single-layer neural network in Tensorflow that takes a state and produces an action. By using a policy-gradient update method, we can have the network learn to take actions that maximize its reward. Below is the iPython notebook walking through the tutorial.\n\nHopefully you\u2019ve found this tutorial helpful in giving an intuition of how reinforcement learning agents can learn to solve problems of varying complexity and interactivity. If you\u2019ve mastered this problem, you are ready to explore the full problem where time and actions matter in Part 2 and beyond of this series."
    },
    {
        "url": "https://medium.com/emergent-future/https-medium-com-joshvandebrake-digital-marketing-and-ai-7de56a231442",
        "title": "Embrace Innovation: Digital Marketing and AI \u2013 Emergent // Future \u2013",
        "text": "The Future of Digital Marketing and AI\n\nInnovation continues, life goes on. Old jobs are made obsolete, new jobs are created. When the greatest minds of the world decide to create something new, their engineers set out to find a way to do it. Concepts are created, wireframes are built, prototypes are designed, algorithms are written. In essence: these ideas start to take shape. These ideas become the innovations of the marketplace. They become disruptions to the status quo. These innovations are technological inventions: steam power, electricity, the light bulb. They are social developments: The Magna Carta, the Constitution, the Bill of Rights.\n\nThese earthshaking innovations take all forms, and they are adopted into society and business as \u201cthe new normal.\u201d Then new innovations shake those old ones off the pedestal. The cycle continues. New things are created, become normal, and get replaced.\n\nBack in time somewhere between the invention of the light bulb, and Back to the Future 2\u2019s version of 2015, Artificial Intelligence (AI) and Machine Learning started taking a forefront in the minds of the world\u2019s technology leaders. Google is investing more heavily than ever in those technologies. Apple has apparently been using it for their Siri program for years (though they didn\u2019t tell anyone until recently). Qualcomm is building much of the hardware this new era will need. Just to mention a few. Though there are many others who are doing the same. There are even some companies whose entire business is based on using AI in places where manual time-consuming work was required since their inception. I can think of a couple places where my own perspective has shifted."
    },
    {
        "url": "https://medium.com/emergent-future/recognising-beer-with-tensorflow-9dedfee3c3c0",
        "title": "Recognising Beer with TensorFlow \u2013 Emergent // Future \u2013",
        "text": "This guide is by-and-large a retread of Google\u2019s own material on the subject with some personal commentary and notes about my experiences.\n\nI initially thought I was going to train the model from scratch, so I pulled out one of our Mac Pro workstations and set to work. One and a half weeks later, the training was still running. Fortunately I had since aborted this plan.\n\nThere are actually some very smart people in Accenture Labs who understand this subject quite well (unfortunately I\u2019m still trying to encourage them to write blogs). Our Systems & Platforms research group has a rig packed with Nvidia\u2019s Titan GPUs that they use exclusively for neural network training. Because I enjoy fumbling through new technologies so much, though, I ignored the large body of research we already had in this area and set out to learn everything the hard way.\n\nIt turns out that training a model like this requires a lot more maths than the Mac Pro\u2019s CPU could get to in any reasonable time. CPUs are super flexible and have lots of neat instructions for doing stuff on usually around four to sixteen cores. GPUs, on the other hand, excel at doing floating point calculations on thousands of cores simultaneously.\n\nFurthermore, neural networks just take a really long time to train, and even with a GPU rig (which I did not borrow) I was still looking at potentially weeks of training time. That\u2019s not agile.\n\nIf academic papers are your thing, read this one. Simply speaking, instead of training an entire neural network it is possible to retrain just the final layer and still get pretty good results. This can be done on a standard laptop, without a GPU (I ran it in a Docker container on my MacBook Pro), in about half an hour.\n\nStep one is to acquire and structure the data. Over an hour or so, Allison and I took around 150 photos of Lagunitas IPA and Crazy Mountain Pale Ale in various lighting conditions, at different levels of zoom, held by people, not held by people\u2026\n\nOnce we had all the data in a central location, I docker cped it over to my TensorFlow container in the following directory structure.\n\nThen it was just a matter of using Google\u2019s provided example code:\n\nOur initial results were lacklustre. We found that, because all the images were taken in the Labs environment, the model was over-indexing on features of our d\u00e9cor that were over-represented in one set or the other. I fixed this in a rather hacky way: by creating a new class called nothing_interesting and filling it with garbage photos of the Lab, containing no beer. For good measure, I chucked in the flowers data from Google\u2019s tutorial to provide more negative examples. The final training data directory looked like this:\n\nAfter retraining the algorithm, it performed incredibly well \u2014 much more so than our existing SURF classifier. The model is in the form of two files: a Protocol Buffer serialisation of the model graph, and a simple ordered text file providing the human-readable names of the class labels (e.g. lagunitas_ipa, crazy_mountain_pale_ale). The model can be easily transported between machines, and is small enough to check in to source control.\n\nNow that we had a trained model, it was time to put it to use."
    },
    {
        "url": "https://medium.com/emergent-future/image-recognition-is-not-enough-293cd7d58004",
        "title": "Image recognition is not enough \u2013 Emergent // Future \u2013",
        "text": "At a recent Deep Learning Investor Conference, a panel of VC and angel investors were asked to discuss the state of Artificial Intelligence (machine learning, neural networks, or big data if you prefer those buzzwords). There was an interesting debate on how well machines handle natural language processing and visual recognition. The consensus is that machines are now very good at visual recognition. For example, if you show a machine a photo of a cat sitting on top of a turtle, the machine will recognize the green-shelled animal as a turtle, and the fuzzy, four-legged animal as a cat. Where the debate was spirited was in whether machines have cracked the natural language comprehension challenge.\n\nAfter some back-and-forth, the panelists clarified the argument. They ended up agreeing that machines are quite adept at transcription, to the point where machines will soon outperform humans in this task. In the case of language comprehension, machines have a ways to go.\n\nPanelist Leonard Speiser used the peanut butter and jelly challenge to articulate the problem with a machine\u2019s language comprehension.\n\nThe computer does this:\n\nAsk a 7 year old the same request. They will know exactly what you want. They may not do it, but that\u2019s a another problem altogether.\n\nLet\u2019s move on. The focus of this post is not to discuss the challenge of natural language comprehension. The object is to show that like language, image recognition requires more than simple object isolation and identification.\n\nLet\u2019s take a look at the above photo. If we were to feed a machine this photo, what does the machine see?\n\nNow let\u2019s contrast that with how a human sees this photo.\n\nPerhaps the machine could fill in some of the blanks to guess the sign in the back says \u201cHappy Birthday\u201d, but if the machine is taking the images literally, it will only register \u201cP P\u201d \u201cB R T\u201d clearly.\n\nAsk the computer what it sees in the image above, and it might answer, 4 missles taking off and one cat. Ask a human, and they will see the logic problem immediately. You might argue that computers are better than humans at detecting fake images by analyzing pixel patterns. That is true. However, like the peanut butter and bread problem, if the computer\u2019s task is to tag major items, it will go about the task without considering the absurdity of the image in its entirety.\n\nThis photo has more going on that just a wife and her groom. A human will see this image and understand that the shot has been setup to take advantage of the angle and the position of the couple to create this optical illusion. Our understanding of the relative heights of the man and the woman allow us to decode this image.\n\nWind, heat, and cold are camera shy. A human can see a picture like the above and see more than a series of trees, bushes, a body of water, a road and building in the distance. The human sees a gusty wind storm.\n\nPanicked groom, bridge and best man? Maybe, but looks more like the couple got hitched on an unseasonable hot day. Outdoor wedding? Look at the best man\u2019s shadow. Looks like an outdoor wedding to me. The likely first names of the bride and groom start with \u201cS\u201d and \u201cN\u201d. Did the computer get all that info?\n\nPeek-a-boo. Now you see me, now you don\u2019t. Babies lack object permanence. With time, they learn that mommy didn\u2019t magically disappear behind some hands. Once they gain object permanence, the game is no longer as fun. For still images, object permanence is not a problem for computers. Once you introduce videos or a series of photos into the mix, object permanence becomes important. The literally killer problem with object permanence is self-driving vehicles. Humans use our periphery vision and object permanence senses to detect changes in our driving environment. A squirrel we see out of the corner of our eyes, seemingly out of harms way, suddenly decides to scramble across the road. We notice the critter changing its course, and we tap the brakes and swerve to the right to allow our bushy-tail neighbor another day to collect acorns. Object identification for frames is not enough in this situation. The objects that could present a hazard need to be tagged and tracked till they are no longer a threat to or from the vehicle. The 4-year old chasing the baseball across the street is the edge case that is sure to keep self-driving engineers up at night unless they are able to account for the object permanence challenge.\n\nYou are finally making time on the commute home on the 280 when you notice the cars to the side of you rapidly slowing down. The car you are following has yet to register any slowdown, but your spider-senses are telling you that their is trouble up ahead. By tracking the vehicles to the side, you instinctively begin your slowdown and cross-your-fingers that the vehicle you are following can hit the brakes in time to avoid a collision with the vehicles ahead. Self-driving cars need to do more than track the vehicle they are following in case that lead car is driving carelessly. By analyzing the entire flow of traffic near, far and in the periphery, the vehicle can better predict dangerous slow-downs and obstacles regardless if the lead vehicle is not making the proper decisions.\n\nLet\u2019s consider the two highly publicized Tesla fatal accidents from early this year. In the case of Joshua Brown, a trailer was traversing Highway 27 in Florida. The Tesla Autopilot did not properly identify or calculate the height of the trailer. It guessed the trailer was a highway overpass. The result is the vehicle continued on its path without slowing down until it collided with the trailer. In this scenario both the object identification and the depth perception of the cameras did not properly compute. As well, the object permanence or lack thereof, of the trailer was misinterpreted. Consider the human reaction to the trailer blocking the freeway.\n\nIn this case, it appears the Tesla was traveling in Autopilot mode, and was following a vehicle. The vehicle directly ahead changed lanes to avoid a street-sweeper. The Tesla continued in its lane and crashed into the street-sweeper without slowing down.\n\nIn this incident, the Tesla seemed to hold too much faith in the vehicle it was directly following. Once the vehicle changed lanes, the Tesla was unable to make the split-second recognition and response to the fact that there was an object in its path.\n\nThis case is less clear than the Florida accident if a human driver would have avoided the fatal accident.\n\nA human driver may have better scanned the horizon ahead of the vehicle in front and may have seen the street-sweeper. If there was traffic (unclear in the report), the human may have noticed a traffic slow-down as the vehicles passed the street sweeper. As the vehicle in front abruptly changed lanes, the human response would be to go on high alert. The dramatic action signals to the driver that something is very wrong, and an immediate brake or change lane action is required. Even if the driver was not able to avoid the collision, they would have recognized the object and attempted to brake ahead of the collision.\n\nSince Tesla\u2019s Autopilot has a strong dependence on the vehicle directly in front, the software should maintain a safe minimum car-length follow distance to allow the system to brake ahead of this Chinese-accident scenario. The event of the sudden braking (deceleration) or abrupt lane change coupled with a possible object in the pathway should put the Autopilot on high alert and begin a braking or lane change decision.\n\nNOTE: The results of the investigation for the Chinese fatality have been limited. There is still some uncertainty if the Autopilot was engaged or if the vehicle was under human driving control. The reason that it is believed to be an Autopilot accident is that a complete lack of any braking action is a very unhuman reaction for this type of accident.\n\nComputer image recognition has greatly improved to a point where objects are being properly recognized in the high 90% range. A computer\u2019s ability to cross check a human\u2019s face and compare against a library of millions of tagged photos is impossible for a human to replicate. Image recognition technology is being applied to advance medical research, self-driving cars, fraud-detection, and fight terrorism. The results are encouraging, and are only getting better. We are at a point, as with natural language, where humans are better at the making sense of the data than machines. As we program learning algorithms for machines to decode dynamic image threads and non-visual clues, machines will eventually out-perform humans in interpreting the context of images and videos. When that day comes, the applications for computer automation are limitless."
    },
    {
        "url": "https://medium.com/emergent-future/data-portability-is-one-way-to-curb-the-power-of-the-tech-giants-fca7ec7c5634",
        "title": "Data Portability Is One Way To Curb The Power Of The Tech Giants",
        "text": "A handful of large commercial entities \u2014 notably Apple, Facebook, Google, Amazon, Microsoft and a few others are emerging as super-heavyweight players who control most of the digital economy.\n\nWe can see a similar concentration of power taking hold in China where players like Alibaba, Xiaomi and Tencent are asserting themselves at the expense of rivals who are falling by the wayside or are being relegated to market niches.\n\nIt is a good time to ask whether this concentration of market power will get more intense in the future, and what it means for the global economy if it does.\n\nFor decades, the workings of the global economy have been controlled by a triptych of powerful actors: macroeconomists, politicians and banks.\n\nBut it is becoming clear that this cosy arrangement is being challenged by arrival of a fourth actor \u2014 technology companies (the usual suspects) \u2014 and their entrance onto the scene is not welcome.\n\nOne reason is the damage that too much disruption could do to the global economy, which is essentially an intellectual construct that only works because senior policy makers are focused on carefully managing systemic confidence.\n\nThis means that digital disruption is OK, provided it remains a microeconomic issue.\n\nAnd so far, this is the case: in spite of their gargantuan size, the world\u2019s most powerful technology companies have so far been operating at the microeconomic level \u2014 by disrupting isolated industries \u2014 like music, news, DVD, advertising and so on.\n\nUp ahead, we can easily see more waves of disruption coming \u2014 the global auto industry, healthcare and finance, for example are all in line. But we\u2019re still talking about microeconomic impacts.\n\nWell, sort of: let\u2019s think for a minute about how this could play out\u2026\n\nIt seems safe to say that some industries (like recorded music) will prove easier to disrupt than others (like banking):\n\nFor instance, taking on a handful of sleepy record labels like EMI, SonyBMG, Warner Music and Universal Music was one thing, but taking on the likes of JPMorgan, GoldmanSachs, plus the 100s of banks who together manage over $200 trillion of assets and have extremely strong political affiliations is another matter entirely.\n\nNevertheless, it seems safe to assume that the vast majority of industrial sectors will experience the effects of digital disruption at some point in the coming few decades, at least to some extent.\n\nThe only industries to escape the clutches of digital technology will be those where the products and services being delivered cannot be improved by the accumulation and analysis of large-scale data assets, and there are very of those.\n\nIf we stand back from this and try to objectively appraise what is happening we are left with at stark conclusion:\n\nA handful of technology companies will disrupt the whole macroeconomy\n\nHere is another way of coming to this conclusion:\n\nIf we take a long-run view of the market \u2014 say 30 years \u2014 then it is plain that the commoditisation of certain aspects of human intelligence will not only further accelerate workplace automation (by dramatically increasing the capabilities of robots) but will enable entirely new categories of digital services.\n\nFor instance, the recent emergence of bots, the delivery of powerful machine learning algorithms as cloud services (e.g. Microsoft Azure Machine Learning, Google Tensor Flow) and the efforts of IBM to create and commercialise multiple, parallel instances of Watson \u2014 each optimised for a given domain of expertise \u2014 will enable services, business and whole industries that we cannot imagine today.\n\nThe debate so far has mostly focused on about how these technological trends will impact the job market, with the prevailing consensus being that about 30% of the G20 workforce will be disrupted.\n\nBut the more fundamental question is what proportion of economic growth in the future will be enabled by these new technologies?\n\nIn other words, will these new technologies become the engine that drives the economy, without which further progress will become impossible?\n\nTo get a grip on this deeper question we should first acknowledge the limitations of using GDP as a way of measuring the economy \u2014 which is a subject I\u2019ve looked at before.\n\nAt the risk of over simplifying a complex question, GDP \u2014 as currently used by the macroeconomists, politicians and banks who manage the economy on our behalf \u2014 cannot be used to properly measure the current size of the digital economy, let alone how important it will become in the future.\n\nIn tacit acknowledgement of this problem, Christine Lagarde, head of the IMF, said in January 2016 at Davos \u201cWe have to go back to GDP, the calculation of productivity, the value of things \u2014 in order to assess, and probably change, the way we look at the economy.\u201d\n\nThe uncomfortable fact is that in order to properly understand how digital disruption will impact the economy then we need to stop using GDP.\n\nIf we think instead in terms of intrinsic value delivery \u2014 for instance the total number of useful hours spent reading Wikipedia, or listening to music services like Spotify \u2014 then the digital economy might already be delivering as much as 20% of total economic value \u2014 rather than just 5%, which is the consensus view today.\n\nThe key point is that the digital economy is already a far more important component of the global macroeconomy that implied by using a GDP-based calculation.\n\nBased on the fact that the actors controlling the technological assets that enable the digital economy \u2014 Apple, Google, Microsoft, plus others along with Silicon Valley \u2014 are not approaching a fundamental plateau then we should ask if digital economy represents 20% of total economic value delivery today then what might it represent in the future \u2014 30%? 50%? 70%?\n\nIt seems that we can again conclude that:\n\nA handful of technology companies will disrupt the whole macroeconomy\n\nFor those who remain unconvinced, here is a third way of coming to this conclusion:\n\nIn its purest form economic growth arises when people try to solve real problems.\n\nEconomic growth \u2014 which should correlate 100% with value delivery (but does not because the economy is not perfectly efficient) \u2014 is a convolution of the difficulty of the problem being solved (e.g. cancer) and the number of people affected by that problem (everybody).\n\nFor most of the past decades economic activity has been mostly focused on solving problems that required the manufacture of tangible products (e.g. sanitation infrastructure, fresh water supplies, washing machines, electric lighting systems, telephony networks, automobiles, clear glass etc.). These are examples of major innovations that each enabled dramatic improvements in the standard of living of billions of people.\n\nBut once the standard of living reaches a certain level \u2014 which is the case for many in the G20 economies \u2014 further gains come more from fixing problems that require intangible solutions (e.g. online retail, genomics, online education etc.).\n\nAs the economy moves farther ahead the problems we\u2019ll need to solve will become progressively harder and increasingly virtual in nature \u2014 and this will require more advanced enabling technologies, more advanced science, more powerful computation and vastly more data.\n\nIn the far future, the collective intelligence needed to solve the problems of the time will outstrip the capacity of human intellect, which is why we will need to use artificially intelligent systems to magnify and multiply human intelligence.\n\nIn other words, the economy is moving from a phase where human intelligence solved easy problems with physical products to a phase where machine-enhanced human intelligence will solve hard problems with virtual solutions.\n\nNot only is this is a deeply profound shift but the technological elements that will drive it will be controlled by a handful of companies, which leads us once again to the conclusion that:\n\nA handful of technology companies will disrupt the whole macroeconomy\n\nIf current trends continue then this would indeed be the conclusion, but the following evidence pattern suggests this it is unlikely:\n\nNo matter how you look at this it is hard to escape the conclusion that we are watching a car crash in slow motion.\n\nWhile it seems impossible to imagine today it is at least possible that one policy response to these growing tensions will be for policy makers to define regulations that strike at the strategic hearts of these gargantuan players, which is the \u2018platform lock-in\u2019 model:\n\nThe more engaged a user becomes with a platform like Facebook, Google or Apple the harder it is to switch: the sheer hassle and complexity of having to transfer emails, pictures, videos, address books, calendars and so on to a rival platform becomes so enormous that the vast majority of users simply can\u2019t be bothered \u2014 even if a rival platform offered better features.\n\nSome would argue that this is bad for competition.\n\nThis is similar problem to a problem faced by telecoms regulators when they were worried about how competition was developing within the mobile phone industry.\n\nIn this case, the requirement for users to change their numbers when they moved between operators proved to be such a severe barrier that most users preferred to stay with their current operators, even if their services were inferior \u2014 which is why media and telecoms regulators decided to introduce number portability.\n\nToday, when we want to change mobile networks, we simply assume that we will be able to take our phone number with us to our new network.\n\nIn the case of the digital economy, if it is determined that the data a user uploads onto a given platform is their property (copyright infringing material aside) then it could be that, one day, regulators will require that digital platform operators \u2014 like Facebook \u2014 allow users to easily transfer their data to a rival platform.\n\nSuch a \u2019data portability\u2019 regime would be far more complex to implement than was the case with number portability in telecoms industry but would not be impossibly so (for companies that are selling artificial intelligence as a cloud service, that is).\n\nData portability is one example of a policy tool could at a stroke result in a flood of competition into the digital platform markets, and reduce the market power of the tech giants.\n\nThe affected parties would surely detest the idea and actively lobby against it but users would be unlikely to object, and nor would the entrepreneurs and venture capitalists who would like to complete with the tech giants, but cannot even get a toehold in the market.\n\nFor sure, the current trajectory of the tech giants is not sustainable without a major rupture in the market landscape. Data portability might be the tool that causes that rupture."
    },
    {
        "url": "https://medium.com/emergent-future/two-more-science-fiction-technologies-became-real-this-week-eec09fd14a73",
        "title": "Two more science fiction technologies became real this week",
        "text": "Sometimes the twenty-first century feels like science fiction. Let me share two stories from this week that show previously speculative technologies entering everyday life.\n\nFirst, the United States Department of Transportation issued regulatory guidelines for autonomous cars. That means the US government is supporting the deployment of self-driving vehicles. The White House described it thusly:\n\nThe president even published an op-ed about this in a Pittsburgh newspaper.\n\nAs one article put it, \u201cThe DOT is not neutral toward AVs. It wants to get them on the road soon. That\u2019s a big deal.\u201d Same article summarizes a chunk of the regulations with this infographic, showing the extensive and practical details realized by the new document:\n\nRegulations aren\u2019t very sexy, and that\u2019s precisely the point. Once a technology has entered the deeply nerdy world of overlapping governmental regulations, you can take that as a sign the thing has become very real indeed.\n\nSecond, in hunting the New York/New Jersey bomber(s), the New York City police department sent out an alert\u2026 to nearly everyone in the city with a smartphone, at the same time. A Facebook friend describes being on a subway when the message arrived, and everyone in the car\u2019s phones going off simultaneously, emitting the same tone.\n\nWhile this sounds like something from cyberpunk fiction (I\u2019m reminded of that great creepy pay phone scene in Neuromancer (1984)), it might already be out of date, like some of cyberpunk fiction. 538 points out that the alert lacks an image, and hence could lead to witch hunting. It was also a broadcast without the ability to track readership, and lacked both non-English versions and identifying marks.\n\nSelf-driving cars and crowdsourcing surveillance through nearly ubiquitous handheld computers \u2014 some days the twenty-first century actually feels like the future."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-notes-from-the-frontier-chat-bots-augmented-reality-autonomous-cars-cacdbbe4cf85",
        "title": "Emergent // Future Weekly: Notes From The Frontier, Chat Bots, Augmented Reality, Autonomous Cars\u2026",
        "text": "You Might Have Heard: The chat bot revolution is upon us, and has the potential to save us time, hassle, and tedium by automating mundane tasks.\n\nThe sea change is driven primarily by three shifts in consumer behavior:\n\ntl;dr many apps are better off as services, natively integrated into messaging tools, because conversation is fluid and more natural.\n\nIt makes sense to be texting with a friend about meeting for drinks, and immediately summoning an Uber within the context of that conversation, rather than hunting around for the app.\n\nBut Did You Know: The chat bot boom is driven in part by a resurgence in artificial intelligence?\n\nComputing is cheaper than ever and phone sensors are collecting more data from more places, allowing for insights to be extracted using machine learning algorithms.\n\nThat\u2019s why we\u2019re building a simple, scalable, machine intelligence API for creating smarter products with your data.\n\nOr, as WIRED says: Need Some AI? Yeah, There\u2019s a Marketplace for That.\n\nPLUS: Nvidia debuts new AI-focused processors for data centers; says the new chips have 4x speed, 3x efficiency of previous models."
    },
    {
        "url": "https://medium.com/emergent-future/the-real-threat-of-artificial-intelligence-d9ff53a05452",
        "title": "The Real Threat of Artificial Intelligence \u2013 Emergent // Future \u2013",
        "text": "I know what your thinking\u2026 This is going to be another one of those \u201cSkynet is going to become self aware and kill us all\u201d posts\u2026 Believe me, I\u2019ve read my fair share and do so at your own risk, trust me. I\u2019ve heard it all\u2026 Elon Musk, Bill Gates, Stephen Hawking, Nick Bostrom, and more\u2026 This huge fear that somehow we\u2019re going to make a computer that is not just an AI, but it has the ability to recursively make itself smarter, therefore jumping so far ahead in intelligence that it makes us look like ants to a human being\u2026 A computer that can solve all problems.\n\nSeriously though, this is the newest cool thing to be worried about on the block if you\u2019re a billionaire, philosopher, or theoretical physicist apparently. I\u2019ve been on a quest to try and get to the bottom of how realistic these horror stories are, especially having lived through Y2K unscathed, and having read the story of The Tower of Babel, where our ancestors thought they would reach heaven by climbing to the top of a tower.\n\nIn any event, I did make a few interesting discoveries thus far: The people creating all of the fear and hype are not AI experts. In fact, people who actually work with AI on a regular basis not only don\u2019t believe that there is going to be a SkyNet any time soon (or ever), their projections for when we may reach this so-called \u201cartificial general intelligence\u201d tends to be farther out than the journalists and paranoid non-experts act like. I mean, let\u2019s face it\u2026 Elon Musk and Stephen Hawking are extremely smart, successful people, but they are not artificial intelligence computer scientists by any means.\n\nPlus, I\u2019ve sorta read a broken explanation of how this artificial superintelligence threat would arise. I\u2019ve read articles which say that a computer could overnight recursively become smarter so that the next day it is 12,000x more intelligent than the smartest human, however, no explanation for how the computer could act on this, and no explanation for how the computer would go through the physical trial-and-error process of learning and improving: because we all know that there is a trial and error process involved.\n\nAll around the internet, there is broken information scattered everywhere. But one thing is for certain: Nobody really seems to know exactly how it will pan out. So the most logical thing would be to look at history: Us humans have a tendency to see an unknown coming and jump to ridiculous fearful conclusions. Remember Y2K? And let us not forget that there are enough nuclear weapons stockpiled in the US and Russia to wipe us all off the face of the planet right now, no AI needed.\n\nHowever, I do see a much more likely threat coming from AI. A simple Google Search shows the steady rise of global population, right? We\u2019re all healthy and having children more and more\u2026 And now we\u2019re trying to create artificial intelligence to handle work for us.\n\nI\u2019m no rocket scientist, but does anyone see a problem here? Not to mention, a decent amount of us struggle to take care of our own humanly concerns, let alone sending a bunch of non-humans out into the workforce. Definitely a more likely concern than the robot apocalypse\u2026 At least for this century. I mean, should we really be spending billions of dollars on getting machines to do work for us when so many of us are struggling to keep work as it is?\n\nOf course there are a zillion and one arguments of how the AI will save us all and we will have effectively created God (not holding my breath) from a whole lot of people who have no AI experience (go figure), but I leave you with food for thought\u2026"
    },
    {
        "url": "https://medium.com/emergent-future/robots-ai-and-the-future-of-your-job-inspired-at-singularity-university-nasa-dccd0dff844c",
        "title": "Robots, AI and the future of your job \u2014 inspired at Singularity University, NASA",
        "text": "My time at Singularity University on this Summer\u2019s global solutions program has taught me many things but one thing is for sure, sci-fi and reality are getting closer faster than we think.\n\nTake Matternet (a Singularity University company), who have just signed a commercial agreement with Mercedes to provide autonomous flying drones that will deliver parcels to your door from a delivery van near you.\n\nTake Otto, the driverless truck software company, who, within six months of launching have been acquired by Uber for $680 million dollars. They enable truckers to keep trucking even when the driver is resting in the back.\n\nOr recently launched, Amazon Echo, the voice activated home controller hoping to transform in-home automation. I recently bought one to test it out and whilst still glitchy, voice controlling my Spotify, Nest and 5 minute meditation sure is handy.\n\nTo the technology fan and entrepreneur in me, these examples all highlight massive progress. Technology can solve real problems, provide value, better prices for consumers and without a doubt enables positive impact across the board in fields ranging from energy to food to health.\n\nWith all these gains there have to be some costs. One for sure is our jobs. Based on international statistics from the World Bank, 57% of jobs are at risk because of automation enabling technologies such as AI and Robotics. Also, the World Economic Forum Future of Jobs report predicts 5 million jobs to be gone by 2020 based on surveying 65% of the world\u2019s workforce. Unsurprisingly, the technology sector that creates these products only hires a small fraction. According to Comptia in the US, it\u2019s 5.7% .\n\nWith this kind of change on the horizon, I found this topic so important and set about talking to experts and researching the field.\n\nSo what does the future look like for us humans and our place in the workforce? Here are three areas I think are particularly crucial and interesting.\n\nUp-skilling becomes more important than ever\n\nBuilding the Robotics and AI of the future will take time and clearly human involvement. That said, skills to be successful in the workplace and the jobs of the workplace are already changing fast.\n\nAccording to the World Economic Forum\u2019s top 10 skills at work at 2020, creativity is set to rise in importance as is emotional intelligence. Clearly job complexity is on the up.\n\nAlso, the job families of administration and manufacturing are set for massive decline in the next 5 years. These are the same job families that Robots and AI are set to be deployed to first.\n\nCould technology be utilised to help? We certainly hope so. That\u2019s the goal of my classmates\u2019 recent startup Udexter. They are launching an AI system that can help anyone re-skill and re-job.\n\nSpeaking to two of the speakers at Singularity University, Vivek Wahdwa, a professor at Carnegie Mellon, and Dambisa Moyo, a leading global economist and author, both were adamant that the solution has to come from government policy. In my opinion, this makes real sense as I struggle to see a future with enough jobs for everyone.\n\nSo what\u2019s being tested today at a policy level? The Netherlands and Finland have pilot projects to test universal basic income in the pipeline. Finland will test providing up to 10,000 people with an income of \u20ac500 to \u20ac700 per month which is below the average income of \u20ac2700 per month. Their experiment is set to measure if the \u2018no strings attached\u2019 income leads to greater employment given that unemployment is at a 15 year high. US isn\u2019t ready for universal basic income testing but the technology startup incubator YCombinator has set about experimenting with basic income in their Californian neighbourhood to see what positive impact it makes.\n\nSweden has just moved to a 6 hr work day. Their research has shown that you can be as productive in 6 hours as in 8 hrs and it\u2019s being implemented by different national employers. Working less hours can be part of the solution to technological unemployment where logically we just don\u2019t need to work as much if automation is able to do it for us.\n\nThis may be the utopian view but if people have enough money to meet their minimum needs or have greater free time, they can then choose to create a life they wish to lead. Some may choose to be with family or support their community. Others may choose the arts or creativity through the rising maker movement. The entrepreneurs will set-about looking to solving problems they see in the world. To me this would be a fantastic outcome - imagine millions more people from different backgrounds and experiences working together to solve problems that matter to them.\n\nThe futurist Glen Hiemstra, predicts that in 2050, we will let machines be smart for us while we will focus our energy on being creative and using our empathetic skills to help others. He suggests that we will see a rise in artists, entertainers and teachers.\n\nWith a four times increase in freelancing predicted by 2020 in the US, work as we know it is already changing. Whatever people choose to do, if it leads to a greater sense of personal purpose, it no doubt leads to greater happiness. This has to be good for society at large and likely reduce demands on national health and social services.\n\nWhat technology can do for us is amazing. However, every gain has a cost. In this case, it\u2019s our jobs. No doubt a complex issue, but one for which we need solutions \u2026for everyone\u2019s sake.\n\nGet involved. What are your thoughts, questions or comments. Get in touch below or at @srisharma !"
    },
    {
        "url": "https://medium.com/emergent-future/the-connective-intelligence-of-life-41f4b31a3ef",
        "title": "The Connective Intelligence of Life \u2013 Emergent // Future \u2013",
        "text": "There\u2019s been a lot of talk in recent years about \u201ccollective\u201d intelligence \u2014 how networks of people can swarm together in social movements to make sense of the world and act upon it appropriately. I want to suggest that it might just be better to describe phenomena like this as connective intelligence instead.\n\nAll living things are enmeshed in dynamic processes of feedback, regeneration and decay. They continually exchange chemicals and energy with their environments. Try to find a boundary anywhere in the system and you\u2019ll quickly realize that no actual boundaries exist, everything is interconnected.\n\nHow can this insight be applied to the convergence of global crises humanity is now confronting? First off, we can name the confusion around \u201cissues\u201d that claim global warming to be separate from violence against women; wealth inequality separate from extreme opulence; police brutality separate from systems of marginalization and oppression; and so on. All of these things are connected.\n\nYet our minds are conditioned to treat them as separate entities.\n\nThis is extremely important. We need to realize that the illusion of separation is what keeps us from behaving collectively in an intelligent manner. When we act upon the world as if it were a collection of individualized entities \u2014 rational \u201cactors\u201d in markets, rogue \u201cagents\u201d in moments of violence; singular \u201cviruses\u201d during a health epidemic; and so forth \u2014 it is our mental constructs that keep us from seeing the systemic point-of-view that makes root causes visible to us.\n\nThus we do not act with intelligence because our conceptions about the world presume disconnection, while in reality it is the interconnections that matter.\n\nA powerful way to learn how connective intelligence works is to think about a multicellular organism like the human body. The body is made up of trillions of cells. According to this estimate, 37.2 trillion\u2026 that\u2019s a LOT of cells! From moment to moment, this vast array of tiny entities is engaged in a cosmic dance of interactions and exchanges. To be alive is to be a dynamic pattern of continual exchange within and throughout the environment. Oxygen must be delivered simultaneously to cells in your brain, your liver and kidneys, and the skin at the end of your toes.\n\nAn amazing feature of all living things is that they are capable of sense-making and taking purposeful actions to continue their existence. This is true for single-celled creatures like paramecium who can detect chemical gradients and follow them to food sources. It is equally true for the human body that uses its central nervous system to create \u201cneural maps\u201d of its holistic condition to track things like blood pressure, levels of different hormones, tension in muscles, and information gathered through sensory pathways like vision, touch, and hearing.\n\nThe brain acts as a synthesizer to help the body coordinate itself so that it remains within safe operating parameters. Keep the brain cooled enough that it doesn\u2019t damage its tissue. Hold the acidity within a range that keeps the stomach from digesting itself. Manage pressure in the arteries so that the heart does not get torn or weakened. This gestalt process is called homeostasis and it is a fundamental governance feature of all living things.\n\nThe intelligence of homeostasis cannot be found in the brain alone. It is only knowable by looking at all the connections throughout the body. Circulatory systems carry oxygen and trace hormones to every organ, every limb. Lymphatic systems monitor hormone levels and regulate glands to link up circulation with cognition. Digestive systems track nutrient levels and signal the glands to alter circulation and impact the overall behaviors of the body so that it seeks food or quenches thirst.\n\nThe intelligence of life is only possible because everything is interconnected.\n\nThe connectedness is what gives rise to the emergence of intelligent behavior. Apply this to chronic problems in the world and you\u2019ll see how the linkages existing between media and politics, for example, are not adequate to achieve homeostasis at the societal level. Or that the financial flows designed to extract and hoard wealth for the few at the expense of the many will not detect the threats of inequality that can unravel entire market economies.\n\nIt is this depth of inspiration from nature that reveals how human societies truly are like organisms. They operate according to Darwinian principles of evolution \u2014 where a variety of possible forms arise and those best adapted to their environments are able to out-produce and survive. Thus we see how societies can actively generate sickness and pathologies because the rules-of-play for competition between them \u201cselect\u201d for things like militarism, over-production of elites, and depletion of economic services for large portions of a citizenry.\n\nIt is in moments like these that economic practices become parasitic and behave like cancer. They serve themselves at the expense of the whole and threaten the survival of the entire society. Humanity is living through a planetary-scale version of this right now. According to the best science available, we have passed four of the nine planetary boundaries for homeostasis of our civilization. We are literally in the \u201covershoot-and-collapse\u201d stage of development.\n\nYet, unlike in times past, we now have a great deal of knowledge about how living systems work. The problem is that this knowledge is not adequately integrated and connected. This task \u2014 to culminate what we collectively know into connected intelligence \u2014 is what will enable us to transition to planetary thriving.\n\nFailure to do so will lead ultimately to collapse. The choice is collectively ours to make. Will we connect and thrive? Or remain divided and die?"
    },
    {
        "url": "https://medium.com/emergent-future/where-to-apply-machine-learning-1f5d10512055",
        "title": "Where to Apply Machine Learning \u2013 Emergent // Future \u2013",
        "text": "Machine Learning is being used to solve many problems, which problems can you use it for?\n\nIn the last 5 years there has been growing success using machine learning. Rapidly increasing processor speed and access to large scale data sets are allowing for many new problems to utilize machine learning successfully. Today machine learning is being applied by innovative companies in almost every field.\n\nUsing machine learning to solve problems is becoming central to many companies core points of differentiation. At every, step in the development of machine learning, there will be huge economic payoff for the companies involved.\n\nMachine Learning is the development of computer programs that use the information in datasets to decide outputs.\n\nTraditional computer programs explicitly define the steps to transform an input to an output. For example, when a sale is made for $2 and paid for with $5, give back $3 change. This is based on mathematical rules which can be executed without knowing anything about other transactions that have occurred.\n\nMachine learning is useful for problems where we want to decide the output based on data. The model that determines the output creates patterns based on variations in the data; it is then trained by the data that passes through and \u2018learning\u2019 occurs which creates outputs closer to the goal.\n\nThe type of problems that we want to use data to make decisions on (i.e. machine learning) are problems where rules of deciding the output are not clearly defined. These types of problems include human interaction and natural systems. These types of interactions are good candidates because there is always some degree of unknown and highly complex rules that will determine the outcome of a natural interaction.\n\nWhen thinking specifically about where machine learning can be applied it is often valuable to identify what the natural impactor in the problem is. For example, in advertising we want to serve the ad that is most likely to get clicked. However, a human will make the decision to click on the ad or not, which is the natural impactor. Natural impactors do not have a clear set of rules, which makes them a good candidate to learn from the data. In this case, we want to learn about how people have acted with the previous ads to make a determination about which ad to show next.\n\nThis thinking pattern can be generalized as the following:\n\nA few examples using this pattern:\n\nOnce we have framed our problem, understanding the goal and the decision we are trying to make using data, we can begin to explore the data/feedback that will be provided to the system. Machine learning problems are classified into three buckets depending on the nature of the learning \u201cfeedback\u201d available to the system. These are:\n\nOften times the most difficult part of effectively using machine learning is creating a suitable data set. A good data set will be standardized and large. Supervised problems require a training data set, which is a collection of data points that have a label defining the correct answer. This training data is used to \u2018train\u2019 the system for data where the answer is not defined. Examples of training datasets are the MINST and ImageNet public datasets.\n\nOnce we have our standardized data we can begin to train our model. There are many machine learning libraries that exist, the most popular lately is TensorFlow, which has been open sourced by Google. TensorFlow offers examples that span many common machine learning problem types. Many of these models can be reused to solve different but structurally similar problems. These libraries will allow you to quickly experiment and apply machine learning with near to state of the art algorithms."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-tesla-updates-autopilot-googles-talking-computers-how-neural-nets-d34f8ae59594",
        "title": "Tesla Updates Autopilot, Google\u2019s Talking Computers, How Neural Nets Work",
        "text": "You Might Have Heard: Tesla is updating Autopilot, the software that powers their self-driving car option, to use radar as the primary control sensor for navigation.\n\nCars will no longer need a camera to confirm visual image recognition during driving, and is geared toward preventing accidents, like the fatal Model S crash.\n\nThe new software will give the cars access to six times as many radar objects using the same hardware, which is available on all cars shipped after October 2014.\n\nTesla\u2019s taking advantage of the fleet of cars already on the road to dynamically learn about the positions and locations of road signs, bridges, and other stationary objects to better map roadways and hazards, and all but eliminate false-positives.\n\nThis real-time learning system is always running, whether or not Autopilot is on. Meaning, the more you drive, the more Tesla learns.\n\nBut Did You Know Tesla\u2019s Autopiliot has driven more than 47M miles in the past six-months? They\u2019re adding more than a million miles of driving data every day.\n\nConversely, Google\u2019s self-driving car, while taking a very different approach to autonomous vehicles, has travelled just 1.5M miles in SIX YEARS.\n\nThere\u2019s concern that Google\u2019s car project is losing out to rivals, like Tesla and Uber.\n\nMeanwhile, Apple has shuttered parts of its self-driving car project, laid off dozens of employees, and is rethinking its strategy.\n\nPLUS: Baidu and Nvidia to Build Artificial Intelligence Platform for Self-Driving Cars\n\nGoogle\u2019s DeepMind can now generate sound waves that mimic human voices \u2014 a 50% improvement over the existing text-to-speech technology.\n\nThey\u2019re calling it WaveNet, which uses neural nets to generate convincing speech and music.\n\nResearchers fed DeepMind basic recorded speech, and used a convolutional neural network to create a complex set of rules about how certain tones follow other tones in the context of speech.\n\nDeepMind famously beat the world\u2019s best Go player in March, and is also being used to more efficiently manage power usage in Google\u2019s data centers. They\u2019ve cut their electricity bill by 15%.\n\nRead the full announcement, and listen to examples.\n\nResearchers from Harvard and MIT say they\u2019ve discovered the secret to neural networks buried in the laws of physics, where a small subset of mathematical functions describes the way the universe operates.\n\nThis is great news, because nobody quite understands why deep neural networks are so good at solving complex problems.\n\nWith physics, structures are formed through a sequence of simple steps: particles form atoms, which form molecules, cells, organisms, planets, solar systems, galaxies, etc.\n\nNeural nets are arranged in layers, where each layer deals with a higher and higher level of abstraction and complexity.\n\nRead the research to learn more about why deep learning works so well."
    },
    {
        "url": "https://medium.com/emergent-future/self-driving-cars-are-going-to-change-everything-e6228c1f9d66",
        "title": "Self-driving cars are going to change everything \u2013 Emergent // Future \u2013",
        "text": "I\u2019m utterly fascinated with self-driving cars. They\u2019re in the headlines every day as Tesla, Uber and Google spar over technical advances. It\u2019s not only the tech that fascinates me though. I\u2019m enamored with how self-driving cars are going to change the world we live in.\n\nI\u2019m assuming you remember life before the iPhone and the shift it\u2019s brought with it in less than a decade. I argue that vehicles are the only other consumer device on par with smartphones both in market penetration and impact on day-to-day life. There are more passenger vehicles on the road in the US than there are smartphone users (260M vs 220M), and that number doesn\u2019t include commercial uses. The arrival of autonomous vehicles will fundamentally redefine our relationship not only with our cars, but also our cities, and each other. And it will cause shifts in quality of life in both directions as it arrives.\n\nThe state of the art today is Tesla\u2019s Autopilot. Yes, Google is driving around Mountain View, and Uber\u2019s first self-driving cars hit the road in Pittsburgh any day now, but Autopilot has logged 130M real-world miles. (In fairness, the efforts shouldn\u2019t be directly compared as they\u2019re currently working on different implementation levels of autonomous technology: Tesla\u2019s Autopilot is a level 2 implementation, Google is shooting to jump immediately to level 4.)\n\nThough certainly not perfected, Tesla will continue to improve Autopilot, both through better hardware on the road and through collecting exponentially more data when the Model 3 launches. Musk predicts fully autonomous cars will hit the road by 2018. He\u2019s known to be aggressive on timelines but not by much. It\u2019s reasonable to say that some cars on the road will be driving themselves by 2020. Depending on how quickly the roll out goes, children born in 2004 may not ever need a drivers license. A child born today certainly will not.\n\nThis opens the door to some very interesting questions that our society will face in the next 10\u201320 years. For example, what happens to car ownership? If the cars drive themselves, that means Uber doesn\u2019t need drivers to bring their own cars. Cars can be on the road nearly 24/7, dispatched by software, and run at a fraction of the cost because there\u2019s no human driver.\n\nThe benefit of owning a car is having instant access at any time. With autonomous vehicles, surge pricing and delays caused by capacity issues become a thing of the past. This gives end users effectively the same level of access as ownership would provide, without requiring the capital commitment. Why would anyone need to own a car in a world of instant and cheap transportation?\n\nOn one hand, it\u2019s easy to assume that incumbent car manufacturers will continue to rely on a direct sales of their cars, but GM\u2019s recent acquisition of Cruise should be looked through the lens of their $500M investment in Lyft as well. Mercedes, BMW, and Audi all currently operate car sharing services so have some proficiency with operating in a service model. And Ford\u2019s gone so far as to say their initial autonomous vehicles will be prohibitively expensive for consumers and will only make economic sense for a service providers. Surprisingly, it\u2019s Tesla that\u2019s most publicly asserted their intent to sell autonomous vehicles to consumers. Part Two of Tesla\u2019s Master Plan includes car sharing as an assumed use case, though expect that to work best with a Tesla-branded service.\n\nOf course there will be push back on giving up the idea of ownership completely. Car ownership is a deep-seated part of the American Dream and a lot of individual personality is tied to one\u2019s choice of car. That makes it a hard consumer behavior to break. But we\u2019re already seeing car ownership in decline today as the general population moves to cities with better transportation options. Autonomous vehicles will accelerate that trend.\n\nThere\u2019s also an increasingly strong argument to take human drivers off the road. 38,300 people were killed on U.S. roads in 2015, 1.25M globally. 95% of those were determined to be the fault of a human operator. It\u2019s the top cause of death among people aged 15 to 29 years.\n\nIt\u2019s easy to see the argument for human drivers being restricted from driving on certain autonomous-only roads or dedicated lanes early in the adoption cycle. From there it will be a slippery slope of restricting human drivers on all freeways (which will then run at higher density and speed), before being relegated to closed-track days altogether.\n\nWith the carrot of lower cost and the stick of restricted road access, it\u2019s hard to imagine a world where car ownership will be anything but a fraction of the level it is today.\n\nIn fact, a recent study predicted that if we gave up traditional car ownership for a Lyft Line/UberPool-style car sharing model, we could take 90% of the cars off the road without any reduction in transit time, while also reclaiming 20% more space inside cities currently occupied by parking and roads.\n\nThis means autonomous vehicles will create opportunities for urban planners to redesign the basic building blocks of cities over the next century. When all vehicles are self-driving, cities can reduce the footprint of roads and parking, and optimize infrastructure, particularly traffic signalling to improve traffic flow. Similarly, smart city tech companies will need to step up to enable cities to gather data and provide automation-friendly data to individual cars. The no-stop self-driving intersection is only a simulation today but it\u2019s a possibility in the future with the right urban planning. Even our homes will change in a world where we no longer need parking for our personal cars. Imagine when having a garage is as outdated as having a carriage house is today.\n\nStill, it\u2019s very hard to predict how our cities themselves will react to easy access to transportation. Does that enable more density in city cores or a resurgence in bedroom communities as the pain of longer distance commuting is eased? Either way, there\u2019s an opportunity for those in real estate to speculate on the changes automation will bring.\n\nOf all the changes that autonomous vehicles bring to our cities though, perhaps the most fascinating is the impacts on mass transit. In a world where large shared self-driving fleets reduce traffic levels and drop the cost of getting from A to B, what does mass transit look like?\n\nIn the last decade, there\u2019s been a revitalization of public investment in rail-based mass transit in the US. But if general availability of autonomous vehicles is five years away, it\u2019s unclear whether decade-long rail infrastructure investments actually make sense.\n\nFor example, today\u2019s choice of trains over expanded bus service is usually one of throughput: the amount of people that can be moved between two points in a fixed period of time. Within cities today, trains on dedicated right-of-way can\u2019t be beat. But buses on dedicated right-of-way (called Bus Rapid Transit) have many of the benefits that rail and signalling provide trains today, and there\u2019s no reason the dedicated busways couldn\u2019t be underground either.\n\nWhen autonomous is added to the mix, buses should be able to run at higher speed in closer proximity with larger vehicles to boot, making them very comparable to trains while still maintaining the flexibility to run in mixed traffic if needed. At the very least, this is a compelling reason for current rail plans to support mixed-mode right-of-way that allows both rail and bus at the same platform, as Seattle\u2019s downtown stations do today.\n\nAutonomous buses may not need to run fixed routes either. A fleet of smaller buses could operate in a Pool/Line-style mode and provide more convenience on lower traffic routes. The main blocker here isn\u2019t so much the buses but adoption of smartphones by the general population to signal for a ride. Regardless, those smaller buses start to look a lot like minivans running in an Uber Pool. In fact, Lyft has already done a partnership with the MTC to support carpooling.\n\nThis begs the question of whether mass transit infrastructure should be owned by the public at all. Taxed-subsidized transportation options are almost certainly still required for public good. It\u2019s unclear whether that\u2019s still best provided by government-owned fleets or whether the complexity of operating an autonomous fleet and dispatch service is better provided by partnering with the private sector.\n\nIt\u2019s worth pointing out as well that there\u2019s a dystopian monopoly scenario here. It\u2019s easy to assume that the cost savings get passed onto the consumer and healthy competition between options remains. On the other hand, one can imagine a world where one car sharing service wins and creates a monopoly on transportation. Though it\u2019s an unlikely outcome, it\u2019s a reason for local governments to invest in public options and ensure that multiple players have a chance to compete.\n\nStill, it\u2019s hard to understate this: all of humanity will benefit from the arrival of autonomous vehicles. All forms of transport, whether that\u2019s driving, biking, or walking, become dramatically safer without human drivers at the wheel. And because they\u2019ll be less likely to be involved in crashes, cars can be built lighter and more efficient, requiring less energy to operate. Combine that with a drop in the number of vehicles on the road and we\u2019ll be creating a meaningful dent in humanity\u2019s energy consumption while improving life expectancy at the same time.\n\nAutonomous vehicles are also going to be particularly impactful for those with limited mobility today. Elderly and disabled will have personalized transportation options, giving them access to life-changing freedom and potentially life-saving as well. In addition, the sensing technology could scale down apply to scooters and other personal mobility devices to help owners simply navigate their homes. Similarly, it will make safe and trustworthy mobility available to parents of young children enabling similar freedom to both.\n\nNot everyone will benefit equally and it\u2019s similarly hard to understate this: Any person who makes their living as a driver or supporting drivers will be impacted by the arrival of vehicles that no longer need human drivers to operate.\n\nIn the coming decades, 2.5M truck drivers, the most common job title in the US, will disappear. Human taxi drivers will disappear as well, though they may take some bitter solace in seeing all Lyft/Uber drivers going as well. Bus drivers and all mass transit drivers will follow. Delivery and garbage will remain in vehicles for a longer period of time, though primarily as passengers until we figure out how to interface with buildings automatically.\n\nBut the impact is not limited to drivers. Anyone who makes their business serving drivers (think roadside diners and motels) will be affected by automated trucks cruising by in the night. The car rental industry will disappear except potentially to provide specialized equipment like UHauls through Uber-like dispatch. And of course, the entire auto insurance industry collapses when individuals stop owning cars and vehicles are involved with a fraction of the accidents.\n\nThere will be a lot of responsibility placed on the shoulders of government to step in with social programs supporting those affected by autonomous vehicles. Basic Income is the silver bullet du jour but the transition will require a portfolio of income and tax relief as well as retraining. It will be rough but it can\u2019t be avoided.\n\nAt the same time, it shouldn\u2019t surprise that a change as drastic as this will create opportunities for many new types of businesses as well as modernized versions of existing ones.\n\nThat\u2019s a lot of opportunity and certainly not an exhaustive list. Many of these topics deserve deeper exploration on their own and many of the questions will become clear in the months, years, and decades ahead as companies place their bets and communities react. The technology is already moving incredibly fast (just keeping up with day-to-day autonomous news triggered several rewrites and additions to this post).\n\nStill, I hope you\u2019ve got a deeper sense of the scale autonomous vehicles\u2019 potential impact. Our relationships with cars will change as we abandon ownership, our relationships with our cities will change as they no longer have to accommodate idle vehicles, and our relationships with each other will change as we take advantage of new levels of freedom and the opportunities they unlock. In my mind, autonomous vehicles represent one of the most interesting and impactful new platforms, both for business and society, in our lifetime. I couldn\u2019t be more excited to be along for the ride!\n\nThanks to Boris Jabes and Josh Jenkins for the editorial assist."
    },
    {
        "url": "https://medium.com/emergent-future/predicting-the-future-the-steamrollers-and-machine-learning-7c6cfca8e9be",
        "title": "Predicting the Future: The Steamrollers and Machine Learning",
        "text": "Four years ago, I wrote that when one wants to predict the future, it is always a good thing to rely on the steamrollers, i.e. the exponential trends on which we can surf to predict the future because they are highly efficient. Let see how this trend continues.\n\nSince 2012, genome sequencing reached a second inflection point but I still expect a third inflection point due to the use of the nanopore sequencing technology as we had mentioned then. So we are good.\n\nFor Moore\u2019s law it is a little more complicated. While the number of transistors do increase (but probably not as fast a Moore\u2019s law anymore. In this most recent graph, the last five year show a slight inflection point) the performance is beginning to stall. Dennard scaling stopped around 2005.\n\nand while the number of transistors increased, we are beginning to see a decrease in the use of that silicon. This last trend has a name: Dark Silicon. Bill Dally, whom we had invited at the now-not-gonna-happen NIPS workshop said the following three years ago:\n\nSince Machine Learning has become the main driver in applications, it has become a target of interest for the silicon industry. In a way, one of the lessons of that story is for architecture of Machine Learning algorithms to be a better fit for silicon architectures. No wonder we recently saw the rise of Google\u2019s TPU, the numerous NVIDIA platforms like the DGX 1, the recent acquisition of silicon-machine learning companies like Nervana and Movidius, the effort of Baidu in the GPU clusters realm\u2026\n\nLet me finish this short history of the times we live in by pointing out that while Silicon will become more efficient because Machine Learning algorithms will be used as a constraint, making sense of the world in the future will still require new technologies. From the recently released report produced by the Semiconductor Industry Association and the Semiconductor Research Corporation entitled \u201cRebooting the IT Revolution: A Call to Action\u201d, one can read that given the data explosion we are predicting, computing in 2040 may become problematic. From Appendix A Data Explosion Facts , A4. Total Effective Capacity to Compute Information in 2040:\n\nAnd according to the authors of \u201cMoore\u2019s law: the future of Si microelectronics\u201d\n\nWe need to find less than radically new devices and better algorithms to make sense of this upcoming Miller\u2019s wave in 2025. It\u2019s time to get cranking."
    },
    {
        "url": "https://medium.com/emergent-future/artificial-intelligence-fintech-big-data-or-what-happens-when-technology-is-faster-than-the-6c2c1528738c",
        "title": "Artificial Intelligence, FinTech, Big Data\u2026Or, What Happens When Technology is Faster than the Law?",
        "text": "A fact-based approach to regulation may have worked relatively well in the past when innovation cycles were longer and the pace of disruptive innovation occurred over decades. Regulators had time necessary to get their facts in order before making a regulatory intervention.\n\nIn today\u2019s world, however, the incessant speed of technological change means that this kind of approach faces insurmountable challenges. The pressure of time means that the facts surrounding a piece of new technology or other innovation may not be there, or the regulators may simply select the \u201cwrong\u201d \u2014 or at least contested or otherwise irrelevant \u2014 facts as the basis of regulation. The lack of time means that establishing facts or negotiating with entrenched interests becomes much more difficult.\n\nTake AirBnB, for example. Regulators in some countries have become concerned that individuals looking to get rich from renting out properties via AirBnB are buying housing in desirable urban residential areas, thus distorting property prices and \u2014 potentially \u2014 creating housing shortages in such areas.\n\nThe solution? A rule that requires those renting accommodation via AirBnB to be actually living in the property when it is being used.\n\nOf course, the selection of the \u201crelevant facts\u201d in this case and the resulting rule benefits certain vested interests, most obviously the hotel industry who stand to lose out from the new competition from AirBnB.\n\nBut are the selected facts in this case relevant or even correct? A possible effect of a rule requiring residency of rented accommodation is that it may limit Airbnb in certain markets, so it is clearly important to get this right.\n\nAre the people intending to offer Airbnb accommodation really only in it for the money? In many cases, renting accommodation may be about connecting with people from other cultures or offering a welcoming experience for tourists visiting a new city.\n\nThe \u201cfactual\u201d premise or basis of the regulation \u2014 i.e., individuals looking to make easy money from residential properties \u2014 may simply be incorrect. The selected facts may not even be facts, or at least, the most relevant facts about a particular innovation.\n\nAs a second example, consider Uber or similar \u201ctaxi-like\u201d car sharing services. There is no doubt that services like Uber are disrupting the taxi industry. The effect is that regulatory debates around Uber are currently dominated by an unfair competition argument:\n\n\u201cTrustworthy and reliable taxi companies are facing unfair competition from Uber and this kind of unlicensed activity poses enormous risks for consumers.\u201d\n\n\u201cUS startup companies, in particular, don\u2019t respect the legal order that protects the European labor market.\u201d\n\nThese \u201cfacts\u201d are then used to justify regulatory intervention that effectively attempts to kill Uber in certain markets.\n\nBut, as with the AirBnB case, are these facts really facts. Or, at least, are they the most relevant facts? Do taxis really offer a better service than Uber? Is Uber any less safe than a licensed taxi?\n\nMost consumers just want a quick, clean and respectful service, but \u2014 all too often \u2013incumbent taxi companies offer a disrespectful or unreliable driver and a dirty cab. There is a disconnect between the facts that regulators identify as important and the experience and wishes of most consumers.\n\nIn an age of constant, complex and disruptive technological innovation, knowing what, when, and how to structure regulatory interventions has become much more difficult. Regulators can find themselves in a situation where they believe they must opt for either reckless action (regulation without sufficient facts) or paralysis (doing nothing). Inevitably in such a case, caution tends to trump risk. The precautionary principle becomes the default position. But such caution merely functions to reinforce the status quo and the result is that new technologies struggle to reach the market in a timely or efficient manner.\n\nLawmaking and regulatory design needs to become more proactive, dynamic and responsive. So how can regulators actually achieve these goals? What can they do to promote innovation and offer better opportunities to people wanting to build a new business around a disruptive technology or simply enjoy the benefits of a disruptive new technology as a consumer?\n\nHere are three principles that we believe can form the basis of regulation tomorrow.\n\nA reliance on different sources of data surrounding new technologies can provide some signals or clues about what, when and, to a certain extent, how to regulate.\n\nOf particular importance in this context is data relating to investment in new technology and innovation. Such data can be used as an index or proxy of the necessity of regulation.\n\nCollecting and collating such data may appear to be a tedious task for policy makers, regulators, lawmakers and alike. However, since government funding is often considered to be the main driver behind disruptive innovations, a plethora of investment data is readily available to make accurate predictions regarding what the next \u201cbig thing\u201d is likely to be.\n\nMoreover, the fact that it is start-up companies that are the ones that usually challenge existing rules, laws and regulations, means that private data sources are widely available. The proliferation of the better hand-collected global databases on the market, such as CB Insights, PitchBook and Mattermark, can make an important contribution to a \u201cdata-driven\u201d regulatory approach.\n\nFigure 1 gives an indication of what such a data-set might look like. The Figure shows the global venture capital investment deals (149,057) per industry tracked by data-provider PitchBook from 2005 to the first half of 2016. We could go much more granular than this, but even a broad perspective clearly indicates that certain areas, such as Fintech, Big Data, and the Internet-of-Things, are attracting more and more attention from investors.\n\nIn this way, investment data can help to develop a list of technologies and issues that need to be the focus of regulatory attention. From such data, we can get a better \u2014 and earlier \u2014 sense of which technologies are developing and which technologies need regulatory attention. This might then allow regulators to be more pro-active and avoid wasting resources on technologies that are unlikely to make it to market. It would also allow regulators to more accurately define the scope of a technology by focusing on the type of firm that is attracting attention.\n\nAs to the question of when to make a regulatory intervention, investment data can be similarly helpful. When early stage investments peak and later stage investments are taking off, it arguably shows demand both on the commercial and consumer side. Data on the timing of investment appears to provide a reliable indicator of the commercial maturity of a technology, in the sense that high levels of investor activity (at the seed, early and later-stage level) indicate that a particular technology is about to be ready for commercial exploitation. Figures 2 gives an example in the field of \u201cartificial intelligence.\u201d\n\nAs to the question of \u201chow to regulate\u201d, the starting point is that regulation needs to be \u201cdemand driven\u201d, i.e., the substantial direction of the regulation needs to be based on the interests of consumers (to a large extent reflected in the investment behavior of investors). If there is a genuine demand for certain products or technologies, then such technologies should, in principle, be permitted.\n\nThe focus on the demands of the consumers does not mean that policy makers, lawmakers and regulators should ignore the negative side effects or other risks of new technologies. What it does mean, however, is that entrenched interests with a clear interest in obstructing a disruptive product or service should not be allowed to dominate the debate.\n\nBig Data is an example of this (see Figure 3). Clearly, the principle of respecting privacy is important, but is not without exception and can, for example, be overridden if there are clear diagnostic benefits in allowing people\u2019s private health information to be used. In this context, regulatory experimentation strategies may offer a potential solution.\n\nRegulatory experimentation particularly affects how we regulate disruptive technologies. A key element of a more open approach involves a shift from rules to principles. Certainly, a principle-based approach facilitates a greater degree of openness and flexibility on the part of regulators and prevents innovative technologies (and the companies that have developed them) from becoming bogged down in the regulatory thicket that often results from a rules-focused approach.\n\nRe-framing regulation in this way and adopting a principle-based approach facilitates action, whilst also allowing future revisions in the regulatory regime to be based on the incorporation of new knowledge or subsequent discoveries.\n\nHowever, despite the clear benefits, companies often raise concerns about risks related to doing business in a principle-based environment. The argument is that it is usually impossible to operate in compliance with principles that could again change \u201cafter the fact\u201d. That is to say, a principle-based approach may facilitate policy makers, lawmakers and regulators in promulgating facts-based laws and rules through the backdoor. How then can we deal with this potential shortcoming of a principle-based strategy?\n\nThe Financial Conduct Authority (FCA), the financial regulatory body in the United Kingdom, may offer some clues as to an answer. In May 2016, the FCA broke new ground by launching a \u201cregulatory sandbox\u201d which allows both startup and established companies to roll out and test new ideas, products and business models in the area of Fintech (i.e., new technologies aimed at making financial services, ranging from online lending to digital currencies, more efficient).\n\nThe investment data suggests that the UK regulator is moving in the right direction with this kind of decision (see Figure 4).\n\nThe idea behind the sandbox is to provide a safe space for testing innovative products and services without being forced to comply with the applicable set of rules and regulations. With the sandbox, the regulator aims to foster innovation by lowering regulatory barriers and costs for testing disruptive innovative technologies, while ensuring that consumers will not be negatively affected.\n\nWhat makes the regulatory sandbox so attractive is that new ideas, product and services can be tested in a \u201clive\u201d environment. In this way, public entitlement to participate in regulatory debates can help to create a renewed sense of legitimacy that justifies the regulation.\n\nIt should thus come as no surprise that \u201cregulatory sandboxes\u201d are currently being discussed and considered by other regulators, such as the Australian Securities and Investment Commission (ASIC), Singapore\u2019s Monetary Authority (MAS) and Abu Dhabi\u2019s Financial Services Regulatory Authority (FSRA)."
    },
    {
        "url": "https://medium.com/emergent-future/people-powered-robots-3e5edca7b4db",
        "title": "People Powered Robots \u2013 Emergent // Future \u2013",
        "text": "The Dirty Little Secret of Artificial Intelligence Machines don\u2019t teach themselves. We are not yet at a point where a computer can simply \u201clook\u201d at a situation and understand what parts of the situation are valuable to learn. Because of this \u2014 the dirty little secret of the artificial intelligence community is that human beings are HEAVILY involved in the process of teaching computers. What does this look like? First off \u2014 many AI companies are simply claiming to be AI when in actually they\u2019re just a sweatshop of mechanical turks entering information into a database. But that\u2019s not what this post is about! Those who are not that deceptive tend to have a system where human beings are able to validate or invalidate the work created by the intelligent machine. At Zero Slant we follow this second trend. Quick background \u2014 Our AI creates stories from spikes in social media. For example \u2014 it will notice that there are a lot of social media posts at the Mercury Ballroom. It will then analyze these posts in order to learn what is going on. Once it understands that the event is a concert, our AI will title that story as the name of the band/artist and it will list keywords that it believes accurately represent the story. The human overseers at Zero Slant (myself included) take a look at the story created by zerobot. If the information is correct it is \u201cpublished.\u201d If it is incorrect, we flag the story, and zerobot learns to be better for the its next creation.\n\nIf you\u2019re interested in how this learning process works, check out my other piece \u2014 https://medium.com/@ryansheffer/how-to-train-your-robot-21361b297ba0#.olpgra7ga In this way, human involvement in AI is largely meant to speed up education. Like people \u2014 intelligent machines learn through a loop of data input, and output validation/invalidation. Telling a computer that a task it completed was done correctly is similar to affirming an action performed by a child for the first time. The concept that they have succeeded is required for them to grow and learn.\n\nIs It Really AI If A Human is Involved? Is it really journalism if an editor is involved? The function of people in the ever-expanding world of artificial intelligence is to check to make sure that the actions taken by these bots accurately reflect what their human counterparts would provide. The human is to the AI as the editor is to the journalist. It\u2019s not to say that the journalist is untrustworthy, but rather that a second pair of eyes can be helpful in making sure that the outcome is as amazing as possible.\n\nNo software is perfect right out the gate. AI is no different. In addition to acting as the editor \u2014 the human partner to the AI is constantly in search of bugs. Looking for inconsistencies and patterns in the actions taken by the intelligent machine that do not reflect the desired outcome. At Zero Slant our time checking the work of our AI, zerobot, showed us a pattern that proved to be a huge issue that was causing our intelligence to become stupider and stupider over time because of some choices we made with the original code. Without a human supervisor zerobot would have stupified himself into uselessness in a matter of months. While AI acts on its own and make decisions without human supervision, we must always remember that the intelligence is simply code \u2014 and code is created by humans (currently) \u2014 and humans make mistakes. Google is the greatest AI company on the planet. Through home grown intelligence and acquisition, no other company compares. After all, the Google search engine is completely built on machine learning. YET, even Google has people manually look at the results of searches in order to improve upon them. The job \u201cSearch Engine Evaluator\u201d is ever-present in a Google search. The Not So Dirty Secret Though there are plenty of companies claiming AI and secretly delivering human-created results, involving humans in the AI process is not a dirty task. It\u2019s actually exactly what needs to happen in order to take AI as quickly as possible to the point where the human onlookers are no longer needed. If you\u2019re building a new AI, you\u2019re involving human oversight. The humans help teach, help edit, and help discover bugs in the machine intelligence. At some point even these tasks will be able to be done by a computer, but for now \u2014 people to the rescue. As always you can reach me at ryan@zeroslant.com"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-self-driving-taxis-debut-voice-recognition-beats-humans-fbs-computer-4f59966cc998",
        "title": "Self-Driving Taxis Debut, Voice Recognition Beats Humans, Computer Vision",
        "text": "Issue 22\n\n This week self-driving taxis debut in Singapore, we learn thatspeech-to-text algorithms are now faster and more accurate than human typing. Let\u2019s take a dive inside Apple and its AI ambitions, and check out Facebook\u2019s open sourced computer vision algorithms.\n\nPlus, we compile our top projects to try at home, and favorite articles from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou Might Have Heard: nuTonomy began their invite-only test of six self-driving taxis in Singapore last week. A driver and researcher will be onboard during the trial phase.\n\nThe three-year-old MIT spin-out beat Uber to the punch by mere days. Uber had previously announced plans to begin trials of its own self-driving cars in Pittsburgh by the end of August.\n\nBut Did You Know? Two top autonomous car tech companies are joining forces to ship a fully autonomous car system by 2019 that could be easily integrated by automakers.\n\nPLUS: Nvidia Unveils Powerful New Processor for Self-Driving Cars\n\nA new study found voice recognition algorithms have improved to the point where they\u2019re significantly faster and more accurate at producing text on a mobile device than we are at typing on its keyboard.\n\nWith speech input, the English input rate was 3x faster with a 20% lower error rate than typing on a smartphone keyboard. Madarin was 2.8x faster with a 63% lower error rate!\n\nThe team used Deep Speech 2, Baidu\u2019s deep learning system that provides end-to-end, low latency speech recognition in English and Mandarin at scale.\n\n\u201cHumanity was never designed to communicate by using our fingers to poke at a tiny little keyboard on a mobile phone,\u201d Baidu chief scientist Andrew Ng says. \u201cSpeech has always been a much more natural way for humans to communicate with each other.\u201d\n\nPlus: How the sad state of mics are holding back Siri and Alexa, and why the billion-dollar industry hasn\u2019t seen much improvement since the launch of the iPhone 5.\n\nApple opens up about its secretive approach to AI and ML in a fascinating long read by Steven Levy at The Backchannel.\n\nApple execs Cue, Schiller, and Federighi, as well as key Siri scientists, all weigh in on the company\u2019s \u201csubtle\u201d use of ML and AI to improve its products.\n\nThe struggle comes down to how the machine learning mindset is at odds with the Apple ethos \u2014 a company that carefully controls the user experience. When engineers use machine learning, the results that emerge don\u2019t always fit with the well-thought-out, curated experience that an Apple designer specified.\n\nSo, can Apple adjust to the modern reality that machine learning systems can themselves have a hand in product design? Read the story to draw your own conclusions.\n\nFacebook is open sourcing the code for DeepMask, SharpMask, and MultiPathNet. Together, the three algorithms allow you to detect, classify, and segment objects in an image. Check out theSharpMask demo, and read the full announcement.\n\nThe code is being released by Facebook AI Research (FAIR), a team advancing the field of machine intelligence, similar to the Google Brain Team.\n\nWe\u2019re working on adding these to Algorithmia so any developer can take advantage of these state-of-the-art algorithms without having to setup, configure, or provision servers. In the meantime, check out a few of our next generation image classifiers:\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here."
    },
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0",
        "title": "Simple Reinforcement Learning with Tensorflow Part 0: Q-Learning with Tables and Neural Networks",
        "text": "For this tutorial in my Reinforcement Learning series, we are going to be exploring a family of RL algorithms called Q-Learning algorithms. These are a little different than the policy-based algorithms that will be looked at in the the following tutorials (Parts 1\u20133). Instead of starting with a complex and unwieldy deep neural network, we will begin by implementing a simple lookup-table version of the algorithm, and then show how to implement a neural-network equivalent using Tensorflow. Given that we are going back to basics, it may be best to think of this as Part-0 of the series. It will hopefully give an intuition into what is really happening in Q-Learning that we can then build on going forward when we eventually combine the policy gradient and Q-learning approaches to build state-of-the-art RL agents (If you are more interested in Policy Networks, or already have a grasp on Q-Learning, feel free to start the tutorial series here instead).\n\nUnlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the value of being in a given state, and taking a specific action there. While both approaches ultimately allow us to take intelligent actions given a situation, the means of getting to that action differ significantly. You may have heard about DeepQ-Networks which can play Atari Games. These are really just larger and more complex implementations of the Q-Learning algorithm we are going to discuss here.\n\nFor this tutorial we are going to be attempting to solve the FrozenLake environment from the OpenAI gym. For those unfamiliar, the OpenAI gym provides an easy way for people to experiment with their learning agents in an array of provided toy games. The FrozenLake environment consists of a 4x4 grid of blocks, each one either being the start block, the goal block, a safe frozen block, or a dangerous hole. The objective is to have an agent learn to navigate from the start to the goal without moving onto a hole. At any given time the agent can choose to move either up, down, left, or right. The catch is that there is a wind which occasionally blows the agent onto a space they didn\u2019t choose. As such, perfect performance every time is impossible, but learning to avoid the holes and reach the goal are certainly still doable. The reward at every step is 0, except for entering the goal, which provides a reward of 1. Thus, we will need an algorithm that learns long-term expected rewards. This is exactly what Q-Learning is designed to provide.\n\nIn it\u2019s simplest implementation, Q-Learning is a table of values for every state (row) and action (column) possible in the environment. Within each cell of the table, we learn a value for how good it is to take a given action within a given state. In the case of the FrozenLake environment, we have 16 possible states (one for each block), and 4 possible actions (the four directions of movement), giving us a 16x4 table of Q-values. We start by initializing the table to be uniform (all zeros), and then as we observe the rewards we obtain for various actions, we update the table accordingly.\n\nWe make updates to our Q-table using something called the Bellman equation, which states that the expected long-term reward for a given action is equal to the immediate reward from the current action combined with the expected reward from the best future action taken at the following state. In this way, we reuse our own Q-table when estimating how to update our table for future actions! In equation form, the rule looks like this:\n\nThis says that the Q-value for a given state (s) and action (a) should represent the current reward (r) plus the maximum discounted (\u03b3) future reward expected according to our own table for the next state (s\u2019) we would end up in. The discount variable allows us to decide how important the possible future rewards are compared to the present reward. By updating in this way, the table slowly begins to obtain accurate measures of the expected future reward for a given action in a given state. Below is a Python walkthrough of the Q-Table algorithm implemented in the FrozenLake environment:\n\nNow, you may be thinking: tables are great, but they don\u2019t really scale, do they? While it is easy to have a 16x4 table for a simple grid world, the number of possible states in any modern game or real-world environment is nearly infinitely larger. For most interesting problems, tables simply don\u2019t work. We instead need some way to take a description of our state, and produce Q-values for actions without a table: that is where neural networks come in. By acting as a function approximator, we can take any number of possible states that can be represented as a vector and learn to map them to Q-values.\n\nIn the case of the FrozenLake example, we will be using a one-layer network which takes the state encoded in a one-hot vector (1x16), and produces a vector of 4 Q-values, one for each action. Such a simple network acts kind of like a glorified table, with the network weights serving as the old cells. The key difference is that we can easily expand the Tensorflow network with added layers, activation functions, and different input types, whereas all that is impossible with a regular table. The method of updating is a little different as well. Instead of directly updating our table, with a network we will be using backpropagation and a loss function. Our loss function will be sum-of-squares loss, where the difference between the current predicted Q-values, and the \u201ctarget\u201d value is computed and the gradients passed through the network. In this case, our Q-target for the chosen action is the equivalent to the Q-value computed in equation 1 above.\n\nBelow is the Tensorflow walkthrough of implementing our simple Q-Network:\n\nWhile the network learns to solve the FrozenLake problem, it turns out it doesn\u2019t do so quite as efficiently as the Q-Table. While neural networks allow for greater flexibility, they do so at the cost of stability when it comes to Q-Learning. There are a number of possible extensions to our simple Q-Network which allow for greater performance and more robust learning. Two tricks in particular are referred to as Experience Replay and Freezing Target Networks. Those improvements and other tweaks were the key to getting Atari-playing Deep Q-Networks, and we will be exploring those additions in the future. For more info on the theory behind Q-Learning, see this great post by Tambet Matiisen. I hope this tutorial has been helpful for those curious about how to implement simple Q-Learning algorithms!"
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-self-driving-ubers-the-future-of-tech-merged-reality-and-more-5545a31326b5",
        "title": "Self-Driving Ubers, The Future of Tech, Merged Reality, and More",
        "text": "Issue 21\n\n This week we look at Uber\u2019s self-driving cars launching in Pittsburgh this month, and how Ford and BMW are keeping pace. We take a look at Intel\u2019s \u201cmerged reality\u201d headset. And, recap Chris Dixon\u2019s 11 transformative technologies, along with news from the past week that should restore your faith in humanity.\n\nPlus, we compile our top projects to try at home, and favorite articles from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou Might Have Heard: Uber\u2019s launching a fleet of autonomous cars this summer in Pittsburgh.\n\nLater this month, Uber will allow customers in downtown Pittsburgh to summon self-driving cars from their phone.\n\nThe cars will be supervised by humans from driver\u2019s seat, and launches with custom Volvo XC90s and Ford Fusions outfitted with sensors, cameras, lasers, radar, and GPS receivers.\n\nMissing out on being first to market with a self-driving car-sharing service would be an \u201cexistential threat,\u201d Uber CEO Travis Kalanick says.\n\nBut Did You Know? Uber acquired the self-driving truck startup Otto, which makes a kit that turns big-rig trucks into autonomous highway vehicles.\n\nPLUS: Ford plans to have a fleet of autonomous cars operating in a riding-hailing service by 2021, while BMW\u2019s roadmap includes an autonomous i20 by 2025.\n\nOh, and Airbus is working on a flying taxi.\n\nVenture capitalist Chris Dixon outlines 11 new technologies that will transform the world and improve human welfare. The list includes popular technologies, like self driving cars, clean energy, VR/AR, drones, and flying cars.\n\nAs well as what it means to have the entire world outfitted with super computers in their pocket, how blockchain technologies are an innovative new protocol, the creation a new space age, and what technology means to education, healthcare, and our ability to create healthy, sustainable food alternatives.\n\nAlthough every month this year has been the hottest in recorded history, here\u2019s a few headlines from the past week sure to helprestore your faith in humanity:\n\nENERGY \u2014 New lithium metal batteries could make smartphones, drones, and electric cars last twice as long. SolarCity is developingroofs made of solar cells. A study finds that electric cars could replace 90% of cars on the road, reducing emissions by 30%. Check out Block Island, a wind farm out in the Atlantic Ocean\n\nEDUCATION \u2014 A free coding school is opening in California. Four coding bootcamps granted Government financial aid eligibility.\n\nSPACE \u2014 SpaceX landed their fourth rocket on a drone ship.NASA\u2019s research is now available for free online. Astronomers discovered an \u201cEarth-like\u201d planet nearby.\n\nHEALTHCARE \u2014 Computers trounce pathologists in predicting lung cancer type and severity. Kernel is developing a brain prosthetic to improve memory; fight Alzheimer\u2019s and dementia. Scientists created nanorobots that travel down the bloodstream to target cancerous tumors.\n\nIntel showed off Project Alloy, their new VR headset. Unlike Oculus Rift or HTC Vive, the headset is an all-in-one, wireless unit with all of its cameras, sensors, and input controls are built-in.\n\nThe company\u2019s calling it \u201cmerged reality,\u201d which blends real life images from the real world into the virtual environment.\n\nPlus: After getting lapped by NVIDIA, Intel\u2019s now betting big on deep learning.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-recapping-the-google-brain-teams-reddit-ama-ee2f2a89c300",
        "title": "Emergent // Future Weekly: Recapping the Google Brain Team\u2019s Reddit AMA",
        "text": "Issue 20\n\n We change things up for our 20th issue with a recap of the Google Brain Team\u2019s Reddit AMA. The team spent nearly five hours answering questions about artificial intelligence, machine learning, deep learning, and the future of the team.\n\nPlus, we compile our top projects to try at home, and favorite articles from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nThe Google Brain Team is a group of research scientists and engineers that work to improve people\u2019s lives by creating intelligent machines. For the last five years they\u2019ve conducted research and built systems to advance this mission.\n\nHere are the 11 things we learned from their Reddit AMA on the future of artificial intelligence, machine learning, and data science. We\u2019ve pulled the most insightful quotes for brevity. Click through to see the full responses in context of the user-submitted questions, and other comments.\n\nOn the Team\u2019s Long Term Goals\n\n We want to do research on problems that we think will help in our mission of building intelligent machines, and to use this intelligence to improve people\u2019s lives.\n\nOn the State of Machine Learning\n\n Exciting: anything related to deep reinforcement learning and low sample complexity algorithms for learning policies. We want intelligent agents that can quickly and easily adapt to new tasks.\n\nUnderrated: maybe not a technique, but the general problem of intelligent automated collection of training data is IMHO under-studied right now\n\nOn Backpropogation\n\n Backpropagation has endured as the main algorithm for training neural nets since the late 1980s. This longevity, when presumably many people have tried to come up with alternatives that work better, is a reasonable sign that it will likely remain important.\n\nOn Producing Consistently High-Quality Research\n\n Learn what times of day you are at your most creative / productive, and try to protect those times to do your most important work: inventing algorithms, coding, writing papers.\n\nOn General AI\n\n Question: On a scale of 1\u201310, 10 being tomorrow and 1 being 50 years, how far away would you all estimate we are from general AI?\n\nAnswer: A 6, but I refuse to be pinned down to whether the scale is linear or logarithmic.\n\nOn Using Machine Learning to Improve People\u2019s Lives\n\n Developing ML techniques to improve the availability and accuracy of medical care is the single greatest opportunity for applied machine learning today.\n\nOn Training Data Required for Machine Learning / Deep Learning\n\n Figuring out how to learn from more with less is a very exciting research area, both inside Google and in the larger research community.\n\nOn Bias\n\n The fundamental problem is that machine learning models learn from data, and they will faithfully attempt to capture correlations that they observe in this data. Most of these correlations are fine and are what give these kinds of models their power. Some, however, reflect the \u201cworld that is\u201d rather than \u201cthe world as we wish it was.\u201d\n\nOn Current Machine Learning Challenges\n\n The biggest challenge is how to build systems that can flexibly learn to accomplish many different tasks, from relatively few examples. This will require lots of work in unsupervised learning, reinforcement learning, transfer learning, and many other machine learning sub-disciplines, but is key to building the kinds of systems we want, which are not systems that are specialized to one or a handful of tasks, but rather intelligent systems or agents that can accomplish a vast array of tasks.\n\nOn Opaque Artificial Intelligence\n\n Neural networks are tricky to understand, and developing techniques to understand them better is an incredibly important research area. There are a number of very promising directions, and we\u2019ve seen a lot of progress (especially optimization-based feature visualization).\n\nOn the TPU (Tensor Processing Unit)\n\n The TPU is designed to do the kinds of computations performed in deep neural nets. It\u2019s not so specific that it only runs one specific model, but rather is well tuned for the kinds of dense numeric operations found in neural nets, like matrix multiplies and non-linear activation functions. We agree that fabricating a chip for a particular model would probably be overly specific, but that\u2019s not what a TPU is.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here."
    },
    {
        "url": "https://medium.com/emergent-future/teslas-autopilot-saves-a-life-fb-s-404-lab-apple-s-bet-on-ai-and-bots-on-bots-on-bots-e2b94883c059",
        "title": "Emergent // Future Weekly: Tesla\u2019s Autopilot Saves a Life, FB\u2019s 404 Lab, Apple\u2019s Bet on AI, and\u2026",
        "text": "Issue 19\n\n This week we look at a self-driving car that saves the driver\u2019s life, the Facebook 404 Hardware lab, how Apple\u2019s betting on AI, why a bunch of bots are hacking other bots, and a reminder that the Google\u2019s Brain Team has an AMA next week. Plus, projects to try at home, and our top reads from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou might have heard: A self-driving Tesla SUV saved a man\u2019s lifeby driving him to an emergency room while he was having a pulmonary embolism. The artery blockage in his lungs could have been fatal had he been forced to pull over and wait for an ambulance.\n\nInstead, the Model X navigated 20 miles of highway until it reached an off-ramp near a hospital. From there, he drove himself a few blocks to the emergency room.\n\nBut did you know: Three members of Google\u2019s self-driving car project are leaving, including CTO Chris Urmson.\n\nUrmson was the main engineer behind the code running the car\u2019s autonomous software.\n\nSome are asking if the company can expect a financial windfall from its moonshot projects since a commercial version of the car is still likely to be several years away.\n\nPlus: A driving dataset for car autopilot AI training based on the paper Learning a Driving Simulator, which uses an agent that learns to clone driver behaviors and maneuvers by simulating future events in the road.\n\nTake a look inside Area 404, Facebook\u2019s 22K square foot hardware lab at its Menlo Park HQ. The social network is using the lab to prototype its solar drones, internet-beaming lasers, VR headsets, and next-generation servers.\n\nIn an effort to \u201cmove fast and break things,\u201d the company developed the lab to avoid outsourcing delays and shorten the time needed to go from concept to prototype.\n\nIt appears that Apple\u2019s dead serious about AI after all. The company acquired Turi, a machine learning startup, for around $200M last week.\n\nThe Seattle startup helped developers build apps using machine learning and AI in the cloud. Apple\u2019s plans for Turi aren\u2019t clear.\n\nThe purchase comes as Apple and the other giants of the tech world \u2014 Facebook, Google, Microsoft and Amazon \u2014 areincreasingly focused on machine learning and artificial intelligence.\n\nWhile Apple was early with its Siri personal assistant, other companies have made big moves into bots, while Siri has evolved rather slowly.\n\nPlus: Computing pioneer Alan Kay on AI, Apple, and the future\n\nA Pentagon-sponsored bot battle last week showed how computers can fix their own flaws, while making the Internet safer in the process.\n\nA Cyber Grand Challenge was staged at the DEF CON hacking conference by the Pentagon\u2019s Defense Advanced Research Projects Agency to spur the invention of software able to automatically spot, test, and fix security flaws.\n\nSeven teams played a 96-round game of \u201cCapture the Flag,\u201d wherebots had to play both offense and defense against other bots, while fixing security holes in their code while exploiting the holes of others.\n\nIn the end, \u201cMayhem,\u201d a Carnegie Mellon team took home the $2 million DARPA prize.\n\nGoogle\u2019s Brain Team will hold an AMA on Reddit on August 11th.\n\nThe team of research scientists and engineers, tasked with building intelligent machines used to improve people\u2019s lives, is gathering questions now.\n\nHead over to /r/MachineLearning and add your question.\n\nNeed Inspiration? Skim through the excellent Quora Session with Yann LeCun, director of AI research at Facebook, for a survey of breakthrough technologies.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here."
    },
    {
        "url": "https://medium.com/emergent-future/apple-pivots-car-project-seymour-papert-has-died-ai-for-office-apps-and-machine-learning-in-4ea1146c409f",
        "title": "Apple Pivots Car Project, Seymour Papert Has Died, AI for Office Apps, and Machine Learning in\u2026",
        "text": "Issue 18\n\n This week we look at Apple\u2019s autonomous car project, reflect on the passing of AI pioneer Seymour Papert, check out the AI updates to Microsoft Office apps, and review some uses of machine learning in science. Plus, projects to try at home, and our top reads from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou might have heard: Apple is shifting the focus of their car projectfrom building a self-driving, electric car to developing an autonomous driving system.\n\nThe news comes after Apple tapped Bob Mansfield to oversee car project, dubbed Project Titan.\n\nApple isn\u2019t giving up on a car, however, but the company is now diving into the software that could power the next generation of carswith a focus on the user experience.\n\n\u201cWe have focused our AI efforts on the features that best enhance the customer experience,\u201d Tim Cook explained. \u201cA killer user experience that is integrated across their lives I think becomes more important, and I think that really plays to our advantage. I also think that the deployment of AI technology is something that we will excel at because of our focus on user experience, and so I like that.\u201d\n\nBut did you know: Tesla is buying SolarCity for $2.6 billion.\n\nA major part of Tesla CEO Elon Musk\u2019s master plan involves developing solutions to generate, store, and enable mass consumption of solar energy.\n\nTesla is also racing to finish the \u201cGigafactory\u201d before the Model 3 rollout in 2018. Musk anticipates that the plant could produce 105 gigawatt hours of battery cells by 2020.\n\nPLUS: Delphi hopes to make self-driving taxis a reality in Singapore by 2022. Delphi was the first car company to complete a cross-country road trip using autonomous technology, driving from San Francisco to New York back in April 2015.\n\nProfessor Emeritus Seymour Papert, a pioneer of constructionist learning, is dead at 88.\n\nThe world-renowned mathematician, learning theorist, and educational-technology visionary was a founding faculty member of the MIT Media Lab.\n\nWell before the advent of the personal computer, Papert foresaw children using computers as instruments for learning and enhancing creativity. Papert\u2019s \u201cconstructionist\u201d theory of education held that kids learn best by building things and making things happen.\n\nPapert was the co-author of the now-classic work on artificial intelligence: \u201cPerceptrons: An Introduction to Computational Geometry. The perceptron was one of the first artificial neural networks to be created. The algorithm uses pattern recognition based on a two-layer computer learning network.\n\nMicrosoft Word, Outlook, and PowerPoint are getting intelligent updates to help improve productivity and performance.\n\nWith the addition of machine learning and natural language processing, Microsoft believes the updates to Office 365 apps will \u201csave you time and produce better results.\u201d\n\nMicrosoft is adding a new Researcher feature to Word, which uses the Bing Knowledge Graph to find content from the internet and pull it straight into Word.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here."
    },
    {
        "url": "https://medium.com/emergent-future/apple-pivots-car-project-seymour-papert-has-died-ai-for-office-apps-and-machine-learning-in-3256e1bcd316",
        "title": "Emergent // Future Weekly: Apple Pivots Car Project, Seymour Papert Has Died, AI for Office Apps\u2026",
        "text": "Issue 18\n\n This week we look at Apple\u2019s autonomous car project, reflect on the passing of AI pioneer Seymour Papert, check out the AI updates to Microsoft Office apps, and review some uses of machine learning in science. Plus, projects to try at home, and our top reads from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou might have heard: Apple is shifting the focus of their car projectfrom building a self-driving, electric car to developing an autonomous driving system.\n\nThe news comes after Apple tapped Bob Mansfield to oversee car project, dubbed Project Titan.\n\nApple isn\u2019t giving up on a car, however, but the company is now diving into the software that could power the next generation of carswith a focus on the user experience.\n\n\u201cWe have focused our AI efforts on the features that best enhance the customer experience,\u201d Tim Cook explained. \u201cA killer user experience that is integrated across their lives I think becomes more important, and I think that really plays to our advantage. I also think that the deployment of AI technology is something that we will excel at because of our focus on user experience, and so I like that.\u201d\n\nBut did you know: Tesla is buying SolarCity for $2.6 billion.\n\nA major part of Tesla CEO Elon Musk\u2019s master plan involves developing solutions to generate, store, and enable mass consumption of solar energy.\n\nTesla is also racing to finish the \u201cGigafactory\u201d before the Model 3 rollout in 2018. Musk anticipates that the plant could produce 105 gigawatt hours of battery cells by 2020.\n\nPLUS: Delphi hopes to make self-driving taxis a reality in Singapore by 2022. Delphi was the first car company to complete a cross-country road trip using autonomous technology, driving from San Francisco to New York back in April 2015.\n\nProfessor Emeritus Seymour Papert, a pioneer of constructionist learning, is dead at 88.\n\nThe world-renowned mathematician, learning theorist, and educational-technology visionary was a founding faculty member of the MIT Media Lab.\n\nWell before the advent of the personal computer, Papert foresaw children using computers as instruments for learning and enhancing creativity. Papert\u2019s \u201cconstructionist\u201d theory of education held that kids learn best by building things and making things happen.\n\nPapert was the co-author of the now-classic work on artificial intelligence: \u201cPerceptrons: An Introduction to Computational Geometry. The perceptron was one of the first artificial neural networks to be created. The algorithm uses pattern recognition based on a two-layer computer learning network.\n\nMicrosoft Word, Outlook, and PowerPoint are getting intelligent updates to help improve productivity and performance.\n\nWith the addition of machine learning and natural language processing, Microsoft believes the updates to Office 365 apps will \u201csave you time and produce better results.\u201d\n\nMicrosoft is adding a new Researcher feature to Word, which uses the Bing Knowledge Graph to find content from the internet and pull it straight into Word.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here."
    },
    {
        "url": "https://medium.com/emergent-future/teslas-big-plans-deepmind-pays-for-itself-internet-drones-and-moore-s-law-54e6bd8e2750",
        "title": "Tesla\u2019s Big Plans, DeepMind Pays For Itself, Internet Drones, and Moore\u2019s Law",
        "text": "Issue 17\n\n This week we review Elon Musk\u2019s big plans for Tesla, how Google uses DeepMind to save millions of dollars, why Zuckerberg is building a fleet of internet drones, and check in on Moore\u2019s Law death watch. Plus, projects to try at home, and our top reads from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou might have heard: Elon Musk outlined his masterplan for Tesla in blog post. For the past 10-years, Tesla\u2019s vision had been to do:\n\nNow, Musk is doubling-down on solar power, Tesla trucks, self-driving cars, and car-sharing \u2014 he wants your car to make you money when you aren\u2019t using it. The company has already started developing electric and autonomous trucks and buses.\n\ntl;dr \u201cWe\u2019re not an electric car company; we\u2019re a futuristic logistics company and manufacturer.\u201d h/t Fusion\n\nBut did you know: Mercedes is testing CityPilot, their semi-autonomous bus program in Amsterdam? The bus recently completed a 12-mile test trip that connected Amsterdam\u2019s Schiphol airport with the nearby town of Haarlem.\n\nAlong the route, the bus, which is fully networked and can communicate with traffic lights and other city infrastructure, had to stop at traffic lights, pass through tunnels, and navigate pedestrians.\n\nALSO: Why GM Is Holding Back Its Bleeding-Edge Tech For Autonomous Vehicles\n\nGoogle\u2019s latest DeepMind experiment has improved the power usage efficiency in their data centers by 15%, and cut the cost used for cooling by 40%.\n\nCompared to five years ago, Google now gets 3.5 times the computing power out of the same amount of energy.\n\nEssentially, the company Google acquired in 2014 for more $600 million, is now paying for itself. Now that\u2019s electric.\n\nFacebook just completed the first test flight of their internet drone, Aquila. The drone is part of Zuckerberg\u2019s plan to bring the internet to all 7 billion people on Earth by launching high-altitude, solar-powered drones that beam internet access to the ground.\n\nAt cruising altitude, Aquila used 2,000 watts of energy \u2014 the equivalent output of five strong cyclists. We wonder: could DeepMind improve upon that?\n\nFacebook HQ says 60% of the global population doesn\u2019t have internet access. And, as many as 1.6 billion of those unconnected live in remote locations with no access to mobile broadband networks.\n\nBy 2021 Moore\u2019s Law will be dead. That\u2019s the word from the Semiconductor Industry Association, which says that transistors will stop shrinking.\n\nAll is not lost, however. Processors could continue to fulfill Moore\u2019s Law by increasing in vertical density.\n\nMeanwhile, Nvidia unveiled their new flagship graphics card: the $1,200 Titan X with 12GB GDDR5X memory.\n\nThe company claims it\u2019s 60% faster than previous Titan X. It\u2019s no surprise that Nvidia calls their new processor: \u201cThe Ultimate. Period.\u201d\n\nTo put Moore\u2019s Law in perspective: 69 years ago, scientists built the first transistor. Today, there\u2019s a graphics card with 12 billion of them."
    },
    {
        "url": "https://medium.com/emergent-future/how-will-apps-benefit-from-deep-learning-and-ai-9d5554f13a87",
        "title": "How will apps benefit from deep learning and AI? \u2013 Emergent // Future \u2013",
        "text": "Deep learning and AI enable us to solve certain kinds of problems that would otherwise take an unreasonably long time to work out using other techniques.\n\nToday the typical app experience resembles filling out a stack of form sheets. The app developer\u2019s job is to mould user input into some sort of data structure and this is an incentive to be as unambiguous as possible in conveying what the app expects the user to do. Collecting unstructured data (i.e. data that does not fit the mould) is considered a waste by the developer and the user may be left guessing why their data is being collected even though they are receiving no clear benefit from it. However, deep learning and AI excel at solving problems that involve unstructured data such as freeform text, audio, images, and video. Collecting large amounts of unstructured data actually makes it easier to solve a problem using these methods.\n\nWhat will be the catalyst that will imbue \u201call serious applications\u201d with deep learning and AI? Several services and frameworks currently offer AI algorithm sandboxes but none of them promote AI enabled user experience stories. This leaves a void between UX design and data science. An innovator in this space could start, for example, with contemplating how Apple\u2019s Human Interface Guidelines (the HIG) could be revised to reflect deep learning and AI enabled design patterns. Bot enabled platforms will have an advantage as they encourage freeform user input by design.\n\nToday a small number of apps collect the bulk of unstructured user data and are often reluctant to share it. Sharing of user data with other apps is a double edged sword: on one hand the user experience is improved from better understanding of the user and on the other hand the user might feel uncomfortable with what is shared and who has access to their data."
    },
    {
        "url": "https://medium.com/emergent-future/machine-learning-ninjas-autonomous-cars-computer-vision-delivery-drones-and-more-29c97c383460",
        "title": "Machine Learning Ninjas, Autonomous Cars, Computer Vision, Delivery Drones, and More",
        "text": "Issue 15\n\n This week we learn about the machine learning ninjas at Google, check in with a company retrofitting old cars with new tech, discover Apple\u2019s computer vision capabilities, and anxiously wait for delivery drones taking flight. Plus, what we\u2019re reading and a few things for you to try at home.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou Might Have Heard: Google is remaking itself as a machine learning company by teaching its employees to use the latest learning algorithms.\n\nThe Machine Learning Ninja Program \u2014 no joke \u2014 embeds select Googlers with their machine learning team for six months where they work on ML projects, like Smart Reply in Gmail, and RankBrain.\n\nIn an earnings call late last year, CEO Sundar Pichai laid out Google\u2019s new vision: \u201cMachine learning is a core, transformative way by which we\u2019re rethinking how we\u2019re doing everything. We are thoughtfully applying it across all our products, be it search, ads, YouTube, or Play. And we\u2019re in early days, but you will see us \u2014 in a systematic way \u2014 apply machine learning in all these areas.\u201d\n\nIf Google\u2019s going to build machine learning into all its products, it needs engineers who have mastery of those techniques.\n\nBut Did You Know: Google\u2019s RankBrain machine learning system is now used to process every search query, and is now considered the third most important ranking factor?\n\nPLUS: Benedict Evans weighs in on the state of AI, Apple and Google\n\nWhile mums the word on Apple\u2019s car plans, Pearl Automation has big plans to transform millions of today\u2019s cars into the self-driving vehicles of tomorrow.\n\nArmed with 50 former Apple employees, the team has plans for a slew of devices that can upgrade your car by augmenting it with the latest driving capabilities.\n\nWhile their entry into the auto accessory aftermarket is sort of an anticlimactic entry to the market, the Verge writes. \u201cThe differentiator is the attention to quality and detail that Apple is known for. This craftsmanship shines through in Pearl\u2019s offering, as well.\u201d\n\nPLUS: Behind the race in Silicon Valley and Detroit to build a self-driving car\n\nYou might remember that Apple added facial and object recognition to the Photos app for iPhone and iPad to bundle photos according to events and places.\n\nWell, a developer has already uncovered that the new Photos app can distinguish 7 facial expressions, generate 33 Moments categories, and detect 4,432 searchable objects.\n\nSo, what\u2019s so great about this?\n\nUnlike Google Photos, Apple has made sure that Photos only uses machine learning to analyze the photos that are stored locally on your iPhone, eliminating the need to upload your photos to Apple\u2019s servers for processing.\n\nThe FAA unveiled new rules last week to allow for commercial operation of low-altitude drones weighing less than 55 pounds.\n\nAs long as you can pass the skill test, you can operate a drone legally.\n\nHowever, drones cannot be flown at night, and must always remain within eyeshot of the pilot, making it unlikely that autonomous drone delivery will be flying anytime soon.\n\nThe FAA says the rules could \u201ccould generate more than $82 billion for the U.S. economy and create more than 100,000 new jobs over the next 10 years.\u201d\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here."
    },
    {
        "url": "https://medium.com/emergent-future/twitters-machine-learning-plans-is-deep-learning-magic-net-neutrality-upheld-and-more-4189942473d8",
        "title": "Twitter\u2019s Machine Learning Plans, Is Deep Learning Magic?, Net Neutrality Upheld, and More",
        "text": "Issue 14\n\n This week we check in on Twitter\u2019s machine learning plans, ask if deep learning is magic or just math, celebrate the latest net neutrality ruling, and share some videos to watch. Plus, what we\u2019re reading and a few things for you to try at home.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou Might Have Heard: Twitter announced Monday that it acquired the AI company Magic Pony Technology.\n\nThe London-based company uses machine learning and neural networks to identify features in video, enhance imagery, and create graphics for virtual and augmented reality.\n\nIn other words, Twitter\u2019s big bet is that the algorithms will improve video streaming for Vine and Periscope by automatically filling in patchy video feeds, and increasing the resolution of pixelated video and images.\n\nThe new tech might come in handy this Fall when Twitter begins providing free, live streaming video of the NFL\u2019s Thursday Night Football games to more than 800 Million users worldwide on mobile phones, tablets, PCs and connected TVs.\n\nMagic Pony is the third machine learning acquisition by Twitter, joining Madbits, and Whetlab. Sources say the deal was worth $150M and includes a team of 11 PhDs with expertise in computer vision, machine learning, high-performance computing, and computational neuroscience.\n\nPLUS: Google opened a dedicated machine learning research center in Zurich to focus on machine intelligence, natural language processing, and machine perception.\n\nDeep learning isn\u2019t a dangerous magic genie, and it\u2019s far from magic, Oren Etzioni says, CEO of the Allen Institute for Artificial Intelligence and a computer scientist at the University of Washington.\n\nGoogle, Facebook, Microsoft and others continue to push AI into everyday online services while pundits describe deep learning as an imitation of the human brain. It\u2019s really just simple math, Oren says, executed on an enormous scale.\n\nAnd, amazingly, the artificial intelligence we were promised is finally coming as deep learning algorithms can now recognize images, voice, text, and more.\n\n\u201cThere is almost nothing we can think of that cannot be made new, different, or interesting by infusing it with some extra IQ,\u201d Kevin Kelly writes.\n\nIt\u2019s not all peaches and cream. Oxford philosopher and author Nick Bostrom warns that artificial intelligence is a greater threat to humanity than climate change.\n\nSo, what\u2019s next for AI? The best minds in AI weigh in on what life will look like in the age of the machines.\n\nConfused About AI? This two-part series on the AI revolution and the road to superintelligence is an excellent primer. (Part 1) (Part 2)\n\nPLUS: A guide to staying human in the machine age\n\nA federal appeals court has ruled that high-speed internet can be defined as a utility, putting it on par with other essential services like power and the phone.\n\nThe ruling clears the way for rigorous policing of broadband providers like AT&T, Comcast, and Verizon, limiting their ability to shape the user experience by blocking content, prioritizing paid content, or creating fast and slow lanes on the internet.\n\n\u201cThis is an enormous win for consumers,\u201d said Gene Kimmelman, president of the public interest group Public Knowledge. \u201cIt ensures the right to an open internet with no gatekeepers.\u201d\n\nWith the win, the internet will remain a platform for innovation, free expression, and economic growth, Tom Wheeler, chairman of the F.C.C., said in a statement.\n\nBut, hold the celebration, the fight for net neutrality isn\u2019t over.\n\nAT&T is already vowing to fight the decision, and expects this to ultimately be decided by the Supreme Court. \ud83d\ude48"
    },
    {
        "url": "https://medium.com/emergent-future/twitters-machine-learning-plans-is-deep-learning-magic-net-neutrality-upheld-and-more-6dc3c8455672",
        "title": "Twitter\u2019s Machine Learning Plans, Is Deep Learning Magic?, Net Neutrality Upheld, and More",
        "text": "Issue 14\n\n This week we check in on Twitter\u2019s machine learning plans, ask if deep learning is magic or just math, celebrate the latest net neutrality ruling, and share some videos to watch. Plus, what we\u2019re reading and a few things for you to try at home.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou Might Have Heard: Twitter announced Monday that it acquired the AI company Magic Pony Technology.\n\nThe London-based company uses machine learning and neural networks to identify features in video, enhance imagery, and create graphics for virtual and augmented reality.\n\nIn other words, Twitter\u2019s big bet is that the algorithms will improve video streaming for Vine and Periscope by automatically filling in patchy video feeds, and increasing the resolution of pixelated video and images.\n\nThe new tech might come in handy this Fall when Twitter begins providing free, live streaming video of the NFL\u2019s Thursday Night Football games to more than 800 Million users worldwide on mobile phones, tablets, PCs and connected TVs.\n\nMagic Pony is the third machine learning acquisition by Twitter, joining Madbits, and Whetlab. Sources say the deal was worth $150M and includes a team of 11 PhDs with expertise in computer vision, machine learning, high-performance computing, and computational neuroscience.\n\nPLUS: Google opened a dedicated machine learning research center in Zurich to focus on machine intelligence, natural language processing, and machine perception.\n\nDeep learning isn\u2019t a dangerous magic genie, and it\u2019s far from magic, Oren Etzioni says, CEO of the Allen Institute for Artificial Intelligence and a computer scientist at the University of Washington.\n\nGoogle, Facebook, Microsoft and others continue to push AI into everyday online services while pundits describe deep learning as an imitation of the human brain. It\u2019s really just simple math, Oren says, executed on an enormous scale.\n\nAnd, amazingly, the artificial intelligence we were promised is finally coming as deep learning algorithms can now recognize images, voice, text, and more.\n\n\u201cThere is almost nothing we can think of that cannot be made new, different, or interesting by infusing it with some extra IQ,\u201d Kevin Kelly writes.\n\nIt\u2019s not all peaches and cream. Oxford philosopher and author Nick Bostrom warns that artificial intelligence is a greater threat to humanity than climate change.\n\nSo, what\u2019s next for AI? The best minds in AI weigh in on what life will look like in the age of the machines.\n\nConfused About AI? This two-part series on the AI revolution and the road to superintelligence is an excellent primer. (Part 1) (Part 2)\n\nPLUS: A guide to staying human in the machine age\n\nA federal appeals court has ruled that high-speed internet can be defined as a utility, putting it on par with other essential services like power and the phone.\n\nThe ruling clears the way for rigorous policing of broadband providers like AT&T, Comcast, and Verizon, limiting their ability to shape the user experience by blocking content, prioritizing paid content, or creating fast and slow lanes on the internet.\n\n\u201cThis is an enormous win for consumers,\u201d said Gene Kimmelman, president of the public interest group Public Knowledge. \u201cIt ensures the right to an open internet with no gatekeepers.\u201d\n\nWith the win, the internet will remain a platform for innovation, free expression, and economic growth, Tom Wheeler, chairman of the F.C.C., said in a statement.\n\nBut, hold the celebration, the fight for net neutrality isn\u2019t over.\n\nAT&T is already vowing to fight the decision, and expects this to ultimately be decided by the Supreme Court. \ud83d\ude48\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here."
    },
    {
        "url": "https://medium.com/emergent-future/flying-cars-understanding-ai-and-machine-learning-wwdc-news-and-more-38eb94e3d3ff",
        "title": "Flying Cars, Understanding AI and Machine Learning, WWDC News, and More",
        "text": "You Might Have Heard: Larry Page has a secret flying-car factory.\n\nNot only that, but he\u2019s invested over $100M into the flying-car startup Zee.Aero, and is also an investor in the competing firm, Kitty Hawk.\n\nFlying cars have always been a part of our collective imagination, for better or worse, but for flying cars to become a reality they need to be able to fly autonomously.\n\n\u201cWith ultralight composites and better battery tech we may actually be drawing near a basic functional design,\u201d TechCrunch writes. \u201cIn a few decades it may seem absurd that we drove our own cars for a century, with the dead from traffic accidents totaling in the millions.\u201d\n\nThis video presentation by a16z Deal and Research head Frank Chen, walks through the basics of AI, deep learning, and machine learning, explaining how we got here, why now, and what the next breakthrough is.\n\nEvery company is now a data company, capable of using machine learning in the cloud to deploy intelligent apps at scale, thanks to three machine learning trends: data flywheels, the algorithm economy, and cloud-hosted intelligence.\n\nDespite playing catchup to Google and Microsoft, Facebook wants to dominate in AI and machine learning. They\u2019ve tripled their investment in processing power for research, and hired up more than 150 people.\n\nMeanwhile, a patent for \u201cFace Detection Using Machine Learning\u201d was just granted. \ud83d\ude33\n\nPLUS: A few considerations when setting up deep learning hardware.\n\nThe big news from Apple WWDC this year is that they\u2019re opening up Siri to app developers in an effort to keep pace with Amazon, Google, Facebook, and Microsoft, all of which are betting that voice commands and chatbots will be one of the next big computing paradigms.\n\nHomeKit, the Apple Internet of Things platform, got a big updateand a dedicated app to control all the devices in your home, andfeatures access to connected cameras from the lock screen, geofencing, automation, Siri commands, and Apple Watch support.\n\nOne More Thing: Apple added facial and object recognition to the iPhone and iPad.\n\nThe computer vision tech runs natively on the device, and doesn\u2019t require you to upload all the images to the cloud. With the Photos app, you can now recognize faces and objects across your photo library to help you to find a specific photo with ease.\n\nIn addition, the app has a new feature called Memories, which bundles photos according to events and places.\n\nPLUS: The 13 biggest announcements from Apple WWDC 2016\n\nThe VR headset from HTC started shipping last week in 24 countries, with delivery in two to three days. Head to a Microsoft, GameStop, and Micro Center store for a demo before plopping down $799.\n\nBut, should you buy an HTC Vive right now?\n\nStill unsure? Here\u2019s an HTC Vive vs Oculus Rift comparison.\n\ntl;dr \n\nThe Rift has better sound and ergonomics; The Vive has better controllers and smoother tracking of head and hand motions.\n\n\u201cBoth of these devices are good enough to deliver a great VR experience to millions of people. This is the beginning of something big.\u201d\n\nPLUS: VR rollercoasters are coming, and they look amazing.\n\nALSO: Augmented reality startup Blippar unveils its \u201cvisual browser,\u201d an app for recognizing real-world objects using machine learning."
    },
    {
        "url": "https://medium.com/emergent-future/machine-learning-trends-and-the-future-of-artificial-intelligence-2016-15c15cd6c129",
        "title": "Machine Learning Trends and the Future of Artificial Intelligence 2016",
        "text": "Every company is now a data company, capable of using machine learning in the cloud to deploy intelligent apps at scale, thanks to three machine learning trends: data flywheels, the algorithm economy, and cloud-hosted intelligence.\n\nThat was the takeaway from the inaugural Machine Learning / Artificial Intelligence Summit, hosted by Madrona Venture Group* last month in Seattle, where more than 100 experts, researchers, and journalists converged to discuss the future of artificial intelligence, trends in machine learning, and how to build smarter applications.\n\nWith hosted machine learning models, companies can now quickly analyze large, complex data, and deliver faster, more accurate insights without the high cost of deploying and maintaining machine learning systems.\n\n\u201cEvery successful new application built today will be an intelligent application,\u201d Soma Somasegar said, venture partner at Madrona Venture Group. \u201cIntelligent building blocks and learning services will be the brains behind apps.\u201d\n\nBelow is an overview of the three machine learning trends leading to a new paradigm where every app has the potential to be a smart app."
    },
    {
        "url": "https://medium.com/emergent-future/newsletters-for-those-curious-about-the-future-f483233150c1",
        "title": "Newsletters for those curious about the future \u2013 Emergent // Future \u2013",
        "text": "Newsletters for those curious about the future\n\nOne of the best ways to learn something new is to leverage somebody else\u2019s knowledge. Smartly curated newsletters are one way of getting a quickly gaining chunks of knowledge.\n\nI\u2019m deeply interested in the future as it relates to culture being shaped by science and technology. That said, here\u2019s a personal round up of some of my favorite newsletters providing a diversity of opinions, thoughts, and perspectives.\n\nAm I missing a newsletter that you love? Let me know, and I\u2019ll add it."
    },
    {
        "url": "https://medium.com/emergent-future/emergent-future-report-internet-trends-report-facebooks-ai-tool-elon-musk-blade-runner-and-64e75a5b763f",
        "title": "Emergent // Future Report: Internet Trends Report, Facebook\u2019s AI Tool, Elon Musk, Blade Runner, and\u2026",
        "text": "Issue 12\n\n This week we recap Mary Meeker\u2019s latest Internet Trends Report, and check in on Facebook\u2019s latest AI project. Prepare to be shocked by Elon Musk\u2019s wildest ideas yet (spoiler: you live in a video game, and we\u2019re going to Mars!), and a researcher used a neural network to recreate Blade Runner. Plus, what we\u2019re reading and a few things for you to try at home.\n\nYou Might Have Heard: Mary Meeker released her annual Internet Trends report.\n\nNow in its 21st year (!), the 213-slide deck covers all sides of the internet economy, including the rise of messaging apps, voice assistants, and more. This essential report is the fastest way to learn everything going on in tech.\n\nFacebook announced DeepText, their AI system built to understand the meaning and sentiment of all text posts on the platform, with the goal of building a better search engine.\n\n\u201cWith this new project, Facebook is essentially building the capacity to track all the information put into the network, just as Google crawls the entire web for information and indexes it,\u201dQuartz writes.\n\nDeepText will analyze thousands of posts per second across 20 languages with near-human accuracy.\n\n\u201cThe gap between the AI haves and have-nots is widening,\u201dTechCrunch writes.\n\n\u201cIf every News Feed post looks interesting, you\u2019ll spend more time on Facebook, you\u2019ll share more text there, DeepText will get smarter, and the Facebook AI feedback wheel will spin faster and faster towards becoming the perfect content recommendation engine.\u201d\n\nThe tech billionaire says \u201cthere\u2019s a billion to one chance we\u2019re living in base reality.\u201d\n\nMeaning, our existence is probably really just a video game.\n\nSimulation or not, Musk said he plans to send people to Mars as early as 2024 during an interview with Kara Swisher and Walt Mossberg at Code Conference.\n\nBut that\u2019s not all, here\u2019s 7 other not-so-crazy crazy things Elon Musk believes.\n\nPLUS: These are the vehicles that will being taking you to space\n\nA London researcher trained a computer to watch the movie Blade Runner. Then, using a neural network, had the computer attempt to reconstruct its impression of the movie in order based on what it had seen.\n\nEssentially creating a new film from the \u201cmemories\u201d the neural net had formed \u2014 the computer\u2019s interpretation of the film through the eyes of an AI.\n\nBut that\u2019s not the weird part.\n\nWarner Bros., which owns the Blade Runner copyright, issued a DMCA takedown notice (!) for an \u201capparently real\u201d film.\n\nIn other words, Vox writes: \u201cWarner had just DMCA\u2019d an artificial reconstruction of a film about artificial intelligence being indistinguishable from humans, because it couldn\u2019t distinguish between the simulation and the real thing.\u201d \ud83d\ude33\n\nTo its credit, Warner later rescinded the DMCA request.\n\nThe researcher, Terence Broad, has posted a detailed post about autoencoding Blade Runner, and reconstructing films with artificial neural networks.\n\nPLUS: \u20182001\u2019 rendered in the style of Picasso using deep neural networks"
    },
    {
        "url": "https://medium.com/emergent-future/weekly-tech-news-roundup-google-ai-hyperloop-vr-and-more-44f482a0c9f8",
        "title": "Weekly Tech News Roundup: Google, AI, Hyperloop, VR, and More",
        "text": "Issue 11\n\n Google\u2019s annual I/O conference was all about the future of AI, researchers unveiled a Hyperloop prototype, and VR goes IMAX. Plus, we share a few surprising discoveries, projects to try at home, and our top reads from the past week.\n\nYou Might Have Heard: Google announced a bunch of new tech last week during their 10th annual I/O developers conference.\n\nTheir vision for a more ubiquitous and conversational way of interacting with technology was on display, highlighted by:\n\n\u201cGoogle is doubling down on artificial intelligence as the next great phase of computing,\u201d Walt Mossberg writes. \u201cAnd they believe Google can do it better than anyone else.\u201d\n\nThey better hope they can, since we\u2019ve already seen these products and services from other companies, Buzzfeed writes. In a sense, Google is playing catchup.\n\nBut Did You Know: Google is now building its own chips to expedite its machine learning algorithms? They\u2019ve created a Tensor Processing Unit, a custom ASIC specifically tailored for TensorFlow.The new hardware is \u201croughly equivalent to fast-forwarding technology about seven years into the future.\u201d\n\nResearchers from MIT unveiled a prototype pod that uses magnetic levitation to travel at about 229 miles per hour. The team will test the pod this summer at a one-mile racetrack near SpaceX\u2019s headquarters in Hawthorne, CA. A trip from San Francisco to Los Angeles could be made in as little as 30 minutes.\n\nSome, however, are wondering if Hyperloop is really the future of travel, or really just an expensive option for the rich.\n\nMore: Hyperloop Technologies, an LA startup, has started shooting mag-lev capsules along a track in Las Vegas to show off a radical idea for the future of freight and mass transit.\n\nWhat We\u2019re Keeping An Eye On: IMAX and Google are teaming up to develop a \u201ccinema-grade\u201d VR rig to capture 360-degree images. IMAX plans to launch six VR experiences at multiplexes and malls later this year.\n\nThe move makes sense given Google\u2019s announcement of Daydream, their new VR headset and content platform. Google says Daydream will start shipping this fall.\n\nPLUS: Learn more about Daydream, its Cardboard origins, and Google\u2019s ambitions with this new VR platform\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come."
    },
    {
        "url": "https://medium.com/emergent-future/weekly-tech-news-roundup-viv-delivery-drones-smart-homes-and-more-571b836e5121",
        "title": "Weekly Tech News Roundup: Viv, Delivery Drones, Smart Homes, and More",
        "text": "Issue 10\n\n In this issue we meet Siri\u2019s sister Viv, learn how Amazon is equipping their delivery drones with computer vision, what Google is doing to own the smart home market and makeaugmented reality ubiquitous, how VR will change how you experience concerts, and we top things off with a collection of links from the near // future.\n\nYou Might Have Heard: The creator of Siri showed off the first public demo of Viv last week. The multi-platform, AI-powered personal assistant aims to be the \u201cintelligent interface for everything.\u201d\n\nViv is more natural in conversation than Siri, Cortana, or Alexa, thanks to dynamic program generation \u2014 aka software that writes itself. Meaning, Viv can understand not only the intent of a question, but follow-up questions as well.\n\n\u201cViv has the ability to create programs out of thin air to complete commands that may not fit the pre-defined mold,\u201d The Next Web writes. \u201cThis is a stark contrast to Siri, which defaults to a Web search when it doesn\u2019t understand \u2014 or can\u2019t find \u2014 an answer.\u201d\n\nNot only that, but Viv aims to be \u201cthe connective tissue between all your meaningful digital interactions\u201d by creating an open marketplace for third-party developers \u2014 like the App Store, but for bots.\n\n\u201cI\u2019m sure others have said it, but five years later, this is where Siri should be,\u201d blogger Cody Lee tweeted. \u201cDynamic learning, developer API, etc.\u201d\n\nWatch: The demo of Viv from Disrupt\n\nAmazon has a team building \u201csense-and-avoid technology\u201d for their drone delivery program, Prime Air.\n\nTo be truly autonomous, UAVs need to be able to \u2018see\u2019 the world around them, which is why the team is comprised primarily of top computer vision experts.\n\n\u201cAmazon\u2019s drones are aiming to deliver packages under five pounds in 30 minutes or less with a range of 15 miles,\u201d the Verge writes. \u201cAfter leaving the controlled airspace of the warehouse with their cargo, the drones will rise up to a cruising altitude of several hundred feet. They will travel to the destination at around 60 miles an hour, then attempt to land and drop off the goods.\u201d\n\nGoogle\u2019s answer to the Amazon Echo is code-named \u201cChirp,\u201d and is expected to launch later this year.\n\nDoes this mean competition for Alexa? Probably. Chirp is bringing \u2018Okay Google\u2019 to the home, which combines Google\u2019s search engine with its voice assistant technology.\n\n\u201cI\u2019d like to see the day when both Alexa and Chirp are bidding for my conversational economy in my home,\u201d entrepreneur Ben Huh tweeted.\n\nSpeaking of voice assistant technology: Now anyone can add voice recognition to their app. Google open-sourced SyntaxNet last week,a TensorFlow neural network framework for parsing natural language.\n\nOut of the box, the release includes Parsey McParseface \u2014 not to be confused with Boaty McBoatface, the British polar research ship \u2014 the most accurate English parser currently available (about 95% accurate).\n\nThe SyntaxNet release also includes all the code needed to train new models with your own data, too. Stay tuned: We\u2019ll be adding Parsey McParseface to Algorithmia in the coming weeks.\n\nThat\u2019s not all: Nest, the Google smart home device, released OpenThread this week, an open-sourced version of its Thread home automation networking protocol.\n\nThread, the low-power mesh network, is attempting to be the standard for connected devices, snag more IoT partners, and take on Amazon.\n\n\u201cAs more silicon providers adopt Thread, manufacturers will have the option of using a proven networking technology rather than creating their own, and consumers will have a growing selection of secure and reliable connected products to choose from,\u201d the company said in a press release.\n\nPLUS: Smart-home technology must work harder to create smarter consumers\n\nNextVR signed a five-year deal with Live Nation to offer up hundreds of concerts in virtual reality, leading some to ask: Is this be the non-gaming experience the industry has been waiting for?\n\nWith VR poised to revolutionize media content like concerts, the bad news is Gear VR will be the only headset to get these streaming concerts, though NextVR promises more platforms are coming soon (Gizmodo).\n\nNot only that, the current headsets need to evolve for VR to bring truly immersive experiences to the masses (TechCrunch).\n\nPLUS: Charlie Rose looks at VR technology and possibilities\n\nGoogle plans to digitally map the interiors of buildings in 3D through Project Tango.\n\nThe future of Google\u2019s Android platform lies with this project, which gives phones a new sense of their surroundings \u201cby adding an additional camera and more sophisticated sensors to a smartphone or tablet.\u201d\n\nThe cameras and sensors Tango provides will play a crucial role in Google\u2019s efforts to make its augmented reality platform \u201cubiquitous.\u201d\n\n\u201cFire up the application and point the device at a space and it sucks in images and depth information to re-create the environment on the screen and locates itself within that new digital realm.\u201d (Bloomberg).\n\nGoogle will showcase progress this week during their I/O developer conference.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come."
    },
    {
        "url": "https://medium.com/emergent-future/weekly-tech-news-roundup-ai-as-a-service-chatbots-oculus-rift-silicon-valley-and-more-880dac076442",
        "title": "Weekly Tech News Roundup: AI as a Service, Chatbots, Oculus Rift, Silicon Valley, and More",
        "text": "Issue 9\n\n In this issue we look at Facebook\u2019s internal A.I. platform, how companies not named FB can gain access to these same tool, why you shouldn\u2019t write-off chatbots, how to get your first look at the Oculus Rift, and what the hit HBO show \u2018Silicon Valley\u2019 is doing now.\n\nFacebook launched FBLearner Flow, an A.I. platform that ensures algorithms and machine learning models are developed once, and then implemented in a reusable manner. More than 25 percent of the FB engineering team shares and reuses code using this tool. For now, it\u2019s only available to engineers at FB.\n\nAlgorithmia has created a similar platform, with more than 2,000 algorithms in its library, open to all developers everywhere.\n\nThe crowd-sourced API has built a community around algorithm development, ensuring that all developers \u2014 not just those working at FB \u2014 have access to state-of-the-art algorithms for their applications.\n\n\u201cAlgorithms are being developed all the time, but are not getting into the hands of people and applications that could benefit from them,\u201d Diego Oppenheimer said, founder and CEO of Algorithmia.\n\nBoth FBLearner Flow, and Algorithmia provide the platform and tools needed to enable engineers to experiment with machine learning in a scalable, extensible way \u2014 any developer can build upon the work of others without ever having to setup or configure a server.\n\nPLUS: How the Algorithm Economy and Containers are Changing the Way We Build and Deploy Apps Today\n\nThanks to big data and advances in artificial intelligence, it\u2019s now possible to teach machines to understand and speak to humans.\n\nFacebook and Microsoft are betting big on a future full of people talking to intelligent chatbots.\n\nWith Facebook\u2019s Bot Engine, and Microsoft\u2019s Bot Framework, both companies are providing the tools, platform, and the \u201cprepackaged intelligence that give bots the ability to understand natural language, for example, or analyze and label images.\u201d\n\nThe recent obsession with chatbots is really just a reflection of an ongoing explosion in A.I. development. And, chatbots in particular represent both the \u201cmanifestation of humanity\u2019s biggest hopes and fears for technology\u201d (NY Times).\n\nShort-term, chatbots will disrupt how businesses engage with consumers, but long-term we\u2019re witnessing a larger paradigm shift in how people interact with machines.\n\n\u201cIt\u2019s clear to me [A.I.] is going to be the foundation for the next layer of programming,\u201d Eric Schmidt says, Alphabet (Google) chairman.\n\nThe next disruptive business will likely emerge from this \u201cpotent mix\u201d of A.I. running in the cloud (Financial Times).\n\nMicrosoft CEO Satya Nadella calls this \u201cconversation as a platform,\u201d suggesting it\u2019ll be more significant than what touchscreens did for smartphones.\n\nWhile most chatbots are mostly just a fancy interface hiding actual humans doing the work of reading emails, scheduling meetings, and ordering Chipotle \u2014 or Domino\u2019s pizza if that\u2019s what you\u2019re into \u2014 it\u2019s clear to Google\u2019s CEO Sundar Pichai that the future is A.I.\n\n\u201cWe\u2019re moving from a mobile-first, to an A.I.-first world.\u201d\n\nWhile chatbots might seem cute today, venture capitalist Chris Dixon points out: The next big thing always starts out looking like a toy.\n\nPLUS: The Complete Beginner\u2019s Guide To Chatbots\n\nYou might have heard: A small number of Oculus Rift headsets went on sale last week at Best Buy, and online at Amazon, and Microsoft.\n\nOculus originally started taking preorders for the Rift in January, but struggled to keep up with demand due to a component shortage, causing delays in fulfillment. The preorders were expected to ship in March.\n\nThis is the first time the Rift has been available for in-store purchase.\n\nBut did you know: You can get a hands-on demo of the Oculus Rift at select Best Buy stores? Head to Oculus Live to schedule a demo.\n\nSilicon Valley, the hit HBO show about the tech industry that blurs the lines between parody and reality, has taken things a step further:\n\nThey\u2019re now publishing satirical news stories into Google Search using a new tool called Post, revealing easter eggs from the show and more.\n\nGo ahead, search \u201cSilicon Valley\u201d or \u201cSilicon Valley HBO\u201d on Google. We\u2019ll wait.\n\nYou\u2019ll find \u201cnews\u201d about Stanford\u2019s damaged robots, Hooli\u2019s soaring stock price, and a CEO change at Pied Piper. Or, just check it all out here.\n\nHooli, of course, isn\u2019t just changing Silicon Valley; they\u2019re changing the world.\n\nIt\u2019s not the first time Google and the show have had fun together. Last year, when Google co-founder Larry Page unveiled the new parent company, Alphabet, he embedded a link to Hooli.xyz in theannouncement.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come."
    },
    {
        "url": "https://medium.com/emergent-future/accelerating-a-i-development-51fd6d726012",
        "title": "Accelerating A.I. Development \u2013 Emergent // Future \u2013",
        "text": "Artificial intelligence is set to hit the mainstream, thanks to improved machine learning tools, cheaper processing power, and a steep decline in the cost of cloud storage. As a result, firms are piling into the AI market, and pushing the pace of AI development across a range of fields.\n\n\u201cGiven sufficiently large datasets, powerful computers, and the interest of subject-area experts, the deep learning tsunami looks set to wash over an ever-larger number of disciplines.\u201c\n\nWhen it does, Nvidia will be there to capitalize. They announced a new chip design specifically for deep learning, with 15 billion transistors (a 3x increase) and the ability to process data 12x faster than previous chips.\n\n\u201cFor the first time we designed a [graphics-processing] architecture dedicated to accelerating AI and to accelerating deep learning.\u201c\n\nImproved hardware contributes to Facebook\u2019s ability to use artificial intelligence to describe photos to blind users, and it\u2019s why Microsoft can now build a JARVIS-like personal digital assistant for smartphones.\n\nGoogle, meanwhile, has its sights set on \u201csolving intelligence, and then using that to solve everything else,\u201d thanks to better processors and their cloud platform.\n\nThis kind of machine intelligence wouldn\u2019t be possible without improved algorithms.\n\n\u201cI consider machine intelligence to be the entire world of learning algorithms, the class of algorithms that provide more intelligence to a system as more data is added to the system,\u201d Shivon Zilis, creator of the machine intelligence framework, told Fast Forward Labs. \u201cThese are algorithms that create products that seem human and smart.\u201d\n\nIf you want to go deeper on the subject, O\u2019Reilly has a free ebook out on The Future of Machine Intelligence, which unpacks the \u201cconcepts and innovations that represent the frontiers of ever-smarter machines\u201d through ten interviews, spanning NLP, deep learning, autonomous cars, and more.\n\nSo, you might be wondering: Is the singularity near? Not at all, the NY Times says.\n\nWant more updates like this? Join Emergent Future, a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come."
    },
    {
        "url": "https://medium.com/emergent-future/spacex-tesla-and-elon-musk-s-wild-week-a3f1e0e482b0",
        "title": "SpaceX, Tesla, and Elon Musk\u2019s Wild Week \u2013 Emergent // Future \u2013",
        "text": "Last week SpaceX, the Elon Musk space company,successfully landed their Falcon 9 rocket on a drone ship for the first time last week.\n\nThe rocket landed vertically on a barge floating in choppy waters out in the Atlantic Ocean.\n\nBloomberg calls the landing a \u201cmoon walk\u201d moment for the New Space Age. Whereas Musk simply mused: it\u2019s just \u201canother step toward the stars.\u201d\n\nBut that\u2019s not all: Tesla, Musk\u2019s other company, received more than 325,000 preorders for their Model 3 last week, making it the \u201cbiggest one-week launch of any product.\u201d\n\nThe preorders represent $14 billion in potential sales for the $35,000 electric car, which is expected to ship late-2017. The unprecedented demand is a \u201cwatershed moment\u201d for electric vehicles, the WSJ says.\n\nAs electric vehicles go mainstream, speculation mounts that the petrol car could be dead as early as 2025. We are now witnessing the slow-motion disruption of the global auto industry, Quartz adds. At least one car maker has taken notice: GM has invested $500 million in Lyft, with plans to build self-driving electric cars.\n\nWant more updates like this? Consider joining Emergent Future, a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come."
    },
    {
        "url": "https://medium.com/emergent-future/going-mainstream-integrating-machine-learning-in-the-cloud-244038078423",
        "title": "Going Mainstream: Integrating Machine Learning in the Cloud",
        "text": "You might have heard: Google unveiled their new machine learning platform to \u201cpour ML all over the cloud.\u201d The moves makes TensorFlow available to developers to do machine learning in the cloud with their own data.\n\nMachine learning is developing fast, and \u201cwhat will distinguish the good companies from the rest are things like domain expertise, quality of the dataset, and the ability to find the right problems to solve.\u201d The Economist adds, \u201cthe firms that develop an early edge in artificial intelligence may reap the greatest rewards and erect barriers to entry.\u201d\n\n\u201cAt its highest level, machine learning is about understanding the world through data,\u201d said Geoffrey Gordon, acting chair of Carnegie Mellon University\u2019s Machine Learning Department. \u201cAnything you can think of \u2014 public policy, finance, automobiles and robotics, for example \u2014 there\u2019s a role for machine learning.\u201d\n\nWhile algorithm development is still broken, Coursera\u2019s new Data Science Masters program is a step in the right direction. And, here are the top machine learning books for data scientists and machine learning engineers.\n\nConfused about ML? Here\u2019s how to approach machine learning as a non-technical person.\n\nDid you enjoy this? Consider joining Emergent Future, a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come."
    },
    {
        "url": "https://medium.com/emergent-future/spam-detection-using-neural-networks-in-python-9b2b2a062272",
        "title": "Spam detection using neural networks in Python \u2013 Emergent // Future \u2013",
        "text": "Neural networks are powerful machine learning algorithms. They can be used to transform the features so as to form fairly complex non linear decision boundaries. They are primarily used for classification problems.\n\nA common classification problem is that of spam identification. In this problem, we are given a bunch of emails (in raw form or in processed form) and we are also given labels of those emails (spam or no spam). Then, we are given a set of new emails (test data) and we have to label each email in the test set as spam or no spam.\n\nYou can read about Neural Networks and how they work on Wikipedia\n\nWe have created a 3 layer neural network for spam detection:\n\nThe input layer takes in the 57 features of the email as a vector and passes it to the middle layer. Finally, the output layer outputs a real number in the interval (0, 1) which in some sense serves as a probability of the mail being a spam.\n\nHere is a link to pre-processed email dataset (make sure to save it with the name \u2018Train.csv\u2019)\n\nTo get an idea of what each column in the dataset means, have a look at this link\n\nHere is the Python code for our spam detector:\n\nWe have used the standard back propagation algorithm for training the neural network\n\nThis simple algorithm achieves an accuracy of 90%, which is a great start!"
    },
    {
        "url": "https://medium.com/emergent-future/reality-check-is-augmented-and-virtual-reality-ready-for-prime-time-8e8e0ebb91eb",
        "title": "Reality Check: Is Augmented and Virtual Reality Ready for Prime Time?",
        "text": "Virtual reality and augmented reality are on the cusp of going mainstream. Facebook\u2019s Oculus Rift, HTC Vive, and Sony\u2019s PlayStation VR will all release VR headsets this year.\n\nNot to mention HoloLens, Microsoft\u2019s slick augmented reality headset that \u201cblends computer-generated imagery with the real world.\u201d It\u2019s already shipping. The Oculus Rift deliveries have been slowed by a component shortage.\n\nIn an Oculus vs HoloLens showdown, TechCrunch says, \u201cThere\u2019s no way of explaining how fast you\u2019ll get used to the world suddenly having a layer of data over it.\u201d\n\nA new type of \u201cconsole war\u201d is emerging, with companies racing to build market share among early adopters. You can understand why: Bloomberg reports that VR could turn into a $1.5 billion business.\n\nThat is, of course, if developers can \u201ccraft immersive content that educates, informs or entertains\u201d to meet customer demand.\n\nAnd, what does VR mean for art institutions anyway?\n\nDid you enjoy this? Consider joining Emergent Future, a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come.\n\nEmergent Future is powered by Algorithmia, an open marketplace for algorithms, enabling developers to create tomorrow\u2019s smart applications today"
    },
    {
        "url": "https://medium.com/emergent-future/artificial-intelligence-deep-learning-and-the-arms-race-to-control-tech-s-future-58bc06f7e10e",
        "title": "Artificial Intelligence, Deep Learning, and the Arms Race to Control Tech\u2019s Future",
        "text": "Artificial Intelligence represents the next chapter of the Information Age, and Google, Microsoft, Amazon, IBM, and others are engaging in an arms race to control the platform that dictate tech\u2019s future writes the New York Times. \u201cThe relationship between big companies and deep machine intelligence is just starting.\u201d\n\nSo, what counts as artificially intelligent anyway? The Verge explains the difference between machine learning, deep learning, and neural networks, how they work, and why the future of AI is likely to be more subtle than you think. The next wave in technology isn\u2019t about the technology, but rather the market that emerges from the technology.\n\nDeep learning is \u201ca killer technology,\u201d Nvidia\u2019s CEO says in a Fortune article that covers all things AI, data centers, autonomous vehicles, and more. While AI is still in its infancy, the road to super intelligent apps and machines is starting to come into focus.\n\nWhen you\u2019re ready to deeply learn more, this eBook will teach you how to use deep learning to solve problems related to image recognition, speech recognition, and natural language processing.\n\ntl;dr A Googler explains deep learning in just one minute\n\nDid you enjoy this? Consider joining Emergent Future, a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Emergent Future is powered by Algorithmia, an open marketplace for algorithms, enabling developers to create tomorrow\u2019s smart applications today"
    },
    {
        "url": "https://medium.com/emergent-future/terminating-tay-a-microsoft-ai-experiment-gone-wrong-73b3071683ff",
        "title": "Terminating Tay \u2014 A Microsoft AI Experiment Gone Wrong",
        "text": "You Might Have Heard: The Microsoft AI experiment with Tay, their machine learning Twitter bot, ended after a mere 24-hours. The company pulled the plug when she almost immediately turned into a sexist, racist Nazi. Tay was suppose to learn how to communicate like a human by engaging in conversations with Twitter users.\n\n\u201cThis gets to the underlying problem,\u201d Vice argues. \u201cMicrosoft\u2019s AI developers sent Tay to the internet to learn how to be human, but the internet is a terrible place to figure that out.\u201d\n\nThe New Yorker writes that \u201cTay\u2019s breakdown occurred at a moment of enormous promise for A.I.\u201d Earlier in the week, an AI-written novel passed the first round of a literary competition in Japan, and last week AlphaGo, the AI from Google\u2019s DeepMind,defeated the top-ranked Go player in the world.\n\nAs information destined for humans is increasingly handled by AI\u2019s, the need for an open dialogue about the ethics grows. Google and DeepMind still haven\u2019t revealed who sits on their AI ethics board.\n\n+ A question of lesser importance: why are AI\u2019s like Siri and Cortana so clever, but so bad at empathy anyway? A recent study might hold the key.\n\nDid you enjoy this? Consider joining Emergent Future, a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Emergent Future is powered by Algorithmia, an open marketplace for algorithms, enabling developers to create tomorrow\u2019s smart applications today"
    },
    {
        "url": "https://medium.com/emergent-future/how-algorithms-running-as-microservices-change-software-development-7b0c37b358be",
        "title": "How Algorithms Running as Microservices Change Software Development",
        "text": "The fundamental shift of container technology, the algorithm economy, and algorithms packaged as microservices creates an environment where rapid prototyping has never been easier due to a reduction in the infrastructure needed to build and deploy apps.\n\nLiked this? Check out Algorithmia, a Seattle-based startup building an open marketplace for algorithms."
    },
    {
        "url": "https://medium.com/emergent-future/alphago-s-historic-victory-the-brain-vs-deep-learning-and-more-from-the-department-of-bots-497d0e865b6",
        "title": "AlphaGo\u2019s Historic Victory, The Brain vs Deep Learning, and more from the Department of Bots",
        "text": "Welcome to Emergent Future, a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.\n\nYou may have heard about AlphaGo: Go has officially fallen to machines, just like Jeopardy did before it to Watson, and chess before that to Deep Blue. Now that artificial intelligence has mastered Go, New Scientist asks what game should it take on next. Deep-Q is learning not only Pong with Tensorflow and PyGame, but also Flappy Bird. If that wasn\u2019t enough, here\u2019s a timeline of artificial intelligence victories from 1997\u20133041. You read that correctly.\n\nDepartment of Bots\n\n Motherboard argues that joke-telling robots represent the final frontier of A.I., since humor requires self-awareness, spontaneity, linguistic sophistication, and empathy. That\u2019s not an easy task for a bot. Speaking of, why do developers love chatbots so much? Facebook\u2019s Messenger Bot Store is coming, and it could be the most important launch since the App Store. We\u2019re believers, but will robots take your job?\n\nThe Brain vs Deep Learning\n\n Want to know why the singularity is anywhere but near? Read this great examination of the brain\u2019s electrochemical and biological information processing pipeline as it relates to deep learning. There are a few problems with consciousness as it relates to superintelligence. The DeepMind founder has plans beyond just Go. He\u2019s designing for healthcare, robots, and your phone. Use Neural Doodle to turn your two-bit doodles into fine artworks with deep neural networks. Very cool.\n\nPoint/Counterpoint\n\n Harvard Business Review argues that you need an algorithm, not a data scientist. Not so fast, says Data Science Central. You need a data scientist, and then an algorithm. But, what you\u2019re really looking for is the Algorithm Economy.\n\nDebunking A.I. Myths\n\n Thanks to the pioneering work of scientists, a clearer picture is emerging about A.I., and the most common misconceptions and myths. These are the 7 biggest myths about A.I., and 17 predictions about the future of big data.\n\nThe Internet of (Broken) Things\n\n A security expert hacked a hotel\u2019s Android-based light-switch tablet, and then gained control to the electronics in every single room. Oof. This is going to be a continual challenge for companies as they integrate digital technologies in meaningful ways to enhance homes and improve their lives. Here\u2019s your chance to meet the 10 pigeons(!) live tweeting London\u2019s air pollution. Oh, and by the way, they\u2019re wearing tiny backpacks."
    },
    {
        "url": "https://medium.com/emergent-future/the-emergent-future-and-the-shape-of-things-to-come-9f7694c1136a",
        "title": "The Emergent Future and the Shape of Things to Come",
        "text": "We\u2019ve started a newsletter called the Emergent Future. It\u2019s a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. \u200bEF is published every Tuesday and is powered by Algorithmia."
    },
    {
        "url": "https://medium.com/emergent-future/how-the-algorithm-economy-and-containers-are-changing-the-way-we-build-and-deploy-apps-today-4ecdbb59318d",
        "title": "How the Algorithm Economy and Containers are Changing the Way We Build and Deploy Apps Today",
        "text": "In the age of Big Data, algorithms give companies a competitive advantage. Today\u2019s most important technology companies all have algorithmic intelligence built into the core of their product: Google Search, Facebook News Feed, Amazon\u2019s and Netflix\u2019s recommendation engines.\n\n\u201cData is inherently dumb,\u201d Peter Sondergaard, senior vice president at Gartner and global head of Research, said in The Internet of Things Will Give Rise To The Algorithm Economy. \u201cIt doesn\u2019t actually do anything unless you know how to use it.\u201d\n\nGoogle, Facebook, Amazon, Netflix and others have built both the systems needed to acquire a mountain of data (i.e. search history, engagement metrics, purchase history, etc), as well as the algorithms responsible for extracting actionable insights from that data. As a result, these companies are using algorithms to create value, and impact millions of people a day.\n\n\u201cAlgorithms are where the real value lies,\u201d Sondergaard said. \u201cAlgorithms define action.\u201d\n\nFor many technology companies, they\u2019ve done a good job of capturing data, but they\u2019ve come up short on doing anything valuable with that data. Thankfully, there are two fundamental shifts happening in technology right now that are leading to the democratization of algorithmic intelligence, and changing the way we build and deploy smart apps today:\n\nThe confluence of the algorithm economy and containers creates a new value chain, where algorithms as a service can be discovered and made accessible to all developers through a simple REST API. Algorithms as containerized microservices ensure both interoperability and portability, allowing for code to be written in any programming language, and then seamlessly united across a single API.\n\nBy containerizing algorithms, we ensure that code is always \u201con,\u201d and always available, as well as being able to auto-scale to meet the needs of the application, without ever having to configure, manage, or maintain servers and infrastructure. Containerized algorithms shorten the time for any development team to go from concept, to prototype, to production-ready app.\n\nAlgorithms running in containers as microservices is a strategy for companies looking to discover actionable insights in their data. This structure makes software development more agile and efficient. It reduces the infrastructure needed, and abstracts an application\u2019s various functions into microservices to make the entire system more resilient.\n\nThe \u201calgorithm economy\u201d is a term established by Gartner to describe the next wave of innovation, where developers can produce, distribute, and commercialize their code. The algorithm economy is not about buying and selling complete apps, but rather functional, easy to integrate algorithms that enable developers to build smarter apps, quicker and cheaper than before.\n\nAlgorithms are the building blocks of any application. They provide the business logic needed to turn inputs into useful outputs. Similar to Lego blocks, algorithms can be stacked together in new and novel ways to manipulate data, extract key insights, and solve problems efficiently. The upshot is that these same algorithms are flexible, and easily reused and reconfigured to provide value in a variety of circumstances.\n\nFor example, we created a microservice at Algorithmia called Analyze Tweets, which searches Twitter for a keyword, determining the sentiment and LDA topics for each tweet that matches the search term. This microservice stacks our Retrieve Tweets With Keywords algorithm with our Social Sentiment Analysis and LDA algorithms to create a simple, plug-and-play utility.\n\nThe three underlying algorithms could just as easily be restacked to create a new use case. For instance, you could create an Analyze Hacker News microservice that uses the Scrape Hacker News and URL2Text algorithms to extract the text for the top HN posts. Then, you\u2019d simply pass the text for each post to the Social Sentiment Analysis, and LDA algorithms to determine the sentiment and topics of all the top posts on HN.\n\nThe algorithm economy also allows for the commercialization of world class research that historically would have been published, but largely under-utilized. In the algorithm economy, this research is turned into functional, running code, and made available for others to use. The ability to produce, distribute, and discover algorithms fosters a community around algorithm development, where creators can interact with the app developers putting their research to work.\n\nAlgorithm marketplaces function as the global meeting place for researchers, engineers, and organizations to come together to make tomorrow\u2019s apps today.\n\nContainers are changing how developers build and deploy distributed applications. In particular, containers are a form of lightweight virtualization that can hold all the application logic, and run as an isolated process with all the dependencies, libraries, and configuration files bundled into a single package that runs in the cloud.\n\n\u201cInstead of making an application or a service the endpoint of a build, you\u2019re building containers that wrap applications, services, and all their dependencies,\u201d Simon Bisson at InfoWorld said in How Containers Change Everything. \u201cAny time you make a change, you build a new container; and you test and deploy that container as a whole, not as an individual element.\u201d\n\nContainers create a reliable environment where software can run when moved from one environment to another, allowing developers to write code once, and run it in any environment with predictable results \u2014 all without having to provision servers or manage infrastructure.\n\nThis is a shot across the bow for large, monolithic code bases. \u201c[Monoliths are] being replaced by microservices architectures, which decompose large applications \u2014 with all the functionality built-in \u2014 into smaller, purpose-driven services that communicate with each other through common REST APIs,\u201d Lucas Carlson from InfoWorld said in 4 Ways Docker Fundamentally Changes Application Development.\n\nThe hallmark of microservice architectures is that the various functions of an app are unbundled into a series of decentralized modules, each organized around a specific business capability.\n\nMartin Fowler, the co-author of the Agile Manifesto, describes microservices as \u201can approach to developing a single application as a suite of small services, each running in its own process and communicating with lightweight mechanisms, often an HTTP resource API.\u201d\n\nBy decoupling services from a monolith, each microservice becomes independently deployable, and acts as a smart endpoint of the API. \u201cThere is a bare minimum of centralized management of these services,\u201d Fowler said in Microservices: A Definition of this New Architectural Term, \u201cwhich may be written in different programming languages and use different data storage technologies.\u201d\n\nSimilar to the algorithm economy, containers are like Legos for cloud-based application development. \u201cThis changes cloud development practices,\u201d Carlson said, \u201cby putting larger-scale architectures like those used at Facebook and Twitter within the reach of smaller development teams.\u201d"
    },
    {
        "url": "https://medium.com/emergent-future/algorithm-development-is-broken-187fd6e31559",
        "title": "Algorithm Development is Broken \u2013 Emergent // Future \u2013",
        "text": "Highly complex algorithm development is often at the core of innovative software products. The most famous algorithm of the last decade led to the development of the $396 billion dollar titan Google, which changed our lives forever with a simple search box.\n\nToday, an estimated 80% of equity trading in the U.S. is done using automated algorithms. Even our phones use algorithms to figure out where we are, what we are doing, and what we might want to do next. A few days ago a company, DeepMind, that developed the next generation of Artificial Intelligence algorithms was bought for a record sum. These examples are just scratching the surface of what algorithms can do.\n\nAs the volumes of data we collect on a day-to-day basis grow, data architects are on a race to have more, better, and faster hardware; yet Moore\u2019s Law is showing a slow down, indicating that throwing more hardware at a data problem is no longer going to fulfill the needs of organizations. As organizations are increasingly and continuously hungry for understanding on how they operate and how they can improve, they must concentrate more on the efficiency and quality of the underlying algorithms that compose suites of data analysis packages versus the hardware they run on.\n\nIt\u2019s safe to say that on any given day there are thousands of brilliant computer scientists developing the state of the art and pushing the limits of software, yet there lies a serious problem: Algorithms are being developed all the time, but are not getting into the hands of people and applications that could benefit from them.\n\nFor example, a while back I was researching Latent Dirichlet Allocation. In layman\u2019s terms, it is an algorithm that allows for the extraction of topics from documents without an understanding of the language itself. A quick search on Google brought up a dozen research papers and a couple of libraries where this algorithm had been implemented. So where do I start? I guess I could go ahead and implement one of the libraries, but which one? Do I even know if this is going to work? Once I do figure it out will it work in my current system?\n\nAlgorithmia was born out of frustration with these problems and the current state of algorithm development and deployment. Algorithmia is moving away from developers working in isolation and toward providing a community for algorithm developers to share knowledge, test algorithms, and run them directly in their applications. What\u2019s different is that every algorithm is live, so no more spending time finding the right libraries, compilers, and/or virtual machines\u008d \u2014 Algorithmia takes care of all of that for you. Find a useful algorithm in our API? Great, use that in your own applications.\n\nAlgorithmia is a live, crowd-sourced algorithm API. Our goal with Algorithmia is to make applications smarter, by building a community around algorithm development, where state-of-the-art algorithms are always live and accessible to anyone.\n\nIntrigued? Come check us out."
    }
]