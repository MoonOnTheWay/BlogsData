[
    {
        "url": "https://towardsdatascience.com/yet-another-twitter-sentiment-analysis-part-1-tackling-class-imbalance-4d7a7f717d44?source=user_profile---------1----------------",
        "title": "Yet Another Twitter Sentiment Analysis Part 1 \u2014 tackling class imbalance",
        "text": "I finished an 11-part series blog posts on Twitter sentiment analysis not long ago. Why do I want to do the sentiment analysis again? I wanted to extend further and run sentiment analysis on real retrieved tweets. And there are other limits to my previous sentiment analysis project.\n\nRegarding neutral class, it might be possible to set a threshold value for negative, neutral, positive class, and map the final output probability value to one of three classes, but I wanted to train a model with training data, which has three sentiment classes: negative, neutral, positive.\n\nSince I already wrote quite a lengthy series on NLP, sentiment analysis, if a concept was already covered in my previous posts, I won\u2019t go into the detailed explanation. And also the main data visualisation will be with retrieved tweets, and I won\u2019t go through extensive data visualisation with the data I use for training and testing a model.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nIn order to train my sentiment classifier, I need a dataset which meets conditions below.\n\nWhile googling to find a good data source, I learned about renowned NLP competition called SemEval. \u201cSemEval (Semantic Evaluation) is an ongoing series of evaluations of computational semantic analysis systems, organized under the umbrella of SIGLEX, the Special Interest Group on the Lexicon of the Association for Computational Linguistics.\u201d\n\nYou might have already heard of this if you\u2019re interested in NLP. Highly-skilled teams from all around the world compete on a couple of tasks such as \u201csemantic textual similarity\u201d, \u201cmultilingual semantic word similarity\u201d, etc. One of the competition tasks is the Twitter sentiment analysis. It also has a couple of subtasks, but what I would want to focus on is \u201cSubtask A. : Message Polarity Classification: Given a message, classify whether the message is of positive, negative, or neutral sentiment\u201d.\n\nLuckily the dataset they provide for the competition is available to download. The training data consists of SemEval\u2019s previous training and test data. What\u2019s even better is they provide test data, and all the teams who participated in the competition are scored with the same test data. This means I can compare my model performance with 2017 participants in SemEval.\n\nThere are 11 txt files in total, spanning from SemEval 2013 to SemEval 2016. While trying to read the files into a Pandas dataframe, I found two files cannot be properly loaded as tsv file. It seems like there are some entries not properly tab-separated, so end up as a chunk of 10 or more tweets stuck together. I could have tried retrieving them with tweet ID provided, but I decided to first ignore these two files, and make up a training set with only 9 txt files.\n\nOnce I import basic dependencies, I\u2019ll read the data to a Pandas dataframe.\n\nThe dataset looks fairly simple with individual tweet ID, sentiment label, and tweet text.\n\nThere are total 41,705 tweets. As another sanity check, let\u2019s take a look at how many words are there in each tweet.\n\nOK, the token length looks fine, and the tweet for maximum token length seems like a properly parsed tweet. Let\u2019s take a look at the class distribution of the data.\n\nThe data is not well balanced, and negative class has the least number of data entries with 6,485, and the neutral class has the most data with 19,466 entries. I want to rebalance the data so that I will have a balanced dataset at least for training. I will deal with this after I define the cleaning function.\n\nData cleaning process is similar to my previous project, but this time I added a long list of contraction to expand most of the contracted form to its original form such as \u201cdon\u2019t\u201d to \u201cdo not\u201d. And this time, instead of Regex, I used Spacy to parse the documents, and filtered numbers, URL, punctuation, etc. Below are the steps I took to clean the tweets.\n\nOK now let\u2019s see how this custom cleaner works with tweets.\n\nIt looks like it\u2019s doing what I intended it to do. I\u2019ll clean the \u201ctext\u201d column and create a new column called \u201cclean_text\u201d.\n\nBy running the cleaning function I can see it encountered some \u201cinvalid escape sequence\u201d. Let\u2019s see what these are.\n\nThe tweets that contain \u2018\\m\u2019 were actually containing an emoticon \u2018\\m/\u2019 I didn\u2019t know about this until I googled it. Apparently \u2018\\m/\u2019 stands for the horn sign you make with your hand. This hand sign is popular in metal music. Anyway, this is just a warning and it is not an error. Let\u2019s see how the cleaner deals with this.\n\nAgain it seems like to be doing what I intended it to do. So far so good.\n\n\u201cThe class imbalance problem typically occurs when, in a classification problem, there are many more instances of some classes than others. In such cases, standard classifiers tend to be overwhelmed by the large classes and ignore the small ones.\u201d\n\nAs I have already realised, the training data is not perfectly balanced, \u2018neutral\u2019 class has 3 times more data than \u2018negative\u2019 class, and \u2018positive\u2019 class has around 2.4 times more data than \u2018negative\u2019 class. I will try fitting a model with three different data; oversampled, downsampled, original, to see how different sampling techniques affect the learning of a classifier.\n\nThe simple default classifier I\u2019ll use to compare performances of different datasets will be the logistic regression. From my previous sentiment analysis project, I learned that Tf-Idf with Logistic Regression is a pretty powerful combination. Before I apply any other more complex models such as ANN, CNN, RNN etc, the performances with logistic regression will hopefully give me a good idea of which data sampling methods I should choose. If you want to know more about Tf-Idf, and how it extracts features from text, you can check my old post, \u201cAnother Twitter Sentiment Analysis with Python-Part5\u201d.\n\nIn terms of validation, I will use K-Fold Cross Validation. In my previous project, I split the data into three; training, validation, test, and all the parameter tuning was done with reserved validation set and finally applied the model to the test set. Considering that I had more than 1 million data for training, this kind of validation set approach was acceptable. But this time, the data I have is much smaller (around 40,000 tweets), and by leaving out validation set from the data we might leave out interesting information about data.\n\nWith data as it is without any resampling, we can see that the precision is higher than the recall. If you want to know more about precision and recall, you can check my old post, \u201cAnother Twitter sentiment analysis with Python \u2014 Part4\u201d.\n\nIf we take a closer look at the result from each fold, we can also see that the recall for the negative class is quite low around 28~30%, while the precisions for the negative class are high as 61~65%. This means the classifier is very picky and does not think many things are negative. All the text it classifies as negative is 61~65% of the time really negative. However, it also misses a lot of actual negative class, because it is so very picky. We have a low recall, but a very high precision. The intuition behind this precision and recall has been taken from a Medium blog post by Andreas Klintberg.\n\nThere is a very useful Python package called \u201cimbalanced-learn\u201d, which helps you deal with class imbalance issues, it is compatible with Scikit Learn, and easy to implement.\n\nWithin imbalanced-learn, there are different techniques you can use for oversampling. I will use below two.\n\nThere is one more point to consider if you are cross-validating with oversampled data. Oversampling the minority class can result in overfitting problems if we oversample before cross-validating. Why is that so? Because by oversampling before cross validation split, you are leaking the information of validation data already to your training set. As they say \u201cWhat has been seen, cannot be unseen.\u201d\n\nIf you want more detailed explanation, I recommend this Youtube video \u201cMachine Learning \u2014 Over-& Undersampling \u2014 Python/ Scikit/ Scikit-Imblearn\u201d\n\nLuckily cross-validation function I defined above as \u201clr_cv()\u201d will fit the pipeline only with the training set split after cross-validation split, thus it is not leaking any information of validation set to the model.\n\nRandom over-sampling is simply a process of repeating some samples of the minority class and balance the number of samples between classes in the dataset.\n\nBefore we fit each pipeline, let\u2019s see what the RadomOverSampler does. In order to make it easier to see I defined some toy text data below, and the target sentiment value for each text.\n\nMy toy data has 5 entries in total, and the target sentiments are three positives and two negatives. In order to be balanced, this toy data needs one more entry of negative class.\n\nOne thing is over sampler won\u2019t be able to handle raw text data. It has to be transformed into a feature space for over sampler to work. I\u2019ll first fit TfidfVectorizer, and oversample using Tf-Idf representation of texts.\n\nBy running RandomOverSampler, now we have one more entry at the end. The last entry added by RandomOverSampler is exactly same as the fourth one (index number 3) from the top. RandomOverSampler simply repeats some entries of the minority class to balance the data. If we look at the target sentiments after RandomOverSampler, we can see that it has now a perfect balance between classes by adding on more entry of negative class.\n\nCompared to the model built with original imbalanced data, now the model behaves in opposite way. The precisions for the negative class are around 47~49%, but the recalls are way higher at 64~67%. Now we have a situation of high recall, low precision. What this means is that the classifier thinks a lot of things are negative. However, it also thinks a lot of non-negative texts are negative. So from our set of data we got a lot of texts classified as negative, many of them were in the set of actual negative, however, a lot of them were also non-negative.\n\nBut without resampling, the recall rate was as low as 28~30% for negative class, the precision rate for the negative class I get from oversampling is more robust at around 47~49%.\n\nAnother way to look at it is to look at the f1 score, which is the harmonic average of precision and recall. The original imbalanced data had 66.51% accuracy and 60.01% F1 score. However with oversampling, we get a slightly lower accuracy of 65.95%, but a much higher F1 score of 64.18%\n\nSMOTE is an over-sampling approach in which the minority class is over-sampled by creating \u201csynthetic\u201d examples rather than by over-sampling with replacement.\n\nAccording to the original research paper \u201cSMOTE: Synthetic Minority Over-sampling Technique\u201d (Chawla et al., 2002), \u201csynthetic samples are generated in the following way: Take the difference between the feature vector (sample) under consideration and its nearest neighbour. Multiply this difference by a random number between 0 and 1, and add it to the feature vector under consideration. This causes the selection of a random point along the line segment between two specific features. This approach effectively forces the decision region of the minority class to become more general.\u201d What this means is that when SMOTE creates a new synthetic data, it will choose one data to copy, and look at its k nearest neighbours. Then, on feature space, it will create random values in feature space that is between the original sample and its neighbours.\n\nOnce you see the example with the toy data, it will become clearer.\n\nThe last entry is the data created by SMOTE. To make it easier to see, let\u2019s see only the negative class.\n\nThe top two entries are original data, and the one on the bottom is synthetic data. You can see it didn\u2019t just repeat original data. Instead, the Tf-Idf values are created by taking random values between the top two original data. As you can see, if the Tf-Idf values for both original data are 0, then synthetic data also has 0 for those features, such as \u201cadore\u201d, \u201ccactus\u201d, \u201ccats\u201d, because if two values are the same there are no random values between them. I specifically defined k_neighbors as 1 for this toy data, since there are only two entries of negative class, if SMOTE chooses one to copy, then only one other negative entry left as a neighbour.\n\nNow let\u2019s fit the SMOTE pipeline to see how it affects performance.\n\nSMOTE sampling seems to have a slightly higher accuracy and F1 score compared to random oversampling. With the results so far, it seems like choosing SMOTE oversampling is preferable over original or random oversampling.\n\nHow about downsampling. If we oversample the minority class in the above oversampling, with downsampling, we try to reduce the data of majority class, so that the data classes are balanced.\n\nAgain, before we run the pipeline, let\u2019s apply this to the toy data to see what it does.\n\nCompared with the original imbalanced data, we can see that downsampled data has one less entry, which is the last entry of the original data belonging to the positive class. RandomUnderSampler reduces the majority class by randomly removing data from the majority class.\n\nNow the accuracy and the F1 score has significantly dropped. But the characteristic of low precision and high recall is as same as oversampled data. Only its overall performance dropped.\n\nAccording to the documentation of \u201cimbalanced-learn\u201d, \u201cNearMiss adds some heuristic rules to select samples. NearMiss implements 3 different types of heuristic which can be selected with the parameter version. NearMiss heuristic rules are based on nearest neighbors algorithm.\u201d\n\nThere is also a good paper on resampling techniques. \u201cSurvey of resampling techniques for improving classification performance in unbalanced datasets\u201d (Ajinkya More, 2016)\n\nI borrowed the explanation of three different versions of NearMiss from More\u2019s paper.\n\nIn NearMiss-1, those points from majority class are retained whose mean distance to the k nearest points in minority class is lowest. Which means it will keep the points of majority class that\u2019s similar to the minority class.\n\nWe can see that NearMiss-1 has eliminated the entry for the text \u201cI adore cats\u201d, which makes sense because both words \u201cadore\u201d and \u201ccats\u201d are only appeared in this entry, so makes it the most different from minority class in terms of Tf-Idf representation in feature space.\n\nIt seems like both the accuracy and F1 score got worse than random undersampling.\n\nIn contrast to NearMiss-1, NearMiss-2 keeps those points from the majority class whose mean distance to the k farthest points in minority class is lowest. In other words, it will keep the points of majority class that\u2019s most different to the minority class.\n\nNow we can see that NearMiss-2 has eliminated the entry for the text \u201cI like dogs\u201d, which again makes sense because we also have a negative entry \u201cI don\u2019t like dogs\u201d. Two entries are in different classes but they share two same tokens \u201clike\u201d and \u201cdogs\u201d.\n\nBoth accuracy and F1 score got even lower compared to NearMiss-1. And we can also see that all the metrics fluctuate from fold to fold quite a lot.\n\nThe final NearMiss variant, NearMiss-3 selects k nearest neighbours in majority class for every point in the minority class. In this case, the undersampling ratio is directly controlled by k. For example, if we set k to be 4, then NearMiss-3 will choose 4 nearest neighbours of every minority class entry.\n\nThen we\u2019ll end up with either more or fewer samples of majority class than minority class depending on n neighbours we set. For example, with my dataset, if I run NearMiss-3 with default n_neighbors_ver3 of 3, it will complain and the number of neutral class(which is majority class in my dataset) will be smaller than negative class(which is minority class in my dataset). So I explicitly set n_neighbors_ver3 to be 4, so that I\u2019ll have enough majority class data at least the same number as the minority class.\n\nOne thing I\u2019m not completely sure is that what kind of filtering it applies when all the data selected with n_neighbors_ver3 parameter is more than the minority class. As you will see below, after applying NearMiss-3, the dataset is perfectly balanced. However, if the algorithm simply chooses the nearest neighbour according to the n_neighbors_ver3 parameter, I doubt that it will end up with the exact same number of entries for each class.\n\nNearMiss-3 produced the most robust result within NearMiss family, but slightly lower than RandomUnderSampling.\n\n5-fold cross validation result (classifier used for validation: logistic regression with default setting)\n\nBased on the above result, the sampling technique I\u2019ll be using for the next post will be SMOTE. In the next post, I will try different classifiers with SMOTE oversampled data.\n\nThank you for reading and you can find the Jupyter Notebook from the below link:"
    },
    {
        "url": "https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35?source=user_profile---------2----------------",
        "title": "Sentiment Analysis with PySpark \u2013",
        "text": "One of the tools I\u2019m deeply interested but haven\u2019t had many chances to explore is Apache Spark. Most of the time, Pandas and Scikit-Learn is enough to handle the size of data I\u2019m trying to build a model on. But that also means that I haven\u2019t had a chance to deal with petabytes of data yet, and I want to be prepared for the case I\u2019m faced with a real big-data.\n\nI have tried some basic data manipulation with PySpark before, but only to a very basic level. I want to learn more and be more comfortable in using PySpark. This post is my endeavour to have a better understanding of PySpark.\n\nPython is great for data science modelling, thanks to its numerous modules and packages that help achieve data science goals. But what if the data you are dealing with cannot be fit into a single machine? Maybe you can implement careful sampling to do your analysis on a single machine, but with distributed computing framework like PySpark, you can efficiently implement the task for large datasets.\n\nSpark API is available in multiple programming languages (Scala, Java, Python and R). There are debates about how Spark performance varies depending on which language you run it on, but since the main language I have been using is Python, I will focus on PySpark without going into too much detail of what language should I choose for Apache Spark.\n\nSpark has three different data structures available through its APIs: RDD, Dataframe (this is different from Pandas data frame), Dataset. For this post, I will work with Dataframe, and the corresponding machine learning library SparkML. I first decided on the data structure I would like to use based on the advice from the post in Analytics Vidhya. \u201cDataframe is much faster than RDD because it has metadata (some information about data) associated with it, which allows Spark to optimize query plan.\u201d You can find a comprehensive introduction from the original post.\n\nAnd there\u2019s also an informative post on Databricks comparing different data structures of Apache Spark: \u201cA Tale of Three Apache Spark APIs: RDDs, DataFrames, and Datasets\u201d.\n\nThen I figured out that I need to use SparkML instead SparkMLLib if I want to deal with Dataframe. SparkMLLib is used with RDD, while SparkML supports Dataframe.\n\nOne more thing to note is that I will work in local mode with my laptop. The local mode is often used for prototyping, development, debugging, and testing. However, as Spark\u2019s local mode is fully compatible with the cluster mode, codes written locally can be run on a cluster with just a few additional steps.\n\nIn order to use PySpark in Jupyter Notebook, you should either configure PySpark driver or use a package called Findspark to make a Spark Context available in your Jupyter Notebook. You can easily install Findspark by \u201cpip install findspark\u201d on your command line. Let\u2019s first load some of the basic dependencies we need.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nFirst step in any Apache programming is to create a SparkContext. SparkContext is needed when we want to execute operations in a cluster. SparkContext tells Spark how and where to access a cluster. It is first step to connect with Apache Cluster.\n\nThe dataset I\u2019ll use for this post is annotated Tweets from \u201cSentiment140\u201d. It originated from a Stanford research project, and I used this dataset for my previous series of Twitter sentiment analysis. Since I already cleaned the tweets during the process of my previous project, I will use pre-cleaned tweets. If you want to know more in detail about the cleaning process I took, you can check my previous post: \u201cAnother Twitter sentiment analysis with Python-Part 2\u201d .\n\nAfter successfully loading the data as Spark Dataframe, we can take a peek at the data by calling .show(), which is equivalent to Pandas .head(). After dropping NA, we have a bit less than 1.6 million Tweets. I will split this into three parts; training, validation, test. Since I have around 1.6 million entries, 1% each for validation and test set will be enough to test the models.\n\nThrough my previous attempt at sentiment analysis with Pandas and Scikit-Learn, I learned that TF-IDF with Logistic Regression is quite a strong combination, and showed robust performance, as high as Word2Vec + Convolutional Neural Network model. So in this post, I will try to implement TF-IDF + Logistic Regression model with PySpark.\n\nBy the way, if you want to know more in detail about how TF-IDF is calculated, please check my previous post: \u201cAnother Twitter sentiment analysis with Python \u2014 Part 5 (Tfidf vectorizer, model comparison, lexical approach)\u201d\n\n0.86! That looks good, maybe too good. Because I already tried the same combination of techniques with the same data in Pandas and SKLearn, I know that the result for unigram TF-IDF with Logistic Regression is around 80% accuracy. There can be some slight difference due to the detailed model parameters, but still, this looks too good.\n\nAnd by looking at the Spark documentation I realised that what BinaryClassificationEvaluator evaluates is by default areaUnderROC.\n\nAnd for binary classification, Spark doesn\u2019t support accuracy as a metric. But I can still calculate accuracy by counting the number of predictions matching the label and dividing it by the total entries.\n\nNow it looks more plausible, actually, the accuracy is slightly lower than what I have seen from SKLearn\u2019s result.\n\nThere\u2019s another way that you can get term frequency for IDF (Inverse Document Frequency) calculation. It is CountVectorizer in SparkML. Apart from the reversibility of the features (vocabularies), there is an important difference in how each of them filters top features. In case of HashingTF it is dimensionality reduction with possible collisions. CountVectorizer discards infrequent tokens.\n\nLet\u2019s see if performance changes if we use CountVectorizer instead of HashingTF.\n\nIt looks like using CountVectorizer has improved the performance a little bit.\n\nIn Scikit-Learn, n-gram implementation is fairly easy. You can define a range of n-grams when you call TfIdf Vectorizer. But with Spark, it is a bit more complicated. It does not automatically combine features from different n-grams, so I had to use VectorAssembler in the pipeline, to combine the features I get from each n-gram.\n\nI first tried to extract around 16,000 features from unigram, bigram, trigram. This means I will get around 48,000 features in total. Then I implemented Chi-Squared feature selection to reduce the number of features to 16,000 in total.\n\nAnd now I\u2019m ready to run the function I defined above.\n\nAccuracy has improved, but as you might have noticed, fitting the model took 4 hours! And this is mainly because of ChiSqSelector.\n\nWhat if I extract 5,460 features each from unigram, bigram, trigram in the first place, to have around 16,000 features in total in the end, without Chi Squared feature selection?\n\nThis has given me almost same result, marginally lower, but the difference is in the fourth digit. Considering it takes only 6 mins without ChiSqSelector, I definitely choose the model without ChiSqSelector.\n\nAnd finally, let\u2019s try this model on the final test set.\n\nThrough this post, I have implemented a simple sentiment analysis model with PySpark. Even though it might not be an advanced level use of PySpark, but I believe it is important to keep expose myself to new environment and new challenges. Exploring some basic functions of PySpark really sparked (no pun intended) my interest.\n\nI am attending Spark London Meetup tomorrow (13/03/2018) for \u201cApache Spark: Deep Learning Pipelines, PySpark MLLib and models in Streams\u201d. I can\u2019t wait to explore deeper into PySpark world!!\n\nThank you for reading and you can find the Jupyter Notebook from the below link:"
    },
    {
        "url": "https://towardsdatascience.com/bayesball-bayesian-analysis-of-batting-average-102e0390c0e4?source=user_profile---------3----------------",
        "title": "Bayesball: Bayesian analysis of batting average \u2013",
        "text": "*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nOne of the topics in data science or statistics I found interesting, but having difficulty understanding is Bayesian analysis. During the course of my General Assembly\u2019s Data Science Immersive boot camp, I have had a chance to explore Bayesian statistics, but I really think I need some review and reinforcement.\n\nThis is my personal endeavour to have a better understanding of Bayesian thinking, and how it can be applied to real-life cases.\n\nFor this post, I am mainly inspired by a Youtube series by Rasmus B\u00e5\u00e5th, \u201cIntroduction to Bayesian data analysis\u201d. He is really good at giving you an intuitive understanding of Bayesian analysis, not by bombarding you with all the complicated formulas, but by providing you with a thought-process of Bayesian statistics.\n\nThe topic I chose for this post is baseball. To be honest, I\u2019m not a big sports fan. I rarely watch sports. As a Korean, baseball is the most famous sports in Korea, and I believe there are some Korean players in MLB as well. It\u2019s a bit embarrassing to admit, but I have heard of Chan-Ho Park, but that\u2019s about it.\n\nThey say baseball is probably the world\u2019s best documented sports. The history has cumulated records in the past hundred years of the baseball statistics. However having collected stats alone doesn\u2019t make baseball interesting in terms of statistics. Maybe the more important aspect is the individual nature of the game. For example, during an at-bat, who is playing in the outfield has very little effect on whether or not the batter can hit a home run. In other sports, especially football and basketball, the meaning of individual statistics can be diluted by the importance of what is going on elsewhere on the field or the court. This is what makes baseball stats useful for player comparison.\n\nBaseball stats consist of numerous metrics, some of them straight-forward, some of them quite advanced. The metric I chose to take a look at is batting average(AVG). In baseball, the batting average is defined by the number of hits divided by at bats. It is usually reported to three decimal places.\n\nThere can be criticism on batting average, but according to C. Trent Rosecrans, \u201cStill, what batting average does have over all the other statistics is history and context. We all know what a .300 hitter is, we know how bad a .200 hitter is and how great a .400 hitter is.\u201d\n\nIt seems like the regular season hasn\u2019t started yet, and will start soon (29th of March). But there\u2019s spring training. In Major League Baseball (MLB), spring training is a series of practices and exhibition games preceding the start of the regular season.\n\nThe questions I would try to answer are as follows:\n\nBefore I jump into code, I will briefly touch on what Rasmus B\u00e5\u00e5th explained in his videos.\n\nWe first need three things to implement Bayesian analysis.\n\n1. Data\n\n2. Generative Model\n\n3. Prior\n\nIn my case, the data will be the batting average records from 2018 spring training. The data is simply what we observed.\n\nA Generative Model is the model that generates data when given parameters as input. The parameters are values you\u2019ll need to generate a distribution. For example, if you know the mean and the standard deviation, you can easily generate normally distributed data of your chosen size by running below code. We will see other types of distribution later to use in Bayesian analysis.\n\nIn the case of Bayesian analysis, we inverse the generative model and try to infer parameters with observed data.\n\nFinally, Prior is the information that the model has before seeing the data. Is any probability equally likely? Or do we have some prior data that we can utilise? Or is there any educated guess that we can make?\n\nI will first define a function to scrape Fox Sports\u2019 stats page for a player. I defined it as to be able to extract BATTING stats for either spring training or regular season.\n\nNow let\u2019s see who I should choose for analysis.\n\nThe above screen is spring training stats page for NY Mets (As I have already admitted, I know little about baseball, and I chose NY Mets because I liked the logo). If you arrange the players by their batting average (AVG), you can see Dominic Smith (DS) as the first, and Gavin Cecchini (GC) as the second. Are they good players? I don\u2019t know. But by looking at the AVG only, DS is the best with 1.000 AVG.\n\nBut by some googling, I found out that \u201cIn recent years, the league-wide batting average has typically hovered around .260\u201d. If so, then the AVG for DS and GC seems too high. By further looking at At-Bats (AB), Hits (H) of both players, it is clear that DS had only 1 AB and CS had 7. And also by looking further at AB for other players, the highest AB is 13 for 2018, and in 2017 the maximum AB is 60 within NY Mets.\n\nLet\u2019s assume that I know nothing about their past performance, and the only data I observed is 2018 spring training. And I don\u2019t know what value range I should expect from AVG. Based on this, how should I interpret the stats from 2018 spring training?\n\nThe prior represents our beliefs before we see the data. In the above distribution, any probability is almost equally likely (There are slight differences due to the random generation). Thus this means I know nothing about the player, and I don\u2019t even have any educated guess to make about AVG. I assume that 0.000 AVG is equally like as 1.000 AVG or any other probability between 0 and 1.\n\nNow the data we observed says there was 1 AB, and 1 H, hence 1.000 AVG. This can be represented by Binomial distribution. A random variable X that has a binomial distribution represents the number of successes in a sequence of n independent yes/no trials, each of which yields success with probability p. In case of AVG, AVG is the probability of success, AB is the number of trials, and H is the number of success.\n\nKeeping these in mind, we can define our inverse generative model.\n\nWe will randomly pick one probability value from the uniform distribution we defined, and use this value as a parameter for our generative model. Let\u2019s say the value we randomly picked is 0.230, this means 23% chance of success in Binomial distribution. The number of trials is 1 (DS has 1 AB), and if the result of the generative model matches the result we observed (in this case, DS has 1 H), then we keep the probability value 0.230. If we repeat this generation and filtering, we will finally get a distribution of probability that has generated the same result as we observed.\n\nThis becomes our Posterior.\n\n95% quantile interval in posterior distribution is called credible interval and should be seen slightly different from confidence interval in Frequentists\u2019 sense. There is another credible interval you can use, and I will get back to this when I mention Pymc3.\n\nOne major distinction between Bayesian\u2019s credible interval and Frequentist\u2019s confidence interval is their interpretation. The Bayesian probability reflects a person\u2019s subjective beliefs. Following this approach, we can make the claim that true parameter is inside a credible interval with measurable probability. This property is appealing because it enables you to make a direct probability statement about parameters. Many people find this concept to be a more natural way of understanding a probability interval, which is also easier to explain. A confidence interval, on the other hand, enables you to make a claim that the interval covers the true parameter. If we gather a new sample, and calculate the confidence interval, and repeat this many times, 95% of those intervals we calculated will have true AVG value within the interval.\n\nCredible Interval: \u201cGiven our observed data, there is a 95% probability that the true value of AVG falls within the credible interval\u201d\n\nConfidence Interval: \u201cThere is a 95% probability that when I compute confidence interval from data of this sort, the true value of AVG will fall within the confidence interval.\u201d\n\nNote the difference: the credible interval is a statement of probability about the parameter value given fixed bounds. The confidence interval is a probability about the bounds given a fixed parameter value.\n\nOften in real-life, what we would like to know is about the true parameters not about the bounds, in that case, the Bayesian credible interval is the right way to go. In this case, we are interested in true AVG of the player.\n\nWith above posterior distribution, I am 95% certain that DS true AVG will be somewhere between 0.155 to 0.987. But that is a very broad statement to make. In other words, I am not quite certain about the true AVG of DS, after I observe just one trial with no prior knowledge.\n\nFor the second scenario, let\u2019s assume that we know spring training stats from previous year.\n\nNow we have 2017 spring training stats, and our prior should reflect this knowledge. This is not a uniform distribution anymore since we know that in 2017 spring training, DS\u2019s AVG was 0.167.\n\nThe Beta distribution is a continuous probability distribution having two parameters, alpha and beta. One of its most common uses is to model one\u2019s uncertainty about the probability of success of an experiment. In particular, the conditional distribution of X, conditional on having observed k successes out of n trials, is a Beta distribution with parameters k+1 as alpha and n\u2212k+1 as beta.\n\nThe 95% quantile region has been narrowed compared to the posterior with the uniform prior in Scenario 1. Now I can say that I am 95% certain that the true AVG of DS will lie between 0.095 to 0.340. However, considering that above 0.300 AVG is often called best hitters, the statement means that the player can be either worst hitter or the best hitter. We need more data to narrow our region of credibility.\n\nFor this scenario, let\u2019s assume that I not only have stats from 2017 spring training, but also stats from 2017 regular season. How does this affect my statement after I get the posterior?\n\nNow I can say that I am 95% certain that the true AVG of DS will lie between 0.146 to 0.258. It may not be pin-point but compared to Scenario 1 and 2, the credible interval is much narrower now.\n\nI want to compare two players and see who\u2019s better in terms of AVG. The data I observed is result from 2018 spring training, and the prior knowledge I have is of 2017 spring training and regular season. Now I want to compare DS to GC.\n\nUp until Scenario 3, I simulated the sampling by rejecting all the parameters which yielded the result different from what I observed. But this type of random sample generation and filtering is often computationally expensive, and slow to run. But luckily, there\u2019s a tool that we can use to enable the sampler spends more time in regions of high probability, raising efficiency. Probabilistic programming tools such as Pymc3 can efficiently handle sampling procedure by making use of clever algorithms such as HMC-NUTS.\n\nLet\u2019s first start by scraping stats for Gavin Cecchini from Fox Sports.\n\nNow we are ready to fit a Pymc3 model.\n\nIf we plot the posterior distributions of DS_AVG, GC_AVG, and DvG (DS_AVG \u2014 GC_AVG) using plot_posterior function in Pymc3, we see the term HPD instead of quantile. Highest Probability Density (HPD) interval is another type of credible interval we can use with posteriors. HPD interval chooses the narrowest interval, which will involve choosing those values of highest probability density including the mode.\n\nAgain I found another post by Rasmus B\u00e5\u00e5th provides an easy-to-understand visual comparison of quantile interval and highest density interval. Below are the mode and the highest density intervals covering 95% of the probability density for the six different posterior distributions.\n\nThe quantile interval includes the median, and having 50% of the probability to its left and 50% to its right and the quantile interval leaving, say, 2.5% probability on either side (in case of 95% credible interval).\n\nIn the case of batting average for DS and GC, it looks like the mode and the median is not that different, and if so HPD interval will be similar to quantile interval. Let\u2019s see how they look.\n\nWe can see that both for DS and GC, HPD interval and quantile interval is either exactly the same or slightly different in decimal places.\n\nThe question I wanted to answer was who is the better player in terms of AVG, and I should say I can\u2019t be certain. At least, I can\u2019t be 95% certain that these two players are different in terms of AVG. The difference I calculated and plotted shows that the difference of AVG of two players (DS \u2014 GC, so if DvG is more positive then it means DS is better, else if DvG is more negative then it means GC is better), can be somewhere between -0.162 to 0.033.\n\nThis interval includes 0.000 which represents there is no difference between two players\u2019 AVG. Thus, there is some evidence that GC is better than DS (since the DvG posterior distribution has a larger region in negative area than in the positive area), but I can\u2019t be 95% certain that these two players are different in terms of AVG.\n\nMaybe with more data, I might be able to be certain about their difference. After all, that is the essence of Bayesian thinking. It is not that the truth doesn\u2019t exist, but it is that we can\u2019t know it perfectly, and all we could hope to do is update our understanding as more and more evidence became available.\n\nThank you for reading, and you can find the whole Jupyter Notebook from the below link."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-11-cnn-word2vec-41f5e28eda74?source=user_profile---------4----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 11 (CNN + Word2Vec)",
        "text": "This is the 11th and the last part of my Twitter sentiment analysis project. It has been a long journey, and through many trials and errors along the way, I have learned countless valuable lessons. I haven\u2019t decided on my next project. But I will definitely make time to start a new project. You can find the previous posts from the below links.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nIn the last post, I have aggregated the word vectors of each word in a tweet, either summation or calculating mean to get one vector representation of each tweet. However, in order to feed to a CNN, we have to not only feed each word vector to the model, but also in a sequence which matches the original tweet.\n\nFor example, let\u2019s say we have a sentence as below.\n\nAnd let\u2019s assume that we have a 2-dimensional vector representation of each word as follows:\n\nWith the above sentence, the dimension of the vector we have for the whole sentence is 3 X 2 (3: number of words, 2: number of vector dimension).\n\nBut there is one more thing we need to consider. A neural network model will expect all the data to have the same dimension, but in case of different sentences, they will have different lengths. This can be handled with padding.\n\nLet\u2019s say we have our second sentence as below.\n\nwith the below vector representation of each word:\n\nThe first sentence had 3X2 dimension vectors, but the second sentence has 4X2 dimension vector. Our neural network won\u2019t accept these as inputs. By padding the inputs, we decide the maximum length of words in a sentence, then zero pads the rest, if the input length is shorter than the designated length. In the case where it exceeds the maximum length, then it will also truncate either from the beginning or from the end. For example, let\u2019s say we decide our maximum length to be 5.\n\nThen by padding, the first sentence will have 2 more 2-dimensional vectors of all zeros at the start or the end (you can decide this by passing an argument), and the second sentence will have 1 more 2-dimensional vector of zeros at the beginning or the end. Now we have 2 same dimensional (5X2) vectors for each sentence, and we can finally feed this to a model.\n\nLet\u2019s first load the Word2Vec models to extract word vectors from. I have saved the Word2Vec models I trained in the previous post, and can easily be loaded with \u201cKeyedVectors\u201d function in Gensim. I have two different Word2Vec models, one with CBOW (Continuous Bag Of Words) model, and the other with skip-gram model. I won\u2019t go into detail of how CBOW and skip-gram differs, but you can refer to my previous post if you want to know a bit more in detail.\n\nBy running below code block, I am constructing a sort of dictionary I can extract the word vectors from. Since I have two different Word2Vec models, below \u201cembedding_index\u201d will have concatenated vectors of the two models. For each model, I have 100 dimension vector representation of the word, and by concatenating, each word will have 200 dimension vector representation.\n\nNow we have our reference to word vectors ready, but we still haven\u2019t prepared data to be in the format I have explained at the start of the post. Keras\u2019 \u2018Tokenizer\u2019 will split each word in a sentence, then we can call \u2018texts_to_sequences\u2019 method to get a sequential representation of each sentence. We also need to pass \u2018num_words\u2019 which is a number of vocabularies you want to use, and this will be applied when you call \u2018texts_to_sequences\u2019 method. This might be a bit counter-intuitive. Because if you check the length of all the word index, it will not be the number of words you defined, but the actual screening process happens when you call \u2018texts_to_sequences\u2019 method.\n\nBelow are the first five entries of the original train data.\n\nAnd the same data prepared as sequential data is as below.\n\nEach word is represented as a number, and we can see that the number of words in each sentence is matching the length of numbers in the \u201csequences\u201d. We can later make connections of which word each number represents. But we still didn\u2019t pad our data, so each sentence has varying length. Let\u2019s deal with this.\n\nThe maximum number of words in a sentence within the training data is 40. Let\u2019s decide the maximum length to be a bit longer than this, let\u2019s say 45.\n\nAs you can see from the padded sequences, all the data now transformed to have the same length of 45, and by default, Keras zero-pads at the beginning, if a sentence length is shorter than the maximum length. If you want to know more in detail, please check the Keras documentation on sequence preprocessing.\n\nThere\u2019s still one more thing left to do before we can feed the sequential text data to a model. When we transformed a sentence into a sequence, each word is represented by an integer number. Actually, these numbers are where each word is stored in the tokenizer\u2019s word index. Keeping this in mind, let\u2019s build a matrix of these word vectors, but this time we will use the word index number so that our model can refer to the corresponding vector when fed with integer sequence.\n\nBelow, I am defining the number of words to be 100,000. This means I will only care about 100,000 most frequent words in the training set. If I don\u2019t limit the number of words, the total number of vocabulary will be more than 200,000.\n\nAs a sanity check, if the embedding matrix has been generated properly. In the above, when I saw the first five entries of the training set, the first entry was \u201chate you\u201d, and the sequential representation of this was [137, 6]. Let\u2019s see if 6th embedding matrix is as same as vectors for the word \u2018you\u2019.\n\nNow we are done with the data preparation. Before we jump into CNN, I would like to test one more thing (sorry for the delay). When we feed this sequential vector representation of data, we will use Embedding layer in Keras. With Embedding layer, I can either pass pre-defined embedding, which I prepared as \u2018embedding_matrix\u2019 above, or Embedding layer itself can learn word embeddings as the whole model trains. And another possibility is we can still feed the pre-defined embedding but make it trainable so that it will update the values of vectors as the model trains.\n\nIn order to check which method performs better, I defined a simple shallow neural network with one hidden layer. For this model structure, I will not try to refine models by tweaking parameters, since the main purpose of this post is to implement CNN.\n\nAs a result, the best validation accuracy is from the third method (fine-tune pre-trained Word2Vec) at 82.22%. The best training accuracy is the second method (learn word embedding from scratch) at 90.52%. Using pre-trained Word2Vec without updating its vector values showed the lowest accuracy both in training and validation. However, what\u2019s interesting is that in terms of training set accuracy, fine-tuning pre-trained word vectors couldn\u2019t outperform the word embeddings learned from scratch through the embedding layer. Before I tried the above three methods, my first guess was that if I fine-tune the pre-trained word vectors, it would give me the best training accuracy.\n\nFeeding pre-trained word vectors for an embedding layer to update is like providing the first initialisation guideline to the embedding layer so that it can learn more efficiently the task-specific word vectors. But the result is somewhat counterintuitive, and in this case, it turns out that it is better to force the embedding layer to learn from scratch.\n\nBut premature generalisation could be dangerous. For this reason, I will compare three methods again in the context of CNN.\n\nYou might have already seen how Convolutional Neural Network (CNN) works on image data. There are many good sources that you can learn basics of CNN. In my case, the blog post, \u201cA Beginner\u2019s Guide To Understanding Convolutional Neural Networks\u201d by Adit Deshpande really helped me a lot to grasp the concept. If you are not familiar with CNN, I highly recommend his article, so that you will have a firm understanding of CNN.\n\nNow I will assume you have an understanding of CNN in case of image data. How can this be applied to text data then? Let\u2019s say we have a sentence as follows:\n\nWith word vectors (let\u2019s assume we have 200-dimensional word vectors for each word), the above sentence can be represented in 5X200 matrix, one row for each word. You remember we added zeros to pad a sentence in the above where we prepared the data to feed to an embedding layer? If our decided word length is 45, then the above sentence will have 45X200 matrix, but with all zeros in the first 40 rows. Keeping this in mind, let\u2019s take a look at how CNN works on image data.\n\nIn the above GIF, we have one filter (kernel matrix) of 3X3 dimension, convolving over the data (image matrix) and calculate the sum of element-wise multiplication result, and record the result on a feature map (output matrix). If we imagine each row of the data is for a word in a sentence, then it would not be learning efficiently since the filter is only looking at a part of a word vector at a time. The above CNN is so-called 2D Convolutional Neural Network since the filter is moving in 2-dimensional space.\n\nWhat we do with text data represented in word vectors is making use of 1D Convolutional Neural Network. If a filter\u2019s column width is as same as the data column width, then it has no room to stride horizontally, and only stride vertically. For example, if our sentence is represented in 45X200 matrix, then a filter column width will also have 200 columns, and the length of row (height) will be similar to the concept of n-gram. If the filter height is 2, the filter will stride through the document computing the calculation above with all the bigrams, if the filter height is 3, it will go through all the trigrams in the document, and so on.\n\nIf a 2X200 filter is applied with stride size of 1 to 45X200 matrix, we will get 44X1 dimensional output. In the case of 1D Convolution, the output width will be just 1 in this case(number of filter=1). The output height can be easily calculated with below formula (assuming that your data is already padded).\n\nNow let\u2019s try to add more filters to our 1D Convolutional layer. If we apply 100 2X200 filters with stride size of 1 to 45X200 matrix, can you guess the output dimension?\n\nAs I have already mentioned in the above, now the output width will reflect the number of filters we apply, so the answer is we will have 44X100 dimension output. You can also check the dimensions of each output layer by looking at the model summary after you define the structure.\n\nNow if we add Global Max Pooling layer, then the pooling layer will extract the maximum value from each filter, and the output dimension will be a just 1-dimensional vector with length as same as the number of filters we applied. This can be directly passed on to a dense layer without flattening.\n\nNow, let\u2019s define a simple CNN going through bigrams on a tweet. The output from global max pooling layer will be fed to a fully connected layer, then finally the output layer. Again I will try three different inputs, static word vectors extracted from Word2Vec, word embedding being learned from scratch with embedding layer, Word2Vec word vectors being updated through training.\n\nThe best validation accuracy is from the word vectors updated through training, at epoch 3 with the validation accuracy of 83.25%. By looking at the training loss and accuracy, it seems that word embedding learned from scratch tends to overfit to the training data, and by feeding pre-trained word vectors as weights initialisation, it somewhat more generalises and ends up having higher validation accuracy.\n\nBut finally! I have a better result than Tf-Idf + logistic regression model! I have tried various different methods with Doc2Vec, Word2Vec in the hope of outperforming a simple logistic regression model with Tf-Idf input. You can take a look at the previous post for detail. Tf-Idf + logistic regression model\u2019s validation accuracy was at 82.91%. And now I\u2019m finally beginning to see a possibility of Word2Vec + neural network outperforming this simple model.\n\nLet\u2019s see if we can do better by defining a bit more elaborate model structure. The CNN architecture I will implement below is inspired by Zhang, Y., & Wallace, B. (2015) \u201cA Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification\u201d.\n\nBasically, the above structure is implementing what we have done above with bigram filters, but not only to bigrams but also to trigrams and fourgrams. However this is not linearly stacked layers, but parallel layers. And after convolutional layer and max pooling layer, it simply concatenated max pooled result from each of bigram, trigram, and fourgram, then build one output layer on top of them.\n\nThe model I defined below is basically as same as the above picture, but the differences are that I added one fully connected hidden layer with dropout just before the output layer, and also my output layer will have just one output node with Sigmoid activation instead of two.\n\nThere is also another famous paper by Y. Kim(2014), \u201cConvolutional Neural Networks for Sentence Classification\u201d. https://arxiv.org/pdf/1408.5882.pdf\n\nIn this paper, he implemented more sophisticated approach by making use of \u201cchannel\u201d concept. Not only the model go through different n-grams, his model has multi-channels (eg. one channel for static input word vectors, another channel for word vectors input but set them to update during training). But in this post, I will not go through multi-channel approach.\n\nSo far I have only used Sequential model API of Keras, and this worked fine with all the previous models I defined above since the structures of the models were only linearly stacked. But as you can see from the above picture, the model I am about to define has parallel layers which take the same input but do their own computation, then the results will be merged. In this kind of neural network structure, we can use Keras functional API.\n\nKeras functional API can handle multi-input, multi-output, shared layers, shared input, etc. It is not impossible to define these types of models with Sequential API, but when you want to save the trained model, functional API enables you to simply save the model and load, but with sequential API it is difficult.\n\nThe best validation accuracy is 83.33%, slightly better than the simple CNN model with bigram filters, which yielded 83.25% validation accuracy. I could even define a deeper structure with more hidden layers, or even make use of multi-channel approach that Yoon Kim(2014) has implemented, or try different pool size to see how the performance differs, but I will stop here for now. However if you happen to try more complex CNN structure, and get the result, I would love to hear about it.\n\nSo far I have tested the model on the validation set to decide the feature extraction tuning and model comparison. Now I will finally check the final result with the test set. I will compare two different models: 1. Tf-Idf + logistic regression, 2. Word2Vec + CNN. As another measure for comparison, I will also plot ROC curve of both models.\n\nAnd the final result is as below.\n\nThank you for reading. You can find the Jupyter Notebook from the below link."
    },
    {
        "url": "https://towardsdatascience.com/efficient-frontier-portfolio-optimisation-in-python-e7844051e7f?source=user_profile---------5----------------",
        "title": "Efficient Frontier Portfolio Optimisation in Python",
        "text": "My personal interest in finance has led me to take an online course on investment management in Coursera. It is a 5-course specialisation by the University of Geneva partnered with UBS. It is not specifically for financial modelling, but more for general introduction in investment strategies and the theories surrounding them. As someone who doesn\u2019t have any experience in the industry, the course is really helpful to understand the big picture. I am currently on the 3rd course within the specialisation, and I learned something very interesting called \u201cModern Portfolio Theory\u201d\n\nWhile I was going through the course, I thought it would be a very good material to practice my Python skills. Even though the course did not provide any technical details of how to actually implement it, with some digging I found a couple of very useful blog posts I can refer to.\n\nBased on what I have learned through the course, and also from the above blog posts, I have tried to replicate it in my own way, tweaking bit and pieces along the way.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nA good portfolio is more than a long list of good stocks and bonds. It is a balanced whole, providing the investor with protections and opportunities with respect to a wide range of contingencies. \u2013 Harry Markowitz\n\nModern Portfolio Theory (MPT) is an investment theory developed by Harry Markowitz and published under the title \u201cPortfolio Selection\u201d in the Journal of Finance in 1952.\n\nThere are a few underlying concepts that can help you understand MPT. If you are familiar with finance, you might know what the acronym \u201cTANSTAAFL\u201d stands for. It is a famous acronym for \u201cThere Ain\u2019t No Such Thing As A Free Lunch\u201d. This concept is also closely related to \u2018risk-return trade-off\u2019.\n\nHigher risk is associated with greater probability of higher return and lower risk with a greater probability of smaller return. MPT assumes that investors are risk-averse, meaning that given two portfolios that offer the same expected return, investors will prefer the less risky one. Thus, an investor will take on increased risk only if compensated by higher expected returns.\n\nAnother factor comes in to play in MPT is \u201cdiversification\u201d. Modern portfolio theory says that it is not enough to look at the expected risk and return of one particular stock. By investing in more than one stock, an investor can reap the benefits of diversification \u2014 chief among them, a reduction in the riskiness of the portfolio.\n\nWhat you need to understand is that \u201crisk of a portfolio is not equal to average/weighted-average of individual stocks in the portfolio\u201d. In terms of return, yes it is the average/weighted average of individual stock\u2019s returns, but that\u2019s not the case for risk. The risk is about how volatile the asset is, if you have more than one stock in your portfolio, then you have to take count of how these stocks movement correlates with each other. The beauty of diversification is that you can even get lower risk than a stock with the lowest risk in your portfolio, by optimising the allocation.\n\nI will try to explain as I go along with the actual code. First, let\u2019s start by importing some libraries we need. \u201cQuandl\u201d is a financial platform which also offers Python library. If you haven\u2019t installed it before, of course, you first need to install the package in your command line \u201cpip install quandl\u201d, and before you can use it, you also need to get an API key on Quandl\u2019s website. Sign-up and getting an API key is free but has some limits. As a logged-in free user, you will be able to call 2,000 calls per 10 minutes maximum (speed limit), and 50,000 calls per day (volume limit).\n\nIn order to run the below code block, you will need your own API key. The stocks selected for this post is Apple, Amazon, Google, Facebook. Below code block will get daily adjusted closing price of each stock from 01/01/2016 to 31/12/2017 (2 years\u2019 price data).\n\nBy looking at the info() of data, it seems like the \u201cdate\u201d column is already in datetime format. Let\u2019s transform the data a little bit to make it easier to work with.\n\nIt looks much better now. Let\u2019s first look at how the price of each stock has evolved within the given time frame.\n\nIt looks like that Amazon and Google\u2019s stock price is relatively more expensive than those of Facebook and Apple. But since Facebook and Apple are squashed at the bottom, it is hard to see the movement of these two.\n\nAnother way to plot this is plotting daily returns (percent change compared to the day before). By plotting daily returns instead of actual prices, we can see the stocks\u2019 volatility.\n\nAmazon has two distinctive positive spikes and a couple of negative ones. Facebook has one highest positive spike. And Apple also has some spikes stand out from the plot. From the above plot, we can roughly see that Amazon looks like a quite risky stock, and Google seems to be the most stable one among them.\n\nWe have 4 stocks in our portfolio. One decision we have to make is how we should allocate our budget to each of stock in our portfolio. If our total budget is 1, then we can decide the weights for each stock, so that the sum of weights will be 1. And the value for weights will be the portion of budget we allocate to a specific stock. For example, if weight is 0.5 for Amazon, it means that we allocate 50% of our budget to Amazon.\n\nLet\u2019s define some functions to simulate random weights to each stock in the portfolio, then calculate the portfolio\u2019s overall annualised returns and annualised volatility.\n\n\u201cportfolio_annualised_performance\u201d function will calculate the returns and volatility, and to make it as an annualised calculation I take into account 252 as the number of trading days in one year. \u201crandom_portfolios\u201d function will generate portfolios with random weights assigned to each stock, and by giving num_portfolios argument, you can decide how many random portfolios you want to generate.\n\nFrom the above code block, there are two things I want to point out.\n\nThe first is the calculation for portfolio\u2019s volatility in \u201cportfolio_annualised_performance\u201d function. If you look up \u201cportfolio standard deviation formula\u201d, you will come across formulas as below.\n\nThis formula can be simplified if we make use of matrix notation. Again Bernard Brenyah, whom I mentioned at the beginning of the post, has provided a clear explanation of how the above formula can be expressed in matrix calculation in one of his blog posts.\n\nWith the above matrix calculation, we get the part inside the square root in the original formula. Now, all we need to do is put them inside the square root. Same as the annualised return, I took into account of 252 trading days (in this case, the square root of 252) to calculate the annualised standard deviation of a portfolio.\n\nThe second thing I would like to point out is the Sharpe ratio. In order to understand the Sharpe ratio, it is essential to understand the broader concept of risk-adjusted return. Risk-adjusted return refines an investment\u2019s return by measuring how much risk is involved in producing that return, which is generally expressed as a number or rating. There could be a number of different methods of expressing risk-adjusted return, and the Sharpe ratio is one of them.\n\nThe Sharpe ratio was derived in 1966 by William Sharpe, another winner of a Nobel Memorial Prize in Economic Sciences.\n\nThe ratio describes how much excess return you are receiving for the extra volatility that you endure for holding a riskier asset. The Sharpe ratio can be expressed in below formula.\n\nThere are some criticisms on how the Sharpe ratio uses the standard deviation of returns as a denominator, which assumes the normal distribution of the returns. However, more often than not, the returns on financial assets tend to deviate from a normal distribution and may make interpretations of the Sharpe ratio misleading. It is for this reason that there are other methods which adjust or modify the original Sharpe ratio. But it is a more advanced topic, and I will stick to the traditional Sharpe ratio for this post.\n\nNow let\u2019s get the needed argument values for our functions. You can easily get daily returns by calling pct_change on the data frame with the price data. And the mean daily returns, the covariance matrix of returns are needed to calculate portfolio returns and volatility. We will generate 25,000 random portfolios. Finally, the risk-free rate has been taken from U.S. Department of The Treasury. The rate of 1.78% is the 52week treasury bill rates at the start of 2018. The rationale behind this is that the historical price data is from 2016~2017, and if I assume that I implement this analysis at the start of 2018, the most updated Treasury bill rate is from the start of 2018. And I also chose 52weeks treasury bill rates to match with the annualised return and risk I calculated.\n\nLet me briefly explain what below function is doing. First, it generates random portfolio and gets the results (portfolio returns, portfolio volatility, portfolio Sharpe ratio) and weights for the corresponding result. Then by locating the one with the highest Sharpe ratio portfolio, it displays maximum Sharpe ratio portfolio as red star sign. And does similar steps for minimum volatility portfolio, and displays it as a green star on the plot. All the randomly generated portfolios will be also plotted with colour map applied to them based on the Sharpe ratio. Bluer, higher the Sharpe ratio.\n\nAnd for these two optimal portfolios, it will also show how it allocates the budget within the portfolio.\n\nFor minimum risk portfolio, we can see that more than half of our budget is allocated to Google. If you take another look at the daily return plot from earlier, you can see that Google is the least volatile stock of four, so allocating a large percentage to Google for minimum risk portfolio makes intuitive sense.\n\nIf we are willing to take higher risk for higher return, one that gives us the best risk-adjusted return is the one with maximum Sharpe ratio. In this scenario, we are allocating a significant portion to Amazon and Facebook, which are quite volatile stocks from the previous plot of daily returns. And Google which had more than 50% in the case of minimum risk portfolio, has less than 1% budget allocated to it.\n\nFrom the plot of the randomly simulated portfolios, we can see it forms a shape of an arch line on the top of clustered blue dots. This line is called efficient frontier. Why is it efficient? Because points along the line will give you the lowest risk for a given target return. All the other dots right to the line will give you higher risk with same returns. If the expected returns are the same, why would you take an extra risk when there\u2019s an option with lower risk?\n\nThe way we found the two kinds of optimal portfolio above was by simulating many possible random choices and pick the best ones (either minimum risk or maximum risk-adjusted return). We can also implement this by using Scipy\u2019s optimize function.\n\nIf you are an advanced Excel user, you might be familiar with \u2018solver\u2019 function in excel. Scipy\u2019s optimize function is doing the similar task when given what to optimize, and what are constraints and bounds.\n\nBelow functions are to get the maximum Sharpe ratio portfolio. In Scipy\u2019s optimize function, there\u2019s no \u2018maximize\u2019, so as an objective function you need to pass something that should be minimized. That is why the first \u201cneg_sharpe_ratio\u201d is computing the negative Sharpe ratio. Now we can use this as our objective function to minimize. In \u201cmax_sharpe_ratio\u201d function, you first define arguments (this should not include the variables you would like to change for optimisation, in this case, \u201cweights\u201d). At first, the construction of constraints was a bit difficult for me to understand, due to the way it is stated.\n\nThe above constraint is saying that sum of x should be equal to 1. You can think of the \u2018fun\u2019 part construction as \u20181\u2019 on the right side of equal sign has been moved to the left side of the equal sign.\n\nAnd what does this mean? It simply means that the sum of all the weights should be equal to 1. You cannot allocate more than 100% of your budget in total.\n\n\u201cbounds\u201d is giving another limit to assign random weights, by saying any weight should be inclusively between 0 and 1. You cannot give minus budget allocation to a stock or more than 100% allocation to a stock.\n\nWe can also define an optimising function for calculating minimum volatility portfolio. This time we really do minimise objective function. What do we want to minimise? We want to minimise volatility by trying different weights. \u201cconstraints\u201d and \u201cbounds\u201d are same as the above.\n\nAs I already mentioned above we can also draw a line which depicts where the efficient portfolios for a given risk rate should be. This is called \u201cefficient frontier\u201d. Below I define other functions to compute efficient frontier. The first function \u201cefficient_return\u201d is calculating the most efficient portfolio for a given target return, and the second function \u201cefficient_frontier\u201d will take a range of target returns and compute efficient portfolio for each return level.\n\nLet\u2019s try to plot the portfolio choices with maximum Sharpe ratio and minimum volatility also with all the randomly generated portfolios. But this time, we are not picking the optimal ones from the randomly generated portfolios, but we are actually calculating by using Scipy\u2019s \u2018minimize\u2019 function. And the below function will also plot the efficient frontier line.\n\nWe have almost the same result as what we have simulated by picking from the randomly generated portfolios. The slight difference is that the Scipy\u2019s \u201coptimize\u201d function has not allocated any budget at all for Google on maximum Sharpe ratio portfolio, while one we chose from the randomly generated samples has 0.45% of allocation for Google. There are some differences in the decimal places but more or less same.\n\nInstead of plotting every randomly generated portfolio, we can plot each individual stocks on the plot with the corresponding values of each stock\u2019s annual return and annual risk. This way we can see and compare how diversification is lowering the risk by optimising the allocation.\n\nAs you can see from the above plot, the stock with the least risk is Google at around 0.18. But with portfolio optimisation, we can achieve even lower risk at 0.16, and still with a higher return than Google. And if we are willing to take slightly more risk at around the similar level of risk of Google, we can achieve a much higher return of 0.30 with portfolio optimisation.\n\nConsidering how vast and the deep the finance field is, I\u2019ve probably only scratched the surface. But I had fun going through coding and trying to understand the concept. And I\u2019m learning every day. After finishing this implementation, I definitely know better than yesterday\u2019s me. And if I keep on going and learning, in about a couple of year\u2019s time, I will know a whole lot more than today\u2019s me. If you have any comments or questions, feel free to leave a comment. Any feedback would be appreciated.\n\nThank you for reading. You can find the Jupyter Notebook from the below link."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-10-neural-network-with-a6441269aa3c?source=user_profile---------6----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 10 (Neural Network with\u2026",
        "text": "This is the 10th part of my ongoing Twitter sentiment analysis project. You can find the previous posts from the below links.\n\nIn the previous post, I implemented neural network modelling with Tf-idf vectors, but found that with high-dimensional sparse data, neural network did not perform well. In this post, I will see if feeding document vectors from Doc2Vec models or word vectors is any different from Tf-idf vectors.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nBefore I jump into neural network modelling with the vectors I got from Doc2Vec, I would like to give you some background on how I got these document vectors. I have implemented Doc2Vec using Gensim library in the 6th part of this series.\n\nThere are three different methods used to train Doc2Vec. Distributed Bag of Words, Distributed Memory (Mean), Distributed Memory (Concatenation). These models were trained with 1.5 million tweets through 30 epochs and the output of the models are 100 dimension vectors for each tweet. After I got document vectors from each model, I have tried concatenating these (so the concatenated document vectors have 200 dimensions) in combination: DBOW + DMM, DBOW + DMC, and saw an improvement to the performance when compared with models with any single pure method. Using different methods of training and concatenating them to improve the performance has already been demonstrated by Le and Mikolov (2014) in their research paper.\n\nFinally, I have applied phrase modelling to detect bigram phrase and trigram phrase as a pre-step of Doc2Vec training and tried different combination across n-grams. When tested with a logistic regression model, I got the best performance result from \u2018unigram DBOW + trigram DMM\u2019 document vectors.\n\nI will first start by loading Gensim\u2019s Doc2Vec, and define a function to extract document vectors, then load the doc2vec model I trained.\n\nWhen fed to a simple logistic regression, the concatenated document vectors (unigram DBOW + trigram DMM) yields 75.90% training set accuracy, and 75.76% validation set accuracy.\n\nI will try different numbers of hidden layers, hidden nodes to compare the performance. In the below code block, you see I first define the seed as \u201c7\u201d but not setting the random seed, \u201cnp.random.seed()\u201d will be defined at the start of each model. This is for a reproducibility of various results from different model structures.\n\n*Side Note (reproducibility): To be honest, this took me a while to figure out. I first tried by setting the random seed before I import Keras, and ran one model after another. However, if I define the same model structure after it has run, I couldn\u2019t get the same result. But I also realised if I restart the kernel, and re-run code blocks from start it gives me the same result as the last kernel. So I figured, after running a model the random seed changes, and that is the reason why I cannot get the same result with the same structure if I run them in the same kernel consecutively. Anyway, that is why I set the random seed every time I try a different model. For your information, I am running Keras with Theano backend, and only using CPU not GPU. If you are on the same setting, this should work. I explicitly specified backend as Theano by launching Jupyter Notebook in the command line as follows: \u201cKERAS_BACKEND=theano jupyter notebook\u201d\n\nPlease note that not all of the dependencies loaded in the below cell has been used for this post, but imported for later use.\n\nDefining different model structures will be quite repetitive codes, so I will just give you two examples so that you can understand how to define model structures with Keras.\n\nAfter trying 12 different models with a range of hidden layers (from 1 to 3) and a range of hidden nodes for each hidden layer (64, 128, 256, 512), below is the result I got. Best validation accuracy (79.93%) is from \u201cmodel_d2v_09\u201d at epoch 7, which has 3 hidden layers of 256 hidden nodes for each hidden layer.\n\nNow I know which model gives me the best result, I will run the final model of \u201cmodel_d2v_09\u201d, but this time with callback functions in Keras. I was not quite familiar with callback functions in Keras before I received a comment in my previous post. After I got the comment, I did some digging and found all the useful functions in Keras callbacks. Thanks to @rcshubha for the comment. With my final model of Doc2Vec below, I used \u201ccheckpoint\u201d and \u201cearlystop\u201d. You can set the \u201ccheckpoint\u201d function with options, and with the below parameter setting, \u201ccheckpoint\u201d will save the best performing model up until the point of running, and only if a new epoch outperforms the saved model it will save it as a new model. And \u201cearly_stop\u201d I defined it as to monitor validation accuracy, and if it doesn\u2019t outperform the best validation accuracy so far for 5 epochs, it will stop.\n\nIf I evaluate the model I just run, it will give me the result as same as I got from the last epoch.\n\nBut if I load the saved model at the best epoch, then this model will give me the result at that epoch.\n\nIf you remember the validation accuracy with the same vector representation of the tweets with a logistic regression model (75.76%), you can see that feeding the same information to neural networks yields a significantly better result. It\u2019s amazing to see how neural network can boost the performance of dense vectors, but the best validation accuracy is still lower than the Tfidf vectors + logistic regression model, which gave me 82.92% validation accuracy.\n\nIf you have read my posts on Doc2Vec, or familiar with Doc2Vec, you might know that you can also extract word vectors for each word from the trained Doc2Vec model. I will move on to Word2Vec, and try different methods to see if any of those can outperform the Doc2Vec result (79.93%), ultimately outperform the Tfidf + logistic regression model (82.92%).\n\nTo make use of word vectors extracted from Doc2Vec model, I can no longer use the concatenated vectors of different n-grams, since they will not consist of the same vocabularies. Thus below, I load the model for unigram DMM and create concatenated vectors with unigram DBOW of 200 dimensions for each word in the vocabularies.\n\nWhat I will do first before I try neural networks with document representations computed from word vectors is that I will fit logistic regressions with various methods of document representation and with the one that gives me the best validation accuracy, I will finally define a neural network model.\n\nI will also give you the summary of result from all the different word vectors fit with logistic regression as a table.\n\nThere could be a number of different ways to come up with document representational vectors with individual word vectors. One obvious choice is to average them. For every word in a tweet, see if trained Doc2Vec has word vector representation of the word, if so, sum them up throughout the document while counting how many words were detected as having word vectors, and finally by dividing the summed vector by the count you get the averaged word vector for the whole document which will have the same dimension (200 in this case) as the individual word vectors.\n\nAnother method is just the sum of the word vectors without averaging them. This might distort the vector representation of the document if some tweets only have a few words in the Doc2Vec vocabulary and some tweets have most of the words in the Doc2Vec vocabulary. But I will try both summing and averaging and compare the results.\n\nThe validation accuracy with averaged word vectors of unigram DBOW + unigram DMM is 71.74%, which is significantly lower than document vectors extracted from unigram DBOW + trigram DMM (75.76%), and also from the results I got from the 6th part of this series, I know that document vectors extracted from unigram DBOW + unigram DMM will give me 75.51% validation accuracy.\n\nI also tried scaling the vectors using ScikitLearn\u2019s scale function and saw significant improvement in computation time and a slight improvement of the accuracy.\n\nLet\u2019s see how summed word vectors perform compared to the averaged counterpart.\n\nThe summation method gave me higher accuracy without scaling compared to the average method. But the simple logistic regression with the summed vectors took more than 3 hours to run. So again I tried scaling these vectors.\n\nSurprising! With scaling, logistic regression fitting only took 3 minutes! That\u2019s quite a difference. Validation accuracies with scaled word vectors are 72.42% with averaging, 72.51% with summing.\n\nIn the 5th part of this series, I have already explained what TF-IDF is. TF-IDF is a way of weighting each word by calculating the product of relative term frequency and inverse document frequency. Since it gives one scalar value for each word in the vocabulary, this can also be used as a weighting factor of each word vectors. Correa Jr. et al (2017) has implemented this Tf-idf weighting in their paper \u201cNILC-USP at SemEval-2017 Task 4: A Multi-view Ensemble for Twitter Sentiment Analysis\u201d\n\nIn order to get the Tfidf value for each word, I first fit and transform the training set with TfidfVectorizer and create a dictionary containing \u201cword\u201d, \u201ctfidf value\u201d pairs. Also, I defined a function \u201cget_w2v_general\u201d to get either averaged word vectors or summed word vectors with given Word2Vec model. Finally, I calculate the multiplication of word vectors with corresponding Tfidf values. From below code, the Tfidf multiplication part took quite a bit of time. To be honest, I am still not sure why it took so long to compute the Tfidf weighting of the word vectors, but after 5 hours it finally finished computing. You can also see later that I tried another method of weighting but that took less than 10 seconds. If you have an answer to this, any insight would be appreciated.\n\nThen I can call \u201cget_w2v_general\u201d function in the same way I did with the above word vectors without weighting factors, then fit a logistic regression model. The validation accuracy for mean is 70.57%, for sum is 70.32%. The results are not what I expected, especially after 5 hours of waiting. By weighting word vectors with Tfidf values, the validation accuracy dropped around 2% both for averaging and summing.\n\nIn the 3rd part of this series, I have defined a custom metric called \u201cpos_normcdf_hmean\u201d, which is a metric borrowed from the presentation by Jason Kessler in PyData 2017 Seattle. If you want to know more in detail about the calculation, you can either check my previous post or you can also watch Jason Kessler\u2019s presentation. To give you a high-level intuition, by calculating harmonic mean of CDF(Cumulative Distribution Function) transformed values of term frequency rate within the whole document and the term frequency within a class, you can get a meaningful metric which shows how each word is related to a certain class.\n\nI have used this metric to visualise tokens in the 3rd part of the series, and also used this again to create custom lexicon to be used for classification purpose in the 5th part. I will use this again as a weighting factor for the word vectors and see how it affects the performance.\n\nThe validation accuracy for mean is 73.27%, and for sum is 70.94%. Unlike Tfidf weighting, this time with custom weighting it actually gave me some performance boost when used with averaging method. But with summing, this weighting has performed no better than the word vectors without weighting.\n\nGloVe is another kind of word representation in vectors proposed by Pennington et al. (2014) from the Stanford NLP Group.\n\nThe difference between Word2Vec and Glove is how the two models compute the word vectors. In Word2Vec, the word vectors you are getting is a kind of a by-product of a shallow neural network, when it tries to predict either centre word given surrounding words or vice versa. But with GloVe, the word vectors you are getting is the object matrix of GloVe model, and it calculates this using term co-occurrence matrix and dimensionality reduction.\n\nThe good news is you can now easily load and use the pre-trained GloVe vectors from Gensim thanks to its latest update (Gensim 3.2.0). In addition to some pre-trained word vectors, new datasets are also added and this also can be easily downloaded using their downloader API. If you want to know more about this, please check this blog post by RaRe Technologies.\n\nThe Stanford NLP Group has made their pre-trained GloVe vectors publicly available, and among them, there are GloVe vectors trained specifically with Tweets. This sounds like something definitely worth trying. They have four different versions of Tweet vectors each with different dimensions (25, 50, 100, 200) trained on 2 billion Tweets. You can find more detail on their website.\n\nFor this post, I will use 200 dimensions pre-trained GloVe vectors. (You might need to update Gensim if your version of Gensim is lower than 3.2.0)\n\nBy using pre-trained GloVe vectors, I can see that the validation accuracy significantly improved. So far the best validation accuracy was from the averaged word vectors with custom weighting, which gave me 73.27% accuracy, and compared to this, GloVe vectors yield 76.27%, 76.60% for average and sum respectively.\n\nWith new updated Gensim, I can also load the famous pre-trained Google News word vectors. These word vectors are trained using Word2Vec model on Google News dataset (about 100 billion words) and published by Google. The model contains 300-dimensional vectors for 3 million words and phrases. You can find more detail in the Google project archive.\n\nThe validation accuracy for mean is 74.96%, and for sum is 74.92%. Even though it gives me a better result than the word vectors extracted from custom trained Doc2Vec models, but it fails to outperform GloVe vectors. And the vector dimension is even larger in Google News word vectors.\n\nBut, this is trained with Google News, and GloVe vector I used was trained specifically with Tweets, thus it is hard to compare each other directly. What if Word2Vec is specifically trained with Tweets?\n\nI know I have already tried word vectors I extracted from Doc2Vec models, but what if I train separate Word2Vec models? Even though Doc2Vec models gave good representational vectors of document level, would it be more efficiently learning word vectors if I train pure Word2Vec?\n\nIn order to answer my own questions, I trained two Word2Vec models using CBOW (Continuous Bag Of Words) and Skip Gram models. In terms of parameter setting, I set the same parameters I used for Doc2Vec.\n\nWith above settings, I defined CBOW model by passing \u201csg=0\u201d, and Skip Gram model by passing \u201csg=1\u201d.\n\nAnd once I get the results from two models, I concatenate vectors of two models for each word so that the concatenated vectors will have 200-dimensional representation of each word.\n\nPlease note that in the 6th part, where I trained Doc2Vec, I used \u201cLabeledSentence\u201d function imported from Gensim. This has now been deprecated, thus for this post I used \u201cTaggedDocument\u201d function instead. The usage is the same.\n\nThe concatenated vectors of unigram CBOW and unigram Skip Gram models has yielded 76.50%, 76.75% validation accuracy respectively with mean and sum method. These results are even higher than the results I got from GloVe vectors.\n\nBut please do not confuse this as a general statement. This is an empirical finding in this particular setting.\n\nAs a final step, I will apply the custom weighting I have implemented above and see if this affects the performance.\n\nFinally I get the best performing word vectors. Averaged word vectors (separately trained Word2Vec models) weighted with custom metric has yielded the best validation accuracy of 77.97%! Below is the table of all the results I tried above.\n\nThe best performing word vectors with logistic regression was chosen to feed to a neural network model. This time I did not try various different architecture. Based on what I have observed during trials of different architectures with Doc2Vec document vectors, the best performing architecture was one with 3 hidden layers with 256 hidden nodes at each hidden layer.\n\nI will finally fit a neural network with early stopping and checkpoint so that I can save the best performing weights on validation accuracy.\n\nThe best validation accuracy is 80.48%. Surprisingly this is even higher than the best accuracy I got by feeding document vectors to neural network models in the above.\n\nIt took quite some time for me to try different settings, different calculations, but I learned some valuable lessons through all the trial and errors. Specifically trained Word2Vec with carefully engineered weighting can even outperform Doc2Vec in the classification task.\n\nIn the next post, I will try more sophisticated neural network model, Convolutional Neural Network. Again I hope this will give me some boost of the performance.\n\nThank you for reading, and you can find the Jupyter Notebook from the below link."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-9-neural-networks-with-tfidf-vectors-using-d0b4af6be6d7?source=user_profile---------7----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 9 (Neural Networks with Tfidf vectors using\u2026",
        "text": "This is the 9th part of my ongoing Twitter sentiment analysis project. You can find the previous posts from the below links.\n\nIn the previous post, I took a detour of implementing dimensionality reduction before I try neural network modelling. In this post, I will implement neural network first with the Tfidf vectors of 100,000 features including up to trigram.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nMy first idea was, if logistic regression is the best performing classifier, then this idea can be extended to neural networks. In terms of its structure, logistic regression can be thought as a neural network with no hidden layer, and just one output node. You can see this relationship more clearly from the pictures below.\n\nI will not go through the details of how neural networks work, but if you want to know more in detail, you can take a look at the post I wrote previously on implementing a neural network from scratch with Python. But for this post, I won\u2019t implement it from scratch but use a library called Keras. Keras is more of a wrapper, which can be run on top of other libraries such as Theano or TensorFlow. It is one of the most easy-to-use libraries with intuitive syntax yet powerful. If you are a newbie to neural network modelling as myself, I think Keras is a good place to start.\n\nThe best performing Tfidf vectors I got is with 100,000 features including up to trigram with logistic regression. Validation accuracy is 82.91%, while train set accuracy is 84.19%. I would want to see if the neural network can boost the performance of my existing Tfidf vectors.\n\nI will first start by loading required dependencies. In order to run Keras with TensorFlow backend, you need to install both TensorFlow and Keras.\n\nThe structure of below NN model has 100,000 nodes in the input layer, then 64 nodes in a hidden layer with Relu activation function applied, then finally one output layer with sigmoid activation function applied. There are different types of optimizing techniques for neural networks, and different loss function you can define with the model. Below model uses ADAM optimizing, and binary cross entropy loss.\n\nADAM is an optimisation algorithm for updating the parameters and minimising the cost of the neural network, which is proved to be very effective. It combines two methods of optimisation: RMSProp, Momentum. Again, I will focus on sharing the result I got from my implementation, but if you want to understand properly how ADAM works, I strongly recommend the \u201cdeeplearning.ai\u201d course by Andrew Ng. He explains the complex concept of neural network in a very intuitive way. If you want more in-depth material on the topic, you can also take a look at the research paper \u201cADAM: A Method for Stochastic Optimization\u201d by Kingma & Ba (2014).\n\nBefore I feed the data and train the model, I need to deal with one more thing. Keras NN model cannot handle sparse matrix directly. The data has to be dense array or matrix, but transforming the whole training data Tfidf vectors of 1.5 million to dense array won\u2019t fit into my RAM. So I had to define a function, which generates iterable generator object, so that it can be fed to NN model. Note that the output should be a generator class object rather than directly returning arrays, this can be achieved by using \u201cyield\u201d instead of \u201creturn\u201d.\n\nIt looks like the model had the best validation accuracy after 2 epochs, and after that, it fails to generalise so validation accuracy slowly decreases, while training accuracy increases. But if you remember the result I got from logistic regression (train accuracy: 84.19%, validation accuracy: 82.91%), you can see that the above neural network failed to outperform logistic regression in terms of validation.\n\nLet\u2019s see if normalising inputs have any effect on the performance.\n\nThen I redefined the model and refit the model with \u201cx_train_tfidf_norm\u201d I got from the above normaliser.\n\nAnd the result comes out almost as same as without normalisation. And it is at this point I realised that Tfidf is already normalised by the way it is calculated. TF (Term Frequency) in Tfidf is not absolute frequency but relative frequency, and by multiplying IDF (Inverse Document Frequency) to the relative term frequency value, it further normalises the value in a cross-document manner.\n\nIf the problem of the model is a poor generalisation, then there is another thing I can add to the model. Even though the neural network is a very powerful model, sometimes overfitting to the training data can be a problem. Dropout is a technique that addresses this problem. If you are familiar with the concept of ensemble model in machine learning, dropout can also be seen in the same vein. According to the research paper \u201cImproving neural networks by preventing co-adaptation of feature detectors\u201d by Hinton et al. (2012), \u201cA good way to reduce the error on the test set is to average the predictions produced by a very large number of different networks. The standard way to do this is to train many separate networks and then to apply each of these networks to the test data, but this is computationally expensive during both training and testing. Random dropout makes it possible to train a huge number of different networks in a reasonable time.\u201d\n\nDropout is simulating as if we train many different networks and averaging them by randomly omitting hidden nodes with a certain probability throughout the training process. With Keras, this can be easily implemented just by adding one line to your model architecture. Let\u2019s see how the model performance changes with 20% dropout rate. (*I will gather all the results and present them with a table at the end.)\n\nThrough 5 epochs, the train set accuracy didn\u2019t get as high as the model without dropout, but validation accuracy didn\u2019t drop as low as the previous model. Even though the dropout added some generalisation to the model, but still the validation accuracy is still underperforming compared to logistic regression result.\n\nThere is another method I can try to prevent overfitting. By presenting the data in the same order for every epoch, there\u2019s a possibility that the model learns the parameters which also includes the noise of the training data, which eventually leads to overfitting. This can be improved by shuffling the order of the data we feed the model. Below I added shuffling to the batch generator function and tried with the same model structure and compared the result.\n\nThe same model with non-shuffled training data had training accuracy of 87.36%, and validation accuracy of 79.78%. With shuffling, training accuracy decreased to 84.80% but the validation accuracy after 5 epochs has increased to 82.61%. It seems like the shuffling did improve the model\u2019s performance on the validation set. And another thing I noticed is that with or without shuffling also for both with or without dropout, validation accuracy tends to peak after 2 epochs, and gradually decrease afterwards.\n\nI also tried the same model with 20% dropout with shuffled data, this time only 2 epochs that I will share the result at the end.\n\nAs I was going through the \u201cdeeplearning.ai\u201d course by Andrew Ng, he states that the first thing he would try to improve a neural network model is tweaking the learning rate. I decided to follow his advice and try different learning rates with the model. Please note that except for the learning rate, the parameter for \u2018beta_1\u2019, \u2018beta_2\u2019, and \u2018epsilon\u2019 are set to the default values presented by the original paper \u201cADAM: A Method for Stochastic Optimization\u201d by Kingma and Ba (2015).\n\nHaving tried four different learning rates (0.0005, 0.005, 0.01, 0.1), none of them outperformed the default learning rate of 0.001.\n\nMaybe I can try to increase the number of hidden nodes, and see how it affects the performance. Below model has 128 nodes in the hidden layer.\n\nWith 128 hidden nodes, validation accuracy got close to the performance of logistic regression. I could experiment further with increasing the number of hidden layers, but for the above 2 epochs to run, it took 5 hours. Considering that logistic regression took less than a minute to fit, even if the neural network can be improved further, this doesn\u2019t look like an efficient way.\n\nBelow is a table with all the results I got from trying different models above. Please note that I have compared performance at 2 epochs since some of the models only ran for 2 epochs.\n\nExcept for ANN_8 (with the learning rate of 0.1), the model performance only varies in the decimal place, and the best model is ANN_9 (with one hidden layer of 128 nodes) at 82.84% validation accuracy.\n\nAs a result, in this particular case, neural network models failed to outperform logistic regression. This might be due to the high dimensionality and sparse characteristics of the textual data. I have also found a research paper, which compared model performance with high dimension data. According to \u201cAn Empirical Evaluation of Supervised Learning in High Dimensions\u201d by Caruana et al.(2008), logistic regression showed as good performance as neural networks, in some cases outperforms neural networks.\n\nThrough all the trials above I learned some valuable lessons. Implementing and tuning neural networks is a highly iterative process and includes many trials and errors. Even though neural network is a more complex version of logistic regression, it doesn\u2019t always outperform logistic regression, and sometimes with high dimension sparse data, logistic regression can deliver good performance with much less computation time than neural network.\n\nIn the next post, I will implement a neural network with Doc2Vec vectors I got from the previous post. Hopefully with dense vectors such as Doc2Vec, the neural network might show some boost. Fingers crossed.\n\nAs always, thank you for reading. You can find the whole Jupyter Notebook from the link below."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-8-dimensionality-reduction-chi2-pca-c6d06fb3fcf3?source=user_profile---------8----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 8 (Dimensionality reduction: Chi2, PCA)",
        "text": "This is the 8th part of my ongoing Twitter sentiment analysis project. You can find the previous posts from the below links.\n\nIn the previous post, I have combined phrase modeling with doc2vec models and saw a slight improvement to the validation accuracy, and I was going to move on to neural networks to see how ANN can boost the performance. But I decided to take a detour, and try dimensionality reduction on the features I got from Tfidf vectorizer and Doc2Vec vectors.\n\nSo far, in terms of feature extraction, I have tried three different methods: count vectorizer, Tfdif vectorizer, doc2vec. The best validation results I got from each is as below.\n\nAnd in addition to these, I also have my custom-defined classifier using lexical approach.\n\nApart from the lexical approach, the number of features needed for the models seem quite big, so I decided to see if I can reduce feature dimensions of Tfidf vectorizer and doc2vec vectors. The vectors from Doc2Vec model is of 200 dimensions, sounds quite small compared to 100,000 features of Tfidf vectorizer. However, these 200-dimensional vectors are dense matrices with all real numbers, while 100,000 features are sparse matrices with lots of zeros. Thus in terms of computation, Doc2Vec vectors also take a bit of time for computation. So if I can reduce dimensions, then it will be helpful to run various settings of hyperparameter tuning of models.\n\nFirst, let\u2019s try to run dimensionality reduction on Tfidf vectors with chi-squared feature selection.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nIn Scikit-learn library, there are three methods you can use for feature selection with sparse matrices such as Tfidf vectors or count vectors. By looking at the documentation, you can see that chi2, mutual_info_regression, mutual_info_classif will deal with the data without making it dense. In my case, I have 1.5 million tweets and want to reduce dimensions from 100,000 features, thus transform this into dense matrices is not an option. It will not fit into my RAM.\n\nThe chi-squared statistic measures the lack of independence between a feature (in this case, one term within a tweet) and class (whether the tweets are positive or negative).\n\nYou can first think of the relationship between a term within a tweet and the class that the tweet belongs to as in terms of a contingency table. The contingency table is just a fancy word for a table that displays frequency distribution.\n\nLet\u2019s say we have three sentences in our corpus as below.\n\nAnd let\u2019s say the sentiment class of each tweet is positive, negative, positive. (By the way, I love dogs and cooking and cats too)\n\nLet\u2019s define \u2018t\u2019 as a specific term we are looking at, in this case, \u201cdogs\u201d, and \u2018c\u2019 as the class, since the class only have two classes, it will be either 1 (positive) or 0 (negative). Using contingency table, where A is the number of times \u2018t\u2019 occurs and \u2018c\u2019 is positive, B is the number of times \u2018t\u2019 occurs and \u2018c\u2019 is negative, C is the number of times \u2018t\u2019 doesn\u2019t occur and \u2018c\u2019 is positive, finally D is the number of times \u2018t\u2019 doesn\u2019t occur and \u2018c\u2019 is negative. Now we are ready to calculate Chi-squared statistic.\n\nWhere N is the total number of samples, Chi-square score for term \u201cdogs\u201d is 3! Since what chi-square measures is lack of independence between a feature and class, if a feature has high chi-squared score compared to other features, it means that the feature is useful to predict the class.\n\nI will first transform the train data into Tfidf vectors of 100,000 features and see which features chi2 has chosen as useful features. Let\u2019s plot the scores we got on a graph and see which word features are useful for prediction. I will plot only the top 20 features in the below chart, but you can plot as many as you wish as long as your computer screen allows.\n\nThe most useful feature selected by chi2 is the word \u201cthanks\u201d, and I assume that this came from mostly positive tweets. The second most useful features it chose is the word \u201csad\u201d, and this time I guess it came from negative tweets. If you consider how chi2 is calculated, it will not only score highly on terms predictive of positive class but also score highly on terms predictive of negative class.\n\nSince now we have an idea of how the chi2 feature selection works, let\u2019s reduce the dimensions to different numbers of features, and also check the accuracy on the validation set.\n\nOne more thing. Tfidf vectorizer can limit the number of features in the first place when you fit and transform the corpus. I want to compare the validation accuracy at the same number of features 1) when the number of features has been limited from Tfidf vectorizing stage, 2) when the number of features has been reduced from 100,000 features using chi2 statistic.\n\n*In the code block below, I\u2019m plotting the result I got from running Tfidf vectorizer on different max features, again you can find the full code on the Jupyter Notebook at the end of this post.\n\nOn the above graph, the red dotted line is validation set accuracy from dimensionality reduction, and the blue line is the result of limiting the number of features in the first place when fitting Tfidf vectorizer. We can see that limiting the number of features in the first place with Tfidf vectorizer yield better result than reducing the dimensions from bigger features. This is not a general statement, but what I have found within this particular setting. If you have a different result with other corpora, I would love to know how it differs.\n\nPCA is a dimension reduction tool that can be used to reduce a large set of variables to a small set that still contains most of the information in the original set. That sounds cool, you can reduce the features of your data but not retaining the most of the information needed. But if you have tried googling \u201cPCA\u201d, you might know that it will give you back the result with all the difficult-sounding terms, such as \u201cEigenvalue\u201d, \u201cEigenvector\u201d, \u201cmatrix projection\u201d, etc.\n\nIn this post, I will not go into too much detail of how PCA is actually calculated but will try to keep it to intuitive level, so that anyone who reads this can understand the underlying basics of it, and implement with Python.\n\nWhat PCA does is that it transforms the coordinate system so that the axes become the most concise, informative descriptors of our data as a whole.\n\nAbove picture has been borrowed from Meng Xu\u2019s blog post on PCA. I found an explanation by Meng Xu really helps a lot to understand the concept intuitively. The shape you see in figure (A) is on 3 dimensions, but if we focus on the shape of data, not the axis, the shape itself is flat two-dimension surface. By running PCA, we find new coordinates for the data, which will best describe how the data is shaped. The first Principal Component is one that explains the most variance in the data. In figure (B), we see that by drawing the line \u201cComponent 1\u201d, it is able to retain the information of the most dispersed points of data. And by adding the line \u201cComponent 2\u201d, this \u201cComponent 2\u201d line explains the second most variance in our data. The next step is to transform the original data onto our new found axis which is just two instead of original three dimensions. The final result in figure ( C) gives us a pretty good picture of how the data is shaped only with two dimensions even though we have discarded the third dimension.\n\nAnd eigenvectors and eigenvalues are used when implementing this transformation of the data. An eigenvector specifies a direction through the original coordinate space, while eigenvalues indicate the amount of variance in the direction of its corresponding eigenvector.\n\nIf you want to dive deeper into the concept of PCA, there are some more blog posts I found useful.\n\nNext, let\u2019s try to reduce dimensions of doc2vec vectors with PCA. We can also plot the result on a graph and see if it\u2019s feasible to reduce the number of features to a smaller set of principal components, and how much of the variance the given number of principal components can explain about the original features.\n\nIn the above graph, the red line represents cumulative explained variance and the blue line represents explained the variance of each principal component. By looking at the graph above, even though the red line is not perfectly linear, but very close to a straight line. Is this good? No. This means each of the principal components contributes to the variance explanation almost equally, and there\u2019s not much point in reducing the dimensions based on PCA. This can also be seen from the blue line, which is very close to a straight line in the bottom.\n\nIt is a bit disappointing, that in my specific case with tweets text data, dimensionality reduction is not very helpful. Tfidf vectors showed a better result when the features are limited with Tfidf vectorizer in the first place than to reduce dimension afterwards, and doc2vec vectors seem to carry a roughly similar amount of information through its 200 dimension feature space.\n\nAgain this is not a general statement, this is just what I found out with my particular data. And especially with PCA, when it is applied to numerical features, I saw it successfully reduce the dimension of the data from 100 or more features to around 10 features while being able to explain 90% of the data variance.\n\nIf you have expected neural network modeling for this post, I am sorry I had to take a detour, but in the next post, I will definitely go through neural network modeling. As always, thank you for reading.\n\nYou can find the Jupyter Notebook from the link below:"
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-7-phrase-modeling-doc2vec-592a8a996867?source=user_profile---------9----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 7 (Phrase modeling + Doc2Vec)",
        "text": "This is the 7th part of my ongoing Twitter sentiment analysis project. You can find the previous posts from the below links.\n\nIn the last post, I implemented Doc2Vec and try a simple logistic regression with the document vectors I get from Doc2Vec model. Unfortunately, the 100 dimension document vectors did not outperform 100,000 dimension Tfidf features in sentiment analysis task. This time, I will try phrase modelling to see if I can improve the performance of Doc2Vec vectors any further.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nAnother thing that can be implemented with Gensim library is phrase detection. It is similar to n-gram, but instead of getting all the n-gram by sliding the window, it detects frequently used phrases and stick them together.\n\nThis has been introduced by Mikolov et. al (2013), and it is proposed to learn vector representation for phrases, which have a meaning that is not a simple composition of the meanings of its individual words. \u201cThis way, we can form many reasonable phrases without greatly increasing the size of the vocabulary.\u201d\n\nPatrick Harrison in PyData DC 2016 has also provided a very intuitive explanation of Gensim\u2019s phrase modelling.\n\nOK let\u2019s see how this actually works.\n\nNow we have a bigram model that will detect frequently used phrases of two words, and stick them together. For example, let\u2019s say we have a sentence \u201cThe mayor of New York was there\u201d, and see what happens if we feed this sentence into the bigram phrase model.\n\nAs you can see from the above example, with the tweets corpus it has learned \u201cNew York\u201d as a frequently used phrase. So now feeding the \u201cbigram\u201d with tokens separated \u201cnew\u201d and \u201cyork\u201d, it will automatically put them together into one word as \u201cnew_york\u201d.\n\nLet\u2019s test the bigram with one of the training data from the tweet dataset. Below is the original tweet. (And yes that is indeed very sad)\n\nIf we feed this sentence into the bigram model,\n\nwe can see that the bigram model has recognised \u201cvanilla_ice\u201d as a phrase. But if it can only catch \u201cvanilla_ice\u201d not \u201cvanilla_ice_cream\u201d, sometimes \u201cvanilla_ice\u201d can mean something else.\n\nI will come back to this later.\n\nNow let\u2019s transform our corpus with this bigram model.\n\nAfter I get the corpus with bigram phrases detected, I went through the same process of Doc2Vec I did with unigram.\n\nIn the below code block, you will find some of the dependencies used without importing, and also custom defined function \u201cget_vectors\u201d. You can find the whole version of Jupyter Notebook at the end of this post.\n\nRegarding validation accuracy, I will show you later with all the doc2vec models tried up until trigram.\n\nUnlike DBOW model, DM model can learn meaningful semantic vector representation of each word as well.\n\nSince now we have bigram phrase detected corpus if we look for the most similar words to \u201cnew_york\u201d, the model says it is \u2018ny\u2019, which is pretty amazing, and you can also see other city names as \u2018chicago\u2019, \u2018berlin\u2019, etc.\n\nAnd as I did in the last post, I tried the two combined models: DBOW + DMC, DBOW + DMM. You can find the code in the Jupyter Notebook I will share at the end.\n\nAnd if we run the same phrase detection again on bigram detected corpus, now it will detect trigram phrases.\n\nAgain I ran three different models of doc2vec with trigram detected corpus, and below is the result I got from unigram to trigram with different doc2vec models.\n\nThe best validation accuracy I can get was from dbow+dmm model.\n\nDMM model tends to perform better with increased n-gram, while pure DBOW model tends to perform worse with increased n-gram. The combined models are (i.e. dbow +dmc) and (dbow +dmm) produced a lower result with bigram and a higher result with trigram, but considering these differences are all in the decimal places, it might be hard to say that phrase detection had effects on the combined model structure. (Please note that this is an empirical finding in this particular setting, but if you would like to try this with your chosen corpus, I would definitely want to know whether this holds in different situation or not.)\n\nBefore I move on to next step, I would like to try one more thing, which is creating joint vectors across different n-grams. By looking at the above table, for DBOW model unigram performed the best, so I will use vectors from unigram DBOW model and join this together with trigram DMM vectors.\n\nAnd the validation accuracy for \u2018unigram DBOW + trigram DMM\u2019 model is 75.76%, which is slightly better than \u2018trigram DBOW + DMM\u2019 model, but again the difference is in the decimal place, and I am not sure if I can say that this is a significant improvement.\n\nIn the next post, I will implement neural network models with both the vectors from Doc2Vec and Tfidf sparse matrix, and compare the results with simple logistic regression.\n\nThank you for reading, and you can find the Jupyter Notebook from the below link."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-6-doc2vec-603f11832504?source=user_profile---------10----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 6 (Doc2Vec)",
        "text": "This is the 6th part of my ongoing Twitter sentiment analysis project. You can find the previous posts from the below links.\n\n*In addition to short code blocks I will attach, you can find the link for the whole Jupyter Notebook at the end of this post.\n\nBefore we jump into doc2vec, it will be better to mention word2vec first. \u201cWord2vec is a group of related models that are used to produce word embeddings. These models are shallow, two-layer neural networks that are trained to reconstruct linguistic contexts of words.\u201d\n\nWord2vec is not a single algorithm but consists of two techniques \u2014 CBOW(Continuous bag of words) and Skip-gram model. Both of these techniques learn weights which act as word vector representations. With a corpus, CBOW model predicts the current word from a window of surrounding context words, while Skip-gram model predicts surrounding context words given the current word. In Gensim package, you can specify whether to use CBOW or Skip-gram by passing the argument \u201csg\u201d when implementing Word2Vec. By default (sg=0), CBOW is used. Otherwise (sg=1), skip-gram is employed.\n\nFor example, let\u2019s say we have a following sentence: \u201cI love dogs\u201d. CBOW model tries to predict the word \u201clove\u201d when given \u201cI\u201d, \u201cdogs\u201d as inputs, on the other hand Skip-gram model tries to predict \u201cI\u201d, \u201cdogs\u201d when given the word \u201clove\u201d as input.\n\nBelow picture represents more formally how these two models work.\n\nBut what\u2019s used as word vectors are actually not the prediceted results from these models but the weights of the trained models. By extracting the weights, such a vector comes to represent in some abstract way the \u2018meaning\u2019 of a word. If you want to know more in detail about how word2vec model works, there is a great paper by Xin Rong (2016), which explains each step of the model in detail.\n\nThen what is doc2vec? Doc2vec uses the same logic as word2vec, but apply this to document level. According to Le and Mikolov(2014), \u201cevery paragraph is mapped to a unique vector, represented by a column in matrix D and every word is also mapped to a unique vector, represented by a column in matrix W. The paragraph vector and word vectors are averaged or concatenated to predict the next word in a context\u2026The paragraph token can be thought of as another word. It acts as a memory that remembers what is missing from the current context \u2014 or the topic of the paragraph.\u201d\n\nDM: This is the Doc2Vec model analogous to CBOW model in Word2vec. The paragraph vectors are obtained by training a neural network on the task of inferring a center word based on context words and a context paragraph.\n\nDBOW: This is the Doc2Vec model analogus to Skip-gram model in Word2Vec. The paragraph vectors are obtained by training a neural network on the task of predicting a probability distribution of words in a paragraph given a randomly-sampled word from the paragraph.\n\nI implemented Doc2Vec model using a Python library, Gensim. In case of DM model, I implemented both averaging method and concatenating method. This is inspired by the research paper from Le and Mikolov (2014). In their paper, they have implemented DM model in two different ways, using avearage calculation process, or concatenating calculation method. This has also been shown in Gensim\u2019s tutorial.\n\nBelow are the methods I used to get the vectors for each tweet.\n\nWith the vectors I got from above models, I fit a simple logistic regression model, and evaluated the result on the validation set.\n\nAs a preparation, in addition to loading the needed dependencies, we also need to labelise each tweet with unique IDs using Gensim\u2019s LabeledSentence function.\n\nFor training Doc2Vec, I used the whole data set. The rationale behind this is that the doc2vec training is completely unsupervised and thus there is no need to hold out any data, as it is unlabelled. This rationale is inspired by the rationale of Lau and Baldwin (2016) in their research paper \u201cAn Empirical Evaluation of doc2vec with Practical Insights into Document Embedding Generation\u201d\n\nAlso the same rationale has been applied in the Gensim\u2019s Doc2Vec tutorial. In the IMDB tutorial, vector training is occuring on all documents of the data set, including all train/test/dev set.\n\nLet\u2019s first train pure DBOW model. Note that DBOW model will not yield any meaningful vectors at word level, due to the way it is trained. But when we get to Distributed Memory model, we will take a look at word vectors as well.\n\nAccording to the developer Radim \u0158eh\u016f\u0159ek who created Gensim, \u201cOne caveat of the way this algorithm runs is that, since the learning rate decrease over the course of iterating over the data, labels which are only seen in a single LabeledSentence during training will only be trained with a fixed learning rate. This frequently produces less than optimal results.\u201d\n\nBelow iteration implement explicit multiple-pass, alpha-reduction approach with added shuffling. This has been already presented in Gensim\u2019s IMDB tutorial.\n\nIn the above code block, I also defined a function \u2018get_vectors\u2019 to extract document vectors from the trained doc2vec model, this function will be reused with other doc2vec models as well.\n\nThe accuracy tested on the validation set with logistic regression is 73.89%. Even though the DBOW model doesn\u2019t learn the meaning of individual word, but as features to feed to a classifier, it seems like it\u2019s doing its job.\n\nBut it doesn\u2019t seem to outperform count vectorizer or Tfidf vectorizer (Tfidf vectorizer with 100,000 features, the validation set accuracy was 82.92% with logistic regression).\n\nNow let\u2019s move on to Distributed Memory model, and see how this performs. I will first try with concatenation method for training.\n\nUnfortunately, the accuracy on the validation set turned out to be 66.47%, which is a bit disappointing. But this does not mean that the model has failed to learn the valid vector representation at word level.\n\nBy looking at similar words to \u201cfacebook\u201d after training, it looks like the model properly catches the meaning of SNS, web service. Also the model successfully catches the comparative form of \u201csmall\u201d, on feeding the word \u201cbig\u201d and \u201cbigger\u201d. The above line of code is like asking the model to add the vectors associated with the word \u201cbigger\u201d and \u201csmall\u201d while subtracting \u201cbig\u201d is equal to the top result, \u201csmaller\u201d.\n\nLet\u2019s try another method of training DM model.\n\nThe validation set accuracy is 72.56%, which is much better than DMC model and slightly lower than DBOW model. Let\u2019s also take a look at what it has learned.\n\nIt looks like it has learned well about the meaning of \u2018good\u2019, and the most similar word the model chose for \u2018good\u2019 is \u2018great. Great!\n\nSince I have the document vectors from three different models, now I can concatenate them in combination to see how it affects the performance. Below I defined a simple function to concatenate document vectors from different models.\n\nThe validation set accuracy for DBOW + DMC model is 74.58%, which has improved from pure DBOW model (73.89%). Let\u2019s try another combination.\n\nThis time by combining DBOW and DMM together, I get validation set accuracy of 75.51%.\n\nIn case of unigram, I learned that concatenating document vectors in different combination boosted the model performance. The best validation accuracy I got from single model is from DBOW at 73.89%. With concatenated vectors, I get the highest validation accuracy of 75.51% with DBOW+DMM model.\n\nIn the next post, I will take a look at phrase modeling using Gensim, and apply this to Doc2Vec to see if this affects performance.\n\nThank you for reading, and you can find the Jupyter Notebook from the below link."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-5-50b4e87d9bdd?source=user_profile---------11----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 5 (Tfidf vectorizer, model comparison\u2026",
        "text": "This is the 5th part of my ongoing Twitter sentiment analysis project. You can find the previous posts from the below links.\n\nIn the last part, I tried count vectorizer to extract features and convert textual data into a numeric form. In this part, I will use another feature extraction technique called Tfidf vectorizer.\n\nTFIDF is another way to convert textual data to numeric form, and is short for Term Frequency-Inverse Document Frequency. The vector value it yields is the product of these two terms; TF and IDF.\n\nLet\u2019s first look at Term Frequency. We have already looked at term frequency with count vectorizer, but this time, we need one more step to calculate the relative frequency. Let\u2019s say we have two documents in our corpus as below.\n\nRelative term frequency is calculated for each term within each document as below.\n\nFor example, if we calculate relative term frequency for \u2018I\u2019 in both document 1 and document 2, it will be as below.\n\nNext, we need to get Inverse Document Frequency, which measures how important a word is to differentiate each document by following the calculation as below.\n\nIf we calculate inverse document frequency for \u2018I\u2019,\n\nOnce we have the values for TF and IDF, now we can calculate TFIDF as below.\n\nFollowing the case of our example, TFIDF for the term \u2018I\u2019 in both documents will be as below.\n\nAs you can see, the term \u2018I\u2019 appeared equally in both documents, and the TFIDF score is 0, which means the term is not really informative in differentiating documents. The rest is same as count vectorizer, TFIDF vectorizer will calculate these scores for terms in documents, and convert textual data into the numeric form.\n\nOnce I instantiate Tfidf vectorizer, and fit the Tfidf-transformed data to logistic regression, and check the validation accuracy for a different number of features.\n\nFrom this post I will attach a Gist link to a code block when I mention it rather than pasting the whole code as snippet directly inside the post, moreover, you can also find the whole Jupyter Notebook from the link I will share at the end of this post.\n\nSince I also have the result from count vectorizer, I tried in the previous post, I will plot them together on the same graph to compare.\n\nFrom above chart, we can see including bigram and trigram boost the model performance both in count vectorizer and TFIDF vectorizer. And for every case of unigram to trigram, TFIDF yields better results than count vectorizer.\n\nThe best result I can get with logistic regression was by using TFIDF vectorizer of 100,000 features including up to trigram. With this I will first fit various different models and compare their validation results, then will build an ensemble (voting) classifier with top 5 models.\n\nI haven\u2019t included some of the computationally expensive models, such as KNN, random forest, considering the size of data and the scalability of models. And the fine-tuning of models will come after I try some other different vectorisation of textual data.\n\nI will not go into detail of explaining how each model works since it is not the purpose of this post. You can find many useful resources online, but if I get many questions or requests on a particular algorithm, I will try to write a separate post dedicated to the chosen model.\n\nAnd the results for comparison is as below.\n\nIt looks like logistic regression is my best performing classifier.\n\nAnd the result for the ensemble classifier, which takes votes from the top 5 model from the above result (linear regression, linear SVC, multinomial NB, ridge classifier, passive-aggressive classifier) is as below. Note that I did not include \u201clinear SVC with L-1 based feature selection\u201d model in the voting classifier, since it is the same model as Linear SVC, except for the fact that it filters out features first by L-1 regularization, and comparing the results linear SVC without the feature selection showed a better result.\n\nThe validation set accuracy of the voting classifier turned out to be 82.47%, which is worse than the logistic regression alone, which was 82.92%.\n\nWhat I have demonstrated above are machine learning approaches to text classification problem, which tries to solve the problem by training classifiers on a labeled data set. Another famous approach to sentiment analysis task is the lexical approach. \u201cIn the lexical approach the definition of sentiment is based on the analysis of individual words and/or phrases; emotional dictionaries are often used: emotional lexical items from the dictionary are searched in the text, their sentiment weights are calculated, and some aggregated weight function is applied.\u201d http://www.dialog-21.ru/media/1226/blinovpd.pdf\n\nIn the part 3 of this series, I have calculated harmonic mean of \u201cpositive rate CDF\u201d and \u201cpositive frequency percent CDF\u201d, and these have given me a good representation of positive and negative terms in the corpus. If it successfully filters which terms are important to each class, then this can also be used for prediction in lexical manner.\n\nSo I decided to make a simple predictor, which make use of the harmonic mean value I calculated. Below I go through the term frequency calculation, and the steps to get \u2018pos_normcdf_hmean\u2019, but this time I calculated term frequency only from the train set. (* Since I learned that I don\u2019t need to transform sparse matrix to dense matrix for term frequency calculation, I computed the frequency directly from sparse matrix)\n\nIf you want a more detailed explanation of the formula I have applied to come up with the final values of \u201cpos_norcdf_hmean\u201d, you can find it in part 3 of this series.\n\nThe calculation of the positivity score I decided is fairly simple and straightforward. For each word in a document, look it up in the list of 10,000 words I built vocabulary with, and get the corresponding \u2018pos_normcdf_hmean\u2019 value, then for the document calculate the average \u2018pos_normcdf_hmean\u2019 value. If none of the words can be found from the built 10,000 terms, then yields random probability ranging between 0 to 1. And the single value I get for a document is handled as a probability of the document being positive class.\n\nNormally, a lexical approach will take many other aspects into the calculation to refine the prediction result, but I will try a very simple model.\n\nWith the average value of \u201cpos_hmean\u201d, I decide the threshold to be 0.56, which means if the average value of \u201cpos_hmean\u201d is bigger than 0.56, the classifier predicts it as a positive class, if it\u2019s equal to or smaller than 0.56, it will be predicted as a negative class. And the result from the above model is 75.96%. The accuracy is not as good as logistic regression with count vectorizer or TFIDF vectorizer, but compared to null accuracy, 25.56% more accurate, and even compared to TextBlob sentiment analysis, my simple custom lexicon model is 15.31% more accurate. This is an impressive result for such a simple calculation and also considering the fact that the \u2018pos_normcdf_hmean\u2019 is calculated only with the training set.\n\nIn the next post, I will try to implement Doc2Vec to see if the performance gets better.\n\nThank you for reading, and you can find the Jupyter Notebook from the below link."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-4-count-vectorizer-b3f4944e51b5?source=user_profile---------12----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 4 (Count vectorizer, confusion matrix)",
        "text": "This is the part 4 of my ongoing Twitter sentiment analysis project. You can find the previous posts from below links.\n\nIn part 3, I mainly focused on EDA and data visualisation, now it\u2019s time to prepare for model building!\n\nBefore we can train any model, we first consider how to split the data. Here I chose to split the data into three chunks: train, development, test. I referenced Andrew Ng\u2019s \u201cdeeplearning.ai\u201d course on how to split the data.\n\nThe ratio I decided to split my data is 98/1/1, 98% of data as the training set, and 1% for the dev set, and the final 1% for the test set. The rationale behind this ratio comes from the size of my whole data set. The dataset has more than 1.5 million entries. In this case, only 1% of the whole data gives me more than 15,000 entries. This is more than enough to evaluate the model and refine the parameters.\n\nAnother approach is splitting the data into only train and test set, and run k-fold cross validation on the training set, so that you can have an unbiased evaluation of a model. But considering the size of the data, I have decided to use the train set only to train a model, and evaluate on the dev set, so that I can quickly test different algorithms and run this process iteratively.\n\nIf you want to know more in detail about data split for modelling, I recommend a blog post from \u201cMachine Learning Mastery\u201d. The blog has well-documented posts about a wide range of topics in data science, highly recommended.\n\nOnce I load the cleaned tweets dataset, I run data split as below. Note that I have split the data two times, the first to split train and dev+test, and the second to split dev and test.\n\nWhen comparing various machine learning algorithms, baseline provides a point of reference to compare. The most popular baseline is the Zero Rule (ZeroR). ZeroR classifier simply predicts the majority category (class). Although there is no predictability power in ZeroR, it is useful for determining a baseline performance as a benchmark for other classification methods. As you can see from the above validation set class division, the majority class is negative with 50.40%, which means if a classifier predicts negative for every validation data, it will get 50.40% accuracy.\n\nAnother baseline I wanted to compare the validation results with is TextBlob. TextBlob is a python library for processing textual data. Apart from other useful tools such as POS tagging, n-gram, The package has built-in sentiment classification. This is a so-called out-of-the-box sentiment analysis tool, and in addition to the null accuracy, I will also keep in mind of the accuracy I get from TextBlob sentiment analysis to see how my model is performing.\n\nTextBlob sentiment analysis yielded 60.65% accuracy on the validation set, which is 10.25% more accurate than null accuracy (50.40%).\n\nIf we want to use text in machine learning algorithms, we\u2019ll have to convert them to a numerical representation. One of the methods is called bag-of-words approach. The bag of words model ignores grammar and order of words. Once we have a corpus (text data) then first, a list of vocabulary is created based on the entire corpus. Then each document or data entry is represented as numerical vectors based on the vocabulary built from the corpora.\n\nWith count vectorizer, we merely count the appearance of the words in each text. For example, let\u2019s say we have 3 documents in a corpus: \u201cI love dogs\u201d, \u201cI hate dogs and knitting\u201d, \u201cKnitting is my hobby and my passion\u201d. If we build vocabulary from these three sentences and represent each document as count vectors, it will look like below pictures.\n\nBut if the size of a corpus gets big, the number of vocabulary gets too big to process. With my 1.5 million tweets, if I build vocabulary without limiting the number of vocabulary, I will have more than 260,000 vocabularies. This means that the shape of training data will be around 1,500,000 x 260,000, this sounds too big to train various different models with. So I decided to limit the number of vocabularies, but I also wanted to see how the performance varies depending on the number of vocabularies.\n\nAnother thing I wanted to explore is stop words. Stop Words are words which do not contain important significance, such as \u201cthe\u201d, \u201cof\u201d, etc. It is often assumed that removing stop words is a necessary step, and will improve the model performance. But I wanted to see for myself if this is really the case. So I ran the same test with and without stop words and compared the result. In addition, I also defined my custom stop words list, which contains top 10 most frequent words in the corpora: \u201cto\u201d, \u201cthe\u201d, \u201cmy\u201d, \u201cit\u201d, \u201cand\u201d, \u201cyou\u201d, \u201cnot\u201d, \u201cis\u201d, \u201cin\u201d, \u201cfor\u201d.\n\nA model I chose to evaluate different count vectors is logistic regression. It is one of linear models, so computationally scalable to big data, compared to models like KNN or random forest. And once I have the optimal number of features and make a decision on whether to remove stop words or not, then I will try different models with the chosen number of vocabularies\u2019 count vectors.\n\nBelow I define two functions to iteratively train on the different number of features, then check the accuracy of logistic regression on the validation set.\n\nThen I checked the accuracy on validation set for the different number of features by calling the \u201cnfeature_accuracy_checker\u201d I defined above. In addition, I have defined the custom stop words of top 10 frequent term to compare the result. Note that \u2018term_freq_df.csv\u2019 is the file I created from previous part of the project with the corpus.\n\nJust to double check if these top 10 words are actually included in SKLearn\u2019s stop words list, I run a small code as below and see all the 10 words are also in SKLearn\u2019s stop words list.\n\nNow I can run \u201cnfeature_accuracy_checker\u201d on three different conditions. First with stop words removal, second with custom defined stop words removal, third without stop words removal.\n\nI will show the result from above accuracy check with a graph.\n\nBy looking at the evaluation result, removing stop words did not improve the model performance, but keeping the stop words yielded better performance. I wouldn\u2019t say that removing stop words are not helping the model performance every time, but as empirical findings, in this particular setting, keeping the stop words improve the model performance.\n\nAccording to Wikipedia, \u201cn-gram is a continuous sequence of n items from a given sequence of text or speech\u201d. In other words, n-grams are simply all combinations of adjacent words or letters of length n that you can find in your source text. Below picture represents well how n-grams are constructed out of source text.\n\nIn this project, I will extend the bag-of-words to trigrams, and see how it affects the performance.\n\nAfter I run the above code, I plot the results of unigram, bigram, trigram together on a graph as below.\n\nThe best validation set accuracy for each n-gram is as below.\n\nBelow I defined another function to take a closer look at best performing number of features with each n-gram. Below function not only reports accuracy but also gives confusion matrix and classification report.\n\nBefore I run the defined function, let me briefly explain about confusion matrix and classification report. In order to evaluate the performance of a model, there are many different metrics that can be used. Below I will talk in case of binary classification, in which the target variable only has two classes to be predicted. In the case of this project, the classes are either \u201cnegative\u201d or \u201cpositive\u201d.\n\nOne obvious measure of performance can be accuracy. It is the number of times the model predicted correctly for the class over the number of the whole data set. But in case of classification, this can be broken down further. Below is a representation of confusion matrix.\n\nIn the above matrix, each row represents the instances in an actual class while each column represents the instances in a predicted class, and it can be also presented swapping rows and columns (column for the actual class, row for predicted class). So the accuracy (ACC) I mentioned above can be expressed as below.\n\nWhen the distribution of the classes in data is well balanced, accuracy can give you a good picture of how the model is performing. But when you have skewed data, for example, one of the class is dominant in your data set, then accuracy might not be enough to evaluate your model. Let\u2019s say you have a dataset which contains 80% positive class, and 20% negative class. This means that by predicting every data into the positive class, the model will get 80% accuracy. In this case, you might want to explore further into the confusion matrix and try different evaluation metrics.\n\nThere can be 9 different metrics, just from the combination of numbers from confusion matrix, but I will talk about two of them in particular, and another metric which combines these two.\n\n\u201cPrecision\u201d (also called Positive Predictive Value) tells you what proportion of data predicted as positive actually is positive. In other words, the proportion of True Positive in the set of all positive predicted data.\n\n\u201cRecall\u201d (also called Sensitivity, Hit Rate, True Positive Rate) tells you what proportion of data that actually is positive were predicted positive. In other words, the proportion of True Positive in the set of all actual positive data.\n\nBelow is the image of confusion matrix of cancer diagnose. If you think of \u201ccancer\u201d as positive class, \u201cno cancer\u201d as a negative class, the image explains well how to think of precision and recall in terms of the confusion matrix.\n\nAnd finally, F1 score is the harmonic mean of precision and recall. The harmonic mean is a specific type of average, which is used when dealing with averages of units, like rates and ratios. So by calculating the harmonic mean of the two metrics, it will give you a good idea of how the model is performing both in terms of precision and recall. The formula is as below.\n\nFrom the above classification reports, we can see that model has slightly higher precision in negative class and higher recall in positive class. But this averages out by calculating the F1 score, and for both classes, we get the almost same F1 score for both positive and negative class. There is also a way to visualise the model performance by plotting ROC curve, but I will explain more in detail later.\n\nIn this post, I have looked at how the logistic regression is performing based on the features extracted with count vectorizer. In the next post, I will experiment with Tfidf Vectorizer and see how the results differ.\n\nThank you for reading this long post! You can find the Jupyter Notebook of above code from below link."
    },
    {
        "url": "https://towardsdatascience.com/basic-time-series-analysis-and-trading-strategy-with-bitcoin-price-data-1a8f1a30f11?source=user_profile---------13----------------",
        "title": "Basic Time Series Analysis and Trading Strategy with Bitcoin Price Data",
        "text": "Time series analysis is not an easy topic I must say. At least for me. During my class in General Assembly London\u2019s Data Science Immersive course, I had a chance to learn the topic, and it took me some time to get my head around it.\n\nThe purpose of this post is to reinforce what I have learned by implementing and trying to explain to others. I am not an expert in finance, so any advice or feedback from someone who\u2019s familiar with the field would be highly appreciated.\n\nA time series is a series of data points indexed (or listed or graphed) in time order. Time series data should be treated differently to other types of data. One of the statistical data assumptions is its independence. Independence means the value of one observation does not influence or affect the value of other observations. But in time series data, each data point is close together in time, and they are not completely independent of their adjacent values. So we need a different approach to model time series data.\n\nFirst, start by loading dependencies. Pandas_datareader is a useful library to extract financial data from the web. For this post, I will extract data from Yahoo Finance.\n\nOK, let\u2019s get the price data of BTC-USD (Bitcoin value in USD) starting from the start of 2017 to 27/12/2017.\n\nAs you can see, the data has six columns. \u201cOpen\u201d for the opening price for the day, \u201cHigh\u201d for the highest price during the day, \u201cLow\u201d for the lowest price during the day, \u201cClose\u201d for the closing price for the day, \u201cAdj Close\u201d for the adjusted closing price, \u201cVolume\u201d for transaction volume. In stock price, adjusted closing price reflects a stock\u2019s closing price on any given day of trading that has been amended to include any distributions and corporate actions that occurred at any time prior to the next day\u2019s open.\n\nBut to be 100% honest, I am not sure what factors will be taken into account for the adjusted closing price of a currency, but by looking at the difference of \u201cClose\u201d column, and \u201cAdj Close\u201d column, it seems like they are exactly the same.\n\nIt looks like the USD value of Bitcoin has been steady during the first half of 2017, but started to oscillate and steeply went upward from around November.\n\nOne of the basic analysis technique for time series data is moving average. As the name suggests, rather than calculating the average on the whole dataset, moving average (also called rolling mean) calculates the average of a subset with a certain window size, and shifts forward. Moving average is used to smooth out short-term fluctuations and highlight longer-term trends or cycles.\n\nLet\u2019s see how moving average works on a graph by plotting price data from 01/10/2017 to 27/12/2017. I chose the 4th quarter of 2017 to plot where there is a strong trend in the data to clearly see how moving average works.\n\nCompared to the original observation, which is plotted with a blue line, we can see the curve of the lines get smoother as window sizes get bigger.\n\nAnother use case of moving average is in a trading strategy called dual moving average crossover. I learned about this through a Medium blog post, and it was very well explained, not only about momentum strategy but also general use case of Python in trading strategy modeling. I highly recommend the post if you are a beginner who\u2019s interested in financial modeling with Python.\n\nWhat I implemented below is just an application of the tutorial from the blog post I mentioned, but I changed to short window to 10 days period rather than 40 days of the original post, and for longer term I used 50 days and named it mid_window instead of using 100 days as the author of the original post did. I guess by changing the time frames shorter, the strategy will be more short-sighted. But I am not a finance expert, and I am not sure if I am making any fundamental mistakes by changing the time frame shorter. If anyone who\u2019s reading this is a finance expert, please feel free to correct me if there\u2019s any mistake in this logic.\n\nThe concept of a dual moving average crossover is fairly straightforward. Calculate two moving averages of the price, one average would be the short term and the other long term. The long term moving average will have a lower variance and will move in the same direction as the short term moving average but at a different rate. The different rates of direction induces points where the values of the two moving averages may equal and or cross one another. These points are called the crossover points. In the dual moving average crossover trading strategy, these crossovers are points of decision to buy or sell the currencies.\n\nAccording to the author of the original blog post, \u201cA buy signal is generated when the short-term average crosses the long-term average and rises above it, while a sell signal is triggered by a short-term average crossing long-term average and falling below it\u201d.\n\nHowever, while looking for more material on the subject, I have also found out that there is an opposite approach to the same signal points. The above method introduced by the author is called The Technical Approach and the other approach is called The Value Approach.\n\n\u201cThe Value Approach offers the opposite trading signals to the Technical Approach. The Value Approach claims that when the short-term average crosses from below to above the long-term average, that the investment is now overvalued, and should be sold. Conversely when the currency short-term average moves below the long-term average then the currency is undervalued it should be bought\u201d.\n\nIt is very interesting to see that there are two contradictory views on the same situation. But in this post, I will focus on The Technical Approach. Now let\u2019s see how this can be applied to real data.\n\nNow we can plot it to see how it looks on the chart.\n\nWith The Techincal Approach, I would have started my investment with 1,048.89 USD, and during the time period of roughly a year, I would have needed 1,551.77 USD to keep investing, and the final value of my 1 Bitcoin would have been valued at 1,3864.87 USD now (27/12/2017). It might be a naive calculation, not taking into account transaction fees or other costs which might have occurred. But in a simplified calculation, not a bad investment.\n\nIf The Value Approach is applied to the same graph, the situation might not be the same. There are the concepts of \u201cselling short\u201d and \u201cselling long\u201d in finance. \u201cSelling short an asset means selling an asset we do not currently hold and receiving its value in cash. Selling short is different than selling an asset we already own, which is called selling long.\u201d To me this always sounds a bit counter-intuitive, selling something one doesn\u2019t even own. If you are interested in learning more about this, you might find this page from Investopedia helpful.\n\nFrom The Technical Approach, the last transaction was \u201cbuy\u201d, but this in The Value Approach would have been \u201csell\u201d instead. As you can see the price doesn\u2019t fall and keeps going up, and in this case, the short seller may be subject to a margin call from his broker.\n\nAnother smoothing technique of time series data is EMA (Exponential Moving Average). The EMA is a weighted average of the last n (window size) prices, where the weighting decreases exponentially with each previous price/period. In simple words, recent prices are given more weight than past prices, and the degree of the contribution exponentially decay as the time period goes further to the past from the current observation.\n\nThe (adjusted) exponential moving average for time t is defined as:\n\nHere alpha is the decaying factor. But there is something I can\u2019t understand. I can understand the logic, but when it comes to Pandas\u2019 \u201cewm\u201d function, I couldn\u2019t figure out what is the default value of alpha the calculation uses. Maybe there is a standard that everyone agrees even without explicitly stating it, but I have spent a good amount of time trying to figure this out, but I am still not sure. Maybe I\u2019m missing something important. If you are familiar with this, any help would be appreciated.\n\n*update: I have found the answer to my own question above. I have to admit that I didn\u2019t look thoroughly into the documentation. It is really strange sometimes, I was looking at all the explanation right there in the documentation, but somehow I couldn\u2019t catch it. With Pandas\u2019 EWM function, there is no default value, and you have to specify decay. There are four different ways you can specify it. In the below code, I specified it in terms of \u201cspan\u201d, in this case alpha is defined as 2/(span+1). Thus by specifying \u201cspan\u201d as 10, I was specifying alpha as around 0.18. You can also directly specify alpha from Pandas version 0.18.0.\n\nBut intuitively, compared to simple moving average, exponential moving average will react faster to the more recent movement of the price by giving more weights to the current values.\n\nFrom the above chart, you can see EMA in red line catches the downwards movement in mid-December faster than SMA in yellow line, and also at the very end, EMA started to catch upwards movement, while SMA still showing downwards trend.\n\nYou can apply the same logic of Dual Moving Average Crossover to the original observation and EMA. Only this time we look for the crossing points between the original observation and EMA. Let\u2019s see how this will perform on a short-time period starting from 01/10/2017 up to 27/12/2017.\n\nNow let\u2019s look at this on a chart.\n\nWith EMA trading strategy, I would have started my investment with 4,321.44 USD, and during the time period of roughly two months, I would have needed 6,665.21 USD to keep investing, and the final value of my 1 Bitcoin would have been valued at 8,751.43 USD now (27/12/2017).\n\nIt was a fun toy project to actually implement what I have learned during the class, and I will try to explore further with time series analysis and Bitcoin data. But should I invest in Bitcoin? I am not sure yet, maybe I\u2019ll find some answer as I explore further.\n\nThank you for reading and you can find the Jupyter Notebook of the above code from the below link."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-3-zipfs-law-data-visualisation-fc9eadda71e7?source=user_profile---------14----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 3 (Zipf\u2019s Law, data visualisation)",
        "text": "This is the third part of Twitter sentiment analysis project I am currently working on as a capstone for General Assembly London\u2019s Data Science Immersive course. You can find the links to the previous posts below.\n\nAt the end of the second blog post, I have created term frequency data frame looks like this.\n\nThe indexes are the token from the tweets dataset (\u201cSentiment140\u201d), and the numbers in \u201cnegative\u201d and \u201cpositive\u201d columns represent how many times the token appeared in negative tweets and positive tweets.\n\nZipf\u2019s Law is first presented by French stenographer Jean-Baptiste Estoup and later named after the American linguist George Kingsley Zipf. Zipf\u2019s Law states that a small number of words are used all the time, while the vast majority are used very rarely. There is nothing surprising about this, we know that we use some of the words very frequently, such as \u201cthe\u201d, \u201cof\u201d, etc, and we rarely use the words like \u201caardvark\u201d (aardvark is an animal species native to Africa). However, what\u2019s interesting is that \u201cgiven some corpus of natural language utterances, the frequency of any word is inversely proportional to its rank in the frequency table. Thus the most frequent word will occur approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\u201d\n\nIf you want to know a bit more about Zipf\u2019s Law, I recommend the below Youtube video.\n\nZipf\u2019s Law can be written as follows: the rth most frequent word has a frequency f(r) that scales according to\n\nLet\u2019s see how the tweet tokens and their frequencies look like on a plot.\n\nOn the X-axis is the rank of the frequency from highest rank from left up to 500th rank to the right. Y-axis is the frequency observed in the corpus (in this case, \u201cSentiment140\u201d dataset). One thing to note is that the actual observations in most cases does not strictly follow Zipf\u2019s distribution, but rather follow a trend of \u201cnear-Zipfian\u201d distribution.\n\nEven though we can see the plot follows the trend of Zipf\u2019s Law, but it looks like it has more area above the expected Zipf curve in higher ranked words.\n\nAnother way to plot this is on a log-log graph, with X-axis being log(rank), Y-axis being log(frequency). By plotting on a log-log scale the result will yield roughly linear line on the graph.\n\nAgain we see a roughly linear curve, but deviating above the expected line on higher ranked words, and at the lower ranks we see the actual observation line lies below the expected linear line.\n\nAt least, we proved that even the tweet tokens follow \u201cnear-Zipfian\u201d distribution, but this introduced me to a curiosity about the deviation from the Zipf\u2019s Law. Even though the law itself states that the actual observation follows \u201cnear-Zipfian\u201d rather than strictly bound to the law, but is the area we observed above the expected line in higher ranks just by chance? Or does it mean that tweets use frequent words more heavily than other text corpora? Is there statistically significant difference compared to other text corpora?\n\nEven though all of these sounds like very interesting research subjects, but it is beyond the scope of this project, and I will have to move to the next step of data visualisation.\n\nAfter having seen how the tokens are distributed through the whole corpus, the next question in my head is how different the tokens in two different classes(positive, negative). This time, the stop words will not help much, because the same high-frequency words (such as \u201cthe\u201d, \u201cto\u201d) will equally frequent in both classes. If these stop words dominate both of the classes, I won\u2019t be able to have a meaningful result. So, I decided to remove stop words, and also will limit the max_features to 10,000 with countvectorizer.\n\nI will not go through the countvectorizing steps since this has been done in a similar way in my previous blog post. But it will be in my Jupyter Notebook that I will share at the end of this post. Anyway, after countvectorizing now we have token frequency data for 10,000 tokens without stop words, and it looks as below.\n\nLet\u2019s see what are the top 50 words in negative tweets on a bar chart.\n\nEven though some of the top 50 tokens can provide some information about the negative tweets, some neutral words such as \u201cjust\u201d, \u201cday\u201d, are one of the most frequent tokens. Even though these are the actual high-frequency words, but it is difficult to say that these words are all important words in negative tweets that characterises the negative class.\n\nLet\u2019s also take a look at top 50 positive tokens on a bar chart.\n\nAgain, neutral words like \u201cjust\u201d, \u201cday\u201d, are quite high up in the rank.\n\nWhat if we plot the negative frequency of a word on X-axis, and the positive frequency on Y-axis?\n\nMost of the words are below 10,000 on both X-axis and Y-axis, and we cannot see meaningful relations between negative and positive frequency.\n\nIn order to come up with a meaningful metric which can charaterise important tokens in each class, I borrowed a metric presented by Jason Kessler in PyData 2017 Seattle. In the talk, he presented a Python library called Scattertext. Even though I did not make use of the library, the metrics used in the Scattertext as a way of visualising text data are very useful in filtering meaningful tokens from the frequency data.\n\nLet\u2019s explore what we can get out of frequency of each token. Intuitively, if a word appears more often in one class compared to another, this can be a good measure of how much the word is meaningful to characterise the class. In the below code I named it as \u2018pos_rate\u2019, and as you can see from the calculation of the code, this is defined as\n\nWords with highest pos_rate have zero frequency in the negative tweets, but overall frequency of these words are too low to consider it as a guideline for positive tweets.\n\nAnother metric is the frequency a word occurs in the class. This is defined as\n\nBut since pos_freq_pct is just the frequency scaled over the total sum of the frequency, the rank of pos_freq_pct is exactly same as just the positive frequency.\n\nWhat we can do now is to combine pos_rate, pos_freq_pct together to come up with a metric which reflects both pos_rate and pos_freq_pct. Even though both of these can take a value ranging from 0 to 1, pos_rate has much wider range actually spanning from 0 to 1, while all the pos_freq_pct values are squashed within the range smaller than 0.015. If we average these two numbers, pos_rate will be too dominant, and will not reflect both metrics effectively.\n\nSo here we use harmonic mean instead of arithmetic mean. \u201cSince the harmonic mean of a list of numbers tends strongly toward the least elements of the list, it tends (compared to the arithmetic mean) to mitigate the impact of large outliers and aggravate the impact of small ones.\u201d The harmonic mean H of the positive real number x1,x2,\u2026xn is defined as\n\nThe harmonic mean rank seems like the same as pos_freq_pct. By calculating the harmonic mean, the impact of small value (in this case, pos_freq_pct) is too aggravated and ended up dominating the mean value. This is again exactly same as just the frequency value rank and doesn\u2019t provide a much meaningful result.\n\nWhat we can try next is to get the CDF (Cumulative Distribution Function) value of both pos_rate and pos_freq_pct. CDF can be explained as \u201cdistribution function of X, evaluated at x, is the probability that X will take a value less than or equal to x\u201d. By calculating CDF value, we can see where the value of either pos_rate or pos_freq_pct lies in the distribution in terms of cumulative manner. In the below result of the code, we can see a word \u201cwelcome\u201d with pos_rate_normcdf of 0.995625, and pos_freq_pct_normcdf of 0.999354. This means roughly 99.56% of the tokens will take a pos_rate value less than or equal to 0.91535, and 99.99% will take a pos_freq_pct value less than or equal to 0.001521.\n\nNext, we calculate a harmonic mean of these two CDF values, as we did earlier. By calculating the harmonic mean, we can see that pos_normcdf_hmean metric provides a more meaningful measure of how important a word is within the class.\n\nNext step is to apply the same calculation to the negative frequency of each word.\n\nNow let\u2019s see how the values are converted into a plot. In order to compare, I will first plot neg_hmean vs pos_hmean, and neg_normcdf_hmean vs pos_normcdf_hmean.\n\nNot much difference from the just frequency of positive and negative. How about the CDF harmonic mean?\n\nIt seems like the harmonic mean of rate CDF and frequency CDF has created an interesting pattern on the plot. If a data point is near to the upper left corner, it is more positive, and if it is closer to the bottom right corner, it is more negative.\n\nIt is good that the metric has created some meaningful insight out of frequency, but with text data, showing every token as just a dot is lacking important information on which token each data point represents. With 10,000 points, it is difficult to annotate all of the points on the plot. For this part, I have tried several methods and came to a conclusion that it is not very practical or feasible to directly annotate data points on the plot.\n\nSo I took an alternative method of an interactive plot with Bokeh. Bokeh is an interactive visualisation library for Python, which creates graphics in style of D3.js. Bokeh can output the result in HTML format or also within the Jupyter Notebook. And below is the plot created by Bokeh.\n\nSince the interactive plot can\u2019t be inserted to Medium post, I attached a picture, and somehow the Bokeh plot is not showing on the GitHub as well. So I am sharing this with the link you can access.\n\nWith above Bokeh plot, you can see what token each data point represents by hovering over the points. For example, the points in the top left corner show tokens like \u201cthank\u201d, \u201cwelcome\u201d, \u201ccongrats\u201d, etc. And some of the tokens in bottom right corner are \u201csad\u201d, \u201churts\u201d, \u201cdied\u201d, \u201csore\u201d, etc. And the color of each dot is organised in \u201cInferno256\u201d color map in Python, so yellow is the most positive, while black is the most negative, and the color gradually goes from black to purple to orange to yellow, as it goes from negative to positive.\n\nDepending on which model I will use later for classification of positive and negative tweets, this metric can also come in handy.\n\nNext phase of the project is the model building. In this case, a classifier that will classify each tweet into either negative or positive class. I will keep sharing my progress through Medium.\n\nThank you for reading, and you can find the Jupyter Notebook from below link."
    },
    {
        "url": "https://towardsdatascience.com/shallow-neural-network-from-scratch-deeplearning-ai-assignment-320f57c581cd?source=user_profile---------15----------------",
        "title": "Shallow Neural Network from scratch (deeplearning.ai assignment)",
        "text": "From above dependencies, \u201cplanar_utils\u201d is a small python file with functions to create data points, plot the data points, etc. You can find this file in the same directory that I put my Jupyter Notebook in github.\n\nThe problem we are trying to solve is classification problem. On the plot, blue and red represent each class of the corresponding data points, and to classify each point\u2019s class accurately a linear decision boundary won\u2019t be able to do a good job.\n\nThe shape of X is horizontally wide data, with each column representing one data point, and Y is 1 dimensional vector with all the class labels for data points in X.\n\nAt this point, I made a slight modification to the original code. The original code for the assignment is taking the whole X and Y to fit and predict, and calculate the accuracy on the whole dataset, but I also wanted to see how the result changes if I split the data into training and test set, and compare the accuracies of train and test.\n\nIn neural network modelling, keeping track of the matrix dimension is crucial for vectorized implementation of calculation anywhere possible. I had to keep reminding myself to use vectorized calculation instead of for loop wherever possible, this will not only make the code simpler but also reduce the computation time significantly.\n\nFirst, let\u2019s see how a simple linear regression model will perform with this data. I fit the model with training set, and the test the fitted model with test set.\n\nOK, as expected, linear decision boundary is not a good fit for this type of data. Now let\u2019s build a neural network step by step to see how it performs.\n\nIt will consists of below steps, and when we have all the functions defined, we can combine them together to make a simple neural networks.\n\nThe first function is quite simple. It just defines the size of each layer. Below picture can give you a clearer picture of the model we\u2019re building.\n\nWe have two input features for each data entry, and one hidden layer with 4 units (using tanh activation function), and one output layer with 1 unit (using sigmoid activation function). So with above layer size defining function, it will be\n\nThe next function to define is initialising weights and biases parameters. In logistic regression, it was OK to initialise both \u201cw\u201d and \u201cb\u201d 0, and the model worked perfectly fine learning right parameters by iterating. But in neural networks, this causes a problem, since we have multiple units in hidden layer. If we initialise \u201cw\u201d matrix with all the same value, these weights will be updated to the same values through the whole hidden layer, and that means each hidden unit will end up calculating exact same function. So, we initialise matrix of \u201cw\u201d as random numbers, and vector \u201cb\u201d as zeros. The reason why we multiply \u201cw\u201d by 0.01 is to make this \u201cw\u201d small numbers. If values of \u201cw\u201d are too big, when it comes to activation function, it will have slope very small, and this might affect the time it takes to train the model.\n\nFrom the above code, you can see the dimension of W1 is 4x2 matrix, and b1 is 4x1 vector, W2 is 1X4 vector, b2 is 1x1 scalar value. If you take another look at the model graph I attached above, you can make sense of it. The hidden layer has 4 units, and input layers has 2 units, so the weights that will be applied to X will have 4X2 dimension, and each hidden unit will have one bias each so b1 is 4x1 vector. The output layer only has one unit, and hidden layer has 4 units, so W2 which will be applied to the activated values from hidden layer will have 1x4 dimension, and finally b2 which will be added to one output unit naturally has 1x1 dimension.\n\nThe next part is forward propagation.\n\nThe forward propagation function takes values from input layer, and multiply it by W1 and add biases b1, and then this is put into tanh activation function. This is the results of the hidden layer. After we get this, we can do another multiplication by second weights W2 and add bias b2, then finally put this value into sigmoid function to get the final result. In the function, \u201cassert\u201d is just a sanity check to see if the dimension of the final output is what we expected. What we are calculating above can be expressed as below formulas. In below formula, superscript square bracket denotes layer number, and superscript round bracket denotes i-th training data.\n\nNext function is to compute the cost of the output we get from forward propagation.\n\nWhat we are calculating as cost from the above function can be expressed as below.\n\nI won\u2019t go into the detail of this formula, but intuition for the above formula is that, in case of binary classification, when there are more correct prediction the cost J will be small, and with above one formula we can handle two case of classes (when y=0, and also when y=1).\n\nThis is the most difficult and also most important part of neural network, back propagation. Intuition is we are trying to find out how the change of W1, b1, W1, b2 affects the final output value, and by adjusting Ws and bs, we want to minimise the cost function J, so that we will have more predictions correct. In order to find this out, we calculate derivatives of Ws and bs, and update their values with every iteration based on the learning rate we define.\n\nAnd the gradient descent used in the above code can be expressed as below.\n\nNext is updating the parameters Ws and bs.\n\nYou can define your own learning rate for this. But if learning rate is too low, it will take long to train the model, if it is too high, gradient descent will overshoot and might not be able to converge.\n\nNow let\u2019s combine the above functions together to make a neural network function, we will define prediction function separately after this.\n\nFinally! We have a working model of neural network. Let\u2019s see how it runs.\n\nYou can see the cost is reduction with iteration, and W1, W2, b1, b2 values are the final values that has been updated through the whole iterations.\n\nAs one final step we will define prediction function. Since we have the optimal parameters for Ws and bs, now we should fit this parameters to our forward propagation and get the prediction values. But one thing to remember is that, this prediction value is not 0,1. The output values are the probability of the data being class 1. Let\u2019s decide 0.5 as our threshold, which means if the output probability is greater than 0.5, then we will classify it as class 1, otherwise class 0.\n\nAnd now we have everything in our hand, let\u2019s predict the class of train set first and see whether it performs better than logistic regression.\n\nIt looks much better than the previous logistic regression model!! Impressive!\n\nLet\u2019s plot it with test set as well.\n\nExcept for a couple of outliers, our simple neural network does a brilliant job.\n\nBut how will it change if we change the number of hidden units in hidden layer. And also how will train accuracy and test accuracy change with number of hidden units?\n\nIt looks like 5 hidden units are the optimal choice for this particular dataset. We can see with 20 and 50 hidden units, model starting to overfit the training data.\n\nAnd finally, out of curiosity, I wanted to see how train accuracy and test accuracy change depending on the number of hidden units. This was not required by the assignment, but I plotted this to find out. My first guess was if the number of hidden units get bigger, then it will overfit the training data, thus the test accuracy will gradually decrease with the increase of number of hidden units. But interestingly, below is the result that I plot with 1 to 100 hidden units and its train accuracy and test accuracy accordingly.\n\nTest accuracy in blue line does not go down lower than around 85%, even if the number of hidden units increases. How interesting. Unfortunately I don\u2019t have answer for this, and I hope to find out as I proceed with the course.\n\nBy the way, if you have read this post until here, I salute your patience!\n\nThis has been a rather long post, but this will serve as my learning note, and if this helps any other people who\u2019s interested in neural network, I can\u2019t be happier.\n\nOh and last but not least, if you are taking the same course as me, and thinking about copying and pasting the code to pass the assignment,\u2026.Don\u2019t. You can take this as reference, but thinking hard to solve a problem and going through trial and errors can be really helpful in your learning in the long run!\n\nYou can find the Jupyter Notebook for above code from the link below."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-2-333514854913?source=user_profile---------16----------------",
        "title": "Another Twitter sentiment analysis with Python-Part 2",
        "text": "This blog post is the second part of the Twitter sentiment analysis project I am currently doing for my capstone project in General Assembly London. You can find the first part here.\n\nBefore I move on to EDA, and data visualisation, I have made some changes to the data cleaning part, due to the faults of the data cleaning function I defined in the previous post.\n\nThe first issue I realised is that, during the cleaning process, negation words are split into two parts, and the \u2018t\u2019 after the apostrophe vanishes when I filter tokens with length more than one syllable. This makes words like \u201ccan\u2019t\u201d end up as same as \u201ccan\u201d. This seems like not a trivial matter for sentiment analysis purpose.\n\nThe second issue I realised is that, some of the url link doesn\u2019t start with \u201chttp\u201d, sometimes people paste link in \u201cwww.websitename.com\" form. This wasn\u2019t properly handled when I defined the url address regex pattern as \u2018https?://[A-Za-z0\u20139./]+\u2019. And another problem of this regex pattern is that it only detects alphabet, number, period, slash. This means it will fail to catch the part of the url, if it contains any other special character such as \u201c=\u201d, \u201c_\u201d, \u201c~\u201d, etc.\n\nThe third issue is with the regex pattern for twitter ID. In the previous cleaning function I defined it as \u2018@[A-Za-z0\u20139]+\u2019, but with a little googling, I found out that twitter ID also allows underscore symbol as a character can be used with ID. Except for underscore symbol, only characters allowed are alphabets and numbers.\n\nBelow is the updated datacleaning function. The order of the cleaning is\n\nAfter I updated the cleaning function, I re-cleaned the whole 1.6 million entries in the dataset. You can find the detail of the cleaning process from my previous post.\n\nAfter the cleaning, I exported the cleaned data as csv, then load it as data frame, and it looks as below.\n\nIt looks like there are some null entries in the data, let\u2019s investigate further.\n\nIt seems like 3,981 entries have null entries for text column. This is strange, because the original dataset had no null entries, thus if there are any null entries in the cleaned dataset, it must have happened during the cleaning process.\n\nBy looking these entries in the original data, it seems like only text information they had was either twitter ID or url address. Anyway, these are the info I decided to discard for the sentiment analysis, so I will drop these null rows, and update the data frame.\n\nThe first text visualisation I chose is the controversial word cloud. A word cloud represents word usage in a document by resizing individual words proportionally to its frequency, and then presenting them in random arrangement. There were a lot of debates around word cloud, and I somewhat agree to the points of people who are against using word cloud as data analysis. Some of the concerns over word cloud is that, it supports only the crudest sorts of textual analysis, and it is often applied to situations where textual analysis is not appropriate, and it leaves viewers to figure out the context of the data by themselves without providing the narrative.\n\nBut in the case of tweets, textual analysis is the most important analysis, and it provides a general idea of what kind of words are frequent in the corpus, in a sort of quick and dirty way. So, I will give it a go, and figure out what other methods can be used for text visualisation.\n\nFor the word cloud, I used the python library wordcloud.\n\nSome of big words can be interpreted quite neutral, such as \u201ctoday\u201d,\u201dnow\u201d,etc. I can see some of the words in smaller size make sense to be in negative tweets, such as \u201cdamn\u201d,\u201dugh\u201d,\u201dmiss\u201d,\u201dbad\u201d, etc. But there is \u201clove\u201d in rather big size, so I wanted to see what is happening.\n\nOK, even though the tweets contain the word \u201clove\u201d, in these cases it is negative sentiment, because the tweet has mixed emotions like \u201clove\u201d but \u201cmiss\u201d. Or sometimes used in a sarcastic way.\n\nAgain I see some netural words in big size, \u201ctoday\u201d,\u201dnow\u201d, but words like \u201chaha\u201d, \u201clove\u201d, \u201cawesome\u201d also stand out.\n\nInterestingly, the word \u201cwork\u201d was quite big in negative word cloud, but also quite big in positive word cloud. It might implies that many people express negative sentiment towards work, but also many people are positive about works.\n\nIn order for me to implement a couple of data visualisation in the next step, I need term frequency data. What kind of words are used in the tweets, and how many times it is used in entire corpus. I used count vectorizer to calculate the term frequencies, even though the count vectorizer is also for fit, train and predict, but at this stage, I will just be extracting the term frequencies for the visualisation.\n\nThere are parameter options available for count vectorizer, such as removing stop words, limiting the maximum number of terms. However, in order to get a full picture of the dataset first, I implemented with stop words included, and not limiting the maximum number of terms.\n\nOK, it looks like the count vectorizer has extracted 264,936 words out of the corpus.\n\n!Important update (10/01/2018): I just realised that I didn\u2019t have to go through all the batching I did below.\n\nYes! All the batching and computation I did below can be done in much less lines of code (not exactly one line though). Once you transform the data with the fitted count vectorizer, you can directly get the term frequency from the sparse matrix. And all this can be done in less than a second! If you read below, you can see I spent almost 40mins to get the term frequency table. I guess I learned this lesson the hardest way possible. The code for getting the term frequency directly from sparse matrix is as below.\n\nI will leave below as I have originally written, as a reminder of my own stupidity for myself. The moral of the story is, if you can work directly with sparse matrix, you should definitely do it without transforming into dense matrix!"
    },
    {
        "url": "https://medium.com/@rickykim78/i-was-asked-to-organise-a-secret-santa-so-i-did-it-in-python-1eaa3801c074?source=user_profile---------17----------------",
        "title": "I was asked to organise a Secret Santa, so I did it in Python!",
        "text": "Christmas is just around the corner. How is your Christmas preparation going?\n\nI recently been asked to organise a Secret Santa for me and my friends. In case you don\u2019t know what Secret Santa is, \u201cSecret Santa is a Western Christmas tradition in which members of a group or community are randomly assigned a person to whom they give a gift. The identity of the gift giver is a secret not to be revealed.\u201d\n\nBut the problem is, it might be too much hassle for everyone to meet up just to draw the names and assign secret Santa. I know there\u2019s already a website you can use for organising Secret Santa, but I wanted to put my Python skills into test to see what I can come up with.\n\nEven though I\u2019ve been using Python as a tool for data science, I haven\u2019t written a full Python software before, and the environment I\u2019m familiar with is Jupyter Notebook. So, this is not about full program you can deploy as an app, but a rather simple Python code blocks you can run on your Jupyter notebook.\n\nThis was a fun small project for me, since I haven\u2019t coded any general purpose Python code before, and the code I have been writing for my study is all about making data analysis and making predictive models.\n\nFirst, let\u2019s start by loading dependencies to use in the code.\n\nNext step is to define the names and email addresses of the people, who will be in the Secret Santa.\n\nNow we have to define our Secret Santa function. Before I start coding, I thought this will be easily done, but when I started coding I realised this is more complicated than I thought. First thing need to be considered is one person should not draw his/her own name, and once one name has been drawn, this needs to be eliminated from the list, so that the next person will not draw the same name.\n\nThe above function will return a list of tuples, and each tuple contains the name of the person who will give the present as the first entry, and the person who should receive the present as the second entry.\n\nAnd in my case, I am also participating in the secret Santa, and I didn\u2019t want look at the result and ruin the fun. So I assigned the result to a variable without looking to use it to send as emails.\n\nThe next part is the actual email sending part. If your email address is gmail, you can easily run this code just by putting your password into the code. And I also attached an image to the email. You can attach any image you want, as long as you put it in the same directory as the notebook, and pass this name together with extension in \u201cfilename\u201d, and \u201cattachment\u201d in below code block.\n\nThe only problem of this is, if you are also in the Secret Santa, you can find out who drew who by checking your gmail sent box. But why would you ruin the fun? As long as you don\u2019t open the sent emails in your sent box, you can still have fun!\n\nI guess there could be more elegant and pythonic way to do the same thing, but for now I am happy that I did automate something with Python, and can\u2019t wait to find out who my secret Santa is. And no, I won\u2019t check my gmail sent box until Christmas.\n\nYou can find the Jupyter Notebook with the code from below link."
    },
    {
        "url": "https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90?source=user_profile---------18----------------",
        "title": "Another Twitter sentiment analysis with Python \u2014 Part 1",
        "text": "It has been a while since my last post. During my absence in Medium, a lot happened in my life. I finally gathered my courage to quit my job, and joined Data Science Immersive course in General Assembly London.\n\nIt was a big decision in my life, but I don\u2019t regret it. Actually, I think it is the best decision I ever made so far. The course is pretty intense, but I enjoy every bits of it. I am learning a lot about more rigorous approach to data science, and most importantly the feedbacks I can get from tutors and class mates really help me improve. It\u2019s not easy but it\u2019s definitely worth it!\n\nI am currently on the 8th week, and preparing for my capstone project. And as the title shows, it will be about Twitter sentiment analysis.\n\nAt first, I was not really sure what I should do for my capstone, but after all, the field I am interested in is natural language processing, and Twitter seems like a good starting point of my NLP journey. You can find my initial project idea proposal from here.\n\nTo give you a brief summary about the project, it is about building a model that can detect sentiment, so that I can apply this model to tweets from different cities, and compare/analyse how happy the tweets are from different cities. This started from my own curiosity. For me, being happy is quite important factor of life, and I think it depends highly on your environment. So naturally, I wanted to see how happy the citizens in different cities are.\n\nThere could be many different ways to do the sentiment analysis, some of the libraries provide out-of-the-box sentiment analysis function that you can directly use on text, but there\u2019s no fun in there (is there?). So I decided to train my own model, which I can apply to the tweets I will gather. In order for a model to have decent performance, I need a relatively good size of dataset to train. The dataset for training, I chose \u201cSentiment140\u201d, which originated from Stanford University. More info on the dataset can be found from the link. http://help.sentiment140.com/for-students/\n\nThe dataset can be downloaded from the below link.\n\nhttp://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n\nBy looking at the description of the dataset from the link, the information on each field can be found.\n\n0 \u2014 the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n\n1 \u2014 the id of the tweet (2087)\n\n2 \u2014 the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n\n3 \u2014 the query (lyx). If there is no query, then this value is NO_QUERY.\n\n4 \u2014 the user that tweeted (robotickilldozr)\n\n5 \u2014 the text of the tweet (Lyx is cool)\n\nOK, let\u2019s first take a look at the data\n\nDataset has 1.6million entries, with no null entries, and importantly for the \u201csentiment\u201d column, even though the dataset description mentioned neutral class, the training set has no neutral class.\n\n50% of the data is with negative label, and another 50% with positive label.\n\nWe can see there\u2019s no skewness on the class division.\n\nI first started by dropping the columns that I don\u2019t need for the specific purpose of sentiment analysis.\n\n\u201cid\u201d column is unique ID for each tweet\n\n\u201cdate\u201d column is for date info for the tweet\n\n\u201cquery_string\u201d column indicates whether the tweet has been collected with any particular query key word, but for this column, 100% of the entries are with value \u201cNO_QUERY\u201d\n\n\u201cuser\u201d column is the twitter handle name for the user who tweeted\n\nI decided to drop above four columns.\n\nBy looking at some entries for each class, it seems like that all the negative class is from 0~799999th index, and the positive class entries start from 800000 to the end of the dataset.\n\nAs a way of sanity check, let\u2019s look at the length of the string in text column in each entry.\n\nBelow is the first draft of the data dictionary for the dataset, but as I go through preparation, this will need to be updated.\n\nI will also plot pre_clean_len with box plot, so that I can see the overall distribution of length of strings in each entry.\n\nThis looks a bit strange, since the twitter\u2019s character limit is 140. But from the above box plot, some of the tweets are way more than 140 characters long.\n\nIt looks like it\u2019s time for some cleaning!\n\nIt looks like HTML encoding has not been converted to text, and ended up in text field as \u2018&\u2019,\u2019\"\u2019,etc. Decoding HTML to general text will be my first step of data preparation. I will use BeautifulSoup for this.\n\nThe second part of the preparation is dealing with @mention.\n\nEven though @mention carries a certain information (which another user that the tweet mentioned), this information doesn\u2019t add value to build sentiment analysis model.\n\nThe third part of the cleaning is dealing with URL links, same with @mention, even though it carries some information, for sentiment analysis purpose, this can be ignored.\n\nBy looking at the above entry, I can see strange patterns of characters \u201c\\xef\\xbf\\xbd\u201d. After some researching, I found that these are UTF-8 BOM.\n\n\u201cThe UTF-8 BOM is a sequence of bytes (EF BB BF) that allows the reader to identify a file as being encoded in UTF-8.\u201d\n\nBy decoding text with \u2018utf-8-sig\u2019, this BOM will be replaced with unicode unrecognisable special characters, then I can process this as \u201c?\u201d\n\nSometimes the text used with hashtag can provide useful information about the tweet. It might be a bit risky to get rid of all the text together with the hashtag.\n\nSo I decided to leave the text intact and just remove the \u2018#\u2019. I will do this in the process of cleaning all the non letter characters including numbers.\n\nWith above five data cleaning task, I will first define data cleaning function, and then will be applied to the whole dataset. Tokenization, stemming/lemmatization, stop words will be dealt with later stage when creating matrix with either count vectorizer or Tfidf vectorizer.\n\nFor the rest, you get the idea, I divided the whole dataset into four batches, and cleaned them.\n\nSince the post is getting quite long, I will stop here and try to continue at the next post. If you have any questions, or opinions, or advice, please do not hesitate to leave a comment! Thank you.\n\nYou can find Jupyter Notebook file from below Github repo."
    },
    {
        "url": "https://towardsdatascience.com/learning-data-science-part-2-podcasts-blogs-8bed327eb1b5?source=user_profile---------19----------------",
        "title": "Learning Data Science \u2014 Part 2: Podcasts, Blogs \u2013",
        "text": "This is the second part of the \u201cLearning Data Science\u201d, links and sources I gathered while I was trying to immerse myself in data science. I first started this to keep track of all the learning resources I find useful, but if it also can be a help to anyone out there, that\u2019s \u2018two birds with one stone\u2019. In the first part \u201cLearning Data Science \u2014 Part 1: MOOC, Youtube, Ted\u201d, I listed up mostly video resources. Even though visual aids are helpful when learning, I want to effectively use my commute time, and oftentimes I can\u2019t get signal inside the tube, or streaming videos is a bit too heavy on my mobile data usage.\n\nChris Albon at Partially Derivative summed up how listening to podcasts can help you at a live talk organised by Metis, \u201cDemystifying Data Science\u201d (The live talk was on 27/09/2017, but you can still access recordings by registering on their website. I wasn\u2019t sure if there will be recordings, so I binge watched 12 hours talks from 3PM to 3AM in UK time\u2026). Podcastas are about \u201cgetting your head hear the terms over and over again\u201d. Of course 30 minute episode of podcast won\u2019t make you an expert, but you will hear how the actual data scientists talk about a subject, how they apply it to real problems. After an episode of podcast, you will know a bit better about the topic than before.\n\nThere are so many good blogs out there, and it is not easy to keep up with every single blog that you are interested in. That\u2019s where RSS feeder comes in handy. I recently discovered Feedly, and it saved me from rummaging through bookmarks and to see if any blogs are updated. You just copy and paste blog address and click \u2018follow\u2019. Then you can just go to Feedly to check updates of all the blogs you follow. (I am on free membership, and the sources I can add is limited to 100)"
    },
    {
        "url": "https://towardsdatascience.com/pokemon-generation-gap-python-data-analysis-part-1-cleaning-eda-265ff3cb5dff?source=user_profile---------20----------------",
        "title": "Pokemon Generation Gap? (Python Data Analysis) \u2014 Part 1: cleaning, EDA",
        "text": "I have been learning Python for data analysis through online courses, and sometimes Youtube videos, etc. I know I only took the first step, but I couldn\u2019t resist the urge to put what I have learned into a test. I strongly believe that knowledge is not yours until you actually apply it and use it. I don\u2019t have that \u201cBeautiful Mind\u201d type of brain, I can\u2019t understand things instantly in my head just by watching. Considering my current level of Python fluency, I will start with some simple basic analysis.\n\nI am not a Pokemon geek, but I did play Pokemon Go. Unfortunately I was one of those early leavers. I think I levelled up until level 25, and that was it. I did watch some episodes of TV series, but just by chance. Then, why Pokemon? I wanted to start somewhere light, not too serious and not too complicated. While I was looking through Kaggle\u2019s datasets, I found Pokemon dataset! The size of the data is manageable for me, doesn\u2019t contain too many columns and each column is easy enough for me to understand. And Pokemons are cute\u2026\n\nI first looked through all the kernels in Pokemon datasets, many cool analysis some too advanced for me, some I can understand. So I first start with a simple question. \u201cIs there any difference in Pokemons in different generations?\u201d (There are 7 different generations spanning from 1996 to 2017)\n\nFirst, I downloaded the dataset and saved it in my computer to read and see in Python.\n\nBy looking at first 5 entries of data, I already saw two problems. One, \u2018#\u2019 column has duplicates, and it seems that same ID numbers have been given to Pokemons\u2019 Mega-evolve form as their original form (Mega-evolution is an ultimate form of a Pokemon, not applicable to all Pokemons). But before and after Mega-evolution are certainly different. Not only their stats change, but also their looks. For example, while the third entry Vensaur\u2019s total stat point is 525, and the fourth entry Mega Venusaur\u2019s Total is 625. The looks are as below.\n\nSecond problem is that the fourth entry Mega Venusaurs has duplicated Venusaur at the front in the Name column value; \u201cVenusaurMega Venusaur\u201d. This needs to be dealt with.\n\nLooking at just first five entries didn\u2019t give me a full picture of data. So to see broader picture, I called \u201cinfo\u201d method.\n\nI can see there are 800 entries, and values for the columns are \u201cobject\u201d or \u201cint64\u201d or \u201cbool\u201d, means string or integer or boolean values. For \u201cType 2\u201d variable, there are only 414 non-null entries, which means there are 386 missing values. Missing values needs to be handled cautiously. There might be a reason why they are missing, and you might find some useful insight by figuring out the reason. Sometimes missing part might even distort the whole data. But in this case, it is just because some of Pokemons do not have secondary type. For my analysis, I won\u2019t be using \u201cType 2\u201d attribute, so I will just leave it as it is.\n\nNow it\u2019s time for some EDA (Exploratory Data Analysis)! There can be two aspects of EDA, numerical summarisation, and some visual method such as graphs. In Python, summary statistics can be easily extracted just by one method, \u201cdescribe\u201d.\n\nI can see count, mean, standard deviation, minimum, interquartile range, maximum all in one table, and now I have a better understanding of the data. (\u201cdescribe\u201d method only grabs results from numerical values. So the columns with string, boolean values: \u2018Name\u2019, \u2018Type 1\u2019, \u2018Type 2\u2019, \u2018Legendary\u2019 are not shown here.) Even though this dataset is easy to see what each column means, sometime it is very helpful to see the original documentation of the dataset. According to Alberto Barradas who uploaded the dataset to Kaggle, the descriptions are as below.\n\nI guess \u2018Generation\u2019 and \u2018Legendary\u2019 variables are added later. Just to help your understanding, \u2018Generation\u2019 variable is about which generation of Pokemon it is, and \u2018Legendary\u2019 is whether the Pokemon is a legendary class or not.\n\nLet\u2019s move on. I first want to tackle two problems I saw when I called \u201cdf.head()\u201d. The duplicates of \u2018#\u2019 column and strange names in \u2018Name\u2019 column.\n\nOK. I dropped \u2018#\u2019 column entirely, and also removed repetitive Pokemon name in front of its Mega-evolve forms. It looks better now. Let\u2019s see the end of the entries this time by calling \u201ctail\u201d method.\n\nOh wait. Something looks not right. Entry number 797, 798 seems to have duplicates in the front of the name. For your information, Hoopa is the right name, not HoopaHoopa, and they look like below.\n\nI better tidy that name for Hoopa. So I wrote another code to fix just two Hoopa entries.\n\nTo be honest, in order to figure out if there are any other strange names to fix, I also opened the csv file, and scanned through the \u2018Name\u2019 column for all 800 entries. I guess I can\u2019t do that for 10,000 or 100,000 entries, and might need to think about smarter way to tackle this. But anyway I found out that only Pokemons with Mega-evolve forms or Hoopa have this kind of problem in their \u2018Name\u2019 column values.\n\nFinally, I am ready to move on to some visual EDA. Since I want to find out how Pokemons differ from generation to generation, it is a good idea to group the entries by different generations and see how they look on plots.\n\n\u2018Total\u2019 column is the sum of all the stats values, and I think it is a good indicator of overall stats of a Pokemon. So, let\u2019s see how \u2018Total\u2019 values are distributed across Pokemons within same generation.\n\nAccording to Seaborn\u2019s documentation, \u201cA violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared\u201d. The strengths of violin plots are you can see the distribution shape from the graph. With box plots, you can never know if the distribution is unimodal or multimodal.\n\nIt looks like Generation 1 and 3 are more dispersed than others, while Generation 4 looks the shortest with two distinctive peaks. At least, from the plot, it looks like there might be some significant difference between generations, but too early to tell.\n\nWhile I was looking for definition and explanations about violin plots, I found out one interesting thing. While Violin plots display more information, they can be more noisier then a Box Plot. Hmmm is that so? I think I will have to do a box plot and see how they differ.\n\nI think the post is getting too long. I will have to stop here and continue on the second part."
    },
    {
        "url": "https://towardsdatascience.com/learning-data-science-part-1-mooc-youtube-ted-20617dbb7f4b?source=user_profile---------21----------------",
        "title": "Learning Data Science \u2014 Part 1: MOOC, Youtube, Ted \u2013",
        "text": "I have started studying data science quite recently, around two months ago. I don\u2019t have degree in Statistics, Mathematics, Data Science, Engineering, Economics, or anything similar. To be honest, Mathematics and Statistics were not my strong suit in my student days. I was better at languages. It might be a myth to say that some people are more linguistic than mathematical or vice versa, but that\u2019s what I felt.\n\nI studied management for BA, and curation for MA. I wrote my thesis, but it was more literature review focused than applying quantitative research methods. And I have been working in fashion as merchandiser, buyer. So through my whole education and experience I was not that close with Math and Stat.\n\nThen why am I even studying data science? I am so intrigued by the concept of \u201cgaining insights from data\u201d. So intrigued that I even decided to overcome my long-term fear of Math and Stat. Considering we create same amount of data up to 2003 in just two days(as of now it might even be 10 mins), and only 1% of them is analysed, the field is full of possibilities. \u201cI need to be a part of it\u201d, that\u2019s what I thought and still believe in it.\n\nCalculus One (I wasn\u2019t paying much attention during my math classes, and I definitely needed refresher.)\n\nBasic Statistics (I heard of it once or twice, but couldn\u2019t say I know anything)\n\nMachine Learning (It\u2019s that famous course by Andrew Ng, but at around week 6, I realised that I really need to have some more basic knowledge on programming, math, stat, so I put it on hold)\n\nPython for Everybody (This is a highly recommended specialisation(5 courses), if you are like me, coming from non-tech background, this is such a good introduction to programming with Python)\n\nThere are different categories that you might find useful.\n\nThe first is \u201ctutorial\u201d type. \n\nMachine Learning\n\nThe second is \u201cconference\u201d type.\n\nSome of the materials might not be easy to digest for a beginner, but it doesn\u2019t hurt to see what\u2019s on the horizon, what are the recent developments, topics in data science.\n\nThe third is data science schools/boot camps\u2019 own Youtube channel\n\nThe last is Ted Talks. I\u2019m such a sucker for Ted talks. So I put it in separate category from normal conference type.\n\nGreat speakers on variety of topics, I always discover something new, realise what I didn\u2019t know, get inspired, motivated, it just makes me want to be a better human. It might not teach you how to build an algorithm, but it can tell you why and what for.\n\nBy the way, Ted Talks has two different names. \u201cTED Talks\u201d and \u201cTedx Talks\u201d. I didn\u2019t know the difference until I googled it.\n\nIf you search \u201cted\u201d in Youtube, the first entry you will get is \u201cTED Talks\u201d Youtube page.\n\nhttps://www.youtube.com/user/TEDtalksDirector\n\nWhich has 7.9 million subscribers with 2,488 videos (01/10/2017)\n\nAnd on the 17th entry in the search result, there is \u201cTEDx Talks\u201d page.\n\nhttps://www.youtube.com/user/TEDxTalks\n\nWhich has 8.9 million subscribers with 99,031 videos (01/10/2017)\n\nAnyway, below are some of the talks I really enjoyed."
    }
]