[
    {
        "url": "https://towardsdatascience.com/coding-neural-network-parameters-initialization-f7c2d770e874?source=user_profile---------1----------------",
        "title": "Coding Neural Network \u2014 Parameters\u2019 Initialization \u2013",
        "text": "Optimization, in Machine Learning/Deep Learning contexts, is the process of changing the model\u2019s parameters to improve its performance. In other words, it\u2019s the process of finding the best parameters in the predefined hypothesis space to get the best possible performance. There are three kinds of optimization algorithms:\n\nIn this post, we\u2019ll look at three different cases of parameters\u2019 initialization and see how this affects the error rate:\n\nWe\u2019ll be using functions we wrote in \u201cCoding Neural Network \u2014 Forward Propagation and Backpropagation\u201d post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.\n\nTo illustrate the above cases, we\u2019ll use the cats vs dogs dataset which consists of 50 images for cats and 50 images for dogs. Each image is 150 pixels x 150 pixels on RGB color scale. Therefore, we would have 67,500 features where each column in the input matrix would be one image which means our input data would have 67,500 x 100 dimension.\n\nLet\u2019s first load the data and show a sample of two images before we start the helper functions.\n\nWe\u2019ll write now all the helper functions that will help us initialize parameters based on different methods as well as writing L-layer model that we\u2019ll be using to train our neural network.\n\nHere, we\u2019ll initialize all weight matrices and biases to zeros and see how this would affect the error rate as well as the learning parameters.\n\nAs the cost curve shows, the neural network didn\u2019t learn anything! That is because of symmetry between all neurons which leads to all neurons have the same update on every iteration. Therefore, regardless of how many iterations we run the optimization algorithms, all the neurons would still get the same update and no learning would happen. As a result, we must break symmetry when initializing parameters so that the model would start learning on each update of the gradient descent.\n\nThere is no big difference if the random values are initialized from standard normal distribution or uniform distribution so we\u2019ll use standard normal distribution in our examples. Also, we\u2019ll multiply the random values by a big number such as 10 to show that initializing parameters to big values may cause our optimization to have higher error rates (and even diverge in some cases). Let\u2019s now train our neural network where all weight matrices have been intitialized using the following formula:\n\nRandom initialization here is helping but still the loss function has high value and may take long time to converge and achieve a significantly low value.\n\nWe\u2019ll train the network using both methods and look at the results.\n\nAs shown from applying the four methods, parameters\u2019 initial values play a huge role in achieving low cost values as well as converging and achieve lower training error rates. The same would apply to test error rate if we had test data.\n\nDeep Learning frameworks make it easier to choose between different initialization methods without worrying about implementing it ourselves. Nonetheless, it\u2019s important to understand the critical role initial values of the parameters in the overall performance of the network. Below are some key takeaways:\n\nThe source code that created this post can be found here."
    },
    {
        "url": "https://towardsdatascience.com/coding-neural-network-gradient-checking-5222544ccc64?source=user_profile---------2----------------",
        "title": "Coding Neural Network \u2014 Gradient Checking \u2013",
        "text": "In the previous post, Coding Neural Network \u2014 Forward Propagation and Backpropagation, we implemented both forward propagation and backpropagation in . However, implementing backpropagation from scratch is usually more prune to bugs/errors. Therefore, it\u2019s necessary before running the neural network on training data to check if our implementation of backpropagation is correct. Before we start, let\u2019s revisit what back-propagation is: We loop over the nodes in reverse topological order starting at the final node to compute the derivative of the cost with respect to each edge\u2019s node tail. In other words, we compute the derivative of cost function with respect to all parameters, i.e \u2202J/\u2202\u03b8 where \u03b8 represents the parameters of the model.\n\nThe way to test our implementation is by computing numerical gradients and compare it with gradients from backpropagation (analytical). There are two way of computing numerical gradients:\n\nTwo-sided form of approximating the derivative is closer than the right-hand form. Let\u2019s illustrate that with the following example using the function f(x) = x\u00b2 by taking its derivative at x = 3.\n\nAs we see above, the difference between analytical derivative and two-sided numerical gradient is almost zero; however, the difference between analytical derivative and right-sided derivative is 0.01. Therefore, we\u2019ll use two-sided epsilon method to compute the numerical gradients.\n\nIn addition, we\u2019ll normalize the difference between numerical. gradients and analytical gradients using the following formula: If the difference is \u2264 10e-7, then our implementation is fine; otherwise, we have a mistake somewhere and have to go back and revisit backpropagation code.\n\nBelow are the steps needed to implement gradient checking:\n\n4. Compute the gradients using our back-propagation implementation.\n\n5. Compute the numerical gradients using the two-sided epsilon method.\n\n6. Compute the difference between numerical and analytical gradients.\n\nWe\u2019ll be using functions we wrote in \u201cCoding Neural Network \u2014 Forward Propagation and Backpropagation\u201d post to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.\n\nNext, we\u2019ll write helper functions that faciltate converting parameters and gradients dictionaries into vectors and then re-convert them back to dictionaries.\n\nFinally, we\u2019ll write the gradient checking function that will compute the difference between the analytical and numerical gradients and tell us if our implementation of back-propagation is correct. We\u2019ll randomly choose 1 example to compute the difference.\n\nBelow are some key takeaways:\n\n2. Since gradient checking is very slow:\n\n3. Gradient checking doesn\u2019t work when applying drop-out method. Use keep-prob = 1 to check gradient checking and then change it when training neural network.\n\n4. Epsilon = 10e-7 is a common value used for the difference between analytical gradient and numerical gradient. If the difference is less than 10e-7 then the implementation of backpropagation is correct.\n\n5. Thanks to Deep Learning frameworks such as Tensorflow and Pytorch, we may find ourselves rarely implement backpropagation because such frameworks compute that for us; however, it\u2019s a good practice to understand what happens under the hood to become a good Deep Learning practitioner.\n\nThe source code that created this post can be found here."
    },
    {
        "url": "https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76?source=user_profile---------3----------------",
        "title": "Coding Neural Network \u2014 Forward Propagation and Backpropagtion",
        "text": "According to Universal Approximate Theorem, Neural Networks can approximate as well as learn and represent any function given a large enough layer and desired error margin. The way neural network learns the true function is by building complex representations on top of simple ones. On each hidden layer, the neural network learns new feature space by first compute the affine (linear) transformations of the given inputs and then apply non-linear function which in turn will be the input of the next layer. This process will continue until we reach the output layer. Therefore, we can define neural network as information flows from inputs through hidden layers towards the output. For a 3-layers neural network, the learned function would be: f(x) = f_3(f_2(f_1(x))) where:\n\nTherefore, on each layer we learn different representation that gets more complicated with later hidden layers.Below is an example of a 3-layers neural network (we don\u2019t count input layer):\n\nFor example, computers can\u2019t understand images directly and don\u2019t know what to do with pixels data. However, a neural network can build a simple representation of the image in the early hidden layers that identifies edges. Given the first hidden layer output, it can learn corners and contours. Given the second hidden layer, it can learn parts such as nose. Finally, it can learn the object identity.\n\nSince truth is never linear and representation is very critical to the performance of a machine learning algorithm, neural network can help us build very complex models and leave it to the algorithm to learn such representations without worrying about feature engineering that takes practitioners very long time and effort to curate a good representation.\n\nThe post has two parts:\n\nThis post will be the first in a series of posts that cover implementing neural network in numpy including gradient checking, parameter initialization, L2 regularization, dropout. The source code that created this post can be found here.\n\nThe input X provides the initial information that then propagates to the hidden units at each layer and finally produce the output $\\widehat{Y}$. The architecture of the network entails determining its depth, width, and activation functions used on each layer. Depth is the number of hidden layers. Width is the number of units (nodes) on each hidden layer since we don\u2019t control neither input layer nor output layer dimensions. There are quite a few set of activation functions such Rectified Linear Unit, Sigmoid, Hyperbolic tangent, etc. Research has proven that deeper networks outperform networks with more hidden units. Therefore, it\u2019s always better and won\u2019t hurt to train a deeper network (with diminishing returns).\n\nLets first introduce some notations that will be used throughout the post:\n\nNext, we\u2019ll write down the dimensions of a multi-layer neural network in the general form to help us in matrix multiplication because one of the major challenges in implementing a neural network is getting the dimensions right.\n\nThe two equations we need to implement forward propagations are: These computations will take place on each layer.\n\nWe\u2019ll first initialize the weight matrices and the bias vectors. It\u2019s important to note that we shouldn\u2019t initialize all the parameters to zero because doing so will lead the gradients to be equal and on each iteration the output would be the same and the learning algorithm won\u2019t learn anything. Therefore, it\u2019s important to randomly initialize the parameters to values between 0 and 1. It\u2019s also recommended to multiply the random values by small scalar such as 0.01 to make the activation units active and be on the regions where activation functions\u2019 derivatives are not close to zero.\n\nThere is no definitive guide for which activation function works best on specific problems. It\u2019s a trial and error process where one should try different set of functions and see which one works best on the problem at hand. We\u2019ll cover 4 of the most commonly used activation functions:\n\nIf you\u2019re not sure which activation function to choose, start with ReLU. Next, we\u2019ll implement the above activation functions and draw a graph for each one to make it easier to see the domain and range of each function.\n\nGiven its inputs from previous layer, each unit computes affine transformation z = W^Tx + b and then apply an activation function g(z) such as ReLU element-wise. During the process, we\u2019ll store (cache) all variables computed and used on each layer to be used in back-propagation. We\u2019ll write first two helper functions that will be used in the L-model forward propagation to make it easier to debug. Keep in mind that on each layer, we may have different activation function.\n\nWe\u2019ll use the binary Cross-Entropy cost. It uses the log-likelihood method to estimate its error. The cost is: The above cost function is convex; however, neural network usually stuck on a local minimum and is not guaranteed to find the optimal parameters. We\u2019ll use here gradient-based learning.\n\nAllows the information to go back from the cost backward through the network in order to compute the gradient. Therefore, loop over the nodes starting at the final node in reverse topological order to compute the derivative of the final node output with respect to each edge\u2019s node tail. Doing so will help us know who is responsible for the most error and change the parameters in that direction. The following derivatives\u2019 formulas will help us write the back-propagate functions: Since b^l is always a vector, the sum would be across rows (since each column is an example).\n\nThe dataset that we\u2019ll be working on has 209 images. Each image is 64 x 64 pixels on RGB scale. We\u2019ll build a neural network to classify if the image has a cat or not. Therefore, y^i \u2208 {0, 1}.\n\nNow, our dataset is ready to be used and test our neural network implementation. Let\u2019s first write multi-layer model function to implement gradient-based learning using predefined number of iterations and learning rate.\n\nNext, we\u2019ll train two versions of the neural network where each one will use different activation function on hidden layers: One will use rectified linear unit (ReLU) and the second one will use hyperbolic tangent function (tanh). Finally we\u2019ll use the parameters we get from both neural networks to classify training examples and compute the training accuracy rates for each version to see which activation function works best on this problem.\n\nPlease note that the accuracy rates above are expected to overestimate the generalization accuracy rates.\n\nThe purpose of this post is to code Deep Neural Network step-by-step and explain the important concepts while doing that. We don\u2019t really care about the accuracy rate at this moment since there are tons of things we could\u2019ve done to increase the accuracy which would be the subject of following posts. Below are some takeaways:\n\nA. Small \u03b1 leads to slow convergence and may become computationally very expensive.\n\nB. Large \u03b1 may lead to overshooting where our learning algorithm may never converge.\n\n2. Number of hidden layers (depth): The more hidden layers the better, but comes at a cost computationally.\n\n3. Number of units per hidden layer (width): Research proven that huge number of hidden units per layer doesn\u2019t add to the improvement of the network.\n\n4. Activation function: Which function to use on hidden layers differs among applications and domains. It\u2019s a trial and error process to try different functions and see which one works best."
    },
    {
        "url": "https://towardsdatascience.com/predicting-loan-repayment-5df4e0023e92?source=user_profile---------4----------------",
        "title": "Predicting Loan Repayment \u2013",
        "text": "The two most critical questions in the lending industry are: 1) How risky is the borrower? 2) Given the borrower\u2019s risk, should we lend him/her? The answer to the first question determines the interest rate the borrower would have. Interest rate measures among other things (such as time value of money) the riskness of the borrower, i.e. the riskier the borrower, the higher the interest rate. With interest rate in mind, we can then determine if the borrower is eligible for the loan.\n\nInvestors (lenders) provide loans to borrowers in exchange for the promise of repayment with interest. That means the lender only makes profit (interest) if the borrower pays off the loan. However, if he/she doesn\u2019t repay the loan, then the lender loses money.\n\nWe\u2019ll be using publicly available data from [LendingClub.com]. The data covers the 9,578 loans funded by the platform between May 2007 and February 2010. The interest rate is provided to us for each borrower. Therefore, so we\u2019ll address the second question indirectly by trying to predict if the borrower will repay the loan by its mature date or not. Through this excerise we\u2019ll illustrate three modeling concepts:\n\nBelow is a short description of each feature in the data set:\n\nSource code that created this post can be found here.\n\nIt looks like we have only one categorical feature (\u201cpurpose\u201d). Also, six features have missing values (no missing values in labels). Moreover, the data set is pretty imbalanced as expected where positive examples (\u201cnot paid fully\u201d) are only 19%. We\u2019ll explain in the next section how to handle all of them after giving an overview of ensemble methods.\n\nEnsemble methods can be defined as combining several different models (base learners) into final model (meta learner) to reduce the generalization error. It relies on the assumption that each model would look at a different aspect of the data which yield to capturing part of the truth. Combining good performing models the were trained independently will capture more of the truth than a single model. Therefore, this would result in more accurate predictions and lower generalization errors.\n\nDi\ufb00erent ensemble methods construct the ensemble of models in di\ufb00erent ways. Below are the most common methods:\n\nSince we\u2019ll be using Random Fores (bagging) and Gradient Boosting (boosting) classifiers as base learners in the ensemble model, we\u2019ll illustrate only averaging and stacking ensemble methods. Therefore, modeling parts would be consisted of three parts:\n\nBefore going further, the following data preprocessing steps will be applicable to all models:\n\nAlmost always real world data sets have missing values. This can be due, for example, users didn\u2019t fill some part of the forms or some transformations happened while collecting and cleaning the data before they send it to you. Sometimes missing values are informative and weren\u2019t generated randomly. Therefore, it\u2019s a good practice to add binary features to check if there is missing values in each row for each feature that has missing values. In our case, six features have missing values so we would add six binary features one for each feature. For example, \u201clog_annual_inc\u201d feature has missing values, so we would add a feature \u201cis_log_annual_inc_missing\u201d that takes the values \u2208 {0, 1}. Good thing is that the missing values are in the predictors only and not the labels. Below are some of the most common strategies for dealing with missing values:\n\n2. Second step: For each feature that has missing values, we take all other features as predictors (including the ones that had missing values) and try to predict the values for this feature using linear regression for example. The predicted values will replace the old values for that feature. We do this for all features that have missing values, i.e. each feature will be used once as a target variable to predict its values and the rest of the time as a predictor to predict other features\u2019 values. Therefore, one complete cycle (iteration) will be done once we run the model $k$ times to predict the $k$ features that have missing values. For our data set, each iteration will run the linear regression 6 times to predict the 6 features.\n\n3. Third step: Repeat step 2 until there is not much of change between predictions.\n\nTo evaluate each strategy, we\u2019ll use Random Forest classifier with hyperparameters\u2019 values guided by Data-driven Advice for Applying Machine Learning to Bioinformatics Problems as a starting point.\n\nLet\u2019s first create binary features for missing values and then prepare the data for each strategy discussed above. Next, we\u2019ll compute the 10-folds cross validation AUC score for all the models using training data.\n\nLet\u2019s plot the feature importances to check if the added binary features added anything to the model.\n\nGuided by the 10-fold cross validation AUC scores, it looks like all strategies have comparable results and missing values were generated randomly. Also, the added six binary features showed no importance when plotting feature importances from Random Forest classifier. Therefore, it\u2019s safe to drop those features and use Median Imputation method as a transformer later on in the pipeline.\n\nClassification problems in most real world applications have imbalanced data sets. In other words, the positive examples (minority class) are a lot less than negative examples (majority class). We can see that in spam detection, ads click, loan approvals, etc. In our example, the positive examples (people who haven\u2019t fully paid) were only 19% from the total examples. Therefore, accuracy is no longer a good measure of performance for different models because if we simply predict all examples to belong to the negative class, we achieve 81% accuracy. Better metrics for imbalanced data sets are AUC (area under the ROC curve) and f1-score. However, that\u2019s not enough because class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. As a result, we\u2019ll explore different methods to overcome class imbalance problem.\n\na. Compute the difference between minority sample and its randomly chosen neighbor (from previous step).\n\nb. Multiply the difference by random number between 0 and 1.\n\nc. Add the obtained feature to the synthesized sample attributes.\n\n4. Repeat the above until we get the number of synthesized samples needed. More information can be found here.\n\nThere are other methods such as and that we will not cover in this post and are rarely used in practice.\n\nIn most applications, misclassifying the minority class (false negative) is a lot more expensive than misclassifying the majority class (false positive). In the context of lending, loosing money by lending to a risky borrower who is more likely to not fully pay the loan back is a lot more costly than missing the opportunity of lending to trust-worthy borrower (less risky). As a result, we can use that changes the weight of misclassifying positive example in the loss function. Also, we can use different cut-offs assign examples to classes. By default, 0.5 is the cut-off; however, we see more often in applications such as lending that the cut-off is less than 0.5. Note that changing the cut-off from the default 0.5 reduce the overall accuracy but may improve the accuracy of predicting positive/negative examples.\n\nWe\u2019ll evaluate all the above methods plus the original model without resampling as a baseline model using the same Random Forest classifier we used in the missing values section.\n\nEasyEnsemble method has the highest 10-folds CV with average AUC = 0.665.\n\nWe\u2019ll build ensemble models using three different models as base learners:\n\nThe ensemble models will be built using two different methods:\n\nWe\u2019ll use logistic regression as the meta-learner for the stacked model. Note that we can use k-folds cross validation to validate and tune the hyperparameters of the meta learner. We will not tune the hyperparameters of any of the base learners or the meta-learner; however, we will use some of the values recommended by the Pennsylvania Benchmarking Paper. Additionally, we won\u2019t use EasyEnsemble in training because, after some experimentation, it didn\u2019t improve the AUC of the ensemble model more than 2% on average and it was computationally very expensive. In practice, we sometimes are willing to give up small improvements if the model would become a lot more complex computationally. Therefore, we will use . Also, we\u2019ll impute the missing values and standardize the data beforehand so that it would shorten the code of the ensemble models and allows use to avoid using . Additionally, we will plot ROC and PR curves using test data and evaluate the performance of all models.\n\nAs we can see from the chart above, stacked ensemble model didn\u2019t improve the performance. One of the major reasons are that the base learners are considerably highly correlated especially Random Forest and Gradient Boosting (see the correlation matrix below).\n\nIn addition, with classification problems where False Negatives are a lot more expensive than False Positives, we may want to have a model with a high precision rather than high recall, i.e. the probability of the model to identify positive examples from randomly selected examples. Below is the confusion matrix:\n\nLet\u2019s finally check the partial dependence plots to see what are the most important features and their relationships with whether the borrower will most likely pay the loan in full before mature data. we will plot only the top 8 features to make it easier to read. Note that the partial plots are based on Gradient Boosting model.\n\nAs we might expected, borrowers with lower annual income and less FICO scores are less likely to pay the loan fully; however, borrowers with lower interest rates (riskier) and smaller installments are more likely to pay the loan fully.\n\nMost classification problems in the real world are imbalanced. Also, almost always data sets have missing values. In this post, we covered strategies to deal with both missing values and imbalanced data sets. We also explored different ways of building ensembles in sklearn. Below are some takeaway points:"
    },
    {
        "url": "https://towardsdatascience.com/character-level-language-model-1439f5dd87fe?source=user_profile---------5----------------",
        "title": "Character-Level Language Model \u2013",
        "text": "Have you ever wondered how Gmail automatic reply works? Or how your phone suggests next word when texting? Or even how a Neural Network can generate musical notes? The general way of generating a sequence of text is to train a model to predict the next word/character given all previous words/characters. Such model is called a Statistical Language Model. What is a statistical language model? A statistical language model tries to capture the statistical structure (latent space) of training text it\u2019s trained on. Usually Recurrent Neural Network (RNN) models family are used to train the model due to the fact that they are very powerful and expressive in which they remember and process past information through their high dimensional hidden state units. The main goal of any language model is to learn the joint probability distribution of sequences of characters/words in a training text, i.e. trying to learn the joint probability function. For example, if we\u2019re trying to predict a sequence of T words, we try to get the joint probability P(w_1,w_2, \u2026,w_T) as big as we can which is equal to the product of all conditional probabilities \u220f P(w_t/w_{t-1}) at all time steps (t) .\n\nIn this post, we\u2019ll cover the Character-Level Language Model where almost all the concepts hold for any other language models such as word-language models. The main task of the character-level language model is to predict the next character given all previous characters in a sequence of data, i.e. generates text character by character. More formally, given a training sequence (x\u00b9, \u2026 , x^T), the RNN uses the sequence of its output vectors (o\u00b9, \u2026 , o^T) to obtain a sequence of predictive distributions P(x^t/x^{t-1}) = softmax(o^t).\n\nLet\u2019s illustrate how the character-level language model works using my first name (\u201cimad\u201d) as an example (see figure 1 for all the details of this example).\n\nThe objective is to make the green numbers as big as we can and the red numbers as small as we can in the probability distribution layer. The reason is that the true index should have the highest probability by making it as close as we can to 1. The way to do that is to measure the loss using cross-entropy and then compute the gradients of the loss w.r.t. all parameters to update them in the opposite of the gradient direction. Repeating the process over many times where each time we adjust the parameters based on the gradient direction \u2013> model will be able to correctly predict next characters given all previous ones using all names in the training text. Notice that hidden state $h\u2074$ has all past information about all characters.\n\nNote: To shorten the length of the post, I deleted all the docstrings of python functions and I didn\u2019t include some functions that i didn\u2019t think are necessary to understand the main concepts. The notebook and the script that created this post can be found here and here.\n\nThe dataset we\u2019ll be using has 5,163 names: 4,275 male names, 1,219 female names, and 331 names that can be both female and male names. The RNN architecture we\u2019ll be using to train the character-level language model is called many to many where time steps of the input T_x= time steps of the output T_y. In other words, the sequence of the input and output are synced (see figure 2).\n\nThe character-level language model will be trained on names; which means after we\u2019re done with training the model, we\u2019ll be able to generate some interesting names :).\n\nIn this section, we\u2019ll go over four main parts:\n\nWe\u2019ll be using Stochastic Gradient Descent (SGD) where each batch consists of only one example. In other words, the RNN model will learn from each example (name) separately, i.e. run both forward and backward passes on each example and update parameters accordingly. Below are all the steps needed for a forward pass:\n\nNotice that we use hyperbolic tangent as the non-linear function. One of the main advantages of the hyperbolic tangent function is that it resembles the identity function.\n\nThe softmax layer has the same dimension as the output layer which is vocab_size x 1. As a result, y^t[i] is the probability of index i being the next character at time step (t).\n\nSince we\u2019ll be using SGD, the loss will be noisy and have many oscillations, so it\u2019s a good practice to smooth out the loss using exponential weighted average.\n\nWith RNN based models, the gradient-based technique that will be used is called Backpropagation Through Time (BPTT). We start at last time step $T$ and backpropagate loss function w.r.t. all parameters across all time steps and sum them up (see figure 3).\n\nIn addition, since RNNs are known to have steep cliffs (sudden steep decrease in L, gradients may overshoot the minimum and undo a lot of the work that was done even if we are using adaptive learning methods such as RMSProp. The reason is because gradient is a linear approximation of the loss function and may not capture information further than the point it was evaluated on such as the curvature of loss curve. Therefore, it\u2019s a common practice to clip the gradients to be in the interval [-maxValue, maxValue]. For this exercise, we\u2019ll clip the gradients to be in the interval [-5, 5]. That means if the gradient is > 5 or < -5, it would be clipped to 5 and -5 respectively. Below are all the formulas needed to compute the gradients w.r.t. all parameters at all time steps.\n\nNote that at last time step T, we\u2019ll initialize dh_next to zeros since we can\u2019t get values from future. To stabilize the update at each time step since SGD may have so many oscillations, we\u2019ll be using one of the adaptive learning method optimizers. More specifically, we\u2019ll use Root Mean Squared Propagation (RMSProp) which tends to have acceptable performance.\n\nSampling is what makes the text generated by the RNN at each time step an interesting/creative text. On each time step (t), the RNN output the conditional probability distribution of the next character given all the previous characters, i.e. P(c_t/c_1, c_2, \u2026, c_{t-1}). Let\u2019s assume that we are at time step t = 3 and we\u2019re trying to predict the third character, the conditional probability distribution is: P(c_3/c_1, c_2) = (0.2, 0.3, 0.4, 0.1). We\u2019ll have two extremes:\n\nAs we increase randomness, text will lose local structure; however, as we decrease randomness, the generated text will sound more real and start to preserve its local structure. For this exercise, we will sample from the distribution that\u2019s generated by the model which can be seen as an intermediate level of randomness between maximum and minimum entropy (see figure 4). Using this sampling strategy on the above distribution, the index 0 has 20% probability of being picked, while index 2 has 40% probability to be picked.\n\nTherefore, sampling will be used at test time to generate names character by character.\n\nAfter covering all the concepts/intuitions behind character-level language model, now we\u2019re ready to fit the model. We\u2019ll use the default settings for RMSProp\u2019s hyperparameters and run the model for 100 iterations. On each iteration, we\u2019ll print out one sampled name and smoothed loss to see how the names generated start to get more interesting with more iterations as well as the loss will start decreasing. When done with fitting the model, we\u2019ll plot the loss function and generate some names.\n\nBelow is few of output generated during training:\n\nThe names that were generated started to get more interesting after 15 epochs. I didn\u2019t include the results of all epochs to shorten the post; however, you can check the results in the notebook associated with this post. One of the interesting names is \u201cYasira\u201d which is an Arabic name :).\n\nStatistical language models are very crucial in Natural Language Processing (NLP) such as speech recognition and machine translation. We demonstrated in this post the main concepts behind statistical language models using character-level language model. The task of this model is to generate names character by character using names obtained from census data that were consisted of 5,163 names. Below are the main key takeaways:"
    },
    {
        "url": "https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3?source=user_profile---------6----------------",
        "title": "Gradient Descent Algorithm and Its Variants \u2013",
        "text": "Optimization refers to the task of minimizing/maximizing an objective function f(x) parameterized by x. In machine/deep learning terminology, it\u2019s the task of minimizing the cost/loss function J(w) parameterized by the model\u2019s parameters w \u2208 R^d. Optimization algorithms (in case of minimization) have one of the following goals:\n\nThere are three kinds of optimization algorithms:\n\nGradient Descent is the most common optimization algorithm in machine learning and deep learning. It is a first-order optimization algorithm. This means it only takes into account the first derivative when performing the updates on the parameters. On each iteration, we update the parameters in the opposite direction of the gradient of the objective function J(w) w.r.t the parameters where the gradient gives the direction of the steepest ascent. The size of the step we take on each iteration to reach the local minimum is determined by the learning rate \u03b1. Therefore, we follow the direction of the slope downhill until we reach a local minimum.\n\nIn this article, we\u2019ll cover gradient descent algorithm and its variants: Batch Gradient Descent, Mini-batch Gradient Descent, and Stochastic Gradient Descent.\n\nLet\u2019s first see how gradient descent works on logistic regression before going into the details of its variants. For the sake of simplicity, let\u2019s assume that the logistic regression model has only two parameters: weight w and bias b.\n\n1. Initialize weight w and bias b to any random numbers.\n\n2. Pick a value for the learning rate \u03b1. The learning rate determines how big the step would be on each iteration.\n\nTherefore, plot the cost function against different values of \u03b1 and pick the value of \u03b1 that is right before the first value that didn\u2019t converge so that we would have a very fast learning algorithm that converges (see figure 2).\n\n3. Make sure to scale the data if it\u2019s on a very different scales. If we don\u2019t scale the data, the level curves (contours) would be narrower and taller which means it would take longer time to converge (see figure 3).\n\nScale the data to have \u03bc = 0 and \u03c3 = 1. Below is the formula for scaling each example:\n\n4. On each iteration, take the partial derivative of the cost function J(w) w.r.t each parameter (gradient):\n\nNow let\u2019s discuss the three variants of gradient descent algorithm. The main difference between them is the amount of data we use when computing the gradients for each learning step. The trade-off between them is the accuracy of the gradient versus the time complexity to perform each parameter\u2019s update (learning step).\n\nBatch Gradient Descent is when we sum up over all examples on each iteration when performing the updates to the parameters. Therefore, for each update, we have to sum over all examples:\n\nInstead of going over all examples, Mini-batch Gradient Descent sums up over lower number of examples based on the batch size. Therefore, learning happens on each mini-batch of b examples:\n\nThe batch size is something we can tune. It is usually chosen as power of 2 such as 32, 64, 128, 256, 512, etc. The reason behind it is because some hardware such as GPUs achieve better run time with common batch sizes such as power of 2.\n\nWith large training datasets, we don\u2019t usually need more than 2\u201310 passes over all training examples (epochs). Note: with batch size b = m (number of training examples), we get the Batch Gradient Descent.\n\nInstead of going through all examples, Stochastic Gradient Descent (SGD) performs the parameters update on each example (x^i,y^i). Therefore, learning happens on every example:\n\nIt shares most of the advantages and the disadvantages with mini-batch version. Below are the ones that are specific to SGD:\n\nBelow is a graph that shows the gradient descent\u2019s variants and their direction towards the minimum:\n\nAs the figure above shows, SGD direction is very noisy compared to mini-batch.\n\nBelow are some challenges regarding gradient descent algorithm in general as well as its variants \u2014 mainly batch and mini-batch:\n\nAs a result, the direction that looks promising to the gradient may not be so and may lead to slow the learning process or even diverge."
    },
    {
        "url": "https://towardsdatascience.com/predicting-employee-turnover-7ab2b9ecf47e?source=user_profile---------7----------------",
        "title": "Predicting Employee Turnover \u2013",
        "text": "Employee turnover refers to the percentage of workers who leave an organization and are replaced by new employees. It is very costly for organizations, where costs include but not limited to: separation, vacancy, recruitment, training and replacement. On average, organizations invest between four weeks and three months training new employees. This investment would be a loss for the company if the new employee decided to leave the first year. Furthermore, organizations such as consulting firms would suffer from deterioration in customer satisfaction due to regular changes in Account Reps and/or consultants that would lead to loss of businesses with clients.\n\nIn this post, we\u2019ll work on simulated HR data from kaggle to build a classifier that helps us predict what kind of employees will be more likely to leave given some attributes. Such classifier would help an organization predict employee turnover and be pro-active in helping to solve such costly matter. We\u2019ll restrict ourselves to use the most common classifiers: Random Forest, Gradient Boosting Trees, K-Nearest Neighbors, Logistic Regression and Support Vector Machine.\n\nThe data has 14,999 examples (samples). Below are the features and the definitions of each one:\n\nSource code that created this post can be found here.\n\nLet\u2019s take a look at the data (check if there are missing values and the data type of each features):\n\nSince there are no missing values, we do not have to do any imputation. However, there are some data preprocessing needed:\n\nThe data is now ready to be used for modeling. The final number of features are now 17.\n\nLet\u2019s first take a look at the proportion of each class to see if we\u2019re dealing with balanced or imbalanced data, since each one has its own set of tools to be used when fitting classifiers.\n\nAs the graph shows, we have an imbalanced dataset. As a result, when we fit classifiers on such datasets, we should use metrics other than accuracy when comparing models such as f1-score or AUC (area under ROC curve). Moreover, class imbalance influences a learning algorithm during training by making the decision rule biased towards the majority class by implicitly learns a model that optimizes the predictions based on the majority class in the dataset. There are three ways to deal with this issue:\n\nNonetheless, there is no definitive guide or best practices to deal with such situations. Therefore, we have to try them all and see which one works best for the problem on hand. We\u2019ll restrict ourselves to use the first two, i.e, assign larger penalty to wrong predictions from the minority class using in classifiers that allows us do that and evaluate upsampling/downsampling on the training data to see which gives higher performance.\n\nFirst, split the data into training and test sets using 80/20 split; 80% of the data will be used to train the models and 20% to test the performance of the models. Second, Upsample the minority class and downsample the majority class. For this data set, positive class is the minority class and negative class is the majority class.\n\nI don\u2019t think we need to apply dimensionality reduction such as PCA because: 1) We want to know the importance of each feature in determining who will leave versus who will not (inference). 2) Dimension of the data set is decent (17 features). However, it\u2019s good to see how many principal components needed to explain 90%, 95% and 99% of the variation in the data.\n\nLooks like it needs 14, 15 and 16 principal components to capture 90%, 95% and 99% of the variation in the data respectively. In other words, this means that the data is already in a good space since eigenvalues are very close to each other and gives further evidence that we don\u2019t need to compress the data.\n\nThe methodology that we\u2019ll follow when building the classifiers goes as follows:\n\nI. Standardizing the data to speed up convergence and make all features on the same scale.\n\nII. The classifier ( ) we want to use to fit the model.\n\n2. Use to tune hyperparameters using 10-folds cross validation. We can use which is faster and may outperform especially if we have more than two hyperparameters and the range for each one is very big; however, will work just fine since we have only two hyperparameters and descent range.\n\n4. Plot both confusion matrix and ROC curve for the best estimator using test data.\n\nRepeat the above steps for Random Forest, Gradient Boosting Trees, K-Nearest Neighbors, Logistic Regression and Support Vector Machine. Next, pick the classifier that has the highest cross validation f1 score. Note: some of the hyperparameter ranges will be guided by the paper Data-driven Advice for Applying Machine Learning to Bioinformatics Problems.\n\nFirst, we will start by fitting a Random Forest classifier using unsampled, upsampled and downsampled data. Second, we will evaluate each method using cross validation (CV) f1-score and pick the one with the highest CV f1-score. Finally, we will use that method to fit the rest of the classifiers.\n\nThe only hyperparameters we\u2019ll tune are:\n\nRandom Forest is an ensemble model that has multiple trees ( ). The final prediction would be a weighting average (regression) or mode (classification) of the predictions from all estimators. Note: a high number of trees doesn't cause overfitting.\n\nUpsampling yielded the highest CV f1-score with 99.8%. Therefore, we\u2019ll be using the upsampled data to fit the rest of the classifiers. The new data now has 18,284 examples: 50% belonging to the positive class, and 50% belonging to the negative class.\n\nLet\u2019s refit the Random Forest with Upsampled data using best hyperparameters tuned above and plot confusion matrix and ROC curve using test data.\n\nGradient Boosting trees are the same as Random Forest except for:\n\nTherefore, we can think of each tree as a weak learner. The two other hyperparameters than and that we're going to tune are:\n\nLet\u2019s fit GB classifier and plot confusion matrix and ROC curve using test data.\n\nKNN is called a lazy learning algorithm because it doesn\u2019t learn or fit any parameter. It takes points from the training data closest to the point we're interested to predict it's class and take the mode (majority vote) of the classes for the neighboring point as its class. The two hyperparameters we're going to tune are:\n\nWe won\u2019t use any non-linearities such as polynomial features.\n\nSVM is comutationally very expensive to tune it\u2019s hyperparameters for two reasons:\n\nTherefore, we\u2019ll use recommended hyperparameters\u2019 values from the paper we mentioned before that showed to yield the best performane on Penn Machine Learning Benchmark 165 datasets. The hyperparameters that we usually look to tune are:\n\nLet\u2019s conclude by printing out the test accuracy rates for all classifiers we\u2019ve trained so far and plot ROC curves. Then we will pick the classifier that has the highest area under ROC curve.\n\nEven though Random Forest and Gradient Boosting Trees have almost equal AUC, Random Forest has higher accuracy rate and an f1-score with 99.27% and 99.44% respectively. Therefore, we safely say Random Forest outperforms the rest of the classifiers. Let\u2019s have a look of feature importances from Random Forest classifier.\n\nLooks like the five most important features are:\n\nThe take home message is the following:"
    }
]