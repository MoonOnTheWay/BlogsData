[
    {
        "url": "https://medium.com/@karpathy/software-2-0-a64152b37c35?source=user_profile---------1----------------",
        "title": "Software 2.0 \u2013 Andrej Karpathy \u2013",
        "text": "I sometimes see people refer to neural networks as just \u201canother tool in your machine learning toolbox\u201d. They have some pros and cons, they work here or there, and sometimes you can use them to win Kaggle competitions. Unfortunately, this interpretation completely misses the forest for the trees. Neural networks are not just another classifier, they represent the beginning of a fundamental shift in how we write software. They are Software 2.0.\n\nThe \u201cclassical stack\u201d of Software 1.0 is what we\u2019re all familiar with \u2014 it is written in languages such as Python, C++, etc. It consists of explicit instructions to the computer written by a programmer. By writing each line of code, the programmer is identifying a specific point in program space with some desirable behavior.\n\nIn contrast, Software 2.0 is written in neural network weights. No human is involved in writing this code because there are a lot of weights (typical networks might have millions), and coding directly in weights is kind of hard (I tried). Instead, we specify some constraints on the behavior of a desirable program (e.g., a dataset of input output pairs of examples) and use the computational resources at our disposal to search the program space for a program that satisfies the constraints. In the case of neural networks, we restrict the search to a continuous subset of the program space where the search process can be made (somewhat surprisingly) efficient with backpropagation and stochastic gradient descent.\n\nIt turns out that a large portion of real-world problems have the property that it is significantly easier to collect the data (or more generally, identify a desirable behavior) than to explicitly write the program. A large portion of programmers of tomorrow do not maintain complex software repositories, write intricate programs, or analyze their running times. They collect, clean, manipulate, label, analyze and visualize data that feeds neural networks.\n\nSoftware 2.0 is not going to replace 1.0 (indeed, a large amount of 1.0 infrastructure is needed for training and inference to \u201ccompile\u201d 2.0 code), but it is going to take over increasingly large portions of what Software 1.0 is responsible for today. Let\u2019s examine some examples of the ongoing transition to make this more concrete:\n\nVisual Recognition used to consist of engineered features with a bit of machine learning sprinkled on top at the end (e.g., SVM). Since then, we developed the machinery to discover much more powerful image analysis programs (in the family of ConvNet architectures), and more recently we\u2019ve begun searching over architectures.\n\nSpeech recognition used to involve a lot of preprocessing, gaussian mixture models and hidden markov models, but today consist almost entirely of neural net stuff. A very related, often cited humorous quote attributed to Fred Jelinek from 1985 reads \u201cEvery time I fire a linguist, the performance of our speech recognition system goes up\u201d.\n\nSpeech synthesis has historically been approached with various stitching mechanisms, but today the state of the art models are large ConvNets (e.g. WaveNet) that produce raw audio signal outputs.\n\nMachine Translation has usually been approaches with phrase-based statistical techniques, but neural networks are quickly becoming dominant. My favorite architectures are trained in the multilingual setting, where a single model translates from any source language to any target language, and in weakly supervised (or entirely unsupervised) settings.\n\nGames. Go playing programs have existed for a long while, but AlphaGo Zero (a ConvNet that looks at the raw state of the board and plays a move) has now become by far the strongest player of the game. I expect we\u2019re going to see very similar results in other areas, e.g. DOTA 2, or StarCraft.\n\nRobotics has a long tradition of breaking down the problem into blocks of sensing, pose estimation, planning, control, uncertainty modeling etc., using explicit representations and algorithms over intermediate representations. We\u2019re not quite there yet, but research at UC Berkeley and Google hint at the fact that Software 2.0 may be able to do a much better job of representing all of this code.\n\nDatabases. More traditional systems outside of Artificial Intelligence are also seeing early hints of a transition. For instance, \u201cThe Case for Learned Index Structures\u201d replaces core components of a data management system with a neural network, outperforming cache-optimized B-Trees by up to 70% in speed while saving an order-of-magnitude in memory.\n\nYou\u2019ll notice that many of my links above involve work done at Google. This is because Google is currently at the forefront of re-writing large chunks of itself into Software 2.0 code. \u201cOne model to rule them all\u201d provides an early sketch of what this might look like, where the statistical strength of the individual domains is amalgamated into one consistent understanding of the world.\n\nWhy should we prefer to port complex programs into Software 2.0? Clearly, one easy answer is that they work better in practice. However, there are a lot of other convenient reasons to prefer this stack. Let\u2019s take a look at some of the benefits of Software 2.0 (think: a ConvNet) compared to Software 1.0 (think: a production-level C++ code base). Software 2.0 is:\n\nComputationally homogeneous. A typical neural network is, to the first order, made up of a sandwich of only two operations: matrix multiplication and thresholding at zero (ReLU). Compare that with the instruction set of classical software, which is significantly more heterogenous and complex. Because you only have to provide Software 1.0 implementation for a small number of the core computational primitives (e.g. matrix multiply), it is much easier to make various correctness/performance guarantees.\n\nSimple to bake into silicon. As a corollary, since the instruction set of a neural network is relatively small, it is significantly easier to implement these networks much closer to silicon, e.g. with custom ASICs, neuromorphic chips, and so on. The world will change when low-powered intelligence becomes pervasive around us. E.g., small, inexpensive chips could come with a pretrained ConvNet, a speech recognizer, and a WaveNet speech synthesis network all integrated in a small protobrain that you can attach to anything.\n\nConstant running time. Every iteration of a typical neural net forward pass takes exactly the same amount of FLOPS. There is zero variability based on the different execution paths your code could take through some sprawling C++ code base. Of course, you could have dynamic compute graphs but the execution flow is normally still significantly constrained. This way we are also almost guaranteed to never find ourselves in unintended infinite loops.\n\nConstant memory use. Related to the above, there is no dynamically allocated memory anywhere so there is also little possibility of swapping to disk, or memory leaks that you have to hunt down in your code.\n\nIt is highly portable. A sequence of matrix multiplies is significantly easier to run on arbitrary computational configurations compared to classical binaries or scripts.\n\nIt is very agile. If you had a C++ code and someone wanted you to make it twice as fast (at cost of performance if needed), it would be highly non-trivial to tune the system for the new spec. However, in Software 2.0 we can take our network, remove half of the channels, retrain, and there \u2014 it runs exactly at twice the speed and works a bit worse. It\u2019s magic. Conversely, if you happen to get more data/compute, you can immediately make your program work better just by adding more channels and retraining.\n\nModules can meld into an optimal whole. Our software is often decomposed into modules that communicate through public functions, APIs, or endpoints. However, if two Software 2.0 modules that were originally trained separately interact, we can easily backpropagate through the whole. Think about how amazing it could be if your web browser could automatically re-design the low-level system instructions 10 stacks down to achieve a higher efficiency in loading web pages. With 2.0, this is the default behavior.\n\nIt is better than you. Finally, and most importantly, a neural network is a better piece of code than anything you or I can come up with in a large fraction of valuable verticals, which currently at the very least involve anything to do with images/video, sound/speech, and text.\n\nThe 2.0 stack also has some of its own disadvantages. At the end of the optimization we\u2019re left with large networks that work well, but it\u2019s very hard to tell how. Across many applications areas, we\u2019ll be left with a choice of using a 90% accurate model we understand, or 99% accurate model we don\u2019t.\n\nThe 2.0 stack can fail in unintuitive and embarrassing ways ,or worse, they can \u201csilently fail\u201d, e.g., by silently adopting biases in their training data, which are very difficult to properly analyze and examine when their sizes are easily in the millions in most cases.\n\nFinally, we\u2019re still discovering some of the peculiar properties of this stack. For instance, the existence of adversarial examples and attacks highlights the unintuitive nature of this stack.\n\nIf you think of neural networks as a software stack and not just a pretty good classifier, it becomes quickly apparent that they have a huge number of advantages and a lot of potential for transforming software in general.\n\nMore than that, this lens immediately suggests more work that is needed in the area. For example, in the 1.0 stack LLVM IR forms the middle layer between a number of front ends (languages) and back ends (architectures) and provides an opportunity for optimization. With neural networks we\u2019re already seeing an explosion of front ends (PyTorch, TF, Chainer, mxnet, etc) and back ends (CPU, GPU, TPU?, IPU?, \u2026), but what is a fitting IR, and how we can optimize it (Halide-like)?\n\nAs another example, we have a huge amount of tooling that assists humans in writing 1.0 code, like powerful IDEs with features like syntax highlighting, debuggers, profilers, go to def, etc. With Software 2.0 we don\u2019t write the code, but we do assemble datasets which imply the code through the optimization. Who is going to develop the first powerful Software 2.0 IDEs, which help us accumulate, visualize, clean, label, and source data? Perhaps the IDE bubbles up images that the network suspects are mislabeled, or assists in labeling, or finds more examples where the network is currently uncertain.\n\nFinally, in the long term, the future of Software 2.0 is bright because it is increasingly clear to many that when we develop AGI, it will certainly be written in Software 2.0.\n\nAnd Software 3.0? That will be entirely up to the AGI."
    },
    {
        "url": "https://medium.com/@karpathy/alphago-in-context-c47718cb95a5?source=user_profile---------2----------------",
        "title": "AlphaGo, in context \u2013 Andrej Karpathy \u2013",
        "text": "Update Oct 18, 2017: AlphaGo Zero was announced. This post refers to the previous version. 95% of it still applies.\n\nI had a chance to talk to several people about the recent AlphaGo matches with Ke Jie and others. In particular, most of the coverage was a mix of popular science + PR so the most common questions I\u2019ve seen were along the lines of \u201cto what extent is AlphaGo a breakthrough?\u201d, \u201cHow do researchers in AI see its victories?\u201d and \u201cwhat implications do the wins have?\u201d. I thought I might as well serialize some of my thoughts into a post.\n\nAlphaGo is made up of a number of relatively standard techniques: behavior cloning (supervised learning on human demonstration data), reinforcement learning (REINFORCE), value functions, and Monte Carlo Tree Search (MCTS). However, the way these components are combined is novel and not exactly standard. In particular, AlphaGo uses a SL (supervised learning) policy to initialize the learning of an RL (reinforcement learning) policy that gets perfected with self-play, which they then estimate a value function from, which then plugs into MCTS that (somewhat surprisingly) uses the (worse!, but more diverse) SL policy to sample rollouts. In addition, the policy/value nets are deep neural networks, so getting everything to work properly presents its own unique challenges (e.g. value function is trained in a tricky way to prevent overfitting). On all of these aspects, DeepMind has executed very well. That being said, AlphaGo does not by itself use any fundamental algorithmic breakthroughs in how we approach RL problems.\n\nZooming out, it is also still the case that AlphaGo is a narrow AI system that can play Go and that\u2019s it. The ATARI-playing agents from DeepMind do not use the approach taken with AlphaGo. The Neural Turing Machine has little to do with AlphaGo. The Google datacenter improvements definitely do not use AlphaGo. The Google Search engine is not going to use AlphaGo. Therefore, AlphaGo does not generalize to any problem outside of Go, but the people and the underlying neural network components do, and do so much more effectively than in the days of old AI where each demonstration needed repositories of specialized, explicit code.\n\nI wanted to expand on the narrowness of AlphaGo by explicitly trying to list some of the specific properties that Go has, which AlphaGo benefits a lot from. This can help us think about what settings AlphaGo does or does not generalize to. Go is:\n\nHaving enumerated some of the appealing properties of Go, let\u2019s look at a robotics problem and see how well we could apply AlphaGo to, for example, an Amazon Picking Challenge robot. It\u2019s a little comical to even think about.\n\nIn short, basically every single assumption that Go satisfies and that AlphaGo takes advantage of are violated, and any successful approach would look extremely different. More generally, some of Go\u2019s properties above are not insurmountable with current algorithms (e.g. 1,2,3), some are somewhat problematic (5,7), but some are quite critical to how AlphaGo is trained but are rarely present in other real-world applications (4,6).\n\nWhile AlphaGo does not introduce fundamental breakthroughs in AI algorithmically, and while it is still an example of narrow AI, AlphaGo does symbolize Alphabet\u2019s AI power: in both the quantity/quality of the talent present in the company, the computational resources at their disposal, and the all in focus on AI from the very top.\n\nAlphabet is making a large bet on AI, and it is a safe one. But I\u2019m biased :)\n\nEDIT: the goal of this post is, as someone on reddit mentioned, \u201cquelling the ever resilient beliefs of the public that AGI is right down the road\u201d, and the target audience are people outside of AI who were watching AlphaGo and would like a more technical commentary."
    },
    {
        "url": "https://medium.com/@karpathy/icml-accepted-papers-institution-stats-bad8d2943f5d?source=user_profile---------3----------------",
        "title": "ICML accepted papers institution stats \u2013 Andrej Karpathy \u2013",
        "text": "The accepted papers at ICML have been published. ICML is a top Machine Learning conference, and one of the most relevant to Deep Learning, although NIPS has a longer DL tradition and ICLR, being more focused, has a much higher DL density.\n\nI thought it would be fun to compute some stats on institutions. Armed with Jupyter Notebook and regex, we look for all of the institution mentions, add up their counts and sort. Modulo a few annoyances:\n\nIn total we get 961 institution mentions, 420 unique. The top 30 are:\n\nI\u2019m not quite sure about \u201cNone\u201d (15) in there. It\u2019s listed as an institution on the ICML page and I can\u2019t tell if they have a bug or if that\u2019s a real cool new AI institution we don\u2019t yet know about.\n\nTo get an idea of how much of the research is done at industry, I took the counts for the largest industry labs (DeepMind, Google, Microsoft, Facebook, IBM, Disney, Amazon, Adobe) and divide by the total. We get 14%, but this doesn\u2019t capture the looong tail. Looking through the tail, I think it\u2019s fair to say that\n\nor rather, approximately three quarters of all papers at ICML have come entirely out of Academia. Also, since DeepMind/Google are both Alphabet, we can put them together (giving 60 total), and see that\n\nIt would be fun to run this analysis over time. Back when I started my PhD (~2011), industry research was not as prevalent. It was common to see in Graphics (e.g. Adobe / Disney / etc), but not as much in AI / Machine Learning. A lot of that has changed and from purely subjective observation, the industry involvement has increased dramatically. However, Academia is still doing really well and contributes a large fraction (~75%) of the papers.\n\nEDIT 1: fixed an error where previously the Alphabet stat above read 10% because I incorrectly added the numbers of DM and Google, instead of properly collapsing them to a single Alphabet entity.\n\nEDIT 2: some more discussion and numbers on r/ML thread too."
    },
    {
        "url": "https://medium.com/@karpathy/a-peek-at-trends-in-machine-learning-ab8a1085a106?source=user_profile---------4----------------",
        "title": "A Peek at Trends in Machine Learning \u2013 Andrej Karpathy \u2013",
        "text": "Have you looked at Google Trends? It\u2019s pretty cool \u2014 you enter some keywords and see how Google Searches of that term vary through time. I thought \u2014 hey, I happen to have this arxiv-sanity database of 28,303 (arxiv) Machine Learning papers over the last 5 years, so why not do something similar and take a look at how Machine Learning research has evolved over the last 5 years? The results are fairly fun, so I thought I\u2019d post.\n\nLet\u2019s first look at the total number of submitted papers across the arxiv-sanity categories (cs.AI,cs.LG,cs.CV,cs.CL,cs.NE,stat.ML), over time. We get the following:\n\nYes, March of 2017 saw almost 2,000 submissions in these areas. The peaks are likely due to conference deadlines (e.g. NIPS/ICML). Note that this is not directly a statement about the size of the area itself, since not everyone submits their paper to arxiv, and the fraction of people who do likely changes over time. But the point remains \u2014 that\u2019s a lot of papers to be aware of, skim, or (gasp) read.\n\nThis total number of papers will serve as the denominator. We can now look at what fraction of papers contain certain keywords of interest.\n\nTo warm up let\u2019s look at the Deep Learning frameworks that are in use. To compute this, we record the fraction of papers that mention the framework somewhere in the full text (anywhere \u2014 including bibliography etc). For papers uploaded on March 2017, we get the following:\n\nThat is, 10% of all papers submitted in March 2017 mention TensorFlow. Of course, not every paper declares the framework used, but if we assume that papers declare the framework with some fixed random probability independent of the framework, then it looks like about 40% of the community is currently using TensorFlow (or a bit more, if you count Keras with the TF backend). And here is the plot of how some of the more popular frameworks evolved over time:\n\nWe can see that Theano has been around for a while but its growth has somewhat stalled. Caffe shot up quickly in 2014, but was overtaken by the TensorFlow singularity in the last few months. Torch (and the very recent PyTorch) are also climbing up, slow and steady. It will be fun to watch this develop in the next few months \u2014 my own guess is that Caffe/Theano will go on a slow decline and TF growth will become a bit slower due to PyTorch.\n\nFor fun, how about if we look at common ConvNet models? Here, we can clearly see a huge spike up for ResNets, to the point that they occur in 9% of all papers last March:\n\nAlso, who was talking about \u201cinception\u201d before the InceptionNet? Curious.\n\nIn terms of optimization algorithms, it looks like Adam is on a roll, found in about 23% of papers! The actual fraction of use is hard to estimate; it\u2019s likely higher than 23% because some papers don\u2019t declare the optimization algorithm, and a good chunk of papers might not even be optimizing any neural network at all. It\u2019s then likely lower by about 5%, which is the \u201cbackground activity\u201d of \u201cAdam\u201d, likely a collision with author names, as the Adam optimization algorithm was only released on Dec 2014.\n\nI was also curious to plot the mentions of some of the most senior PIs in Deep Learning (this gives something similar to citation count, but 1) it is more robust across population of papers with a \u201c0/1\u201d count, and 2) it is normalized by the total size of the pie):\n\nA few things to note: \u201cbengio\u201d is mentioned in 35% of all submissions, but there are two Bengios: Samy and Yoshua, who add up on this plot. In particular, Geoff Hinton is mentioned in more than 30% of all new papers! That seems like a lot.\n\nFinally, instead of manually going by categories of keywords, let\u2019s actively look at the keywords that are \u201chot\u201d (or not).\n\nThere are many ways to define this, but for this experiment I look at each unigram or bigram in all the papers and record the ratio of its max use last year compared to its max use up to last year. The papers that excel at this metric are those that one year ago were niche, but this year appear with a much higher relative frequency. The top list (slightly edited out some duplicates) comes out as follows:\n\nFor example, ResNet\u2019s ratio of 8.17 is because until 1 year ago it appeared in up to only 1.044% of all submissions (in Mar 2016), but last last month (Mar 2017) it appeared in 8.53% of submissions, so 8.53 / 1.044 ~= 8.17. So there you have it \u2014 the core innovations that became all the rage over the last year are 1) ResNets, 2) GANs, 3) Adam, 4) BatchNorm. Use more of these to fit in with your friends. In terms of research interests, we see 1) style transfer, 2) deep RL, 3) Neural Machine Translation (\u201cnmt\u201d), and perhaps 4) image generation. And architecturally, it is hot to use 1) Fully Convolutional Nets (FCN), 2) LSTMs/GRUs, 3) Siamese nets, and 4) Encoder decoder nets.\n\nHow about the reverse? What has seen many fewer submissions over the last year than has historically had a higher \u201cmind share\u201d? Here are a few:\n\nI\u2019m not sure what \u201cfractal\u201d is referring to, but more generally it looks like bayesian nonparametrics are under attack.\n\nNow is the time to submit paper on Fully Convolutional Encoder Decoder BatchNorm ResNet GAN applied to Style Transfer, optimized with Adam. Hey, that doesn\u2019t even sound too far-fetched."
    },
    {
        "url": "https://medium.com/@karpathy/iclr-2017-vs-arxiv-sanity-d1488ac5c131?source=user_profile---------5----------------",
        "title": "ICLR 2017 vs arxiv-sanity \u2013 Andrej Karpathy \u2013",
        "text": "I thought it would be fun to cross-reference the ICLR 2017 (a popular Deep Learning conference) decisions (which fall into 4 categories: oral, poster, workshop, reject) with the number of times each paper was added to someone\u2019s library on arxiv-sanity. ICLR 2017 decision making involves a number of area chairs and reviewers that decide the fate of each paper over a period of few months, while arxiv-sanity involves one person working 2 hours once a month (me), and a number of people who use it to tame the flood of papers out there. It is a battle between top down and bottom up. Lets see what happens.\n\nHere are the decisions for ICLR 2017. A total of 491 papers were submitted, of which 15 (3%)will be an oral, 183 (37.3%) a poster, 48 (9.8%)were suggested for workshop and 245 (49.9%) were rejected. The accepted papers will be presented at ICLR on April 24\u201327 in Toulon, which I am really looking forward to. Look how amazing it looks:\n\nOn the other hand we have arxiv-sanity, which has a library feature. In short, any registered user can add a paper to their library, and arxiv-sanity will train a personalized SVM on bigram tfidf features of the full text of all papers to make content-based recommendations to the user. For example, I have a number of RL/generative models/CV papers in my library and whenever there is a new paper on these topics it will come up on top in my \u201crecommended\u201d tab. The review pool of arxiv-sanity is as of now a total of 3195 users \u2014 this is the number of people with an account that have at least one paper in the library. Together, these users have so far included 55,671 papers into their libraries, i.e. an average of 17.4 papers.\n\nAn important feature of arxiv-sanity is that users don\u2019t just upvote papers with no repercussions. Adding a paper to your library has some weight, because that paper will influence your recommendations. You have an incentive to only include things that really matter to you in there. It\u2019s clever right? No? Okay fine.\n\nLong story short, I loop over all papers in ICLR and try to find them on arxiv using an exact match on the title. Some ICLR papers are not on arxiv, and some won\u2019t get matched because the authors renamed them, or they contain weird characters, etc.\n\nFor example, lets look at the papers that got an oral at ICLR 2017. We get:\n\nHere we see that we matched 10 out of 15 oral papers on arxiv, and the number next to each one is the number of people who have added that paper to their library. E.g. \u201cReinforcement Learning with Unsupervised Auxiliary Tasks\u201d was in a library of 64 arxiv-sanity users. I also had to truncate some paper names because medium.com is improperly conceived and doesn\u2019t let you change the font size.\n\nNow lets look at the posters:\n\nSome got a lot of love (149!), and some very little (0). For workshop suggestions we get:\n\nand I won\u2019t list all 200-something papers that were rejected, but lets look at the few that arxiv-sanity users really liked, but the ICLR ACs and reviewers did not:\n\nHere is the full version, which was not truncated to fit here. There are a few papers on the top of this list that were possibly unfairly rejected.\n\nHere\u2019s another question \u2014 what would ICLR 2017 look like if it were simply voted on by the crowd of arxiv-sanity users (of the papers we can find on arxiv)? Here is an excerpt:\n\nAgain, the full listing can be found here. Note that in particular, some ICLR2017 papers that were rejected would have been almost an oral based on arxiv-sanity users alone, especially the Predictron, RL\u00b2, \u201cUnderstanding intermediate layers\u201d, and \u201cHierarchical Memory Networks\u201d. Conversely, some accepted papers had very little love from arxiv-sanity users. Here is a full confusion matrix:\n\nAnd here is the confusion matrix in text, for each cell, together with the paper titles. This doesn\u2019t look too bad. The two groups don\u2019t agree on the orals at all, agree on the posters quite a bit, and most importantly there are very few confusions between oral/poster and rejection. Also, congratulations to Max et al. for \u201cReinforcement Learning with Unsupervised Auxiliary Tasks\u201d, which is the only paper that both groups agree should be an oral :)\n\nFinally, I read the following Medium post a few days ago: \u201cTen Deserving Deep Learning Papers that were Rejected at ICLR 2017\u201d, by Carlos E. Perez. It seems that arxiv-sanity users agree with this post, and all papers listed there (including LipNet)(that we could also find on arxiv) would have been accepted by arxiv-sanity users.\n\nAn asterisk. There are several factors that skew these results. For example, the size of arxiv-sanity user base grows over time, so these results likely slightly favor papers that were published on arxiv later than earlier, as these would have come to more user\u2019s attention as new papers on the site. Also, papers are not seen with equal frequencies \u2014 for instance if some paper gets tweeted out by someone popular, more people will see it, and more people might add it to their library. And finally, a good argument could be made that on arxiv-sanity \u201crich get richer\u201d, because arxiv papers are not anonymous and celebrities could get more attention. In this particular case, ICLR 2017 is single-blind so this is not a differentiating factor.\n\nOverall, my own conclusion from this experiment is that there is quite a bit of signal here. And we\u2019re getting it \u201cfor free\u201d from a bottom up process on the internet, instead of something that takes a few hundred people several months. And as someone who has had a good amount of long, painful, stressful, rebuttals back and forth on both submitting/reviewing sides that dragged on for multiple weeks/months, I say: Maybe we don\u2019t need it. Or at the very least maybe there is a lot of room for improvement.\n\nEDIT1: someone suggested the fun idea that we add up the number of citations of these papers in ICLR 2018 submitted/accepted papers, and see which ranking \u201cwins\u201d on that metric. Looking forward to that :)"
    },
    {
        "url": "https://medium.com/@karpathy/virtual-reality-still-not-quite-there-again-5f51f2b43867?source=user_profile---------6----------------",
        "title": "Virtual Reality: still not quite there, again. \u2013 Andrej Karpathy \u2013",
        "text": "The first time I tried out Virtual Reality was a while ago \u2014 somewhere in the late 1990's. I was quite young so my memory is a bit hazy, but I remember a research-lab-like room full of hardware, wires, and in the middle a large chair with a big helmet that came down over your head. I was put through some standard 3 minute demo where you look around, things move around you, they scare you by dropping you down, basics. The display was low-resolution, had strong ghosting artifacts, there was long response lag, and the whole thing was quite a terrible experience. I remember finally putting up the helmet and feeling simultaneously highly nauseated and\u2026 extremely excited. This was the future! I imagined the imminent miniaturization, rapid improvements in visual fidelity, VR arcades, holodecks!\n\nAnd then\u2026 nothing happened. I never saw that headset again. I rarely ever saw VR mentioned in tech headlines. No VR arcades sprung up across the street that I could visit with my friends. For the young me, VR became an embarrassing misprediction, right next to \u201cobviously we\u2019ll have flying cars/jetpacks soon\u201d. Eventually, dreams of fantastical digital universes slipped from my mind entirely. Over time I realized that some technical advances are too easy to imagine, but too hard to execute, and VR falls into this category. I just had to patiently wait for its time to come.\n\nFast forward to 2012, you can imagine my excitement when I saw the Oculus Kickstarter campaign. It was a dream come true: someone was actually building a serious consumer-grade VR headset, and Gabe Newell was personally endorsing the campaign! Starstruck, I impulsively reached for the \u201cBack this project\u201d button\u2026 but then realized I forgot my Kickstarter password. That, and I was afraid this was vaporware. They promised a consumer version eventually and I had decided I can just wait for that.\n\nI don\u2019t want this to be too long of a story. TLDR: I start obsessively checking all updates on Oculus. They get acquired by Facebook. Vive gets introduced. I end up buying both Vive and Oculus consumer version, then cancel the latter based on Reddit discussions. So somewhere mid 2016, a Vive gets delivered to my doorstep somewhere around noon and I skip work, intending to play with it the whole day. I was living in a small dorm room at the university so I moved almost everything from my room to the (shared) living room to get enough space to just barely satisfy the minimum size requirement for a room-scale setup.\n\nI think I played with the Vive for about 2 hours that day and you know what? It was\u2026 pretty cool. I powered down the computer and went back to work for the remainder of the day.\n\nThis is the phrase I would come to hear over and over again, as I demoed my Vive to my friends. I tried to AB test many aspects of my presentation: the games that I launched, their order, how I described VR or its possibilities, but nothing changed this reaction too much. My friends would put it on, try out some of the games and then, quite content, hand it back to me. They\u2019d insist it was \u201ccool\u201d, some were even \u201cblown away\u201d, but it was clear they also weren\u2019t too eager to get back. I later discovered that out of ~few hundred friends (most of them science/tech) I only had a single friend who actually bought a VR system like I did (and I almost bought TWO!, nearly squandering all the savings I could afford with my sorry PhD \u201csalary\u201d). It seemed that none of my friends were too excited (beyond the pretty cool first experience) and, somehow, neither was I.\n\nToday, my Vive is organized in a messy pile of wires in the corner my room. I turn it on from time to time to try out the latest and greatest, but for the most part it collects dust. I did get to explore quite a bit though, so I feel qualified to have some opinions on what things VR do/do not work today.\n\nOverpriced tech demos. The first issue I noticed immediately is that VR games are expensive (e.g. up to $59.99), but as a friend of mine described it, many of them are \u201cnot too deep\u201d. They are the $0.99 games for your phone, except on your face, and for $29.99. I think I ended up spending several hundred dollars buying games for a total of maybe 10 hours of game time. There are many other games that are obviously lazily ported over from PC, in many cases resulting in terrible user experiences. Some of the games were so overpriced and under-cooked (with game-breaking bugs) that I ended up spending the time to file Steam complaints to get money back. Luckily, Steam is quite fast with this and promptly reimbursed the games. A VR consumer has to be careful out there.\n\nVR design anti-patterns. It\u2019s also surprising how many developers try to ignore the new form factor and its constraints. For instance, you cannot translate or rotate (or worse, accelerate) the camera because it gives people nausea. You\u2019d think this simple fact would be obvious common knowledge, but more than 50% of VR games still think that accelerating you around is okay. For example, the PlayStation VR Shark Encounter features a shark aggressively shaking your cage, except of course you don\u2019t feel it. It\u2019s wrong.\n\nA single bug away from nausea. I\u2019ve also experienced game bugs that do weird things to your field of view. Games can switch rapidly from a 3D view to a 2D view \u201cglued\u201d to your eyes, or the screen can flicker, or everything will briefly reverse along some axis, or the inputs to your eyes get switched, or the camera will rapidly spiral out, or something weird. These are extremely negative experiences that usually result in tearing your headset from your head and having to sit down for a while, or completely give up for the evening. VR headsets can also lose tracking in weird ways, which can cause the camera to either drift in some random direction or cause a jitter. In short, the cost of camera errors is extremely high.\n\nI found that it\u2019s not too difficult to create an experience that a person would describe as \u201cpretty cool\u201d. Even the number of people who have their \u201cmind blown\u201d is by itself irrelevant to the success of the platform. What is difficult is making one that a person wants to come back to. Only a few games have achieved this for me so far. They fall into three categories:\n\n1. Full body experiences. These are games like AudioShield and Holopoint. There is usually some background music and you have to move your body to achieve game goals. I find these games fun and repeatable whenever I\u2019m in a dancing/moving/feeling really cool mood. Similarly, I love games that get you to manipulate things with your controllers (e.g. Job Simulator). If you\u2019ve only used VR with a gamepad, you are missing out.\n\n2. Creative experiences. These are things like TiltBrush, or any other app that channels your creativity in this new paradigm. TiltBrush today is like the MSPaint of the past, with just the most basic tools and features, but I believe there is a lot of potential for applications like this.\n\n3. Social experiences. These are things like AltspaceVR, Rec Room, or Keep Talking and Nobody Explodes. The genius part about these experiences is that the developers don\u2019t have to do the hard work of importing too much complexity into the game. All they have to do is involve people, and their social interactions deliver the complexity and repeatability. I\u2019ve spent quite a bit of fun time in AltspaceVR: watching projected videos in the simulated living room, dancing with people, etc. I met a real-life friend in Altspace and we went for a walk and threw objects at each other, it was great.\n\nIn my opinion the experiences that will eventually make VR most compelling will have these features, and ideally all of them combined. They will connect people over multiplayer (3), give them ways of creating and sharing (2), and take advantage of full body presence in inventive ways (1).\n\nIf you look at the experiences built for VR today, you\u2019ll notice that most of them are (usually single-player) games. For example I looked at 100 \u201cnew releases\u201d for VR today and 100% of them are games. This might be because games are easier or faster to build.\n\nIt is notoriously difficult to predict future uses of novel technologies. In 1980\u2019s Personal Computer software involved games and personal finance applications. Amusingly, today all of the action is in a single binary application (the browser), but we can look at some of the 20 most popular websites and see that they cluster around some basic human wants:\n\nIn particular, the most valuable companies here have little to do with games, and all of their products are free to use. The PC gaming market is still there and doing fine, estimated to be worth around $36B in 2016, most of it from free-to-play online titles. We can see similar trends reflected in the mobile market. Looking at some of the top used apps, we see:\n\nAgain, we see very little gaming, we see that all of these are free-to-use, and several of these apps play to the unique strengths of the form factor that differentiate it from what already exists, such as Maps (GPS, compass, \u2026), and especially photo sharing (camera).\n\nLong story short, currently the most content made for VR are games, but looking at the trends above it seems quite unlikely to me that games (in any quantity) will be the thing that makes VR go big. It\u2019s also not clear to me that many people \u201cget\u201d this, or agree with it, with the possible exception of Facebook, judging by Zuck\u2019s latest VR demo.\n\nSo what does the future of VR look like? In terms of the well-known and often-referenced hype curve, one might argue that VR is finally in the stage of slowly climbing, after its peak of expectations around the 90s. However, I think a more nuanced view is that some technologies (especially those that are 1) easy to predict and 2) potentially very impactful) can in fact undergo multiple cycles whenever something exciting happens in the space. No one wants to miss the possibly few hundred $B wave that is coming at some point. I think it is likely that we are in such a situation now (right):\n\nArtificial Intelligence (something that I know much more about) also falls into the category of \u201ceasy to predict and potentially very impactful\u201d, and has similarly gone through several periods excitement followed by \u201cAI winters\u201d.\n\nWhy not yet? Despite the excitement that goes back to my childhood, I\u2019ve come to be more pessimistic than optimistic about VR in the short term. There are still too many problems. VR is not like a TV that you can leave running in the background while you chat with a friend or cook a dinner. It\u2019s not like a mobile phone that I can keep around and casually glance at to get some instant gratification. Today, VR is an activity (you have to take a long sequence of non-default actions to plug in), it disconnects you from your immediate surroundings, any interruptions are costly (e.g. I get a phone call, or I need to eat or use the restroom), and also makes you look quite silly. You also won\u2019t avoid some stigma associated with escapism / being a nerd, something that you cannot fix in a few years.\n\nWhen it does happen. In the medium term (e.g. a decade or two), I could see us make a lot of progress on the hardware and mitigate many of the above problems. For instance, we could shrink VR into face/hand-tracking, high-visual-fidelity, very-low-tracking-error-rate Ray-Ban sunglasses that make you look cool, and you can just slip on in a second to \u201cplug in\u201d. If this does happen, I feel confident making a few predictions about what the killer app for VR would look like. It:\n\nOr hey, even more amusingly, a killer app could be something B2B, like enabling remote robotic work, where the worker\u2019s commands get recorded and become training data for autonomous robotic systems. This is the core premise of my short story on AI, which I can now plug here. woohoo!\n\nThe long term. And finally in the long term, how likely is it that we\u2019ll have a compelling parallel digital universe (e.g. Ready Player One style) that a good fraction of humanity will plug into for a good portion of their life? On this time scale I\u2019m relatively optimistic. After all, we\u2019ll need some artificial difficulty in form of social fun and games when the AIs are doing all the work. Just kidding. I think. A great way to end the post."
    },
    {
        "url": "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=user_profile---------7----------------",
        "title": "Yes you should understand backprop \u2013 Andrej Karpathy \u2013",
        "text": "When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:\n\nThis is seemingly a perfectly sensible appeal - if you\u2019re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of \u201cit\u2019s worth knowing what\u2019s under the hood as an intellectual curiosity\u201d, or perhaps \u201cyou might want to improve on the core algorithm later\u201d, but there is a much stronger and practical argument, which I wanted to devote a whole post to:\n\n> The problem with Backpropagation is that it is a leaky abstraction.\n\nIn other words, it is easy to fall into the trap of abstracting away the learning process \u2014 believing that you can simply stack arbitrary layers together and backprop will \u201cmagically make them work\u201d on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.\n\nWe\u2019re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can \u201csaturate\u201d and entirely stop learning \u2014 your training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):\n\nIf your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (\u201cvanish\u201d), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.\n\nAnother non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you\u2019re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.\n\nTLDR: if you\u2019re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn\u2019t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.\n\nAnother fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:\n\nIf you stare at this for a while you\u2019ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn\u2019t \u201cfire\u201d), then its weights will get zero gradient. This can lead to what is called the \u201cdead ReLU\u201d problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron\u2019s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It\u2019s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.\n\nTLDR: If you understand backpropagation and your network has ReLUs, you\u2019re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.\n\nVanilla RNNs feature another good example of unintuitive effects of backpropagation. I\u2019ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):\n\nThis RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you\u2019ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.\n\nWhat happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b\u2026)? This sequence either goes to zero if |b| < 1, or explodes to infinity when |b|>1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.\n\nTLDR: If you understand backpropagation and you\u2019re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.\n\nLets look at one more \u2014 the one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector \u2014 turns out this trivial operation is not supported in TF). Anyway, I searched \u201cdqn tensorflow\u201d, clicked the first link, and found the core code. Here is an excerpt:\n\nIf you\u2019re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \\gamma \\argmax_a Q(s\u2019,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.\n\nThe problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.\n\nThe clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:\n\nIt\u2019s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can\u2019t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.\n\nI submitted an issue on the DQN repo and this was promptly fixed.\n\nBackpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because \u201cTensorFlow automagically makes my networks learn\u201d, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.\n\nThe good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.\n\nThat\u2019s it for now! I hope you\u2019ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I\u2019m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that :)"
    },
    {
        "url": "https://medium.com/@karpathy/cs183c-assignment-3-649181b07fa0?source=user_profile---------8----------------",
        "title": "CS183c Assignment #3 \u2013 Andrej Karpathy \u2013",
        "text": "The last few weeks we heard from several excellent guests, including Selina Tobaccowala from Survey Monkey, Patrick Collison from Stripe, Nirav Tolia from Nextdoor, Shishir Mehrotra from Google, and Elizabeth Holmes from Theranos. The topic of discussion was scaling beyond the tribe phase to the Village/City phases of a company.\n\nMy favorite among these was the session with Patrick (video), which I found to be rich with interesting points and mental models. In what follows I will try to do a brain dump of some of these ideas in my own words.\n\nI found Patrick\u2019s slight resentment of new organizational structure ideas refreshing and amusing. We hear a lot about disruption, thinking from first principles, being contrarian, etc. Patrick was trying to make the point that the mean organizational structure of a company that we\u2019ve converged on (e.g. hierarchies etc.), and the way things are done is actually quite alright. That in fact we know a lot about how to organize groups of people towards a common goal, and that one must resist the temptation to be too clever with it. He also conceded that a lot of things have changed over the last several years (especially with respect to technology), but made the point that human psychology has in comparison stayed nearly constant, which means we can more confidently draw on past knowledge in these areas.\n\nThis part was quite amusing because I felt like Patrick was being contrarian by defending the standard.\n\nSeveral interesting points \u2014 first, it has become meaningless to regurgitate the fact that hiring is very important. What that actually means when you want to translate it to a statement with actual bits of information, Patrick argues, is to rephrase it as \u201cyou have to be very very persistent\u201d in recruiting the best people, and willing to to do it very slowly over time (e.g. they took 6 months to hire 2 employees early on).\n\nAnother interesting analogy was in thinking of good people as airship carriers. Your job during hiring is to find them and steer them towards a direction, as opposed to thinking about it the other way around, where you try to find people aligned with your direction first and then build them up over time. The argument is in favor of the former order of precedence.\n\nThe last point that resonated with me strongly was the realization that it is incorrect to think about hiring on an individual basis. When you\u2019re hiring a person you\u2019re in fact hiring an entire cone of people in expectation, because good people in an area attract more good people like them, dramatically reducing the barrier for another similar hire.\n\nI also really liked the idea that as the company grows more, the communication channels should tend toward writing more than talking. The comparison to the printing press was interesting, and the notion that what made writing so powerful was not only the dissemination, but also the concreteness and rigidity of written text. The fact that writing encourages the full serialization a concept, much more than a speech. The idea that you can point to paragraph 2, sentence 5 and say \u201cno, that\u2019s wrong\u201d.\n\nThe entire conversation was peppered with interesting mental models for thinking about different facets of companies: The idea that an optimal 1-year plan can be very distinct from the optimal 5-year plan. The idea of Stripe as a blob in a philosophical space, and that different people within the organization are in different angles/distances with respect to its center of mass, and have to be actively pulled towards it. The 3 jobs of a CEO. etcetc.\n\nThere were also several other interesting insights from the other guests, which I will dump here in an unsorted form.\n\nThe SurveyMonkey back story was fun to hear about \u2014 a great example of an organically rapidly growing company with huge profit margins. Also, an example of a company that may seem not too exciting until you think about it more. Selina also made the interesting observation that some people have preferences towards particular stages of the company, and that some might not scale, or not be willing to scale, up.\n\nNirav described a very nice example of doing things that don\u2019t scale with the early days of Nextdoor. They attracted only 170 neighborhoods in their first year, by hand. Nirav also mentioned an interesting concept of a \u201ctreadmill company\u201d - a metaphor for a company where you can\u2019t easily step back and enjoy the view \u2014 it requires constant struggle and active involvement. Finally, in terms of extracting value from customers without explicitly charging them Nirav mentioned two modes of operation: the demand fulfillment model (e.g. Google) and demand generation model (e.g. Facebook).\n\nShishir brought up an interesting idea called the \u201ctombstone test\u201d, as a way of determining how to spend your time. In short, if you can\u2019t imagine something being on your tombstone then it is not worth working on. Hm!\n\nThat\u2019s it! The insights I like the most are the ones that point out mental models that distill a situation to something that preserves most of its core dynamics, but is easier to think about. This distillation process cleans an idea, strips it from the irrelevant and preserves the core nugget of insight. The last few weeks were quite rich!"
    }
]