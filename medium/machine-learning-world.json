[
    {
        "url": "https://medium.com/machine-learning-world/useful-snippets-for-google-colaboratory-free-gpu-included-d976d6b3e6de?source=---------0",
        "title": "Useful snippets for Google Colaboratory. Free GPU included.",
        "text": "For people who are new or still didn\u2019t use it \u2014 Google Colab is Jupyter service with dedicated server(with GPU) that you get free for about 12 hours(after it you need to reinitialize it). So it\u2019s good place where you can test your Python scripts, or use it while you are learning Machine Learning & Data Science.\n\nIn my work i often use Colab, for two reason. First it gives free computation power and even free Nvidia Tesla K80. Second it\u2019s very convenient way of sharing some ML & DS examples to people (which i use in every article on Medium).\n\nHere i gathered useful snippets that i use every day, that makes my life easier :)\n\nDon\u2019t forget to copy all intensive reading data from Drive to instance to make access faster."
    },
    {
        "url": "https://medium.com/machine-learning-world/how-i-hacked-xiaomi-miband-2-to-control-it-from-linux-a5bd2f36d3ad?source=---------1",
        "title": "How I hacked my Xiaomi MiBand 2 fitness tracker \u2014 a step-by-step Linux guide",
        "text": "This is a step by step guide into how I hacked my Bluetooth Low Energy (BLE) fitness tracker so I could control it from Linux.\n\nThis story started from a Facebook post I made talking about a problem of the absence of an API for fitness trackers, and why that\u2019s not helping data scientists make cool stuff for them.\n\nThe post generated quite a lot of discussion and after caught the attention of my friend Volodymyr Shymanskyy who tried to help me and found some code on Github from Leo Soares for my fitness tracker MiBand 2. He tried to run it, but there were some problems with the connection. He fixed the problem in a few hours, commited the code and sent me a link.\n\nThat got me off to a head start. The code I had allowed me to connect to the MiBand 2 device, run notifications, and get a heart measurement. But that was not enough for what I wanted to do. I needed to get real-time raw data from the sensors to use it in my data science experiments. I wanted to make a gym exercise predictor.\n\nSo I decided to hack the hell out of my fitness tracker.\n\nI didn\u2019t have any experience on working with BLE devices, so first I tried to understand how the technology worked and how the different pieces fitted together. As it turned out, it was pretty simple and here is a summary.\n\nThis is basically all you need to know to start working with the fitness tracker.\n\nYou\u2019ll also need two apps that will help you debug the BLE device - Wireshark protocol analyzer and BLE debugger. And you will need to get access to your Android phone developer options (Sorry iOS folks but you\u2019ll have to find the equivalent options for the iOS platform).\n\nTo start, you will need to unpair your MiBand2 device from the phone app.\n\nSo let\u2019s see now what services and characteristics our band has. Open BLE debugger and run a scan. You\u2019ll see something like this.\n\nSave your device MAC address somewhere, we will need it later. Now let\u2019s connect to it and see what services and characteristics does it have."
    },
    {
        "url": "https://medium.com/machine-learning-world/shape-context-descriptor-and-fast-characters-recognition-c031eac726f9?source=---------2",
        "title": "Shape Context descriptor and fast characters recognition",
        "text": "Today i will try to give you an idea how it works and why it so cool :)\n\nMatching shapes can be much difficult task then just matching images, for example recognition of hand-written text, or fingerprints. Because most of shapes that we trying to match is heavy augmented. I can bet that you will never write to identical letters for all your life. And look at this from the point of people detection algorithm based on handwriting matching \u2014 it would be just hell.\n\nOf course in the age of Neural networks and RNNs it also can be solved in a different way then just straight mathematics, but not always you can use heavy and memory hungry things like NNs. Even today most of the hardware like phone, cameras, audio processors using good-old mathematics algorithms to make things running. And also it always cool to do some mathematics magic :)\n\nThe main idea is quite simple \u2014 build histogram based on points in Polar coordinates system. So for each point you just get Euclidean distance and angles against other points, norm them and based on logspace map count number of points occurred in each map region. And that\u2019s all.\n\nTo make things rotation invariant i prefer to just rotate this map on minus angle between to most furthers points, but it can be done in other ways too.\n\nSo now lets try to understand how correctly compute it (cause debuging of math algorithms can be not very cool)\n\n1.Set number of radius segments and angle segments (this is standard settings)\n\n2. Calculate distance between points and normalize them by mean. Also get two points with max distance\n\n3. Create logspace and count number of occurrences in a logspace interval\n\n4. Compute angles between points, normalize it with our norm angle and add 2*Pi so we have interval [0, 2*Pi]\n\n5. Quantize our angles to get histogram the same as for distance\n\n6. Get all things together and build shape context descriptor. Just counting number of points in each radius and angle region.\n\nBy paper for counting cost matrix we should use Pearson\u2019s chi-squared test\n\nBut we also can use cosine distance (which is much faster)\n\nTo get point-to-point matching we should solve the linear sum assignment problem which is not fast thing (~O(n\u00b3)). For this we will use scipy\n\nHere is an example of point-to-point matching:\n\nNow we gonna try to use what we have learn to make simple task of character recognition based on vocabulary search."
    },
    {
        "url": "https://medium.com/machine-learning-world/feature-extraction-and-similar-image-search-with-opencv-for-newbies-3c59796bf774?source=---------3",
        "title": "Feature extraction and similar image search with OpenCV for newbies",
        "text": "I think all of you saw Google Image Search and asked yourself \u201cHow it works?\u201d, so today i will give you an answer on this question and we will build simple script for making image search right inside your console.\n\nImage features\n\nFor this task, first of all, we need to understand what is an Image Feature and how we can use it.\n\nImage feature is a simple image pattern, based on which we can describe what we see on the image. For example cat eye will be a feature on a image of a cat. The main role of features in computer vision(and not only) is to transform visual information into the vector space. This give us possibility to perform mathematical operations on them, for example finding similar vector(which lead us to similar image or object on the image)\n\nOk, but how to get this features from the image?\n\nThere are two ways of getting features from image, first is an image descriptors(white box algorithms), second is a neural nets(black box algorithms). Today we will be working with the first one.\n\nThere are many algorithms for feature extraction, most popular of them are SURF, ORB, SIFT, BRIEF. Most of this algorithms based on image gradient.\n\nToday we will use KAZE descriptor, because it shipped in the base OpenCV library, while others are not, just to simplify installation.\n\nMost of feature extraction algorithms in OpenCV have same interface, so if you want to use for example SIFT, then just replace KAZE_create with SIFT_create.\n\nSo extract_features first detect keypoints on image(center points of our local patterns). The number of them can be different depend on image so we add some clause to make our feature vector always same size(this is needed for calculation, cause you can\u2019t compare vectors of different dimensions)\n\nThen we build vector descriptors based on our keypoints, each descriptor has size 64 and we have 32 such, so our feature vector is 2048 dimension.\n\nbatch_extractor just run our feature extractor in a batch for all our images and saves feature vectors in pickled file for further use.\n\nNow it\u2019s time to build our Matcher class that will be matching our search image with images in our database.\n\nHere we are loading our feature vectors from previous step and create from them one big matrix, then we compute cosine distance between feature vector of our search image and feature vectors database, and then just output Top N results. \n\nOf course this is just a demo, for production use better to use some algorithm for fast computation of cosine distance for millions of images. I would recommend to use Annoy Index which is simple in use and pretty fast(search in 1M of images is taking about 2ms)\n\nNow just put it all together and run\n\nYou can download this code from my github\n\nOr run it right away in Google Colab (free service for online computation even with GPU support): https://colab.research.google.com/drive/1BwdSConGugBlGzPLLkXHTz2ahkdzEhQ9\n\nWhen you run this code you will see that similar images are not always similar as we understand it. That\u2019s because this algorithms is context-unaware, so they better in finding same images even modified, but not similar. If we want to find context similar images then we should use Convolutional Neural Network, and the next article will be about them, so don\u2019t forget to follow me :)\n\nIf you like my articles, you can always support me with some beer-money https://paypal.me/creotiv\n\nRead my other fresh articles about Computer Vision"
    },
    {
        "url": "https://medium.com/machine-learning-world/convolutional-neural-networks-for-all-part-ii-b4cb41d424fd?source=---------4",
        "title": "Convolutional Neural Networks For All | Part II \u2013 Machine Learning World \u2013",
        "text": "Object localization \u2014 In order to classify objects, you need to localize them first. This lecture covers three methods of classification: Image classification, Classification with localization and Landmark Detection. Image classification classifies an image, e.g. contains a cat or not. Classification with localization classifies an object with its bounding box in an image. Detection is used to locate multiple objects in an image. The output of the CNN has the following format as shown in the image below. The CNN object detection minimizes the loss function, e.g. the squared error, between y and predicted y.\n\nLandmark detection \u2014 Is used to define landmarks in an image. Can be used to mark eyes in a face. Then train the CNN to output the locations of these landmarks. The identities of the landmarks have to be consistent in the entire training set, meaning the first landmark always has to mark the corner of the left eye, for example. It can also be used to detect the whole shape of a face or the action that an athlete performs in an image.\n\nObject detection \u2014 Detect objects in an image through \u201csliding windows\u201d. First, train a CNN on closely cropped pictures of the object. Next, create a window frame of a smaller size than the image and place it in a corner over the image. Run a CNN to determine if this window frame matches the cropped version of the object. Move the window to the next part of the image and detect the object again. Repeat this procedure for the entire image. Object detection is very computing intense and is prone to miss the object if the sizes of the window and the object don\u2019t match.\n\nConvolutional implementation of sliding window \u2014 A convolutional implementation of the \u201csliding windows\u201d method is much more efficient. Run a CNN over the entire image once. The final convolutional layer shows the object detection for every sliding windows frame in one iteration.\n\nBounding Box Prediction \u2014 The YOLO algorithm helps you detect accurate bounding boxes for object detection. To train a CNN using YOLO, you first have to place a grid over your training image, often a 19x19 grid is used. Next, you create the output labels for every grid. Does this grid contain the center of a relevant object? If so, draw a bounding box around it and label the output vector y accordingly. This way, label as many images as possible. Your CNN architecture has to result in the final layer having the shape of the grid cells for width and height and as many channels as the number of elements in a single y vector. If you want to detect five classes of objects, your output vector y has 10 elements. 1 element to indicate whether an object exists or not, 4 elements to indicate the objects center and its width and height plus 5 elements, indicating the class the object belongs to. Backpropagation now adjusts the weights of the CNN so that it learns to identify the objects. You only need a single forward propagation step to identify the objects in an image.\n\nYOLO algorithm \u2014 The algorithm, short for \u201cYou Only Look Once\u201d, is more accurate compared to \u201csliding windows\u201d. It returns the exact boundaries of the object even if the window size doesn\u2019t exactly match the object.\n\nIntersection over union \u2014 Is used in non-max suppression to support YOLO finding the exact boundaries of the object. The formula divides the size of the object by the size of the union of two windows. If IoU > 0.5, then the intersection contains the object. IoU is a measure of the overlap of two bounding boxes.\n\nNon-max suppression \u2014 It cleans up the result of the YOLO detections in an image. YOLO sometimes identifies the same object in multiple grid cells with slightly different bounding boxes. Use non-max suppression to find the true bounding box of the object with the help of IoU. All boxes with a high IoU will be suppressed. First, discard all boxes with a probability of containing the object < 0.5. Next, output the box with the highest probability and suppress all boxes with an IoU > 0.5.\n\nAnchor boxes \u2014 Allows CNN to detect two overlapping objects, e.g. dog standing in front of a bike. It is used to learn shapes of wide cars and tall pedestrians in images. First, create anchor box shapes. If center points of two objects overlap, associate them with two different anchor boxes. The object is now assigned to the grid cell with contains the object center point and to the anchor box which has the highest IoU with grid cell. The output y is stacked together with top part belonging to anchor box 1 and second part to anchor box 2. If only an object from anchor box 2 is in this grid cell, y will only fill in values for lower part of output y and \u201cdon\u2019t care\u201d values for the upper part, corresponding to anchor 1.\n\nPutting it together \u2014 YOLO algorithm: 2 anchor boxes are used to detect multiple objects in an image. The output layer will be of the following shape: 3x3 (grid cell shape) x2 (# of anchor boxes) x8 (output vector containing probability of image detection, 4 numbers to describe location in object, and 3 numbers to describe the class that is detected). First, the CNN generates two anchor boxes for every grid. Second, the anchor boxes with a low probability are discarded. Third, non-max suppression is used to detect the final bounding boxes of the objects.\n\n(Optional) Region Proposals \u2014 R-CNN classifiers detect interesting regions in an image and classify those rather than through using grids. A segmentation algorithm creates about 2k blobs and tries to detect interesting regions and objects out of these regions. For more information on the quite popular variations of R-CNNs, check out this great post or Facebook\u2019s latest Detectron library."
    },
    {
        "url": "https://medium.com/machine-learning-world/netflix-or-coursera-how-to-finish-andrew-ngs-1st-deep-learning-course-in-7-days-6fa293ee83d8?source=---------5",
        "title": "Netflix or Coursera? How to finish Andrew Ng\u2019s 1st Deep Learning Course in 7 days",
        "text": "The project plan shows how much time on which day you should allocate to a given task. The project plan starts on Wednesday but feel free to start at any given day of the week. Nevertheless I would recommend starting Monday \u2014 Wednesday, so that you have the weekend to work on the week three and four assignments.\n\nStart off by watching all the lectures for week one and finish the test right away. It should not take you longer than one hour and will get you excited for the upcoming exercises.\n\nThe lectures in week two will take longer than in other weeks. If you\u2019re familiar with numpy and python broadcasting, you can safely skip the second part of the video lectures and will still be able to complete the programming exercise. Try to spend half an hour on the optional numpy programming and 1.5h on the logistic regression exercise. If you read the exercises carefully, you should be able to finish the tasks in two hours.\n\nIn week three things will start to get interesting. If you follow the project plan, it should be Saturday now. Finish the lectures plus the quiz in one hour and the programming exercise afterwards in two hours. This will take up half of your day, but afterwards you still have half a day left.\n\nWeek four will prove to be the most challenging part. If possible, try to finish the lectures and coding exercises on Sunday. But to keep you mentally sane, it is also no problem if you just watch the video lectures on Sunday and you take Monday and Tuesday to finish the coding exercises.\n\nEt voil\u00e1 \u2014 you pulled through and finished an online course in one week what should\u2019ve taken you a month!\n\nNow is really the time to be proud of yourself, pat yourself on the shoulder and check if the living dead are still walking and dead.\n\nThanks for reading and if you have any questions about the project plan or the course, feel free to reach out to me."
    },
    {
        "url": "https://medium.com/machine-learning-world/how-to-replicate-lets-enhance-service-without-even-coding-3b6b31b0fa2e?source=---------6",
        "title": "How to replicate \u201cLet\u2019s Enhance\u201d service without even coding",
        "text": "How to replicate \u201cLet\u2019s Enhance\u201d service without even coding\n\nYou may see small difference in colors between outputs, it\u2019s because additional filters(just some standard WB autobalance/jpeg noise reduction filters that you may find in Photoshop or other photo editors) that Let\u2019s Enhance project use, and i not.\n\nSo how to implement this?\n\n6. Go to the ./result/images and see result\n\nPS: I\u2019ve post this cause i don\u2019t like when people give out other people\u2019s inventions for their own."
    },
    {
        "url": "https://medium.com/machine-learning-world/artificial-intelligence-terminologies-260f1d6d299f?source=---------7",
        "title": "Artificial Intelligence terminologies \u2013 Machine Learning World \u2013",
        "text": "We do not need to know everything in a conversation, but we should at least know the terms used in the conversation.\n\nIf we are talking about physics we need to know for example that when we are talking about velocity we mean the speed that an object takes to travel a space in a certain period of time. I think in Artificial Intelligence shouldn\u2019t be different, so, in this post, I\u2019ll let you know the meaning of the most used terminologies (and their acronym) so that the next time you meet with an AI post, you can read it with a deep understanding.\n\nAI (Artificial Intelligence) \u2014 The first thing we need to do is understand what an AI actually is. The term \u201cartificial intelligence\u201d refers to a specific field of computer science that focuses on creating systems capable of gathering data and making decisions and/or solving problems.\n\nAGI (Artificial General Intelligence) \u2014 is an emerging field aiming at the building of \u201cthinking machines\u201d; that is, general-purpose systems with intelligence comparable to that of the human mind, also called \u201cStrong AI\u201d, \u201cHuman-level AI\u201d, etc.\n\nANI (Artificial Narrow Intelligence) \u2014 A one trick pony, they can play chess, recognize faces, translate foreign languages.\n\nASI (Artificial Super Intelligence) \u2014 Smarter than the best human brains and has the ability to apply that to absolutely anything (This is the AI that people like Stephen Hawking, Elon Musk, etc. are scared of).\n\nAgent \u2014 also called assistants, brokers, bots, intelligent agents is an autonomous entity which observes through sensors and acts upon an environment using actuators.\n\nChatbot \u2014 A computer program that conducts conversations with human users by simulating how humans would behave as a conversational partner.\n\nData \u2014 Any collection of information converted into a digital form.\n\nData Mining \u2014 The process by which patterns are discovered within large sets of data with the goal of extracting useful information from it.\n\nDeep Learning \u2014 A subset of AI and Machine learning in which Neural networks are \u201clayered\u201d, combined with plenty of computing power, and given a large measure of training data to create extremely powerful learning models capable of processing data in new and exciting ways in a number of areas, e.g. advancing the field of computer vision.\n\nGenetic Algorithm \u2014 A method for solving optimization problems by mimicking the process of natural selection and biological evolution. The algorithm randomly selects pairs of individuals from the population (whereby the best performing individuals are more likely to be chosen) to be used as parents.\n\nHeuristics \u2014 It is the knowledge based on Trial-and-error, evaluations, and experimentation.\n\nML (Machine Learning) \u2014 A subsetof AI in which computer programs and algorithms can be designed to \u201clearn\u201d how to complete a specified task, with increasing efficiency and effectiveness as it develops. Such programs can use past performance data to predict and improve future performance.\n\nNLG (Natural Language Generation) \u2014 A machine learning task in which an algorithm attempts to generate language that is comprehensible and human-sounding. The end goal is to produce computer-generated language that is indiscernible from language generated by humans\n\nNLP (Natural Language Processing) \u2014 The ability of computers to understand, or process natural human languages and derive meaning from them. NLP typically involves machine interpretation of text or speech recognition\n\nRNN (Recurrent Neural Network) \u2014 A type of artificial neural network in which recorded data and outcomes are fed back through the network forming a cycle.\n\nOCR (Optical Character Recognition)\u2014 A computer system that takes images of typed, handwritten or printed text and converts them into machine-readable text.\n\nRNN (Recurrent Neural Network) \u2014 A type of artificial neural network in which recorded data and outcomes are fed back through the network forming a cycle.\n\nReinforcement Learning \u2014 A type of machine learning in which machines are \u201ctaught\u201d to achieve their target function through a process of experimentation and reward. In reinforcement learning, the machine receives positive reinforcement when its processes produce the desired result, and negative reinforcement when they do not.\n\nRule \u2014 It is a format of representing knowledge base in Expert System. It is in the form of IF-THEN-ELSE\n\nSupervised learning: A type of machine learning in which human input and supervision are an integral part of the machine learning process on an ongoing basis, like a teacher supervising a student; more common than unsupervised learning.\n\nStrong AI \u2014 An area of AI development that is working toward the goal of making AI systems that are as useful and skilled as the human mind.\n\nTuring Test \u2014 A test developed by Alan Turing 1950, which is meant as a means to identify true artificial intelligence. The test is based on a process in which a series of judges attempt to discern interactions with a control (human) from interactions with the machine (computer) being tested.\n\nUnsupervised learning: A type of machine learning algorithm used to draw inferences from datasets consisting of input data without labeled responses.\n\nIf you enjoyed the writings leave your comment and claps \ud83d\udc4f to recommend this article so that others can see it.\n\nThanks Carla Francisco \u2764 for recommending this very interesting topic, I enjoyed writing it."
    },
    {
        "url": "https://medium.com/machine-learning-world/are-we-supposed-to-fear-ai-4792b0dbe933?source=---------8",
        "title": "Are we supposed to fear AI? \u2013 Machine Learning World \u2013",
        "text": "In science fiction often portrays AI as robots with human-like characteristics, AI can encompass anything from simple applications (SIRI) to self-driving cars to autonomous weapons\u2026.\n\nThis year (2017), I started learning AI at the college, where we talked about the four approaches of AI:\n\nWhen we debated about the first approach, a big question came to head, then I asked the teacher:\n\n\u2014 Teacher, aren\u2019t we building something that as a potential to exterminate us????\n\nWell, during some reading, I found out that I\u2019m not the only person who is aware of AI potential, recently Stephen Hawking, Elon Musk, Steve Wozniak, Bill Gates, Mark Zuckerberg, and many other big names in science and technology have recently expressed concern in the media and via open letters about the risks posed by AI and cording to it, I dived in to further reading to see if is it worth to continue studying and developing AI agents, and in the reading, I found this:\n\nSome AI systems are programmed to do some devastating things:\n\n\u2014 Autonomous weapons are artificial intelligence systems that are programmed to kill. In the hands of the wrong person, these weapons could easily cause mass casualties, and if can take it\u2019s own decision, I dont know what can happen.\n\n\u2014 Some AI systems are programmed to do something beneficial, but it can develops a destructive method for achieving its goal: This can happen whenever we fail to fully align the AI\u2019s goals with ours, which is strikingly difficult. If you ask an obedient intelligent car to take you to the airport as fast as possible, it might get you covered in vomit, doing not what you wanted but literally what you asked for. See more here\n\nThe first thing I loved to hear was: with the development of strong artificial intelligent agents, we can join our forces to fight against problems, so that we can have more time to be more human (Fei Fei li) \ud83d\ude09\ud83d\ude09\ud83d\ude09\ud83d\ude0a\n\nIt\u2019s Cost-Effective \u2014 Unlike humans, robots and machines do not have to get paid every month for the work they do.\n\nIt Enhances Efficiency \u2014 Depending on the type of intelligence. If machines would be built without any flaw. So, no doubt they would be able to perform even the most complex tasks without any error.\n\nThey Don\u2019t Take a Rest \u2014 Unlike humans, machines can train over and over again the same task to master in it, without needing to take a break\n\nMachines Don\u2019t Have Emotions \u2014 This is a great advantage because having no emotions means that nothing is going to affect their performance\n\nYes we are supposed to fear AI, but it doesn\u2019t mean that we should stop developing AI systems (I think that we can\u2019t stop AI), I particularly think we should keep our studies in AI, so that if the apocalypse is coming we should be prepared to face it, and if we don\u2019t develop it, somebody else will in secret and if something wrong happens we will be unprepared to fight against it.\n\nBeside the possibilities of apocalypse AI can help us achieve what would take us years.\n\nIf you enjoyed the writings leave your comment and claps \ud83d\udc4f to recommend this article so that others can see it."
    },
    {
        "url": "https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-enhancing-classic-strategies-a517f43109bf?source=---------9",
        "title": "Neural networks for algorithmic trading: enhancing classic strategies",
        "text": "Hello everyone! In five last tutorials we were discussing financial forecasting with artificial neural networks where we compared different architectures for financial time series forecasting, realized how to do this forecasting adequately with correct data preprocessing and regularization, performed our forecasts based on multivariate time series and could produce really nice results for volatility forecasting and implemented custom loss functions. In the last one we have set and experiment with using data from different sources and solving two tasks with single neural network and optimized hyperparameters for better forecasts.\n\nToday I want to make a sort of conclusion of financial time series with a practical forecasting use case: we will enhance a classic moving average strategy with neural network and show that it really improves the final outcome and review new forecasting objectives you most probably would like to play with.\n\nYou can check the code for training the neural network on my Github.\n\nWe already have seen before, that we can forecast very different values \u2014 from price changes to volatility. Before we were considering these forecasts as something kind of abstract and even tried to trade just looking on these \u201cup-down\u201d predictions, which wasn\u2019t that good. But we also know, that there are a lot of other trading strategies, that are based on technical analysis and financial indicators. For example, we can build moving averages of different window (one \u201clong\u201d, let\u2019s say 30 days, and one more \u201cshort\u201d, probably, 14 days) and we believe that crossing points are the moments where the trend changes:\n\nBut this trading strategy has one main pitfall: on the flat regions we will still do the trades on the points where nothing actually changes, so we will lose money:\n\nHow we can overcome this with use of machine learning?\n\nAs a forecast objective I want to try skewness \u2014 a measure of asymmetry of a distribution. Let us assume, that if we forecast a change in a distribution it will mean that our current trend (not only flat region) will change in the future.\n\nHere we will use pandas and PyTi to generate more indicators to use them as input as well. We will use MACD, Ichimocku cloud, RSI, volatility and others. All these values will form multivariate time series which will be flatten for later use in MLP or will stay for CNN or RNN.\n\nHere I want to show one of the option how to train regularized MLP for time series forecasting:\n\n\u201cNovel\u201d point here is adding small noise to the input and to the output of the single layer of our neural network. It can work very similar to L2 regularization, mathematical explanation you can check in this amazing book.\n\nNeural network is trained in a usual way, let\u2019s check how our forecasts of skewness can improve (or no) the moving averages strategy.\n\nWe train our network on AAPL prices from 2012 to 2016 and as test on 2016\u20132017 as we did in one of previous tutorials.\n\nAfter training of a network I have plotted our close prices, moving averages and vertical lines on crossing points: red and orange lines represent points where we would like to trade and green ones \u2014 where we better don\u2019t. It doesn\u2019t look perfect, but let\u2019s do backtesting to judge it.\n\nI used backtesting described in this post, so I will provide just key metrics and plots:\n\nHow we will use only \u201cred\u201d and \u201corange\u201d trading signals and skip the green ones. As we can see, this such a strategy did 2 trades less and it helped us to reduce the first drawdown a bit and increase final return almost twice!\n\nSeems like this idea at least has some sense! I would like to introduce you some possible improvements I highly recommend you to try by your own:\n\nWith this post I would like to finish (at least for a while) financial time series forecasting topic using neural networks. Let\u2019s be honest, it\u2019s definitely not a Holy Graal and we can\u2019t use them directly to predict if price will go up or down to make a lot of money. We considered different data sources and objectives, dealt carefully with overfitting and optimized hyperparameters. What conclusions we can do?\n\nI hope that this series of posts was useful to someone, I will come back rather soon with news topics\u2026 Stay tuned! :)"
    },
    {
        "url": "https://medium.com/machine-learning-world/deblur-photos-using-generic-pix2pix-6f8774f9701e",
        "title": "Deblur Photos Using Generic Pix2Pix \u2013 Machine Learning World \u2013",
        "text": "Last week my partner came across a problem at work. There were some poorly shot photos that were quite blurry and needed to be repaired. Unsharp masking didn\u2019t work well, along with a few free reparing softwares. The problem was solved by manually recreate important parts of the photo using Photoshop. But I couldn\u2019t help but wonder if deblurring can be done via some generic deep learning algorithms.\n\nI started with some super resolution algorithms, but soon realized that there are some differences. De-blurring, in essence, is trying to reverse convolution on an image (blind decovolution). Super-resolution, on the other hand, is trying to reverse the down-sampling on an image. Therefore I found pix2pix model should be more adequate for this task (as paired mappings between blurry photos and clear photos)[1]\n\nThe code is based on pix2pix implementation by mrzhu-cool on Github, with the following modifications.\n\nThe MIRFLICKR-25k dataset is used (in hope of generalizing better with real-life photos). The first 20k photos is used in training. Scaling and random cropping is applied. For the last 5k photos, around 2k are used as validation/development set, and the rest is reserved as test set (not used yet).\n\nThe artificial blurring is created by applying an uniform 3x3 filter and an Gaussian 5x5 filter (there\u2019s a lot of rooms to be improved):\n\nThe code released on Github. It has been tested on ceshine/cuda-pytorch:0.2.0 Docker image. Please check the accompanying Dockerfile for details."
    },
    {
        "url": "https://medium.com/machine-learning-world/tutorial-counting-road-traffic-capacity-with-opencv-998580f1fbde",
        "title": "Tutorial: Counting Road Traffic Capacity with OpenCV",
        "text": "Today I will show you very simple but powerful example of how to count traffic capacity with the algorithm that you can run on devices.\n\nAs always, here is full code of this project\n\nAlso, I recommend you to read my first article about road traffic classification, cause it cool and also maintains part of base pipeline architecture that we will you in this project.\n\nSo this algorithm works in 4 steps:\n\n1. Get frame edges.\n\n2. Blur them to get the more filled area.\n\n3. Binary threshold blurred the image.\n\n4. Overlap threshold image with ROI(you mask where you count) and count black pixels/white pixels which gives you traffic capacity.\n\nYou can see each step on image below:\n\nHere we use CLAHE equalization to remove noise from the image that can occur on cheap/old cameras at night. It not the best thing, but gives a better result.\n\nThen we use Canny Edge Detector to get edges from the image. We invert it to get white background(just for visual convenient)\n\nWe use basic blur with bilateral filtering which removes some color noise and gives better segmentation.\n\nThe last filter is a binary threshold which we use to get only white and black pixels which give as our segmentation on car/not car.\n\nAnd the last simple step just divides the number of black pixels with the number of white pixels to get traffic capacity.\n\nBecause of some camera noise and different outdoor conditions, accuracy may not so big ~ 70\u201385%. \n\nBut that is not a big problem because we can set min/max limits, or use additional filtering based on light conditions and for example amount of edges on some test ROI(for example some white rectangle).\n\nAnd also this data mainly used as additional data, so only relative values are needed.\n\nAll data is needed, even if you don\u2019t know how to use it right now :)\n\nIn our case with this data, we can say why traffic was limited at some point in time.\n\nThe main thing that you must remember working on Data Science projects that they should not only be effective in the domain but also cost-effective for business which includes speed, memory usage, scalability, one hour runtime costs for one task and in scale.\n\nThere are no algorithms that run efficiently in any conditions, for example, sort algorithms that used for local projects will never be used in big data projects because will be slow, and big data algorithms will not be used for local projects cause also will be slow(cause they faster only on big amount of data).\n\nSo you should build your pipeline with understanding your projects and business limitation.\n\nIf you have questions/ideas don\u2019t hesitate to post them in comments.\n\nPS: Also I got few emails where people asked me if it\u2019s ok to critic my code/ideas. So yes, it\u2019s ok, and moreover, I ask you to do so, so we all can get benefit from this.\n\nIf you like my articles, you can always support me with some beer-money https://paypal.me/creotiv"
    },
    {
        "url": "https://medium.com/machine-learning-world/tutorial-making-road-traffic-counting-app-based-on-computer-vision-and-opencv-166937911660",
        "title": "Tutorial: Making Road Traffic Counting App based on Computer Vision and OpenCV",
        "text": "In this lesson, we will use MOG algorithm for background subtraction and after processing, it looks like this:\n\nBut in some cases, we cant get static frame because lighting can change, or some objects will be moved by someone, or always exist movement, etc. In such cases we are saving some number of frames and trying to figure out which of the pixels are the same for most of them, then this pixels becoming part of background_layer. Difference generally in how we get this background_layer and additional filtering that we use to make selection more accurate.\n\nThere are many different algorithms for background subtraction, but the main idea of them is very simple. Let\u2019s assume that you have a video of your room, and on some of the frames of this video there is no humans & pets, so basically it\u2019s static, let\u2019s call it background_layer. So to get objects that are moving on the video we just need to:\n\nToday we will learn how to count road traffic based on computer vision and without heavy deep learning algorithms. For this tutorial, we will use only Python and OpenCV with the pretty simple idea of motion detection with help of background subtraction algorithm.\n\nAs you can see there is some noise on the foreground mask which we will try to remove with some standard filtering technic.\n\nRight now our code looks like this:\n\nFor our case we will need this filters: Threshold, Erode, Dilate, Opening, Closing. Please go by links and read about each of them and look how they work (to not make copy/paste)\n\nSo now we will use them to remove some noise on foreground mask.\n\nFirst, we will use Closing to remove gaps in areas, then Opening to remove 1\u20132 px points, and after that dilation to make object bolder.\n\nAnd our foreground will look like this\n\nFor this purpose we will use the standard cv2.findContours method with params:\n\nOn the exit, we add some filtering by height, width and add centroid.\n\nPretty simple, yeah?\n\nYou must understand that in ML and CV there is no one magic algorithm that making altogether, even if we imagine that such algorithm exists, we still wouldn\u2019t use it because it would be not effective at scale. For example a few years ago Netflix created competition with the prize 3 million dollars for the best movie recommendation algorithm. And one of the team created such, problem was that it just couldn\u2019t work at scale and thus was useless for the company. But still, Netflix paid 1 million to them :)\n\n \n\nSo now we will build simple processing pipeline, it not for scale just for convenient but the idea the same.\n\nAs input constructor will take a list of processors that will be run in order. Each processor making part of the job. So let\u2019s create contour detection processor.\n\nSo just merge together out bg subtraction, filtering and detection parts.\n\nNow let\u2019s create a processor that will link detected objects on different frames and will create paths, and also will count vehicles that got to the exit zone.\n\nThis class a bit complicated so let\u2019s walk through it by parts.\n\nThis green mask on the image is exit zone, is where we counting our vehicles. For example, we will count only paths that have length more than 3 points(to remove some noise) and the 4th in the green zone.\n\nWe use masks cause it\u2019s many operation effective and simpler than using vector algorithms. Just use \u201cbinary and\u201d operation to check that point in the area, and that\u2019s all. And here is how we set it:\n\nOn first frame. we just add all points as new paths.\n\nNext if len(path) == 1, for each path in the cache we are trying to find the point(centroid) from newly detected objects which will have the smallest Euclidean distance to the last point of the path.\n\nIf len(path) > 1, then with the last two points in the path we are predicting new point on the same line, and finding min distance between it and the current point.\n\nThe point with minimal distance added to the end of the current path and removed from the list.\n\nIf some points left after this we add them as new paths.\n\nAnd also we limit the number of points in the path.\n\nNow we will try to count vehicles that entering in the exit zone. To do this we just take 2 last points in the path and checking that last of them in exit zone, and previous not, and also checking that len(path) should be bigger than limit.\n\nThe part after else is preventing of back-linking new points to the points in exit zone.\n\nAnd the last two processor is CSV writer to create report CSV file, and visualization for debugging and nice pictures.\n\nCSV writer is saving data by time, cause we need it for further analytics. So i use this formula to add additional frame timing to the unixtimestamp:\n\nso with start time=1 000 000 000 and fps=10 i will get results like this\n\nframe 1 = 1 000 000 000 010\n\nframe 1 = 1 000 000 000 020\n\n\u2026\n\nThen after you get full csv report you can aggregate this data as you want.\n\nSo as you see it was not so hard as many people think.\n\nBut if you run the script you will see that this solution is not ideal, and having a problem with foreground objects overlapping, also it doesn\u2019t have vehicles classification by types(that you will definitely need for real analytics). But still, with good camera position(above the road), it gives pretty good accuracy. And that tells us that even small & simple algorithms used in a right way can give good results.\n\nSo what we can do to fix current issues?\n\nOne way is to try adding some additional filtration trying to separate objects for better detection. Another is to use more complex algorithms like deep convolution networks (about which i will tell in the next article)\n\nIf you like my articles, you can always support me with some beer-money https://paypal.me/creotiv"
    },
    {
        "url": "https://medium.com/machine-learning-world/how-to-debug-neural-networks-manual-dc2a200f10f2",
        "title": "How to debug neural networks. Manual. \u2013 Machine Learning World \u2013",
        "text": "Debugging neural networks can be a tough job even for field expert. Millions of parameters stuck together where even one small change can break all your hard work. Without debugging and visualization all your actions is popping a coin, and what worse it eating your time. Here i gather practices that will help you find problems earlier.\n\nTry to overfit your model with small dataset\n\nGeneral you neural net should overfit your data in a few hundreds of iterations. If your loss doesn\u2019t go down, then your problem is deeper.\n\nUse iterative logic in solving problem \n\nTry to build the simplest network that solve your main problem and then move step by step to global problem. For example, if you are creating style transfer network, try first train your script to transfer style on one image. If it work, only then create model that will transfer style to any image.\n\nUse balanced datasets with distortions \n\nFor example, if you train your network to classify data, your training data should have same number of inputs for each class. In other situation there is possibility of overfitting by class. Neural nets are not invariant to all distortions, and you need to train them specifically for that. So making input distortions will increase your network accuracy.\n\nNetwork capacity vs dataset size\n\nYour dataset should be enough for network to learn. If you have small dataset and big network it will stop learning(in some cases this will cause same result for big number of different inputs). If you have big dataset and small network then you will see jumping of loss, cause network capacity can\u2019t store so much information.\n\nUse mean centering \n\nThis will remove noise data from your network and increase training performance and also in some cases will help with NaN problem. But remember that if you have time-series data then you should use batch centering and not global.\n\nFirst try simpler model\n\nI saw many cases when people first tried some standard big network like ResNet-50, VGG19, etc, but then found that their problem can be solved on network with just few layers. So if you have not standard problem first start from small networks. The more stuff you add the more harder to train model to solve your problem, so starting from small network will also save time. Also you should remember that big networks eat much memory and ops.\n\nVisualization is a must\n\nIf you are using Tensorflow then definitely start using Tensorboard. If not, try find some visualization tool for your framework, or write it by yourself. This will help you great in finding problems on earlier stages of training. Things that you should definitely visualize: losses, histograms of weights, variables and gradients. If you working with CV then always visualize filters to understand what actually network is seeing.\n\nWeights initialization \n\nIf you set weights incorrectly your network can become untrainable because of zero gradients, or similar updates for all neurons, etc. Also you should remember that weights is coupled with learning rate, so big learning rate and large weights can lead to NaN problem.\n\nFor small networks it\u2019s enough to use some Gaussian distribution initializers around 1e-2\u20131e-3.\n\nFor deep networks this will not help, because your weights will be multiplied with each other many times that will lead to very small numbers which will almost kill gradients on back-propagation step. Thanks to Ioffe and Szegedy we now have Batch-Normalization that alleviates a lot of headaches.\n\nUse standard network for standard problem \n\nThere are plenty of pre-trained models(1)(2) that you can use right away. In some cases you can use them right away or you can use fine-tuning technique that will save time on training. The main idea is that most of the network capacity is the same for different problems. For example, if we talking about computer vision than first layers of network will consist from simple filters like lines, dot, angel that same for all images, and you don\u2019t need to retrain them.\n\nUse decay for learning rate\n\nThis will almost always give you a boost. There are plenty of different decay schedulers in Tensorflow\n\nUse Grid Search or Random Search or Config file for tuning hyper-parameters \n\nDon\u2019t try to check all parameters manually it\u2019s very time consuming and not effective. I usually use global config for all parameters and after run checking results to see in which direction i should investigate further. If this method not helping then you can use Random Search, or Grid Search.\n\nNetwork accuracy degradation on deep networks\n\nThe problem that really deep networks from some point starting to behave as a broken phone. Thus adding more layers decrease network accuracy. Fix for this is to use Residual layers that passing some portion of input across all layers. On image bottleneck residual layer.\n\nIf you like my articles, you can always support me with some beer-money https://paypal.me/creotiv"
    },
    {
        "url": "https://medium.com/machine-learning-world/learning-path-for-machine-learning-engineer-a7d5dc9de4a4",
        "title": "How To Become A Machine Learning Engineer: Learning Path",
        "text": "We will walk you through all the aspects of machine learning from simple linear regressions to the latest neural networks, and you will learn not only how to use them but also how to build them from scratch.\n\nBig part of this path is oriented on Computer Vision(CV), because it\u2019s the fastest way to get general knowledge, and the experience from CV can be simply transferred to any ML area.\n\nWe will use TensorFlow as a ML framework, as it is the most promising and production ready.\n\nLearning will be better if you work on theoretical and practical materials at the same time to get practical experience on the learned material.\n\nAlso if you want to compete with other people solving real life problems I would recommend you to register on Kaggle, as it could be a good addition to your resume.\n\nRequirement:\n\nPython. You don\u2019t have to be a guru, the basic knowledge will be just fine. For anything else there are manuals)\n\n1.2 Machine Learning by Stanford University\n\nThese first two will teach you the basic things about Data Science and machine learning and will prepare you for a real hard stuff)\n\n1.3 Deep Learning course from Andrew Ng\n\nGood Courses from famous Andrew Ng\n\n1.3 CS231n: Convolutional Neural Networks for Visual Recognition 2017 (2016)\n\nThat\u2019s where the party\u2019s starting, it\u2019s one of the best courses that you can find on the Internet about ML & CV. It will not only show you how deep is the rabbit hole, but also will give you good base for further investigation.\n\n1.4* Deep Learning by Google\n\nOptional course. You can take only practical part from it.\n\n1.5* CS224d: Deep Learning for Natural Language Processing\n\nOptional course for those who want to work with Natural Language Processing. And yeah, it is also great)\n\n1.6* Deep Learning book\n\nGood handbook which covers many aspects of ML\n\nThis list consist of many tutorials and projects, that you should try, understand how they work, and think how you can improve them. This list is created to increase your expertise and interest in ML, so don\u2019t be afraid if some of the tasks are hard for you, you can come back to them when you are ready.\n\n \n\n2.1. Simple practical course on Tensorflow from Kadenze \n\n2.1. Tensorflow cookbook\n\n2.2. Tensorflow-101 tutorial set\n\n2.3. IBM Code Patterns\n\nCode patterns from IBM which also includes DataScience & Analytics\n\n2.4. Fast Style Transfer Network\n\nThis will show how you can use neural network to transfer styles from famous paintings to any photo.\n\n2.6. Object detection with SSD\n\nOne of the fastest (and also simpler) models for object detection.\n\n2.8. Reinforcement learning\n\nVery useful thing especially if you want to build a robot or the next Dota AI :)\n\n2.9. Magenta project from Google Brain team\n\nProject that aims to creating compelling art and music with the help of neural networks. And the results are remarkable.\n\n2.10. Deep Bilateral Learning for Real-Time Image Enhancement\n\nNew awesome algorithm of the photo enhancement from Google\n\n2.11. Self driving-car project\n\nWant to make your car fully automatic? \u2014 that\u2019s a good starting point.\n\nWhat to do if you are stuck?\n\nFirst, you must understand that ML it\u2019s not something that 100% precise \u2014 most of the cases are just a good guess and tons of tuning iterations. So to come up with some unique idea is very hard in most cases, because of the time and resources you will spend on training the model. So don\u2019t try to figure out solution by yourself \u2014 search for papers, projects, people that can help you. The faster you get experience, the better.\n\nSome websites that can help you: http://www.gitxiv.com/, http://www.arxiv-sanity.com/, https://arxiv.org/, https://stackoverflow.com\n\nWhy papers do not fully cover the problem or are wrong in some places?\n\nIt\u2019s a pity to say, but not all tech guys want to open their work on public, but all of them need publications to get grants and fame. So some of them publish just a part of the material, or make mistakes in formulas. That\u2019s why it\u2019s always better to search for the code and not for the paper. You should think about the papers as an evidence or a fact that certain problem was solved.\n\nWhere can I find fresh materials?\n\nI use this two websites http://www.gitxiv.com/, http://www.arxiv-sanity.com/, https://arxiv.org/. First one finds not only a paper, but also a code for it, so it is more practical.\n\nShould I use Cloud or PC/Laptop for computing?\n\nCloud is the best fit for intense computing of production models. For learning and tests it is much cheaper to use PC/Laptop with CUDA graphic card. For example, I train all models on my laptop with GTX GeForce 960M with 690 CUDA Cores.\n\nOf course, if you have grants for cloud or free money for it, you can use it.\n\nHow can I improve tuning of hyperparameters of the models?\n\nThe main problem in training is time. You can\u2019t just sit and watch the training stats. For this reason I would recommend you to use Intelligent Grid Search. Basically, just create the sets of hyperparameters and model architecture, which you think can work better and then run them one after another in stream, saving results. Thus you can run training at night and compare results the next day, finding the most promising one.\n\nYou can look how this was done in sklearn library: http://scikit-learn.org/stable/modules/grid_search.html\n\nIf you like my articles, you can always support me with some beer-money https://paypal.me/creotiv\n\nAlso recommend to bookmark this article, you will need it as you start practicing :)"
    },
    {
        "url": "https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-hyperparameters-optimization-cb2b4a29b8ee",
        "title": "Neural networks for algorithmic trading. Hyperparameters optimization",
        "text": "Hello everyone! First of all I am thankful to you all, who is reading my blog, subscribing, and sharing opinions. It really makes me feel like what I do is not totally senseless and helps someone \u2764.\n\nIn five last tutorials we were discussing financial forecasting with artificial neural networks where we compared different architectures for financial time series forecasting, realized how to do this forecasting adequately with correct data preprocessing and regularization, performed our forecasts based on multivariate time series and could produce really nice results for volatility forecasting and implemented custom loss functions. In the last one we have set and experiment with using data from different sources and solving two tasks with single neural network.\n\nI think you have noticed that I usually take some architecture of the network as granted and don\u2019t explain why I take this particular number of layers, this particular activation function, this loss function etc. This is really tricky question. Yes, it\u2019s \u201cnormal\u201d in deep learning community to take ReLU (or more modern in 2k17 alternative like ELU or SELU) as activation and be happy with this, but we usually don\u2019t think if it\u2019s correct. Talking about number of layers or learning rate for optimizer \u2014 we just take something standard. Today I want to talk about the way how to automatize the process of making a choice from these options.\n\nAs always, code is available on the Github.\n\nEvery machine learning has a lot of parameters to choose before starting to train a model, and in case of deep learning this list increases exponentially. You can see on the picture above typical list of parameters you pick from when you train some computer vision convolutional neural network.\n\nBut there is a way to automatize this! Very briefly, when you have a bunch of parameters and choices of their values, you can:\n\nIn this post we will consider last option as a black-box, concentrating on practical implementation and results analysis.\n\nI used data from Kaggle where user @Zielak posted last 5 years 1-minute prices data for Bitcoin.\n\nWe will take a subset of last 10000 minutes and will try to build the best model to predict a change of price for next 10 minutes based on some historical period that we will choose later.\n\nAs input I want to take OHLCV tuple plus volatility and flatten this array in order to pass it to MLP model:\n\nFor hyperparameter optimization we will use library Hyperopt, that gives easy interface for random search and Tree of Parzen Estimators (one variant of Bayesian optimization).\n\nWe simply need to define the space of hyperparameters (keys of dictionary) and sets of options for them (values). You can define a discrete choice of options (for activation functions options) or uniformly sample from some range (for learning rate):\n\nIn our case I want to check if:\n\nAnd after we replace real parameters of layers, or data preparation, or training process with corresponding values of params dictionary. I suggest you to check whole code here.\n\nWe will check performance during first 5 epochs of training a network. After running this code, we will wait till 50 iterations (experiments) with different parameters will be executed and Hyperopt will choose the best option for us, which is:\n\nIt means that we want last two layers have 64 neurons and first \u2014 512 neurons, use sigmoid as activation (wow, interesting), take typical learning rate (0.001) and take window just of 30 minutes to predict next 10. Hmmmm\u2026 Okay.\n\nFirst I want to build a network with a \u201cpyramid\u201d pattern I usually take for new data. I also mostly start with ReLU as activation function and take standard for Adam optimizer learning rate 0.002:\n\nLet\u2019s check performance, blue is our forecast and black \u2014 original differences, MSE = 0.0005, MAE = 0.017:\n\nAnd now let\u2019s check how model with parameters found by Hyperopt will perform on this data:\n\nIn this case, numerical results (MSE = 4.41154599032e-05, MAE = 0.00507) and visual look much better!\n\nTo be honest, I don\u2019t think this is really good option, especially I don\u2019t agree with such a short training time window, I still want to try with 60 minutes and I think that Log-Cosh Loss is much more interesting loss option for regression. But I will stick with sigmoid activation for now, because seems like this is the thing that boosted performance a lot.\n\nHere MSE is 4.38998280095e-05 and MAE = 0.00503, which is just slightly better than what we got with Hyperopt\u2019s parameters, but visually it looks much worse (trends are totally missing).\n\nI strongly recommend you to use hyperparameters search for every model you train, whatever data you\u2019re working with. Sometimes it leads to some unexpected results like a choice of activation function (what, really sigmoid in 2017?) or window range (I didn\u2019t expect half an hour of historical information be better than one hour).\n\nIf you dive a bit deeper into Hyperopt options, you will see how you also can make search for number of hidden of layers, use or ignore of multitask learning and coefficients of loss functions. Basically, you just need to take subset of your data, think of parameters you want to tune and leave your computer for a while. This is a first step to automated machine learning :)"
    },
    {
        "url": "https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-2-1-multivariate-time-series-ab016ce70f57",
        "title": "Neural networks for algorithmic trading. Multivariate time series",
        "text": "In previous post we discussed several ways to forecast financial time series: how to normalize data, make prediction in the form of real value or binary variable and how to deal with overfitting on highly noisy data. But what we skipped (on purpose) \u2014 is that our .csv file with prices basically has much more data that we may use. In last post only close prices with some transformation were used, but what can happen if we will consider also high, low, open prices and volume of every historical day? This leads us to working with multidimensional, e.g. multivariate time series, where on every time stamp we have more than just one variable \u2014 in our case we will work with whole OHLCV tuple.\n\nIn this article we will see how to preprocess multivariate time series, in particular, what to do with every dimension, how to define and train a neural network on this kind of data and will compare results with what we had in last post.\n\nAs always, you can jump directly to the code.\n\nTo understand better what multidimensional time series is, let\u2019s remember how look images, that in fact also have not just two dimensions (height and width), but also \u201cdepth\u201d that represents color channels:\n\nIn case of time series, our image is just 1D (the plot we usually see on the graph) and the role of channels play different values \u2014 open, high, low, close prices and volume of operations. You can also think about it from other point of view \u2014 on any time stamp our time series is represented not with a single value, but with a vector (open, high, low, close prices and volume of every day), but metaphor with images is more useful to understand why we will apply convolutional neural networks to this problem today.\n\nOne of the most important moment about multivariate time series \u2014 the dimensions can come from different sources, can have different nature and can be totally uncorrelated and have different distribution, so we have to normalize them independently! We will use an ugly, but more or less adequate trick from last post:\n\nBut we are going to normalize every dimension of time window independently:\n\nBut as we want to forecast movement of a price up or down next day, we need to consider the change of a single dimension:\n\nSo, the data we will train on \u2014 are time windows of, like before, 30 days, but now on every day we will consider whole OHLCV data correctly normalized to predict the direction of close price movement. Full code for data preparation and neural network training you can find here.\n\nAs I mentioned before, I would like to use CNN as a classifier. Mainly I choose it because of flexibility and interpretability of hyperparameters (convolutional kernel, downsampling size etc) and performance similar to RNNs, better than MLP with much faster training.\n\nThe code for our network for today looks like:\n\nThe only difference from an architecture from a very first post is changing the EMB_SIZE variable to 5 in our case.\n\nFrom the plots we can clearly see that network trained adequately (for very noisy data), the loss of training set was decreasing with time while accuracy \u2014 increasing. And, what\u2019s the most important, comparing to univariate time series from previous post we improved the performance from 58% to almost 65% of accuracy!\n\nTo check overfitting we can also plot confusion matrix:\n\nand we will get:\n\nwhich shows that we predict \u201cUP\u201d movement with 75% of accuracy and \u201cDOWN\u201d with 53% of accuracy and this results of course can be balanced for the test dataset.\n\nInstead of predicting the binary variable, we can predict the real value \u2014 next day return or close price. In our previous experiments we didn\u2019t succeed to produce good results.\n\nUnfortunately, for returns it still works bad:\n\nFor prediction of value of close price the situation isn\u2019t better:\n\nI am still trying different things for regression problem in financial data (like custom loss functions), if you have some suggestions, I\u2019d like to discuss them in comments or PM.\n\nWe discussed the general pipeline of data preparation and normalization in case of multivariate time series, trained a CNN on them and we can report significant (+7%) improvement of classification problem \u2014 predicting if stock price will go up or down next day. Don\u2019t forget to check the full code and run it on your machine!\n\nMeanwhile we still can state that regression problem is still too complicated for us and we will work on it later, choosing correct loss metrics and activation functions.\n\nIn next post I would like to introduce the concept of multimodal learning and we will use parameters not just from our .csv file with OHLCV tuples, but much more interesting things."
    },
    {
        "url": "https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-1-2-correct-time-series-forecasting-backtesting-9776bfd9e589",
        "title": "Neural networks for algorithmic trading. Correct time series forecasting + backtesting",
        "text": "Hi everyone! Some time ago I published a small tutorial on financial time series forecasting which was interesting, but in some moments wrong. I have spent some time working with different time series of different nature (applying NNs mostly) in HPA, that particularly focuses on financial analytics, and in this post I want to describe more correct way of working with financial data. Comparing to previous post, I want to show different way of data normalizing and discuss more issues of overfitting (which definitely appears while working with data that has stochastic nature). We won\u2019t compare different architectures (CNN, LSTM), you can check them in previous post. But even working only with simple feed-forward neural nets we will see important things. If you want to jump directly to the code \u2014 check out IPython Notebook. For Russian speaking readers, it\u2019s a translation of my post here and you can check webinar on backtesting here.\n\nOther posts are here:\n\nLet\u2019s take historical time series of Apple stock prices starting from 2005 till today. You can easily download them from Yahoo Finance as .csv file. In this file data is in \u201creversed\u201d order \u2014 from 2017 till 2005, so we need to reverse it back first and have a look:\n\nAs we discussed in previous post, we can treat problem of financial time series forecasting in two different ways (let\u2019s omit volatility forecasting, anomaly detection and other interesting things for now):\n\nThe main problem of financial time series \u2014 they\u2019re not stationary, which means, that their statistical properties (mean, variance, maximal and minimal values) change over time and we can check it with augmented Dickey-Fuller test. And because of this we can\u2019t use classical data normalization methods like MinMax or Z-score normalization.\n\nIn our case, we will cheat a bit for classification problem. We don\u2019t need to predict some exact value, so expected value and variance of the future isn\u2019t very interesting for us \u2014 we just need to predict the movement up or down. That\u2019s why we will risk and normalize our 30-days windows only by their mean and variance (z-score normalization), supposing that just during single time window they don\u2019t change much and not touching information from the future:\n\nFor regression problem we already can\u2019t cheat like this, so will use returns (percentage of how much price changed comparing to yesterday) with pandas and it looks like:\n\nAs we can see, this data is already normalized and lies from -0.5 to 0.5.\n\nAs I said before, we will work only with MLPs in this article to show how easy to overfit neural networks on financial data (and actually what happened in previous post) and how to prevent it. Expand these ideas on CNNs or RNNs will be relatively easy, but it\u2019s much more important to understand the concept. As before, we use Keras as main framework for neural nets prototyping.\n\nOur first net will look like this:\n\nI can suggest always use Batch normalization after every affine or convolutional layer and Leaky ReLU as basic activation function, just because it\u2019s already became \u201cindustrial standard\u201d \u2014 they help to train nets way much faster. Other nice thing is reducing learning rate during training, Keras makes this with ReduceLROnPlateau:\n\nThis is how we launch training:\n\nAnd this is how we will visualize results (let\u2019s judge loss and accuracy plots)\n\nThe results aren\u2019t good at all, our test loss doesn\u2019t change at all, we can see clear overfit, let\u2019s make a deeper network and try it:\n\nHere we see more or less the same, even worse\u2026 It\u2019s time to add some regularization to the model, starting with adding L2 norm on sum of weights:\n\nIt works better, but still not good enough (even loss is decreasing, but accuracy is bad). It\u2019s happening very often while working with financial data \u2014 it\u2019s explained very nicely here\n\nThe next thing I want to do looks very weird, but we gonna regularize already regularized network adding hardcore dropout with 0.5 rate (it\u2019s random ignoring some weights while backpropagation to avoid neurons coadaptation and therefore overfitting):\n\nAs we can see, plots look more or less adequate and we can report about 58% of accuracy, which is slightly better than random guessing.\n\nFor regression, we will use returns data, previous successful neural network architecture (but without dropouts) and check how regression works:\n\nAnd here is code for plotting forecasts visually:\n\nIt works simply bad, even isn\u2019t worth to comment it. I will tell some tips that can help with regression problem in conclusion part.\n\nLet\u2019s remember why are we messing with all these time series in general? We want to build a trading system, which means, it has to make some deals \u2014 buy, sell stocks and, hopefully, grow your portfolio.\n\nThere are a lot of good ready solutions to backtest your strategies (like Quantopian), but I decided to learn how they\u2019re built from inside and bought the following book with details of implementation (not a product placement ahahah):\n\nThe strategy I\u2019ve tested is extremely simple: if our network says that price will go up, we buy the stock and sell it only after network says that price will go down and will wait for the next buying signal. The logic looks like:\n\nHere are the results of training classification network on data from 2012 to 2016 and testing from 2016 to the May of 2017:\n\nBlue plot shows portfolio value growth (wow, 3% in 1.5 years), black shows \u201cactivity\u201d and red one \u2014 drawdowns (periods of losing money).\n\nOn the first glimpse, results are bad. Horrible regression and not really amazing classification (58% of accuracy) are asking us to leave this idea. And after seeing that \u201cincredible\u201d 3% income (it would be easier just to buy Apple stocks and hold, they grew in 20% for that time) you maybe want to close laptop and do something that doesn\u2019t involve finance or machine learning. But there are lot of ways to improve our results (and what people do in funds):\n\nForecasting of financial data is extremely complicated. It\u2019s easy to overfit, we don\u2019t know correct historical range to train on and it\u2019s difficult to get all data needed. But as we can see, it works, and even can give some profits. This article can be good starting point and pipeline for further research and discovery.\n\nIn next posts I plan to show automated hyperparameter search process, add more data (full OHLCV and financial indicators), apply reinforcement learning to learn the strategy and check if reinforcement agent will trust our predictions. Stay tuned!"
    },
    {
        "url": "https://medium.com/machine-learning-world/using-caffe-with-your-own-dataset-b0ade5d71233",
        "title": "Using Caffe with your own dataset \u2013 Machine Learning World \u2013",
        "text": "What a time to be alive! We have a lot of tutorials for Tensorflow, Keras, Torch, even Caffe, but most of them use standard datasets as MNIST or IMDB comments. Couple of years ago I was figuring out how to use my own heap of pictures with Caffe but, actually, on the Internet there are still a few tutorials on this topic. So I think it\u2019s not bad idea to write another one step-by-step and more detailed guide about:\n\nUsual situation looks like this \u2014 you have pictures downloaded from Internet, or from other source. Let\u2019s say we solve classification problem and I hope you at least saved your different class pictures to different folders. Almost the same structure has Stanford Dogs Breeds Dataset. You can download it from http://vision.stanford.edu/aditya86/ImageNetDogs/.\n\nLet\u2019s don\u2019t rely on train/test split from the website and build our own. For further Caffe dataset creation we will need two files: train.txt and val.txt. They will contain paths to images and class number from train and test data respectively. Part of file can look like following:\n\nFor example you can do it with my old script (not that good, but works tho)\n\nSo, let\u2019s assume now we have our dataset in a folder and we have train.txt and val.txt with corresponding paths and labels.\n\nTo feed Caffe with large images dataset it\u2019s good choice to use LMDB format for our dataset. We already have an example of a script in Caffe folder (I suppose you have Caffe built on your machine) here caffe/examples/imagenet/create_imagenet.sh.\n\nWe need to change following things:\n\nYou can also use (you will need it for some Caffe prototxt\u2019s) make_mean.sh to generate mean file from input images (for further substraction in preprocessing step)\n\nLet\u2019s open network architecture files in caffe/models. We should change bold things in the next snippet: size of images (from RESIZE=true) and paths to LMDB images (from $EXAMPLE/dogs_train_lmdb, $EXAMPLE/dogs_val_lmdb) in file train_val.prototxt. In our case, let\u2019s try with GoogleNet.\n\nNext, in the same time we need to change number of classes in last, fully-connected layer (for GoogleNet there are 3 of them), 120 in our case. In the same file we look for next part:\n\nHere you can read more about other parameters in prototxt file: http://caffe.berkeleyvision.org/tutorial/layers.html. After you can change parameters of training a network (learning rate, weight decay and so on) in models/bvlc_googlenet/quick_solver.prototxt.\n\nIf you\u2019re going to finetune your network, Caffe supports it in very easy way. You just have to rename layer names, which weights you don\u2019t want to save from pretrained network, but want to learn from random initialization:\n\nYou can do the same for all other layers, maybe you want to learn from scratch also last convolutional layers, not only fully-connected.\n\nFor training a network from scratch we already prepared everything we need. Following line from ./caffe will run your training process.\n\nFor finetuning let\u2019s say from GoogleNet trained on ImageNet dataset, first, you have to download weights http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel here. And after that place them into models/bvlc_googlenet/.\n\nTo run finetuning process slightly change the line from above like this:\n\nAfter all, you have to achieve 70\u201375% accuracy in a short time.\n\nThank you for attention, I hope this small guide was useful for you. As you can see, you already have all things needed in a box with Caffe \u2014 you just have to fix basic config files and scripts. In you have some questions or comments \u2014 I\u2019ll be glad to talk in comments :)"
    },
    {
        "url": "https://medium.com/machine-learning-world/rbm-based-autoencoders-with-tensorflow-509fb9727ebb",
        "title": "RBM based Autoencoders with tensorflow \u2013 Machine Learning World \u2013",
        "text": "Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in Semantic Hashing paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with weights that were pre-trained with RBM autoencoders should converge faster. So I\u2019ve decided to check this.\n\nThis post will describe some roadblocks for RBMs/autoencoders implementation in tensorflow and compare results of different approaches. I assume reader\u2019s previous knowledge of tensorflow and machine learning field. All code can be found in this repo\n\nRBMs different from usual neural networks in some ways:\n\nNeural networks usually perform weight update by Gradient Descent, but RMBs use Contrastive Divergence (which is basically a funky term for \u201capproximate gradient descent\u201d link to read). At a glance, contrastive divergence computes a difference between positive phase (energy of first encoding) and negative phase (energy of the last encoding).\n\nAlso, a key feature of RMB that it encode output in binary mode, not as probabilities. More about RMBs you may read here or here.\n\nAs prototype one layer tensorflow rbm implementation was used. For testing, I\u2019ve taken well known MNIST dataset(dataset of handwritten digits).\n\nAt first, I\u2019ve implement multilayers RBM with three layers. Because we do not use usual tensorflow optimizers we may stop gradient for every variable with tf.stop_gradient(variable_name) and this will speed up computation a little bit. After construction two questions arose:\n\nSo I\u2019ve run the model with all binary units and only with last binary unit. And it seems that model with only last layer binarized trains better. After a while, I note that this approach was already proposed in the paper, but I somehow miss this.\n\nSo let\u2019s stop with the last layer binarized and try different train approaches. To build model that will train only pair of layers we need train two layers model, save it, build new model with one more layer, load pre-trained first two layers weights/biases and continue train last two layers (code). During implementation I\u2019ve met some trouble \u2014 tensorflow have no method to initialize all not initialized previously variables method. Maybe I just didn\u2019t find this. So I\u2019ve finished with approach when I directly send variable that should be restored and variables that should be initialized.\n\nAfter testing seems that both training approaches converge to approximately same error. But some another cool stuff \u2014 the model that was trained by pair lairs trains faster in time.\n\nSo we stop with RBM trained with only last layer binarized and with two layers only strategy.\n\nAfter getting pre-trained weights from RMB, it\u2019s time to build autoencoder for fine tuning. To get encoding layer output as much as possible binarized as per paper advice we add Gaussian noise before layer. To simulate deterministic noise behavior, noise generated for each input prior training and not changed during training. Also, we want compare autoencoder loaded from RBM weights with self-initialized usual autoencoder. Code for autoencoder.\n\nIt seems that RBM initialized autoencoder continue training, but newly initialized autoencoder with same architecture after a while stuck at some point.\n\nAlso, I\u2019ve trained two autoencoders without Gaussian noise. Now we can see through distribution what embedding most similar to binary (code for visualization):\n\nWe can see that RBM based autoencoder with Gaussian noise works better than other for our purposes.\n\nTo validate received embeddings I generate them for test and train sets for such networks:\n\nand use two validation approaches:\n\nTrain SVM with the train set and measure accuracy on the test set. SVM was used from sklearn with \u2018rbf\u2019 kernel with no max_iter == 50. Results table were generated with this code\n\nWith Hamming distance or dot product find ten most similar pictures/embeddings to provided one and check how many labels are the same to the submitted array label. Code to check distance accuracies.\n\nAs we can see embeddings can save some strong features, that can be used for future clusterization very well. But these features are not linearly correlated \u2014 so when we measure accuracy for most similar embeddings, we get results worse than when we use full MNIST images. Of course, maybe autoencoder should be trained with another learning rate/longer, but this is the task for future research.\n\nFor RBM training such params were used network was trained with:\n\nFor RBM training such params were used network was trained with:\n\nFor autoencoder learning rate was changed to 1.0 because of another optimization rule."
    },
    {
        "url": "https://medium.com/machine-learning-world/saving-humanity-from-dangerous-artificial-intelligence-scenario-223273cf8810",
        "title": "Saving Humanity From Dangerous Artificial Intelligence Scenario",
        "text": "An overview of how the Westworld TV show displays a perfect set of solutions to prevent bad things from happening\n\nMore and more people are becoming aware that truly smart things are already here and we are definitely seeing a massive trend of artificial intelligence being used across commercial products. This makes people anxious, especially after watching a couple of episodes of Westworld. And knowing there is an AI in your todo list or in Alexa device on your kitchen table doesn\u2019t help that feeling at all.\n\nOften we hear of the bright minds of our world talking about existential threats and dangers of AI in a vague manner. We talk about implications but we rarely sit down and actually talk through possible simple solutions how to prevent inevitable scenarios.\n\nLets try to systemize those simple solutions that everyone can understand through the prism of an effective architecture that runs the amusement park at Westworld, which I\u2019m truly amazed by, much more than by the plot.\n\nLets imagine there are two super AIs capable of destroying the human race. One is a supercomputer sitting somewhere at the IBM data center (known as Watson). The second one is a supercomputer sitting in your living room. As a human, or as a human-like robot.\n\nNow let\u2019s assume that the AI personality goes wild, like the peaceful leader Gandhi in a computer game in which he starts destroying the world with nuclear weapons because of a developer\u2019s bug.\n\nWhich one has more power to destroy the human race? The soft humanoid body that you can shoot or the data-center monster that no one even sees apart from the system administrators?\n\nIf you think about it, the internet and global connectivity are far more dangerous than we realize, and the main reason is that they are entirely hidden from our basic senses and our brain. We only see the end results. For example, a Medium post of someone, may not have come to be from the actual action of an author typing. The scope of global connectivity is difficult to grasp. We can\u2019t effectively perceive of millions of people typing their Medium posts simultaneously, as well as we can\u2019t see deviant AIs doing crazy stuff behind the web.\n\nHow soon would you notice that the robot goes wild and starts to display deviant undesired behavior? Or moreover wants to destroy every human on Earth just because something glitched or some crazy researcher decided to run an experiment? We all could agree that it\u2019s far more noticeable than having IBM\u2019s Watson silently take over all of the world\u2019s communications and start nuking Russia.\n\nOne of the main impressive technological showcases in the show is the notion of Explainable AI, which by the way is a hot topic funded by the Defense Advanced Research Projects Agency (DARPA).\n\nWe\u2019ve come far to create super intelligent machines but we are far from understanding how they are achieving certain results. Which is critical to debugging why certain machines take certain actions.\n\nOf course, there is a painful risk associated with giving AI a physical presence, but it\u2019s an individual risk rather than global human race risk. It is hard to imagine how a single robot can stab a million people or get through armies to kill world leaders. Far more imaginable is an IBM supercomputer silently hijacking a drone to send missiles into the White House.\n\nHow influential is a single person/human being versus a system of computers? A naked person on the street can definitely influence and grab attention of a number of people but those are very limited numbers. How more influential would a supercomputer be that hijacks Facebook and Twitter to display fake facts to stimulate the needed behavior? Or even generate the voice of Barack Obama and produce a video of him talking on a desired topic?\n\nWith the power of social networks and how fast the fake news spread nowadays, dangerous AI may have even more power over a crowd than we can imagine.\n\nWe, as humans, are very input/output dependent creatures, we still can\u2019t communicate wirelessly or directly through our brainwaves. We still can\u2019t communicate faster than 150 words per minute on average. Moreover sometimes we can\u2019t even understand each other while speaking the same language.\n\nComputers are the opposite. If you ever played any games with even a basic AI you know that they don\u2019t have to communicate with themselves through the chat window as you and your friends do. They use internal communication which is way faster and follows a direct structure with no misconceptions, in contrast to our primitive form of communication called language.\n\nCan you imagine a robot trying to explain to another robot why they should destroy all humans? I\u2019m sure we will see good attempts on that in the next episodes of Westworld where Maeve continues to assemble an army of her own. Yet I hardly imagine that happening in a real world environment, where robots are killed every day for the sake of entertainment, unless we will create such a park for real. I truly hope not, otherwise we are even more screwed as human beings.\n\nThus, the communication protocol limited to the primitive human language might be a great preventive measure, even more effective if you think how much we\u2019ve recently achieved in surveillance and spying on specific individuals which may one day be robots.\n\nIf you are following Westworld, you can clearly see that no machine is connected to a network, that\u2019s why the only way to connect to it is to wirelessly communicate through the \u201ctablet.\u201d And the only way to influence behavior of a robot is to directly speak to it or use some kind of local wireless stimuli that makes all robots in an area stop functioning. No internal connection or output to the \u201cmother\u201d system exists for obvious reasons discussed here, since it\u2019s a direct risk of letting AI get out and influence it\u2019s own behavior.\n\nThis is a concept that already works within military, police and even corporate structures, where you obey the orders of someone higher than you. As in the show, there is no single scenario where the creator of the park was ever harmed by the machine, unless directly programmed to do so. But other guests were killed, which means in theory a machine can harm any human, other than it\u2019s creator. Which can be implemented by using the notion of control hierarchies. Once we reach the spread of AI across our everyday life, including police and other crime preventive forces, we will definitely have to remove the limitations of not harming humans and move towards the unquestionable order obedience of the hierarchy.\n\nThis thought is tough and brings another magnitude of risks associated with misuse of robots by humans. But those are still human actions. If a human decides to end humanity it\u2019s not really a dangerous AI scenario, right?\n\nIt does seem like we need to encapsulate AI inside a human-like body and be able to address all of the aspects above to make most of the risks of a so-called dangerous AI more feasible to prevent.\n\nIt still doesn\u2019t guarantee anything but at least makes it much harder for an AI to go crazy and destroy the human race in a set of unnoticeable actions. At least actions will be trackable and there will be a much higher chance of prevention, especially if supported by 7 billion individuals interested in doing just that."
    },
    {
        "url": "https://medium.com/machine-learning-world/keras-lstm-to-java-a3124402d69",
        "title": "Keras LSTM to Java \u2013 Machine Learning World \u2013",
        "text": "We have lot of amazing frameworks for deep learning which allow us easy and fast prototyping and learning complex architectures even not thinking about what happening inside of them. But sometimes you need to deploy your model somewhere\u2026 let\u2019s say where you can\u2019t use your favorite\n\nI recently faced this problem, when I had to deploy recurrent neural network for action recognition trained in Keras in Java. My client doesn\u2019t want to use some microservices architecture, he wants everything in Java and basta cosi :)\n\nSo, let\u2019s see how we can do it.\n\nFirst, I trained 2-layers LSTM model with softmax on the top, classifying in 3 classes:\n\nFirst, let\u2019s load our weights from .hdf5 file and see the structure:\n\nOutput looks like this:\n\nparam_0, param_1 doesn\u2019t look very representative, I can\u2019t understand what these weights are responsible for. But output of layer.trainable_weights shows us exactly what we want. And if we check one of the most popular tutorials in LSTMs\u2026 We are just lucky! Notations of weight matrices are the same! We can understand, that param_0 is just W_i, param_1 \u2014 U_i and so on.\n\nNow let\u2019s save matrices in easy to read format:\n\nNow they are nicely stored as .txt files looking like:\n\nI am going to follow mentioned above tutorial for implementing LSTM. For all details check out code on Github, here are just some parts of it.\n\nFirst, I decided to use jblas for matrices routines, we will use them a lot.\n\nWe also need classes for:\n\nYou can check them out on Github, here I just post code for forward propagation routine in LSTM. Everything is pretty straightforward, just step-by-step matrices multiplications (carefully think about dimensions!).\n\nTo build our network we can use next approach (yeah-yeah, just making it look prettier, adding layers like in Keras :D)\n\nJust left to check results and compare them to Keras output. They are the same :)\n\nActually, it was a nice exercise to code some math, especially when you are used to use \u201cout-of-box\u201d instruments. Also it shows, that LSTMs are not that complicated in implementation and in case of need you can always port it to any language to make it work on any devices.\n\nThank you for attention!"
    },
    {
        "url": "https://medium.com/machine-learning-world/neural-networks-for-algorithmic-trading-part-one-simple-time-series-forecasting-f992daa1045a",
        "title": "Neural networks for algorithmic trading. Simple time series forecasting",
        "text": "This is first part of my experiments on application of deep learning to finance, in particular to algorithmic trading.\n\nI want to implement trading system from scratch based only on deep learning approaches, so for any problem we have here (price prediction, trading strategy, risk management) we gonna use different variations of artificial neural networks (ANNs) and check how well they can handle this.\n\nNow I plan to work on next sections:\n\nI highly recommend you to check out code and IPython Notebook in this repository.\n\nIn this, first part, I want to show how MLPs, CNNs and RNNs can be used for financial time series prediction. In this part we are not going to use any feature engineering. Let\u2019s just consider historical dataset of S&P 500 index price movements. We have information from 1950 to 2016 about open, close, high, low prices for every day in the year and volume of trades. First, we will try just to predict close price in the end of the next day, second, we will try to predict return (close price \u2014 open price). Download the dataset from Yahoo Finance or from this repository.\n\nWe will consider our problem as 1) regression problem (trying to forecast exactly close price or return next day) 2) binary classification problem (price will go up [1; 0] or down [0; 1]).\n\nFor training NNs we gonna use framework Keras.\n\nFirst let\u2019s prepare our data for training. We want to predict t+1 value based on N previous days information. For example, having close prices from past 30 days on the market we want to predict, what price will be tomorrow, on the 31st day.\n\nWe use first 90% of time series as training set (consider it as historical data) and last 10% as testing set for model evaluation.\n\nHere is example of loading, splitting into training samples and preprocessing of raw input data:\n\nIt will be just 2-hidden layer perceptron. Number of hidden neurons is chosen empirically, we will work on hyperparameters optimization in next sections. Between two hidden layers we add one Dropout layer to prevent overfitting.\n\nImportant thing is Dense(1), Activation(\u2018linear\u2019) and \u2018mse\u2019 in compile section. We want one output that can be in any range (we predict real value) and our loss function is defined as mean squared error.\n\nLet\u2019s see what happens if we just pass chunks of 20-days close prices and predict price on 21st day. Final MSE= 46.3635263557, but it\u2019s not very representative information. Below is plot of predictions for first 150 points of test dataset. Black line is actual data, blue one \u2014 predicted. We can clearly see that our algorithm is not even close by value, but can learn the trend.\n\nLet\u2019s scale our data using sklearn\u2019s method preprocessing.scale() to have our time series zero mean and unit variance and train the same MLP. Now we have MSE = 0.0040424330518 (but it is on scaled data). On the plot below you can see actual scaled time series (black)and our forecast (blue) for it:\n\nFor using this model in real world we should return back to unscaled time series. We can do it, by multiplying or prediction by standard deviation of time series we used to make prediction (20 unscaled time steps) and add it\u2019s mean value:\n\nMSE in this case equals 937.963649937. Here is the plot of restored predictions (red) and real data (green):\n\nNot bad, isn\u2019t it? But let\u2019s try more sophisticated algorithms for this problem!\n\nI am not going to dive into theory of convolutional neural networks, you can check out this amazing resourses:\n\nLet\u2019s define 2-layer convolutional neural network (combination of convolution and max-pooling layers) with one fully-connected layer and the same output as earlier:\n\nLet\u2019s check out results. MSEs for scaled and restored data are: 0.227074542433; 935.520550172. Plots are below:\n\nEven looking on MSE on scaled data, this network learned much worse. Most probably, deeper architecture needs more data for training, or it just overfitted due to too high number of filters or layers. We will consider this issue later.\n\nAs recurrent architecture I want to use two stacked LSTM layers (read more about LSTMs here).\n\nPlots of forecasts are below, MSEs = 0.0246238639582; 939.948636707.\n\nRNN forecasting looks more like moving average model, it can\u2019t learn and predict all fluctuations.\n\nSo, it\u2019s a bit unexpectable result, but we can see, that MLPs work better for this time series forecasting. Let\u2019s check out what will happen if we swith from regression to classification problem. Now we will use not close prices, but daily return (close price-open price) and we want to predict if close price is higher or lower than open price based on last 20 days returns.\n\nCode is changed just a bit \u2014 we change our last Dense layer to have output [0; 1] or [1; 0] and add softmax output to expect probabilistic output.\n\nTo load binary outputs, change in the code following line:\n\nAlso we change loss function to binary cross-entopy and add accuracy metrics.\n\nOh, it\u2019s not better than random guessing (50% accuracy), let\u2019s try something better. Check out the results below.\n\nWe can see, that treating financial time series prediction as regression problem is better approach, it can learn the trend and prices close to the actual.\n\nWhat was surprising for me, that MLPs are treating sequence data better as CNNs or RNNs which are supposed to work better with time series. I explain it with pretty small dataset (~16k time stamps) and dummy hyperparameters choice.\n\nYou can reproduce results and get better using code from repository.\n\nI think we can get better results both in regression and classification using different features (not only scaled time series) like some technical indicators, volume of sales. Also we can try more frequent data, let\u2019s say minute-by-minute ticks to have more training data. All these things I\u2019m going to do later, so stay tuned :)"
    },
    {
        "url": "https://medium.com/machine-learning-world/bot-hype-what-to-expect-in-2016-44b0a278214f",
        "title": "Bot Hype: What to expect in 2016 \u2013 Machine Learning World \u2013",
        "text": "With the burst of AI (artificial intelligence) technologies and growing messaging ecosystem there is a trending belief that the future of apps lies in conversation. Despite the current trending #ConvComm hashtag and @chrismessin article, I believe we are seeing an emergence of something more important than just a conversational user interface.\n\nWe are very far from a truly intelligent conversation interfaces for many reasons. If you look at the most conversation based messenger apps like Magic, Facebook M and others - they are all human assisted and they will stay like that for a while until we figure out a way to transition AI from delayed model training to real time learning, which will lead us to cracking the code of how people learn. The basis for this is described by \u201cMoravec\u2019s paradox\u201d.\n\nNot all applications should be conversation based, can Dropbox by itself be transformed into conversation bot? I don\u2019t believe so, but Dropbox may have a bot, the one that you can program to interact with your files, photos and/or shared folder members.\n\nThere will be a major paradigm shift in how people interact with services and bots will be tied in with the each stage of this interaction, starting from the initial contact to the selling and shipping process. Bots will be the interface itself, highly configurable and proactive; think of existing API\u2019s with a reasoning components built-in.\n\nis very hard to predict the taxonomy of bots right now but if we dive deep into the history of how APIs and SDKs evolved we can somewhat see patterns for future types of bots and what they will do.\n\nBots that digest huge amounts of data and are able to extract relevant information in a matter of seconds. We\u2019ve already seen that and the only company that is truly capable of delivering on promise is Google. Though there will be many more local finders that will operate in the personal space such as Findo.\n\nThe ones that deliver your email to slack channels already or the ones that will probably handle your next Amazon delivery. Basically, most of Slack integrations/apps that transfer data from one source to another fall into this category.\n\nIntelligent scrapers that will let us know when the new \u201cHouse of Cards\u201d episode is out. There are a lot of companies in this space right now, ranging from SaaS for competitor intelligence to consumer apps such as Facebook Notify.\n\nThe most complicated part of conversational apps is an automated sequential data input that may or may not require natural language processing. Most probably will make a strong use within the SaaS space.\n\nSomething that gets very little attention nowadays but I believe is a huge game changer of the way in how people will work in the next 5\u201310 years.\n\nAll of our mundane tasks and routines can be improved by a data enrichment. Think of a designer having a task to design a landing page for a car dealership. It will be a time saver to pull up top car dealership sites in the context and top tips for designing a landing page, along with some of the top rated landing page designs from Dribbble.\n\nThink I\u2019ve missed out on some type? Tweet or reply to this post."
    }
]