[
    {
        "url": "https://medium.com/@14prakash/the-intuition-behind-retinanet-eb636755607d?source=user_profile---------1----------------",
        "title": "The intuition behind RetinaNet \u2013 Prakash Jay \u2013",
        "text": "The end goal of this blog post is to make readers intuitively understand the deep working of RetinaNet. FAIR has released two papers in 2017 and 2018 respectively on their state of the art object detection frameworks. We will see how various layers come together to form a robust object detection pipeline. As usual I will take the math approach to explain everything. In short we will discuss the following two papers The following topics will be discussed Anchor boxes were first introduced in Faster RCNN paper and later became a common element in all the following papers like yolov2, ssd and RetinaNet. Previously selective search and edge boxes used to generate region proposals of various sizes and shapes depending on the objects in the image, with standard convolutions it is highly impossible to generate region proposals of varied shapes, so anchor boxes comes to our rescue.\n\nThe below diagram helps in viewing the valid anchor boxes on a Image, To the above diagram 1 attach two heads one for regression and the other for classification as shown below. We will discuss how these heads are designed below. Regression head: The output of the Faster RPN network as discussed and shown in the image above is a 50*50 feature map. A conv layer [kernal 3*3] strides through this image, At each location it predicts the 5 [x1, y1, h1, w1] values for each anchor boxes (9). In total, the output layer has 50*50*9*4 output probability scores. Usually this is represented in numpy as np.array(2500, 36). Classification head: Similar to the Regression head, this will predict the probability of an object present or not at each location for each anchor bos. This is represented in numpy array as np.array(2500, 9) The Feature map created after a lot of subsampling losses a lot of semantic information at low level, thus unable to detect small objects in the image. [Feature Pyramid networks solves this] The loss functions uses negative hard-mining by taking 128 +ve samples, 128 -ve samples because using all the labels hampers training as it is highly imbalanced and there will be many easily classified examples. [Focal loss solves this] In RPN, we have built anchor boxes only using the top high level feature map. Though convnets are robust to variance in scale, all the top entries in ImageNet or COCO have used multi-scale testing on featurized image pyramids. Imagine taking a 800 * 800 image and detecting bounding boxes on it. Now if your are using image pyramids, we have to take images at different sizes say 256*256, 300*300, 500*500 and 800*800 etc, calculate feature maps for each of this image and then apply non-maxima supression over all these detected positive anchor boxes. This is a very costly operation and inference times gets high. The authors of this paper observed that deep convnet computes a feature hierarchy layer by layer, and with subsampling layers the feature hierarchy has an inherent multi-scale, pyramidal shape. For example, take a Resnet architecture and instead of just using the final feature map as shown in RPN network, take feature maps before every pooling (subsampling) layer. Perform the same operations as for RPN on each of these feature maps and finally combine them using non-maxima supression. This is the crude way of building the feature pyramid networks. But there is one of the problem with this approach, there are large semantic gaps caused by different depths. The high resolution maps (earlier layers) have low-level features that harm their representational capacity for object detection.\n\nWe can design these blocks with more sophistication but the authors have found marginal improvements, so they have chosen to keep the network simple. The predictions are made on each level independently. Since the pyramids are of different scales, no need to have multi-scale anchors on a specific level. We define the anchors to have size of [32, 54, 128, 256, 512] on P3, P4, P5, P6, P7 respectively. We use anchors of multiple aspect ratio [1:1, 1:2, 2:1]. so in-total there will be 15 anchors over the pyramid at each location. All the anchor boxes outside image dimensions were ignored. positive if the given anchor box has highest IoU with the ground truth box or if the IoU is greater than 0.7. negative if the IoU is less than 0.3. The scales of the ground truth boxes are not used to assign them to levels of the pyramid. Instead, ground-truth boxes are associated with anchors, which have been assigned to pyramid levels. This above statement is very important to understand. I had two confusions here, weather we need to assign ground truth boxes to each level separately or compute all the anchor boxes and then assign label to the anchor box with which it has max IoU or IoU greater than 0.7. Finally I have chosen the second option to assign labels. The input image is resized such that its shorter side has 800 pixels. A mini-batch involves 2 images per GPU and 256 anchors per image. weight decay of 0.0001 and a momentum of 0.9. The learning rate is 0.02 for first 30k iteration and 0.002 for next 10k iteration. Training RPN with FPN on 8 GPUs takes 8 hours on COCO. Further a lot of ablation studies were done to prove that their choice of FPN is correct. One-stage detectors that are applied over a regular, dense sampling of possible object locations have the potential to be faster and simpler, but have trailed the accuracy of two-stage detectors because of extreme class imbalance encountered during training. Focal loss is the reshaping of cross entropy loss such that it down-weights the loss assigned to well-classified examples. The novel focal loss focuses training on a sparse set of hard examples and prevents the vast number of easy negatives from overwhelming the detector during training. Lets look at how this focal loss is designed. We will first look at binary cross entropy loss for single object classification For high class imbalance, we add a weighting parameter. usually this is inverse class frequency or treated as hyper-parameter set by cross-validation. Here we will term it as alpha called balancing param. As mentioned in paper, easily classified negatives comprise the majority of the loss and dominate the gradient. While alpha balances the importance of positive/negative examples, it does not differentiate between easy/hard examples. So the authors have reshaped the cross entropy function and come up with focal loss as mentioned below Here gamma is called the focusing param and alpha is called the balancing param. Lets get an intuition for this can help us: Say we have an easily classified foreground object with p=0.9. Now usual cross entropy loss for this example is Now, consider easily classified background object with p=0.1. Now usual cross entropy loss for this example is again the same Now, consider focal loss for both the cases above. We will use alpha=0.25 and gamma = 2 Say we have an misclassified foreground object with p=0.1. Now usual cross entropy loss for this example is Now, consider misclassified background object with p=0.9. Now usual cross entropy loss for this example is again the same Now, consider focal loss for both the cases above. We will use alpha=0.25 and gamma = 2 Say we have an easily classified foreground object with p=0.99. Now usual cross entropy loss for this example is Now, consider easily classified background object with p=0.01. Now usual cross entropy loss for this example is again the same Now, consider focal loss for both the cases above. We will use alpha=0.25 and gamma = 2 These three scenarios clearly show that Focal loss add very less weight to well classified examples and large weight to miss-classified or hard classified examples. This is the basic intuition behind designing Focal loss. The authors have tested different values of alpha and gamma and final settled with the above mentioned values. When training for object detection, the focal loss is applied to all ~100k anchors in each sampled image. The total focal loss of an image is computed as the sum of the focal loss over all ~100k anchors, normalized by the number of anchors assigned to a ground-truth box. gamma =2 and alpha =0.25 works best and in general alpha should be decreased slightly as gamma is increased. RetinaNet is a single, unified network composed of a backbone network and two task-specific subnetworks. The backbone is responsible for computing a conv feature map over an entire input image and is an off-the-self convolution network. The first subnet performs classification on the backbones output; the second subnet performs convolution bounding box regression. Backbone: Feature Pyramid network built on top of ResNet50 or ResNet101. However we can use any classifier of your choice; just follow the instructions given in FPN section when designing the network. Classification subnet: It predicts the probability of object presence at each spatial position for each of the A anchors and K object classes. Takes a input feature map with C channels from a pyramid level, the subnet applies four 3x3 conv layers, each with C filters amd each followed by ReLU activations. Finally sigmoid activations are attached to the outputs. Focal loss is applied as the loss function. Box Regression Subnet: Similar to classification net used but the parameters are not shared. Outputs the object location with respect to anchor box if an object exists. smooth_l1_loss with sigma equal to 3 is applied as the loss function to this part of the sub-network. Initialization of the network is very important. The authors have assumed a prior probability of 0.01 for all the anchor boxes and assigned this to the bias of last conv layer of classification sub net. I have overlooked this when implementing the net, The loss function blows up if you don\u2019t take care of this. The intuition behind attaching this prior probability is that the foreground (All positive anchor boxes) to background objects (All negative anchor boxes) in image is 1000/100000 = 0.01. Weight decay of 0.0001 and momentum of 0.9 with initial learning rate of 0.01 is used for first 60k Iterations. learning rate is reduced by 10 after 60k iterations and 80k iterations. Achieved an mAP of 40.8 using ResNeXt-101-FPN backend on MS-coco dataset To improve speed, Decode box predictions from at most 1k top-scoring predictions per FPN level, after thresholding the detector confidence at 0.05. The top predictions from all levels are merged and non-maximum suppression with a threshold of 0.5 is applied to yield the final decisions. [1708.02002] Focal Loss for Dense Object Detection\n\nAbstract: The highest accuracy object detectors to date are based on a two-stage approach popularized by R-CNN, where a\u2026arxiv.org [1612.03144] Feature Pyramid Networks for Object Detection\n\nAbstract: Feature pyramids are a basic component in recognition systems for detecting objects at different scales. But\u2026arxiv.org Clap and share if you like this post. Comment below if you have any feedback or doubt. Thanks for your time. Hope it helps. A big thanks to Fractal Analytics and my colleagues here. Special thanks to Soumendra and Prato_s [The AIJournal Team]."
    },
    {
        "url": "https://medium.com/@14prakash/using-focal-loss-for-deep-recommender-systems-9a7a87ca6415?source=user_profile---------2----------------",
        "title": "Using Focal Loss for Deep Recommender systems. \u2013 Prakash Jay \u2013",
        "text": "A student solves a series of challenges (Programming exercises) on an online platform. The problem here is to predict what challenges a user solves given the data of the first 10 challenges he solved.\n\nmAP@3 is used as the metric.\n\nSince there is a high imbalance in the output (3/5501), I used focal loss to train the network. Used 0.25 and 2 as balancing param and focusing param respectively.\n\nFor people who want to experiment, I have hosted the code along with the data.\n\nThank you Analytics Vidhya for clean dataset."
    },
    {
        "url": "https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cc5d0adf648e?source=user_profile---------3----------------",
        "title": "Understanding and Implementing Architectures of ResNet and ResNeXt for state-of-the-art Image\u2026",
        "text": "This is Part 2 of two-part series explaining blog post exploring residual networks.\n\nFor people who have understood part-1 this would be a fairly simple read. I would follow the same approach as part-1.\n\nThis paper gives the theoretical understanding of why vanishing gradient problem is not present in Residual networks and the role of skip connections by replacing Identity mapping (x) with different functions.\n\nF is a stacked non-linear layer and f is a Relu activation function.\n\nThey found that when both f(y1) and h(x1) are identity mappings, the signal could be directly propagated from one unit to any other units, in both forward and backward direction. Also, both achieve minimum error rate when they are identity mappings. Lets look at each case individually.\n\nCase-1, Lambda = 0: This will be a plain network. Since w2, w1, w0 are all between {-1, 1}, the gradient vanishes as the network depth increases. This clearly shows vanishing gradient problem\n\nCase-2, Lambda >1: In this case, The backprop value increases incrementally and lead to exploding of gradients.\n\nCase-3, Lambda <1: For shallow networks this might not be problem. But for extra large networks, weight+lambda is still less than <1 in most cases and it achieves the same problem as case-1\n\ncase-4, Lambda =1: In this case, Every weight is incremented by 1, This eliminates the problem of multiplying with very large numbers as in case-2 and small numbers as in case-1 and acts as a good barrier.\n\nThe paper also reviews by adding backprop and convolution layers to the skip-connection and found that network performance degraded. Below are the 5 experiment networks they have tried, out of which only the first one (a) gave minimum error rate.\n\nThe above 5 architectures were studied on ResNet-110 and ResNet-164 and they obtained the following results. In both the networks pre-activation outperformed all other networks. So having a simple additive and Identity mapping instead of Relu f(x) function is more appropriate. Having a Relu and BN layers in the Residual layer helped the network to optimize quick and regularize better (Less test error) , thus reducing over-fitting.\n\nSo having identity short-cut connections (Case-1) and identity after-addition activation are essential for making information propagation smooth. Ablation experiments are consistent with the derivations discussed above.\n\nResNeXt won 2nd place in ILSVRC 2016 classification task and also showed performance improvements in Coco detection and ImageNet-5k set than their ResNet counter part.\n\nThis is a very simple paper to read which introduces a new term called \u201ccardinality\u201d. The paper simply explains this term and make use of it in ResNet networks and does various ablation studies.\n\nThe paper made several attempts to describe the complexity of Inception networks and why ResNeXt architecture is simple. I m not going to do this here as it would require the reader to understand Inception networks. I will just talk about the architecture here.\n\nBelow is the difference in architecture between ResNet and ResNeXt\n\nSo a resnext_32*4d represents network with 4 bottleneck [one block in the above diagram] layers, and each layer having cardinality of 32. later we will observe resnext_32*4d and resnext_64*4d implementations in pytorch.\n\nCardinality vs width: with C increasing from 1 to 32, we can clearly see a descrease in top-1 % error rate. Therefore, Increasing the C by decreasing the width has improved the performance of the model.\n\n2. Increasing Cardinality vs Deeper/Wider: Basically 3 cases were studied. 1) Increasing the number of layers to 200 from 101. 2) Going wider by increasing the bottleneck width. 3) Increasing cardinality by doubling C.\n\nThey have observed that increasing the C gave better performance improvements. below are the results.\n\nAn ensemble of different ResNeXt architecture gave a top-5 error rate of 3.03% thus winning second position in ILSVRC competition.\n\nThe architecture is simple in design compared to Inception modules.\n\nResNeXt is not officially available in Pytorch. Cadene has implemented and made the pre-trained weights also available.\n\nI have taken his code and made them easy to experiment different Transfer learning techniques.\n\nI also wrote a blog post explaining how to use this repo. You can find ResNeXt implementations here. Both ResNeXt-32*4d and ResNext-64*4d are available along with image-net pre-trained weights.\n\nPlease share this with all your Medium friends and hit that clap button below to spread it around even more. Also add any other tips or tricks that I might have missed below in the comments!"
    },
    {
        "url": "https://medium.com/@14prakash/understanding-and-implementing-architectures-of-resnet-and-resnext-for-state-of-the-art-image-cf51669e1624?source=user_profile---------4----------------",
        "title": "Understanding and Implementing Architectures of ResNet and ResNeXt for state-of-the-art Image\u2026",
        "text": "This is Part 1 of two-part series explaining blog post exploring residual networks.\n\nWe will review the following three papers introducing and improving residual network:\n\nWhen deeper networks starts converging, a degradation problem has been exposed: with the network depth increasing, accuracy gets saturated and then degrades rapidly.\n\nLet us take a shallow network and its deeper counterpart by adding more layers to it.\n\nWorst case scenario: Deeper model\u2019s early layers can be replaced with shallow network and the remaining layers can just act as an identity function (Input equal to output).\n\nRewarding scenario: In the deeper network the additional layers better approximates the mapping than it\u2019s shallower counter part and reduces the error by a significant margin.\n\nExperiment: In the worst case scenario, both the shallow network and deeper variant of it should give the same accuracy. In the rewarding scenario case, the deeper model should give better accuracy than it\u2019s shallower counter part. But experiments with our present solvers reveal that deeper models doesn\u2019t perform well. So using deeper networks is degrading the performance of the model. This papers tries to solve this problem using Deep Residual learning framework.\n\nInstead of learning a direct mapping of x ->y with a function H(x) (A few stacked non-linear layers). Let us define the residual function using F(x) = H(x) \u2014 x, which can be reframed into H(x) = F(x)+x, where F(x) and x represents the stacked non-linear layers and the identity function(input=output) respectively.\n\nIf the identity mapping is optimal, We can easily push the residuals to zero (F(x) = 0) than to fit an identity mapping (x, input=output) by a stack of non-linear layers. In simple language it is very easy to come up with a solution like F(x) =0 rather than F(x)=x using stack of non-linear cnn layers as function (Think about it). So, this function F(x) is what the authors called Residual function.\n\nThe authors made several tests to test their hypothesis. Lets look at each of them now.\n\nTake a plain network (VGG kind 18 layer network) (Network-1) and a deeper variant of it (34-layer, Network-2) and add Residual layers to the Network-2 (34 layer with residual connections, Network-3).\n\nThere are two kinds of residual connections:\n\n2. When the dimensions change, A) The shortcut still performs identity mapping, with extra zero entries padded with the increased dimension. B) The projection shortcut is used to match the dimension (done by 1*1 conv) using the following formula\n\nThe first case adds no extra parameters, the second one adds in the form of W_{s}\n\nEven though the 18 layer network is just the subspace in 34 layer network, it still performs better. ResNet outperforms by a significant margin in case the network is deeper\n\nThe following networks are studied\n\nEach ResNet block is either 2 layer deep (Used in small networks like ResNet 18, 34) or 3 layer deep( ResNet 50, 101, 152).\n\nPytorch Implementation can be seen here:\n\nThe Bottleneck class implements a 3 layer block and Basicblock implements a 2 layer block. It also has implementations of all ResNet Architectures with pretrained weights trained on ImageNet.\n\nI have a detailed implementation of almost every Image classification network here. A Quick read will let you implement and train ResNet in fraction of seconds. Pytorch already has its own implementation, My take is just to consider different cases while doing transfer learning.\n\nI wrote a detailed blog post of Transfer learning. Though the code is implemented in keras here, The ideas are more abstract and might be useful to you in prototyping.\n\nPlease share this with all your Medium friends and hit that clap button below to spread it around even more. Also add any other tips or tricks that I might have missed below in the comments!"
    },
    {
        "url": "https://medium.com/@14prakash/almost-any-image-classification-problem-using-pytorch-i-am-in-love-with-pytorch-26c7aa979ec4?source=user_profile---------5----------------",
        "title": "Almost any Image Classification Problem using PyTorch",
        "text": "This is an experimental setup to build code base for PyTorch. Its main aim is to experiment faster using transfer learning on all available pre-trained models. We will be using the plant seedlings classification dataset for this blog-post. This was hosted as a play-ground competition on Kaggle. More details here.\n\nI have already discussed the intuition behind transfer learning in my previous blog post. So I will just mention them here.\n\nThis is very much straight forward in PyTorch if you know how the models are structured and wrapped. All the models used above are written differently. Some use Sequential containers, which contain many layers and some directly contain just the layer. So it is important to check how these models are defined in PyTorch.\n\nAs mentioned before there are several Resnets and we can use whichever we need. Since the Imagenet dataset has 1000 layers, We need to change the last layer as per our requirement. We can freeze whichever layer we don\u2019t want to train and pass the remaining layer parameters to the optimizer(we will see later).\n\nLets check what this model_conv has, In PyTorch there are children (containers) and each children has several childs (layers). Below is the example for resnet50,\n\nNow if we want to freeze few layers before training, We can simple do using the following command:\n\nChanging the last layer to fit our new_data is a bit tricky and we need to carefully check how the underlying layers are represented. We have already seen for Resnet and Inception_V3. Lets check for other networks\n\nThere are two variants of squeeze-net in PyTorch and we can use any of it. Unlike resnet which has a fc layer in the end, Squeeze-net final layer is wrapped inside a container(Sequential) So we need to first list all the children layer inside it and convert the required layers according to our dataset and convert into back to a container and write it back to the class. This is explained in the below code in detailed manner.\n\nIt is very similar to Resnet but the last layer is named as classifier. Below is the code\n\nIt is similar to Squeeze-net. The last fc layers are wrapped inside a container, so we need to read that container and change the last fc layer as per our dataset requirements.\n\nWe have seen how to freeze required layers and change the last layer for different networks. Now lets train the network using one of the nets. I am not going to mention this here in detail as it is already made available in my Github repo.\n\nLike any deep learning model, We need to first\n\nNow lets look how this done for inception_v3 in PyTorch. We will be freezing first few layers and train the network using an SGD optimizer with momentum and use Cross-Entropy loss.\n\nI have gone a bit further to check how these models perform under different setting. My idea was to train all the networks and see how individual models work and later apply different ensembling methods to improve the accuracy. I also thought of knowing how diverse are these models. So here are the metrics on the Train and validation dataset.\n\nCadene has trained several nets not available in Pytorch. I have used some of his code and trained the following models.\n\nI am facing issues with bninception and vggm. Will update soon\n\nStanding at 29 on Kaggle Leaderboard at the time of submission.\n\nVikas Challa is working on Mix-Up. We will publish the results soon."
    },
    {
        "url": "https://medium.com/@14prakash/playing-with-caffe-and-docker-to-build-deep-learning-models-99c9570ffc3d?source=user_profile---------6----------------",
        "title": "Playing with Caffe and Docker to build Deep Learning models.",
        "text": "For a long time, I had an aversion for Docker. I thought they were just to make things complex and was some fancy \u2018DevOps\u2019 thing that I could never wrap my head around. But this statement isn\u2019t entirely correct as I found out later, It also helps Data scientists while building ML/DL models in the following ways.\n\nA lot of people have written about Docker and its advantage. My goal for this post will be to give you an hands on experience on building deep learning models using Docker. This is somewhat I didn\u2019t find it on Internet and with the help of my colleague Prathamesh Sarang, I have gained enough knowledge on Docker and thought of sharing my experience.\n\nCaffe is one of the most popular Deep learning framework developed by Berkeley Vision and Learning Center(BVLC).\n\nIts primary advantage is command line interface and Model Zoo. All the popular deep learning models have been developed on caffe and made available in Model Zoo. RCNN, Fast RCNN and Faster RCNN ( Object detection algorithms), which I need to implement and experiment around for my project were developed using Caffe.\n\nAt least for image classification purpose, I loved Caffe. It took me just 6 hours (minus the time I spent on Installation setup) to learn Caffe and run my first deep learning model.\n\nThough this part of the tutorial primary focus is on \u201cUsing Caffe with Docker\u201d. All Deep Learning frameworks have their own Docker images made available. All you need to do is just pull them from Docker hub and run in your local systems.\n\nMy colleague Prathamesh Sarang has written about Docker and that was my starting point. I will share along a few other links which helped me in the process:\n\nThere are 4 steps in building deep learning models\n\nIts platform dependent and you can check the official Docker documentation for more details . Below are the instructions to download Docker in Ubuntu.\n\nI will be explaining each line below. To run the above file, you need to go to the particular folder and execute the following command\n\nFROM bvlc/caffe:cpu : This is the Docker image you are pulling from Docker hub. Officially caffe has released two Docker images \u2014 one for CPU and the other for GPU. We are downloading the CPU version here. All the required software is installed inside the Docker image and you just need to run it for building the models. Running this for the first time, will take some time for downloading the image. below is a simple Dockerfile to check the version of caffe we are using.\n\nWe can see that it consists of caffe 1.o.0.\n\nAdd . /model1 : Add all the files in the folder into the Docker container that we will create later on.\n\nWORKDIR /model1 : This will convert the model1 into a working directory from where we will be operating primarily.\n\nVolumes are very important. Following are the advantages:\n\nSo creating volumes will help. volumes are the way Docker communicates with your local systems. Here I kept all my data in data folder and all the code (jupyter notebooks) are saved inside my notebooks folder. Make sure that you have a data folder and notebooks folder before running the Docker file.\n\nEXPOSE 8888: This will expose the Docker port 8888. you can connect it to your local port.\n\nCMD jupyter notebook \u2014 no-browser \u2014 ip=0.0.0.0 \u2014 allow-root \u2014 NotebookApp.token=\u2019demo\u2019: This will help you run your jupyter notebook.\n\nNow your Dockerfile is completed. You need to run it. In the below image you can see the commands and their outputs.\n\nThe above command starts a browser with jupyter notebook. The command is self-explanatory, you are just telling Docker to which folders should communicate with which volumes.\n\nNow go to the web browser and run localhost:8887. you should see the below output. Enter the token \u201cdemo\u201d."
    },
    {
        "url": "https://medium.com/@14prakash/loved-moving-averages-should-explore-much-beyond-av-hiring-hackathon-22f5dfa161a0?source=user_profile---------7----------------",
        "title": "Loved Moving Averages. Should Explore Much Beyond \u2014 AV Hiring hackathon",
        "text": "Loved Moving Averages. Should Explore Much Beyond \u2014 AV Hiring hackathon This was my writeup on the approaches I have taken during a competition which deals with stock predictions.\n\nWelcome to Antallagma \u2014 a digital exchange for trading goods. Antallagma started its operations 5 years back and has supported more than a million transactions till date. The Antallagma platform enables working of a traditional exchange on an online portal. Download dataset here\n\nOverall Error = Lambda1 x RMSE error of volumes + Lambda2 x RMSE error of prices Where Lambda1 and Lambda2 are normalizing parameters.\n\n- Used exponential moving average on daily data using the formula\n\nIt took 40 mins to run on my system which has 32GB RAM.\n\nThere were in total 2489 participants out of which 211 participants made it to the Leader-board.\n\nPS: My code is very messy and verbose in its present form. I can reduce it by writing compelling functions but replicating the results would take hours and I don\u2019t want to mess with it now. Apologies for it."
    },
    {
        "url": "https://medium.com/@14prakash/recurrent-neural-network-using-tensor-maths-af63b95bf7a8?source=user_profile---------8----------------",
        "title": "Recurrent Neural Network using Tensor Maths \u2013 Prakash Jay \u2013",
        "text": "Understanding how the RNN network flows internally is of primary importance. Lets deal with it in this blog post.\n\nRNN Theory is written by many people and two links I shared below are undoubtedly the best. In simple words , RNNs are Networks in Loops, Allowing information to persist.\n\nWe will be using TensorFlow for Tensor math. PyTorch is equally good and dynamic too.\n\nLets import TensorFlow, number of steps is the max_length of a sentence, state_size is the internal state size (3 neurons here ). Lets consider 400 words(n_words) to be in the corpus.\n\ncreate the placeholders for input and output vectors.\n\nNow convert x to one_hot vector and unstack all the vectors (Each input will go into each FC network ( See the above diagram where word1, word2 inputs will be each network). In the below photo you can clearly see rnn_input[0] which will be input to our first word1 network, We have 30 (num_steps or max_length of a sentence) such vectors each entering the network.\n\nNow define Weights and bias . There will be two weight vectors\n\nNow define an rnn_cell which will essentially do simple Feed forword multiplication and hidden_state multiplication. Later combine them and add bias to it. In the end send it through an activation function (used tanh here )\n\nWe need to perform the above cell operation on all 30(num_words in a sequence) inputs. We can run a simple for loop and add the last output as an input to next state.\n\nLets add an output layer to the last state. This will be one layer below the Dense Layer we have mentioned in the diagram.\n\nLets add one more dense layer ( You can add more depending on the complexity of the work you are doing)\n\nAdd the Final output layer and Add softmax to it.\n\nIts too much of a code and effort to write down about LSTM but worth the short if you want to exactly understand how it works. try it if you have time.\n\nNo. You can wrap these all under a function and use it every time. All Deep Learning frameworks provide rnn cell function. you can just use them. It will be just one line.\n\nI love Keras and use it to experiment different models but if you really want to under stand deep learning jump on to TensorFlow or Pytorch and use Tensor math to solve your problems. This will help you in long run.\n\nLet me know your feedback. If you like it, please recommend and share it. Thank you."
    },
    {
        "url": "https://medium.com/@14prakash/back-propagation-is-very-simple-who-made-it-complicated-97b794c97e5c?source=user_profile---------9----------------",
        "title": "Back-Propagation is very simple. Who made it Complicated ?",
        "text": "Back-Propagation is very simple. Who made it Complicated ? Learning Outcome: You will be able to build your own Neural Network on a Paper.\n\nAlmost 6 months back when I first wanted to try my hands on Neural network, I scratched my head for a long time on how Back-Propagation works. When I talk to peers around my circle, I see a lot of people facing this problem. Most people consider it as a black-box and use libraries like Keras, TensorFlow and PyTorch which provide automatic differentiation. Though it is not necessary to write your own code on how to compute gradients and backprop errors, having knowledge on it helps you in understanding a few concepts like Vanishing Gradients, Saturation of Neurons and reasons for random initialization of weights More about why is it important to Understand? Andrej Karapathy wrote a blog-post on it and I found it useful. Yes you should understand backprop\n\nWhen we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to\u2026medium.com Build a small neural network as defined in the architecture below. Backprop and adjust the weights and bias accordingly Build a Feed Forward neural network with 2 hidden layers. All the layers will have 3 Neurons each. 1st and 2nd hidden layer will have Relu and sigmoid respectively as activation functions. Final layer will have Softmax. I have taken inputs, weights and bias randomly\n\nEdit1: As Jmuth pointed out in the comments, the output from softmax would be [0.19858, 0.28559, 0.51583] instead of [0.26980, 0.32235, 0.40784]. I have done a/sum(a) while the correct answer would be exp(a)/sum(exp(a)) . Please adjust your calculations from here on using these values. Thank you. The Actual Output should be [1.0, 0.0, 0.0] but we got [0.2698, 0.3223, 0.4078]. We are done with forward pass. Now let us see backward pass\n\nLet us calculate a few derivatives upfront so these become handy and we can reuse them whenever necessary. Here are we are using only one example (batch_size=1), if there are more examples, We just need to average everything. By symmetry we can calculate other derivatives also Next let us calculate the derivative of each output with respect to their input. By symmetry we can calculate other derivatives also For each input to neuron let us calculate the derivative with respect to each weight. Now let us look at the final derivative Example: Derivative of input to output layer wrt weight By symmetry we can calculate other derivatives also values of derivative of input to output layer wrt weights. Finally Let us calculate the change in\n\nLet us calculate a few handy derivatives before we actually calculate the error derivatives wrt weights in this layer. Values of derivative of output of layer-2 wrt input of layer1 For each input to neuron let us calculate the derivative with respect to each weight. Now let us look at the final derivative By symmetry we can calculate: Now we will calculate the derivative of By symmetry we get the final matrix as,\n\nWe have already calculated the 2nd and 3rd term in each matrix. We need to check on the 1st term. If we see the matrix, the first term is common in all the columns. So there are only three values. Let us look into one value Lets see what each individual term boils down too. by symmetry we get the final matrix as, Again the first two values are already calculated by us when dealing with derivatives of W_{kl}. We just need to calculate the third one, Which is the derivative of input to each output layer wrt output of hidden layer-2. It is nothing but the corresponding weight which connects both the layers. Final Matrix of derivative of total error wrt output of hidden layer-2 All Values are calculated before we just need to impute the corresponding values for our example. calculations using an example Let us look at the final matrix\n\nConsider a learning rate (lr) of 0.01 We get our final Weight matrix as So, We have calculated new weight matrix for W_{jk}. Now let us move to the next layer: Edit:1 the following calculations from here are wrong. I took only wj1k1 and ignored wj1k2 and wj1k3. This was pointed by an user in comments. I would like someone to edit the jupyter notebook attached at the end. Please refer to some other implementations if u still didn\u2019t understand back-prop here.\n\ncalculations in our example Consider a learning rate (lr) of 0.01 We get our final weight matrix as using learning rate we get final matrix I have completely eliminated bias when differentiating. Do you know why ? Backprop of bias should be straightforward. Try on your own. I have taken only one example. What will happen if we take batch of examples? Though I have not mentioned directly about vanishing gradients. Do you see why it occurs? What would happen if all the weights are the same number instead of random ? I have hand calculated everything. Let me know your feedback. If you like it, please recommend and share it. Thank you."
    },
    {
        "url": "https://medium.com/@14prakash/transfer-learning-using-keras-d804b2e04ef8?source=user_profile---------10----------------",
        "title": "Transfer Learning using Keras \u2013 Prakash Jay \u2013",
        "text": "When you look at what these Deep Learning networks learn, they try to detect edges in the earlier layers, Shapes in the middle layer and some high level data specific features in the later layers. These trained networks are generally helpful in solving other computer vision problems. Lets have a look at how to do transfer learning using Keras and various cases in Transfer learning.\n\nMost of the Computer Vision Problems I faced doesn\u2019t have very large datasets(5000 images \u2014 40,000 images). Even with extreme data augmentation strategies it is difficult to achieve decent accuracy. Training these networks with millions of parameters generally tend to overfit the model. So Transfer learning comes to our rescue.\n\nTransfer learning, is a research problem in machine learning that focuses on storing knowledge gained while solving one problem and applying it to a different but related problem.\n\nKeeping in mind that convnet features are more generic in early layers and more original-dataset-specific in later layers, here are some common rules of thumb for navigating the 4 major scenarios:\n\nThere is a problem of over-fitting, if we try to train the entire network. Since the data is similar to the original data, we expect higher-level features in the ConvNet to be relevant to this dataset as well. Hence, the best idea might be to train a linear classifier on the CNN codes.\n\nSo lets freeze all the VGG19 layers and train only the classifier\n\nSince we have more data, we can have more confidence that we won\u2019t overfit if we were to try to fine-tune through the full network.\n\nIn case if you want to freeze the first few layers as these layers will be detecting edges and blobs, you can freeze them by using the following code.\n\nSince the dataset is very small, We may want to extract the features from the earlier layer and train a classifier on top of that. This requires a little bit of knowledge on h5py.\n\nThe above code should help. It will extract the \u201cblock2_pool\u201d features. In general this is not helpful as this layer has (64*64*128) features and training a classifier on top of it might not help us exactly. We can add a few FC layers and train a neural network on top of it. That should be straight forward.\n\nThis is straight forward. since you have large dataset, you can design your own network or use the existing ones.\n\nLet me know your feedback and interesting things you were able to do with transfer learning."
    }
]