[
    {
        "url": "https://medium.com/@memoakten/deepdream-is-blowing-my-mind-6a2c8669c698?source=tag_archive---------0----------------", 
        "text": "The poetry is blowing my mind at every step of the process\u2026\n\nWhen an artificial neural network receives an input such as an image, it tries to make sense of it based on what it already knows. The image data flows through the network, \u2018activating\u2019 neurons. Effectively the image is ripped apart and scanned for features that the network recognises. This can be thought of as asking the network \u201cBased on what you already know, can you see anything here that you recognise?\u201d.\n\nThis of course is how we make sense of the world. It\u2019s analogous to asking us to recognise objects in clouds or ink / Rorschach tests. But I don\u2019t even mean just visually. We try to frame everything that we see, hear or learn within the context of what we already know, and we build on top of that. This can be purely visual like seeing faces in clouds. Or it can be more critical as it affects how we learn, interpret information, make decisions, construct theories or develop prejudices based on the limited knowledge that we have. If we don\u2019t have sufficient information, the assumptions we make are likely to be incorrect, as are the decisions we make as a result of them.\n\nWhen the network is processing the image, some of these recognitions might be weak firings within the network. These weak neural firings can be thought of as almost sub-conscious level \u201cI think I see a little bit of a lizard-like texture over here, perhaps something that resembles a bridge over there\u201d. But if these are very weak activations in the deep layers, they\u2019ll dissipate within the network and won\u2019t elevate to higher layers, or influence the final output.\n\nBut in the case of #deepdream, we choose a particular group of neurons dedicated to detecting particular features\u200a\u2014\u200ae.g. those which respond to lizard like features\u200a\u2014\u200aand we take a snapshot image, from inside the network. Whatever features a particular group of neurons respond to, will be dominant in the snapshot image created from those neurons. (NB. Technically speaking, by \u2018take a snapshot\u2019 I mean we choose a group of neurons, and we modify the input image such that it amplifies the activity in that neuron group. See my other article for more non-technical technical info).\n\nThis snapshot shows what that particular group of neurons are responding to, or \u2018thinking about\u2019.\n\nWhen we feed that snapshot image back into the network as a new input, the network recognises those exact same features but with more confidence, because those patterns in the new image are now stronger, so those same neurons fire stronger. And when we take another snapshot of the same neurons, and feed that back in, it becomes even stronger. What was an initial \u201cmaybe I see inklings of little lizard-like features over here\u201d on a deep sub-conscious level, starts to become \u201cyea, I think they might be lizard-like features\u201d, to \u201coh definitely, that\u2019s a lizard-skin puppy-slug\u201d at a well defined, visible high level. These activations are now strong enough to not dissipate and disappear in the depths of the network, and can propagate to higher levels, potentially even affecting the final output or decision.\n\nThis creates a positive feedback loop, reinforcing the bias in the system. Building confidence with each iteration. Transforming what was subtle, unnoticeable trends deep within the network, to strong, visible, defining biases that affect the decisions of the network.\n\nThis is almost like asking you to draw what you think you see in the clouds, and then asking you to look at your drawing and then draw a new image of what you think you are seeing in your drawing. And repeating this.\n\nBut that last sentence was not even fully accurate. It would be accurate, if instead of asking you to draw what you think you saw in the clouds, we scanned your brain, looked at a particular group of neurons which we know responds to a particular pattern, then we reconstructed an image based on the firing patterns of those neurons, and gave that image to you to look at. And then we scanned the same neurons again to produce a new image and showed you that etc.\n\nThe critical difference is, if we\u2019d asked you what you saw in the clouds, we\u2019d be representing the final conscious decision you made regarding what you saw. Whereas by scanning and extracting the snapshot from a group of neurons, we\u2019re preying on and amplifying a particular thread of thought to create a strong bias. Like an indoctrination on a neurological level.\n\nThis of course is analogous to so many aspects of how our mind functions already. We see the world through the filter of a biased mind. A product of our upbringing, everything we've ever seen or learnt, the culture in which we live or come from. We project this bias onto everything we perceive, and if we\u2019re not very careful, everything we perceive will in turn reinforce the very bias that shaped it.\n\nThe face in the clouds looks more and more like a face the more we think it\u2019s a face. The shadow in the alley looks more and more like a mugger the more afraid we become. The image of the virgin mary on a piece of toast is more tangible the more we want to believe in it. The more convinced we are of a certain hypothesis, the more inclined we are\u200a\u2014\u200asubconscious or not\u200a\u2014\u200ato find that every piece of evidence confirms that hypothesis.\n\nInterestingly, even if you don\u2019t agree with my previous points, you've probably already confirmed them. If you see a human or ape like face in the image below; or [bird, slug, reptile, worm, puppy, sloth]-like creatures; then you have just demonstrated it. Perhaps you see something else? Something I can\u2019t see? Then you've confirmed my point even stronger.\n\nThere are no faces, birds, slugs, reptiles, worms, puppies, sloths in the image above.\n\nYour mind is projecting those meanings, trying to recognise patterns based on what it already knows, what it\u2019s been trained on. Neurons in your brain stimulated by different features of these abstract shapes are trying to make sense of what you\u2019re seeing and frame it in context of something familiar. (NB. Also see See apophenia and pareidolia).\n\nJust like the #deepdream neural network.\n\nYou\u2019re looking into a mirror of your own mind.\n\nEven more interestingly, remember that these images generated by the #deepdream process are not what the network is seeing on a high level. These images are extracts from inside the network. Abstract representations from the depths of its memory. Snapshots from inside the AI\u2019s brain.\n\nThese were weak neural firings, that we amplified. Without us interfering, these firings might not have even elevated to higher levels, and would have remained latent in the network. On a higher level the AI might not even be aware of these features. But deep inside the artificial neural network, certain neurons fired weakly, responding to certain features that the network recognised.\n\nAnd then you look at these images. You find patterns in them. You recognise the exact same features that the artificial neural network had recognised, and amplified. You project meaning onto this noise, the abstract representations extracted from inside the hidden depths of the artificial neural network. In recognising these forms, you are confirming what the #deepdream neural network recognises but doesn't know that it\u2019s seeing. The same neurons which fired weakly in the depths of the artificial neural network, are now firing in your brain.\n\nYou are completing this cycle of recognition and meaning in your mind.", 
        "title": "#Deepdream is blowing my mind \u2013 Memo Akten \u2013"
    }, 
    {
        "url": "https://medium.com/@memoakten/background-info-for-deepdream-is-blowing-my-mind-1983fb7420d9?source=tag_archive---------1----------------", 
        "text": "An Artificial Neural Network (ANN) can be thought of as analogous to a brain (immensely, immensely simplified. Nothing like a brain really).\n\nIt\u2019s not really like a brain, it\u2019s just metaphorically similar. And initially was inspired by (what we think we thought we knew about) how the brain works. But there are some similarities on a very high level.\n\nAn ANN consists of \u2018neurons\u2019 and \u2018connections\u2019 between neurons. The neurons are usually organized in layers. See this image from Wikipedia:\n\nData flows in one side of this neuron network (via input nodes), gets processed along the network, and something is output on the other side via output nodes. (NB. In this context \u2018Data\u2019 and \u2018Information\u2019 mean the same thing, numbers. Long sequences of numbers)\n\nEach connection (i.e. the arrows in the image above) between two nodes has a weight associated with it. This number is the strength of that connection. (NB. When data is fed to input nodes, they get passed down all of the arrows, multiplied by the \u2018weight\u2019 of each connection. The receiving nodes add up all of the numbers they receive from all their connections, put them through a little function called an \u2018activation function\u2019, and sends the results down their own arrows to the next nodes. This is repeated throughout, all the way to the output nodes -> RESULT)\n\nIn short an ANN processes and maps an arbitrary number of inputs, to an arbitrary number of outputs. In this way the ANN acts like (and is often said to \u2018model\u2019) a mapping function.\n\nAnd most importantly: Information (the function it models) is stored in the network as \u2018weights\u2019 (strengths) of connections between neurons.\n\nIf we feed a network some inputs, and the output depends on these connection weights, how do we know what weights we should use?\n\nThat\u2019s where training comes in.\n\nIn what\u2019s known as supervised learning, you provide the network with a bunch of training examples, in simple terms: input-output pairs. (NB. There are other types of learning too. Like unsupervised, where you don\u2019t provide training examples, you just give the network a bunch of data, and it tries to extract patterns and relationships. Or semi-supervised learning which is a mixture of both.)\n\nYou would effectively say \u201cFor this input A, I want this output X; For this input B, I want this output Y; for this input C, I want this output Z\u201d. This is analogous to pointing to pictures of animals with a toddler going \u201cCAT\u201d, \u201cDOG\u201d etc. You\u2019re associating inputs (pictures of cats and dogs) with outputs (the words \u201cCAT\u201d and \u201cDOG\u201d).\n\nThen you say LEARN! And a long iterative process tries to solve the network. The problem it\u2019s trying to solve is: what are the weights I need on each connection, such that when I feed in the inputs of the training examples, I get the corresponding outputs of the training examples. (NB. In reality you will never be able to train the network such that you get the exact same outputs for the training inputs. So it\u2019s more about trying to minimise the error.)\n\nAfter you've trained the network, the network has (hopefully) optimum weights on each connection. Such that if you were to feed it the inputs from the training examples, you (hopefully) get the same (or near) results as the corresponding outputs.\n\nWhere it gets interesting and potentially useful (and potentially wrong and scary), is if you feed it new input data that it hasn't seen before, and it tries to interpolate / extrapolate / calculate / predict relevant new output based on the patterns it\u2019s found from the training data. Current models predict on a spectrum ranging from spectacularly accurate, to spectacularly wrong.\n\nSo how the hell do you use this for complex tasks like image or voice recognition?\n\nThe network image from Wikipedia above is a very simple network. Suited to simple (i.e. low dimensional) problems. But to solve complex problems with it, you\u200a\u2014\u200aas the trainer of the network\u200a\u2014\u200awould need to be very specific in the data you feed it. You couldn't just feed it raw image data (i.e. millions of pixels), the network wouldn't be able to cope with the immense amount of information found in a raw image. You would need to do a hell of a lot of manual, handcrafted feature extraction first to reduce the dimensions (i.e. amount of information). Instead of feeding it raw images, you might need to run filters on the image first, find edges, break it down into simpler shapes, etc. And then feed those reduced, simplified features into the network for training and processing. This process of manually identifying and extracting features\u200a\u2014\u200acalled feature engineering\u200a\u2014\u200ais a major bottleneck. It\u2019s difficult, time-consuming and requires domain specific knowledge and skill.\n\nYou can feed deep networks complex, raw inputs. And they will do the feature extraction for you, so you don\u2019t have to. Which features do they extract? Whatever they need, they learn that too! They figure it out based on the data. The deep learning model is a essentially a stack of parameterised, non-linear feature transformations that can be used to learn hierarchical representations. During training, each layer learns which transformation to apply\u200a\u2014\u200ai.e. it learns which feature to extract\u200a\u2014\u200aand how. As a result, the deep learning model stores a hierarchy of features with an increasing level of abstraction. It\u2019s pretty insane. (NB. This is why Deep Neural Networks are often referred to as \u2018black boxes\u2019. You just feed them inputs and they give you outputs. It\u2019s quite difficult to get an intuition of what\u2019s happening inside. That\u2019s why people are trying to invert them and visualise layers, more on that below).\n\nIt\u2019s worth pointing out a caveat to this: we don\u2019t need to manually handcraft the features, but instead the network architecture needs to be manually handcrafted depending on the type input data. So the network is very domain specific. E.g. usually for image classification very specific convolutional networks are used. Yann Lecun\u2019s LeNet was the original. AlexNet, GoogLeNet, VCGNet are some modern ones. For speech recognition completely different Recurrent Neural Networks are used. And there\u2019s loads of different options. This is quite a bottleneck, and we\u2019re still a long way from having a single, universal, general purpose learning algorithm. (Though Jeff Hawkins\u2019s team is working on it).\n\nAn important point to make clear is: An artificial neural network does not store any of the training data. The training data was only used to learn the weights for the connections, and once trained, the data is not necessary.\n\nImagine a complex neural network is trained on millions of images (e.g. http://image-net.org)\n\nThis network just stores the weights of the connections required to recognise images. In doing so it is storing abstract representations of various image features that it has learnt is required to identify the different categories.\n\nThis is worth re-iterating: E.g. for #deepdream, the original images that the network was trained on, is over 1,200,000 MB (1.2 TB) whereas the trained network itself is only ~50 MB\u200a\u2014\u200aand that\u2019s all you need to make predictions.\n\nNormally you feed data into the input layers of a neural network, that data is fed through the network being processed, and results come out of the output layer.\n\nThere is a recently discovered process called \u2018inverting the network\u2019.\n\nIn this case you are effectively asking the network \u201cWhat type of input do you need, to give this particular output\u201d. Of course the network doesn't explicitly know that. But there are ways of manipulating an input image, such that we get the desired results. This is effectively running the network backwards.\n\nA very crude way of putting this is you give the network a completely random picture that looks nothing like a cat and you ask it \u201cdoes this look like a cat?\u201d, the network says \u201cno\u201d. You make a few random changes and ask \u201cwhat about this?\u201d, and the network says \u201cno\u201d. And you keep repeating. If the network says \u201cyea, maybe that looks a bit more like a cat\u201d you say \u201caha! ok so I\u2019ll make more changes in that direction, how about this?\u201d. It\u2019s actually not exactly like that, but you get the idea. And you can see why it takes so long.\n\nThe key thing is, the network doesn't actually know what a cat is, it only recognizes one when it sees it.\n\nThe above technique can be applied to not only end results. I.e. \u201cdoes this input look like a cat\u201d. But also intermediary (hidden) layers, by looking for inputs which maximize activity on specific neurons. So you can pick a neuron in the network, and evolve the input such that you get maximum activity on that particular neuron. Then effectively we are finding what that neuron responds to, i.e. what it represents. (NB. This is similar to how neuroscientists figure out which neurons correspond to which types of inputs. e.g. Feeding specific images to the eye and measuring what happens in the brain, they can see how some neurons respond to diagonal lines, or vertical lines, or to light then dark, or dark then light, horizontal movement, vertical movement etc.)\n\nOne interesting feature, is that the lower layers (close to the inputs) respond (i.e. represent) low-level, abstract features. Like corners, edges, oriented lines etc. Moving up the layers represent higher level, more defined features. This is remarkably analogous to how information is thought to be processed in our own brain (and the mammalian cerebral cortex). A hierarchy of features, starting at low-level abstract features close to the input, and layers building on top, establishing higher and higher level representation with each layer. (NB. Of course it\u2019s way more complex in the brain, and no one really knows absolutely for sure, but most accepted theories point this way).", 
        "title": "Background info for \u201c#Deepdream is blowing my mind\u201d"
    }
]