[
    {
        "url": "https://medium.com/@majortal/learning-common-sense-170b367dcbb6?source=tag_archive---------0----------------", 
        "text": "It\u2019s hard for me to describe how frustrated I am by my inability to SOLVE the Winograd Schema Challenge. The \u201cTest\u201d is a list of multiple choice questions which are super easy for us humans to solve, but nearly impossible for the state of the art computers to resolve. This questions set is often proposed as a replacement for the more famous Turing Test, and for good reason. We want our computers to achieve true intelligence, not fake human behavior.\n\nHere is an example from the test, which is named after this guy: https://en.wikipedia.org/wiki/Terry_Winograd\n\nThere is nothing in the WORDS of the sentence to tell us the answer. There is nothing in the STRUCTURE of the sentence that can give us the answer. The answer emerges only when we apply the magic ingredient called \u201cCommon Sense\u201d. And that annoys me to the point that I find it hard to focus on other things. Allow me to explain.\n\nI\u2019ve been doing Natural Language Understanding for the past ~8 years. The collection of algorithms fall into the category of Artificial Narrow Intelligence, or \u201cWeak AI\u201d. The system we have built acts intelligently for a specific domain. It has built in a bunch of \u201cBusiness Logic\u201d that acts as though the Bot has common sense, and for all practical purposes\u200a\u2014\u200ait does. We have everything from wild heuristics that just work to state of the art Deep Neural Networks that learn from the data with no feature engineering, but there is nothing in my arsenal that can solve an unseen Winograd.\n\nHere is another example:\n\nWhy is the answer so obvious to us and so hard for computers to resolve?\n\nAn interesting characteristic of many of these challenges is that replacing a single word changes the answer to the question. Consider this variant:\n\nObviously the answer is now reversed despite the fact that the sentence structure looks the same and I replaced one simple adjective used to describe the size of objects with another.\n\nBTW, this is a very simple sentence. Consider this variant:\n\nThis version has many more words to confuse the natural language processing, yet when I read the sentence, I have no problem VISUALIZING the situation and answering the question. And therein lies the rub: my algorithms do not know how to visualize these mini-stories.\n\nI have some thoughts on how to proceed, but caveat emptor\u200a\u2014\u200athese have not been validated yet on any data.\n\nThe Trophy problem hinges on our understanding of the \u201cfit in\u201d relationship. We all know that these relationships describes an inner object and a container. The container will not fit if it is too small or if the given object is to big. Simple, right? (check out RDF Schema as a knowledge representation data model).\n\nOOh\u200a\u2014\u200aall we need is a big dictionary of objects and relationships! We will then apply this magic thingy on the sentence and be able to resolve it! Well, it turns out we already tried that once and the approach did not work (check out Cyc).\n\nSee, nothing in natural language is as simple as it first appears. Consider:\n\nIs college the container? Was the mustache really that big? Obviously the \u201cFit In\u201d expression has hid a new trick up its sleeve, and there are more where this one came from. Here is another one:", 
        "title": "Learning Common Sense \u2013 Tal Weiss \u2013"
    }, 
    {
        "url": "https://medium.com/@mycampus.io/%EC%8A%AC%EB%A1%9C%EC%9A%B0%EC%BA%A0%ED%8D%BC%EC%8A%A4-%EA%B0%95%EC%9D%98-machine-learning-%EA%B0%9C%EC%9A%94-%EB%B0%8F-%EC%9D%B4%EB%A1%A0-6dff6eb2ecce?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "[\uc2ac\ub85c\uc6b0\ucea0\ud37c\uc2a4 \uc138\ubbf8\ub098] Machine Learning \uac1c\uc694 \ubc0f \uc774\ub860 \u2013 \ub9c8\uc774\ucea0\ud37c\uc2a4 \u2013"
    }, 
    {
        "url": "https://medium.com/@Zephyr_tq/ted-talks-our-approach-to-innovation-is-dead-wrong-by-diana-kander-14b4f431575a?source=tag_archive---------2----------------", 
        "text": "idea, business plan, investment, product and customers according to your plan. \u201cEvery one has a plan UNTIL they get punch on the FACE\u201d\n\nwe spent more time in planning and building rather than researching that whether our product is required in the market or not because human behavior is quite difficult to predict.\n\nvalue of adjustments is very important \u201cwe have to stop teaching innovators they have to be future tellers instead they have to teach them they have to be detectors real world in action\n\nwe are force to believe in facts rather than on fiction. we became so busy to try to think or act to think out of the box that we sometimes losses our common sense to use. We act more worse than a child in kinder garten as we feel so pressurize to behave like what we are suppose to do and for that we start working with initial/partial planning we became so busy in focusing for what is next rather than focusing on what to do next.", 
        "title": "TED Talks \u201cOur Approach to Innovation is dead wrong\u201d by Diana Kander"
    }
]