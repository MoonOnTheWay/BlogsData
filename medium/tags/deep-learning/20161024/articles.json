[
    {
        "url": "https://medium.com/@zuzoovn/top-down-learning-path-machine-learning-for-software-engineers-a3a58b5bcad3?source=tag_archive---------0----------------", 
        "text": "This is my multi-month study plan for going from mobile developer (self-taught, no CS degree) to machine learning engineer.\n\nMy main goal was to find an approach to studying Machine Learning that is mainly hands-on and abstracts most of the math for the beginner. This approach is unconventional because it\u2019s the top-down and results-first approach designed for software engineers.\n\nI\u2019m following this plan to prepare for my near future job: Machine learning engineer. I\u2019ve been building the native mobile application (Android/iOS/Blackberry) since 2011. I have a Software Engineering degree, not a Computer Science degree. I have itty bitty of basic knowledge about: Calculus, Linear Algebra, Discrete Mathematics, Probability & Statistics at university. Think about my interest in machine learning:\n\nI find myself in times of trouble.\n\nAFAIK, There are two sides to machine learning:\n\nI think the best way for practice-focused methodology is something like \u2018practice\u200a\u2014\u200alearning\u200a\u2014\u200apractice\u2019, that means where students first come with some existing projects with problems and solutions (practice) to get familiar with traditional methods in the area and perhaps also with their methodology. After practicing with some elementary experiences, they can go into the books and study the underlying theory, which serves to guide their future advanced practice and will enhance their toolbox of solving practical problems. Studying theory also further improves their understanding on the elementary experiences, and will help them acquire advanced experiences more quickly.\n\nIt\u2019s a long plan. It\u2019s going to take me years. If you are familiar with a lot of this already it will take you a lot less time.", 
        "title": "Top-down learning path: Machine Learning for Software Engineers"
    }, 
    {
        "url": "https://medium.com/udacity/this-week-in-machine-learning-21-october-2016-8181a9a2351b?source=tag_archive---------1----------------", 
        "text": "This week\u2019s top Machine Learning stories, including a rise in freelance AI gigs, new ways to tackle wage gap issues, and more!\n\nMachine Learning is one of the most exciting fields in the world. Every week we discover something new, something amazing, something revolutionary. It\u2019s incredible, but it can also be overwhelming. That\u2019s why we created This Week in Machine Learning! Each week we publish a curated list of Machine Learning stories as a resource to help you keep pace with all these exciting developments. New posts will be published here first, and previous posts are archived on the Udacity blog.\n\nWhether you\u2019re currently enrolled in our Machine Learning Nanodegree program, already working in the field, or just pursuing a burgeoning interest in the subject, there will always be something here to inspire you!\n\nPhysicist Stephen Hawking speaks at the opening of the Leverhulme Centre for the Future of Intelligence, a research hub for exploring humanity\u2019s future in the context of AI.\n\nGlassdoor, the popular workplace review site, leverages the data it has gathered to inform employees whether they are being paid fairly based on their skills and experience.\n\nThe trend toward machine learning reaches freelancers as Upwork notes a significant increase in the opportunities for freelance AI work.\n\nAt the ACM\u2019s conference on Multimedia in Amsterdam, deep learning is a focal point, from predicting the \u201cinterestingness\u201d of GIFs to detecting loiterers.\n\nSift Science and other startups tackle internal expense fraud, leveraging machine learning to detect and predict instances of fraud in enterprise commerce technology.", 
        "title": "This Week in Machine Learning, 21 October 2016 \u2013 Udacity Inc \u2013"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/10-lessons-learned-from-building-deep-learning-systems-d611ab16ef66?source=tag_archive---------2----------------", 
        "text": "Deep Learning is a sub-field of Machine Learning that has its own peculiar ways of doing things. Here are 10 lessons that we\u2019ve uncovered while building Deep Learning systems. These lessons are a bit general, although they do focus on applying Deep Learning in a area that involves structured and unstructured data.\n\nThe one tried and true way to improve accuracy is to have more networks perform the inferencing and combining the results. In fact, techniques like DropOut is a means of creating \u201cImplicit Ensembles\u201d were multiple subsets of superimposed networks cooperate using shared weights.\n\nThe current state of Deep Learning is that it works well only in a supervised context. The rule of thumb is around 1,000 samples per rule. So if you are given a problem where you don\u2019t have enough data to train with, try considering an intermediate problem that does have more data and then run a simpler algorithm with the results from the intermediate problem.\n\nNot all data is nicely curated and labeled for machine learning. Many times you have data that are weakly tagged. If you can join data from disparate sources to achieve a weakly labeled set, then this approach works surprisingly well. The most well known example is Word2Vec where you train for word understanding based on the words that happen to be in proximity with other words.\n\nOne of the spectacular capabilities of Deep Learning networks is that bootstrapping from an existing pre-trained network and using it to train into a new domain works surprisingly well.\n\nData usually have meaning that a human may be aware of that a machine can likely never discover. One simple example is a time feature. From the perspective of a human the day of the week, whether this is a holiday or not or the time of the day may be important attributes, however a Deep Learning system may never be able to surface that if all its given are seconds since Unix epoch.\n\nL1 and L2 regularizations are not the only regularizations that are out there. Explore the different kinds and perhaps look at different regularizations per layer.\n\nThere are multiple techniques to initialize your network prior to training. In fact, you can get very far just training the last layer of a network with the previous layers being mostly random. Consider using this technique to speed up you Hyper-tuning explorations.\n\nA lot of researchers love to explore end-to-end deep learning research. Unfortunately, the most effective use of Deep Learning has been to couple it with out techniques. AlphaGo would not have been successful if Monte Carlo Tree Search was not employed. If you want to make an impact in the Academic community then End-to-end Deep Learning might be your gamble. However in a time constrained industrial environment that demands predictable results, then you best be more pragmatic.\n\nIf you can, try to avoid using multiple machines (with the exception of hyper-parameter tuning). Training on a single machine is the most cost effective way to proceed.\n\n10. Convolution Networks work pretty well even beyond Images\n\nConvolution Networks are clearly the most successful kind of network in the Deep Learning space. However, ConvNets are not only for Images, you can use them for other kinds of features (i.e. Voice, time series, text).\n\nThat\u2019s all I have for now. There certainly a lot more other lessons. Let me know if you stumble on others.\n\nYou can find more details of these individual lessons at http://www.deeplearningpatterns.com and Intuition Machine.", 
        "title": "10 Lessons Learned from Building Deep Learning Systems"
    }, 
    {
        "url": "https://medium.com/@inginnovationstudio/how-a-small-fintech-startup-got-to-trial-their-neural-network-at-ing-e79882152cfa?source=tag_archive---------3----------------", 
        "text": "Giacomo Barigazzi knows his startup can help banks use their data better, yet they found it is incredibly hard to get banks to try out their product.\n\nAxyon AI has joined the Innovation Studio in September, and we are excited that they are about to create a proof of concept of their product for ING Wholesale Banking.\n\nWe would like to share a bit more of how they got here, so we have asked co-founder Giacomo to tell us about the origin story of neural networks and deep learning start-up Axyon AI.\n\nThe friends wanted to build something more technologically advanced and had the profit to invest in the research. The consultancy started thinking about how they could apply neural networks in the world of finance.\n\nWhen he had the chance to work with his friends everything fell into place. Giacomo, who always knew he wanted his own company, became co-founder.\n\nThe promising start-up set out to build neural networks that could make useful predictions in the financial world, as opposed to the traditionally used statistical models. It took two years until there were results good enough to indicate they were on to something.\n\nAxyon AI\u2019s predictive tool proved to provide predictions so good that investment from a hedge fund enabled them to develop it further.\n\nBanks know that neural networks are usually better than the predictive models they are using now, but find them hard to use. The Axyon AI platform lets banks analyse the data they have and get an accurate predictive model as an output.\n\nGetting a B2B product to market is always hard, you have to deal with long sales cycles, complex decision making and bureaucracy. Getting banks to consider using a new piece of software or a predictive model, let alone try it out is especially difficult.\n\nTo solve this problem, Giacomo and Co-Founder Daniele started speaking at conferences about their product. Figuring out whom to talk to about trialling their product was a challenge.\n\nStuck, they decided that they needed help.\n\nThere are plenty of accelerators in the start-up landscape. Daniele and Giacomo decided they were going to apply to various banks\u2019 accelerator programs, hoping that banks at least would be able to teach them how to sell to banks. Not only that, but they could also use some financial expertise banks have plenty of in-house.\n\nAxyon AI joined the ING Innovation Studio in September, product in hand. To develop their product further and, more importantly, find a use case.\n\nThe Innovation Studio was able to connect Axyon AI to ING\u2019s data-analysts. They, in turn, suggested that the Axyon AI platform could work great in the Wholesale Banking department of the bank.", 
        "title": "How A Small Fintech Startup Got to Trial Their Neural Network at ING"
    }, 
    {
        "url": "https://medium.com/@nonmult/%E5%BD%93%E3%81%A6%E3%82%8B-%E3%81%A8-%E5%88%86%E3%81%8B%E3%82%8B-ec610e1e6ed0?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u300c\u5f53\u3066\u308b\u300d\u3068\u300c\u5206\u304b\u308b\u300d \u2013 nonmult \u2013"
    }
]