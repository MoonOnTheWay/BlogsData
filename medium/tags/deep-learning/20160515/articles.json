[
    {
        "url": "https://medium.com/@tim_ng/review-deep-networks-with-stochastic-depth-51bd53acfe72?source=tag_archive---------0----------------", 
        "text": "Since the introduction of Residual Network in 2015 which won the ImageNet competition, it seems everybody are trying to incorporate the new architecture into their own work, and improved results are often guaranteed. To name a few, MSRA won the MS COCO 2015 object detection competition by using ResNet-101 along with Faster-RCNN, Google tried to incorporate the residual block their own inception layers to generate a so-called Inception-Resnet\u00a0, which leads to better convergence and performance lift. More works such as Renset-in-Resnet, resent with ELU, SqueezeNet with complex bypass, etc. In my opinion, most of these works directly borrows the strength of residual module, without inputting any new ideas or exploring new intrinsic possibility of the skip path in residual unit.\n\nOn the contrary, the paper Deep Networks with Stochastic Depth published on the April fool\u2019s day really caught my eyes. It\u2019s not only not foolish but could be a ground-breaking new practice for training deep neural network. The idea is so simple in the sense that the title is self-contained. It leverages the skip path in the residual architecture to remedy the problem of performance drop when training very deep neural network, leading to state-of-the-art result on augmented CIFAR10 and CIFAR100 datasets. The author did not train on ImageNet which I think will be more persuasive if done.\n\nThe idea is to randomly bypass the convolution layers and non-linear activation layers in the l-th residual block (top path below) but let through only the identity of the raw feature in skip path.\n\nThere is also a survival probability assigned for each block which defines the probability to use that block. The probability will be different for each mini-batch. The author has tried different distribution on the survival probability and found that linear decay performs the best.\n\nAn intuitive explanation of why this trick work is that it reduces overfitting. Dropout is a successful trick to tackle such problem by randomly masking some nodes during training, which intuitively prevent the co-adaption of neighboring units. DropConnect is a similar trick which mask the weights. We may say that Dropout and DropConnect remedies co-adaption within layer, and Stochastic Depth remedy co-adaption between layers. We could simply regard it as an implicit ensemble of network with various depths being trained. A strong evidence is that the author applied stochastic depth on the 1202-layer ResNet which leads to smaller test error in contrast to the original overfitted setting.\n\nThis is one of those simple ideas that you thought you could have came up with. The are people viewing it as simply beautiful while some regard it as too \u201cengineering\u201d. In my opinion, this is the era of great exploration in deep learning. Tables and graphs are more common that theorems and proofs. Any engineering idea is precious and should provide insights to this great journey of artificial intelligence research. It will be no surprise to see deeper and stronger neural network to appear with beautiful ideas like this along with more powerful hardware.", 
        "title": "Review: Deep Networks with Stochastic Depth \u2013 Tim NG \u2013"
    }, 
    {
        "url": "https://medium.com/perspicacious-perspectives/what-you-are-asking-for-is-an-enlightened-relationship-38562dd3ef5a?source=tag_archive---------1----------------", 
        "text": "I have this with my significant other, for 20 years. It wasn\u2019t always as enlightened, since she was somewhat resistant. I learned about how to be with someone taking 100% responsibility for how I feel from my guru and other teachers I have had. I teach this. My partner had stuff she has taught me, so our relationship has been nicely balanced.\n\nThe prerequisite is an open mind willing to learn, because all relationships are about learning lessons we were never taught and can only learn in relationships. Ego trips, ego beliefs, ego anything is not an enlightened consciousness and it is not open by definition.\n\nTo have an enlightened relationaship, both must be evolved or one is evolved and the other receptive to evolving. Relationships are changing, growing, or evolving or they are a misery. And we can also learn from misery; pain is a great teacher.\n\nI am open to counseling anyone about their relationship stuff; nothing is more fascinating and rewarding for me to deal with. It is always about love; that said, love has very little to do with another person.", 
        "title": "What you are asking for is an enlightened relationship"
    }
]