[
    {
        "url": "https://medium.com/deep-learning-101/how-to-generate-a-video-of-a-neural-network-learning-in-python-62f5c520e85c?source=tag_archive---------0----------------", 
        "text": "As part of my quest to learn about AI, I generated a video of a neural network learning.\n\nMany of the examples on the Internet use matrices (grids of numbers) to represent a neural network. This method is favoured, because it is:\n\nHowever, it\u2019s difficult to understand what is happening. From a learning perspective, being able to visually see a neural network is hugely beneficial.\n\nThe video you are about to see, shows a neural network trying to solve this pattern. Can you work it out?\n\nIt\u2019s the same problem I posed in my previous blog post. The trick is to notice that the third column is irrelevant, but the first two columns exhibit the behaviour of a XOR gate. If either the first column or the second column is 1, then the output is 1. However, if both columns are 0 or both columns are 1, then the output is 0.\n\nSo the correct answer is 0.\n\nOur neural network will cycle through these 7 examples, 60,000 times. To speed up the video, I will only show you 13 of these cycles, pausing for a second on each frame. Why the number 13? It ensures the video lasts exactly as long as the music.\n\nEach time she considers an example in the training set, you will see her think (you will see her neurons and her synaptic connections glow). She will then calculate the error (the difference between the output and the desired output). She will then propagate this error backwards, adjusting her synaptic connections.\n\nGreen synaptic connections represent positive weights (a signal flowing through this synapse will excite the next neuron to fire). Red synaptic connections represent negative weights (a signal flowing through this synapse will inhibit the next neuron from firing). Thicker synapses represent stronger connections (larger weights).\n\nIn the beginning, her synaptic weights are randomly assigned. Notice how some synapses are green (positive) and others are red (negative). If these synapses turn out to be beneficial in calculating the right answer, she will strengthen them over time. However, if they are unhelpful, these synapses will wither. It\u2019s even possible for a synapse which was originally positive to become negative, and vice versa. An example of this, is the first synapse into the output neuron\u200a\u2014\u200aearly on in the video it turns from red to green. In the beginning her brain looks like this:\n\nDid you notice that all her neurons are dark? This is because she isn\u2019t currently thinking about anything. The numbers to the right of each neuron, represent the level of neural activity and vary between 0 and 1.\n\nOk. Now she is going to think about the pattern we saw earlier. Watch the video carefully to see her synapses grow thicker as she learns.\n\nDid you notice how I slowed the video down at the beginning, by skipping only a small number of cycles? When I first shot the video, I didn\u2019t do this. However, I realised that learning is subject to the \u2018Law of diminishing returns\u2019. The neural network changes more rapidly during the initial stage of training, which is why I slowed this bit down.\n\nNow that she has learned about the pattern using the 7 examples in the training set, let\u2019s examine her brain again. Do you see how she has strengthened some of her synapses, at the expense of others? For instance, do you remember how the third column in the training set is irrelevant in determining the answer? You can see she has discovered this, because the synapses coming out of her third input neuron have almost withered away, relative to the others.\n\nLet\u2019s give her a new situation [1, 1, 0] to think about. You can see her neural pathways light up.\n\nShe has estimated 0.01. The correct answer is 0. So she was very close!\n\nPretty cool. Traditional computer programs can\u2019t learn. But neural networks can learn and adapt to new situations. Just like the human mind!\n\nHow did I do it? I used the Python library matplotlib, which provides methods for drawing and animation. I created the glow effects using alpha transparency.\n\nYou can view my full source code here:\n\nIf you enjoyed reading this article, please click the heart icon to \u2018Recommend\u2019.", 
        "title": "Video of a neural network learning \u2013 Deep Learning 101 \u2013"
    }, 
    {
        "url": "https://medium.com/s-c-a-l-e/how-baidu-mastered-mandarin-with-deep-learning-and-lots-of-data-1d94032564a5?source=tag_archive---------1----------------", 
        "text": "SCALE: How accurate is Deep Speech at translating Mandarin?\n\nAWNI HANNUN: It has a 6 percent character error rate, which essentially means that it gets wrong 6 out of 100 characters. To put that in context, this is in my opinion\u200a\u2014\u200aand to the best of our lab\u2019s knowledge\u200a\u2014\u200athe best system at transcribing Mandarin voice queries in the world.\n\nIn fact, we ran an experiment where we had a few people at the lab who speak Chinese transcribe some of the examples that we were testing the system on. It turned out that our system was better at transcribing examples than they were\u200a\u2014\u200aif we restricted it to transcribing without the help of the internet and such things.\n\nWhat is it about Mandarin that makes it such a challenge compared with other languages?\n\nThere are a couple of differences with Mandarin that made us think it would be very difficult to have our English speech system work well with it. One is that it\u2019s a tonal language, so when you say a word in a different pitch, it changes the meaning of the word, which is definitely not the case in English. In traditional speech recognition, it\u2019s actually a desirable property that there is some pitch invariance, which essentially means that it tries to ignore pitch when it does the transcription. So you have to change a bunch of things to get a system to work with Mandarin, or any Chinese for that matter.\n\nHowever, for us, it was not the case that we had to change a whole bunch of things, because our pipeline is much simpler than the traditional speech pipeline. We don\u2019t do a whole lot of pre-processing on the audio in order to make it pitch-invariant, but rather just let the model learn what\u2019s relevant from the data to most effectively transcribe it properly. It was actually able to do that fine in Mandarin without having to change the input.\n\nThe other thing that is very different about Chinese\u200a\u2014\u200aMandarin, in this case\u200a\u2014\u200ais the character set. The English alphabet is 26 letters, whereas in Chinese it\u2019s something like 80,000 different characters. Our system directly outputs a character at a time as it\u2019s building its transcription, so we speculated it would be very challenging to have to do that on 80,000 characters at each step versus 26. That\u2019s a challenge we were able to overcome just by using characters that people commonly say, which is a smaller subset.\n\nBaidu has been handling a fairly high volume of voice searches for a while now. How is the Deep Speech system better than the previous system for handling queries in Mandarin?\n\nBaidu has a very active system for voice search in Mandarin, and it works pretty well. I think in terms of total query activity, it\u2019s still a relatively small percentage. We want to make that share larger, or at least enable people to use it more by making the accuracy of the system better.\n\nCan you describe the difference between a search-based system like Deep Speech and something like Microsoft\u2019s Skype Translate, which is also based on deep learning?\n\nTypically, the way it\u2019s done is there are three modules in the pipeline. The first is a speech-transcription module, the second is the machine-translation module and the third would be the speech-synthesis module. What we\u2019re talking about, specifically, is just the speech-transcription module, and I\u2019m sure Microsoft has one as part of Skype Translate.\n\nOur system is different than that system in that it\u2019s more what we call end-to-end. Rather than having a lot of human-engineered components that have been developed over decades of speech research\u200a\u2014\u200aby looking at the system and saying what what features are important or which phonemes the model should predict\u200a\u2014\u200awe just have some input data, which is an audio\u00a0.WAV file on which we do very little pre-processing. And then we have a big, deep neural network that outputs directly to characters. We give it enough data that it\u2019s able to learn what\u2019s relevant from the input to correctly transcribe the output, with as little human intervention as possible.\n\nOne thing that\u2019s pleasantly surprising to us is that we had to do very little changing to it\u200a\u2014\u200aother than scaling it and giving it the right data\u200a\u2014\u200ato make this system we showed in December that worked really well on English work remarkably well in Chinese, as well.\n\nWhat\u2019s the usual timeline to get this type of system from R&D into production?\n\nIt\u2019s not an easy process, but I think it\u2019s easier than the process of getting a model to be very accurate\u200a\u2014\u200ain the sense that it\u2019s more of an engineering problem than a research problem. We\u2019re actively working on that now, and I\u2019m hopeful our research system will be in production in the near term.\n\nBaidu has plans\u200a\u2014\u200aand products\u200a\u2014\u200ain other areas, including wearables and other embedded forms of speech recognition. Does the work you\u2019re doing on search relate to these other initiatives?\n\nWe want to build a speech system that can be used as the interface to any smart device, not just voice search. It turns out that voice search is a very important part of Baidu\u2019s ecosystem, so that\u2019s one place we can have a lot of impact right now.", 
        "title": "Baidu explains how it\u2019s mastering Mandarin with deep learning"
    }
]