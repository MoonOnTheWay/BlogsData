[
    {
        "url": "https://medium.com/neon-blog/teaching-a-neural-net-to-be-interesting-5c96f8689024?source=tag_archive---------0----------------", 
        "text": "If you\u2019re reading this, doubtless you\u2019ve seen this post on Google\u2019s new YouTube thumbnail selection model. It came across my Twitter feed, followed by Slack, and even in texts from friends who know I am an engineer at Neon. \u201cLook at what Google does!\u201d they told me, breathlessly. I had one thought: it was pretty damn cool. In light of this, I figured it was time to share a bit about how Neon does what we do.\n\nGoogle and Neon, and all companies who use machine learning and deep learning, need a model (to do the predicting) and an objective function (to tell the model how it\u2019s performing). In the case of Neon and Google, these are coupled with some fancy calculus to ensure that the model is always improving.\n\nThough our methods are broadly similar, the interesting bit is that Neon and Google have different objective functions. Google wants to take a video and predict which frames are the most like those selected by a video uploader. Neon takes a different tack and, given images, predicts which will be perceived as the most interesting by a potential viewer.\n\nPredicting which images are the \u201cmost interesting\u201d sounds pretty simple, but it\u2019s actually incredibly complex. How can you even measure interestingness? You might be inclined to think, \u201csimple\u200a\u2014\u200ahave a bunch of people rate a bunch of images on a scale of 1 to 10.\u201d There\u2019s a problem, though. Let\u2019s imagine someone is rating some images.\n\nClearly, if doughnuts are a 9 out of 10, Beyonc\u00e9 is something like 42 out of 10. Doughnuts are definitely awesome, but they\u2019re way less awesome than Beyonc\u00e9.\n\nThis is an exaggeration, but it illustrates the problem well: when people are asked to sequentially rate images (or, anything really), their ratings are biased and tend to depend on the order in which things are presented. This is a problem. We want the truth!\n\nI won\u2019t bore you with the details, but the short answer is that we use math to directly measure how good the ranked images are.\n\nThe next thing to figure out is what a deep neural network model needs to know in order to predict how interesting an image is. Believe it or not, deep neural nets need to be trained to do things, just like kids need to be trained not to eat things they find on the ground. Again, I won\u2019t bore you with the details (besides, they\u2019re top secret), but in short, it involves image rankings plus what we know of how the brain perceives.\n\nLet\u2019s go back a minute. That part was important. We\u2019re not training our models to find images it thinks people will select as a thumbnail. We\u2019re actually trying to figure out which images people will perceive as the most interesting.\n\nI had another thought while reading the Google post. Instead of automating what humans already do (in this case, picking thumbnails), Google is actually just normalizing what humans already do. The model doesn\u2019t know anything about the aesthetic or novelty value of the images (since it\u2019s not trained to know that). Instead, it\u2019s doing something kind of weird. Think about it: people pick YouTube thumbnails based on the message they think the thumbnails are going to send. Thus, Google\u2019s model isn\u2019t trying to predict how good it thinks an image is. It\u2019s trying to predict how much the average human being thinks other people think it\u2019s good.\n\nIn fact, as time passes and the model picks more thumbnails, it could begin shaping what people think content should be, based on the statistical average of human behavior. Thumbnails will begin to converge on a set of common tropes\u200a\u2014\u200anot because they\u2019re good tropes, and not because they look particularly nice, but because the model creates homogeneity. It reduces variance. It\u2019s like The Lottery, but with thumbnails!\n\nOf course, I\u2019m not seriously comparing Google to a \u201cchilling tale of conformity gone mad.\u201d But it\u2019s interesting to think about. And Google is certainly not the only one who is trying to automate this kind of thing. What worries me is that models like these predict how the average person acts, rather than model how we perceive.", 
        "title": "Teaching a neural net to be interesting \u2013 Neon Open \u2013"
    }
]