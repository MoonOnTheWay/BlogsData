[
    {
        "url": "https://medium.com/@ilblackdragon/tensorflow-tutorial-part-2-9ffe47049c92?source=tag_archive---------0----------------", 
        "text": "In the previous Part 1 of this tutorial, I introduced a bit of TensorFlow and Scikit Flow and showed how to build a simple logistic regression model on Titanic dataset.\n\nIn this part let\u2019s go deeper and try multi-layer fully connected neural networks, writing your custom model to plug into the Scikit Flow and top it with trying out convolutional networks.\n\nOf course, there is not much point of yet another linear/logistic regression framework. An idea behind TensorFlow (and many other deep learning frameworks) is to be able to connect differentiable parts of the model together and optimize them given the same cost (or loss) function.\n\nScikit Flow already implements a convenient wrapper around TensorFlow API for creating many layers of fully connected units, so it\u2019s simple to start with deep model by just swapping classifier in our previous model to the TensorFlowDNNClassifier and specify hidden units per layer:\n\nThis will create 3 layers of fully connected units with 10, 20 and 10 hidden units respectively, with default Rectified linear unit activations. We will be able to customize this setup in the next part.\n\nI didn\u2019t play much with hyperparameters, but previous DNN model actually yielded worse accuracy then a logistic regression. We can explore if this is due to overfitting on under-fitting in a separate post.\n\nFor the sake of this example, I though want to show how to switch to the custom model where you can have more control.\n\nThis model is very similar to the previous one, but we changed the activation function from a rectified linear unit to a hyperbolic tangent (rectified linear unit and hyperbolic tangent are most popular activation functions for neural networks).\n\nAs you can see, creating a custom model is as easy as writing a function, that takes X and y inputs (which are Tensors) and returns two tensors: predictions and loss. This is where you can start learning TensorFlow APIs to create parts of sub-graph.\n\nWhat kind of TensorFlow tutorial would this be without an example of digit recognition?\u00a0:)\n\nThis is just an example how you can try different types of datasets and models, not limiting to only floating number features. Here, we take digits dataset and write a custom model:\n\nWe\u2019ve created conv_model function, that given tensor X and y, runs 2D convolutional layer with the most simple max pooling\u200a\u2014\u200ajust maximum. The result is passed as features to skflow.models.logistic_regression, which handles classification to required number of classes by attaching softmax over classes and computing cross entropy loss.\n\nIt\u2019s easy now to modify this code to add as many layers as you want (some of the state-of-the-art image recognition models are hundred+ layers of convolutions, max pooling, dropout and etc).\n\nThe Part 3 is expanding the model for Titanic dataset with handling categorical variables.\n\nPS. Thanks to Vlad Frolov for helping with missing articles and pointing mistakes in the draft\u00a0:)", 
        "title": "Tensorflow Tutorial \u2014 Part 2 \u2013 Illia Polosukhin \u2013"
    }, 
    {
        "url": "https://medium.com/data-science-analytics/black-friday-data-science-hackathon-4172a0554944?source=tag_archive---------1----------------", 
        "text": "Analytics Vidhya, a community of analytics professionals and data scientists hosted an online data analytics hackathon on 20\u201322nd of November, 2015. After my very bad performance on their previous hackathon, I was geared up to do well, try new things and learn during the process.\n\nThe challenge was to predict purchase prices of various products purchased by customers based on historical purchase patterns. The data contained features like age, gender, marital status, categories of products purchased, city demographics etc.\n\nAfter working on a host of classification problems, a regression problem this time was very refreshing for me and I wanted to make sure I try different approaches to find out various intricacies of how to effectively handle regression problems.\n\nAs a first step, I did some exploratory analysis work, trying to identify if there are any outliers and any missing values. The dataset size was pretty big as well. This is when I realized that it is a very nice dataset (hunch). No outliers. Only a couple of columns having missing values and a couple of submissions showed nice correlation between Cross-validation scores and Leaderboard scores. My hunch was validated!\n\nAs usual, I started with linear models and moved soon to Random Forests and XGBoost. One of the unique aspects of this competition was apart from XGBoost, no other model was doing well. Not even marginally close. I guess this happened mainly due to the large number of user_ids and product_ids in the dataset. Simpler models were unable to capture the non-linearity in the dataset.\n\nEven after extensive tuning of the model hyperparameters, I couldn\u2019t break the 2470 RMSE score barrier which was 8th place on leaderboard. My XGBoost was giving me around a 4-fold cross-validation score of 2477 RMSE. I thought the guys at the top were using something more (than just tuning the model) to reach lower RMSE. You could see that (1,2), (3,4) and (5,6,7,8,9,10) form clusters on leaderboard.\n\nSo, we were clearly missing out on some information. So, what did they do? I mentioned it below. Keep reading!\n\nAfter trying many more models (explained below), I just averaged 5 XGBs to ensure seed stability and finished 9th on the leaderboard. Finally, capping the highest target to 99.9 percentile and replacing negative predictions to minimum target value in train set improved my score by ~1\u20132\u00a0.\n\nWell, I finished 10th and 9th on public and private leaderboards respectively out of 162 participants. It\u2019s a decent finish, but I think I could have done much better. In retrospect, I am happy that I tried new approaches and it was a weekend well spent.\n\nThank you Analytics Vidhya for conducting this hackathon. Come up with nice datasets and we will never disappoint you!\u00a0;-)\n\nThe datasets are available at: https://drive.google.com/folderview?id=0B4-c0K3SnXFcbWFRbjNUaWl1ekU&usp=sharing\u200a\u2014\u200ago ahead and give it a shot if you are interested in exploring the dataset.", 
        "title": "Black Friday Data Science Hackathon \u2013 Data Science | Analytics \u2013"
    }
]