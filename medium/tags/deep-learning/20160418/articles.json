[
    {
        "url": "https://medium.com/all-of-us-are-belong-to-machines/the-gentlest-introduction-to-tensorflow-248dc871a224?source=tag_archive---------0----------------", 
        "text": "Summary: Tensorflow (TF) is Google\u2019s attempt to put the power of Deep Learning into the hands of developers around the world. It comes with a beginner & an advanced tutorial, as well as a course on Udacity. However, the materials attempt to introduce both ML and TF concurrently to solve a multi-feature problem\u200a\u2014\u200acharacter recognition, which albeit interesting, unnecessarily convolutes understanding. In this series of articles, we present the gentlest introduction to TF that starts off by showing how to do linear regression for a single feature problem, and expand from there.\n\nThis is part of a series:\n\nWe are going to solve an overly simple, and unrealistic problem, which has the upside of making understanding the concepts of ML and TF easy. We want to predict a single scalar outcome, house price (in $) based on a single feature, house size (in square meters, sqm). This eradicates the need to handle multi-dimensional data, enabling us to focus solely on defining a model, implementing, and training it in TF.\n\nWe start with a set of data points that we have collected (chart below), each representing the relationship between two values \u2014an outcome (house price) and the influencing feature (house size).\n\nHowever, we cannot predict values for features that we don\u2019t have data points for (chart below)\n\nWe can use ML to discover the relationship (the \u2018best-fit prediction line\u2019 in the chart below), such that given a feature value that is not part of the data points, we can predict the outcome accurately (the intersection between the feature value and the prediction line.\n\nTo do prediction using ML, we need to choose a model that can best-fit the data that we have collected.\n\nWe can choose a linear (straight line) model, and tweak it to match the data points by changing its steepness/gradient and position.\n\nWe can also choose an exponential (curve) model, and tweak it to match the same set of data points by changing its curvature and position.\n\nTo compare which model is a better-fit more rigorously, we define best-fit mathematically as a cost function that we need to minimize. An example of a cost function can simply be the absolute sum of the differences between the actual outcome represented by each data point, and the prediction of the outcome (the vertical projection of the actual outcome onto the best-fit line). Graphically the cost is depicted by the sum of the length of the blue lines in the chart below.\n\nNOTE: More accurately the cost function is often the squared of the difference between actual and predicted outcome, because the difference can sometimes can be negative; this is also known as min least-squared.\n\nIn the spirit of keeping things simple, we will model our data points using a linear model. A linear model is represented mathematically as:\n\nTo tweak the model to best fit our data points, we can:\n\nBy going through many values of W, b, we can eventually find a best-fit linear model that minimizes the cost function. Besides randomly trying different values, is there a better way to explore the W, b values quickly?\n\nIf you are on an expansive plateau in the mountains, when trying to descent to the lowest point, your viewpoint looks like this.\n\nThe direction of descent is not obvious! The best way to descend is then to perform gradient descent:\n\nMinimizing the cost function is similar because, the cost function is undulating like the mountains (chart below), and we are trying to find the minimum point, which we can similarly achieve through gradient descent.\n\nWith the concepts of linear model, cost function, and gradient descent in hand, we are ready to use TF.\n\nPlaceholder: Represent an entry point for us to feed actual data values into the model when performing gradient descent, i.e., the house sizes (x), and the house prices (y_).\n\nVariable: Represent a variable that we are trying to find \u2018good\u2019 values that minimizes the cost function, e.g., W, and b.\n\nThe linear model (y = W.x + b) in TF then becomes:\n\nSimilarly to feed actual house prices (y_) of the data points into the model, we create a placeholder.\n\nOur cost function of least-min squared becomes:\n\nSince we do not have actual data points for house price (y_), house size (x), we generate them.\n\nWe set the house price (ys) to always be 2 times the house size (xs) for simplicity.\n\nWith the linear model, cost function, and data, we can start performing gradient descent to minimize the cost function, to obtain the \u2018good\u2019 values for W, b.\n\nThe 0.00001 is the size of the \u2018step\u2019 we take in the direction of steepest gradient each time perform a training step; this is also called learning rate.\n\nTraining involves performing gradient descent a pre-determined number of times or until the cost is below a pre-determined threshold.\n\nAll variables needs to be initialize at the start of training otherwise they may hold remnant values from previous execution.\n\nAlthough TF is a python library, and python is an interpreted language, TF operations, by default are NOT interpreted for performance reasons. Thus the init above is NOT executed. Instead TF executes within a session; create a session (sess) and then execute stuff using sess.run().\n\nSimilarly we execute the train_step above within a loop by calling it within sess.run().\n\nThe reason why you need to feed actual data points into feed, which is composed of x, y_ is that TF resolves the train_step into its dependencies:\n\nAt the bottom of the dependencies are the placeholders x, y_; and as we learned earlier tf.placeholders are used to indicate where we will feed actual data point values house price (y_), and house size (x).\n\nThe print statement in the loop will show how TF learn the \u2018good\u2019 values for W, and b over each iteration.\n\nWe have learned about Machine Learning in its simplest form; predict an outcome from a single feature. We chose a linear model (for simplicity) to fit our data points, define a cost function to represent best-fit, and train our model by repeatedly tweaking its gradient variable, W, and position variable b to minimize the cost function.\n\nIn the next article, we will:", 
        "title": "Gentlest Introduction to Tensorflow #1 \u2013 All of Us are Belong to Machines \u2013"
    }
]