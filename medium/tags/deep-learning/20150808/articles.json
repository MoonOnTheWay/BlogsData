[
    {
        "url": "https://engineering.huew.co/deep-learning-and-beyond-visual-recommendations-part-i-6a2342ab3e56?source=tag_archive---------0----------------", 
        "text": "At Huew, we truly believe that \u201ca picture is worth a thousand words\u201d. Decoding a picture is not just about visual pattern extraction but describing the picture in words: how many and what kind of objects exist in the picture? where do they appear in the picture and what is their mutual relationship? can we describe these objects themselves using fine-grained attributes?\n\nAs mentioned in our previous posts, we are working on artificial intelligence/deep learning based products to \u201cdescribe pictures\u201d for the market where it matters the most in our everyday lives: online-retail marketplaces. Everyday, millions of listings get created on tens of online-retail marketplaces and e-commerce websites, and beyond any doubt, photos for each of these listing play a key role in making these listings discover-able and likable.\n\nAt high level, based on the nature of the problems and innovation in the solutions for online-retailers, our \u201cphoto analytics\u201d product can be classified into two segments: seller-side and buyer-side. In this post, we give you an overview of problems and solutions in these two segments.\n\nSeller-side presents much more harder and most important photo-analytics problems for online-retailers than buyer-side. This is because these problems not only pollute the platform (e.g., duplicate listing) but also affect quality of experience of buyers (e.g., no relevant response to search queries). We tackle two main root-level problems on this front; cleaning up the listings and making listings/products discover-able.\n\nOn seller-side, we have (a) a highly scalable mechanism to detect duplicates across tens of millions of listings, and (b) a highly accurate set of deep neural network models to tag photos automatically before their boarding on the platform (these tags are not just restricted to determining color or shape of photos, for instance; we can determine if the fabric of a t-shirt is silky or not, or whether the t-shirt is a party-ware or a formal-ware! This is the power of our deep learning studio.)\n\nOur duplicate detection mechanism is super-fast yet accurate. Through extensive research, we have developed a state-of-the-art mechanism which combines hashing and deep learning to detect duplicates across tens of millions of photos within only a few tens of milliseconds. This can not only help online-retailers to detect duplicates within a listing but across entire the e-commerce platform. Furthermore, listings for the same product can be serialized based on the photo-duplicates; this can automatically maintain consistency of listings across the platform.\n\nUsing our deep learning studio, we have devised novel mechanisms to build custom deep neural network models in less than 48 hours. These models drastically reduce manual efforts to tag products with pertinent attributes. A key and competitive advantage of our platform is that for every product category, we can quickly generate a customized solution to detect attributes of products; for instance, we can tag a clothing with its fabric type and dressing style, we can tag a shoe with its brand and activity-fit, and we can tag a car with its model and its view in the picture.\n\nIn the second part of this post, we focus on buyer-side problems for online retailers. We describe yet another state-of-the-art feature of our product: automatically localizing and identifying multiple types of objects in an image. This feature is immensely useful to online retailers to link products on their website from any photo in the wild. It can further help online retailers to recommend visually similar items to their customers.", 
        "title": "Deep Learning and Beyond Visual Recommendations: Part I"
    }, 
    {
        "url": "https://engineering.huew.co/deep-learning-and-beyond-visual-recommendations-part-ii-6b2454cd1bb1?source=tag_archive---------1----------------", 
        "text": "In our previous post, we talked about seller-side pain points for online retailers. In this post, we briefly talk about how innovations developed at Huew can bridge the gap between a retail-platform and its buyers.\n\nOne of the key problems in online retail is the \u2018search problem\u2019. How do we search for a product which we have \u2018seen\u2019 somewhere else? \u2018A picture is worth a thousand words\u2019 and certainly we can not type such a long a query in search text box. How about searching for products using product photos?\n\nWith emerging and developed countries coming alive on \u2018mobile\u2019 technology, taking pictures through a camera and searching online, or searching using a photo already displayed in an App or Browser window during a browsing session is becoming popular (we believe the latter will pick up soon since it does not need to alter user behavior as in the former case). Such searching for products using photos is typically referred to as \u201cvisual search\u201d in online-retail lingo. In visual search, a pattern, based on color or shape, can be extracted from such photos and then it can be matched against similar patterns of existing product photos.\n\nTo be frank, we are not the first and only company to do visual search or visual recommendation based on pattern matching.\n\nHowever, we are perhaps one of the few companies who has intelligence to go beyond simple visual search and visual pattern matching.\n\nThere are three main aspects to such visual search/recommendation. (1) Semantic similarity based on automatic object recognition (whether photo is of a coffee mug, or a handbag, or a watch), (2) Visual similarity based on automatic color, pattern representation (whether a coffee mug is of red color with stripe pattern), and (3) Semantic+Visual similarity which combines intelligence on \u201cwhat type of product it is\u201d (so that recommendation for a coffee mug is another coffee mug and not a \u201csimilar looking (red color, stripe) handbag\u201d) while being pertinent to the structure and style of the product.\n\nMost of our competitors fall into the visual similarity domain. No wonder adoption and effectiveness of visual search today is very poor.\n\nFigure 1: Recognizing different objects from a photo and recommending relevant products\n\nWe have developed state-of-the-art techniques using deep learning which first \u201cidentifies\u201d and \u201crecognizes\u201d the class/category/type of a product (coffee mug vs a t-shirt vs bottle vs book) from a photo and then gives semantically+visually most similar items using deeper representation of images. This is shown in figure 1 above.\n\nFurthermore, our techniques are faster and much more accurate as compared to visual recommendation solutions currently employed by fashion or online marketplaces. We have scientifically evaluated our techniques on mean average precision and recall metrics and our findings show that we can perform several folds betters than existing solutions.\n\nWe believe that with a comprehensive suite of technology to cater to seller-side and buyer-side problems faced by online retailers today, we can truly change the experience of online shopping. This can not only help online retailers who provide its consumers one-touch points to shop but to you and me to find relevant products in much less time.", 
        "title": "Deep Learning and Beyond Visual Recommendations: Part II"
    }
]