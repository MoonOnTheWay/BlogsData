[
    {
        "url": "https://medium.com/@NathanBenaich/investing-in-artificial-intelligence-a-vc-perspective-afaf6adc82ea?source=tag_archive---------0----------------", 
        "text": "My (expanded) talking points from a presentation I gave at the Re.Work Investing in Deep Learning dinner in London on 1st December 2015.\n\nTL;DR Check out the slides here.\n\nIt\u2019s my belief that artificial intelligence is one of the most exciting and transformative opportunities of our time. There\u2019s a few reasons why that\u2019s so. Consumers worldwide carry 2 billion smartphones, they\u2019re increasingly addicted to these devices and 40% of the world is online (KPCB). This means we\u2019re creating new data assets that never existed before (user behavior, preferences, interests, knowledge, connections).\n\nThe costs of compute and storage are both plummeting by orders of magnitude, while the computational capacity of today\u2019s processors is growing. We\u2019ve seen improvements in learning methods, architectures and software infrastructure. The pace of innovation can therefore only be accelerating. Indeed, we don\u2019t fully appreciate what tomorrow will look and feel like.\n\nAI-driven products are already out in the wild and improving the performance of search engines, recommender systems (e.g. e-commerce, music), ad serving and financial trading (amongst others). Companies with the resources to invest in AI are already creating an impetus for others to follow suit or risk not having a competitive seat at the table. Together, therefore, the community has a better understanding and is equipped with more capable tools with which to build learning systems for a wide range of increasingly complex tasks.\n\nMore on this discussion here. A key consideration, in my view, is that the open sourcing of technologies by large incumbents (Google, Microsoft, Intel, IBM) and the range of companies productising technologies for cheap means that technical barriers are eroding fast. What ends up moving the needle are: proprietary data access/creation, experienced talent and addictive products.\n\nThere are two big factors that make involving the user in an AI-driven product paramount. 1) Machines don\u2019t yet recapitulate human cognition. In order to pick up where software falls short, we need to call on the user for help. 2) Buyers/users of software products have more choice today than ever. As such, they\u2019re often fickle (avg. 90-day retention for apps is 35%). Returning expected value out of the box is key to building habits (hyperparameter optimisation can help). Here are some great examples of products which prove that involving the user-in-the-loop improves performance:\n\nWe can even go a step further, I think, by explaining how machine-generated results are obtained. For example, IBM Watson surfaces relevant literature when supporting a patient diagnosis in the oncology clinic. Doing so improves user satisfaction and helps build confidence in the system to encourage longer term use and investment. Remember, it\u2019s generally hard for us to trust something we don\u2019t truly understand.\n\nTo put this discussion into context, let\u2019s first look at the global VC market. Q1-Q3 2015 saw $47.2bn invested, a volume higher than each of the full year totals for 17 of the last 20 years (NVCA). We\u2019re likely to breach $55bn by year end. There are circa 900 companies working in the AI field, most of which tackle problems in business intelligence, finance and security. Q4 2014 saw a flurry of deals into AI companies started by well respected and achieved academics: Vicarious, Scaled Inference, MetaMind and Sentient Technologies.\n\nSo far, we\u2019ve seen circa 300 deals into AI companies (defined as businesses whose description includes keywords: artificial intelligence, machine learning, computer vision, NLP, data science, neural network, deep learning from Jan 1st 2015 thru 1st Dec 2015, CB Insights). In the UK, companies like Ravelin, Signal and Gluru raised seed rounds. Circa $2bn was invested, albeit bloated by large venture debt or credit lines for consumer/business loan providers Avant ($339m debt+credit), ZestFinance ($150m debt), LiftForward ($250m credit) and Argon Credit ($75m credit). Importantly, 80% of deals were < $5m in size and 90% of the cash was invested into US companies vs. 13% in Europe. 75% of rounds were in the US.\n\nThe exit market has seen 33 M&A transactions and 1 IPO (Adgorithms on the LSE). Six events were for European companies, 1 in Asia and the rest were accounted for by American companies. The largest transactions were TellApart/Twitter ($532m; $17m raised), Elastica/Blue Coat Systems ($280m; $45m raised) and SupersonicAds/IronSource ($150m; $21m raised), which return solid multiples of invested capital. The remaining transactions were mostly for talent, given that median team size at the time of the acquisition was 7ppl median.\n\nAltogether, AI investments will have accounted for circa 5% of total VC investments for 2015. That\u2019s higher than the 2% claimed in 2013, but still tracking far behind competing categories like adtech, mobile and BI software. The key takeaway points are a) the financing and exit markets for AI companies are still nascent, as exemplified by the small rounds and low deal volumes, and b) the vast majority of activity takes place in the US. Businesses must therefore have exposure to this market.\n\nI spent a number of summers in university and 3 years in grad school researching the genetic factors governing the spread of cancer around the body. A key takeaway I left with is the following: therapeutic development is a very challenging, expensive, lengthy, regulated and ultimately offers a transient solution to treating disease. Instead, I truly believe that what we need to improve healthcare outcomes is granular and longitudinal monitoring of physiology and lifestyle. This should enable early detection of health conditions in near real-time, drive down cost of care over a patient\u2019s lifetime, while consequently improving outcomes.\n\nConsider the digitally connected lifestyles we lead today. The devices some of us interact with on a daily basis are able to track our movements, vital signs, exercise, sleep and even reproductive health. We\u2019re disconnected for fewer hours of the day than we\u2019re online and I think we\u2019re less apprehensive to storing various data types in the cloud (where they can be accessed, with consent, by 3rd parties). Sure, the news might paint a different, but the fact is that we\u2019re still using the web and it\u2019s wealth of products.\n\nOn a population level, therefore, we have the chance to interrogate data sets that have never before existed. From these, we could glean insights into how nature and nurture influence the genesis and development of disease. That\u2019s huge. Look at today\u2019s clinical model: a patient presents into the hospital when they feel something is wrong. The doctor has to conduct a battery of tests to derive a diagnosis. These tests address a single (often late stage) time point, at which moment little can be done to reverse damage (e.g. in the case of cancer). Now imagine the future. In a world of continuous, non-invasive monitoring of physiology and lifestyle, we could predict disease onset and outcome, understand which condition a patient likely suffers from and how they\u2019ll respond to various therapeutic modalities. There\u2019s loads of applications for artificial intelligence here: intelligence sensors, signal processing, anomaly detection, multivariate classifiers, deep learning on molecular interactions\u2026\n\nSome companies are already hacking away at this problem:\n\nA point worth noting is that the UK has a slight leg up on the data access front. Initiatives like the UK Biobank (500k patient records), Genomics England (100k genomes sequenced), HipSci (stem cells) and the NHS care.data programme are leading the way in creating centralised data repositories for public health and therapeutic research. Cheers for pointing out, Hari Arul.\n\nCould businesses ever conceivably run themselves? AI-enabled automation of knowledge work could cut employment costs by $9tn by 2020 (BAML). Coupled to the efficiency gains worth $1.9tn driven by robots, I reckon there\u2019s a chance for near complete automation of core, repetitive businesses functions in the future. Think of all the productised SaaS tools that are available off the shelf for CRM, marketing, billing/payments, logistics, web development, customer interactions, finance, hiring and BI. Then consider tools like Zapier or Tray.io, which help connect applications and program business logic. These could be further expanded by leveraging contextual data points that inform decision making. Perhaps we could eventually re-image the new eBay, where you\u2019ll have fully automated inventory procurement, pricing, listing generation, translation, recommendations, transaction processing, customer interaction, packaging, fulfilment and shipping. Of course, probably a ways off\u00a0:)\n\nI\u2019m bullish on the value to be created with artificial intelligence across our personal and professional lives. I think there\u2019s currently low VC risk tolerance for this sector, especially given shortening investment horizons for value to be created. More support is needed for companies driving long term innovation, especially that far less is occurring within Universities. VC was born to fund moonshots.\n\nWe must remember that access to technology will, over time, become commoditised. It\u2019s therefore key to understand your use case, your user, the value you bring and how it\u2019s experience and assessed. This gets to the point of finding a strategy to build a sustainable advantage such that others find it hard to replicate your offering. Aspects of this strategy may in fact be non-AI and non-technical in nature (e.g. the user experience layer\u200a\u2014\u200athanks for highlighting this Hari Arul). As such, there\u2019s a renewed focused on core principles: build a solution to an unsolved/poorly served high-value, persistent problem for consumers or businesses.\n\nFinally, you must have exposure to the US market where the lion\u2019s share of value is created and realised. We have an opportunity to catalyse the growth of the AI sector in Europe, but not without keeping close tabs on what works/doesn\u2019t work across the pond first-hand.\n\nWorking in the space? We\u2019d love to get to know you\u00a0:)", 
        "title": "Investing in Artificial Intelligence \u2013 Nathan Benaich \u2013"
    }, 
    {
        "url": "https://medium.com/@arthurb/traces-probability-and-learning-435a1abdcdc?source=tag_archive---------1----------------", 
        "text": "This is a collection of thoughts I\u2019ve had over the concept of program traces. The draft has resisted my attempts to clean it up and edit it for clarity for the past six months. I have chosen to post it as is rather than let it rot in my folders, please forgive its rough edges. I found it useful to write, perhaps someone will find it useful to read.\n\nProgrammers typically only encounter program traces in the context of debugging. However, looking at them under the lenses of probabilistic programming and recursive neural networks shows that they are of great conceptual importance.\n\nA program trace is a sequence of state transitions that a program undergoes at runtime. You could think of these states as the content of a computer\u2019s memory, but they could also be the different stages in the reduction of a lambda expression. In the following, we assume that the program is a state machine, but the arguments presented hold for other paradigms of computation.\n\nProbabilistic models are mathematical expressions containing random variables. Their goal is to describe observations as the byproduct of latent (or \u201chidden\u201d) variables. They are often called generative model because the emphasis is on describing how the observations originated, or graphical models because the expression can be parsed as a graph, generally a DAG.\n\nOne typically sets a prior over the latent variables and attempts to estimate their posterior distribution given the observations. This estimate can be formed by deriving the posterior distribution analytically, by sampling (using Monte-Carlo techniques) or\u200a\u2014\u200ain variational approaches\u200a\u2014\u200aby deriving analytical approximations to the posterior.\n\nProbabilistic programs are a generalization of probabilistic models replacing expressions with Turing-complete programs. They also generalize programs by enriching their semantic with two primitives:\n\nSuch programs are particularly well suited to represent Bayesian non-parametric models which generally involve processes (e.g. the Chinese restaurant process, the Dirichlet process, etc.)\n\nA key difference between probabilistic models and probabilistic programs is that\u200a\u2014\u200ain the latter\u200a\u2014\u200athe number and type of the random variables are not known before runtime, and not all random variables may be instantiated in a given execution.\n\nOver what space is the posterior distribution then defined? In general, it is defined over program traces. The probabilistic program defines a prior over traces, and the goal is to sample traces from the posterior distribution given the observations.\n\nA common approach to sample from these probabilistic programs uses particle filters. To get a better intuition of the motivation behind the algorithm, let\u2019s start with a naive filter and build from there. We\u2019ll set aside concerns about program termination: for all practical purposes, time bounds are known or can be imposed without major consequences.\n\nA naive filter would run many instances of the program (or \u201cparticles\u201d) in parallel, making random draws when needed. Whenever an observation is encountered, all the particles in disagreement with it are terminated while the others are allowed to continue their execution.\n\nUnfortunately, in all but the simplest cases, no particle will happen to be in agreement with all the observations. This is particularly true when the observed variables are floating point numbers.\n\nIn most cases, though, observations are noisy. They might take the form:\n\nA naive filter would have every particle sample a different value of e, and retain only those particles for which x+e exactly matches the observation x\u2019. A more astute approach consists in maintaining weights for each particle. Initially, those weights are all equal and sum to one but, when we encounter such an observation, we multiply each particle\u2019s weight by P\u2019s probability density function at point (x\u2019-x). We then renormalize the particle\u2019s weights and carry on.\n\nSo for instance, if e is a standard normal variable, we would multiply each particle\u2019s weight by\n\nand then renormalize the weights to make sure they sum to one. Instances which predicted a value of x close to the observed value x\u2019 will have their weight increased relative to the instances which predicted a value of x far from x\u2019.\n\nThe log sum of the weights before renormalization is a likelihood estimate of making this observation conditional on past observations.\n\nWe\u2019ve cheated a bit. We\u2019ve marginalized over e by peeking into the future and using algebra. The equivalent, unmarginalized, version would have stumped our filter.\n\nThough the particles may not fully disappear anymore, a single fortuitous particle will often end up bearing all the probability mass of the sample, which greatly increases the bias of the filter. Thus, a second trick is used: the particles are regularly resampled. Periodically, the set of particles is replaced by performing a random draw with replacement (also known as a bootstrap sample) from the set of weighted particles. This technique, known as sequential importance resampling, has the effect of pruning some of the very low weight particles and duplicating successful ones, without introducing bias in the limit.\n\nAs such, this algorithm performs filtering, which means that at each point in time, the particles represent an approximate sample from the forward, or filtering, distribution. That is, the distribution of program states, at this point in the execution, given all of the past observations. In contrast, the smoothing distribution would incorporate future observations as well.\n\nInterestingly, the fraction of the probability mass lost after conditioning on each observation gives us an estimate of how surprising that observation is. Their product across every execution step in the program gives an unbiased estimate of the likelihood of the observed data under the model.\n\nThe set of instances reaching the end of the program represents the leaves of a forest of program traces, the roots of which form a subset of the initial particles. In fact, for most problems, this forest will typically consist of a single tree, rooted in one of the most fortuitous initial choices.\n\nBy randomly sampling one of the leaves and following the trace back to the initial state, one can obtain a proposal for a Markov-Chain Monte Carlo algorithm, where the acceptance ratio is given by the ratio between the (noisy) estimate of the likelihood of the observations in the forward pass where we sampled the candidate trace and the (noisy) estimate of the likelihood of the observations in the forward pass where the current trace was sampled. This is the heart of the breakthrough PMCMC algorithm discovered by Andrieu, Doucet, and Holenstein. This variant is generally dubbed PIMH.\n\nAn alternative to this algorithm called Particle Gibbs consists in sampling a trace and performing the next filtering pass conditional on that trace. That is, one introduces an instance that merely replays the previously sampled trace, and resampling is performed at every step. This has the effect of \u201cguiding\u201d the sampling procedure around the previously sampled trace.\n\nSo far, this looks great\u200a\u2014\u200aand it is, provided that the traces do not have strong autocorrelation. This is what happens if many random decisions are made early on in the program execution, the consequences of which are only observed much later. We\u2019ve already described one such case: if we restricted ourselves to sharp observations, most particles would sample the corrupting noise variable blindly and would never exactly hit the right observation. Marginalizing over this error is a nice trick that allows the filter to peek a little bit ahead. Unfortunately, there are many cases where this is not enough. Consider for instance the trivial program:\n\nWhile we can easily show that the posterior factorizes as x[i] ~ bernoulli(100/101) for all i, the filter will not be able to see that. It will try to sample random 64 bit vectors, and hopelessly realize that they do not fit observations very well. Ironically, the equivalent program:\n\nis trivial to sample from. The key difference is that in the second program, the observations come as we sample the x[i], thus constraining our choices progressively. The particle filter chokes when it is forced to maintain a representation of a high dimensional joint distribution, even though it is factorial in practice. The most obvious trick around this would be to use lazy evaluation of the random variables. However, consider now:\n\nOnce again, it is trivial when looking at the program to see that all the x[i] must be true, but lazy evaluation will not save the filter here! Indeed, the observation of y implies that at least one of the x[i] must be true, and it forces the filter to sample potential values of x.\n\nHow do these pathologies present themselves when we sample from the full posterior using PIMH or particle Gibbs?\n\nIn PIMH, we\u2019ll get sticky proposals. That is, one particle, during one pass will happen to sample more true values than any other has before, and this proposal will be accepted with high probability. However, that candidate will remain undethrowned for a very long time, and it cannot be used as a starting point to find a better candidate. Every pass will require blind luck, and our chain will have a very low acceptance probability.\n\nThe situation is somewhat better with particle Gibbs. In this case, we are likely to sample candidates which are closer and closer to the observation, as the sampled trace branches out into other possibilities. However, very often, a successful trace will be found which starts with an erroneous choice. This initial erroneous choice is unlikely to be corrected, as an alternative trace branching out early is unlikely to randomly rediscover the rest of the path. Particle Gibbs with ancestral sampling attempts to remedy this problem, but it is less straightforward to apply to program traces.\n\nPathological cases are the rule, not the exception.\n\nThe motivation for probabilistic programming is often to encode existing knowledge about a generating process and update this belief based on available data. In such case, we do not really collect any information during the execution of the program, we only observe an output at the very end. This is the worst case scenario for a filter, as these perform well when information is distilled over the course of the execution, not in one chunk, at the end. It means the filter has to make a lot of lucky guesses for its particles not to lose all of their mass by the end.\n\nThere are some ways one can work around these problems, by incorporating extra MCMC steps, or by exploiting the topological structure of the problem. In general, though, things would clearly be much easier if we could make observations along the way, and not only at the end. In fact, the more observations, the better. In the limit, we would observe every variable exactly and have nothing to do.\n\nSide channel attacks, when things go right (but shouldn\u2019t)\n\nA common class of attacks against cryptographic systems are side channel attacks. They allow attackers to break cryptographic systems by using information leaked during the execution of the algorithm. This information could be the blinking of an LED, the time taken by the execution, voltage patterns, or the size of compressed packets.\n\nThis should be no surprise given what we\u2019ve discussed so far. The secret in the algorithm acts as high dimensional random variable initialized early on, and it is probably impossible for a simple forward filter to reverse it better than brute force. However, the side channel gives our filter observations about the trace of the program, thus allowing us to progressively condition our distribution before the output is produced. The side channel is what guide our filtered traces towards the posterior distribution.\n\nA very useful probabilistic program would be an interpreter for a randomly generated deterministic program. Here, our only assumption would be that the observed data was produced by some algorithm, with some prior on the complexity of the algorithm.\n\nThis is, in fact, the basis of Solomonoff induction which argues that the prior on our experience of reality should the set of all possible programs, weighted by the inverse of the exponential of their binary encoding length. The biggest advantage of this prior is that, up to a constant, it does not depend on your model of computation. That is, in the limit, as you collect more data, you\u2019ll reach the same posterior regardless of what model of computation you started with.\n\nIn practice, such a prior is uncomputable, but we\u2019d like to do something similar\u2026 given some data, wouldn\u2019t it be awesome to sample from a set of concise programs that can produce this data? Such programs would likely display good generalization properties, and help us make predictions\u200a\u2014\u200aafter all, understanding the world is all about describing the latent mechanisms at play.\n\nUnfortunately, by this point, it is clear why this seems hopeless if we solely rely on a forward filtering approach. We would end up sampling programs and evaluating their outputs like monkeys throwing typewriters at a dartboard.\n\nIn general, my (pessimistic) intuition is that you cannot sample your way out of this problem because sampling is too myopic. You have to form theories about the data and about your program. A known manifestation of a similar problem arises in constraint satisfaction. Given n holes, can you put n+1 pigeons into the holes, so that no two pigeons share the same hole? The obvious answer is no, but if you express this problem as a Boolean circuit and restrict yourself to first order logic, you will need a proof of exponential size to show it. To solve the problem efficiently, you need to form a theory of integers and use that powerful theory to solve the problem. The problem expressed as a Boolean circuit cannot efficiently be solved from within. MCMC sampling is typically such a \u201cwithin\u201d approach, it does not see symmetries in the model, it does not see independence or algebraic marginalizations.\n\nThe recent successes of SMT solvers can be attributed to the fact that instead of throwing away the structure of the program by encoding everything in binary, as previous approaches had tried, they preserve as much of the structure as possible. Successful generic probabilistic programming approaches will need to do something similar, and detect and exploit any mathematical structure they can find in the programs.\n\nRecent advances in deep learning\u200a\u2014\u200aand deep recurrent neural networks in particular\u200a\u2014\u200ahave made for spectacular demonstrations. Such networks provide state of the art speech recognition, natural language modelling, automated translation, image recognition, etc. An interesting approach, recently revived by Deepmind, consists in building recurrent networks that embody algorithms, and not merely expressions. I\u2019ll draw examples from two of their papers, the neural Turing machine (or NTM) and the DRAW network.\n\nThe idea behind the Neural Turing Machine is to build a recurrent neural network that can access a memory. The memory is accessible both by index and by content using reading and writing \u201cheads\u201d. A key aspect of the design is that this machine is end to end differentiable. By that, it is meant that at every step in time, the state of the machine is a differentiable function of the previous state of the machine. Thus, the output will be a differentiable function of the input\u200a\u2014\u200aalbeit possibly a highly non linear function. The main subtlety is to define differentiable reading and writing operations in the memory.\n\nSince a universal Turing machine can be expressed by such an RNN, Turing completeness follows. Less obvious is the ability of the machine to learn an algorithm using gradient descent only.\n\nMerely having a gradient is no panacea. It is trivial for instance to turn any Boolean SAT problem into a differentiable problem by mapping the Booleans to the reals. Replace, for example, every Boolean term by a real number in [0,1] and use the differentiable NAND gate f(x,y) = 1-xy. Or, if you want an unbounded domain, use f(x,y) = Log(e^-x + e^-y + e^(-x-y)). However, there is no guarantee that applying gradient descent to this formula will yield correct solutions or even useful approximations.\n\nThe NTM paper does demonstrate positive results involving sorting and copying, but it is not clear whether more complex algorithms can be learned.\n\nAnother interesting paper is the DRAW paper. In this architecture, a recurrent neural network learns to recognize hand written digits by simultaneously learning to draw them. It does so using a recurrent attention model. Instead of learning to generate a whole hand written digit at once, it learns to focus its gaze to one part of the intput, and draw or erase one part of the output.\n\nSuch an architecture has in fact much in common with the neural Turing machine. The memory has been replaced with a canvas, and the writing operations are performed by drawing on this canvas. The fovea is the reading head and the paintbrush the writing head. Much as humans can carry arbitrary computations with pen an paper, so can the DRAW network.\n\nSuppose we wanted to teach the DRAW network to perform long multiplication. One way to do this would be to use a form of supervised learning where a teacher literally performs long multiplication (using a graphic tablet for instance)\u00a0, and the network learns to predict the strokes of the teacher on the canvas. At first, the network would learn a model of drawing digits, as they explain much of what the teacher is doing. Then, it would learn that these digits are written in particular places, generally from right to left and from top to bottom. It would learn that the value of a digit depends on the value of other digits in certain positions. It would learn to write the carry as superscript and to use it.\n\nIn addition to writing on the canvas, the teacher could also point to important or relevant parts. It might demonstrate the operation of summing two digits and a carry by pointing successively at all three. This would make it far easier for the network to train its attention model to focus on the correct parts at the correct time.\n\nIn the end the network would likely learn an algorithm that performs long multiplication. It would likely generalize well to arbitrary number of digits, since it only needs a fixed number of registers to carry on the execution.\n\nConsider now presenting the same network with fully formed long multiplications, in pixel space. This time, there is no teacher drawing the numbers in order or pointing out which parts to pay attention to. This is a much harder problem, and I suspect one that may not be solvable by mere gradient descent. (I do plan to try at some point, but there are so many hours in a day\u2026)\n\nIn both cases, we are relying on supervised learning, but in the first case, the information given to the network is almost a full trace of the program. Learning an algorithm from outputs is hard, learning it from traces is easier. This is also what happens when the network becomes privy to the instructor\u2019s gaze; the gaze correlates with some latent states in the algorithm and thus make it easier for the network to learn to produce such states.\n\nAs a thought experiment, consider a recurrent neural network attempting to learn the game of Go by learning to predict the moves of an expert player. If all this armchair machine learning is correct, such an algorithm would learn to make better predictions faster if it were given access to the expert\u2019s player gaze. The gaze correlates with the internal algorithm happening inside the expert\u2019s head. Instead of merely telling the learner which piece it should move, we\u2019re providing it with some insight into how to reach the right conclusion.\n\nIn fact, consider how a master might teach a human student. What would he do? He would point at the board, and say things like \u201cSee this piece? It\u2019s important because of that other piece here\u201d. He would provide a dump of his internal trace. What is the accessory most typically associated with a teacher? A pointer.\n\nIn the limit, the learner would observe the state of every neuron of the teacher over time. The learning task would then boil down to modelling the behavior of neurons in general, a much easier task.\n\nIt would not necessarily be practical to do so, and it would require considerable computing power, but as a pure learning problem, it is conceptually much easier than learning the entire Go playing algorithm by merely observing its inputs and outputs.\n\nA testable prediction of all this armchair machine learning is that intelligence is taught. This means that for connectionist AI to go beyond animal intelligence and learn abstract, general, reasoning skills, it will need to learn an algorithm. I suspect that such an algorithm is not accessible by gradient descent but was discovered by evolution and is being transmitted through language, which is is a great way to explicitly expose internal mental states and their progression.", 
        "title": "Traces, probability, and learning \u2013 ArthurB \u2013"
    }
]