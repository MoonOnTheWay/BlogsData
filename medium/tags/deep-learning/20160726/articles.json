[
    {
        "url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99?source=tag_archive---------0----------------", 
        "text": "What is a model and why would we want to use one? In this case, a model is going to be a neural network that attempts to learn the dynamics of the real environment. For example, in the CartPole we would like a model to be able to predict the next position of the Cart given the previous position and an action. By learning an accurate model, we can train our agent using the model rather than requiring to use the real environment every time. While this may seem less useful when the real environment is itself a simulation, like in our CartPole task, it can have huge advantages when attempting to learn policies for acting in the physical world.\n\nUnlike in computer simulations, physical environments take time to navigate, and the physical rules of the world prevent things like easy environment resets from being feasible. Instead, we can save time and energy by building a model of the environment. With such a model, an agent can \u2018imagine\u2019 what it might be like to move around the real environment, and we can train a policy on this imagined environment in addition to the real one. If we were given a good enough model of an environment, an agent could be trained entirely on that model, and even perform well when placed into a real environment for the first time.\n\nHow are we going to accomplish this in Tensorflow? As I mentioned above, we are going to be using a neural network that will learn the transition dynamics between a previous observation and action, and the expected new observation, reward, and done state. Our training procedure will involve switching between training our model using the real environment, and training our agent\u2019s policy using the model environment. By using this approach we will be able to learn a policy that allows our agent to solve the CartPole task without actually ever training the policy on the real environment! Read the iPython notebook below for the details on how this is done.\n\nSince there are now two network involved, there are plenty of hyper-parameters to adjust in order to improve performance or efficiency. I encourage you to play with them in order to discover better means of combining the the models. In Part 4 I will be exploring how to utilize convolutional networks to learn representations of more complex environments, such as Atari games.", 
        "title": "Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL"
    }, 
    {
        "url": "https://medium.com/@Zelros/how-deep-learning-solved-phase-1-of-the-data-science-game-2712b949963f?source=tag_archive---------1----------------", 
        "text": "The selective phase of the Data Science Game is now over. More than 140 registered teams, from more than 30 countries participated in this first stage, hosted on Kaggle In Class.\n\nThe goal was to classify satellite images of roofs, to identify their orientation (this could be used to estimate the solar energy potential of a country).\n\nWe had the opportunity to meet the two best teams at our office in Paris\u00a0:\n\nHere is what we learned.\n\nWe met at the UPMC Data Science master, and currently work together in the same lab and same office. It was natural to build a team, and it was easy for us to meet physically everyday during the competition to organize our strategy.\n\nWe were all students at Ecole Polytechnique, at different levels. We received a message through Facebook, inviting us to participate. That\u2019s how we initially build the team. And we also used Facebook to communicate during the challenge\u00a0!\n\nAn other team from Polytechnique also finished among the 20 best teams selected for the final challenge. Unfortunately, only one team per university is allowed to be finalist. It\u2019s a pity, because they were very serious candidates for the challenge title\u00a0!\n\nWhen we discovered the data and the challenge, Remi naturally took the lead of the machine learning part\u00a0: indeed he already worked on similar problems of image classification, had the tools and proper code framework to start from. The rest of the team worked on data analysis and visualization, features extraction, data augmentation and stacking.\n\nThe work breakdown was quite natural\u00a0: Guillaume was the most experienced, and had the suitable hardware\u200a\u2014\u200aso he was responsible for running the neural networks itself. The rest of the team was mainly in charge of data preparation, data augmentation, and ensembling (see below).\n\nWe spent the first three weeks brainstorming and reading papers. Our conclusion was that Inception V3 (GoogleNet) seemed to present better transfer learning properties than VGG\u200a\u2014\u200athat\u2019s why we decided to focus on it for the rest of the competition.\n\nWe used a Torch implementation optimized by Remi, and Nvidia cuDNN over a cluster of 4 TitanX GPUs. We started from a model pre-trained on ImageNet, and fine tuned it on the images set of the challenge.\n\nWe augmented the training dataset with images transformations (flips, rotations,\u00a0\u2026), but didn\u2019t use the unlabeled images. Indeed our thought was that labelling these images would not help the models\u00a0: it would provide no extra information (for the images trivially labeled by the models), or noise (for the images incorrectly labels by the models).\n\nWe had a lot of variance in our cross-validation score. To reduce it, we used bagging of bootstrapped models. We also generated models with different hyperparameters, to create diversity. Our final submission was a blend of almost 100 models.\n\nOur cornerstone was the VGG very deep ConvNet, from the University of Oxford (the 19 layers flavour). We started with a network pre-trained on ImageNet, and then fine-tuned the weights by continuing the backpropagation on data provided for the challenge (transfer learning).\n\nTo have more training data, we used data augmentation technics to extend the available images, like flips or 90\u00b0 / 180\u00b0 / 270\u00b0 rotations for example.\n\nWhat\u2019s more, as there were unlabeled images, we used a semi-supervised technic\u00a0: we labelled the unlabeled images with our classifiers, and used this newly labeled data to augment our training test. Surprisingly, this didn\u2019t improved results a much as we would have expected.\n\nUsing GPUs helped us to iterate quicker. For a given set of hyperparameters it took around 30 min to train a model.\n\nTo blend our models, we didn\u2019t use an average of the soft decisions. Instead, we used a majority vote of the hard decisions\u200a\u2014\u200awhich worked better for us.\n\nWe also tweaked the predictions of our final submission to counteract the fact that the train set had unbalanced classes, while the test set was balanced. This improved a bit our score, even though without this trick we still were ranked first.\n\nTuning Deep Neural Networks is a lot about experience and know-how. Even if you know the theory, you need to have tuned dozens of networks before, to get the best of it. Our previous experiments with this technology helped us a lot. But more than that, without GPUs we won\u2019t have been able to reach this performance.\n\nStacking different models also helped us to improve accuracy (at the end we had 84 models\u00a0: 12 root models, trained on 7 different versions of the images). Our best submission was a classifier trained on the predictions of these 84 models.", 
        "title": "How Deep Learning Solved Phase 1 Of The Data Science Game"
    }, 
    {
        "url": "https://medium.com/nlml/letting-your-chatbots-explore-efedf4ffe9c0?source=tag_archive---------2----------------", 
        "text": "When we use machine learning for our chatbots, it\u2019s usually the area known as supervised learning. At each potential branch in a conversation we provide examples of what messages would lead down each path. If you see \u201cWhat\u2019s the weather like?\u201d run look_up_weather(). If you see \u201cTell me a joke\u201d run tell_a_joke(). After enough examples sentences are categorized, the chatbot starts making the right decision when the user finds a novel way to phrase a request.\n\nSupervised learning is the most studied, most developed area of machine learning. There are hundreds of algorithms and software packages available. The approach of providing examples along with the correct responses provides a powerful signal for the computer to learn from. \u201cIf you see things like x, =do y.\u201d And the computer obeys. But what if you don\u2019t know how the chatbot should respond? You want your users to laugh, or to purchase a movie ticket, or to use your chatbot again. But what will achieve that? If the user requests a suggestion for a movie to see, you want the chatbots response to lead to a ticket sale. But which movie should it suggest? Should it include a synopsis? A quote from a review? Something about who\u2019s starring in it? We can easily code up each alternative, but it\u2019s not so easy to provide advice about what the chatbot should do. Without explicit \u201ccorrect\u201d answers, supervised learning fails. Fortunately, even without explicit correct answers there is still signal. There is information for your chatbot to learn from. Whatever response your chatbot gives, the human will take some action that you find desirable or undesirable. The human will purchase a product (or not). They will use the chatbot again (or not). They will type \u201chahaha\u201d at the chatbot\u2019s joke (or type \u201c*groan*\u201d). Although it\u2019s not always obvious why the user acted as they did, across many interactions the chatbot can learn how to elicit the responses you want to see. Using this sort of delayed, indirect signal for training a machine learning algorithm is known as \u201creinforcement learning.\u201d This technique played an important role in the recent triumph of machine over humans in the game of \u201cGo.\u201d\n\nTo use reinforcement learning you provide choices to your chatbot rather than singular instructions. \u201cHere are the products to consider,\u201d or \u201chere are possible phrasings for this response,\u201d or \u201cyou can offer the user a 10% discount and see if that gets the sale.\u201d These may be simple alternatives, or parameterized sets of decision points. Whenever the chatbot gets to a decision point it consults the reinforcement learning system. A decision is provided and recorded. When the human finishes the interaction, you provide a score to the reinforcement learning algorithm for how well that went. That information is used to make better decisions next time. At Lexalytics, we are implementing leading edge machine learning technologies like these; keeping us at the forefront of features, performance, and accuracy. Reinforcement learning involves an interesting interplay between exploration and maximization. If you always take the same action, you\u2019ll get a predictable outcome. If you try something new (or something that didn\u2019t work well before) you might get a worse outcome. On the other hand, it might be a better outcome and that\u2019s the action you should keep taking. Reinforcement learning balances these two goals automatically, creating a chatbot that gives good results while also trying out new responses and learning from the human reactions. One more reason reinforcement learning is an exciting new opportunity for chatbots? An open source implementation has just been released. You can run this on your own servers or deploy it quickly into Azure. As an avid user of machine learning techniques, it\u2019s exciting to see another technique making its way from academia into the mainstream. I\u2019m excited to see the uses chatbot developers find for this tool, as it feels to me like a very natural fit for this domain.", 
        "title": "Letting Your Chatbots Explore \u2013 NLML \u2013"
    }
]