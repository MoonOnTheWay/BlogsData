[
    {
        "url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-4-deep-q-networks-and-beyond-8438a3e2b8df?source=tag_archive---------0----------------", 
        "text": "Welcome to the latest installment of my Reinforcement Learning series. In this tutorial we will be walking through the creation of a Deep Q-Network. It will be built upon the simple one layer Q-network we created in Part 0, so I would recommend reading that first if you are new to reinforcement learning. While our ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n\nIt was these three innovations that allowed the Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent. We will be walking through each individual improvement, and showing how to implement it. We won\u2019t stop there though. The pace of Deep Learning research is extremely fast, and the DQN of 2014 is no longer the most advanced agent around anymore. I will discuss two simple additional improvements to the DQN architecture, Double DQN and Dueling DQN, that allow for improved performance, stability, and faster training time. In the end we will have a network that can tackle a number of challenging Atari games, and we will demonstrate how to train the DQN to learn a basic navigation task.\n\nSince our agent is going to be learning to play video games, it has to be able to make sense of the game\u2019s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. In this way, they act similarly to human receptive fields. Indeed there is a body of research showing that convolutional neural network learn representations that are similar to those of the primate visual cortex. As such, they are ideal for the first few elements within our network.\n\nIn Tensorflow, we can utilize the tf.contrib.layers.convolution2d function to easily create a convolutional layer. We write for function as follows:\n\nHere num_outs refers to how many filters we would like to apply to the previous layer. kernel_size refers to how large a window we would like to slide over the previous layer. Stride refers to how many pixels we want to skip as we slide the window across the layer. Finally, padding refers to whether we want our window to slide over just the bottom layer (\u201cVALID\u201d) or add padding around it (\u201cSAME\u201d) in order to ensure that the convolutional layer has the same dimensions as the previous layer. For more information, see the Tensorflow documentation.\n\nThe second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent\u2019s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of <state,action,reward,next state>. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we will build a simple class that handles storing and retrieving memories.\n\nThe third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network\u2019s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network\u2019s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner.\n\nInstead of updating the target network periodically and all at once, we will be updating it frequently, but slowly. This technique was introduced in another DeepMind paper earlier this year, where they found that it stabilized the training process.\n\nWith the additions above, we have everything we need to replicate the DWN of 2014. But the world moves fast, and a number of improvements above and beyond the DQN architecture described by DeepMind, have allowed for even greater performance and stability. Before training your new DQN on your favorite ATARI game, I would suggest checking the newer additions out. I will provide a description and some code for two of them: Double DQN, and Dueling DQN. Both are simple to implement, and by combining both techniques, we can achieve better performance with faster training times.\n\nThe main intuition behind Double DQN is that the regular DQN often overestimates the Q-values of the potential actions to take in a given state. While this would be fine if all actions were always overestimates equally, there was reason to believe this wasn\u2019t the case. You can easily imagine that if certain suboptimal actions regularly were given higher Q-values than optimal actions, the agent would have a hard time ever learning the ideal policy. In order to correct for this, the authors of DDQN paper propose a simple trick: instead of taking the max over Q-values when computing the target-Q value for our training step, we use our primary network to chose an action, and our target network to generate the target Q-value for that action. By decoupling the action choice from the target Q-value generation, we are able to substantially reduce the overestimation, and train faster and more reliably. Below is the new DDQN equation for updating the target value.\n\nIn order to explain the reasoning behind the architecture changes that Dueling DQN makes, we need to first explain some a few additional reinforcement learning terms. The Q-values that we have been discussing so far correspond to how good it is to take a certain action given a certain state. This can be written as Q(s,a). This action given state can actually be decomposed into two more fundamental notions of value. The first is the value function V(s), which says simple how good it is to be in any given state. The second is the advantage function A(a), which tells how much better taking a certain action would be compared to the others. We can then think of Q as being the combination of V and A. More formally:\n\nThe goal of Dueling DQN is to have a network that separately computes the advantage and value functions, and combines them back into a single Q-function only at the final layer. It may seem somewhat pointless to do this at first glance. Why decompose a function that we will just put back together? The key to realizing the benefit is to appreciate that our reinforcement learning agent may not need to care about both value and advantage at any given time. For example: imagine sitting outside in a park watching the sunset. It is beautiful, and highly rewarding to be sitting there. No action needs to be taken, and it doesn\u2019t really make sense to think of the value of sitting there as being conditioned on anything beyond the environmental state you are in. We can achieve more robust estimates of state value by decoupling it from the necessity of being attached to specific actions.\n\nNow that we have learned all the tricks to get the most out of our DQN, let\u2019s actually try it on a game environment! While the DQN we have described above could learn ATARI games with enough training, getting the network to perform well on those games takes at least a day of training on a powerful machine. For educational purposes, I have built a simple game environment which our DQN learns to master in a couple hours on a moderately powerful machine (I am using a GTX970). In the environment the agent controls a blue square, and the goal is to navigate to the green squares (reward +1) while avoiding the red squares (reward -1). At the start of each episode all squares are randomly placed within a 5x5 grid-world. The agent has 50 steps to achieve as large a reward as possible. Because they are randomly positioned, the agent needs to do more than simply learn a fixed path, as was the case in the FrozenLake environment from Tutorial 0. Instead the agent must learn a notion of spatial relationships between the blocks. And indeed, it is able to do just that!\n\nThe game environment outputs 84x84x3 color images, and uses function calls as similar to the OpenAI gym as possible. In doing so, it should be easy to modify this code to work on any of the OpenAI atari games. I encourage those with the time and computing resources necessary to try getting the agent to perform well in an ATARI game. The hyperparameters may need some tuning, but it is definitely possible. Good luck!", 
        "title": "Simple Reinforcement Learning with Tensorflow Part 4: Deep Q-Networks and Beyond"
    }, 
    {
        "url": "https://medium.com/the-downlinq/establishing-a-machine-learning-workflow-530628cfe67?source=tag_archive---------1----------------", 
        "text": "While the rapid advancements in neural networks offer enormous promise for the field of imagery analytics, application of these technologies against an ever-increasing volume of commercially available satellite imagery remains in its early stages. CosmiQ Works has developed and tested a feasible technical workflow to quantitatively assess the performance of open-source machine learning (ML) algorithms against satellite images.\n\nThe workflow, illustrated below, includes all of the necessary steps to test the performance of ML algorithms against satellite imagery. At the highest level, the workflow can be classified into two parts: human intensive activities; and computationally intensive activities. It is important to determine the necessary human and computational resources before beginning an ML experiment.\n\nWe will explore each part of the workflow in greater detail in upcoming blog posts. We plan to apply a majority of our focus to the most resource intensive activities: data labeling, training the imagery classifier and testing the imagery classifier.\n\nSeveral logos appear in the workflow to represent products that CosmiQ Works used during each of the steps of the workflow. The list of logos is not meant to be exhaustive but rather suggestive of products that facilitate the ML process, some with special support for satellite imagery.\n\nThe human intensive aspect of ML is the labeling of the data. Proper labels often require subject matter expertise in the objects of interest and in the desired labels to be associated with the objects. Satellite imagery has attributes that complicate the labeling process. For example, satellite imagery is large, multi-channeled, geo-tagged, and each pixel may store more than eight bits per channel. Common image libraries typically do not handle the imagery very well. CosmiQ Works used the Open Source Geographic Information Systems QGIS as a framework for managing, displaying, and labeling the satellite imagery. The label is stored as a vector layer with geo-references and is typically output as either a GeoJSON file or as an Esri Shapefile. For feature extraction, OpenCV supports standard 3-band imagery while Orfeo Toolbox is specifically designed for remote sensing applications. GDAL is a useful library for processing the geospatial data from the command line or Python scripts.\n\nAutomated analysis is usually optimized for a specific type of input, e.g., size of input image, contrast within the image, and physical dimension of the image. To improve accuracy, imagery is pre-processed by a variety of techniques. CosmiQ Works performed only minimal processing in terms of pixel normalization and image scaling using a standard computer vision library OpenCV.\n\nFeature extraction differentiates deep learning from classical machine learning. In classical machine learning features are determined a priori to the training. Sometimes these features are extracted manually (hand-crafted) and can be extremely effective for identifying regions and objects of interest, but often require expertise. More frequently, however, features are extracted automatically by a pre-determined suite of algorithms. On the other hand deep learning algorithms have the capability of learning a hierarchy of features during the training process, which can greatly increase the flexibility of the models; some learned features are optimized for the intended task and some are general enough to be used across multiple tasks.\n\nWhile OpenCV is good for manipulating 3-band imagery, it does not support the geographic data embedding in the imagery. GDAL is a useful software library for working with satellite imagery, supporting multi-band, 16-bit color depth, geotagged imagery. Many GIS frameworks rely on GDAL for basic functionality. Classical feature extraction (such as SIFT/SURF/Canny edge detection) is supported by OpenCV as well as the Orfeo Toolbox specifically designed for remote sensing.\n\nThe reason for dividing data into training and testing data sets is to provide independent validation in order to prevent overfitting model parameters during the training process. While this may seem straightforward for computer vision, geographic and temporal correlations are unavoidable in satellite imagery. For our experiments, we wrote Python scripts to randomly divide labeled sub-regions of satellite imagery into either a testing set or a training set. The ML algorithm only optimizes using the training set. The testing set is used to provide a measure of overfitting of the training data. For additional validation, we reserved separate imagery to validate the trained classifiers.\n\nWhen there is a significant difference in accuracy of a trained classifier on a test set compared to the accuracy on a training set, overfitting is the likely culprit. Increasing the amount of labeled data helps mitigate overfitting. For satellite imagery, the objects of interest may be either limited or expensive to find and label. Using symmetries in the imagery, one can simulate new images from the initial training set. For example, some common symmetries include image rotations, increased noise, change in lighting, and scaling of imagery. Generating new images using these symmetries introduces correlations in training data but the benefits usually outweigh the newly introduced correlations.\n\nIn deep learning, one can reduce training time and data requirements by importing parameters from previously trained neural networks. Even if the applications are different, the features represented by the lower layers of a trained neural network are often sufficient features for training a new classifier. AlexNet, GoogLeNet, and ResNet are previous ImageNet winners that have publicly available neural networks and trained weights. While these networks have not been optimized for satellite imagery, we have used the pre-trained weights as starting points for the training process.\n\nThe computational intensive part of ML is the optimization of the parameters of the classifier. Neural networks commonly have millions of parameters and can only be optimized using special hardware, software libraries, and starting values for the parameters.\n\nWe used high-end consumer hardware to train algorithms for image classification and object detection; our computational server is the NVIDIA DevBox with four Maxwell GeForce Titan X GPUs. Advantages for different hardware choices depend on the application, available budget, and the desired training time.\n\nThere are several software frameworks for performing deep learning; most support parallelization on NVIDIA GPUs. Initially, we chose to work with the deep learning framework Caffe because of the Python support and the access to pre-trained networks. For programmers comfortable with Python or C++, Tensorflow is a well-documented framework with a growing developer base. We generally design new network architectures in Tensorflow, but use Caffe to fine-tune pre-trained networks. NVIDIA Digits is a polished frontend to labeled data management and Caffe-based model training.", 
        "title": "Establishing a Machine Learning Workflow \u2013 The DownLinQ \u2013"
    }, 
    {
        "url": "https://medium.com/@databolism/getting-start-with-deep-learning-a-hands-on-guide-for-complete-beginners-part-1-setting-up-c2737a2fc0d?source=tag_archive---------2----------------", 
        "text": "Let\u2019s import all the library that we need, and then click run cell button. The hash character, #\u00a0, is for comments.\n\nWe will use MNIST dataset for our network, and we will use Keras to load the data for us. This dataset consists of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images.\n\nWe are going to build our network based on the Lenet-5 architecture [1].\n\nINPUT (gray scale image)\n\nLayer C1 is a convolution layer with 6 feature maps and a 5\u00d75 kernel for each feature map.\n\nLayer S2 is a subsampling layer with 6 feature maps and a 2\u00d72 kernel for each feature map.\n\nLayer C3 is a convolution layer with 16 feature maps and a 5\u00d75 kernel for each feature map.\n\nLayer S4 is a subsampling layer with 16 feature maps and a 2\u00d72 kernel for each feature map.\n\nLayer C5 is a convolution layer with 120 feature maps and a 5\u00d75 kernel for each feature map.\n\nLayer F6 is a fully connected layer\n\nOUTPUT returns the final label (out of 10 classes)\n\nIf all these layers look confusing to you, here\u2019s more explanation: http://neuralnetworksanddeeplearning.com/chap6.html#introducing_convolutional_networks\n\nTraining the model may take a few minutes if you are on a CPU, but only a few seconds on GPU. I\u2019m using just the CPU, so\u2026\n\nThe test score is % loss on the test set, i.e., MSE. And the test accuracy is, well, the % accuracy on the test set. The lower the test score, the better. And the higher the accuracy, the better (obviously). With this set-up, we have test score at about 3% and test accuracy at about 99% using just 2 epoch\u200a\u2014\u200aquite impressive!\n\nNow, if you use only the CPU and get \u201cOptimization failure\u201d during the training step, you can try installing Blas (but installing Blas is ridiculously confusing and probably doesn\u2019t worth our time\u2026\u00a0!) Here\u2019s a work around, for the sake of simplicity, change \u2018border_mode\u2019 to \u2018valid\u2019 and change kernel size for Layer C5 to 4x4.\n\nDon\u2019t forget that we have 10,000 images on our test set. Here, we will display the results of the first 25 images.\n\nSo, now what? We can play around the architecture, change the layers, change the epoch number, and see whether the result is better or worse. Or we can just save our pre-trained model for the future classification. Or we can print out all the 10,000 images and try to find which one has an incorrect label\u2026\u00a0! (Note: there\u2019s an easier way to do this).\n\n[1] Y. Lecun, L. Bottou, Y. Bengio and P. Haffner, \u201cGradient-based learning applied to document recognition,\u201d in Proceedings of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, Nov 1998.", 
        "title": "Getting Start with Deep Learning: \u2013 Akina M. \u2013"
    }
]