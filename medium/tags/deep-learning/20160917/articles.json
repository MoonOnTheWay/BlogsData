[
    {
        "url": "https://medium.com/@acrosson/building-a-deep-learning-box-d17d97e2905c?source=tag_archive---------0----------------", 
        "text": "This is part one of our building a deep learning machine series. You can find the other posts here:\n\nDeep learning is this amazing subfield of machine learning that has exploded in recent years. Many deep learning models have been arounds for over two decades, but it wasn\u2019t until the last few years that they started becoming popularized.\n\nA deep learning model requires two things: 1. a ton of data, 100s of megabytes, if not gigabytes of data (as a minimum) and 2. high computational power. It was discovered around 2009 that graphics cards (GPUs) can be used to supercharge deep learning. Standard computers have a few cores, maybe a dozen on a higher end machine. GPUs have thousands of cores, whereby computations can be parallelized increasing computing time by orders of magnitude.\n\nIf you\u2019re serious about deep learning, or building an AI startup, it might be a good idea to build your own rig. Amazon charges you an arm and a leg, and their hardware is obsolete. In this post I\u2019m going to outline how to build a computer dedicated to deep learning, while keeping the price tag below $5,000.\n\nThe first thing we\u2019re going to need are some GPUs. We chose to get 4 Asus GTX 1080s. The new GTX Titan Xs were only recently announced, and have an increased price tag. We purchased the Asus GTX 1080 Founders Edition. If we could do it again, we would not get the founders edition, simply because of the increased cost. Each GPU cost around $740 (on average). Finding them proves to be difficult. Online marketplaces like Amazon consistently run out. We ended up purchasing 1 from Amazon, 1 from B&H, and 2 from eBay.\n\nDepending on your needs you can probably get away with the GTX 1070s, but we felt the 1080s had the best bang for the buck.\n\nFor the motherboard, you\u2019re going to need to find one that has 4 PCI 3.0 slots. There aren\u2019t many motherboards with this option, so it\u2019s slim pickings. We settled for the Gigabyte x99p-SLI. It cost $284.\n\nFor RAM, we decided to get 4\u201316GB Ballistix DDR4 and 2\u20138GB Ballist DDR = 80GB of total RAM. The motherboard supports up to 8 sticks of RAM, so if you feel inclined to, MAX it out. The additional 8GB sticks were only purchased to save some money and get a little more memory.\n\nFor the CPU we chose Intel\u2019s 5930k i7, a 6 core processor (12 virtual cores). We originally made the mistake of purchasing the 6700k i7, but found it was incompatible with our motherboard. And then we tried the 5820k, but realized it only supports 3 PCI slots. Finally we bought the 5930k i7 and we were off to the races.\n\nI would suggest using PC Part Picker to ensure everything is compatible before purchasing your hardware. Don\u2019t make the same mistakes we did\u200a\u2014\u200ait ended up costing us more and a few trips to Fry\u2019s Electronics.\n\nIn order to power the RIG, you\u2019ll need a lot of watts/amps. We purchased the Hercules 1600W power supply. While expensive, the extra power ensures we can get all the performance out of the GPUs. The packaging also looks cool.\n\nFor storage we purchased 5\u2013500GB Samsung 850 EVO solid state hard drives. We decided on getting a few hard drives because we wanted to create virtual machines (VM) for each GPU and have the data allocated to each VM; saving the 5th hard drive for the host. If you\u2019re building a custom rig, I would recommend just purchasing a 1TB SSD or possibly a 2TB if you have the cash.\n\nIf you plan on putting your machine inside a server rack, I would recommend purchasing the CHENBRO RM41300-FS81. We originally used a different 4 unit server chassis and realize that it didn\u2019t have 8 expansions slots (2 for each GPU). In fact the CHENBRO is the only reasonably priced server chassis with 8 expansion slots.\n\nIf you have any questions about purchasing parts, or building the machine feel free to tweet us @acrosson @calerogers\n\nLike and share if you find this helpful!", 
        "title": "Building a Deep Learning Box \u2013 Alexander Crosson \u2013"
    }, 
    {
        "url": "https://medium.com/self-driving-cars/tensorflow-vs-tf-learn-vs-keras-vs-tf-slim-b83811966020?source=tag_archive---------1----------------", 
        "text": "One module in Udacity\u2019s Self-Driving Car Nanodegree program will cover deep learning, with a focus on automotive applications.\n\nWe\u2019ve decided to use the TensorFlow library that Google has built as the main tool for this module.\n\nCaffe, an alternative framework, has lots of great research behind it, but TensorFlow uses Python, and our hope is that this will make learning it a lot easier for students.\n\nEven with TensorFlow, however, we face a choice of which \u201cfront-end\u201d framework to use. Should we use straight TensorFlow, or TF Learn, or Keras, or the new TF-Slim library that Google released within TensorFlow.\n\nRight now we\u2019re learning toward TF Learn, almost by default. Straight TensorFlow is really verbose, TF-Slim seems new and under-documented. Keras and TF Learn both seem solid, but the TF Learn syntax seems a little cleaner.\n\nOne big drawback to TF Learn, though, is the lack of easily integrated pre-trained models. I spent a while today trying to figure out how to migrate pre-trained AlexNet weights from Caffe to TF Learn.\n\nSo far, no one solution is jumping out at me as perfect. Let me know in the comments if you\u2019ve got a suggestion.", 
        "title": "TensorFlow vs. TF Learn vs. Keras vs. TF-Slim \u2013 Self-Driving Cars \u2013"
    }, 
    {
        "url": "https://medium.com/@dudchuk/data-scientist-%D0%B2-luka-fb26eed16769?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Data Scientist \u0432 Luka \u2013 Philip Dudchuk \u2013"
    }, 
    {
        "url": "https://medium.com/@andraganescu/shameless-plug-but-sincere-nonetheless-c65382d0aa5?source=tag_archive---------3----------------", 
        "text": "Hi Nitin, I really enjoyed this story even more so as it is something I\u2019ve mulled on for a lot of time. I liked this idea the most:\n\nIt is one of the most not obvious features of the universe and of humanity in general, not obvious because we\u2019re so conditioned to survive and hence paying attention to individual units far more than to what emerges overall.\n\nI\u2019d like to invite you, when you have the time and patience, to take a skim of my essay linked below, as it might be fun for you considering your plan to extend the framework expressed in this story to higher level concepts such as morality or karma.\n\nIt is a longish read, 66mins Medium says, but hey I did add a ToC for convenience. The reason I am sending you there is that I\u2019d love a dialogue maybe we can dive into these ideas we found independently but which appear to have some common ground.", 
        "title": "Shameless plug, but sincere nonetheless \u2013 Andrei Draganescu \u2013"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-7-209b01cdb931?source=tag_archive---------4----------------", 
        "text": "Microsoft reports that in a recent benchmark evaluation against the industry standard Switchboard speech recognition task, its researchers achieved a word error rate (WER) of 6.3 percent, the lowest in the industry.\n\nThe company has rebuilt SwiftKey\u2019s language engine from the ground up using the power of neural network technology\u200a\u2014\u200atheir first instance of neural networks being used locally on a smartphone.\n\nSingapore\u2019s ViSenze raises $10.5M to bring the benefits of AI to e-commerce\n\nViSenze develops artificial intelligence for use in e-commerce, working with companies like Myntra in India and Zalora in Southeast Asia.\n\nMarketplace for Algorithms Offers the Latest in AI\n\nAlgorithmia joins the effort to democratize AI, using the same marketplace model that startups have applied to so many other goods and services.", 
        "title": "\u2014 Issue #7 \u2013"
    }, 
    {
        "url": "https://charliegedeon.com/the-school-of-3e5fbca2e8a9?source=tag_archive---------5----------------", 
        "text": "As part of this final project, I\u2019ve been thinking a lot about standardization in regards to the educational system. Personally, one of the biggest burns I experienced in high-school was having to adhere to a system that had no room for computation and design\u200a\u2014\u200atwo of the main things I\u2019ve been doing as work and now studying at CIID, a progressive project-based design school in Copenhagen.\n\nIn a flurry of research on education, motivation, and pedagogy, there are a few things surrounding schools and learning that I concluded:\n\nFollowing these discoveries, a friend pointed me in the direction of a critical pedagogy project by Ted Hunt called The School Of _____.\n\nI decided to have a crack at it by combining all I know and enjoy about technology, and all the angst I have against standardization in education. The result is 2 schools: The School of Open-Source Everything and The School of Snowflake curriculums. You can read the 200 word submissions for each below.\n\nWith the advent of technology, the possibility to make the source of knowledge not only globally accessible, but also editable is finally a reality.\n\nAs OSES (Open-Source Everything School), we allow and encourage students to manipulate any data pertaining to the school, including, but not limited to, their own curriculum, their peers\u2019 curriculum, studying material, assessment questions, administration tools, and the school\u2019s website.\n\nStudents, faculty and administration at OSES are all called contributors. Contributors ranks are defined by the amount they contribute to the data, as well as by peer-reviewed quality checks, much like existing open-source communities such as Wikipedia.\n\nAll changes and contributions are always tracked and anyone can choose to revert to previous versions if need be. One of the benefits of an open-source system is that members that don\u2019t agree with the source code changes, but want to develop on that base, can create a fork and continue to develop it on their own.\n\nWe believe this highly fluid model, based largely on mutual trust, will empower our contributors to become active participants in society outside of the school, and help them develop their own communities as well.\n\nNo two snowflakes are ever alike, and nor should this be true of any two curriculums. However, we can\u2019t expect of any human, or group of humans for that matter, to achieve this utopic ideal in a way that is economically viable and psychologically healthy.\n\nThanks to advances in machine learning, we can envision a school with an operating system taking the place of people doing repetitive administrative work. SchoolOS, as it would be called, would be able to observe students\u2019 growth based on a variety of inputs including assignments, psychological tests, and student preferences. It will then build a completely customized curriculum and study guide for said student that will be reactionary and evolve dynamically.\n\nSchoolOS will free up the time from faculty by eliminating all administrative tasks so that the teacher can focus on what a teacher does best: facilitate group dynamics and expose children to new materials.\n\nThe excuses pertaining to cost and human resources for standardized testing and curriculums will be rendered immediately null with the introduction of SchoolOS. It will ensure that students are learning what both they, and society, need but provide it at a time and pace that is suited entirely to the individual.", 
        "title": "Two Visions for Radically Technological Schools \u2013"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/predict-the-production-gas-quality-of-a-chemical-plant-the-deep-learning-1635fd647907?source=tag_archive---------6----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Predict The Production Gas Quality of A Chemical Plant The Deep Learning"
    }
]