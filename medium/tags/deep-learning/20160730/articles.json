[
    {
        "url": "https://artplusmarketing.com/ais-road-to-the-mainstream-3a04b2aebe8e?source=tag_archive---------0----------------", 
        "text": "When I enrolled in Computer Science in 1995, Data Science didn\u2019t exist yet, but a lot of the algorithms we are still using already did. And this is not just because of the return of the neural networks, but also because probably not that much has fundamentally changed since back then. At least it feels to me this way. Which is funny considering that starting this year or so AI seems to finally have gone mainstream.\n\n1995 sounds like an awful long time ago, before we had cloud computing, smartphones, or chatbots. But as I have learned these past years, it only feels like a long time ago if you haven\u2019t been there yourself. There is something about the continuation of the self which pastes everything together and although a lot has changed, the world didn\u2019t feel fundamentally different than it does today.\n\nNot even Computer Science was nowhere as mainstream as it was today, that came later, with the first dot com bubble around the year 2000. Some people even questioned my choice to study computer science at all, because apparently programming computers was supposed to become so easy no specialists are required anymore.\n\nActually, artificial intelligence was one of the main reasons for me to study computer science. The idea to use it as an constructive approach to understanding the human mind seemed intriguing to me. I went through the first two years of training, made sure I picked up enough math for whatever would lie ahead, and finally arrived in my first AI lectured held by Joachim Buhmann, back then professor at the University of Bonn (where Sebastian Thrun was just about to leave for the US).\n\nI would have to look up where in his lecture cycle I joined but he had two lectures on computer vision, one on pattern recognition (mostly from the old editions of the Duda & Hart book), and one in information theory (following closely the book by Cover & Thomas). The material was interesting enough, but also somewhat disappointing. As I now know, people stopped working on symbolic AI and instead stuck to more statistical approaches to learning, where learning essentially was reduced to the problem of picking the right function based on a finite amount of observations.\n\nThe computer vision lecture was even less about learning and relied more on explicit physical modelling to derive the right estimators, for example, to reconstruct motion from a video. The approach back then was much more biologically and physically motivated than nowadays. Neural networks existed, but everybody was pretty clear that they were just \u201canother kind of function approximators.\u201d\n\nEveryone with the exception of Rolf Eckmiller, another professor where I worked as a student. Eckmiller had built his whole lab around the premise that \u201cneural computation\u201d was somehow inherently better than \u201cconventional computation\u201d. This was back in the days when NIPS had full tracks devoted to studying the physiology and working mechanisms of neurons, and there were people who believed there is something fundamentally different happening in our brains, maybe on a quantum level, that gives rise to the human mind, and that this difference is a blocker for having truly intelligent machines.\n\nWhile Eckmiller was really good at selling his vision, most of his staff was thankfully much more down to earth. Maybe it is a very German thing, but everybody was pretty matter of fact about what these computational models could or couldn\u2019t do, and that has stuck with me throughout my studies.\n\nI graduated in October 2000 with a pretty farfetched master thesis trying to make a connection between learning and hard optimization problems, then started on my PhD thesis and stuck around in this area of research till 2015.\n\nWhile there had always been attempts to prove industry relevance, it was a pretty academic endeavor for a long while, and the community was pretty closed up. There were individual success stories, for example around handwritten character recognition, but many of the companies around machine learning failed. One of these companies I remember was called Beowulf Labs and one NIPS they went around recruiting people with a video which promised it to be the next \u201cmathtopia\u201d. In essence, this was the story of DeepMind, recruiting a bunch of excellent researchers and then hoping it will take off.\n\nThe whole community also revolved around one fashion to the next. One odd thing about machine learning as a whole is that there exist only a handfull of fundamentally different problems like classification, regression, clustering, and so on, but a whole zoo of approaches. It is not like in physics (I assume) or mathematics where some generally agreed upon unsolved hard problems exist whose solution would advance the state of the art. This means that progress is often done laterally, by replacing existing approaches with a new one, still solving the same problem in a different way. For example, first there were neural networks. Then support vector machines came, claiming to be better because the associated optimization problem is convex. Then there was boosting, random forests, and so on, till the return of neural networks. I remember that Chinese Restaurant Processes were \u201chot\u201d for two years, no idea what their significance is now.\n\nThen there came Big Data and Data Science. Being still in academia at the time, it always felt to me as if this was definitely coming from the outside, possibly from companies like Google who had to actually deal with enormous amounts of data. Large scale learning always existed, for example for genomic data in bioinformatics, but one usually tried to solve problems by finding more efficient algorithms and approximations, not by parallelizing brute force.\n\nCompanies like Google finally proved that you can do something with massive amounts of data, and that finally changed the mainstream perception. Technologies like Hadoop and NoSQL also seemed very cool, skillfully marketing themselves as approaches so new, they wouldn\u2019t suffer from the technological limitations of existing systems.\n\nBut where did this leave the machine learning researchers? My impression always was that they were happy that they finally got some recognition, but they were also not happy about the way this happened. To understand this, one has to be aware that most ML reseachers aren\u2019t computer scientists or very good or interested in coding. Many come from physics, mathematics or other sciences, where their rigorous mathematical training was an excellent fit for the algorithm and modeling heavy approach central to machine learning.\n\nHadoop on the other hand was extremely technical. Written in Java, a language perceived as being excessively enterprise-y at the time, it felt awkward and clunky compared to the fluency and interactiveness of first Matlab and then Python. Even those who did code usually did so in C++, and to them Java felt slow and heavy, especially for numerical calculations and simulations.\n\nStill, there was no way around it, so they rebranded everything they did as Big Data, or began to stress, that Big Data only provides the infrastructure for large scale computations, but you need someone who \u201cknows what he is doing\u201d to make sense of the data.\n\nWhich is probably also not entirely wrong. In a way, I think this divide is still there. Python is definitely one if the languages of choice for doing data analysis, and technologies like Spark try to tap into that by providing Python bindings, whether it makes sense from a performance point of view or not.\n\nEven before DeepDream, neural networks began making their return. Some people like Yann LeCun have always stuck to this approach, but maybe ten years ago, there where a few works which showed how to use layerwise pretraining and other tricks to train \u201cdeep\u201d networks, that is larger networks than one previously thought possible.\n\nThe thing is, in order to train neural networks, you evaluate it on your training examples and then adjust all of the weights to make the error a bit smaller. If one writes the gradient across all weights down, it naturally occurs that one starts in the last layer and then propagate the error back. Somehow, the understanding was that the information about the error got smaller and smaller from layer to layer and that made it hard to train networks with many layers.\n\nI\u2019m not sure that is still true, as far as I know, many people are just using backprop nowadays. What has definitely changed is the amount of available data, as well as the availability of tools and raw computing power.\n\nSo first there were a few papers sparking the interest in neural networks, then people started using them again, and successively achieved excellent results for a number of application areas. First in computer vision, then also for speech processing, and so on.\n\nI think the appeal here definitely is that you can have one approach for all. Why the hassle of understanding all those different approaches, which come from so many different backgrounds, when you can understand just one method and you are good to go. Also, neural networks have a nice modular structure, you can pick and put together different kinds of layers and architectures to adapt them to all kinds of problems.\n\nThen Google published that ingenious deep dream paper where they let a learned network generate some data, and we humans with our immediate readiness to read structure and attribute intelligence picked up quickly on this.\n\nI personally think they were surprised by how viral this went, but then decided the time is finally right to go all in on AI. So now Google is an \u201cAI first\u201d company and AI is gonna save the world, yes.\n\nMany academics I have talked to are unhappy about the dominance of deep learning right now, because it is an approach which works well, maybe even too well, but doesn\u2019t bring us much closer to really understand how the human mind works.\n\nI also think the fundamental problem remains unsolved. How do we understand the world? How do we create new concepts? Deep learning stays an imitation on a behavioral level and while that may be enough for some, it isn\u2019t for me.\n\nAlso, I think it is dangerous to attribute too much intelligence to these systems. In raw numbers, they might work well enough, but when they fail they do so in ways that clearly show they operate in an entirely different fashion.\n\nWhile Google translate lets you skim the content of website in a foreign language, it is still abundantly clear that the system has no idea what it is doing.\n\nSometimes I feel like nobody cares, also because nobody gets hurt, right? But maybe it is still my German cultural background that would rather prefer we see things as they are, and take it from there.", 
        "title": "AI\u2019s Road to the Mainstream \u2013"
    }, 
    {
        "url": "https://medium.com/@EnchantedCshel/what-do-videos-fergie-chrissy-teigen-kim-kardashian-and-nursing-moms-have-in-common-57a2250a3fdd?source=tag_archive---------1----------------", 
        "text": "Huh? What did I say? Have I gone completely BONKERS? The jury is still out on THAT, but hopefully, it\u2019ll all make sense very soon.\n\nIf you use videos in your blogs as much as I love to do, you\u2019ll definitely want to read this post!\n\nMarshall McLuhan had a lot more to say than his ubiquitous \u201cThe medium is the message.\u201d\n\n\u201cWe shape our tools and thereafter our tools shape us.\u201d\n\n\u201cThe new electronic interdependence recreates the world in the image of a global village.\u201d\n\n\u201cAdvertising is the greatest art form of the 20th century.\u201d\n\nAnd this is one of my faves: \u201cDiaper backward spells repaid\u201d. (Think about it.)\n\nWhich leads me into this message of MY medium.\n\nI\u2019ve shared a bit about my son we all refer to as Angel Boy, esteemed professor in the Pacific Northwest with a Ph.D. from Yale, and I\u2019ve shared a few stories about Angel Boy 2.0, my one and only amazing and brilliant grandson\u200a\u2014\u200abut I\u2019ve not talked much about my DIL, other than to tell you that she\u2019s also brilliant and even more spectacular is the fact that she successfully gestated the world\u2019s most beautiful manchild (lol).\n\nAfter doing something like that, there\u2019s not really anything else she\u2019d need to do to become my favorite daughter-in-law in the whole world, but there\u2019s more to tell\u2026\n\nI am so VERY proud of her!\n\nMore than 1.8 billion pictures are uploaded to the Internet every day\u200a\u2014\u200aand she knows exactly which ones you\u2019ll choose to look at.\n\nWith a Ph.D. in Neuroscience from Brown University, my DIL is co-founder of Neon\u2013a company that uses human neuroscience and machine learning to automatically select images for some of the world\u2019s largest publishers and platforms.\n\nNeon is supported by leading Silicon Valley venture capitalists, the National Science Foundation, and has been recognized by the World Economic Forum, The White House, Fast Company and others.\n\nGet to know a bit more about Sophie HERE where she was chosen one of Fast Company\u2019s most creative people of 2015.\n\nDIL has been working hard juggling start up life, delivering a beautiful baby, AND A BEAUTIFUL APP.\n\nHere\u2019s the cool part for us bloggers and anyone who loves videos\u2013 the app is user-friendly for those of us who are less than brainy techies\u2013YAY!!\n\nNeon Pro is FREE and available at https://app.neon-lab.com.\n\nI know you\u2019ll do me a HUGE favor and re-post and share and post on your walls and tweet all about this amazing FREE app!\n\nSince the title of this post should contain content that makes SENSE, here\u2019s my breastfeeding connection. Thirty-five years ago, I did it night and day with my Angel Boy. Now DIL feeds precious AB 2.0 the same way. Mothers have been feeding their babies this way FOREVER. Animals do it; whales and dolphins, too. It\u2019s beautiful and natural. Although I was a stay-at-home-mom, I am in awe of working moms who manage to do it all like my DIL, Fergie, Chrissy Teigen, and even Kim Kardashian.\n\nHave you seen Fergie\u2019s new video?\n\nI processed Fergie\u2019s M.I.L.F.$ video (with her celebrity momfriends) through the Neon app. MILF, anyone?\n\nHere\u2019s another video I processed of the paddleboarder in Half Moon Bay who had an encounter with a whale.\n\n\u00a0http://neon.li/2aleepl\n\nNow YOU try it! Neon Pro is FREE and available at https://app.neon-lab.com.\n\nBREAKING NEWS\u2026San Francisco\u200a\u2014\u200aJuly 28, 2016\u200a\u2014\u200aNeon Labs (https://neon-lab.com/), the video and image performance company, today announced the availability of Neon Pro, a free web app that makes the company\u2019s deep learning technology, and NeonScoreTM, available to individual content creators. Previously, the technology\u200a\u2014\u200awhich identifies and serves high performing video thumbnails and images\u200a\u2014\u200awas only available to global image, video, eCommerce and content platforms operating at massive scale, through Neon EnterpriseTM.\n\nFounded on a decade of neurocognitive research at Brown University, Carnegie Mellon University, and Harvard Medical School, Neon technology combines the science of human perception, deep learning, and the world\u2019s largest and most comprehensive dataset of emotional responses to images. Using deep neural nets trained on human visual perception data, rather than just clickstream data, Neon predicts how people will emotionally respond to an image, and how effective the image will be in driving engagement.\n\nNeon\u2019s predictive image technology helps businesses drive significantly higher clicks, likes and shares for videos and images, resulting in increased revenue. The company guarantees that Neon Enterprise customers will increase their overall engagement rate. Depending on the content and context, increases in engagement of 30% and higher are common.\n\nNeon Pro, a free and slimmed-down version of the company\u2019s enterprise offering, now available as a web app, allows individuals and content producers to get NeonScores for their videos and images. Neon Pro identifies the thumbnails and images that are guaranteed to increase engagement over human-selected images and thumbnails.\n\nNeon Pro is free and available at https://app.neon-lab.com.\n\nHow NeonScore Works:\n\n\u00a0The NeonScore is a number from 0 to 99, common to Neon Pro and Neon Enterprise, that measures the predicted emotional impact of an image for a given audience, device or platform. The higher the number, the higher the predicted engagement. NeonScore uses the company\u2019s patent pending methods to analyze every image or video frame for over 1,000 unique and interrelated \u201cvalence\u201d features that drive human interest, such as eye gaze, instability, brightness, and incompleteness.\n\n\u201cUnlike the traditional deep neural networks that are trained to identify objects in a scene, Neon uses deep learning in a creative way to predict the emotional response to images at massive scale. This novel approach has helped us identify the features of an image that drive engagement, allowing us to predict the images that will go viral, even before they are published\u201d said Sophie Lebrecht, Neon Chief Science Officer.", 
        "title": "What do videos, Fergie, Chrissy Teigen, Kim Kardashian, and nursing moms have in common?"
    }
]