[
    {
        "url": "https://medium.com/@kidargueta/deep-feelings-about-deep-learning-8e300cdcc047?source=tag_archive---------0----------------", 
        "text": "So I want to build Artificial Emotional Intelligence (AEI), and I already wrote about a possible application to treat mental health problems. Even the big guns like Apple Inc. are trying to build AEI (for some obscure reason). So the obvious step when you want to build something is to study and to do research.\n\nAs much as I tried not to fall for the hype recently gained by Deep Learning, I could not really resist to explore their promises. Let me quickly explain. In order to build real AEI I wanted to start by the component that can understand our words. This belongs to the fields called Natural Language Processing (NLP), and Computational Linguistics (CL). Building powerful and useful NLP/CL systems is extremely challenging. It took me nearly 3 years to build a system that can guess your emotions from what you write, and the accuracy is far from perfect. The reason is that such systems are traditionally built using manually defined rules, features, and algorithms tailored for specific tasks.\n\nDeep Learning, on the other hand, promises to replace handcrafted features with efficient algorithms able to \u201clearn\u201d the features automatically from some input data, saving you all the hard work. So yeah! When you think about this it makes sense to want to give it a try. And so I did. First I studied the basics of Artificial Neural Networks using the awesome Coursera Machine Learning Course. Then, to complement that knowledge I read this great online book, and checked these fantastic video tutorials. All that taught me to play with toy Deep Networks on code fully written by me. When I was ready I jumped to TensorFlow, a full-fledged Deep Learning software library and followed their tutorials to train Deep Networks to classify handwritten characters. My reaction? A rush of elation followed by a bit of disappointment.\n\nDon\u2019t take me wrong, Deep Learning is awesome. There is mathematical proof that in theory they can solve any problem. The handwritten characters classification tutorial, although simple, hints to that. Yet there is something about Deep Learning that leaves a sour taste in the mouth. During my previous research project, I always felt I was in control, and in most cases I could justify why things worked. With Deep Learning, it all felt like magic. Except for the valid mathematical intuition, you can\u2019t really understand what\u2019s going on inside the black box that is the constructed Networks. Moreover, even the state-of-the-art systems where constructed in an empirical way, by testing different network architectures until finding the best performer, with little clue of why it performs better.\n\nSo yes, Deep Learning can solve complex problems, yes it can save time and effort, but without clear understanding of what is going on inside, it might lead to many frustrations in the process. As soon as I move past the tutorials and into developing the first part of my AEI, I will post more about my feelings towards Deep Learning.", 
        "title": "Deep Feelings about Deep Learning \u2013 Carlos Argueta \u2013"
    }, 
    {
        "url": "https://medium.com/@hciresearcher/best-gpus-for-deep-learning-a84d4d87c196?source=tag_archive---------1----------------", 
        "text": "To quote the article\u2019s TL;DR,\n\nBest GPU overall: GTX Titan X\n\nCost efficient but expensive: GTX Titan X, GTX 980, GTX 980 Ti\n\nCost efficient but troubled: GTX 580 3GB (lacks software support) or GTX 970 (has memory problem)\n\nCheapest card with no troubles: GTX 960 4GB or GTX 680\n\nI work with data sets > 250GB: GTX Titan, GTX 980 Ti or GTX Titan X\n\nI have little money: GTX 680 3GB eBay\n\nI have almost no money: AWS GPU spot instance\n\nI do Kaggle: GTX 980 or GTX 960 4GB\n\nI am a researcher: 1\u20134x GTX Titan X\n\nI want to build a GPU cluster: This is really complicated, you can get some ideas here\n\nI started deep learning and I am serious about it: Start with one GTX 680, GTX 980, or GTX 970 and buy more of those as you feel the need for them; save money for Pascal GPUs in 2016 Q2/Q3 (they will be much faster than current GPUs)", 
        "title": "Best GPUs for Deep Learning \u2013 Minsuk \u2013"
    }
]