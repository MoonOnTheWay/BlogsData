[
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------0----------------", 
        "text": "Let\u2019s tackle this problem one step at a time. For each step, we\u2019ll learn about a different machine learning algorithm. I\u2019m not going to explain every single algorithm completely to keep this from turning into a book, but you\u2019ll learn the main ideas behind each one and you\u2019ll learn how you can build your own facial recognition system in Python using OpenFace and dlib.\n\nThe first step in our pipeline is face detection. Obviously we need to locate the faces in a photograph before we can try to tell them apart!\n\nIf you\u2019ve used any camera in the last 10 years, you\u2019ve probably seen face detection in action:\n\nFace detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we\u2019ll use it for a different purpose\u200a\u2014\u200afinding the areas of the image we want to pass on to the next step in our pipeline.\n\nFace detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We\u2019re going to use a method invented in 2005 called Histogram of Oriented Gradients\u200a\u2014\u200aor just HOG for short.\n\nTo find faces in an image, we\u2019ll start by making our image black and white because we don\u2019t need color data to find faces:\n\nThen we\u2019ll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it:\n\nOur goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker:\n\nIf you repeat that process for every single pixel in the image, you end up with every pixel being replaced by an arrow. These arrows are called gradients and they show the flow from light to dark across the entire image:\n\nThis might seem like a random thing to do, but there\u2019s a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the direction that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve!\n\nBut saving the gradient for every single pixel gives us way too much detail. We end up missing the forest for the trees. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image.\n\nTo do this, we\u2019ll break up the image into small squares of 16x16 pixels each. In each square, we\u2019ll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc\u2026). Then we\u2019ll replace that square in the image with the arrow directions that were the strongest.\n\nThe end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:\n\nTo find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:\n\nUsing this technique, we can now easily find faces in any image:\n\nIf you want to try this step out yourself using Python and dlib, here\u2019s code showing how to generate and view HOG representations of images.\n\nWhew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer:\n\nTo account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps.\n\nTo do this, we are going to use an algorithm called face landmark estimation. There are lots of ways to do this, but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Sullivan.\n\nThe basic idea is we will come up with 68 specific points (called landmarks) that exist on every face\u200a\u2014\u200athe top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face:\n\nHere\u2019s the result of locating the 68 face landmarks on our test image:\n\nNow that we know were the eyes and mouth are, we\u2019ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. We won\u2019t do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called affine transformations):\n\nNow no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate.\n\nIf you want to try this step out yourself using Python and dlib, here\u2019s the code for finding face landmarks and here\u2019s the code for transforming the image using those landmarks.\n\nNow we are to the meat of the problem\u200a\u2014\u200aactually telling faces apart. This is where things get really interesting!\n\nThe simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right?\n\nThere\u2019s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can\u2019t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.\n\nWhat we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you\u2019ve ever watched a bad crime show like CSI, you know what I am talking about:\n\nOk, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?\n\nIt turns out that the measurements that seem obvious to us humans (like eye color) don\u2019t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.\n\nThe solution is to train a Deep Convolutional Neural Network (just like we did in Part 3). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face.\n\nThe training process works by looking at 3 face images at a time:\n\nThen the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:\n\nAfter repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.\n\nMachine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist.\n\nThis process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy.\n\nBut once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!\n\nSo all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here\u2019s the measurements for our test image:\n\nSo what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn\u2019t really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person.\n\nIf you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file. You run it like this.\n\nThis last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image.\n\nYou can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We\u2019ll use a simple linear SVM classifier, but lots of classification algorithms could work.\n\nAll we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person!\n\nSo let\u2019s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:", 
        "title": "Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning"
    }, 
    {
        "url": "https://medium.com/machina-sapiens/aprendizagem-de-m%C3%A1quina-%C3%A9-divertido-parte-4-reconhecimento-facial-moderno-com-deep-learning-72525d9684c2?source=tag_archive---------1----------------", 
        "text": "Detec\u00e7\u00e3o de rosto \u00e9 uma \u00f3tima fun\u00e7\u00e3o para c\u00e2meras. Se a c\u00e2mera pode detectar automaticamente os rostos, ela pode garantir que todos est\u00e3o em foco antes de fazer a foto. Mas vamos utilizar esta fun\u00e7\u00e3o com outra finalidade\u200a\u2014\u200aencontrar as \u00e1reas na imagem que queremos passar para o pr\u00f3ximo passo do nosso pipeline.\n\nA ideia b\u00e1sica \u00e9 identificar 68 pontos espec\u00edficos que existem em todo rosto (chamados marcas\u200a\u2014\u200alandmarks), a ponta do queixo, a borda externa de cada olho, a borda interna de cada sobrancelha, etc. Iremos ent\u00e3o treinar um algoritmo de aprendizagem para encontrar estes 68 pontos no rosto:\n\nA abordagem mais simples para o problema de reconhecimento de rosto \u00e9 comparar o rosto desconhecido que foi encontrado no passo 2 com todas as imagens de pessoas que j\u00e1 foram previamente identificadas. Quando encontramos um rosto previamente identificado que se parece muito similar com nosso rosto desconhecido, deve ser a mesma pessoa. Parece uma boa ideia, certo?\n\nInfelizmente h\u00e1 um grande problema com esta abordagem. Um site como Facebook com bilh\u00f5es de usu\u00e1rios e um trilh\u00e3o de fotos n\u00e3o pode varrer em tempo h\u00e1bil todas as fotos pr\u00e9-identificadas para compar\u00e1-las com uma nova foto que tenha sido adicionada. Isto iria demorar tempo demais. Eles precisam poder reconhecer um rosto em milisegundos, n\u00e3o em horas.\n\nO que precisamos \u00e9 uma maneira de extrair algumas caracter\u00edsticas b\u00e1sicas de cada rosto. Assim, poder\u00edamos mensurar o rosto desconhecido do mesmo modo e encontrar os rostos que tenham as medidas b\u00e1sicas mais similares. Por exemplo, poder\u00edamos medir o tamanho de cada orelha, o espa\u00e7o entre os olhos, o tamanho do nariz, etc. Se voc\u00ea j\u00e1 assitiu s\u00e9ries de TV como CSI, voc\u00ea sabe do que estou falando:\n\nAcontece que as caracter\u00edsticas que parecem t\u00e3o \u00f3bvias para n\u00f3s (como cor dos olhos) n\u00e3o fazem sentido para o computador que est\u00e1 considerando pixels individuais em uma imagem. Pesquisadores descobriram que a abordagem mais precisa \u00e9 deixar o computador encontrar ele mesmo as caracter\u00edsticas a serem coletadas. Deep learning tem um desempenho melhor que humanos na descoberta de quais caracter\u00edsticas do rosto s\u00e3o mais importantes para se medir.\n\nTudo que temos que fazer, ent\u00e3o, \u00e9 analisar nossas imagens com as redes pr\u00e9-treinadas e obter as 128 medidas para cada rosto. Aqui est\u00e1 a lista de medidas para nossa imagem de teste:", 
        "title": "Aprendizagem de M\u00e1quina \u00e9 Divertido! Parte 4 \u2013 Machina Sapiens \u2013"
    }, 
    {
        "url": "https://medium.com/this-week-in-machine-learning-ai/machine-learning-for-datacenter-optimization-a-crazy-new-gpu-from-nvidia-faster-rnn-training-20be24202422?source=tag_archive---------2----------------", 
        "text": "This week\u2019s show covers Google\u2019s use of machine learning to cut datacenter power consumption, NVIDIA\u2019s new \u2018crazy, reckless\u2019 GPU, and a new Layer Normalization technique that promises to reduce the training time for deep neural networks. Plus, a bunch more.\n\nThis week\u2019s podcast is sponsored by Cloudera, organizers of the Wrangle Conference which is coming up in San Francisco on July 28th. Check out theevent page for information on the great talks and speakers they\u2019ve got planned, and if you decide to register, use the code \u201cCOMMUNITY\u201d for 20% off!", 
        "title": "Machine Learning for Datacenter Optimization, a \u2018Crazy\u2019 New GPU from NVIDIA & Faster RNN Training \u2014\u2026"
    }
]