[
    {
        "url": "https://medium.com/transmission-newsletter/deep-learning-is-revolutionary-d0f3667bafa0?source=tag_archive---------0----------------", 
        "text": "1 Stuck with a low-resolution photo? Deep learning can predict what the higher-resolution photo might look like, and add missing details\n\n2 Deep learning has enabled a text-to-speech system that is almost indistinguishable from human voice. Think of the possibilities!\n\n3 Deep learning can compose classical music that you\u2019d believe to be created by a human\n\n4 Deep learning can replicate the style of your favorite painter with the image of your choice\n\n6Deep learning can auto-fill missing parts of an image, by predicting what should be in that space\n\n7Deep learning can train a robot to walk like a human\n\n8Deep learning can caption an image, just like a human would\n\n9Want to sketch a beautiful landscape but can\u2019t draw? Don\u2019t worry, deep learning can take it from here!\n\n10Best of all (at least to me), you can train a deep neural network to steer a car, just like a human\n\n\u2026it even turns out Grand Theft Auto is a great simulation environment for training a self-driving car\n\nWe\u2019ve spent decades building and refining machines to be computationally powerful. In the next decade I hope we discover what the human race will be empowered to do when our software thinks.", 
        "title": "Deep Learning is Revolutionary \u2013 Transmission \u2013"
    }, 
    {
        "url": "https://medium.com/devopsion/how-to-fake-it-as-an-artist-with-docker-aws-and-deep-learning-6d42f4acd890?source=tag_archive---------1----------------", 
        "text": "Co-authored with \u00c1lvaro Barbero, Docker ninja and Chief Data Scientist at Instituto de Ingenier\u00eda del Conocimiento (IIC)\n\nIn UK Channel 4 documentaries series \u201cFaking it\u201d, Paul O\u2019Hare, a painter and decorator from Liverpool, was given just four weeks to transform himself into a fine artist and attempt to fool the critics at a London art gallery. We are going to show how to do it in less than half an hour and with a little help of Docker, AWS and Deep Learning, including the time you need to read this entry. And for less than $10.\n\nIn order to speed up your transformation, we are going to rely on an artificial intelligence system. This AI system is based on a Deep Neural Network that creates artistic images indistinguishable (we think) from the works of an artist. How is this achieved? By combining the content of one image\u200a\u2014\u200aa portrait or a landscape photography\u200a\u2014\u200awith the style of another image\u200a\u2014\u200atypically, the works of a recognized artist\u200a\u2014\u200a. We\u2019d use an algorithm called neural-style, based on a powerful deep network for image processing.\n\nBut it\u2019s much easier to see what it does by looking at the resulting pictures. These pictures are worth a thousand hours of research or lines of code\u00a0;)\n\nThe mysterious workings of this algorithm start by reusing a prebuilt deep neural network known as VGG19, developed by Oxford researchers and winner of the ImageNet Challenge 2014 image processing competition. This network employs multiple layers of Convolutional Neural Networks (CNNs) to refine an image from raw pixels to higher-level, more conceptual representations of the picture. So high in fact that the style can be faithfully represented by the correlations emerging at the deepest levels of the network. It is through this raw-pixels-versus-style decomposition that the network can be exploited to create new images, redrawing the pixels of a photo in the style of a different picture.\n\nUntil now implementing and deploying this algorithm was not easy task for a number of reasons. We\u2019ll explain in the rest of this blog post how to do it with no more than three commands. But before we do it, let us explained a few more details. Now that we figured out how to perform the strokes (VGG19) we need:\n\nWe are nearly there. But first we have to resolve our last challenge. Innovation, dependencies and changes in ML, tools and libraries often mean that we could easily break our working environment. Just the sheer amount of libraries and requisites needed to run this algorithm poses a problem:\n\nWe need all these software to automate the procedure of running the algorithm with different images, styles and parameters in order to attain the most impressive results. All of these resulted in a brittle environment, where the whole setup stops working if any unaverted update is introduced in the GPU drivers or torch version. This means that not only the first environment setup is burdensome, but also that we will be required to repeat this effort again from time to time to keep everything in good shape. This is far from ideal: the canvas must be at hand when sudden inspiration strikes the artist, and likewise our tools must be ready to use once we feel the burst of creativity, never subject to the pesky dependencies of an ever-changing environment.\n\nDocker comes to mind as the obvious solution to this problem. But there is a catch: Docker isolates our process from its environment, and that includes the particular hardware resources in the host machine. Unfortunately, our deep networks algorithm requires direct access to a GPU. Docker is the way, but we need something else.\n\nThe answer to our demands is nvidia-docker. A wrapper to docker, that allows us to run containers leveraging NVIDIA GPUs. Through this command, Docker will automatically mount the host GPU drivers into any running container through a volume, thus creating a loophole through which any Docker process can run code in the host GPU. This will work regardless of the particular GPU available on the host or the code running in the container.\n\nIn our particular artistic project, our host requirements are reduced dramatically: out of the long list above, now we just need to install Docker, nvidia-docker and the appropriate GPU drivers. The rest of dependencies will be contained within a Docker image, built in a reproducible way through the use of a Dockerfile and guaranteeing that all the moving pieces are frozen at interoperable and working versions.\n\nNow we only need a place to run our system. We are going to rely on the public cloud, in particular, AWS and their GPU-optimized virtual machines. AWS offer two families of GPU instances but we are going to use the brand-new P2 AWS EC2 instances. These instances are designed to chew through tough deep learning workloads like the one we have in our hands. Let\u00b4s provision it:\n\nBefore running this command, you will need to:\n\nInstalling NVIDIA drivers and nvidia-docker is our second step in our journey to become a (fake) artist. We\u2019ve put together a simple script that does the work for us:\n\nIf everything goes according to plan, you\u00b4ll get the following output:\n\nWith this last command we are just installing nvidia software package and querying the GPU card to make sure that everything is ok.\n\nSaid once Picasso. But before you close your eyes, here\u00b4s our last step: deploying the ML algorithm that is going to do the magic:\n\nNow, you only have to download the resulting images and publish them in an art forum like Devianart or show to your local art gallery\u00a0;). From your laptop/desktop, launch this:\n\nAnd gather your results in the current directory (in our example, goldengate_by_vangogh.jpg).\n\nHere are a few more examples of what you could do:\n\n\u2026and I\u2019ll fill it, said once again Pablo Picasso. We could now, too! And without spending a lifetime on it. We have provisioned some styles and some content to get you started, so you only have to sit back, relax, close your eyes and sing. By just applying these different styles to the same content image, we could have an idea on how different painters would have represented the same scene or tackled a portrait.\n\nAs a tribute to Docker and for the hours, days, weeks of our lives that saved us\u2026.we\u2019ve decided to open our own Docker museum:\n\nNow it\u00b4s your turn. Pick your favourite artist or art piece and some of your photographs and transform them into a piece of art. Share your work with us! And follow us on Twitter (@albarjip and @lherrerabenitez).\n\nPS\u00a0: Don\u2019t forget to stop and delete your P2 instances when you finish.\n\nNew to ML, DL or Docker and want to know more Try these links\u2026\n\nWe\u00b4d like to improve and reduce the size of our docker image. Could you help us?", 
        "title": "How to Fake It As an Artist with Docker, AWS and Deep Learning"
    }, 
    {
        "url": "https://gab41.lab41.org/lab41-reading-group-skip-thought-vectors-fec68c05aa92?source=tag_archive---------2----------------", 
        "text": "Continuing the tour of older papers that started with our ResNet blog post, we now take on Skip-Thought Vectors by Kiros et al. Their goal was to come up with a useful embedding for sentences that was not tuned for a single task and did not require labeled data to train. They took inspiration from Word2Vec skip-gram (you can find my explanation of that algorithm here) and attempt to extend it to sentences. Skip-thought vectors are created using an encoder-decoder model. The encoder takes in the training sentence and outputs a vector. There are two decoders both of which take the vector as input. The first attempts to predict the previous sentence and the second attempts to predict the next sentence. Both the encoder and decoder are constructed from recurrent neural networks (RNN). Multiple encoder types are tried including uni-skip, bi-skip, and combine-skip. Uni-skip reads the sentence in the forward direction. Bi-skip reads the sentence forwards and backwards and concatenates the results. Combined-skip concatenates the vectors from uni- and bi-skip. Only minimal tokenization is done to the input sentences. A diagram indicating the input sentence and the two predicted sentences is shown below.\n\nTheir model requires groups of sentences in order to train, and so trained on the BookCorpus Dataset. The dataset consists of novels by unpublished authors and is (unsurprisingly) dominated by romance and fantasy novels. This \u201cbias\u201d in the dataset will become apparent later when discussing some of the sentences used to test the skip-thought model; some of the retrieved sentences are quite exciting! Building a model that accounts for the meaning of an entire sentence is tough because language is remarkably flexible. Changing a single word can either completely change the meaning of a sentence or leave it unaltered. The same is true for moving words around. As an example: One difficulty in building a model to handle sentences is that a single word can be changed and yet the meaning of the sentence is the same. One challenge in building a model to handle sentences is that a single word can be changed and yet the meaning of the sentence is the same. Changing a single word has had almost no effect on the meaning of that sentence. To account for these word level changes, the skip-thought model needs to be able to handle a large variety of words, some of which were not present in the training sentences. The authors solve this by using a pre-trained continuous bag-of-words (CBOW) Word2Vec model and learning a translation from the Word2Vec vectors to the word vectors in their sentences. Below are shown the nearest neighbor words after the vocabulary expansion using query words that do not appear in the training vocabulary:\n\nSo how well does the model work? One way to probe it is to retrieve the closest sentence to a query sentence; here are some examples: Query: \u201cI\u2019m sure you\u2019ll have a glamorous evening,\u201d she said, giving an exaggerated wink. Retrieved: \u201cI\u2019m really glad you came to the party tonight,\u201d he said, turning to her. Query: Although she could tell he hadn\u2019t been too interested in any of their other chitchat, he seemed genuinely curious about this. Retrieved: Although he hadn\u2019t been following her career with a microscope, he\u2019d definitely taken notice of her appearance. The sentences are in fact very similar in both structure and meaning (and a bit salacious, as I warned earlier) so the model appears to be doing a good job. To perform more rigorous experimentation, and to test the value of skip-thought vectors as a generic sentence feature extractor, the authors run the model through a series of tasks using the encoded vectors with simple, linear classifiers trained on top of them. They find that their generic skip-thought representation performs very well for detecting the semantic relatedness of two sentences and for detecting where a sentence is paraphrasing another one. Skip-thought vectors perform relatively well for image retrieval and captioning (where they use VGG to extract image feature vectors). Skip-thought performs poorly for sentiment analysis, producing equivalent results to various bag of word models but at a much higher computational cost. We have used skip-thought vectors a little bit at the Lab, most recently for the Pythia challenge. We found them to be useful for novelty detection, but incredibly slow. Running skip-thought vectors on a corpus of about 20,000 documents took many hours, where as simpler (and as effective) methods took seconds or minutes. I will update with a link to their blog post when it comes online.", 
        "title": "Lab41 Reading Group: Skip-Thought Vectors \u2013"
    }, 
    {
        "url": "https://medium.com/axiomzenteam/if-its-not-perfect-don-t-do-it-8baa78e6722f?source=tag_archive---------3----------------", 
        "text": "If at first you don\u2019t succeed, try, try again.\n\nIt\u2019s a great philosophy for life, but is it a good philosophy for machine learning? Ramtin Seraj isn\u2019t so sure. He believes that deep learning\u200a\u2014\u200athe kind of machine learning currently preferred by companies like Google and IBM, which allows computers to teach themselves\u200a\u2014\u200aneeds to steer clear of tasks that require perfect accuracy. He sat down with us this week to explain why.\n\nGoogle has recently announced that they\u2019re teaching algorithms to encrypt text messages, with the goal of automating both encryption and decryption. This is a lofty goal, and it will no doubt take a long time before they reach 100% accuracy in these trials. The problem comes about if they never hit that 100% mark.\n\nMicrosoft recently announced they had achieved better-than-human results in speech recognition. That\u2019s amazing, but humans actually only hit about 80% accuracy. For voice commands, that\u2019s a solid margin of error. After all, if the computer thinks it didn\u2019t understand you, it can do as Siri does: \u201cI\u2019m sorry, I\u2019m not sure I understand.\u201d\n\nBut what if the task set before the computer is something with no room for error? What if algorithms are performing surgery unassisted\u200a\u2014\u200aor handling sensitive data? In cases like these, a 20% accuracy rate could be literally fatal.\n\nAs Ramtin Explains, \u201cWhile it\u2019s highly unlikely that Google will solve out these encryption algorithms without proof that they work, that proof is very hard to get. Using test data to train or validate your machine learning models means you can\u2019t prove it will work for future unseen data or cases.\u201d In essence\u200a\u2014\u200ayou never know for sure until it goes live. That\u2019s why you end up with instances like Microsoft\u2019s Tay, which worked so well in a closed environment but couldn\u2019t survive when up against the human factor.\n\nBut in this nascent field, there\u2019s a lot to lose in the war of public opinion.\n\nIf a big, public encryption trial failed, and everyone\u2019s data was potentially exposed, it could cause serious problems for deep learning as a buzzword, potentially causing deep learning projects to lose funding and impacting machine learning as an industry.\n\nSo how do we solve this problem? Seraj suggests we steer clear of tasks that require 100% accuracy\u200a\u2014\u200aat least for now. Give machine learning a chance to grow.\n\nOr it might never reach adulthood.", 
        "title": "If It\u2019s Not Perfect, Don\u2019t Do It \u2013 Axiom Zen Team \u2013"
    }, 
    {
        "url": "https://medium.com/@daj/how-does-prisma-work-f434273da92a?source=tag_archive---------4----------------", 
        "text": "Prisma is a cool mobile app that you can use to apply crazy filters to your photos. For example:\n\nIt works by applying a machine learning principle call \u201cconvolutional neural networks\u201d (CNN). This Quora post goes into more detail.\n\nI\u2019ve also heard this be referred to as \u201cneural style\u201d. At DevFestDC one of the talks showed this awesome neural-style GitHub project. It shows how you can apply the style from any picture, and apply it to another.\n\nFor example, let\u2019s take van Gogh\u2019s \u201cStarry Night\u201d:\n\n\u2026and apply it to a random photo of my kid:\n\nIt looks like Prisma probably use this kind of technology. They\u2019ve probably tested a lots of style images and picked their favorites. Unless they\u2019ve done something hardcore to optimize it, I\u2019m assuming they do the processing in the cloud on a high spec machine with at least one Graphics Processing Unit (GPU), since GPUs will make this run much, much faster.\n\nIt took about 2 hours to get all the dependencies for the neural-style GitHub project installed on my Mac OS X Yosemite laptop. It was surprisingly painful to get everything set up. One tip: if you\u2019re going to use Pip to install everything, make sure you are running Pip v1.5.4, or just update to the newest to play it safe. I had to carry out a couple of undocumented steps to get TensorFlow working, and it might have been due to using an old Pip version.\n\nI then ran the script for 1000 iterations, which took about 10 hours on my Mid 2013 Macbook Air (no GPU)\u200a\u2014\u200amore discussion on performance is below.\n\nI set it to take a checkpoint image every 5 iterations so I could see how it progressed. It starts with random noise, and gradually reveals the desired image.\n\nLet\u2019s take a look at some of the individual frames.\n\nAfter 30 iterations it\u2019s still mostly noise, but you can just about see her eyes and outline, but only if you know what you\u2019re looking for:\n\nBy 100 iterations it\u2019s looking quite cool:\n\nAt this point the rate of improvement appears to slow.\n\nIt took 10 hours to run on my Mid 2013 Macbook Air with no GPU. Apparently, if you have a GPU this would only have taken about 30 minutes to run.\n\nIf you\u2019re looking for an excuse to play with AWS, then see if you can get it running in AWS using a machine image (AMI) which already has all the dependencies you need. I raised a GitHub issue suggesting this, as that would make life a lot easier, and you could pick a high spec machine to get results super quickly.\n\nOnce I had the 200 checkpoint images, I really wanted to create a video. I found this useful Stackexchange question explaining how to use the ffmpeg command line utility to create an mp4 file from a bunch of image frames.\n\nI had to manually add some leading zeros onto the checkpoint filenames before I could get ffmpeg to use them in the correct order, but otherwise it was relatively painless (I suspect the neural-style \u201ccheckpoint-output\u201d option could have been used to generate them in exactly the right format\u200a\u2014\u200amaybe I\u2019ll try that next time).\n\nOnce I had the mp4, I used this free website to convert it to an animated GIF (since Medium only allows image files to be embedded, not videos).", 
        "title": "How does Prisma work? \u2013 Dan Jarvis \u2013"
    }, 
    {
        "url": "https://medium.com/@arne.osterthun/artificial-brains-taking-over-your-home-ddd8aa706e28?source=tag_archive---------5----------------", 
        "text": "Research in artificial intelligence probably is as old as computation devices itself. Especially in the last few years the interest in artificial intelligence by software companies grew exponentially. But why is that? Probably because of the opportunities in data analysis and therefore user experience artificial intelligence could create. The possibility of using the large amounts of data that are available on the internet to find user behavior patterns no rule based program could ever detect just is astonishing. But why am I talking about all of this with you?\n\nWell, the truth is that I am just thrilled by thus opportunities that lay ahead waiting to be found and developed to enhance peoples\u200a\u2014\u200aour\u200a\u2014\u200alifes.\n\nNow let me tell you about the opportunity I see in artificial intelligence: Home automation! You heard right. I am definitely not the first one with the idea of applying machine learning concepts to home automation. As a matter of fact Michael C Mozer already wrote about this topic in 1998 way before machine learning was the next big thing. Now 18 years later I tried to use the insights researchers have made in the recent time to implement such a system as an iOS application to show that it could be the future of home automation.\n\nThe roadmap for the project was very clear from the beginning. The system should use sensor data from any kind of third party home automation system that implements a basic protocol. And consequently use this data to learn the users behavior and therefore save the user from having to fiddle with these different systems to configure them to his beliefs\u200a\u2014\u200aover and over again for every change in his daily live.\n\nSpecial thanks also have to go out to Bernard Wessels and the techlab for supporting and working with me on this project. If you do not know techlab or the enera project, you should definitely check out their website and follow them on twitter to stay informed on the latest and greatest in renewable energy and home automation.\n\nIf you are interested in the work that has already been done to create a proof of concept you are invited to further look into my research results on Github:https://github.com/aosterthun/homeBrain/blob/master/thesis.pdf", 
        "title": "Artificial Brains Taking Over Your Home \u2013 arne.osterthun \u2013"
    }
]