[
    {
        "url": "https://medium.com/@Pinterest_Engineering/introducing-automatic-object-detection-to-visual-search-e57c29191c30?source=tag_archive---------0----------------", 
        "text": "When we launched visual search last year, we gave a first look at what\u2019s possible when you use images as search queries. Now, more than 130 million visual searches are done every month, as people search for the objects, styles and colors they see in Pins and get related recommendations. It\u2019s a whole new kind of search, and a technological challenge.\n\nToday, we\u2019re introducing automatic object detection for the most popular categories on Pinterest, so people can visually search for products within a Pin\u2019s image. As we look to the future of visual search, we\u2019re also starting to preview new camera search technology that\u2019ll give Pinners recommendations for the products they find in the real world. Pinners will soon be able to snap a photo of a single object like sneakers\u200a\u2014\u200aand get recommendations on Pinterest, or even take a photo of an entire room and get results for multiple items.\n\nVisual search is one of the many fields transformed in recent years by the advances in deep learning. Convolutional neural networks represent images and videos as feature vectors which preserve both semantic concepts and visual information, and allows for fast retrieval when using optimized nearest neighbor techniques. We leveraged this idea, along with our richly annotated image dataset, last November when we released a visual search product that makes searching inside a Pin\u2019s image as simple as dragging a cropper. For our initial launch, we extracted the fully-connected-6 layer of a fine tuned VGG model over a billion Pinterest images and indexed them into a distributed service, as described in our KDD paper.\n\nYour browser does not support the video tag.\n\nSince an image can contain dozens of objects, we wanted to make it as simple as possible to start a discovery experience from any of them. In the same way auto-complete improves the experience of text search, automatic object detection makes visual search a more seamless experience. Object detection in visual search also enables new features, like object-to-object matching. For example, say you spot a coffee table you love either on Pinterest or at a friend\u2019s house, soon you\u2019ll be able to see how it would look in many different home settings.\n\nOur first challenge in building automatic object detection was collecting labeled bounding boxes for regions of interest in images as our training data. Since launch, we\u2019ve processed nearly 1 billion image crops (visual searches). By aggregating this activity across the millions of images with the highest engagement, we learn which objects Pinners are interested in. We aggregate annotations of visually similar results to each crop and assign a weak label across hundreds of object categories. An example of how this looks is shown in the heatmap visualization below, where two clusters of user crops are formed, one around the \u201cscarf\u201d annotation, and another around the \u201cbag\u201d annotation.\n\nSince our visual search engine can use any image as a query\u200a\u2014\u200aincluding unseen content from the web and even your camera\u200a\u2014\u200adetection must happen in real-time, in a fraction of a second. One of most widely used detection models we\u2019ve experimented with extensively is Faster R-CNN, which uses a deep network to detect objects within images in two major steps. First, it identifies regions of an image that are likely to contain objects of interest by running a fully convolutional network over the input image to produce a feature map. For each location on the feature map, the network considers a fixed set of regions, varying in size and aspect ratio, and uses a binary softmax classifier to determine how likely each of these regions is to contain an object of interest. If a promising region is found, the network also outputs adjustments to this region so that it better frames the objects.\n\nOnce the network has found regions of interest, it examines the most promising ones and attempts to either identify each as a particular category of object or discards it if no objects are found. For each candidate region, the network performs spatial pooling over the corresponding portion of a convolutional feature map, thereby producing a feature vector with a fixed size independent of the size of the region. This pooled feature is then used as the input to a detection network, which uses a softmax classifier to identify each region as either background or one of our object categories. If an object is detected, the network once again outputs adjustments to the region boundaries to further refine detection quality. Finally, a round of non-maximum suppression (NMS) is performed over the detections to filter out any duplicate detections, and the results are presented to the user.\n\nOne of the key tricks that enables high-speed detection with Faster R-CNN is the convolutional features used in both the region proposer and the detection network are one and the same. A significant portion of the network latency is spent producing this intermediate convolutional feature map, and by sharing it between the two network components, we reduce the amount of redundant computation. This enables us to identify objects in a fraction of a second.\n\nYour browser does not support the video tag.\n\nLast year, we deployed our own implementation of this model to compute targeted visual similarity features in Related Pins, one of our recommendations products, which resulted in a 4 percent increase in engagement, as detailed in our technical report.\n\nSince then, we\u2019ve worked on improving both the accuracy and efficiency of this model by applying the recently published advances in deep residual networks (ResNets). Despite the resulting network consisting of more than 100 convolutional layers, we\u2019ve focused on reducing the GPU memory footprint of this model to be suitable for deployment on AWS while keeping latencies under 300ms.\n\nWith real-time object detection for any image from anywhere, on Pinterest, the web or in the real world, visual search on Pinterest becomes even better. Object detection will roll out this out to all Pinners and platforms in the coming weeks.\n\nWe\u2019re also building technology that will help people get recommendations on Pinterest for products they discover in the real world, by simply taking a photo. This will enable a new kind of visual search experience, combining image retrieval, object detection and the power of our interest graph. Stay tuned for more information on camera search technology.\n\nAcknowledgements \n\nVisual Search is a collaborative effort at Pinterest, and we\u2019d like to thank Kelei Xu, Vishwa Patel, Andrew Zhai, Shirley Du, Zhiyuan Zhang, Michelle Vu, Michael Feng and Kevin Jing, along with Eric Tzeng, Jeff Donahue, and Trevor Darrell from Berkeley Vision and Learning Center (BVLC). Additionally, we\u2019d like to thank Mike Repass, Naveen Gavini and Albert Pereta for making the product launch possible.", 
        "title": "Introducing automatic object detection to visual search"
    }, 
    {
        "url": "https://medium.com/data-science-brigade/o-que-%C3%A9-machine-learning-e-deep-learning-f573fae2f16b?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "O que \u00e9 Machine Learning? E Deep Learning? \u2013 Data Science Brigade \u2013"
    }, 
    {
        "url": "https://medium.com/mouse-org/learning-loves-play-snippets-to-play-with-e00f3c58b1c7?source=tag_archive---------2----------------", 
        "text": "Originally published May 25th, 2016 at the Partnership for 21st Century Learning\u2019s Blogazine, Volume 3, Issue 4, Number 15.\n\nThe first of Mouse\u2019s five learning design principles: \u201cLearning loves play.\u201d\n\nThe etymology of \u201cplay\u201d includes \u201cvigorous movement,\u201d \u201cleap for joy, dance,\u201d \u201ccare for / tend to (with devotion),\u201d \u201ccompete.\u201d\n\nWe are nowhere more serious than at play\u200a\u2014\u200athe stakes are too high.\n\n\u201cOver the past half century, in the United States and other developed nations, children\u2019s free play with other children has declined sharply. Over the same period, anxiety, depression, suicide, feelings of helplessness, and narcissism have increased sharply in children, adolescents, and young adults.\u201d\n\nChildren know what adults forget or ignore.\n\nAt play we conquer time\u200a\u2014\u200afor a while at least.\n\nAt play we are Someone Else and Someplace Else and sometimes No Place At All. (Who says a wardrobe in a spare room is just a wardrobe? Or a phantom tollbooth not real? Who can deny that some rabbit holes in fact do become \u201ccuriouser and curiouser\u201d?).\n\nRavel and rivet, paper hats and wings made of wire, the chime of the bell and the tock of the gears, creating worlds on the forge of our learnings.\n\n\u201cAlthough play is often thought frivolous, it may be essential.\u201d\n\nThe \u201cturn to play\u201d over the past few decades\u200a\u2014\u200ain education, in anthropology, in business\u200a\u2014\u200areflects a tectonic shift in social relations.\n\nAs we urbanize and digitize, as the cutting-edge of work relies more and more on conceptualizing and collaborating, we seek our way back to shared hunter/gatherer legacies.\n\nAgriculture and especially industry coordinated large numbers of individuals, serried in rows and ranks, supervised and controlled, with clearly defined rules established from on high.\n\nTo hunt, to gather, is to collaborate in small groups, with the right (and need!) to set the rules of the game as a group, in response to shifting weather, the vagaries of the terrain, the unpredictability of root and antler, the ambiguities of sight and hearing.\n\nAlmost all play feels like hunting & gathering. (Tag, you\u2019re it!) Almost none of it follows industrial-era protocols, which are less and less relevant as work evolves in post-industrial economies.\n\n(Riffing with Philip K. Dick: \u201cWill androids play?\u201d Perhaps the ability to play\u200a\u2014\u200aor not\u200a\u2014\u200awill be the frontier between the human and the robot/ AI. Maybe we need a new form of the Turing Test, as we prepare today\u2019s young learners for an ever-more cybernetic future).\n\nPlay subverts and challenges specific rules. Most importantly, play redefines order (note this does not mean eliminating order or sponsoring disorder). Play re-orders hierarchy. It \u201cworries the line.\u201d\n\nHip-hop, Dada, the surrealists, the great soccer teams, the great musical trios and quartets, our most inventive film\u200a\u2014\u200aand digital games makers\u2026they should be our models as we help young learners learn.\n\nMe, I would put Janelle Monae videos on the curriculum, and those by OutKast, Lady Gaga, OK Go\u2026 a very long list, one we could debate in particulars but I\u2019d battle hard to defend the concept.\n\nBring in Herbie Hancock and Anoushka Shankar, Esperanza Spalding, Vijay Iyer, the work of Kehinde Wiley, Kara Walker and Betye Saar, poetry by Lucille Clifton and Kevin Young.\n\nMatisse\u2019s cut-outs, Sonia Delaunay\u2019s chromatics, Klee and his colleagues at the Bauhaus (as distinct from Gropius and van der Rohe), Motherwell, Twombly, Kapoor\u2026Duchamps and the ready-mades! Calder\u2019s circus!\n\nParse The Matrix and listen in at Ice Cube\u2019s barbershop.\n\nArtists using playful means to make bone-serious points. Precisely because the points matter deeply, we play to make them.\n\nFormal, institutional education over the past two centuries chose to deride and reject play\u2026perhaps because the facts Education chose to convey were not important enough to merit play.\n\nMessi and his Barcelona teammates with tiki taka, or Curry and his Golden State teammates on the court\u2026 real learning takes place in these conditions, as play-work and work-play. How else will we find solutions to the grand challenges that face the species?\n\nNational Museum of Play and its lead publication: American Journal of Play\n\nScholarly work by Kathy Hirsh Pasek, Dorothy Singer (also with her husband Leonard), Sarah Fine, Hilary Conklin, Helle Nebelong, Brian Sutton-Smith, Peter Gray, Margaret Honey & David Kanter.", 
        "title": "LEARNING LOVES PLAY. SNIPPETS TO PLAY WITH. \u2013 mouse_org \u2013"
    }
]