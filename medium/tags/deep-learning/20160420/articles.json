[
    {
        "url": "https://medium.com/@joooopiter/4%EC%B0%A8-%EC%82%B0%EC%97%85%ED%98%81%EB%AA%85%EA%B3%BC-%EC%9D%B8%EA%B3%B5%EC%A7%80%EB%8A%A5-%EB%94%A5%EB%9F%AC%EB%8B%9D-cede02b059d3?source=tag_archive---------0----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "4\ucc28 \uc0b0\uc5c5\ud601\uba85\uacfc \uc778\uacf5\uc9c0\ub2a5, \ub525\ub7ec\ub2dd \u2013 Jaeyoung Jun \u2013"
    }, 
    {
        "url": "https://medium.com/@xamat/football-or-futbol-or-why-deep-learning-will-not-make-other-machine-learning-approaches-obsolete-666658ed4167?source=tag_archive---------1----------------", 
        "text": "You have probably heard a lot about Deep Learning and how it is taking over the world in general, and the area of Machine Learning in particular. But, does that mean that Deep Learning will be the solution to all our (machine learning) problems? No. There are several reasons why there will always be a place for other algorithms to be better suited than deep learning in some applications.\n\nThere are many cases where you need to have an understanding of the domain in order to have optimal results. While some proponents of Deep Learning describe their approach as being general-purpose, I don\u2019t think that will ever be true.\n\nGiven two models that perform more or less equally, you should always prefer the one that is less complex. Even the authors of the Deep Learning book mention this. For this reason, there will always be cases where Deep Learning will not be preferred, even if it has managed to squeeze an extra 1% in accuracy on the testing set.\n\nEven in the very unlikely case where most ML problems end up being suited for Deep Learning Approaches, there will always be a place for ensembles. Given the output of a Deep Learning prediction, I am sure you will be able to combine it with some other model or feature to improve the results.\n\nLet\u2019s imagine that your very demanding boss asks you to implement an image classifier to detect whether sport images contain scenes coming from a Football (US) game or a Football (Soccer) game. You are well read in Deep Learning literature, so you train a two-class classifier using Convolutional Neural Networks (CNNs) feeding it thousands of labeled images containing both sports. The classifier works out pretty good and you get an accuracy of, say, 95%.\n\nYou present the work to your boss and she is super impressed. Next thing you know, your deep classifier is being used on a dataset of images from Spain. Now of course, every now and then, the image is really poor quality and blurry, so the classifier has a hard time. Let\u2019s say it finds an image where the probability of it being Football is 60%, should it decide to classify it as such? Of course not! An image with 60% probability of being Football coming from a Spanish database should be classified as Futbol for sure\u2026 almost nobody watches Football in Spain!\n\nAs a matter of fact, in this example, a dumb classifier that just labels everything as Futbol for this dataset would manage to have much more than a 95% accuracy and would work much better than your fancy Deep Neural Network!\n\nSo, what\u2019s going on? Well, you should have thought better about the problem you had at hand and realized that it it not only an image recognition problem. Things like \u201cpopularity\u201d or \u201clocation\u201d could make a huge difference. Of course these are simple features, you could think about many more, but you get the point.\n\nYou could feed those features into your problem by using an ensemble as mentioned above. Or you could use a simple Bayesian prior. You could even force it as a feature into the Neural Network.\n\nIn any case, this simple example proves that you will always have the need to understand your domain and do some feature engineering, favor simple models whenever possible, and most likely use ensembles.", 
        "title": "Football or Futbol? Or why Deep Learning will not make other Machine Learning approaches obsolete"
    }, 
    {
        "url": "https://medium.com/@ursushorribilis/teaching-algorithms-to-learn-bbfc2ff594cf?source=tag_archive---------2----------------", 
        "text": "Philosophers say that you can never cross the same river twice. They did not know that this would not apply to machine learning models. Imagine cloning yourself with the exact same set of experiences until today, and having your clones go separate paths. You could ask your clones the same set of questions and you would get the exact same set of answers. At least until your clones start learning about new things.\n\nThe models can also learn in parallel, and at the end you can choose to \u201cmerge\u201d multiple versions of the model into a more knowledgeable one. It\u2019s as if one of your clones went skiing, the second one snorkeling and at the end of the holidays you merge their experience into a new great skier and snorkeler you.\n\nLet me explain how this is done and why it differs from standard software practices. In open source software it is clear how to contribute a bug fix or new features to an existing project. In software the engineers are constantly adding/removing or editing lines of code. A \u201cchange request\u201d is a text file that documents these changes in source code. There are plenty of tools that allow for these changes to be reviewed by other developers. The changes are then accepted or rejected based on this evaluation.\n\nHowever machine learning models can not be just merged that way. Deep Learning models are distributed with both their algorithm and a pre-trained set of coefficients that can be used to, for example, caption images. You can take these pre-trained models and start using them out of the box. While the algorithms are basically code as in the example above, the pre-trained set of coefficients are not.\n\nOnce your models are running, you can choose to train them on your own data, which would result in the creation of the multiple clones I mentioned above. Andrej Karpathy has written a short story that illustrates the concept quite well. He calls the training of the algorithms \u201cShaping\u201d.\n\nWe have been using two different Image Captioning deep learning models for this. Karpathy\u2019s Neuraltalk and Google\u2019s Imagenet. We created a page to allow users to create a story based on a seed photo that they uploaded. We also added the capability for the user to correct both the captions and the objects that the algorithms detected. The feedback on the stories that the model created was all over the place. It kind of depends which photo you throw at it. For example in this picture:\n\nthe system correctly identified a sea turtle, and corals. But it also saw broccoli, probably triggered by the small sprouts on the left.\n\nSome of the resulting misunderstandings where quite interesting, like the picture of a tulip in a pot. The algorithm identified this correctly and the first photos in the sequence were of tulips and flowers. But since we are using Twitter as one of the sources of the photos for our generated albums, and tulips do have a strong correlation with Holland, somehow pictures of \u201cthe other kind of pot\u201d started to appear on the album as well.\n\nA user told us that our system felt like chatting to a 3 year old kid. Getting some things savant-right (Scientific name of animals being one example, dinosaur names being another) and others extremely wrong. We want our models to become better at describing the world. The question is:\n\nWe will continue training the model with our visitors feedback. However, we could also speed up training by exposing our models to data collected elsewhere as well. The screen below shows how a visitor can deselect the \u201cnames\u201d that the system got wrong and even enter new \u201cnames\u201d that the object would belong to:\n\nIn the example above we see both the \u201cbrilliant savant\u201d and the lack of common sense of the model. The Strix Nebulosa happens to be the scientific name of the Great Grey Owl. But the model sees the owl on a branch of a tree, instead of a wooden fence. This is where the analogy of a 3 year old kid fits in.\n\nThere is an issue with our current approach. We have no control as of what the users will try to teach the system.\n\nOur system had not been live more than a few hours when our marketing guy called me to tell me that we already had some porn. So on a quick fix we created a list of \u201csexually explicit\u201d words that the system is now filtering out to avoid \u201cout of context\u201d pictures appearing in stories. While this system is necessary now to avoid scaring visitors to the site, it is probably not the long term solution.\n\nRemember that book: \u201cAll I needed to know I learned in Kindergarten\u201d? Kindergarten was the first place where you learned things in a group without the careful supervision of your parents. Your first \u201cbad words\u201d are probably from there. And probably also some of your first misbehavior. At some point in time we will need to expose the model to the bizarre world of the Twitteraty, where selfies and explicit content are the norm.\n\nThe difference between \u201cshaping\u201d a model and parenting a toddler is that you can retrain the model over and over. It is always a blank slate. And you can \u201cmerge personalities\u201d, in that you feed the same model data collected by other systems. Therefore we will be making available our trained data collected from all our users to other model holders. Use our contact info to get in touch with us if you want a copy of our data.\n\nWhat do you think is the best approach to train the system?\n\nWanna help us train the system? Get a picture ready and use this link to try it out", 
        "title": "Teaching algorithms to learn \u2013 Miguel Rodriguez \u2013"
    }, 
    {
        "url": "https://medium.com/@vashkelis/how-deep-is-your-mind-alphago-f38945b1eba8?source=tag_archive---------3----------------", 
        "text": "March 15, 2016, was the happy day for all AI enthusiasts and probably a sad day for the rest of the humans. This day AI-powered AlphaGo program (designed and trained by Google DeepMind lab) defeated Lee Sedol, one of the world\u2019s top players with 9-dan rank, by four games to 1.\n\nMany lances have been broken discussing what does this mean to all of us. Indeed, it is the first time in history when AI stood over the human in such variative and abstract game as Go. However, for engineering guys like us, it is more interesting to see what is under the hood of AlphaGo.\n\nThis post is based on Mastering the Game of Go with Deep Neural Networks and Tree Search from Google Deep Mind.\n\nThe fundamental difference between AlphaGo and previous AI monsters like IBM Deep Blue is that Deep Blue, as well as most chess-playing computers, used Monte Carlo Tree Search(MCST) algorithm. This is kind of brute-force approach (for sure, with a lot of smart tweaks), recursively computing the optimal move for every game state, that can\u2019t be efficiently used in case of Go.\n\nThe game of Go has long been viewed as the most challenging of classic games for artificial intelligence due to its enormous search space and the difficulty of evaluating board positions and moves.\n\nAs it widely accepted, the number of possible combinations in Go exceeds the number of atoms in the universe. Indeed, the number of combinations in Go can be estimated as1.74\u00d710172 (compare with an estimation of 1080 atoms for the observable universe).\n\nDue to this, the search over all possible positions is mostly infeasible. However, AlphaGo has gracefully solved this issue with the smart combination of MCST and complicated Deep Learning.\n\nAlphaGo uses two types of neural networks\u200a\u2014\u200avalue network to evaluate board positions andpolicy networks to select moves.\n\nSince theoretical breads of Go (number of possible moves) is enormous, the policy networktries to minimize it by predicting most probable opponent\u2019s move based on previous training results.\n\nValue network gives an estimation of current position strength or in other words, rapidly estimates the probability of game winning for every given position.\n\nAs a result, AlphaGo evaluates thousands of times fewer positions than Deep Blue did by selecting positions more intelligently (using the policy network) and evaluating them more precisely (using the value network).\n\nBoth neural networks are trained by a combination of supervised learning and reinforcement learning from games of self-play. So let\u2019s look how this training is done.\n\nThe architecture of the policy networks is 12 convolutional layers. A final softmax layer outputs a probability distribution over all legal moves. Since the purpose of the network is to predict the next opponent\u2019s move it was trained on the database of the real Go games available online (~150K real games, ~30 million positions). Interesting, that this dataset did not include the games of top players but rather the games of Go-enthusiasts playing on KGS Go Server.\n\nThe network takes 48 inputs as described below\n\nAchieved prediction accuracy is about 57% on a held-out test set, using all input features, and 55.7% using only raw board position and move history as inputs. By the way, these relatively poor results simply mean that humans are the subjects hard to predict.\n\nNext step is reinforcement learning (RL). On this stage, various versions of the policy networks play against each other. Reinforcement learning improves the SL policy network by optimising the final outcome of games of self-play. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy.\n\nThis significantly improves the strength of RL-policy network over initial implementation of SL-policy network.\n\nThe final stage of the training pipeline focuses on position evaluation predicting the outcome from the current position of the game. Value network has a similar architecture to the policy networks but outputs a single prediction instead of a probability distribution.\n\nThe naive approach of predicting game outcomes from data consisting of complete games leads to overfitting because successive positions are strongly correlated, differing by just one stone, but the regression target is shared for the entire game. When trained on the real games dataset, the value network memorised the game outcomes rather than generalising to new positions.\n\nTo mitigate this problem, AlphaGo\u2019s team generated a new self-play dataset consisting of 30 million distinct positions, each sampled from a separate game. Each game is played between the RL policy network and itself until the game terminated.\n\nFinally, AlphaGo combines the policy and value networks in an MCTS algorithm.\n\nTo efficiently combine MCTS with deep neural networks, AlphaGo uses an asynchronous multi-threaded search that executes simulations on CPUs, and computes policy and value networks in parallel on GPUs. The final version of AlphaGo used 40 search threads, 48 CPUs, and 8 GPUs.", 
        "title": "How deep is your mind, AlphaGo? \u2013 Vadim Vashkelis \u2013"
    }
]