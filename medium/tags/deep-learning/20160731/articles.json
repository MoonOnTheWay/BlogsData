[
    {
        "url": "https://medium.com/@YvesMulkers/introducing-cloud-hosted-deep-learning-models-e8fec4941074?source=tag_archive---------0----------------", 
        "text": "At Algorithmia, we believe in democratizing access to state-of-the-art algorithmic intelligence. That\u2019s why we\u2019re introducing a solution for hosting and distributing trained deep learning models on Algorithmia using GPUs in the cloud.\n\nToday, researchers and developers can train their neural nets locally, and deploy them to Algorithmia\u2019s scalable, cloud infrastructure, where they become smart API endpoints for other developers to use.\n\nWe\u2019re excited to announce initial native support for the Caffe, Theano, and TensorFlow frameworks, and have added 16 open source deep learning models that run as microservices to start. Support for Torch and MxNet are coming soon.\n\nPlus, we\u2019ve created two demos to showcase how easy it is to build applications off of hosted models:\n\nWe want Algorithmia to be the place for researchers and developers creating deep learning models to showcase their work, distribute it, and make it available for other developers to use.\n\nWe believe algorithms and models should be implemented once, reusable by developers anywhere, in any language.\n\nYou should train your model using the tools you\u2019re comfortable with. And, when you\u2019re ready, deploy it to our infrastructure, where your model will join a collection of more than 2,200 algorithmic microservices other developers can use to obtain real-time predictions, and build production-ready, machine intelligent apps.\n\nThanks to an abundance of digital data, and powerful GPUs, we are now capable of teaching computers to read, see, and hear.\n\nJust this year, a handful of high-profile experiments came into the spotlight, including Microsoft Tay, Google DeepMind AlphaGo, and Facebook M.\n\nThese experiments all relied on a technique known as deep learning, which attempts to mimic the layers of neurons in the brain\u2019s neocortex. This idea\u200a\u2014\u200ato create an artificial neural network by simulating how the neocortex works\u200a\u2014\u200ahas been around since the 1980s.\n\nDuring the training process, the algorithm learns to discover useful patterns in the digital representation of data, like sounds and images. In a very real sense, we\u2019re teaching machines to teach themselves.\n\nAs a result, it\u2019s become clear that deep learning is the next frontier in machine learning and artificial intelligence.\n\nYet, despite plentiful data, and abundant computing power, deep learning is still very hard.\n\nThe bottleneck is the lack of developers trained to use these deep learning techniques. Machine learning is already a highly specialized domain, and those with the knowledge to train deep learning models are even more select.\n\nFor instance, Google can\u2019t recruit enough developers with deep machine learning experience. Their solution? Teach their developers to use ML instead. When Facebook\u2019s engineers were struggling to take advantage of machine learning, they created an internal tool for visualizing ML workflows.\n\nBut, where does that leave the other 99% of developers that don\u2019t work at one of these top tech company?\n\nVery few people in the world know how to use these tools.\n\n\u201cMachine learning is a complicated field,\u201d S. Somasegar says, venture partner at Madrona Venture Group and the former head of Microsoft\u2019s Developer Division. \u201cIf you look up the Wikipedia page on deep learning, you\u2019ll see 18 subcategories underneath Deep Neural Network Architectures with names such as Convolutional Neural Networks, Spike-and-Slab RBMs, and LTSM-related differentiable memory structures.\u201d\n\n\u201cThese are not topics that a typical software developer will immediately understand.\u201d\n\nYet, the number of companies that want to process unstructured data, like images or text, is rapidly increasing.", 
        "title": "Introducing Cloud Hosted Deep Learning Models \u2013 Yves Mulkers \u2013"
    }, 
    {
        "url": "https://medium.com/@IgorCarron/nuit-blanche-in-review-for-july-2016-8cc7a0b76c2d?source=tag_archive---------1----------------", 
        "text": "Much happened this past month since the last Nuit Blanche in Review (June 2016). We submitted a proposal for a two day workshop at NIPS on \u201cMapping Machine Learning to Hardware\u201d (the list of proposed speakers is included in the post).\n\nWe also featured two proofs on the same problem from two different groups (the third proof being on a different problem altogether), one implementation, a survey and a video on randomized methods, some exciting results on random projections in manifolds and in practice with deep learning, a continuing set of posts showing the Great Convergence in action, a few applications centered posts and much more.\n\nFinally, we were able to see the first image of the inside of the current state of reactor 2 at Fukushima Daiichi. That image uses Muon tomography and a reconstruction algorithm we mentioned back in Imaging Damaged Reactors and Volcanoes. Without further ado, here is the Nuit Blanche in Review post. Enjoy\u00a0!", 
        "title": "Nuit Blanche in Review for July 2016 \u2013 Igor Carron \u2013"
    }, 
    {
        "url": "https://medium.com/the-data-intelligence-connection/introducing-cloud-hosted-deep-learning-models-c6fa527ea3af?source=tag_archive---------2----------------", 
        "text": "At Algorithmia, we believe in democratizing access to state-of-the-art algorithmic intelligence. That\u2019s why we\u2019re introducing a solution for hosting and distributing trained deep learning models on Algorithmia using GPUs in the cloud.\n\nToday, researchers and developers can train their neural nets locally, and deploy them to Algorithmia\u2019s scalable, cloud infrastructure, where they become smart API endpoints for other developers to use.\n\nWe\u2019re excited to announce initial native support for the Caffe, Theano, and TensorFlow frameworks, and have added 16 open source deep learning models that run as microservices to start. Support for Torch and MxNet are coming soon.\n\nPlus, we\u2019ve created two demos to showcase how easy it is to build applications off of hosted models:\n\nWe want Algorithmia to be the place for researchers and developers creating deep learning models to showcase their work, distribute it, and make it available for other developers to use.\n\nWe believe algorithms and models should be implemented once, reusable by developers anywhere, in any language.\n\nYou should train your model using the tools you\u2019re comfortable with. And, when you\u2019re ready, deploy it to our infrastructure, where your model will join a collection of more than 2,200 algorithmic microservices other developers can use to obtain real-time predictions, and build production-ready, machine intelligent apps.\n\nThanks to an abundance of digital data, and powerful GPUs, we are now capable of teaching computers to read, see, and hear.\n\nJust this year, a handful of high-profile experiments came into the spotlight, including Microsoft Tay, Google DeepMind AlphaGo, and Facebook M.\n\nThese experiments all relied on a technique known as deep learning, which attempts to mimic the layers of neurons in the brain\u2019s neocortex. This idea\u200a\u2014\u200ato create an artificial neural network by simulating how the neocortex works\u200a\u2014\u200ahas been around since the 1980s.\n\nDuring the training process, the algorithm learns to discover useful patterns in the digital representation of data, like sounds and images. In a very real sense, we\u2019re teaching machines to teach themselves.\n\nAs a result, it\u2019s become clear that deep learning is the next frontier in machine learning and artificial intelligence.\n\nYet, despite plentiful data, and abundant computing power, deep learning is still very hard.\n\nThe bottleneck is the lack of developers trained to use these deep learning techniques. Machine learning is already a highly specialized domain, and those with the knowledge to train deep learning models are even more select.\n\nFor instance, Google can\u2019t recruit enough developers with deep machine learning experience. Their solution? Teach their developers to use ML instead. When Facebook\u2019s engineers were struggling to take advantage of machine learning, they created an internal tool for visualizing ML workflows.\n\nBut, where does that leave the other 99% of developers that don\u2019t work at one of these top tech company?\n\nVery few people in the world know how to use these tools.\n\n\u201cMachine learning is a complicated field,\u201d S. Somasegar says, venture partner at Madrona Venture Group and the former head of Microsoft\u2019s Developer Division. \u201cIf you look up the Wikipedia page on deep learning, you\u2019ll see 18 subcategories underneath Deep Neural Network Architectures with names such as Convolutional Neural Networks, Spike-and-Slab RBMs, and LTSM-related differentiable memory structures.\u201d\n\n\u201cThese are not topics that a typical software developer will immediately understand.\u201d\n\nYet, the number of companies that want to process unstructured data, like images or text, is rapidly increasing.", 
        "title": "Introducing Cloud Hosted Deep Learning Models \u2013 The Data Intelligence Connection \u2013"
    }
]