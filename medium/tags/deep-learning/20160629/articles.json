[
    {
        "url": "https://blog.init.ai/three-impactful-machine-learning-topics-at-icml-2016-465be5ae63a?source=tag_archive---------0----------------", 
        "text": "The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning, attracting 2000+ participants. This year it was held in NYC and I attended on behalf of Init.ai. Three of the tutorial sessions I attended were quite impactful. Anyone working on conversational apps, chatbots, and deep learning would be interested in these topics.\n\nI\u2019ve written before about Residual Neural Network research, but listening to Kaiming was informative. In the talk, he described motivations for increasing the depth of neural networks. He demonstrated obstacles to increasing depth and initial solutions. Additionally, He showed how residual networks increase accuracy with increased depth beyond these initial solutions. Moreover, Kaiming justified using identity mappings in both the shortcut connection and the post-addition operation. Finally, He gave empirical results that ResNets yield representations generalizing to many problems.\n\nKaiming showed how deeper neural networks had won recent ImageNet competitions. Yet, extending them beyond a depth of about twenty layers decreases performance.\n\nA few techniques are enough to get this far. Careful weight initialization and batch normalization enable networks to train beyond ten layers.\n\nWeight initialization reduces vanishing and exploding behavior in the forward and backward signals. For healthy propagation, one should force the product of all layers\u2019 scaled variances to be constant. Thus, one should rescale the scaled variance of each layer to be one. For a linear activation, one can use:\n\nFor a rectified-linear (ReLU) activation, one can use:\n\nFor a rectified-linear network with 22 layers, initializing with the second equation converges faster. The same network with 30 layers requires the second form to progress at all. The second form makes sense because ReLU drops half of the input space.", 
        "title": "Three Impactful Machine Learning Topics at ICML 2016"
    }, 
    {
        "url": "https://blog.init.ai/icml-2016-memory-networks-for-language-understanding-f2ed4c8819c4?source=tag_archive---------1----------------", 
        "text": "The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning, attracting 2000+ participants. This year it was held in NYC and I attended on behalf of Init.ai. Three of the tutorial sessions I attended were quite impactful. Anyone working on conversational apps, chatbots, and deep learning would be interested in these topics.\n\nThis second post on Memory Networks is part of a three part series on these important topics.\n\nJason Weston motivated building an end-to-end dialog agent. He detailed a simple model that makes headway toward this goal: Memory Networks. He provided means to test this model against a set of toy benchmarks. He described the benchmarks as an escalating sequence of tasks. Jason showed a revised memory network model that learns end-to-end without explicitly supervised attention. He gave real-world datasets where memory networks do well and where they do poorly. He portrayed a way to scale efficiently to large datasets. He presented two revisions: one using key-value pairs and another learning from textual feedback. Finally, he asked questions motivating future research.\n\nFirst, Jason introduced a set of beliefs describing an ideal dialog agent. It should use all its knowledge to perform complex tasks. It should converse at length and understand the motives underlying the dialog. It should be able to grow its capabilities while conversing. It should learn end-to-end.\n\nNext, Memory Networks (MemNNs) were introduced. Memory Networks combine inputs with attention on memories to provide reasoned outputs. He limits the first iteration\u2019s scope to be as simple as possible. It consists of a recurrent controller module that accepts an initial query. To start, its memory is loaded with a set of facts. The query and facts are bag-of-words vectors. The controller predicts an attention vector (with a supervision signal) to choose a fact. It reads the chosen memory to update its hidden state. After several repetitions, or hops, it formulates an output. The output ranks possible responses from a dictionary of words. Error signals back-propagate through the network via the output and the supervised attention episodes.\n\nHe described a set of toy benchmarks of increasing complexity. Each benchmark consists of a set of short stories. Each story is a sequence of statements about an evolving situation. The model should read a single story and answer one or more questions about it. Within a benchmark, the stories test the same skill. Across the different benchmarks, the skills get more difficult.\n\nA revised model, the End-to-end Memory Network (MemN2N) learns without attention supervision. It uses soft-attention (a probability vector) to read the memory. Thus, it is fully-differentiable and can learn from output supervision alone. The newer model still fails on some toy benchmark tasks. Yet, it succeeds on several real-world benchmarks, such as children\u2019s books and news question sets.\n\nAnother revision, the Key-Value Memory Network splits each memory cell into two parts. The first part is a lookup key used to match the incoming state vector. The second is a value combined with attention to produce the read value. Key-Value MemNNs closely match state-of-the-art on some real-world question-answering datasets.\n\nFinally, a third revision learns only through textual feedback. It learns to predict the response of a \u201cteacher\u201d agent that provides feedback in words. Mismatches between predicted and actual feedback provide a training signal to the model.\n\nExplore the papers, code and datasets in slide 87. Find questions for future research in slide 10, slide 83 and slide 88.", 
        "title": "ICML 2016: Memory Networks for Language Understanding"
    }, 
    {
        "url": "https://blog.init.ai/icml-2016-deep-residual-networks-2f1665cd8624?source=tag_archive---------2----------------", 
        "text": "The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning, attracting 2000+ participants. This year it was held in NYC and I attended on behalf of Init.ai. Three of the tutorial sessions I attended were quite impactful. Anyone working on conversational apps, chatbots, and deep learning would be interested in these topics.\n\nThis first post on deep residual networks is part of a three part series on these important topics.\n\nI\u2019ve written before about Residual Neural Network research, but listening to Kaiming was informative. In the talk, he described motivations for increasing the depth of neural networks. He demonstrated obstacles to increasing depth and initial solutions. Additionally, He showed how residual networks increase accuracy with increased depth beyond these initial solutions. Moreover, Kaiming justified using identity mappings in both the shortcut connection and the post-addition operation. Finally, He gave empirical results that ResNets yield representations generalizing to many problems.\n\nKaiming showed how deeper neural networks had won recent ImageNet competitions. Yet, extending them beyond a depth of about twenty layers decreases performance.\n\nA few techniques are enough to get this far. Careful weight initialization and batch normalization enable networks to train beyond ten layers.\n\nWeight initialization reduces vanishing and exploding behavior in the forward and backward signals. For healthy propagation, one should force the product of all layers\u2019 scaled variances to be constant. Thus, one should rescale the scaled variance of each layer to be one. For a linear activation, one can use:\n\nFor a rectified-linear (ReLU) activation, one can use:\n\nFor a rectified-linear network with 22 layers, initializing with the second equation converges faster. The same network with 30 layers requires the second form to progress at all. The second form makes sense because ReLU drops half of the input space.", 
        "title": "ICML 2016: Deep Residual Networks \u2013"
    }, 
    {
        "url": "https://medium.com/@BonsaiAI/this-is-the-third-in-our-series-of-posts-of-how-to-train-an-ai-agent-to-play-breakout-using-bonsai-c189db24a344?source=tag_archive---------3----------------", 
        "text": "Once the brain is created, we load the inkling file, breakout.ink, containing the mental model and attached curriculums and lessons.\n\nThe first step the BRAIN Server takes is to pick an appropriate learning algorithm to train the Mental Model. Choosing the right algorithm is a critical step in training any AI model; it takes data scientists years of expertise and study to learn to do this effectively. The BRAIN Server has knowledge of many of the available learning algorithms and has a set of heuristics for picking an appropriate algorithm as well as an initial configuration from which to train.\n\nFor example, if the BRAIN Server were to pick Deep Q-Learning for training a Mental Model, it would also need to pick an appropriate topology, hyper-parameters, and initial weight values. A benefit of having the heuristics available to be used programmatically is that the BRAIN server is not limited to a single choice; it can select any number of possible algorithms, topologies, etc., train all of them in parallel, and pick the best result. The only limit is the amount of computing resources the user wants to throw at the problem.\n\nNext, we connect our breakout simulator to the BRAIN server using the Bonsai Python SDK.\n\nHere we hook the simulator to the BRAIN:\n\nFinally, we give the command to start training:\n\nTraining is initiated with an explicit command from the user. In order for training to proceed, the user needs to have already submitted compiled Inkling code and registered all simulators with the BRAIN Server as we did above.\n\n\u00a0\n\nOnce an algorithm is chosen, the BRAIN Server will proceed with training the BRAIN\u2019s Mental Model via the Curricula. The BRAIN Server manages all of the data streaming, data storage, efficient allocation of hardware resources, choosing when to train each Concept, how much (or little) to train a Concept given its relevance within the Mental Model (i.e. dealing with the common problems of overfitting and underfitting), and generally is responsible for producing a trained BRAIN based on the given Mental Model and Curricula.\u00a0\n\n\u00a0\n\nAs is the case with picking an appropriate learning algorithm, guiding training\u200a\u2014\u200anotably avoiding overfitting and underfitting\u200a\u2014\u200ato produce an accurate AI solution is a task that requires knowledge and experience in training AIs, and the BRAIN server has an encoded set of heuristics to manage this without user involvement.\n\nThe process of picking the right algorithm and guiding training is itself a BRAIN that Bonsai has trained (and will continue to train), meaning Bonsai BRAIN will get better at building BRAINs each time a new one is built.\n\nOnce a BRAIN has been sufficiently trained, it can be deployed such that it can be used by the user\u2019s production application. The interface for using a deployed BRAIN is simple: the user submits data (of the same type as the BRAIN was trained with) to a BRAIN Server API and receives the BRAIN\u2019s evaluation of that data.", 
        "title": "How we taught an AI to play Breakout \u2013 Bonsai \u2013"
    }, 
    {
        "url": "https://blog.init.ai/icml-2016-non-convex-optimization-5b824f0b724?source=tag_archive---------4----------------", 
        "text": "The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning, attracting 2000+ participants. This year it was held in NYC and I attended on behalf of Init.ai. Three of the tutorial sessions I attended were quite impactful. Anyone working on conversational apps, chatbots, and deep learning would be interested in these topics.\n\nThis third post on non-convex optimization is part of a three part series on these important topics.\n\nAnima Anandkumar covered methods that achieve guaranteed global optimization for non-convex problems. Machine learning problems are optimization problems, often non-convex. But, non-convex problems have an exponential number of critical points. These saddle points impede the progress of gradient descent and Newton\u2019s method. She detailed conditions that define different types of critical points. She gave algorithms to escape well-behaved functions to find local optima. Such well-behaved functions are twice-differentiable and have non-degenerate saddle points. Stochastic gradient descent and Hessian methods can escape saddle points efficiently. She showed how higher-order critical points impede the progress of these algorithms. She detailed specific problems for which global optima can be reached: matrix eigen-analysis and orthogonal tensor decomposition.", 
        "title": "ICML 2016: Non-Convex Optimization \u2013"
    }, 
    {
        "url": "https://medium.com/@andraganescu/step-into-the-loop-59ea935aafb3?source=tag_archive---------5----------------", 
        "text": "I am made of things I know, about the things I know.\n\nIf you strip away the things I know, about the things I know,\n\nI am left only with the things I know\u200a\u2014\u200a\n\nwhich keep me alive,\u00a0\n\nbut I will not be me anymore,\n\nI will just be It.\n\nMany times I see the It.\u00a0\n\nMany times I see\n\nthe me on top of the it which makes me I.\n\nI see some around me and I call them them.\u00a0\n\nThe It calls them the others.\u00a0\n\nThe others become them to me only when\u00a0\n\nI see the me in them.\n\nI see the me in them,\n\nbecause of the things I know, about the things I know, about the others.\n\nAnd there are those who are among them,\n\nabout whom I even know things,\n\nabout the things I know, about the things I know, about them,\n\nand I call them we.\n\nAnd there is the other me in we,\n\nConfusing my it,\n\nwho doesn\u2019t get how an other can be me,\n\nthat other me out there in we,\u00a0\n\nwhich makes my it let go of me,\n\nbecause of you.\n\nI am now split in a me and a you,\n\nand I call them us.", 
        "title": "Step into the loop \u2013 Andrei Draganescu \u2013"
    }
]