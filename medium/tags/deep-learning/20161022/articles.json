[
    {
        "url": "https://medium.com/@derekchen14/training-a-neural-network-ff5c4494a770?source=tag_archive---------0----------------", 
        "text": "One way to look at training a neural network is that you are minimizing a cost function to increase accuracy. This is done through gradient descent, not trying to calculate the hessian. Because the overall search space is non-convex and complicated.\n\nOne way to look at trying to live a successful life is that you are minimizing the resources you spend (e.g. time and money) to increase your happiness. This is done through getting better every day, not trying to have a master plan for life. Because the overall experience of life is constantly changing.\n\nThere are some standard procedures people agree on. Such as more layers and more units is generally better. Sometimes a network even has 1000s of layers, which might be overkill, but certainly 100 layers is better than 10, and 10 layers is better than 1\u200a\u2014\u200athat\u2019s the whole point of deep learning right?\n\nThere are some standard procedures people agree on. Such as more schooling and more degrees is generally better. Sometimes a person might even have PhD, which might be overkill, but certainly a masters is better than an undergrad degree, and an undergrad degree is better than a high school diploma - that's the whole point of education right?\n\nBut common sense is never where people like to focus. Most people like to tweak mini-batch sizes and testing 10 new features to improve your word embeddings (positional encoding, topic modeling, n-gram statistics, matrix factorization, etc.), even though the real gains come in gathering and pre-processing data rather than trying to build a fancier model\u200a\u2014\u200ai.e. the dirty work.\n\nBut common sense is never where people like to focus. Most people like to go to self-improvement workshops or read 10 ways to hack your productivity, even though the real gains come from putting in the hard work rather than trying to take shortcuts - i.e. the dirty work.\n\nPeople like to focus on optimization techniques (Adam, Adagrad, Momentum), but honestly plain SGD with mini-batches works perfectly fine most of the time. This is because the majority of the work comes during the training, not whether you chose the right optimizer.\n\nPeople like to focus on optimization techniques (Pomodoro, GTD, Kanban), but honestly just setting goals and then doing them works perfectly fine most of the time. This is because the majority of the work comes during the execution of the task, not whether you have a fancy todo-list app.\n\nWhat are some gains that are truly worthwhile? Well, we\u2019ve seen that proper initialization is pretty critical. But the \u201cright way\u201d to initialize is really to just have random weights centered on the mean. In other words, you know you can\u2019t set everything to zero, but that doesn\u2019t necessarily mean you know where to start either.\n\nWhat are some gains that are truly worthwhile? Well, we've seen that choosing the right career that you're passionate about is pretty critical. But the \"right way\" to start your career is really to just try random activities centered around your interests. In other words, you know you can't just sit on the couch all day, but that doesn't necessarily mean you know where to start either.\n\nOne way to start at good initialization is through pre-training. This alternate task isn\u2019t your final goal, but it points you in the right direction. And on the topic of initialization, we can\u2019t forget the benefits of batch norm in starting each layer the best way possible. You can start with VGG-net if you really want, but that jolt from transfer learning will only take you so far. Eventually, your model needs to do some learning for itself. Finally, let\u2019s not forget about dropout. It\u2019s like building an ensemble of multiple networks and getting the best experiences of every network involved.\n\nOne way to make sure you are starting the right career is through internships. This alternate task isn't your final goal, but it points you in the right direction. And on the topic of initialization, we can't forget the benefits of a good night's sleep to help starting each day the best way possible. You can drink all the coffee you want, but that jolt of caffeine will only take you so far. Eventually, you need to do just grind through some long nights yourself. Finally, let's not forget about learning from other's mistakes and mentors. It's like building an ensemble of multiple lives and getting the best experiences of every person involved.\n\nEven with all these tips and tricks though, sometimes you just need to restart the whole training process with a new model.\n\nEven with all these tips and tricks though, sometimes you just need to restart the whole growth process with a new career.\n\nHopefully, you understood your situation well enough to not require such a drastic change as going from a CNN to a LSTM, but maybe you need to make a more subtle change from a 3-layer bi-LSTM with character inputs into a 2-layer GRU with morpheme inputs.\n\nLet\u2019s be honest though, even the more \u201csubtle\u201d change is really difficult, and often requires a completely new set of hyper-params, so that means a completely new grid search process. This is not to mention that re-training itself takes a long time, even if you happened to magically have all the right params from the start.\n\nHopefully, you understood your situation well enough to not require such a drastic change as going from a being a doctor to being an engineer, but maybe you need to make a more subtle change from a web developer focused on e-commerce into a systems engineer focused on healthcare.\n\nLet's be honest though, even the more \"subtle\" change is really difficult, and often requires a completely new set of industry connections, so that means a completely new networking period to meet people. This is not to mention that building credibility in a new field itself takes a long time, even if you happened to magically have all the right skills for the job from the start.\n\nSo what happens? Most people just pick a standard off-the-shelf model and start training. And if their test accuracy is low, they complain about their slow CPU (eg. if only I were rich enough to afford a GPU) or try to redirect people\u2019s attention to their amazing validation accuracy (or even their train accuracy if they\u2019re desperate). Thus most results end up being very average.\n\nBut you already have a NIPs conference to attend and the deadline for submitting a paper is coming soon. Plus, you just received that new research grant, and the committee is expecting some results soon, so right now isn\u2019t really the time to rock the boat.\n\nSo what happens? Most people just pick a safe career and start getting promoted. And if their success is poor, they complain about their unreasonable boss (eg. if only I were rich enough to quit immediately so I could look for a new job) or try to redirect people's attention to their amazing college years (or even the glory days of high school if they're desperate). Thus most lives end up being very average.\n\nBut you already have a mortgage to pay and that deadline for figuring out your health insurance is coming soon. Plus your wife is 8-months pregnant, and the baby is expected to come out any day now, so right now isn't really the time to rock the boat.\n\nStarting from the bottom again is just way too painful and time consuming. The model I have now isn\u2019t horrible\u200a\u2014\u200amy current perplexity score is certainly nothing to be ashamed of. There\u2019s no point in trying to compete with Jeff Dean, that guy knows everything.\n\nStarting from the bottom again is just way too painful and time consuming. The career path I have now isn't horrible - my current salary is certainly nothing to be ashamed of. There's no point in trying to compete with Jeff Dean, that guy knows everything.\n\nSo, stop. This is where you have to ask yourself \u201cAre you really satisfied with where you are?\u201d Sure, not everyone can get state-of-the-art results, but have you actually collected all the data that you could before giving up? Is it really too late to start with a new model? Maybe getting this done right isn\u2019t about just trying to just beat current benchmarks by 1.2%. Maybe you need to stop comparing yourself to everyone else\u2019s results on ArXiv and just ask yourself what really matters.\n\nSo, stop. This is where you have to ask yourself \"Are you really satisfied with where you are?\" Sure, not everyone can get the perfect job, but have you actually tried learning everything you could before giving up? Is it really too late to start with a new career? Maybe getting this done right isn't about just trying to just beat your friend's stock options package by 1.2%. Maybe you need to stop comparing yourself to everyone else's lives on Facebook and just ask yourself what really matters.\n\nBecause you know you can do better. Because you know you were meant for something more.\n\nBecause you know you can do better. Because you know you were meant for something more.", 
        "title": "Training a Neural Network \u2013 Derek Chen \u2013"
    }, 
    {
        "url": "https://medium.com/@DockerTurtle/lets-start-with-tensor-flow-open-sourced-by-google-for-machine-learning-a06dece484b5?source=tag_archive---------1----------------", 
        "text": "What is TensorFlow\u00a0? you must have heard of TF, if not, lets learn about TF\u00a0:) I have just started learning TF.\n\nTF is a library, used for designing and running ML & Deep learning.\n\nWe can create our own Neural network using TF and we write our TF programs in Python.\n\nTF has its own datatype called \u201cTensor\u201d, same as NumPy library.\n\nTF is based on lazy computing, same as Functional programming.\n\nWe can create Tensor objects [Array/Pandas/List]. Once the Tensor object is created, the computational Graph is created automatically.\n\nThe TF graph contains multiple default operations(start, stop, etc).\n\nWe need to use Session object to actually run the Graph (pass input values to Tensor). Session is the runtime in which the Tensor graph operations will run.\n\nAs a developer, we need to build the Graph or use the default Graph.\n\nTF will keep updating the Parameters (m, b) in multiple Steps to minimize the Cost. TF will try to reduce the Cost in sequence of Steps by changing the Parameters, so that the accuracy rate % is high close to 90+%.\n\nAt every Step, TF will change the Parameters (m,b).", 
        "title": "Lets start with \u201cTensor Flow\u201d open-sourced by Google \u2026.for Machine learning"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-12-f0f4367cf6db?source=tag_archive---------2----------------", 
        "text": "Microsoft Artificial Intelligence and Research reported a speech recognition system that makes the same or fewer errors than professional transcriptionists. The team reported a word error rate (WER) of 5.9 percent, down from the 6.3 percent WER the team reported just last month.\n\nRuss Salakhutdinov is joining Apple as a director of AI research in addition to working at CMU\n\nApple Inc. hires the prominent artificial intelligence researcher from Carnegie Mellon University as it seeks to regain lost ground against competitors such as Google, Microsoft Corp. and Amazon.com Inc. in machine learning.\n\nArtificial Intelligence Open Network: a 100% open-source AI research community was launched earlier this week and starts out with 10 open problems.", 
        "title": "\u2014 Issue #12 \u2013"
    }, 
    {
        "url": "https://medium.com/@DockerTurtle/understand-what-is-a-machine-learning-pipeline-42c511bc3153?source=tag_archive---------3----------------", 
        "text": "This video is simple to understand for anyone who is new to ML.\n\nUnderstand the process of design and developing an ML solution.", 
        "title": "Understand what is a Machine learning \u2014 pipeline ? \u2013 Docker Turtle \u2013"
    }
]