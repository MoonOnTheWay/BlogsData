[
    {
        "url": "https://medium.com/the-mission/self-driving-cars-are-hurtling-towards-an-ai-brick-wall-932e16bd7777?source=tag_archive---------0----------------", 
        "text": "Self-Driving Cars are Hurtling Towards an AI Brick\u00a0Wall\n\nChip manufacturers have stopped developing their industry-wide roadmap for the foreseeable future because by 2020 it will be too expensive to build semiconductors smaller than 10 nanometers. It will cost more, create an increasing amount of heat, and require more electricity than the world currently produces.\n\nOur use of computing power is not shrinking. In fact it\u2019s growing faster than even the number of humans born every year. As more objects become smart, connected to the cloud, and we run more machine learning on the data we collect, it means the exponential growth with begin to grow exponentially.\n\nI know that\u2019s a bit of a brain sidewinder. Exponential growth, exponentially? Just replace it with \u201ca helluva lot\u201d.\n\nAs you can see, we\u2019re reaching fundamental limits of human manufacturing. Of physical things. We\u2019re getting down to atomic levels that have very real stop signs based on the known laws of physics.\n\nIf you\u2019ve followed tech for some time you immediately recognize this as an intersection of forces that makes it ripe for opportunity. A leap of innovation.\n\nWe either put our heads together to rethink the way we do things, on a truly fundamental basis, or we stagnate as a species. Do you really believe that all we as a species can achieve is Google search and Facebook apps running on an iPhone?\n\nOf course not.\n\nSo let\u2019s talk about the ways we may be able to fundamentally disrupt the infrastructure we\u2019ve spent the last 100 years building.\n\nThe first is energy. Without some type of power, we can\u2019t do anything else. No light. No movement. No computation. So we need an abundant power source. More than the amount of fossil fuels on earth. More than the electricity we can generate from water or wind. We need that giant fusion reactor in the sky (dyson sphere, anyone?). And we need to collect much more than the 25% limit that we currently get from the best solar panels.\n\nBut that\u2019s a side technology compared to what\u2019s needed for exploring and transporting things from one place to another. That\u2019s why this post is about autonomous navigation.\n\nEveryone is chasing the same white horse. Truly human-less navigation from one point to another. Level 2 autonomy, which is basically what the Tesla feature does today, is about as far as we\u2019ve come for something running in production. Normal people can buy level 2 autonomous vehicles today.\n\nIt tracks lane markings on highways, and that helps you while commuting in a traffic jam. Basically, lane keeping. But Tesla is still about 2 steps away from true autonomy with their Summon feature (i.e., push a button and your car drives across country to the exact place you\u2019re standing).\n\nThat\u2019s a helluva differentiating feature that everyone is chasing but nobody yet knows how to do.\n\nAside from that, the only other compelling feature I can come up with for electric vehicles is an interchangeable parts system for interior design purposes.\n\nWhat happens when you put a car in an area that Google or Apple hasn\u2019t tracked with their map database? What happens when you\u2019re driving between LA and Vegas and your car\u2019s cell signal drops out? Does the car just stop?\n\nTaken further, how will flying drones not run into new construction or other drones that can\u2019t be mapped by satellites in real-time? What happens with rovers on the surface of Mars or with mining operations on asteroids when there\u2019s no internet, communication loops take nearly an hour, and have no maps to speak of?\n\nHow does a machine know where to navigate when nobody has ever navigated it before? And how does it do this in real-time where you can\u2019t spend days or weeks training to recognize a specific rock formation or sky line so your billion dollar space rover doesn\u2019t drive right off a cliff?\n\nHave a look at the machine intelligence 2.0 chart below showing the various startups in the industry. In the red-colored upper-right hand corner, you\u2019ll see the autonomous systems. There\u2019s something that stuck out to me. Where is space? The graphic below only has air, ground, sea, and industrial. I think I know why. Because self-driving is exponentially harder in space.\n\nIt\u2019s also because the answer won\u2019t come from Deep Learning. But before we get into why, it\u2019s important we define the different levels of autonomy. The public may think they\u2019re all one in the same, but the capabilities are staggeringly different.\n\nThere\u2019s been a few things published about it, but I don\u2019t think any of them are very good. Below is my first stab at it based on NHTSA\u2019s description, where I think each level is best applied to products, and what job is done by software and by humans.\n\nTesla\u2019s current autopilot system in production is Level 2. GoogleX\u2019s car in R&D is Level 3. Some other R&D labs are potentially in Level 4, including SpaceX. But everyone, regardless of industry, is pushing hard towards Level 5.\n\nIt\u2019s what I call Wayfinding AI. It gets you from point A to point B without any human intervention. That\u2019s the type of self-driving we need on Earth and on Mars.\n\nHowever, the car market here on Earth is about $1.5 trillion. That\u2019s a lot of money at stake and one reason why GM bought Cruise for a reported $1 billion dollars (new news states it was less than that), and Uber bought OTTO after 5 months for $680 million. The market is red hot for a new approach that goes beyond the limits of deep learning.\n\nWhy? Because if training data has never seen a Black Swan event before, how can it decide how to handle it?\n\nThe answer to Level 5 autonomy (i.e., Wayfinding AI) does not lie in the same approach as what\u2019s required to reach Level 3 or 4. Rather, we need to make another quantum leap. We need some other type of innovation to jump over that brick wall standing in our way. Much like the answer to traveling isn\u2019t hotels but AirBnB, and for transportation isn\u2019t your own car but an Uber, the answer to Level 5 autonomous driving will not be mathematical, but biological.\n\nThere are only about 3 people in the world who get this. That\u2019s because it requires a math degree, a computer science degree, and a biology degree. 3 incredibly different and difficult skills to develop expertise on. As one of my friends who has some of these unicorns skills recently said while working on this:\n\nThat right there is what we\u2019re talking about. If you don\u2019t have the right experience, everything looks like a mathematical nail for your deep learning trained hammer. I used to be one of them. Heck, I have a degree in theoretical mathematics. So I get it, guys. I was in the same camp until the day I had the epiphany below.\n\nDeep Learning is just a \u201ctrendline\u201d approach and some of the concepts are even taught in high school (mostly it\u2019s linear algebra and regression in college). But for autonomous driving, it requires a massive amount of data, large parallel processing from GPUs, lots of electricity and days to train a model.\n\nThat\u2019s not how we learn or navigate the world. We touch a hot stove once, it hurts, we don\u2019t do that again. We don\u2019t have to burn our hands 1 million times before we learn not to touch the stove. Once is painful enough (secret algorithm alert!). In fact, our biology is so good that we don\u2019t even have to burn ourselves to learn. We can watch someone else burn their hand on a stove and realize we shouldn\u2019t do that either. In short, we can learn from the experience of others.\n\nI\u2019d like to give some more detail, so what follows is a comment I added to The Information\u2019s article on Tesla losing its mapping leader.\n\nThe best way to get a robot to move about in the world isn\u2019t to try to recreate a robot from scratch, but rather to steal from nature. Why try to recreate evolution in a deep learning simulation when we can already use it as a head start?\n\nWe have just begun mapping the human connectome (brain + nervous system), but we are incredibly far away from completing it. Note that the nervous system part is often what gets left out when describing intelligence. We need that in order to move. We need muscles. That\u2019s why robotics hasn\u2019t gotten to the dream of science fiction. We\u2019re all focused on the brain, but forget that the definition of Neurons include nervous system connections as well brain connections. There are 100 billion neural connections in the human body that needs to be traced. We are 0% complete with that. But we do have a few other animals fully and partially mapped.\n\nIn terms of learnings, there are three major things we\u2019ve begun to understand about this new approach:\n\nThe connectome is a better approach, but that doesn\u2019t mean it solves every problem. As you level up into more generalized autonomous mobility, there are new and different problems to solve. You also need the ability to see and hear, which likely will require deep learning to make sense of the sensory input.\n\nIn short, you need the biological approach at the core for human decision-making, and the mathematical approach at the edges to give it superhuman senses.\n\nBelow is the most recent paper on the Human Connectome and gives you some sense of the complexity we\u2019re dealing with. As you read through it, I think you might begin to believe what I now believe. That deep learning is only one tool to get closer to AGI, but isn\u2019t the best hammer for the self-driving nail.\n\nAs a neurobiologist already working on this connectomic self-driving solution, Timothy Busbice, said to me recently:\n\nNote: I am an investor and insider in the biological self-driving AI product being discussed above. Learn more at http://prome.ai", 
        "title": "Self-Driving Cars are Hurtling Towards an AI Brick Wall"
    }, 
    {
        "url": "https://hi.stamen.com/visualizing-global-satellite-coverage-with-planet-labs-db6158d639e1?source=tag_archive---------1----------------", 
        "text": "In the examples below (which uses historical sample data), bright is good (coverage very often), and dark is bad (coverage not so often) and of course, forecasts are subject to dependancies with launch schedules :\n\nThere are a number of reasons why a global daily picture of the planet is trickier than it might seem at first (in addition to being, you know, totally awesome). The first is complex. To see why, take a look at this animation of predicted Planet satellite locations on August 31, 2016. The orbital data to create the visualization was collected from Dr. T.S Kelso\u2019s Celestrak.\n\nPlanet\u2019s satellites are released in \u201cFlocks\u201d. The individual satellites in each Flock have a similar orbital path that drifts over time. Two satellites on the same orbit, but at different positions in their orbit, will pass over different areas due to the Earth\u2019s rotation. But notice that even with the many satellites plotted above, there are vast areas without any coverage at a given time.\n\nBesides simply having enough satellites, another challenge is putting up satellites with high inclination. This is the angle of the orbit makes with the plane which passes through the Earth\u2019s equator. The inclination determines both the maximum and minimum latitude a satellite will reach during its orbit. Flock 1C and 2P are both in polar orbits, meaning they have large inclinations and will pass over both Antarctica and the Arctic Circle.\n\nThe second is a lot simpler: clouds! It turns out the world is a very cloudy place, and while Google Maps has conditioned us to think that we can look at any place on the planet and see the ground, they in fact do a tremendous amount of work to strip out all the clouds that block satellites\u2019 views. And Planet\u2019s in the business of supplying data and pictures of things on the ground, so while images of clouds are cool, they don\u2019t consider a place properly imaged unless you can see the ground clearly (and do a lot of work to make this happen).\n\nThe cool thing about this, from the point of view of people who like to tell stories with data, is that you can start to see things about the world that you might not have been specifically looking for (which is one of the reasons that what Planet does is so amazing). Planet\u2019s improving their coverage all the time and sending more and more Doves into orbit, so it makes sense that if we look at this data over time, more and more of the planet would be regularly photographed. In this image, there\u2019s a deep dark band across India and Southeast Asia, and in the chart you can see a corresponding drop in the effective revisit rate during July and August. Monsoon!", 
        "title": "Visualizing global satellite coverage with Planet \u2013"
    }, 
    {
        "url": "https://resources.trainingdata.com/searching-rat-brains-for-clues-on-how-to-make-smarter-machines-937fb95dab0e?source=tag_archive---------2----------------", 
        "text": "Larger data sets and faster computers have enabled a recent flurry of progress\u200a\u2014\u200aand investment\u200a\u2014\u200ain artificial intelligence. David Cox of Harvard thinks the next big jump will depend on understanding what happens inside the head of a rat when it plays video games. Cox leads a $28 million project called Ariadne, funded by the U.S. Office of the Director of National Intelligence, that is looking for clues in mammalian brains to make software smarter. \u201cThis is a huge, moonshot-like effort to go into the brain and see what clues and tricks are hiding there for us to find,\u201d he said today at EmTech MIT 2016\u2026", 
        "title": "Searching Rat Brains for Clues on How to Make Smarter Machines"
    }, 
    {
        "url": "https://medium.com/@AlethiaMcNeely/reading-between-the-lines-bda22da02fb4?source=tag_archive---------3----------------", 
        "text": "So many things that aren\u2019t said, aren\u2019t vocalized. Those thoughts we have we keep to ourselves, we bind them inside.\n\nI\u2019m good with reading between the lines, its a talent I learned very young. When you are in interesting circles of people doing illegal things, that they shouldn\u2019t be doing, and surrounded both by safe and unsafe people, personal safety requires you to read between the lines. You learn the things unspoken are just as important, often more important than the things said.\n\nBody language speaks alot\u200a\u2014\u200amore than most people realize. Voices too give away much, with the tone, the pauses the inflections and the minor shifts in speed, pitch and animation. Chosen words, posturing and what happens when the words stop, these also give clues, like puzzles to unravel.\n\nThis reading between the lines is a great skill in a world that is in person, in the face, touching each other. Its an intensively valuable skill in that world; one most people didn\u2019t bother to cultivate, many of the ones that have are called \u201cstreet smart\u201d\u200a\u2014\u200athose that didn\u2019t are often viewed as Not being \u2018street smart\u201d or being a bit naive.\n\nHow does one leverage this skill in a world of digital input? Where worlds of unspoken and between the lines exist, and aren\u2019t available to be deciphered by body language, or even necessarily voice. What if the chosen words don\u2019t necessarily leave breadcrumbs of understanding?\n\nThere is the obvious answer of asking, but sometimes the question doesn\u2019t get answered, sometimes the answer still leaves words unspoken\u200a\u2014\u200apaths unclear.\n\nThere is the cloak and dagger method of investigation, and intrigue, but its just so so time consumptive, and often leads more to assumptions than fact. Assumptions about what the information revealed speaks, and whether it was the between the lines or not.\n\nWhen someone says \u201cprobably\u201d\u200a\u2014\u200adoes it really mean its likely to happen? Or does it mean no? When someone says \u201cmaybe\u201d is this really a pretty social lie way of saying no?\n\nHow does one accurately get to the heart of those words, so as to maintain social eloquence while at the same time retaining \u2018street smarts\u2019 because street smarts, is even more critical in this digital day and age then it ever was in the non digital day and age.\n\nSo much of our minds, hearts, lives, finances, personality, and personal details are laid out before the masses whether we are aware or unaware of this being the case. All of these programs that farm our digital snip-its of data, mining them and molding them to caterer the message to be one that is pleasing to us, or to obtain some result from our activities. All of this would be so much more safe and secure with digital street smarts.\n\nI mean there are courses on cyber security, they like \u201cbook learning\u201d will teach you the fundamentals of protecting your assets. They will teach you the intimate details of what to do and what not to do. They will not however, teach you how to determine what a person your engaging with, a person that you naturally want to trust, is not saying between the lines.\n\nIn this digital world, language is changing\u200a\u2014\u200abrb, afk, lol, roflmao, lul, gg\u2026 all of these letters in form have meaning, they have depths of meaning that are relative to this medium. Its a cultural difference that transcends language, nation, nationality, race, creed, color, religion. All of the defining blocks of face to face interaction are clean slates in this digital world, because while all of our data is available to be mined and geared and advocated, the average user doesn\u2019t have access\u200a\u2014\u200aso we are forced to interact with them using the communication skills we have available.\n\nI don\u2019t know where one learns to read between the lines, but its been a journey I\u2019ve been trying to master for a few years now. In my journeys, I met an unusual individual that has been helping me to learn this skill, helping mold me to see from the perspective of someone that grew up in this land, on this digital street, and has taken my hand to show me the ropes. We don\u2019t talk about this, but its one of those things we read between the lines in our conversations.", 
        "title": "Reading between the lines \u2013 Alethia \u2013"
    }
]