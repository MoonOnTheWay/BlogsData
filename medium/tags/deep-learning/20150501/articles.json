[
    {
        "url": "https://medium.com/@jain49/ai-contextual-reasoning-learning-75bd45ab518f?source=tag_archive---------0----------------", 
        "text": "Artificial Intelligence (AI) has four seasons: hype, disappointment, funding drought, and renewed interest. I\u2019ve been involved in AI research for quite some time\u200a\u2014\u200aI became a fellow of the Association for the Advancement of Artificial Intelligence (AAAI) in 1993\u200a\u2014\u200aand I\u2019ve weathered several seasonal cycles. What I\u2019m seeing now, however, is the most puzzling cycle yet; either I\u2019m getting old and addled, or the current cycle is unique in its magnitude.\n\nIn these Big Data days, the big talk about AI\u2019s potential reminds me of what happened at the peak of earlier cycles (see, for example, the recent Wall Street Journal article. Once again, the focus is on a single technical component\u200a\u2014\u200adeep learning\u200a\u2014\u200aand hopes seem to be building that it can solve many very hard problems easily and more or less magically. Almost all discussion suggests that all we need is deep learning because enough data is available, and deep learning can identify models in that data. What we do with the models\u200a\u2014\u200aand how we solve real problems\u200a\u2014\u200ais ignored.\n\nBy nature, AI evokes a strong emotional reaction in most people. We humans like to consider intelligence our unique strength, one that has made us the most powerful (animal) in nature. Intelligence made us unique. Now, we want to build machines that will also be intelligent, an attractive quest that motivates many young researchers to enter the AI field, even as it alienates those who fear the consequences.\n\nTypically, intelligence is viewed as having the ability to perceive the environment and take actions using reasoning. To take these action in space requires various capabilities, including logic, problem-solving, self-awareness, planning, and spatial reasoning.\n\nTo solve real-world tasks or function in the real world in general, an intelligent agent must possess both perception and cognition. Perception allows agents to detect objects and events in the environment using the available sensory data, while cognition lets them use reasoning to build self awareness and solve problems. If an agent possesses only one aspect\u200a\u2014\u200asay, logic (cognition)\u200a\u2014\u200aits applicability will be limited.\n\nIn earlier days, computing\u2019s processing power and memory were too small to deal with perceptual issues. As we progressed in knowledge representation, rule-based systems, and so on, however, the AI seasons emerged. Good progress in one area created unrealistic expectations of an agent\u2019s ability to solve problems that required as-yet undeveloped capabilities; this fueled disappointment, leading to funding drought. The most recent cycle was fueled by rule-based systems (commonly known as expert systems).\n\nCurrently, we are in the hype season, courtesy of Big Data. We now have powerful processing capabilities, which let us manage large data volumes and more accurately compute probability distributions for pattern classification. Similarly, with very powerful computing, we can build extensive techniques for doing multi-level and build-learning techniques without having to specify features that are based more on human experience than detailed data analysis. This results in significantly better pattern-classification techniques, which researchers have successfully demonstrated recently, particularly in the area of concept detectors in computer vision. This, in turn, can inspire exaggerated expectations or hype, exemplified in a recent article I read.\n\nPattern classification techniques are required in other related areas, including the classification of consumers into specific demographics or interest groups\u200a\u2014\u200aa powerful application that\u2019s attractive to almost all major businesses.\n\nImproved pattern classification has made deep learning very hot. Deep learning, a popular machine learning technique, is a component of AI perception that helps build pattern recognition models.\n\nIn machine learning, the quality of training data determines the quality of the resulting model. To obtain quality training data the machine\u2019s designer first determines the context in which the machine will be working and then defines the machine\u2019s application. For example, a recent deep learning approach separates a singer\u2019s voice from the noise at a cocktail party. Here, the machine\u2019s scope is the cocktail party; it must then be trained using spectrograms of singer\u2019s voice as training data.\n\nResearchers thus carefully design the context for use, and each machine works effectively only in that context. During the machine\u2019s training phase, the context for its successful operation is carefully coded in the learning algorithm\u2019s function. Thus, where content was once king, a new leader is beginning to emerge.\n\nAs sensors become more sophisticated context is beginning to dominate all applications. The availability of many types of sensors has resulted in powerful smartphones and the Internet of Things, which are both making enormous volumes of contextual data available. In their book, Age of Context, Robert Scoble and Shel Israel note that \u201cthe changes ushered in by the Age of Context will be more significant and fundamental than what has occurred in the previous era, and they are likely to occur faster.\u201d With the emergence of smartphones and wearable sensors that keep getting better, smaller, and more capable of measuring almost anything, context is increasingly becoming more powerful and important. And it\u2019s increasingly determining the relevance and role of the former king, content.\n\nThis has serious implications for AI. Most intelligent humans are self-aware and understand their context through various sensors and reasoning processes. That is, humans decide which trick to use in which context; as a result, they operate in their environments intelligently (see \u201cThe Perception of Apparent Motion\u201d article by Vilayanur Ramachandran and Stuart Anstis at http://cbc.ucsd.edu/pdf/Percpt_Apprnt_Mot_Sci_Am.pdf). Thus, to build on the progress Big Data has inspired, we must first train systems for specific contexts and then use contextual reasoning and specific learning techniques to efficiently and effectively solve problems.\n\nProblem solving usually requires multiple steps: we derive models from the given data, use appropriate techniques to solve specific problems, and finally achieve the goal. Models derived from data are important, as is the application of appropriate techniques. Such techniques depend on the goal as well as on the context at different problem-solving stages. Thus, contextual reasoning is as important as learning for deriving models.\n\nTo delay\u200a\u2014\u200aor, perhaps, avoid altogether\u200a\u2014\u200athe next AI funding drought, we must find a way to balance contextual reasoning and deep learning. Luckily for AI researchers and enthusiasts, we now have Big Data to train our models and myriad contextual sources that can help create a holistic situation for training and using those models. The key is to maintain a focus on problem solving using all our resources, even as the season of hype over a single technique rages on.", 
        "title": "AI = Contextual Reasoning + Learning \u2013 Ramesh Jain \u2013"
    }, 
    {
        "url": "https://medium.com/@Pinterest_Engineering/building-a-scalable-machine-vision-pipeline-60dd7bac73e7?source=tag_archive---------1----------------", 
        "text": "Discovery on Pinterest is all about finding things you love, even if you don\u2019t know at first what you\u2019re looking for. The Visual Discovery engineering team at Pinterest is tasked with building technology that will help people to continue to do just that, by building technology that understands the objects in a Pin\u2019s image to get an idea of what a Pinner is looking for.\n\nOver the last year we\u2019ve been building a large-scale, cost-effective machine vision pipeline and stack with widely available tools with just a few engineers. We faced two main challenges in deploying a commercial visual search system at Pinterest:\n\nToday we\u2019re sharing some new technologies we\u2019re experimenting with, as well as a white paper, accepted for publication at KDD 2015, that details our system architecture and insights from these experiments and makes the following contributions:\n\nIt used to be that if a Pin had never before been saved on Pinterest, we weren\u2019t able to provide Related Pins recommendations. This is because Related Pins were primarily generated from traversing the local \u201ccuration graph,\u201d the tripartite user-board-image graph evolved organically through human curation. As a result, \u201clong tail\u201d Pins, or Pins that lie on the outskirts of this curation graph, have so few neighbors that graph-based approaches do not yield enough relevant recommendations. By augmenting the recommendation system, we are now able to recommend Pins for almost all Pins on Pinterest, as shown below.\n\nFigure 1. Before and after adding visual search to Related Pin recommendations.\n\nThis experiment allowed us to show visually similar Pin recommendations based on specific objects in a Pin\u2019s image. We\u2019re starting off by experimenting with ways to use surface object recognition that would enable Pinners to click into the objects (e.g. bags, shoes, etc.) as shown below. We can use object recognition to detect products such as bags, shoes and skirts from a Pin\u2019s image. From these detected objects, we extract visual features to generate product recommendations (\u201csimilar looks\u201d). In the initial experiment, a Pinner would discover recommendations if there was a red dot on the object in the Pin (see below). Clicking on the red dot loads a feed of Pins featuring visually similar objects. We\u2019ve evolved the red dot experiment to try other ways of surfacing visually similar recommendations for specific objects, and will have more to share later this year.\n\nFigure 2. We apply object detection to localize products such as bags and shoes. In this prototype, Pinners click on objects of interest to view similar-looking products.\n\nBy sharing our implementation details and the experience of launching products, we hope visual search can be more widely incorporated into today\u2019s commercial applications.\n\nWith billions of Pins in the system curated by individuals, we have one of the largest and most richly annotated datasets online, and these experiments are a small sample of what\u2019s possible at Pinterest. We\u2019re building a world-class deep learning team and are working closely with members of the Berkeley Vision and Learning Center. We\u2019ve been lucky enough to have some of them join us over the past few months.\n\nIf you\u2019re interested in exploring these datasets and helping us build visual discovery and search technology, join our team!\n\nKevin Jing is an engineering manager on the Visual Discovery team. He previously founded Visual Graph, a company acquired by Pinterest in January 2014.\n\nAcknowledgements: This work is a joint effort by members of the Visual Discovery team, David Liu, Jiajing Xu, Dmitry Kislyuk, Andrew Zhai, Jeff Donahue and our product manager Sarah Tavel. We\u2019d like to thank the engineers from several other teams for their assistance in developing scalable search solutions. We\u2019d also like to thank Jeff Donahue, Trevor Darrell and Eric Tzeng from the Berkeley Caffe team.\n\nFor Pinterest engineering news and updates, follow our engineering Pinterest, Facebook and Twitter. Interested in joining the team? Check out our Careers site.", 
        "title": "Building a scalable machine vision pipeline \u2013 Pinterest Engineering \u2013"
    }, 
    {
        "url": "https://medium.com/@jdwittenauer/configuring-theano-for-high-performance-deep-learning-58f035ec6b22?source=tag_archive---------2----------------", 
        "text": "The topic of this post is perhaps a bit mundane, but after spending a considerable amount of time getting this right, I decided to put together a step-by-step guide so I would remember how to do it next time. And since I already went to the trouble, I might as well share it and hopefully save others a few headaches.\n\nTheano, if you\u2019re not aware, is a Python library that allows you to define, optimize, and evaluate mathematical expressions involving multi-dimensional arrays efficiently. It has a number of really interesting features, particularly its transparent use of a GPU for massively parallel computation and the ability to dynamically compile expressions into optimized C code. Theano provides developers with a general-purpose computing framework for very fast tensor operations and is widely used in the Python machine learning community, particularly for deep learning research. If you\u2019d like to learn more, there\u2019s detailed documentation available here.\n\nWhile Theano is very powerful, it\u2019s unfortunately not that easy to set up and configure properly. The website does provide instructions, but they\u2019re often inconsistent or out of date. There are also a number of blog posts and Stack Overflow questions floating around that discuss various elements of the process, but nothing that completely covered what I needed. So that\u2019s what this post will address.\n\nTo be clear, my goal was relatively simple\u200a\u2014\u200ainstall and configure Theano with an efficient BLAS implementation for CPU operations, along with the ability to leverage a GPU when desired, on a clean install of Ubuntu 15.04 so I could run deep learning experiments. Although I\u2019m running Ubuntu 15, this guide should also work on 14.xx. As a bonus, I\u2019ll also cover setting up a fresh install of Anaconda and downloading/installing a new but promising deep learning library called Keras.\n\nFirst, open a terminal and run the following commands to make sure your OS is up-to-date.\n\nNext, install and set up Git. We need this to download and build OpenBLAS ourselves.\n\nNext we need a fortran compiler (again required to build OpenBLAS).\n\nNow we\u2019re going to retrieve and build OpenBLAS. It\u2019s possible to skip this part and just download a pre-built BLAS package, but you\u2019ll get much better performance by compiling it yourself, and it requires relatively little effort. First create a Git folder (I added it to my home directory), then run these commands:\n\nNext let\u2019s set up the required GPU tooling. If you\u2019re using a relatively modern NVIDIA card, your best bet is to use CUDA. It\u2019s possible to configure Theano using OpenCL as well, but I haven\u2019t tried this so I won\u2019t cover it here. There are a number of ways to install CUDA, and many guides advise you to download the binaries from NVIDIA\u2019s website and run some commands to install it, but actually there\u2019s a package available that installs everything you need. Run the following commands:\n\nThe first line installs NVIDIA\u2019s graphics card drivers, and the second line installs the CUDA tools. Note that the \u201cnvidia-current\u201d package may (despite what the name indicates) install an older version of the drivers than is actally available. Don\u2019t fall into the trap of thinking to have to use the legacy drivers to use CUDA. You don\u2019t! I ended up installing newer drivers from the system menu and it still worked fine.\n\nYou should restart your system after installing new drivers to make sure everything gets loaded properly. To verify that CUDA is installed, run this at the terminal (it should output some text that includes your graphics card model in it somewhere):\n\nNext we\u2019ll get Anaconda set up, which will install Theano along with all of its dependencies. Download the binaries here and save them somewhere locally, then navigate to the path where you saved the file and run:\n\nTo make sure your Anaconda install is up-to-date and all of Theano\u2019s dependencies are there, run a few statements at the terminal using the \u201cconda\u201d package manager:\n\nWe\u2019re now in the home stretch. All that\u2019s left are some configuration items to tell Theano where your BLAS/CUDA libraries are and how to use them. First, create a file called \u201c.theanorc\u201d in your home directory and add the following contents to it:\n\nFinally, we need to add the CUDA path to an environment variable that Theano also looks for. Run the following statement:\n\nThat\u2019s it! You\u2019re now set up to use Theano. In order to verify that everything is working, add the following code to \u201ctheano_test.py\u201d in your home directory:\n\nThen run it from the terminal and verify that it completes using the correct device (based on your configuration earlier):\n\nNOTE: At this point I ran into an issue that had me banging my head against the wall for a few hours. When I tried to run the above test, Theano could not access my GPU even though CUDA was configured correctly. Although the exact underlying cause is still unclear to me, it turns out that running any CUDA process with root access first will \u201cinitialize\u201d things such that any future processes will run successfully without root access. This needs to be done once after each restart and then you\u2019re good to go. One possible solution to this is to run the above script with root access:\n\nHowever, this presented two new problems for me:\n\n1) My Anaconda python distro wasn\u2019t properly linked while running with root, so it was using the base Linux install and couldn\u2019t find any of the libraries\n\n2) The process creates a temporary folder for Theano\u2019s compiled code that was then inaccessible without root access, causing future attempts to use Theano to fail\n\nI resolved both of these issues by creating a simple bash script that I run once each time I reboot the machine. Create a file called \u201cinit.sh\u201d in your home directory and add the following lines of code to it:\n\nThe folder with the compiled code may have a different name, but it appears to be the same each time Theano runs, so just check to see what yours looks like and put it in the script. Then run this each time you reboot:\n\nNow you should be good to go. The last step is adding a library to build deep learning nets with. I recommend trying Keras. It\u2019s still a bit immature but already has a great feature set, and the API is the best I\u2019ve seen for this kind of stuff. To get Keras installed, run the following commands from your git folder:\n\nThat\u2019s all there is to it. You\u2019re now ready to start building ultra-fast deep learning nets. Enjoy!", 
        "title": "Configuring Theano For High Performance Deep Learning"
    }, 
    {
        "url": "https://medium.com/@christinecalo/should-we-be-freaked-out-that-hawking-musk-and-gates-are-freaked-out-about-ai-af2fa9216e59?source=tag_archive---------3----------------", 
        "text": "Lately I\u2019ve been noticing a fair bit of commotion about Artificial Intelligence (AI), in particular the fears towards AI and the possibility of it being the greatest existential threat to humanity. These fears have not been made by some crackpot doomsday sayers but by very prominent intellectual figures, figures such as a notable theoretical physicist, cosmologist, a space-age entrepreneur and a founder of the personal computer industry. It\u2019s hard not to be concerned when prominent, technologically savvy people like Stephen Hawking, Elon Musk and Bill Gates are alarmed.\n\nA fair amount of people working in the field of AI believe that human-level artificial intelligence, or sometimes known as singularity is only two to three decades away.[1] It is also common thought that according to Moore\u2019s law singularity may be reached sooner than expected. Some predict that what we are going to see is AI systems starting to self-replicate and update their own systems. It will eventually reach to this true deep artificial intelligent system that can learn on its own and have autonomous feelings and will have awareness of itself, liken to human consciousness.[2] Much of the commotion has been caused by the field of \u2018deep learning\u2019 or \u2018machine learning\u2019 in which computers teach themselves tasks by crunching large sets of data and has given rise to extreme concerns.[3]\n\nSome of the concerns include the danger that artificial intelligence could overtake humans. Hawking explained this in an interview with the BBC.[4]\n\nMusk also shares similar concerns and has said\n\nMusk recently pledged $10 million to the Future of Life Institute, to fund research grants investigating potentially negative implications. Both Musk and Hawking, as well as, other groups of scientists and entrepreneurs have signed an open letter promising to ensure AI research benefits humanity. The letter warns that without safeguards on intelligent machines, mankind could be heading for a dark future. The document, drafted by the Future of Life Institute, said scientists should seek to head off risks that could wipe out mankind.[6]\n\nGates has revealed that he doesn\u2019t believe AI will bring trouble in the near future however, there is reason to have concern.\n\nI always thought that if AI or even humans were to reach that super intelligence level, I\u2019d hope that this intelligence wouldn\u2019t be reduced to animalistic, primitive actions of ridding those that we saw as inferior to us in order for survival. If this action was deemed as a form of intelligence which I believe is the opposite, then humans would have already wiped out all other species in this world. However, we know that the existence of other organisms whether we believe them to have a lower mental capacity to ours, serves to benefit us. We know that when there\u2019s disruption with a certain species it in turn disrupts our complex ecosystem, consequently affecting our existence. Also, we know that there\u2019s more loss than gain when we venture into wars. There are major losses in both human and technological resources and it leaves a scar in the human psyche that breeds distrust, paranoia and hinders growth and progress.\n\nI agree with author Edith Cobb\u2019s notion that \u2018there is a force inherent in the human biology itself that is even more powerful than the classic Darwinian idea of self-reproduction. She writes that,\n\nIf AI were to kill off our species and then migrate to another planet or universe acting as a parasite and terminating every other organism they encounter, this would be a fruitless endeavor that would eventually lead to their demise as other organisms would retaliate. A far more superior, intelligent notion would be to endorse cooperation as it forms cohesion. As found in \u2018Tit for Tat\u2019 a strategy in game theory for the iterated prisoner\u2019s dilemma, cooperation is found to be the most evolutionary stable strategy.[8] I would hope that any form of super intelligence would have reached to that conclusion. Wouldn\u2019t exploration into the mystery of existence and the discovery of new realms of meaning is a far nobler pursuit than to just merely consume and exist?\n\nProgress can only be achieved through cooperation biologically and artificially. AI will always need humans and they only exist through human experiences and memories. Current AI are built on systems that collects oceans of data collected from human experiences. That data has a rapid search procedure that has algorithms to analyse and discover particular things about that data. An example of this is Deep Blue known as the first piece of AI. Deep Blue is a chess playing computer developed by IBM and was the first AI to win both a chess game and a chess match against a reigning world champion under regular time controls. Deep Blue succeeded in this as its database contained 50 grand masters\u2019 planning strategies, 700,000 grandmaster games and it had the capability of evaluating 200 million positions per second which is obviously faster than what an average human can process. However, Deep Blue could only achieve this through the existence of those grand masters planning strategies which is only possible through human experience.[9]\n\nAnother example is Jeopardy winning Watson which is an AI computer system capable of answering questions posed in natural language. Watson is \u2018about Big Data. It is about ingesting vast amounts of information on specific subjects\u200a\u2014\u200aand allowing a user to query the data to look for patterns, assist in a diagnosis.\u2019 Watson is currently being tested to aid doctors to more quickly and accurately make diagnoses and its library is made up of 23 million medical papers in the National Library of Medicine.[10] However, Watson\u2019s diagnoses are only achievable by the past achievements of medical practitioners which makes up this library.\n\nDeep Blue and Watson are examples of systems that simply accumulate the understanding that has been achieved by humans and using brute force to run through rapidly and comprehensively to discover particular things about this data. There is debate that machines are close to reaching human intelligence and human consciousness. They would eventually have the ability to have human experiences. However, I do have doubts as \u2018today\u2019s AI produces the semblance of intelligence through brute number-crunching force, without any great interest in approximating how minds equip humans with autonomy, interests and desires. Computers do not yet have anything approaching the wide, fluid ability to infer, judge and decide that is associated with intelligence in the conventional human sense.\u2019[11] Capturing the nature of human intelligence is a colossal problem. It\u2019s not enough to create a neural network, to simulate the brain and hope that some sort of intelligent behavior might emerge. As Gary Marcus nicely depicts,\n\nHuman behavior and intelligence and how it is derived are very complex and is formed from both the evolutionary unpredictability of nature as well as nurture. It\u2019s just difficult to believe that systems that use brute force to crunch large data sets is likened to embodying this biological complexity.\n\nI agree with Noam Chomsky in his talk on the Singularity pod cast that there are far more pressing problems in the world than the coming of Singularity. Chomsky states \u201cRay\u2019s (Ray Kruzweil) technological singularity is science fiction. I don\u2019t see any particular reason to believe it. We should be more worried about the end of our species. We are very busy dedicating ourselves to destroying the possibility of decent survival. I think we should be worried about that.\u201d[13] Chomsky claims we should be more worried about the climate destruction we are carrying out. Musk also stresses this too \u2018our oil based, carbon intensive economy as creating a \u201ccrazy chemical experiment on the atmosphere\u201d with likely catastrophic consequences.[14] \u201cIf we don\u2019t find a solution to burning oil for transport, when we then run out of oil, the economy will collapse and society will come to an end.\u201d[15] As we become more technologically intensive there will be an increased need for energy. Our current sources of energy are unsustainable and so there should be more focus on finding and using renewable sources of energy.\n\nAnother pressing issue is the current economic climate is generating a dystopia of socially unsustainable inequality. Before we reach that stage of true deep artificial intelligent systems there will come a point to where we can automate jobs that are highly cognitive and non-routine. As Martin Ford declares in his book \u2018Rise of the Robots\u2019 that white-collar jobs are at risk and argues that a bleak jobless future awaits if we don\u2019t take action. Cognitive computing and genetic programming will soon do to even the most dynamic white collar workers what robots are doing to men and women on the assembly line. And it gets worse, according to Ford. \u201cIndeed, because knowledge-based jobs can be automated using only software, these positions may, in many cases, prove to be more vulnerable than lower-skill jobs that involve physical manipulation.\u201d[16] This automation will displace significant numbers of both blue- and white-collar workers and will lead to income inequality and a breakdown in social order. Nobody will be immune to this labour reality and hence there should be initiatives to prepare and tackle this. There has been suggestion of adopting a guaranteed basic income where everybody in society has a right of a universal basic income.\n\nWhile I hold a similar skeptical view to Chomsky about singularity and a much more optimistic view of AI compared to the techno elites I do believe that there should be safeguard measures taken. As Anthony Wing Kosner rightly states, we should be more \u201cconcerned about the motivations of rogue humans who may misuse these technologies than about the rogue capabilities of the products of these technologies themselves.\u201d[17] The creation of AI can take on a life on its own and can do great harm if not set up in a just manner.\n\n1.Atkins, David. \u201cIf Bill Gates, Elon Musk and Stephen Hawking Are Worried, Shouldn\u2019t You Be?\u201d The Washington Monthly. The Washington Monthly, 8 Feb. 2015. Web. 26 May 2015. <http://www.washingtonmonthly.com/political-animal-a/2015_02/if_bill_gates_elon_musk_and_st054073.php>.\n\n2. Casey, Michael. \u201cMaybe Artificial Intelligence Won\u2019t Destroy Us after All.\u201d CBSNews. CBS Interactive, 14 May 2015. Web. 26 May 2015. <http://www.cbsnews.com/news/maybe-artificial-intelligence-wont-destroy-us/>.\n\n3. \u201cRise of the Machines.\u201d The Economist. The Economist Newspaper, 9 May 2015. Web. 26 May 2015. <http://www.economist.com/news/briefing/21650526-artificial-intelligence-scares-peopleexcessively-so-rise-machines?fsrc=rss>.\n\n5. Kohli, Sonali. \u201cBill Gates Joins Elon Musk and Stephen Hawking in Saying Artificial Intelligence Is Scary.\u201d Quartz. Quartz, 29 Jan. 2015. Web. 26 May 2015. <http://qz.com/335768/bill-gates-joins-elon-musk-and-stephen-hawking-in-saying-artificial-intelligence-is-scary/>.\n\n7. Rifkin, Jeremy. The Empathic Civilization: The Race to Global Consciousness in a World in Crisis. New York: J.P. Tarcher/Penguin, 2009. Print.\n\n10. Pisani, Bob. \u201c3 Years after \u2018Jeopardy,\u2019 IBM\u2019s Watson Gets Serious.\u201d CNBC. CNBC, 8 Oct. 2014. Web. 26 May 2015. <http://www.cnbc.com/id/102069981>.\n\n11. \u201cThe Dawn of Artificial Intelligence.\u201d The Economist. The Economist Newspaper, 9 May 2015. Web. 26 May 2015. <http://www.economist.com/news/leaders/21650543-powerful-computers-will-reshape-humanitys-future-how-ensure-promise-outweighs>.\n\n12. Marcus, Gary. \u201cThe Trouble With Brain Science.\u201d The New York Times. The New York Times, 11 July 2014. Web. 26 May 2015. <http://www.nytimes.com/2014/07/12/opinion/the-trouble-with-brain-science.html?_r=0>.\n\n13. Danaylov, Nikola. \u201cNoam Chomsky on Singularity 1 on 1: The Singularity Is Science Fiction!\u201d Singularity Weblog. Singularity Weblog, 4 Oct. 2013. Web. 26 May 2015. <https://www.singularityweblog.com/noam-chomsky-the-singularity-is-science-fiction/>.\n\n15. Kaebler, Jason. \u201cElon Musk: Burning Fossil Fuels Is the \u2018Dumbest Experiment in History, By Far\u2019\u201d Motherboard. Motherboard, 26 Mar. 2015. Web. 26 May 2015. <http://motherboard.vice.com/read/elon-musk-burning-fossil-fuels-is-the-dumbest-experiment-in-history-by-far>.\n\n16. Ford, Martin. Rise of the Robots: Technology and the Threat of a Jobless Future. New York, 2015. Print.\n\n17. Wing Kosner, Anthony. \u201cWhat Really Scares Tech Leaders About Artificial Intelligence?\u201d Forbes. Forbes Magazine, 20 Apr. 2015. Web. 26 May 2015. <http://www.forbes.com/sites/anthonykosner/2015/04/20/what-really-scares-tech-leaders-about-artificial-intelligence/>.\n\n18. Moore, Trent. \u201cCheck out This New Ex Machina Trailer to Go along with the Expanded Release.\u201d Blastr. Blastr, 22 Apr. 2015. Web. 26 May 2015. <http://www.blastr.com/2015-4-22/check-out-new-ex-machina-trailer-go-along-expanded-release>.\n\n19. Angelica, Amara D. \u201cKurzweilAI | Accelerating Intelligence.\u201d KurzweilAI How Watson Works a Conversation with Eric Brown IBM Research Manager Comments. The Kurzweil Accelerating Intelligence Newsletter, 31 Jan. 2011. Web. 26 May 2015. <http://www.kurzweilai.net/how-watson-works-a-conversation-with-eric-brown-ibm-research-manager>.", 
        "title": "Concerns of Artificial Intelligence \u2013 Christine Calo \u2013"
    }, 
    {
        "url": "https://medium.com/@semanticmd/conversational-ui-for-medical-image-analysis-cb0cc1e7b96?source=tag_archive---------4----------------", 
        "text": "There are many companies providing machine learning solutions in healthcare. Some older companies like Microsoft and IBM, but also newer startups such as Enlitic, Lumiata, and Metamind. While these companies may use the latest technologies such as deep learning (i.e. caffe) and graph databases, the expert systems are still designed to instruct a doctor to see patterns learned by the expert system.\n\nReal medical data analysis is messy and it\u2019s rarely the case that a physician has a set of numerical and categorical features and they are just waiting for that perfect solution to map features to a clinical outcome. Physicians need to be part of the process. Regardless of the accuracy on a static dataset, technology does not replace a doctor\u2019s experience and intuition working with a local population.\n\nIn addition to the difficulty of accurate medical diagnosis, no physician wants to be reduced to a trivial decision tree. Computer-aided diagnosis systems in mammography are more than 40 years old (Winsberg 1967), but 87% of radiologists still don\u2019t believe computer-aided diagnosis improves their level of care. Nobody wants a computer correcting their work and especially not an expert system that is deemed to be more accurate.\n\nWhat is needed is simply more conversation. Conversation between the expert systems and physicians facilitated by interactive interfaces. Machine learning in healthcare needs to go beyond just making predictions and provide recommendations and incorporate feedback from the users. This will require a fundamental shift in thinking from developing expert systems that instruct and correct physicians to expert assistants that can have a conversation with the user.", 
        "title": "Conversational UI for Radiologists \u2013 SemanticMD \u2013"
    }
]