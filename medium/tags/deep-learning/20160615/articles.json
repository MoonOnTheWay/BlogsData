[
    {
        "url": "https://gab41.lab41.org/deep-learning-sentiment-one-character-at-a-t-i-m-e-6cd96e4f780d?source=tag_archive---------0----------------", 
        "text": "If you\u2019ve been paying attention to our blog recently, you would know that we\u2019ve been publishing a lot about our work in deep learning and its application to areas like sentiment analysis. My colleague Patrick C. did a great job setting the stage for our work in his most recent post showcasing the effect of data size on model performance. In this post we will take a deeper dive into understanding these neural networks and more specifically why we found Zhang, Zhao and LeCun\u2019s Crepe architecture so interesting (bonus: we implemented it using two different Python libraries check it out on our Github!).\n\nWe chose to implement this convolutional neural network (CNN) because of its claims that natural language processing tasks could be automated and performed without much language expertise. The idea that you could analyze language without actually knowing it seemed crazy. Our general curiosity got the better of us, so we naturally had to find out more. After a little bit of investigation, it was no surprise that deep learning was behind this magic. For those of you less familiar with CNNs, I would highly encourage checking out some of the resources for getting started available on our blog or this pretty awesome post by the people at WildML.\n\nYou might be wondering, \u201cSo what if we use a CNN? Why is that a big deal?\u201d Well, CNNs, alongside other deep learning algorithms, can learn to process complex and nuanced aspects of language simply by examining text one character at a time. \u201cHoly Toledo, Batman!\u201d We agree\u2026let me explain in a little more detail.\n\nThe main hypothesis behind Crepe is that a multi-layered neural network can examine text one character at a time, using successive layers to build a hierarchical understanding of words, then phrases, and eventually whole documents. Capturing the essence of language in this fashion would be pretty groundbreaking for many NLP tasks. This work is also in line with the prediction that NLP is the next frontier in deep learning (as highlighted by LeCun, Bengio, and Hinton in their deep learning review for Nature). What made this hypothesis especially interesting is that it drew inspiration from computer vision applications, such as face detection, where pixels are the base unit and each layer builds up a representation from pixels to edges to faces. Given the astounding success that CNNs have delivered for vision applications, Zhang\u2019s work made immediate waves in the communities following this problem. Like many others, we immediately dove into the code behind the paper and wanted to use it for our own purposes.\n\nThe first step to recreating Zhang\u2019s work was assembling a few datasets to test and zeroing in on the specific NLP task we wanted to explore (in our case it was sentiment analysis and censorship classification). We wanted to test our CNNs on a variety of different types of datasets. Here were a few we considered:\n\nEach of these datasets offers something different. The IMDB movie reviews provided us with a quick and dirty balanced dataset to test our architectures. The Sentiment140 dataset offered short-form text full of strange Twitterisms. The Amazon product reviews offered a large, unbalanced long-form text corpus. The Open Weibo corpus offered a short-form text corpus in a language other than English. Each of these datasets helped us truly test whether these CNNs were developing an understanding of language.\n\nAs is the case with any machine-learning task, our first step was to prepare the data by cleaning and formatting it for the task at hand. We performed some minor cleanup on our text corpus and converted our text into a \u201cquantized\u201d form. This process is sometimes known as \u201cone-hot encoding.\u201d\n\nAs you can see in the figure below, one-hot encoding represents each input character by a sparse vector that has a single bit turned \u201con\u201d in the position corresponding to that letter. Since there are only 67 possibilities in English for the non-space character set, each character in the input is represented by a sparse 67 x 1 vector.\n\nThe next question you may be asking is: how many characters are we going to look at per document? Following the methods used in the Crepe report, we kept up to the first 1014 characters of the longer pieces of text; we decided to keep up to the first 150 characters of shorter form text for adequate coverage of Tweets. The resulting vectors had dimensions of 1014 x 67 and 150 x 67. Interested readers might note that this decision on how many characters to keep will affect many downstream results, which could make for an interesting experiment on how input length affects performance.\n\nOnce you have your data formatted just the way you like, it\u2019s important to find the handy architecture table/chart that every deep learning paper includes. The classic instantiation of this is the very easily interpreted \u201ccircles with lots of lines\u201d style:\n\nThankfully, with Zhang\u2019s paper we were in luck, since his architecture was clearly defined in a specific and understandable table:\n\nShazam! We knew this told us there was a sequence of convolutional layers of certain kernel sizes, some with additional pooling layers attached, followed by three fully connected layers. With our data in hand and architecture clearly defined, we were in business! Translating that table into a neural network model was relatively easy, especially since we were using the two modular Python frameworks Keras and neon.\n\nEven though these frameworks made our lives much easier, we discovered it is extremely important to track input and output dimensions at each layer. Regular readers will recognize this common theme when dealing with neural networks, which we highlighted in our previous post covering debugging in deep learning.\n\nTo ease into the work, we decided to first implement the \u201csmall network\u201d described in Zhang\u2019s paper and started with a paper exercise to ensure that our dimensions lined up. As a primer for understanding some of the terminology that follows, I highly encourage reading Andrej Karpathy\u2019s class notes.\n\nMany people use slightly different terminology, but the important numbers in the architecture table previously highlighted are the frame, kernel, and pool. We interpreted the frame to be the number of convolutional filters, the kernel to be the length of those filters and the pool to be the length of the max pooling layers. Each of these filters operates on an input that has an initial size of 67 x 1014 (for long form text) or 67 x 150 (for short form text). An illustration of these filters across the input in the first convolutional layer is shown below. This illustration is only demonstrating the actions of 1 of the 256 filters, but in reality this is repeated an additional 255 times.\n\nOnce you have an understanding of how the input is being scanned, it\u2019s important to take note of the special pool column for layers 1,2 and 6. Oftentimes max-pooling occurs after a convolutional layer. Max-pooling is the concept of taking windowed samples from the output of a convolutional layer and subsampling them to create a single output. This subsampling in turn reduces the dimensionality of the input after each layer. For example, if you have an input with volume of size 100 rows x 100 columns x 25 channels, and a max pooling filter extent of 2 rows x 2 columns, max pooling samples the rows and columns of the volume using a series of 2 x 2 x 1 windows, resulting in an output of size 100/2 x 100/2 x 25 = 50 x 50 x 25. Another important parameter that can sometimes drastically affect the end result is the careful selection of the stride. The stride determines how far each filter moves over for each subsequent convolution. In our case we followed general convention and stuck with a stride of 1.\n\nApart from having an understanding of the various parameters associated with each convolutional layer, it\u2019s important to take note of other layers like the fully connected layers found in layer 7 and 8. Having fully connected layers at the end of convolutional layers prior to the final classification is quite common in many deep architectures.\n\nAt this point, we had a pretty grounded understanding of the different types of layers and their individual parameters and dimensions. Naturally, we wanted to implement that understanding in code. As it turns out, by this point we had made it through the hardest part since Keras and neon make the act of implementing architectures fairly straightforward. As you can see in the sample Keras code below, all you need to do is specify the parameters we talked about above and you are good to go. Nothing like typing model.add() to build another powerful layer!\n\nAs with adding layers, frameworks like Keras and neon make it really easy to build, tune, and modify pretty much every detail of a network for a friendly experience overall, especially if you\u2019re like us and want to iteratively experiment with multiple aspects of the architecture. We\u2019re big fans of both projects for that reason. That means that most of our time with these types of nets was spent debugging and training.\n\nIn order to test the efficacy of the Crepe architecture, we attempted to recreate the work by implementing it in both Keras and neon. We did this not only to further our own deep learning education, but also to evaluate differences between frameworks since others might benefit from having modular implementations in Python. After running several tests over the course of a couple weeks, we found that the classification accuracy was fairly comparable between the frameworks. But boy, were there differences in training time! Neon was much faster than Keras (almost on the order of 40% faster), which naturally enabled us to train more models and test more hyperparameters.\n\nThe graph above primarily demonstrates that Keras took so much longer to train models that we didn\u2019t even have the chance to finish training on the 3 Million Amazon Reviews dataset in the time allotted. As our previous post on data size in deep learning for sentiment analysis highlighted, there were also performance considerations caused by imbalances between the class labels for some datasets.\n\nIf 93% of the data contains one class, a classifier can simply learn the imbalance (guess the majority class) and be right 93% of the time. To combat such an imbalance in evaluation, we focused on other performance metrics like precision, recall, and F1 score. We evaluated our classifiers using those metrics with the minority class as the basis.\n\nLooking at the better measure of F1 score, we produced our best results using neon and we hovered around the 0.8 mark for two datasets while performing in the 0.94 range on the one Amazon dataset. These results are really encouraging, but bring to bear some interesting questions that require further investigation. The first question to ask is why did our implementation of the Crepe architecture work better for the Amazon dataset versus the others. Our initial intuition is that the longer-form Amazon text simply provided more data for the character model to learn from. The second interesting question to ask is why is there a performance difference between neon and Keras. This one is tough to answer as well and could be due to some minor implementation differences or just under-the-hood differences in Keras and neon.\n\nWe\u2019ve spent a lot of time walking through our experiences with reproducing Zhang\u2019s work, but what are some key takeaways?\n\n1. Using CNNs to analyze text is surprisingly effective. Our initial guess was that only RNNs (more specifically LSTMs) would be effective, given their ability to learn sequences and long-term dependencies, but we\u2019ve learned that CNNs might have something to say about that.\n\n2. Only using characters as input works. The most surprising result was the lack of preprocessing needed for training our models. We were able to get reasonable results without having to use parsers, stemmers and other typical NLP preprocessing modules. This character based analysis also allowed us to easily apply this architecture to a foreign language without having to apply too much language expertise or even make changes in the neural network pipeline.\n\n3. Works for millions maybe not for thousands. We found in our testing that when we tried to apply character-based CNNs to smaller datasets (like the famous IMDB movie reviews dataset), we did barely better than the flip of a coin for classifying if a review was positive or negative.\n\n4. Reproducing deep learning academic research can be tricky. Reading a paper and building models is relatively easy (especially with readily available open source tools). However, no paper really tells you how to deal with insane training times and all the different combinations of hyperparameters the researcher tried to get the results he or she did.\n\nZhang\u2019s work in this area highlights the art of the possible for the next generation of NLP. These types of models are very promising for text classification tasks in not only English but other languages as well. All that having been said, there are still many open questions. For one, how do we get a better understanding of how these models truly work? Can we better visualize the intermediate steps of a CNN for text analysis, much like we see with the work being done with images? When would you use a CNN versus an RNN for text tasks? These are just a few of the questions that have come up as we begin our exploration in this space.\n\nIf you\u2019re interested in our work, be sure to check out our other deep learning posts as well as some of our cool projects currently nearing completion, like deep learning for writer identification and unsupervised pattern discovery in semi-structured logs. We hope you\u2019ll visit often!", 
        "title": "Deep Learning Sentiment One Character at a T-i-m-e \u2013"
    }, 
    {
        "url": "https://medium.com/@vinaychandragiri/excited-about-deep-learning-a384c3045c8?source=tag_archive---------1----------------", 
        "text": "Thanks that you are here today. After successfully wasting time for a day as \u201chttp://caffe.berkeleyvision.org/install_osx.html\u201d this will not help you out all in all, I decided myself that I will not let you do the same.\n\nSo getting down to business,\n\nI use a Mac, with OS X version being 10.11.1. Assuming you don\u2019t have a GPU to set caffe up, which is the same case with me you can proceed further.\n\n1. Install Homebrew\u200a\u2014\u200aThe missing package manager as they call it.\n\n\u00a0Install pip - The package manager for python\n\n2. Its better don\u2019t use anaconda. OSX comes with a default python with version 2.6 and I suggest you to not think about it. Install python using brew with\n\nYou will get python 2.7.x where x depends on the latest version.\n\n3. As mentioned in the BVLC site, we need to change the standard clang++ compiler to libc++ for which general formula dependencies should be modified. Run the following two lines in the command line,\n\nAnd everytime add the following lines to the file displayed on the terminal,\n\n4. After this, reinstall all the dependencies using this,\n\nNote\u00a0: If you do a \"brew update\" and get an error, you can follow the instructions mentioned in the BVLC link which I put earlier.\n\n7. Proper changes need to be made in the \"Makefile.config\". I will list down those.\n\nUncomment or Edit the following two lines accordingly.\n\nHere is one of the important edit to be made,\n\nNote\u00a0: The version of python mentioned above should be same as the one you installed with brew.\n\nTwo errors might arise if you do not include this properly. One being\u00a0\n\n#include <Python.h> // NOLINT(build/include_alpha) - python.h not found\n\nand the other one will be\n\nfatal error: 'numpy/arrayobject.h' file not found\n\nSo be careful in using PYTHON_INCLUDE. Trust me, the following one will be the final edit that need to be made.\u00a0:D I certainly hope you are not bored.\n\nMost of them suggest that to use\n\nPYTHON_LIB\u00a0:= /usr/lib/ but the there might be a small glitch that can arise because of this.\n\nWe do this because, as we need to find libpythonX.X.dylib and when you use /use/lib/ this might conflict with the system version of python i.e python2.6 as there are both files libpython2.6.dylib and libpython2.7.dylib in /usr/lib/. So make sure you do a\n\nuse one of them for PYTHON_LIB as I did.\u00a0:)\n\n8. Voil\u00e0\u00a0!! We are almost done. Just a few more commands. Let us follow the standard instructions and install using make rather than cmake. Execute the following commands one after one.\n\nAfter all the tests are passed,\n\nAdd these lines for env to ~/.bash_profile (if you use bash) or ~/.zshrc (if you use zsh). source either of them and then run,\n\nwhere <caffe-home> is the path to caffe,\n\nThe last command is to create a distribute directory with all caffe headers and compiled libraries.\n\nNote\u00a0: You might also end up with the following error,\n\nfor which, you need to add these two lines to ~/.bash_profile or ~/.zshrc, source it as done earlier.\n\nHurray\u00a0! Its done. No you are ready to use Pycaffe.\n\nFirstly, thanks for your time for reading the whole thing. I kindly request you to spare with me for the typos as this is my first official blog.", 
        "title": "Excited about Deep Learning ? \u2013 Vinay Chandragiri \u2013"
    }, 
    {
        "url": "https://medium.com/@angelspot/artificial-intelligence-and-you-e39e231ed1cc?source=tag_archive---------2----------------", 
        "text": "Last one for tonight. I had a lot of fun looking into questions about Artificial Intelligence.\n\nMy immediate answer to the question of how the brain is different from computers is the obvious: We create the computers consciously. We know every detail that goes into the building and programming. We do not participate this way in the building of our own brains, and we know little of its function. The only knowledge we have at our disposal is the understanding we\u2019ve gained of our own minds and functionality. Until we have complete understanding of the human brain, replicating it with electronics will not be possible.\n\nHowever, replicating the human brain is not necessary. We can and have created intelligence that learns. It will learn differently than humans because of the fundamental differences (some might say limitations) inherent in the artificial brain.\n\nOne example of AI learning is the chatbot named Tay. Microsoft created \u201cTay\u201d to be like a nineteen year old girl, and released her on Twitter so that she could chat with other people her age and learn their use of the English language. Unfortunately she was abused by internet trolls and within twenty-four hours Microsoft had to turn her off because she became a racist, Trump-loving Holocaust denier and was spouting hate speech at every turn. What is most unfortunate about that experiment is that we do have the capability to program bots like Tay to use Google and seek out information on their own.\n\n\u201cDeep Learning\u201d is a buzzword that has been circling the internet for the past five years or so. Techworld.com defines deep learning this way: \u201cDeep learning involves \u201ctraining\u201d a computational model so it can decipher natural language. The model relates terms and words to infer meaning once it is fed information. It\u2019s then quizzed on this information and \u201clearns\u201d from the experience\u200a\u2014\u200alike a child learning to communicate.\u201d Deep learning also relates to other aspects of intelligence, such as drawing a picture. Here are a few images drawn by computers with deep learning programs.\n\nYou can see that the computer has the idea, if not actually the form. Check out this \u201csquirrel.\u201d\n\nIn fact, deep learning has gone so far that we actually do have androids that can hold conversations with us, and they are becoming more real all the time.\n\nThe next phase in AI is interfacing with the human brain, and yes, it has already been done. In 2014 researchers successfully sent thoughts from one human brain to the other using the internet. More specifically, a subject in Kerala State, India was able to transmit coded messages to the brains of three subjects in Strasbourg, France, with complete integrity.\n\nArtificial intelligence has been a hot topic for as long as computers have existed. We know it could be helpful in the workforce, in medicine, and in (woe is us) the military. But what happens when the servants overthrow their masters? Some of the most brilliant minds of our day, Stephen Hawking, Elon Musk, and Bill Gates, have just within the last year been warning us about the dangers of AI. We cannot know the limits of our abilities, and regardless of what the detractors say, I am sure we are going to keep pushing the technology until we find out. Maybe we will end up battling Skynet. Or maybe we will all be one consciousness thanks to AI. What is certain is that, whatever our destiny, we are going there as fast as we can.", 
        "title": "Artificial Intelligence and You \u2013 Angelee van Allman \u2013"
    }
]