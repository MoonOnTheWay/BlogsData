[
    {
        "url": "https://towardsdatascience.com/deep-learning-d5fe55326e57?source=tag_archive---------0----------------", 
        "text": "I am starting a series of blog explaining concept of Machine Learning and Deep Learning or can say will provide short notes from following books. For this purpose I would be following few books namely:\n\n3. The Elements of Statistical Learning: By\u200a\u2014\u200aTrevor Hastie, Robert Tibshirani and Jerome Fried\n\nToday, Arti\ufb01cial intelligence(AI) is a thriving \ufb01eld with many practical applications and active research topics. The true challenge to arti\ufb01cial intelligence is to solve problems that human solve intuitively and by observing things like spoken accent and faces in an image.\n\nThe solution to the above problem is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept de\ufb01ned in terms of its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all of the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones. If we draw a graph showing how these concepts are built on top of each other, the graph is deep, with many layers. For this reason, we call this approach to AI deep learning.\n\nSeveral arti\ufb01cial intelligence projects have sought to hard-code knowledge about the world in formal languages. A computer can reason about statements in these formal languages automatically using logical inference rules. This is known as the knowledge base approach to arti\ufb01cial intelligence. None of these projects has led to a major success. One of the most famous such projects is Cyc (Lenat and Guha, 1989).\n\nNow, this is not always possible to hard-core each feature in our machine. So the ability to acquire their own knowledge is necessary, can be gained by extracting patterns from raw data. This capability is known as machine learning. The performance of the simple machine learning algorithms depends heavily on the representation of the data they are given. Each piece of information included in the representation of our desired problem is known as a feature (Fig 1).\n\nImportance of features is very crucial, for instance take an example of human beings, we can easily perform arithmetic on Arabic numbers, but doing arithmetic on Roman numerals is much more time-consuming. It is not surprising that the choice of representation has an enormous e\ufb00ect on the performance of machine learning algorithms.\n\nNow to solve this problem we can use machine learning not only to discover mapping from representation to output but also representation itself. This is called as representation learning. Representation learning, i.e., learning representations of the data that make it easier to extract useful information when building classifiers or other predictors. In the case of probabilistic models, a good representation is often one that captures the posterior distribution of the underlying explanatory factors for the observed input(We will revisit this topic later in greater detail). Talking about representational learning the autoencoder are good example. An autoencoder is the combination of an encoder function that converts the input data into a di\ufb00erent representation, and a decoder function that converts the new representation back into the original format.\n\nOf course, it can be very di\ufb03cult to extract such high-level, abstract features from raw data. Many of representations, such as a speaker\u2019s accent, can be identi\ufb01ed only using sophisticated, nearly human-level understanding of the data. It is nearly as di\ufb03cult to obtain a representation as to solve the original problem, representation learning does not, at \ufb01rst glance, seem to help us.\n\nDeep learning solves this central problem in representation learning by introducing representations that are expressed in terms of other, simpler representations. Deep learning allows the computer to build complex concepts out of simpler concepts(Fig 3). There are two main ways of measuring the depth of a model (Fig 4).\n\nIt is not always clear which of these two views\u200a\u2014\u200athe depth of the computational graph, or the depth of the probabilistic modeling graph\u200a\u2014\u200ais most relevant, and because di\ufb00erent people choose di\ufb00erent sets of smallest elements from which to construct their graphs, there is no single correct value for the depth of an architecture, just as there is no single correct value for the length of a computer program. Nor is there a consensus about how much depth a model requires to qualify as \u201cdeep.\u201d\n\nComing on to the division of various type of learning Fig 5 will give you a great idea about the difference and similarity between them.\n\nThe earliest predecessors of modern deep learning were simple linear models. These models were designed to take a set of n input values x1,\u00a0.\u00a0.\u00a0.\u00a0, xn and associate them with an output y.These models would learn a set of weights w1,\u00a0.\u00a0.\u00a0.\u00a0, wn and compute their output f(x, w) =x1*w1+\u00b7\u00b7\u00b7+xn*wn. This \ufb01rst wave of neural networks research was known as cybernetics. In the 1950s, the perceptron (Rosenblatt, 1958, 1962) became the \ufb01rst model that could learn the weights de\ufb01ning the categories given examples of inputs from each category. The adaptive linear element(ADALINE), which dates from about the same time, simply returned the value of f(x) itself to predict a real number (Widrow and Ho\ufb00, 1960), and could also learn to predict these numbers from data.\n\nModels based on the f(x, w) used by the perceptron and ADALINE are called linear models. Linear models have many limitations. Most famously, they cannot learn theXOR function, where f([0,1], w) = 1 and f([1,0], w) = 1 but f([1,1], w) = 0 and f([0,0], w) = 0(Fig 7).\n\nThis limitation of linear model has lead to more sophisticated techniques. Today deep learning is booming at a much higher rate than any before because of the following possible reasons:\n\nPlease provide your feedbacks, so that I can improve in further articles.", 
        "title": "DEEP LEARNING \u2013"
    }, 
    {
        "url": "https://medium.com/@YvesMulkers/10-reasons-to-be-excited-about-data-analytics-in-2017-64a929c0c8?source=tag_archive---------2----------------", 
        "text": "10 reasons to be excited about data analytics in 2017\n\n\u00a0Follow me on Google+\u00a0, LinkedIn\u00a0, Twitter\n\n\u00a0We live in an exciting time of accelerated innovation and heightened global competition, full of unprecedented levels of opportunity for analytics professionals. The exponential growth of data and intelligent things in an environment of ubiquitous Internet connectivity is enabling a fourth industrial revolution\u200a\u2014\u200adigital business transformation.\n\n\u00a0Information is vital to evolving digital businesses because analytics touches everyone. Indeed, in the modern business environment every business is in the business of analytics. What, then, should we expect to see in the analytics industry as we head into 2017? Here are my top 10 predictions:\u00a0\n\n\u00a0Cognitive computing technologies such as IBM Watson progress from successful early adoption into early majority. In 2016, we saw cognitive, deep learning and natural language technologies take on increasingly prominent roles in businesses while capturing news headlines around the world. In 2017, this strategic analytics technology will become a must-have in analytics roadmaps.\n\n\u00a0\n\n\u00a0Embedded analytics continues to retain the top spot in self-service analytics technology growth. The years leading up to 2017 have seen an analytics-based cultural shift toward data-driven decision making. Accordingly, we have seen a growing need for bringing analytics closer to the user\u200a\u2014\u200ain the app, when and where decisions are made. In 2017, we will see cognitive, predictive and prescriptive analytics increasingly being embedded into line-of-business apps.\n\n\u00a0\n\n\u00a0Cloud and hybrid analytics continue growing in 2017, even in historically cloud-unfriendly markets. Infrastructure in our increasingly connected world is getting better and is almost invisible in mature markets. Cloud fears are expected to ease as more organizations witness the triumphs of early adopters. Compelling transfer of data security risks to modern cloud providers will bring executives peace of mind. Instant, easy cloud solutions continue to win the hearts and minds of users. Finally, the cloud will accelerate time to market, allowing innovation at faster speeds than ever before.\n\n\u00a0\n\n\u00a0Intelligent Internet of Things (IoT) use expands in 2017 from innovators to early adopters. Digital devices will become ever more pervasive, delivering the power of cognitive-enabled systems to homes and offices as well as throughout the external world.\n\n\u00a0\n\n\u00a0Streaming analytics and data ingestion technologies are adopted in conjunction with the Internet of Things to import, monitor and understand in real time what is happening. Predictive algorithms will detect exceptions and provide proactive alerts about notable changes in data patterns.", 
        "title": "10 reasons to be excited about data analytics in 2017"
    }, 
    {
        "url": "https://medium.com/the-data-intelligence-connection/10-reasons-to-be-excited-about-data-analytics-in-2017-3983c4ec0a20?source=tag_archive---------3----------------", 
        "text": "10 reasons to be excited about data analytics in 2017\n\n\u00a0Follow me on Google+\u00a0, LinkedIn\u00a0, Twitter\n\n\u00a0We live in an exciting time of accelerated innovation and heightened global competition, full of unprecedented levels of opportunity for analytics professionals. The exponential growth of data and intelligent things in an environment of ubiquitous Internet connectivity is enabling a fourth industrial revolution\u200a\u2014\u200adigital business transformation.\n\n\u00a0Information is vital to evolving digital businesses because analytics touches everyone. Indeed, in the modern business environment every business is in the business of analytics. What, then, should we expect to see in the analytics industry as we head into 2017? Here are my top 10 predictions:\u00a0\n\n\u00a0Cognitive computing technologies such as IBM Watson progress from successful early adoption into early majority. In 2016, we saw cognitive, deep learning and natural language technologies take on increasingly prominent roles in businesses while capturing news headlines around the world. In 2017, this strategic analytics technology will become a must-have in analytics roadmaps.\n\n\u00a0\n\n\u00a0Embedded analytics continues to retain the top spot in self-service analytics technology growth. The years leading up to 2017 have seen an analytics-based cultural shift toward data-driven decision making. Accordingly, we have seen a growing need for bringing analytics closer to the user\u200a\u2014\u200ain the app, when and where decisions are made. In 2017, we will see cognitive, predictive and prescriptive analytics increasingly being embedded into line-of-business apps.\n\n\u00a0\n\n\u00a0Cloud and hybrid analytics continue growing in 2017, even in historically cloud-unfriendly markets. Infrastructure in our increasingly connected world is getting better and is almost invisible in mature markets. Cloud fears are expected to ease as more organizations witness the triumphs of early adopters. Compelling transfer of data security risks to modern cloud providers will bring executives peace of mind. Instant, easy cloud solutions continue to win the hearts and minds of users. Finally, the cloud will accelerate time to market, allowing innovation at faster speeds than ever before.\n\n\u00a0\n\n\u00a0Intelligent Internet of Things (IoT) use expands in 2017 from innovators to early adopters. Digital devices will become ever more pervasive, delivering the power of cognitive-enabled systems to homes and offices as well as throughout the external world.\n\n\u00a0\n\n\u00a0Streaming analytics and data ingestion technologies are adopted in conjunction with the Internet of Things to import, monitor and understand in real time what is happening. Predictive algorithms will detect exceptions and provide proactive alerts about notable changes in data patterns.", 
        "title": "10 reasons to be excited about data analytics in 2017"
    }
]