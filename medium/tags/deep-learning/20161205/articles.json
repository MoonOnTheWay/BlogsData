[
    {
        "url": "https://hackernoon.com/traffic-signs-classification-with-deep-learning-b0cb03e23efb?source=tag_archive---------0----------------", 
        "text": "You can find my code here. It might be more up to date than the article\u00a0;).\n\nThis is the second article in the self driving cars series. If you want to know why I\u2019m sharing this and more about my journey, please read this.\n\nImagine you need to build a program that recognizes written digits.\n\nThis is a 5. But it could arguably be a 3.\n\nWhat cutoffs/rules, would you use to go from a 3 to a 5?\n\nInstead of trying to handpick all the rules and build a very complicated program, researchers have decided to show a computer thousands of examples and let it try to solve the problem by experience. This is the beginning of machine Learning.\n\nOne of the main problems with machine learning is feature extraction. Even though we show the computer thousands of examples, we still needed to tell him what features it should focus on. For complex problems, this was not good enough.\n\nDeep learning models circumvent that. They learn by themselves what features they should focus on.\n\nFor the sake of brevity, I\u2019m not going to dive in the mathematical explanations of how deep learning works. It took me around 20 hours to understand the concepts and use them. Instead, I\u2019ll try to explain the intuition behind deep learning. I\u2019ll post some videos and lectures I used if you want to go deeper. No pun intended.\n\nAs humans, recognizing an object seems like a pretty simple task. There is hardly any effort involved on our part, at least not consciously. But there is actually a lot of work done by our brain before we can really understand what we\u2019re looking at.\n\nIn the late 1950s, David Hubel and Torsten Wiesel, two famous neurophysiologists, made experiments on a cat to show how the neurons in the visual cortex work.\n\nFor one, they showed that nearby cells process information from nearby visual fields, forming a topographical map. Moreover, their work determined that neurons with similar functions are organized into columns, tiny computational machines that relay information to a higher region of the brain, where a visual image is progressively formed.\n\nThe brain basically combines low level features such as basic shapes, curves and builds more complex shapes out of it.\n\nA deep learning convolutional neural network is similar. It first identifies low level features and then learns to recognize and combines these features to learn more complicated patterns. These different levels of features come from different layers of the network.\n\nDeep Learning is a fascinating field and I hope I gave you a clear enough introduction. I encourage you to watch the wonderful Stanford class about the subject.\n\nIf you prefer reading, I\u2019d advise you Goodfellow, Bengio, and Courville\u2019s book.\n\nDetecting and Classifying Traffic signs is a mandatory problem to solve if we want self driving cars.\n\nThe dataset we will be using is a German Traffic sign dataset available online.\n\nIt contains more than 50,000 images in total, divided into 43 different classes: speed limits, dangerous curves, slippery road\u2026 Here are some of them.", 
        "title": "Traffic signs classification with Deep Learning. \u2013"
    }, 
    {
        "url": "https://blog.hutoma.ai/hu-toma-bot-building-101-just-add-txt-bd03d757d2f9?source=tag_archive---------1----------------", 
        "text": "Building a \u201cchatbot\u201d is currently the on-trend thing to do, but why? Corporate innovation teams, SME\u2019s and freelancers alike see the opportunity in adding conversational functionality to their web & mobile applications.\n\nWe\u2019re fast becoming a culture that spends a lot of time in chat applications, and unsurprisingly, businesses want to adapt and take advantage of this. Mark Zuckerberg said in 2014 that \u201cmessaging is one of the few things that people do more than social networking.\u201d According to SimilarWeb, for the average american, over 23 minutes a day are spent in chat applications.\n\nSlack, the largest workplace chat application, was built with bots at its core. Artificially intelligent (AI) bots have the power to improve user experience, increase user productivity, and customer engagement by delivering contextual information.\n\nThis trend has given rise to numerous companies creating chatbots, digital assistants, and conversational user interfaces. These chatbots can do anything from scheduling meetings like x.ai to giving you the weather forecast like Poncho. We at hu:toma have created a platform for 3rd parties to build these domain specific chat bots and distribute them.\n\nCreating an AI is as simple as uploading a text training file. It\u2019s that easy!\n\nI\u2019ve decided to create a Police Bot using some frequently asked questions from the Metropolitan Police\u2019s website.\n\nTo set up your chatbot, add your training file to a Text Editor and save. Make sure the questions and answer span only one line each.\n\nThen upload the training file and let hu:toma\u2019s deep learning technology based on Recurring Neural Networks (RNN) do all the work. There is no need to manually create conversation trees, which can quickly become incredibly complex for more than a limited conversation.\n\nYou can choose how often the AI can create new answers that fall outside of your data set / training file, and can also choose whether the AI will learn from its users or not.\n\nOnce Phase 1 of the training is complete, you will be able to chat with your AI in the platform chat window. As you chat, you can also look at the JSON file that is created if you wish to test. Phase 2 of the training is the \u201cdeep learning in action.\u201d This allows the AI to create new answers based on the training file. We\u2019re busy building integrations for all the major chat platforms, so you can be assured that you\u2019ll be able to communicate with all of your end users.\n\nAND hu:toma sets itself further apart by allowing users to publish to the Bot Store, a marketplace that allows developers to monetise their creations, increase their proliferation and drive sales through integrations with 3rd party applications.\n\nThe hu:toma Bot Store allows you to combine multiple AI\u2019s that are trained with data from domain experts to create your ideal user experience. Imagine allowing your users to order coffee from Starbucks Coffee, get it delivered by Stuart, receive their favourite content daily from Medium, and book a spot on their Spinning Class at Pure Gym, all from within a single hu:toma powered chatbot.\n\nBe first to use the product by joining our waitlist.", 
        "title": "hu:toma Bot Building 101 ( Just add .txt ) \u2013"
    }, 
    {
        "url": "https://machinelearnings.co/machine-learnings-20-ai-bots-need-to-be-funny-or-people-get-pissed-7c935428fa3a?source=tag_archive---------2----------------", 
        "text": "#Awesome \n\n\u201cAn AI physician\u2019s assistant could plug into a universally accessible electronic health record that keeps all your information\u200a\u2014\u200across referencing your symptoms and medical history with the most up-to-date recommendations to guide treatment choice. It could also alert your doctor about new research that could be of interest.\u201d\u200a\u2014\u200aBahar Gholipour, Science Journalist. Learn More on WIRED >\n\n#Not Awesome\n\n\u201cThe rise of AI is likely to extend job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining. This in turn will accelerate the already widening economic inequality around the world.\u201d\u200a\u2014\u200aStephen Hawking, Theoretical Physicist. Learn More on The Guardian >\n\n1/ Companies are hiring comedy writers to make their chatbots funny so users don\u2019t get pissed when the bot doesn\u2019t understand them. Learn More on Fast Co.Design >\n\n2/ Facebook\u2019s AI guru admits they have (or can build) the tech to filter out fake news, but are worried about the implications of censorship. Learn More on The Wall Street Journal >\n\n3/ Senator Ted Cruz organized a group of top AI experts to discuss the future of artificial intelligence and how it will impact political policy and the economy. Learn More on U.S. Senate Committee on Commerce, Science, & Transportation >\n\n4/ Apple finally admits that it\u2019s investing heavily in AI and building autonomous vehicles. Learn More on The Wall Street Journal >\n\n5/ Major economists worry that AI quickly taking jobs from people can lead to long periods of time \u201cwith a large fraction of people not working.\u201d Learn More on Newsweek >\n\n6/ A survey of 60 Asia-based business executives sheds light on the effect AI will have on Asia\u2019s business landscape. Learn More on MIT Technology Review >\n\n7/ Google researchers built an AI that reads retinas and identifies a popular cause of blindness. Learn More on WIRED >\n\nI\u2019m a fan of machine learning in almost all forms, but I get especially pumped when I learn of ways it can protect me from physical harm.\n\nNexar is building an AI dashcam app that uses machine learning to detect dangerous events while you\u2019re driving and give you \u201clife-saving warnings.\u201d Check it out. \ud83d\ude98\ud83d\udca5\ud83d\udce3\ud83d\ude97\ud83d\udca8\n\n\u201cSupposing AI powered systems will replace most of today\u2019s human jobs, how will this transition be? Which jobs will go first and which ones will rely the most on human abilities?\u201d\n\nThe most interesting AI-related conversation I saw on the internet this weekend broke out in response to the questions above. Check it out and leave a comment if you feel inspired!\ud83d\udc77\u231b\ufe0f\ud83e\udd16\u2692\ud83d\ude13\n\nMachine Learning and AI are having a huge impact on our lives. Our mission is to create space for discussion and learning that helps you become prepared to handle these changes as they happen.", 
        "title": "AI bots need to be funny or people get pissed \u2014 #20"
    }, 
    {
        "url": "https://medium.com/health-ai/artificial-intelligence-in-health-care-weekly-roundup-4-88148f95c346?source=tag_archive---------3----------------", 
        "text": "\u201cHuman beings, viewed as behaving systems, are quite simple. The apparent complexity of our behavior over time is largely a reflection of the complexity of the environment in which we find ourselves.\u201d\u00a0\n\n\u2015 Herbert A. Simon, The Sciences of the Artificial\n\nEver wondered how the skill of one surgeon compares to another? Truth is, surgical skills are rarely compared. Evaluating a surgeons skill has predominantly been a subjective task. As robotic surgery becomes more common it is possible to track and evaluate movements associated with each step of a surgery. In this paper researchers used machine learning to classify different movements made by surgeons to evaluate: completion time, path length, depth perception, speed, smoothness and curvature. The study showed the ability of machine learning methods to automatically classify expert and novice surgeons.\n\nRead more: Machine Learning Approach for Skill Evaluation in Robotic-Assisted Surgery\n\nWhat if you could know the genetic make up of a tumor without ever performing a biopsy? These researchers are using deep learning to predict the genetic makeup of brain tumors based on subtle differences in brain MRI that the human eye cannot detect. Predicting genetic status non-invasively from MR images allows doctors to select effective treatment strategies for patients with brain cancer without the need for surgical biopsy.\n\nRead More: Predicting 1p19q Chromosomal Deletion of Low-Grade Gliomas from MR Images using Deep Learning\n\nThe role of radiologists in the age of Artificial Intelligence is rapidly changing. In this paper, the authors discuss how AI should rather be regarded as a form of intelligence amplification (IA) for radiologists, a technique enabling them to add value to the radiology report. In other words, AI could be used to consolidate the radiologists\u2019 role instead of replacing them.\n\nRead More: The Impact of Information Technology on Radiology Services: An Overview", 
        "title": "Artificial Intelligence in Health Care Weekly Roundup #4"
    }, 
    {
        "url": "https://medium.com/@rongou/what-you-are-describing-is-not-really-embedding-e42459a695a7?source=tag_archive---------4----------------", 
        "text": "What you are describing is not really embedding. At least in the deep learning context, embedding usually refers to the (trainable) step to convert high-dimensional but sparse input data, which would otherwise be one-hot encoded, into lower-dimensional but dense distributed representation. This works for words, but also categorical features if you have lots of categories.\n\nSince your input data are already dense, you can feed them directly into a neural network. What you are calling \u201cembedding\u201d is really just a fully connected layer. People have tried something similar, mainly to reduce computational complexity. This paper (https://arxiv.org/abs/1402.1128) talks about \u201cprojection\u201d layers after the LSTM layer. A projection layer is a fully connected layer without the activation.\n\nOne thing I don\u2019t understand is how you can predict short-term VIX changes using historical daily summaries. Don\u2019t you need intra-day data?", 
        "title": "What you are describing is not really embedding. \u2013 Rong Ou \u2013"
    }, 
    {
        "url": "https://medium.com/@k2DV/bots-bots-bots-5c9a8ffd58d8?source=tag_archive---------5----------------", 
        "text": "\u201eI\u2019m sorry Dave, I\u2019m afraid I can\u2019t do that\u201d\u200a\u2014\u200athis famous quote from 2001: Space Odyssey shows the fear that one day our computers would not only talk to us\u200a\u2014\u200athey will even be intelligent enough to stop obeying our orders. How far from truth is this?\n\nBots are programs that can communicate via \u201enatural language\u201d with users. How different is it from standard communication with any software? Well, the difference is significant: you do not need to know any commands, menus, press F1 for help or read manuals. You can simply type or say what you want via so called \u201cconversational interface\u201d.\n\nThis means that bots can make use of any technology available for any user\u200a\u2014\u200abe it; IT specialist, kid or elderly person. The only thing user needs to know is what she / he wants to achieve. Take a look at what Google wants us to do with their search: some years ago asking a specific questions required learning tricks like putting \u201c-\u201d before a word or phrase that you wanted to exclude from search results or entering exact phrases in quotation marks. Now the easiest way is just typing a simple question\u200a\u2014\u200ajust like you would ask your colleague via e-mail. Or take a look at Amazon\u2019s Echo device with Alexa assistant\u200a\u2014\u200ayou can simply ask what\u2019s the weather forecast for your city for Sunday and you get answer in seconds. The first use of this technology is as easy as using it for the 100th time. And you can use 100% of its capabilities from the first usage\u200a\u2014\u200ayou are the power user from the start!\n\nOK, many of you probably asked Google, Siri or Alexa and know that it\u2019s pretty much theoretical for now. Of course \u201enatural language\u201d is not yet fully available and even powerful software companies will be struggling for some time with many languages\u2019\u2019 grammar rules and pronunciation before programs like Siri or Alexa can fully understand you. This process can take a while. But does this mean that bots are only available for huge and rich corporations investing billions of USD in the technology? Fortunately, not.\n\nProducing a bot that can \u201cdo anything\u201d or can \u201canswer any question\u201d as Google, Apple of Amazon want means that the bot has to understand and decode any sentence and any word existing. It required powerful, self-learning AI, constantly developed.\n\nBut hey, how many Googles are there in the world? Fortunately, most products and technologies are focused on performing one task or a set of simple tasks. Bot that will help you set up your washing machine to clean delicate wool will never be asked what\u2019s the weather like\u200a\u2014\u200asame as you will not ask a bot in your weather station what is the optimal temperature for washing silk. Users are very predictable in what and where they ask about, so a single-task bot is fairly easy to prepare. Even such a complicated service as bank account has, in fact, a limited set of tasks that we can do. Try to write down more than 10 different tasks that you did at your bank: money withdrawal, balance check, money transfer, find operation in history\u200a\u2014\u200adifficult to find many more, huh?\n\nWe can use bots in any place where users can enter text or say something. This means that we can potentially use natural language anywhere. Now it is limited to usage while sitting at the computer or via smartphone. But remember that more and more devices can be easily equipped with microphone or paired with smartphone, so spoken interface seems to be available to anything soon.\n\nInstalling and opening a separate app to steer with each single device or service would be a nightmare. This is why apps that use natural language are usually open to be integrated with external bots, like FB Messenger and other instant messaging apps. You only need to add your bank to you contacts and you can start \u201ctalking\u201d to your account. Or to your home entertainment system (imagine saying \u201cpause music\u201d instead of searching for the remote). Or even to your car (imagine asking \u201chow much petrol is left? Will we make to the next station, or should I fill up right now?\u201d).\n\nCustomers are not patient\u200a\u2014\u200atherefore not loyal. They do not want to learn how to use products for too long. Especially in a world where they change products frequently and use many technologies on daily basis. If they need to find a certain function of your product, obtain product support or simply use the product\u2019s complicated functions, they want it fast. And they do accept automatic answers to their question\u200a\u2014\u200abig companies already showed them it works.\n\nThis means one thing: bots can help in this multi-device, multi-brand and multi-software world. Because you do not need to re-learn your language every time you use a different device or service! Imagine that you produce a new washing machine with 48 programs for any kind of dirt and material. How can you force users to learn the whole interface? To remember which button activates which function and to learn which settings to use for frying pans and which for dessert plates? What if the dishwasher asks a few questions, gets answers and sets everything as needed? And in case a user is not sure if the dishwasher switches off automatically, she or he can just ask this question and get answer within a second, without searching for manual (usually stored deep in that drawer, among other manuals, but always missing if you look for it). You have a better product performance, ease of use, fast customer support even in the middle of the night.\n\nWhatever business you run, you should take a look at bot technology and ask yourself a question: would an automatic, natural-language support of my business help me reduce costs, gain customers, make my product easier to use? If so, start exploring bots. Or talk to those who did this job already\u200a\u2014\u200awell, you know how to find us\u00a0;-)", 
        "title": "Bots, bots, bots \u2013 K2DigitalVentures Blog \u2013"
    }, 
    {
        "url": "https://medium.com/@mslavescu/open-source-self-driving-car-kick-off-meeting-to-build-one-in-toronto-763d63d622e5?source=tag_archive---------6----------------", 
        "text": "On Dec 14, 6pm-9pm EST, we will have our first meeting. Let me know if you would like to attend remotely, for in person here is the info:\n\nFor those in Toronto/GTA area I scheduled the kick off meeting to start to build an Open Source SDC with GTA Robotics community,\u00a0here\u00a0are\u00a0the\u00a0details:\n\nIf you prefer EventBrite to register, here is the link:\n\nJoin the discussion and participate on the project\u2019s OSSDC Slack team here:", 
        "title": "Open Source Self Driving Car - Kick off meeting, to build one in Toronto"
    }, 
    {
        "url": "https://medium.com/@rorodata/every-company-is-a-data-company-febbf63c08eb?source=tag_archive---------7----------------", 
        "text": "The rapid digitization of consumer touch-points and business processes made \u201cdata\u201d all pervasive. Look around you. Data is transforming many industry sectors\u00a0,\u200a\u2014\u200afrom healthcare imaging to smartphone behavior, tracking sleep data to tracking remote assets, border surveillance to security and law enforcement. Falling memory prices and ever increasing computational power has been key drivers. Businesses can now capture, store and process large volumes data. Data is the new \u201coil\u201d and businesses must refine this commodity not just to grow, but to survive.\n\nInnovation in data tech is moving fast. Before we could wrap heads around the concept of \u201cBig Data\u201d, the narrative has already moved to \u201cmachine learning\u201d (ML), \u201cdeep learning\u201d (DL) and \u201cartificial intelligence\u201d (AI).\n\nBig data is more than a buzz word today, it\u2019s core to business. Harnessing large datasets for innovation and to gain competitive edge is getting important, fast. But is it easier said than done; going from data to dollars requires businesses to have the data, talent, and right culture. Great tools can be a big asset in this journey!\n\nBig data has been through it\u2019s \u201cIntel\u201d moment. Everyone wanted that chip in their computers but no one knew it was and what it could do. But, Intel meant better performance. Likewise, every business wants something to do with big data. Beyond all the hype, at its core, big data, data analytics is just a piece of enterprise tech. Enterprise tech in not easy.\n\nInternet startups/companies in early 2000s led the development of big data technologies. We call them \u201cdigital natives\u201d. They were both builders and users of new technologies and hence, better placed to leverage power of data. Digital natives used data to identify user behavior, offer personalized service, etc. For example, Amazon\u2019s book suggestions, Netflix\u2019s movie/TV show recommendation. These personalized services were not based on the traditional statistical models, companies mined humongous amounts of text, images, and videos to come with insights. In short, Digital Natives were born-into big data.\n\nBut this kind of work with data was not the easy for \u201ctraditional companies\u201d. First of all, they are not wired that way (although things have changed now). Some may also argue that it is not easy for digital natives either.\n\nTraditional companies have many legacy systems, and no one is going to rip-apart an existing system that \u201cworks fine\u201d for a new piece of tech. Traditional companies have been through many hype cycles and usually take a wait and watch approach\u200a\u2014\u200aespecially when the benefits are not apparent.\n\nBigger multinational players started early on big data, created budgets, rolled out pilots, tested departmental deployments. Some early endeavors worked, but many failed. In hindsight, the most commonly cited reasons for failure include: (a) Lack of proper strategy and design (b) Poor planning, (c)Limited adoption of new tools, (d) No cultural alignment and (e) Poor quality data (it\u2019s true!)\n\nWhile big players have capital cushion to to invest in exploratory projects, most medium and small sized players, both digital natives and traditional businesses, do not have such deep pockets. These businesses want big data to work from them on a budget. With as low integration and maintenance overheads as possible.\n\nCan it be done?\n\nAs Matt Truck from First Capital puts it\n\nA quick glance at some successful data-driven businesses will confirm this observation.\n\nEarly developers and adopters have done a lot of the hard work; the infrastructure for big data is in early stages of maturity. What industry needs are tools that make it possible to do the sophisticated data handling necessary without resorting to complex, hard-wired technical solutions. What we need are tools to make the journey from data to decision straight-forward. We need tools that make short work of complex tasks such as infrastructure provisioning, scaling systems and code deployment.\n\nWith such an infrastructure base or platform in place, it should be possible for businesses to manipulate data, build analytics, and build and deploy machine learning applications on the fly, without extraordinary effort.\n\nMany unique, innovative solutions for the business will be built by teams that are closest to the business problems. Data and analytics in the hands of every business function will free up the users from concerns about technology and scalability.\n\nConsistent with the above theme, we\u2019re building a simple, unified, powerful data analytics platform\u200a\u2014\u200arorodata. It is a collaborative, self-service data analytics platform for data users of all types\u200a\u2014\u200aExecutive, Business, and Data Scientists alike. Our aim is to provide all users with a fully managed cloud platform to build and and deploy analytical applications across all business functions.\n\nOn the platform, users can build applications without writing any code. An indicative list of applications could be -\n\nWe want every company to become a great data company. We want to enable businesses to leverage data and deploy analytical applications with ease, and to expedite their journey from data to dollars!\n\nWe\u2019re excited about building analytical applications in a truly new way. We would love to engage with like-minded folks who share our enthusiasm. You can reach us at rorodata.team[at]gmail[dot]com.", 
        "title": "Every company is a data company \u2013 rorodata \u2013"
    }, 
    {
        "url": "https://medium.com/cusp-civic-analytics-urban-intelligence/the-power-and-impact-of-small-data-in-education-2e71ff2d1415?source=tag_archive---------8----------------", 
        "text": "For over a decade, \u201cbig data\u201d and \u201canalytics\u201d have increasingly become a part of the education world. Technology allows teachers and administrators to capture, organize and analyze this information, so they can act on it in a timely way. But is it having an effect on student learning and achievement?\n\nOne thing that distinguishes schools in the United States from schools around the world is how data walls, which typically reflect standardized test results, decorate hallways and teacher lounges. Green, yellow, and red colors indicate levels of performance of students and classrooms which has been referred as \u201csummative assessment\u201d. The most notable problem to this approach has been the lack of immediate feedback and remediation on concepts that a student may be struggling to learn. For serious reformers, this is the type of transparency that reveals more data about schools and is seen as part of the solution to how to conduct effective school improvement. These data sets, however, often don\u2019t spark insight about teaching and learning in classrooms; They report outputs and outcomes, not the impacts of learning on the lives and minds of learners, they are based more on analytics and statistics, not on emotion and relationships that drive learning in schools. An alternative based on continual monitoring and remediation, called \u201cMastery Learning,\u201d was introduced in the early 1900s, but the time required to implement this kind of approach always precluded its widespread adoption.\n\nMastery learning has not been lurking on the fringe because its efficacy is debated, but because we just haven\u2019t been able to pull it off. The key to mastery learning is being able to observe and measure concepts of learning and student progress while providing interventions in real time for concepts not mastered. This is where technology and \u201cSmall Data\u201d come in\u200a\u2014\u200athey\u2019re the missing links to enabling teachers, students and parents to observe and collaborate on vital learning outcomes.\n\nWhile big data, which includes an array of demographic and student achievement data, among other things, can be very helpful, it is not enough. Teachers and administrators need access to data about what\u2019s going on behind classroom doors and across the school. Martin Lindstrom, a Danish author and Time magazine Influential 100 Honoree, calls it in his book \u201cSmall Data\u201d: small clues that uncover huge trends.\n\nFinally, cloud and mobile technology have matured to the point that mastery-based assessment is a viable option. What were once curriculum assessment systems are now becoming Mastery Learning Systems that are giving teachers the ability to analyze the performance data of their students and then, most importantly, do something about it in real time.\n\nContrary to big data, which puts the information of many into the hands of a few, \u201csmall data\u201d puts the information of the few into the hands of the few so they can take advantage of it. We\u2019re on the verge of mastery learning gaining widespread adoption thanks to a growing appetite for real change and a mature technology infrastructure to make it possible. Now it\u2019s time to shutter up the old factory and embrace the one-on-one instruction that is finally within reach.", 
        "title": "The Power and impact of Small Data in Education \u2013 Civic Analytics & Urban Intelligence \u2013"
    }, 
    {
        "url": "https://medium.com/bbm406f16/visual-saliency-estimation-d0d79b47a1e?source=tag_archive---------9----------------", 
        "text": "We mentioned that we will use Visual Saliency on Week 1 post. We will detail this idea in this post. Most food photos have irrelevant background. We want to eliminate this irrelevant background. For example in Figure 1, \u201cHamburger\u201d must be predicted in our program.\n\nBut there are unnecessary objects like drink and fries. We want to use deep learning for this problem. In this approach, raw image pixels are given to computer directly. No need to extract features. For this purpose, we want to eliminate irrelevant pixels. We think we will get a better result with this idea. Let\u2019s think, Figure 2 is trained for our model. There are no irrelevant pixels. Training will be better with Figure 2.\n\nWe will use Aykut Erdem and Erkut Erdem\u2019s paper for Journal of Vision which name is \u201cVisual saliency estimation by nonlinearly integrating features using region covariances\u201d. Project website is pointed out in references section[1].\u00a0\n\nWe downloaded MATLAB source code from project website. We just ran the \u201cdemo.m\u201d file and give Figure 1 for source image. The result is shown in Figure 3.\n\nWe can estimate hamburger\u2019s location with this approach. Then we can just create a bounding box and crop the image. We can get a similar result with Figure 2.\u00a0\n\nBut there are some difficulties. For example in deep learning, all the input images must have the same size. While using this approach, the output image may have different sizes. We think these kinds of problems. We will explain solutions for these difficulties in other posts.\n\nWe are also trying to collect food images from different datasets and sources. Datasets that we are using:\n\nFooDD: Food Detection Dataset[3]:\n\nThis dataset is prepared by three people(P. Pouladzadeh, A. Yassine, and S. Shirmohammadi) from University of Ottawa, Canada. The dataset includes images of various foods, taken in different conditions(different cameras, backgrounds, lightings). They use this dataset to design and test Computer Vision techniques. It has 20 classes and 3886 images. Classes are generally fruits or vegetables.\n\nMMSPG Food Image Dataset\u200a\u2014\u200a11[4]:\n\nThis dataset is from Multimedia Signal Processing Group (Swiss Federal Institute of Technology (EPFL), Lausanne, Switzerland). They have two datasets which are Food-5K and Food-11. Food-5K has 2500 food and 2500 non-food images. It is for Food/Non-food Image Classification so it has only two labels(not good for our project). Food-11 contains 16643 food images grouped in 11 major food categories(Bread, Dairy product, Dessert, Egg, Fried food, Meat, Noodles/Pasta, Rice, Seafood, Soup, and Vegetable/Fruit). But classes should be food names, not general categories. Our project must tell us the name of the food. We are working on this dataset.\n\nFood-101[5]:\n\nThis dataset is from ETH Zurich (Lukas Bossard, Matthieu Guillaumin, Luc Van Gool). It has 101 classes. Every class has 1000 images. Photographs are taken from foodspotting.com. We will be taking food images from foodspotting.com too.\n\nMenu-Match(Microsoft)[6]:\n\nThis is a very small dataset from Microsoft(Oscar Beijbom, Neel Joshi, Dan Morris, Scott Saponas, Siddharth Khullar). It has 646 images. It also has calorie stats but this is not in the scope of our project. We are working on this too because most of the images have multiple foods. In our project, every image must have only one label. We must separate the images which have only one food.\n\nUEC FOOD 100[7]:\n\nThis dataset is from The University of Electro-Communications, Tokyo, Japan. It has two versions. One has 100 kinds of foods and the other one has 256 kinds of foods. This is a very large dataset which will be very useful for us but it has multiple foods in some images like in the Menu-Match dataset. We must separate them.\n\nTurkish Foods Dataset:\n\nAnd we are trying to make our dataset which has various Turkish foods (Figure 4). Progress is slow but we will try our best.\n\nSee you in next week!", 
        "title": "[Week 2 \u2014 What Am I Eating?] \u2013 bbm406f16 \u2013"
    }, 
    {
        "url": "https://medium.com/@sbaechler/a-quick-introduction-to-the-python-ecosystem-for-developers-cd4bdf6939e8?source=tag_archive---------10----------------", 
        "text": "Are you interested in Deep Learning? Then there is almost no way around Python and Numpy. Python is very powerful and has an easy to read syntax. Unfortunately the ecosystem around Python is not as elegant as the language itself.\n\nUnlike most languages that have a C-Style syntax like Java and Javascript, Python doesn\u2019t use curly braces or semicolons. Instead it uses significant whitespace. A new line usually means the end of a statement. Indentation is used for scoping.\n\nVariables do not have to be declared before they can be used.\n\nThere is an official coding style called PEP-8 and it is used by most Python developers. The most important points are:\n\nThe transition to Python 3 has not been fully completed yet even though the new version was released over 8 years ago in December of 2008. Mac OSX Sierra ships with both versions. Python 3 can be started with the command .\n\nThe main (and backwards incompatible) differences are:\n\nPython 2 does have unicode strings as well. However, they need to have a prefix while binary strings need to have a prefix in Python 3. See the following code for an example. The text encoding has to be declared in the first line of the file in Python 2 for non-ascii files. Python 3 expects UTF-8 encoded files.\n\nMost of the new features on Python 3 have been backported to Python 2.7. The ones that break existing code are available in a package called . That\u2019s why all modules that support both versions of Python are most likely to have this import statement at the top:\n\nThis allows you to write Python 3 syntax that can be run in Python 2.7.\n\nThere is another compatibility module called which you might encounter from time to time in 3rd party libraries.\n\nBy now most libraries have been ported to Python 3. There are just a few exceptions, e.g. wx. Unless you need to use one of those libraries I suggest using Python 3 for your project. Tensorflow works with both versions of Python.\n\nPython has its own package repository called \u2018Python Package index\u2019 (PyPI).\n\nThe package manager is called \u2018pip\u2019 or \u2018pip3\u2019 for Python 3. The dependencies of an application are stored in a file called \u2018requirements.txt\u2019.\n\nUnlike npm, Pip does not offer local installs out of the box. All packages are installed globally by default. To have local installs you need a tool called Virtualenv. Since this tool is such a necessity, it has finally been added to the Python 3.3 core. Once created, the python version of the virtual env cannot be easily changed. The simplest way to change the Python version is to delete the virtualenv directory and create a new one.\n\nIn Python 3 you create a Virtualenv in the folder \u2018venv\u2019, activate it and install the dependenices of an application with the following commands:\n\nYou can exit a Virtualenv with the command .\n\nMake sure the Virtualenv is activated before you run .\n\nThe first library you should install is iPython. This interpreter adds tab completion and a context sensitive help function to the Python interpreter. Its debugger \u2018ipdb\u2019 is also better than the standard Python debugger \u2018pdb\u2019.\n\nThere is another interpreter called \u2018PyPy\u2019. It contains a JIT compiler and is therefore much faster for most applications than the default interpreter. It is not 100% compatible with native modules since it requires special C-bindings and it currently only supports Python 2 syntax.\n\nPython has still a few skeletons in the closet. One of them is the Python Imaging Library (PIL). This is the default library when working with images in Python. The official version has been discontinued long time ago and does not work on 64 bit systems or Python 3. However, there is a fork called \u2018pillow\u2019 which is still maintained. So whenever you need PIL you have to install the \u2018pillow\u2019 package. The module that you import is still called \u2018PIL\u2019.\n\nFor http requests the official libraries are urllib and urllib2. Forget those, they are way to complicated. Just use requests.", 
        "title": "A quick Introduction to the Python Ecosystem for Developers"
    }, 
    {
        "url": "https://medium.com/@mslavescu/i-would-love-to-see-this-happening-but-for-now-i-dont-see-how-6090b1da1762?source=tag_archive---------12----------------", 
        "text": "I would love to see this happening, but for now I don\u2019t see how.\n\nI was very excited initially, but as I mentioned here the core components are still closed source:\n\nFor an end to end Open Source Self Driving Car initiative based on Autoware and ROS you could:", 
        "title": "I would love to see this happening, but for now I don\u2019t see how."
    }
]