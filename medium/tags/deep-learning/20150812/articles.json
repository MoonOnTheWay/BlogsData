[
    {
        "url": "https://medium.com/@VentureScanner/the-state-of-artificial-intelligence-in-six-visuals-8bc6e9bf8f32?source=tag_archive---------0----------------", 
        "text": "We cover many emerging markets in the startup ecosystem. Previously, we published posts that summarized Financial Technology, Internet of Things, Bitcoin, and MarTech in six visuals. This week, we do the same with Artificial Intelligence (AI). At this time, we are tracking 855 AI companies across 13 categories, with a combined funding amount of $8.75billion. To see all of our AI related posts, check out our blog!\n\nThe six Artificial Intelligence visuals below help make sense of this dynamic market:\n\nDeep Learning/Machine Learning Applications: Machine learning is the technology of computer algorithms that operate based on its learnings from existing data. Deep learning is a subset of machine learning that focuses on deeply layered neural networks. The following companies utilize deep learning/machine learning technology in a specific way or use-case in their products.\n\nComputer Vision/Image Recognition: Computer vision is the method of processing and analyzing images to understand and produce information from them. Image recognition is the process of scanning images to identify objects and faces. The following companies either build computer vision/image recognition technology or utilize it as the core offering in their products.\n\nDeep Learning/Machine Learning (General): Machine learning is the technology of computer algorithms that operate based on its learning from existing data. Deep learning is a subset of machine learning that focuses on deeply layered neural networks. The following companies either build deep learning/machine learning technology or utilize it as the core offering of their products.\n\nNatural Language Processing: Natural language processing is the method through which computers process human language input and convert into understandable representations to derive meaning from them. The following companies either build natural language processing technology or utilize it as the core offering in their products (excluding all speech recognition companies).\n\nSmart Robots: Smart robot companies build robots that can learn from their experience and act and react autonomously based on the conditions of their environment.\n\nVirtual Personal Assistants: Virtual personal assistants are software agents that use artificial intelligence to perform tasks and services for an individual, such as customer service, etc.\n\nNatural Language Processing (Speech Recognition): Speech recognition is a subset of natural language processing that focuses on processing a sound clip of human speech and deriving meaning from it.\n\nComputer Vision/Image Recognition: Computer vision is the method of processing and analyzing images to understand and produce information from them. Image recognition is the process of scanning images to identify objects and faces. The following companies utilize computer vision/image recognition technology in a specific way or use-case in their products.\n\nRecommendation Engines and Collaborative Filtering: Recommendation engines are systems that predict the preferences and interests of users for certain items (movies, restaurants) and deliver personalized recommendations to them. Collaborative filtering is a method of predicting a user\u2019s preferences and interests by collecting the preference information from many other similar users.\n\nGesture Control: Gesture control is the process through which humans interact and communicate with computers with their gestures, which are recognized and interpreted by the computers.\n\nVideo Automatic Content Recognition: Video automatic content recognition is the process through which the computer compares a sampling of video content with a source content file to identify what the content is through its unique characteristics.\n\nContext Aware Computing: Context aware computing is the process through which computers become aware of their environment and their context of use, such as location, orientation, lighting and adapt their behavior accordingly.\n\nSpeech to Speech Transition: Speech to speech translation is the process through which human speech in one language is processed by the computer and translated into another language instantly.\n\nThe bar graph above summarizes the number of companies in each Artificial Intelligence category to show which are dominating the current market. Currently, the \u201cDeep Learning/Machine Learning Applications\u201d category is leading the way with a total of 200 companies, followed by \u201cNatural Language Processing (Speech Recognition)\u201d with 130 companies.\n\nThe bar graph above summarizes the average company funding per Artificial Intelligence category. Again, the \u201cDeep Learning/Machine Learning Applications\u201d category leads the way with an average of $13.8M per funded company. The SEM category includes companies that help marketers with managing and scaling their paid-search programs.\n\nThe graph above compares total venture funding in Artificial Intelligence to the number of companies in each category. \u201cDeep Learning/Machine Learning Applications\u201d seems to be the category with the most traction.\n\nThe following infographic is an updated heat map indicating where Artificial Intelligence startups exist across 62 countries. Currently, the United States is leading the way with 415 companies. The United Kingdom is in second with 67 companies followed by Canada with 29.\n\nThe bar graph above summarizes Artificial Intelligence by median age of category. The \u201cSpeech Recognition\u201d and \u201cVideo Content Recognition\u201d categories have the highest median age at 8 years, followed by \u201cComputer Vision (General)\u201d at 6.5 years.\n\nAs Artificial Intelligence continues to develop, so too will its moving parts. We hope this post provides some big picture clarity on this booming industry.\n\nVenture Scanner enables corporations to research, identify, and connect with the most innovative technologies and companies. We do this through a unique combination of our data, technology, and expert analysts. If you have any questions, reach out to info@venturescanner.com.", 
        "title": "The State of Artificial Intelligence in Six Visuals"
    }, 
    {
        "url": "https://medium.com/@Rapchik/deep-neural-networks-with-caffe-c888ad54c0f5?source=tag_archive---------1----------------", 
        "text": "Working in experiential marketing, I wanted to be able to target campaigns more intelligently than conventional wisdom and use technology to help classify live viewers. The idea was to have camera\u2019s on outdoor activations at eye level and classify in real time the age group, gender and ethnicity of each viewer.\n\nThough such systems are not new and have already been done by companies like Intel (Aim Suite) and Quividi but it seemed like a fair start before extending the module with systems like Facebook\u2019s PANDA to enhance the accuracy by detecting attributes from objects the person is carrying (clutch purse, pants, skirts, etc) to further increase the accuracy of the system.\n\nIt has been a year since I worked on the system and this post is to document the process and how I went about doing things.\n\nAfter reading lots of research papers on the problem set at hand I knew using an SVM could actually work fair enough for the first part of image classification like Aim Suite does by detecting faces via HAAR classifier and using feature detection + SVM to classify between gender (ear detection, 85% of images where the software detected ears were male) and similarly eye tracking to detect how long the viewer was watching the advertisement.\n\nBut such assumptions pile up to increasingly inaccurate result. My approach was to train multiple DNNs each classifying an attribute (age, gender and ethnicity). To start experimenting on this I used the amazing Caffe Framework which is open source thanks to the brilliant work by the guys at Berkley. The Caffe framework now uses cuDNN which is NVIDIAs framework for accelerated deep learning and boosts the performance of the framework incredibly.\n\nI used Caffe under Ubuntu 14.0 as it made far more sense to use linux over windows for compiling and working with such a framework. Firstly I would really recommend you to have a decent NVIDIA GPU for Caffe as training the models without CUDA is terribly slow. I used a 750 GTX TI for all experiments and had very good train times.\n\nTo setup Caffe, I pulled the latest branch off Github and followed the instructions on the website. A few gotcha\u2019s under linux were:\n\nAfter I built Caffe via make, I ran all the tests to make sure the dependencies are working fine. I used Caffe Python bindings to do all the work as Caffe allows you to create NN models and create test/validation sets in text files which it then parses and creates the model from. Using python bindings for Caffe helped me a lot as it allowed me to rapidly change models or data sets and test the results.\n\nOnce all the tests are successful I had Caffe ready to roll on my system. Next I tried running an example from Caffe. The Web Demo for Caffe comes bundled with the caffe source, instructions for setting it up are available on the wiki.\n\nAfter setting up Caffe, I started exploring different data sets to train using the same Convolution Neural Net model that Caffe used for Image Net Classification. I explored a lot of different face data sets including Face Tracer, FDDB, LFW and finally locking down on the FERET Database.\n\nThe FERET Database has 1208 different people with multiple pictures for each person variating in yaw, pitch, roll of the head, facial expressions and lighting conditions.\n\nTo work with the FERET database firstly I need to start filtering down data that I need. I created a quick python script to read the ground truths and create a compilation of only frontal images.\n\nNow, after I have all the frontal images in one place I need to figure out what kind of data I need to extract from these images to use as the input to the neural net. To understand this remember that a neural net just takes a 1D array of values and trains itself onto that array.\n\nSo I need to find out what kind of array I want to create to represent this image. There are many approaches to this some prefer using data from edge detection algorithms or using feature descriptors like SURF, SIFT, HOG. I planned to use raw image pixels as descriptors instead.\n\nThere are both pros and cons of using raw image pixels as input for DNN but the ground truth from FERET had very tight face rectangles and the eyes of subjects were almost aligned at the same position in all the images after cropping. This would actually improve my odds as the nerual net would be able to pick up smaller changes and have less variance.\n\nTo crop all the frontal images I created a quick python script that would pull the face rectangle data from the ground truth and crop the images accordingly using image magick. Now that I have the images cropped they still have color which makes it more complex for us to train the DNN which requires a 1D array and makes the DNN more susceptible to error.\n\nTo normalize the images, initially I just merged the RGB channels and normalized the output. The result without the cropping was:\n\nAs you can see the images are quite dark and are loosing out on a lot of data. To enhance detail, I cropped the images to just the face rectangle and then normalized the histogram. It is important to crop the image to the face area before doing histogram normalization as otherwise background pixels would be accounted for in the histogram. The final conditioned images looked like:\n\nAs you can see the final images are covering the entire RGB range quite nicely and hence have a lot more detail visible. Secondly, Thanks to all the pictures being frontal the eye coordinates of all the images are almost exactly at the same position.\n\nAfter conditioning the images, I used the train/validation model that came with caffe and was used to train the image net classifier. The model is used to create the internal DNN structure and was very well suited to a generic classifier like the one used on image net and hence would make sense to use as a testing model for a gender classifier.\n\nWhen training a model in caffe we need to split the dataset into two parts, training and validation dataset. There are multiple techniques to split the model into a train/validation sets, I used 90% of the images to train and 10% to validate the model.\n\nTo create the sets, I created a python script that would check the ground truth for each subject and determine whether the person was a male or female and then list them down into a text file like so:\n\nWhere the 1/0 at the end suggest what label to give to the data in the DNN. I just had 2 categories and hence 0 was male and 1 was female. A similar validation file was created which was passed to caffe for training.\n\nAfter training the data set for 20,000 iterations, I managed to achieve an accuracy of ~66% of gender classification and had similar results during deployment to test with end users.\n\nTo test this in real-time I created a quick OpenCV application that would detect faces using the HAAR classifier, cut the face region from the image and pass the image to a python script which categorized the image in real-time and returned the gender. The entire experiment worked well during real-time usage, and I was highly impressed by caffe\u2019s performance during run-time.", 
        "title": "Deep Neural Networks with Caffe \u2013 Umar Nizamani \u2013"
    }, 
    {
        "url": "https://medium.com/@smactoback/my-stylist-is-a-machine-5fcc86b96bc2?source=tag_archive---------2----------------", 
        "text": "At a Japanese fashion show in 2009 the HRP-C4 \u2018cybernetic human\u2019 female fashion model robot made her catwalk debut. Luckily the integration of fashion and tech has taken a different path since then. With many recent advances, deep learning is being used to tell us who we are looking at, what we would look good in and whether we are actually seriously ill. Visual search and discovery is a new market that is now beginning to be explored by a number of new startups, starting with e-commerce and fashion.\n\nFacebook has been a major contributor in deep learning research from their (not at all creepily named) group DeepFace. The group has developed a system to recognize whether two faces are of the same person. According to Facebook, a human running this task manually is able to identify correctly 97.5% of all faces (from the Faces in the Wild library). The DeepFace system on the same library of facial images is able to correctly identify 97.3% of faces. This represents a 27% increase in accuracy from the previous system. It also is representative of the global advancement of machine learning and deep learning in particular.\n\nClarifai, founded in 2013 by Matt Zeiler and headquarted in NYC provides accurate image recognition systems for a new set of companies being built relying on image search and image recognition. Clarifai scored the top 5 positions in ImageNet\u2019s image classification competition in 2013 and is backed by $10M in Series A funding from USV, Google Ventures and Qualcomm among others. Clarifai has now begun running its classification system on video, an exciting development. Matt Zeiler (an NYU machine learning PhD) has built a great team at Clarifai. Their CTO is a 10 year veteran of Google and other team member\u2019s hail from Stanford, UIUC, Google, high profile startups and many other fine establishments.\n\nCortexica was founded in 2008, spinning out of research from Imperial University in London. Focusing on image recognition for fashion e-commerce, Cortexica allows users to upload an image of a product (garment) and the Cortexica findSimilar tool provides visually similar options with in built buy options. Companies that have been using their API include Macy\u2019s and Rent the Runway. (I tested out the API in Macy\u2019s app. It wasn\u2019t a highlighted feature and once the image was taken, of a blender, I had to provide the parent and child categorization which kind of felt like it defeated the purpose of image search).\n\nIn an impressive demo, Thread Genius is aiming to provide answers to two questions when shopping for clothes: \u201cWhat do people wear with this\u201d and \u201cWhat\u2019s the $20 version of this\u201d. The founding team includes Andrew Shum (MIT, Spotify, Google), Rohan Agrawal (Columbia MS, Spotify) and Ahmed Qamar (UChicago). The demo shows the proprietary deep learning tech able to \u201csee\u201d a product (sunglasses, shows) and provide visually similar products. In a nice differentiator, the products are categorized from cheap, standard and expensive.\n\nThread Genius also brings something else very important to the table. Computers are able to run things fast and very accurately. But humans have traditionally been better at tasks that involve flexibility, creativity and onboarding new information. Fashion involves all three of those things (I\u2019m looking forward to the day my robot has choice in fashion. Fashion tech coders: please hard code in no Ed Hardy). So to help a computer be good at these things, Thread Genius uses a library that humans have already built for fun and for free. They use popular fashion blogs. By recognizing the products on these blogs and creating a library of images that \u201cmatch\u201d in a fashion sense, Thread Genius is able leverage fashion blogs and provide machine styling.\n\nIt\u2019s still super early for Thread Genius (and even all other deep learning e-commerce based apps). But these apps have in-built monetization (through commission referrals from stores) and very nice scalability trajectories. They seem to be following what Connie Chan wrote about in her article on WeChat for A16Z: apps (at least in China) focus on how central they are to daily life not how many users they have every day. Hopefully we can find some decent style for our Japanese cybernetic human soon.", 
        "title": "My Stylist is a Machine \u2013 Stephen McAnearney \u2013"
    }
]