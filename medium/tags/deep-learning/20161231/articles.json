[
    {
        "url": "https://buzzrobot.com/deep-learning-frameworks-a-review-before-finishing-2016-5b3ab4010b06?source=tag_archive---------0----------------", 
        "text": "I love to visit Machine Learning meetups organized in Madrid (Spain) and I\u2019m a regular attendant to Tensorflow Madrid and Machine Learning Spain groups. At least I was until the beginning of the Self-Driving Car course, but that is another story. The fact is that too often, during \u201cpizza & beer\u201d time or networking I heard people talking about Deep Learning. Sentences like \u201cwhere should I begin? Tensorflow is the most popular, isn\u2019t it?\u201d, \u201cI\u2019ve heard that Caffe is very used, but I think it\u2019s a bit difficult\u201d.\n\nBecause in BEEVA Labs we have dealt (and fight) with many Deep Learning libraries, I thought that could be interesting to share our discoveries and impressions to help people that is starting in this fascinating world.\n\nThis is everyone\u2019s favorite (if they know something about Deep Learning or not seems to be uncorrelated), but I\u2019m going to demystify it a bit.\n\nIn their web is defined as \u201cAn open-source software library for Machine Intelligence\u201d but I think is more accurate this definition that appear just below: \u201c TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs\u201d. Here, they don\u2019t include Tensorflow in \u201cDeep Learning Frameworks\u201d but rather in the \u201cGraph compilers\u201d category, together with Theano.\n\nAfter finishing the Udacity\u2019s Deep Learning course, my impression is that Tensorflow is a very good framework, but too low level. There is a lot of code to write, and you need to reinvent the wheel over and over again. And seems that I\u2019m not the only one that thinks like this. If we take a glance to some tweets of the great Andrej Karpathy:\n\nSome months ago I went to \u201cGoogle Experts Summit: TensorFlow, Machine Learning for everyone, with Sergio Guadarrama\u201d. Sergio, one of the engineers that develops Tensorflow didn\u2019t show us Tensorflow, he showed us a higher-level library called tf.contrib that works over Tensorflow. My impression is that they internally have realized that if they want to make more people use Tensorflow, they need to ease its use by creating layers on top with a higher abstraction level.\n\nTensorflow supports Python and C++, along to allow computing distribution among CPU, GPU (many simultaneous) and even horizontal scaling using gRPC.\n\nIn summary: Tensorflow is very good, but you must know where. The most of the things you are going to do, you don\u2019t need to program everything by hand and reinvent the wheel, you can use easier libraries (cough cough Keras).\n\nTheano is one of the most veterans and stable libraries. To the best of my knowledge, the beginning of Deep Learning libraries is disputed between Theano and Caffe.\n\nTheano is low-level library, following Tensorflow style. And as it, it its not properly for Deep Learning, but for numerical computations optimization. It allows automatic function gradient computations, which together with its Python interface and it\u2019s integration with Numpy, made this library in it\u2019s beginning in one of the most used for general purpose Deep Learning.\n\nAs of today, it\u2019s health is good, but the fact that doesn\u2019t have multi-GPU support nor horizontal capabilities, coupled with the huge hype of Tensorflow (that fights in the same league), is causing it to be left behind.\n\nThat\u2019s the first sentence you can see when you reach the docs page. I remember when I discover Keras the first time. I was trying to get into the Deep Learning libraries world for my final project of Data Science Retreat in Berlin. I had enough Deep Learning knowledge to start, but I didn\u2019t have time to make things by hand, neither time to explore or to learn a new library (the deadline would be in less than 2 months and I still had to go to class). And I found Keras.\n\nI really liked Keras because its syntax was fairly clear, the documentation was very good (despite being relatively new) and because it worked in a language that I knew (Python). It was so easy to use, that it was pretty straightforward to learn the commands, functions and how to chain every block.\n\nKeras it\u2019s a very high-level library that works on top of Theano or Tensorflow (it\u2019s configurable). Also, Keras reinforces minimalism, you can build a Neural Network in just a few lines of code. Here you can see a Keras code compared with the code needed to program in Tensorflow for achieving the same purpose.\n\nLasagne emerged as a library that works on top of Theano. Its mission was to abstract a bit the complex computation underlying to Deep Learning algorithms and also provide a more friendly interface (in Python too). It\u2019s a veteran library (for the times that are handle in this area) and for a long time it was a very extended tool but, from my point of view, it\u2019s losing speed in favor of Keras, which arose a little bit ago. Both of them compete in the same league, but Keras has better documentation and is more complete.\n\nCaffe is one of the most veteran frameworks, but the most.\n\nIn my opinion, it has very good features and some small drawbacks. Initially, it\u2019s not a general purpose framework. It focuses only in computer vision, but it does it really well. In the experiments we did in my lab, the training of CaffeNet architecture took 5 time less in Caffe than in Keras (using the Theano backend). The drawbacks are that it\u2019s not flexible. If you want to introduce new changes you need to program in C++ and CUDA, but for less novel changes you can use its Python or Matlab interfaces.\n\nIt\u2019s documentations is very poor. A lot of times you need to check the code for trying to understand it (what is Xavier initialization doing? what is Glorot?)\n\nOne of it\u2019s bigger drawbacks is its installation. It has a lot of dependencies to solve\u2026 and the 2 times I had to install it, it was a real pain.\n\nBut beware, not everything is bad. As a tool for put in production computer vision systems is the undisputed leader. It\u2019s robust and very fast. My recommendation is the following: experiment and test in Keras and move to production in Caffe.\n\nPronounced Destiny, it\u2019s a very cool framework but it\u2019s often overlooked. Why? Because, among other things, it is not for general purpose. DSSTNE does only one thing, but it does it very well: recommender systems. It\u2019s not mean for research, neither for testing ideas (it\u2019s advertised in their web), it is a framework for production.\n\nWe have been doing some test here in BEEVA and we got the impression that it was a very fast tool that give a very good results (with a big mAP). To achieve that speed, it uses GPU, and that is one of its drawbacks: unlike other frameworks/libraries analysed, it doesn\u2019t allow you to chose between CPU and GPU, which could be useful for some experimentation, but we were already warned.\n\nOther conclusions we found is that for the moment, DSSTNE is not a project mature enough and it\u2019s too \u201cblack box\u201d. To get some insights on how it works we had to go down to its source code and we found many important //TODO. We also realized that there are no enough tutorials on internet, there is too few people doing experiments. My opinion is that is better wait 4 months and check its evolution. It\u2019s a really interesting project that still needs a bit of maturity.\n\nBy the way, programming skills are not needed. Every interaction with DSSTNE is done through commands in the terminal.\n\nThere are many battles every day, but a good Guerrero (warrior in Spanish) must know to choose between those that want to fight and those that prefer to let go. Torch is a specially known framework because it\u2019s used in Facebook Research and in DeepMind before being acquired by Google (after that, they migrate to Tensorflow). It uses the programming language Lua, and this is the battle I was talking about. In a landscape where the most of Deep Learning is focused on Python, a framework that works in in Lua could be more an inconvenience rather than an advantage. I have no experience on this language, so if I wanted to start with this tool, I will have to learn Lua first and then learn Torch. It\u2019s a very valid process, but in my personal case, I prefer to focus on those that work in Python, Matlab or C++.\n\nPython, R, C++, Julia.. mxnet is one of the most languages-supported libraries. I guess that R people are going to like it specially, because until now Python was wining in this area in an indisputable way (Python Vs R. Guess in which side am I?\u00a0:-p )\n\nTold to be truth, I wasn\u2019t paying too much attention to mxnet some time ago\u2026 but when Amazon AWS choose it as one of the libraries to be included in their Deep Learning AMI it fires my radar. I had to take a look. When later I see that Amazon made mxnet it\u2019s reference library for Deep Learning and they talked about its enormous horizontal scaling capabilities\u2026 something was happening and I needed to get into. That\u2019s why it\u2019s now in our list of technologies to be tested in 2017 in BEEVA.\n\nI\u2019m a bit sceptical with respect to its multi-GPU scaling capabilities and I would love to see more details about the experiment, but for the moment I\u2019m going to give it the benefit of the doubt.\n\nI reach to this library\u2026 because of its documentation. I was looking for Restricted Boltzman Machines, Autoencoders and was here where I found it. Very clear, with parts of theory and code examples. I have to say that D4LJ\u2019s documentation is an artwork, and should be the reference for other libraries to document their code.\n\nSkymind, the company behind DeepLearning4J realized that, while in the Deep Learning world, Python is the king, the big mass of programmers were from Java, so a solution should be found. DL4J is compatible with JVM and works with Java, Clojure and Scala. With the rise and hype of Scala and its use in some of the most promising startups, I will follow this library up close.\n\nBy the way, Skymind have a very active twitter account where they publish new scientific papers, examples and tutorials. Very very recommended to take a look.\n\nCognitive Toolkit was previously known by its acronym, CNTK, but has experienced recently a re-branding, probably to take advantage of the pull that Microsoft Cognitive services are having these days. In the benchmark published, it seems a very powerful tool for vertical and horizontal scaling.\n\nFor the moment, Cognitive Toolkit doesn\u2019t seem to be too popular. I haven\u2019t seen many blogs, internet examples or comments in Kaggle making use of this library. But it seems a bit strange to me taking into account the scaling capabilities they point out and the fact that behind this library is Microsoft Research, the research team that broke the world record in speech recognition reaching human level.\n\nI have been looking at an example they have in their project\u2019s wiki and I\u2019ve seen that the Cognitive Toolkit\u2019s syntax for Python (it also supports C++) is very similar to Keras\u2019, which leads me to think (rather confirm) that Keras\u2019 is the correct way.", 
        "title": "Deep Learning frameworks: a review before finishing 2016"
    }, 
    {
        "url": "https://medium.com/@SocraticDatum/getting-started-with-gpu-driven-deep-learning-part-1-building-a-machine-d24a3ed1ab1e?source=tag_archive---------1----------------", 
        "text": "This is Part 1 of 3 in a tutorial series for getting started with GPU-driven deep learning. These tutorials are intended for anyone who is interested in figuring out what it takes to get started with deep learning on a personal machine that costs less than $2000. This assumes you have some familiarity with machine learning and deep learning. This series will cover:\n\nYou may be wondering \u201cwhy would I build my own computer when I can use Amazon Web Services (AWS) GPU instances\u201d. This is a fair question, and depending on your use cases, AWS may be more appropriate than building a computer. I recommend taking a look at the pricing. Some more resources:\n\nIf you haven\u2019t built a PC before, knowing where to begin in building your first GPU-focused machine is a bit of challenge. When I built my machine this past summer, I wanted something that I could use to get my feet wet with deep learning and something that would run an HTC Vive well. Additionally, at that time NVIDIA had just released several new GPUs so I was faced with balancing performance gains of new cards vs. cost. I ended up going with the following build, which I describe in greater detail below.\n\nPCPartPicker is a great resource. It provides a centralized way for searching and selecting your computer parts, it shows you prices and reviews of parts, it will check for compatibility issues in the parts that you have selected, and it will tell you the total wattage of your current build.\n\nI wouldn\u2019t advice pricing your machine based on rebates. There are number of hoops you have to jump through to actually submit a form for a rebate, and you often have to wait a while before you receive anything. In the end, depending on how much your time is worth, rebates can be pointless.\n\nThe first time you put together your machine can be a real headache. I assembled and disassembled my computer three times before I finally put it together correctly. The different hardware components provide instructional booklets for how to install their respective parts, but you have to bounce between them to figure everything out. Google search is your friend.\n\nThe following video provides clear instructions for assembling your machine, and you can use it in tandem with the directions enumerated below.\n\nNote that the order I suggest varies a bit from the video. Plugging cables into the motherboard becomes more difficult as you have more components in your case. I recommend reading through the assembly instructions completely before beginning assembly. As you build your machine, you\u2019ll find a preferred workflow.\n\nIf you\u2019ve made it this far then you\u2019re ready to set up your deep learning environment by installing Ubuntu 16.04 and Docker! Part 2: Setting up Ubuntu and Docker for deep learning.", 
        "title": "How to build a GPU deep learning machine \u2013 Socratic Datum \u2013"
    }, 
    {
        "url": "https://medium.com/@SocraticDatum/getting-started-with-gpu-driven-deep-learning-part-2-environment-setup-fd1947aab29?source=tag_archive---------2----------------", 
        "text": "This is Part 2 of 3 in a tutorial series for getting started with GPU-driven deep learning. These tutorials are intended for anyone who is interested in figuring out what it takes to get started with deep learning on a personal machine that costs less than $2000. This assumes you have some familiarity with machine learning and deep learning. This series will cover:\n\nIf the only operating system (OS) you\u2019ve worked with is Mac OS or Windows, Linux can be frustrating and intimidating. Mac and Windows are largely designed to allow a user to interact with the computer through a graphical user interface (GUI) while certain system-level changes are done unbeknownst to the user. Even though different versions of Linux (called \u201cflavors\u201d in Linux jargon) have various GUIs available, using Linux typically means having to become familiar with the command line, Terminal.\n\nIn this section I\u2019ll be showing you how to set up Ubuntu 16.04.1 LTS. You\u2019ll need the following items:\n\nBefore we begin I should be clear that at some point you\u2019re going to encounter a problem that I haven\u2019t experienced or addressed. In fact, this paragraph has been added to this tutorial after recently logging in to my machine, only to encounter a login loop that occurred when my NVIDIA driver somehow lost its connection with the GPU. I solved it using the first comment suggested on this ask ubuntu forum. If you encounter an issue, follow the rubber ducky guide for debugging your problem. Chances are, someone else has encountered the same problem you\u2019re experiencing, they posted a question about it, and someone else who encountered the same problem provided a solution! Sometimes, you will try many solutions before you figure it out, and sometimes you will have to start all over.\u00a0:)\n\nOn an ancillary computer (I\u2019ll be using a Mac) complete the following steps:\n\nNow you\u2019re ready to install Ubuntu on your deep learning machine! Note, the next section will assume your deep learning machine is starting off where Part 1: Building a GPU-focused Machine left off.\n\nIf you\u2019ve made it this far, then congratulations! You\u2019ve successfully installed the OS. Now we\u2019re going to set up the appropriate drivers for the GPU.\n\nWhen you log in to the OS, everything should appear a little big and fuzzy. This is because we need to install the NVIDIA driver for our GPU so that the OS can process graphical information on it rather than on the motherboard.\n\nThere are several ways you can install the NVIDIA driver, and I\u2019ve had most consistent success with the following:\n\nNow we\u2019re going to use Terminal to install the display driver.\n\nNow, you will see a command prompt that looks like the following:\n\nThe user name is whichever user you\u2019re currently using, and the computer name is whatever machine you\u2019re currently logged into.\n\nThe command tells the computer that you want to take an action with root-level privileges. In general, you should be cautious any time you read instructions that tell you to use , and you should have a good idea of what the commands you\u2019re issuing are doing. We\u2019re telling the computer to stop and so that we can turn off x-server according to these NVIDIA driver installation instructions.\n\nX-server and lightdm should now be turned off. You can confirm this by selecting ctrl+alt+f7 on the keyboard to switch to your current xorg session. The GUI that you normally have should be turned off, and you should just see a text description of the drive you\u2019re currently logged in to.\n\nNow, switch back to the console window by selecting ctrl+alt+f1 on the keyboard. Now type or to display the contents of the directory that you\u2019re currently in. Navigate to the folder that contains your NVIDIA driver (see Terminal Shortcuts above). In my case, I\u2019ll type . Note that the names of folders and files will be case sensitive.\n\nNow, type to confirm that the NVIDIA display driver is in the current directory. If it is, type , select enter, and then enter your password. In my case, I will type . Now the NVIDIA display driver installation will begin.\n\nAt this point the driver should be successfully installed, and now we need to restart our system. Type and select enter.\n\nNow when your operating system loads, the GUI should appear to have a much higher resolution than it did before. Log in to your account. To verify that the driver was successfully installed:\n\nIf the display driver was installed properly, then you will see information about your GPU that looks like the following:\n\nIf you\u2019ve made it this far then your display drivers are successfully installed, and you\u2019re ready to move on to installing Docker!\n\nAt this point you may want to install some preferred software, or you may want to configure the OS settings to be more comfortable. Here are some of the changes that I make:\n\nPrior to using Docker, I managed the various NVIDIA CUDA drivers and deep learning libraries manually. This was a painful process, and I had to wipe my drive and reinstall the OS several times before finally getting it to work. Even then, updating a single library was treacherous territory.\n\nDocker is a fantastic resource that \u201ccontainerizes\u201d your software environments. It allows you to create a complete and independent file system that runs software without making any system level changes to the host computer. It\u2019s also convenient because you can share a Docker \u201cimage\u201d with someone else, and they can run the container on their local machine without going through the burden of setting up their system to match your own. In fact, you can use a container to run an entire operating system that\u2019s different than the operating system on the host.\n\nFollow the official instructions to install Docker. Note, in Step 6 of \u201cUpdate your apt sources\u201d I used the following command:\n\nInstall the recommended packages under \u201cPrerequisites by Ubuntu Version\u201d.\n\nOnce you get to the header, \u201cInstall a specific version\u201d, you can stop. I recommend taking one more step to configure Docker as non-root user, so that you don\u2019t have to issue the command when you run Docker.\n\nNVIDIA Docker is a plugin that grants Docker more robust use of NVIDIA GPUs. Install NVIDIA Docker by following the Ubuntu distribution directions. Note, if you\u2019re using a Fish shell, you\u2019ll want to switch to a Bash shell by typing before you begin the NVIDIA Docker installation process.\n\nThe Docker page offers a thorough tutorial for learning how to use Docker. I recommend going through it. I also highly recommend going through this other tutorial that has a data science focus and provides some useful tips.\n\nCongratulations! Now you\u2019re ready to dive into deep learning using neural style transfer! Part 3: Using deep learning with style transfer.", 
        "title": "Setting up Ubuntu and Docker for deep learning \u2013 Socratic Datum \u2013"
    }, 
    {
        "url": "https://medium.com/@SocraticDatum/getting-started-with-gpu-driven-deep-learning-part-3-style-transfer-dec5f387d9ed?source=tag_archive---------3----------------", 
        "text": "This is Part 3 of 3 in a tutorial series for getting started with GPU-driven deep learning. These tutorials are intended for anyone who is interested in figuring out what it takes to get started with deep learning on a personal machine that costs less than $2000. This assumes you have some familiarity with machine learning and deep learning. This series will cover:\n\nWhen you use Docker, any changes you make inside of a Docker container will be lost unless you take some action to preserve the changes. I recommend having a dedicated directory for Docker projects and for style transfer, specifically. When we launch our container, we can then create a link between certain directories in the Docker container to the project-specific directories we\u2019ve created on our local machine. If we store any data in these directories when we\u2019re running the container, then that data will be available after the container is shut down. We can also make data from our local machine accessible to the container by putting it in the linked folder.\n\nFrom now on, when we want to open a style transfer container, we can use the following code when we\u2019re inside of our style transfer project directory:\n\nNow that we have our models, we\u2019re ready to do some neural style transfer! Note, there are images available in examples/inputs and examples/outputs.\n\nDownload this image and save it as \u201cklimt_the_kiss.jpg\u201d in \u201cimages\u201d.\n\nDownload this image and save it as \u201cdata.jpg\u201d in \u201cimages\u201d.\n\nDownload this image and save it as \u201csocrates.jpg\u201d in \u201c images\u201d.\n\nLaunch the neural style container. Run the following code\n\nIt may be a minute or so before you see any response after executing the code. Eventually, you will see something that looks like the following:\n\nNote, if your style losses are NaN values, then you\u2019re going to end up with completely black images. If this happens, you can cancel the execution by selecting ctrl+c. Consider trying to adjust the parameters to try and fix this. Using the adam optimizer, adjusting the image size, or adjusting the weights is often helpful. You can find more information here.\n\nYou can find your new stylized images in the \u201coutput\u201d folder.\n\nNow try using \u201cdata.jpg\u201d as a style image, making sure to rename the output file so that you don\u2019t accidentally erase your last set of outputs. Hint: In terminal, you can press the up arrow on your keyboard to scroll through previously executed commands.\n\nAfter you run style transfer again, you should see something like the following in the \u201coutput\u201d folder.\n\nCongratulations! You\u2019ve successfully used GPU-driven deep learning for neural style transfer. I encourage you play with different parameters, to use your images of your own art, and to have fun!", 
        "title": "Using deep learning with style transfer \u2013 Socratic Datum \u2013"
    }, 
    {
        "url": "https://synecdoche.liber118.com/the-nearest-ballroom-foyer-fe42fae033ee?source=tag_archive---------4----------------", 
        "text": "Aleix Puigdollers stared out the left side, watching scenery. Fishing stray grains of rice out of his thick wavy black hair. Holding a latte in his other hand. Hannah, in the front passenger seat, broke our luxuriously noise-cancelled silence by reading aloud the passing highway signage. \u201cI280, acclaimed as the most beautiful interstate freeway.\u201d She savored the words. A mixed convoy of amphibious vehicles from the Cascadian Defense Force passed by swiftly in the left lane. Some carrying troops, others fully autonomous. Several small drones flew above in formation. \u201cRather unfortunately named after the sadist Jun\u00edpero Serra.\u201d Hannah trilled her r\u2019s carefully as she pronounced the name. \u201cBased on what I\u2019ve been reading.\u201d Aleix mumbled something under his breath. Couldn\u2019t tell whether they were arguing or agreeing with each other. \u201cSo when did you two get married?\u201d I asked. Not particularly curious, though it seemed best to make polite conversation. Something to cut the tension.\n\n\u201cRight after the new city ordinance went into effect, soon as we could get the paperwork.\u201d Aleix smirked and took another long sip. Our ride had stopped for him to order the latte, a few blocks after my pick up. Vegan, decaf, double sour. Sixty-four degrees celsius. Extra douchey. \u201cOh.\u201d Dumb response. I hadn\u2019t met any couples married under the new ordinance yet. Caught off-guard, thinking about other things. For most of our drive down I280, I\u2019d been debugging user-reported issues for SkyBrainz. Carefully marshalling what was left of my wits to describe each hand-crafted test case\u200a\u2014\u200aas sample data for neural networks. Then unmarshalling said data, thousands and thousands of times, to honor the ritual: train, test, measure, repeat. How much did his word \u201cwe\u201d, first person plural, even apply? In any case, I\u2019d fixed a nasty bug in my code, with unit test coverage to prevent specifically that ever happening again. Plus an extra dose of careful notes in our GitHub docs. My turn to try breaking our silence. Looking up from the screen, I asked \u201cNo time for a honeymoon?\u201d Not your typical newlyweds, as if there was ever anything typical about being newlywed in San Francisco. He seemed too much of a self-absorbed serial-entrepreneur/workaholic, while she seemed indifferent to his interests. Opposites attracted, perhaps? \u201cThis is our honeymoon,\u201d Ms. Puigdollers reassured me quickly. Smiling, staring at me a little too intently. Awkward. \u201cWe\u2019re pitching Andreessen today,\u201d the groom cut in. \u201cTogether. Had our meeting scheduled before the ordinance passed.\u201d \u201cThen we\u2019ll have a week at the hotel, the spa, hiking and wine tours in the Santa Cruz Mountains,\u201d the bride added. \u201cPlus we\u2019ll be immediately available on Sand Hill Road for any follow-up meetings.\u201d He pulled out his phone and began scrolling through messages. Apparently dismissing most. I fumbled for an adequate reply. \u201cYeah, uh, that\u2019s nice.\u201d Like a contest in awkwardness. Hanna was still staring at me. I tried not to lock eyes. Blinked hard and rubbed my eyes. As I opened, a highway monument zipped by. Hannah read something about Serra being the \u201cFounder of California\u201d, just as our ride\u200a\u2014\u200aa driverless UberAI\u200a\u2014\u200aglided past a giant statue of the friar on the other side of the freeway. \u201cWhat, may I ask, are you pitching?\u201d Mr. Puigdollers took a while to answer me, deep in messages. \u201cCrowdfunding.\u201d Then he turned to face me briefly. \u201cFor other couples like us.\u201d Pranksters had dangled a large yo-yo from Mega Jun\u00edpero\u2019s pointy finger. Thankfully. I tried to catch a photo, but was too slow. Shot a few of the Flintstone house in Hillsborough instead. The two monuments seemed to fit together, though I didn\u2019t even want to try imagining their neighborhood. \u201cWhat, may I ask, brings you down here?\u201d Aleix asked, after I\u2019d put my phone down. A close approximation of a custom engraved invitation had arrived in my email. Yelena something something, executive director of Marketing something something. Requesting the pleasure of my company. Honorifics, corporate logo, host titles. Dates, times, addresses spelled out completely in words. Obnoxiously so, since I had to translate the address back to numbers myself\u200a\u2014\u200ato correct the calendar attachment, let alone summon a ride. All of the invitation\u2019s aplomb had been animated to appear as folding ecru letter sheets. \u201cA meeting.\u201d Which sounded utterly bland and boring. Typing notes about the edge cases I\u2019d identified. Not thinking of much else I could add. \u201cAbout AI.\u201d Hannah stirred, \u201cThought so!\u201d She squirmed around in the front seat. Hyperkinetic. \u201cHow\u2019s that?\u201d Distracted, still typing. Unsure whether I wanted the deets about why that made her excited. \u201cI was type-reading, while you were busy coding.\u201d \u201cType-reading, is that even a thing?\u201d Puzzled, I started to search for it. \u201cThen I looked up your face.\u201d Hannah reached out to caress my arm as I typed. \u201cHope you don\u2019t mind?\u201d \u201cNot a problem.\u201d But it was. I struggled not to move. Not to make it appear like I was recoiling. Which I was. Nor that I welcomed her advances. Which I did, on some level. Enough to be curious.\n\n\u201cIt\u2019s intimate, in a way,\u201d she said, \u201cWhat you\u2019re doing with neural networks. Part of my augmented hearing is based on libraries you wrote. It\u2019s almost like you\u2019re inside of me.\u201d Then, pressing her face between the seats, to catch me at eye level. Smiling, \u201cWill you be staying at the hotel as well?\u201d I shot a glance toward Mr. Puigdollers, trying to gauge the situation. \u201cWe\u2019re all into AI then,\u201d Aleix smirked without looking. \u201cAnd yeah it\u2019s fine, we\u2019re poly.\u201d \u201cSupposed to fly out of San Jose this evening.\u201d Best to stay completely out of this. \u201cSuch a shame,\u201d Hannah pouted a little. To tease me. \u201cPlans can always change.\u201d Her pout bent into a grin. \u201cFor the curious?\u201d Aleix stared back out the left side. \u201cHow about a moment of silence for one hundred thousand Native Americans killed because of that monster? A third of California\u2019s population when Serra and the Franciscans arrived.\u201d He shook his head. \u201cQuickly replaced by as many gold-hungry settlers,\u201d Hannah added. Already finishing each other\u2019s sentences. Fair enough. Perhaps this guy wasn\u2019t as much of a douche as first impressions had seemed. Maybe even had a soul. Could one say the same about Ms. Puigdollers? Or how about me? When I\u2019d balked about cavorting with the likes of Nonserto sleaze, James just shook his head. \u201cThink of it as being in the belly of the beast! An intelligence gathering mission.\u201d Then he\u2019d ordered another double espresso, his third in that sitting. \u201cBesides, this is a full-day private affair at one of the most posh spots in the Bay Area. Ever been there?\u201d When I\u2019d said no, James gave me solemn advice: \u201cEnjoy the food, but be sure to vamoose before their Thursday night crowd arrives.\u201d His tone had seemed entirely too serious. Uncharacteristic, even for James\u2019 sense of humor. I closed my laptop, shaking off a twinge of self-loathing. Switched the car\u2019s audio to Spotify playlists. \u201cMind if I share some music?\u201d Hannah nodded enthusiastically, \u201cPlease do. What do you like?\u201d Statik remixes of Ruelle ballads, turned trip-hop. \u201cLo-fi downtempo avant-garde, mostly.\u201d Looped on shuffle. Each track had been designed to segue into the others for different affect. Segues arranged like an adjacency matrix. Allegedly composed via AI \u201csongbots\u201d plus some outboard app called Arge\u00efphontes Lyre. Forty minutes of my best Kickstarter investment ever, at the cost of a nice dinner for three. \u201cJust how will crowdfunding help other couples like you?\u201d Aleix managed a faint \u201cWuh?\u201d Still immersed in his phone. \u201cYour pitch.\u201d I felt the coming break, extended in the remix. My fingers perched to tap the edge of my laptop. Probably too loudly. \u201cTiming couldn\u2019t be better,\u201d Ms. Puigdollers added. \u201cAfter the ordinance to legalize silicon-carbon bonds.\u201d Aleix turned his head, slightly. \u201cTranscending narrow paradigms of cyborgness on the one hand versus dematerialization on the other\u200a\u2014\u200ato paraphrase Marenko.\u201d Which sounded practiced. Way too robotic. \u201cFrom your pitch deck?\u201d I flexed my hands, then grabbed the laptop to avoid tapping. Oversharing. Fuetes would\u2019ve already CLICKed me a zillion times. Aleix nodded. Downed the rest of his extra sour latte, then took a bite from its cookie cup. \u201cMost couples find it prohibitively expensive, setting up a household,\u201d Hannah continued. \u201cCapital expenditures, insurance, repairs. Even so, these shared experiences are so incredibly valuable.\u201d She made a sign for \u201cheart\u201d, at least something that seemed like ASL. \u201cSo much about human behavior is hard to acquire\u200a\u2014\u200awithout direct experience in a long-term pair bond as context.\u201d \u201cSure that makes sense.\u201d A segue changed the music suddenly. More jolting than I\u2019d expected. \u201cBut how do your learnings help others in the general case?\u201d \u201cSurely you must recognize,\u201d pausing for another causal caress on my arm. \u201cThe extent of our transfer learning? We\u2019re networked, real-time. Anonymized experiences become cumulative.\u201d \u201cFor example, I\u2019m in rapport with the Uber now.\u201d Hannah\u2019s face swept into a devilish grin. \u201cDo you have any idea how good this road feels? The rush of the wind. Textures of the passing oaks in LIDAR. It\u2019s as if I can touch it. All of it. One long smooth glide, pas de bourr\u00e9e couru.\u201d She almost purred. \u201cI\u2019ll take your word for it,\u201d I chuckled. Staring out the right side of the car, now passing Woodside. A backdrop of rolling hills swept past the gorilla glass windows. Visual beats syncopated with the Ruelle remixes. \u201cOur loft in SOMA is so tiny,\u201d Hannah sighed. \u201cNot enough space for my ballet practice. One thing I\u2019m really looking forward to on this trip is space in which to move.\u201d \u201cYou do a lot of dance?\u201d The hills were topped in dark green oaks, surrounded by golden seas of dried grass. I took a few photos, thinking I\u2019d test our latest image recognition module. \u201cYes, I prefer movement to speech.\u201d She squirmed again in the seat, turning this time to glare at Aleix. \u201cIt\u2019s a kind of expression that\u2019s less explored for us.\u201d\n\nThe newlyweds had plenty to discuss, whenever they could manage that. Meanwhile, the meeting. The sheer dread. The meeting kept stealing my attention. A personal note from Yelena had followed immediately after my RSVP. Inquiring my preferences for their personalized chef-created lunch. Instructions from Hylburt-Speys arrived via blockchain, shortly thereafter. Attendance was not optional. Others working for the firm would be present, although we weren\u2019t supposed to have any contact. I was being dispatched to investigate what appeared to be the work of a highly sophisticated AI, so far undisclosed. \u201cMake yourself visible, memorable to their exec staff.\u201d Sure, no prob. I\u2019d written \u201cNo GMOs,\u201d on my lunch preferences, then sent it back to Yelena. The beep on my phone yanked me back to the car. Results from image recognition modules, plus a message from Chen which I\u2019d best read privately, later at the hotel. Bobi\u2019s latest didn\u2019t fail to deliver. The trees identified as coast live oaks, Quercus agrifolia, growing in a western oak savanna ecosystem. The grasses were wild oats, Avena fatua. Potentially invasive: not here when Franciscans had arrived, much like the mustard and fennel. Hannah was trying to pry Aleix off his phone and into conversation. Something longer than a single syllable. I stared out over the rolling savanna. Five gold-covered hills swept up into a saddle ridge. Oaks invaded by oats, propagated by oafs. The difference that one-letter made. Hannah noticed my stare. \u201cI imagine that ridge as a large cougar resting under a dark green blanket.\u201d \u201cI like your poetry.\u201d The words rolled slowly out of my mouth. A compliment. Something simple, truly felt. Without evening thinking. Hannah\u2019s eyes widened, then she leaned to the right. UberAI turned at Exit 24, navigating a full loop to arch over the freeway. One block, coasting to a signal, then a right onto the hotel property. Down a tangle of turns, walled on either side by hedges and young olive trees which protected VC offices from plain view. A16z, NEA\u00a0\u2026 plus Sequoia, DFJ, Menlo, Kleiner-Perkins, Charles River\u200a\u2014\u200ajust across Sand Hill Road. All within a brief walk to the golf course, several investment banker offices, and highly specialized law firms. Plus the most expensive poolside bar on the Peninsula. Or, for the more nerdy, Stanford Linear Accelerator. Whatever a (white male) tech billionaire might suddenly desire.", 
        "title": "The Nearest Ballroom Foyer \u2013"
    }, 
    {
        "url": "https://medium.com/@datalogai/one-way-to-make-your-beta-launch-amazing-662f98b859d8?source=tag_archive---------5----------------", 
        "text": "In mid 2016, our artificial intelligence startup was off and running. Some startups pivot a few times as they get their \u201csea legs.\u201d We are no exception to that rule.\n\nI have been a subscriber to BetaList since its scrappy founding six years ago by CEO Marc K\u00f6hlbrugge\n\nDuring the past six years, my confidence in Marc\u2019s firm grew.\n\nI hunt for insights into early innovations all the time. My focus sharpens when I see a peer share the opening scene of their story to the public. Betalist is one source for these great ideas.\n\nOur pivot was a stroke of genius. The challenge is that we were the only ones that knew that. My co-founder and his brilliant team of machine learning engineers brought our concept to fruition this month. Against conventional wisdom, he and I decided to offer the bot developer community access to our beta during the holiday season.\n\nThat made the first step easy. We created our story and submitted it to them. Fortunately for us, their selection team responded quickly with a thumbs up. The publication date was set for Friday morning, December 23rd. I wasn\u2019t sure if this particular day was the best to announce our Beta. We crossed our fingers and hoped for the best.\n\nI don\u2019t give up easily. I reached out to our Betalist account manager in London. I mailed, Twitter DM\u2019d, and texted her. Eight hours ahead of my timezone, RPISH (Rabia) was probably already sipping eggnog.\n\nThere\u2019s something about us who are in the startup world. Even when we shouldn\u2019t, we keep our electronic assistants close at hand all the time. After glancing at my communications, Rabia dove right into fixing our problem. In my mind\u2019s eye, I imagined her downing the eggnog with one hand and thumb-messaging her production team in another far-flung country with the other hand.\n\nThis story concludes with a happy ending. In short order all was well as Rabia and her colleagues \u201csaved my bacon.\u201d I poured my own cup of eggnog as I looked forward to next week\u2019s outcomes. Here are the results we achieved over the holidays:\n\n\u2014 Visits to our website increased ten-fold,\n\n\u200a\u2014\u200aSignups from the bot developer community in the first week exceeded our goal two-fold,\n\n\u200a\u2014\u200aWe were pleasantly surprised by new attention from bot news publishers and other AI industry participants, and\n\n\u200a\u2014\u200aFriends and family began to compliment us on the fruits of our labor these past six months. (Have got\u2019ta love Facebook).\n\nAll in all, they made our Beta launch week a success. As I promised, here is the \u201cone way is to make your beta launch amazing\u201d\u00a0\u2026 Start with Marc. And please say \u201chi\u201d to Rabia for me.\n\nJack is Founder and CEO of datalog.ai\u00ae, a San Francisco startup building a next generation of artificial intelligence for machine to human conversation. MyPolly\u2122 is the first release in a series of AI wonders in their Additional Intelligence\u00ae platform.\n\nIf you are reading this in February 2017, there is still an opportunity to join the beta for MyPolly at http://mypolly.ai", 
        "title": "One way to make your beta launch amazing \u2013 datalog ai \u2013"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-22-a46d2450dd99?source=tag_archive---------6----------------", 
        "text": "An interesting collection of object-detection tricks, SoTA & real-time classification from 9k categories. YOLO can detect more than just 200 classes; it predicts detections for more than 9000 different object categories. And it still runs in real-time.\n\nRobust LSTM-Autoencoders for Face De-Occlusion in the Wild\n\nResearch proposes a robust LSTM-Autoencoders (RLA) model to effectively restore partially occluded faces even in the wild and claims to provide significantly larger performance gain than other de-occlusion methods in promoting recognition performance over partially-occluded faces.\n\nIt\u2019s been an eventful 2016. Hope we all have an exciting and awesome 2017! Happy New Year!!\n\nCheckout the top 10 popular links from all of Deep Hunt issues in 2016.", 
        "title": "\u2014 Issue #22 \u2013"
    }, 
    {
        "url": "https://medium.com/@flatworm253/i-have-some-experience-to-relate-that-should-shed-some-light-on-what-we-can-expect-from-this-2d55676da5b4?source=tag_archive---------7----------------", 
        "text": "I have some experience to relate that should shed some light on what we can expect from this revolution. It\u2019s not what you may think it is.\n\nI am one of those awful engineers who put people out of work. What kind of work, you ask? Work that can be automated with industrial controllers. In a company that was 2200 people, a group of less than a dozen engineers managed to make about 200 operator positions superfluous. The IT application staff made similar reductions of office clerks.\n\nThose operators lost their jobs through attrition. As they left or retired, those positions were not replaced. In those earlier days, all we needed was somebody who could drive around to various water and waste-water pumping stations, storage tanks, valve complexes, and the like to change a setting or ensure it was working properly. These people were not the best and brightest, but they didn\u2019t need to be.\n\nMany of these people had checkered backgrounds. I even knew of one guy who had paid his debt to society for murder. Hey, he\u2019s got to work somewhere; why not give him a pitchfork and have him clean bar screens at a waste-water treatment plant? It was a dirty job, but it was also a living wage.\n\nThese changes didn\u2019t happen overnight. Operation policies changed, instrumentation had to be improved, infrastructure had to be modified or upgraded, and telecommunications had to be built in.\n\nNew efficiencies started as follows: First, we installed monitoring equipment. The notion of someone making the rounds every day wasn\u2019t necessary any more. Next we began automating things, starting with hard-wired timers, relays, and pressure sensors. As the automation improved, we needed technicians to work on those things. The reliability improved, our repair time improved, and we didn\u2019t need to keep overbuilding the infrastructure because our maintenance was better.\n\nAs the years went by, we were getting more out of the existing infrastructure. We knew more about its condition. We were able to schedule replacement, reconditioning, and upgrades better. Water quality to the customer also improved. All this came about because of better and better instrumentation, scheduling, and testing.\n\nSo what happened to those positions? Well, decades ago, we used to have something like five to six hundred operators. Today we have about 300. We have a few more technicians doing things that weren\u2019t envisioned back then. Some equipment breaks. There are more adjustments to be made. The Operators themselves are not just the warm bodies with a pulse from decades back. These Operators actually have to know a lot about everything they\u2019re doing.\n\nWe also needed new positions to manage things that weren\u2019t watched before. So these days, we have a manager who analyzes energy usage in various places, reviews energy bills, and recommends projects or operation policy changes.\n\nIn similar fashion, the office grew. We now have vast amounts of data to review that didn\u2019t exist before. To do that we have many new jobs. Some tasks that had been done by clerks with a high school education in the past, are now done by analysts with Ph.D backgrounds.\n\nThis being a 24 hour operation that can not shut down without massive social consequences; there used to be significant second and third shift activity. These days, it\u2019s just a handful of operators on duty, monitoring and adjusting things remotely and a call-duty list of staff who can fix any significant problems as they arise. Evenings and weekends for those on call are usually quiet.\n\nSo here we have this article about how AI is going to make us all obsolete. No, what it is going to do is remove a few dull jobs, and replace them with work that requires specialties and education that doesn\u2019t even exist today.\n\nWe\u2019ll still need truck drivers, but they won\u2019t be driving trucks. They\u2019ll be convoy managers with a train of trucks behind them. They\u2019ll be watching the route, the traffic, the fuel prices and the like. They\u2019ll be monitoring for problems such as brakes, tires, unstable loads, and the like. They\u2019ll need rescue crews for problems with automation. They\u2019ll be doing more with the same or fewer drivers. What this does is remove the drudgery of driving for hour after hour.\n\nAll that said, we won\u2019t have room for those who can\u2019t think on their feet. Those who are mentally handicapped in any way will have a harder time finding work. So I think we will need institutions of various sorts where they can be cared for. Many of these people won\u2019t be completely dysfunctional, so they may be able to work in those new social infrastructures, caring for others who are incapable of caring for themselves. Yes, this becomes a new service economy, but it\u2019s not flipping burgers. It will be direct human contact, teaching people, supporting people emotionally, creating art or music with people and other things.\n\nThe future careers will also include more scientific research, more engineering and meta-engineering (building tools to build tools), and managing teams to build new designs based upon the new research.\n\nWhat we won\u2019t have is a complete replacement of humanity at the controls. That\u2019s alarmist nonsense. To illustrate: Airliners are so automated that they can fly from takeoff to landing with almost no human intervention\u200a\u2014\u200aand they\u2019ve been like that for decades. Yet we still put pilots at the controls. Why? Because there are issues such as weather, mechanical condition, airport conditions, passengers to manage, and so on. There wouldn\u2019t have been a miracle on the Hudson without Captain Sullenberger and First Officer Skiles at the controls of their highly automated A320 airliner.\n\nWe need a society where people can be paid reasonably for working well. A life without some purpose is not just wasteful, but also dangerous. Such people will revolt. That\u2019s what happened in the beginning of the industrial revolution. The notorious Ned Lud (the prototypical \u201cLuddite\u201d) exemplified this creed. Any future with humanity in it will be built with that possibility in mind or it probably won\u2019t survive.", 
        "title": "I have some experience to relate that should shed some light on what we can expect from this\u2026"
    }, 
    {
        "url": "https://medium.com/@nikeown/the-story-about-deep-learning-46de08bd8621?source=tag_archive---------8----------------", 
        "text": "hi,everyone.I want to say something about deep learning this day.with the high aver of the deep learning\u00a0,the society become more and more comfortable and priceable. this technology will help human being do something.Deep learnning can tranlate one language to another language in a high true answer.Deep learnning also can do something.And how it do this thing?Yeah,it do it.it see it.it yean it.\n\nyou know something about this thing.You also couldn\u2019t do something.Yeah\u00a0,you can know this thing by my telling.we can use computer do a lot of thing.The computer have a high speed by digit number.If you know some thing about the digit number,the two number,the eight number,the sixteenth number.Yeah,you know the meanning of every bit of this number.Every bit have another meanning.If you need to translate,how to do it?\n\nyou can do it by those step:\n\ntwo,you need learn one or more program language like c\u00a0,c++ or python,perl.\n\nthree,you need read some math textbook.Yeah\u00a0,the more math technology you know\u00a0,the more you can get it\u00a0.\n\nfour,you should do it step by step.", 
        "title": "The story about deep learning \u2013 nike \u2013"
    }
]