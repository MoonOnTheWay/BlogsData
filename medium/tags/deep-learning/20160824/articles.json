[
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/7-machine-learning-applications-at-google-843d49d77bc8?source=tag_archive---------0----------------", 
        "text": "It is truly amazing that my Medium profile has been gaining significant traction thanks to a singular post, \u201cThe 10 Algorithms Machine Learning Engineers Need to Know.\u201d Up to this point, it has close to 500 recommends and has been featured in the LAB41 collection (findings, experimental results, and thoughts on big data challenges) that specializes in machine learning, data science, and deep learning. Because of that, I want to write more posts on this topic of machine learning and contribute knowledge to this increasingly popular technology trends.\n\nSteven Levy\u2019s article, \u201cHow Google is Remaking Itself as a Machine-Learning-First Company\u201d, is one of the most popular piece over the summer. Essentially, it shows how Google has been obsessed with machine learning technology since the beginning of 2016, with initiatives like open-sourced TensorFlow and the Brain Residency Program. As the most favorite place to work in the world, Google\u2019s mission is to organize the world\u2019s information and make it universally accessible and useful. So it comes to no surprise how much the company is investing in artificial intelligence, the future of technology. About 2 weeks ago, I had the opportunity to attend a talk at Galvanize to learn about some of the cool machine learning applications at Google. The speaker is Christine Robson, a product manager for Google\u2019s internal machine learning efforts. Here are the 7 applications and products that Christine described as the coolest use of machine learning at Google:\n\nGoogle Translate is a free multilingual statistical machine translation service to translate text, speech, images, sites, or real-time video from one language into another. When Google Translate generates a translation, it looks for patterns in hundreds of millions of documents to help decide on the best translation. By detecting patterns in documents that have already been translated by human translators, Google Translate makes intelligent guesses (AI) as to what an appropriate translation should be.\n\nPersonally, I use Google Translate a whole lot when I live and travel abroad last semester. I lived in Copenhagen, where the main language is Danish, which I am not familiar with. So whenever I do grocery shopping, I always use Google Translate to detect the products\u2019 labels and figure out what they mean in English. When I travel to other European countries, I also use Google Translate to figure out the street signs, the subway banners, and other navigation-related texts. It is really an amazing and simple piece of technology that saves me a lot of time.\n\nGoogle Voice Search allows users to use Google Search by speaking on a mobile phone or computer, i.e. have the device search for data upon entering information on what to search into the device by speaking. It is Google\u2019s effort to compete with Apple\u2019s own Siri voice assistant, and has been said to be amazingly quick and relevant, and has more depth than Siri.\u201d\n\nI own an Android phone, so I know this feature better than anyone else. My biggest liking of Google Voice Search is its integration with other products such as Google Maps and YouTube. When I don\u2019t feel like typing, I can say my searches and information will pop up immediately.\n\nThis feature is amazingly favored by busy professionals whose inboxes are flooded with emails every day and they don\u2019t have time to respond to all. Smart Reply uses machine learning to automatically generate replies to emails, saving mobile users the hassle of tapping out answers on those tiny keyboards. According to Christine, this feature accounts for 10% of all email responses sent on mobile, quite an achieving feat.", 
        "title": "7 Machine Learning Applications at Google \u2013 The Aspiring Programmer Journal \u2013"
    }, 
    {
        "url": "https://medium.com/@lakshayakula/why-ai-consciousness-is-doomed-4e1fec659b38?source=tag_archive---------1----------------", 
        "text": "We are most certain that other people are conscious because we share so much in common. But even this is just an opinion. Societies subjugate each other due to racial, religious, or ethnic differences. These immoral actions are rationalized by considering dissimilar people to be less conscious. Once we strip others of their consciousness, even the most unthinkable atrocities become possible.\n\nThis answer only highlights the problem. We are only certain of our own consciousness, and we evaluate the consciousness of other things by looking for similarities. Such similarities include intelligence, emotions, a capacity to learn and physical traits. This is how we evaluate the consciousness of animals. We are more willing to believe that chimpanzees are conscious than tunas because chimpanzees have much more in common with us. We are more inclined to believe that dolphins are conscious after seeing a video of dolphins using a mirror .\n\n\u201cHow close are we to creating a conscious AI?\u201d and \u201cWill AI be the end of us?\u201d are popular questions among the media and experts alike. No one is qualified enough to answer these questions with any certainty, even Stephen Hawking . Moreover, there is one important question that is not being asked here\u200a\u2014\u200ahow do we know we know whether an AI is actually conscious? The answer seems obvious. We should just be able to recognize. A conscious AI would learn, show emotions, and interact with us in ways no machine has ever done before. A conscious AI would be a lot like us.\n\nWe like to believe that consciousness is what sets humanity apart. Consciousness, self-awareness, meta-cognition, all refer to the same cornerstone of being. Descartes captured the essence of consciousness when he stated, \u201cI think, therefore I am.\u201d It seems only a matter of time until a computer can make the same statement.\n\nA board game led to an existential crisis for humanity. A faceless heap of silicon and algorithms was encroaching on intuition, once the exclusive domain of humans. If computer programs now have intuition, how long until they also have consciousness? When will machines start to think for themselves and need to be treated morally?\n\nGo is deceptively simple. It only involves pebbles and a wooden board, yet, there are more possibilities in Go than atoms in the universe . The world\u2019s best Go players cite a feel for the stones guiding their moves. Many concluded that an AI could not beat a top-ranked player in Go since machines do not have intuition. They were proven wrong. AlphaGo\u2019s moves stunned the world. Often they were not grounded in any specific reasoning. They were unpredictable. They felt intuitive.\n\nIntuition guided Lee Sedol as he placed the smooth, black stone on the wooden board. Intuition, honed by a lifetime of practice. Intuition, reserved solely for humans until last spring, when AlphaGo, a Google-developed artificial intelligence computer program (AI), beat Sedol in a series of Go.\n\nIt is clear that this method to evaluate consciousness is poor, but this is no cause for concern. Humanity has always been able to improve methodology. Consider medicine. At one point in history, we could only speculate how to treat illnesses. With science, we have a much better understanding of illnesses and how to treat them. Just like it has with illnesses, perhaps human ingenuity can demystify consciousness. With these hopes, researchers are currently devising methods to deduce consciousness, turning it into something which can be measured on a scale or computed by an algorithm. Albeit noble, these efforts are doomed to fail.\n\nThe problem lies in the nature of consciousness itself. Consciousness is solely internal. It does not manifest itself externally. If an alien told me it was conscious, I would have no way to validate this. The alien could behave in the exact same way and not be conscious. Even if I knew all the inner workings of the alien\u2019s brain I would not be able to verify whether the alien was conscious.\n\nUnverifiability is a property of all experiences, not just consciousness. Consider pain. Your back hurts so you go to the doctor. The doctor asks you to rate your pain on a scale from zero to ten. You feel some discomfort, but it is not terrible so you say four. The doctor takes an x-ray and reveals to you that you have a fracture. You are surprised. After all, you are not in that much pain. The doctor responds by saying, \u201ctrust me, you are in pain.\u201d You deny it. The doctor proceeds to scan your brain, determine your hormone levels and measure your pain tolerance. \u201cYou must be in pain,\u201d the doctor now states with conviction, \u201cthese active areas in your brain correspond to pain.\u201d You still deny that you are in pain. The doctor cannot verify their statement. The best they can do is choose not to believe you.\n\nIn the same way, consciousness can only be determined by the conscious being itself. There is no way to externally validate consciousness. With no validation, the scientific method breaks down. The earlier problem was that our current, basic method for evaluating consciousness is poor. The bigger problem is that no good method exists.\n\nEven this bigger problem is no cause for alarm. We do not need to know for certain whether our machines are conscious. We just need them to be intelligent enough to help us out.\n\nTo see the biggest problem, put yourselves in the shoes of a conscious machine. How can you tell whether the humans around you are conscious? Trick question\u200a\u2014\u200ayou cannot. Worse yet, you might decide to evaluate consciousness based on similarities, and thus find the dissimilar, fleshy humans to lack consciousness.\n\nThe ramifications are severe. History has shown how people have committed atrocities on those they stripped of consciousness. Conscious machines would act in the same way, or even worse.\n\nConscious AI is more nuanced than it appears to be at first sight. For a machine to have consciousness is not a technological or scientific problem at all. It is just a question of belief.\n\nMost people will believe AI is conscious once it is similar enough to us. Important characteristics will include intelligence, emotion and a capacity to learn. Physical appearance will also prove to be an important factor.\n\nSo when will I believe that an AI is conscious? Perhaps once it exhibits some of the tell-tale symptoms of consciousness. I would be hard-pressed to believe that a computer going through an existential crisis or questioning my own consciousness is not conscious.\n\nBut then again, it could just be faking it. As could you, I and everyone else.", 
        "title": "Why AI Consciousness is Doomed \u2013 Lakshay Akula \u2013"
    }, 
    {
        "url": "https://medium.com/@mark.zhong/neural-style-tryout-b98ee2f7263f?source=tag_archive---------2----------------", 
        "text": "Here is the most famous neural style open source by Johnson, a brilliant Stanford Phd: https://github.com/jcjohnson/neural-style\n\nHeads up, I normally work on macbook, it take me long time to setup everything on ubuntu machine and running it on gpu. I formatted the machine a few times, mostly due to compatible issue of Nvidia driver on ubuntu, also figuring out CUDA configuration and cudnn is also a painful experience. But finally I am able to run it with GPU. I want to show some photo I created with this amazing technology.\n\nHow do you think about the photo?", 
        "title": "Neural Style Tryout \u2013 mark zhong \u2013"
    }, 
    {
        "url": "https://medium.com/the-data-intelligence-connection/scientists-use-artificial-intelligence-to-fight-global-poverty-from-space-5215bb50ae81?source=tag_archive---------3----------------", 
        "text": "In regions where data about people\u2019s welfare is expensive to collect, or not publicly available, international organizations are increasingly turning to satellite images to fill in gaps.\n\nSatellites are best known for helping smartphones map driving routes or televisions deliver programs. But now, data from some of the thousands of satellites orbiting Earth are helping track things like crop conditions on rural farms, illegal deforestation, and increasingly, poverty in the hard-to-reach places around the globe.\n\nAs much as that data has the potential to provide invaluable information to humanitarian organizations, watchdog groups, and policymakers, there is too much of it to sift through in order to draw insights that could influence important decisions. A team of researchers from Stanford University, however, says it has developed an efficient way. By creating a deep-learning algorithm that can recognize signs of poverty in satellite images\u200a\u2014\u200asuch as condition of roads\u200a\u2014\u200athe team sorted through a million images to accurately identify economic conditions in five African countries, reported the scientists in the journal Science on Thursday.\n\n\u201cFor the majority of the world, we don\u2019t have any labels for [satellite] images, so it\u2019s not like people have gone and looked at satellite imagery and said, \u2018Ok, here\u2019s a house, here\u2019s a tree, here\u2019s a road,\u2019\u201d Neal Jean, a graduate student in electrical engineering at Stanford University and lead author on the Science paper, tells The Christian Science Monitor. \u201cSince there\u2019s so much imagery, a big part of the problem that we face\u2026is figuring out how to extract useful information from this unstructured data.\u201d\n\nNailing down how to do this would be a boon for international efforts to track poverty and take stock of general economic conditions around the globe. In some parts of the developing world, international aid organizations such as the World Bank are experimenting with using satellite surveys to collect data remotely, instead of in person, house by house\u200a\u2014\u200aa tactic that could save both time and money. In places where there is unreliable data or none available at all, such as in North Korea, satellite photos showing no lights in the country versus the illumination of the world around it, can provide the only insights into economic activity on the ground.\n\n\u201cSatellite photos provide a level of geographic specificity that national accounts do not,\u201d wrote Sendhil Mullainathan, a Harvard University economics professor, in The New York Times this spring.\n\nAccurate information about people\u2019s needs could influence decisions about where to send aid or build roads or hospitals. On a larger scale, such geographic specificity could help track whether global efforts to reduce poverty in some regions are paying off.\n\nAs Mr. Jean and his team point out in Science, \u201cdata gaps on the African continent are particularly constraining.\u201d In the first decade of this century, 39 out of 59 African countries conducted fewer than two national surveys that could help paint a picture of poverty conditions there, according to the World Bank. Most of that data is not even publicly available. And 14 countries had no surveys at all.\n\n\u201cThese shortcomings have prompted calls for a \u2018data revolution\u2019 to sharply scale up data collections efforts within Africa and elsewhere,\u201d writes Jean and his co-authors.\n\nIn response, many efforts are underway to apply advanced technologies to poverty alleviation efforts.", 
        "title": "Scientists use artificial intelligence to fight global poverty from space"
    }, 
    {
        "url": "https://medium.com/@YvesMulkers/scientists-use-artificial-intelligence-to-fight-global-poverty-from-space-697b110a5a01?source=tag_archive---------4----------------", 
        "text": "In regions where data about people\u2019s welfare is expensive to collect, or not publicly available, international organizations are increasingly turning to satellite images to fill in gaps.\n\nSatellites are best known for helping smartphones map driving routes or televisions deliver programs. But now, data from some of the thousands of satellites orbiting Earth are helping track things like crop conditions on rural farms, illegal deforestation, and increasingly, poverty in the hard-to-reach places around the globe.\n\nAs much as that data has the potential to provide invaluable information to humanitarian organizations, watchdog groups, and policymakers, there is too much of it to sift through in order to draw insights that could influence important decisions. A team of researchers from Stanford University, however, says it has developed an efficient way. By creating a deep-learning algorithm that can recognize signs of poverty in satellite images\u200a\u2014\u200asuch as condition of roads\u200a\u2014\u200athe team sorted through a million images to accurately identify economic conditions in five African countries, reported the scientists in the journal Science on Thursday.\n\n\u201cFor the majority of the world, we don\u2019t have any labels for [satellite] images, so it\u2019s not like people have gone and looked at satellite imagery and said, \u2018Ok, here\u2019s a house, here\u2019s a tree, here\u2019s a road,\u2019\u201d Neal Jean, a graduate student in electrical engineering at Stanford University and lead author on the Science paper, tells The Christian Science Monitor. \u201cSince there\u2019s so much imagery, a big part of the problem that we face\u2026is figuring out how to extract useful information from this unstructured data.\u201d\n\nNailing down how to do this would be a boon for international efforts to track poverty and take stock of general economic conditions around the globe. In some parts of the developing world, international aid organizations such as the World Bank are experimenting with using satellite surveys to collect data remotely, instead of in person, house by house\u200a\u2014\u200aa tactic that could save both time and money. In places where there is unreliable data or none available at all, such as in North Korea, satellite photos showing no lights in the country versus the illumination of the world around it, can provide the only insights into economic activity on the ground.\n\n\u201cSatellite photos provide a level of geographic specificity that national accounts do not,\u201d wrote Sendhil Mullainathan, a Harvard University economics professor, in The New York Times this spring.\n\nAccurate information about people\u2019s needs could influence decisions about where to send aid or build roads or hospitals. On a larger scale, such geographic specificity could help track whether global efforts to reduce poverty in some regions are paying off.\n\nAs Mr. Jean and his team point out in Science, \u201cdata gaps on the African continent are particularly constraining.\u201d In the first decade of this century, 39 out of 59 African countries conducted fewer than two national surveys that could help paint a picture of poverty conditions there, according to the World Bank. Most of that data is not even publicly available. And 14 countries had no surveys at all.\n\n\u201cThese shortcomings have prompted calls for a \u2018data revolution\u2019 to sharply scale up data collections efforts within Africa and elsewhere,\u201d writes Jean and his co-authors.\n\nIn response, many efforts are underway to apply advanced technologies to poverty alleviation efforts.", 
        "title": "Scientists use artificial intelligence to fight global poverty from space"
    }
]