[
    {
        "url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-1-fd544fab149?source=tag_archive---------0----------------", 
        "text": "Reinforcement learning provides the capacity for us not only to teach an artificial agent how to act, but to allow it to learn through it\u2019s own interactions with an environment. By combining the complex representations that deep neural networks can learn with the goal-driven learning of an RL agent, computers have accomplished some amazing feats, like beating humans at over a dozen Atari games, and defeating the Go world champion.\n\nLearning how to build these agents requires a bit of a change in thinking for anyone used to working in a supervised learning setting though. Gone is the ability to simply get the algorithm to pair certain stimuli with certain responses. Instead RL algorithms must enable the agent to learn the correct pairings itself through the use of observations, rewards, and actions. Since there is no longer a \u201cTrue\u201d correct action for an agent to take in any given circumstance that we can just tell it, things get a little tricky. In this post and those to follow, I will be walking through the creation and training of reinforcement learning agents. The agent and task will begin simple, so that the concepts are clear, and then work up to more complex task and environments.\n\nThe simplest reinforcement learning problem is the n-armed bandit. Essentially, there are n-many slot machines, each with a different fixed payout probability. The goal is to discover the machine with the best payout, and maximize the returned reward by always choosing it. We are going to make it even simpler, by only having two possible slot machines to choose between. In fact, this problem is so simple that it is more of a precursor to real RL problems than one itself. Let me explain. Typical aspects of a task that make it an RL problem are the following:\n\nThe n-armed bandit is a nice starting place because we don\u2019t have to worry about aspects #2 and 3. All we need to focus on is learning which rewards we get for each of the possible actions, and ensuring we chose the optimal ones. In the context of RL lingo, this is called learning a policy. We are going to be using a method called policy gradients, where our simple neural network learns a policy for picking actions by adjusting it\u2019s weights through gradient descent using feedback from the environment. There is another approach to reinforcement learning where agents learn value functions. In those approaches, instead of learning the optimal action in a given state, the agent learns to predict how good a given state or action will be for the agent to be in. Both approaches allow agents to learn good behavior, but the policy gradient approach is a little more direct.\n\nThe simplest way to think of a Policy gradient network is one which produces explicit outputs. In the case of our bandit, we don\u2019t need to condition these outputs on any state. As such, our network will consist of just a set of weights, with each corresponding to each of the possible arms to pull in the bandit, and will represent how good our agent thinks it is to pull each arm. If we initialize these weights to 1, then our agent will be somewhat optimistic about each arm\u2019s potential reward.\n\nTo update our network, we will simply try an arm with an e-greedy policy (See Part 7 for more on action-selection strategies). This means that most of the time our agent will choose the action that corresponds to the largest expected value, but occasionally, with e probability, it will choose randomly. In this way, the agent can try out each of the different arms to continue to learn more about them. Once our agent has taken an action, it then receives a reward of either 1 or -1. With this reward, we can then make an update to our network using the policy loss equation:\n\nis advantage, and is an essential aspect of all reinforcement learning algorithms. Intuitively it corresponds to how much better an action was than some baseline. In future algorithms, we will develop more complex baselines to compare our rewards to, but for now we will assume that the baseline is 0, and it can be thought of as simply the reward we received for each action.\n\nis the policy. In this case, it corresponds to the chosen action\u2019s weight.\n\nIntuitively, this loss function allows us to increase the weight for actions that yielded a positive reward, and decrease them for actions that yielded a negative reward. In this way the agent will be more or less likely to pick that action in the future. By taking actions, getting rewards, and updating our network in this circular manner, we will quickly converge to an agent that can solve our bandit problem! Don\u2019t take my word for it though. Try it out yourself.\n\nIf you\u2019d like to follow my work on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjliani.\n\nIf this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!", 
        "title": "Simple Reinforcement Learning in Tensorflow: Part 1 - Two-armed Bandit"
    }, 
    {
        "url": "https://medium.com/software-is-eating-the-world/ai-machine-leaning-and-deep-learning-a-primer-e1e6c85ab2e3?source=tag_archive---------1----------------", 
        "text": "George Hotz built a self-driving car in 2015. By himself. One person in a literal garage. To put that in context, none of the participants in the inaugural 2004 DARPA Grand Challenge went more than 7.2 miles out of the 150 course in the Mojave Desert.\n\nArtificial intelligence technology is clearly improving very, very fast. Sundar Pichai says Google is now an AI-first company rather than a search- or mobile-first company.\n\nHow did this happen? Where did AI come from? What\u2019s the difference between machine learning and deep learning? And could AI be as profound a platform shift as the shift to cloud or the shift to mobile?\n\nI explore some of these questions in a 45-minute podcast with slides and a demo. Yes, it\u2019s long, but I\u2019m told it\u2019s worth it:\n\nEven actual deep learning researchers think so, though they seem a little surprised about it:", 
        "title": "AI, Machine Learning, and Deep Learning: A Primer \u2013 Software Is Eating the World \u2013"
    }, 
    {
        "url": "https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-2-making-deep-nets-shallow-701b2fbd3ca9?source=tag_archive---------3----------------", 
        "text": "The first question that comes to mind when thinking about making deep nets shallow is why do we need to make them deep in the first place?\n\nThe answer is that any network needs to learn a complex function which maps the inputs to the output labels, and the present learning methodologies are not capable of teaching a shallow network the complex function directly. Prof. Rich Caruana discusses this in his paper \u201cDo Deep Nets really need to be Deep?\u201d. He proves that by training a given dataset using an ensemble of deep nets and then transferring the function learned on these deep nets into a shallow net, it is possible to match the accuracy of the shallow net to that of the ensemble. However, simply training the shallow net using the original data gives an accuracy which is significantly lower. Thus, by transferring the function we can make the shallow network learn a much more complex function which otherwise would not have been possible by training directly on the dataset.\n\nBut how do we transfer the learned function? We run the dataset through the ensemble and store the logits (the values that are input to the SoftMax layer). Then we train the shallow network using the logits instead of the original labels. This can be intuitively understood because by training on the logits, we are not only training the model on the right answer but also on the \u201calmost right\u201d answers as well as \u201cnot at all right\u201d answer.\n\nDr. Geoffrey Hilton took it a step further and proposed that we should use the soft targets (weighted SoftMax function outputs) instead of logits. In his paper \u201cDistilling the Knowledge in a Neural Network\u201d he proposes that this provides more information to be learned by the shallow model.\n\nNow that we know how to make our deep networks shallow (and thereby save processing time and power), in the next part we will look at techniques to make the network run faster.\n\nP.S. Check out Prof. Caruana\u2019s talks here and here.\n\nP.P.S. Dr. Hilton also shows that the model can learn to recognise inputs that it has never seen before just by inferring its structure from the soft targets. He calls it Dark Knowledge. Check out his talk if you\u2019re interested.\n\nUpdate: Prof. Caruana\u2019s published a new paper\u200a\u2014\u200a\u201cDo Deep Convolutional Nets Really Need to be Deep (Or Even Convolutional)?\u201d in which he concludes that shallow nets can emulate deep nets given that they have multiple convolutional layers.", 
        "title": "\u201cMobile friendly\u201d deep convolutional neural networks \u2014 Part 2 \u2014 Making Deep Nets Shallow"
    }, 
    {
        "url": "https://medium.com/@andraganescu/life-causes-uncertainty-and-other-problems-7e2e6ee721ef?source=tag_archive---------4----------------", 
        "text": "Life is not a phenomenon. Biology is a phenomenon caused by life. Just like time is a property of the time-space continuum, life is a property of a larger continuum space-time-life.\n\nConsider for a moment that life is an embedded property of the universe, just like time and space are.\n\nWe could call this anything else instead of \u201clife\u201d, but I chose to call it \u201c\u2018life\u201d because the local entropy decrease is the root cause for biological phenomenons and evolutionary processes, which together we empirically associate with the concept of life.\n\nTime and life are two opposing fundamental forces of the universe. Time is the global entropy increase and life is the local entropy decrease. I don\u2019t know if they\u2019re symmetric, but they are closely so. It is a fractal who grows the same over and over again, no surprises here.\n\nBoth awareness and consciousness are information constructs rising in the electrochemical equilibrium in the brain, equilibrium reached in the brain\u2019s effort for more accurate predictions. It is neither consciousness nor awareness that causes the weird measurement problem, but the fact that when we measure we are observing time-space-life instead of simply time-space which we think we\u2019ll find. Because we may be nothing more than self sustaining processes manifesting as biological phenomenons in the physical reality, by measurement our contained hyperlocal space infused with the effects of life\u200a\u2014\u200athe space-life, transforms time-space into time-space-life.\n\nLife is a hyperlocal property of the universe which makes extremely low entropy in the form of functional systems, the pinnacle of which are self sustaining processes.\n\nBecause of its nature, life develops resilience just as time, because of its nature, develops decay.\n\nThe first result of low enough entropy is the appearance of structure. The easiest example of structure are crystals. Structure is the first power of resilience.\n\nA process has far lower entropy than a structure. Because a process has steps, a start and an end, the constraints to build a process are huge, much bigger than the constraints to build a structure. A process is the natural result of lowering entropy below the threshold of a structure. Homeostasis is a process resulting from lowering the entropy of a biological structure.\n\nA self sustaining process has even lower entropy than a process. All natural self sustaining processes feed upon themselves. Any outside feed and the entropy would increase. In general, time injects outside feeds into self sustaining processes created by life, this way increasing their entropy and hence lowering their resilience.\n\nLife, as an embedded property of the universe, generates structures. You might wonder then what is the difference between a crystal and a virus? Protein. It is very likely that viruses simply appear randomly as part of the process of life in the local time-space-life continuum.\n\nThese two properties of the universe we call life and time are \u201cperceived\u201d by us in a certain way: the animus and the tenses. They both are however way beyond the scope of our perception. Time is so global it encompasses our infinite universe, life is so local that it is broken into an infinite of pieces. Both time and life act upon space.\n\nBecause life is the inverse function of time, we may say it builds on the future to create the past, while time builds on the past to create the future. From life\u2019s perspective any high entropy space is in the future and from time\u2019s perspective any low entropy space is in the past. Both time and life are intertwined with space.\n\nThe phenomenon of biology is the manifestation in the physical reality of life\u2019s process creation. Evolution is a form of creating resilience by compounding on previous states and it is a process that results spontaneously given enough detail is present in the structures and processes that result in life-space.\n\nAs life acts in reverse of time and lowers entropy, it follows that its peculiarity and special requirements to grow complex self sustaining processes is quite normal. By lowering entropy levels, processes become more and more complex and this complexity is what holds advanced biologic phenomenons to be more common in the observable universe. But the catch is that the rarity of advanced biologic phenomenons is not because of the lack of water and oxygen, it is because of process complexity. There may very well be biologic phenomenons based on silicon and nitrogen but there has to be a whole other array of compatible things around for the self sustaining processes that appear in life-space to keep compounding on themselves.\n\nMinimizing entropy means lowering side effects. Inside a process, to lower disorder you guard for side effects. That is why every advanced evolutionary process is three things: self sustaining, self referential and selfish.\n\nThe great filter is the maximization of the three properties of the biological phenomenons: the self sustaining feeding, the self referential information chains and the selfish bias. If there is a great filter it is only within ourselves and it may be beyond our capacity to avoid it. If there is a great filter and other civilizations have reached it, it is very likely they have destroyed themselves. Self destruction is the last name of the great filter.\n\nHuman self destruction is caused by one or more of the following local maximums: missing sustainability by feeding too much into the process itself (e.g. ecosystems break down), misinterpretation of perception by relying exclusively on interpretation (e.g. humanity is on a pedestal and the other animals are on a lower step) and protecting your kin by limiting outside feeding into the particular facet of the process we\u2019re into (war!).\n\nAt present we could be in a race between developing true AI (or artificial super intelligence, or artificial life as I call it), which is the next lower entropy step for a super intelligent evolutionary process, and reaching our self destruction great filter. Best of luck.", 
        "title": "Could the fabric of the universe be made of more than space and time?"
    }, 
    {
        "url": "https://medium.com/shuri-org/lots-of-ai-nlp-and-deep-learning-applied-in-ios-1cd8946c174e?source=tag_archive---------5----------------", 
        "text": "I watched the Apple keynote and took some notes of all the AI and deep learning used. It\u2019s cool to see this \u201cnew\u201d tech applied towards good user experience.\n\nSiri will be available on the desktop and will be open for developers!\n\nSiri will be extensible through natural language understanding for specific domains, such as messaging. So I guess the way you extend Siri, is by extending a specific domain: messaging, ride booking (uber), photo search, start workouts, and payments.\n\nThis is actually a really interesting problem, how do you design an extensible conversational interface where extensions or apps can add functionality but can\u2019t or unlikely interfere with existing functionality or other apps. You effectively open Siri to learning new things without risking it learning wrong or horrible things.\n\nThey use LSTM, a deep learning technique, to improve keyboard word suggestions. It\u2019s interesting and cool that they actually named LSTM in the keynote! Using this technique the beginning of the sentence can affect word recommendations for the end of the sentence:\n\nThey extract information from a conversation that can then be used to setup a meeting.\n\nThe photos can be grouped using face recognition:\n\nFinally, photos and movies can be grouped, recognizing the important moments and topics. In addition, Memories can auto-create movies from photos and videos, adding music, recognizing the appropriate mood. Very cool\u00a0:)!\n\nLots of cool stuff, what did I miss?", 
        "title": "Lots of AI, NLP and Deep Learning applied in iOS \u2013 shuri.org \u2013"
    }
]