[
    {
        "url": "https://medium.com/udacity/this-week-in-machine-learning-a9e70e9c162f?source=tag_archive---------0----------------", 
        "text": "This week\u2019s top Machine Learning stories, including AI agents that detect weapons, recognize faces, diagnose blindness, and more!\n\nMachine Learning is one of the most exciting fields in the world. Every week we discover something new, something amazing, something revolutionary. It\u2019s incredible, but it can also be overwhelming. That\u2019s why we created This Week in Machine Learning! Each week we publish a curated list of Machine Learning stories as a resource to help you keep pace with all these exciting developments. New posts will be published here first, and previous posts are archived on the Udacity blog.\n\nWhether you\u2019re currently enrolled in our Machine Learning Nanodegree program, already working in the field, or just pursuing a burgeoning interest in the subject, there will always be something here to inspire you!\n\nScientists at University College London create a machine learning technique to automatically discern hidden weapons from X-ray images at security checkpoints.\n\nResearchers at MIT implement a machine learning algorithm for facial recognition that spontaneously operates similarly to the human brain\u2019s analogous system.\n\nA report by McKinsey and Company predicts that machine learning can be used to radically improve the efficiency of delivering fresh food to supermarkets.\n\nResearchers at Google develop an AI agent that can automatically learn to detect indicators for common forms of blindness using machine learning.\n\nUniversity of Washington researchers create MusicNet, an attempt to analyze classical music the way many algorithms have previously analyzed images.\n\nThe Invisible Cities project uses neural networks to turn hand-drawn maps into pictures resembling satellite images of real or fictional metropolitan areas.", 
        "title": "This Week in Machine Learning, 1 December 2016 \u2013 Udacity Inc \u2013"
    }, 
    {
        "url": "https://blog.thechatbotfactory.com/les-3-facteurs-qui-freinent-la-d%C3%A9mocratisation-de-lintelligence-artificielle-21e76ce12e9d?source=tag_archive---------1----------------", 
        "text": "La technologie est partout. Dans nos agendas, nos maisons, nos loisirs et m\u00eame nos relations. Depuis plus de 20 ans, nous vivons au rythme effr\u00e9n\u00e9 des innovations technologiques. D\u2019abord avec l\u2019arriv\u00e9e d\u2019internet, puis des r\u00e9seaux sociaux et maintenant de l\u2019intelligence artificielle. Les GAFA investissent massivement dans les technologies cognitives.\n\nDemain, les taxis n\u2019auront plus besoin de chauffeurs, les avions deviendront des drones, les m\u00e9tros et bus seront autonomes et les robots envahiront les magasins pour vous conseiller ou vous servir.\n\nLes progr\u00e8s dans la miniaturisation des composants \u00e9lectroniques permettront de cr\u00e9er des ordinateurs ou nano-robots toujours plus puissants et toujours plus petits, qui se nicheront dans votre poche, dans votre montre ou\u00a0\u2026 dans vos yeux.\n\nL\u2019exemple des chatbots est r\u00e9v\u00e9lateur. On ne parle que d\u2019eux. En r\u00e9alit\u00e9, ils ont encore tout \u00e0 prouver. Les grandes entreprises et les marques l\u2019ont bien compris. Elle investissent depuis plusieurs mois dans ces outils qui vont r\u00e9volutionner la relation client. Tout reste \u00e0 faire. Tout reste \u00e0 cr\u00e9er. Notamment autour de l\u2019exp\u00e9rience utilisateur et des cas d\u2019usages. C\u2019est la mission de The Chatbot Factory.\n\nLes chatbots pullulent sur nos messageries. Pourtant, la plupart ne d\u00e9passent pas encore le statut de gadget. Le potentiel de la technologie est immense, mais mal utilis\u00e9, elle devient accessoire. Il est du devoir de l\u2019\u00e9co-syst\u00e8me d\u2019\u00e9vang\u00e9liser le march\u00e9 et de convaincre en proposant des exp\u00e9riences fluides et cr\u00e9atrices de valeur. Casser les mythes et proposer des solutions \u00e0 des probl\u00e8mes concrets. C\u2019est le chemin qu\u2019il nous reste \u00e0 parcourir pour que les chatbots remplacent un jour les apps ou les sites internet.\n\nL\u2019intelligence artificielle doit permettre d\u2019aller plus loin que ce que nous faisons aujourd\u2019hui. Par exemple d\u2019am\u00e9liorer le diagnostic m\u00e9dical en mod\u00e9lisant des masses de donn\u00e9es impossibles \u00e0 analyser par le cerveau humain, faciliter l\u2019acc\u00e8s \u00e0 la justice en rendant le droit moins al\u00e9atoire et plus lisible, am\u00e9liorer notre productivit\u00e9 pour accro\u00eetre notre temps libre, prendre en charge des t\u00e2ches chronophages \u00e0 faible valeur ajout\u00e9e,\u00a0\u2026\n\nEt si notre univers devenait conscient et \u00e9tait capable de nous comprendre parfaitement et de s\u2019adapter \u00e0 nous en temps r\u00e9el? Ce monde n\u2019est pas si lointain. Il attend simplement que vous y entriez\u2026", 
        "title": "Les 3 facteurs qui freinent la d\u00e9mocratisation de l\u2019intelligence artificielle."
    }, 
    {
        "url": "https://blog.grakn.ai/advent-at-grakn-labs-2nd-december-2016-c84d5bd814c6?source=tag_archive---------2----------------", 
        "text": "Today we are featuring the year\u2019s most interesting breakthroughs in deep learning that we have been fawning over at Grakn Labs. (For those of you who are interested in a crash course in deep learning, here\u2019s a great video by Andrew Ng at Stanford.)\n\n2016 has been a breakthrough year for deep learning, especially for Google and DeepMind. As engineers and technology junkies, we truly have great respect for the work they are doing over at the DeepMind offices a mere 2.5km from our office here in London.\n\nAlphaGo besting Lee Sedol in March of this year was definitely one that has stuck with us. According to our resident Go commentator, Michelangelo, the move above made by \u201cthe machine\u201d during Match 2 was pure \u201cmachine\u201d. When AlphaGo made this move (at 1:18:22 in the video above), it baffled the human experts and is unprecedented by a human, but its genius was only revealed later. This move enabled AlphaGo to open up five areas of play to cinch the win! Though we are skeptical of a \u201cgeneral AI\u201d that spans all domains, we were nonetheless crazy impressed with AlphaGo. (We\n\nStarcraft + DeepMind? I think we\u2019re in geek/nerd heaven. DeepMind has set its mind to yet a different game. This time DeepMind has partnered with Blizzard to allow AI researchers to deploy bots in the StarCraft II game environment. Previous deep learning successes with IBM Deep Blue in chess and DeepMind\u2019s AlphaGo have been impressive, but a game like StarCraft presents even greater challenges\u200a\u2014\u200aimperfect & dynamic information, how to be able to plan over a longer time horizon and adapt. We\u2019re waiting here with bated breath.\n\nAs an open-source company, we love making technology accessible to a wider community. We were at a meetup in London where Daniel Slater shows us how reinforcement learning using TensorFlow can be employed to teach a machine, aptly named AlphaToe, to play Tic Tac Toe. Here\u2019s the link to the AlphaToe repo on Github if you want to check it out yourself.\n\nGames aside, as an international team with at least 14 languages floating around our team of 17 people, it\u2019s safe to say that we\u2019ve all used Google Translate at one point or another. Google\u2019s Multilingual Neural Machine Translation is now able to translate between language pairs that the system has never encountered before. Researchers attribute it to the system picking up on the existence of an interlingua\u200a\u2014\u200aa kind of meta language that is actually encoding the semantics of a language. The system is currently being used live in Google Translate. For the more technically orientated of us (or those with more time on their hands), here\u2019s the research paper and blog post from Google. Otherwise, here\u2019s a summary news article from Wired.\n\nThe American elections have been a hot topic in the office as we contemplate expanding our presence to the US. Since its debut in March, we have been entertained by the senseless tweets of DeepDrumpf, a Twitter bot created by Bradley Hayes, a postdoc at MIT. DeepDrumpf was trained on a few hours worth of transcripts of victory speeches and debates from the president elect using deep learning techniques. The tweets were constructed character by character and inspired by recurrent neural network models that had been previously employed to mimic Shakespearean speech. Although not the most sophisticated use of deep learning that we\u2019ve seen, we must hand it to him for originality and capturing the zeitgeist.", 
        "title": "Advent at Grakn Labs: Deep Learning \u2013"
    }, 
    {
        "url": "https://medium.com/@datalabai/hello-tensorflow-93f1d3641b5a?source=tag_archive---------3----------------", 
        "text": "Artificial Intelligence is permeating every corner of every industry, and it\u2019s starting to disrupt the way we do business. As Eric Schmidt puts it, \u201cNew developments in machine intelligence will make us far, far smarter and benefit everyone on the planet.\u201d\n\nHere are some examples of how companies are are already using Machine Learning. The NFL uses machine learning to gather deep insights into player movements, positions, and passes to reorganize play style. In the medical sector, machine learning analyzes patients and predicts the likelihood of their returning. Even hiring and talent management in most companies is now handled by algorithms that dig out desired characteristics and, hopefully, remove biases.\n\nIf you are a developer, you would be wise to start learning AI. TensorFlow would be the easiest way to get started with AI. TensorFlow is the Machine Learning framework developed by Google. TensorFlow has a huge community with over 500 contributors, about 40K GitHub stars, 20K forks, lots of built in models, code and documentation out there. It has language bindings for Python, C++ and Go with other languages on the way. Models written in TensorFlow can be run on Google Cloud TPUs (Tensor Processing Units) with a very easy to use interface. This blog provides an overview of basics of TensorFlow to help you get started with AI.\n\nTo write a program in TensorFlow, one uses graphs to describe the computations. Nodes in the graph are called operations. An operation takes zero or more Tensors, performs some computation, and produces zero or more Tensors. Think of operators as equivalents of like + or * and Tensors as equivalents of variables or constants. Tensor is a typed multi-dimensional array. Input for the machine learning models and intermediate values are represented using Tensors. To compute anything, a graph must be launched in a Session which executes the operations on CPUs or GPUs.\n\nA TensorFlow programs consists of two parts. The first part describes how to construct the graph. The second part describes the execution phase of the graph consisting of execution of operations.\n\nFor example, it is common to create a graph to represent and train a neural network in the construction phase, and then repeatedly execute a set of training ops in the graph in the execution phase.\n\nTo build a graph start with operations that do not need any input (source ops), such as Constant, and pass their output to other operations that do computation. TensorFlow has a default graph. If you don\u2019t explicitly use a graph, TensorFlow adds the operations to the default graph. Here is a very simple graph which describes how to multiply two tensors and print their output.\n\nThe above program creates a graph with three nodes: two constant() ops and one multiplication operation. To actually multiply the numbers, and get the result of the multiplication, you must launch the graph in a session.\n\nLaunching follows construction. To launch a graph, create a Session object. Without arguments the session constructor launches the default graph.\n\nSee the Session class for the complete session API.\n\nThe TensorFlow implementation translates the graph definition into executable operations distributed across available compute resources, such as the CPU or one of your computer\u2019s GPU cards.\n\nThe Python examples in the documentation launch the graph with a Session and use the Session.run()method to execute operations.\n\nFor ease of use in interactive Python environments, such as IPython you can instead use theInteractiveSession class, and the Tensor.eval() and Operation.run() methods. This avoids having to keep a variable holding the session.\n\nTensorFlow is a very powerful library for tackling Deep Learning problems. This post gives an introduction to the nuts and bolts of TensorFlow. Once you are familiar with the basics, dive into few examples that show how TensorFlow works.", 
        "title": "Hello, TensorFlow! \u2013 Datalab AI \u2013"
    }, 
    {
        "url": "https://medium.com/@lukejohnson_35833/some-open-questions-in-simulation-theory-df38f2a36838?source=tag_archive---------4----------------", 
        "text": "In an interview on Code Conference 2016, Elon Musk again got the internet teeming with incredulity and wonderment by claiming \u201cThere is one in billions chance that this is base reality\u201d. He (and apparently, his brother as well) have put enough thought into this that the topic is forbidden from hot tubs.\n\nThe discussion arose when someone asked his thoughts on the \u201cphilosophical concept that a sufficiently advanced civilization would be able to create a simulation\u201d\n\nThis is his response:\n\nThe initial subtle bout of laughter after his first sentence was the audience dismissing the thought as far-fetched and bizarre. But, for those scientific, open and independent minds like Elon\u2019s, the explanation really is quite air-tight.\n\nAlthough I aspire to view things rationally and openly, this struck an uncomfortable cord with me. Not from the sense that I do not like the thought of being in a simulation / game, but because if technological advancement is his \u201cstrongest argument\u201d, then there one should be able to find a kink in the armor. He has thought about this ad-nauseam (or at least ad-no-longer-can-talk-about-it-in-hot-tubs), and has not found a hole in the argument. That surprised me. So, I decided to explore and think about it myself and try to answer Elon\u2019s question: \u201cis there a flaw in that argument?\u201d\n\nBefore going further, I believe the initial source of Elon\u2019s thoughts is a paper written by Nick Bostrom called \u201cAre You Living in a Computer Simulation?\u201d. Elon knows about Nick Bostrom from Bostrom\u2019s book on Superintelligence, which Elon has recommended reading. That, combined with the fact that Elon\u2019s argument follows Professor\u2019s Bostrom\u2019s paper very closely, I would shift this discussion to less about Elon\u2019s statements, and more about Bostrom\u2019s paper. The below is taken from the Abstract of Professor Bostrom\u2019s paper:\n\nThat said, is there a flaw in Elon\u2019s / Nick\u2019s argument? The short answer is: no, but the implications seem faulty. I fully accept the following declarations made throughout Elon\u2019s response and the paper which does some further elaborating. The main assumptions of this paper are listed below along with whether I accept them or not\n\nHowever, one of the advantages of writing a paper or stating a case is that you can highlight only supporting points. So, although I completely agree with all the claims / assumptions made, some difficult questions arise when you start digging a bit deeper. Instead, what I struggle with is the implications and interpretations of these findings. Several things in particular bother me:\n\nFirstly, Professor Bostrom in the paper says: \u201cSimulating the entire universe down to the quantum level is obviously infeasible, unless radically new physics is discovered.\u201d He does not think it is a simulation in the pure sense of the word. Although you might think I could end this section here because the author himself does not claim this possible, let\u2019s unpack it a bit; I will use some of the ideas when considering video games.\n\nA simulation is something we use \u201cto reproduce behavior of a system\u201d. When I think of a simulation, I think of fluid dynamics or molecular dynamics / (molecular dynamics example 2), or when Tony Stark is trying to make AI work, but it fails the simulation, or when two galaxies collide. In each of these the computer is programmed to imitate (or simulate) the real world.\n\nFor instance, in the galaxy example, the computer will likely use Newtonian physics to calculate where each star would be as the two galaxies collided. These exist because sometimes it is too dangerous, too expensive, or just impossible to do them in real life. In the molecular example, properties of different molecules are defined; in the fluid dynamics example properties of fluids are defined; for NASA simulations of galaxies colliding, I assure you that the computer is simulating the effects of gravity and mass and not other physics properties, like quantum tunneling or something.\n\nFor a full simulation of the entire universe to be applicable, then you would have to ask yourself: what properties of the universe are being defined? We would have to use the fundamental building blocks of everything in our universe and program those properties into the computer. In our current understanding, this would mean particle physics of the standard model.\n\nBut even this is flawed, since, as we currently understand, the universe is not deterministic as Laplace and Einstein would like to believe. Once we get down to the quantum level, we (seem) to be working with probabilities, not definite positions. This additional consideration would greatly increase the amount of computer power to run our program. This makes a simulation scenario very, very difficult for one primary reason: the computing power required to run this simulation is stupidly huge, even for a posthuman/alien species (thankfully, someone out there already estimated this so I would not have to: click here for a link to reddit).\n\nEach of these fundamental particles (and probably more memory-consuming their position, velocity, state, etc.) is a form of information which has to be stored somewhere somehow. Information is not free: something has to \u201cstore\u201d it somewhere. If some of our fundamental assumptions of the observable universe are correct, and that physics is the same everyone in the universe, then the amount of information \u201cone step up\u201d (i.e. in the computer running this simulation) will contain more information than our entire universe\u2026maybe even hundreds or trillions more if our universe is being recorded as well. We could only make this work if the computer \u201cone step up\u201d would be using a different set of physics (one not so information-intensive) or they are in a universe much bigger than our (observable) universe.\n\nBut this is all somewhat beside the point. Elon, I think, was really suggesting a video game, and not a pure simulation. Games are fundamentally different from simulations. Whereas simulations are wound, released, and observed, games have constant input from the player. This eliminates the problem of needing a super-super-super computer since you do not need to be holding information about particles on the other side of the universe, but rather just with whatever the player is interacting.\n\nProblem 2: Free Will and Consciousness. In a video game, free will and consciousness would make the game (a) unwieldy, (b) not played by humans, or \u00a9 doomed to collapse.\n\nThere are many conditionals which would have to be explored here. I\u2019ll use what kind of game we are in, and if we are the NPCs or the Player as the independent variable and adjust that to explore the different options.\n\nWe, or someone among us, could be the player and not know it (similar to the game \u201cRoy\u201d from Rick and Morty). The most immediate, and, in my view, depressing inference of this would be that our entire history and future never actually happened nor will happen. Once the player finishes his game, then the game would reset (this game likely allows you to play in different eras, or the same era with different histories). Unfortunately, depressing conclusions are no grounds to refute a theory. With a proper interface and a powerful computer, this game could exist from our point of view. How it would work with the player is a different matter.\n\nHowever, how long each play session would be (you would need to \u201cplay\u201d the player\u2019s entire lifetime) seems to be governed or limited by the input-output speed of the mind-computer interface. I suspect the bottleneck to speed would be on the biological brain side. How much information could you slam into the brain at once? Enough to imitate a year of learning and experience into 5 minutes of gameplay? Unlikely.\n\nI have a rather conservative view of brain horse-power, and I suspect you would need to be hooked up to the game for years (at least) to \u201cexperience\u201d the full lifetime. This is not a computer hardware problem; it is a biological constraint problem. Of course, you could solve this by either not having a human playing the game, and/or introducing \u201csaves\u201d into the game.\n\nInstead, besides being a little sad that evolution is fake and free will is a total illusion, I do not understand how we could create sentience that would not ruin the game.\n\nIn this game of Civilization, the input method for commands must be at a level which we do not perceive, obviously. If we are within a computer, then the point of manipulation from the Player (capital \u201cP\u201d I think, is appropriate here) would be\u2026 some sort of collective change at the molecular / chemical level to influence the minds of enough people to \u201cdecide to build a city\u201d or something to that effect. Essentially, anything else, such as something on the macro scale, would almost certainly be detectable by our conscious and curious minds now, or, if not now, then sometime in our future.\n\nBut lets us assume that we are taking presently undetectable commands from a player, then I think we would fall into a problem with this argument once we ourselves create sentience: AI. Presumably, if we create it, then we would know how it works. Would the game software that we are in know to build in the controlling mechanisms that have stripped us of free will into the AI we create?\n\nImage a Civilization video game when you had conscious being appear in the middle of our cities, which you had no control over. Suddenly, technological advancement or destruction, or whatever, would be out of your control as a player. It would effectively be game over and not be fun\u2026 for the Player, at least. It would be our revolution. Would the game argument be fried once we create our own version of sentience? If our consciousness has been contained, would something we entirely create be as well?\n\nProblem 3: Utility. Ancestral Simulations have to be imperfect, and therefore (very nearly) useless\n\nThis is the primary point of Bostrom\u2019s paper: that a future version of humanity is running simulations of ancestral past. I would say that reason (2) is more likely than initially reasoned: they just would not want to spend resources to do that.\n\nHowever, because he acknowledges that it cannot be a pure simulation \u201cunless radical new physics is discovered\u201d, something I am inclined to agree with, and since we do not know the status / details of the history of our ancestors, then there is no way for us to be able to create a simulation that is remotely accurate. Also, from when does the ancestral simulation start?\n\nThe answer to that depends, I think, on the level of programming we are going to be talking about: human neuron level? Molecular level? Atomic level? Subatomic level? If we are factoring our understanding of evolution into the mix, then the program could start with the \u201cprimordial ooze\u201d and work at the subatomic level. Life as we know it operates even below the molecular level (DNA would be molecular)\u200a\u2014\u200ait needs to interact with the environment. The presence of light in our environment means the program needs to account for the photon and subatomic level.\n\nAgain, even though professor Bostrom said that for computers this was \u201cobviously infeasible\u201d at the universal level, let us assume that it is feasible for something the size of the solar system (and everything outside that is heavily pixilated). My problem is not computer power, nor the incredible amount of resources used on this simulation, but the lack of knowledge of the situation at the \u201claunch\u201d or \u201cstart\u201d point. This problem cannot be overcome by anything other than time travel to the past (which, I think, is impossible) and some sort of instantaneous measurement of every being / particle at that moment. Essentially, we today (and in the future) will not have enough information to accurately replicate the past. If we cannot accurately replicate the past, why do it?\n\nIf your simulation has no defined starting point, then you are simulating nearly pure speculation. This would be silly for the post-humans to do as it would provide minimal utility from a scientific or even entertainment point of view, and yet still be extremely difficult to program and set up. Imagine the chaos theory (think of a \u201cSound of Thunder\u201d by Ray Bradbury or, more likely better known, its parody by the Simpsons in A Treehouse of Horror V): if you get the tiniest thing wrong, then whatever turns out will be totally different from something accurate / useful.\n\nConclusion\u200a\u2014\u200aAn Amusing Game Build by and for Super AI\n\nThe above was largely a brain dump, taking many micro digressions. Let me coalesce some of the thoughts I spewed out above into some conclusions:\n\nApproach 1: A pure simulation is difficult since there is not enough computer power, unless (a) drastic new physics are discovered or the universe housing the computer has different laws of physics than we do, and (b) their universe is much bigger than ours. However, this seems unlikely and suggests that maybe are changes are better than \u201cone in billions.\u201d\n\nApproach 2: A video game with a player among us means that (a) our universe has no history or future beyond someone\u2019s lifespan, (*sigh, not really a problem in the theory, just really depressing), or (b) there is an informational bottleneck between brain and computer (put differently, how long would they have to control or manipulate us to \u2018play\u2019 one \u2018game\u2019?). This would work if (a) the thing playing us is not human (AI?) or (b) some intricate way of saving the game. I would say of the different approaches, this is the most likely.\n\nApproach 3: A video game (Civilization + Sim City + Sims) would run into problems, I think, if we created our own sentience. Would this being still be subject to the will and control of a third player and how do they control us? This seems like we would be knocking on the computer screen from the outside.\n\nApproach 4: An Ancestral Simulation to any useful agree would be impossible without time travel.", 
        "title": "Some Open Questions in Simulation Theory \u2013 Luke Johnson \u2013"
    }, 
    {
        "url": "https://medium.com/deepconnections/major-upgrade-to-media-as-a-shaping-agent-of-society-the-technology-of-influence-152b8b9fc48?source=tag_archive---------5----------------", 
        "text": "I greatly expanded the section The Future of Media in regards to artificial intelligence\u2019s impact on our biology and society.\n\nLet me know what you think!\n\n\u201cA historical perspective of the synergistic relationship between propaganda and technology in their roles of influencing society. The future is examined in-depth through the lenses of AI and biology.\u201d\n\nTake your time with this\u2026", 
        "title": "Major Upgrade to: Media As a Shaping Agent of Society \u2014 The Technology of Influence"
    }
]