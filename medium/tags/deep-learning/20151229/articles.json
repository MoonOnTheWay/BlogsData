[
    {
        "url": "https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa?source=tag_archive---------0----------------", 
        "text": "This week I implemented highway networks to get an intuition for how they work. Highway networks, inspired by LSTMs, are a method of constructing networks with hundreds, even thousands, of layers. Let\u2019s see how we construct them using TensorFlow.\n\nFor comparison, let\u2019s start with a standard fully-connected (or \u201cdense\u201d) layer. We need a weight matrix and a bias vector then we\u2019ll compute the following for the layer output:\n\nHere\u2019s what a dense layer looks like as a graph in TensorBoard:\n\nFor the highway layer what we want are two \u201cgates\u201d that control the flow of information. The \u201ctransform\u201d gate controls how much of the activation we pass through and the \u201ccarry\u201d gate controls how much of the unmodified input we pass through. Otherwise, the layer largely resembles a dense layer with a few additions:\n\nWhat happens is that when the transform gate is 1, we pass through our activation (H) and suppress the carry gate (since it will be 0). When the carry gate is 1, we pass through the unmodified input (x), while the activation is suppressed.\n\nHere\u2019s what the highway layer graph looks in TensorBoard:\n\nUsing a highway layer in a network is also straightforward. One detail to keep in mind is that consecutive highway layers must be the same size but you can use fully-connected layers to change dimensionality. This becomes especially complicated in convolutional layers where each layer can change the output dimensions. We can use padding (\u2018SAME\u2019) to maintain each layers dimensionality.\n\nOtherwise, by simply using hyperparameters from the TensorFlow docs (i.e. no hyperparameter search) the fully-connected highway network performed much better than a fully-connected network. Using MNIST as my simple trial:\n\nNow that we have a highway network, I wanted to answer a few questions that came up for me while reading the paper. For instance, how deep will the network converge? The paper briefly mentions 1000 layers:\n\nCan we train with 1000 layers on MNIST?\n\nYes, also reaching around 95% accuracy. Try it out with a carry bias around -20.0 for MNIST (from the paper the network will only utilize ~15 layers anyway). The network can probably even go deeper since the it\u2019s just learning to carry the last 980 layers or so. We can\u2019t do much useful at or past 1000 layers so that seems sufficient for now.\n\nWhat happens if you set very low or very high carry biases?\n\nIn either extreme the network simply fails to converge in a reasonable amount of time. In the case of low biases (more positive), the network starts as if the carry gates aren\u2019t present at all. In the case of high biases (more negative), we\u2019re putting more emphasis on carrying and the network can take a long time to overcome that. Otherwise, the biases don\u2019t seem to need to be exact, at least on this simple example. When in doubt start with high biases (more negative) since it\u2019s easier to learn to overcome carrying than without carry gates (which is just a plain network).\n\nOverall I was happy with how easy highway networks were to implement. They\u2019re fully differentiable with only a single additional hyperparameter for the initial carry bias. One downside is that highway layers do require additional parameters for the transform weights and biases. However, since we can go deeper, the layers do not need to be as wide which can compensate.\n\nHere\u2019s are the complete notebooks if you want to play with the code: fully-connected highway repo and convolutional highway repo.\n\nFollow me on Twitter for more posts like these. If you\u2019d like building very deep networks in production, I do consulting.", 
        "title": "Highway Networks with TensorFlow \u2013 Jim Fleming \u2013"
    }
]