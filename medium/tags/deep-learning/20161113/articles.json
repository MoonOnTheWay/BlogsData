[
    {
        "url": "https://medium.com/intuitionmachine/11-biases-why-experts-are-missing-the-train-in-deep-learning-1c092fa9cace?source=tag_archive---------0----------------", 
        "text": "I spend most of my waking time ( and likely my subconscious works overtime while I sleep ) studying Deep Learning. Peter Thiel has a phrase, \u201cThe Last Company Advantage\u201d[THI.] Basically you don\u2019t necessarily need to have the \u201cFirst Mover Advantage\u201d however you absolutely want to be the last company standing in your kind of business. So Google may be the last Search company, Amazon may be the last E-Commerce company and Facebook hopefully will not be the last Social Networking company. What keeps me awake at night though is that Deep Learning could in fact be the \u201cLast Invention of Man\u201d!\n\nHowever, let\u2019s ratchet it down a little bit here. After all, Kurzweil\u2019s Singularity (estimate is 2045) is still 3 decades away. That\u2019s still plenty of time for us humans to scheme on our little monopolies. Your objective in the next 30 years of humankind is to figure out if you are going to be living in Elysium or in some unnamed decaying backwater:\n\nCredit: Elysium the movie, not the life-extension supplement.\n\nTo aid you in your decision making, here are 11 reasons why your \u201cexperts\u201d will lead you to miss the all important Deep Learning revolution:\n\nPractitioner\u2019s introduction to neural networks are almost always via the introduction of linear regression and then to logistic regression. That\u2019s because the mathematical equations for an artificial neural network (ANN) are identical. So there immediately is a bias here that the characteristics of these classical ML methods would also convey into the world of DL. After all, DL in its most naive explanation is nothing more than multiple layers of ANN.\n\nThere are also other kinds of ML methods that have equations that are different from DL. The basic objective however for all ML methods is a general notion of curve fitting. That is if you can have a good fit of a model with the data then that perhaps is a good solution. Unfortunately with DL systems, due to the fact that the number of parameters in the model are so large, these systems by default will over-fit any data. This is enough of a tell that a DL is an entirely different kind of animal from an ML system.\n\nDL systems have a loss function that is a measure of how well its predictions match its input data. Classic optimization problems also have loss functions (also known as objective functions). In both systems, different kinds of heuristics are used to discover an optimal point in a large configuration space. It was once thought that the solution surface of a DL system was sufficiently complex enough that it would be impossible to arrive at a solution. However, curiously enough, one of the most simple methods of optimization, the Stochastic Gradient Descent algorithm, is all that is need to arrive at surprising results.\n\nWhat this tells you is that is something else going on here that is actually very different from what optimization folks are used to.\n\nA lot of Data Scientists have an aversion for DL because of the lack of interpretability of its predictions. This is a characteristic of not only DL methods but classical ML methods as well. Data Scientists would rather use Probabilistic methods where they can have better control of the models or priors. As a result have systems that are able to make predictions with the least number of parameters. All driven by the belief that parsimony or Occam\u2019s razor is the optimal explanation for everything.\n\nUnfortunately, probabilistic methods are not competitive in classifying images, speech or even text. That\u2019s because DL methods are superior in discovering models than human beings. Brute force just happens to trump wetware. No Data Scientist has ever been able to find the \u2018principal components\u2019 that will do image classification well. Furthermore, there\u2019s no experimental evidence in the DL space that parsimonious models work any better than entangled models. For those cases where it is an absolute requirement to have some kind of explanation, there are now newer methods in DL that provide aid to interpretability as well as uncertainty. If a DL system can generate the captions in an image, then there is a good chance that it can be trained to generate an explanation of a prediction.\n\nThis is a natural bias that something that is around 5 years old and rapidly evolving is too new and volatile a technology to trust. I think we all said the same thing when the microprocessor, internet, web, mobile technologies came along. Wait and see was the safe approach for most everyone. This is certainly a reasonable approach for anyone who has not really spent the time investigating the details. However, it is a very risky strategy, ignorance may be bliss but another company eating your lunch can mean extinction.\n\nThere are a lot of things that DL can do that were deemed inconceivable just a couple years ago. Nobody expected a computer to beat the best human player in Go. Nobody expected self-driving cars to exist today. Nobody expected to see Star Trek universal translator like capabilities. It is so unbelievable that it must likely be an exaggeration than something that may be real. I hate however to burst your bubble of ignorance, DL is in fact very real and you experience it yourself with every smartphone.\n\nWe\u2019ve had so many times where the promise of AI had lead to disappointing results. The argument goes further that because it has happened so often before, that it is also bound to happen again. The problem with this argument is that despite the disappointment, AI research has led to many software capabilities that we do take for granted today and thus never notice its existence. Good old fashioned AI (GOFAI) are embedded in many systems today.\n\nThe current pace of DL development is accelerating and there are certainly certain big problems that need to be solved. The need for a lot of training data and the lack of unsupervised training are two problems. This however doesn\u2019t mean that what we have today has no value. DL can already drive cars, that in itself tells you that even if another AI winter arrives, we would have achieved a state of development that is still quite useful. Andrew Ng has more about this [NG].\n\nThe research community does not have a solid theoretical understanding as to why DL works so effectively. We have some idea as to why a multi-layer neural network is more efficient in fitting functions than one with fewer layers. We, however, don\u2019t have an understanding as to why convergence even occurs or why good generalization happens. DL at this time is very experimental and we are just learning to characterize these kinds of systems. Meanwhile, despite not having a good theoretical understanding, the engineering barrels forward. Researchers, using their intuition and educated guesses are able to build exceedingly better models. In other words, nobody is stopping their work to wait for a better theory. It is almost analogous with what happens in biotechnology research. People are experimenting with many different combinations and arriving at new discoveries that they have yet to explain. Scientific and technological progress is very messy and one shouldn\u2019t shy away from the benefits because of the chaos.\n\nDL system are very unlike the neurons in our brain. The mechanism of how DL learns (i.e. SGD) is not something we can explain happening in our brain. The argument here though is that if it doesn\u2019t resemble the brain then it is unlike to be able to perform the kind of inference and learning of a brain. This, of course, is an extremely weak argument. After all, planes don\u2019t look like birds, but they certainly can fly.\n\nNot having expertise in-house shouldn\u2019t be an excuse for avoiding finding expertise outside. Furthermore, should prevent you from having your experts learn this new technology. However, if these experts are of the dogmatic persuasion, then that should be a tell for you to get a second and unbiased opinion.\n\nBusinesses are composed of many business processes. Unless you have not gone through the exercise of examining which processes can be automated with current DL technologies, then you are not in a position to make the statement that DL does not apply to you. Furthermore, you may discover new processes and business opportunities may not exist today but are possible with the exploitation of DL technology. You cannot really answer this question until you invested in some due diligence work.\n\nThe large internet companies like Google and Facebook have gobbled up a lot of the Deep Learning talent out there. These companies have very little interest in working with a small business to identify their specific needs and opportunities. However, fortunately, these big companies have been gracious enough to allow their researchers to publish their work. We, therefore, do have a view into their latest developments and thus are able to take what they\u2019ve learned and apply it to your context. There are companies like Intuition Machine that do have an on-boarding process for you to get a competitive head start in DL technologies.\n\nIf you want a first mover advantage, then urgently reach out to Intuition Machine. They\u2019ve got a Deep Learning guide to turn your business into one that can be potentially disruptive.", 
        "title": "11 Arguments Experts get Wrong about Deep Learning \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/@ilhamadun/pembelajaran-mesin-dan-deep-learning-5c86c7cf77b9?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Pembelajaran Mesin dan Deep Learning \u2013 Ilham Imaduddin \u2013"
    }, 
    {
        "url": "https://medium.com/awesome-humans/grant-gochnauer-awesome-humans-issue-64-57b9474e0e50?source=tag_archive---------3----------------", 
        "text": "Carl Sagan on Moving Beyond Us vs. Them, Bridging Conviction with Compassion, and Meeting Ignorance with Kindness\u200a\u2014\u200awww.brainpickings.org\u00a0\n\n\u00a0\u201cIn the course of looking deeply within ourselves, we may challenge notions that give comfort before the terrors of the world.\u201d\n\nJohn Donahoe: Dump the Myth of the High Achiever\u200a\u2014\u200awww.gsb.stanford.edu\u00a0\n\n\u00a0The former CEO of eBay encourages students to build a life outside of work and find creative solutions to problems.\n\nWhy Vulnerability Is So Important\u200a\u2014\u200alifehacker.com\u00a0\n\n\u00a0You know how important it is to break out of your comfort zone, say yes more often, and allow yourself to be vulnerable. But what if that vulnerability makes you so anxious you find it hard to function? The key is learning to embrace vulnerability without allowing it to take over.\n\nIntroduction to Deep Learning 2016\u200a\u2014\u200ablog.algorithmia.com\u00a0\n\n\u00a0Companies are turning to deep learning to solve hard problems, like speech recognition, object recognition, and machine translation.\n\nU.S. Military Tests Brain Stimulation to Sharpen Mental Skills\u200a\u2014\u200awww.smithsonianmag.com\u00a0\n\n\u00a0Could electrodes one day replace pill bottles in the theatre of war?\n\nwww.washingtonpost.com\u00a0\n\n\u00a0Scientists have developed a wireless device that allowed monkeys with spinal cord injuries to move their legs via a signal from their own brains.\n\nLeaked NASA paper shows the \u2018impossible\u2019 EM Drive really does work\u200a\u2014\u200awww.sciencealert.com\u00a0\n\n\u00a0Did we just achieve fuel-less propulsion?\n\nInside Magic Leap, The Secretive $4.5 Billion Startup Changing Computing Forever\u200a\u2014\u200awww.forbes.com\u00a0\n\n\u00a0Top-secret startup Magic Leap has raised a record-breaking amount of money in pursuit of a radical concept: blending the digital and the physical.\n\nThe First Drug Breakthrough in the Race to Cure Alzheimer\u2019s By 2025\u200a\u2014\u200abigthink.com\u00a0\n\n\u00a0This drug combined with antibody therapies could prevent or even cure the neurodegenerative disorder.", 
        "title": "Grant Gochnauer: Awesome Humans \u2014 Issue #64 \u2013 Awesome Humans \u2013"
    }, 
    {
        "url": "https://medium.com/@hellobly/why-chatbots-are-the-last-bridge-to-true-ai-2793a7d596e7?source=tag_archive---------4----------------", 
        "text": "Humans have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in 3000 BCE. Since then, we have continuously developed more and more sophisticated means to communicate and push information. Whether unconsciously or consciously, we seem to always need more data, faster than ever. And with every technological breakthrough that comes along, we also have a set of new concepts that reshape our world.\n\nWe can think back, for example, to Gutenberg\u2019s printing press. Invented in 1440, it pushed printing costs down and gave birth to revolutionary concepts like catalogs (the first was published in 1495 in Venice by publisher Aldus Manutiu and listed all the books that he was printing), mass media (which enabled revolutionary ideas to transcend borders), magazines, newspapers, and so on. All these concepts emerged from a single \u201cmaster\u201d technology breakthrough and have had a great impact on every single aspect of individuals\u2019 lives and the global world picture.\n\nA hundred years later, the core idea of data distribution has not changed much. We still browse catalogs to buy our next pair of shoes, we create catalogs to sell our products and services, and we still browse publications looking for information.\n\nThis question may sound silly, but technology and the quantity of data produced are intrinsically related. The more technology advances, the more data quantity increases, and the more we need new, advanced technologies to process this data and extract valuable meaning from it. Data has three primary uses:\n\n1) It can be used for \u201caction,\u201d such as optimizing the distribution of electricity.\n\n2) It can be used to facilitate social change, such as through donations and online voting.\n\n3) It can be used for innovation, such as helping us understand climate change or molecules, and building and testing new theories.\n\nThe ratio of data quantity produced versus data impact is an important indicator in innovation\u2019s engine speed.\n\nUntil the beginning of 21st century, the ratio of human data gathered vs. data impact (that is, social change, innovation, etc.) was very high. We are where we are thanks to our brains\u2019 capacity to imagine and build models for the data we\u2019ve collected and to draw lessons from that data. But we long ago reached a \u201cconsumption saturation\u201d point, whereby the amount of data is increasing so exponentially that the old way of accessing data has become inefficient and obsolete. For example, in today\u2019s super interconnected world, when people from virtually every culture and language are putting valuable information on the internet, we still rely on keywords searches to find information. This is a rudimentary approach. Instead, we could be using an AI-powered search engine that gives us an answer after considering all the available sources and languages.\n\nData access and sharing technologies are the most important fuel of the innovation engine.\n\nDon\u2019t get me wrong; we are still innovating and using data in various ways. But the ratio of data quantity produced to data impact is really small because datasets are getting more and more complex and need more and more intelligence to process them.\n\nTechnology obviously influences the way we interact, how social circles are formed, how we make decisions, and even the way we think, but it has not changed our core nature. We are still social beings, and with all of today\u2019s technologies, word of mouth still plays an important role in our lives. We still ask our friends for advice and recommendations and ask our friends for help when making important decisions in our lives. It\u2019s no wonder that the most successful products out there are the most social ones.\n\nWe have always tried to \u201chumanize technology,\u201d and future breakthroughs in AI will go a step further to radically change the way we access and share information. We are truly going to experience AI-powered social technologies everywhere. And, for the first time, everyone will have a smart AI-powered friend that understands them, memorizes their habits and preferences, gathers meaning from the petabytes of data available on the web, gives advice, and so on. For the first time, data access will be social and efficient.\n\nSure, we haven\u2019t developed \u201cHAL 9000\u201d or \u201cHer\u201d yet, but in the meantime, the recent progress made in machine learning and the democratization of information\u200a\u2014\u200acombined with the explosion of messaging\u200a\u2014\u200ahas already given us the possibility of throwing most of the current apps in the trash and starting to develop real human interactions via chatbots.\n\nChatbots are an old concept. Messaging apps are also an old concept. So why are chatbots rising now?\n\nIf we really delve into the reasons websites replaced client applications and apps replaced websites and we look at why messaging bots can and will replace mobile apps, we will find that users\u2019 behavior changes when the technology advances significantly and users reach a saturation point in terms of usage. For client applications, it was the widespread availability of the internet and the need to get things done anywhere and everywhere. For websites, it was the smartphone revolution and the need to get things done on the go. For apps now, it is the advances in AI and the need to get everything done immediately. Users no longer tolerate the hassle of finding and downloading an app on the go; it is usually a waste of data, patience, and time. And no matter how great your app\u2019s UI/UX is, there will always be users who simply don\u2019t understand it and get frustrated because the process isn\u2019t natural. With chatbots, you don\u2019t have to download anything. It\u2019s a more natural experience and you can go straight to your goal.\n\nCurrent limitations in natural language processing and bad implementation have caused some critics to speak out against chatbots. But like any new concept, it takes time to define standards and frameworks (technical and conversational) and put best practices in place. But this is it, we are in the chatbot era. I believe that every era has a purpose, and the purpose of the chatbot era is to get people comfortable interacting with a virtual entity as a way to get things done. That\u2019s it.\n\nThis will be, perhaps, the last stop before the arrival of true AI.", 
        "title": "Why chatbots are the last bridge to true AI \u2013 Bly \u2013"
    }
]