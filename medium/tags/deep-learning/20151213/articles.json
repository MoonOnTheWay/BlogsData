[
    {
        "url": "https://gab41.lab41.org/taking-keras-to-the-zoo-9a76243152cb?source=tag_archive---------0----------------", 
        "text": "If you follow any of the popular blogs like Google\u2019s research, FastML, Smola\u2019s Adventures in Data Land, or one of the indie-pop ones like Edwin Chen\u2019s blog, you\u2019ve probably also used ModelZoo. Actually, if you\u2019re like our boss, you affectionately call it \u201cThe Zoo\u201d. (Actually x 2, if you have interesting blogs that you read, feel free to let us know!)\n\nUnfortunately, ModelZoo is only supported in Caffe. Fortunately, we\u2019ve taken a look at the difference between the kernels in Keras, Theano, and Caffe for you, and after reading this blog, you\u2019ll be able to load models from ModelZoo into any of your favorite Python tools.\n\nWhy this post? Why not just download our Github code?\n\nIn short, it\u2019s better you figure out how these things work before you use them. That way, you\u2019re better armed to use the latest TensorFlow and Neon toolboxes if you\u2019re prototyping and transitioning your code to Caffe.\n\nSo, there\u2019s Hinton\u2019s Dropout and then there\u2019s Caffe\u2019s Dropout\u2026and they\u2019re different. You might be wondering, \u201cWhat\u2019s the big deal?\u201d Well sir, I have a name of a guy for you, and it\u2019s Willy\u2026Mr. Willy Nilly. One thing Willy Nilly likes is the number 4096. Another thing he likes is to introduce regularization (which includes Dropout) arbitrarily, and Bayesian theorists aren\u2019t a fan. Those people try to fit their work into the probabilistic framework, and they\u2019re trying to hold onto what semblance of theoretical bounds exist for neural networks. However, for you as a practitioner, understanding who\u2019s doing what will save you hours of debugging code.\n\nWe singled out Dropout because the way people have implemented it spans the gamut. There\u2019s actually some history as to this variation, but no one really cared, because optimizing for it has almost universally produced similar results. Much of the discussion stems from how the chain rule is implemented since randomly throwing stuff away is apparently not really a differentiable operation. Passing gradients back (i.e., backpropagation) is a fun thing to do; there\u2019s a \u201ctechnically right\u201d way to do it, and then there\u2019s what\u2019s works.\n\nBack to ModelZoo, where we\u2019d recommend you note the only sentence of any substance in this section, and the sentence is as follows. While Keras and perhaps other packages multiply the gradients by the retention probability at inference time, Caffe does not. That is to say, if you have a dropout level of 0.2, your retention probability is 0.8, and at inference time, Keras will scale the output of your prediction by 0.8. So, download the ModelZoo *.caffemodels, but know that deploying them on Caffe will produce non-scaled results, whereas Keras will.\n\nHinton explains the reason why you need to scale, and the intuition is as follows. If you\u2019ve only got a portion of your signal seeping through to the next layer during training, you should scale the expectation of what the energy of your final result should be. Seems like a weird thing to care about, right? The argument that minimizes x is still the same as the argument that minimizes 2x. This turns out to be a problem when you\u2019re passing multiple gradients back and don\u2019t implement your layers uniformly. Caffe works in instances like Siamese Networks or Bilinear Networks, but should you scale your networks on two sides differently, don\u2019t be surprised if you\u2019re getting unexpected results.\n\nWhat does this look like in Francois\u2019s code? Look at the \u201cDropout\u201d code on Github, or in your installation folder under keras/layers/core.py. If you want to make your own layer for loading in the Dropout module, just comment out the part of the code that does this scaling:\n\nYou can modify the original code, or you can create your own custom layer. (We\u2019ve opted to keep our installation of Keras clean and just implemented a new class that extended MaskedLayer.) BTW, you should be careful in your use of Dropout. Our experience with them is that they regularize okay, but could contribute to vanishing gradients really quickly.\n\nEveryday except for Sunday and some holidays, a select few machine learning professors and some signal processing leaders meet in an undisclosed location in the early hours of the morning. The topic of their discussion is almost universally, \u201cHow do we get researchers and deep learning practitioners to code bugs into their programs?\u201d One of the conclusions a while back was that the definition of convolution and dense matrix multiplication (or cross-correlation) should be exactly opposite of each other. That way, when people are building algorithms that call themselves \u201cConvolutional Neural Networks\u201d, no one will know which implementation is actually being used for the convolution portion itself.\n\nFor those who don\u2019t know, convolutions and sweeping matrix multiplication across an array of data, differ in that convolutions will be flipped before being slid across the array. From Wikipedia, the definition is:\n\nOn the other hand, if you\u2019re sweeping matrix multiplications across the array of data, you\u2019re essentially doing cross-correlation, which on Wikipedia, looks like:\n\nLike we said, the only difference is that darned minus/plus sign, which caused us some headache.\n\nWe happen to know that Theano and Caffe follow different philosophies. Once again, Caffe doesn\u2019t bother with pleasantries and straight up codes efficient matrix multiplies. To load models from ModelZoo into either Keras and Theano will require the transformation because they strictly follow the definition of convolution. The easy fix is to flip it yourself when you\u2019re loading the weights into your model. For 2D convolution, this looks like:\n\nHere, the variable \u201cweights\u201d will be inserted into your model\u2019s parameters. You can set weights by indexing into the model. For example, say I want to set the 9th layer\u2019s weights. I would type:\n\nIncidentally, and this is important, when loading any *.caffemodel into Python, you may have to transpose it in order to use it. You can quickly find this out by loading it if you get an error, but we thought it worth noting.\n\nAlright, alright, we know what you\u2019re really here for; just getting the code and running with it. So, we\u2019ve got some example code that classifies using Keras and the VGG net from the web at our Git (see the link below). But, let\u2019s go through it just a bit. Here\u2019s a step by step account of what you need to do to use the VGG caffe model.\n\nAnd now you have the basics! Go ahead and take a look at our Github for some goodies. Let us know!", 
        "title": "Taking Keras to the Zoo \u2013"
    }, 
    {
        "url": "https://medium.com/@muhammadabduh/deep-learning-phenomena-it-just-happened-bebdd1dab9b2?source=tag_archive---------1----------------", 
        "text": "A few days ago, I was surprised to know that Facebook has opened its AI hardware called Big Sur. It was not so long after Google released its open source library for deep learning called Tensor Flow. But the most astonishing news regarding deep learning is just two days ago Elon Musk and his colleagues established a new non-profit company called OpenAI. It is like these guys are really serious in competing and proving who is the most intelligent company in this world. Just a brief explanation, deep learning is more or less a method to built an artificial intelligence agent. So, don\u2019t be puzzled with the different terms in this text.\n\nSo, why this trend is happening right now?\n\nTwo days ago I discussed this phenomena with my seminar\u2019s supervisor and he gave me an interesting insight about it.\n\nLet\u2019s go back in time. A long time ago, research about machine learning or artificial intelligence has already started. We can divide this research into generative-model based and discriminative-model based. The simple difference between them is generative model can generate a new data point in the process, but the discriminative model does not. I won\u2019t explain the details about two models here, instead I will just give the example of both. We can see the discriminative-model is like a Support Vector Machine and generative-model is like Neural Network (Deep Beliefs Nets or Deep learning or whatever).\n\nBack then, many researchers were stuck with the research in generative-model and many of them just thought that this generative-model would not work. So, they changed their research focus into discriminative-model, improving it, try to construct a new method, and so on.\n\nIn 2006, an ingenious man published his research regarding generative-model and it was about how to do fast learning using Deep Beliefs Nets (We call it deep learning now). He found the reason why people who did research in generative-model always got unsatisfied result. His name is Geoffrey Hinton, who opened a closed door for human kind to make artificial intelligence more feasible. His research was a game changing and at that time everybody just gave their attention to Deep Learning area until now. With the support of Big Data and increasing processing speed as well as storage, Deep Learning becomes more demanding. Many new companies in the field of AI are established and many big companies also take apart in it.\n\nFrom now on, let\u2019s see how far this deep learning could do to enable a new technology for us.\n\nI am not an expert in this deep learning things and just want to share some of my thoughts. There might be some inaccurate informations in this text. But, I hope you can understand the point that I want to share.", 
        "title": "Deep Learning Phenomena. It Just Happened. \u2013 Muhammad Abduh \u2013"
    }
]