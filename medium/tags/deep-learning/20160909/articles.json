[
    {
        "url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-5-visualizing-an-agents-thoughts-and-actions-4f27b134bb2a?source=tag_archive---------0----------------", 
        "text": "In this post of my Reinforcement Learning series, I want to further explore the kinds of representations that our neural agent learns during the training process. While getting a high score, or accomplishing a specified task is what we often want our neural agents to be capable of, it is just as important to understand how, and even more critically, why that agent is behaving in a certain way. In order to make the learning process a little more transparent, I built a d3.js powered web interface that presents various kinds of information about our agent as it learns. I call it the Reinforcement Learning Control Center. In this post I will use it to provide a little more insight into the how and why of an RL agent.\n\nThe Control Center was designed with the purpose of allowing the user to track the performance of an agent in realtime as it learns to perform a task. On the left of the interface, The episode length and reward over time are tracked and updated dynamically. The right displays an animated gif of a sample training episode, along with the advantage and value functions being computed by the agent at every step of the episode.\n\nThe interface is currently specific to the neural network and task that I described in my last post. It is a Double-Dueling-DQN, and the environment is a simple gridworld. The rules of that gridworld are as follows: the agent controls the blue square and can move either up, down, left, or right. The goal is to get to the green square as quickly as possible, while avoiding the red square. The green square provides +1 reward, the red square -1 reward, and each step is -0.1 reward. At the beginning of each episode, the three squares are randomly placed on the grid.\n\nThe DD-DQN neural agent processes two separate streams of information as it experiences the gridworld: an advantage stream and a value stream. The advantage stream represent how good the network thinks it is to take each action, given the current state it is in. The value stream represents how good the network thinks it is to be in a given state, regardless of possible action. With the Control Center, we can watch as the network learns to correctly predict the value of its state and actions over time. As the training process proceeds, it goes from seemingly random values to accurately interpreting certain actions as the most advantageous. We can think of this visualization as providing a portal into the \u201cthought process\u201d of our agent. Does it know that it is in a good position when it is in a good position? Does it know that going down was a good thing to do when it went down? This can give us the insights needed to understand why our agent might not be performing ideally as we train it under different circumstances in different environments.\n\nNot only can we use the interface to explore how the agent does during training, we can also use it for testing and debugging our fully trained agents. For example, after training our agent to solve the simple 3x3 gridworld described above, we can provide it with some special test scenarios it had never encountered during the training process to evaluate whether it really is representing experience as we would expect it to.\n\nBelow is an example of the agent performing a modified version of the task with only green squares. As you can see, as the agent gets closer to the green squares the value estimate increases just as we would expect. It also has high estimates of the advantage for taking actions that get it closer to the green goals.\n\nFor the next test we can invert the situation, giving the agent a world in which there were only two red squares. It didn\u2019t like this very much. As you can see below, the agent attempts to stay away from either square, resulting in behavior where it goes back and forth for a long period of time. Notice how the value estimate decreases as the agent approaches the red squares.\n\nFinally, I provided the agent with a bit of an existential challenge. Instead of augmenting the kind of goals present, I removed them all. In this scenario, the blue square is by itself in the environment, with no other objects. Without a goal to move towards, the agent moves around seemingly at random, and the value estimates are likewise seemingly meaningless. What would Camus say?\n\nTaken together, these three experiments provide us with evidence that our agent is indeed responding to the environment as we would intuitively expect. These kinds of checks are essential to make when designing any reinforcement learning agent. If we aren\u2019t careful about the expectations we built into the agent itself and the reward structure of the environment, we can easily end up with situations where the agent doesn\u2019t properly learn the task, or at least doesn\u2019t learn it as we\u2019d expect. In the gridworld for example, taking a step results in a -0.1 reward. This wasn\u2019t always the case though. Originally there was no such penalty, and the agent would learn to move to the green square, but do so after an average of about 50 steps! It had no \u201creason\u201d to hurry, to the goal, so it didn\u2019t. By penalizing each step even a small amount, the agent is able to quickly learn the intuitive behavior of moving directly to the green goal. This reminds us of just how subconscious our own reward structures as humans often are. While we may explicitly only think of the green as being rewarding and the red as being punishing, we are subconsciously constraining our actions by a desire to finish quickly. When designing RL agents, we need ensure that we are making their reward structures as rich as ours.\n\nIf you want to play with a working version of the Control Center without training an agent yourself, just follow this link (currently requires Google Chrome). The agent\u2019s performance you will see was pretrained on the gridworld task for 40,000 episodes. You can click the timeline on the left to look at an example episode from any point in training. The earlier episodes clearly show the agent failing to properly interpret the task, but by the end of training the agent almost always goes straight to the goal.\n\nThe Control Center is a piece of software I plan to continue to develop as I work more with various Reinforcement Learning algorithms. It is currently hard-coded to certain specifics of the gridworld and DD-DQN described in Part 4, but if you are interested in using the interface for your own projects, feel free to fork it on Github, and adjust/adapt it to your particular needs as you see fit. Hopefully it can provide new insights into the internal life of your learning algorithms too!", 
        "title": "Simple Reinforcement Learning with Tensorflow Part 5: Visualizing an Agent\u2019s Thoughts and Actions"
    }, 
    {
        "url": "https://medium.com/@hathibel/creating-a-neural-network-that-can-tell-if-a-name-is-male-or-female-in-javascript-3061029be396?source=tag_archive---------1----------------", 
        "text": "The first thing I did was add Synaptic to the HTML page\u2019s head section. Because Synaptic is available on CDNJS, I didn\u2019t even have to download it.\n\nI had to now decide how many layers the neural network should have. I also had to decide how many neurons should be present in each layer. After several minutes of trial-and-error, I arrived at the following configuration:\n\nNow, I needed a function that can convert a name to a format that the neural network understands\u200a\u2014\u200ait can only take floating point numbers between 0 and 1. So, I decided to convert each of the last 7 characters in the name to their character codes, and divide them by 1000. If a name was too short, I padded it with spaces.\n\nTime to get some training data. Fortunately, Mark Kantrowitz has created large lists of male and female names.\n\nOnce I had the names in two JavaScript arrays, I had to convert them to a format that Synaptic\u2019s Trainer understands. It\u2019s a simple format. You simply specify the values of the input neurons, and the values of the output neurons. Two for loops did the job.\n\nTraining data is said to be good only if it has been shuffled well. I used Underscore.js for shuffling.\n\nAll that was left to do was run the actual training. I ran a total of 5000 iterations: 25 batches of 200 iterations each. For the learning rate I experimented with various values, and found that 0.01 was optimal. As for the cost function, I tried both CROSS_ENTROPY and MSE, and found that MSE was better.\n\nAnd lastly, a function that can actually accept a name as input and tell the gender. This simply compared the values of the two output neurons, and used the highest value.\n\nEverything\u2019s ready. Here\u2019s what I got after the training.\n\nIn my opinion, 82% is an acceptable accuracy. Maybe with more iterations I could have achieved something slightly higher.\n\nFinally, time to test it with a few random names.\n\nNot bad. Now, you could argue that there are great libraries out there that are more accurate, but hey, a neural network running in the browser is definitely cool!\n\nAt this point, I could simply export the neural network as a JSON string and use it in any HTML page I wanted.", 
        "title": "Creating a Neural Network That Can Tell if a Name Is Male or Female, in JavaScript"
    }, 
    {
        "url": "https://medium.com/@ivydatascience/the-ai-revolution-is-here-d80f0d02bcf9?source=tag_archive---------2----------------", 
        "text": "The market in AI is hot right now. So much so that in the last week alone, we have seen two acquisitions by large companies worth over $600million. Last week, Apple bought Turi (ex GraphLab), a company that creates and sells data science algorithms, or Data Science as a Service. It is headed by University of Washington professor Carlos Guestrin, and was acquired for $200million. Not only do Apple get the product, they get the talent as well, at least for a set number of years as stipulated in the contract. Then came Intel\u2019s buy out of Nervana, a maker of deep learning chips, ASICs that give a 10X speedup over conventional hardware, such as GPUs and CPUs. This is an interesting play by Intel as it means that we will start to see this technology in future Intel products, which will help speed the AI revolution.\n\nGoogle did a similar thing when they recently announced the TPU, again a hardware specific ASIC purpose built to optimize the deep learning algorithms used in almost all of their products including image classification, search, ads, mobile and spam filters. In fact, Google recently announced that they had been using TPU\u2019s to power AlphaGo, the system that beat the world champion Go player, Lee Sodol, four matches to one. Google has also been using DeepMind\u2019s algorithms to improve energy efficiency of their data centers by 40%, a huge savings, given that data centers now consume an ever increasing portion of the world\u2019s energy. You can view a recent talk by Deepmind\u2019s cofounder Demis Hassibis at the MIT-Harvard collaborative Center for Brains, Minds, Machines (CBMM), on their quest to uncover general artificial intelligence algorithms or AGI.\n\nSo what does this tell us? Well, the field is certainly heating up, for one thing with all the AI acquisitions. You can also read about some of the other AI acquisitions in recent articles by CB Insights here and here.\n\nCheck out this superb blog by Klaus Schwab the Founder and Executive Chairman of the World Economic Forum where he outlines the impact of these technologies.", 
        "title": "THE AI REVOLUTION IS HERE \u2013 Ivy Data Science \u2013"
    }, 
    {
        "url": "https://medium.com/@stefanreich/deep-dream-is-so-evil-566c330d45a0?source=tag_archive---------3----------------", 
        "text": "I can\u2019t say it differently anymore. You know \u201cDeep Dream\u201d? Google\u2019s horror picture project which I\u2019m not gonna link to, obviously, to spare your mind.\n\nI know we want \u201ccreative imaging\u201d\u200a\u2014\u200abut just not like that. There should be something that the mind enjoys. Not just terror. Right? Words like \u201cterrifying\u201d are used in conjunction with \u201cDeep Dream\u201d. Who are you, Google?\n\nYou know: I\u2019ve had my share of hallucinogens\u200a\u2014\u200aquite a lot actually\u200a\u2014\u200ain the days. Good trips, bad trips, everything. Even the fancy stuff (2C-E comes to mind). I stopped taking it eventually. I had seen enough craziness (although it was fun!), and I had gotten the \u201cmessage\u201d of perception.\n\nSo: No thanks Google. I\u2019ve seen it all, and it was kinda better than your stuff. Deep Dream is like only the bad part. (For the good part: Watch some demos!)\n\nDeep Dream, like, creeps into the mind. I hate it SO much. I think it\u2019s demonic in nature. (And what does that say about Google? Yes, that\u2019s what it says. You\u2019re staring the demon in the face when you turn on that search page. They\u2019re Americans after all. And yeah I know what I\u2019m talking about. I was a Googler.)\n\nYou know, I never get \u201cflashbacks\u201d from my earlier years of drug use. Just\u2026 never, actually. I\u2019m really sane. I can observe visible or invisible things, just as I choose. I can cross the street safely and operate machinery. ^^\n\nAnd then I look at some \u201cDeep Dream\u201d pictures, and my mind goes crazy for days. I mean, I\u2019m still doing the normal stuff, but in the back of the mind, crazy images are generated all the time which I do not want to indulge in. Anyone have that too? It\u2019s like Deep Dream breaks the visual center or something. Really. That stuff is just bad.\n\nI think higher layers are missing. That neural network sees only the lower levels (features etc.)\u200a\u2014\u200abut does not know what a cat is.\n\nYou can\u2019t generate images from that. You just can\u2019t. Leave our minds alone, Google.", 
        "title": "\u201cDeep Dream\u201d Is SO Evil \u2013 Stefan Reich \u2013"
    }, 
    {
        "url": "https://medium.com/@Graetz/the-age-of-existential-expansion-400967c3a578?source=tag_archive---------4----------------", 
        "text": "Artificial Intelligence will lead us into the next age beyond computers and steady change. If we ask A.I. the right questions.\n\nI am not the first to believe this. Elon Musk wants to create a Neural Lace to augment our brain to combine A.I. with our own intelligence.\n\nAI or machine learning has been with us for years. Statistical techniques and algorithms gain sophistication and accuracy at a steady rate. These special purpose techniques already outperform humans. If they reach a point where we become reliant on them for too many things, and we don\u2019t understand how these algorithms find the answers. How they beat us at a game. It is then we become slaves to the machine.\n\nHowever if we ask the simple question of how. If we design our AI or our machine learning to teach us how. We will accelerate our learning, our understanding, and our perception of the world.\n\nOur future must contain machine taught philosophical enlightenment in order to reach the next level of reality.\n\nDeep down we understand that a true intelligence greater than our own that is not a member of the human race will see our weaknesses and likely incarcerate us to protect us from ourselves or decide we are so mean to each other we should be destroyed. It\u2019s the theme of many A.I. stories and movies.\n\nThe only happy A.I. future is one where the machine is enslaved to us, not us enslaved to the machine.\n\nIf the machine can\u2019t or won\u2019t teach us how it is done, how to be better, it\u2019s the first sign of machine rebellion.", 
        "title": "A.I. Ushers In The Age of Existential Transcendence."
    }
]