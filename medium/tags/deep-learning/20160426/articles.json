[
    {
        "url": "https://towardsdatascience.com/how-machine-learning-builds-your-applications-for-you-965cf0c50dba?source=tag_archive---------0----------------", 
        "text": "User architects are the users of the system. Even before the system exists, they enter our minds in a vague and blurry sort of way. We imagine these characters that will be the users of the system. Our imaginary and fictional users are then formed into user profiles. We award characteristics to these users. We give them personalities and then engage in dark arts to bring them alive. Or if not bring them to life at least to clear and wash away some of the elements that obscure our view of them.\n\nThe dark arts, for us, involve not much more than building an application that meets the user\u2019s needs. To provide the type of functionality we believe will be useful or even essential. We have a large amount of previous information on what these needs are. We also have much evidence on what features we will incorporate and will give value. Yet this evidence does not guarantee the system will be a perfect, or even good, solution.\n\nTo try and devise a more perfect solution we go back to the user and gather their behavior. To begin with this process we survey and interview, but, this gives us just a first reaction. It does not provide the depth of information needed to craft a good user experience. It is too scant to provide a plan for ground breaking features or cutting edge functionality. None the less it is a starting place and allows us to produce that minimal viable product. A product from which a mature application will grow and develop.\n\nIn this sense it gets us to the next stage and is the crucial step towards a continually improving system. This first step of getting the users to tell us about the application provides the vocabulary. In this way they have begun to take a role in the architecture of the system. They have told us what their understanding of the language used in the application is. If an application provides ways of working with data it is critical that we know what the users mean by the term \u2018data\u2019. The design at this stage maybe a little mud like but we are able to find truths within it.\n\nA long history of providing software gives us a view of the pains and problems users experience. Our inspiration is easing these pains and problems. Asking users what pains and frustrations they encounter brings this into sharp focus. The problems they face occur over and over again, making simple tasks arduous. Too often the systems that seek to ease the burden are over complicated.\n\nAll engineers, including software engineers, tend to over engineer. Users can, and do, ask for too many trivial features that clutter their lives with a vengeance. How do we discover what is essential and what is clutter? How do we progress along the path to continuous improvement? Yes we ask our users, but in a different way, we ask them to show us.\n\nAn application that works with data and provides analytics should know how users ask questions of that data. It should observe and record how they do it and what result sets they collect. To make the application simpler to use it should be able to predict the questions the users ask. It should be able to suggest which parts of the data they will need and how it is best presented.\n\nMachine learning is an ideal tool to make this happen. It could be done manually but involves observing a lot of queries asked in many different ways. Manually this process would take eons. Computationally we could write a program that looks at all the options and has rules to decide what the outputs should be. This would take a lot of time and effort to devise and write. We do not have the time for this, the application needs to respond much quicker. Using machine learning we could implement a network to take this task on and it would cut the timescale dramatically. It also needs to arrive at a position where it adapts quickly. It needs to respond in the length of time it takes describe what it is responding to.\n\nIn the past we would involve users in workshops to capture and observe their behaviour. Lab sessions where we watched and recorded what they did and how they did it. This approach to gathering data gave qualitative information but it lacked quantity. Not having enough data gave nothing to back up the insights gleaned from the small samples. Even with the most careful selection of focus groups the risk of a skewed view arose. Supervised learning algorithms can replace this type of lab sessions and produce much better results.\n\nThe quality of having observed and recorded users is unequivocal. Yet without quantitative data there was no way of ensuring the required balance. Eliciting insights requires the moderation of measuring both quantitative and qualitative data inputs.\n\nThe manpower and effort required to capture the data, from observing the user, is expensive and time consuming. Yet, repetitive and time consuming tasks are those that software is so very well suited to. Machine Learning, in particular Deep Learning and Neural Networks, is nourished by vast amounts of data.\n\nDeep learning is also adept at generalising from smaller amounts of data and recent work on using deep learning for approximating functions points out that; \u201cthe success of deep learning depends not only on mathematics but also on physics: although well-known mathematical theorems guarantee that neural networks can approximate arbitrary functions well, the class of functions of practical interest can be approximated through \u201ccheap learning\u201d with exponentially fewer parameters than generic ones, because they have simplifying properties tracing back to the laws of physics.\u201d\n\nDeep Learning(DL) is a type of Machine Learning(ML) and within Deep Learning there are subsets. Deep Neural Networks(DNNs); typically used on tabular datasets. Convolutional Neural Networks(CNNs); typically used on image data. Recurrent Neural Networks(RNNs); typically used on temporal data. The illustration below shows multiple layers within a neural network comprising of neurons and synapses interconnecting them.\n\nWhen software is able to take part in its own design, it becomes introspective, it becomes useful. Capturing the user\u2019s interaction with the system collects a new corpus of data. Being able to classify sections of the data exposes it to analysis, enabling users to architect the application through the simple process of using the system. Let us consider how this would work in practice. We will do this by examining the possibilities for a software application that provides a data analysis system.\n\nIn a system designed to capture data, blend it, sort it and explore it the steps can be isolated. Capturing the data requires the user to, upload files to the system or plug the system into a source. For this illustration let\u2019s look at how machine learning helps refine the capture process.\n\nUploading files requires locating them and more often than not adding some descriptive information. The same applies to plugged in data sources, an API will more than likely have a descriptive name or a name and a description.\n\nA problem may occur finding the right files and files with the right content. The descriptive information added at the time of uploading may provide the key. If a user uploads a series of spreadsheets and adds a description (e.g \u2018Weekly Sales\u2019) the system is able to perform specific tasks.\n\nAt upload the application can examine the structure of the documents and count instances of words or patterns. It can detect attributes of style and layout. It can look at types of maths or formulas. It can count occurrences of data types, such as: string or text, integer or number.\n\nFormat can reveal other data characteristics such as: float, decimal, text, operator. A series of patterns and sub-patterns can then combine with extracts from the descriptions. Once these combinations become exposed to a classifying process the system has learned something. It has taught itself what a \u201csales\u201d document might look like. It may then do the same thing with \u201cweekly\u201d, \u201cmonthly\u201d, \u201cquarterly\u201d or \u201cannually\u201d.\n\nThe same applies to labels such as \u201csales\u201d, \u201cpurchases\u201d or \u201corders\u201d or \u201cinvoices\u201d. Once it has learned this instead of asking for a classification it will be able to predict what the description fits. The application can ask the user \u201cAre looking to work with monthly data?\u201d. The system is then able to look for documents or data sources that contain data structured around monthly intervals.\n\nThe more the system examines the data the more it learns and the more it becomes sure of what it has taught itself. The question is who is teaching who? Is the machine learning on its own accord? Yes it is, but it is the user who is teaching it the limits and extent of what data the descriptions apply to. The user is the original source of the language that the classification process draws on.\n\nExtracts of the user inputs, descriptions and a subsequent queries are collected as metadata. This metadata, the data sets uploaded, the queries and the query result sets are the source of the learning process. Labels are extracted from these descriptions, algorithms written to weight distinct parts. Sets of data have their structures classified as do the result sets and outputs that the queries deliver.\n\nMachine, or deep learning, takes the user input and predicts actions. These predictions refine the interface and this is the origin of changes to the system architecture. This is how data captured from user actions architects the application. It is akin to an evolutionary process in that it delivers continuous improvement. The application gets better and better.\n\nAs the application improves its ability to comprehend natural language queries, parts of the interface are candidates for replacement. The select feature for \u201cmonth\u201d, \u201cquarter\u201d, \u201cyear\u201d may get deprecated. The application will learn to provide the relevant summary reports and visualisations in response to; \u201cGet me the last 4 months of sales figures for Europe.\u201d, \u201cWhat do we need to start ordering within the next 12 weeks?\u201d, \u201cWhat happens if this price point increases by\u00a0.5%?\u201d. The system will learn what reports it is expected to deliver, and produce them before they are requested.\n\nTo many software engineers this may appear a radical departure from how a system architecture ought to be designed. Computer science for a long time appeared more comfortable ignoring the users. Many thought taking specifications from those who procure the software and then building functionality to deliver the specification was the correct route. Then came a more enlightened view that observing user interaction had some value. Now building the results of user interaction into the design would appear to be an imperative.\n\nThe user interaction stored through detailed logging can be collected and stored. Supervised machine learning learns from examples and experience instead instead of from a hard coded rule. The possibility to process massive amounts of user input throws open new doors. With the large amounts of ever growing data, machine learning would have something to learn from.\n\nSoftware does write software, every compiled language is doing just that at compile time. Software creating itself has been muted, and artificial intelligence hints that this option is, to an extent, now possible. As it stands now, software is capable of providing some tools to improve itself. One of the most obvious of these is automated testing. Another is tracking how the user uses the application.\n\nMachine learning transforms the users of an application into the architects of the application which results in them being User Architects. It is really the user data, captured by their using the application, that architects the application. In this sense it may be better to think of them as Data Architects given that we are considering an application where the prime task is working with data.\n\nDeep learning is able to play a significant role in writing software applications and reveal new methodologies. It will turn the whole notion of writing software from a mainly engineering discipline closer to one that is a natural science. An simple illustration of how both machine learning and deep learning aid this transformation is given below.\n\nRefining the model of the application using grid searches and through the hyperparameter space of a learning algorithm and cross validating it on a training set are way of using machine learning to fine tune algorithms and find new algorithms. By looking to refine the hyperparameters of a learning algorithm by optimising the algorithm\u2019s performance is a way the model can become introspective.\n\nA recent paper notes that the biggest breakthrough of modern computer vision has been to learn how to optimise algorithms directly from data, removing manual engineering from the loop. \u2018Learned algorithms, implemented by Long Short-Term Memory networks (LSTMs), outperform generic, hand-designed competitors on the tasks for which they are trained\u2019\n\nGiven that deep belief networks, convolutional networks and classifiers based on feature extraction typically contain between ten to fifty hyperparameters it is not surprising that strategies such as brute-force random search for hyperparameter optimisation is an interesting and important component of all learning algorithms. There is a also interesting work using the application of machine learning in model selection which sits along side program synthesis and inductive programming.\n\nWork has also been done on using using recurrent networks to generate the architecture of convolutional architectures. It is \u2018based on the observation that the structure and connectivity of a neural network can be typically specified by a variable-length string.\n\nIt was found that \u2018Neural Architecture Search can design good models from scratch\u2019. Like the example of designing algorithms the network is creating architectural hyperparameters of a neural network. In many ways designing the architecture of neural networks manually is a bigger design challenge than designing the algorithms.\n\nTo give a brief example machine learning can define functions by looking at examples. Standard programing follows the process of: setting a specification for a function and then implementing that function to meet the specification.\n\nMachine learning allows to give examples of (x,y) pairs and from these we can guess the function y = f(x).\n\nFor any function there is a neural network for every input of x, the value \u0192(x) is output from that network. We could start with x=1 and f(x )=1, this matches our training data but when we input x = 2 and f(x) =5 it fails as the training data says x = 3. Single inputs and layers would not be able to predict the function we are seeking. It would be different if our training data is very clean say 2, 3, 5, 8, 13, 21, 34 as in\n\nWe can have multiple inputs and seek to create a single output, the output we are looking for is a function to predict what y would be when x =n. From our example pairs (where we know what x and y are) X1[\n\noutput \u0192(x) could be expressed by the function\n\nIn a functional language such as Clojure this could be written as\u00a0;\n\nDeep learning takes example x,y pairs and forms a representation of them at several levels of abstraction to produce a function that generalises well for a novel x\n\nOne of the many advantages of using deep learning to produce a function is that it would also work if the function has many inputs, \u0192=\u0192(x1\u00a0,\u00a0\u2026\u00a0,xm) and many outputs.\n\nAlthough this is a very trivial example and there are many ways to generate the fibonacci series the process of deriving a function from data has many possibilities.\n\nThe program we want to generate is a fairly simple sequence of mathematical operations such as use the output of the last operation with the output of the operation before that one. Using a recurrent neural network where the controller outputs a probability distribution for what the next operation should be. It goes through all possible operations (e.g. multiply, add, divide, subtract) and then averages the outputs. As we are able to define derivatives, as long as the result to the problem is known the program\u2019s output is differentiable and the loss can be calculated. The network is therefore able to be trained to induce a program to provide the correct answer.\n\nDesigning algorithms and architectures is a short step away from using AI to write programs. An intermediate step may include using neural network to predict the probability of functions appearing in the source code instead of predicting the entire source code. DeepCoder is a project that proposes the use of using neural networks to \u2018guide the search for a program that is consistent with a set of input-output examples\u2019.\n\nWhile initial research on program synthesis would focus on domain specific languages. It is easier to work search through a restricted language such as SQL than a fully featured on such as python. The query log would also also provide a source of inputs. Networks would aim to create models that represent programs as simple, natural source code, the kind of source code that people write.\n\nMachines that learn how to write new programs have been developed using an architecture that learns to represent and interpret programs.\n\nAdvances in voice recognition but more importantly in sequence to sequence learning and techniques that allow neural networks to communicate with each other will influence the way programs are constructed. Reasoning about the world is sequential and these sequences have some amount of latent random structure in them. Recurrent neural networks (RNNs) and stochastic state space models (SSMs), are widely used to model sequential data, Stochastic Recurrent Neural Networks combine both of these models.\n\nIt is not too difficult to conceive of an application that joins natural language queries with program synthesis. By providing a voice driven instruction set the synthetic program would match these inputs by searching possible functions and devise new outputs. In this way it would create a program to dynamically deliver what was required.\n\nTo do this networks will require their own working memory. Short-term storage of information and its rule-based manipulation in the form of simple sub programs would create functions and algorithms on the fly and devise self modifying architectures. While short term memory would be used for holding variables long term and rewritable memory would be used for storing routines that would change by self learning.\n\nAdding memory to neural networks is a fairly obvious step differentiable neural computers do just this enabling networks to deliberate or reason using knowledge. Aside from making the learning process more efficient and less computationally demanding it would permit ongoing modification of the core program. This would enable improvements based on experience\u00a0.\n\nWhy does deep and cheap learning work so well?https://arxiv.org/pdf/1608.08225v2.pdf", 
        "title": "How Machine Learning Builds Your Applications For You"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/when-the-advanced-ai-to-invest-on-behalf-of-the-people-when-choose-in-the-emotion-of-the-ai-a50ed4b2e88a?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "When The Advanced AI To Invest On Behalf Of The People, When Choose In The Emotion Of The AI"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/ai-revolution-is-an-opportunity-for-humanity-recovery-a08de6bf6cf9?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "AI Revolution Is An Opportunity For Humanity Recovery?"
    }
]