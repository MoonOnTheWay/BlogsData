[
    {
        "url": "https://medium.com/jim-fleming/highway-networks-with-tensorflow-1e6dfa667daa?source=tag_archive---------0----------------", 
        "text": "This week I implemented highway networks to get an intuition for how they work. Highway networks, inspired by LSTMs, are a method of constructing networks with hundreds, even thousands, of layers. Let\u2019s see how we construct them using TensorFlow.\n\nFor comparison, let\u2019s start with a standard fully-connected (or \u201cdense\u201d) layer. We need a weight matrix and a bias vector then we\u2019ll compute the following for the layer output:\n\nHere\u2019s what a dense layer looks like as a graph in TensorBoard:\n\nFor the highway layer what we want are two \u201cgates\u201d that control the flow of information. The \u201ctransform\u201d gate controls how much of the activation we pass through and the \u201ccarry\u201d gate controls how much of the unmodified input we pass through. Otherwise, the layer largely resembles a dense layer with a few additions:\n\nWhat happens is that when the transform gate is 1, we pass through our activation (H) and suppress the carry gate (since it will be 0). When the carry gate is 1, we pass through the unmodified input (x), while the activation is suppressed.\n\nHere\u2019s what the highway layer graph looks in TensorBoard:\n\nUsing a highway layer in a network is also straightforward. One detail to keep in mind is that consecutive highway layers must be the same size but you can use fully-connected layers to change dimensionality. This becomes especially complicated in convolutional layers where each layer can change the output dimensions. We can use padding (\u2018SAME\u2019) to maintain each layers dimensionality.\n\nOtherwise, by simply using hyperparameters from the TensorFlow docs (i.e. no hyperparameter search) the fully-connected highway network performed much better than a fully-connected network. Using MNIST as my simple trial:\n\nNow that we have a highway network, I wanted to answer a few questions that came up for me while reading the paper. For instance, how deep will the network converge? The paper briefly mentions 1000 layers:\n\nCan we train with 1000 layers on MNIST?\n\nYes, also reaching around 95% accuracy. Try it out with a carry bias around -20.0 for MNIST (from the paper the network will only utilize ~15 layers anyway). The network can probably even go deeper since the it\u2019s just learning to carry the last 980 layers or so. We can\u2019t do much useful at or past 1000 layers so that seems sufficient for now.\n\nWhat happens if you set very low or very high carry biases?\n\nIn either extreme the network simply fails to converge in a reasonable amount of time. In the case of low biases (more positive), the network starts as if the carry gates aren\u2019t present at all. In the case of high biases (more negative), we\u2019re putting more emphasis on carrying and the network can take a long time to overcome that. Otherwise, the biases don\u2019t seem to need to be exact, at least on this simple example. When in doubt start with high biases (more negative) since it\u2019s easier to learn to overcome carrying than without carry gates (which is just a plain network).\n\nOverall I was happy with how easy highway networks were to implement. They\u2019re fully differentiable with only a single additional hyperparameter for the initial carry bias. One downside is that highway layers do require additional parameters for the transform weights and biases. However, since we can go deeper, the layers do not need to be as wide which can compensate.\n\nHere\u2019s are the complete notebooks if you want to play with the code: fully-connected highway repo and convolutional highway repo.\n\nFollow me on Twitter for more posts like these. If you\u2019d like building very deep networks in production, I do consulting.", 
        "title": "Highway Networks with TensorFlow \u2013 Jim Fleming \u2013"
    }, 
    {
        "url": "https://medium.com/@NathanBenaich/investing-in-artificial-intelligence-a-vc-perspective-afaf6adc82ea?source=tag_archive---------1----------------", 
        "text": "My (expanded) talking points from a presentation I gave at the Re.Work Investing in Deep Learning dinner in London on 1st December 2015.\n\nTL;DR Check out the slides here.\n\nIt\u2019s my belief that artificial intelligence is one of the most exciting and transformative opportunities of our time. There\u2019s a few reasons why that\u2019s so. Consumers worldwide carry 2 billion smartphones, they\u2019re increasingly addicted to these devices and 40% of the world is online (KPCB). This means we\u2019re creating new data assets that never existed before (user behavior, preferences, interests, knowledge, connections).\n\nThe costs of compute and storage are both plummeting by orders of magnitude, while the computational capacity of today\u2019s processors is growing. We\u2019ve seen improvements in learning methods, architectures and software infrastructure. The pace of innovation can therefore only be accelerating. Indeed, we don\u2019t fully appreciate what tomorrow will look and feel like.\n\nAI-driven products are already out in the wild and improving the performance of search engines, recommender systems (e.g. e-commerce, music), ad serving and financial trading (amongst others). Companies with the resources to invest in AI are already creating an impetus for others to follow suit or risk not having a competitive seat at the table. Together, therefore, the community has a better understanding and is equipped with more capable tools with which to build learning systems for a wide range of increasingly complex tasks.\n\nMore on this discussion here. A key consideration, in my view, is that the open sourcing of technologies by large incumbents (Google, Microsoft, Intel, IBM) and the range of companies productising technologies for cheap means that technical barriers are eroding fast. What ends up moving the needle are: proprietary data access/creation, experienced talent and addictive products.\n\nThere are two big factors that make involving the user in an AI-driven product paramount. 1) Machines don\u2019t yet recapitulate human cognition. In order to pick up where software falls short, we need to call on the user for help. 2) Buyers/users of software products have more choice today than ever. As such, they\u2019re often fickle (avg. 90-day retention for apps is 35%). Returning expected value out of the box is key to building habits (hyperparameter optimisation can help). Here are some great examples of products which prove that involving the user-in-the-loop improves performance:\n\nWe can even go a step further, I think, by explaining how machine-generated results are obtained. For example, IBM Watson surfaces relevant literature when supporting a patient diagnosis in the oncology clinic. Doing so improves user satisfaction and helps build confidence in the system to encourage longer term use and investment. Remember, it\u2019s generally hard for us to trust something we don\u2019t truly understand.\n\nTo put this discussion into context, let\u2019s first look at the global VC market. Q1-Q3 2015 saw $47.2bn invested, a volume higher than each of the full year totals for 17 of the last 20 years (NVCA). We\u2019re likely to breach $55bn by year end. There are circa 900 companies working in the AI field, most of which tackle problems in business intelligence, finance and security. Q4 2014 saw a flurry of deals into AI companies started by well respected and achieved academics: Vicarious, Scaled Inference, MetaMind and Sentient Technologies.\n\nSo far, we\u2019ve seen circa 300 deals into AI companies (defined as businesses whose description includes keywords: artificial intelligence, machine learning, computer vision, NLP, data science, neural network, deep learning from Jan 1st 2015 thru 1st Dec 2015, CB Insights). In the UK, companies like Ravelin, Signal and Gluru raised seed rounds. Circa $2bn was invested, albeit bloated by large venture debt or credit lines for consumer/business loan providers Avant ($339m debt+credit), ZestFinance ($150m debt), LiftForward ($250m credit) and Argon Credit ($75m credit). Importantly, 80% of deals were < $5m in size and 90% of the cash was invested into US companies vs. 13% in Europe. 75% of rounds were in the US.\n\nThe exit market has seen 33 M&A transactions and 1 IPO (Adgorithms on the LSE). Six events were for European companies, 1 in Asia and the rest were accounted for by American companies. The largest transactions were TellApart/Twitter ($532m; $17m raised), Elastica/Blue Coat Systems ($280m; $45m raised) and SupersonicAds/IronSource ($150m; $21m raised), which return solid multiples of invested capital. The remaining transactions were mostly for talent, given that median team size at the time of the acquisition was 7ppl median.\n\nAltogether, AI investments will have accounted for circa 5% of total VC investments for 2015. That\u2019s higher than the 2% claimed in 2013, but still tracking far behind competing categories like adtech, mobile and BI software. The key takeaway points are a) the financing and exit markets for AI companies are still nascent, as exemplified by the small rounds and low deal volumes, and b) the vast majority of activity takes place in the US. Businesses must therefore have exposure to this market.\n\nI spent a number of summers in university and 3 years in grad school researching the genetic factors governing the spread of cancer around the body. A key takeaway I left with is the following: therapeutic development is a very challenging, expensive, lengthy, regulated and ultimately offers a transient solution to treating disease. Instead, I truly believe that what we need to improve healthcare outcomes is granular and longitudinal monitoring of physiology and lifestyle. This should enable early detection of health conditions in near real-time, drive down cost of care over a patient\u2019s lifetime, while consequently improving outcomes.\n\nConsider the digitally connected lifestyles we lead today. The devices some of us interact with on a daily basis are able to track our movements, vital signs, exercise, sleep and even reproductive health. We\u2019re disconnected for fewer hours of the day than we\u2019re online and I think we\u2019re less apprehensive to storing various data types in the cloud (where they can be accessed, with consent, by 3rd parties). Sure, the news might paint a different, but the fact is that we\u2019re still using the web and it\u2019s wealth of products.\n\nOn a population level, therefore, we have the chance to interrogate data sets that have never before existed. From these, we could glean insights into how nature and nurture influence the genesis and development of disease. That\u2019s huge. Look at today\u2019s clinical model: a patient presents into the hospital when they feel something is wrong. The doctor has to conduct a battery of tests to derive a diagnosis. These tests address a single (often late stage) time point, at which moment little can be done to reverse damage (e.g. in the case of cancer). Now imagine the future. In a world of continuous, non-invasive monitoring of physiology and lifestyle, we could predict disease onset and outcome, understand which condition a patient likely suffers from and how they\u2019ll respond to various therapeutic modalities. There\u2019s loads of applications for artificial intelligence here: intelligence sensors, signal processing, anomaly detection, multivariate classifiers, deep learning on molecular interactions\u2026\n\nSome companies are already hacking away at this problem:\n\nA point worth noting is that the UK has a slight leg up on the data access front. Initiatives like the UK Biobank (500k patient records), Genomics England (100k genomes sequenced), HipSci (stem cells) and the NHS care.data programme are leading the way in creating centralised data repositories for public health and therapeutic research. Cheers for pointing out, Hari Arul.\n\nCould businesses ever conceivably run themselves? AI-enabled automation of knowledge work could cut employment costs by $9tn by 2020 (BAML). Coupled to the efficiency gains worth $1.9tn driven by robots, I reckon there\u2019s a chance for near complete automation of core, repetitive businesses functions in the future. Think of all the productised SaaS tools that are available off the shelf for CRM, marketing, billing/payments, logistics, web development, customer interactions, finance, hiring and BI. Then consider tools like Zapier or Tray.io, which help connect applications and program business logic. These could be further expanded by leveraging contextual data points that inform decision making. Perhaps we could eventually re-image the new eBay, where you\u2019ll have fully automated inventory procurement, pricing, listing generation, translation, recommendations, transaction processing, customer interaction, packaging, fulfilment and shipping. Of course, probably a ways off\u00a0:)\n\nI\u2019m bullish on the value to be created with artificial intelligence across our personal and professional lives. I think there\u2019s currently low VC risk tolerance for this sector, especially given shortening investment horizons for value to be created. More support is needed for companies driving long term innovation, especially that far less is occurring within Universities. VC was born to fund moonshots.\n\nWe must remember that access to technology will, over time, become commoditised. It\u2019s therefore key to understand your use case, your user, the value you bring and how it\u2019s experience and assessed. This gets to the point of finding a strategy to build a sustainable advantage such that others find it hard to replicate your offering. Aspects of this strategy may in fact be non-AI and non-technical in nature (e.g. the user experience layer\u200a\u2014\u200athanks for highlighting this Hari Arul). As such, there\u2019s a renewed focused on core principles: build a solution to an unsolved/poorly served high-value, persistent problem for consumers or businesses.\n\nFinally, you must have exposure to the US market where the lion\u2019s share of value is created and realised. We have an opportunity to catalyse the growth of the AI sector in Europe, but not without keeping close tabs on what works/doesn\u2019t work across the pond first-hand.\n\nWorking in the space? We\u2019d love to get to know you\u00a0:)", 
        "title": "Investing in Artificial Intelligence \u2013 Nathan Benaich \u2013"
    }, 
    {
        "url": "https://medium.com/jim-fleming/loading-tensorflow-graphs-via-host-languages-be10fd81876f?source=tag_archive---------2----------------", 
        "text": "Check out the related post: Loading a TensorFlow graph with the C++ API.\n\nEven though the full C API for TensorFlow is not yet available, we can still use it load TensorFlow graphs and evaluate them from other languages. This is incredibly useful for embedding pre-trained models in other applications. Embedding is one of the most interesting use cases for TensorFlow as it cannot be accomplished as easily with Theano.\n\nNote that while all of the examples here will use Node.js the steps are nearly identical in any language with C FFI support (e.g. Rust, Go, C#, etc.)\n\nWe\u2019ll start by compiling a shared library from TensorFlow using Bazel.\n\nUPDATE: The following build rule for creating a shared library is now part of TensorFlow: https://github.com/tensorflow/tensorflow/pull/695\n\nBelow is the complete BUILD file:\n\nNow that we have our shared library, create a new folder for the host language. Since this is for Node.js I\u2019ll name it tensorflowjs/. This folder can exist outside of the TensorFlow repo since we now have everything needed in the shared library. Copy libtensorflow.so into the new folder.\n\nIf you\u2019re on OS X and using Node.js you\u2019ll need to rename the shared library from libtensorflow.so to libtensorflow.dylib. TensorFlow produces an\u00a0.so however the standard on OS X is dylib. The Node FFI library doesn\u2019t look for\u00a0.so, only\u00a0.dylib; however it can read both formats, so we just rename it.\n\nJust like with the previous C++ tutorial we\u2019re going to create a minimal graph and write it to a protobuf file. (Be sure to name your variables and operations.)\n\nNow we can go through the TensorFlow C API header, almost line by line, and write the appropriate binding. Most of the time this is fairly direct, simply copying the signature of the function. I also created variables for many of the common types so they were more legible. For example, any structs which map to void* I declared as variables named after the struct. We can also use the ref-array Node module which provides helpers for types like long long* (essentially an array of long long types) so we\u2019ll define a LongLongArray type to correspond. Otherwise, we just copy the signature:\n\nI also defined a few helper functions to eliminate some of the boilerplate when working with the TensorFlow interface. The first is TF_Destructor, a default tensor destructor for TF_NewTensor. This comment in the TensorFlow source makes it sound like it\u2019s optional but it\u2019s not:\n\nAdditionally, many TensorFlow functions return a TF_Status struct and checking the status can get tedious. So I defined a function called TF_CheckOK that simply checks if the status code is TF_OK using TF_GetCode. If its not, we throw an error using TF_Message to hopefully get a useful error message. (This function loosely corresponds to TF_CHECK_OK in the TensorFlow source.)\n\nAnd finally, reading a tensor with TF_TensorData only returns a pointer but to actually read the data we need to extend the returned Buffer to the appropriate length. Creating a Buffer with the correct size is a few lines of boiler plate so I wrapped TF_TensorData to create TF_ReadTensorData which handles that boilerplate for us. Here are the helpers:\n\nNow that we\u2019ve defined our interface the steps for loading the graph are the same as with C++:\n\nWe can load and execute TensorFlow graphs from Node.js! I\u2019ve put the whole thing together into a repo here (you\u2019ll need to provide graph.pb and libtensorflow.dylib since they\u2019re kinda large): https://github.com/jimfleming/tensorflowjs\n\nFollow me on Twitter for more posts like these. If you\u2019d like help deploying TensorFlow in production, I do consulting.", 
        "title": "Loading TensorFlow graphs from Node.js \u2013 Jim Fleming \u2013"
    }, 
    {
        "url": "https://medium.com/@genekogan/from-pixels-to-paragraphs-eb2763da0e9b?source=tag_archive---------3----------------", 
        "text": "In recent months, a raft of art projects concerned with deep learning [1][2] have appeared, helping to popularize a branch of machine learning that has rapidly gained traction within the research community. Characterized by more complex architectures than previous generations of machine learning, large-scale deep learning methods have only recently become practical with modern computing power. As with all labels, no one can guess how long this particular one will stick, and even some of its proponents dislike certain claims made about it. Nevertheless, a number of algorithms associated with it have set new benchmarks in image, sound, and natural language processing over the last few years [1][2], prompting a flurry of speculations into its computational, commercial, and creative potential. [1][2][3][4][5][6]\n\nArtistic applications of scientific research often start within the academic institutions that initiate them, as was the case for Andrej Karpathy, a graduate student at Stanford who wrote a blog post earlier this year, \u201cThe Unreasonable Effectiveness of Recurrent Neural Networks\u201d, popularizing a type of neural network capable of generating text reminiscent of Shakespeare, Wikipedia, the source code of Linux, and anything else it happened to be trained on. Karpathy released his code, provoking a series of projects by others repurposing RNNs for everything from generating food recipes and bible verses to rendering fake emojis.\n\nThis was followed by the arrival of a number of software packages transforming images in novel ways, most notably Google\u2019s Deepdream / Inceptionism project, as well as \u201cA neural algorithm of artistic style,\u201d a technique by which convolutional neural networks could recompose images in different styles. The unprecedented regenerative capabilities of these programs further pushed deep learning to a wider audience, leaving social media awash with pictures teeming with otherworldly creatures and psychedelic visuals, reimaginings of iconic paintings, and countless other machine-made artifacts [1][2].\n\nMore recently, a group of researchers published code demonstrating the use of generative adversarial networks [1][2] for modeling large image corpora and \u201challucinating\u201d convincing samples which appeared to resemble the originals.\n\nOne of the authors, Alec Radford, foreshadowed the impressive abilities of GANs by training them on a large dataset of faces and interpolating smoothly through the parametric space the GAN learned to represent. Radford\u2019s experiments recalled the work of Heather Dewey-Hagborg, one of the first artists to examine what high-level information could be extracted from unordered atoms of low-level data. In \u201cStranger Visions,\u201d she experimented with nascent DNA forensics by fabricating faces whose features were inferred from phenotypic analyses of genetic materials discarded by strangers in public places, calling to attention the public\u2019s lack of awareness about how revealing bits of personal data could be.\n\nThe preceding works illustrated the capture and reorientation of visual information from text, pixels, and other minutiae. But it turns out those same pixels can literally \u201ctell\u201d us much more than that. Programs able to automatically annotate and even describe images in human language have made headway this year. A codebase by Ryan Kiros called neural-storyteller was adapted by Samim Winiger who trained it on romance novels and Taylor Swift lyrics and let it tell stories about a series of photographs, to humorous effect.\n\nSimilarly, Andrej Karpathy released code for real-time image captioning, and it was subsequently taken for a walk around Amsterdam by Kyle McDonald, casually confronting the public with a program displaying intelligent behavior not typically associated with simple webcams. To be sure, these early attempts still produce dubious transcriptions, but their steady improvement forces us to contemplate what we can and cannot eventually entrust them with as they increasingly exhibit competence for higher order tasks. Simultaneously, promising approaches to the reverse process\u200a\u2014\u200agenerating images from captions\u200a\u2014\u200aare progressing rapidly and hint to a future where computers can autonomously exchange information end-to-end across different media. We need not strain hard to imagine the ramifications in a world whose accumulation of data is still accelerating.\n\nHow do such experiments help inform the public? Artistic works often suggest approachable metaphors for subjects which may otherwise be shrouded by layers of technical obfuscation, helping to illuminate the counterintuitive properties of these new techniques. Recontextualized, they show us how small bits of information in other domains become collectively meaningful. As with all successful technologies, these advances will gradually be absorbed into our existing infrastructures and institutions as they mature, forcing us to make crucial decisions along the way about how to implement them justly. The extent to which the public is aware of what these machines can\u200a\u2014\u200aand cannot\u200a\u2014\u200ado is the extent to which they can be regarded as trustworthy.\n\nThe number of open source deep learning libraries has dramatically increased over the past year [1][2][3], equipping amateurs with the same programs that are achieving state-of-the-art results in industry and academia. Numerous initiatives for demystifying machine learning and making it more accessible have popped up [1][2][3][4][5], including most recently OpenAI, a non-profit with over $1 billion backing from Elon Musk, among others, and a roster of well-known practitioners who have promised to promote transparency and collective benefit in their research. It seems the playing field has never been so level as it is today with respect to AI research. Yet despite these encouraging trends, we must proceed cautiously and thoughtfully as we know well by now that the history of computer technology has a familiar theme; public expenditure leads to rapid improvement of an emerging technology, whose faculties become gradually obscured from the public as they become commercially viable. This knowledge imbalance enables those with access to oversell its positive aspects and omit the potentially harmful use cases. As intelligent machines come to inhabit more and more of our daily lives, the stakes will only get higher. Another long stagnation should be the least of our concerns.", 
        "title": "From Pixels to Paragraphs \u2013 Gene Kogan \u2013"
    }, 
    {
        "url": "https://medium.com/planet-stories/this-company-is-using-timely-satellite-imagery-and-deep-learning-to-predict-a-67-billion-u-s-7346bd0f3643?source=tag_archive---------4----------------", 
        "text": "A data analysis company named Descartes Labs can predict the percentage of farms in the United States that will grow soy or corn next year\u200a\u2014\u200aand this year, their predictions may be more accurate than the US Government\u2019s forecast.\n\nThat may not seem like a big deal, but consider the value of a prediction like this\u200a\u2014\u200acorn is a $67 billion business in America. To understand why and how Descartes Labs made this prediction, it\u2019s good to know a bit about land use maps and how they\u2019re used in the agriculture business.\n\nIn any given square mile of the U.S., land might be covered by concrete for a road, cement for a strip mall, or corn for crops. Using traditional methods, it takes a lot of time to map out all of these different land uses; the process is arduous and must be done by hand, by analysts who pore over historical satellite imagery, ground observations, and surveys. Economic forecasters for agribusiness companies need to know this information\u200a\u2014\u200ahow much of a crop is being planted\u200a\u2014\u200ato predict seasonal outputs. It\u2019s a big business, so they pay for the analysts\u2019 time. And this is where Descartes Labs steps in: They\u2019re able to simplify the process\u200a\u2014\u200asaving time and money\u200a\u2014\u200aby processing a massive amount of data, efficiently and accurately, with the help of Planet Labs imagery.\n\nThe farmlands of United States are some of the highest producing crops in the world.\n\nToday, the benchmark for metrics on land use come in the form of yearly national Cropland Data Layer (CDL) maps published by the U.S. Department of Agriculture (USDA). These maps use year-old data and are inherently not up-to-date when released. They also use freely available, but lower resolution imagery from Landsat 8, a satellite operated by NASA. Landsat 8 collects an image of an area every sixteen days. In short, this means that economic forecasters are using low-resolution imagery that is a year old to predict crop outputs\u200a\u2014\u200aand that leaves a lot of room for improvement.\n\nWith land use changing frequently and a greater need for up-to-date and accurate maps, Descartes Labs is using deep learning technology to analyze imagery faster and predict the future.", 
        "title": "This Company is Using Satellite Imagery & Deep Learning to Predict a $67B Corn Market"
    }, 
    {
        "url": "https://medium.com/google-cloud/what-google-cloud-vision-api-means-for-deep-learning-startups-cd39226922e5?source=tag_archive---------5----------------", 
        "text": "Google announced today the Cloud Vision API, which makes available to the general public their Deep Learning algorithms for image recognition, OCR, facial detection, etc.\n\nThere are already many Deep Learning startups offering these services, in my opinion Google could easily capture most of the market.\n\nThere are 3 reasons behind the Deep Learning renaissance, each of them a potential competitive advantage, let\u2019s see them:\n\nHow could Deep Learning startups then compete with Google?\n\nGoogle has often failed at monetizing their business services, startups may hope that it will happen again. I would not sleep well on this thought, the competitive advantage is dramatically on Google side and they will keep improving the algorithms for their internal services, regardless of API revenues.\n\nToday Google entered the Vision market, the same could soon be true also for Natural Language Processing (NLP), the other field which is seeing lots of improvements with Deep Learning. For example, most of the available services are in English, probably limited by the fact that there are much more available datasets and corpuses in this language. Google is in the best position to provide NLP services in other languages too.\n\nWhile it could be a bad day for many Deep Learning startuppers, once they get over it and refocus I think there will be a lot of very smart people working on new hard problems, making the world a better place in the process.\n\nThese are my 2 cents, what is you opinion on this topic?", 
        "title": "What Google Cloud Vision API means for Deep Learning Startups"
    }, 
    {
        "url": "https://gab41.lab41.org/taking-keras-to-the-zoo-9a76243152cb?source=tag_archive---------6----------------", 
        "text": "If you follow any of the popular blogs like Google\u2019s research, FastML, Smola\u2019s Adventures in Data Land, or one of the indie-pop ones like Edwin Chen\u2019s blog, you\u2019ve probably also used ModelZoo. Actually, if you\u2019re like our boss, you affectionately call it \u201cThe Zoo\u201d. (Actually x 2, if you have interesting blogs that you read, feel free to let us know!)\n\nUnfortunately, ModelZoo is only supported in Caffe. Fortunately, we\u2019ve taken a look at the difference between the kernels in Keras, Theano, and Caffe for you, and after reading this blog, you\u2019ll be able to load models from ModelZoo into any of your favorite Python tools.\n\nWhy this post? Why not just download our Github code?\n\nIn short, it\u2019s better you figure out how these things work before you use them. That way, you\u2019re better armed to use the latest TensorFlow and Neon toolboxes if you\u2019re prototyping and transitioning your code to Caffe.\n\nSo, there\u2019s Hinton\u2019s Dropout and then there\u2019s Caffe\u2019s Dropout\u2026and they\u2019re different. You might be wondering, \u201cWhat\u2019s the big deal?\u201d Well sir, I have a name of a guy for you, and it\u2019s Willy\u2026Mr. Willy Nilly. One thing Willy Nilly likes is the number 4096. Another thing he likes is to introduce regularization (which includes Dropout) arbitrarily, and Bayesian theorists aren\u2019t a fan. Those people try to fit their work into the probabilistic framework, and they\u2019re trying to hold onto what semblance of theoretical bounds exist for neural networks. However, for you as a practitioner, understanding who\u2019s doing what will save you hours of debugging code.\n\nWe singled out Dropout because the way people have implemented it spans the gamut. There\u2019s actually some history as to this variation, but no one really cared, because optimizing for it has almost universally produced similar results. Much of the discussion stems from how the chain rule is implemented since randomly throwing stuff away is apparently not really a differentiable operation. Passing gradients back (i.e., backpropagation) is a fun thing to do; there\u2019s a \u201ctechnically right\u201d way to do it, and then there\u2019s what\u2019s works.\n\nBack to ModelZoo, where we\u2019d recommend you note the only sentence of any substance in this section, and the sentence is as follows. While Keras and perhaps other packages multiply the gradients by the retention probability at inference time, Caffe does not. That is to say, if you have a dropout level of 0.2, your retention probability is 0.8, and at inference time, Keras will scale the output of your prediction by 0.8. So, download the ModelZoo *.caffemodels, but know that deploying them on Caffe will produce non-scaled results, whereas Keras will.\n\nHinton explains the reason why you need to scale, and the intuition is as follows. If you\u2019ve only got a portion of your signal seeping through to the next layer during training, you should scale the expectation of what the energy of your final result should be. Seems like a weird thing to care about, right? The argument that minimizes x is still the same as the argument that minimizes 2x. This turns out to be a problem when you\u2019re passing multiple gradients back and don\u2019t implement your layers uniformly. Caffe works in instances like Siamese Networks or Bilinear Networks, but should you scale your networks on two sides differently, don\u2019t be surprised if you\u2019re getting unexpected results.\n\nWhat does this look like in Francois\u2019s code? Look at the \u201cDropout\u201d code on Github, or in your installation folder under keras/layers/core.py. If you want to make your own layer for loading in the Dropout module, just comment out the part of the code that does this scaling:\n\nYou can modify the original code, or you can create your own custom layer. (We\u2019ve opted to keep our installation of Keras clean and just implemented a new class that extended MaskedLayer.) BTW, you should be careful in your use of Dropout. Our experience with them is that they regularize okay, but could contribute to vanishing gradients really quickly.\n\nEveryday except for Sunday and some holidays, a select few machine learning professors and some signal processing leaders meet in an undisclosed location in the early hours of the morning. The topic of their discussion is almost universally, \u201cHow do we get researchers and deep learning practitioners to code bugs into their programs?\u201d One of the conclusions a while back was that the definition of convolution and dense matrix multiplication (or cross-correlation) should be exactly opposite of each other. That way, when people are building algorithms that call themselves \u201cConvolutional Neural Networks\u201d, no one will know which implementation is actually being used for the convolution portion itself.\n\nFor those who don\u2019t know, convolutions and sweeping matrix multiplication across an array of data, differ in that convolutions will be flipped before being slid across the array. From Wikipedia, the definition is:\n\nOn the other hand, if you\u2019re sweeping matrix multiplications across the array of data, you\u2019re essentially doing cross-correlation, which on Wikipedia, looks like:\n\nLike we said, the only difference is that darned minus/plus sign, which caused us some headache.\n\nWe happen to know that Theano and Caffe follow different philosophies. Once again, Caffe doesn\u2019t bother with pleasantries and straight up codes efficient matrix multiplies. To load models from ModelZoo into either Keras and Theano will require the transformation because they strictly follow the definition of convolution. The easy fix is to flip it yourself when you\u2019re loading the weights into your model. For 2D convolution, this looks like:\n\nHere, the variable \u201cweights\u201d will be inserted into your model\u2019s parameters. You can set weights by indexing into the model. For example, say I want to set the 9th layer\u2019s weights. I would type:\n\nIncidentally, and this is important, when loading any *.caffemodel into Python, you may have to transpose it in order to use it. You can quickly find this out by loading it if you get an error, but we thought it worth noting.\n\nAlright, alright, we know what you\u2019re really here for; just getting the code and running with it. So, we\u2019ve got some example code that classifies using Keras and the VGG net from the web at our Git (see the link below). But, let\u2019s go through it just a bit. Here\u2019s a step by step account of what you need to do to use the VGG caffe model.\n\nAnd now you have the basics! Go ahead and take a look at our Github for some goodies. Let us know!", 
        "title": "Taking Keras to the Zoo \u2013"
    }, 
    {
        "url": "https://medium.com/machine-intelligence-report/deep-learning-for-hackers-with-mxnet-1-gpu-installation-and-mnist-428a3e165ca?source=tag_archive---------7----------------", 
        "text": "I am going to have a series of blogs about implementing deep learning models and algorithms with MXnet. The topic list covers MNIST, LSTM/RNN, image recognition, neural artstyle image generation etc. Everything here is about programing deep learning (a.k.a. deep learning for hackers), instead of theoritical tutorials, so basic knowledge of machine learning and neural network is a prerequisite. I assume readers know how neural network works and what backpropagation is. If difficulties, please review Andew Ng\u2019s coursera class Week 4: https://www.coursera.org/learn/machine-learning.\n\nSurely, this blog doesn\u2019t cover everything about deep learning. It is very important to understand the fundamental deep learning knowledge. For readers who want to know in-depth theoritical deep learning knowledge, please read some good tutorials, for example, http://deeplearning.net/reading-list/tutorials/.\n\nMXnet is a deep learning toolkit written in C++11, and it comes with DMLC (Distributed (Deep) Machine Learning Common http://dmlc.ml/). You might have known MXnet\u2019s famous DMLC-sibling xgboost https://github.com/dmlc/xgboost, a parallel gradient boosting decision tree which dominates most Kaggle competitions and is generally used in many projects.\n\nMXnet is very lightweight, dynamic, portable, easy to distribute, memory efficient, and one of the coolest features is, it can run on portable devices (e.g. image recognition on your Android phone ) MXnet also has clear design plus clean C++11 code, let go star and fork it on github: https://github.com/dmlc/mxnet\n\nRecently MXnet has received much attention in multiple conferences and blogs for its unique features of speed and efficient memory usage. Professionals are comparing MXnet with Caffe, Torch7 and Google\u2019s TensorFlow. These benchmarks show that MXnet is a new rising star. Go check this recent tweet from Quora\u2019s Xavier Amatriain: https://twitter.com/xamat/status/665222179668168704\n\nMXnet natively supports multiple platforms (Linux, Mac OS X and Windows) and multiple languages (C++, Java, Python, R and Julia, plus a recent support on javascript, no joking MXnet.js). In this tutorial, we use Ubuntu 14.04 LTS and Python for example. Just a reminder that, since we use CUDA for GPU computing and CUDA hasn\u2019t yet support ubuntu 15.10 or newer (with gcc 5.2), let\u2019s stay with 14.04 LTS, or, at latest 15.04.\n\nThe installation can be done on physical machines with nVidia CUDA GPUs or cloud instance, for example AWS GPU instance g2.2xlarge or g2.8xlarge. The following steps mostly come from the official installation guide http://mxnt.ml/en/latest/build.html#building-on-linux, with some CUDA modification.\n\nPlease note: for installing CUDA on AWS from scratch, some additional steps are needed for updating linux-image-extra-virtual and disabling nouveau, for more details, please refer to Caffe\u2019s guide: https://github.com/BVLC/caffe/wiki/Install-Caffe-on-EC2-from-scratch-(Ubuntu,-CUDA-7,-cuDNN)\n\nMXnet only needs minimal dependency: gcc, BLAS, and OpenCV (optional), that is it. One can install git just in case it hasn\u2019t been installed.\n\nJust another reminder that\u200a\u2014\u200arecursive is needed: MXnet depends on DMLC common packages mshadow, ps-lite and dmlc-core, where\u200a\u2014\u200arecursive can clone all necessary ones. Please don\u2019t compile now, and we need to install CUDA firstly.\n\nCUDA installation here is universal for other deep learning packages. Please go to https://developer.nvidia.com/cuda-downloads for selecting the CUDA installation for the corresponding system. For example, installing CUDA for Ubuntu 14.04 should looks like this, and deb(network) is suggested for fastest downloading from the closest Ubuntu source.\n\nOr, here it is the command-line-only solution\uff1a\n\nIf everything goes well, please check the video card status by nvidia-smi, and it should look like this\uff1a\n\nCPU info may vary, and I am using a GTX 960 4GB (approximately 200$ now). MXnet has very efficient memory usage, and 4GB is good for most of the problems. If your video card has only 2GB, MXnet is fine with it with some small parameter tunes too.\n\nOptional: CuDNN. Mxnet supports cuDNN too. cuDNN is nVidia deep learning toolkit which optimizes operations like convolution, poolings etc, for better speed and memory usage. Usually it can speed up MXnet by 40% to 50%. If interested, please go apply for the developer program here https://developer.nvidia.com/cudnn, and install cuDNN by the official instruction when approved,\n\nMXnet needs to turn on CUDA support in the configuration. Please find config.mk from mxnet/make/, copy to mxnet/, and edit these three lines:\n\nwhere the second line is for CUDA installation path. The path usually is /usr/local/cuda or /usr/local/cuda-7.5. If readers prefer other BLAS implementations. e.g. OpenBlas or Atlas, please change USE_BLAS to openblas or atlas and add the blas path to ADD_LDFLAGS and ADD_CFLAGS.\n\nWe can compile MXnet with CUDA (-j4 for multi-thread compiling):\n\nOne more reminder that, if one has non-CUDA video cards, for example Intel Iris or AMD R9, or there is not video card, please change USE_CUDA to 0. MXnet is dynamic for switching between CPU and GPU: instead of GPU version, one can compile multi-theading CPU version by setting USE_OPENMP = 1 or leave it to 0 so BLAS can take care of multi-threading, either way is fine with MXnet.\n\nMXnet natively supports Python, one can simply do:\n\nPython 2.7 is suggested while Python 3.4 is also supported. One might need setuptools and numpy if not yet installed. I personally suggest Python from Anaconda or Miniconda\n\nNow we have a GPU-ready MXnet, let\u2019s have the first deep learning example: MNIST. MNIST is a handwritten digit dataset with 60,000 training samples and 10,000 testing samples, where each sample is a 28X28 greyscale picture of digits, and the goal of MNIST is training a smart machine learning model for recognizing hand-writing digits, for example, it recognizes the zip code that people write on envelops and helps our post masters distribute the mails.\n\ntrain_mnist.py will download MNIST dataset for the first time, please be patient while downloading. The sample code will print out the loss and precision for each step, and the final result should be approximately 97%.\n\ntrain_mnist.py by default uses CPU only. MXnet has easy switch between CPU and GPU. Since we have GPU, let\u2019s turn it on by:\n\nThat is it.\u200a\u2014\u200agpus 0 means using the first GPU. If one has multiple GPUs, for example 4 GPUs, one can set\u200a\u2014\u200agpus 0,1,2,3 for using all of them. While running with GPU, the nvidia-smi should look like this\uff1a\n\nwhere one can see python is using GPU. Since MNIST is not a heavy task, with MXnet efficient GPU meomory usage, GPU usage is about 30\u201340% while memory usage at 67MB\u3002\n\nWhen run with GPU for the first time, readers may encounter something like this\uff1a\n\nIt is because of the PATH of CUDA dynamic link lib, one can add this to\u00a0./bashrc\uff1a\n\nOr compile it to MXnet by adding in config.mk:\n\nIn train_mnist.py, there is an function get_mlp(). It implements a multilayer perceptron (MLP). In MXnet, a MLP needs some structure definition, like this in the code:\n\nLet\u2019s understand what is going on for this neural network. Samples in MNIST look like these:\n\nWith this network structure, MXnet also needs the stucture of input. Since each sample is 28X28 grey scale, MXnet takes the grey scale value vector of 28X28=784 elements, and give a python iterator get_iterator() for feeding data to the network defined above. The detailed code is in the example which is very clean, so I don\u2019t copy-n-paste here.\n\nThe final step is running the model. If readers know scikit-learn, MXnet\u2019s python looks very familiar, right?\n\nCongratulations! We can implement a MLP! It is the first step of deep learning, not hard, right?\n\nIt is Q&A time now. Some of my dear readers may ask, \u201cDo I need to design my MNIST recognizer or some other deep learning network exactly like what you did here?\u201d \u201chey, I see another function get_lenet() in the code, what is that?\u201d\n\nThe answer to these two questions can be, most real life problems on deep learning are about designing the networks, and, no, you don\u2019t have to design your network exactly like what I did here. Designing the neural network is art, and each problem needs a different network, and a single problem may have multiple solutions. In the MNIST example code, get_lenet() implements Yann Lecun\u2019s convolution network LeNet for digit recognition, where each layer needs Convolution Activation and Pooling where the kernel size and filter are needed, instead of FullyConnected and ReLU. FYI: the detailed explanation of super cool convolution network (ConvNet) can be found at Yann Lecun\u2019s tutorial: http://www.cs.nyu.edu/~yann/talks/lecun-ranzato-icml2013.pdf\u00a0. Another good reference can be \u201cUnderstanding ConvNet\u201d by Colah. I may later on write a blog post for explaining ConvNet, since convnet is my personal favorite.\n\nI have a fun homework for my dear readers. Let\u2019s tune up some network parameters like the number of nodes and the activation function in get_mlp(), and see how it helps the precision and accuracy. One can also try changing num_epoch (number of iterations of learning from the data) and learning_rate (the speed of gradient decent, a.k.a the rate of learning) for better speed and precision. Please leave your comment for your network structure and precision score. Kaggle also has a MNIST competition, one can also go compete with mxnet MNIST models, please mention it is MXnet model. The portal: https://www.kaggle.com/c/digit-recognizer\n\nThanks for reading the first blog of \u201cDeep learning for hackers with MXnet\u201d. The following blogs will include some examples in MXnet, which may include RNN/LSTM for generating a Shakespeare script (well, looks like Shakespeare), generative models of simulating Van Gogh for painting a cat, etc. Some of these models are available now on MXnet github https://github.com/dmlc/mxnet/tree/master/example. Is MXnet cool? Go star and fork it on github https://github.com/dmlc/mxnet", 
        "title": "Deep learning for hackers with MXnet (1) GPU installation and MNIST"
    }, 
    {
        "url": "https://medium.com/@DemiurgeTech/the-quest-for-the-brain-chip-c8cbacda378e?source=tag_archive---------8----------------", 
        "text": "The topic of brain chips has seen a surge of interest lately around the world. A variety of scientific and commercial efforts seek ways to literally model the brain in silicon in hopes of enabling unprecedented human-like capabilities by decidedly unhuman devices like drones, robots and driverless cars. Large-scale collaborations that bring together neuroscientists and computer scientists\u200a\u2014\u200asuch as Europe\u2019s The Human Brain Project, the DARPA-fundedSYNAPSE program and the American BRAIN Initiative\u200a\u2014\u200acapture headlines and imaginations.\n\nHaving recently attended two relevant conferences\u200a\u2014\u200athe Brain Forum and the CapoCaccia Neuromorphic Engineering Workshop\u200a\u2014\u200aI have to wonder, though, if we are setting off in the right direction in our pursuit of perhaps the loftiest of all scientific aspirations.\n\nLet\u2019s get our bearings first. A computer is a machine that represents and processes information. Ever since the advent of modern computers, the goal of brain science is to understand how the brain works as a computer, and the goal of artificial intelligence is to build brain-like computers. The founding fathers of modern computers, however, differ in their views of whether brains are essentially modern computers: Alan Turing insisted that brains and modern computers share the same computational model that bears his name, whereas John von Neumann believed that brains are fundamentally different from the architecture of modern computers that also bears his name.\n\nDeep learning and neuromorphic engineering are prime examples of the cross-fertilization between the goals of brain science and artificial intelligence. There is a wide consensus among deep learning and neuromorphic engineering research groups that both Turing and von Neumman are correct: Understanding how the brain works and building brain-like computers need to change the von Neumann architecture while preserving the Turing computational model.\n\nHowever, it can\u2019t be the case that both Turing and von Neumann are correct for one simple reason: a computer architecture is merely a physical implementation scheme of a computational model that is actually a mathematical construction. As a matter of fact, the von Neumann architecture can\u2019t be fundamentally changed without a fundamental change to the underlying Turing computational model, and vice versa. How soon should we expect either the theoretical or the engineering fundamental change to happen? Which fundamental change should we expect to come first?\n\nIt is natural to think that a fundamentally new theory of computation should come before a fundamentally new architecture of a computer since the history of technology is a never-ending demonstration of a theoretical model heralding a physical system. On the other hand, the history of science has countless examples of how blind we are at understanding the theoretical models of even simple physical system through reverse engineering. After all, Turing\u2019s theory came almost a decade earlier than von Neumman\u2019s architecture for modern computers.\n\nGiven that the brain is arguably the most complex physical system in the universe, we would be most blind at reverse-engineering the brain. Such blindness has directed the long list of broken promises in the short history of artificial intelligence. The ignorance of such blindness is precipitating artificial intelligence and brain science into another crisis of putting the cart before the horse. Brain projects like the Human Brain Project and the BRAIN Initiative promote an open collaboration of collecting enormous volumes of comprehensive brain data, but there exists not even a remote analogy of such commitment to an open collaboration of proposing comprehensive brain theories. Neuromorphic projects like IBM TrueNorth and Qualcomm Zeroth claim to have successfully developed brain-inspired architectures fundamentally different from that of von Neumann, but none of them has a clue about the supposedly novel computational model fundamentally different from Turing\u2019s.\n\nWhy are many bright minds and resourceful organizations expecting a revolution but gearing towards an evolution? My observations at the aforementioned events indicate that it may be a problem of benchmarking progresses in artificial intelligence and brain science. Evolution needs benchmarks for improvements that give us better results, yet revolution requires benchmarks for breakthroughs that bring us closer to our goals. The engineering culture of getting something done to make things work better has led to very effective benchmarks like ImageNet in computer vision, yet it further tethers the community to the ignorance of our blindness to the flaws of reverse-engineering. It is simply impossible to accelerate fast enough to stay in the correct lanes if we keep gearing towards evolution on the highway of revolution. Although it\u2019s much harder to come up with a breakthrough benchmark, we would nonetheless be able to see quite a few principled clues from comparing biological brains with modern computers.\n\nThe first clue is that brains and modern computers deal with different kinds of information. Brains of every species from the 302-neuron C.elegans to the 100-billion-neuron H.sapiens deal with sensory information collected from species-specific multi-modal sensors. In contrast, modern computers of every kind from smartphones to supercomputers deal with symbolic information predefined by application-oriented human programmers. Consistent with the Moravec\u2019s paradox, which is considered to be the most significant discovery in artificial intelligence, sensory information processing is extremely easy for brains but extremely hard for modern computers, whereas symbolic information processing is comparably hard for brains but extremely easy for modern computers. Even if the brain\u2019s computational model could be as universal as the Turing Machine, they are fundamentally different models processing different kinds of information.\n\nThe second clue is that brains and modern computers have different power-performance dynamics. Brains achieve orders-of-magnitude better performance and orders-of-magnitude lower power consumption than modern computers for sensory information processing. In the brain\u2019s computational model, power consumption scales sublinearly with algorithmic performance. In the Turing\u2019s computational model, however, power consumption scales superlinearly with algorithmic performance. Even if a neuromorphic architecture like IBM TrueNorth achieves brain-level power consumption or a deep neural network like Facebook DeepFace achieves human-level performance, neither can be called a breakthrough if the relative power-performance dynamics remains unchanged.\n\nThe third clue is that brains and modern computers have different processor-memory relationships. Brains feature a process-memory unity such that every computational unit is equally responsible for data processing, storage and transportation. In contrast, modern computers feature a processor-memory dichotomy such that some computational units are responsible for data processing and others are responsible for data storage. If the brain\u2019s computational model dictates processor-memory unity, then even if a neuromorphic architecture minimizes the process-memory distance to its theoretical limit, it\u2019s not fundamentally different from a von Neumann architecture.\n\nThe exponential growth of sensors and sensory information is catalyzing the paradigm shift from symbolic computing to sensory computing. Yet it often gets overlooked that sensory computers have already existed for hundreds of millions of years before the first symbolic computer was invented seventy years ago. The history of science has indicated that many observably complex systems are actually governed by rather simple universal laws, of which the modern computer is a paradigmatic example. Hence, the fourth clue is that the computational model of sensory computers has to be at least as simple and elegant as the Turing Machine. Otherwise, it would be impossible for biological sensory computers to survive and thrive in a far more energy-stingy world than electricity-powered artificial symbolic computers.\n\nIt is always possible but actually improbable to reverse-engineer the brain because we are most likely to start with wrong assumptions to which we are blind until at the end of our tether. However, from those four clues above we can piece together a breakthrough benchmark to guide our seemingly intractable quest for finding the \u201cTuring Machine\u201d for sensory computers: Binary codes are used by brains to represent sensory information generated physically by sensors, and they are also used by modern computers to represent symbolic information defined arbitrarily by humans. Every pair of identical binary codes respectively generated by a brain and a modern computer can still represent different information because every bit of the brain\u2019s binary code is subjective to additional physical constraints that are totally lacking for every bit of the computer\u2019s binary code. If two tapes of otherwise identical binary codes could have different spaces of information, then clearly Turing Machine can\u2019t handle such differences.\n\nAs far as neural networks and neuromorphic chips are concerned, we are at a turbulent stage awaiting for the next Alan Turing. The field of neuromorphic computing has long been dominated by improvements made by European researchers. My observations in Switzerland and Italy also indicate that Europe is more readily geared towards a breakthrough in computing than anywhere else in the world because European leaders have built the right kinds of infrastructures to facilitate homerun neuromorphic innovations:Steve Furber\u2019s group at the University of Manchester has built the world\u2019s first digital architecture SpiNNaker for general-purpose neural network modeling. Giacomo Indiveri\u2019s group at the Institute of Neuroinformatics of the University of Zurich and ETH Zurich has built the world\u2019s finest subthreshold-domain analogue architecture for cognitive neural network implementation.\n\nThe boundary between the academia and the industry have never been so blurred and the scientific and economic interests have never been so aligned that a breakthrough in the neural network model would bring quantum leaps to entire humanity. The global ecosystem of biological and artificial neural network research and applications thus call for new models of innovation like the CapoCaccia Workshop and new criteria of evaluation such as the ones in the Misha Mahoward Prize to accelerate such breakthroughs. We can\u2019t predict the time and the place to find the next Alan Turing, yet we can definitely prepare the ecosystem to attract the next Alan Turing.", 
        "title": "The Quest for the Brain Chip: \u2013 Demiurge Technologies AG \u2013"
    }, 
    {
        "url": "https://medium.com/@DemiurgeTech/demiurge-raised-9-5-million-to-upgrade-deep-reinforcement-learning-with-spiking-neural-networks-d03465926bb2?source=tag_archive---------9----------------", 
        "text": "Demiurge Technologies AG (www.demiurge.technology) is a Switzerland-based artificial intelligence company developing the next generation of deep neural networks and brain chips for mobile robots. Demiurge has raised $9.5 million funding 6 months after its launch in May 2015, including $1.8 million angel round from Lun Feng at Vantone Holdings among three angel investors; and $7.7 million A-1 round from Hongdao Capital.\n\nTrue intelligence requires a self-supervised, fully-adaptive, and always-online learning of the world model via dynamic physical agent-environment interactions through a closed sensory-motor feedback loop. Most of the envisaged AI products and applications (e.g. fully autonomous vehicles, home service robots, space exploratory robots, etc.) require realized true intelligence to deliver much cheaper solutions with much better performance under stringent constraints (power, latency, stability, etc.) in real user cases.\n\nDeep Reinforcement Learning cannot realize true intelligence because deep learning and reinforcement learning are two wrong parts that couldn\u2019t make a right whole. On the one hand, the neurons of deep learning are too simple to take advantage of spatiotemporal complexity for modeling objects and understanding scenes. On the other hand, the rewards of reinforcement learning are too simple to take advantage of spatiotemporal perception-action correlations for finding the optimal policy. In summary, the blindness of deep learning and the naiveness of reinforcement learning prohibit deep reinforcement learning from realizing high-performance autonomous learning via the closed sensory-motor feedback loop.\n\nDeep Learning 2.0 with redesigned neuronshas the potential of realizing true intelligence as it is optimal for spatiotemporal pattern recognition and action selection, and it is developed from a rebuilt foundation of mathematics, physics, neuroscience and computer science. Deep Learning 2.0 in this context refers to the general physical mechanism of sensory information processing in biological neural networks.\n\nThe new world of artificial intelligence can only be discovered when they sail away from the charted island of deep learning towards the uncharted waters. Understanding directions may be unnecessary for wanderers on the island but critical for sailers in the ocean, as the race of discovery in artificial intelligence ends when it starts with wrong directions. Demiurge has set course with the compass of first principles and has maintained top speed by adopting the synthetic methodology that bridges model testing and product prototyping by iteratively building entire systems and thus could substantially reduce the time-to-market of artificial intelligence products.\n\nIt is one thing for islanders to imagine the impact of discoveries from the new world, but it is another thing for explorers to deliver the benefit of those discoveries to the entire mankind. Demiurge\u2019s core value is to benefit all lives with true intelligence, and they are creating children-friendly workspace, pioneering family-based benefits, celebrating a communal culture, and championing meritocracy and candor.\n\nThe Demiurge family is the crew of a ship embarking on a journey towards the greatest unknowns. It takes extraordinary courage to elbow through the crowded pier, and it takes extraordinary composure to enjoy the focus from loneliness and the hope from uncertainty. All members of the Demiurge crew lead independently with head and interdependently with heart, because their life trajectories have shown enduring genuine interest in Demiurge\u2019s mission, tailored interdisciplinary background to advance it, and tested fearless mindset to achieve it. You are most welcome to board Demiurge (send cover letter and CV to crew@demiurge.technology) if you are ready for the new Age of Discovery!", 
        "title": "Demiurge Raised $9.5 Million to Develop Deep Learning 2.0 for Mobile Robots"
    }
]