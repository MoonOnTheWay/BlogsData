[
    {
        "url": "https://medium.com/@DemiurgeTech/the-quest-for-the-brain-chip-c8cbacda378e?source=tag_archive---------0----------------", 
        "text": "The topic of brain chips has seen a surge of interest lately around the world. A variety of scientific and commercial efforts seek ways to literally model the brain in silicon in hopes of enabling unprecedented human-like capabilities by decidedly unhuman devices like drones, robots and driverless cars. Large-scale collaborations that bring together neuroscientists and computer scientists\u200a\u2014\u200asuch as Europe\u2019s The Human Brain Project, the DARPA-fundedSYNAPSE program and the American BRAIN Initiative\u200a\u2014\u200acapture headlines and imaginations.\n\nHaving recently attended two relevant conferences\u200a\u2014\u200athe Brain Forum and the CapoCaccia Neuromorphic Engineering Workshop\u200a\u2014\u200aI have to wonder, though, if we are setting off in the right direction in our pursuit of perhaps the loftiest of all scientific aspirations.\n\nLet\u2019s get our bearings first. A computer is a machine that represents and processes information. Ever since the advent of modern computers, the goal of brain science is to understand how the brain works as a computer, and the goal of artificial intelligence is to build brain-like computers. The founding fathers of modern computers, however, differ in their views of whether brains are essentially modern computers: Alan Turing insisted that brains and modern computers share the same computational model that bears his name, whereas John von Neumann believed that brains are fundamentally different from the architecture of modern computers that also bears his name.\n\nDeep learning and neuromorphic engineering are prime examples of the cross-fertilization between the goals of brain science and artificial intelligence. There is a wide consensus among deep learning and neuromorphic engineering research groups that both Turing and von Neumman are correct: Understanding how the brain works and building brain-like computers need to change the von Neumann architecture while preserving the Turing computational model.\n\nHowever, it can\u2019t be the case that both Turing and von Neumann are correct for one simple reason: a computer architecture is merely a physical implementation scheme of a computational model that is actually a mathematical construction. As a matter of fact, the von Neumann architecture can\u2019t be fundamentally changed without a fundamental change to the underlying Turing computational model, and vice versa. How soon should we expect either the theoretical or the engineering fundamental change to happen? Which fundamental change should we expect to come first?\n\nIt is natural to think that a fundamentally new theory of computation should come before a fundamentally new architecture of a computer since the history of technology is a never-ending demonstration of a theoretical model heralding a physical system. On the other hand, the history of science has countless examples of how blind we are at understanding the theoretical models of even simple physical system through reverse engineering. After all, Turing\u2019s theory came almost a decade earlier than von Neumman\u2019s architecture for modern computers.\n\nGiven that the brain is arguably the most complex physical system in the universe, we would be most blind at reverse-engineering the brain. Such blindness has directed the long list of broken promises in the short history of artificial intelligence. The ignorance of such blindness is precipitating artificial intelligence and brain science into another crisis of putting the cart before the horse. Brain projects like the Human Brain Project and the BRAIN Initiative promote an open collaboration of collecting enormous volumes of comprehensive brain data, but there exists not even a remote analogy of such commitment to an open collaboration of proposing comprehensive brain theories. Neuromorphic projects like IBM TrueNorth and Qualcomm Zeroth claim to have successfully developed brain-inspired architectures fundamentally different from that of von Neumann, but none of them has a clue about the supposedly novel computational model fundamentally different from Turing\u2019s.\n\nWhy are many bright minds and resourceful organizations expecting a revolution but gearing towards an evolution? My observations at the aforementioned events indicate that it may be a problem of benchmarking progresses in artificial intelligence and brain science. Evolution needs benchmarks for improvements that give us better results, yet revolution requires benchmarks for breakthroughs that bring us closer to our goals. The engineering culture of getting something done to make things work better has led to very effective benchmarks like ImageNet in computer vision, yet it further tethers the community to the ignorance of our blindness to the flaws of reverse-engineering. It is simply impossible to accelerate fast enough to stay in the correct lanes if we keep gearing towards evolution on the highway of revolution. Although it\u2019s much harder to come up with a breakthrough benchmark, we would nonetheless be able to see quite a few principled clues from comparing biological brains with modern computers.\n\nThe first clue is that brains and modern computers deal with different kinds of information. Brains of every species from the 302-neuron C.elegans to the 100-billion-neuron H.sapiens deal with sensory information collected from species-specific multi-modal sensors. In contrast, modern computers of every kind from smartphones to supercomputers deal with symbolic information predefined by application-oriented human programmers. Consistent with the Moravec\u2019s paradox, which is considered to be the most significant discovery in artificial intelligence, sensory information processing is extremely easy for brains but extremely hard for modern computers, whereas symbolic information processing is comparably hard for brains but extremely easy for modern computers. Even if the brain\u2019s computational model could be as universal as the Turing Machine, they are fundamentally different models processing different kinds of information.\n\nThe second clue is that brains and modern computers have different power-performance dynamics. Brains achieve orders-of-magnitude better performance and orders-of-magnitude lower power consumption than modern computers for sensory information processing. In the brain\u2019s computational model, power consumption scales sublinearly with algorithmic performance. In the Turing\u2019s computational model, however, power consumption scales superlinearly with algorithmic performance. Even if a neuromorphic architecture like IBM TrueNorth achieves brain-level power consumption or a deep neural network like Facebook DeepFace achieves human-level performance, neither can be called a breakthrough if the relative power-performance dynamics remains unchanged.\n\nThe third clue is that brains and modern computers have different processor-memory relationships. Brains feature a process-memory unity such that every computational unit is equally responsible for data processing, storage and transportation. In contrast, modern computers feature a processor-memory dichotomy such that some computational units are responsible for data processing and others are responsible for data storage. If the brain\u2019s computational model dictates processor-memory unity, then even if a neuromorphic architecture minimizes the process-memory distance to its theoretical limit, it\u2019s not fundamentally different from a von Neumann architecture.\n\nThe exponential growth of sensors and sensory information is catalyzing the paradigm shift from symbolic computing to sensory computing. Yet it often gets overlooked that sensory computers have already existed for hundreds of millions of years before the first symbolic computer was invented seventy years ago. The history of science has indicated that many observably complex systems are actually governed by rather simple universal laws, of which the modern computer is a paradigmatic example. Hence, the fourth clue is that the computational model of sensory computers has to be at least as simple and elegant as the Turing Machine. Otherwise, it would be impossible for biological sensory computers to survive and thrive in a far more energy-stingy world than electricity-powered artificial symbolic computers.\n\nIt is always possible but actually improbable to reverse-engineer the brain because we are most likely to start with wrong assumptions to which we are blind until at the end of our tether. However, from those four clues above we can piece together a breakthrough benchmark to guide our seemingly intractable quest for finding the \u201cTuring Machine\u201d for sensory computers: Binary codes are used by brains to represent sensory information generated physically by sensors, and they are also used by modern computers to represent symbolic information defined arbitrarily by humans. Every pair of identical binary codes respectively generated by a brain and a modern computer can still represent different information because every bit of the brain\u2019s binary code is subjective to additional physical constraints that are totally lacking for every bit of the computer\u2019s binary code. If two tapes of otherwise identical binary codes could have different spaces of information, then clearly Turing Machine can\u2019t handle such differences.\n\nAs far as neural networks and neuromorphic chips are concerned, we are at a turbulent stage awaiting for the next Alan Turing. The field of neuromorphic computing has long been dominated by improvements made by European researchers. My observations in Switzerland and Italy also indicate that Europe is more readily geared towards a breakthrough in computing than anywhere else in the world because European leaders have built the right kinds of infrastructures to facilitate homerun neuromorphic innovations:Steve Furber\u2019s group at the University of Manchester has built the world\u2019s first digital architecture SpiNNaker for general-purpose neural network modeling. Giacomo Indiveri\u2019s group at the Institute of Neuroinformatics of the University of Zurich and ETH Zurich has built the world\u2019s finest subthreshold-domain analogue architecture for cognitive neural network implementation.\n\nThe boundary between the academia and the industry have never been so blurred and the scientific and economic interests have never been so aligned that a breakthrough in the neural network model would bring quantum leaps to entire humanity. The global ecosystem of biological and artificial neural network research and applications thus call for new models of innovation like the CapoCaccia Workshop and new criteria of evaluation such as the ones in the Misha Mahoward Prize to accelerate such breakthroughs. We can\u2019t predict the time and the place to find the next Alan Turing, yet we can definitely prepare the ecosystem to attract the next Alan Turing.", 
        "title": "The Quest for the Brain Chip: \u2013 Demiurge Technologies AG \u2013"
    }, 
    {
        "url": "https://medium.com/@DemiurgeTech/demiurge-raised-9-5-million-to-upgrade-deep-reinforcement-learning-with-spiking-neural-networks-d03465926bb2?source=tag_archive---------1----------------", 
        "text": "Demiurge Technologies AG (www.demiurge.technology) is a Switzerland-based artificial intelligence company developing the next generation of deep neural networks and brain chips for mobile robots. Demiurge has raised $9.5 million funding 6 months after its launch in May 2015, including $1.8 million angel round from Lun Feng at Vantone Holdings among three angel investors; and $7.7 million A-1 round from Hongdao Capital.\n\nTrue intelligence requires a self-supervised, fully-adaptive, and always-online learning of the world model via dynamic physical agent-environment interactions through a closed sensory-motor feedback loop. Most of the envisaged AI products and applications (e.g. fully autonomous vehicles, home service robots, space exploratory robots, etc.) require realized true intelligence to deliver much cheaper solutions with much better performance under stringent constraints (power, latency, stability, etc.) in real user cases.\n\nDeep Reinforcement Learning cannot realize true intelligence because deep learning and reinforcement learning are two wrong parts that couldn\u2019t make a right whole. On the one hand, the neurons of deep learning are too simple to take advantage of spatiotemporal complexity for modeling objects and understanding scenes. On the other hand, the rewards of reinforcement learning are too simple to take advantage of spatiotemporal perception-action correlations for finding the optimal policy. In summary, the blindness of deep learning and the naiveness of reinforcement learning prohibit deep reinforcement learning from realizing high-performance autonomous learning via the closed sensory-motor feedback loop.\n\nDeep Learning 2.0 with redesigned neuronshas the potential of realizing true intelligence as it is optimal for spatiotemporal pattern recognition and action selection, and it is developed from a rebuilt foundation of mathematics, physics, neuroscience and computer science. Deep Learning 2.0 in this context refers to the general physical mechanism of sensory information processing in biological neural networks.\n\nThe new world of artificial intelligence can only be discovered when they sail away from the charted island of deep learning towards the uncharted waters. Understanding directions may be unnecessary for wanderers on the island but critical for sailers in the ocean, as the race of discovery in artificial intelligence ends when it starts with wrong directions. Demiurge has set course with the compass of first principles and has maintained top speed by adopting the synthetic methodology that bridges model testing and product prototyping by iteratively building entire systems and thus could substantially reduce the time-to-market of artificial intelligence products.\n\nIt is one thing for islanders to imagine the impact of discoveries from the new world, but it is another thing for explorers to deliver the benefit of those discoveries to the entire mankind. Demiurge\u2019s core value is to benefit all lives with true intelligence, and they are creating children-friendly workspace, pioneering family-based benefits, celebrating a communal culture, and championing meritocracy and candor.\n\nThe Demiurge family is the crew of a ship embarking on a journey towards the greatest unknowns. It takes extraordinary courage to elbow through the crowded pier, and it takes extraordinary composure to enjoy the focus from loneliness and the hope from uncertainty. All members of the Demiurge crew lead independently with head and interdependently with heart, because their life trajectories have shown enduring genuine interest in Demiurge\u2019s mission, tailored interdisciplinary background to advance it, and tested fearless mindset to achieve it. You are most welcome to board Demiurge (send cover letter and CV to crew@demiurge.technology) if you are ready for the new Age of Discovery!", 
        "title": "Demiurge Raised $9.5 Million to Develop Deep Learning 2.0 for Mobile Robots"
    }
]