[
    {
        "url": "https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02?source=tag_archive---------0----------------", 
        "text": "Here are some mostly more advanced thoughts about this project, what other things I might try and why it makes sense to me that this may actually work.\n\nLiquidity and efficient use of capital\n\nGenerally the more liquid a particular market is the more efficient that is. I think this is due to a chicken and egg cycle, whereas a market becomes more liquid it is able to absorb more capital moving in and out without that capital hurting itself. As a market becomes more liquid and more capital can be used in it, you\u2019ll find more sophisticated players moving in. This is because it is expensive to be sophisticated, so you need to make returns on a large chunk of capital in order to justify your operational costs.\n\nA quick corollary is that in less liquid markets the competition isn\u2019t quite as sophisticated and so the opportunities a system like this can bring may not have been traded away. The point being were I to try and trade this I would try and trade it on less liquid segments of the market, that is maybe the TASE 100 instead of the S&P 500.\n\nThis stuff is new\n\nThe knowledge of these algorithms, the frameworks to execute them and the computing power to train them are all new at least in the sense that they are available to the average Joe such as myself. I\u2019d assume that top players have figured this stuff out years ago and have had the capacity to execute for as long but, as I mention in the above paragraph, they are likely executing in liquid markets that can support their size. The next tier of market participants, I assume, have a slower velocity of technological assimilation and in that sense, there is or soon will be a race to execute on this in as yet untapped markets.\n\nWhile I mentioned a single stream of inputs in the above, I imagine that a more efficient way to train would be to train market vectors (at least) on multiple time frames and feed them in at the inference stage. That is, my lowest time frame would be sampled every 30 seconds and I\u2019d expect the network to learn dependencies that stretch hours at most.\n\nI don\u2019t know if they are relevant or not but I think there are patterns on multiple time frames and if the cost of computation can be brought low enough then it is worthwhile to incorporate them into the model. I\u2019m still wrestling with how best to represent these on the computational graph and perhaps it is not mandatory to start with.\n\nWhen using word vectors in NLP we usually start with a pretrained model and continue adjusting the embeddings during training of our model. In my case, there are no pretrained market vector available nor is tehre a clear algorithm for training them.\n\nMy original consideration was to use an auto-encoder like in this paper but end to end training is cooler.\n\nA more serious consideration is the success of sequence to sequence models in translation and speech recognition, where a sequence is eventually encoded as a single vector and then decoded into a different representation (Like from speech to text or from English to French). In that view, the entire architecture I described is essentially the encoder and I haven\u2019t really laid out a decoder.\n\nBut, I want to achieve something specific with the first layer, the one that takes as input the 4000 dimensional vector and outputs a 300 dimensional one. I want it to find correlations or relations between various stocks and compose features about them.\n\nThe alternative is to run each input through an LSTM, perhaps concatenate all of the output vectors and consider that output of the encoder stage. I think this will be inefficient as the interactions and correlations between instruments and their features will be lost, and thre will be 10x more computation required. On the other hand, such an architecture could naively be paralleled across multiple GPUs and hosts which is an advantage.\n\nRecently there has been a spur of papers on character level machine translation. This paper caught my eye as they manage to capture long range dependencies with a convolutional layer rather than an RNN. I haven\u2019t given it more than a brief read but I think that a modification where I\u2019d treat each stock as a channel and convolve over channels first (like in RGB images) would be another way to capture the market dynamics, in the same way that they essentially encode semantic meaning from characters.", 
        "title": "Deep Learning the Stock Market \u2013 Tal Perry \u2013"
    }, 
    {
        "url": "https://medium.com/@baralsn/im-not-sure-about-your-first-step-embedding-your-4000-dimensional-vector-into-a-300-dimensional-3ada4b23865a?source=tag_archive---------1----------------", 
        "text": "I\u2019m not sure about your first step\u200a\u2014\u200aembedding your 4000 dimensional vector into a 300 dimensional space a la word2vec. Word2vec operates under the assumption that \u201cwords that appear in the same contexts, are similar\u201d. This is the basis of training the word2vec model. In the case of stocks, its not clear what \u201cthe same context means\u201d. Maybe you\u2019ve figured out what \u201cthe same context\u201d means for the stocks, but it wasn\u2019t clear from the article. Feel free to ping me if you want to continue to discuss this\u00a0:)", 
        "title": "I\u2019m not sure about your first step \u2014 embedding your 4000 dimensional vector into a 300 dimensional\u2026"
    }, 
    {
        "url": "https://chatbotslife.com/build-and-host-a-sentiment-analysis-model-based-on-naive-bayes-on-azure-ffef1bcc4ca0?source=tag_archive---------2----------------", 
        "text": "The dataset used for training the model consists of public tweets which are already labelled\u200a\u2014\u200apositive or negative. There are lots of npm package that implement the Naive Bayes algorithm out there and most of them are very similar in the way they implement Bayes theorem. Some packages are a bundle of NLP algorithms. The package used in the demo is this.\u00a0\n\nAfter initializing the bayes classifier in your node application, it is very easy to train the model and use it for real world applications.\u00a0\n\n\u00a0To train the model,\n\n\n\nTo identify the category of the sentence after building the model,\n\n\n\nTo extract the trained model in JSON,\n\n\n\n2. Using Socket.io for communication to server.\n\nOnce the model has been trained we need to host the JSON file alongwith our Nodejs application on a server. A html page has been created to collect sentences and determine its sentiment.\n\n3. Host on Azure\u00a0\n\nThis post would help in hosting your Nodejs application on Azure.\n\nDownload the code from git repo here", 
        "title": "Build and host a Sentiment Analysis Model based on Naive Bayes on Azure"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-18-c58516ba1e78?source=tag_archive---------4----------------", 
        "text": "It\u2019s finally here. NIPS 2016 is going to start on 10th December in Barcelona!\n\nBringing the Magic of Amazon AI and Alexa to Apps on AWS\n\nAnazon launches three new AI services that eliminate much of heavy lifting, making AI broadly accessible to all app developers\u200a\u2014\u200aAmazon Lex, Amazon Polly, and Amazon Rekognition.\n\nOpenAI co-founder and others testify at the US Senate hearing \u201cThe Dawn of Artificial Intelligence\u201d\n\nThe hearing conducted a broad overview of the state of artificial intelligence, including policy implications and effects on commerce and had Dr. Andrew Moore, Dr. Eric Horvitz and Greg Brockman among others on witness panel.", 
        "title": "\u2014 Issue #18 \u2013"
    }
]