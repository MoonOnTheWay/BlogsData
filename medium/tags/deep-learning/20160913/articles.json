[
    {
        "url": "https://chatbotnewsdaily.com/tensorflow-in-a-nutshell-part-two-hybrid-learning-98c121d35392?source=tag_archive---------0----------------", 
        "text": "A Wide and Deep Network combines a linear model with a feed forward neural net so that our predictions will have memorization and generalization. This type of model can be used for classification and regression problems. This allows for less feature engineering with relatively accurate predictions. Thus, getting the best of both worlds.\n\nThese hybrid learning methods are already in production by Google in the Play store for app suggestions. Even Youtube is using similar hybrid learning techniques to suggest videos.\n\nIn this article we will Demonstrate a Wide \u2018N Deep Network that will use wide linear model trained simultaneously with a feed forward network for more accurate predictions than some tradition machine learning techniques. This hybrid learning method will be used to predict Survival probability of Titanic passengers.\n\nWe are going to be using the Titanic Kaggle data to predict whether or not the passenger will survive based on certain attributes like Name, Sex, what ticket they had, the fare they paid the cabin they stayed in etc. For more information on this data set check out here at Kaggle.\n\nFirst off we\u2019re going to define all of our columns as Continuos or Categorical.\n\nContinuous columns\u200a\u2014\u200aany numerical value in a continuous range. Pretty much if it is a numerical representation like money, or age.\n\nCategorical columns\u200a\u2014\u200apart of a finite set. Like male or female, or even what country someone is from.\n\nSince we are only looking to see if a person survived, this is a binary classification problem. We predict a 1 if that person survives and a 0\u2026 if they do not\u00a0:(\u00a0, We then create a column solely for our survived category.\n\nNow we can get to creating the columns and adding embedding layers. When we build our model were going to want to change our categorical columns into a sparse column. For our columns with a small set of categories such as Sex or Embarked (S, Q, or C) we will transform them into sparse columns with keys\n\nThe other categorical columns have many more options than we want to put keys, and since we don\u2019t have a vocab file to map all of the possible categories into an integer we will hash them.\n\nOur continuous columns we want to use their real value. The reason passengerId is in continuous and not categorical is because they\u2019re not in string format and they\u2019re already an integer ID.\n\nWe are going to bucket the ages. Bucketization allows us to find the survival correlation by certain age groups and not by all the ages as a whole, thus increasing our accuracy.\n\nAlmost done, we are going to define our wide columns and our deep columns. Our wide columns are going to effectively memorize interactions between our features. Our wide columns don\u2019t generalize our features, this is why we have our deep columns.\n\nThe benefit of having these deep columns is that it takes our sparse high dimension features and reduces them into low dimensions.\n\nWe finish off our function by creating our classifier with our deep columns and wide columns,\n\nThe last thing we will have to do before running the network is create mappings for our continuous and categorical columns. What we are doing here by creating this function, and this is standard throughout the Tensorflow learning code, is creating an input function for our dataframe. This converts our dataframe into something that Tensorflow can manipulate. The benefit of this is that we can change and tweak how our tensors are being created. If we wanted we could pass feature columns into\u00a0.fit\u00a0.feature\u00a0.predict as an individually created column like we have above with our features, but this is a much cleaner solution.\n\nNow after all this we can write our training function\n\nWe read in our csv files that were preprocessed, like effectively imputed missing values, for simplicity sake. Details on how the files were preprocessed along with the code are contained in the repo.\n\nThese csv\u2019s are converted to tensors using our input_fn by lambda. we build our estimator then we print our predictions and print out our evaluation results.\n\nRunning our code as is gives us reasonably good results with out adding any extra columns or doing any great acts of feature engineering. With very little fine tuning this model can be used to achieve relatively good results.\n\nThe ability of adding an embedding layer along with tradition wide linear models allows for accurate predictions by reducing sparse dimensionality down to low dimensionality.\n\nThis part deviates from traditional Deep Learning to illustrate the many uses and applications of Tensorflow. This article is heavily based on the paper and code provided by Google for wide and deep learning. The research paper can be found here. Google uses this model as a product recommendation engine for the Google Play store and has helped them increase sales on app suggestions. Youtube has also released a paper about their recommendation system using hybrid learning as well available here. These models are starting to be more prevalent for recommendation by various companies and will likely continue to be for their embedding ability.", 
        "title": "TensorFlow in a Nutshell \u2014 Part Two: Hybrid Learning"
    }, 
    {
        "url": "https://medium.com/@michaeltefula/googles-wavenet-could-replace-your-voice-22c9afd20702?source=tag_archive---------1----------------", 
        "text": "Technology has a habit of eating jobs. We saw it in the 19th century (machines eliminated 98 percent of the labour required to weave cloth) and we are seeing it today (just this year robots replaced 60,000 factory workers in China.) These examples usually get the most attention since the number of employees involved is significant and visible. What we don\u2019t see as much, however, are the minority jobs that are also disrupted by technology.\n\nFor example, a small group of medieval monks who used to painstakingly copy books by hand were put out of work by the printing press; knocker uppers who used to manually wake up industrial workers in Britain lost their jobs to alarm clocks; and milkmen who used to deliver milk on a daily basis were put out of work by something rather plain: a fridge.\n\nSuch disruption will continue into the future and I can\u2019t help but wonder what other unsuspecting jobs will face competition from the machines. With that said, finding examples isn\u2019t too hard.\n\nLast week Google announced WaveNet, a new speech synthesis platform developed by the company\u2019s artificial intelligence division DeepMind. Instead of relying on pre-recorded audio samples that are stitched together or robotic-sounding vocoders, WaveNet uses deep learning concepts to model raw audio waveforms that sound very human.\n\nHow real do these artificial waveforms sound? In a blind test involving human subjects and over 500 ratings of 100 test sentences, WaveNet outperformed all the other major text-to-speech systems. Check out the samples below to hear the differences.\n\nWaveNet isn\u2019t quite there yet but it\u2019s significantly better than the alternatives. Furthermore, since the software models raw waveforms users will ultimately be able to fine tune accents, tone, as well as emotions. Just think what this could mean for Hollywood and the media industry in general.\n\nAt first, systems like WaveNet could be used to add minor bits of dialogue to a movie when an actor isn\u2019t available for additional takes. His or her voice would of course have to be fed to machine learning software. But once this process is complete, machines could step in for an actor when it\u2019s too expensive to get them in a recording studio again.\n\nFast-forward to a time when WaveNet is indistinguishable from human speech and it could very well replace the voices of humans in animated movies and other forms of media. The level of control engineers will be able to exercise with speech software (e.g. accent, tone, emotion) will surpass anything that can be achieved with a traditional voice actor in a recording studio. It will be faster, more accurate, and cheaper than directing a human being.\n\nBut speech software applications don\u2019t stop there. As a hobbyist musician, I look forward to a time when I can make beats and program a Beyonc\u00e9 or Michael Jackson voice simulation, all without having to pay millions for an exclusive or bringing a legend back from the dead!", 
        "title": "Google\u2019s WaveNet Could Replace Your Voice \u2013 Michael \u2013"
    }, 
    {
        "url": "https://medium.com/ai-business/prisma-8bc341a992a5?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u4f7f\u7528\u306e\u753b\u50cf\u52a0\u5de5\u30a2\u30d7\u30eaPrisma \u2013 Team AI Blog \u2013"
    }, 
    {
        "url": "https://medium.com/ai-business/ai-news-13th-sep-9166b081b22?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u4eba\u5de5\u77e5\u80fd\u30cb\u30e5\u30fc\u30b9\u30d8\u30c3\u30c9\u30e9\u30a4\u30f3 2016/9/13 \u2013 Team AI Blog \u2013"
    }, 
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-tesla-updates-autopilot-googles-talking-computers-how-neural-nets-d34f8ae59594?source=tag_archive---------4----------------", 
        "text": "You Might Have Heard: Tesla is updating Autopilot, the software that powers their self-driving car option, to use radar as the primary control sensor for navigation.\n\nCars will no longer need a camera to confirm visual image recognition during driving, and is geared toward preventing accidents, like the fatal Model S crash.\n\nThe new software will give the cars access to six times as many radar objects using the same hardware, which is available on all cars shipped after October 2014.\n\nTesla\u2019s taking advantage of the fleet of cars already on the road to dynamically learn about the positions and locations of road signs, bridges, and other stationary objects to better map roadways and hazards, and all but eliminate false-positives.\n\nThis real-time learning system is always running, whether or not Autopilot is on. Meaning, the more you drive, the more Tesla learns.\n\nBut Did You Know Tesla\u2019s Autopiliot has driven more than 47M miles in the past six-months? They\u2019re adding more than a million miles of driving data every day.\n\nConversely, Google\u2019s self-driving car, while taking a very different approach to autonomous vehicles, has travelled just 1.5M miles in SIX YEARS.\n\nThere\u2019s concern that Google\u2019s car project is losing out to rivals, like Tesla and Uber.\n\nMeanwhile, Apple has shuttered parts of its self-driving car project, laid off dozens of employees, and is rethinking its strategy.\n\nPLUS: Baidu and Nvidia to Build Artificial Intelligence Platform for Self-Driving Cars\n\nGoogle\u2019s DeepMind can now generate sound waves that mimic human voices\u200a\u2014\u200aa 50% improvement over the existing text-to-speech technology.\n\nThey\u2019re calling it WaveNet, which uses neural nets to generate convincing speech and music.\n\nResearchers fed DeepMind basic recorded speech, and used a convolutional neural network to create a complex set of rules about how certain tones follow other tones in the context of speech.\n\nDeepMind famously beat the world\u2019s best Go player in March, and is also being used to more efficiently manage power usage in Google\u2019s data centers. They\u2019ve cut their electricity bill by 15%.\n\nRead the full announcement, and listen to examples.\n\nResearchers from Harvard and MIT say they\u2019ve discovered the secret to neural networks buried in the laws of physics, where a small subset of mathematical functions describes the way the universe operates.\n\nThis is great news, because nobody quite understands why deep neural networks are so good at solving complex problems.\n\nWith physics, structures are formed through a sequence of simple steps: particles form atoms, which form molecules, cells, organisms, planets, solar systems, galaxies, etc.\n\nNeural nets are arranged in layers, where each layer deals with a higher and higher level of abstraction and complexity.\n\nRead the research to learn more about why deep learning works so well.", 
        "title": "Tesla Updates Autopilot, Google\u2019s Talking Computers, How Neural Nets Work"
    }
]