[
    {
        "url": "https://medium.com/intuitionmachine/the-only-way-to-make-deep-learning-interpretable-is-to-have-it-explain-itself-1e874a73108f?source=tag_archive---------0----------------", 
        "text": "One of the great biases that Machine Learning practitioners and Statisticians have is that our models and explanations of the world should be parsimonious. We\u2019ve all bought into Occam\u2019s Razor:\n\nHowever, does that mean that our machine learning model\u2019s need to be sparse? Does that mean that true understanding can only come from closed form analytic solutions? Do our theories have to be elegant and simple?\n\nYann LeCun in a recent FaceBook post commenting about a thesis on \u201cDeep Learning and Uncertainty\u201d points out to a 1987 paper by his colleagues at Bell Labs titled \u201cLarge Automatic Learning, Rule Extraction, and Generalization\u201d. This paper emphasizes the problem:\n\nOne of the probable reasons why Deep Learning requires an inordinate amount of iterations and training data is because we seek Occan\u2019s Razor, that sparse solution. What if however, the solution to unsupervised learning (aka Predictive Learning) is in embracing randomness?\n\nLet\u2019s table the proof of this for a later time, and assume its validity for argument\u2019s sake. That is, randomness is the natural equilibrium state (is it not obvious?). What this implies is that the model parameters will be completely random and interpretability will be completely hopeless. Unless of course, we can ask the machine to explain itself!\n\nI was about to end this post with the last paragraph, but I thought that some examples may help explore this idea much more thoroughly.\n\nStephen Merity (MetaMind) has a detailed examination of Google\u2019 Neural Machine Translator (GNMT) that is worth a read. The interesting thing about GNMT is that Google headlines this as \u201cZero-Shot Translation\u201d:\n\nThis zero-shot capability here refers to the capability of this machine to learn for example a Japanese to English translation even if it was never trained with this particular translation pair! To quote them:\n\nWill we perhaps be able to decipher this new \u201cinterlingua\u201d or \u201cesperanto\u201d that this machine created? Do we have a priori ideas as how this interligua is supposed to look like and perhaps performing a kind of regularization to make it more interpretable for humans? Will the act of insisting on interpretability lead to a less capable translator? Are Vulcan Mind-Melds necessary?\n\nIt just seems that we should leave the representation as it is and use the machine to perform the translation into English. In fact, that is already what it currently does. We don\u2019t need some new kind of method to interpret the representation. The capability is already baked in there.\n\nThis is in fact what the folks at MIT, who have researched about \u201cMaking computers explain themselves\u201d, have done:\n\nThey\u2019ve trained their network to learn how to explain itself.\n\nUpdate: Here are some slides from DARPA project XAI exploring explainability.\n\nIf you were able to grok this article, then feel free to join the conversation at this LinkedIn group: https://www.linkedin.com/groups/8584076", 
        "title": "The Only Way to make Deep Learning Interpretable is to Have it Explain Itself"
    }, 
    {
        "url": "https://gab41.lab41.org/nips-2016-review-day-2-daff1088135e?source=tag_archive---------1----------------", 
        "text": "Why good morning again, fellow machine learners. It\u2019s another day at NIPS, and what a marathon. The sessions ran from 9am to 9pm last night, and I was there for most of it! (Check out my NIPS 2016 Review, Day 1 for the low-down on yesterday\u2019s action.) Ok, let\u2019s get crackin\u2019.\n\nOverall, today\u2019s talks were better polished and more easy to follow, though that\u2019s partly because of jet-lag yesterday. You\u2019ll see a bit more coverage today over yesterday. The posters were still difficult to navigate, and so I just took note of which ones had the most people around them. (Generally, those were the ones with big company names behind the authors.) I\u2019m not going to go into detail with those here, mostly because I couldn\u2019t ever get close enough to snap a photo of the poster.\n\nAlso, on a more fun note, I talked with someone from Google\u2019s DeepMind. Apparently, they\u2019ve got some 150 people here. That\u2019s about half the group, with the other half being support staff!\n\nToday\u2019s first invited talk: machine learning in particle physics. The speaker is Kyle Cranmer, and he\u2019s from the Large Hadron Collider and did some work with the Higgs Boson. He now works on a project at CERN called Atlas. Their work has a lot of great innovations with PGM\u2019s, and shows what they\u2019ve been doing with likelihood free estimation.\n\nThe basic idea is that they\u2019ve got a lot of parameters in particle physics. That means your likelihood function is nuts. So, then you try to do inference on it. You then realize, that doing that is nuts. The example that he gave was in the following forward model (I guess, maybe looking for the Higgs Boson particle):\n\nThey\u2019ve had discussions with David Blei, and their machine learning contingent is quite strong; Cranmer\u2019s team has more machine learning staff than physicists. His modeling efforts have yielded a pretty generalizable chart shown below with CARL being his invention:\n\nOne of the more entertaining talks since there were so many videos. Here\u2019s a whole bunch of robots that they\u2019ve built.\n\nThey\u2019ve built a fairly robust humanoid robot (The Next Generation Atlas). From my coworker, Brad:\n\nAnother newer robot is called Spot. It was a crowd favorite: knocking it over, closing doors on it, hitting stuff it was trying to pick up, and throwing a banana on the ground for it to trip over. The engineers were the Lucy to it being Charlie Brown.\n\nIt\u2019s interesting to note that they are doing no learning whatsoever. At a conference where that play prominent, they were especially cognizant of that, and I think they\u2019ll be looking for AI experts in the near future.\n\nLearning by Poking\u200a\u2014\u200aThere was a similar talk in the afternoon session. This robot actually learns to manipulate objects by poking it. The experimental setup is this arm hanging over an object and you want it to move this object to another location.\n\nThe training setup: an initial image and a goal image. The robot then makes random pokes and will get rewarded if after it pokes the object, and the object gets closer to where it appears in the goal image. It then memorizes where it pokes the object and what happened to its orientation and position afterwards. They tried to make it harder by adding other random objects that are distractions to the the objective. In many cases, there are complex movements that are required to manipulate the object to get to the goal. Also, the objects that it pokes aren\u2019t your typical blocks; they\u2019re coffee cups (that roll) and other stuff that\u2019s randomly shaped.\n\nHere, we\u2019d like to point out that because it\u2019s second, I don\u2019t think it\u2019s less \u201cbest\u201d than the \u201cbest\u201d paper award. It\u2019s a very rigorous proof answering some questions on why non-convex optimization works well on training machine learning models, specifically for the matrix completion problem.\n\nI didn\u2019t know this, and it appears like it\u2019s recent work (2015), but there\u2019s been some stuff on building a conjectural unified theory, saying that all local minima are (approximately) global minima. This paper establishes this property for the matrix completion problem, implying that stochastic gradient descent (SGD) converges to a global minima. There\u2019s an initial sampling element, but with high probability under random initialization while using popular optimization techniques, the end minimum value stays consistent and solvable in polynomial time. The proof was a bit quickly glossed over, so it\u2019ll be worth looking at the paper.\n\nThere were two other talks relating to global and local minima. One was titled Without-Replacement Sampling for SGD, the other was Deep Learning Global Minima. Because the latter was in the same room, I opted to stay.\n\nDeep Learning without Poor Local Minima\u200a\u2014\u200aLooks like global minima is the rage now (as well it should be). There were some strong statements in this work, but the idea is that while random initialization gives you all sorts of rando-weights, if you train with similar optimization functions, you\u2019re going to get similar performance.\n\nThe proof begins with the fact that there are apparently seven strong assumptions to ensure convexity (that is, local minima = global minima) under linear neural networks. He essentially shows that under assumptions 1\u20134, there are no bad local minima, which I guess there was a 1989 paper on. He then moves onto nonlinear neural networks (with ReLU activation), and with even fewer assumptions, asserts the same. I would encourage you to read this stack overflow for some background. I originally saw this on arXiv, and there\u2019s a good thread on Hacker News.\n\nFollowing the trend of neural network image and video synthesis, there\u2019s been some interesting stuff at this conference. At ICML, we saw Scott Reed\u2019s image from text work. Today, just a half year later, more of Scott Reed. Some other works without the need for GANs (congrats, Katie!) show some impressive predictive video.\n\nModeling Future Frames from an Image\u200a\u2014\u200aExactly what the title says. The demo was of a lady doing exercise, and it trains using the motion information from similar video sequences. They delivered it very well, and you can catch a lot of this at their website and on the video:\n\nGenerative Adversarial What-Where Networks\u200a\u2014\u200aAnother GAN from Scott Reed. He\u2019s at all the big conferences. It\u2019s similar to his Generating Interpretable Images talk. But now you can put these birds and stuff anywhere you want to. You just tell it some keypoints, and then off it goes. You can take a look at his code at: https://github.com/reedscot/nips2016. Conclusions: location conditioning is useful for image synthesis. It adds an additional layer of control to get more interpretability. Works well for birds, not so well for humans and faces.\n\nWeight Normalization\u200a\u2014\u200aThis is yet another normalization come optimization time of deep learning neural networks, though the results that he showed at the conference were more on accuracy rather than computational performance. Their argument is that batch normalization adds noise to your gradient updates. While noise is probably good when you\u2019re training images with CNNs because it adds a bit of regularization (e.g., it can take care of invariances and stuff that doesn\u2019t matter), it\u2019s not so useful when you want to do reinforcement learning. Instead of normalizing batches, he does normalization directly onto the weights, the contributions being: weight normalization + data dependent initialization. He showed this on reinforcement learning with DQN. Looks like the scores got better, sometimes 30% more. Their code is at https://github.com/openai/weightnorm. For keras, it\u2019s two lines of code.\n\nSupervised Word Mover\u2019s Distance\u200a\u2014\u200aKillian Weinberger is co-author on this work that can do document comparisons. It\u2019s based on their ICML 2015 paper, titled From Word Embeddings To Document Distances, but this one is supervised. The idea is essentially to use Earth Mover\u2019s Distance on a Bag of Word Vectors. He calls it Word Mover\u2019s Distance, to which my colleagues and I laughed at the fact that Matt Kusner, the graduate student, had WMDs. It\u2019s a pretty interesting metric: you take the L2 distances of word vectors from one document and fill it into the second document. Their contribution is to learn a matrix that takes care of known similar words and apply it to this distance metric.\n\nThe Chinese Voting Process\u200a\u2014\u200aThough the title is a play on the Chinese Restaurant Process, being from Korea, I would\u2019ve expected the author to know full well that the Chinese don\u2019t vote. His talk deals with bias in up-voting and helpful reviews in product reviews, Amazon stuffs, and stack overflow. There were a few good examples: apparently presentation bias (the tendency for people to conform to other\u2019s opinions) plays strong on stack overflow. In another example, he searched on Amazon shopping with a bold query of \u201cnips\u201d. Luckily, he came back with some chocolate cookies, but there are some interesting implications on how voting can be biased and reinforced due to position. His method takes care of this through a generative process called the Chinese Voting Process. This process essentially models (temporally) how people would vote based on the reviews already existing in the corpus. It\u2019s similar to the restaurant process, but the idea is that certain reviews build momentum and trickle up to the top, not by any merit of the review itself, but because of the biases inherent in behavior.\n\nWe\u2019ve undoubtedly missed something, I\u2019m sure! Be sure to get our take on Day 1 too. Please let us know what other fun things were at NIPS if you\u2019ve attended via e-mail (kni@iqt.org). Tune in tomorrow for the final blog post on NIPS 2016. It\u2019s a blast out here in Barcelona, and we love sharing what we\u2019re seeing!", 
        "title": "NIPS 2016 Review, Day 2 \u2013"
    }, 
    {
        "url": "https://medium.com/@RiskSc/actor-observer-bias-psychology-term-review-1-7e4388ae91cd?source=tag_archive---------2----------------", 
        "text": "One of the most common everyday little psychological quirks that we commit and others do as well is Actor-Observer Bias. Simply put, when we are the \u201cobserver\u201d watching someone else, we attribute their short comings, behaviors, and overall actions to internal factors and overarching disposition. Now, when we are the \u201cjudge\u201d we tend to see our own actions and shortcomings as a result of simple situational and external factors as opposed to some sort of lack of internalization or character. Actor-Observer Bias falls under the category of cognitive bias.\n\nIt is important to note that a large amount of social psychology has a tendency for things to be more pronounced in situations where the outcomes are negative. Actor-Observer bias is no different. Interestingly enough, researchers have found that people tend to not fall prey to this bias nearly as frequently with people they know well such as family members and close friends. In social psychology, attribution is the process of inferring the causes of events or behaviors. Because we tend to have more information about the motivations, needs, and thoughts of individuals that are closer to us, we are more likely to take account for the external forces that impact behavior other than just simple observer assumptions.\n\nA very good example of actor-observer bias is a simple test. You walk into a room to take a test. Your friend Bob across the room is also taking the test. The room has too much light coming in from the window in the corner, the room is hot and stuffy, your pencil keeps breaking, and your buddy Joe all the way across the room is tapping his foot like a machine gun, and to top it all off, Jill is playing the piano with her fingers on the desk.\n\nWhen you get your results back and see that you did poorly, you are now in the judge position. In this situation, we immediately blame the external distractions for the poor test performance instead of acknowledging the lack of studying as well as inefficient practice leading up to the test. (We also don\u2019t want to acknowledge we stopped studying to browse Reddit for a little while, until two hours later we realize we need to be asleep.)\n\nNow on the flip side, when your friend Bob takes a tests and fails, you immediately attribute this to Bob having poor work ethic, laziness, never reading his textbook, and never taking notes. When you are in this observer situation, attribution has us focusing on the internal characteristics of Bob as opposed to the same situational variables that you feel contributed to your own substandard test score.\n\nActor-observer bias can clearly be problematic and is often a common cause of misunderstandings and arguments. Simply put, it is common for both sides to see the other side being at fault as a result of internal characteristics and attributes, whereas seeing themselves and their behavior as simply a situation issue and not an internal one.\n\nSo how can we fix this bias? Simply put, whenever you are in a situation where you start to begin to have attribution about someone, take a step back and potentially see the flaw in the angle you are coming in at. Along with this, if you ever make a mistake or have an argument, it is critical to take a step back and search ourselves to make sure that we aren\u2019t missing our own internal faults. Have the patience to give yourself a self check before you judge a result of your own behavior in a situation as well as have the patience to see the possibility of external factors effecting someone else\u2019s behavior.\n\nThanks for reading everyone! This was Psychology Term day #1\u00a0! Hope you enjoyed\u00a0:) You can find me on twitter here.", 
        "title": "Actor-Observer Bias Psychology Term Review #1 \u2013 The T\u00ed\u00f0r of The Karl D\u00e6ma \u2013"
    }, 
    {
        "url": "https://medium.com/@ramesh.panuganty/9-flaws-that-make-voice-assistants-fundamentally-wrong-e23f8245dc6?source=tag_archive---------3----------------", 
        "text": "There\u2019s a lot of buzz on the \u2018Virtual Assistants\u2019 from Google Home and Amazon Echo. I have been studying both the devices for a good amount of time now, since I wanted to see how they handle the user experience, and must say that I was pretty disappointed.\n\nBoth Google Home & Amazon Echo are built on some fundamentally flawed designs, detailed below. Note that I am not comparing the product specs, speaker technology or the aesthetics of these devices\u200a\u2014\u200abut purely talking about the underlying features of the virtual assistance, and whether the device is designed to truly understand and answer the user\u2019s question or not. My observations are common to both the devices, and I am using the term \u201cthe device\u201d to refer to either or both of the devices in this write-up.\n\nThe device is built on an assumption that was originally constructed by our search engines. It answers the same way a search engine answers a query; however, it returns just the first answer. It neither tries to validate the user\u2019s question for completeness, nor does any disambiguation on the question itself. For example, a question like \u201ctell me about George Bush\u201d gets responded with information on the Junior Bush because his search ranking is higher than his father\u2019s (probably no one did the SEO for the Senior Bush).\n\nHow can the device assume of a particular George Bush? Why doesn\u2019t it ask the users which George Bush they are referring to? This minimalist approach is just not how a teacher answers a student\u2019s question, or a parent would answer a child\u2019s question.\n\nThe device can\u2019t assume that the first answer from the search is the right answer, and this is totally wrong. There needs to be an interaction before answering the questions, or a communication on alternatives along with the current answer.\n\nHey Google & Amazon, please don\u2019t create a falsified world for the next generation who might pretty much assume that these devices are answering correctly to a question.\n\nWhile the device tries to get into answering mode very quickly, it doesn\u2019t tell what it is answering. This is a different problem compared to the previous one, because the context of the query object can vary.\n\nFor example, for the question \u201ctell about Gandhi\u201d, the device quickly responds with \u201cGandhi was released in India on 30 November 1982, and in the United States on 6 December. It was nominated for Academy Awards in eleven categories, winning eight.\u201d It is so unfortunate that \u2018Gandhi\u2019 is assumed as a movie title when he is more than a person, and is often treated as a common noun.\n\nIt would have been great to start the response with \u201cGandhi is a movie and was released in\u2026\u201d. Otherwise, it is a bad idea to assume that the question was above a movie and not even tell the user about it. It is again a fundamental problem as the underlying technology is just indexing all of Wikipedia, local businesses, restaurants and some top search results, and just throwing the top most (SEO\u2019ed) answer. The user just can\u2019t go by the answers and be fooled!\n\nAnother example for the same flaw\u2026 try asking \u201cwhen did Iron Man release\u201d, and the device starts responding when the first sequel of Iron Man movie was released. Hmm, I didn\u2019t say that I was referring to the movie, I could be thinking about the book \u201cIron Man\u201d, or the movie\u2019s sequel 1 or sequel 2. Even if it is a movie, why the sequel 1, and not sequel 2?\n\nI have seen many scenarios where there is no way to ask a question. For example, \u201cwhat are the nearby pizza joints\u201d returns \u201cPizza my heart, Big Apple Pizza, Amici\u2019s\u201d, and not \u201cPizza Hut, Round Table Pizza\u201d. I wasn\u2019t sure why did it not give details of Pizza Hut which is far closer to my home than the 3 restaurants that it responded with. I tried asking \u201cwhat are the nearby pizza fast food restaurants\u201d and it responds with \u201cKFC, Subway and McDonald\u201d. Really? What just went wrong?\n\nUsers ask give more details either to confirm what they are asking, or they don\u2019t know how to ask. Clarify with the user on how to ask. I could have simply got an answer to the above question by searching for \u201cpizza\u201d in Google Maps, but asking for \u201cpizza\u201d in Google Home takes it for a spin.\n\nIn this particular case, I couldn\u2019t figure out why McDonald came in the answers because it doesn\u2019t sell any pizza product at all, and the user doesn\u2019t have a way to respond for \u201csearch instead for\u201d questions.\n\nThe context of the question is never understood comprehensively. For example, I asked the question \u201chow many songs do I have in my music library\u201d. It gets responded with \u201cshuffling your music\u201d and then one of the songs starts playing.\n\nIt is very evident that there is a rush to do something for every question, and not really understand the question itself. What sort of assumptions do they make on the user while building these products? Is it an adult or a child; a tech savvy or a non-tech savvy; an active or a lazy person? I would like to understand the assumptions on the emotional part of the user that either Google or Amazon has built this for.\n\nEven in English, depending on the country\u200a\u2014\u200apeople call \u201cmovie\u201d as a \u201ccinema\u201d, or a \u201cfilm\u201d, or a \u201cpicture\u201d or even a \u201cshow\u201d. As of now, because there is no communication, this question doesn\u2019t arise, but if in future the device communicates back to the user\u200a\u2014\u200ait would be interesting to see how its product management has addressed this.\n\nThe device doesn\u2019t go beyond keywords and ignores the complete meaning of the question. For example, try \u201ctell the names of 5 American presidents\u201d, and all the device responds with one president\u2019s name (thankfully, as \u2018Barrack Obama\u2019). The device understands what an American president is, but not 5 of them. Similar example can be \u201ctell me 5 jokes\u201d which just returns only 1 joke.\n\nToo bad guys\u200a\u2014\u200athere is a lot more maturity that is expected from this device. Instead of looking my example in isolation, look at the big picture\u200a\u2014\u200athe device understands only the keywords and not beyond them. On a different note, the question \u201ctell the name of Russian president\u201d is not understood\u00a0:-\n\nI was first intrigued when I got different status messages for the not-understood questions. I am not blaming the device for not understanding the question itself, but disappointed about how it handled the responses when it did not understand.\n\nTry to ask the same question that is not expected to be understood at all multiple time, like \u201cwhat is John Martin\u2019s phone number\u201d and you can see that answers randomly vary from \u201cMy apologies, I don\u2019t understand\u201d, or \u201cSorry, I can\u2019t help with that yet\u201d, or \u201cHmm, something went wrong\u201d, to \u201cSorry, I don\u2019t know how to help with that yet\u201d, or \u201cSorry, I am not sure how to help with that yet. I am still learning\u201d, or \u201cHmm, I wasn\u2019t able to understand the question I heard\u201d.\n\nI would have loved if the device responded in this case as \u201cSorry, personal phone numbers are not supported. Only business listings are supported at this point.\u201d\n\nFor any enterprise product, it is almost mandatory to document how to handle an error or an exception message. In the consumer world, companies assume that not having a user guide is a fashion and they just take customers for a ride. These devices are not simple enough and need trouble-shooting for the user to know how to ask questions better.\n\nEvery question is assumed to be about an Internet data (public data) with no personalization whatsoever. The public indexed data such as Wikipedia, movies, local businesses, music albums, weather, or news, far dominate the personal data in these devices and they are just not designed to answer questions on your own personal data. For example, ask \u201ctell me about Jack Reacher\u201d and the device assumes it is a movie.\n\nIf all that the user is looking for a voice response to a google.com search box, this device is not worth of even 1 cent! It should read my personal data for my questions.\n\nAnytime I ask about a movie or a book in Alexa like \u201ctell about Harry Potter\u201d, it answers from wiki, and continues to either order the book/DVD for me (forcing me with a purchase), or says that it is not in my library.\n\nTreat the voice assistant as an independent product, that the customer has paid money for, and respect the customer. Don\u2019t try to bully or fool the customer.\n\nCan the device features be inquired? For example, a question like \u201cwhat\u2019s your current volume\u201d doesn\u2019t get understood at all, while \u201cincrease the volume\u201d or \u201cdecrease the volume\u201d gets understood. Similar failures with questions like \u201chow many alarms can you set?\u201d or \u201cwhat music sources do you support?\u201d\n\nWhen the skills or the vocabulary give only a couple of choices, the device makes it difficult to interact for the user.\n\nIn the end, I felt these voice assistants are more like remote controls with lots and lots buttons, which you can press by voice instead of using a tactile input. You need to remember all the buttons though\u00a0:-)\n\n[disclaimer: These observations are noted at the time of writing this article. I would be glad to be proven wrong in the future.]\n\nRamesh Panuganty is the Founder & CEO of Drastin, the world\u2019s first conversational analytics platform.", 
        "title": "9 flaws that make voice assistants fundamentally wrong"
    }, 
    {
        "url": "https://medium.com/@morillas/artificial-intelligence-vs-machine-learning-6f9dc51be25a?source=tag_archive---------4----------------", 
        "text": "I don\u2019t know if it\u2019s only me, but I\u2019ve seen lately growing a lot the number of articles talking about AI and how it\u2019s going to affect us in a near future. Robots dreaming or the future of jobs with robots make us think that we are now closer than farther to feel and see what we have already seen in movies like Eva, Her, HAL in 2001 space odyssey, in the so trendy nowadays tv series Westworld or read in some of the Isaac Asimov books.\n\nBut all this hype in regards of AI is happening with other technologies we have been reading about for long time like Machine Learning or Deep Learning. If you are one of those that, like me, are starting to get confused and wondering what is the difference between all these names, I hope this article can be as useful as it was to me to understand them.\n\nTL;DR If, on the other hand, you are not a long articles person and prefer a summary, I\u2019d say that Artificial Intelligence is a generic term used for all the technologies that \u201cmake machines think like us\u201d. Machine Learning is a type of AI which uses technologies that allows machines to take decisions from a bunch of data. Finally, Deep Learning, is a set of algorithms that allow to implement Machine Learning solutions.\n\nSo, in terms of from generic to more specific it could be said that: Artificial Intelligence > Machine Learning > Deep Learning.", 
        "title": "Artificial Intelligence vs Machine Learning \u2013 Dani Morillas \u2013"
    }, 
    {
        "url": "https://medium.com/@wschae/yet-another-reading-on-history-of-neural-nets-9b59e0463532?source=tag_archive---------5----------------", 
        "text": "Realizing my ignorance while reading Fortune article, I decided to further read about neural nets and found a gem: \u201cA \u2018Brief\u2019 History of Neural Nets and Deep Learning\u201d by Andrey Kurenkov.", 
        "title": "Yet another reading on \u201cHistory of Neural Nets\u201d \u2013 WONSEOK CHAE \u2013"
    }
]