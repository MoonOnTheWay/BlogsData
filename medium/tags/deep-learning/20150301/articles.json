[
    {
        "url": "https://medium.com/crossing-the-pond/into-the-age-of-context-f0aed15171d7?source=tag_archive---------0----------------", 
        "text": "I spent most of my early career proclaiming that \u201cThis!\u201d was the \u201cyear of mobile\u201d. The year of mobile was actually 2007 when the iPhone launched and accelerated a revolution around mobile computing. As The Economist recently put it \u201cJust eight years later Apple\u2019s iPhone exemplifies the early 21st century\u2019s defining technology.\u201d\n\nIt\u2019s not a question of whether Smartphones have become our primary computing interaction device, it\u2019s a question of by how much relative to other interaction mediums.\n\nSo let\u2019s agree that we are currently living in the Era of Mobile. Looking forward to the next 5 year though, I personally believe we will move from the Era of Mobile to the Age of Context. (credit to Robert Scoble and Shel Israel for their book with that same term).\n\nLet me first define what I mean by Age of Context. In the Age of Context personal data (ex: calendar and email, location and time) is integrated with publicly available data (ex: traffic data, pollution level) and app-level data (ex: Uber surge pricing, number of steps tracked by my FitBit) to intelligently drive me towards an action (ex: getting me to walk to my next meeting instead of ordering a car). It is an age in which we, and the devices and sensors around us, generate massive reams of data and in which self-teaching algorithms drill into that data to derive insight and recommend or auto-generate an action. It is an era in which our biological computational capacity and actions, are enhanced (and improved) by digital services.\n\nThe Age of Context is being brought about by a number of technology trends which have been accelerating in a parallel and are now coming together.\n\nThe first, and most obvious trend, is the proliferation of supercomputers in our pockets. Industry analysts forecast 1.87 billion phones will be shipped by 2018.\n\nThese devices carry not only a growing amount of processing power, but also the ecosystem of applications and services which integrate with sensors and functionality on the device to allow us to, literally, remote control our life. In the evolution from the current Era of Mobile to the future Age of Context, the supercomputers in our pocket evolve from information delivery and application interaction layers, to notification context-aware action drivers.\n\nSmartphones will soon be complemented by wearable computing devices (be that the Apple Watch or a future evolution of Google Glass). These new form factors are ideally suited for an Era in which data needs to be compiled into succinct notifications and action enablers.\n\nIn the last 10 years, the \u201cweb\u201d has evolved into a social web on top of which identities and deep insight into each of us powers services and experiences. It allows Goodreads to associate books with my identity, Vivino to determine that I like earthy red wines, Unilever to best target me for an ad on Facebook and Netflix to mine my data to then commission a show it knows I will like. This identity layer is now being overlayed with a financial layer in which, associated with my digital identity, I also have a secure digital payment mechanism. This transactional financial layer will begin to enable seamless transactions.\n\nIn the Age of Context, the Starbuck app will know that I usually emerge from the tube at 9:10am and walk to their local store to order a \u201ctall Americano, extra shot.\u201d At 9:11, as I reach street level my phone, or watch, or wearable computing device will know where I am (close to Starbucks and to the office), know my routine, have my payment information stored and simply generate an action-driver that says \u201cTall Americano, extra shot. Order?\u201d A few minutes later I can pick up my coffee, which has already been paid for. These services are already possible today.\n\nA parallel and accelerating trend which will power the Age of Context is the proliferation of intelligent and connected sensors around us. Call that Internet of Things or call it simply a democratization and consumerization of devices that capture data (for now) and act on data (eventually).\n\nWhile the end number varies, industry analysts all believe the number of connected devices starts to get very big very fast. Gartner predicts that by 2020 there will be 25 billion connected devices with the vast majority of those being consumer-centric. Today my Jawbone is a fairly basic data collection device. It knows that I walked 8,000 steps and slept too little, but it doesn\u2019t drive me to action other than providing me with a visualization of the data. In the Age of Context this will change, as larger and larger data sets of sensor data, combined with other data combined with intelligent analytics allows data to become actionable.\n\nIn the future my Jawbone won\u2019t simply count my steps, it will also be able to integrate with other data sets to generate personal health insights. It will have tracked over time that my blood pressure rises every morning at 9:20 after I have consumed the third coffee of the day. Comparing my blood rate to thousands of others of my age range and demographic background it will know that the levels are unhealthy and it will help me take a conscious decision not to consume that extra coffee through a notification. Data will derive insight and that insight will, hopefully, drive action.\n\nOne could argue that the parallel trends of mobile, sensors and the social web are already mainstream. What then is bringing them together to weave the Age of Context? The glue is data. The massive amounts of data the growing number of internet users and connected devices generate each day.\n\nMore critically, the cost of storing this data has dropped to nearly zero. Deloitte estimated that in 1992 the cost of storing a Gigabyte of data was $569 and that by 2012 the cost had dropped to $0.03.\n\nBut data by itself is just bits and bytes. The second key trend that is weaving the Age of Context is the breakthroughs in algorithms and models to analyze this data in close-to-real-time. For the Age of Context to come about, systems must know how to query and how to act on all the possible contextual data points to drive the simplified actions outlined in the examples above. The advances (and investment) into machine learning and AI are the final piece of the puzzle needed to turn data from information to action.\n\nThe most visible example of the Age of Context today is Google Now. Google has a lot of information about me: it knows what \u201cwork\u201d is as I spend most of the time there between 9am and 7pm, it knows what \u201chome\u201d is as I spend most of the evenings there. Since I use Google Apps it knows what my first meeting is. Since I search for Duke Basketball on a regular basis it knows I care about the scores. Since I usually take the tube and Google has access to the London TfL data, it knows that I will be late to my next meeting.\n\nBut even though Google Now recently opened up its API to third party developers, it is still fairly Google-biased and Google-optimized. For the Age of Context to thrive the platforms that power it must be interlinked across data and applications.\n\nWhether this age comes about through intelligent agents (like Siri or Viv or the character from Her) or a \u201cmeta-app\u201d layer sitting across vertical apps and services is still unclear. The missing piece for much of this to come about is a common meta-language for vertical and punctual apps to share data and actions. This common language will likely be an evolution of the various deep-linking standards being developed.\n\nFacebook has a flavour, Android has a flavour, and a myriad of startups have flavours. An emerging standard will not only enable the Age of Context but also probably crown the champion of this new era as the standard will also own the interactions, the interlinkages and the paths to monetization across devices and experiences.\n\nThe trends above are all happening around us, the standards and algorithms are all being built by brilliant minds across the world, the interface layers and devices are already with us. The Age of Context is being created at an accelerating pace and I can\u2019t wait to see what gets built and how our day to day lives are enhanced by this new era.\n\nThanks to John Henderson for his feedback and thoughts on this post.", 
        "title": "Into the Age of Context \u2013 Crossing the Pond \u2013"
    }, 
    {
        "url": "https://medium.com/@LamineSy/how-to-answer-the-open-challenge-our-big-bet-for-the-future-of-bill-melinda-gates-foundation-41d8f955cb86?source=tag_archive---------1----------------", 
        "text": "Accent to the future: Technology for people\n\nBreakthroughs in technology are disrupting the way we apprehend and comprehend our surroundings. They are stretching boundaries, giving rise to an era of smarter human enterprise and enabling incredible level of opportunities, across the world. Despite the situation of knowledge-access and their many hidden facets, advanced analytics and machine learnings are driving the disruption and reshaping of entire ecosystems. Emerging technology breakthroughs and innovations that help deliver those things to more people are unique opportunities to understand the world and leverage it\u2019s current beauty including to build an equitable home for people in a finite planet.\n\nBut how to humanize the technology to our needs instead of us running after that next technology to disrupt our lives? How to make them work for us, not us at their mercy? So who can change the narrative and build a circular advantage for humans? We say hourra to hackers, those tech disruptors like Apple, Amazon, Facebook, Google, IBM, Microsoft\u00a0\u2026\n\nThe circular advantage of big boys in technology lies in the circular economy\u200a\u2014\u200awhere growth is decoupled from the use of scarce resources through a people first approach to disruptive technology and business models based on longevity, renewability, reuse, repair, upgrade, refurbishment, capacity sharing and dematerialization. Leaders of the era of the intelligent human enterprise can play an important role and help tackle the challenges of shaping the Cognitive Era.", 
        "title": "How to answer the challenge \u2014 \u201cOur Big Bet for the Future\u201d \u2014 of Bill & Melinda Gates Foundation"
    }, 
    {
        "url": "https://medium.com/@ktinboulder/a-deep-learning-primer-for-product-managers-db1b7f12a380?source=tag_archive---------2----------------", 
        "text": "Most of my discussions lately with CTOs and Product Managers have ended with me emailing over some links and info about Deep Learning. I\u2019m a relative newbie to the space myself so hopefully this is a good primer.\n\nFor Product Managers and CTOs, having an understanding of what\u2019s possible with Deep Learning and how the technology could impact your features and stack is quickly becoming very important.\n\nA great summary from Lee Gomes in this article:\n\nHere are few more article I like:\n\nAlbert Wenger from USV talks about Machine Intelligence\n\nShivon Zilis from Bloomberg Beta created a Machine Intelligence Landscape\n\nAnd, here\u2019s a list of terms to explore:\n\nWord Embeddings\n\nNodes and edges\n\nPrecision and recall\n\nDeterministic\n\nFrame semantics\n\nKnowledge graph\n\nHypernym and Hyponym\n\nWord sense disambiguation\n\nHearst Patterns\n\nGPU computing\n\nDropout\n\nStochastic Pooling\n\nCognitive Computing\n\nAnd, some good people to follow in the Deep Learning space:\n\nYann LeCun\u200a\u2014\u200aFacebook\u2019s Director of AI Research\n\nGeoffrey Hinton\u200a\u2014\u200aDistinguished Researcher at Google and Distinguished Professor at University of Toronto\n\nYoshua Bengio\u200a\u2014\u200aFull Professor Department of Computer Science and Operations Research and Canada Research Chair in Statistical Learning Algorithms\n\nElliot Turner\u200a\u2014\u200aFounder AlchemyAPI\n\nDerrick Harris\u200a\u2014\u200aSenior Writer at GigaOm\n\nSeth Grimes\u200a\u2014\u200aIndustry Analyst\n\nAnd, some websites to explore:", 
        "title": "A Deep Learning Primer for Product Managers \u2013 Kelly Taylor \u2013"
    }, 
    {
        "url": "https://labs.beeva.com/deep-shallow-and-stupid-learning-2ad28267f96e?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep, shallow and stupid learning \u2013"
    }, 
    {
        "url": "https://medium.com/@krmarko/intel-s-deep-learning-play-next-gen-xeon-phi-could-challenge-nvidia-gpus-48a0b97ca879?source=tag_archive---------4----------------", 
        "text": "Intel Intel disclosed new technical details about its next next-generation Xeon Phi processor to a small group of technology analysts-journalists at one of its Hillsboro sites this week. First revealed last year, the \u201cKnights Landing\u201d chip is expected to ship later this year, however Intel showed pre-production parts parts running in a Hillsboro test lab. The briefing included a whiteboard design walkthrough by Avinash Sodani, Knights Landing Chief Architect at Intel, a product update by Hugo Saleh, Marketing Director of Intel\u2019s Technical Computing Group, an interactive technical Q&A and a lab demo of a Knights Landing system running on an Intel reference-design system. The architectural differences between Knights Landing and prior Phi products is substantial and should translate to much easier adoption and a broader set of commercial applications, including areas like deep learning, genomics, databases and general purpose containerized (Docker) workloads. Unlike more specialized processors, Intel describes Knights Landing as taking a \u201cholistic approach\u201d to new breakthrough applications.\n\nUnlike the current generation Phi design, which operates as a coprocessor, Knights Landing incorporates x86 cores and can directly boot and run standard operating systems and application code without recompilation. The test system, which resembled a standard server board, with socketed CPU and memory modules was running a stock Linux distribution. This is made possible by the inclusion of a modified version of the Atom Silvermont x86 cores as part of each Knights Landing \u2019tile\u2019: the chip\u2019s basic design unit consisting of dual x86 and vector execution units alongside cache memory and intra-tile mesh communication circuitry. Each multi-chip package includes the a processor with 30 or more (rumored up to 36) tiles and eight high-speed memory chips. Although it wouldn\u2019t offer specifics, Intel said the on-package memory, totalling 16GB, is made by Micron with custom I/O circuitry and might be a variant of Micron\u2019s announced, but not yet shipping Hybrid Memory Cube, but this wasn\u2019t confirmed. The high-speed memory is conceptually similar to the DDR5 devices used on GPUs like NVIDIA\u2019s NVIDIA\u2019s Tesla.\n\nIntel is long on device-level technology details, but short on specifics regarding the types of business problems, applications and users Phi targets. So far, Phi has been used by HPC applications like scientific simulations, modeling and design, with product announcements often happening during supercomputing conferences. Yet as NVIDIA demonstrated at GTC,, the opportunities for compute hardware supporting wide, vector operations built for highly parallelized algorithms, whether general purpose GPUs or now Xeon Phi, extends far beyond the supercomputer niche. As I previously discussed, NVIDIA has morphed its GPU architecture into a powerful engine for calculating deep learning neural network algorithms that is now being applied to accelerate SQL database operations and analytics.\n\nThe internals of a GPU and Xeon Phi are much different, however both share many common traits: dozens (Phi) to thousands (GPU) of lower performance (Phi) to relatively simple cores (GPU), vector processing units and very high-speed local memory and buses. The creative exploitation of this capability isn\u2019t limited to scientific or graphics calculations and seems limited only by the imagination of developers. But on this front, Intel didn\u2019t have encouraging news. Although Saleh mentioned deep learning as a potential application, he couldn\u2019t point to specific customers or researchers using Phi for anything but traditional scientific HPC problems.\n\nUnlike NVIDIA, which announced the price and availability of a Titan X development box designed for researchers exploring GPU applications to deep learning, Intel wouldn\u2019t share details about when OEM partners will have Knights Landing systems, whether any ould be sized and priced for individual developers, nor a timeline for what Saleh characterized as a Phi developer program, which will include access to hardware, updated software tools and training materials. But developer resources, outreach and evangelism are critical to any success Phi is to have, particularly since the hardware itself represents a substantial departure from the standard x86 microarchitecture, and here NVIDIA has a big head start.\n\nYet thinking of Phi as an alternative GPU or vector processor is wrong since it\u2019s actually a hybrid that includes dozens of full-fledged 64-bit x86 cores. This unique design, if used right, could significantly accelerate certain parallelizable application categories besides HPC simulation like deep learning and data analytics that use vector calculations, while offering drop-in computational offload for standard x86 code.\n\nOne intriguing possibility is using Phi as an app container processor for Docker or Rocket microservices, where each core has an Atom processor, 512-bit vector unit, a chunk (100MB or more) of on-package high-speed memory and access to 4\u20138GB of system RAM. Using the test hardware Intel demonstrated, that\u2019s 240 cores, or 960 application threads per 2U server. For the right workloads, Phi even makes sense as a virtualization platform hosting dozens, if not hundreds, of guest operating systems.\n\nTechnically, Intel has achieved a tour de force with its Xeon Phi overhaul, however like any technology that breaks existing paradigms, it will take time for both Intel and developers to fully understand its optimal use and opportune applications. Hopefully the chip and package aren\u2019t so difficult to produce that it\u2019s prohibitively expensive for anyone but government-funded research labs because the sooner Intel can get Knights Landing into the hands of ordinary developers with a clever idea, the sooner we\u2019ll all better understand its potential and witness a new crop of groundbreaking applications.", 
        "title": "Intel\u2019s Deep Learning Play: Next-Gen Xeon Phi Could Challenge NVIDIA GPUs"
    }, 
    {
        "url": "https://medium.com/@krmarko/no-longer-just-about-graphics-nvidia-highlights-gpu-ai-business-applications-bf786ee6e02c?source=tag_archive---------5----------------", 
        "text": "Unless you are a gamer, graphics developer or research scientist NVIDIA is probably isn\u2019t on your radar. Even then, the most likely association is with big, expensive graphics cards for movie animator workstations or the types of gaming PCs all but the hardcore system builder have ditched for a PS4 or Xbox One. Graphics processing chips (GPUs) powered NVIDIA to nearly $5 billion in sales and a $12 billion market cap, but the company sees new applications like image and speech recognition and database acceleration powering the next phase of its growth. At NVIDIA\u2019s premier developer event, the GPU Technology Conference (GTC), CEO Jen-Hsun Huang and his team decidedly emphasized broader and more innovative applications for the company\u2019s parallel processing technology, notably as the brains processing sensor information to instruct autonomous cars. Indeed, Elon Musk\u2019s appearance on the keynote stage served to underscore both the inevitability of self-driving vehicles (Musk: \u201cIn the future, we may outlaw driving cars. Can\u2019t have a person driving a two-ton death machine.\u201d) and NVIDIA\u2019s positioning as a key automotive technology provider.\n\nHuang\u2019s keynote had plenty of details about new hardware and hints of NVIDIA\u2019s technology roadmap, but the broad theme is the application of deep learning algorithms to a host of real world problems and how GPU technology can dramatically accelerate performance and bootstrap product development and deployment. Deep learning is the current descriptor for a class of brain-inspired neural network algorithms that recursively process raw data, whether image pixels, audio samples, or unstructured text, to ascertain patterns and meaning. Deep learning is what enables Google to automatically categorize photos or Facebook to tag faces in snapshots, but broader business applications are only beginning to be probed.\n\nThe \u2018G\u2019 (graphics) label for NVIDIA\u2019s main product is becoming an anachronism. Instead NVIDIA\u2019s hardware, software and engineering output are manifested in algorithms and APIs, not circuits and interconnects. Its product, like the Tesla server GPU cards, which don\u2019t even have a video output, are increasingly used to compute neural networks and process databases, not render virtual images. Indeed, GPUs are a disruptive technology for databases, business analytics and robotics that will allow unknown startups like those in the GTC Emerging Companies Summit and giant corporations like IBM and Baidu to reshape markets.\n\nAlthough GTC is designed for engineers and developers, with content that\u2019s often inscrutable to those not immersed in the particular specialties, other interesting themes emerged that have implications far beyond the R&D lab. Paramount and spotlighted by Huang is deep learning, a label for the application of increasingly detailed and complex neural network algorithms that adapt and automatically improve based on new, added information and that are particularly good at performing image and speech recognition. The link between deep learning research and NVIDIA is simple: its parallel processing technology, developed for graphics processors but now generalized via the CUDA platform, is tuned for algorithms that can be highly partitioned and parallelized and that benefit from a GPU\u2019s very fast memory subsystem. Problems like image and speech recognition can be broken apart into easily computable chunks and reassembled into an answer. In essence, the algorithms for rendering graphical features and detecting them share common traits that GPUs are designed for.\n\nNVIDIA pioneered the GPU, but now that the processor in every smartphone has more than enough horsepower to render Angry Birds or stream a movie, the company needs an outlet for its higher-end technology. Although there remains a niche market for ever more sophisticated graphics rendering, witness the special effects forthcoming in the next Star Wars movie, it\u2019s not enough to fuel a growing company\u2019s ambitions. That\u2019s where deep learning and its applications, particularly to autonomous vehicles comes in.\n\nThat explains the prominence of several car companies and Musk\u2019s keynote appearance. The man behind the world\u2019s best electric vehicle technology was almost nonchalant when talking about the future of autonomous vehicles, stating that self-driving cars will soon become the norm. Indeed, Musk said society may one day outlaw driving one\u2019s own car, quipping \u201cWe can\u2019t have a person driving a two-ton death machine.\u201d NVIDIA intends to be a key arms merchant for future car technology.\n\nYet real time vehicular image processing is merely one application for GPUs. NVIDIA also announced hardware and software systems designed to accelerate development for deep learning researchers and big data analysts with an eye to bootstrapping a new generation of applications based on its CUDA parallel computing platform and programming model.\n\nAside from car automation, one of the most interesting applications of GPU technology is to speech recognition and voice-to-text transcription. It\u2019s appeal was palpable in the packed session, where the line started forming almost an hour ahead of time, by a researcher from Baidu on the use of deep learning neural networks to dramatically improve the speed and accuracy of speech recognition.\n\nOne application with tangible business benefits is the acceleration of database queries and analysis. A prime example is MapD, a four-person startup based on the grad school research of founder Todd Mostak that won last year\u2019s GTC emerging startups award for its GPU-powered database cum data visualization platform. After a briefing and demo with Mostak and attending his GTC talk, it\u2019s easy to see why. MapD\u2019s technology has tremendous advantages for a wide range of mid-sized data problems that don\u2019t require Hadoop-scale infrastructure. Mostak\u2019s demo included slicing and dicing a 60 million record data set of airline flight information turned the task of data analysis into a real time, interactive experience.\n\nOther examples at GTC included an IBM talk and demonstration on a database system using NVIDIA Tesla GPUs paired with IBM\u2019s POWER8 CPU that doubled query performance across a range of benchmarks and stealthy startup Graphistry with a presentation on using GPUs to power its data visualization engine.\n\nGTC demonstrated that the use of GPUs to tackle a diverse set of computational problems is expanding to the point that the acronym itself is an anachronism: parallel processing unit is far more appropriate. With performance improvements accelerating as laid out in Huang\u2019s keynote, expect to see a GPU powering more and more business applications.", 
        "title": "No Longer Just About Graphics, NVIDIA Highlights GPU AI, Business Applications"
    }, 
    {
        "url": "https://medium.com/@haras.adam/ubuntu-14-04-install-opencv-with-cuda-aae90500f928?source=tag_archive---------7----------------", 
        "text": "Today I\u2019ll show you how to compile and install OpenCV with support for Nvidia CUDA technology which will allow you to use GPU to speed up image processing.\n\n\u00a0I assume that you already have CUDA toolkit installed. If not there is a very good tutorial prepared by Facebook AI Research (FAIR). Just look at the Install CUDA section in FAIR\u2019s instruction.\n\nCheck cmake\u2019s output and make sure that CUDA and CUBLAS are enabled:\n\nIf everything is correct you can install OpenCV:", 
        "title": "Ubuntu 14.04 \u2014 install OpenCV with CUDA \u2013 Adam Harasimowicz \u2013"
    }
]