[
    {
        "url": "https://medium.com/planet-stories/from-the-firehose-the-explosive-growth-of-solar-power-in-china-c845367fa66a?source=tag_archive---------0----------------", 
        "text": "China is adopting solar power faster than any other country. According to Reuters, China added 15 gigawatts of solar power in 2015. For comparison, the total installed solar capacity in the United States is 24.1 gigawatts. China\u2019s total capacity was 43 gigawatts by the end of 2015.\n\nThe epicenter of the Chinese solar industry is in Qinghai Province near the city of Golmud, under the clear, dry skies of the Tibetan Plateau. Construction of solar parks in the region began in 2009, and the first facilities began operating in 2011. By 2013, solar panels had spread over a wide expanse of the high desert to the east of the city.\n\nTake a look at this Landsat 8 image of the area from 2013:\n\nNow let\u2019s zoom in, and look at a detail of this scene, captured by Planet Labs three years later, in 2016:\n\nNASA\u2019s Landsat 8 and the ESA\u2019s Sentinel-2a satellites provide a wide-angle view of infrastructure projects like China\u2019s rapidly expanding solar farms. Their well-calibrated, multispectral sensors enable precision measurements every few weeks.\n\nPlanet\u2019s constellation of high-resolution satellites collects a complementary data set. Frequent, high-resolution data from our constellation gives you a detailed look at these projects more often. Individual rows of panels are visible in the imagery, as well as the long shadows cast by the towers holding up transmission lines.\n\nIn recent months, construction hasn\u2019t slowed. Take a look at how this solar farm changes over a four month period:\n\nAnd more construction is just around the corner. China expects to add an additional 15 gigawatts of solar power this year. As our fleet of on-orbit satellites grows in 2016, we\u2019ll be able to monitor these solar farms even more acutely as they expand into the desert.\n\nIn the meantime, we\u2019ll keep our eyes on our imagery firehose for more interesting stories.\n\nSee more of our imagery and stories at Planet.com. If you\u2019re a developer, access our platform for free here: https://www.planet.com/open-california/.", 
        "title": "From the Firehose: The Explosive Growth of Solar Power in China"
    }, 
    {
        "url": "https://gab41.lab41.org/can-word-vectors-help-predict-whether-your-chinese-tweet-gets-censored-711e7682d12f?source=tag_archive---------1----------------", 
        "text": "Can Word Vectors Help Predict Whether Your Chinese Tweet Gets Censored?\n\nIn our project Sunny-Side-Up, we explored neural network approaches for binary classification of sentiment in several datasets. We implemented several deep learning architectures, including ConvNets and LSTMs, also using word vectors as feature learners for several machine learning classifiers. Much of that work focused on canonical English research datasets, but our ultimate goal was to analyze previously-unseen foreign texts. \u201cWouldn\u2019t it be great,\u201d we thought, \u201cif these techniques provided a more reliable way to gauge sentiment when language experts weren\u2019t readily available?\u201d\n\nWorking towards that \u201cless-language-dependent\u201d goal, we started looking at techniques that would work on Chinese messages since it is the second most frequently-used language on the Internet. We also had the idea that instead of looking at sentiment, we might be able to identify messages that could end up in the crosshairs of Chinese censorship. As readers of our blog are probably aware, Chinese censorship is a well-researched field that depends on several factors, including the particular site (or social network), usage trends, and specific events in and about China. Without trying to generalize too much, we wanted to examine how well the content of the messages themselves could predict censorship for one social network over a specific period of time. In this post, we\u2019ll walk through our approach, assumptions, and results involved in that experiment.\n\nBefore we dive into algorithms and experimental design, it\u2019s worth highlighting a few specifics of the dataset we chose to use from the Open Weiboscope project. First, it is free to download and includes 227M messages sent in 2012 on the Chinese microblogging site Sina Weibo. In addition to being large and freely available, we liked that the dataset is a real-world representation of actual Chinese social media posts. Additionally, it contains two inherent \u201clabels\u201d within the metadata fields\u200a\u2014\u200anamely, whether the post was deleted by the user or rendered inaccessible by other users. Since supervised classification techniques learn features towards specific labels, we tested both metadata fields with an eye towards the latter as a potential stand-in for censorship.\n\nIf you\u2019re interested in reading more about the dataset and potential uses, we suggest, \u201cAssessing Censorship on Microblogs in China: Discriminatory Keyword Analysis and Impact Evaluation of the \u201cReal Name Registration\u201d Policy\u201d from which the data came. Otherwise, read on for details on the algorithms and data experiments we conducted.\n\nAs our Anything2Vec post explained, the Word2Vec algorithm uses a two-layer neural network to build N-dimensional vectors of input text. Those vectors can be used for a variety of machine learning tasks, including sentiment classification. One of the great things about that algorithm is the ready availability of several pre-trained vectors anyone can download and use. That means that instead of going through the pain of acquiring a huge amount of training data AND training the algorithm, organizations like Google have been kind enough to do the heavy lifting and make the results available for widespread use. Similarly, developers of the GloVe algorithm, which also builds N-dimensional feature vectors using a different approach, also provide freely available pre-trained sets to use. Furthermore, it is possible to train each algorithm to generate new vectors from your own dataset, which we did later with the Word2Vec algorithm to compare against the pre-trained vectors.\n\nFor our Twitter sentiment analysis purposes, we wanted to use a relevant set of pre-trained vectors from each algorithm. For Word2Vec, we chose the set trained on over 100 Billion words from Google News articles. We thought the vocabulary size of 3M would provide good coverage of many common words. For GloVe, we chose the set trained on 2 Billion Tweets since it came from the same source (Twitter) as our target dataset.\n\nEven though those pre-trained vectors will prove useful for many (most?) of our use cases, we wanted to know how they held up to unforeseen foreign text. Working towards that goal, we decided to first test the \u201cunforeseen\u201d aspect before tackling a foreign text.\n\nFor datasets where no pre-trained set of feature vectors is appropriate, can you get good results from training your own? We expected the answer was \u201cyes\u201d\u200a\u2014\u200aotherwise, Word2Vec probably wouldn\u2019t have caught on\u200a\u2014\u200abut we weren\u2019t sure how much data we needed or what kind of performance we could expect. As an experiment, we decided to compare the classification performance of pre-trained vectors with a set we trained from a target dataset. For that dataset, we chose the Sentiment140 research set, which contains 1.6M Tweets (800K of each of positive and negative labels). Overall, we liked that the size and format of the messages matches our end goal for Twitter sentiment analysis.\n\nTo recap, here\u2019s what we tested:\n\nNote that for the dataset-generated vectors, we wanted to mirror the real world scenario where we wouldn\u2019t have perfect coverage of the vocabulary in our dataset. That means we needed to simulate dictionary misses in the test set. We did that by training the vectors on the same 80% subset we used for training the classifiers. For the classifiers, we chose three that represented commonly-used types of probabilistic, linear, and ensemble classifiers. Here\u2019s how the various word vectors fared against each classifier according to F1 score:\n\nAs you can see above, generating vectors from (a subset of) a relatively small dataset delivered pretty good performance. More importantly, they went toe-to-toe with using pre-trained vectors from huge datasets. We thought this was great news for our efforts and left the question of, \u201cHow small of a subset can you use?\u201d to other curious readers.\n\nWith sufficient confidence in generating vectors from a non-big-data corpus, we next wanted to test a foreign text. As mentioned above, we decided to use the Open Weiboscope dataset as the foreign text because it mirrored the structure of Twitter messages and contained two metadata \u201clabels\u201d we found interesting.\n\nSimilar to our Sentiment140 experiment, we wanted to compare the classification performance of pre-trained vectors with a set we trained from that target dataset. To give you an idea of what a word vector looks like, here is the 200-dimensional Word2Vec representation of \u6781\u7aef\u611a\u8822 (unicode: u\u2019\\u6781\\u7aef\\u611a\\u8822'):\n\nUpon seeing these results, we became very excited by the possibilities. After all, the Sina Weibo-generated vectors not only delivered usable results, but also blew the vectors from other datasets out of the water! We\u2019ll gladly take our F1 score of 0.934, thank you very much. But wait. As we mentioned before, the set of 56,000 messages with that censor label is significantly smaller than Sentiment140's 1.6M messages. We didn\u2019t think it prudent to rest on such a small sample size and wanted a bit more perspective before jumping to any conclusions.\n\nAs a final test, we re-ran the above experiment using the metadata field that indicates whether or not the user deleted the message. That label provided the benefit of yielding a much larger population size, although we wondered how noisy it is since users probably delete messages for various and possibly unrelated reasons. Basically, we weren\u2019t sure if the messages would share enough features in common for the machine learning classifiers to learn.\n\nNotice that while none of the word vectors produced stellar results, the Chinese-trained vectors still edged out a victory in each case:\n\nJust in case you wanted a side-by-side comparison, here are the differences for how the classifiers learned the two different metadata fields. It\u2019s pretty clear the labeling strategy played a significant role, probably impacting performance more than choice of classifier or source of word vectors:\n\nWe\u2019ve walked through a lot of results, so we wanted to highlight our key takeaways here:\n\nThat\u2019s all for now. Until next time, \u7334\u5e74\u5feb\u4e50\u00a0!", 
        "title": "Can Word Vectors Help Predict Whether Your Chinese Tweet Gets Censored?"
    }
]