[
    {
        "url": "https://medium.com/@zan2434/building-a-deep-learning-powered-gif-search-engine-a3eb309d7525?source=tag_archive---------0----------------", 
        "text": "Don\u2019t worry if the above doesn\u2019t make sense\u200a\u2014\u200aif you\u2019d like to know more read on and I\u2019ll explain how the individual pieces work below.\n\nSpecifically, these models are the VGG16 16-layer CNN pre-trained on ImageNet, the Skip Thoughts GRU RNN pre-trained on the BooksCorpus, and a set of 2 linear embedding matrices trained jointly with the others on the videos and sentence descriptions from the Microsoft Video Description Corpus. This framing does constrain the resulting model to only working well with live-action videos that are similar to the videos in the MVDC, but the pre-trained image and sentence models help it generalize to pairings in that domain it has never before seen.\n\nBy formulating the problem generally we can get away with only having to learn a shallow neural network model that embeds hidden layer representations from two pre-trained models\u200a\u2014\u200aa convolutional neural network pre-trained to classify objects in images, and a recurrent neural network pre-trained to predict surrounding context in text. We train a shallow neural network to embed the representations from these models into a joint space together based on associations from a corpus of short videos and their sentence descriptions.\n\nThis seems quite magical\u200a\u2014\u200ayou type in a phrase and get exactly the GIF you were thinking of\u200a\u2014\u200abut behind the scenes it\u2019s a matter of glueing two machine learning models pre-trained on massive datasets together by training a third, smaller model on a dataset.\n\nThey say a picture\u2019s worth a thousand words, so GIFs are worth at least an order of magnitude more. But what are we to do when the experience of finding the right GIF is like searching for the right ten thousand words in a library full of books, and your only aid is the Dewey Decimal System ?\n\nFirst, we have our CNN or convolutional neural network, pre-trained to classify the objects found in images. At a high level, a convolutional neural network is a deep neural network with a specific pattern of parameter reuse that enables it to scale to large inputs (read: images). Researchers have trained convolutional neural networks to exhibit near-human performance in classifying objects in images, a landmark achievement in computer vision and artificial intelligence in general.\n\nNow what does this have to do with GIFs? Well, as one may expect, the \u201cskills\u201d learned by the neural network in order to classify objects in an image should generalize to other tasks requiring understanding images. If you taught a robot to tell you what\u2019s in an image for example, and then started asking it to draw the boundaries of such objects (a vision task that is harder, but requires much of the same knowledge), you\u2019d hope it would pick up this task more quickly than if it had started on this new task from scratch.\n\nWe can use an understanding of how neural networks function to figure out exactly how to achieve such an effect. Deep neural networks are so called because they contain layers of composed pieces\u200a\u2014\u200aeach layer is simply a matrix multiplication followed by an activation function.\n\nThis means, for a given input, we multiply it by a matrix, then pass it through one of those functions, then multiply it by another matrix, then pass it through one of those functions again, until we have the numbers we want. In classification, the numbers we want are a probability distribution over classes/categories, and this is necessarily far fewer numbers than in our original input.\n\nIt is well understood that matrix multiplications simply parametrize transformations of a space of information\u200a\u2014\u200ae.g. for images you can imagine that each matrix multiplication warps the image a bit so that it is easier to understand for subsequent layers, amplifying certain features to cover a wider domain, and shrinking others that are less important. You can also imagine that, based on the shape of the common activation functions (they \u201csaturate\u201d at the limits of their domain from -\u221e to \u221e, and only have a narrow range around their center when they aren\u2019t strictly one number or another), they are utilized to \u201cdestroy\u201d irrelevant information by shifting and stretching their narrow range of effectiveness to the region of interest in the data. Further, by doing this many times rather than only once, the network can combine features from disparate parts of the image that are relevant to one another.\n\nWhen the task at hand is classification, then it transforms the image information until only the information critical to making a class decision is available. We can leverage this understanding of the neural network to realize that just prior to the layer that outputs class probabilities we have a layer that does most of the dirty work in understanding the image except reducing it to class labels.\n\nNow we can reuse learned abilities from our previous task, and generalize far beyond our limited training data for this new task. More concretely, for a given image, we recognize that this penultimate layer\u2019s output may be a more useful representation than the original (the image itself) for a new task if it requires similar skills. For our GIF search engine this means that we\u2019ll be using the output of the VGG-16 CNN from Oxford trained on the task of classifying images in the ImageNet dataset as our representation for GIFs (and thereby the input to the machine learning model we\u2019d like to learn).\n\nWhat is the equivalent representation for sentences? That brings us to the second piece of our puzzle, the SkipThoughts GRU (gated-recurrent-unit) RNN (recurrent neural network) trained on the Books Corpus. Like convolutional networks share their parameters across the width and height of an image, recurrent ones share their parameters across the length of a sequence. Convolutional networks\u2019 parameter sharing relies on an assumption that only local features are relevant at each layer of the hierarchy, and these features are then integrated by moving up the hierarchy, incrementally summarizing and distilling the data below at each step. Recurrent networks however accumulate data over time, adding the input they are currently looking at to a history. In this manner, they effectively have \u201cmemory\u201d, and can operate on arbitrary sequences of data\u200a\u2014\u200apen strokes, text, music, speech, etc. Like convolutional neural networks, they represent the state of the art in many sequence learning tasks like speech recognition, sentiment analysis from text, and even handwriting recognition.\n\nWhile characterizing an image as a 2D array of numbers may be somewhat intuitive, transforming sentences into the same general space won\u2019t be. This is because while you can easily treat the brightness/color of a pixel in an image as a number on some range, the same doesn\u2019t seem as intuitive for words. Words are discrete while the colors of pixels are continuous.\n\nImportantly (and necessarily for our application), these definitions aren\u2019t as incontrovertible as they may seem. While words themselves are certainly distinct, they represent ideas that aren\u2019t necessarily so black and white. There may not be any words between cat and dog, but we can certainly think of concepts between them. And for some pairs of words, there actually are plenty of words in between (i.e. between hot and cold).\n\nFor example, the space of colors is certainly continuous, but the space of named colors is not. There are infinitely many colors between black and white, but we really only have a few words for them (grey, steel, charcoal, etc.).\n\nWhat if we could find that space of colors from the words for them, and use that space directly?\n\nWe would only require that the words for colors that are similar also be close to each other in the color space. Then, despite the color names being a discrete space, operations we want to do on or between the colors (like mixing colors or finding similar ones) become simple once we first convert them to the continuous space.\n\nOur method for bringing the discrete world of language into a continuous space like images involves a step like that of the colors. We will find the space of meaning behind the words, by finding embeddings for every word such that words that are similar in meaning are close to one another.\n\nResearchers at Google Brain did exactly this, with their software system Word2Vec. The realization key to their implementation is that, although words don\u2019t have a continuous definition of meaning we can use for the distance optimization, they do approximately obey a simple rule popular in the Natural Language Processing Literature\n\nAt a high level, this means that rather than optimizing for similar words to be close together, they assume that words that are often in similar contexts have similar meanings, and optimize for that directly instead.\n\nMore specifically, the prevailing success was with a model called Skip-grams, which tasked their model with directly outputting a probability distribution of neighboring words (not always directly neighboring, they would often skip a few words to make the data more diverse, hence the name \u201cskip grams\u201d). Once it was good at predicting the probability of words in its context, they took the hidden layer weight matrix and used it as a set of dense continuous vectors representing the words in their vocabulary.\n\nOnce this optimization is completed, the resulting word vectors have exactly the property we wanted them to have\u200a\u2014\u200aamazing! We\u2019ll see that this is exactly the pattern of success here\u200a\u2014\u200ait doesn\u2019t take much, just a good formulation of what you\u2019re optimizing for. The initial Word2Vec results contained some pretty astonishing figures\u200a\u2014\u200ain particular, they showed that not only were similar words near each other, but that the dimensions of variability were consistent with simple geometric operations.\n\nFor example, you could take the continuous vector representation for king, subtract from it the one for man, add the one for woman, and the closest vector to the result is the representation for queen.\n\nThat is the power of density, by forcing these representations to be close to one another, regularities in the language become regularities in the embedded space. This is a desirable property for getting the most out of your data, and is generally necessary in our representations if we are to expect generalization.\n\nNow that we have a way to convert words from human-readable sequences of letters into computer readable sequences of N-dimensional vectors, we can process our sentences similarly to our GIFs\u200a\u2014\u200awith dimensions: the dimensionality of the word vectors, and the sentence length.\n\nThis leaves us with our sentences looking somewhat like rectangles, with durations and heights, and our GIFs looking like rectangular prisms, with durations, heights, and widths.\n\nJust like with the CNN\u200a\u2014\u200awe\u2019d like to take an RNN trained on a task that requires skills we want to reuse, and isolate the representation from the RNN that immediately precedes the specificity of said task. There are many classical natural language understanding tasks like sentiment analysis, named entity recognition, coreference resolution, etc. but surprisingly few of them require general language understanding. Often, classical NLP methods that pay attention to little more than distinct word categories perform about as well as state-of-the-art deep learning powered systems. Why? In all but rare cases, these problems simply don\u2019t require much more than word level statistics. Classifying a sentence as positive or negative sentiment is roughly analogous to classifying whether an image is of the outdoors or indoors\u200a\u2014\u200ayou\u2019ll do pretty well just learning which colors are outdoors/indoors exclusive and classifying your image on that alone. For sentiment analysis, that method amounts to learning negative/positive weights for every word in a vocabulary, then to classify a sentence multiply the words found in that sentence by their weights and add it all up.\n\nThere are more complex cases, that require nuanced understanding of context and language to classify correctly, but those instances are infrequent. What often separates these remarkably simple cases from the more complex ones is the independence of the features: only weighting words as negative or positive would never correctly classify \u201cThe movie was not good\u201d\u200a\u2014\u200aat best it would appear neutral when you add up the effects of \u201cnot\u201d and \u201cgood\u201d. A model that understands the nuance of the language would need to integrate features across words\u200a\u2014\u200alike our CNN does with its many layers, and our RNN is expected to do over time.\n\nWhile the language tasks above rarely depend on this multi-step integration of features, some researchers at the University of Toronto found an objective that does\u200a\u2014\u200aand called it Skip-Thoughts. Like the skip-grams objective for finding general word embeddings, the skip-thoughts objective is that of predicting the context around a sentence given the sentence. The embedding comes from a GRU RNN instead of a shallow single hidden-layer neural network, but the objective, and means of isolating the representation, are the same.\n\nWe now have most of the pieces required to build the GIF search engine of our dreams. We have generic, relatively low dimensional, dense representations for both GIFs and sentences\u200a\u2014\u200athe next piece of the puzzle is comparing them to one another. Just as we did with words, we can embed completely different media into a joint space together, so long as we have a metric for their degree of association or similarity. Once a joint embedding like this is complete, we will be able to find synonymous GIFs the same way we did words\u200a\u2014\u200ajust return the ones closest in the embedding space.", 
        "title": "Building a Deep Learning Powered GIF Search Engine \u2013 Zain Shah \u2013"
    }, 
    {
        "url": "https://medium.com/@vivek.yadav/dealing-with-unbalanced-data-generating-additional-data-by-jittering-the-original-image-7497fe2119c3?source=tag_archive---------1----------------", 
        "text": "In many deep learning applications, we often come across data sets where one type of data may be seen more than other types. For example, in a traffic sign identification task, there may be more stop signs than speed limit signs. Therefore, in these cases, we need to make sure that the trained model is not biased towards the class that has more data. As an example, consider a data set where there are 5 speed limit signs and 20 stop signs. If the model predicts all signs to be stop signs, its accuracy is 80%. Further, f1-score of such a model is 0.88. Therefore, the model has high tendency to be biased toward the \u2018stop\u2019 sign class. In such cases, additional data can be generated to make the size of data sets similar.\n\nOne way to collect more data is to take the picture of the same sign from different angles. This can be done easily in openCV by applying affine transformations, such as rotations, translations and shearing. Affine transformations are transformations where the parallel lines before transformation remain parallel after transformation.\n\nBelow I present a function that can be used to generate jittered images. The function takes in the original image, range of angle rotation, range of translation and range of shearing and returns a jittered image. As the function chooses true transformation values from a uniform distribution that is specified by these ranges.\n\nNote, the same techniques can be applied using image generator in Keras. However, the transform_image() function provided here will help you play with the parameters and see whats happening under the hood.\n\nAfter applying the function transform_image() found at\u00a0. The new generated images are shown in the figure below.\n\nThe code to execute the command above can be found here, https://github.com/vxy10/ImageAugmentation", 
        "title": "Dealing with unbalanced data: Generating additional data by jittering the original image"
    }, 
    {
        "url": "https://medium.com/ai-business/dkc-award-2724d3c82883?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Team AI\u304cDKC\u8cde\u3092\u53d7\u8cde\u3057\u307e\u3057\u305f \u2013 Team AI Blog \u2013"
    }, 
    {
        "url": "https://cenksezgin.com/deep-learning-cant-solve-this-yet-3dbb3fd68bd1?source=tag_archive---------3----------------", 
        "text": "Let\u2019s first start by translating these sentences in another language\u00a0:\n\nThe trophy would not fit into the suitcase as it was too big.\n\nThe trophy would not fit into the suitcase as it was too small.\n\nNot obvious, right\u00a0? This is where convolutional neural networks and deep learning got stuck for now. Even though it can be trained with a large and specific dataset, it still needs to know that large things don\u2019t fit in smaller things. And that, you can\u2019t train with a translation data set.\n\nThis is why i think that Deep Learning is great, but it\u2019s not the only thing that will solve everything. In addition to many things that we can\u2019t think of now, we need, like a baby, to learn basic foundational things like larger things don\u2019t fit in smaller things. Therefore I believe we would be judging in a few years intelligence of Virtual Assistant like Cortana, M, Alexa or Siri. We will be talking about\u00a0\u2026 is so stupid or\u00a0\u2026 more intelligent than\u00a0\u2026 in these areas but for this specific area\u00a0\u2026 is better.\n\nFinally, as there is no perfect human being who is the role model for the world, and we had wars for thousands of years and we still have, I believe that it won\u2019t be that easy to build the perfect virtual assistant and we will have to continue to think about these topics for many many years.", 
        "title": "Deep Learning can\u2019t solve this (yet) \u2013"
    }, 
    {
        "url": "https://cenksezgin.com/biology-and-deep-learning-bb529836b36c?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Biology and Deep Learning \u2013"
    }, 
    {
        "url": "https://cenksezgin.com/deep-learning-why-now-c7c0f991ef83?source=tag_archive---------5----------------", 
        "text": "I would like to share my thoughts on where we are now, in terms of computer intelligence evolution and why we are experiencing a historical milestone today. Before starting I will analyse the situation in two categories, for that I will use an analogy I like to call\u00a0:\n\nBasically, in order to understand this analogy, what you need to realise is that the eye can perceive hundreds/thousands of information in less than 1 second almost without any limit, but on the other hand the mouth can\u2019t say much in this short amount of time. There is a big difference between what the eye can perceive and a mouth can speak and this creates an Input/Output problem, you have a lot of input very fast, but you can\u2019t give an output that fast, there is a capacity issue with the output. I believe this gives a clue on the brain capacity needed to perceive things.\n\nNow let\u2019s come back to the computers\u2019 evolution. First, we taught computers to take action instead of us, because we were lazy and it was the first thing any human would do. We didn\u2019t wanted to do things, work harder and this laziness developed computers in an action-focused way. Now there are thousands of smart tasks a computer can do instead of you (get the total of your expenses, calculate sinus(37), remind tasks, copy/paste your friend\u2019s homework etc.).\n\nThen we started asking for more from computers, for smarter tasks. For that computers needed to sense and perceive things they weren\u2019t trained for. And if you can project the Eye Mouth Analogy to this situation, it is not hard to realise that even though the evolution until now was huge, compared to what we need to achieve is not that much. Because there is a huge difference of brain capacity between action-based tasks and perception-based tasks (i.e. understanding the meaning of a speech correctly). This will be of course accelerated due to foundational problems we solved in the challenge earlier, but as the things we are trying to achieve (like imitating brain\u2019s perception-based skills) aren\u2019t even totally understood biologically by humans, we need to use the best part of the brain which we still can\u2019t really explain how it works, called self-learning. And this is where we need approaches like Deep Learning. With these algorithms based on neuroscience, we will be able to build things that can learn by itself where we can\u2019t really describe how.\n\nFinally we will be able to join action-based and perception-based skills together to create virtual brains that can learn and execute the tasks we need help with, this could be finding out how to solve cold fusion with 128 virtual brains, or accelerate the execution of a very complex computing that would normally take 6 months.", 
        "title": "Deep Learning, Why Now ? \u2013"
    }
]