[
    {
        "url": "https://medium.com/@4quant/tricking-deep-learning-ae54b23c36cd?source=tag_archive---------0----------------", 
        "text": "Since Neural Networks and Deep Learning are becoming such popular themes we show a few areas where they have substantial room for improvement. Based on the work of Anh Nguyen et al we show how easy it is to manipulate some of these networks.\n\nBasically we use the same basic process for teaching the network to recognize the image. Instead of changing the weights to optimize the outcome, we change the input image.\n\nTo visually represent this we will use an idea of a computational graph. Basically this graph shows a flow chart of how the final result is computed. Since Deep Neural Networks are so named because they have so many layers these are not always easy to draw. The image below shows just part of a standard image classification graph. We can look at smaller pieces to understand them better.\n\nLooking at a small section of the network we can describe what the training process does in more detail. Each node represents either an operation or a value and the arrows show how information flows through the system. Google\u2019s Deep Learning framework, TensorFlow, is named after exactly this idea of tensors (images, matrices, words, vectors,\u00a0\u2026) flowing through such systems. The important aspect for training is breaking the network into two components. The first is the output, in this case softmax. This is used to create the loss function (not shown) which is the way of scoring how well the network is accomplishing the desired task. The second are the variables, in this case softmax/weights and softmax/biases. The variables are the portions which can be changed and updated to improve the final result.\n\nHere we change the way the training works to make changes or updates to the input image instead of the weights and biases. We also modify the loss function and output so it tries to increase the jellyfish-ness. The network is then run through many iterations with each iteration causing a small change to the input image.\n\nHere we show the trickery as it evolves. The most important aspects to pay important to are the final predictions (bottom left) and the loss history (bottom right).\n\nWhile the results might initially seem quite drastic, and it might seem logical to completely distrust any results from neural networks that is probably a bit exaggerated. Since we had access to the complete network and could train as we wanted the results are significantly more successful than they would be on a blackbox network (which is the case for most public image APIs for example).\n\nThe more important take away message is that the networks trained, even if they have been trained on millions of images, still do not really \u2018understand\u2019 the images. They ultimately are still recognizing certain combinations of features and this \u2018trickery\u2019 shows how irrelevant many of these features still are. As these networks become more complicated and are trained on even larger datasets, many of these problems will be solved or at least greatly diminished on their own. Fundamentally such sanity checks are an important step to evaluate such networks and see if they are in fact learning the correct information from images.\n\nAdditionally new approaches to training and improving these networks like Adversarial Learning can further improve networks beyond the training data. These ideas have already been successfully deployed in a number of applications and were a component that gave AlphaGo by DeepMind the upper hand.\n\nWe can apply the same process as above to the standard panda image bundled with the Inception network. Here we apply a few small (but more noticeable than with the jellyfish) changes to change the panda into a porcupine.\n\nWe can similarly plot the associated metrics and difference images to follow more closely what happens as these change take place.\n\nFor this example we show a man transforming into a pig without any perceivable change to the image. If you\u2019ve ever wished you could determine how \u2018pig-like\u2019 your friend is, now you can put a number on it: 100% pig in 300 iterations or less\n\n4Quant specializes in delivering Big Image Analytics solutions using our analytics platform built on and tightly integrated with Apache Spark and Google TensorFlow.", 
        "title": "Tricking Deep Learning \u2013 Kevin Mader \u2013"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/ai-research-institute-was-established-by-nec-and-aist-f145b7a06bfb?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "AI Research Institute was Established by NEC and AIST"
    }
]