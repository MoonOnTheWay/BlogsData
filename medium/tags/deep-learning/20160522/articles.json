[
    {
        "url": "https://medium.com/@scottlegrand/first-dsstne-benchmarks-tldr-almost-15x-faster-than-tensorflow-393dbeb80c0f?source=tag_archive---------0----------------", 
        "text": "Today, I\u2019d like to report the first official benchmarks of DSSTNE on training the MovieLens 20M view dataset. Upon its release, Amazon reported that DSSTNE is approximately 2.1X faster than an equivalent TensorFlow implementation on a single GPU of an AWS g2.8xlarge instance. The data presented today will make two further points:\n\nUpdate 1/14/2017: not only does TensorFlow continue to not support automagic model parallelism, but the single GPU benchmark in TensorFlow now seg faults, sigh.\n\nUpdate 4/10/2018: In the subsequent year or so, not a single other major framework added support for intra-layer model parallelism or sparse backpropagation. I am fascinated by this. Model parallelism not only allows one to train and operate larger models, but as the data here indicates, one can even scale small models by exploiting increased multi-GPU memory bandwidth and FLOPs whilst minimizing communication costs relative to data parallelism.\n\nFor K520 numbers, all benchmarks were run on a single g2.8xlarge AWS instance, using system memory MPI collectives to transmit data between virtualized GPUs.\n\nFor K80 numbers, all benchmarks were run bare metal.\n\nFinally, for Tesla M40, all benchmarks were run on an Asus X99-E WS motherboard, powered by an Intel 5930K CPU, with 64GB of system memory and a 1500W power supply, essentially a DIY Digits Dev Box.\n\nDSSTNE is available from https://github.com/amznlabs/amazon-dsstne\n\nTensorFlow 0.8 was used for all measurements here. The TensorFlow benchmark code is available from https://github.com/amznlabs/amazon-dsstne/tree/master/benchmarks/tf\n\nFinally, directions for running these benchmarks are available from https://github.com/amznlabs/amazon-dsstne/blob/master/benchmarks/Benchmark.md", 
        "title": "First DSSTNE Benchmarks TLDR: Up to Almost 15x Faster than TensorFlow"
    }, 
    {
        "url": "https://medium.com/@scottlegrand/putting-the-p-u-in-tpu-6643ebaccf55?source=tag_archive---------1----------------", 
        "text": "This past week Google announced that they\u2019ve been using a custom low-power ASIC for production-level ML inference for the past year or so (it even helped drive AlphaGo\u2019s victory over Lee Sedol). They claimed it delivers 10x better perf/W compared to all other (unspecified, perhaps intentionally) options. On the surface, this seems pretty cool. For last year, Altera/Intel was shopping around Arria 10 for this task (https://www.altera.com/en_US/pdfs/literature/solution-sheets/efficient_neural_networks.pdf) saying it classified ~9 images/second/Watt while NVIDIA was suggesting Tesla M4 (http://www.nvidia.com/object/tesla-m4.html) saying it classified ~20 images/s/W. Taking this at face value, achieving 200 images/s/W (10 * 20) would be pretty cool, right? And Google jumped right on the live grenade stating they had advanced Moore\u2019s Law by 7 years(tm) with this widget.\n\nExcept\u2026 They didn\u2019t specify what tasks at which this chip is \u201can order of magnitude better perf/W\u201d nor did they provide any absolute performance numbers. Was their benchmark the near standard of GPUs running AlexNet for inference? And if so, was that with the optimized Neon or cuDNN GPU kernels? Or was it with the not-so-optimized original TensorFlow kernels, which upon release, were over 3x slower than TensorFlow\u2019s competing frameworks (http://web.archive.org/web/20151208145149/https://github.com/soumith/convnet-benchmarks). Google\u2019s not saying so far nor are they providing any sort of raw performance numbers for any task, just the vague assertion that the TPU is 10x better perf/W. In my experience and opinion, such perf/W without accompanying perf is all hat and no horse, especially coming from the author of this whitepaper: http://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36448.pdf.\n\nAlso\u2026 As more information was revealed, we found out that the Google TPU is instead based on 8-bit integer math (http://www.nextplatform.com/2016/05/19/google-takes-unconventional-route-homegrown-machine-learning-chips/), while the numbers supplied above for FPGAs and GPUs are based on the use of full IEEE-compliant 32-bit floating point math. This changes the story a lot for me. For example, an off-the-shelf 10W NVIDIA Tegra X1 can exploit non-compliant 16-bit floating point math to achieve ~45 images/s/W with cuDNN (https://devblogs.nvidia.com/parallelforall/inference-next-step-gpu-accelerated-deep-learning/). And assuming this race to relive all the fun of 1980s dynamic range headaches continues (https://arxiv.org/pdf/1502.02551.pdf, spoilers, stochastic sampling lets you take this silliness down to 1-bit per weight per minibatch), rather than advancing Moore\u2019s Law, I think we\u2019ve just been given a preview of a future 4\u2013way vectorized SIMD PTX instruction for Volta and later GPUs (http://docs.nvidia.com/cuda/parallel-thread-execution/#simd-video-instructions-vadd4-vsub4-vavrg4-vabsdiff4-vmin4-vmax4). Because, ya know, ~90 images/s/W would not suck and I think GTX 1080 OoTB is likely to hit ~70 Images/s/W in FP16 so we\u2019re talking $500-$700 for a consumer chip that could classify~140 images/s/W within a year or so, roughly twice that for a Grid/Quadro/Tesla variant.\n\nAddendum 06/06/2016: The recently released GP104 (GTX 1080) has HW support for 8-bit and 16-bit integer/fixed-point math/training/inference. TLDR: 35 TDLOPS (deep learning ops, NVIDIA\u2019s silliest term since redefining \u201ccore\u201d, not mine) for $600. I\u2019m betting that\u2019s a lot better than the TPU and the difference is that you can buy one. Count on DSSTNE support for this feature by year\u2019s end.\n\nEverything depends on what inadequate low-power options Google rejected in favor of spending tens of millions of dollars to build this custom ASIC. Is this a case where Google really made a breakthrough in perf/W no one could hope to match? Did the HW vendors just make a mess out of their supposedly low-power options and lose the sale here (The TPU has a 20W TDP compared to 65 and 75 for Altera\u2019s and NVIDIA\u2019s datacenter options respectively)? Should NVIDIA have just built a 10W Tegra X1 card for inference? Or is Google playing 8-bit games with us all like their neural networks play Atari 2600 classics by defining 10x better perf/W as whatever they decide it is? Google\u2019s not talking so far.", 
        "title": "Putting The P U in TPU \u2013 Scott Le Grand \u2013"
    }, 
    {
        "url": "https://medium.com/@_futureality/ml-day-9-beginning-to-study-tensorflow-4b5927343323?source=tag_archive---------2----------------", 
        "text": "I\u2019m pretty excited because I finally understand the beginner\u2019s intro to TensorFlow. When TF first came out a few months ago I was excited, but lost in the definitions. Over the last few weeks my readings have finally put me in a place where I can understand what\u2019s going on. Yay!\n\nI\u2019m going to summarize it in terms that I hope non-technical folks who are following along can understand.\n\nBasically, a neural net can be thought of as a function, or a transformation on data. If you put in some data to the input (say, some pixels in an image), the NN does some stuff and then outputs some data. In the diagram below, the input is defined by a, b, c, and d, and the output is u, x, y, z. In principle, there can be many inputs, and many outputs, and the numbers of each need not be the same. The interesting thing about a neural net is that its structure is defined by what are sometimes called \u201chyperparameters.\u201d In the case below, we already have two hyperparameters: the number of inputs is 4, and the number of outputs is 4. There are also hyperparameters which describe the \u201chidden layers.\u201d For the purposes of this description I\u2019m going to just call those hyperparameters H and not get into the specifics of what H looks like. That means that this diagram is fully described by {4, 4, H}.\n\nNow, without a \u201cjob\u201d that this neural net is supposed to do, it\u2019s fairly meaningless. When we think about jobs that it might do we might think of classifications (e.g. \u201cis the input describing a dog?,\u201d \u201cis the input a song?,\u201d \u201cis the input a \u2018good song?\u2019\u201d) Really any predicate can define a classification, but the classification need not be a yes or no question. E.g. we might want to say that if u is close to 1, while x, y, and z are close to 0 then this means that the picture has a mammal, x = 1 & y, u, z close to 0 means that the picture has a lizard, y = 1\u2026 means that the picture has a fish, etc. We get to define what our output \u201cmeans.\u201d We also get to describe what our intput \u201cmeans.\u201d This is pretty cool if you think about it, because it means that the system is actually a semantic transformation. To the extent that we know what the input \u201cmeans\u201d and what the output \u201cmeans\u201d we can say whether or not the NN is doing a \u201cgood job\u201d or a \u201cbad job.\u201d\n\nTensorFlow lets you describe this as precisely as you want, with a \u201ccost function.\u201d The cost function basically tells the system how good/bad a job the system is doing.\n\nAt this point, TF has allowed us to describe the hyperparameters of our NN (essentially a basic description of the system), and also a cost function which describes how good the NN is at doing a particular job we\u2019re interested in having it do. The next step is where the magic happens! If we\u2019ve defined these two pieces of information precisely, TF has many algorithms for letting us evolve our hyperparameters to make the NN do a better job. One method for doing this is called \u201cGradient Descent\u201d\u200a\u2014\u200aessentially, TF can look at our current set of hyperparameters and the value of the cost function on a particular piece or set of data, and make small adjustments to the hyperparameters to make the NN perform slightly better on this data set. In this way, the NN slowly \u201clearns\u201d to do its job better.", 
        "title": "ML Day #9 \u2014 Beginning to study TensorFlow \u2013 Manfred Macx \u2013"
    }
]