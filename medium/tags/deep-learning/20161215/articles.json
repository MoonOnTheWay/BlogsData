[
    {
        "url": "https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5?source=tag_archive---------0----------------", 
        "text": "Now that we know a little bit about dropout and the motivation, let\u2019s go into some detail. If you just wanted an overview of dropout in neural networks, the above two sections would be sufficient. In this section, I will touch upon some more technicality.\n\nIn machine learning, regularization is way to prevent over-fitting. Regularization reduces over-fitting by adding a penalty to the loss function. By adding this penalty, the model is trained such that it does not learn interdependent set of features weights. Those of you who know Logistic Regression might be familiar with L1 (Laplacian) and L2 (Gaussian) penalties.\n\nDropout is an approach to regularization in neural networks which helps reducing interdependent learning amongst the neurons.\n\nTraining Phase: For each hidden layer, for each training sample, for each iteration, ignore (zero out) a random fraction, p, of nodes (and corresponding activations).\n\nUse all activations, but reduce them by a factor p (to account for the missing activations during training).", 
        "title": "Learning Less to Learn Better \u2014 Dropout in (Deep) Machine learning"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/the-end-of-monolithic-deep-learning-86937c86bc1f?source=tag_archive---------1----------------", 
        "text": "Deep Learning compared to other Machine Learning methods is remarkably modular. This modularity gives it unprecedented capabilities that places Deep Learning head and shoulders above any other conventional Machine Learning approach. Recent research however is pointing to even greater modularity than previously. It is likely that quite soon, monolithic Deep Learning systems will become a thing of the past.\n\nBefore I discuss what is coming in the future, let me first discuss the concept of modularity. It is a concept that software engineering is familiar with, but the idea is not as commonly found in machine learning.\n\nIn computer science, we build up complex systems from modules. One module built from more simple modules. It is what enables us to build our digital world based on just NAND or NOR gates. Universal Boolean operators are a necessity, but they are not sufficient enough to build complex system. Complex computing systems require modularity so that we have a tractable way of managing complexity.\n\nHere are six core design operators that are required to be supported in a modular systems:\n\nThese operators are of a general nature and inherent in any modular design. They allow the modification of existing structure into new structures in well-defined ways. In the context applied to software this can mean refactoring operators at the source code level, language constructs at specification time or can mean component models at configuration time. These operators are complete in that they are capable of generating any structure in computer design.\n\nThe six operator definition focuses on functional invariance in the presence of design transformations. Said more clearly, we can apply these operators and not affect the function of the whole.\n\nIn the context of deep learning, the modularity operators are enabled as follows:\n\nThe two remaining modularity operators are not available to current monolithic DL systems.\n\nNevertheless, despite these two short comings, DL systems are unmatched advantage over competing machine learning techniques.\n\nOne of the reasons for the tight coupling of layers in a monolithic DL system can be traced back to Stochastic Gradient Descent (SGD). SGD works in lock-step mode with training. It is a very highly synchronized mechanism that requires behavioral coordination across all layers.\n\nThis monolithic construction is however being replaced by an even more modular system. Here are two interesting developments related to this.\n\nDeepMind has researched a method called \u201cSynthetic Gradients\u201d that shows a way towards more loosely coupled layers. The method essentially inserts a proxy neural network in between layers to approximate the gradient descent:\n\nThe second development that can lead to greater modularity is the concept of generative adversarial networks (GANs). Typical GANs have two competing neural networks that are essentially decoupled, but contribute to the global objective function. However, we are now seeing in research the emergence of more complicated configurations like this:\n\nWhere you have a ladder like network of decoupled encoder, generators and discriminators. The prevalent pattern here is that every conventional function in a neural network has been also replaced by a neural network. More specifically, the SGD algorithm and the objective function have been themselves been replaced by neural networks. Gone are any analytic functions! This is what happens when you have Deep Meta-Learning.\n\nAnother very impressive result that was recently published with also the same name (i.e. StackGAN), shows how effective multiple decoupled GANs can be:\n\nThe task here is to take as input a text description and generate an image corresponding to the description. Here we have two GANs staged one after the other. The second GAN is able to refine the fuzzy image into one of a higher resolution. Modular networks have the capability of factorizing capabilities that would otherwise be entangled in an end-to-end network.\n\nIn software engineering we have the concept of APIs. That is, a restrictive language that communicates between different modules. In the scenario above neural networks that \u201clearning to communicate\u201d acts as the bridge APIs between the modules. More generally, we have networks that \u201clearn how to interface\u201d. From \u201cLearning to Communicate with Deep Multi-Agent Reinforcement Learning\u201d:\n\nAnother recent paper titled \u201cGenerative Adversarial Parallelism\u201d explores this further in relationship to GANs.\u00a0. In this work, the authors attempt to address the difficulty in training GANs by extending the usual two player generative adversarial games into a multi-player game. They train many GAN-like variants in parallel and while doing so, they periodically swap around the discriminator and generator pairs. The motivation here is to achieve a better decoupling between the pairs. There is still much work to be done to determine if if decoupled interfaces between networks lead to be better generalization.\n\nThere\u2019s still a ton of research to be done to begin to understand how to build the APIs for DL modules. Right now however, we have the benefit of meta-learning techniques that will automatically learn the interface specification. This indeed is a very interesting research topic. How do decoupled networks learn how to interface?", 
        "title": "The Emergence of Modular Deep Learning \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/@Francesco_AI/ai-and-speech-recognition-a-primer-for-chatbots-a63af042526a?source=tag_archive---------2----------------", 
        "text": "Our smartphone currently represents the most expensive area to be purchased per squared centimeter (even more expensive than the square meters price of houses in Beverly Hills), and it is not hard to envision that having a bot as unique interfaces will make this area worth almost zero.\n\nNone of these would be possible though without heavily investing in speech recognition research. Deep Reinforcement Learning (DFL) has been the boss in town for the past few years and it has been fed by human feedbacks. However, I personally believe that soon we will move toward a B2B (bot-to-bot) training for a very simple reason: the reward structure. Humans spend time training their bots if they are enough compensated for their effort.\n\nThis is not a new concept, and it is something Li Deng (Microsoft) and his group are really aware of. He actually provides a great threefold classification of AI bots:\n\nFor the first two, the reward structure is indeed pretty easy to be defined, while the third one is more complex, which makes it more difficult to be approached nowadays.\n\nWhen this third class will be fully implemented, though, we would find ourselves living in a world where machines communicate among themselves and with humans in the same way. In this world, the bot-to-bot business model will be something ordinary and it is going to be populated by two types of bots: master bots and follower bots.\n\nI believe that research in speech recognition adds up, as well as the technology stacks in this specific space. This would result in some players creating \u201cuniversal\u201d bots (master bots) which everyone else will use as gateways for their (peripheral) interfaces and applications. The good thing of this centralized (and almost monopolistic) scenario is, however, that in spite of the two-levels degree of complexity, we won\u2019t have the black box issue affecting the deep learning movement today because bots (either master or follower) will communicate between themselves in plain English rather than in any programming language.\n\nTraditionally, we can think of deep learning models for speech recognition as either retrieval-based models or generative-models. The first class of models uses heuristics to draw answers from predefined responses given some inputs and context, while the latter generates new responses from scratch each time.\n\nThe state-of-art of speech recognition today has raised a lot since 2012, with deep-q networks (DQNs), deep belief networks (DBN), long short-term memory RNN, Gated Recurrent Unit (GRU), Sequence-to-sequence Learning (Sutskever et al., 2014), and Tensor Product Representations (for a great overview on speech recognition, look at Deng and Li, 2013).\n\nSo, if DFL breakthroughs were able to improve our understanding of the machine cognition, what is preventing us from realizing the perfect social bots? Well, there are at least a couple of things I can think of.\n\nFirst of all, machine translation is still in its infancy. Google has recently created a \u201cNeural Machine Translation\u201d, a relevant leap ahead in the field, with the new version even enabling zero-short translation (in languages which they were not trained for).\n\nSecond, speech recognition is still mainly a supervised process. We might need to put further effort into Unsupervised Learning, and eventually even better integrate the symbolic and neural representations.\n\nFurthermore, there are many nuances of human speech recognition which we are not able to fully embed into a machine yet. MetaMind is doing a great work in the space and it recently introduced Joint Many-Tasks (JMT) and the Dynamic Coattention Network(DCN), respectively an end-to-end trainable model which allows collaboration between different layers and a network that reads through documents having an internal representation of the documents conditioned on the question that it is trying to answer.\n\nFinally, the automatic speech recognition (ASR) engines created so far were either lacking personality or completely missing the spatiotemporal context. These are two essential aspects for a general CUI, and only a few works have been tried up to date (Yao et al., 2015; Li et al., 2016).\n\nThis was not originally intended to part of this article, but I found useful to go quickly through main players in the space in order to understand the importance of speech recognition in business contexts.\n\nThe history of bots goes back to Eliza (1966, the first bot ever), Parry (1968) to eventually ALICE and Clever in the nineties and Microsoft Xiaoice more recently, but it evolved a lot over the last 2\u20133 years.\n\nI like to think about this market according to this 2 by 2 matrix. You can indeed classify bots as native or enablers, designed for either specific or generic applications. The edges of this classification are only roughed out and you might actually have companies operating at the intersection between two of these quadrants:\n\nFollowing this classification, we can identify four different types of startups:\n\nA few (non-exhaustive) examples of companies operating in each group have been provided, but it is clear how this market is becoming crowded and really profitable.\n\nIt is an exciting time to be working on deep learning for speech recognition. Not only the research community but the market as well are quickly recognizing the importance of the field as an essential step to the development of an AGI.\n\nThe current state of ASR and bots reflect very well the distinction between narrow AI and general intelligence, and I believe we should carefully manage the expectations of both investors and customers. I am also convinced is not a space in which everyone will have a slice of the pie and that a few players will eat most of the market, but it is so quick-moving that is really hard to make predictions on it.\n\nDeng, L., Li, X. (2013). \u201cMachine Learning Paradigms for Speech Recognition: An Overview\u201d. IEEE Transaction on audio, speech, and language processing 21(5).", 
        "title": "AI and Speech Recognition: A Primer for Chatbots \u2013 Francesco Corea \u2013"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-could-be-market-driven-de770aeebd3?source=tag_archive---------3----------------", 
        "text": "Deep Learning (DL) systems will evolve from the current monolithic systems, to more modular systems. The traditional DL system is trained end-to-end with a single objective function and optimization algorithm. We however are already seeing newer systems like GANs, that involve more than one DL system. GANs employ a generator and a discriminator, that are in an adversarial relationship, competing against each other. The main difficulty of training GANs is that finding an equilibrium is difficult.\n\nAs we progress to even more complex systems, that involve more than two participating DL systems, we need to have some guidance as how to coordinate multiple parties. Here is an example of one of the newer networks that reveal 6 different DL systems working in an adversarial context:\n\nMore advanced DL systems will require even more complex coordination mechanisms.\n\nCivilization actually has a mechanism that is very efficient in coordinating multiple parties. We use currency (i.e. money) to coordinate multiple parties with competing agendas. Perhaps, through the use of market driven dynamics, we can invent a better mechanism for handling coordination.\n\nIt is indeed interesting that in Physics, there are many conservation laws. These laws such as the conservation of momentum or the conservation of energy govern the physical world and give order to chaos. Without conservation laws, strange things would happen in our world all the time. The current understanding of the Higgs boson (aka The God particle) is that its mass is at a certain level that if it were any less or any more, a universe like ours would not exist. Our universe is in exists, because some God particle is of a certain specific mass. The ultimate hyper-parameter!\n\nMarket driven systems have money as the conserved quantity (with the exception of Quantitative Easing). The basic conceptual idea here is to have the market of networks coordinate themselves with money. A virtual currency so to speak. To drive the system towards objects, one would institute monetary incentives or disincentives. There isn\u2019t any explicit objective function here other than having mechanism that would \u201cencourage\u201d certain behaviors.\n\nMarket driven machine learning has in fact been written about previously. In a paper entitled \u201cAn Introduction to Artificial Prediction Markets for Classification\u201d the authors describe a framework for fusing multiple classifiers:\n\nAnother paper entitled \u201cMulti-period Trading Prediction Markets with Connections to Machine Learning\u201d goes one step further by introducing market makers into the mix:\n\nFinally, David Balduzzi has a paper \u201cCortical Prediction Markets\u201d where he poses the argument that the spiking neural networks are driven by market forces:\n\nA recent DeepMind paper hidden in the ICLR 2017 haystack entitled \u201cMetacontrol for Adaptive Imagination-Based Optimization\u201d discusses a particular kind of RL algorithm that coordinates the behavior of multiple experts through a market driven approach:\n\nIn short, you have here systems that pursue solutions via the allocation of a scarce commodity (i.e. currency). Complex organizations require coordination and a distributed mechanism to perform that coordination is through a currency. That\u2019s the beauty of markets, coordination is distributed and that\u2019s a key idea to take away from this.\n\nThis indeed is an extremely promising principled approach (rather than the ad-hoc way we have today) to the problem of discovering an equilibrium in a the Collaborative Classification with Imperfect Knowledge (CCIK) level of DL intelligence. See: \u201cThe Five Capability Levels of Deep Learning Intelligence\u201d.\n\nJohn Holland in 2010 had a TEDx talk about \u201cBuilding Blocks and Innovation\u201d where he describes 2 big problems about Complex Adaptive Systems (CAS):\n\nThese two problems are features of market driven systems.\n\nhttps://arxiv.org/pdf/1612.05159v1.pdf Improving Scalability of Reinforcement Learning by Separation of Concerns", 
        "title": "Equilibrium Discovery in Modular Deep Learning Architectures"
    }, 
    {
        "url": "https://blog.grakn.ai/advent-at-grakn-labs-15th-december-2016-30c929bedcc?source=tag_archive---------4----------------", 
        "text": "I am not going to get into too many details in this article as the field is quite large and I am far from an expert. I am just going to touch on the general process used when trying to make predictions using historical data. Then I am going to poke my head into some cool tech within this field.\n\nDoes this mean we can predict future lottery numbers based on past lottery numbers? Sadly no, but, if anyone wants to prove me wrong, I will require at least 3 successful live demonstrations before I am convinced.\n\nPredictive analytics is an umbrella term used to describe the process of applying various computational techniques with the objective of making some predictions about the future based on past data. This encompasses a variety of techniques including data mining, modelling, pattern recognition, and even graph analytics.\n\nSo now it\u2019s my turn to peak into our virtual advent calendar and today\u2019s techie topic is\u00a0.\u00a0.\u00a0. Predictive Analytics . Great, analytics is quite a large field so today lets focus on common goal of the field which is making predictions based on past observations.\n\nThe first step in the process is usually all about data mining and filtering. Many data sources are often quite large and unstructured. So this step is all about extracting structured data from sources. On the topic of sources, be sure to select relevant and trusted sources. If I were trying to predict election results I would probably avoid using The Onion\u200a\u2014\u200aalthough given political outcomes this year I may be wrong.\n\nHere we need to start focusing on the contents of the data. This alone can prove to be quite a challenge. For example, if I am trying to make predictions about my own health, what information should I take into account? Do I smoke? What is my favourite colour? Where do I work? Often determining what is relevant and what is not is its own challenge. Proper pre-processing and filtering techniques are a must when cleaning up your data.\n\nYou should also ensure your data is of good quality. A reliable source alone does not ensure quality. What if you scraped your data from wikipedia on the day someone thought it would be fun to vandalise the articles you were mining? Running your data through existing analysis pipelines could be quite informative and a simple method of spotting questionable data. More formally you can use confirmatory factor analysis to ensure your extracted data will at least fit your model. It is also recommend that you apply other statistical techniques to ensure your data can account for variance, false positives, and other issues which often crop up from real world data.\n\nThis step is fundamental as it allows you to structure your data in such a way that you can start recognising patterns that potentially allow you to extract future trends. Models also allow you to formally describe your data. This is helpful in understanding the results you get from your data analysis but is also a good starting point when it comes time to visualise your results.\n\nSimilarly to data extraction, your models should undergo the same scrutiny. You should ensure that your models are valid representations of the issue you are trying to predict. Consulting with domain experts is often a good idea. Trying to predict inflation for the next years? Well you should probably speak to an economist as a first step when defining the model. When modelling ontologies at Grakn Labs I cannot count the times an expert on hand would have saved us from hours of deliberation.\n\nYour data is extracted, cleaned, quality checked, and fits your model. Time to start peering into your crystal ball and predicting the future.\u00a0.\u00a0. Oh wait, there are multiple crystal balls to use and you not sure which one will work.\n\nThis is where the massive field of machine learning can come into play. There are a multitude of ways to start recognising patterns in your data and exploiting those patterns. Neural Networks, Linear Regression, Bayesian Networks, Deep Learning: all of these and many more can help you to start making predictions. Personally, I recommend Graph Based Analytics, but I may be a bit biased here.\n\nLuckily, data analytics is becoming so desirable these days that many of these tools are available as simple applications. This means that it is now much easier to start analysing your data without the need to understand how each crystal ball works.", 
        "title": "Advent at Grakn Labs: Predictive Analytics \u2013"
    }, 
    {
        "url": "https://medium.com/@clementbergantz/thinking-technology-as-homo-sapiens-not-as-people-with-limited-lifespan-f040e423e956?source=tag_archive---------5----------------", 
        "text": "I started reading Yuval Noah Harari\u2019s bestseller \u201cSapiens: A Brief History of Humankind\u201d a few days ago.\n\nThe book explores in a chronological order, the ways in which biology and history have defined us. I read the first part, on the cognitive revolution and I started the second one, on the agricultural revolution this morning. So I only read 20% of the book but it already challenged the way I think about new technologies and the role they play in our evolution.\n\nYuval Noah Harari explains that the average hunter-gatherer (characterised by their nomadic lifestyle, collecting wild plants and pursuing wild animal) in Jericho of 8500 BC lived a harder life than the average farmer (settled in permanent villages and relying mainly on domesticated species) in Jericho of 9500 BC.\n\nTonight, switching from my Kindle\u200a\u2014\u200aand these disturbing words\u200a\u2014\u200ato my Facebook newsfeed on my iPhone, I came across two daily tech news\u00a0:\n\nThe first one is from the the smartest company in the world according to MIT\u2019s 2016 ranking (be careful with this ranking, Monsanto was ranked 50th)\u00a0: Amazon.\n\n\u201cAmazon\u2019s drone delivery service just got a big redesign! Delivery to your doorstep in 30 minutes\u201d said Product Hunt.\n\nThe second one came from Travis Kalanick, Uber\u2019s co-founder and CEO\u00a0: \u201cExcited to bring self-driving cars to our hometown of SF\u201d.\n\nI truly believe these two innovations and few others, will make people\u2019s live easier from today\u2019s perspective, as ancient hunter-gatherers did by investing more effort in cultivating wheat 11500 years ago in Jericho, Palestine. But how can we ensure that these wonderful improvements seen as people with limited lifespan are not harming us as homo sapiens\u00a0? How will our society evolve if we no longer have physical stimulation for example\u00a0?\n\nRight now, I don\u2019t have the answer, but I wish the great leaders and pioneers in deep learning technologies, who are shaping our future, got it. It is our responsibility. We can\u2019t turn back the clock and restart without these improvements if they are adopted.\n\nIf the answer is in the next chapters, please don\u2019t spoil me \ud83d\ude4f", 
        "title": "Thinking technology as homo sapiens, not as people with limited lifespan"
    }, 
    {
        "url": "https://medium.com/@novich/guacamole-theory-ultimate-palate-training-wheels-2da90e843e9?source=tag_archive---------6----------------", 
        "text": "Disclaimer: This entire article is opinion, and my sole personal opinion at that.\n\nGuacamole: so simple, but to make it taste friggin\u2019 great requires careful attention to a subset of important dimensions of taste. One must pay careful attention to:\n\nFailure to overlook one of these dimensions during production will result in depressing or gross tasting (sometimes even looking) guacamole. Pay attention, and you\u2019ll be rewarded with ultimate delciousness.\n\nTo be clear, I do not proclaim to have an ultimate recipe for guacamole. Far from it. It\u2019s a highly improvisational dish. And that\u2019s what makes it great for anyone learning to cook. It\u2019s so simple, but there are so many dials one can turn to tune the dish to one\u2019s tongue\u2019s mood. Yet\u200a\u2014\u200adespite taste being subjective, there are still clear objective boundaries (for the average person at least). No acid? No depth and color will quickly brown due to oxidation. No salt? Bland.\n\nThere are three (3) fundamental ingredients to making \u201cgood\u201d guacamole:\n\nIf one cannot make a good tasting guacamole with just these three ingredients, one has no business making guacamole at all. If one wants to get serious about guacamole (and I\u2019d argue cooking in general), making a simple basic guacamole is a first step.\n\nIt\u2019s not about recipe. It\u2019s about palate and process.\n\nThe avocado itself matters. Greatly. Its ripeness in particular. If it\u2019s not so ripe: it will taste bland and be difficult to mash. If it\u2019s super ripe: one\u2019s guacamole will quickly become mushy and lack texture. When one is at the market, it is important to lightly squeeze the avocado to determine its state of ripeness.\n\nNote that I am being careful with my words. I am not claiming that there is a fundamentally correct ripeness. Even if one\u2019s avocado is near the extremes\u200a\u2014\u200ashort of the avocado being so unripe it\u2019s rock hard or so ripe it\u2019s moldy\u200a\u2014\u200aas long as one strikes the right balance with one\u2019s other ingredients, one should be able to produce something that the average guacamole lover will proclaim as \u201cgood\u201d (though perhaps not \u201cgreat\u201d).\n\nWhen selecting an avocado at the market, one must also consider when one intends to make the guacamole. The selection of avocado available to a person may also provide guidance as to when the guacamole should be made. If the avocados at the market are all unripe, all is not lost\u200a\u2014\u200afor leaving the avocado out for a day or two will enable it to ripen further. If the avocado is subjectively deemed \u201cjust right\u201d in its ripeness\u200a\u2014\u200athen producing the guacamole should be an imminent affair.\n\nBy selecting and determining when to make the guacamole, one is controlling texture.\n\nWhen the avocado is deemed to be appropriately ripe, it is time to make the guacamole. Begin with the avocado. I will not go into the fine details of how to prepare it, short of that cubing the flesh is\u200a\u2014\u200ain my opinion\u200a\u2014\u200amost appropriate. Further, making it into the biggest possible cube size that one would be willing to eat such a cube by itself or on a tortilla chip is appropriate here.\n\nNext is mashing\u200a\u2014\u200aanother dial for controlling the fundamental dimension of texture. This is entirely subjective and to taste. One may wish for a chunky guacamole, in which case one might not mash even at all. Alternatively, one may wish for a smooth spreadable guacamole, in which case one might mash significantly. Or perhaps somewhere in between is the sweet-spot. But take note: subsequent steps will require some degree of mixing, which will further soften a mushify the guacamole. So, like avocado selection, a future outlook matters.\n\nSalt brings out the flavor of the guacamole. Salt must be added sparingly and in increments with tasting in between. Add some salt. Stir. Taste. Repeat this process until a desired level of saltiness has been reached. It\u2019s purely subjective, but I did enjoy David Chang\u2019s rambling on the topic in this Wired article.\n\nUnder-salting is recoverable. Add more salt. Over-salting is sometimes recoverable (e.g. adding more stock to a soup, or adding more avocado to the guac), but much more difficult to mitigate.\n\nI prefer using Kosher salt, because it\u2019s easy to sprinkle with my hands and I like how it distributes. There are no set rules here as to technique and type of salt.\n\nAcid adds some much needed depth to the guacamole. Importantly, it also helps to stave off oxidation of the guacamole\u200a\u2014\u200aturning it brown. I personally like to add acid (lime juice) after the salt. My tongue sometimes mistakes the two, and this can lead to adding too much acid.\n\nSimilar to salting, the acid should also be added in small increments with frequent tasting. Too much acid will overpower the guacamole and can destroy the texture.\n\nOnce one has mastered a basic guacamole and respects the fundamental process and ingredients, it is then appropriate to explore further. For then, one will understand the implications of how one\u2019s additional ingredients and techniques will impact their guacamole. Some examples of additional ingredients commonly seen:\n\nSome less common, but awesome ingredients to try:\n\nAnd the rabbit hole goes far deeper than this. While I want to say the sky\u2019s the limit, I\u2019m still in the midst of a philosophical crisis over NYT\u2019s pea guac\u200a\u2014\u200abut I can\u2019t knock it til I try it (if I ever will).\n\nI like it simple. Avocado (slightly underripe/chunky), lime juice, salt, maybe some fresh black pepper, diced white onion for a bit of crunch (not too finely chopped nor too thick).\n\nMy wife\u2019s family mixes avocado with chunky salsa. Surprisingly, it gets the job done. If it\u2019s good, it\u2019s good. No need to be pretentious!", 
        "title": "Guacamole Theory: Ultimate Palate Training Wheels \u2013 Scott Novich \u2013"
    }, 
    {
        "url": "https://medium.com/@dataaspirant/amazon-go-technology-fdbbf13bb511?source=tag_archive---------7----------------", 
        "text": "Physical store shopping could be a bottleneck when we are rushing to attend somewhere and the bill payment line more than expected. It will be more painful when we were doing physical store shopping on weekends. To address this issue, Amazon comes with an just walkout technology Amazon go.\n\nThis Just walkout technology developed using the similar kind of technologies used in the self-driving car. These are computer vision, Sensor fusion algorithms, and deep learning.", 
        "title": "Amazon Go Technology \u2013 dataaspirant \u2013"
    }
]