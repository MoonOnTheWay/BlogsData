[
    {
        "url": "https://medium.com/@nitin_pande/deep-neural-nets-and-the-purpose-of-life-d3d60a38d108?source=tag_archive---------0----------------", 
        "text": "A few weeks ago, I was in the process of transitioning out from one project to another at work. This provided an awesome time window to read up on some of the long pending topics of interest. Machine learning topped that list. It is a field that has already permeated the technology world deeply but I had no understanding of what it is all about. Just a few weeks of surface level reading since then (and playing around with some of the tools) has left me pretty convinced that we are fast accelerating towards a general artificial intelligence. The advances in the field of deep learning along with the acceleration of humungous data from fast spreading IOT devices will ensure that this future is more near than far. But for purview of this essay, I\u2019ll stick to talking about my hypotheses that everything around us is a deep neural net and the metaphysical implications that arise if we embrace this view point.\n\nAs a machine learning(ML) newbie (rather wannabe), my first exposure to ML was pretty magical. Google\u2019s new ML library called TensorFlow[1] allowed me to train a deep neural net (DNN) model that could very precisely label different types of room images in a house with just a few hours of training on my own dataset of images scraped from google and that too on my normal MacBook Pro. I was overjoyed by this new magical power and could not sleep that night in excitement\u00a0:). After reading a bit more, and once the initial euphoria had subsided, the keyword that stayed on with me was this interesting concept of \u201ctransfer learning\u201d. As google puts it\u200a\u2014\u200a\u201cTransfer learning is a technique where we start with a model that has been already trained on another problem. We will then be retraining only a few layers of the model on a new problem such that the resulting new model exactly works to solve the new problem at hand. Deep learning from scratch can take days, but transfer learning can be done in much short time.\u201d\n\nDeep learning derives its name from the algorithms that it uses for learning called the deep neural networks (DNNs). They are \u2018deep\u2019 because they have a lot of layers of neural nets in them. Each layer of neural nets (or a set of them) creates an output that the next layer(s) can build upon[3]. So in my case above, the google pre-trained image model already had lower layers that identified the simple image components like edges and corners, and the intermediate layers that could figure out shapes. It was only the final layer that I retrained which did the top level job of bringing it all together to give the final interpretation and label to the image.\n\nA few days after the above episode, I encountered Ray Kurzweil\u2019s amazing talk on \u201cThe Accelerating Future\u201d [2] where he talks about the human brains as DNN patterns recognisers and also that there are parts of our brains (the old brain) that are exactly same as that of lizards.\n\nCombined with the concept of \u201ctransfer learning\u201d, this presented DNNs as a great architecture that can explain the transfer of learning from generation to generation of the same species as well as transfer across species. (I agree that this might be kind of obvious given that the architecture of neural nets is inspired from the working of the brain itself. But the key concept to focus here is the deep part of the DNN.)\n\nSo our mind is a DNN(or series of them) which is based on the learning models of the previous species from whom we have evolved. We should also have in our minds the models that got genetically transferred from our own parents and other ancestors. Apart from the basic knowledge of the world and survival recipes, these models probably also contain our sense of right and wrong (morality) fine tuned over millions of years of experimentation.\n\nEven learning, during a single lifetime, seems like a DNN model creation process. During the learning phase our mind is working hard to create the right model (by triggering different types of neural patterns) which consistently outputs the desired results like playing the guitar well. Once the DNN model/pattern is created, it is computationally faster to run the trained model in real life situation or to play the next new song on the guitar. This is the reason that the process of learning itself is difficult but once learnt, a skill is easy to execute.\n\nExperience then could just be an increase in the depth of our various neural nets.\n\nAnd creativity could be result of the concept of transfer learning. Creatives probably work on the ability to keep retraining the top layers of their DNNs while still using the using the base models trained in another domain. On the other hand experts in a particular field have trained their DNNs much deeper in order to solve a particular problem in extreme detail.\n\nI think a important feature of DNNs is that they can act both as a machine as well as storage. So when there is energy flowing through them, they are living machines, directing energy to create dynamic patterns that are optimising to get a desired output. And when energy stops flowing they become these static patterns which we call models (DNN models).\n\nOne interesting concept is to think of seeds (of plants, animals, etc) as DNN models. So seeds are essentially trained models (learnt & stored DNN patterns) passed from generation to generation, wherein every generation tries to improve upon the previous model by giving it more training and optimising to thrive in the environment where it is put into.\n\nTaking this thought a step further\u200a\u2014\u200aif seeds are models then we probably are machines.\n\nSo the plant that arises from the seed is a manifestation of the trained model. This physical manifestation is essentially a learning DNN machine which works on new experiments to learn from its current existence in order to improve the underlying model from which it got created. And it adds new neural layers to it, if it finds some new information that helps its next generation adapt better than what the current model allows for.\n\nDelving deeper on the thought, it is possible that everything around us is actually a deep neural pattern in space and time. In fact the whole universe is probably a giant DNN machine\u00a0:). These DNNs, though theoretically are built on top of each other (layer by layer), physically might reside inside each other as well. That corresponds to the fractal nature of things around us. We are DNNs inside DNNs inside DNNs. The only problem with this theory is that it is unclear as to what is the giant universal DNN optimising for. Because if we can lay our hands on that purpose, we will probably have a fair understanding of the purpose of everything within the universe.\n\nNow if we are DNN machines inside other DNN machines (like the earth), then what could be our purpose. I think our purpose probably is no more than the purpose of the electrical signal in our brain. The purpose is to flow, to flow in a direction which may or may not be part of the winning neural pattern (or model). Our purpose is no more than the purpose of the water particle that is part of the flowing water. We as a unit do not matter. What matters is the emergent behaviour (the neural pattern) out of the collective work of everyone who is at this layer of the cosmic deep neural net. The species, the social constructs, the cultures, the religions, the countries, the languages, the environments like the arid desert and the abundant rainforests are all essentially experimental dynamic patterns trying to together optimise themselves in the service of a emergent mega universal pattern which itself is trying to optimise for \u2018something\u2019. That something is still unclear to me. But our individual existence devoid of the mega machines we are in, is essentially meaningless.\n\nThis is the first draft of this thought framework and it is a long way from being very coherent. But there are a few key elements that are strongly embedded in this thought:\n\nGiven the above few concepts I next intend to apply this framework to some metaphysical concepts like karma, life & death, morality and more to see if there is something interesting to found out there. More soon!\n\n[3]This is What Happens When Deep Learning Neural Networks Hallucinate\u200a\u2014\u200ahttp://thenewstack.io/deep-learning-neural-networks-google-deep-dream/", 
        "title": "Deep neural nets and the purpose of life \u2013 Nitin Pande \u2013"
    }, 
    {
        "url": "https://chatbotsmagazine.com/the-ai-machine-learning-glossary-for-marketers-dbf5f2658c84?source=tag_archive---------1----------------", 
        "text": "As a marketer, it can sometimes seem like the work my engineering colleagues are doing is magic\u200a\u2014\u200apure computer science sorcery.\n\nI mean, they\u2019re building a chatbot that can communicate using human language and learn from the conversations it\u2019s having.\n\nI\u2019m not going to sugarcoat it. AI can be a tough topic to wrap your head around. And with all of the various branches\u200a\u2014\u200amachine learning, deep learning, natural language processing\u200a\u2014\u200ait\u2019s not a topic that you can hope to master by reading a single blog post. Or even a single book.\n\nSo the purpose of this post isn\u2019t to provide you with an exhaustive, engineering-degree-level understanding of AI. Instead, it\u2019s to \u201ctranslate\u201d some of the most commonly used AI terms into everyday language so you can understand them at a basic level.\n\nMoving forward, it\u2019s likely that AI will be playing a more significant role in the work marketers do. Becoming fluent in AI-speak now can help prepare us for the road ahead.\n\nOr if you\u2019re talking about AI as a discipline, it\u2019s figuring out how to make computers do things that humans need intelligence to do.\n\nUsing logic, forming hypotheses, solving problems\u200a\u2014\u200athose are a few examples of activities that we typically think of as requiring a human-level of intelligence. When a computer or computer program is able to do those types of things, it\u2019s considered artificially intelligent.\n\nOr at least, some people would consider it artificially intelligent.\n\nAs computer science professor Toshinori Munakata wrote in Fundamentals of the New Artificial Intelligence, \u201cThere is no standard definition of exactly what artificial intelligence is. If you ask five computing professionals to define \u201cAI\u201d, you are likely to get five different answers.\u201d\n\nNot really helping my cause here, professor.\n\nHistorically, the Turing test has been the gold standard for determining whether or not a computer is truly intelligent.\n\nFirst described by computing pioneer Alan Turing in a 1950 paper, the Turing test invites a participant to exchange messages, in real-time, with an unseen party. In some cases that unseen party is another human, in other cases it\u2019s a computer. If the participant is unable to distinguish the computer from the human, the computer is said to have passed the Turing test and can be considered intelligent.\n\nSo that settles it, right? When a computer\u2019s behavior becomes indistinguishable from the behavior of a human, we can say that AI has been achieved.\n\nA separate camp of AI researchers argues that framing AI as a quest to understand and imitate human intelligence is the wrong approach.\n\nAfter all, they argue, humans didn\u2019t achieve \u201cartificial flight\u201d through building machines that flap their wings like birds, bats, or bugs. Instead of imitating nature, humans relied on other engineering principles in order to create the planes we fly around in today.\n\nOperating with this approach, the goal of AI isn\u2019t to build computers that can behave like humans, but to build highly flexible, rational computers that can perceive their environments and take actions that maximize their chances of success toward some goal.\n\nThis, as it turns out, is in-line with how Amazon\u2019s AI-powered program Alexa \u201cthinks\u201d about AI.\n\nWhen I asked Alexa to define AI, she replied that it\u2019s \u201cthe branch of computer science that deals with writing computer programs that can solve problems creatively.\u201d\n\nOn a related note, when I asked Alexa if she could pass the Turing test, she replied, \u201cI don\u2019t need to pass that\u200a\u2014\u200aI\u2019m not pretending to be human.\u201d", 
        "title": "The AI & Machine Learning Glossary for Marketers \u2013"
    }, 
    {
        "url": "https://medium.com/plos-comp-biol-field-reports-2016/computational-biology-data-ismb16-1607154654ea?source=tag_archive---------2----------------", 
        "text": "There are still some flaws with regards to big data\u200a\u2014\u200abut it still represents big hope for computational biology.\n\nAccording to Uri Laserson from Cloudera (a leader platform for data management and analysis), \u201cGenomic is not special\u201d. He expressed the view that:\n\nEven though the statement is bold, there is some truth in it. It looks like industrial solutions for big data somehow outperform research solutions. This is why more collaboration between research and industry is needed to fill the gap. On the other hand, this collaboration cannot happen at the price of data ownership. Academy needs to find a solution to attract world-scale talents that right now goes where big data is linked tightly to big money.\n\nAnother recurrent problem for many computational biologists and bioinformaticians is their place in biological research. Despite an improving amount of recognition of computational work in wet labs, there is still a number of places that lack of expertise in data analysis. Old analysis methods are reproduced and consequently spread used by others in the field\u200a\u2014\u200aall because a number of researchers are using them.\n\nProgramming or statistical input is not acknowledged enough and this is where there can some friction between experimentalists and theoreticians that can hinder progress. Strolling through alleys of the poster hall at ISMB 2016, overheard many accounts of interactions between wet and dry lab. Most of them recalled tension, disappointment or misunderstanding. This is why education is really crucial. No one cannot be an expert in all domains, but a comprehensible overview of each other worlds, common vocabulary and goals are essential to building a common future.", 
        "title": "Computational Biology & Data - #ISMB16 \u2013 PLOS Comp Biol Field Reports Blog \u2013"
    }, 
    {
        "url": "https://medium.com/sergey-enin/artificial-intelligence-speaks-the-deepest-voice-with-your-customers-f3e59e257150?source=tag_archive---------3----------------", 
        "text": "Recently Google DeepMind have released paper, which leads to consider such possibility seriously. Paper states that DeepMind created neural network called WaveNet, which is a generative model for raw audio:\n\nFor those of you who prefer to go straight to the examples\u200a\u2014\u200athere is a separate post in DeepMind blog available.\n\nThe paper derives from the earliest work by part of the authors and continue investigating neural autoregressive generative models that model complex distributions such as text, images and eventually raw sound, which is challenging since sound is a way more complex structure entity\u00a0. The joint probability of a waveform X = {X1,\u2026,Xt} is factorised as a product of conditional probabilities of each audio samples\n\neach audio sample Xt is therefore conditioned on the samples at all previous timesteps.\n\nFrom the Neural Net architecture point of view, the net is represented by a stack of causal convolutional layers:\n\nDuring the training, the network is being fed by real recordings of the human speaking. After the training the neural network produces sample by sample realistic-sounding audio fragments. Of course, the network should be fed with the text which should be transformed to speaking as well.\n\nIt is a huge deal for the voice communication.\n\nMost likely, it makes a way more tricky speaker identification, putting requirements on speaker verification systems which should not trust voice biometric methods exclusively.\n\nFrom other hand, it a great news for the automated customers service provided via voice, giving opportunity to make it more smoothly and pleasant for the customers. The voice of favourite client\u2019s singer asking: \u201cWould you like try our new product as well?\u201d. The sweetest dream of any sales/customer care department. Isn\u2019t it?", 
        "title": "Artificial Intelligence speaks The Deepest Voice with your customers"
    }
]