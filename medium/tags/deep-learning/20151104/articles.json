[
    {
        "url": "https://gab41.lab41.org/anything2vec-e99ec0dc186?source=tag_archive---------0----------------", 
        "text": "This might be old news to you, but if you\u2019re considering the use of word embeddings, our suggestion: just take the plunge. We\u2019ve read a \u201cfew\u201d studies documenting their effectiveness, not the least of which is our personal favorite:\n\nBaroni, Dinu, & Kruszewski are a bit dramatic, but they get the point across. You may not be able to explain why (though we\u2019ll try, here), but embeddings sure done work good. And as the most popular word embedding algorithm, not many techniques have caught fire as much as Tomas Mikolov\u2019s word2vec algorithm.\n\nIn 2013, word2vec made waves in the neural network world. The code was readable and really fast (multi-threaded C code), helping it achieve wide adoption. Since then, Google archived the code (though it\u2019s still available), and Mikolov changed companies. Still, that didn\u2019t stop the momentum, and there\u2019s been a glut of implementations on platforms and software packages like Python, Scala (and ML-Lib in Spark), DL4J, gensim, and maybe a zillion more, to make it fit into the everyday man\u2019s NLP toolbox.\n\nIn fact, word2vec has been applied to not-so-obvious applications: twitter network analysis, network traffic analytics, named entity association, and the list goes on. But the 2Vec craze doesn\u2019t end with word embeddings. If you\u2019ve been to CVPR 2015 or participated in Baylearn 2015, you\u2019d know that Yann LeCun is bent on embedding the world, i.e. \u201cWorld2Vec\u201d (make sure Google doesn\u2019t autocorrect you when you search for it.) The World2Vec pitch in the linked video starts at 50:24, but the LeCunian entertainment is present throughout the video.\n\nLeCun\u2019s brand of enthusiasm might make you doubt the extravagant mantra (that vector space can, but that\u2019s not just crazy talk, since Hinton\u2019s group has similar ideas. And yes, we take it literally when we say ideas: the group (Ryan Kiros, primarily) has, in fact, implemented a type of \u201cidea-2-vec\u201d in recent Q&A work, which is open on Git. They call it Skip-Thought Vectors, which is synonymously known as sent2vec. Our personal favorite from \u201cawesome deep vision\u201d is where Baidu\u2019s chatbot claims to be able to fool you into thinking you\u2019re talking to a human.\n\nAs an aside, we\u2019re \u201cmeh\u201d on this. Claims regarding chatbots being Turing Complete have always been suspect. That\u2019s a whole other blog post (which we\u2019ll write later, time permitting, but Google for NPR\u2019s Fresh Air\u2019s take on it.) It really depends on the situation. For example, a statue could pass the Turing test\u2026if a dog administered it:\n\nSorry, we\u2019re getting carried away. Anyway, so why this blog post? Why not read the papers? After talking with several colleagues and friends spanning multiple countries, as it turns out, not a lot of people know why it works and why it\u2019s called a neural network. They had a good grasp of what was happening, but formalizing it in deep learning frameworks was difficult and the papers weren\u2019t helping.\n\nReally, the concepts as explained are surprisingly simple. But, if you\u2019re like us, who jumped on the DL-train to Bandwagon-Town, we like writing Keras/Theano/Torch/Caffe layers to fit into existing and new architectures. And although the DL-train is full-steam ahead and doesn\u2019t often stop for explanations, we feel like this one deserves a deeper look.\n\nThere are a lot of DL packages out there, and nearly all of them have a good suite of objective/cost functions. For the uninitiated, objective/cost functions are those equations you\u2019re trying to minimize, and they usually follow some logical principle like, make the error between what we predict and the truth as small as possible. Look at the Keras toolbox\u2019s \u201cAvailable Objectives\u201d for the most commonly used ones, you\u2019ll see:\n\nIf you\u2019ve been involved in the ImageNet competition, you probably love categorical cross-entropy so much you want to marry it. In this blog post, we\u2019re going to step away from that function. Why? Because chances are, you\u2019re using a softmax layer before optimizing, normalizing the output to a posterior probability. That\u2019s nice, but only where you know your input relates to exactly one of the classes in the set of candidates. In the case of text prediction and vector space construction (the key idea of word2vec), where a predicted value should relate to multiple labels, we prefer binary crossentropy (and heck, even, MSE). That optimization (according to Hinton) is written as:\n\nHere, the y\u2019s are the true outputs and h(x) is the output of the neural network that you\u2019re building. We\u2019ll try not to get too math-y, and at this point in time, you\u2019re probably asking, what\u2019s our point? Well, not a lot of people know that word2vec is precisely a 2-layer conventional neural network with a slightly modified binary cross-entropy optimization. If you can visualize that, there\u2019s no need to read further. Otherwise, we\u2019re going to show you that you can make word2vec out of all the traditional objective function parts and layers from your toolboxes with some slight modifications.\n\nChances are, if you\u2019ve read anything about word2vec, you\u2019ve seen something that looks like this:\n\nThis is great for explaining out word2vec works (look up anything written by Chris Manning). Unfortunately, these diagrams, IMHO, do us a disservice because they obscure how they relate to conventional neural networks. Also unfortunate is that the people call this \u201cdeep learning\u201d, because doing so, again IMHO, makes the term lose some of its original meaning. I\u2019d call vector spaces learning wide learning rather than deep learning.\n\nMikolov\u2019s Word2Vec runs on something called negative sampling, a generalization of Noise Contrastive Estimation (NCE). It\u2019s a math-y paper, but worth the read if you\u2019ve got the time and patience. One thing that\u2019s glossed over is that NCE can approximate the minimization of the log probability of the softmax. As it turns out, instead, negative sampling approximates the binary cross-entropy cost function (the log loss for some of you).\n\nNegative sampling can be expressed by the below equation (taken directly from Mikolov\u2019s paper):\n\nHere, vwO is the output word vector (relating to wO), vwI is the input vector (relating to wI), and we are drawing word wn from the negative distribution of words: Pn(w). We\u2019re summing over only a select number of negative examples (i.e., n goes from 1 to a small number\u2026let\u2019s say 10). To build some intuition, it might be straightforward to see how maximizing this function produces the vectors that you want (in both input and output spaces).\n\nEasy, right? No? It\u2019s a bunch of fancy letters just to say that you\u2019re pushing words close to their context and pulling them away from the background distribution. Assuming that we are already taking samples from the background distribution (i.e., getting rid of the expectation notation), we can just sum the correlation of a randomly sampled vector with the input vector. If we say that y is an indicator as to whether or not a word is in the context of another word (1 = in context, 0 = not in context), then the value for y (let\u2019s call it yOI) relating to the output (denoted by subscript O) and input vector (denoted by subscript I) is equal to one. Likewise, the value for ynI relating to a negative sample (denoted by subscript n) and the input vector (again denoted by I) is zero. Then, after some derivations, the cost function looks suspiciously familiar:\n\nPay attention to that last line; not only did we introduce the binary y vector, but we\u2019ve also introduced this si value, which stands for sample i. It\u2019s one or zero depending on if you\u2019re actually using this sample. That is, if your negative sample includes the label category i.\n\nSo look at this last line and look at the binary cross-entropy function equation in the previous section. The only difference between the word2vec cost function and binary cross-entropy is that you\u2019re doing negative sampling: the si, which effectively chooses a subset of the samples when y=0. That\u2019s what that si term is in there for. It\u2019s really just saying, I don\u2019t want to take every word vector that\u2019s not related to my input word vector into account\u2026that would be too expensive. No, let\u2019s just take the one that\u2019s the most relevant.\n\nThat\u2019s the cost function, now the architecture. Let\u2019s say that we have a vocabulary, and instead of drawing the vectors individually, let\u2019s draw them all together. We can collect a whole bunch of vectors of words and assemble them into a matrix. Let\u2019s call this matrix VI.\n\nOn the flip side, you remember the \u201cone-hot\u201d encoding vector idea? If the ith word in the dictionary is being used, then the ith element of a one hot vector xI is nonzero:\n\nLet\u2019s say we\u2019re looking at the ith word. Then, that means we want to optimize for the ith vector. To extract that vector from the matrix Vi from in a linear algebraic sense, it\u2019s simply a matrix multiplication: vwI=VI xI. If you need a picture, take a gander at this:\n\nRecall the objective function, where we\u2019re multiplying the vector input with multiple output vectors. Let\u2019s say, for example, I want to multiply two output vectors with the input vectors. So similarly, let\u2019s select the output vectors, and multiply. Visually, this looks like:\n\nwhich is the same as:\n\nLet\u2019s turn the diagram 90 degrees, and put a cherry on the top by adding a sigmoid nonlinearity function.\n\nSo, if I were taking the sum of all the outputs, this would just look like:\n\nThis should be more what you\u2019re used to, though if you implement it like this with traditional tools, it\u2019s probably really inefficient since you\u2019re using matrix-vector multiplications rather than Mikolov\u2019s vector-vector multiplications. Depending on how many vectors you use for negative sampling, you really only need to update a few vectors every iteration. So, obviously, you won\u2019t want to use the BLAS library on the entire matrix vector multiplication, but if you plug in our equation from the previous section in, you\u2019ll see that this is exactly your run of the mill neural network implementation, just tons more efficient.\n\nAs a whole, current implementations (especially the multi-threaded) are cool because you\u2019re sampling on the foreground and background distribution, and so in the limit, you\u2019ll converge to a solution that places vectors based on their co-occurrence in text. There\u2019s also a random component to it that could potentially get you out of local minima, given enough iterations. We\u2019re real fans.\n\nThere you have it. The word2vec architecture is a neural network with two parameter matrices followed by a sigmoid activation function. It\u2019s optimized by a binary cross-entropy cost function, using random negative sampling. If you want some code, feel free to get it at our Lab41 Github page.", 
        "title": "Anything2Vec \u2013"
    }
]