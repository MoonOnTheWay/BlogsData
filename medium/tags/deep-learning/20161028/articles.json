[
    {
        "url": "https://medium.com/@ilblackdragon/tensorflow-tutorial-part-4-958c29c717a0?source=tag_archive---------0----------------", 
        "text": "I originally planned to go over some examples in TF.Learn for natural language problems, but somehow work on TF.Learn itself got me busy.\n\nIn previous Part 3 of this tutorial we have reviewed how to add categorical variables to your model.\n\nBut the other day, @philbort filed a bug https://github.com/ilblackdragon/tf_examples/issues/7 and asked how to combine continues and categorical variables in one model.\n\nAfter trying to do it quickly in response, I realized it\u2019s quiet hard. Now, we are going to fix some of that in upcoming changes to TensorFlow, but in a meanwhile I want a way to put together various pieces of TF.Learn to achieve the goal and also talk about some new concepts.\n\nLet\u2019s start with input function\u200a\u2014\u200afunction you can pass to your fit / predict as alternative to x and y data arrays. Idea here is that you want build piece of the graph that would read and sample your data instead of keeping it always in memory. For example if you have a csv file, you can write an input function like this:\n\nWe use read_batch_examples to setup a reader (TextLineReader) that would read lines from my.csv and batch them into a string tensor of [32]. Then we call decode_csv, which parses each string in tensor into list of columns. We define number and dtypes of this tensors by providing record_defaults. Finally we return features (string to tensor) and target tensor.\n\nAdditional things to know about input functions\u200a\u2014\u200ais that depending on flags to read_batch_examples this may return data infinitely (as long as we are asking for it) or for specific number of epochs. And it randomizes order by default, to disable it pass randomize_input=False.\n\nNow as you see features would contain both continues features like Fare and Age as well as string features like Sex and Pclass. There are different ways to handle this, for example using tf.contrib.lookup.HashTable to build a lookup table right in the graph.\n\nHere I want to talk about alternative path\u200a\u2014\u200adoing everything with Pandas and then passing already preprocessed data into the model. This is less scalable (e.g. won\u2019t work in distributed environment very well), but works for local training.\n\nCurrently (2016/10/27) there is a limitation if x is DataFrame, what will model receive (due to legacy reasons, it translates it into a matrix).\n\nTo work around it, we will write an input function that would feed preprocessed DataFrame in correct format. Then we will write a model that can use already mapped categorical variables into indices together with continues variables.\n\nHere we write pandas_input_fn that uses learn.dataframe.queues.feeding_functions.enqueue_data to feed DataFrame into the model (e.g. adds nodes in the graph that in parallel are fed with data) in separate thread. This also should work faster then passing x, y into fit because it doesn\u2019t lock training loop to fetch new records.\n\nNow in our model function, we use process features: continues ones are mapped to float and reshaped into [batch_size, 1]. Categorical features are all embedded using different embedding matrices (see Part 2 for more details about embeddings). Then all this features are concatenated into one feature vector and passed into deep 3-layers neural network. The later part is the same as in previous models.\n\nThe final results are better the either just categorical or just continues variables, getting after a bit of training:\n\nAs always, you can find all code on github: https://github.com/ilblackdragon/tf_examples. Feel free to create an issue or file a pull request!\n\nHopefully, this time I\u2019ll be able to write a bit more about text classification. Stay tuned!", 
        "title": "TensorFlow: Combining Categorical and Continuous Variables"
    }, 
    {
        "url": "https://blog.deepomatic.com/ai-for-everyone-d687522934af?source=tag_archive---------1----------------", 
        "text": "Recently, Augustin Marty, CEO at Deepomatic, gave a talk at Hello Tomorrow about tackling the data challenges in deep learning and AI.\u00a0\n\nYou can watch the entire talk here and find the main takeaways below.\n\nIn the past few years we have been witnessing incredible progress in the field of computer vision, mainly due to deep learning. The rise of deep learning, which had a profound impact on solving image-related problems, was partly made possible because the tech giants such as Microsoft and Google, started massively investing in it, developing better algorithms, designing bigger models with more layers and millions of parameters.\u00a0\n\n\u00a0\n\nThe advances led to the creation of new interesting applications in the field of image recognition, such as Google\u2019s Show and Tell image captioning model, or style transfer and video colourisation applications.\n\nHowever these general applications do not always work for your company and industry-specific needs. If you send an image of an automotive part to Microsoft\u2019s image captioning model, it may mistake it for something completely different. It\u2019s not one-size fits all, which is why every company will have it\u2019s own way to tackle its challenges.\n\nAI is such a trending topic, but many companies do not think it may concern them. Well, as stressed in the talk: \u201cAI is for everyone\u201d.\n\nA general image-recognition model isn\u2019t trained to identify and process the specific needs of a given company, but that doesn\u2019t mean there isn\u2019t a solution. AI can move any company\u2019s business forward: used for image or text analysis for instance. This requires lots of data.\n\nThere is a way to tackle image recognition challenges specific to your industry. For this you need 3 major elements:\n\n1 a framework\u00a0\n\n(many are open-sourced and easily accessible for an engineer)\n\n2 annotated images\u00a0\n\n(they must be relevant to the task you wish to accomplish)\n\nThis may all seem relatively simple but there\u2019s a catch: you need to have a very good dataset for it to work well. Data is the true bottleneck of deep learning and artificial intelligence. It is what stalls progress and advances since annotating datasets is very time consuming: to train models it is necessary to have huge amounts of high quality, annotated data.\n\nDeepomatic is working on industrialising and optimising dataset creation.\u00a0\n\nWe have developed a software\u200a\u2014\u200aa sort of photoshop for annotation\u200a\u2014\u200awhich involves optimised UX but also AI to improve the productivity of human annotators, rendering the entire process time and cost efficient.\n\nFor this we have been working on:\n\nThese elements will help reduce the time of annotating and labelling datasets, allowing companies to access a dataset for training much quicker than before.", 
        "title": "AI For Everyone \u2013"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/deep-learning-tool-lets-you-pick-your-pastiche-mostly-monet-a-dab-of-dor%C3%A9-and-a-pinch-of-picasso-5041ce6f7bc?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep Learning Tool Lets You Pick Your Pastiche: Mostly Monet, a Dab of Dor\u00e9 and a Pinch of Picasso"
    }
]