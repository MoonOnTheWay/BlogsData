[
    {
        "url": "https://blog.chappiebot.com/l%C3%ADnh-m%E1%BB%9Bi-deep-learning-62e6abf81739?source=tag_archive---------0----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "L\u00ednh m\u1edbi & Deep Learning \u2013"
    }, 
    {
        "url": "https://gab41.lab41.org/effectively-running-thousands-of-experiments-hyperopt-with-sacred-dfa53b50f1ec?source=tag_archive---------1----------------", 
        "text": "We are currently nearing the end of our project cycles, which means we are running a LOT of tests. On the Pythia project there are a lot of different hyperparameters (experimental parameters) that we can choose between. We started investigating the different types of hyperparameter search techniques that are available in order to hopefully find the best solution without running tests for months. We found a few different options:\n\nHyperopt uses a method called the Tree-structured Parzen Estimator (TPE); an approach introduced at NIPS 2011 by Bergstra et. al. One of the main advances of TPE over other probabilistic methods is that most other methods cannot retain the dependencies between parameters. TPE is able to retain these relationships as it models the density of a parameter for \u2018good\u2019 experiments and compares this to its density for \u2018bad\u2019 experiments. It is then able to use these models to determine expected improvement of the objective function for any values a parameter can take.\n\nTPE is useful for the Pythia project as we have some parameters describing which algorithm to run (e.g., logistic regression versus XGBoost) as well as parameters pertaining to that algorithm (e.g., the penalty and tolerance of the logistic regression model). I\u2019ll show you what this looks like a little later in this post.\n\nI\u2019m a very visual person, so I wanted to see how hyperopt was working. As this is nearly impossible within the context of Pythia due to the number of parameters, I decided to analyze hyperopt with a different 2D optimization problem\u200a\u2014\u200anamely, the Rosenbrock function. This quadratic function is complex enough to be difficult as it has a long and narrow near-optimal region, but is still easy enough to understand. The minimum value of this optimization is zero which is located at (1,1). The value of the optimization is displayed below in the form of a heat map. Lower values are shown in red while higher values in blue.\n\nI assessed the performance of 100 random search tests, and compared these to the performance of 100 hyperopt tests. For this test there are two parameters\u200a\u2014\u200athe value of x and the value of y. I let these vary between -0.5 and 2.5. The results of each test were analyzed and plotted on top of the heat map of the Rosenbrock function. Generally, hyperopt got close to the optimal point, but random search also did fairly well.\n\nMore of the hyperopt tests were closer to the optimal point compared to random search, and found a better overall solution. This suggested to me that hyperopt is indeed reducing the parameter search space in an intelligent manner. Secondly, random is less likely to stumble upon the best solution in a higher dimensional problem (curse you curse of dimensionality!). Finally, Bergstra et. al. finds TPE is faster to find some near-optimal input than random.\n\nA very good introduction to hyperopt may be found at fastml.com, but in short, to run hyperopt you define the objective function, the parameter space, the number of experiments to run and optionally set a constructor to keep the experiments. There are both continuous and categorical methods to describe the parameters, and as I\u2019ve mentioned, these parameters can be defined hierarchically. The full hyperopt code for optimizing the Rosenbrock function above is pretty simple.\n\nAnd now we enter the \u2018Sacred Trees of Parzen\u2019\n\nHyperopt looks like it could work well for our use case, but colleague Dave has told me to show my work\u200a\u2014\u200aso how do I go about writing a program that uses hyperopt and Sacred? As Dave points out, Sacred is an open source Python package to \u201chelp you configure, organize, log and reproduce experiments.\u201d When using Sacred with hyperopt, you set up Sacred in a very similar manner, specifying a configuration and an observer.\n\nOne of the largest modifications to Sacred that you should likely make is a patch to avoid having to declare all parameters (huge hat-tip to my colleague Yonas Tesfaye for discovering this patch). This patch is very beneficial if you do not want hyperopt to consider some parameters which is often the case in experiment testing. For example if we run Pythia with one algorithm, the other algorithm parameters are not specified.\n\nThe other major trick to be able to still log everything in Sacred correctly is to use global variables. The reason for this is that Sacred\u2019s main function Experiment.main() has one argument, a function object that it calls with no dependent variables. In addition, the function returns more results than the single objective hyperopt can handle. Global variables solve both of these issues. I\u2019ll show a simplified example below, but if you are really curious, check out the function we are using for Pythia on Github.\n\nFirst, let us set up the callable object that Sacred can call. Sacred will log anything that is returned when the main function Experiment.main() is called. In the example below this is all_results, which is a dictionary created by running one complete experiment in Pythia. However, before we return all of our results to be logged, we want to keep track of a single global variable for hyperopt to use\u200a\u2014\u200ain this case the aptly named result. For Pythia we are going to use the average F1 score, and because hyperopt will try to find the minimum value, we will provide it with the negative F1 score.\n\nNow we can set up the objective() function that hyperopt will use. Again, we are going to be using the global values so that our function run_pythia_with_global_args() can see the experiment parameters and set the variable result. Mostly in our objective() function we are setting up and running a Sacred Experiment. You have to set up Experiment.main() with the function to call, and then update the variables with config_updates=pythia_args.\n\nThe last part of the function is pure hyperopt. If you remember, hyperopt needs an objective (which we have) as well as a parameter space and the number of experiments to complete. Now we can truly take advantage of hyperopt\u2019s ability to retain hierarchical parameters. This is shown with the variable algorithm_type. When the algorithm XGBoost is chosen, hyperopt determines how to set the variables XGB_LEARNRATE and XGB_MAXDEPTH, but does not try to set LOG_C or LOG_TOL as these parameters are associated with the logistic regression algorithm. This ability seems so simple, but is really important and beneficial! The last little bit of code is the actual call to run hyperopt where all of the pieces fit together.\n\nYes!\u2026 but it needed a little help to find the highest value. One of the main lessons I have learned by using hyperopt is that you still need a large number of runs for hyperopt to be able to learn which parameters are best. Humans, on the other hand, can often see performance trends given fewer data points. It is definitely important to acknowledge biases we might have when hand-tuning, but hand-tuning often works quite well.\n\nTo demonstrate just how many samples you need to find the best result let\u2019s go back to our 2D example with the Rosenbrock function. If you let each optimizer try out 50 different test points, TPE will find a better (e.g., lower) point than random search around 55% of the time. If you try 500 samples though, TPE will find a lower point than random search 85% of the time. So even in our simpler 2D example, we need a lot of tests. Thus we know in Pythia we are going to need to run a really large number of experiments!\n\nHowever, in order to run any number of experiments, we have to limit the types of featurization techniques as some simply take too long to run. To combat this problem, we have been using a hand-tuned hyperopt strategy (as an aside there are some other cool techniques that do take run time into account). Similar to hand-tuned random we choose which parameters to let hyperopt toggle and which to set to a particular value (in this case, off). Luckily, these methods are generally not performing as well, so I don\u2019t get too much heartburn by leaving them out. With hand-tuning, hyperopt has completed some of the best performing experiments in less than a day, but without hand-tuning, it can take three to four days for comparable results.\n\nWe would love to know if you are using Hyperopt or Sacred in your work, or both of them together! How has your performance with hyperopt been?", 
        "title": "Effectively running thousands of experiments: Hyperopt with Sacred"
    }, 
    {
        "url": "https://medium.com/the-downlinq/the-effect-of-resolution-on-deep-neural-network-image-classification-accuracy-d1338e2782c5?source=tag_archive---------2----------------", 
        "text": "In this post, we further explore the boat-heading classification problem that we examined in previous posts (1, 2, 3). Specifically, we explore the impact of both spatial resolution and training dataset size on the classification performance of deep neural networks (DNNs). Results are similar to those achieved with a HOG-based classifier, and we provide a full comparison later in the post*.\n\nTraining and validation datasets are comprised of DigitalGlobe imagery cutouts of both boats and background regions, described in 1. High resolution image cutouts are augmented by re-projecting to a lower resolution (see Section 1 of 3). In brief: for classifier training we utilize labeled cutouts from two DigitalGlobe images at native resolution (0.34m and 0.5m) and down-sampled imagery at 0.5m and 1.0 m ground sample distance (GSD). For evaluation, we use DigitalGlobe imagery from a third validation image re-projected to [0.5, 1, 2, 3, 4, 5, 10] meter GSD, as well as Planet data assumed to be at native 3m GSD.\n\nUsing Caffe and Digits, we trained a 73 class (72 rotations, plus null) DNN classifier based on the AlexNet architecture. CosmiQ initialized weights and biases from the Caffe implementation of AlexNet that was trained using ImageNet data.\n\nTraining CosmiQ\u2019s DNN required six hours on four high-end consumer NVIDIA GPUs (Titan X\u2019s). The lengthy training (and evaluation) time of our model inhibits the use of bootstrap resampling to estimate confidence intervals that was employed in 2, 3. Since we are counting discrete events in our test dataset, we assume a Poisson distribution and therefore estimate the fractional error as N^(-1/2), where N = the number of boats. Our validation chipset contains 516 DigitalGlobe cutouts and 278 Planet images, with the distribution of boat lengths shown below in Figure 1.\n\nUsing our re-projected dataset we can study the effects of resolution on classification accuracy. Figures 2 and 3 below detail the performance of the classifier as resolution degrades.\n\nIn Figure 3, we present the results for the more difficult classification problem of boat heading. To properly capture the impact of object size on the results, we break out performance of the classifier by vessel size bin. Scoring was calculated consistent with the methodology described in Section 1 of 3.\n\nThere are a few results worth noting in Figure 3. Not surprisingly, headings of larger vessels are better classified than those of smaller vessels as the GSD increases. For the DNN classifier the Planet data yield results somewhat worse than those from the corresponding blurred DigitalGlobe imagery, and worse than those from the HOG predictions. While we cannot be certain why this is the case, this may be an example of overtraining of the DNN (recall that no Planet data was used for training). The HOG-based classifier relies on relatively simple gradient-based features for classification that may translate well between different datasets, whereas the far greater number of parameters of the DNN may be overtraining on features specific to DigitalGlobe data.\n\nWe combine the results of Figure 3 above with results from 3 (Figure 5) below. Recall that the HOG+LogReg classifier utilizes bootstrap resampling in estimating error bars, as opposed to the simple N^(-1/2) scaling used for the DNN.", 
        "title": "The Effect of Resolution on Deep Neural Network Image Classification Accuracy"
    }, 
    {
        "url": "https://medium.com/@Francesco_AI/a-new-ai-framework-the-37-78-paradigm-d393527b416b?source=tag_archive---------3----------------", 
        "text": "I truly believe AI is life-changing, and I want to highlight three final characteristics that AI-as-a-technology is introducing. First of all, AI is disrupting the traditional IoT business model because is bringing analytics to final customers instead of centralizing it.\n\nSecond, it is forcing business to keep customers in the loop\u200a\u2014\u200aand the reasons why are manifold:\n\ni) establishing trust into the product and the company;\n\nThe shifting focus on the final user as part of the product development is quickly becoming essential, to such a point that it represents a new business paradigm, i.e., the \u201cParadigm 37\u201378\u201d. I named this new pattern after the events of March 2016, in which AlphaGo defeated Lee Sedol in the Go game. In the move 37, AlphaGo surprised Lee Sedol with a move that no human would have ever tried or seen coming, and thus it won the second game. Lee Sedol rethought about that game, getting used to that kind of move and building the habit of thinking with a new perspective. He started realizing (and trusting) that the move made by the machine was indeed superb, and in game four he surprised in turn AlphaGo at Move 78 with something that the machine would not expect any human to do.\n\nThe Paradigm 37\u201378 is indeed a way to acknowledge that the users are the real value driver for building an effective AI engine: we make the machine better, and they make us better off in turn.\n\nThe last feature AI is changing is the way we think about data. First, AI is pushing business to question whether the information is always good and if the benefits linearly increase with a higher volume. This aspect is really important because AI is trained on data that have to be high quality to be effective (and this is why Twitter turned Microsoft\u2019s bot into a Hitler-loving sex robot).\n\nIt is also forcing us to reflect on storing data that matter (rather than storing just for the sake of doing it), and to use correctly data exhaust, i.e., those data generated as a by-product of online actions\u200a\u2014\u200ain other words, they are not business core data, and they are by definition multiplicative with respect to the initial information (and thus much bigger).\n\nFinally, AI necessities are clearly underlining the cost-benefit trade-off of the inversely related relationship between accuracy and implementation time (either time to train the model, or time to produce the results and provide answers). The discussion on this specific topic is highly dependent on the sector and problem tackled: there are cases in which it is better the dollar cost of learning is largely overcome from higher accuracy, while others in which faster and responsive answers are way better than an incredibly accurate one.\n\nData is by far the perfect good: it does not deteriorate over time and can be reused; it is multipurpose; it multiplies by using or sharing. It is clearly up to date one of the greatest sources of competitive advantage for any machine learning firm, which represents also a problem: data polarization might result into few companies that channel and attract most of the data traffic, and other ones being (almost) completely excluded. In few years, this exponential trend might generate an enormous barrier to entry for the sector, compelling companies to create strategic partnerships with incumbents.\n\nFortunately, there are already stealth-mode companies working on reducing the dependency of AI on extremely large datasets (such as Vicarious or Geometric Intelligence for example): machines should indeed be able to learn from just a few instances as humans do. It is also not a coincidence that they are led by academics, because if the solution for the business is feeding the model with more data (narrowing down the bottleneck), for academics is instead focusing on transforming the algorithms for the better, and laying the foundation for the next evolutionary step.", 
        "title": "The 37\u201378 Paradigm \u2013 Francesco Corea \u2013"
    }, 
    {
        "url": "https://planetachatbot.com/neurociencias-ia-arte-fb19357472ef?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Neurociencias, IA, arte \u2013"
    }, 
    {
        "url": "https://gungorbasa.com/deep-learning-on-ios-part1-57a2a3849124?source=tag_archive---------5----------------", 
        "text": "Deep Learning (DL) is the new era of Machine Learning. With Deep Learning (CNN, RNN, LSTM etc.) scientists had great success on object detection, natural language processing, and many other areas last couple years. Apple, Google, Microsoft and many big companies improved their products with this technology.\n\nThere are couple different libraries for DL for iOS platform. This blog post\u2019s topic is one of the most famous ones, Torch. Let\u2019s get hands dirty now.\n\nTo be able to continue, you should have Brew package manager If you haven\u2019t installed it to your computer, install it with below command.\n\nNow, we have Brew in our computer. It is time to install Torch on our Macs. Open your terminal and type below commands. These commands help you to install Torch automatically (painless way\u00a0:).\n\nDuring the install phase installer may ask you;\n\nIf you are using zshrc shell, type yes, if you are not using like me, type no and enter. To be able to complete the install, we should add the library to to path. I prefer bash for my terminal, so I do below step. If you are using, something different than zshrc or bash, please find appropriate commands to add Torch install location to Path. For bash terminal open ~/.bash_profile,\n\nand add below line to end of the file\n\nNow based on your terminal type, source our terminal to refresh environment variables.\n\nIf we do everything correctly, Torch should be installed to your OSx. To test, type \u201cth\u201d command into your terminal.\n\nIf you see above prompt, congratulations you have Torch on your computer. Now, it is time to learn how to use it as iOS library. (To exit type \u201cos.exit()\u201d)\n\nAbove commands will build all Torch libraries as static library for iOS and put it into \u201c~/torch_ios/framework\u201d directory. Now we are ready to use Torch in iOS environment.", 
        "title": "Deep Learning on iOS \u2014 Part1 \u2013"
    }, 
    {
        "url": "https://medium.com/@itsquiz15/how-to-avoid-illusions-of-learning-72add2abc3ec?source=tag_archive---------6----------------", 
        "text": "Are you ready for this secret to become an expert in your field?\n\nResearches found that many learners have inflated perception of their knowledge. Incompetent people don\u2019t know each detail of the industry and the list of necessary skills that are required. That\u2019s why they may think that they masters. Scientists proved that some persons intuitively use it to improve their social status.\n\nEliminate this illusion through communicating with advanced professionals, assessments, and other techniques. It is important to know your level and fulfill knowledge gaps for the self-realization and better job and life satisfaction.\n\nYou may recall the main ideas after the reading to see how you understand the concept. Retell the key concept or you can use online quiz creator https://app.itsquiz.com to estimate your knowledge. The education platform has many tests or you can create your own quizzes to notice your weaknesses and progress.\n\nNot to read passively\u200a\u2014\u200atake notes and try to use the knowledge if you can implement the skills. For example, after the learning programming language\u200a\u2014\u200astart coding and you will have many questions that will lead you in the right direction. You need the practice to become an expert and remember better the material.\n\n\u201cOn the basis of the available evidence, we rate highlighting and underlining as having low utility. In most situations that have been examined and with most participants, highlighting does little to boost performance. It may help when students have the knowledge needed to highlight more effectively, or when texts are difficult, but it may actually hurt performance on higher-level tasks that require inference making. Future research should be aimed at teaching students how to highlight effectively, given that students are likely to continue to use this popular technique despite its relative ineffectiveness,\u201d Association for Psychological Science.\n\nHighlighting creates a sense of mastering, so try to minimize it and not to use underlining as well. Just take notes, write valuable ideas and try to reread. You have to refresh the knowledge in your mind from time to time and make them as your personal experience. Teach someone to improve your level and it helps you to figure out the key points and you will grow through the explanation. You may learn new material with someone else to help each other and support.\n\nTo learn to ride on the bike\u200a\u2014\u200ayou need to fall and try again and again. You will remember better from your own experience and errors that are a natural part of our growth. The fear of falling makes us incompetent and passive. Be proactive, try new things, experiment to use your knowledge and skills in different ways. Experts advise to experiment to make some valuable achievement and create innovation. For example, musicians become professionals when they can improvise and feel free to change the way how they act. You need to know your field from different perspectives and combine different approaches. Be flexible and agile to learn more and more and update your mastery and knowledge.\n\nYou have to be curious, ask many questions, not to be satisfied with the skills that you already have. Find a coach or mentor that will provide you with useful insights and information.\n\nConnect new issues to the previous information and gain critical thinking to analyze different solutions and generate useful ideas. Identify the knowledge gaps and work on them. You can\u2019t move forward without the understanding basic things. Know your learning needs, your future goals and align education plan with them. Pay attention to the hard questions and focus on long-term results.\n\nGive special attention not only to the first piece of information but be mindful to the full portion of knowledge. You have to make time for the education to get good results. When you are overwhelmed, you cannot learn.\n\nUse each possibility to learn through apps, books, articles, online courses and learning network. It helps you to be aware of new trends and keep learning.\n\nIt is important to know your level and fulfill knowledge gaps for the self-realization and better job and life satisfaction. Not to read passively\u200a\u2014\u200atake notes and try to use the knowledge if you can implement the skills. For example, after the learning programming language\u200a\u2014\u200astart coding and you will have many questions that will lead in the right direction. You need the practice to become an expert and remember better the material.\n\nHighlighting creates a sense of mastering, so try to minimize it and not to use underlining as well. Just take notes, write valuable ideas and try to reread. You have to refresh the knowledge in your mind from time to time and make them as your personal experience. You need to know your field from different perspectives and combine different approaches. Be flexible and agile to learn more and more and update your mastery and knowledge.", 
        "title": "How to Avoid Illusions of Learning \u2013 Itsquiz \u2013"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/how-artificial-intelligence-is-changing-online-retail-forever-1e485962deff?source=tag_archive---------7----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "How Artificial Intelligence is Changing Online Retail Forever"
    }
]