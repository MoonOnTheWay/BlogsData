[
    {
        "url": "https://chatbotslife.com/ultimate-guide-to-leveraging-nlp-machine-learning-for-you-chatbot-531ff2dd870c?source=tag_archive---------0----------------", 
        "text": "The Code and data for this tutorial is on Github.\n\nThe vast majority of production systems today are retrieval-based, or a combination of retrieval-based and generative. Google\u2019s Smart Reply is a good example. Generative models are an active area of research, but we\u2019re not quite there yet. If you want to build a conversational agent today your best bet is most likely a retrieval-based model.\n\nIn this post we\u2019ll work with the Ubuntu Dialog Corpus (paper, github). The Ubuntu Dialog Corpus (UDC) is one of the largest public dialog datasets available. It\u2019s based on chat logs from the Ubuntu channels on a public IRC network. The paper goes into detail on how exactly the corpus was created, so I won\u2019t repeat that here. However, it\u2019s important to understand what kind of data we\u2019re working with, so let\u2019s do some exploration first.\n\nThe training data consists of 1,000,000 examples, 50% positive (label 1) and 50% negative (label 0). Each example consists of a context, the conversation up to this point, and an utterance, a response to the context. A positive label means that an utterance was an actual response to a context, and a negative label means that the utterance wasn\u2019t\u200a\u2014\u200ait was picked randomly from somewhere in the corpus. Here is some sample data.\n\nNote that the dataset generation script has already done a bunch of preprocessing for us\u200a\u2014\u200ait hastokenized, stemmed, and lemmatized the output using the NLTK tool. The script also replaced entities like names, locations, organizations, URLs, and system paths with special tokens. This preprocessing isn\u2019t strictly necessary, but it\u2019s likely to improve performance by a few percent. The average context is 86 words long and the average utterance is 17 words long. Check out the Jupyter notebook to see the data analysis.\n\nThe data set comes with test and validations sets. The format of these is different from that of the training data. Each record in the test/validation set consists of a context, a ground truth utterance (the real response) and 9 incorrect utterances called distractors. The goal of the model is to assign the highest score to the true utterance, and lower scores to wrong utterances.\n\nThe are various ways to evaluate how well our model does. A commonly used metric is recall@k. Recall@k means that we let the model pick the k best responses out of the 10 possible responses (1 true and 9 distractors). If the correct one is among the picked ones we mark that test example as correct. So, a larger k means that the task becomes easier. If we set k=10 we get a recall of 100% because we only have 10 responses to pick from. If we set k=1 the model has only one chance to pick the right response.\n\nAt this point you may be wondering how the 9 distractors were chosen. In this data set the 9 distractors were picked at random. However, in the real world you may have millions of possible responses and you don\u2019t know which one is correct. You can\u2019t possibly evaluate a million potential responses to pick the one with the highest score\u200a\u2014\u200athat\u2019d be too expensive. Google\u2019sSmart Reply uses clustering techniques to come up with a set of possible responses to choose from first. Or, if you only have a few hundred potential responses in total you could just evaluate all of them.\n\nBefore starting with fancy Neural Network models let\u2019s build some simple baseline models to help us understand what kind of performance we can expect. We\u2019ll use the following function to evaluate our recall@k metric:\n\nHere, y is a list of our predictions sorted by score in descending order, and y_test is the actual label. For example, a y of [0,3,1,2,5,6,4,7,8,9] Would mean that the utterance number 0 got the highest score, and utterance 9 got the lowest score. Remember that we have 10 utterances for each test example, and the first one (index 0) is always the correct one because the utterance column comes before the distractor columns in our data.\n\nIntuitively, a completely random predictor should get a score of 10% for recall@1, a score of 20% for recall@2, and so on. Let\u2019s see if that\u2019s the case.\n\nGreat, seems to work. Of course we don\u2019t just want a random predictor. Another baseline that was discussed in the original paper is a tf-idf predictor. tf-idf stands for \u201cterm frequency\u200a\u2014\u200ainverse document\u201d frequency and it measures how important a word in a document is relative to the whole corpus. Without going into too much detail (you can find many tutorials about tf-idf on the web), documents that have similar content will have similar tf-idf vectors. Intuitively, if a context and a response have similar words they are more likely to be a correct pair. At least more likely than random. Many libraries out there (such as scikit-learn) come with built-in tf-idf functions, so it\u2019s very easy to use. Let\u2019s build a tf-idf predictor and see how well it performs.\n\nWe can see that the tf-idf model performs significantly better than the random model. It\u2019s far from perfect though. The assumptions we made aren\u2019t that great. First of all, a response doesn\u2019t necessarily need to be similar to the context to be correct. Secondly, tf-idf ignores word order, which can be an important signal. With a Neural Network model we can do a bit better.\n\nThe Deep Learning model we will build in this post is called a Dual Encoder LSTM network. This type of network is just one of many we could apply to this problem and it\u2019s not necessarily the best one. You can come up with all kinds of Deep Learning architectures that haven\u2019t been tried yet\u200a\u2014\u200ait\u2019s an active research area. For example, the seq2seq model often used in Machine Translation would probably do well on this task. The reason we are going for the Dual Encoder is because it has been reported to give decent performance on this data set. This means we know what to expect and can be sure that our implementation is correct. Applying other models to this problem would be an interesting project.\n\nThe Dual Encoder LSTM we\u2019ll build looks like this (paper):\n\nIt roughly works as follows:\n\nTo train the network, we also need a loss (cost) function. We\u2019ll use the binary cross-entropy loss common for classification problems. Let\u2019s call our true label for a context-response pair y. This can be either 1 (actual response) or 0 (incorrect response). Let\u2019s call our predicted probability from 4. above y\u2019. Then, the cross entropy loss is calculated as L= \u2212y * ln(y\u2019) \u2212 (1 \u2212 y) * ln(1\u2212y\u2019). The intuition behind this formula is simple. If y=1 we are left with L = -ln(y\u2019), which penalizes a prediction far away from 1, and if y=0 we are left with L= \u2212ln(1\u2212y\u2019), which penalizes a prediction far away from 0.\n\nFor our implementation we\u2019ll use a combination of numpy, pandas, Tensorflow and TF Learn (a combination of high-level convenience functions for Tensorflow).\n\nThe dataset originally comes in CSV format. We could work directly with CSVs, but it\u2019s better to convert our data into Tensorflow\u2019s proprietary Example format. (Quick side note: There\u2019s alsotf.SequenceExample but it doesn\u2019t seem to be supported by tf.learn yet). The main benefit of this format is that it allows us to load tensors directly from the input files and let Tensorflow handle all the shuffling, batching and queuing of inputs. As part of the preprocessing we also create a vocabulary. This means we map each word to an integer number, e.g. \u201ccat\u201d may become 2631. The TFRecord files we will generate store these integer numbers instead of the word strings. We will also save the vocabulary so that we can map back from integers to words later on.\n\nEach Example contains the following fields:\n\nThe preprocessing is done by the prepare_data.py Python script, which generates 3 files:train.tfrecords, validation.tfrecords and test.tfrecords. You can run the script yourself or download the data files here.\n\nIn order to use Tensorflow\u2019s built-in support for training and evaluation we need to create an input function\u200a\u2014\u200aa function that returns batches of our input data. In fact, because our training and test data have different formats, we need different input functions for them. The input function should return a batch of features and labels (if available). Something along the lines of:\n\nBecause we need different input functions during training and evaluation and because we hate code duplication we create a wrapper called create_input_fn that creates an input function for the appropriate mode. It also takes a few other parameters. Here\u2019s the definition we\u2019re using:\n\nThe complete code can be found in udc_inputs.py. On a high level, the function does the following:\n\nWe already mentioned that we want to use the recall@k metric to evaluate our model. Luckily, Tensorflow already comes with many standard evaluation metrics that we can use, including recall@k. To use these metrics we need to create a dictionary that maps from a metric name to a function that takes the predictions and label as arguments:\n\nAbove, we use functools.partial to convert a function that takes 3 arguments to one that only takes 2 arguments. Don\u2019t let the name streaming_sparse_recall_at_k confuse you. Streaming just means that the metric is accumulated over multiple batches, and sparse refers to the format of our labels.\n\nThis brings is to an important point: What exactly is the format of our predictions during evaluation? During training, we predict the probability of the example being correct. But during evaluation our goal is to score the utterance and 9 distractors and pick the best one\u200a\u2014\u200awe don\u2019t simply predict correct/incorrect. This means that during evaluation each example should result in a vector of 10 scores, e.g. [0.34, 0.11, 0.22, 0.45, 0.01, 0.02, 0.03, 0.08, 0.33, 0.11], where the scores correspond to the true response and the 9 distractors respectively. Each utterance is scored independently, so the probabilities don\u2019t need to add up to 1. Because the true response is always element 0 in array, the label for each example is 0. The example above would be counted as classified incorrectly by recall@1because the third distractor got a probability of 0.45 while the true response only got 0.34. It would be scored as correct by recall@2 however.\n\nBefore writing the actual neural network code I like to write the boilerplate code for training and evaluating the model. That\u2019s because, as long as you adhere to the right interfaces, it\u2019s easy to swap out what kind of network you are using. Let\u2019s assume we have a model functionmodel_fn that takes as inputs our batched features, labels and mode (train or evaluation) and returns the predictions. Then we can write general-purpose code to train our model as follows:\n\nHere we create an estimator for our model_fn, two input functions for training and evaluation data, and our evaluation metrics dictionary. We also define a monitor that evaluates our model every FLAGS.eval_every steps during training. Finally, we train the model. The training runs indefinitely, but Tensorflow automatically saves checkpoint files in MODEL_DIR, so you can stop the training at any time. A more fancy technique would be to use early stopping, which means you automatically stop training when a validation set metric stops improving (i.e. you are starting to overfit). You can see the full code in udc_train.py.\n\nTwo things I want to mention briefly is the usage of FLAGS. This is a way to give command line parameters to the program (similar to Python\u2019s argparse). hparams is a custom object we create in hparams.py that holds hyperparameters, nobs we can tweak, of our model. This hparams object is given to the model when we instantiate it.\n\nNow that we have set up the boilerplate code around inputs, parsing, evaluation and training it\u2019s time to write code for our Dual LSTM neural network. Because we have different formats of training and evaluation data I\u2019ve written a create_model_fn wrapper that takes care of bringing the data into the right format for us. It takes a model_impl argument, which is a function that actually makes predictions. In our case it\u2019s the Dual Encoder LSTM we described above, but we could easily swap it out for some other neural network. Let\u2019s see what that looks like:\n\nThe full code is in dual_encoder.py. Given this, we can now instantiate our model function in the main routine in udc_train.py that we defined earlier.\n\nThat\u2019s it! We can now run python udc_train.py and it should start training our networks, occasionally evaluating recall on our validation data (you can choose how often you want to evaluate using the\u200a\u2014\u200aeval_every switch). To get a complete list of all available command line flags that we defined using tf.flags and hparams you can run python udc_train.py\u200a\u2014\u200ahelp.\n\nAfter you\u2019ve trained the model you can evaluate it on the test set using python udc_test.py\u200a\u2014\u200amodel_dir=$MODEL_DIR_FROM_TRAINING, e.g. python udc_test.py\u200a\u2014\u200amodel_dir=~/github/chatbot-retrieval/runs/1467389151. This will run the recall@k evaluation metrics on the test set instead of the validation set. Note that you must call udc_test.py with the same parameters you used during training. So, if you trained with\u200a\u2014\u200aembedding_size=128 you need to call the test script with the same.\n\nAfter training for about 20,000 steps (around an hour on a fast GPU) our model gets the following results on the test set:\n\nWhile recall@1 is close to our TFIDF model, recall@2 and recall@5 are significantly better, suggesting that our neural network assigns higher scores to the correct answers. The original paper reported 0.55, 0.72 and 0.92 for recall@1, recall@2, and recall@5 respectively, but I haven\u2019t been able to reproduce scores quite as high. Perhaps additional data preprocessing or hyperparameter optimization may bump scores up a bit more.\n\nYou can modify and run udc_predict.py to get probability scores for unseen data. For example python udc_predict.py\u200a\u2014\u200amodel_dir=./runs/1467576365/ outputs:\n\nYou could imagine feeding in 100 potential responses to a context and then picking the one with the highest score.\n\nIn this post we\u2019ve implemented a retrieval-based neural network model that can assign scores to potential responses given a conversation context. There is still a lot of room for improvement, however. One can imagine that other neural networks do better on this task than a dual LSTM encoder. There is also a lot of room for hyperparameter optimization, or improvements to the preprocessing step. The Code and data for this tutorial is on Github, so check it out.\n\nI hope you have found this Condensed NLP Guide Helpful. I wanted to publish a longer version (imagine if this was 5x longer) however I don\u2019t want to scare the readers away.\n\nAs someone who develops the front end of bots (user experience, personality, flow, etc) I find it extremely helpful to the understand the stack, know the technological pros and cons and so to be able to effectively design around NLP/NLU limitations. Ultimately a lot of the issues bots face today (eg: context) can be designed around, effectively.\n\nIf you have any suggestions on regarding this article and how it can be improved, feel free to drop me a line.\n\nCreator of 10+ bots, including Smart Notes Bot. Founder of Chatbot\u2019s Life, where we help companies create great chatbots and share our insights along the way.\n\nWant to Talk Bots? Best way to chat directly and see my latest projects is via my Personal Bot: Stefan\u2019s Bot.\n\nCurrently, I\u2019m consulting a number of companies on their chatbot projects. To get feedback on your Chatbot project or to Start a Chatbot Project, contact me.", 
        "title": "Ultimate Guide to Leveraging NLP & Machine Learning for your Chatbot"
    }, 
    {
        "url": "https://medium.com/@acrosson/installing-nvidia-cuda-cudnn-tensorflow-and-keras-69bbf33dce8a?source=tag_archive---------1----------------", 
        "text": "This is part three of our building a deep learning machine series. You can find the other posts here:\n\nIn this post I will outline how to install the drivers and packages needed to get up and running with TensorFlow\u2019s deep learning framework.\n\nTo start, install Ubuntu 14.04 Server. The download link is here. If you are using AWS, or some other cloud virtual machine provider, simply create an instance with Ubuntu 14.04 and ssh into the machine. We\u2019re using Ubuntu 14.04 rather than 16.04 because its supported by Cuda.\n\nNow that we have our server up and running, we\u2019ll need to install the various drivers and packages. Here\u2019s a list of what we\u2019re going to install.\n\nWe\u2019re assuming that you\u2019re using a machine which has a GPU installed, more specifically a Nvidia GPU. To find out if your GPU is installed properly and working run the following command.\n\nBefore we jump into anything, be sure to update apt-get\n\nWe also need to ensure gcc is up to date and we have python with pip installed as well as some of the scientfici python packages.\n\nDownload the Nvidia driver via wget and run the script in silent mode.\n\nNote: if you\u2019re using a different GPU other than the GTX 1080, you\u2019ll need to download the driver for that specific GPU.\n\nTo confirm that the driver was installed correctly and that your GPU is being recognized, run nvidia-smi. This command is also useful if you want to check performance metrics of the GPU.\n\nCuda allows us to run our TensorFlow models on the GPUs, without it we would be restricted to the CPU. Download the Cuda 7.5 library run file, using wget and install the driver, the toolkit, and samples.\n\nWe\u2019ll need to add the cuda library to our system path. Modify the\u00a0.bashrc file or run these commands.\n\nCuDNN is a library that helps accelerate deep learning frameworks, such as TensorFlow or Theano. Here\u2019s a brief explanation from the Nvidia website.\n\nBefore installing you\u2019ll need to register for Nvidia\u2019s Accelerated Computing Developer Program. Once registered, login and download CudNN 4.0 to your local computer. Then move the zip to your deep learning server via scp.\n\nUntar the folder and copy the the necessary files to the existing cuda library we installed earlier.\n\nWe\u2019re assuming you\u2019ll be using TensorFlow for building your deep neural network models. We\u2019re currently using TensorFlow 0.10 Simply install via pip with the upgrade flag.\n\nNow you should have everything you need to run a model using your GPU(s) for computations. You can confirm it works without writing your own validation script by running one of TensorFlow\u2019s example convolutional network scripts.\n\nThe first few lines of output should look similar to the following\n\nThe minibatch error should be decreasing with every step. If it\u2019s not, some part of your installation went wrong.\n\nKeras has a few dependencies which they have outline on their website. Running these commands should be everything you need.\n\nThis guide should be a sufficient starting point for building a deep learning machine and get up and running with TensorFlow. If you have any questions feel free to tweet us @acrosson @calerogers\n\nLike and share if you find this helpful!", 
        "title": "Installing Nvidia, Cuda, CuDNN, TensorFlow and Keras"
    }, 
    {
        "url": "https://medium.com/autonomous-agents/bayesian-regularization-for-neuralnetworks-2f2d34f03adc?source=tag_archive---------2----------------", 
        "text": "Bayes\u2019s Theorem fundamentally is based on the concept of \u201cvalidity of Beliefs\u201d. Reverend Thomas Bayes was a Presbyterian minster and a Mathematician who pondered much about developing the proof of existence of God. He came up with the Theorem in 18th century (which was later refined by Pierre-Simmon Laplace) to fix or establish the validity of \u2018existing\u2019 or \u2018previous\u2019 beliefs in the face of best available \u2018new\u2019 evidence. Think of it as a equation to correct prior beliefs based on new evidence.\n\nOne of the popular example used to explain Bayes\u2019s Theorem is to detect if a patient has a certain disease or not.\n\nThe key inferences in the Theorem is a follows:\n\nEvent: An event is a fact. The patient truly having a disease is an event. Also, truly NOT having the disease is also an event.\n\nTest: A test is a mechanism to detect if a patient has the disease (or a test devised to prove that a patient does not have the disease. Note that they are not the same tests)\n\nSubject: A patient is a subject who may or may not have the disease. A test needs to be devised for the subject to detect the presence of disease or devise a test to prove that the disease does not exist.\n\nTest Reliability: A test devised to detect the disease may not be 100% reliable. The test may not possibly detect the disease all the time. When the detection fails to recognize the disease in a subject who truly has the disease, we call them false negatives. Also the test on the subject who truly does not have the disease may show that the subject does have the disease. This is called false positives.\n\nTest Probability: This the probability of a test to detect the event (disease) given a subject (patient). This does not account the Test Reliability.\n\nEvent Probability (Posterior Probability): This is the \u201ccorrected\u201d test probability to detect the event given a subject by considering the reliability of the devised test.\n\nBelief (Prior Probability): A belief, also called a prior probability (or prior in short) is the subjective assumption that disease exits in a patient (based on symptoms or other subjective observations) prior to conducting the test. This is the most important concept in Bayes\u2019s Theorem. You need to start with the priors (or Beliefs) before you make corrections to that belief.\n\nThe following is the equation which shall accommodate the stated concepts.\n\nSo let\u2019s assign the values for each probabilities.\n\nThe posterior probability can be calculated based on Bayes\u2019s Theorem as follows:\n\nSo the posterior probability of the person truly having the disease, given that the test result is positive is only 19%\u00a0!! Note the stark difference in the corrected probability even if the test results are 90% accurate\u00a0? Why do you think, this is the case?\n\nThe answer lies in the \u2018priors\u2019. Note that the \u201cbelief\u201d that only 5% of the population may have a disease, is the strong reason for a 19% posterior probability. It\u2019s easy to prove. Change your prior beliefs (all else being equal) from 5% to let\u2019s say a 30%. Then you shall get the following results.\n\nNote that the posterior probability for the same test with a higher prior jumped significantly to 65%.\n\nHence, while all evidence and tests being equal, Bayes\u2019s theorem is strongly influenced by priors. If you start with a very low prior, even in the face of strong evidence the posterior probability will be closer to the prior (lower).\n\nA prior is not something you randomly make up. It should be based on observations even if subjective. There should be some emphasis on why someone holds on to a belief before assigning a percentage.", 
        "title": "Bayesian Regularization for #NeuralNetworks \u2013 Autonomous Agents \u2014 #AI \u2013"
    }, 
    {
        "url": "https://medium.com/flow-ci/flow-ci-weekly-mashup-12-be59a463a061?source=tag_archive---------3----------------", 
        "text": "- How Developers Use Node.js\u200a\u2014\u200aSurvey Results\n\n- Angular Version 2 Released\n\n- Build Role Play Game in JavaScript\n\n- Deep Learning for Android Keyboard Predictions\n\n- Top 5 Problems with Distributed Teams\n\n- REST 101\u200a\u2014\u200aFree Beginners Guide Ebook\n\nRisingStack the provider of Trace, a Node.js debugging and performance monitoring solution, conducted a survey to gain an insight into how Node.js is used today. 1126 Node.js developers completed the survey 55% of whom had more than 2 years of Node.js experience. The survey questions included:\n\n- What databases are they using?\n\n- Where do they run their Node.js applications?\n\n- What container techs and VMs they are using?\n\n- How do they pick packages?\n\nCheck out the survey on the RisingStack blog.\n\nWhen Angular 1 was first released it provided solutions how to develop for the emerging web. Today, six years later, application developers facing different kinds of challenges. The new release has been optimized for productivity. With ahead-of-time compilation and built in lazy-loading developers can deploy the fastest and smallest applications across different environments. Read the official announcement and get started with Angular 2.0.\n\nEver thought about making a game in JavaScript, but it seemed too intimidating? This step-by-step walkthrough from Robert Skalko published on freeCodeCamp can help to get started. In his article Robert, explains what approach he took for designing and coding the game as well as the issues he encountered including saving game\u2019s state, handling 50+ images with CSS Sprites. Check out his motivating article at freeCodeCamp.\n\nSwiftKey added neural network-based predictions to the latest stable version of its keyboard. Artificial Neural Network is a machine learning method that is inspired by biological neural networks. Neural networks use a large number of inputs to generate outputs which often have a higher accuracy the what traditional rule-based can achieve. This means that the new version of SwiftKey can consider larger context and provide more appropriate predictions. Read more at Android Police.\n\nDistributed teams are increasingly common nowadays. Many of us work with teammates and clients from different cities, countries, or continents. When people are not in the same office many challenges can occur. InfoQ\u2019s article pinpoints the problems that most distributed team face regularly:\n\nRead the full article to understand the nature of these problems and find out how you can solve them.\n\nRepresentational State Transfer (REST) is software architectural style for Web Development. Systems designed in this style are aimed for scalability, fast performance, and reliability. This free REST API ebook from Smart Bear will help you learn the fundamentals of RESTful Web Services and to use and test REST APIs. The key topics in the ebook include:\n\n- What is a Rest API?\n\n- The differences between Rest & Soap.\n\n- When to use Restful web services?\n\n- An introduction to testing Rest APIs.\n\n- Rest testing with SoapUI NG Pro.\n\nToday, repository management services are key components of collaborative software development. In this article we briefly introduce and compare four popular repository management services Github, Bitbucket, Gitlab, and Coding by touching on multiple aspects including basic features, relationship to open source, importing repositories, free plans, could-hosted plans, and self-hosted plans. Read the full article at flow.ci medium publication.", 
        "title": "flow.ci Weekly Mashup 12 \u2013 flow.ci \u2013"
    }, 
    {
        "url": "https://machinelearnings.co/machine-learnings-9-e04fb266319f?source=tag_archive---------4----------------", 
        "text": "1/ Elon Musk says AI tech must be spread out because any \u201cone company having control over AI technology will create an unstable situation.\u201d\n\n2/ Marc Andreessen reverses his stance that big companies have a data advantage over startups building AI technology.\n\n4/ Reid Hoffman sees an opportunity for ML startups that help companies \u201ctune for their specific service.\u201d\n\n5/ Diane Greene thinks ML will transform healthcare with ability to do image analyses (fracture diagnosis) better than a doctor.\n\n7/ IBM Watson helps oncologists to treat cancer patients according to best practice treatment guidelines.\n\n8/ Houzz uses deep learning to scan photos and make product recommendations.\n\n9/ TensorFlow open sources a model trained on the One Billion Word Benchmark.\n\n10/ IBM, GE, and Hilton Worldwide are using algorithms to screen, test, and hire new talent.\n\n12/ Microsoft chose 10 machine learning and data science startups to take part in its accelerator in Seattle.\n\n13/ Comma.ai will ship an autonomous driving add-on by the end of this year.\n\n14/ Bottlenose released a platform to automate the work of data scientists.\n\nSources:\u00a0\n\n1/ Elon Musk on How to Build the Future with Sam Altman\n\nYCombinator\n\n2/ Marc Andreessen on the Atomization of AI\n\nTechCrunch\n\n3/ Uber Starts Self-Driving Car Pickups in Pittsburgh\n\nTechCrunch\n\n4/ Reid Hoffman on Startups Undercutting the Tech Giants in Machine Learning\n\nTechCrunch\n\n5/ Diane Greene on How Machine Learning Will Transform Healthcare\n\nTechCrunch\n\n6/ Unconscious F-16 Pilot Saved from Fatal Crash by Onboard Collision Avoidance Software\n\nPopular Mechanics\n\n7/ IBM Watson: A Doctor\u2019s Digital Assistant\n\nWIRED UK\n\n8/ Houzz Now Uses Deep Learning to Help You Find and Buy Products In Its Photos\n\nTechCrunch\n\n9/ Language Model on One Billion Word Benchmark\n\nGithub\n\n10/ How AI is Changing Human Resources\n\nFast Company\n\n11/ GTA V Becomes An Unlikely Mentor for Artificial Intelligence; Will Teach Self-Driving Cars to Prevent Obstacles\n\nNOWLOADING\n\n12/ These 10 Machine Learning and Data Science Startups Make Up The Newest Accelerator Class\n\nGeekWire\u00a0\n\n13/ Comma.ai Will Ship a $999 Autonomous Driving Add-On By the End of This Year\n\nTechCrunch\n\n14/ Bottlenose Founder Wants to Automate the Work of Data Scientists\n\nThe Wall Street Journal\n\n15/ Evernote Is Moving All Its Data, Machine Learning Tech to Google Cloud Platform\n\nTechCrunch\n\n\u00a016/ Episodic Exploration for Deep Deterministic Policies: An Application to StarCraft Micromanagement Tasks \n\nCornell University Library", 
        "title": "Elon Musk Wants AI Spread Across Companies \u2014 #9 \u2013"
    }, 
    {
        "url": "https://blog.vilynx.com/the-future-of-journalism-a-view-from-tronc-a40f4ddd1f02?source=tag_archive---------5----------------", 
        "text": "Here at Vilynx we have always known that we are one step ahead of the curve with our technology that makes Video Smarter through Machine Learning. So when a Media Company like Tronc comes out and talks about future of Machine Learning and AI for Journalism, we like to pass it along. This video really resonated with us at Vilynx as it highlights much of what we talk about with regards to the why video and Machine Learning is key to the future of publishers. Take a quick view of the video below to understand how Tronc views the future of journalism and how important the role of video to generate their revenue growth engine.\n\nSome of the key nuggets from the video are:\n\n(1) The extent that Tronc is relying on Machine learning to drive the reach of their content through their distribution channels (Chicago Tribune, LA Times, Orlando Sentinel\u2026). See their chart below.\n\n(2) The focus Tronc has on expanding the use of video to drive more revenue on their sites. They currently only have 16% of their website pages loaded with a video player that they can monetize. Their goal is to be able to monetize over 50% of website pages with video by 2017.", 
        "title": "The Future of Journalism (a view from TRONC) \u2013"
    }, 
    {
        "url": "https://medium.com/@chriscaruso/why-ai-will-never-be-perfect-c34aec481048?source=tag_archive---------6----------------", 
        "text": "I see this a lot. People tend to think that because computers are machines that can perform nearly perfect logical and mathematical operations, then so too will anything that runs on them inherit this property of perfection, including AI. This leads people to believe that AI will be perfect and incapable of logical errors.\n\nThis is not the case. This is because modern AI learns by itself, from many examples, and can only ever practically approximate what we want it to learn, thus leaving room for error. This is the case for many different reasons, but it\u2019s worth exploring a few.\n\nOne of the most common ways AI produce errors, occurs because there isn\u2019t enough data for an AI to properly learn the underlying generalization. Think of it this way, if I wanted an AI to learn to distinguish between a cat and a dog, then what I would do is show it a bunch of pictures of cats and dogs, and constantly adjust the AI to better predict which the picture is of. It is possible that there is a kind of cat that looks similar to a dog that I didn\u2019t show it a picture of, so it was never able to learn that kind of cat was actually a cat; therefore, it may mistakenly predict it is a dog. This is a symptom of the AI not being able to project the generalization it learned of a cat from the examples I showed it, onto the cat it had never seen before, and hence produce the error of predicting it was a dog.\n\nHaving too small a dataset can cause overfitting, but overfitting can happen with large datasets too. Overfitting is when an AI starts to lose its ability to generalize a learned solution to other valid examples, producing errors. A good way to think of this, is that the AI starts to memorize the dataset, rather than learn abstract patterns in the data. This, again, results in an error when trying to predict something that it hasn\u2019t seen before. Overfitting is usually a symptom of having too small a dataset, but this is not necessarily the case. If an AI with a sufficiently large dataset is left to learn the data for too long, and isn\u2019t stopped at some point in time where it can sufficiently predict the output, then this can result in overfitting as well.\n\nAs I\u2019ve explained before, in another post, most modern AI takes the form of some kind of neural network, such as those used by deep learning. When it comes down to what these fundamentally are, they\u2019re just large equations. So, yes, it is true that, within error of fixed precision floating point math, the computation of the neural network\u2019s equation is logically correct, but this should not be confused with what the computation represents, which is simply the output in terms of the input (a cat being represented by a picture of a cat for example). The representation is the part which can be incorrect.\n\nThe reason AI can err is the same reason humans can and do frequently. If you think about it for a moment, our brains too can be thought of as large equations, they just work a little differently. Instead of using abstract numbers and variables as the medium for their representations (as neural networks do), our brains compute using the physical medium of biology. Biology is emergent from chemistry, and chemistry is emergent from physics. Physics appears predictable, and follows certain patterns of particle interaction. This causes chemistry to be predictable and follow patterns of interaction, and finally biology to be predictable and follow patterns. What I\u2019m trying to show you, is that this level of logical predictability (computability) is never lost, however complex it becomes. For example, if you could replicate exactly, to every detail, a cell and its environment, down to the smallest level of physical reality, it would produce exactly the same behavior as the original. This is equivalent to producing exactly the same result using an equation, like a neural network. So now, you see, just because our brains run on logically coherent biological systems, doesn\u2019t mean they\u2019re infallible. It\u2019s what their medium of computation represents that is the part which can be fallible, as is the same for neural networks and AI.\n\nIf you dive deep with AI, then you can see the same pattern emerge. What do we compute neural networks on? A computer. What does the computer use to perform computations and calculations? Ultimately, chemistry and physics. Though, notice that there is an extra step, such that representations of what a neural network learns relies on mathematical patterns, and those in turn are represented by the carefully designed mechanisms of a computer. Which might imply that our brains potentially use a similar intermediary representation, but let\u2019s save that discussion for another time.\n\nIn conclusion, you can see that computers and our brains are not too different, and both are subject to logically correct computational patterns that emerge from the systems they\u2019re built on, but both can also err because what they represent can be incorrect, not the underlying systems. Going full loop, back to the beginning, these representations are erroneous primarily because they don\u2019t properly represent what they\u2019re supposed to, typically because they aren\u2019t general enough to encompass the ground truth of what they should. Just as our human brains are perfectly playing out the laws of biology, so too are neural networks perfectly playing out the laws of mathematics, but both are prone to represent things which may not be fully correct in terms of what they should represent.", 
        "title": "Why AI will never be perfect \u2013 Chris Caruso \u2013"
    }
]