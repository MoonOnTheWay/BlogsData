[
    {
        "url": "https://medium.com/intuitionmachine/why-deep-learning-is-radically-different-from-machine-learning-945a4a65da4d?source=tag_archive---------0----------------", 
        "text": "There is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL). There certainly is a massive uptick of articles about AI being a competitive game changer and that enterprises should begin to seriously explore the opportunities. The distinction between AI, ML and DL are very clear to practitioners in these fields. AI is the all encompassing umbrella that covers everything from Good Old Fashion AI (GOFAI) all the way to connectionist architectures like Deep Learning. ML is a sub-field of AI that covers anything that has to do with the study of learning algorithms by training with data. There are whole swaths (not swatches) of techniques that have been developed over the years like Linear Regression, K-means, Decision Trees, Random Forest, PCA, SVM and finally Artificial Neural Networks (ANN). Artificial Neural Networks is where the field of Deep Learning had its genesis from.\n\nSome ML practitioners who have had previous exposure to Neural Networks (ANN), after all it was invented in the early 60\u2019s, would have the first impression that Deep Learning is nothing more than ANN with multiple layers. Furthermore, the success of DL is more due to the availability of more data and the availability of more powerful computational engines like Graphic Processing Units (GPU). This of course is true, the emergence of DL is essentially due to these two advances, however the conclusion that DL is just a better algorithm than SVM or Decision Trees is akin to focusing only on the trees and not seeing the forest.\n\nTo coin Andreesen who said \u201cSoftware is eating the world\u201d, \u201cDeep Learning is eating ML\u201d. Two publications by practitioners of different machine learning fields have summarized it best as to why DL is taking over the world. Chris Manning an expert in NLP writes about the \u201cDeep Learning Tsunami\u201c:\n\nNicholas Paragios writes about the \u201cComputer Vision Research: the Deep Depression\u201c:\n\nThese two articles do highlight how the field of Deep Learning are fundamentally disruptive to conventional ML practices. Certainly it should be equally disruptive in the business world. I am however stunned and perplexed that even Gartner fails to recognize the difference between ML and DL. Here is Gartner\u2019s August 2016 Hype Cycle and Deep Learning isn\u2019t even mentioned on the slide:\n\nWhat a travesty! It\u2019s bordering on criminal that they their customer\u2019s have a myopic notion of ML and are going to be blind sided by Deep Learning.\n\nAnyway, despite being ignored, DL continues to by hyped. The current DL hype tends to be that we have these commoditized machinery, that given enough data and enough training time, is able to learn on its own. This of course either an exaggeration of what the state-of-the-art is capable of or an over simplification of the actual practice of DL. DL has over the past few years given rise to a massive collection of ideas and techniques that were previously either unknown or known to be untenable. At first this collection of concepts, seems to be fragmented and disparate. However over time patterns and methodologies begin to emerge and we are frantically attempting to cover this space in \u201cDesign Patterns of Deep Learning\u201c.\n\nDeep Learning today goes beyond just multi-level perceptrons but instead is a collection of techniques and methods that are used to building composable differentiable architectures. These are extremely capable machine learning systems that we are only right now seeing just the tip of the iceberg. The key take away from this is that, Deep Learning may look like alchemy today, but we eventually will learn to practice it like chemistry. That is, we would have a more solid foundation so as to be able to build our learning machines with greater predictability of its capabilities.\n\nAlso read: 11 Biases Why Experts will miss the Deep Learning revolution.", 
        "title": "Why Deep Learning is Radically Different from Machine Learning"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-the-unreasonable-effectiveness-of-randomness-14d5aef13f87?source=tag_archive---------1----------------", 
        "text": "The paper submissions for ICLR 2017 in Toulon France deadline has arrived and instead of a trickle of new knowledge about Deep Learning we get a massive deluge. This is a gold mine of research that\u2019s hot off the presses. Many papers are incremental improvements of algorithms of the state of the art. I had hoped to find more fundamental theoretical and experimental results of the nature of Deep Learning, unfortunately there were just a few. There was however 2 developments that were mind boggling and one paper that is something I\u2019ve been suspecting for a while now and has finally been confirm to shocking results. It really is a good news, bad news story.\n\nFirst let\u2019s talk about the good news. The first is the mind boggling discovery that you can train a neural network to learn to learn (i.e. meta-learning). More specifically, several research groups have trained neural networks to perform stochastic gradient descent (SGD). Not only have they been able to demonstrate neural networks that have learned SGD, the networks have performed better than any hand tuned human method! The two papers that were submitted were\u201dDeep Reinforcement Learning for Accelerating the Convergence Rate\u201d and \u201cOptimization as a Model for Few-Shot Learning\u201d\u00a0. Unfortunately though, these two groups have been previously scooped by Deep Mind, who showed that you could do this in this paper \u201cLearning to Learn by gradient descent by gradient descent\u201c. The two latter papers trained an LSTM, while the first one trained via RL. I had thought that it would take a bit longer to implement meta-learning, but it has arrived much sooner than I had expected!\n\nNot to be out-done, two other groups created machines that could design new Deep Learning networks and do it in such a way as to improve on the state-of-the-art! This is learning to design neural networks. The two papers that were submitted are \u201cDesigning Neural Network Architectures using Reinforcement Learning\u201d and \u201cNeural Architecture Search with Reinforcement Learning\u201d. The former paper describes the use of Reinforcment Q-Learning to discover CNN architectures. You can find some of their generated CNNs in Caffe here: https://bowenbaker.github.io/metaqnn/\u00a0. The latter paper is truly astounding (you can\u2019t do this without Google\u2019s compute resources). Not only did they show state-of-the-art CNN networks, the machine actually learned a few more variants of the LSTM node! Here are the LSTM nodes the machine created (left and bottom):\n\nSo not only are researcher who hand optimize gradient descent solutions out of business, so are folks who make a living designing neural architectures! This is actually just the beginning of Deep Learning systems just bootstrapping themselves. So I must now share Schmidhuber\u2019s cartoon that aptly describes what is happening:\n\nThis is absolutely shocking and there\u2019s really no end in sight as to how quickly Deep Learning algorithms are going to improve. This meta capability allows you to apply it on itself, recursively creating better and better systems.\n\nPermit me now to deal you the bad news. Here is the paper that is the bearer of that news: \u201cUnderstanding Deep Learning required Rethinking Generalization\u201c. I\u2019ve thought about Generalization a lot, and I\u2019ve posted out some queries in Quora about Generalization and also about Randomness in the hope that someone could give some good insight. Unfortunately, nobody had enough of an answer or understood the significance of the question until the folks who wrote the above paper performed some interesting experiments. Here is a snippet of what they had found:\n\nThe shocking truth revealed. Deep Learning networks are just massive associative memory stores! Deep Learning networks are capable of good generalization even when fitting random data. This is indeed strange in that many arguments for the validity of Deep Learning is on the conjecture that \u2018natural\u2019 data tends to exists in a very narrow manifold in multi-dimensional space. Random data however does not have that sort of tendency.\n\nJohn Hopfield wrote a paper early this year examining the duality of Neural Networks and Associative Memory. Here\u2019s a figure from his paper:\n\nThe \u201cRethinking Generalization\u201d paper goes even further by examining our tried and true tool for achieving Generalization (i.e. Regularization) and finds that:\n\nIn other words, all our regularization tools may be less effective than what we believe! Furthermore, even more shocking, the unreasonable effectiveness of SGD turns out to be:\n\njust a different kind of regularization that just happens to work!\n\nIn fact, a paper submitted for ICLR2017 by another group titled \u201cAn Empirical Analysis of Deep Network Loss Surfaces\u201d confirms that the local minima of these networks are different:\n\nWhich tells you that your choice of learning algorithm \u201crigs\u201d how it arrives at a solution. Randomness is ubiquitous and it does not matter how you regularize your network or what the SGD variant that you employ, the network just seems to evolve (if you set the right random conditions) towards convergence! What are the properties of SGD that leads to machines that can learn? Are the properties tied to differentiation or is it something more general? If we can teach a network to perform SGD, can we teach it to perform this unknown generalized learning method?\n\nThe effectiveness of this randomness was in fact demonstrated earlier this year in a paper: \u201cA Powerful Generative Model Using Random Weights for the Deep Image Representation\u201d also co-authored by John Hopcroft that showed that you could generate realistic imagery using randomly initialized networks without any training! How could this be possible? (Editor\u2019s Note: Initialization with random weights is certainly better than non-random weights, unless those weights are from a pre-trained network)\n\nTherefore to understand Deep Learning, we must embrace randomness. Randomness arises from maximum entropy, which interestingly enough is not without its own structure! The memory capacity of a neural network seems to be highest the closer to random the weights are. The strangeness here is that Randomness is ubiquitous in the universe. The arrow of time is reflected by the direction towards greater entropy. How then is it that this property is also the basis of learning machines?\n\nIf we were to assume that the reasoning (or the intuition) behind hierarchical layers in DL is that the bottom layers consist of the primitive recognition components that are built up, layer by layer, into more complex recognition components.\n\nWhat this implies then is that the bottom components during training should be \u2018searched\u2019 more thoroughly than the top most components. But the way SGD works is that the search is driven from the top and not from the bottom. So the top is searched more thoroughly that the bottom layers.\n\nWhich tells you the bottom layers (the ones closest to inputs) are not optimal in their representation. In fact, they are the kind of a representation that likely will be of the most generalized form. The kind that will have recognizers that will have equal probability of matching anything, in short, completely random!\n\nAs you move up the layers, the specialization happens because it is actually driven from the top which is designed to fit the data. Fine tuning happens at the top.\n\nLet\u2019s make the analogy of this process with languages. The bottom components of a language are letters and the top parts are sentences. In between you have syllables, words, parts of speech etc. However from a Deep Learning perspective, it is as if there are no letters! But rather fuzzy forms of letters. Which builds up into other fuzzy forms of words and so forth. The final layers is like some projection (some wave collapse) into interpretation.\n\nNow let\u2019s throw in the Swap Out learning procedure which tells us that if you sample any subnetwork of the entire network the resulting prediction will be the similar to any other subnetwork you look sample. Just like holographic memory where you can slice of pieces and still recreate the whole. The procedure seems to be that the more random we try to make it to be, the better our learning. That is definitely counter-intuitive!\n\nThe following paper published last week \u201cLearning in the Machine: Random Backpropagation and the Learning Channel\u201d explores the robustness of using random matrices instead of gradients.\n\nPlease see Design Patterns for Deep Learning: Canonical Patterns for additional insight on this intriguing subject.", 
        "title": "Deep Learning: The Unreasonable Effectiveness of Randomness"
    }, 
    {
        "url": "https://medium.com/@vivek.yadav/why-dropouts-prevent-overfitting-in-deep-neural-networks-937e2543a701?source=tag_archive---------2----------------", 
        "text": "Here I will illustrate the effectiveness of dropout layers with a simple example. Dropout layers provide a simple way to avoid overfitting. https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf The primary idea is to randomly drop components of neural network (outputs) from a layer of neural network. This results in a scenario where at each layer more neurons are forced to learn the multiple characteristics of the neural network.\n\nThe true strength of drop out comes when we have multiple layers and many neurons in each layers. For a simple case, if a network has 2 layers and 4 neurons in each layer, then we are over training process making sure than 4C2 X 4C2 = 36 different models learn the same relation, and during prediction are taking average of predictions from 36 models. The strength comes from the fact that when we have many hidden layers and hidden neurons, we end up with a situation where NC(N/2)^h models learn the relation between data and target, which has the effect of taking enseamble over NC(N/2)^h models. For a 2 layer model with 100 neurons in each layer, this results in a scenario where we are taking average over 24502500 possible models.\n\nSay you have 2 neurons, whose values are PA and B, and we randomly drop 1 of them in the training.\n\nSo the possible output during training after drop out layer are,\n\nThe term 2 comes due to scaling. If drop out rate were\u00a0.25, then we would multiply by 4. If drop out rate is p, then we multiply values by 1/p. This comes from expected value literature in probability.\n\nThe main idea of drop out is to to have neuron A and neuron B both to learn something about the data, and the neural network not rely on 1 neuron alone. This has the effect of developing redundant representations of data for prediction. However, we have no idea which one is better, so in the testing phase, we compute outcome from all the neurons and average them. This in effect means you compute outcome without drop outs, and just add the results without scaling.\n\nIf we were to take average with dropouts, output will be (2 N_a A + 2 N_b B)/(N_a + N_b), where N_a and N_b represent number of times A or B were selected. Over sufficiently large number of samples, we can assume that n_A ~ n_B (as drop out rate is 50%). Therefore, the final expected value is A + B. Note N_a/(N_a+N_b) and N_b/(N_a+N_b) are nothing but observed probabilities of selecting A or B. Ideally we should run a large number of experiments and average the values. Mathematically,\n\nSo by using probability theory, we can simply not do dropout and not scale the output of the neurons during testing. This is also illustrated below,\n\nConsider the case when there are 2 hidden layers with neurons A and B in one, and C and D in second. In this case, we have a scenario where during training, we want to train AC, AD, BC and BD all to learn the relation between input and output. Therefore, we have 4 models learning the same relation. This number increases exponentially if we have more layers and more neurons. If we had 4 neurons and 2 layers, the number of possible models is 4C2 X 4C2 = 36, therefore we are taking average over 36 different models. The general formula is, NC(N/2)^h, For 2 layer network with 10 neurons in each layer, this number is around 2025. Therefore, we end up taking average of 2025 different models during testing, and are training on equally large number of models during training. For a 2 layer model with 100 neurons in each layer, this results in a scenario where we are taking average over billion possible models. As a result, the tendency to overfit is significantly reduced.", 
        "title": "Why dropouts prevent overfitting in Deep Neural Networks"
    }, 
    {
        "url": "https://medium.com/@vivek.yadav/deep-learning-machine-first-build-experience-d04abf198831?source=tag_archive---------3----------------", 
        "text": "Update: I had to reinstall all the deep learning software. Details can be found here.\n\nI recently built my first computer. The main purpose of getting a computer was to have a computer of my own. I always had access to computer through work, so never bought my own. However, wanted to have one of my own so I can optimize it the best I can for the simulations I run. Most of my work involves generating mathematical equations to describe physical phenomena, design a controller, and perform simulation experiments to answer specific research questions. For example, am working on simulation models to predict how walking gait changes with prosthetic\u2019s inertial parameters among individuals with trans-tibial amputation, another project am working on involves developing computational models to characterize behavior of stroke surviors (using deep learning, audio processing and language processing for quantifying emotional expression). As my main appointment is in mechanical engineering, am working on developing autonomous robots and courses in control systesm.\n\nIn addition I have been a constant student at Udacity. Recently, I was selected to participate in Udacity\u2019s self-driving car nanodegree, so finally decided to make computer for deep learning applications. My budget was around 2K, and aim was to spend all the $$ solely on parts needed for deep learning applications.\n\nThe process of selecting parts was tedious, and took almost 2 months just to decide on the parts. During this time, I read many blog posts, saw many youtube videos etc. Of all the resources linus tips is my go to channel for hardware updates. I finally settled on the following parts, http://pcpartpicker.com/list/DNjdqk\n\nDetailed part list in the link above, but below are the specific things I got for deep learning application,\n\nI am new to linux, so didn\u2019t go with RAID configurations, but will switch to that eventually.\n\nAssembling the computer was straight forward. I then installed Ubuntu 16.04, and then went to install drivers for titan X. It took me longest to get the Titan X running. I kept getting the infamous purple screen and got stuck in login loop. At the end, disabling the secure boot, and changing the motherboard display to default display fixed the issue. After installing the standard deep learning libraries, I finally got my computer training neural networks on GPU.\n\nI was pleasantly surprised by the build process, and how rewarding it was to see the first boot. It took a full day to build the computer and almost 3 days to get the softwares installed. Once everything was up and running, the computer was chewing through convnets like paper. Models that used to take 3\u20134 hours on my 2011 macbook pro were taking less than a minute on this machine.\n\nMy tip for anyone considering building a computer of their own, just do it. There are many things that will come in your way, and there will be constant anxiety about every small detail. More importantly, do not listen to every self-proclaimed guru on internet. I reinstalled ubuntu 16.04 16 times before I could get it up and running.", 
        "title": "Deep Learning Machine: First build experience \u2013 Vivek Yadav \u2013"
    }, 
    {
        "url": "https://medium.com/gradiant-talks/an%C3%A1lisis-elecciones-generales-26j-espa%C3%B1a-desde-la-perspectiva-de-los-medios-sociales-e01e55e51cc2?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "An\u00e1lisis de las Elecciones Generales del 26J en Twitter, desde la perspectiva del Big Data"
    }, 
    {
        "url": "https://blog.empiricalci.com/a-gpu-enabled-ami-for-deep-learning-5aa3d694b630?source=tag_archive---------5----------------", 
        "text": "TLDR: Use AMI ami-b1e2c4a6, which provides the NVIDIA drivers, docker, and nvidia-docker. Run your projects as docker containers. Using docker to package your projects allows them to be easily ported. Here are a couple of Docker images to get started.\n\nI recently read this post on r/MachineLearning about an AMI pre-built with GPU support and several popular software dependencies for Deep Learning like OpenCV, Caffe, Keras, Theano, Tensorflow, etc.\n\nIt\u2019s definitely very useful to have an environment with everything set up and ready to go. One of the biggest sources of friction when trying a new project is having to set up the environment. Even when the source code and results are available, each project has its own framework with multiple dependencies. They\u2019re implemented in different programming languages, different environments and operative systems. This adds up to make it really cumbersome and time consuming to test a new approach.\n\nFor this purpose, I think using an AMI pre-built with a ready-to-go environment is great and I wanted to follow suit by sharing one of the AMIs we use at empirical.\n\nInstead of including several deep learning packages, our AMI includes only a few dependencies:\n\nDocker allows us to easily install any software or project packaged as a docker image. The NVIDIA driver and nvidia-docker allow us to run these projects with GPU support. (You can learn more about the motivation for nvidia-docker on their wiki.)\n\nI would really like to encourage you to use Docker for your projects. Packaging your projects as Docker images is really great for productivity, because once you do it, you have a portable environment that can be executed anywhere with minimal setup. Using docker images reduces the initial friction for testing a new approach, you can share the docker image with your peers and they can be running commands right away without having to setup anything more that Docker.\n\nNote that GPU support for docker only works on Linux at the moment.\n\nYou can launch the AMI using the AWS UI.\n\nYou can find the details about how to build the AMI from scratch here.\n\nOnce docker is installed you can run your projects as docker containers. For this you need to create a Docker image including your dependencies and your code.\n\nYou can find Dockerfiles and pre-built docker images for OpenCV 3.0 and Caffe with CUDA support here.\n\nAt empirical we aim to be the ArXiv for experiments and we\u2019re currently developing tools for helping scientists and engineers to easily run and replicate experiments. Check us out at https://empiricalci.com.", 
        "title": "A GPU enabled AMI for Deep Learning \u2013"
    }, 
    {
        "url": "https://medium.com/@GarvitBhada/deep-learning-part-1-569de4053bbb?source=tag_archive---------6----------------", 
        "text": "Each day we hear about some machine learning innovation which blows our mind and enthralls us. A lot of these innovations use deep learning concepts. In this post, I will try to jot down a few useful basics. Let\u2019s start with neural networks (say NN).\n\nAn NN can outperform other traditional classification algorithms like Logistic regression, naive bayes etc. in complex tasks which involves large amount of variables and data. However, since NN uses nodes in layers to train and detect pattern, the number of nodes required increases exponentially for too complex tasks. Thereby, making NN unusable. Here comes the application of Deep Nets (DN), which break down complex patterns into simple patterns.\n\nDeep Nets can be used for both supervised and unsupervised learning problems. Now, we still need to train the NNs. For this we use the technique of backpropagation (I wouldn\u2019t go into much detail about how backpropagation works). Backpropagation causes the problem of gradient vanishing. Explained concisely in this link\u200a\u2014\u200ahttps://www.quora.com/What-is-the-vanishing-gradient-problem/answer/Nikhil-Garg\u00a0.\n\nFor dealing with the problem of vanishing gradient we resort to a type of deep net called the RBMs or the Restricted Boltzmann Machines. RBMs reconstruct the inputs (by encoding it into numbers and working backwards to check if it encodes properly, similar to NN process) to find patterns in the data. The term \u2018Restricted\u2019 comes from the fact that it typically uses only two layers. The functioning is similar to an NN. (Note\u200a\u2014\u200aeach node is connected to every other node).\n\nRBMs are a part of the feature extraction NN. But, the question is how does this help with the vanishing gradient problem.\n\nFor solving the vanishing gradient problem, we use a technique in which we stack the RBMs together and just like the perceptrons where stacking multiple perceptrons increased the power of the nets, stacking the RBMs too yield better results. This type of deep net is called Deep Belief Net or DBN. However, unlike the other NNs, each RBMs layer tries to learn the entire output.\n\nDBNs require us to introduce small set of labelled data in order to label the other data. Hence, DBNs are used for supervised learning problems.\n\nDBNs are the solution to the vanishing gradient problem.\n\n(My primary interest in writing this blog is to explore the different domains in which the field of Machine Learning can be applied to the field of Economics. In the next few posts, I will try to compare the techniques of Econometrics and explore how can Machine learning can be linked to that. Stay tuned\u00a0!).", 
        "title": "Deep Learning ( Part 1 ) \u2013 Garvit Bhada \u2013"
    }
]