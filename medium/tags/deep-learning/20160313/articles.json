[
    {
        "url": "https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714?source=tag_archive---------0----------------", 
        "text": "Although we don\u2019t know how brain functions yet, we have the feeling that it must have a logic unit and a memory unit. We make decisions by reasoning and by experience. So do computers, we have the logic units, CPUs and GPUs and we also have memories.\n\nBut when you look at a neural network, it functions like a black box. You feed in some inputs from one side, you receive some outputs from the other side. The decision it makes is mostly based on the current inputs.\n\nI think it\u2019s unfair to say that neural network has no memory at all. After all, those learnt weights are some kind of memory of the training data. But this memory is more static. Sometimes we want to remember an input for later use. There are many examples of such a situation, such as the stock market. To make a good investment judgement, we have to at least look at the stock data from a time window.\n\nThe naive way to let neural network accept a time series data is connecting several neural networks together. Each of the neural networks handles one time step. Instead of feeding the data at each individual time step, you provide data at all time steps within a window, or a context, to the neural network.\n\nA lot of times, you need to process data that has periodic patterns. As a silly example, suppose you want to predict christmas tree sales. This is a very seasonal thing and likely to peak only once a year. So a good strategy to predict christmas tree sale is looking at the data from exactly a year back. For this kind of problems, you either need to have a big context to include ancient data points, or you have a good memory. You know what data is valuable to remember for later use and what needs to be forgotten when it is useless.\n\nTheoretically the naively connected neural network, so called recurrent neural network, can work. But in practice, it suffers from two problems: vanishing gradient and exploding gradient, which make it unusable.\n\nThen later, LSTM (long short term memory) was invented to solve this issue by explicitly introducing a memory unit, called the cell into the network. This is the diagram of a LSTM building block.\n\nAt a first sight, this looks intimidating. Let\u2019s ignore the internals, but only look at the inputs and outputs of the unit. The network takes three inputs. X_t is the input of the current time step. h_t-1 is the output from the previous LSTM unit and C_t-1 is the \u201cmemory\u201d of the previous unit, which I think is the most important input. As for outputs, h_t is the output of the current network. C_t is the memory of the current unit.\n\nTherefore, this single unit makes decision by considering the current input, previous output and previous memory. And it generates a new output and alters its memory.\n\nThe way its internal memory C_t changes is pretty similar to piping water through a pipe. Assuming the memory is water, it flows into a pipe. You want to change this memory flow along the way and this change is controlled by two valves.\n\nThe first valve is called the forget valve. If you shut it, no old memory will be kept. If you fully open this valve, all old memory will pass through.\n\nThe second valve is the new memory valve. New memory will come in through a T shaped joint like above and merge with the old memory. Exactly how much new memory should come in is controlled by the second valve.\n\nOn the LSTM diagram, the top \u201cpipe\u201d is the memory pipe. The input is the old memory (a vector). The first cross \u2716 it passes through is the forget valve. It is actually an element-wise multiplication operation. So if you multiply the old memory C_t-1 with a vector that is close to 0, that means you want to forget most of the old memory. You let the old memory goes through, if your forget valve equals 1.\n\nThen the second operation the memory flow will go through is this + operator. This operator means piece-wise summation. It resembles the T shape joint pipe. New memory and the old memory will merge by this operation. How much new memory should be added to the old memory is controlled by another valve, the \u2716 below the + sign.\n\nAfter these two operations, you have the old memory C_t-1 changed to the new memory C_t.\n\nNow lets look at the valves. The first one is called the forget valve. It is controlled by a simple one layer neural network. The inputs of the neural network is h_t-1, the output of the previous LSTM block, X_t, the input for the current LSTM block, C_t-1, the memory of the previous block and finally a bias vector b_0. This neural network has a sigmoid function as activation, and it\u2019s output vector is the forget valve, which will applied to the old memory C_t-1 by element-wise multiplication.\n\nNow the second valve is called the new memory valve. Again, it is a one layer simple neural network that takes the same inputs as the forget valve. This valve controls how much the new memory should influence the old memory.\n\nThe new memory itself, however is generated by another neural network. It is also a one layer network, but uses tanh as the activation function. The output of this network will element-wise multiple the new memory valve, and add to the old memory to form the new memory.\n\nThese two \u2716 signs are the forget valve and the new memory valve.\n\nAnd finally, we need to generate the output for this LSTM unit. This step has an output valve that is controlled by the new memory, the previous output h_t-1, the input X_t and a bias vector. This valve controls how much new memory should output to the next LSTM unit.\n\nThe above diagram is inspired by Christopher\u2019s blog post. But most of the time, you will see a diagram like below. The major difference between the two variations is that the following diagram doesn\u2019t treat the memory unit C as an input to the unit. Instead, it treats it as an internal thing \u201cCell\u201d.\n\nI like the Christopher\u2019s diagram, in that it explicitly shows how this memory C gets passed from the previous unit to the next. But in the following image, you can\u2019t easily see that C_t-1 is actually from the previous unit. and C_t is part of the output.\n\nThe second reason I don\u2019t like the following diagram is that the computation you perform within the unit should be ordered, but you can\u2019t see it clearly from the following diagram. For example to calculate the output of this unit, you need to have C_t, the new memory ready. Therefore, the first step should be evaluating C_t.\n\nThe following diagram tries to represent this \u201cdelay\u201d or \u201corder\u201d with dash lines and solid lines (there are errors in this picture). Dash lines means the old memory, which is available at the beginning. Some solid lines means the new memory. Operations require the new memory have to wait until C_t is available.\n\nBut these two diagrams are essentially the same. Here, I want to use the same symbols and colors of the first diagram to redraw the above diagram:\n\nThis is the forget gate (valve) that shuts the old memory:\n\nThis is the new memory valve and the new memory:\n\nThese are the two valves and the element-wise summation to merge the old memory and the new memory to form C_t (in green, flows back to the big \u201cCell\u201d):\n\nThis is the output valve and output of the LSTM unit:", 
        "title": "Understanding LSTM and its diagrams \u2013 ML Review \u2013"
    }, 
    {
        "url": "https://medium.com/@isaac.perez.moncho/deep-learning-as-a-commodity-ad57e4edf465?source=tag_archive---------1----------------", 
        "text": "A couple of months ago I thought AI and Deep Learning were something for universities, big corporations and teams with deep mathematical knowledge.\n\nThis may still be true for cases like AlphaGo, but Deep Learning is being used in many fields and use cases.\n\nSo much that it\u2019s likely that most of the recommendations you get in your daily digital day come from some sort of AI, Machine Learning or Deep Learning program.\n\nAnd it\u2019s not only Google, Facebook, Amazon and the likes who are using it.\n\nI was surprised to find that there are a few ready to use APIs and programs that make the use of Deep Learning and Neural Networks a relatively easy thing to do.\n\nI found two which looked easy enough to use, Prediction.io and DeepDetect.\n\nAt The Ethical Monkey we have the very common problem of product classification, as the websites we crawl have all sorts of categories and categorisations, when they have any.\n\nStandardising the categories into our set of categories caused a lot of effort and up until now we were using a part of the sites\u2019 spider we used, which was custom for a given site, hardly any shared component with the spiders from other sites.\n\nI decided to give DeepDetect a try, it seemed easy enough, and I didn\u2019t have much to loose.\n\nThe results were shocking, to me.\n\nI didn\u2019t know much about Deep Learning, other than what I saw in the news and a bit more about the workings behind. So I used the default example in the website for a text categorisation solution.\n\nAfter having it up and running, I copied/pasted the defaults to my training set, started the training and got the results. So far so good, it takes a few minutes to set up in Ubuntu, so with minimal Linux knowledge you are up and running. And keep this in mind: With minimal Linux knowledge, no maths, no AI, no nothing else. Just the ability to install a package in Linux.\n\nFrom the results, there were a few things that shocked me:\n\nSo not only that neural network got a really high accuracy with some less than optimal parameters, it also found our errors and made clear that some categories made no sense.\n\nI\u2019m still shocked and can\u2019t wait to train it again with the new categories and no products in wrong places.\n\nThere are two take aways from this, one company related, one personal:\n\nAI it\u2019s not the future, it\u2019s the present. Maybe general Jarvis like AI is the future, but narrow AI is definitively not.\n\nIt\u2019s here, it\u2019s being used everywhere and it\u2019s going to get better and better much faster that most people predict.\n\nHopefully it will end as Jarvis and not Skynet, but knowing the human race I wouldn\u2019t bet on it.", 
        "title": "Deep learning as a commodity \u2013 Isaac Perez \u2013"
    }, 
    {
        "url": "https://medium.com/@StephenMartindale/alphago-vs-lee-se-dol-game-four-a0604ce0241c?source=tag_archive---------2----------------", 
        "text": "Lee Se-dol obliterated Google DeepMind\u2019s AlphaGo with an inspired \u2018wedge\u2019 in this morning\u2019s game, the fourth game of the challenge match between the 9 dan professional human player and the upstart A.I.\n\nAbsolutely everyone showered praise on Lee Se-dol for his fantastic play, among them, Gu Li, a 9 dan professional player from China who is considered to be one of the strongest professional players on the planet and particularly relevant because of the international rivalry that exists between him and our champion facing DeepMind\u2019s engine, today.\n\nAfter the game, Lee Se-dol was finally ready to tell us where the weaknesses lie in this version 18 of the distributed AlphaGo programme. He said that the A.I. struggled, holding the black stones, and said that \u201csurprises\u201d like his skilful wedge in the centre forced \u201cbugs\u201d to show in the bot\u2019s play.\n\nHe even put his money where his mouth is by explicitly requesting to play as black in Tuesday\u2019s match. Presumably, now that he has identified a vulnerability, he intends to show the world that he can defeat AlphaGo even when it is playing its preferred role in the game.\n\nDuring the press conference that followed the game, one reporter raised concerns that AlphaGo\u2019s database of professional game records equipped it with extensive knowledge of Lee Se-dol but, to his disadvantage, the man knew almost nothing about the machine. He dubbed this imbalance \u201cinformation asymmetry\u201d and, in response to his question, Demis Hassabis, one of the founders of Google DeepMind, made some intriguing statements. Firstly, Mr Hassabis stated that AlphaGo\u2019s training database did not contain any game records from professional games played by Lee Se-dol. He said that its database was populated with amateur dan-level games only and, from there, AlphaGo had trained by playing against itself. He pointed out that even a thousand records of real-world games would be insignificant amongst the millions of records created by self-play. What he says makes sense but I do wonder why they didn\u2019t prime the system with professional game records in addition to amateur ones\u200a\u2014\u200ait seems like an easy thing to do.\n\nIn the first three games of the challenge match, AlphaGo exhibited a high level of skill and mostly played moves that made sense, even to us humans. It earned the respect of its opponent and the Go community in general. Lee Se-dol said it was not unreasonable; I was among those who praised it for its human-like play. When it was ahead, it played moves that could be considered sub-optimal but even those were not absurd. Today, things were very different.\n\nEven after Lee\u2019s wedge, all was not lost, but AlphaGo sealed its fate by playing appallingly once behind on the board\u200a\u2014\u200amany of its moves were simply ludicrous\u200a\u2014\u200aand spurned its hard-won good-will by charging blindly onwards when its situation had become hopeless and the only responsible act was prompt and polite resignation. Both of these behaviours are familiar to anyone who experienced the laughable death-throes of the Monte-Carlo Tree-Search Go engines and, unfortunately, today\u2019s performance presented an ugly glimpse of AlphaGo\u2019s pedigree.\n\nYesterday, we saw proof that AlphaGo has conquered the mystical art of Ko and that assures me that, one day soon, these bots will also be able to fight to the fore after falling behind and learn to resign with dignity but, for now, DeepMind\u2019s \u2018prototype\u2019 still needs work.", 
        "title": "AlphaGo vs. Lee Se-dol: Game Four \u2013 Stephen Martindale \u2013"
    }
]