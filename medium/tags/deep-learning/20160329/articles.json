[
    {
        "url": "https://medium.com/@arnabghosh93/deep-reinforcement-learning-driving-alphago-9881f1a90be4?source=tag_archive---------0----------------", 
        "text": "Deep Reinforcement Learning has been one of the coolest concepts that have come about in the recent past.\n\nIt was this paper by DeepMind that introduced this concept and made the computer learn playing Atari games just by starting off afresh and learning by making errors and slowly getting better after seeing the rewards.\n\nQ-Learning works by learning a state-action table Q(S,A)\u00a0. For every State S, and action A it tries to memorize the maximum reward at the current state and taking the action A from this state S.\n\nThis table Q(S,A) is learnt for several steps at the beginning to get an idea about the terrain and the different positions at the table.The algorithm starts as a toddler and after a few games, based on the rewards it receives, the algorithm learns favorable actions at the various states, thus getting slowly better.\n\nIn the Deep Q-Learning framework\u00a0, instead of the table Q(S,A) they have a neural network\u00a0, possibly multilayered parameterized by theta.\n\nThe crux of Deep Q Learning is that the state, action pair is encoded into a vector and is passed through the Multilayered network and the output of the network is the estimated Q Value. In this manner\u00a0, at every state all possible actions are considered and the best action is selected based on the maximum Q value from the Network.\n\nThe technique known as experience replay is employed in which each experience at each time step is pooled over many episodes called a replay memory. During the training, the replay memory is sampled and is used for training the Multilayered Neural Network using Gradient Descent.\n\nThe Reinforcement Learning framework is the gold standard that we are trying to achieve in terms of Artificial Intelligence, an artificially intelligent agent that learns on its own on a new environment based on the feedback that it gets from the environment. This is exactly how humans and other animals learn from our mistakes and failures.\n\nI believe Deep-Q-Learning is another step forward in this direction using a neural network to predict the estimated Q-Value of any State-Action pair\u00a0, just analogous to how us humans evaluate all the possible steps at a particular stage.If this can be generalized to more generalized tasks then systems such as Jarvis won\u2019t be far into the future.", 
        "title": "Deep Reinforcement Learning \u2014 Driving AlphaGo \u2013 Arnab Ghosh \u2013"
    }, 
    {
        "url": "https://medium.com/planet-stories/mapping-the-peruvian-amazon-with-maap-8a6319356c23?source=tag_archive---------1----------------", 
        "text": "Deep in the Peruvian Amazon, things are changing\u2026quickly.\n\nPlanet Labs recently imaged the leading edge of a large mine in Southern Peru, with the camp of workers plainly visible. But when we showed the imagery to our colleague, Matt Finer, from the Amazon Conservation Association, he looked south. Finer recognized immediately that the activity below the main stem of the river was illegal encroachment into Tambopata National Reserve.\n\nHow? He\u2019d seen it just a few weeks before, in one of his own deforestation alerts.\n\nFiner operates \u201cMAAP\u201d\u200a\u2014\u200aMonitoring of the Andean Amazon Project. Supported by the Amazon Conservation Association and other partners, MAAP tracks deforestation and forest degradation throughout Peru, where the Andean foothills give way to the vast Amazon basin. After Brazil, Peru holds more of the Amazon forest than any other nation. With a recent pledge to achieve zero net deforestation, Peru has made progress\u200a\u2014\u200abut the work of halting deforestation is enormous. By tracking deforestation in satellite and other imagery, MAAP helps serve the most current knowledge of forest loss in Peru.\n\nAs a Planet Ambassador, Finer can add Planet\u2019s unique dataset to his toolkit. Already, deforestation outside of Iberia\u200a\u2014\u200aa town on the Interoceanic Highway\u200a\u2014\u200ahas been observed.\n\nWhile analyses of forest loss are many, one of the things that helps MAAP alerts rise above the fray is the insights provided by Finer and his team. Like Finer, many of his staff have intimate knowledge of the forest landscapes of Peru, where many have worked as researches and conservationists for years. Thus, while they produce maps of forest change of all sorts, the MAAP team specializes in breaking patterns down into straightforward analyses and insights that can readily be ingested by law enforcement, resource management professionals and the policy process in Peru.\n\nFor more on MAAP, check out this Mongabay profile. And for more Planet imagery of Peru\u2019s spectacularly diverse forests and the threats they face\u200a\u2014\u200astay tuned to Finer and company at maaproject.org.", 
        "title": "Mapping the Peruvian Amazon, with MAAP \u2013 Planet Stories \u2013"
    }, 
    {
        "url": "https://medium.com/@4quant/making-television-searchable-with-deep-learning-and-big-image-analytics-747236c57486?source=tag_archive---------2----------------", 
        "text": "Television produces image data at an astounding rate. Netflix alone has over 3 billion frames in their catalog. Looking at each second would take over 2000 years to analyze. While some of these items are tagged with metadata like show name, language, and subtitles, no information is currently available on the content of these images. This dark data is difficult, expensive and time-consuming to process since it involves manual analysis.\n\nContent-based search is actually a surprisingly common idea. When you use a search engine and look for self-driving cars, you would be quite disappointed if it only returned pages with titles that matched exactly that title. You want to find all the pages whose content matches that idea:\n\nWe do the same for videos and television. If you search for tie, you don\u2019t just want to find programs with Tie in the name, you probably want to find someone wearing a bow-tie a Windsor Tie, advertisements for ties, and the like.\n\nThis means that you can search every instant of every video and find every occurrence (shown as a dot below) as well as it\u2019s significance (shown as the size of the dot). From this it is just a single click to load the exact scene and watch it in context.\n\nTake a look at a simple demo video\n\nLooking for the word tie is easy, looking for a tie is much trickier. To do this, every second of the videos need to be carefully analyzed and quantified. To do this we use the combination of Big Image Analytics and Deep Learning.\n\nTo understand Big Image Analytics it is first important to cover the process it is replacing, specifically, Small Image Analytics or the Small Data Approach. For this approach every image is loaded, analyzed, and stored one at a time on one machine. This involves lots of clicking, lots of data management and is very time consuming. It works well for really small datasets but does not scale.\n\nThe easiest way to image Big Image Analytics is a megaphone. Here you use a megaphone to instruct dozens, hundreds, even thousands of computers to process and analyze the data in parallel. The computers and software figure out how to divide, load, and store the data so that you can focus on the task at hand. This approach means you can elastically scale up and down as needed and do not have to worry about all of the minor details or communication.\n\nNow that we can process such massive amounts of data quickly the question becomes how. Computers struggle to understand images since they see an image as a huge list of numbers and not as coherent structures.\n\nDeep Learning, recently popularized by DeepMind (now part of Google) and IBM Watson, takes hints from our biology to allow for very complex representations and layers of abstraction. The human brain is made up of a billions of neurons organized in layers which have specialized functions and tasks.\n\nThis structure enables a computer to make the leap from pixels to more complicated structures and understanding of images. The term itself refers to any network with more than 3 layers but can involves much more complicated systems. On the right side you see how deep such networks can become.\n\nIn general more complicated networks are better suited for mote difficult tasks, but they have their limits. Because training is such a time consuming process and larger networks require much more data to train properly, choosing the network size is not a trivial decision. As in many areas, Occam\u2019s Razor comes into play and the simplest network that can solve the problem is usually the best, but knowing or finding the simplest isn\u2019t always easy.", 
        "title": "Making Television Searchable with Deep Learning and Big Image Analytics"
    }, 
    {
        "url": "https://medium.com/@thomaszheng/on-nets-trees-and-self-improvement-58639d06d6eb?source=tag_archive---------3----------------", 
        "text": "Earlier this month, the world was abuzz with astonishment when a computer program called AlphaGo won the first game against a 7-time world champion, a master Go prodigy, Lee Sedol, in a best of five series match.\n\nA week later, AlphaGo won the match 4 to 1. It must have been a humiliating defeat for Lee Sedol as he had also predicted a 4 to 1 outcome, but with a different victor before the match. At the conclusion of the match, he uttered: \u201cI don\u2019t feel this was a loss for human beings. It showed my weaknesses, not the weaknesses of humanity.\u201d It seems that hubris and humility have both remained unchallenged human capabilities.\n\nWhile a lot of people are ambivalent about this outcome and its implication for humanity, many are more intrigued by the technology and engineering behind AlphaGo. I, for one, think we can learn a lot from taking a closer look at the three building blocks of AlphaGo.\n\nNeural nets were invented decades ago. It could do rather interesting things like classifying numerical digits. It had created more excitement than actual solutions, so it eventually receded to its own little pocket of influence, aka academic conferences.\n\nFast forward a few decades, neural nets are experiencing a kind of renaissance of late. As it turns out, neural nets can do rather amazing things if there are enough data to feed it. The most recent manifestation called the deep neural nets have been making all the headlines. Since 2012, some manifestation of a deep neural network has consistently won the top spot in the ImageNet competition. So what is the magic? Well, it is really a confluence of two factors, the first of which is a direct consequence of the Moore\u2019s law. Processor design and architecture are both immensely interesting topics, but will be distraction for our discussion here.\n\nThe second crucial factor is what I call gratuitous big data that is actually pretty costly to obtain. ImageNet is a non-profit organization which has served as a data compendium for images. Every year, they organize what they call a Large Scale Visual Recognition Challenge (ILSVR), in which they provide image data to teams around the world to compete. However, not all images are created equal from a computer point of view; only those which can be accurately labeled by a human is of value for the computers as far as classification is concerned. This is where things get interesting: in order for the deep neural nets or any machine learning methods to classify images as well as a human can, the neural nets need many millions of hand-labeled images.\n\nFor AlphaGo, it is no different: it needs quality labeled data. Fortunately, there are about 50 million recorded Go games which are available for AlphaGo to churn through. On the other hand, human players can perhaps play about thirty thousand games at the maximum, assuming one is incredibly gifted and plays one Go game everyday for his entire life. In other words, AlphaGo is not very efficient with the data at its disposal. That is one of the critiques levied at neural nets. That is not to mention the number of man-hours it took to prepare the data from 50 million games. I would go so far as to say:\n\nIf you were a computer scientist, you would probably be fascinated with trees because it is one of the most powerful data structures ever invented by men. DeepBlue was the first computer program that used the tree to analyze the state space of a chess board and search for an optimal move. This relatively simple data structure managed to beat the then Chess master, Gary Kasparov, some two decades ago. A lot has improved since then. These days, the tree used to play the game of Go has a closer resemblance to a real tree in the sense that the branches are much more numerous and each branch can have different number of sub-branches (just like the picture below).\n\nThis latest tree-based search method is called Monte-Carlo Tree Search (MCTS). As the name suggests, it is a simulation based method. It is interesting to talk about how this tree grows its branches via simulation. In computational game theory, there is this concept called regret. As the word implies, it takes into account the cost of not trying other options. Intuitively, one wants to minimize regret when faced with multiple options, that is, pick the option that will yield the minimum regret. You might ask: wouldn\u2019t that be nice if you could do it in real life? In computer games, that is an almost trivial task because the computer can simulate a billion games in seconds. Yes, it would be nice to have a Monte-Carlo simulator for making important real life decisions. From this, one may safely conclude:\n\nLast but not least, in order to win any games, AlphaGo needs to be able to improve its capacity as a Go player. How does it do that? AlphaGo can get online and play real life Go games on the internet, but that would literally take a life time to improve its game because it will at best be learning at the human speed. Fortunately, AlphaGo is a program and can be made to play a Go game against itself, or a slight different version of itself.\n\nThis method of self-play is an old idea. More than three decades ago, Gerald Tesauro from IBM built a computer Backgammon player, called TD-Gammon. Nobody had thought it could achieve anything more than an intermediate-level player. As it turned out, after playing against itself for over a million Backgammon games and improved on it after each game, it became just about as good as the best human Backgammon players.\n\nAlphaGo took this idea and scaled it a million times. After all, Moore\u2019s law for the past four or five decades have ensured that everything will get faster and cheaper every 18 months, though the economics of Moore\u2019s law is coming to an end. Faster self-playing aside, this strategy of improving game playing programs is another peculiarity of building strong computer game players. Any computer game is a simulation which can be re-programmed to play against oneself. One more observation for the day:\n\nIn a way, today\u2019s AlphaGo is like the Aristotle from millenniums ago. He might have been the smartest man on the planet, but he was really useless on his own and he needed an army of servants to do his bidding. In the end, the best thing we got out of his wisdom was propositional logic. While we don\u2019t yet know what the best thing we could get out of the AlphaGo at this point, we already know there would be no shortage of people to do his bidding. Beating a master Go player by combining the three powerful AI paradigms: nets, trees, and self-improvement, is a herald of something big and unpredictable in AI. But it seems the economics of AI is still in its infancy.", 
        "title": "On Nets, Trees, and Self-Improvement \u2013 Thomas Zheng \u2013"
    }, 
    {
        "url": "https://medium.com/emergent-future/terminating-tay-a-microsoft-ai-experiment-gone-wrong-73b3071683ff?source=tag_archive---------4----------------", 
        "text": "You Might Have Heard: The Microsoft AI experiment with Tay, their machine learning Twitter bot, ended after a mere 24-hours. The company pulled the plug when she almost immediately turned into a sexist, racist Nazi. Tay was suppose to learn how to communicate like a human by engaging in conversations with Twitter users.\n\n\u201cThis gets to the underlying problem,\u201d Vice argues. \u201cMicrosoft\u2019s AI developers sent Tay to the internet to learn how to be human, but the internet is a terrible place to figure that out.\u201d\n\nThe New Yorker writes that \u201cTay\u2019s breakdown occurred at a moment of enormous promise for A.I.\u201d Earlier in the week, an AI-written novel passed the first round of a literary competition in Japan, and last week AlphaGo, the AI from Google\u2019s DeepMind,defeated the top-ranked Go player in the world.\n\nAs information destined for humans is increasingly handled by AI\u2019s, the need for an open dialogue about the ethics grows. Google and DeepMind still haven\u2019t revealed who sits on their AI ethics board.\n\n+ A question of lesser importance: why are AI\u2019s like Siri and Cortana so clever, but so bad at empathy anyway? A recent study might hold the key.\n\nDid you enjoy this? Consider joining Emergent Future, a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Emergent Future is powered by Algorithmia, an open marketplace for algorithms, enabling developers to create tomorrow\u2019s smart applications today", 
        "title": "Terminating Tay \u2014 A Microsoft AI Experiment Gone Wrong"
    }, 
    {
        "url": "https://engineering.hexacta.com/tensorflow-the-open-library-for-deep-learning-94684466d3a8?source=tag_archive---------5----------------", 
        "text": "After AlphaGo became the first IA to beat a professional player in the GO game and having also beaten the world\u2019s champion in the last days, it seems as if the issue of \u201cartificial neural networks\u201d regained relevance.\n\nAnd yes, the software used by Google for most artificial intelligence needs was released a few months ago under the Apache 2.0 license and is now available for download and use by anyone (students, researchers, hackers, engineers, developers and many others).\n\nIts name is TensorFlow, it is coded through a Python interface or C / C ++, and can run in various environments: In multi CPUs, in video plates, in cloud servers or on mobile devices with Android and IOS.\n\nIn this blog, the intention is not to enter into the code of it, but to rather make a theoretical introduction to such a deep topic that the expression deep learning will remain short to describe it. If what is sought is code, you can always consult TensorFlow\u2019s quick guide on their official website.\n\nArtificial learning! One of the most novel and unexplored areas of computer science. However, there are already some software products that can do this sort of thing: Caffe, Deeplearnig4j, OpenNN and Torch. In any case, if you still do not know where we stand, here is a list of concepts to help us come into topic:\n\nWhile Google has used deep learning almost since its beginning with technologies such as prediction API and DistBelief (TensorFlow\u2019s previous generation) now this renowned software library is used in many popular applications (and where integration is so natural that we forget it exists).\n\nHere are some examples:\n\nTensorFlow represents information as a multidimensional array (not very surprisingly called tensor). In this arrangement, the data available overturns and usually there is a dimension reserved for the number of samples with which it will train. So, if we have for example 55,000 digits written by hand to recognize images of 28\u00d728 pixels, then we fabricate an array of 55000x28x28.\n\nThe necessary neural layers are then deployed to solve the problem and operations to be performed are determined. All these operations can be viewed using TensorBoard, a tool that provides a visual interface. The more layers, the greater the need for processing; that is why each section of the graph can run on different processing units.\n\n\u00a0After the IA has been trained, you can get obtain very interesting displays such as the ones in following boxes that show the evidence for (blue) and against (red) that the line representing the digit shown:\n\nAnd here is an example of how the source code would look like:", 
        "title": "TensorFlow: The open library for deep learning \u2013"
    }
]