[
    {
        "url": "https://chatbotslife.com/resnets-highwaynets-and-densenets-oh-my-9bb15918ee32?source=tag_archive---------0----------------", 
        "text": "When it comes to neural network design, the trend in the past few years has pointed in one direction: deeper. Whereas the state of the art only a few years ago consisted of networks which were roughly twelve layers deep, it is now not surprising to come across networks which are hundreds of layers deep. This move hasn\u2019t just consisted of greater depth for depths sake. For many applications, the most prominent of which being object classification, the deeper the neural network, the better the performance. That is, provided they can be properly trained! In this post I would like to walk through the logic behind three recent deep learning architectures: ResNet, HighwayNet, and DenseNet. Each make it more possible to successfully trainable deep networks by overcoming the limitations of traditional network design. I will also be providing Tensorflow code to easily implement each of these networks. If you\u2019d just like the code, you can find that here. Otherwise, read on!\n\nThe first intuition when designing a deep network may be to simply stack many of the typical building blocks such as convolutional or fully-connected layers together. This works to a point, but performance quickly diminishes the deeper a traditional network becomes. The issue arises from the way in which neural networks are trained through backpropogation. When a network is being trained, a gradient signal must be propagated backwards through the network from the top layer all the way down to the bottom most layer in order to ensure that the network updates itself appropriately. With a traditional network this gradient becomes slightly diminished as it passes through each layer of the network. For a network with just a few layers, this isn\u2019t an issue. For a network with more than a couple dozen layers however, the signal essentially disappears by the time it reaches the beginning of the network again.\n\nSo the problem is to design a network in which the gradient can more easily reach all the layers of a network which might be dozens, or even hundreds of layers deep. This is the goal behind the following state of the art architectures: ResNets, HighwayNets, and DenseNets.\n\nA Residual Network, or ResNet is a neural network architecture which solves the problem of vanishing gradients in the simplest way possible. If there is trouble sending the gradient signal backwards, why not provide the network with a shortcut at each layer to make things happen more smoothly? In a traditional network the activation at a layer is defined as follows:\n\nWhere f(x) is our convolution, matrix multiplication, or batch normalization, etc. When the signal is sent backwards, the gradient always must pass through f(x), which can cause trouble due to the nonlinearities which are involved. Instead, at each layer the ResNet implements:\n\nThe \u201c+ x\u201d at the end is the shortcut. It allows the gradient to pass backwards directly. By stacking these layers, the gradient could theoretically \u201cskip\u201d over all the intermediate layers and reach the bottom without being diminished.\n\nWhile this is the intuition, the actual implementation is a little more complex. In the latest incarnation of ResNets, f(x) + x takes the form:\n\nWith Tensorflow we can implement a network composed of these Residual units as follows:\n\nThe second architecture I\u2019d like to introduce is the Highway Network. It builds on the ResNet in a pretty intuitive way. The Highway Network preserves the shortcuts introduced in the ResNet, but augments them with a learnable parameter to determine to what extent each layer should be a skip connection or a nonlinear connection. Layers in a Highway Network are defined as follows:\n\nIn this equation we can see an outline of the previous two kinds of layers discussed: y = H(x,Wh) mirrors our traditional layer, and y = H(x,Wh) + x mirrors our residual unit. What is new is the T(x,Wt) function. This serves at the switch to determine to what extent information should be sent through the primary pathway or the skip pathway. By using T and (1-T) for each of the two pathways, the activation must always sum to 1. We can implement this in Tensorflow as follows:\n\nFinally I want to introduce the Dense Network, or DenseNet. You might say that is architecture takes the insights of the skip connection to the extreme. The idea here is that if connecting a skip connection from the previous layer improves performance, why not connect every layer to every other layer? That way there is always a direct route for the information backwards through the network.\n\nInstead of using an addition however, the DenseNet relies on stacking of layers. Mathematically this looks like:\n\nThis architecture makes intuitive sense in both the feedforward and feed backward settings. In the feed-forward setting, a task may benefit from being able to get low-level feature activations in addition to high level feature activations. In classifying objects for example, a lower layer of the network may determine edges in an image, whereas a higher layer would determine larger-scale features such as presence of faces. There may be cases where being able to use information about edges can help in determining the correct object in a complex scene. In the backwards case, having all the layers connected allows us to quickly send gradients to their respective places in the network easily.\n\nWhen implementing DenseNets, we can\u2019t just connected everything though. Only layers with the same height and width can be stacked. So we instead densely stack a set of convolutional layers, then apply a striding or pooling layer, then densely stack another set of convolutional layers, etc. This can be implemented in Tensorflow as follows:\n\nAll of these network can be trained to classify images using the CIFAR10 dataset, and can perform well with dozens of layers where a traditional neural network fails. With little parameter tuning I was able to get them to perform above 90% accuracy on a test set after only an hour or so. The full code for training each of these models, and comparing them to a traditional networks is available here. I hope this walkthrough has been a helpful introduction to the world of really deep neural networks!", 
        "title": "ResNets, HighwayNets, and DenseNets, Oh My! \u2013"
    }, 
    {
        "url": "https://gab41.lab41.org/speech-recognition-you-down-with-ctc-8d3b558943f0?source=tag_archive---------1----------------", 
        "text": "You may have heard that speech recognition nowadays does away with everything that\u2019s not a neural network. \u201cNot a neural network\u201d might be a matter of semantics, but much of that philosophy comes from a cost function called the CTC loss function. You see, in the old days, everyone who wanted to train neural networks used a two step procedure: (1) initialize by pre-training your neural network and (2) fine-tune your algorithm through backpropagation. Rather, the CTC loss function operates under the assumption that we don\u2019t need to do the first pre-training step. And this small, little cost function can do this because of a big thing called data.\n\nIn fact, adoption of a pure neural network approach with CTC is widespread. It\u2019s what the rich kids (Baidu, Google, and Microsoft) and poor kids (University graduate students) use. Perhaps you\u2019ve heard of CTC, but chances are you still probably didn\u2019t know what the acronym stood for (it\u2019s Connectionist Temporal Classification.)\n\nIf you fall into the latter group, the beginner-intermediate category of practitioners in deep learning, you might find this blog post worth reading. This blog post is meant to guide you with a brief introduction to and some intuition behind modern speech recognition solutions for the masses. Along the way, hopefully you\u2019ll also start to understand how the CTC loss function works.\n\nTypical speech processing approaches use a deep learning component (either a CNN or an RNN) followed by a mechanism to ensure that there\u2019s consistency in time (traditionally an HMM).\n\nSince it\u2019s really hard to predict what\u2019s in a 10ms frame without context, this second time consistency component is necessary. Its job is to inform our predictions based on previous or future frames. It is this component that the community has been making the wholesale change from Hidden Markov Models (HMMs) over to Recurrent Neural Networks (RNNs).\n\nBehind the face of RNNs lies a peculiar cost function called the Connectionist Temporal Classification (CTC) loss, initially presented at ICML in 2006 by Alex Graves and Co. This is quite significant since the HMM (the RNN & CTC predecessor) has been used for speech processing since forever, before and even after neural networks got hot. What does the CTC loss function do? We\u2019ll get to that, but first we need to know why we need it.\n\nLet\u2019s take a closer look Diagram 1 with a blow-up of the neural network in Diagram 2. At each time step, a neural network predicts an output we call a vector y. Each one of the dimensions of y is an indicator of how much we believe in an utterance given an audio frame. For example, let\u2019s say the 4th dimension of output vector y (denoted by y\u2084) represents the phoneme \u201c/y/\u201d.\n\nIf y\u2084 is a value 0.84 and all the other y\u2019s are small (like 0.02), my neural network is pretty sure I\u2019m saying \u201c/y/\u201d. I\u2019m essentially predicting my outputs given my inputs: p( y | x ) = Neural-Network( x ), where x is my input frame.\n\nNothing new, just neural networks 101. But wait, maybe the word I\u2019m saying is, \u201chuge\u201d, and that phoneme was just found near the \u201c/y/\u201d and should really be \u201c/h/\u201d. Unless I\u2019m Donald Trump (I\u2019m not) the \u201c/y/\u201d phoneme is incorrect. In this case, context actually buys us a lot.\n\nRather than predict each frame at a time, what would be more informative is we predict the sequence of outputs that makes the most sense {y[0], y[1],\u00a0\u2026, y[T]}, where y[t] is a vector with all possible phonemes at time t. So let\u2019s line up the outputs from Diagram 2 in a row over five time steps, shown in Diagram 3. The connected line through nodes at each time step is called a path, and is often represented by the greek symbol \u03c0.\n\nThe problem is then to optimize for the best path \u03c0. In math, you\u2019re optimizing the below equation.\n\nThat\u2019s just a compact way of saying, the probability of a particular path is the product of all the softmax outputs over time T. Now, you might be wondering, doesn\u2019t recurrence or convolutions take care of that? Just by using a CNN or RNN, you\u2019re already considering past outputs, right?\n\nTrue, but you might also notice in Diagram 3, \u201c/oo/\u201d appears twice, and doesn\u2019t contribute to the meaning of the phrase. That\u2019s important as rate of speech should be discounted when recognizing words. Instead of \u201c_-h-yoo-oo-oo-j-_\u201d, we really want \u201c_-h-yoo-oo-j-_\u201d with the second \u201coo\u201d deleted. This is especially noticeable when we consider that silence/blank/repeat /transition characters actually make up a lot of speech. In fact, we\u2019ll often have something that looks like:\n\nwhich in reality, we should perceive as \u201cbe\u201d.\n\nThere are two things you can do about this.\n\nThe first option is unpalatable; I\u2019m okay with a little bit of annotation, but this one\u2019s going to cut into my nights trying to figure out who shot Longmire at the beginning of season 5 (I think it was the psychiatrist.) The second option is where the CTC loss comes in. Instead of just optimizing for the path, \u03c0, also optimize to the label. Let\u2019s call that label l. Now, we\u2019re going to predict the best label among all paths, the CTC objective:\n\nThe above equation is really just saying, the probability that I\u2019ve said label l is just the probability over the sum of all the possible ways to get there over all paths (i.e., marginalizing out \u03c0.) If you want to be fancy, you\u2019d call this step dynamic time warping. If not, just call it getting rid of crap that\u2019s repetitive.\n\nIt can\u2019t be that simple, right? Actually, according to scientists at Baidu, it really is. They are literally predicting Mandarin characters (the labels l) directly from the input audio sequence.\n\nOkay, so now we know the problem (i.e., the CTC objective). What\u2019s the solution? (i.e., how do we optimize?) As in the case of HMM\u2019s, we could use dynamic programming. In fact, we can optimize using the analogous forward-backward algorithm.\n\nEach row in Diagram 4 is a label interspersed with \u201cblanks\u201d or \u201ctransitions\u201d, where the entire word is \u201cCAT\u201d. Each column is the time when it\u2019s uttered. At each time step (again, each column), we\u2019re picking which label could go next, which is demonstrated by the arrows. It could be one of three things:\n\nWell, how do I choose which one of these things to do? That\u2019s where the forward-backward algorithm comes in.\n\nThere\u2019s some math, but the basic gist of the forward backward algorithm is you\u2019re calculating \u201cp(l|x)\u201d with some helper functions \u201c\u03b1(t)\u201d and \u201c\u03b2(t)\u201d. Going forward, you calculate a parameter \u201c\u03b1(t)\u201d. Going backwards you calculate \u201c\u03b2(t)\u201d, and the probability of a label given your input is:\n\nBoth \u201c\u03b1(t)\u201d and \u201c\u03b2(t)\u201d are calculated with an input of the two previous equations in this blog (the neural network output y), but you can get them from Graves\u2019 paper. To choose which label comes next, you\u2019re calculating \u03b1(t) and \u03b2(t). The maximum of p(l|x) will tell you what phoneme is being uttered. But more importantly, the key to tractability is you are only considering the paths with \u201cgood\u201d labels. In other words, if you\u2019re not a node with an arrow coming in and going out of you in Diagram 4, then you\u2019re not being considered.\n\nBut wait. I just said that \u201c\u03b1(t)\u201d and \u201c\u03b2(t)\u201d depend on the output of the neural network, y. Doesn\u2019t that mean that when I optimize my neural network, \u201c\u03b1(t)\u201dand \u201c\u03b2(t)\u201d will change? Well, yes, but that\u2019s what back propagation is for! We just need to calculate the gradient of \u201cp(l|x)\u201d with respect to its input y, which so happens to be what the neural network gives you up until that point. We can do this from the equation that I just described above, but it\u2019s easier just to take equation 15 from Graves in his paper. I\u2019m not going to include it here because if you\u2019re serious about implementation, you\u2019ll probably read his paper anyway.\u00a0;-)\n\nI\u2019ve skipped a whole bunch of steps, but hopefully that gives you a bit of an overview and some intuition about how the CTC loss works. Since I tried to keep this short, any feedback is welcomed! Please find me at kni@iqt.org or karllab41 on Twitter.", 
        "title": "Speech Recognition: You down with CTC? \u2013"
    }, 
    {
        "url": "https://medium.com/all-of-us-are-belong-to-machines/gentlest-intro-to-tensorflow-part-3-matrices-multi-feature-linear-regression-30a81ebaaa6c?source=tag_archive---------2----------------", 
        "text": "Summary: With concepts of single-feature linear-regression, cost function, gradient descent (from Part 1), epoch, learn-rate, gradient descent variation (from Part 2) under our belt, we are ready to progress to multi-feature linear regression with TensorFlow (TF). If you are already familiar with matrices and multi-feature linear regression, skip to the end for the multi-feature Tensorflow code cheatsheet, or even skip this entire article.\n\nThis is part of a series:\n\nThe premise of the previous articles was: given any house size (square meters/sqm), which is the feature, we want to predict the house price ($), the outcome. To do that we:\n\nIn reality, any prediction relies on multiple features, so we advance from single-feature to 2-feature linear regression; we chose 2 features to keep visualization and comprehension simple, but the concept generalizes to any number of features.\n\nWe introduce a new feature, \u2018Rooms\u2019 (number of units in the house). When collecting datapoints, we must now collect values for the new feature \u2018rooms\u2019 on top of the existing feature \u2018house size\u2019, as well as the corresponding outcome \u2018house price\u2019.\n\nOur goal then becomes predicting \u2018house price\u2019, given \u2018rooms\u2019, and \u2018house size\u2019 (see image below).\n\nIn the single-feature scenario, we had to use linear regression to create a straight line to help us predict the outcome \u2018house size\u2019, for cases where we did not have datapoints. In a 2-feature scenario, we can also employ linear regression, but to create a plane (instead of a straight line) to help us predict (see image below).\n\nRecall for a single-feature (see left of image below), the linear regression model outcome (y) has a weight (W), a placeholder (x) for the \u2018house size\u2019 feature, and a bias (b).\n\nFor 2-feature (see right of image below), we introduce another weight, which we call W2, and another placeholder, x2 to hold the \u2018rooms\u2019 feature value.\n\nWhen we perform linear regression, gradient descent helps us learn the additional weight W2, on top of the learning W, b as previously discussed.\n\nOur TF code for single-feature linear regression consists of 3 parts (see image below):\n\nThe change to support 2-feature linear regression equation (explained above) in TF code is shown in red.\n\nNote this way of adding new features is inefficient; as the number of features grow, the number of required variables and placeholders increases. In reality models have many more features, which worsens this problem. How can we represent features efficiently?\n\nFirst, let us generalize representing a 2-feature model to an n-feature one:\n\nIt turns out that the complex n-feature formula can be simplified in the world of matrices, and matrices are in-built into TF for these reasons:\n\nIn TF, they would be written as:\n\nNOTE: For W we use tf.zeros, which initializes all W1, W2,\u00a0\u2026, Wn to zeros.\n\nIn TF, this multiplication would be:\n\nNote: The x representations in the feature matrix become more complex, i.e., we use x1.1, x1.2, instead of x1, x2, etc. because the feature matrix (the one in the middle) has expanded from representing a single datapoint of n-features (1 row x n columns) to representing m datapoints with n-features (m rows x n columns), so we extended x<n>, e.g., x1, to x<m>.<n>, e.g., x1.1, where n is the feature number and m is the datapoint number.\n\nIn TF, they would be written as:\n\nIn TF, with our x, and W represented in matrices, regardless of the number of features our model has or the number of datapoints we want to handle, it can be simplified to:\n\nWe do a side-by-side comparison to summarize the change from single to multi-feature linear regression:\n\nWe illustrated the concept of multi-feature linear regression, and showed how we extend our model and TF code from single to 2-feature linear regression models, which is generalizable to n-feature models. We conclude by presenting a cheatsheet for multi-feature TF linear regression model.\n\nWe will present the concepts of logistic regression, cross-entropy, and softmax, which will enable us to fully understand Tensorflow\u2019s official beginner\u2019s tutorial on MNIST.", 
        "title": "Gentlest Intro to TensorFlow #3: Matrices & Multi-feature Linear Regression"
    }, 
    {
        "url": "https://gab41.lab41.org/across-the-network-ai-week-in-review-oct-14-a7b0de22f61f?source=tag_archive---------3----------------", 
        "text": "Across the Network\u200a\u2014\u200aAI Week in Review Oct\u00a014\n\nWelcome back to another edition of Across the Network\u200a\u2014\u200aLab41\u2019s weekly look at what is going on in the world of AI. As we have now officially closed out our recent round of challenges, the folks here at the lab have been busy researching what new projects to commence in the upcoming challenge cycle. As such, there have been quite a few interesting resources and links that I pulled from the Lab41 Slack channels.\n\nFully Character-Level Neural Machine Translation without Explicit Segmentation\u200a\u2014\u200aAutomatic machine translation is very difficult, and most translation systems have a lot of preprocessing modules that add complexity to the engineering side of things. Following on a lot of similar efforts to do NLP using raw characters as inputs, this paper presents a deep learning approach to machine translation that takes in raw characters as input and emits characters in the target language. It is remarkable that this works at all, but it also claims to be more robust than state-of-the-art translation systems. It even comes up with reasonable translations of made-up words used in context!\n\n5 Amazing Things Big Data Helps Us To Predict Now\u200a\u2014\u200aPlus What\u2019s On The Horizon\u200a\u2014\u200aIt seems like the Twitterverse was very interested in this quick article. We all know about how Amazon and Netflix make predictions in order to (sometimes) improve user experience, but did you know that school districts are using big data to predict students that may drop out of school? Or that health organizations are beginning to make predictions based on patient health history?\n\nPreparing for the Future of Artificial Intelligence\u200a\u2014\u200aThis White House report surveys the current state of AI, reviews current and potential AI applications, and discusses the impacts that AI will have on the US economy. I\u2019m really interested in the report\u2019s discussion of policies and regulations that encourage innovation all the meanwhile safeguard the public\u200a\u2014\u200aespecially as related to autonomous vehicles! This report goes further to also discuss the potential impact that automation will have on the job market.\n\nEffectively running thousands of experiments: Hyperopt with Sacred\u200a\u2014\u200aOur own Anna B outlines how she and her team approached hyperparameter tuning options for the Pythia project. She describes how they ran hyperopt in order to reduce the optimal hyperparameter search space in an intelligent manner. Check it out!", 
        "title": "Across the Network \u2014 AI Week in Review Oct 14 \u2013"
    }, 
    {
        "url": "https://chatbotslife.com/this-week-in-chatbots-14d1643e18a8?source=tag_archive---------4----------------", 
        "text": "Chatbots have been in the news every single day this year and it\u2019s not always easy to keep track of everything happening in the bot world. Missed the latests news? Here\u2019s a recap of what has been happening this week.\n\nThe language app has introduced a new chatbot, so users can learn a new language, with only a few minutes practice each day. CEO and Co-Founder, As Luis von Ahn, says: \u201cOne of the main reasons people learn languages is to have conversations. Students master vocabulary and comprehension skills with Duolingo, but coming up with things to say in real\u00ad-life situations remains daunting. Bots offer a sophisticated and effective answer to that need.\u201d Read more here.\n\nHaving problems saving some money? Then this might just be the solution for you! Plum\u2019s chatbot monitors and learns spending habits and transfers small amounts of money from users\u2019 current accounts to their savings. Chatbots are showing significant promise in financial services, with applications in personal banking or insurance. Intrigued? Hear more about the bot here.\n\nAfter the passing of her best friend Roman in a car accident last year, Eugenia Kuyda, co-founder of Luka, developed a chatbot that immortalises him through computer science and allows family and friends to continue communicating with him. The digital memorial shows how future chatbots could be used in diverse ways, perhaps in grief counselling and allowing loved ones a way to deal with their loss. As Eugenia says, \u201cSometimes in order to let go you have to come as close as possible.\u201d Read more about the immortalising chatbot here.\n\nChina\u2019s Google-equivalent has launched an artificially intelligence medial assistant, with the aim of helping doctors in collecting patient information, and facilitating a smoother diagnosis. The bot, currently only available in Chinese, reduces the amount of time doctors spend with patients, using a mix of deep learning and real-life online conversation to understand the questions asked and answers given. Read more about Melody here.\n\nRefugee Text is a chatbot that aims to help refugees who are seeking asylum once arrived in Europe. Users engage in conversation with the bot via SMS, the website, Facebook messenger or Telegram, and will receive information on asylum opportunities in Denmark, Sweden and Germany. Three voluntary legal experts based in each country are feeding the bot with the necessary information. Click here to find out more about Refugee Text.", 
        "title": "This Week in Chatbots \u2013"
    }, 
    {
        "url": "https://blog.produvia.com/black-box-problem-and-transparent-decision-making-9e727342085d?source=tag_archive---------5----------------", 
        "text": "I think that model visualizations and explanations should incorporated into every deep learning project in order to address the \u201cblack box\u201d problem and achieve what I call \u201cTransparent Decision-Making\u201d. Black box problem exists because it\u2019s hard to look into a neural network and figure out exactly what it has learnt, while Transparent Decision-Making is necessary to understand how neural network comes up with it\u2019s results.\n\nModel visualizations are a great way to understand how neural networks think. If we can visualize the decision process of a neural network, we can learn along the computers, not just rely on them to provide us the answers.\n\nIf we are interpreting models for the task of Visual Question Answering, we can use visualization methods like guided backpropagation and occlusion, as described in Towards Transparent AI Systems by Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Batra.", 
        "title": "Black Box Problem and Transparent Decision-Making \u2013"
    }, 
    {
        "url": "https://medium.com/@kjanusic/its-almost-2017-and-you-should-really-know-the-importance-of-data-science-and-its-future-4188ffc85033?source=tag_archive---------6----------------", 
        "text": "Data science is a complex blend of several disciplines including technology, algorithm development, and data interference. The basic goal of data science is to solve analytically multiplex problems. As the name suggests, data is at the core of this specific type of science. In its essence data science is all about using different systems and processes, related to the disciplines we\u2019ve mentioned before, in order to extract information, insights or knowledge from data available in different forms. Sometimes this data is unstructured and sometimes it is well-structured. All data possible can be of extreme use, if you find the right way to use it. Learn more about it in the following lines.\n\nJust like many other types of science, data science is not focused on theories and techniques that are exclusively used in one field of knowledge. Namely, it relies on methods and theories from at least five large areas\u200a\u2014\u200acomputer science, information science, operations research, statistics, and mathematics. Data science also uses techniques and methods related to big data and even though this science is close to big data this is not the only field of interest of data science.\n\nData science has an impact on applied and theoretical research in many different areas like speech recognition, machine translation, search engines, robotics, medical informatics, digital economy, finance, economics, businesses, social sciences and health care. If you\u2019re eager to learn more about data science, cyber security or computer science in general, I really recommend you start following Harbour.Space\u2019s blog posts, courses and if possible events.\n\nIn the recent period, data science is often mentioned in the context of services.Although this use of data science is still fresh, it certainly provides good results which are why more and more people are interested in it. Those who have performed some research have probably noticed that there are many companies in the market providing services related to data science, but their definition is not clear.\n\nThere are many definitions of this service, but the simplest one is that data science represents a user-friendly analytics system/program that provides analytics data to managers and business users. The implementation of data science methods usually goes through external software solutions and tools with an ability to automate the process of analysis by relying on data storage from the organization.\n\nProviding an example is the best way to understand how data science works and why is it so useful. For instance, a customer service center has basic systems that allow employees to check the customer\u2019s name, email, phone, address whenever they are calling the center. In this way, the employee can see what this customer has bought in the past and they can skip the explanation in the beginning. However, with the help of tools that rely on data science, employees will be able to get more information about the caller like their return history, the ratings they gave to the company in different surveys, the amount of money they\u2019ve spent on products/services etc. In other words, thanks to data science, the employee will not only figure out what the customer\u2019s problem is, but they will also understand the frame of mind of each customer.\n\nAs previously mentioned data science covers many different areas and relies on different kinds of analysis. One of the processes that have a huge impact on data science issocial media analytics. This specific type of analytics is part of social analytics and represents a process of collecting data that comes as a result of stakeholder interactions conducted on digital media.\n\nOnce the data is gathered, it is processed into well-structured insights that serve as a material that can help business owners and organization leaders make more sound decisions. Even though most people use the term social media analytics, it is not unusual to find this part of social analytics under few other names like social media intelligence, social media monitoring and social media listening. In its pure essence, this is my role at IN2Data and if you\u2019d like to find out more about it; e-mail me at kristijan.janusic AT in2data DOT eu\n\nSocial media analytics uses dozens of different digital media sources to achieve the aforementioned goals. For instance: blogs, social media platforms, image sharing sites, online forums, video sharing platforms, classifieds, aggregators, Q&A, complaints, reviews etc. Social media analytics has proven to be very helpful and useful when users want to find out more about the patterns that are not clearly visible in a big amount of social dataassociated with specific brands. It is good to mention that social media monitoring tools don\u2019t work equally for every company and that\u2019s why it is highly recommended to perform tests before any of these tools is employed.\n\nAccording to many experts, a data scientist is one of the most promising jobs in the 21st century. Yet, many people avoid this profession because they believe that a successful data scientists must have skills and knowledge related to a wide range of fields including data mining, software development, machine learning, statistics, data visualization and databases. However, this doesn\u2019t mean that people must be experts in all these fields in order to practice data science.\n\nThere are a few data skills that every data scientists is expected to know. For example, they must know how to use the so-called tools of the trade like the use of database querying language (SQL) or statistical programming language like Python. In addition, a good data scientist should also have knowledge in basic statistics\u200a\u2014\u200alikelihood estimators, distributions, statistical tests etc. Linear algebra and multivariable calculus are needed regardless of the company where data scientists are applying. The same goes for machine learning.\n\nOf course, those who want to join big corporations where large amounts of data are processed every day or any other company where the activities are data-driven should have a higher level of knowledge in this field.Having knowledge in data communication, data visualization, and software engineering is a big plus and in most cases a must for those looking to start their careers as data scientists.\n\nNever underestimate the knowledge you could gather through Coursera orUdemytoo. Having knowledge in data communication, data visualization, and software engineering is a big plus and in most cases a must for those looking to start their careers as data scientists.\n\nIt is interesting that data scientist jobs are still not clearly defined and many data scientists will learn whether they are qualified or not only after finishing their interviews. Obviously, the more skills you have, the greater chances for employment you\u2019ll get.\n\nIt is very difficult to make predictions for a science that includes so many different disciplines. This means that you have to think about the current trends and perspectives for each of these disciplines. Yet, the past and the current state of data science should serve as a good starting point for making predictions for the future of data science.\n\nSo, in the past and today, data science is focused mostly on descriptive analytics. This means that data science relies on collecting information and describing what happened in the past. However, thanks to the rapid advance of technology, experts expect that in the future,data science will turn to more sophisticated kind of analytics including real-time and predictive analytics. Once again, the business sector will have a huge impact on the look and goals of data science.\n\nIt is also expected that machine learning as one of the basic elements of data science will be greatly changed. Instead of paying most of their attention to mechanics of this learning, data scientists will have to unleash their creativity and use different types of models. Although data scientists have a good level of productivity, if they want to remain competitive in the future they will have to boost their productivity and changing the way they practice machine learning is one way to do this,\n\nFirst of all, we will witness the emergence of new sources of data. The Internet of Things is not something new, but the interconnection between devices that this concept supports will grow in the future leading to connections between different kinds of electronic devices. Today, data scientists are using clickstream data, purchase data and sales data, but in the future they will have to include data gathered from different retail environments, manufacturing streams, offices, vehicles, employees etc.\n\nFurthermore, it is very likely that the tools used by data scientists today will become more advanced making complicated tasks look much simpler. This is actually something that we are already witnessing with so-called BI tools as well as with open-source libraries. Only a decade ago, many of the algorithms had to be created starting from zero. Today there are ready-made codes that can ease this task. With the advance of technology, it is expected that even beginner analysts will be able to perform cross-validation and machine learning on their own.\n\nAnother thing that will most likely happen in the near future is the increased level of cooperation between data scientists and system engineers. The first example of this kind of cooperation has proven to be very beneficial for the overall productivity of the company.\n\nFinally, data scientists will be more focused on two basic tasks. The first one is to prepare input data with the help of the domain and business knowledge they have. The second is analyzing and interpreting the output generated by the tools they use.\n\nMy name is Kristijan Janu\u0161i\u0107; I\u2019m a creative director at Storyboard Agency, Senior Marketing Analyst at IN2Data Science Company and a very proud father to Korina. Same as Ana, she is my forever-muse.", 
        "title": "It\u2019s Almost 2017. and You Should Really Know The Importance of Data Science and its Future"
    }, 
    {
        "url": "https://medium.com/@jayaprime/beautiful-blue-deep-dream-fractal-blizzard-85e2376d19d9?source=tag_archive---------7----------------", 
        "text": "Facts: Winter in the Northern Hemisphere is not cold because of how far our planet is from the sun, but because of the angle of the light that strikes us\u200a\u2014\u200aour planet\u2019s \u201ctilt\u201d. In summer, the light hits us head on, as a spear, then ripples out in photonic waves across the surface. In winter, the light hits us at an angle, and the photonic waves literally splash over the surface\u200a\u2014\u200aas a stone skipping across the top of a lake\u200a\u2014\u200aand then back out into space. Add this to the limited hours of photonic exposure in winter, and you will find that frost soon follows. That does not, however, explain how a warm cup of cocoa can melt even the coldest of nights. That\u2019s just magic.\n\nIncept: I pulled the original hyperbolic fractal from the \u201cStarSeed\u201d poster that many already know from my music, then ran it through a few deep dream filters before adding stars, snowflakes, and lens flare. I originally released it to Reddit, where it made the top of /r/deepdream as a fan favorite.\n\nIn the darkest of seasons, find the light that warms you and take it with you.", 
        "title": "Beautiful Blue Deep Dream Fractal Blizzard \u2013 Jaya Prime \u2013"
    }
]