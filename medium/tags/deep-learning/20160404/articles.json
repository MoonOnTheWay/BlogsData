[
    {
        "url": "https://medium.com/jim-fleming/before-alphago-there-was-td-gammon-13deff866197?source=tag_archive---------0----------------", 
        "text": "Before AlphaGo there was TD-Gammon\n\nTL;DR Introduces temporal difference learning, TD-Lambda / TD-Gammon, and eligibility traces. Check out the Github repo for an implementation of TD-Gammon with TensorFlow.\n\nA few weeks ago AlphaGo won a historic tournament playing the game of Go against Lee Sedol, one of the top Go players in the world. Many people have compared AlphaGo to DeepBlue, which won a series of famous chess matches against Gary Kasparov, but a different comparison may be made for the game of backgammon.\n\nBefore DeepMind tackled playing Atari games or built AlphaGo there was TD-Gammon, the first algorithm to reach an expert level of play in backgammon. Gerald Tesauro published his paper in 1992 describing TD-Gammon as a neural network trained with reinforcement learning. It is referenced in both Atari and AlphaGo research papers and helped set the groundwork for many of the advancements made in the last few years.\n\nTD-Gammon consists of a simple three-layer neural network trained using a reinforcement learning technique known as TD-Lambda or temporal-difference learning with a trace decay parameter lambda (\u03bb). The neural network acts as a \u201cvalue function\u201d which predicts the value, or reward, of a particular state of the game for the current player.\n\nDuring training, the neural network iterates over all possible moves for the current player and evaluates each valid move and the move with the highest value is selected. Because the network evaluates moves for both players, it\u2019s effectively playing against itself. Using TD-Lambda we want to improve the neural network so that it can reasonably predict the most likely outcome of a game from a given board state. It does this by learning to reduce the difference between the value for the next state and the current state.\n\nLet\u2019s start with a loss function, which describes how well the network is performing for any state at time t:\n\nHere we want to minimize the mean squared error of the difference between the next prediction and the current prediction. Basically, we want our predictions about the present to match our predictions about the future. This in itself isn\u2019t very useful until we know how the game ends so for the final step of the game we modify the loss function:\n\nWhere z is the actual outcome of the game. Together these two loss functions work okay but the network will converge slowly and never reach a strong level of play.\n\nTo make our predictions more useful we need to solve the problem of temporal credit assignment. Basically, which actions did the player take in the past that resulted in the desired outcome in the future. Right now the loss only incorporates two consecutive steps and we want to stretch that out.\n\nWith the loss function above our parameter updates will look something like this:\n\nWhere \u03b8 is the network\u2019s parameters (weights), \u03b1 is the learning rate and \u03b4 is the difference we defined above:\n\nNow rather than include a single gradient we want to include all past gradients while paying more attention to the most recent. This is accomplished keeping a history of gradients then decaying each by increasing amounts of \u03bb that reflect how old the gradient has become:\n\nKeeping a running history of gradients can become memory intensive depending on the size of the network and the length of the game. An elegant solution to this problem is to use something called an \u201celigibility trace\u201d. Eligibility traces replace the gradient sum of the parameter update with a single moving gradient. The eligibility trace is defined as:\n\nBasically, we decay our eligibility trace by \u03bb then add the new gradient. With this, our parameter update becomes:\n\nThis effectively allows our parameter updates to take into account decisions made in the past. Now when we backpropagate the end game state, we take into account the gradients from earlier states in the game while we avoid keeping a complete history of gradients.\n\nAt the start of training, each game can take hundreds or thousands of turns to complete, effectively taking a random strategy. As the network learns, games require only around 50\u2013100 turns and will outperform an opponent making random moves after around 1000 games (about an hour of training).\n\nThe average loss for a game can never really reach zero because there\u2019s more uncertainty at the beginning of a game but it can be useful to visualize convergence:\n\nHopefully, this post shed some light on a small part of the history of recent deep reinforcement learning papers and the temporal-difference learning algorithm. If you\u2019re interested in learning more about reinforcement learning definitely check out Richard Sutton\u2019s book on the topic. You can also download the code for this implementation of TD-Gammon and play against the pre-trained network included in the repo.\n\nFollow me on Twitter for more posts like these. If you\u2019d like help running reinforcement learning in production, I do consulting.", 
        "title": "Before AlphaGo there was TD-Gammon \u2013 Jim Fleming \u2013"
    }, 
    {
        "url": "https://techstory.shma.so/tflearn-8d8219af8287?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "tflearn \u2013"
    }
]