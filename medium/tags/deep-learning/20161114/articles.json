[
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-7-action-selection-strategies-for-exploration-d3a97b7cceaf?source=tag_archive---------0----------------", 
        "text": "In this entry of my RL series I would like to focus on the role that exploration plays in an agent\u2019s behavior. I will go over a few of the commonly used approaches to exploration which focus on action-selection, and show their comparative strengths and weaknesses, as well as demonstrate how to implement each using Tensorflow. The methods are discussed here in the context of a Q-Network, but can be applied to Policy Networks as well. To make things more intuitive, I also built an interactive visualization to provide a better sense of how each exploration strategy works (It uses simulated Q-values, so there is no actual neural network running in the browser\u200a\u2014\u200athough such things do exist!). Since I can\u2019t embed it in Medium, I have linked to it here, and below. I highly recommend playing with it as you read through the post. Let\u2019s get started!\n\nThe first question one may ask is: why do we need exploration at all? The problem can be framed as one of obtaining representative training data. In order for an agent to learn how to deal optimally with all possible states in an environment, it must be exposed to as many of those states as possible. Unlike in traditional supervised learning settings however, the agent in a reinforcement learning problem only has access to the environment through its own actions. As a result, there emerges a chicken and egg problem: An agent needs the right experiences to learn a good policy, but it also needs a good policy to obtain those experiences.\n\nFrom this problem has emerged an entire subfield within reinforcement learning that has attempted to develop techniques for meaningfully balancing the exploration and exploitation tradeoff. Ideally, such an approach should encourage exploring an environment until the point that it has learned enough about it to make informed decisions about optimal actions. There are a number of frequently used approaches to encouraging exploration. In this post I want to go over some of the basic approaches related to the selection of actions. In a later post in this series I will cover more advanced methods which encourage exploration through the use of intrinsic motivation.\n\nExplanation: All reinforcement learning algorithms seek to maximize reward over time. A naive approach to ensuring the optimal action is taken at any given time is to simply choose the action which the agent expects to provide the greatest reward. This is referred to as a greedy method. Taking the action which the agent estimates to be the best at the current moment is an example of exploitation: the agent is exploiting its current knowledge about the reward structure of the environment to act. This approach can be thought of as providing little to no exploratory potential.\n\nShortcomings: The problem with a greedy approach is that it almost universally arrives at a suboptimal solution. Imagine a simple two-armed bandit problem (for an introduction to multi-armed bandits, see Part 1 of this series). If we suppose one arm gives a reward of 1 and the other arm gives a reward of 2, then if the agent\u2019s parameters are such that it chooses the former arm first, then regardless of how complex a neural network we utilize, under a greedy approach it will never learn that the latter action is more optimal.\n\nExplanation: The opposite approach to greedy selection is to simply always take a random action.\n\nShortcomings: Only in circumstances where a random policy is optimal would this approach be ideal. However it can be useful as an initial means of sampling from the state space in order to fill an experience buffer when using DQN.\n\nExplanation: A simple combination of the greedy and random approaches yields one of the most used exploration strategies: \u03f5-greedy. In this approach the agent chooses what it believes to be the optimal action most of the time, but occasionally acts randomly. This way the agent takes actions which it may not estimate to be ideal, but may provide new information to the agent. The \u03f5 in \u03f5-greedy is an adjustable parameter which determines the probability of taking a random, rather than principled, action. Due to its simplicity and surprising power, this approach has become the defacto technique for most recent reinforcement learning algorithms, including DQN and its variants.\n\nAdjusting during training: At the start of the training process the e value is often initialized to a large probability, to encourage exploration in the face of knowing little about the environment. The value is then annealed down to a small constant (often 0.1), as the agent is assumed to learn most of what it needs about the environment.\n\nShortcomings: Despite the prevalence of usage that it enjoys, this method is far from optimal, since it takes into account only whether actions are most rewarding or not.\n\nExplanation: In exploration, we would ideally like to exploit all the information present in the estimated Q-values produced by our network. Boltzmann exploration does just this. Instead of always taking the optimal action, or taking a random action, this approach involves choosing an action with weighted probabilities. To accomplish this we use a softmax over the networks estimates of value for each action. In this case the action which the agent estimates to be optimal is most likely (but is not guaranteed) to be chosen. The biggest advantage over e-greedy is that information about likely value of the other actions can also be taken into consideration. If there are 4 actions available to an agent, in e-greedy the 3 actions estimated to be non-optimal are all considered equally, but in Boltzmann exploration they are weighed by their relative value. This way the agent can ignore actions which it estimates to be largely sub-optimal and give more attention to potentially promising, but not necessarily ideal actions.\n\nAdjusting during training: In practice we utilize an additional temperature parameter (\u03c4) which is annealed over time. This parameter controls the spread of the softmax distribution, such that all actions are considered equally at the start of training, and actions are sparsely distributed by the end of training.\n\nShortcomings: The underlying assumption made in Boltzmann exploration is that the softmax over network outputs provides a measure of the agent\u2019s confidence in each action. If action 2 is 0.7 and action 1 is 0.2 the tempting interpretation is that the agent believes that action 2 is 70% likely to be optimal, whereas action 1 is 20% likely to be optimal. In reality this isn\u2019t the case. Instead what the agent is estimating is a measure of how optimal the agent thinks the action is, not how certain it is about that optimality. While this measure can be a useful proxy, it is not exactly what would best aid exploration. What we really want to understand is the agent\u2019s uncertainty about the value of different actions.\n\nExplanation: What if an agent could exploit its own uncertainty about its actions? This is exactly the ability that a class of neural network models referred to as Bayesian Neural Networks (BNNs) provide. Unlike traditional neural network which act deterministically, BNNs act probabilistically. This means that instead of having a single set of fixed weights, a BNN maintains a probability distribution over possible weights. In a reinforcement learning setting, the distribution over weight values allows us to obtain distributions over actions as well. The variance of this distribution provides us an estimate of the agent\u2019s uncertainty about each action.\n\nIn practice however it is impractical to maintain a distribution over all weights. Instead we can utilize dropout to simulate a probabilistic network. Dropout is a technique where network activations are randomly set to zero during the training process in order to act as a regularizer. By repeatedly sampling from a network with dropout, we are able to obtain a measure of uncertainty for each action. When taking a single sample from a network with Dropout, we are doing something that approximates sampling from a BNN. For more on the implications of using Dropout for BNNs, I highly recommend Yarin Gal\u2019s Phd thesis on the topic.\n\nShortcomings: In order to get true uncertainty estimates, multiple samples are required, thus increasing computational complexity. In my own experiments however I have found it sufficient to sample only once, and use the noisy estimates provided by the network. In order to reduce the noise in the estimate, the dropout keep probability is simply annealed over time from 0.1 to 1.0.\n\nI compared each of the approaches using a DQN trained on the CartPole environment available in the OpenAI gym. The Bayesian Dropout and Boltzmann methods proved most helpful, at least in my experiment. I encourage those interested to play around with the hyperparameters, as I am sure better performance can be gained from doing so. Indeed, different approaches may be best depending on what hyperparameters are used. Below is the full implementation of each method in Tensorflow:\n\nAll of the methods discussed above deal with the selection of actions. There is another approach to exploration that deals with the nature of the reward signal itself. These approaches fall under the umbrella of intrinsic motivation, and there has been a lot of great work in this area. In a future post I will be exploring these approaches in more depth, but for those interested, here is a small selection of notable recent papers on the topic:", 
        "title": "Simple Reinforcement Learning with Tensorflow Part 7: Action-Selection Strategies for Exploration"
    }, 
    {
        "url": "https://medium.com/@erikhallstrm/tensorflow-rnn-api-2bb31821b185?source=tag_archive---------1----------------", 
        "text": "This post is the follow up of the article \u201cHow to build a Recurrent Neural Network in TensorFlow\u201d, where we built a RNN from scratch, building up the computational graph manually. Now we will utilize the native TensorFlow API to simplify our script.\n\nRemember where we made the unpacking and forward passes in the vanilla RNN?\n\nReplace the piece of code above with this:\n\nYou may also remove the weight- and bias matrices and declared earlier. The inner workings of the RNN are now hidden \u201cunder the hood\u201d. Notice the usage of instead of when assigning the variable. The accepts a list of inputs of shape \u00a0, and the is simply one in our case (input is just a series of scalars). Split doesn\u2019t remove the singular dimension, but unpack does, you can read more about it here. It doesn\u2019t really matter anyways, since we still had to reshape the inputs in our previous example before the matrix multiplication. The unrolls the RNN and creates the graph automatically, so we can remove the for-loop. The function returns a series of previous states as well as the last state in the same shape as we did before manually, here is the printed output of these variables.\n\nHere is the full code:\n\nIn the next post we will improve the RNN by using another architecture called \u201cLong short-term memory\u201d or LSTM. Actually this is not necessary since our network already can solve our toy problem. But remember that our goal is to learn to use TensorFlow properly, not to solve the actual problem which is trivial\u00a0:)", 
        "title": "Using the RNN API in TensorFlow (2/7) \u2013 Erik Hallstr\u00f6m \u2013"
    }, 
    {
        "url": "https://medium.com/@suffiyanz/getting-started-with-machine-learning-f15df1c283ea?source=tag_archive---------2----------------", 
        "text": "The first step to learning something new is assessing what you already know and what you can readily translate. To make this simple I\u2019ve categorized three possible stages of where you may likely be at. These stages are dependent on two of the most fundamental skills that will help you acquire the knowledge you need to getting started with Machine Learning (wait for it)\u200a\u2014\u200a Math and Programming . Now, now, don\u2019t give up already. A relatively good understanding of Math and broad understanding of how to write trivial programs can get you real far.\n\nThis is also a great point to self-evaluate if you truly feel passionate about getting into the weeds with AI. If AI is only something you\u2019d like to read up so you can appear smart during meetings I suggest this budding reading list from Harvard Business Review . But if you want to dive deep into AI and pick up the tricks of the trade then keep reading.\n\nThe first place to start irrespective of where you are in your learning curve is by watching Frank Chen\u2019s AI Primer from Andreessen Horowitz. No questions asked, just watch it. Frank Chen does an amazing job in the primer simplifying Artificial Intelligence for everybody.\n\nThe problem of beginning with Machine Learning right now is there are decades of research and multiple entry points one could start from. It\u2019s a bit like Thomas Edisons 1000 failed attempts to invent the light bulb (or finding 999 ways not to make one) . AI has gone through decades of failed attempts ( AI Winters ), but only now has it really propelled forward. So do you start at the 1st attempt, the 79th attempt, or the 999th attempt? Lucky for you, we can get straight to the point where everything is working (or at least seems to be working\u2026 for now).\n\nThose of you who do end up taking the leap of faith will probably get more out of this journey than anyone else. It will not only open your mind to a whole new way of thinking, it will show you a world of possibilities and what you could do to apply this learning to your world for the better in more creative ways.\n\nIf you\u2019ve clicked on this post then you\u2019re probably deciding on learning Machine Learning. If you haven\u2019t already decided, or are still feeling confused, I want to tell you in advance it\u2019s going to be daunting .\n\nI\u2019m going to confess\u200a\u2014\u200aI had to think real hard before making this recommendation, and I\u2019m making it knowing fully how valuable the next 20 weeks are going to be for an absolute beginner. A strong foundation is necessary to ensure longevity in your skills and success in pursuing Machine Learning (and to go beyond). Knowing you aren\u2019t comfortable with Math (and there could be plenty reasons for that\u200a\u2014\u200abad teachers et all) or Programming means you\u2019ll have to roll up your sleeves and do the due diligence. Challenge yourself to put in the hard work and you will come out strong.\n\nA big part of this recommendation will require you to sign up on Khan Academy (don\u2019t worry it\u2019s completely free and a whole lot more fun). I have a graduate degree in Computer Science Engineering which required me to do complex math. I can wholeheartedly say Khan Academy has given me more confidence to do Math than all of my teachers combined in all my academic career.\n\nWe start with algorithms, and this should go pretty quickly. A good understanding of algorithms and a little proficiency can help you quickly pick up the skills and even master Deep Learning Algorithms down the line. The recent success in the field of AI is credited by one part to better training algorithms. Which is why starting here becomes almost crucial.\n\nI love this recommendation. Mimo is a great app if you want to learn how to program in Python. After a long dormant period of no-coding I\u2019ve rekindled my love for writing code through Mimo. Use it on the subway or in an Uber, you can get quickly from barely knowing how to code to writing cool apps. The app suggests 4.5 hours until you finish core concepts and that may really be all you need to get started with Machine Learning.\n\nThis is NOT something you can ignore. A good understanding of Linear Algebra is necessary for Machine Learning. You can self-pace this section based on how confident you get. Without a doubt, I would recommend completing the entire course and winning all those cool badges on Khan Academy.\n\nFinally, you will need a solid footing in Statistics and Probability. After all, building general intelligence into a machine is all about being able to predict the probability or likeliness of things.\n\nYou have successfully completed this stage of Algorithms, Python Basics, Algebra, and Statistics and Probability give yourself a huge round of APPLAUSE. That could not have been easy, but rest assured it\u2019s going to be completely worth it. You are now a \u2018Positive Beginner\u2019. Not bad eh, you\u2019ve gone from an \u2018Absolute Beginner\u2019 to a \u2018Positive Beginner\u2019 in just 20 weeks. You have nothing but your passion and dedication to thank. Proceed to the \u2018Positive Beginner\u2019 section.\n\nYou would like to really get your fundamentals right so you can arrive at the Colosseum of AI like a gladiator, do the Pro section below.\n\nThe best place for a \u2018Relative Beginner\u2019 to start is by training with a pioneer\u200a\u2014\u200aAndrew Ng. Andrew\u2019s course on Coursera takes approximately 11 weeks to complete and is highly recommended by the folks who\u2019ve taken it. You can even get a Coursera certificate by the end of the course for $79 if you\u2019d like to rack up those AI credits.\n\nPrerequisites for this course:\n\nAt any given point you feel like what Andrew is teaching is too complex, this may be a good time to rethink where you are on the learning curve. Andrew is conducting this course with an assumption you fulfill all the prerequisites. I too have had to occasionally look back to Khan Academy at times to recollect forgotten concepts in Math. If this is something you\u2019re comfortable doing, going back and forth, and are willing to take that effort then go for it.\n\nYou have completed this section proceed to the \u2018Positive Beginner\u2019 section below. Note: The section below should take you a lot less time to complete given you already have had a great start.\n\nCongratulations! You\u2019re ready to become a Jedi Master in Artificial Intelligence. The \u2018Introduction to Artificial Intelligence\u2019 course on Udacity is conducted by Sebastian Thrun (ex-CEO and Cofounder of Udacity, ex-Google Fellow, Stanford Computer Science professor) and Peter Norwig (Director of Research at Google). You\u2019ll be training with the best, through a unique interactive video experience on Udacity. This is a course put together by Google (absolutely free) and while it may seem like it was conducted in the 90s everything they\u2019re teaching is highly relevant and gives you a good understanding of Artificial Intelligence in general. They even touch up on some essential topics as you progress like Game Theory, Computer Vision, Robotics, and Natural Language Processing. You\u2019ll come out learning a lot more than you thought you would.\n\nTake a moment to congratulate yourself on everything you\u2019ve accomplished until now. If you\u2019ve transitioned between the three stages and successfully completed the course on Udacity you deserve a break. That must have been a lot of hard work but I\u2019m sure it was fun and exciting throughout.\n\nTell me how you did, your learning experience, and if there is something you would change about this learning process. It would be immensely helpful for others starting out with AI and ML. Feel free to shoot me a note and please do give this post a BIG LIKE.\n\nAll the best,\n\nP.S. Connect with me on LinkedIn or follow me on Twitter", 
        "title": "Getting Started with Machine Learning \u2013 Suff \u2013"
    }, 
    {
        "url": "https://medium.com/transmission-newsletter/using-deep-learning-to-discover-drugs-classify-pok%C3%A9mon-save-zebras-play-flappy-bird-more-a3a57fc98a15?source=tag_archive---------3----------------", 
        "text": "First impressions are notoriously subjective (and flawed), but now machines are being trained to make similar snap judgments based on human-generated data. Read more\u2026\n\nAI-published books may not be too far away! Researchers at Kyushu University in Japan have trained a deep neural network to study book covers and determine their category. Although a great start, the model isn\u2019t quite to human standards just yet. Read more\u2026\n\nOn a hunt for interesting and high-quality datasets to use for machine learning, I stumbled upon these 20 weird and wonderful sets. Check em\u2019 out!\n\nNeat! Check out this open source deep convolutional neural network that is trained on transformed audio signals to recognize \u201cahem\u201d sounds. Potentially very useful for the awkward speakers out there (myself included)! Check out these slides to learn more\u2026\n\nJen-Hsun Huang (NVIDIA CEO): \u201cAnd I think what Tesla has done by launching and having on the road in the very near-future here, a full autonomous driving capability using AI, that has sent a shock wave through the automotive industry. It\u2019s basically five years ahead. Anybody who\u2019s talking about 2021 and that\u2019s just a non-starter anymore.\u201d Read more\u2026\n\nA fun write-up on an attempt to classify 146 Pok\u00e9mon using a CNN. The most positive outcome of the model showed an accuracy of 96.57%, but there\u2019s still work to do to get reliable results. Read more here\u2026\n\nHotSpotter is a set of deep convolutional neural network algorithms that comb through images and identify a zebra by its barcode-like stripes and body shape. Amazing to see deep learning be used for such a good cause! Read more\u2026\n\nThe work Vijay Pande and team have been doing recently has been tremendous (see DeepChem and more). This is a must read! \u201cRecent advances in machine learning have made significant contributions to drug discovery. However, the applicability of these techniques has been limited by the requirement for large amounts of training data. In this work, we demonstrate how one-shot learning can be used to significantly lower the amounts of data required to make meaningful predictions in drug discovery applications.\u201d Read more\u2026\n\nAn awesome write-up on attempting to automate removal of eyeglasses from a face using deep learning. \u201cWouldn\u2019t it be great if people could leave their glasses on, and the software automatically removed them? Imagine walking into a retail store and having a virtual mirror remove your glasses and replace them with different products in real-time.\u201d Read more\u2026\n\nRemember Flappy Bird, the incredibly frustratingly mobile game? Machine learning has now made easy work of navigating the notorious pipes, and the code is open for the world to see. Try the demo\u2026", 
        "title": "Using Deep Learning to Discover Drugs, Classify Pok\u00e9mon, Save Zebras, Play Flappy Bird & More"
    }, 
    {
        "url": "https://medium.com/my-soul/sigmoid-4ae5c2d4fee5?source=tag_archive---------4----------------", 
        "text": "In computational networks, activation functions play a key role is defining the output of a node given an input or a set of inputs. In the field of Artificial Neural Networks (ANNs), the Sigmoid function is just that. It is a form of an activation function for artificial neurons. It is also called a transfer function but is not to be confused with a linear system\u2019s transfer function.\n\nThere are several types of activation functions\u200a\u2014\u200aa list of which is available on Wikipedia. The most basic form of an activation function is a binary step function. The sigmoid is special case of a logistic function. Each activation function have their uses and are suitable for different scenarios.\n\nIn the case of a sigmoid function,\n\nWhile running ANNs the priority of the modeller/programmer when choosing an activation function are:", 
        "title": "Activation: Sigmoid \u2013 Earth Is Home \u2013"
    }
]