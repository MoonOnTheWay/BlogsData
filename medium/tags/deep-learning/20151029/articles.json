[
    {
        "url": "https://medium.com/@kpcb_edge/our-focus-areas-computer-vision-cf8d733f5046?source=tag_archive---------0----------------", 
        "text": "Today, computers can analyze and understand image data better and faster than ever before. Three recent developments have made building vision systems significantly more feasible: (1) improvements in general-purpose GPU (GPGPU) hardware and hardware accessibility, (2) improvements in tooling around GPU acceleration of network training, and (3) discoveries in applying convolutional neural networks (CNNs) to image recognition problems. Computer vision is one of our core focus areas at KPCB Edge, and we\u2019re excited about it for these reasons and more.\n\nThe first important trend worth noting is the convergence upon GPU use for deep learning applications. In The ImageNet Large Scale Visual Recognition Challenge (ILSVRC), a yearly competition where entrants aim to produce the most accurate image classifier for the ImageNet data set, GPU use led to a significant drop in classification error. The first GPU-driven solution was introduced in 2012, and by 2014, 90% of teams used GPUs to train their models:\n\nLikely driving that shift to some extent, GPGPU performance has improved dramatically over the last few years, significantly outpacing CPU performance improvements for highly parallel tasks. Looking specifically at NVIDIA GPUs, since most deep learning software relies on NVIDIA\u2019s CUDA (Compute Unified Device Architecture) toolkit, per-GPU performance over time has improved exponentially:\n\nMore importantly from a cost standpoint, performance per watt has also improved at an exponential rate. This is projected to continue until at least 2018:\n\nHigh-end GPUs can be rented on an hourly basis using the AWS g2.2xlarge and g2.8xlarge instance types, making it easy to get started without any upfront investment in hardware.\n\nOpenCV, Caffe, and other tools have also made it easier than ever before for the average developer to build computer vision applications. In a little over an hour, with no formal training or background in computer vision (or devops), I was able to get Caffe up and running on a local Ubuntu VM and run a model trained on the full ImageNet data set. I was also able to train a simple network to do handwriting recognition. For the curious, I documented the steps I took on a fresh ubuntu/trusty64 Vagrant VM to set this up.\n\nDevelopers still need to understand the underlying algorithms to get good results, but it\u2019s no longer necessary to implement your own CNN tooling in CUDA or OpenCL to get acceptable performance. Most companies we\u2019ve seen are taking full advantage of these tools, allowing them to spend more time tweaking their network parameters and managing their data sets.\n\nLast, but certainly not least, image recognition accuracy in academic work has continued to improve at a dramatic pace. The ILSVRC has seen a significant downward trend in recognition error since its inception (see the first chart above).\n\nThe most recent competition\u2019s winner surpassed the performance of untrained humans, with the competition results saying that \u201ca significant amount of training time is necessary for a human to achieve competitive performance on ILSVRC\u201d with the winning model.\n\nWe believe these three trends will lead to a significant increase in the number of applications built for computer vision. However, because computer vision tooling is now widely accessible, there are fewer barriers to entry for developers building computer vision products. In other words, it\u2019s more difficult for a company to build a defensible business purely based on technology.\n\nBecause of this decrease in technical defensibility, the most interesting vision companies we\u2019ve seen have a compelling data acquisition model within the vertical they are targeting. This usually involves either access to training data that would be difficult for others to acquire, or finding a clever way to structure a manual training process more tenable for their customers (making data acquisition less important). We\u2019re most interested in seeing companies apply computer vision to specific verticals or problems, especially where humans are currently used to complete tasks (like our portfolio company Mashgin) or perhaps where the size of a task is so large that humans could never scalably do it manually (ex. OCR).\n\nIf you\u2019re a founder at a computer vision company and this resonates with you, we\u2019d love to chat!", 
        "title": "Our focus areas: Computer vision \u2013 KPCB Edge \u2013"
    }, 
    {
        "url": "https://medium.com/the-ai-report/google-search-engine-also-includes-deep-learning-based-signal-c9b7404d3c08?source=tag_archive---------1----------------", 
        "text": "We knew for a while now that Google had embedded some deep learning algorithms into Youtube in order to qualify videos, in Google Now in order to improve the quality of the speech recognition and in Google translate in order to output sentences that actually makes sense.\n\nWe discovered 2 days ago that deep learning was also implemented into their core product, the Google search engine. The existence of this system, nicknamed \u201cRankBrain\u201d, was revealed by Greg Corrado, a senior research scientist with the company, to Bloomberg. According to him a first deployed was made at the beginning of the year and a full release was made \u201cfew months ago\u201d. It is important to understand that RankBrain is not a new algorithm but one signal among many others (\u201cTop Heavy\u201d for blocking ad overpopulated pages, \u201cPirate\u201d for copyrights violations, \u201cPigeon\u201d for pushing local results or \u201cPanda\u201d against spamming) that impact Hummingbird, the main algorithm.\n\nThe project was initiated about a year ago by a group of five Google engineers, including search specialist Yonghui Wu, and deep-learning expert Thomas Strohmann. Getting there was not easy but they seem quite pleased with their work: \u201cI was surprised,\u201d Corrado said. \u201cI would describe this as having gone better than we would have expected.\u201d It now became such a success that turning off this feature \u201cwould be as damaging to users as forgetting to serve half the pages on Wikipedia.\u201d\n\nIt seems that RankBrain is particularly useful for never-seen-before queries, which represent up to 15% of Google queries, that is to say around 450 million queries per day. This AI embeds vast amounts of written language into mathematical entities, called vectors, which the computer can understand. Hence, if RankBrain encounters a sentence it has never seen before, it can make a guess as to what phrases might have a similar meaning and filter the result accordingly.\n\nI personally see two major step forward out of this roll out:\n\nEven more surprising, still according to Corrado, RankBrain has already become the third-most important signal contributing to the result of a search query. He refused to state the two first ones but we can reasonably assume that it is \u201clinks\u201d and \u201ckeywords\u201d.\n\nObserving one of the largest companies in the world enrolling Artificial Intelligence into the very heart of its core product, re-inforce the idea that machine learning and deep learning in particular is not simply a hyped technology. \u201cMachine learning is a core transformative way by which we are rethinking everything we are doing\u201d said Google\u2019s Chief Executive Officer Sundar Pichai, ealier this week.", 
        "title": "Google search engine also includes deep learning based signal"
    }
]