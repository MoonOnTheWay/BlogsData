[
    {
        "url": "https://blog.coast.ai/continuous-video-classification-with-tensorflow-inception-and-recurrent-nets-250ba9ff6b85?source=tag_archive---------0----------------", 
        "text": "A video is a sequence of images. In our previous post, we explored a method for continuous online video classification that treated each frame as discrete, as if its context relative to previous frames was unimportant. Today, we\u2019re going to stop treating our video as individual photos and start treating it like the video that it is by looking at our images in a sequence. We\u2019ll process these sequences by harnessing the magic of recurrent neural networks (RNNs).\n\nTo restate the problem we outlined in our previous post: We\u2019re attempting to continually classify video as it\u2019s streamed, in an online system. Specifically, we\u2019re classifying whether what\u2019s streaming on a TV is a football game or an advertisement.\n\nConvolutional neural networks, which we used exclusively in our previous post, do an amazing job at taking in a fixed-size vector, like an image of an animal, and generating a fixed-size label, like the class of animal in the image. What CNNs cannot do (without computationally intensive 3D convolution layers) is accept a sequence of vectors. That\u2019s where RNNs come in.\n\nRNNs allow us to understand the context of a video frame, relative to the frames that came before it. They do this by passing the output of one training step to the input of the next training step, along with the new frames. Andrej Karpathy describes this eloquently in his popular blog post, \u201cThe Unreasonable Effectiveness of Recurrent Neural Networks\u201d:\n\nWe\u2019re using a special type of RNN here, called an LSTM, that allows our network to learn long-term dependencies. Christopher Olah writes in his outstanding essay about LSTMs: \u201cAlmost all exciting results based on recurrent neural networks are achieved with [LSTMs].\u201d\n\nSold! Let\u2019s get to it.\n\nOur aim is to use the power of CNNs to detect spatial features and RNNs for the temporal features, effectively building a CNN->RNN network, or CRNN. For the sake of time, rather than building and training a new network from scratch, we\u2019ll\u2026\n\nStep 2 is unique so we\u2019ll expand on it a bit. There are two interesting paths that come to mind when adding a recurrent net to the end of our convolutional net:\n\nLet\u2019s say you\u2019re baking a cake. You have at your disposal all of the ingredients in the world. We\u2019ll say that this assortment of ingredients is our image to be classified. By looking at a recipe, you see that all of the possible things you could use to make a cake (flour, whisky, another cake) have been reduced down to ingredients and measurements that will make a good cake. The person who created the recipe out of all possible ingredients is the convolutional network, and the resulting instructions are the output of our pool layer. Now you make the cake and it\u2019s ready to eat. You\u2019re the softmax layer, and the finished product is our class prediction.\n\nI\u2019ve made the code to explore these methods available on GitHub. I\u2019ll pull out a couple interesting bits here:\n\nIn order to turn our discrete predictions or features into a sequence, we loop through each frame in chronological order, add it to a queue of size N, and pop off the first frame we previously added. Here\u2019s the gist:\n\nN represents the length of our sequence that we\u2019ll pass to the RNN. We could choose any length for N, but I settled on 40. At 10fps, which is the framerate of our video, that gives us 4 seconds of video to process at a time. This seems like a good balance of memory usage and information.\n\nThe architecture of the network is a single LSTM layer with 256 nodes. This is followed by a dropout of 0.2 to help prevent over-fitting and a fully-connected softmax layer to generate our predictions. I also experimented with wider and deeper networks, but neither performed as well as this one. It\u2019s likely that with a larger training set, a deeper network would perform best.\n\nNote: I\u2019m using the incredible TFLearn library, a higher-level API for TensorFlow, to construct our network, which saves us from having to write a lot of code.\n\nOnce we have our sequence of features and our network, training with TFLearn is a breeze.\n\nNow, let\u2019s evaluate each of the methods we outlined above for adding an RNN to our CNN.\n\nIntuitively, if one frame is an ad and the next is a football game, it\u2019s essentially impossible that the next will be an ad again. (I wish commercials were only 1/10th of a second long!)\n\nThis is why it could be interesting to examine the temporal dependencies of the probabilities of each label before we look at the more raw output of the pool layer. We convert our individual predictions into sequences using the code above, and then feed the sequences to our RNN.\n\nAfter training the RNN on our first batch of data, we then evaluate the predictions on both the batch we used for training and a holdout set that the RNN has never seen. No surprise, evaluating the same data we used to train gives us an accuracy of 99.55%! Good sanity check that we\u2019re on the right path.\n\nNow the fun part. We run the holdout set through the same network and get\u2026 95.4%! Better than our 93.3% we got without the LSTM, and not a bad result, given we\u2019re using the full output of the CNN, and thus not giving the RNN much responsibility. Let\u2019s change that.\n\nHere we\u2019ll go a little deeper. (See what I did there?) Instead of letting the CNN do all the hard work, we\u2019ll give more responsibility to the RNN by using output of the CNN\u2019s pool layer, which gives us the feature representation (not a prediction) of our images. We again build sequences with this data to feed into our RNN.\n\nRunning our training data through the network to make sure we get high accuracy succeeds at 99.89%! Sanity checked.\n\nHow about our holdout set?\n\n96.58%! That\u2019s an error reduction of 3.28 percentage points (or 49%!) from our CNN-only benchmark. Awesome!\n\nWe have shown that taking both spatial and temporal features into consideration improves our accuracy significantly.\n\nNext, we\u2019ll want to try this method on a more complex dataset, perhaps using multiple classes of TV programming, and with a whole whackload more data to train on. (Remember, we\u2019re only using 20 minutes of TV here.)\n\nOnce we feel comfortable there, we\u2019ll go ahead and combine the RNN and CNN into one network so we can more easily deploy it in an online system. That\u2019s going to be fun.\n\nPart 3 is now available: Five video classification methods implemented in Keras and TensorFlow", 
        "title": "Continuous video classification with TensorFlow, Inception and Recurrent Nets"
    }, 
    {
        "url": "https://medium.com/@jason_trost/collection-of-deep-learning-cyber-security-research-papers-e1f856f71042?source=tag_archive---------1----------------", 
        "text": "This is another quick post. Over the past few months I started researching deep learning to determine if it may be useful for solving security problems. This post on The Unreasonable Effectiveness of Recurrent Neural Networks was what got me interested in this topic, and I highly recommend reading it in its entirety.\n\nThroughout this research, I came across several security related academic and professional research papers on security topics that use Deep Learning as part of their research. What follows is a list of the papers/slides/videos that I found, and these may be useful to others. If you have others that you think should be added to this list, please ping me: @jason_trost.\n\nNote: this was originally published on 12/30/2016 on my blog: covert.io", 
        "title": "Collection of Deep Learning Cyber Security Research Papers"
    }, 
    {
        "url": "https://medium.com/@ricardo.guerrero/frameworks-de-deep-learning-un-repaso-antes-de-acabar-el-2016-5b9bf5b9f9af?source=tag_archive---------2----------------", 
        "text": "En su web lo definen como: \u201cAn open-source software library for Machine Intelligence\u201d (una librer\u00eda de c\u00f3digo libre para Machine Intelligence), pero creo que es m\u00e1s preciso este texto que aparece justo debajo: \u201c TensorFlow\u2122 is an open source software library for numerical computation using data flow graphs\u201d (Tensorflow es una librer\u00eda de c\u00f3digo libre para computaci\u00f3n num\u00e9rica usando grafos de flujo de datos). Aqu\u00ed por ejemplo no lo meten en la categor\u00eda de \u201cDeep Learning Frameworks\u201d, lo meten en la categor\u00eda de \u201cGraph compilers\u201d (compiladores de grafos) junto a Theano.\n\nHace unos meses fui a la charla \u201cGoogle Experts Summit: TensorFlow, Machine Learning for everyone, with Sergio Guadarrama\u201d. Sergio, uno de los ingenieros que desarrolla Tensorflow no nos ense\u00f1\u00f3 Tensorflow, sino que nos ense\u00f1\u00f3 una librer\u00eda de m\u00e1s alto nivel llamada tf.contrib que trabaja sobre Tensorflow. Me da la impresi\u00f3n de que internamente han visto que si quieren hacer que m\u00e1s gente use Tensorflow deben facilitar su uso creando capas por encima con un nivel de abstracci\u00f3n mayor.\n\nEn mi opini\u00f3n, tiene cosas muy buenas y algunas cosillas malas. Por lo pronto, no es un framework de proposito general. Se centra \u00fanicamente en la visi\u00f3n artificial, pero lo hace muy bien. En los experimentos que hicimos aqu\u00ed el entrenamiento de la red CaffeNet tard\u00f3 5 veces menos en Caffe que en Keras sobre Theano. Los inconvenientes que vimos es que no era nada flexible. Si quer\u00edas introducir cambios nuevos ten\u00edas que programar en C++ y en CUDA, aunque para cosas menos \u201cnovedosas\u201d pod\u00edas usar su interfaz de Python o de Matlab.\n\nTodos los d\u00edas se presentan muchas batallas, pero un buen Guerrero debe saber escoger aquellas en las que desea luchar y cuales prefiere dejar. Torch es un framework especialmente conocido por ser el que usa Facebook Research y que usaba DeepMind antes de ser adquirida por Google (posteriormente migraron a Tensorflow). Usa el lenguaje de programaci\u00f3n Lua, y esta es a la batalla a la que me refer\u00eda. En un panorama donde la mayor\u00eda del Deep Learning se centra en Python, un framework que funcione en Lua puede ser m\u00e1s un inconveniente que una ventaja. No tengo experiencia en el lenguaje, por lo que si quisiera meterme con esta herramienta tendr\u00eda primero que aprender Lua y luego aprender Torch. Es un proceso muy v\u00e1lido, pero en mi caso personal, prefiero centrarme en los que funcionan con Python, Matlab o C++.\n\nPython, R, C++, Julia.. mxnet es una de las librer\u00edas que m\u00e1s lenguajes soporta. Creo que especialmente a la gente de R le va a encantar, porque hasta ahora en este \u00e1rea estaba ganando Python de forma indiscutible (Python Vs R. Adivinad en que lado me posiciono\u00a0:-p ).\n\nDe momento Cognitive Toolkit no parece ser demasiado popular. No he visto demasiados blogs, ejemplos en internet o comentarios en Kaggle haciendo uso de esta librer\u00eda. Lo cual se me hace un poco raro teniendo en cuenta el escalado que comentan y que detr\u00e1s de esta librer\u00eda est\u00e1 Microsoft Research, el equipo de investigaci\u00f3n que ha conseguido batir el record mundial en el reconocimiento de voz al llegar a equipararse a niveles humanos.\n\nHe estado ojeando un ejemplo que tienen en la wiki del proyecto y he visto que la sintaxis de Cognitive Toolkit para Python (tambi\u00e9n soporta C++) es muy parecida a las de Keras, lo que me lleva a pensar (m\u00e1s bien confirmar) que la de Keras es la forma correcta.", 
        "title": "Frameworks de Deep Learning: un repaso antes de acabar el 2016"
    }, 
    {
        "url": "https://medium.com/@flavienguillocheau/documenting-docker-with-gpu-deep-learning-for-noobs-2edd350ab2f7?source=tag_archive---------3----------------", 
        "text": "The point of this small tutorial is to make a comprehensible and simple notebook with useful tips and commands to use Docker with NVIDIA GPU for deep learning purposes. It aims to help new developper willing to have a controlled environment for testing and iterating fast.\n\nFirst things first install your environment on your Ubuntu\u2019s VM. I won\u2019t cover anything else than docker related stuff.\n\nNow that you have your environment all setup you need to learn the basics on how to build, run and manage your docker images. Here are a few basic commands:\n\nNow I will cover how to build an image from the DockerFile, to run the image and to execute scripts inside of the container. I might add that having a clearer Dockerfile name is a good thing\u2026 So when you have your Dockerfile all set, there is still a few steps before being able to use the container. For the sake of this tutorial we will use the official GPU image from Tensorflow.\n\nIf you want to access your Tensorboard or Notebook, you might want to use port forwarding on your local machine:\n\nThis is the end of the part 1 on Docker for GPU-Deep Learning. The next post will be about the Dockerfile and how to setup DeepLearning-ready environment with Tensorflow.\n\nIf you have any questions or suggestions feel free to ask @fguilloc or in the comment section.", 
        "title": "GPU-Deep Learning with Docker for noobs \u2013 Flavien Guillocheau \u2013"
    }, 
    {
        "url": "https://medium.com/htc-research-engineering-blog/notes-for-deep-learning-on-nlp-94ddfcb45723?source=tag_archive---------4----------------", 
        "text": "Deep learning gradually plays a major role on NLP (Natural Language Processing). Here I note some technical evolution for the NLP problems.\n\nA continuous text sequence \u201cto be or not to be\u201d can be modelled by:\n\n2-gram(bigram): to be, be or, or not, not to, to be\n\n3-gram(trigram): to be or, be or not, or not to, not to be\n\nN-gram model can solve the problem of next word prediction, e.g., the occurrence of 6-gram model can predict the probability of next word is \u201cbe\u201d if the previous words are \u201cto be or not to\u201d:\n\nP(be|to be or not to) = C(to be or not to be) / C(to be or not to)\n\nTF-IDF indicates the importance of a word.\n\nTerm frequency of a word is the occurrence of the word over all occurrences of words in a document:\n\nTF(\u201ccow\u201d in document) = C(\u201ccow\u201d in document)/C(all words in document)\n\nDocument frequency of a word is the number of documents which contains the word over all the documents:\n\nE.g., If \u201ccow\u201d in document 1 occurs 4 times, and document 1 contains 100 words, the term frequency of the word \u201ccow\u201d on document 1 is 0.04. And if \u201ccow\u201d exists in 100 documents and there are overall 10000 documents, the document frequency of \u201ccow\u201d is log(10000/100) = 2. The TF-IDF is therefore 0.04 * 2 = 0.08.\n\nLSA applies TF-IDF to calculate the relations between words and documents.\n\nLet X be a matrix where element (i,j) implies the TF-IDF of word i in document j. Then we can get the correlations of two words by the matrix product \ud835\udc7f\ud835\udc7f\u1d40 and the correlations of two documents by the matrix product \ud835\udc7f\u1d40\ud835\udc7f.\n\nWith neural network, the idea is proposed to train a shared matrix C which can project each word into a feature vector, and put the vector as the input of a neural network to train the main task.\n\nSuppose the dimension of feature space is M, and vocabluary is V, the projection C is a|V|*M matrix. The input layer contains N-1 previous words in a N-gram model, which is encoded by 1-to-|V| representation. The output layer consists of |V| probabilities of the word in vocabulary.\n\nWord2vec is a two-layer neural network which can extract the feature vector of a word, which is called \u201cword embedding\u201d. Instead of training by the next word, word2vec applies one of the CBOW and Skip-gram network to train the projection matrix.\n\nContinuous bag of words (CBOW) trains the projection by putting surrounding words at input layer to predict the central word. And Skip-gram trains the projection by applying the central word to predict its surrounding words. Both policies can train the projection matrix for word embedding extraction.\n\nTo find out the related words of a target word, we can calculate the cosine similarity(distance) of all other words in vocabulary.\n\nGiven the word vector of words a, band c, we can find d if a:b = c:d by . Then find the word in vocabulary whose word vector has the smallest cosine distance between V\ud835\udc1d.\n\nSince no matter CBOW or Skip-gram models take adjacent N words into consideration to train the neural network, it is not scalable if N becomes very large. To overcome the drawback, a recursive neural network (RNN) is proposed:\n\nConsider the above traditional neural network for training a NLP problem, each word is fed to projection matrix W for a word vector, and convert to output S by hidden layer R. In this case, the input size of R is fixed to be 5*M(assume there are M features in the word vector). And R can\u2019t be scaled if N is changed to be 6.\n\nIn a recursive neural network, instead of traing a hidden layer R, we can only train two vectors at a time with hidden layer A, and recursively merge two words. In this case, the input size of A is always 2*M.\n\nSince recursive neural network is not easy to implement, recurrent neural network(RNN) is proposed to handle the same problem.\n\nA recurrent network\u2019s input is only one word, and redirect the output of hidden layer to part of the input. The whole input size is |V| + M (M is the number of features of previous words, and V is size of vocabulary).\n\nThe above example is an example for next character prediction. \u201ch\u201d is fed into hidden layer to get its word vector, and this vector is feed to part of input with \u201ce\u201d. After all the words are trained, the parameters of hidden layer (the projection matrix) can be trained.\n\nBoth Table 1 and Table2 indicates two discoveries: 1. word vectors from RNN is more accurate than LSA no matter in syntactic or semantic experiments. 2. The accuracy grows if the dimension of word vector is increased while applying RNN.\n\nTable 3 finds out the testing accuracy gets better if adapting a skip-gram network to pre-train the projection matrix to an existed RNN.\n\nTable 4 mentions that Skip-gram training is better than CBOW.\n\nMicrosoft sentence completion challenge: fill in the missed words in a sentence.", 
        "title": "Notes for deep learning on NLP \u2013 DeepQ Research Engineering Blog \u2013"
    }, 
    {
        "url": "https://blog.romanianit.com/artificial-intelligence-machine-learning-101-ro-webinar-40234b950363?source=tag_archive---------5----------------", 
        "text": "As I mentioned in the manifesto article, one of our key communication & education channels are the webinars. They focus on topics our members are experts in and are condensed in ~1h fast-paced, interactive & information packed chunks of video content.\n\nThey are best viewed live, but we record them for people that can\u2019t make it. We\u2019re aware that people are scattered all over the world! Here\u2019s the video in Romanian:\n\nFor those out there that don\u2019t speak this Latin language, I put together a list of key takeaways from our hour long webinar:\n\nThank you for watching / reading, we\u2019re always looking to improve the content, so please let us know what the next topic should be about. I\u2019m thinking ethics, job creation / destruction, the 4th Revolution and much more.\n\n2017 will be the year when AI takes off.\n\nFurther reading: Business Application for AI, State of Machine Intelligence 3.o, AI Industry Consensus\u200a\u2014\u200aQ&A\n\nWhere to start: Stanford Machine Learning Coursera Course", 
        "title": "Artificial Intelligence & Machine Learning 101 #RO Webinar"
    }, 
    {
        "url": "https://medium.com/@dfeehely/found-this-week-35-386ddb7e1584?source=tag_archive---------7----------------", 
        "text": "Merry Christmas everyone\u00a0:-) This week I got some nice footage of Bunratty Castle & Durty Nelly\u2019s in Co.Clare with the Phantom 3 drone, including this snapshot.\n\nThis photograph is available to buy on my Picfair profile.", 
        "title": "Found This Week #35 \u2013 Daryl Feehely \u2013"
    }, 
    {
        "url": "https://medium.com/the-data-intelligence-connection/lessons-learned-what-big-data-and-predictive-analytics-missed-in-2016-623f7075334d?source=tag_archive---------8----------------", 
        "text": "In this era of the software-driven business, we\u2019re told \u201cdata is the new oil\u201d, and that predictive analytics and machine intelligence will extract actionable insights from this valuable resource and revolutionize the world as we know it. Yet, 2016 brought three highly visible failures in this predictive view of the world: the UK\u2019s Brexit plebiscite; the Colombian referendum on FARC; and finally, the U.S. presidential election. What did these scenarios have in common? They all dealt with human behavior. This got me thinking that there might be lessons to be learned that are relevant to analytics.\n\nThe fact that data can be noisy or corrupted is well known. The question is: how does the uncertainty within the data propagate through the analytics and manifest itself in the accuracy of predictions derived from this data? For the purposes of this article, the analysis can be statistical, game-theoretic, deep learning-based, or anything else.\n\nThere is also an important distinction between what I call \u201chard\u201d data and \u201csoft\u201d data. This is not standard terminology, so let me define what I mean by these terms.\n\nHard data comes from observations and measurements of the macroscopic natural world: the positions of astronomical objects, the electrical impulses within the brain, or even the amounts of your credit card transactions. Typically, such data is objective. The observations are numerical, and the uncertainty is adequately characterized as an error zone around a central value. There is an (often unstated) assumption that the observation is trusted and repeatable (i.e., nature is not being adversarial and presenting the observer with misleading results).\n\nMuch effort has gone into designing measurement apparatus, calibration techniques, and experimental design to reduce the error zones. There is even the so-called \u201cpersonal equation\u201d to account for observer bias.", 
        "title": "Lessons Learned: What Big Data and Predictive Analytics Missed in 2016"
    }, 
    {
        "url": "https://medium.com/@YvesMulkers/lessons-learned-what-big-data-and-predictive-analytics-missed-in-2016-959931851312?source=tag_archive---------9----------------", 
        "text": "In this era of the software-driven business, we\u2019re told \u201cdata is the new oil\u201d, and that predictive analytics and machine intelligence will extract actionable insights from this valuable resource and revolutionize the world as we know it. Yet, 2016 brought three highly visible failures in this predictive view of the world: the UK\u2019s Brexit plebiscite; the Colombian referendum on FARC; and finally, the U.S. presidential election. What did these scenarios have in common? They all dealt with human behavior. This got me thinking that there might be lessons to be learned that are relevant to analytics.\n\nThe fact that data can be noisy or corrupted is well known. The question is: how does the uncertainty within the data propagate through the analytics and manifest itself in the accuracy of predictions derived from this data? For the purposes of this article, the analysis can be statistical, game-theoretic, deep learning-based, or anything else.\n\nThere is also an important distinction between what I call \u201chard\u201d data and \u201csoft\u201d data. This is not standard terminology, so let me define what I mean by these terms.\n\nHard data comes from observations and measurements of the macroscopic natural world: the positions of astronomical objects, the electrical impulses within the brain, or even the amounts of your credit card transactions. Typically, such data is objective. The observations are numerical, and the uncertainty is adequately characterized as an error zone around a central value. There is an (often unstated) assumption that the observation is trusted and repeatable (i.e., nature is not being adversarial and presenting the observer with misleading results).\n\nMuch effort has gone into designing measurement apparatus, calibration techniques, and experimental design to reduce the error zones. There is even the so-called \u201cpersonal equation\u201d to account for observer bias.", 
        "title": "Lessons Learned: What Big Data and Predictive Analytics Missed in 2016"
    }
]