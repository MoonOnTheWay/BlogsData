[
    {
        "url": "https://gab41.lab41.org/lab41-reading-group-deep-residual-learning-for-image-recognition-ffeb94745a1f?source=tag_archive---------0----------------", 
        "text": "Today\u2019s paper offers a new architecture for Convolution Networks. It was written by He, Zhang, Ren, and Sun from Microsoft Research. I\u2019ll warn you before we start: this paper is ancient. It was published in the dark ages of deep learning sometime at the end of 2015, which I\u2019m pretty sure means its original format was papyrus; thankfully someone scanned it so that future generations could read it. But it is still worth blowing off the dust and flipping through it because the architecture it proposes has been used time and time again, including in some of the papers we have previously read: Deep Networks with Stochastic Depth. He et al. begin by noting a seemingly paradoxical situation: very deep networks perform more poorly than moderately deep networks, that is, that while adding layers to a network generally improves the performance, after some point the new layers begin to hinder the network. They refer to this effect as network degradation. If you have been following our previous posts this won\u2019t surprise you; training issues like vanishing gradients become worse as networks get deeper so you would expect more layers to make the network worse after some point. But the authors anticipate this line of reasoning and state that several other deep learning methods, like batch normalization (see our post for a summary), essentially have solved these training issues, and yet the networks still perform increasingly poorly as their depth increases. For example, they compare 20- and 56-layer networks and find the 56-layer network performs far worse; see the image below from their paper.\n\nThe authors then set up a thought experiment (or gedankenexperiment if you\u2019re a recovering physicist like me) to demonstrate that deeper networks should always perform better. Their argument is as follows: Start with a network that performs well; Add additional layers that are forced to be the identity function, that is, they simply pass along whatever information arrives at them without change; This network is deeper, but must have the same performance as the original network by construction since the new layers do not do anything; Layers in a network can learn the identity function, so they should be able to exactly replicate the performance of this deep network if it is optimal. This thought experiment leads them to propose their deep residual learning architecture. They construct their network of what they call residual building blocks. The image below shows one such block. These blocks have become known as ResBlocks.\n\nThe ResBlock is constructed out of normal network layers connected with rectified linear units (ReLUs) and a pass-through below that feeds through the information from previous layers unchanged. The network part of the ResBlock can consist of an arbitrary number of layers, but the simplest is two. To get a little into the math behind the ResBlock: let us assume that a set of layers would perform best if they learned a specific function, h(x). The authors note that the residual, f(x) = h(x)\u2212x, can be learned instead and combined with the original input such that we recover h(x) as follows: h(x) = f(x) + x. This can be accomplished by adding a +x component to the network, which, thinking back to our thought experiment, is simply the identity function. The authors hope that adding this \u201cpass-through\u201d to their layers will aid in training. As with most deep learning, there is only this intuition backing up the method and not any deeper understanding. However, as the authors show, it works, and in that end that\u2019s the only thing many of us practitioners care about. The paper also explores a few modifications to the ResBlock. The first is creating bottleneck blocks with three layers where the middle layer constricts the information flow by using fewer inputs and outputs. The second is testing different types of pass-through connections including learning a full projection matrix. Although the more complicated pass-throughs perform better, they do so only slightly and at the cost of training time. The rest of the paper tests the performance of the network. The authors find that their networks perform better than identical networks without the pass-through; see the image below for their plot showing this. They also find that they can train far deeper networks and still show improved performance, culminating in training a 152-layer ResNet that outperforms shallower networks. They even train a 1202-layer network to prove that it is feasible, but find that its performance is worse than the other networks examined in the paper.\n\nSo that\u2019s it! He et al. proposed a new architecture motivated by thought experiments and the hope that it will work better than previous ones. They construct several networks, including a few very deep ones, and find that their new architecture does indeed improve performance of the networks. Although we don\u2019t gain any further understanding of the underlying principles of deep learning, we do get a new method of making our networks work better, and in the end maybe that\u2019s good enough.", 
        "title": "Lab41 Reading Group: Deep Residual Learning for Image Recognition"
    }, 
    {
        "url": "https://medium.com/ai-business/study-group-42d1f85d79de?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u4eba\u5de5\u77e5\u80fd\u52c9\u5f37\u4f1a\u53c2\u52a0\u8005\u306e\u7686\u69d8\u3078\u306e\u304a\u793c \u2013 Team AI Blog \u2013"
    }, 
    {
        "url": "https://medium.com/@derekchen14/why-generative-models-c61a19ad769f?source=tag_archive---------2----------------", 
        "text": "Recently, there has been a lot of talk around generative models as the next big thing in deep learning. There are articles about general adversarial networks (GANs), variational autoencoders (VAEs), PixelRNNs, and perhaps some reinforcement learning augmentation. But why all this sudden interest? Why not pursue memory networks or other ideas? I am by no means an expert, but my suspicion is that generative models offer superior context and speed.\n\nTo start, there is a very practical aspect to strong generative models because they turn unlabeled data into a useful tool that helps the supervised training portion run faster. This happens because a model that is pre-trained using generative models have better initialization weights\u200a\u2014\u200astarting closer to local minima means less road to travel.\n\nMore important than faster convergence though is the ability of generative models to (theoretically) lead to better convergence. In other words, rather than just getting to the answer faster, we actually reach better answers overall, or similar answers with significantly less data than previously possible.\n\nThis happens because if a machine has a decent understanding of how the world works, then this context allows the machine to make inferences about the world to give a reasonable responses even in situations it has not previously encountered. This concept has come up in various form in the past: One-shot learning, Semi-supervised learning, Transfer learning. For example, from a machine\u2019s point of view:\n\nThus, while generating jazz songs or Van Gogh style paintings is certainly entertaining, the true benefit of generative models is allowing practitioners to move more efficiently and effectively than ever before. At least that is my opinion from an outsider\u2019s perspective\u200a\u2014\u200awould love to hear an expert\u2019s thoughts on the subject!", 
        "title": "Why Generative Models? \u2013 Derek Chen \u2013"
    }, 
    {
        "url": "https://medium.com/@QasimAslam/ye-pal-hume-yaad-aae-ge-9a9671d9e32a?source=tag_archive---------3----------------", 
        "text": "As it is the rule at Amal Academy that they enjoyed and spent last day outside. So the day came when there was our last session and we went outside to enjoy our last session. We made plan to visit Old Walled city of Lahore. We planned to reached Minar e Pakistan first by using Metro bus service.\n\nIt was so sunny and hot day. We were full of wet at the beginning of the day. We use our charts for shadow and also cover our head..But we were full enjoying the tour with Amal Team and with fellows. We started from Minar e pakistan and then walked towards the badshahi Masjid. Everyone talking with each other and enjoying. Most of us went there without breakfast so we required breakfast from our Ma\u2019am Zaryab. We visited Minar e Pakistan, badshahi Majid, Shahi Qila, Majid e Wazir Khan, Shahi Hmam. Some of the places were new for us and didn\u2019t see and learn about them before.\n\nFrom my point of view that day also very good and memorable for me because my birthday also on that day. So at the start of the day everyone wish me my birthday, some of my fellows gave me gifts and We also cut the cake at Shahi Qila.\n\nBecause I lived at Lahore by since last six years so I visited two to three times at these places every year with friends or with my guests who visited Lahore for tour. So I thought that I didn\u2019t enjoy much but with fellows and Amal team and also with the support of Tour Guide I enjoyed and learned a lot. Most of the things that I saw before but I didn\u2019t know about those things much but on this day tour Guide(Mudassar Bro) told us each and every thing about these places and attract our attention. He was so funny and has smiling personality and engage each and every fellow through jokes and through questioning. He also engage with us by capturing our pictures.\n\nWhen I visited these places one thing came into my mind that \u201cIt is too easy to be successful, but it too hard to keep on that success.\u201d God gave the Mughals of Sub Continent respect, power and authority and they became the Emperor of Sub Continent. They captured this with efforts but then they forgot their values and use the principles of destroyed nations. They felt proud and showed laziness and then destroyed. Life is a non stop struggle and effort who slower down he must be ruined.\n\nI wants to say thank you to Amal team and our PM & PA who arrange a outside tour for us. And showed a good management and planning. I impressed with the time management skills of Amal Team. They did everything perfect and on time. They gave every historic place a specific and limited time and follow this rule and only spent that time at that place which they fixed before. This thing really impressed me and I will also follow it during my life.", 
        "title": "Ye pal hume Yaad Aae ge\u2026. \u2013 Muhammad Qasim \u2013"
    }
]