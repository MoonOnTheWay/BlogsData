[
    {
        "url": "https://gab41.lab41.org/one-more-reason-not-to-be-scared-of-deep-learning-54c535d42f14?source=tag_archive---------0----------------", 
        "text": "Just how data-hungry is deep learning? It is an important question for those of us who don\u2019t have an ocean of data from somewhere like Google or Facebook and still want to see what this deep learning thing is all about. If you have a moderate amount of your own data and your fancy new model gets mediocre performance, it is often hard to tell whether the fault is in your model architecture or in the amount of data that you have. Learning curves and other techniques for diagnosing training-in-progress can help, and much ink has been spilled offering guidance to young deep learners. We wanted to add to this an empirical case study in the tradeoff between data size and model performance for sentiment analysis.\n\nWe wondered how much data we needed for our work on sunny-side-up, a project assessing deep learning techniques for sentiment analysis (check out our post on learning about deep learning, which also introduces the project). Most real-world text corpora have orders of magnitude fewer documents than, for instance, the popular Amazon Reviews dataset. Even one of the stalwart benchmark datasets for sentiment analysis, IMDB Movie Reviews, has a \u201cmere\u201d tens of thousands of reviews compared to the Amazon dataset\u2019s millions. While deep learning methods have claimed exceptional performance on the IMDB set, some of the top performers are trained on outside datasets. If you were trying to do sentiment analysis in small collections of documents in under-resourced langauges like Hausa or Aymara, then 30 million Amazon Movie reviews might not be a great analogue.\n\nTo look at the effects of data size in deep learning for text, I\u2019ll look at the performance of Zhang, Zhao and LeCun\u2019s Crepe convolutional network architecture on differently sized subsets of the Amazon reviews set. The arXiv manuscript for Crepe claims impressive performance on (a different set of) Amazon reviews, so this is an interesting test bed for examining how much data such an algorithm might need for a sentiment task. Their paper suggests that performance degrades on datasets numbering in the hundreds of thousands of documents (which is still pretty big). But the datasets they compare have many more differences than just size, so it is hard to know how much data size itself impacts performance. Let\u2019s look at performance on differently sized samples from the same dataset.\n\nTo test the Crepe architecture, we used the full \u201cHealth and Personal\u201d section of the Amazon Reviews release, which has 3.7 million reviews. Instead of having the sentiment model try to predict the actual star rating, we threw out all the milquetoast, wishy-washy 3-star reviews and called 4- and 5-star reviews positive, while 1- and 2-star reviews were counted as negative. In sentiment analysis this binarized scale is sometimes called polarity. To compare performance across datasets, we trained Crepe for 5 epochs on subsets of the training set: the full 3 million, 500 thousand, 100 thousand, 50 thousand and 25 thousand. We kept the size and composition of the test set fixed at 700 thousand reviews. We were a bit surprised\u200a\u2014\u200athe final test accuracy over five epochs of training does not actually degrade as much as we would have expected. The two largest subsets offer almost identical validation performance after just one training epoch, and the third and fourth largest sets largely catch up by the end of five epochs. Only the smallest subset, with 25 thousand documents, fails to learn anything at all.\n\nBecause of class imbalances in this dataset, accuracy numbers can be misleading. The reality, which mirrors certain real-world scenarios, is that there is a significant imbalance in the numbers of negative and positive cases. This may be part of the reason the classifiers trained on 25k and 50k samples were largely unable to learn anything, beyond categorically predicting the whole test as one label or the other. Although they start off at 17 percent accuracy (predicting everything as negative), this is not materially different from 83 percent accuracy (predicting everything as positive). Nonetheless, by the end of five epochs, the four largest subsets have each shown some improvement, with the top three highly competitive with each other.\n\nEven though room for improvement on the baseline of 83% is fairly narrow, many real-world datasets are imbalanced in just this way, so it is interesting to examine how sentiment analysis techniques do on this data. We can look at their performance on the rarer class\u200a\u2014\u200anegative reviews\u200a\u2014\u200ato get a good assessment.", 
        "title": "One more reason not to be scared of deep learning \u2013"
    }, 
    {
        "url": "https://medium.com/truly-yours/the-future-of-art-deep-dreams-ai-and-ascribe-3e6b7af1087?source=tag_archive---------1----------------", 
        "text": "A few weeks ago, the Gray Area Foundation for the Arts and Research at Google hosted a benefit auction and exhibition in San Francisco titled DeepDream: The art of neural networks. The exhibition was a unique one. One made up exclusively of artworks created using Google\u2019s artificial neural networks.\n\nIf you\u2019re wondering what exactly \u201cartificial neural networks\u201d means, Google wrote a blog post about how this all works and it\u2019s nothing short of fascinating. To summarize it briefly, what happens is that different layers of the technology are set up to spot patterns or shapes, everything from corners and edges to trees, animals and eyes. Whenever the network spots one of these elements, it interprets and enhances it. In the post, they make a comparison to watching clouds and picking out familiar shapes. They call this \u201cInceptionism.\u201d\n\nThe Deep Dream site is worth the visit. Head to http://deepdreamgenerator.com, upload an image and marvel for yourself at the strange beauty that it spits back at you. Greg McMullen, our CPO, put a photo of me into the generator, the result of which he has rather appropriately named \u201cPuppy Medusa.\u201d\n\nIn the same way that the sound of yesteryear\u2019s dial-up Internet connection could be thought of as digital files trying to speak through a line meant for audio, these images have been described as visual representations of what the dreams of machines look like.\n\nThe entire limited edition collection of works was auctioned off with the proceeds\u200a\u2014\u200atotalling nearly $100,000\u200a\u2014\u200agoing to the Gray Area Foundation for the Arts, which works to bridge the worlds of art and technology.\n\nAt ascribe, we embody this intersection of art and tech ourselves, which makes our collaboration with Gray Area on this project all the more exciting.\n\nascribe is a service that allows creators\u200a\u2014\u200aartists, photographers, writers, graphic designers, musicians\u200a\u2014\u200ato register, transfer and track their digital content. Our goal is to give control back to the creators. We do so by providing the tools to lock in authorship so that work is always properly attributed, create limited digital editions and set specific licensing parameters for seamless consignments, loans and transfers of work to galleries, museums or marketplaces. We also provide visibility into where work spreads online by showing when and where work has appeared. For more on that, check out WhereOnThe.Net. Essentially, we provide the closest possible approximation to ownership on the Internet. By registering work with ascribe at the time of creation, you can set your work free into the wild World Wide Web with ease and confidence. In addition to the amazing artists that we work with on a daily basis, we collaborate with galleries, online marketplaces, collectors, museums, archives and prizes. And now, Gray Area.\n\nAll of the Deep Dream work at the exhibition was registered with ascribe. In this case, the work was uploaded to ascribe and one unique digital edition was created for each piece. The master files that were registered had the associated legals, licensing and the artists\u2019 information attached to it, including full name, name of the piece, year created and so on. They were then consigned to Gray Area for the exhibition so that they could be sold on the artists\u2019 behalf. Technical information was also attached to the registered artworks when consigned so they knew exactly the kind of material, framing and dimensions to use.\n\nThose who made purchases at the auction now have a Certificate of Authenticity (COA) for their piece with a built-in cryptographic ID. These COAs offer a clear provenance, verified authenticity and can be printed out.\n\nAt ascribe, the future of digital art and digital collecting is an exciting one, especially with the advancements in areas like artificial intelligence and virtual reality. Gray Area is the latest in a long list of collaborators who have chosen to use ascribe\u2019s technology and tools to ensure that the artists they work with can properly protect and manage their artwork. Sometimes, it\u2019s just that easy.", 
        "title": "The Future of Art: Deep Dreams, AI and ascribe \u2013 ascribe IO \u2013"
    }, 
    {
        "url": "https://medium.com/@StephenMartindale/alphago-vs-lee-se-dol-game-two-596072d7c24c?source=tag_archive---------2----------------", 
        "text": "Lee Se-dol (9 dan pro.) conceded another match to AlphaGo, today, in Seoul. Google DeepMind\u2019s lead now stands at two games to none and a straight-sets victory over the human player seems entirely possible. We will have to wait until Saturday to find out.\n\nDuring the press conference that followed the game, Lee Se-dol admitted that he felt that black, played by AlphaGo, never fell behind or gave him an easy chance at any point during the match and that the programme made moves that he considered entirely reasonable. Reading between the lines, I suspect he even felt some sort of respect for the thing! He attributed his loss to an inability to find the bot\u2019s weaknesses.\n\nGoogle DeepMind claimed that the A.I. itself was confident of victory for most of the game, particularly during the opening and end-game phases, but that the team behind the scenes were nervous because the opinions of other strong human professionals were widely varied - some predicting that white, Lee Se-dol, would ultimately come out in front.\n\nThe development team also claimed that their only role, during the game, was ensuring that the programme had access to the computational resources and infrastructure that it required.\n\nIn the coming months, I would dearly like to know more about the level of human intervention that took place behind the scenes, the computational resources that powered the engine, and the learning and improvement that took place between the games of the match.\n\nPersonally, I will be cheering for the man who is opposing the machine, on Saturday. AlphaGo has already acquitted itself skilfully and Google DeepMind have already made a smashing d\u00e9but; a close match will be far more satisfying than a clean sweep by the engine.\n\nWhile we wait to see what transpires in game three, it is time to start considering what AlphaGo\u2019s victories mean for the future of the game of Go itself, the future of professional, competitive play and the future direction of research in the field of Computer Go. Perhaps we should all start reading and remembering the ripples that IBM\u2019s DeepBlue sent through the world of Chess.", 
        "title": "AlphaGo vs. Lee Se-dol: Game Two \u2013 Stephen Martindale \u2013"
    }, 
    {
        "url": "https://medium.com/@dave_52446/whats-after-alphago-d25fad1a80e0?source=tag_archive---------3----------------", 
        "text": "AlphaGo is a system that can play Go at least as well as the best humans. Go was widely cited as the hardest (and only remaining) game at which humans could beat machines, so this is a big deal. AlphaGo has just defeated a top-ranked human expert.\n\nGo is hard because the search-space of possible moves is so large that tree search and pruning techniques, such as those used to beat humans at Chess, won\u2019t work\u200a\u2014\u200aor at least, they won\u2019t work well enough, with a feasible amount of memory, to play Go better than the best humans.\n\nInstead, to play Go well, you need to have \u201cintuition\u201d rather than brute search power: To look at the board and spot local (or gross) patterns that represent opportunities or dangers. And in fact, AlphaGo is able to play in this way. It beat the next best computer algorithm \u201cPachi\u201d 85% of the time without any tree search\u200a\u2014\u200ajust predicting the best action based on its interpretation of the current state. The authors of the AlphaGo Nature paper say:\n\n\u00a0\n\n\u00a0\u201cDuring the match against Fan Hui, AlphaGo evaluated thousands of times fewer positions than Deep Blue did in its chess match against Kasparov; compensating by selecting those positions more intelligently, using the policy network, and evaluating them more precisely, using the value network\u200a\u2014\u200aan approach that is perhaps closer to how humans play.\u201d\n\nAlphaGo is trained by both supervised and reinforcement learning. Supervised learning feedback comes from recordings of moves in expert games. However, these are finite in size and used naively, would lead to overfitting.\n\nInstead, in AlphaGo a Supervised Learning deep neural network learns to model and predict expert behaviour in the recorded games, via conventional deep learning techniques. Then, a reinforcement learning network is used to generate reward data for novel games that AlphaGo plays against itself! This mitigates the limited size of the supervised learning dataset.\n\nOf course, AlphaGo also wants the play better than the best play observed in the training data. To achieve this, the reinforcement learning network is further trained by playing pairs of them (networks) against each other\u200a\u2014\u200amixing the pairs up to prevent policies overfitting each other. This is a really clever feature because it allows AlphaGo to go beyond its training data.\n\nNote also that the neural networks cannot possibly fully represent a sufficiently deep tree of board outcomes within their limited set of weights. Instead, the network has to learn to represent good and bad situations with limited resources. It has to form its own representation of the most salient features, during training.\n\nThe neural networks function without pre-defined rules specific to Go; instead they have learned from training data collected from many thousands of human and simulated games.\n\nAlphaGo is an important advance because it is able to make good judgments about play situations based on a lossy interpretation in a finitely-sized deep neural network.\n\nWhat\u2019s more, Go wasn\u2019t simply taught to copy human experts\u200a\u2014\u200ait went further, and improved, by playing against itself.\n\nThe techniques used in deep neural networks have recently been scaled to work effectively on a wide range of problems. In some subject areas, narrow AIs are reaching superhuman performance. However, it is not clear that these techniques will scale indefinitely. Problems such as vanishing gradients have been pushed back, but not necessarily eliminated.\n\nMuch greater scale is needed to get intelligent agents into the real world without them being immediately smashed by cars or stuck in holes. But already, it is time to consider what features or characteristics constitute an artificial general intelligence (AGI), beyond raw intelligence (which AIs now have).\n\nAlphaGo isn\u2019t a general intelligence; it\u2019s designed specifically to play Go. Sure, it\u2019s trained rather than programmed manually, but it was designed for this purpose. The same techniques are likely to generalize to many other problems, but they\u2019ll need to be applied thoughtfully and retrained.\n\nAlphaGo isn\u2019t an Agent. It doesn\u2019t have any sense of self, or intent, and its behaviour is pretty static\u200a\u2014\u200aits policies would probably work the same way in all similar situations, learning only very slowly. You could say that it doesn\u2019t have moods, or other transient biases. Maybe this is a good thing! But this also limits its ability to respond to dynamic situations.\n\nAlphaGo doesn\u2019t have any desire to explore, to seek novelty or to try different things. AlphaGo couldn\u2019t ever choose to teach itself to play Go because it found it interesting. On the other hand, AlphaGo did teach itself to play Go\u2026\n\nAll in all, it\u2019s a very exciting time to study artificial intelligence!", 
        "title": "What\u2019s after AlphaGo? \u2013 David Rawlinson \u2013"
    }
]