[
    {
        "url": "https://medium.com/@camrongodbout/recurrent-neural-networks-for-beginners-7aca4e933b82?source=tag_archive---------0----------------", 
        "text": "What are Recurrent Neural Networks and how can you use them?\n\nIn this post I discuss the basics of Recurrent Neural Networks (RNNs) which are deep learning models that are becoming increasingly popular. I don\u2019t intend to get too heavily into the math and proofs behind why these work and am aiming for a more abstract understanding.\n\nRecurrent Neural Networks were created in the 1980\u2019s but have just been recently gaining popularity from advances to the networks designs and increased computational power from graphic processing units. They\u2019re especially useful with sequential data because each neuron or unit can use its internal memory to maintain information about the previous input. This is great because in cases of language, \u201cI had washed my house\u201d is much more different than \u201cI had my house washed\u201d. This allows the network to gain a deeper understanding of the statement.\n\nThis is important to note because reading through a sentence even as a human, you\u2019re picking up the context of each word from the words before it.\n\nA RNN has loops in them that allow infromation to be carried across neurons while reading in input.\n\nIn these diagrams x_t is some input, A is a part of the RNN and h_t is the output. Essentially you can feed in words from the sentence or even characters from a string as x_t and through the RNN it will come up with a h_t.\n\nThe goal is to use h_t as output and compare it to your test data (which is usually a small subset of the original data). You will then get your error rate. After comparing your output to your test data, with error rate in hand, you can use a technique called Back Propagation Through Time (BPTT). BPTT back checks through the network and adjusts the weights based on your error rate. This adjusts the network and makes it learn to do better.\n\nTheoretically RNNs can handle context from the beginning of the sentence which will allow more accurate predictions of a word at the end of a sentence. In practice this isn\u2019t necessarily true for vanilla RNNs. This is a major reason why RNNs faded out from practice for a while until some great results were achieved with using a Long Short Term Memory(LSTM) unit inside the Neural Network. Adding the LSTM to the network is like adding a memory unit that can remember context from the very beggining of the input.\n\nThese little memory units allow for RNNs to be much more accurate, and have been the recent cause of the popularity around this model. These memory units allow for the ability across inputs for context to be remembered. Two of these units are widely used today LSTMs and Gated Recurrent Units(GRU), the latter of the two are more efficient computationally because they take up less computer memory.\n\nThere are many different applications of RNNs. A great application is in collaboration with Natural Language Processing (NLP). RNNs have been demonstrated by many people on the internet who created amazing models that can represent a language model. These language models can take input such as a large set of shakespeares poems, and after training these models they can generate their own Shakespearean poems that are very hard to differentiate from originals!\n\nBelow is some Shakespeare\n\nThis poem was actually written by an RNN. This was from an awesome article here http://karpathy.github.io/2015/05/21/rnn-effectiveness/ that goes more indepth on Char RNNs.\n\nThis particular type of RNNs is fed in a dataset of text and reads the input in character by character. The amazing thing about these networks in comparison to feeding in a word at a time is that the network can create it\u2019s own unique words that were not in the vocabulary you trained it on.\n\nThis diagram taken from the article referenced above shows how the model would predict \u201chello\u201d. This gives a good visualization of how these networks take in a word character by character and predict the likely hood of the next probable character.\n\nAnother amazing application of RNNs is machine translation. This method is interesting because it involves training two RNNs simultaneously. In these networks the inputs are pairs of sentences in different languages. For example you can feed the network an English sentence paired with its French translation. With enough training you can give the network an english sentence and it will translate it to french! This model is called a Sequence 2 Sequences model or Encoder Decoder model.\n\nThis diagram shows how information flows through Encoders Decoder model. This diagram is using a word embedding layer to get better word representation. A word embedding layer is usally GloVe or Word2Vec algorithm that just takes a bunch of words and creates a weighted matrix that allows similar words to be correlated with each other. Using an embedding layer genererally makes your RNN more accurate because it is a better representation of how similar words are so the net has less to infer.\n\nRecurrent Neural Networks have been becoming very popular as of recently and for a very good reason. They\u2019re one of the most effective models out for natural language processing. New applications of these models are coming out all the time and its exciting to see what researchers come up with.\n\nTo play around with some RNN check out these awesome libraries\n\nKeras\u200a\u2014\u200aa high level machine learning package that runs on top of Tensorflow or Theano: https://keras.io", 
        "title": "Recurrent Neural Networks for Beginners \u2013 Camron Godbout \u2013"
    }, 
    {
        "url": "https://medium.com/the-mission/deep-learning-in-healthcare-challenges-and-opportunities-d2eee7e2545?source=tag_archive---------1----------------", 
        "text": "Dr. Dave Channin received a Bachelor\u2019s degree in computer science and molecular biology from Brandeis University. After graduation, he worked as a programmer for a couple of years, and then left the United States to study medicine at the Facult\u00e9 de Medicine Lariboisi\u00e8re-St. Louis in Paris. Returning to the USA, Dr. Channin completed medical school and residency in radiology at the Penn State College of Medicine. At the completion of residency, Dr. Channin was recruited to Northwestern University as the principal architect of the Northwestern Memorial Hospital PACS. In 2010, Dr. Channin became Chair of the Guthrie Clinic medical imaging service line. There, he had shared administrative responsibility for imaging at 4 hospital and 7 outpatient locations, performing 240,000 procedures per year. In 2015, Dr. Channin left Guthrie to return to his roots in informatics and technology, founding Insightful Medical Informatics, Inc.\n\nThis is healthcare and healthcare, itself, is fundamentally different from every other industry. People assign the highest priority to their health (or lack thereof), expect the highest levels of care and service regardless of cost and are more emotional and ideological about this industry than any other. Because it consumes 17.5% of US GDP and still does not meet societal expectations, it is the most regulated aspect of American society.\n\nActually, I was a computer programmer who became a radiologist and through an interest in R&D became an entrepreneur. Radiology, in particular, is a great specialty in which to find a technology driven path and apply the tools of the programmer.\n\nThe challenge to starting a medically relevant company is identifying the niche upon which you are going to focus. Work backward from the patient and their pain and suffering. Do not underestimate the size, complexity and regulation of the American healthcare system and the scientific rigor to which you will be held. Consider the American healthcare system as an ugly shrub that only 200 years of carefully metered cuts will transform it into the bonsai we all so desire. It is unrealistic to think you will uproot the entire shrub to plant something new. Even your branch may take decades to change.\n\nCollaborate with people who are already in healthcare. You will be surprised by their insights and their desire to improve the system.\n\nIn today\u2019s environment, everything done in healthcare must address the pillars of the Triple AIM; improve the health of populations, lower the cost of care, or improve the patient experience. Some add a fourth aim of improving the provider experience so as to recruit and retain the best people. If your product or service does not address one or more of these, don\u2019t bother.\n\nMedicine is an art and a science but the science dominates the art. Medicine, directly or indirectly, is evidence-based and sooner or later you are going to have to produce hard scientific data to back up your marketing claims. The road from Hippocrates to HIPAA is littered with snake oil and its promoters.\n\nAssume it is a zero sum game. You are going to make money in this business by taking it away from someone else. They, their lobbyists, legal staff and everyone else they can muster are going to try and stop you and maintain their playing field advantages.\n\nYou are dealing with a large number of highly educated, highly trained, highly enculturated individuals. Respect the validated, accumulated knowledge and wisdom and the culture of altruism, empathy and compassion; challenge unvalidated beliefs, disrupt bad workflow and bureaucracy and help these people do what they do best, better.\n\nIt is important to remember that \u2018artificial intelligence\u2019 (in the largest, traditional sense) and \u2018algorithmic learning\u2019 has been applied to medical data including images since the earliest days of computing. Computer assisted diagnosis systems have been around since the 1970s. Automated processing and analysis of one-dimensional time signals (e.g., electrocardiograms) has been around for decades. Computer aided detection and diagnosis of medical images (e.g., Papanicolau smear cytology, detection of masses and microcalcifications in mammograms) have also been around for quite some time. Some of the latter already use deep learning techniques such as convolutional neural networks.\n\nThe current interest in deep learning in healthcare stems from two things. First, the flowering of machine learning techniques, in general, and especially unsupervised learning techniques, in the commercial space with the likes of Google, Facebook and IBM Watson. The second factor is the explosion of available healthcare data (lagging only slightly the explosion of internet data) that was triggered by the HITECH portion of the American Recovery and Reinvestment Act (ARRA). The latter effectively transformed medical records from carbon paper to silicon chips and made that data, structured and unstructured, available.\n\nData in, data out and regulation.\n\nThis is the \u201cdata in\u201d problem. The problem is not privacy. The use of medical subjects and data in research, including research to develop new technologies, is well established both within the context of Federal Policy for the Protection of Human Subjects (the so-called, \u201ccommon rule\u201d) and HIPAA. Even the transfer of technology and intellectual property developed with federal research dollars to the private sector has been facilitated for decades by the Bayh-Dole Act of 1980. Companies in this space \u201conly\u201d need to respect policy, paperwork and process.\n\nThe real \u201cdata in\u201d problem, affecting deep learning applications, especially, but not exclusively, in medical imaging, is truth. Truth means knowing what is in the image. It is very easy to get a large number of images of hats and have people annotate the images that contain red hats or fedoras. Crowdsourcing to millions (billions?) of people, the annotation or validation of data (e.g., CAPTCHA) can also work to create/validate large datasets. Other small and large annotated datasets, for specific recognition tasks, have been created by government, academia and industry at no small cost in time and money.\n\nMedical images are much more complex. There are dozens of kinds of medical imaging devices each producing images according to their respective physical principles. These machines are producing images of hundreds of different anatomic structures and normal variants and pathophysiologic processes resulting in thousands of observable imaging features.\n\nIn the case of supervised learning, and creating annotated datasets, it is important to remember that in the United States, there are only approx. 35,000 people trained and licensed to annotate all of those observable imaging features (though there are perhaps triple that number that could contribute annotations in their specialty areas).\n\nLarge numbers of patient imaging studies performed with digital technologies over the past 30 years have been annotated by this rolling population of 35,000 experts. The vast majority of those annotations, however, are in the form of unstructured free text and are absent links to the coordinates of the pixels containing the image features that engendered the annotation. The good news is that there is a new standard for Annotation and Image Markup (AIM) that was developed under a National Cancer Institute program and anyone developing annotated medical imaging data sets ignores the importance of standardized annotation at their peril.\n\nBut you can\u2019t just take single annotations from one of the 35,000. Even though they are experts and very good at what they do, they are human and make mistakes. So you have to have consensus annotations by multiple expert observers.\n\nWell, yes, you could but you might suffer from garbage in\u200a\u2014\u200agarbage out. There are thousands of imaging procedures. The Current Procedural Terminology (CPT) and other code sets used to classify and bill for these procedures lack the granularity to characterize the exact nature of the imaging performed. It turns out, there are 11 or so ways to produce a radiograph of the chest. The billing code, 71020, can be used for any two of these 11 views. In computed tomography (CT) there are dozens of parameters that can be varied to produce images, including whether or not the patient was injected with contrast media. In magnetic resonance imaging, even more parameters. Which of those parameters are going to affect the output of the unsupervised system? There are no widespread, detailed standards for the acquisition of medical imaging studies. The good news is that there is a developing standard for the nomenclature of imaging studies (the Radiological Society of North America\u2019s RadLex\u2122 playbook now being harmonized with LOINC). Furthermore, medical imaging has one of the best standards, DICOM, that specifies, in infinite detail, the metadata of medical images, so you can use this information to assist an intelligent triage of the images. As the saying goes, \u201cDICOM is always documented in brown, because it is clear as mud, but delivers like UPS.\u201d\n\nStandards for non-image structured data are less, ummm, standardized. Even then, much non-image medical data is still unstructured (e.g., notes or structured laboratory data transformed into unstructured document formats). Vocabularies, lexicons and ontologies are mature but schemata and usage still have large local variance.\n\nLastly, there is no central clearinghouse or national interoperability for medical record data though some has been in development for a decade or more. Each institution, cluster of institutions or other association of data stewards act on their own within the limits of the law. So, obtaining high quality annotated data sets for both supervised and unsupervised learning will remain a costly challenge for years to come.\n\nLet\u2019s say that you\u2019ve overcome the data-in hurdles, you\u2019ve acquired a great, annotated data set and the results on the test set are great. Now you have to validate it; compare the performance of your system to humans for this task and, I would warn, humans are very good at these tasks. This is done by performing an observer performance study and calculating a receiver operating characteristic curve that relates to the observer\u2019s sensitivity and specificity. And since you are hoping the difference between your system and the human is small, the study must be large to have the statistical power to distinguish the two. These experiments take time and are costly to perform. Perhaps the system and the human used together are better than either alone? Does the system speed up the interpretation process or slow it down? I don\u2019t want to throw any shade, but humans can determine gross normality of a chest radiograph in 200 milliseconds (Radiology. 1975 Sep;116(3):527\u201332).\n\nOK. You\u2019ve got an AI and it\u2019s good enough for clinical use. How are you going to deliver your result to the clinician, radiologist or other anticipated user of the system and incorporate it into the electronic medical record? Their eyes are not fixed to generic development platforms like iOS or Android. Rather, they are attached to large, expensive, proprietary, often regulated devices and systems. There are standards for integration and interoperability but they must be addressed.\n\nThe first challenge is not to ignore the 800-pound gorilla in the room. Start early. Find out if your device is a device. I would argue that if your deep learning system is going to do anything meaningful it is going to be a device but there is plenty of guidance available to help the developer make that determination. Once you determine that your device is a device, you can determine what class of device it is and whether any exemptions apply. The class of the device is \u201cbased on the level of control necessary to assure the safety and effectiveness of the device.\u201d These determinations will define the path you will take to FDA approval of your device.\n\nAgain, policy, paperwork, process. One fundamental philosophy of the FDA is \u201cQuality System (QS) Regulation/Medical Device Good Manufacturing Practices.\u201d While we all love \u2018garage code\u2019 that gets us 7 million users in 7 days, the FDA will insist that the code was developed with common good manufacturing process (CGMP). There are many software development methodologies that will meet CGMP and you might as well start using one from day one. Similarly, the FDA will look for GMP and appropriate regulations to have been applied to any data you use and any experiments you perform to validate that data.\n\nIdentify who is going to shepherd your company and product through the FDA process. Do you have a lawyer, accountant and CFO to deal with the IRS? You will probably need similar for the FDA. Prepare as much as you can in advance and work in parallel as much as possible.\n\nHow smart is the gorilla and how good is he at his job? Pretty smart and fairly good. The FDA works by assigning devices for evaluation to one of 16 medical specialty \u201cpanels\u201d. These panels rely on published and unpublished scientific studies. One power of the FDA is its ability to convoke panels of industry and academic experts to analyze the evidence. The radiology panel has, for example, already approved \u201cAnalyzer, Medical Image\u201d (govspeak) systems based on deep learning techniques such as convolutional neural networks.\n\nThe system is, admittedly, slow. This is not, however, solely due to the nature of a large government bureaucracy. Following and documenting the CGMP process, even for software, is tedious and time consuming. Performing and documenting the scientific validation is meticulous and time consuming. Statistical analyses, publishing and analyzing the published and unpublished results all take time. Remember, we are talking about a medical device that could diagnose or steer the diagnosis in many directions. It seems like a demonstration of \u201csafety and effectiveness\u201d is only just that for which your mother would ask before she allowed it to be used on her.\n\nThe value of deep learning systems in healthcare comes only in improving accuracy and/or increasing efficiency. Healthcare, today, is a human\u200a\u2014\u200amachine collaboration that may ultimately become a symbiosis or even cyborg relationship. We are still at the stage, however, that we have both humans and machines each performing both tasks at which they are suboptimal. As deep learning systems develop and evolve they will more and more assist humans with those tasks at which humans are not good. So, for example, humans are very good at processing information from their senses including vision. They are very good at perceiving human emotions. But humans are not so good at remembering things, searching for and organizing data and not too good at correlating and reasoning about that data. So I foresee DL systems that will make physicians and other providers faster and smarter in their diagnoses and reduce uncertainty in their decisions thereby avoiding costs and hazards and saving time.\n\nMedicine, in general, and radiology, perhaps more so than any other specialty, has been very good at developing and adapting to new technology. The golden road to the annual meeting of the Radiological Society of North America (the largest medical meeting in the world) is paved with technological innovation. Many fundamental technology \u201csea changes\u201d have occurred in radiology, in a relatively short time, many within our lifetimes. For example, the transition within a decade or two from film based imaging to digital imaging. Dark room staff (large numbers of whom were blind!)? Eliminated like buggy whip manufacturers. Film file storage (c.f., The Cleveland Clinic X-Ray Fire of 1929) \u201clibrarians\u201d? Reduced or eliminated. Job loss? Some, but not as much as you would think. The transformation to digital and the (ongoing) explosion of new imaging modalities opened new opportunities as did work in the information systems and the changing healthcare environment itself. Industrial disruption? Sure (c.f., Kodak where the small, growing digital siamese twin slew the body of the mighty film producer). Job loss? Some, especially locally. But less than expected given the number of healthcare information technology companies that arose in parallel.\n\nWhat about radiologists? Remarkably adaptable to technology perceived as positive to the patient or the institution. At one institution, in 1999, 25 radiologists went from reading images on film to reading images on computer workstations overnight without a significant degradation in accuracy or efficiency. Eventually, they were faster on the new workstations and with new, learned behaviors could never return to film. Fewer radiologists? Not really as new uses for imaging and new imaging technologies were developed. Look how well radiologists have adapted first to mammography (special techniques and technology) then digital mammography, then digital mammography with computer assisted detection/diagnosis and now digital breast tomosynthesis. Accuracy and efficiency have incrementally increased at each step to the benefit of women everywhere. Fewer mammographers and radiologists? Not really.\n\nWe, as a society, are going to have to face the accelerating pace of automation and its impact on the workforce and society. There is, however, nothing to suggest to me that these effects will occur faster or in different form in healthcare and in particular due to deep learning. Do I still recommend Radiology as a career to high school and college students? Absolutely.\n\nI see unlimited opportunity to improve the system. Despite current best efforts, there are innumerable inaccuracies and inefficiencies in the system that are ripe targets for DL and other technologies. The most important consideration is to choose your target wisely. Don\u2019t lose sight of the link between the accuracy and efficiency you improve and the pain and suffering you reduce.", 
        "title": "Deep Learning in Healthcare: Challenges and Opportunities"
    }, 
    {
        "url": "https://medium.com/udacity/this-week-in-machine-learning-12-august-2016-7a6a8d77919a?source=tag_archive---------3----------------", 
        "text": "This week\u2019s top Machine Learning stories, including detecting zero-day exploits, optimizing deep learning, detecting sarcasm, and more!\n\nMachine Learning is one of the most exciting fields in the world. Every week we discover something new, something amazing, something revolutionary. It\u2019s incredible, but it can also be overwhelming. That\u2019s why we created This Week in Machine Learning! Each week we publish a curated list of Machine Learning stories as a resource to help you keep pace with all these exciting developments. New posts will be published here first, and previous posts are archived on the Udacity blog.\n\nWhether you\u2019re currently enrolled in our Machine Learning Nanodegree program, already working in the field, or just pursuing a burgeoning interest in the subject, there will always be something here to inspire you!\n\nGoogle uses its own DeepMind machine learning algorithms to optimize its data centers, resulting in a 40% drop in the amount of energy devoted to cooling.\n\nA new branch of machine learning, transfer learning, promises to streamline deep learning by discovering features that transfer to new tasks.\n\nA paper presented at the Flash Memory Summit in Santa Clara, California reveals machine learning can be used to extend the life of flash memory.\n\nScientists at Arizona State University develop a new machine learning algorithm for identifying zero-day security exploits as they spread around the hacker community.\n\nApple acquires Seattle-based Turi, a machine learning startup that aims to make machine learning and artificial intelligence more accessible to app developers.\n\nResearchers at the University of Lisbon and UT-Austin create a deep learning model that can identify sarcasm in tweets based on the tweet itself and its account\u2019s history.", 
        "title": "This Week in Machine Learning, 12 August 2016 \u2013 Udacity Inc \u2013"
    }, 
    {
        "url": "https://medium.com/ai-business/dena-ai-usecase-ppt-58dfa619845a?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "DeNA\u306b\u304a\u3051\u308b\u4eba\u5de5\u77e5\u80fd\u6d3b\u7528\u4e8b\u4f8b \u2013 Team AI Blog \u2013"
    }, 
    {
        "url": "https://blog.skillyes.com/accelerating-growth-of-nvidia-40d6ba123f54?source=tag_archive---------5----------------", 
        "text": "", 
        "title": ""
    }, 
    {
        "url": "https://medium.com/@YvesMulkers/data-driven-decision-making-who-should-make-the-decisions-you-or-the-numbers-3e9e0d1572df?source=tag_archive---------6----------------", 
        "text": "With the tsunami of data that business leaders have available, many find themselves trying to strike a delicate balance between acting on what the spreadsheet reveals and what their instincts are telling them.\n\nIf you\u2019re a seasoned leader who is accustomed to relying on your gut, you are not alone.\n\nIn a 2014 PwC survey, only about 30 percent of executives reported relying first and foremost on data for their most recent big business decision.\n\nHowever, according to an Accenture survey in the same year, 89 percent of executives polled rated big data as \u201cvery important\u201d or \u201cextremely important\u201d to the digital transformation of their businesses.\n\nFor graduates of business analytics programs, the best result of the Accenture report is likely the fact that 91 percent of the companies surveyed had plans to increase their data expertise in the very near future.\n\nThe dichotomy between the two reports highlights the fact that both instinct and data are essential to making good business decisions and that the key is learning to nurture them both.\n\nRelated Article: Ask the Market Experts: Which Conference is Indispensable to You?\n\nWith the increasing emphasis on data-based decision-making, many wonder if the role of instinct is becoming extinct in the business world. Sri Sharma, founder and managing director of the award-winning digital advertising agency Net Media Planet, told the Guardian that it\u2019s not: \u201cI think of your instinct, your gut feeling, as a personal radar that is built up over the years. Often the data you analyze confirms the instinct of your personal radar, but it can\u2019t replace it. Instinct is vital.\u201d\n\nIn a piece for Fast Company, serial entrepreneur Faisal Hoque, author of \u201cEverything Connects,\u201d contends that intuition is as important as ever but leaders need to hone their abilities to balance it with a quantifiable approach: \u201cSuccess comes from connecting the dots between our emotional selves and systematic thinking that can be checked quantitatively.", 
        "title": "Data-Driven Decision Making: Who Should Make the Decisions, You or the Numbers?"
    }, 
    {
        "url": "https://medium.com/@Adfab/les-grains-de-la-veille-6-29eebbf80741?source=tag_archive---------7----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Les grains de la veille #6 \u2013 Adfab \u2013"
    }, 
    {
        "url": "https://medium.com/the-data-intelligence-connection/data-driven-decision-making-who-should-make-the-decisions-you-or-the-numbers-405396707398?source=tag_archive---------8----------------", 
        "text": "With the tsunami of data that business leaders have available, many find themselves trying to strike a delicate balance between acting on what the spreadsheet reveals and what their instincts are telling them.\n\nIf you\u2019re a seasoned leader who is accustomed to relying on your gut, you are not alone.\n\nIn a 2014 PwC survey, only about 30 percent of executives reported relying first and foremost on data for their most recent big business decision.\n\nHowever, according to an Accenture survey in the same year, 89 percent of executives polled rated big data as \u201cvery important\u201d or \u201cextremely important\u201d to the digital transformation of their businesses.\n\nFor graduates of business analytics programs, the best result of the Accenture report is likely the fact that 91 percent of the companies surveyed had plans to increase their data expertise in the very near future.\n\nThe dichotomy between the two reports highlights the fact that both instinct and data are essential to making good business decisions and that the key is learning to nurture them both.\n\nRelated Article: Ask the Market Experts: Which Conference is Indispensable to You?\n\nWith the increasing emphasis on data-based decision-making, many wonder if the role of instinct is becoming extinct in the business world. Sri Sharma, founder and managing director of the award-winning digital advertising agency Net Media Planet, told the Guardian that it\u2019s not: \u201cI think of your instinct, your gut feeling, as a personal radar that is built up over the years. Often the data you analyze confirms the instinct of your personal radar, but it can\u2019t replace it. Instinct is vital.\u201d\n\nIn a piece for Fast Company, serial entrepreneur Faisal Hoque, author of \u201cEverything Connects,\u201d contends that intuition is as important as ever but leaders need to hone their abilities to balance it with a quantifiable approach: \u201cSuccess comes from connecting the dots between our emotional selves and systematic thinking that can be checked quantitatively.", 
        "title": "Data-Driven Decision Making: Who Should Make the Decisions, You or the Numbers?"
    }
]