[
    {
        "url": "https://medium.com/intuitionmachine/is-conditional-logic-the-new-deep-learning-hotness-96832774907b?source=tag_archive---------0----------------", 
        "text": "One clear trend is the pervasive use of conditional logic in state-of-the-art Deep Learning (DL) architectures. An even bigger trend to this is that DL architectures are beginning to look more like conventional computers (see: DeepMind\u2019s Differentiable Neural Computer). In fact, you can follow the progress of DL research by recognizing how classical computational constructs are retrofitted into this new architecture.\n\nAnyone who has an exposure to programming (or alternatively flowcharting) is aware that its composed of 3 kinds of things. There are 3 kinds of things that are explicitly obvious: computation, conditional logic and iteration (or recursion). In the beginning there was just the neuron, it had a computational unit (i.e. sum of products) and a conditional unit (i.e. activation function). Layers were added, to give it a way to be composed in an many different ways, this begat the Deep Learning revolution (this is the overly simplified version of the genesis story of DL).\n\nFolks then said, let\u2019s add loops into the network. That begat the RNN. The RNN was quite chaotic so a solution that included memory (or a buffer) that led to the more usable LSTM. The computational unit was further enhanced through a more powerful matching engine known as the convolution, this begat the Convolution Network (ConvNet). The ConvNet also had a more general activation function, the pooling layer. The work horses of classical DL are ConvNets and LSTMs.\n\nThere is plenty of research work with the areas of improving the computational unit as well as in introducing memory. However, I have found the developments in conditional logic quite fascinating and illuminating. This is because a conditional unit is so simple that it can\u2019t possibly be very interesting. In this article, I will show you why the neglected conditional logic unit is the hottest thing since the \u201cResidual network\u201d.\n\nLet\u2019s start then with the Residual network. The Residual network surprised the DL community by besting the 2015 ImageNet benchmark with a record breaking number 152 layers. 8 times deeper than previously seen. There have been several research papers that analyze the behavior of the Residual network. The latest one comes from Schmidhuber et. al. \u201cHighway and Residual Networks learn Unrolled Iterative Estimation\u201d. Their conclusions are important:\n\nThis confirms other research on this subject. In addition though they discovered the following:\n\nResidual networks since 2015 are all the rage everywhere. Systems that have used Residual connections (aka skip, shortcut, passthrough or identity parameterization) have been shown to best the state-of-the-art in all too many occasions. Residual connections have become a mandatory feature for any state-of-the-art architecture. By throwing conditional logic into the concoction we arrive at an even more potent potion.\n\nMicrosoft has some very interesting stuff that is in fact related to this called Conditional networks \u201cDecision Forests, Convolutional Networks and the Models in-Between\u201d:\n\nWith some very impressive results:\n\nThe Microsoft solution uses conditional logic as network routing parameters. It is similar to a gated residual, but one that does not skip layers.\n\nActivation functions and pooling layers have limitations. They are fixed and not modified during training. Furthermore, they are not information preserving in that they always destroy information between layers. Residuals and conditional networks are selective in how information traverses through layers. This kind of routing capability is not entirely new in that we\u2019ve seen it in LSTMs. What is new though is that the routing happens not within a single LSTM node but rather across layers.\n\nWhenever we talk about conditional logic we are faced with the problem of how to handle discrete values. DeepMind appears to have avery interesting ICLR 2017 paper that they call the Concrete distribution ( \u201cThe Concrete Distribution: A Continuous Relaxation of Discrete Random Variables\u201d). The distribution provides a method to backpropagate through a discrete, rather than the typical continuous, variable. These can lead to further refinement on how we train for conditional logic.\n\nNevertheless, when we look at the success of this new kind of architecture, we are left being perplexed in that it is difficult to image how SGD can reach convergence. Conditional logic will imply that entire sections of networks become inactive in certain contexts. I suspect that are certain constraints exists that guide the effective use of conditional logic. We\u2019ll explore this in more detail later.\n\nRecursion divides the problem into smaller pieces and drastically reduces the domain of each neural network component, making it tractable to prove guarantees about the overall system\u2019s behavior. Our experience suggests that in order for neural architectures to robustly learn program semantics, it is necessary to incorporate a concept like recursion.", 
        "title": "Is Conditional Logic the New Deep Learning Hotness?"
    }, 
    {
        "url": "https://medium.com/dt42/what-startup-accelerators-really-give-you-113ef8cf1718?source=tag_archive---------1----------------", 
        "text": "As a startup founder with so little resources, you must have a lot to deal with everyday. You want some help, that\u2019s why people now often ask \u201cDo accelerators really help startups?\u201d \u201cHow does one determine if they\u2019re a good fit or if it\u2019s the right time to join an accelerator?\u201d\n\nThe number of startup accelerators/incubators has increased rapidly over the past years. You must spend some efforts on researches in order to find the right program for your team. In our very humble opinions, you need to clarify your purpose by asking yourself first\n\nAfter five weeks working very closely with the #Zeroth01 program, we believe it\u2019s time to share the values we perceive\u00a0. Here are the 5 reasons why we think Zeroth is so unique\uff1a\n\nThe most obvious benefit of an accelerator is the network of people that you connect with. As a start-up company based in Taiwan, all our members are Taiwanese with limited connections with people from the Western countries. Our goal is to build the super robust deep learning product for the whole world. Like many other entrepreneurs, one of the biggest challenges we faced was to try to build a solid network with the right people.Zeroth is highly specialized in our relevant field\u200a\u2014\u200aAI/ML so that it is able to provide the teams access to world-class connections.\n\nThe first 2 weeks in Zeroth were \u201cMentor Madness\u201d, and it is called madness for a reason. We met about 20 mentors over 10 days. It\u2019s like speed dating, you have 20 minutes to talk about your company, ask the right questions and make an impression. The mentors include successful entrepreneurs, consultants and investors, such as Jaan Tallinn, co-founder of Skype, and Rui Ma, partner at 500 Startups China.\n\nThe top three important instructions we received from Zeroth are focus, focus and focus. Nothing is more important than to focus on improving your core value.\n\nWe believe many of the start-up founders must be familiar with the following situation: You have too much on your plate and too less time to focus on your product. You thought they are all necessary for building your own business, but maybe they are not. Let\u2019s do some math, assuming you spend 10 hours talking with potential customers and investors but only 1 hour on the product per day, that\u2019s only about 10 days a year.\n\nTime is precious. We realize the fact that we cannot take all advices and cannot do all. Staying focused and be exceptional are the keys. Now, whenever there\u2019s a decision to make, we ask ourselves the following questions:- Is this really necessary or it is just another distraction? Will we still be in line with our mission after taking this task? Does this task bring us closer to the goals or farer?\n\nAn accelerator can\u2019t make a decision for you, actually no one can, instead, they will give you guidance, help you make the informed decision.\n\nAnother vital thing we learned from the program is to be true to your business. Being a startup is like being a little kid. A lot of people try to tell you that you need to study very hard to enter the best university in the world. However, they are not you and are not responsible to your life. What if you don\u2019t like study science? What if you are actually a musician? In the end, the most important thing is to find who you are and what you are capable of.\n\nIn Zeroth, they don\u2019t just like put you through another sort of \u201cMBA courses 101\u201d for startups founders. They really get under the skin of the actual thing you\u2019ve built and offer you guidance and direction. Help you shape your business, and maybe help it grow.\n\nAccelerator environments are typically really intense, it pushes you to Do More Faster and makes you to achieve as much as you can in 3 months. This does require commitment, a tremendous dose of passion, and the determination to do everything it takes.\n\nWorking in a dynamic and energetic environment is stimulating to everyone. For the first two weeks in the program, we went back to hotel at late night every single day with the notebooks full of feedback, to-do lists, a stack of business cards, and the tiredness. A good accelerator may be super-intense and fast-paced;that\u2019s how it shortens your roadmap and accelerates your business. When we left Hong Kong and came back to Taiwan\u200a\u2014\u200awhere our business would be built, Zeroth didn\u2019t relieve the pressure when we were away. We kept good communication with the program to make sure our team grew as fast as we were in Hong Kong.\n\nWe love Zeroth for its free milk, detox water, beer on tap all the time. Just kidding. Never give up your equity just for free beer!\n\nPeople are saying that as a tech startup, you are judged by your technology, traction and team. We believe it goes the same way when a startup company is choosing an accelerator:\u00a0\n\nTechnology (product)\u200a\u2014\u200aMentors\uff1b\n\nTraction\u200a\u2014\u200aAlumni\uff1b\n\nTeam\u200a\u2014\u200aThe Program Operator.\u00a0\n\nWe LOVE Tak and his team! We believe a lot in them and we want to cowork with them, grow with them.\n\nIt\u2019s amazing how Zeroth makes it so stressful yet cheerful working with them, so big thanks to Tak, Mike, Sumner, Maggie, Adam and Kate.\n\nHopefully this piece of our story is helpful to other entrepreneurs who are considering having their company participate in a accelerator. Positive experience depends on setting realistic expectations and understanding what these programs can and can\u2019t do. We believe everyone can gain from joining the right accelerator for you.", 
        "title": "What Startup Accelerators Really Give You? \u2013 DT42 \u2013"
    }, 
    {
        "url": "https://medium.com/machine-learning-world/rbm-based-autoencoders-with-tensorflow-509fb9727ebb?source=tag_archive---------2----------------", 
        "text": "Recently I try to implement RBM based autoencoder in tensorflow similar to RBMs described in Semantic Hashing paper by Ruslan Salakhutdinov and Geoffrey Hinton. It seems that with weights that were pre-trained with RBM autoencoders should converge faster. So I\u2019ve decided to check this.\n\nThis post will describe some roadblocks for RBMs/autoencoders implementation in tensorflow and compare results of different approaches. I assume reader\u2019s previous knowledge of tensorflow and machine learning field. All code can be found in this repo\n\nRBMs different from usual neural networks in some ways:\n\nNeural networks usually perform weight update by Gradient Descent, but RMBs use Contrastive Divergence (which is basically a funky term for \u201capproximate gradient descent\u201d link to read). At a glance, contrastive divergence computes a difference between positive phase (energy of first encoding) and negative phase (energy of the last encoding).\n\nAlso, a key feature of RMB that it encode output in binary mode, not as probabilities. More about RMBs you may read here or here.\n\nAs prototype one layer tensorflow rbm implementation was used. For testing, I\u2019ve taken well known MNIST dataset(dataset of handwritten digits).\n\nAt first, I\u2019ve implement multilayers RBM with three layers. Because we do not use usual tensorflow optimizers we may stop gradient for every variable with tf.stop_gradient(variable_name) and this will speed up computation a little bit. After construction two questions arose:\n\nSo I\u2019ve run the model with all binary units and only with last binary unit. And it seems that model with only last layer binarized trains better. After a while, I note that this approach was already proposed in the paper, but I somehow miss this.\n\nSo let\u2019s stop with the last layer binarized and try different train approaches. To build model that will train only pair of layers we need train two layers model, save it, build new model with one more layer, load pre-trained first two layers weights/biases and continue train last two layers (code). During implementation I\u2019ve met some trouble\u200a\u2014\u200atensorflow have no method to initialize all not initialized previously variables method. Maybe I just didn\u2019t find this. So I\u2019ve finished with approach when I directly send variable that should be restored and variables that should be initialized.\n\nAfter testing seems that both training approaches converge to approximately same error. But some another cool stuff\u200a\u2014\u200athe model that was trained by pair lairs trains faster in time.\n\nSo we stop with RBM trained with only last layer binarized and with two layers only strategy.\n\nAfter getting pre-trained weights from RMB, it\u2019s time to build autoencoder for fine tuning. To get encoding layer output as much as possible binarized as per paper advice we add Gaussian noise before layer. To simulate deterministic noise behavior, noise generated for each input prior training and not changed during training. Also, we want compare autoencoder loaded from RBM weights with self-initialized usual autoencoder. Code for autoencoder.\n\nIt seems that RBM initialized autoencoder continue training, but newly initialized autoencoder with same architecture after a while stuck at some point.\n\nAlso, I\u2019ve trained two autoencoders without Gaussian noise. Now we can see through distribution what embedding most similar to binary (code for visualization):\n\nWe can see that RBM based autoencoder with Gaussian noise works better than other for our purposes.\n\nTo validate received embeddings I generate them for test and train sets for such networks:\n\nand use two validation approaches:\n\nTrain SVM with the train set and measure accuracy on the test set. SVM was used from sklearn with \u2018rbf\u2019 kernel with no max_iter == 50. Results table were generated with this code\n\nWith Hamming distance or dot product find ten most similar pictures/embeddings to provided one and check how many labels are the same to the submitted array label. Code to check distance accuracies.\n\nAs we can see embeddings can save some strong features, that can be used for future clusterization very well. But these features are not linearly correlated\u200a\u2014\u200aso when we measure accuracy for most similar embeddings, we get results worse than when we use full MNIST images. Of course, maybe autoencoder should be trained with another learning rate/longer, but this is the task for future research.\n\nFor RBM training such params were used network was trained with:\n\nFor RBM training such params were used network was trained with:\n\nFor autoencoder learning rate was changed to 1.0 because of another optimization rule.", 
        "title": "RBM based Autoencoders with tensorflow \u2013 Machine Learning World \u2013"
    }
]