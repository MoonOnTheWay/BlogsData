[
    {
        "url": "https://medium.com/@hartator/the-challenges-of-machine-learning-on-ios-a5de6bd3adcb?source=tag_archive---------0----------------", 
        "text": "I\u2019ve just published an iOS app that heavily uses machine learning. It tries with more or less success to estimate how much calories from just a food picture. It looks something like this:\n\nI\u2019ll try to explain in this post the challenges encountered while developing this app.\n\nFirst, I wanted to use TensorFlow. I had already learnt TensorFlow at the time for another project. It seems the way to go solution people were using. And it was growing fast. Indeed Google backing, for the best or the worst, in addition of poaching the best people in the machine learning open source community had made TensorFlow a no brainer if you want to start a new machine learning project. Unfortunately, TensorFlow still has short coming. Because of its decentralized nature, TensorFlow is 3x or 4x slower than other frameworks and is a bit more hungry for RAM. Both CPU and RAM are scarce resources on an iPhone. Finally, no one seems to have yet managed to make TensorFlow run on iOS.\n\nI\u2019ve decided then to learn Caffe. Caffe is an awesome machine learning framework. It\u2019s developed by a team of searchers from the Berkeley Vision and Learning Center. It also specializes in computer vision using convolutional neural network. That seemed a good fit for this project. Some members of the Caffe community seemed also to have already managed to make some parts of Caffe run on Xcode. It was a promising start. The less promising part was that one of the developers of Caffe has been hired by Google to work TensorFlow. That will make TensorFlow prominent in the future. However, Caffe models and TensorFlow models are pretty similar. I\u2019ve found actually already tools that allow you to convert Caffe models to TensorFlow models with little work involved. A potential transition in the future should be easy.\n\nUnfortunately, Caffe wasn\u2019t playing well with Xcode 7.x. Some parts needed to be tweaked. Moreover, on iPhones I have only 1GB of RAM to play with. Some Caffe models inflate up to 1.x GB of Ram when they are running. It seems to also depends on the computation. The computation differs for different pictures. So its memory usage differs as well. It makes some bugs difficult to catch. Plus, when you have just at the limit of the iPhone RAM, some times it will work, some times it won\u2019t. For the actual same picture\u2026Memory usage management is a big issue when you want to make convolutional neural networks run on iOS.\n\nThe app technical architecture is still pretty minimal. I\u2019ve been running only 2 models. One for identifying the food. One for giving a scale if the food is greasy or lean. Coupled to average calorie database, a small algorithm is then able to do basic calorie estimation. The model to identify food can identify between 97 different foods. The greasiness/leanness model is still pretty raw. It has only been trained only on around 500 images.\n\nXcode compilation times were an issue. I\u2019ve managed to cut down the compilation times by forcing some optimizations and caching. Nevertheless and oddly, one of ten times, Xcode recompiles everything. It would usually take a few minutes. It can be extremely frustrating.\n\nThere is obvious paths of improvements.\n\nFirst, I need to identify and expand our database to more foods. 300\u2013400 types of food will allow the app to be more precise. Having more rated images of greasy and lean food will be awesome as well. Adding a model to measure precise quantity seems to be necessary as well, but it\u2019s harder to make. Food quantity need to be trained against the different kind of food when greasiness is more easy to evaluate. Last but not least, right now the app can only work with one food at the time. Caffe on iOS is pretty slow. It\u2019s indeed using only the CPU. It doesn\u2019t use the GPU or the metal API. It would be faster to use those. Having it running faster will allow to identify multiple food elements by picture. Without having the user to wait forever. This changes will allow to add more precision to the app. Probably next version.", 
        "title": "The challenges of machine learning on iOS \u2013 hartator \u2013"
    }, 
    {
        "url": "https://humanizing.tech/gpu-wars-have-begun-a-spreadsheet-of-specs-by-phones-vr-self-driving-cars-ai-ba2dae7e3755?source=tag_archive---------1----------------", 
        "text": "GPU Wars Have Begun, A Spreadsheet of Specs by Phones, VR, Self-Driving Cars &\u00a0AI\n\nPay no attention to the onslaught of initials coming at you: GPU, CPU, AI, VR, AR, etc. This is very nerdy post, but with very practical applications. If you play video games, watch videos, and are looking forward to our machine learning enabled future of messaging bots and AR/VR, then you\u2019re going to need GPUs to handle the workload.\n\nAt the moment there are three main places where these GPUs show up:\n\nIn every case, it\u2019s GPUs that are the workhorses, not the CPU (which Intel became famous for developing over the last few decades).\n\nYou might ask what the difference is between CPUs that have been in computers for a long time and the relatively newer GPU. Fancy that, NVIDIA wrote about it. In basic terms, the GPU does lots of similar calculations more quickly and it\u2019s perfect for graphics processing (videos, games) and also machine learning model training (AI).\n\nSo, you might ask, who has the best GPU in the world and why don\u2019t all computers, whether in the \u201ccloud\u201d or in your hands, all use the same one? That\u2019s exactly the question I set out to ask myself a few months ago. After all, 90% of the world chooses a single search engine: Google. So why not the same thing for chips? And as the world moves towards AI and the need for photo-accurate virtual reality landscapes, the GPU is perfectly positioned to handle the workload.\n\nNVIDIA has made the most noise recently with their new GPU aimed at deep learning, self driving cars, and virtual reality gaming. Meanwhile, PowerVR is focused on smart phones, most specifically the iPhone. And so, right out of the gate you start to see a few providers line up against both sides of the market:\n\nBefore we delve deeper into that, it\u2019s worthwhile to note some of the history of why Apple has chosen to go what would appear to be their own direction. Of course, Google also recently threw their hat into the supposed GPU ring with the new custom built chip specifically for running machine learning models in their data center (\u201ccloud\u201d). But not much is known about that very different type of chip.\n\nI\u2019ve probably thrown about 10 hours into the spreadsheet below, trying to pick out nuggets from the manufacturer\u2019s published spec sheets online, various google searches, Anandtech tear downs of devices, and even some forums and blogs experts have written.\n\nWhat I found was that this information is NOT easy to track down. Nobody is really coming out and publishing the fully detailed specs and I can\u2019t seem to find a resource like the spreadsheet below. If you find one, please comment and let me know so I can cross-reference.\n\nI\u2019ve opened it up so anyone, including you, can edit any information in here to make it more accurate. \u201cUse the power of the crowd!\u201d they tell me. And so I have. All that I ask is that you try to keep this as accurate as possible and include a source URL so people can check your work. Please edit anything I may not have the most accurate information on, add more fields, or do whatever you think is best to help build out this resource for the community.\n\nAnd don\u2019t forget to have fun!", 
        "title": "GPU Wars Have Begun, A Spreadsheet of Specs by Phones, VR, Self-Driving Cars & AI"
    }
]