[
    {
        "url": "https://medium.com/intuitionmachine/the-holographic-principle-and-deep-learning-52c2d6da8d9?source=tag_archive---------0----------------", 
        "text": "What I want to talk to you about today is the Holographic Principle and how it provides an explanation to Deep Learning. The Holographic Principle is a theory (see: Thin Sheet of Reality) that explains how quantum theory and gravity interact to construct the reality that we are in. The motivations for this theory comes from the paradox that Hawking created when he theorized that black holes would emanate energy. The fundamental concept that had been violated by Hawking\u2019s theory was that information was destroyed. As a consequence of this paradox, through several decades of research and experimentation, physicists have brought forth a unified theory of the universe that is based on information theoretic principles. The entire universe is a projection of a hologram. It is entirely fascinating that the arrow of time and the existence gravity are but mere manifestations of information entanglement!\n\nNow, you may be mistaken to think that this Holographic Principle is just some fringe idea from physics. It appears at first read to be quite a wild idea! Apparently though, the theory rests on very solid experimental and theoretical underpinnings. Let\u2019s just say that Stephen Hawking who first remarked that is was \u2018rubbish\u2019 has finally agreed to its conclusions. So at this time, it should be relatively safe to start deriving some additional theories of this principle.\n\nOne surprising consequence of this theory is that the hologram is able to capture the dynamics of the universe that has of the order of d^N degrees of freedom (where d is the dimension and N is the number of particles). One would think that the hologram would be of equal size, but it is not. It is a surface area and is proportional only to N\u00b2. This begs the question, how is an structure of order N\u00b2 able to capture the dynamics of a system in d^N?\n\nIn the meantime, Deep Learning (DL) coincidentally has a similar mapping problem. Researchers don\u2019t know how it is possible for DL to perform so impressively well considering the problem domain\u2019s search space has an exceedingly high dimension. So, Max Tegmark and Henry Lin of Harvard, have volunteered their own explanation \u201cWhy does deep and cheap learning work so well?\u201d In their paper they argue the following:\n\nThe authors bring up several promising ideas like the \u201cno-flattening theorems\u201d as well as the use of information theory and the renormalization group as explanations for their conjecture. I however was not sufficiently convinced by their argument. The argument assumes that all problem data follows \u2018natural laws\u2019, but as we all know that DL can be effective in unnatural domains. See, Identifying cars, driving, creating music and playing Go as trivial examples of clearly an unnatural domain. To be fair, I think that they were definitely on to something, and that something I discuss in more detail\u00a0.\n\nIn this article, I make a bold proposal with an argument that is somewhat analogous to what Tegmark and Lin proposed. Deep Learning works so well because of physics. However, the genesis of my idea is that DL works because it uses the leverages the same computational mechanisms underlying the Holographic Principle. Specifically, the capability of representing an extremely high dimensional space (i.e. d^N) with a paltry number of parameters of the order N\u00b2.\n\nThe computational mechanism underpinning the Holographic Principle can be most easily depicted through the use of Tensor Networks (note: These are somewhat different from the TensorFlow or the Neural Tensor Network). Tensor network notation is as follows:\n\nThe value of tensor networks in physics is that they are used to drastically reduce the state space into a network that focuses only on the relevant physics. The primary motivation behind the use of Tensor Networks is to reduce computation. A tensor network is a way to perform computation in a high dimensional space by decomposing a large tensor into smaller more manageable parts. The computation can then be performed with smaller parts at a time. By optimizing each part one effectively optimizes the full larger tensor.\n\nIn the context of the holographic principle, the MERA tensor is used and it is depicted as follows:\n\nIn above the circles depict \u201cdisentanglers\u201d and the triangles \u201cisometries\u201d. One can look at the nodes from the perspective of a mapping. That is the circles map matrices to other matrices. The triangles take a matrix and map it to a vector. The key though here is to realize that the \u2018compression\u2019 capability arises from the hierarchy and the entanglement. As a matter of fact, this network embodies the mutual information chain rule:\n\nIn other words, as you move from the bottom to the top of the network, the information entanglement increases.\n\nI\u2019ve written earlier about the similarities of Deep Learning with \u2018Holographic Memories\u2019 however here I\u2019m going to make one step further. Deep Learning networks are also tensor networks. Deep Learning networks however are not as uniform as a MERA network, however they exhibit similar entanglements. As information flows from input to output in either a fully connected network or a convolution network, the information are similarly entangled.\n\nThe use of tensor networks has been studied recently by several researchers. Miles Stoudenmire wrote a blog post: \u201cTensor Networks: Putting Quantum Wavefunctions into Machine Learning\u201d where he describes his method applied to MNIST and CIFAR-10. He writes about one key idea about this approach:\n\nAmnon Shashua et al. have also done work in this space. Their latest paper (Oct 2016) \u201cTensorial Mixture Models\u201d proposes a novel kind of convolution network.\n\nIn conclusion, the Holographic Principle, although driven by quantum computation, reveals to us the existence of a universal computational mechanism that is capable of representing high dimensional problems using a relatively low number of model parameters. My conjecture here is that this is the same mechanism that permits Deep Learning to perform surprisingly well.\n\nMost explanations about Deep Learning revolve around the 3 Ilities that I described here. These are expressibility, trainability and generalization. There is definitely consensus in \u201cexpressibility\u201d, that is of a hierarchical network requiring less parameters that a shallow network. The open questions however are that of trainability and generalization. The big difficulty in explaining away these two is that they don\u2019t fit with any conventional machine learning notion. Trainability should be impossible in a high-dimensional non-convex space, however simple SGD seems to work exceedingly well. Generalization does not make any sense without a continuous manifold, yet GANs show quite impressive generalizations:\n\nThe above figure shows the StackGAN generating, given text descriptions\u00a0, output images in two stages. For the StackGAN there are two generative networks and it is difficult to comprehend how the second generator captures only image refinements. There are plenty of unexplained phenomena like this. The Holographic Principle provides a base camp to a plausible explanation.\n\nThe current mainstream intuition of why Deep Learning works so well is that there exists a very thin manifold in high-dimensional space that can represent the natural phenomena that it is trained on. Learning proceeds through the discover of this \u2018thin manifold\u2019. This intuition however breaks apart considering the recent experimental data (see: \u201cRethinking Generalization\u201d). The authors of the \u2018Rethinking Generalization) paper write:\n\nBoth the Tegmark argument and the \u2018Thin Manifold\u2019 argument cannot possibly work with random data. This thus lead to the hypothesis that there should exist an entirely different mechanism that is reducing the degrees of freedom (or problem dimension) so that computation is feasible. This compression mechanism exists can be found in the structure of the DL network, just like it exists in the MERA tensor network.\n\nConventional Machine Learning thinking is that it is the intrinsic manifold structure of the data that needs to be discovered via optimization. In contrast, my conjecture claims that the data is less important, rather it is the topology of the DL network that is able to capture the essence of the data. That is, even if the bottom layers have random initializations, it is likely that the network should work well enough subject to a learned mapping at the top layer.\n\nIn fact, I would even make a bigger leap in that in our quest for unsupervised learning, we may have already overlooked the fact that a neural network has already created its own representation of the data at onset of random initialization. It is just our inability to interpret that representation that is problematic. A random representation that preserves invariances (i.e. locality, symmetry etc.) may just be a good as any other representation. Yann LeCun\u2019s cake might already be present and that it is just the icing and cherry that needs to explain what the cake represents.\n\nNote to reader: In 1991, psychologist Karl Pribham with physicist David Bohm had speculated about Holonomic Brain Theory. I don\u2019t know the concrete relationship between the brain and deep learning. So I can\u2019t make the same conclusion that they made in 1991.\n\nI leave you with one quote from Demis Hassabis of DeepMind:\n\n\u2661 Heart if you think this idea has legs!", 
        "title": "The Holographic Principle: Why Deep Learning Works \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://blog.stamplay.com/building-an-ai-powered-digital-asset-manager-with-box-84ebd7cdfc3a?source=tag_archive---------1----------------", 
        "text": "Cloud storage solutions like Box are used to organize content of all kind. Sometime it\u2019s about documents sometimes is about media like images or videos.\n\nKeeping your content organized and searchable turns out to be painful if not done in a smart way, especially for content like images. What if you can automatically add metadata and/or tags to the any image you upload on Box and move them in the right folder with zero manual effort?\n\nWe worked with the Box team and came up with this handy blueprint that will show you how to integrate Clarifai, an AI service for analyzing images and videos, to your Box account to make image organization effortless.\n\nBefore we get started let\u2019s create on Box two folders that our Application will use to organize the content.\n\nNow let\u2019s get a free API Key on Clarifai. Create an account on https://developer.clarifai.com/signup/ then go to Applications and create a new one\n\nOnce the application is created copy the Client ID and Client Secret, we\u2019re going to need them in a couple of minutes.\n\nThis app is available as a Blueprint, a pre-built template to help you get started with proven integration solutions. To get started click here or on the button below:\n\nYou\u2019ll be prompted to pick a name for your project and then a wizard will start. After that Stamplay will prompt you to:\n\nOnce you have connected the two services click on Next.\n\nNow it\u2019s time to fill the blanks of the workflow, you\u2019ll be first asked to select the folder where you\u2019re going to upload your images, that\u2019s why we created the upload folder for.\n\nStart typing \u201cupload\u201d in the dropdown and select the folder once it appears. If it doesn\u2019t it depends by Box timings for indexing new content and make it available for search, so if this is the case you\u2019ll need to wait a few minutes and try again.\n\nOnce the upload folder is selected and you see that the green check below the dropdown you can move to the following step.\n\nThe next step will ask us for the Id of the main folder for this app so we have to provide the Id of the folder \u201cStamplay DAM\u201d. The Id can be found on the address bar of your browser when you\u2019re into it as you can see below:\n\nThe same Id needs to be pasted in the Create Folder step, as this is where our images will be organized and where this application will create subfolders to organize pictures.\n\nLast step is simply to configure and email address where we want to receive notification in case our app is having problems while processing a picture. Type your email address inside the To: field, and a random address in the From field.\n\nThe very last step is to initialize a Box Metadata template by clicking on the link provided. In case your Box is running on a plan that doesn\u2019t support Metadata drop us a note and we can help you setting this app to only use Box Tags.\n\nYou\u2019re good to go now. Just start uploading pictures in the upload folder and you\u2019ll start seeing your content organized.\n\nAt Stamplay we make it easy for people to automate processes and create high value integrations by tying together APIs. If you need help to connect your apps or have an API that you want to make easy to connect with tweet us at @stamplay and/or join our Slack organization.", 
        "title": "Building an AI-powered digital asset manager with Box"
    }, 
    {
        "url": "https://medium.com/@jay.sonavani/what-are-the-possibilities-of-chat-bots-being-hacked-cc78d4aab431?source=tag_archive---------3----------------", 
        "text": "CHAT BOTs have become one of the common terminologies for all of us who are from technology backgrounds. Daily, we hear multiple news about new acquisition, mergers and heavy investment in Bot Technology, Artificial Intelligence, Natural Language Processing and Deep Learning. We all know AI Technology is moving very fast and with all these advancements, one question always hits on my mind\u200a\u2014\u200aHOW SECURE WILL BE THIS chatbot?\n\nWe all know chat bots are programmed to understand human behaviour. These bots learn and grow by interacting with humans and later on it uses this knowledge to provide us more personalised user experience. But now here is the catch, what if hackers decides to misguide BOTs? Consider a scenario where hackers create a similar misguiding BOT or may be a real person who interacts with your business BOT and try to inject incorrect behaviours with misleading and inappropriate responses? How will chatbots be able to overcome such attacks and still behave in a normal way?\n\nFor normal businesses, misleading information may not be a major concern, but for a government organisation, banking & finance sector, healthcare & medicine, incorrect information can really create a havoc.\n\nWhat are your views on it?\n\nShare with me your ideas and views in comment section below. You can reach me directly at jay.sonavani@gatewaytechnolabs.com", 
        "title": "What are the possibilities of CHAT BOTs being HACKED?"
    }
]