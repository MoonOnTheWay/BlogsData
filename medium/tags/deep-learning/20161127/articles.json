[
    {
        "url": "https://medium.com/intuitionmachine/game-theory-maps-the-future-of-deep-learning-21e193b0e33a?source=tag_archive---------0----------------", 
        "text": "If you\u2019ve been following my articles up to now, you\u2019ll begin to perceive, what\u2019s apparent to many advanced practitioners of Deep Learning (DL), is the emergence of Game Theoretic concepts in the design of newer architectures.\n\nThis makes intuitive sense for two reasons. The first intuition is that DL systems will eventually need to tackle situations with imperfect knowledge. In fact, we\u2019ve already seen this in DeepMind\u2019s AlphaGo that uses partial knowledge to tactically and strategically best the world-best human in the game of Go.\n\nThe second intuition is that systems will not remain monolithic as they are now, but rather would involve multiple coordinating (or competing) cliques of DL systems. We actually already do see this now in the construction of adversarial networks. Adversarial networks consists of competing neural networks, a generator, and discriminator, the former tries to generate fake images while the later tries to identify real images. The interesting feature of these systems is that a closed form loss function is not required. In fact, some systems have the surprising capability of discovering its own loss function! A disadvantage of adversarial networks are they are difficult to train. Adversarial learning consists in finding a Nash equilibrium to a two-player non-cooperative game. Yann LeCun, in a recent lecture on unsupervised learning, calls adversarial networks the \u201cthe coolest idea in machine learning in the last twenty years\u201d[LeC].\n\nWe are still in the early stages here of leveraging game theory, but I will point out some papers that have a game theoretic bent in them. David Balduzzi has a framework for deep learning that takes a game theoretic approach. In his paper \u201cSemantics, Representations and Grammars of Deep Learning\u201d[BAL] he writes:\n\nIt is a very elegant approach to covering an otherwise bewildering subject. He has these nice graphs (on adversarial networks) that highlight the strength of his approach:\n\nI would really love to see an entire textbook written with this approach!\n\nDavid Silver and Johannes Heinrich have a paper titled \u201cDeep Reinforcement Learning from Self-Play in Imperfect-Information Games\u201d[SILHEI]. They write:\n\nJason Hartford et al employs Deep Learning to predict human behavior. They write in \u201cDeep Learning for Predicting Human Strategic Behavior\u201d[HAR]:\n\nWhat we see in these 3 players are 3 different ways game theory plays in Deep Learning. (1) As a means of describing and analyzing new DL architectures. (2) As a way to construct a learning strategy and (3) A way to predict behavior of human participants. The last application can make your skin crawl!\n\nMathematics provides us with abstractions that aid us in our understanding of complex systems. However, every form of abstraction has its limitations in that there are certain details that are glossed over. We can sketch out some intuition with geometry, dynamics, and logic as to how these kinds of systems will tend to behave. What we begin to glean from this is that these systems consist of classifiers built from other classifiers. They are a self similar system that should be treated as a collective of many interacting machines. Furthermore, these machines are designed to make predictions out of the future. These predictions need to performed using incomplete and imperfect data. Therefore we need a mathematical framework that studies the behavior of many interacting parties that have different sets of information.\n\nThe classical view of machine learning is that the problem can be cast as an optimization problem where all that is needed are algorithms that are able to search for an optimal solution. However, with machine learning, we want to build machines that don\u2019t overfit the data but rather is able to perform well on data that it has yet to encounter. We want these machines to make predictions about the unknown. This requirement, which is called generalization, is very different from the classical optimization problem. It is very different from the classical dynamics problem where all information is expected to be available. That is why a lot of the engineering in deep learning requires additional constraints on the optimization problem. These, to my disliking, are called \u2018priors\u2019 in some texts and also called regularizations in an optimization problem.\n\nWhere do these regularizations come from and how can we select a good regularization? How do we handle impartial information? This is where a game theoretic viewpoint becomes important. Generalization is sometimes referred to as \u2018structural risk minimization\u2019. In other words, we build mechanisms to handle generalization using strategies similar to how parties mitigate risk. So we have actually returned full circle. Game theory is described as \u201cthe study of mathematical models of conflict and cooperation between intelligent rational decision-makers.\u201d In our quest of understanding learning machines, we end up with mathematics that was meant for the study of the interactions of intelligent beings.\n\nUpdate: New paper on Counter-Factual Reasoning and DL: https://arxiv.org/pdf/1701.01724v1.pdf DeepStack: Expert-Level Artificial Intelligence in No-Limit Poker\n\nFeel to jump into the conversation by requesting an invite at LinkedIn: https://www.linkedin.com/groups/8584076\u00a0, or if you are non-techinical then FB: https://www.facebook.com/groups/deeplearningpatterns or visit us at Intuition Machine.\n\nUpdate: Google/DeepMind releases their new paper \u201cA Unified Game Theoretic Model to Multi-Agent Learning\u201d", 
        "title": "Game Theory reveals the Future of Deep Learning \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/architecture-ilities-for-deep-learning-12a3ff9bec4e?source=tag_archive---------1----------------", 
        "text": "People in software development are familiar with the phrase \u201c-ilities\u201d. It is actually not a word, but you can google it:\n\nYou may think because it is not a real world, that it is some informal convention, some kind of loose jargon. This is actually not the case, software quality attributes have in fact been formalized in ISO 9126:\n\nQuality attributes are realized non-functional requirements used to evaluate the performance of a system. These are informally called \u201cilities\u201d after the suffix that of many of the words share. In software architecture, there is a notion of \u201cilities\u201d that are qualities that are important in evaluating our solutions. Lacking in DL literature is enough of an understanding of how to evaluate quality of a Deep Learning architecture. What then are the \u201cilities\u201d that are specific to evaluating Deep Learning systems?\n\nDespite the newness of the field, there are 3 main \u201cilities\u201d that a practitioner should know of:\n\nExpressibility\u200a\u2014\u200aThis quality describes how well a machine can approximate functions. One of first questions that many research papers have tried to answer is \u201cWhy does a Deep Learning system need to be Deep?\u201d Another way of saying this is, what is the importance of having multiple layers or a hierarchy of layers. There is some consensus in the literature that deeper networks require less parameters than shallow, wider networks to express the same function. You can find more detail of the various explanations here: http://www.deeplearningpatterns.com/doku.php/hierarchical_abstraction. The measure here appears to be, how few parameters (i.e. weights) do we need to effectively create a function approximator. A related research are here is weight quantization, how few bits does one need and not lose precision.\n\nTrainability\u200a\u2014\u200aThe other kind of research that gets published is on how well can a machine learn. You will find hundreds of papers that all try to out do each other by showing how trainable their system is as compared to the \u2018state-of-the-art\u2019. The open theoretical question here is why do these systems even learn at all? The reason this is not obvious is because the work horse of Deep Learning, the stochastic gradient descent (SGD) algorithm, appears absurdly too simplistic to even possibly work! There is a conceptual missing link here that researchers have yet to identify.\n\nGeneralizability\u200a\u2014\u200aThis is a quality that describes how well a trained machine can perform predictions on data that it has not seen before. I\u2019ve written about this in more detail in \u201cRethinking Generalization\u201d where I do describe 5 ways to measure generalization. I think that everyone seems talks about generalization, unfortunately few have a good handle on how to measure it.\n\nIn computer science, we do understand expressibility. This is its most general from is the notion of \u201cTuring Completeness\u201d or \u201cUniversal Computation\u201d (see: \u201cSimplicity of Universal Machines\u201d. Feed-forward networks and Convolution Networks are for example not turing complete simply because the don\u2019t have memory. What Deep Learning brings to the table that is wildly radical from conventional computer science is the latter two capabilities.\n\nTrainability, the ability to train a computer, rather than program a computer is a major capability. This is \u201cautomating automation\u201d. In other words, you don\u2019t need to provide specific detailed instructions, but rather you just need to provide the machine examples of what it needs to do. We\u2019ve actually seen this before in the difference between imperative versus declarative programming. The difference however in Deep Learning (or Machine Learning), we don\u2019t need to define the rules. The machine is able to discover the rules for itself.\n\nEven better, Generalization implies that if the machine, once trained, encounters situations where it has not been shown an example before, is able to figure out how to make the correct prediction. Generalization implies that even after discovering the rules after training, it is now able to create new rules on its own for unexpected situations. The machine has become more adaptable.\n\nThese ilities tie in with the \u201c5 Capability Level of Deep Learning\u201d. At each level we can explore the nature of expressibility, trainability and generalizability we require to achieve that level. So as an example, we can look at machines with the Classification with Memory. What does the additional memory component add to expressibility, trainability and generalizability. In the case for expressibility, we can see that memory permits a machine to perform translation instead of just classification. In terms of trainability, we had to come with additional mechanism to learn how to update memory. Finally, for generalizability we need to use other kind of benchmarks (i.e. BLEU, bAbl) to perform evaluations on this kind of system. At every capability level, we need to re-explore how we achieve each of these 3 ilities.\n\nIdeally we would like to see a framework where one understands how to compose various building blocks driven by an understanding as to how each block contributes to trainability, expressivity or generalization. Deep Learning is still very young in that we have few tools to evaluate the effectiveness of our solutions. Additionally, other ilities such as interpretability, transferability, latency, adversarial stability and security are worth exploring.", 
        "title": "3 Essential Deep Learning Architecture Abilities (i.e. \u201cIlities\u201d)"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/relax-artificial-intelligence-isnt-coming-for-your-job-8f4056de08c6?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Relax, Artificial Intelligence isn\u2019t Coming for Your Job"
    }
]