[
    {
        "url": "https://medium.com/intuitionmachine/the-ultimate-deep-learning-applications-list-434d1425da1d?source=tag_archive---------0----------------", 
        "text": "Deep Learning is eating the world. Here are a few applications that it is digesting.\n\nMicrosoft Skype is able to translate voice into different languages in realtime. Something straight out of the universal translator in Star Trek.\n\nGoogle Photos is able to automatically organize your photographs into collections with common shared themes.\n\nA hobbyist is able to teach his car to self-drive in a few hours.\n\nSketch an image as a query to a visual search.", 
        "title": "40 Ways Deep Learning is Eating the World \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://hackernoon.com/deep-learning-isnt-the-brain-e1d800ebb5a9?source=tag_archive---------1----------------", 
        "text": "DL is power hungry. Alpha GO consumed the power of 1202 CPUs and 176 GPUs, not to train, but just to run. The TenserFlow Processing Unit is an attempt to satiate this hunger, but it\u2019s still not even close to the brain\u2019s power consumption of 20W. IBM\u2019s TrueNorth chip is another example of trying to bring low-power computation, but it\u2019s capabilities are quite limited when compared to other Neuromorphic hardware. Specifically, True North only implements feed-forward networks and has no on-chip learning.\n\nBack-propagation is the foundation of all DL. Although there is evidence that errors being propagated through multiple layers is happening in the brain, no one has come up with a method for back-propagation (back-prop)that doesn\u2019t rely on information propagating backwards through unidirectional synapses. I personally think it\u2019s only a matter of time before a biologically plausible method is discovered, but until then it is unwise to ignore the implementation details and the restrictions it might place on what can be learned.\n\nAlthough it is possible to convert DL networks into spiking neurons for use on neuromorphic hardware, these spikes are not leveraged for specific computational advantages. As far as I know (and I still have some reading to do), spiking computation has yet to be used anywhere for the learning in DL.\n\nDL completely ignores the role of neurotransmitters. However, neurotransmitters have been shown to be computationally significant in adapting the receptive features networks on the fly, something which DL has really hard time doing.\n\nGiven the choice between neuron redundancy and neuron performance, evolution chose to make the brain redundant. Neurons are noisy, which isn\u2019t surprising when you consider the warm, biologically variable environment they\u2019re in. Although certain DL networks can cope with loss of their nodes, DL isn\u2019t known for it\u2019s robustness to noisy input or noisy training data.", 
        "title": "Deep Learning isn\u2019t the brain \u2013"
    }, 
    {
        "url": "https://medium.com/@ash_hathaway/ibm-watson-and-the-machine-intelligence-landscape-95be10d5882e?source=tag_archive---------2----------------", 
        "text": "When I joined the IBM Watson a little over three years ago I stumbled across this presentation showcasing the Machine Intelligence Landscape by Shivon Zilis with Bloomberg Beta. It certainly helped inform my vocabulary and educated me on a few companies and areas that weren\u2019t on my radar at the time. And it\u2019s just cool to look at and think about:\n\nTwo other companies of note are AlchemyAPI under Predictive Analytics- who was then acquired by Watson in 2015. Their service is still available today on Bluemix. And Softbank under Robotics- who is a ridiculously large telecom & interweb conglomerate in Japan. Watson partnered with them for implementations using Nao and Pepper which were created by Aldebaran in Paris. You know. These guys:\n\nLooking back it\u2019s kind of funny to think that IBM Watson and other companies lived in the AI bucket. It was entirely appropriate. Made sense. However, in just over two years it seems our lexicon of how we describe AI and the granularity of how we think about AI has certainly progressed.\n\nFirst we see the word Agents. We\u2019re not really \u201crethinking\u201d things anymore. We\u2019ve got some stability. We\u2019re even taking things by air, by ground, and by sea! Drones are buzzing around the East Bay, people! Enterprise and industry verticals are still the majority of the real estate, but tech user tools are starting to work their way in there and platforms are now thought of around function and medium- data enrichment, machine learning, audio, vision, etc. These core technologies are now being put into the hands of developers and companies to go build new products.\n\nThis time Watson & Alchemy are both in the same box under tech user tools. Totally agree. The focus has shifted from enterprise-level one-off deals to tools for developers.\n\nWhich brings us to now:\n\nEnterprise is now broken into intelligence and functions. Healthcare is also broken up. Agents are still personal or professional. There are also agent enablers. Industries have been boiled down to a few buckets. And the major change this year- technology stacks take the most real estate.\n\nWatson is hanging out up in internal data now. Which is appropriate. We still have one-off giant offerings that are built off the value prop of information security and working with internal data. Data privacy and on-prem solutions built bespoke for large groups are definitely part of Watson\u2019s value prop. But that\u2019s really only part the story. There\u2019s also the APIs those large-scale solutions are built from called The Watson Developer Cloud. Most of our APIs focus on making NLP, speech, or image capabilities really easy. We even have a conversation service that helps you build bots which can then integrate with other platforms. Feel free to check us out on Github.\n\nSo what\u2019s in store for 2017? No doubt technology and technical capabilities will still progress. Some will be evolutionary advances some revolutionary breakthroughs. In my mind we can count on:\n\nIn the actual market though I don\u2019t think we\u2019re seeing as much adoption as there possibly could be. Businesses are no doubt still trying to figure out how to work with most of these tools. There is still perceived cost and risk associated with machine intelligence in most c-suites. Right now AI in products is primarily low barrier to entry functions like transactional-based tasks (\u201cwhat\u2019s my bank account balance?\u201d \u201cwhat\u2019s the weather in New York like?\u201d), recommendations (\u201cyou might enjoy these books\u201d), and automating small tasks (\u201cremind me in 30 minutes to turn off the oven\u201d, \u201cset the alarm\u201d, \u201cturn off the lights\u201d).\n\nOnly a few companies and machine intelligence offerings are investing in really taking AI to the next level. Still, the companies that utilize the technology stacks will be ahead of the curve because they\u2019ll be able to extract more value from their data. If there\u2019s one thing I\u2019ve learned in the past three years at Watson it\u2019s that the companies that have good data or spend time cleansing their data and then utilize deep learning or machine learning technologies are at an advantage over their counterparts. Those that focus on great underlying technology and add great data will be at even more of a long-term advantage as well. This is why those data capture and open-source library categories are going to be interesting to watch in the next year. Because more data means more opportunities for machine intelligence.", 
        "title": "IBM Watson and the Machine Intelligence Landscape \u2013 Ash Hathaway \u2013"
    }, 
    {
        "url": "https://startupsventurecapital.com/observations-from-mlconf-in-san-francisco-6caf9886c9c7?source=tag_archive---------3----------------", 
        "text": "I had the pleasure of attending my second MLconf last week. It was a packed house\u200a\u2014\u200ayou could feel the enthusiasm for machine learning spilling out of the conference hall when you walked into Hotel Nikko.\n\nI bumped into MLconf founder Courtney Burton, who told me that over 700 people had registered for the event. It felt closer to double that number.\n\nI was impressed by the breadth of attendees at MLconf\u2014 from technologists at larger enterprises, to founders straight from the airport hustling their way into conversations with investors and potential recruits, product managers and designers, and VCs (like me) hunting for something magical and new.\n\nTwo quick observations from the conference:\n\nThanks again to Courtney & team for putting together a fantastic event. I\u2019m looking forward to the next MLconf.", 
        "title": "Observations from MLconf in San Francisco \u2013"
    }, 
    {
        "url": "https://medium.com/@brozynamon/the-evolution-and-competitive-advantage-of-machine-learning-cd87fada5b8c?source=tag_archive---------5----------------", 
        "text": "In today\u2019s incessantly connected world businesses and organizations are under competitive pressure to be on top of their customer experience. With the advent of Big Data science and analytics in the \u201990s many hopped on the bandwagon to better deliver on customer expectations.\n\nThe next step was to harness the speed and efficiency of raw data analysis into real time processing so that it could be turned into actionable insights instantly. Fast data analytics sprang from that need. Sectors such as technology, manufacturing, retail, government, healthcare, travel or finance quickly gained advantage(1) from the hands-on understanding brought about by the streamlined interaction with their customers who connect with them via their web and mobile applications, by tracking transactions, identifying hardware or software issues and reducing client complaints.\n\nThe natural progression of getting the upper hand on the market would require avoiding these challenges in the first place. Today we are able to surpass the capabilities of big and fast data analysis by employing predictive analytics techniques, such as data mining, predictive modelling and machine learning. The demand to analyze pre-existing or new data to predict future outcomes and to reduce risks, optimize operations and increase revenue is gathering mainstream momentum (2).\n\nMachine learning is a branch of artificial intelligence (AI) that enables computers to learn from complex data sets without being programmed. In other words, they master results by making autonomous decisions and by adjusting its own program actions instead of extracting the data for human interpretation and iteration.\n\nThe most optimal systems in AI have come thanks to the construction of artificial neural networks(3)\u200a\u2014\u200aa system of hardware and software modelled on the operation of human neurons. The technology focuses on solving complex and pattern recognition problems and is currently applied in speech and gesture recognition (Kinect), natural language processing (Siri), facial recognition (iPhoto), web search, spam filters, fraud detection, stock trading, and drug design.\n\nIn a process referred to as \u201cdeep learning,\u201d training data that is fed to a network\u2019s input nodes is subsequently correlated with the classification category that the network is trying to learn. The algorithm corrects every miscalculation and re-analyzes the data with an aim to produce consistently correct results for the full set of training scenarios. These calculations take milliseconds making it exceptionally efficient at optimizing business decisions and predicting outcomes (4).\n\nHOW CAN IT HELP YOUR BUSINESS\n\nA recent study by Bain & Company found (5) that investing in the right machine learning strategy can be an effective way to leverage your competition and to boost your sales. Companies procuring machine learning and advanced analytics technologies are:\n\nThe employment of machine learning techniques as a competitive advantage is particularly effective when there is no available human expertise on a subject or humans cannot explain their abilities, when solutions change rapidly over time and when they vary from one case to another.", 
        "title": "The Evolution and Competitive Advantage of Machine Learning"
    }
]