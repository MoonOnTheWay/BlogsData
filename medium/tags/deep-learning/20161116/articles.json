[
    {
        "url": "https://chatbotslife.com/regularization-in-deep-learning-f649a45d6e0?source=tag_archive---------0----------------", 
        "text": "Part of the magic sauce for making the deep learning models work in production is regularization. For this blog post I\u2019ll use definition from Ian Goodfellow\u2019s book: regularization is \u201cany modification we make to the learning algorithm that is intended to reduce the generalization error, but not its training error\u201d. For better theoretical understanding, I\u2019d recommend checking out the chapter of the deep learning book dedicated to regularization.\n\n\u00a0\n\nGeneralization in machine learning refers to how well the concepts learned by the model apply to examples which were not seen during training. The goal of most machine learning models is to generalize well from the training data, in order to make good predictions in the future for unseen data. Overfitting happens when the models learns too well the details and the noise from training data, but it doesn\u2019t generalize well, so the performance is poor for testing data. It is a very common problem when the dataset is too small compared with the number of model parameters that need to be learned. This problem is particularly acute in deep neural networks where is not uncommon to have millions of parameters. For the readers who would like more intuitive descriptions of overfitting, I\u2019ve found this thread on Quora explaining it pretty well.\n\nRegularization is a key component in preventing overfitting. Also, some techniques of regularization can be used to reduce model capacity while maintaining accuracy, for example, to drive some of the parameters to zero. This might be desirable for reducing model size or driving down cost of evaluation in mobile environment where processor power is constrained.\u00a0\n\n\u00a0\n\nThis rest of this post reviews some of the most common techniques of regularization used nowadays in industry:\n\nAn overfitting model (neural network or any other type of model) can perform better if learning algorithm processes more training data. While an existing dataset might be limited, for some machine learning problems there are relatively easy ways of creating synthetic data. For images some common techniques include translating the picture a few pixels, rotation, scaling. For classification problems it\u2019s usually feasible to inject random negatives\u200a\u2014\u200ae.g. unrelated pictures.\n\n\u00a0\n\nThere is no general recipe regarding how the synthetic data should be generated and it varies a lot from problem to problem. The general principle is to expand the dataset by applying operations which reflect real world variations as close as possible. Having better dataset in practice significantly helps quality of the models, independent of the architecture.\n\nEarly-stopping combats overfitting interrupting the training procedure once model\u2019s performance on a validation set gets worse. A validation set is a set of examples that we never use for gradient descent, but which is also not a part of the test set. The validation examples are considered to be representative of future test examples. Early stopping is effectively tuning the hyper-parameter number of epochs/steps.\n\nIntuitively as the model sees more data and learns patterns and correlations, both training and test error go down. After enough passes over training data the model might start overfitting and learning noise in the given training set. In this case training error would continue going down while test error (how well we generalize) would get worse. Early stopping is all about finding this right moment with minimum test error.\n\nIn practice, instead of actually stopping, people usually setup checkpoints to save the model at regular time intervals with continuous learning and pick the best candidate after the fact.\n\nAt each training iteration a dropout layer randomly removes some nodes in the network along with all of their incoming and outgoing connections. Dropout can be applied to hidden or input layer.\n\nThe original paper which introduced the concept was published in 2014. For a better understanding, I\u2019d recommend also the Coursera course by Geoffrey Hinton called \u201cNeural networks for machine learning\u201d.\n\nAn intuitive explanation for dropout efficiency might as follows.\n\nImagine that you have a team of workers and the overall goal is to learn how to erect a building. When each of the workers is overly specialized, if one gets sick or makes a mistake, the whole building will be severely affected. The solution proposed by \u201cdropout\u201d technique is to pick randomly every week some of the workers and send them to business trip. The hope is that the team overall still learns how to build the building and thus would be more resilient to noise or workers being on vacation.\n\nBecause of its simplicity and effectiveness, dropout is used today in various architectures, usually immediately after fully connected layers.\u00a0\n\n\u00a0\n\nSome practical usage considerations:\n\nExample of code how to use it:\n\nFor experimentation purposes, I\u2019ve modified CNN implementation (LeNET 5) for MINST from convolutional.py\n\nAn interesting related recent work (2016) which shows good results in various domains is dense-sparse-dense training. The technique consists in 3 steps:\n\nWeight penalty is standard way for regularization, widely used in training other model types. It relies strongly on the implicit assumption that a model with small weights is somehow simpler than a network with large weights. The penalties try to keep the weights small or non-existent (zero) unless there are big gradients to counteract it, which makes models also more interpretable. An alternative name in literature for weight penalties is \u201cweight decay\u201d since it forces the weights to decay towards zero.\n\n\u00a0\n\n\u00a0L2 norm\n\nThe diagrams bellow show how the weights values modify when we apply different types of regularization. Note the sparsity in the weights when we apply L1.", 
        "title": "Regularization in deep learning \u2013"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/rethinking-generalization-in-deep-learning-ec66ed684ace?source=tag_archive---------1----------------", 
        "text": "The ICLR 2017 submission \u201cUnderstanding Deep Learning required Rethinking Generalization\u201d [ICLR-1] is certainly going to disrupt our understanding of Deep Learning. Here is a summary of what they had discovered through experiments:\n\nThe authors actually introduce two new definitions to express what they are observing. The talk about \u201cexplicit\u201d and \u201cimplicit\u201d regularization. Drop out, data augmentation, weight sharing, conventional regularization are all explicit regularization. Implicit regularization is early stopping, batch norm, and SGD. It is an extremely odd definition that we\u2019ll discuss.\n\nI understand regularization ( see: http://www.deeplearningpatterns.com/doku.php/regularization ) as being of two types. I use the terms \u201cRegularization by Construction\u201d and \u201cRegularization by Training\u201d. There is the Regularization by Training that is the conventional use of the term. There is also the Regularization by Construction which is a consequence of the Model choices we select as we construct the elements of our network. The reason why there is a distinction, when mathematically they do appear equivalently as constraint terms, is that Regularization conventionally is not present after training, that is in the inference path. Regularization by Construction is always present, both in the training and the inference stages.\n\nNow the paper has a distinction between explicit and implicit regularization and that is when the main intent of the method is to regularize. One does dropout to regularize, so it is explicit. One does batch normalization (BN) for normalizing the activations of the different input samples but it happens to also regularize, so it is implicit regularization. The distinction between the two is the purpose of regularization or not. The later being implicit generalization. The meaning is that the unintended consequence of the technique is regularization. So when a researcher does not think that a method would lead to regularization and to his surprise it does, then that is what they call \u2018implicit\u2019 regularization. I don\u2019t think however Hinton expected Drop Out to lead to regularization. This is why I think the definition is extremely fuzzy, however, I understand why they introduced the idea.\n\nThe goal of regularization, however, is to improve generalization. That is also what BN does. In fact, for inception architectures, BN is favored over drop out. Speaking about normalization, there are several kinds, Batch and Layer normalization are the two popular versions. The motivation for BN is supposed to be Domain Adaptation. Is Domain Adaptation different from Generalization? Is not just a specific kind of generalization? Are there other kinds of generalization? If so, what are they?\n\nThe authors have made the surprising discovery that methods that don\u2019t seem to generalization, more specifically SGD, in fact, does. Another ICLR 2017 paper An Empirical Analysis of Deep Network Loss Surfaces [ICLR-2] adds added confirmation to this SGD property. This paper shows empirically that the loss surfaces for different SGD methods differ from each other. This tells you that what is happening is very different from traditional optimization.\n\nIt reminds one of quantum mechanics, where probes affect observation. Here learning method affects what is learned. In this new perspective of neural networks, that of brute force memorization or alternatively holographic machines, then perhaps ideas of quantum mechanics may need to come in play. Quantum mechanics emerges because of the non-commutability of poisson brackets in classical dynamics. We have two variables, position, and momentum, that are inextricably tied together. In Deep Learning I have a hunch that there are more than two variables that are tied together that lead to regularization. We at least have 3 variables: learning method, network model and generative model that all seem to have an effect on generalization. The troubling discovery, however, is how ineffective conventional regularization appears to be. \u201cExplicit regularization may improve generalization performance, but is neither necessary nor by itself sufficient for controlling generalization error.\u201d\n\nI think right now we have a very blunt instrument when it comes to our definition of Generalization. I wrote here that there are at least 5 different notions of generalization ( http://www.deeplearningpatterns.com/doku.php/risk_minimization ).\n\nWe can define it as the behavior of our system in response to validation data. That is against data that we have not included as part of the training set. We be a bit more ambitious and define it as behavior when the system is deployed to analyze real world data. We essentially would like to see our trained system perform accurately in the context of data it has never seen.\n\nA second definition is based on the idea of Occam\u2019s Razor. That is, the simplest of explanations is the best explanation. Here we make certain assumptions about the form of the data and we drive our regularization to constrain the solution toward our assumptions. So for example in the field of compressive sensing, we assume that a sparse basis exists. From there we can drive an optimization problem that searches solutions that have a sparse basis.\n\nA third definition is based on the systems ability to recreate or reconstruct the features. This is the approach taken by generative models. If a neural network is able to accurately generate realistic images, then it able to capture the concept of images in its entirety. We see this approach taken by researchers working on generative methods.\n\nA fourth definition involves the notion of ignoring invariant features or nuisance variables. That is, a system is able to generalize well if it is able to ignore invariant features for its tasks. Remove away as many features as possible until you can\u2019t remove any more. This is somewhat similar to the third definition however it tackles the problem from another perspective.\n\nA fifth generalization definition revolves around the idea of minimizing risk. When we train our system, there is an uncertainty in the context in which it will be deployed. So we train our models with mechanisms to anticipate unpredictable situations. The hope is that the system is robust to contexts that have not been previously predicted. This is kind of a game theoretic definition. We can envision an environment where information will always remain imperfect and generalization effectively means executing a particular strategy within the environment. This may be the most abstract definition of generalization that we have.\n\nI\u2019m sure there are many more as we move to a more game theoretic framework of learning. In fact, one effective strategy for learning is driven by the notion of \u201cCuriosity\u201d.\n\nUpdate: December 20, 2016: According to An early overview of ICLR 2017, Understanding deep learning requires rethinking generalization is the highest rated submission.\n\nI would like to dissect and discuss this further, link up to me at http://www.linkedin.com/in/ceperez or send me an email at ceperez AT intuitonmachine.com", 
        "title": "Rethinking Generalization in Deep Learning \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://chatbotslife.com/democratizing-al-and-ml-by-reducing-bottlenecks-in-development-processes-11741cc393da?source=tag_archive---------2----------------", 
        "text": "The debate over whether AI is real or not has been settled. AI is real, and it\u2019s not going away. But the ultimate destiny of AI is still a mystery, replete with unanswered questions, blind alleys and false assumptions.\n\nAt this point, however, several macro trends seem fairly clear:\n\n\u00b7 AI solutions are used increasingly to create or add value.\n\n\u00b7 Implementing AI solutions in the real world is difficult and costly.\n\n\u00b7 Successful implementations of AI often depend on practical knowledge of machine learning and other advanced analytics.\n\n\u00b7 Wider adoption of AI requires user-friendly systems that reduce unnecessary costs and shorten development cycles.\n\nHere\u2019s a typical challenge: Your organization needs to analyze some kind of big data, such as video feeds from thousands of traffic cameras, electronic health records from millions of patients, financial transactions from hundreds of banks, seismic data from the oil industry or astronomical images from the Hubble Space Telescope.\n\nIf you\u2019ve already got an organizational culture that respects and understands advanced analytics, then you\u2019re prepared to move forward swiftly with AI/ML implementations.\n\nBut you have a long and bumpy road ahead of you if your organization hasn\u2019t already invested in staffing and training teams of data scientists, software developers, AI/ML specialists and systems administrators.\n\nSimply hiring more people won\u2019t necessarily solve your problem, because success in AI/ML is highly knowledge- and experience-dependent. The data scientist you hire might not understand how to write efficient code for the hardware system that\u2019s running your analytics. The software developer you hire might not understand the mathematical concepts underlying the data science. It\u2019s quite possible that neither of them would know how to optimize the various computational resources required to crunch through the data and generate usable results.\n\nIn a very real sense, your new hires are likely to become bottlenecks in your big data efforts. Instead of becoming assets, your data scientists and software developers might even become liabilities, adding friction to a process that is already fraught with difficulty.\n\nBy definition, big data is unwieldy and messy. Most big data isn\u2019t conveniently located in a single database. It\u2019s often spread out over multiple databases and collected from multiple sources like social media, consumer profiles, electronic health records, sensor logs from IoT devices, transaction logs from financial systems, and machine logs from manufacturing equipment. As a result, most big data is left untouched or underutilized because it\u2019s not easy to perform distributed computing at scale in heterogeneous environments.\n\nWhile it\u2019s true that tools like TensorFlow make deep learning more accessible to a larger population of developers, deep learning is only one aspect of the larger universe of ML techniques.\n\n\u201cAnd if you\u2019ve used TensorFlow, you know that it\u2019s not set up for beginners. It\u2019s free, but you need to know what you\u2019re doing and you\u2019ve got to be willing to invest a fair amount of time to generate results you can trust,\u201d says Dr. Eric Xing, a professor and associate head of research of the Machine Learning Department in the School of Computer Science at Carnegie Mellon University.\n\nXing is a Fellow of the Association of Advancement of Artificial Intelligence, and has two Ph.D. degrees, one in molecular biology from Rutgers University and another in computer science from UC Berkeley. He is also founder, CEO and chief scientist at Petuum Inc., a startup building an artificial intelligence (AI) and machine learning (ML) solution development platform. Petuum recently closed $15 million in Series A funding led by Advantech Capital, with additional investors Tencent, Northern Light Venture Capital and Oriza Ventures.\n\nPetuum\u2019s goal, says Xing, is building a next generation omni-mount machine learning platform for deep learning (e.g., CNN/RNN), predictive analytics (e.g., regression), knowledge extraction (e.g., topic models), content summarization (e.g., sparse coding), ensemble methods (e.g., gradient boosted trees), for a wide range of applications such as natural language processing, image and video understanding, and anomaly detection in transaction data.\n\nIn addition to serving as a general ML development platform, it\u2019s also a ML computing management and operating system, and an enterprise ML system solution.\n\n\u201cFrom our perspective, the platform\u2019s multiple capabilities allow users to exercise a greater range of creativity and innovation. Developers have more choices and greater flexibility\u200a\u2014\u200athey aren\u2019t limited to a single programming language and a specific computing environment, and the platform will automatically compile their code into a universal format,\u201d says Xing.\n\nAfter the code is complied into a universal format, it will be further transformed into executables, and mounted to different hardware platforms\u200a\u2014\u200aranging from data-center scale distributed computing systems to mobile devices.\n\n\u201cPetuum will provide a remarkably flexible environment for developers, thanks to its parameter server architecture, managed communication, adaptive scheduling and load balancing, elastic resource management\u200a\u2014\u200aand most importantly, a development and operations interface that abstracts away such system details, while seamlessly leveraging their advanced functionalities. Petuum will allow AI or ML programs to run quickly and scale out to many devices, without compromising precision and output quality,\u201d says Xing.\n\nIts wider range of capabilities makes Petuum 10 to 100 times more efficient than other solutions. Unlike TensorFlow, for example, Petuum addresses a broad spectrum of tasks beyond image analysis and speech recognition. Petuum will be mountable on a wide variety of hardware configurations and is multi-tenant, enabling multiple programs to run at the same time.\n\n\u201cBy our calculations, the technology in Petuum is already up to 800 times more efficient than Hadoop/Spark frameworks for running ML-powered applications,\u201d says Xing.\n\nIn addition to being faster and more scalable, Petuum\u2019s technology requires significantly less energy and fewer computational resources than other types of systems designed for big data analytics.\n\nPetuum\u2019s unique design also allows far more sharing of resources, enabling multiple programs to run on a single cluster. That stands in stark contrast to the limitations of monolithic systems, in which programs typically are deployed on single devices and cannot be shared.\n\nFor seasoned developers and operators, the efficiencies offered by the Petuum platform will be impressive, but not necessarily revolutionary in all aspects. For users, however, those efficiencies can easily translate into dramatically faster cycles of discovery and innovation. From our perspective, faster cycles mean less time spent tinkering with implementations and more time spent delivering value.\n\nWhat kind of value are we talking about? For some organizations, that value will be measured in dollars. For others, the value will be measured in safer and more effective therapies for diseases such as diabetes and cancer.\n\nComplex computational tasks are hard; building systems for handling complex tasks is even harder. Petuum reduces the time and effort required to create usable systems for applying AI and ML to real-world problems.\n\nNotably, Petuum will provide a much-needed standard platform for creating new AI and ML solutions. At present, almost all AI and ML solutions are \u201chand-crafted.\u201d Petuum is more like an industrial factory with assembly lines, enabling developers to create solutions using standardized processes that are transparent and repeatable.\n\n\u201cBy removing bottlenecks, reducing friction and providing standard processes, Petuum puts the potential benefits of AI and ML at your fingertips. Our goal isn\u2019t just democratizing the processes underlying the creation of AI systems\u200a\u2014\u200aour goal is putting practical AI into the hands of everyone who needs it,\u201d says Xing.", 
        "title": "Democratizing AI and ML by Reducing Bottlenecks in Development Processes"
    }, 
    {
        "url": "https://medium.com/choosing-our-future/pay-attention-1-25185db2109b?source=tag_archive---------3----------------", 
        "text": "For years I did my best to help people change. From working closely with people, and watching my own life, it became clear how difficult it is to make any significant changes in one\u2019s life. Just try to lose weight and keep it off.\n\nWhat immediately becomes apparent is that people are not logical. We lead with our emotions. We have evolved to act quickly to survive. \u201cFriend, food or foe?\u201d had to be an instant decision for our long-ago ancestors\u00a0. So many studies have shown that we use our logical thinking to justify an emotional decision. We rationalize more easily than we change.\n\nHow many times have I sat with a woman whose was living with a guy who mistreated her, cheated on her, stole from her, knowing that if I told her to just leave then I would be the one she would never see again? She was not going to leave until she was emotionally ready. She would have to work through all of the strongly intertwined emotions of love/fear/sexual attraction/ loss and loneliness. Even when the right thing to do is obvious, it\u2019s never easy.\n\nNow all of us, all over the world, are struggling with major changes; changes that have affected almost every area of our lives. We have been through this before, and it was a struggle then too. Over a hundred years ago many new, disruptive technologies were being developed and introduced into the lives of people in the more developed countries around the world. Many agrarian societies were transitioning into industrial societies. The \u201cIndustrial Revolution\u201d moved people from farms and fields into factories and cities. One generation began to live very differently than their parents.\n\nHow well did the world handle that transition? Many of the conditions of the time were chronicled in the works of Charles Dickens. He wrote about the struggles of the poor, the children of the factory workers and of life in the big industrial cities. There was poverty, pollution, disease and crime. The American Civil War was fought in the early part of the Industrial Revolution as machines began to replace individual labor. The energy policy of the slave states was coming to an end.\n\nThe culmination of the use of all the new engines and mechanical marvels was the biggest, most meaningless waste of human life in history: The First World War. That was a war fought between crumbling forms of governments, run by aristocracies that were almost totally out of touch and unconcerned about the people they ruled. Millions of those people suffered horrible deaths fighting for causes that were never very clear to them, so they called it \u201cpatriotism,\u201d and \u201clove of the homeland.\u201d\n\nNow, a hundred years later, another major disruption is affecting the lives of almost everyone in the world. Since we are now causing disturbances in the planet itself, the consequences of these changes are impossible to avoid. Even the people sitting blissfully on a beautiful south sea island can see that their little patch of paradise is being covered by rising tides.\n\nEveryone\u2019s lifestyle is being threatened in some way, and just like we do as individuals, as a society, we resist. Change is uncomfortable. It takes energy to adjust. We can\u2019t tell who will benefit and who will suffer, but we quickly suspect that we will get the short end. Those who are in power feel threatened because they may lose control. Those who are out of power have learned that they always lose, no matter who wins.\n\nSince there is no job or profession that functions the way that it did twenty years ago, everyone feels a bit insecure. Anxiety is the malady of our times. So much is different and new. Families can be composed differently than twenty years ago. There are families of just men, just women, a single person and child, a man who was a woman living with a woman who was man. Babies can be made in several different ways, with up to five people involved in the process of creation.\n\nWe are again, involved in wars, but they are, at least for now, more limited. They are also being fought very differently than they were a hundred years ago, as we now have the use of drones and suicide bombers. In many ways these weapons consist of limited fire-power, but are the kinds of weapons that create a constant sense of terror. Danger is unlikely, but it feels ever-present. Something could happen anywhere, at anytime, to anyone.\n\nDuring the Industrial Revolution big machine changed the way people worked, and how things were made. Many things that had been made slowly, by hand, were now made quickly, and in great quantity, by machines. Clothing is a clear example. Manufacturers began to use assembly lines to make furniture or cars. Transportation went from horses and barges to cars and trains. Communication went from letters delivered in days or months, to a telegraph, and then telephone. Someone invented a typewriter. Everything began to go faster.\n\nNow, in our digital age, everything goes faster still. But the revolution is not just about work and making things, or transportation, or communication, although all of those things have been radically disrupted. This time there are a lot of changes in the kinds of things our minds and brains need to do. Our brains are constantly being bombarded with new information. The information comes in many forms, speech, pictures, videos, music, virtual reality, augmented reality, symbolic noises such as pings, beeps and buzzes. Much of the speech, videos, music and noises are created by machines and played on machines. We are often speaking to bots or some other kind of artificial intelligence that guides us through a menu, answers our questions., or tells us we have an appointment. Part of the new skills we have to learn in this world is how to sort out the real from the machine created, and the truth from the semi-truth, distortions or outright lies. That is proving more and more difficult to do.\n\nWhether we realize it or not, many algorithms do our thinking for us. They guide our medical care, They select stocks for our 401Ks, They offer suggestions about what we should read, or what music we will like. They tell us what to eat and how much more we should exercise. They manage our money. They pay our bills for us automatically, and on time. They instruct us about what roads to take to get from here to there. Without realizing it, we are turning much of our thinking process over to machines. I discussed this before the election in: https://medium.com/choosing-our-future/thinking-about-thinking-350b2d401656#.ma4698my6\n\nHow will our minds and brains adapt? Will all of these thinking aids enhance our decision making abilities? Will we become less dependent upon emotion and more guided by research that shows what actions have a higher probability of working out? Or will we become increasingly dependent upon machines and as a result lose our ability to think, and to solve difficult problems?\n\nMy initial answers to those questions, based only on forty years of dealing with how people think and make decisions, is that some people will do marvelously well and use all of these new aids to great advantage. Others will, in ways that I will discuss shorty, be more victimized and manipulated. Still others will remain on the outside, hardly aware of what is going on, and will suffer as society moves on without them.\n\nIt is the second group about whom I have the most concern. I also feel that they may be the largest demographic. They are the folks who just want to live their lives in peace and harmony. They want to work at a decent job, take care of their families, have a good fun, relaxing time when they\u2019re not working, and just go along to get along. These are the people who feel that if they take care of themselves, their families and their own responsibilities, then our society, and whoever is running it, should do a good enough job to not mess things up. Life is difficult enough for everyone, especially if you have, or you are, an adolescent kid.\n\nObviously, this is a dangerous assumption. The people who have run almost every country, anywhere, have always found ways to mess it up. They usually do that by taking too much control, and by allowing corruption. But by not taking enough control, things can get bad also. There will always be some people who will take advantage of others, whether they are in or out of government. Those who do not pay attention are the ones who are most easily exploited.\n\nWhat we have learned is that we have to pay attention, not just to government, but to everyone who is trying to get at your mind and your money. These days that is a lot of folks, and there are lots of ways to do that, many more than ever before. Our lives are a constant stream of inputs from everywhere: family, friends, neighborhood organizations, political parties, banks, stores, tech companies, health companies, drug companies, media companies, TV, cable, movies, video games, augmented reality games, your work, your boss, your FitBIt, Facebook, Snapchat, LinkedIn, and your dog. It\u2019s constant, and it\u2019s addicting. We can\u2019t turn it off. In fact we keep turning it on, like that rat pushing the lever for a reward.\n\nIt has become a chore to be selective. There are many apps that help organize and prioritize your apps. It is very difficult to determine whom to trust, what to believe, and how to respond. Most people tend to stick with what\u2019s familiar, and that allows them to miss what is going on on the other side of the mountain. We could be watching football when the volcano blows and we wouldn\u2019t know about it until our phone gave us an emergency text. By then the lava could be fifty feet away.. There may have been other warnings put out on-line but they weren\u2019t on Facebook so we didn\u2019t know.\n\nAlso, and more dangerous, we don\u2019t know who has all this Big Data that has been gathered about us. We only know that many companies and organizations do. In many ways they know more about our behavior patterns than we do. They know where we go, what we buy and how much we are willing to spend, They know whom we hang-out with, and what we like to do with them. They know what we eat, how much we weigh, and if we exercise. They know if we drink or take drugs. They can tell how much sex we are having and probably with whom. They, whoever \u201cthey\u201d are, know more than this, and we don\u2019t really know who they are, what they know, or what they are doing with all of the data.\n\nAnd, if you don\u2019t pay attention, they will sculpt your behavior. If you like this, you\u2019ll like this better, and here is a coupon for it. For you, the price is $39.99, but I\u2019m charging him $109 because I know he doesn\u2019t care. He likes to feel rich.\n\nMany people know the system and can work it to their advantage. They take the bargains they want and leave the rest. They have developed their own core values and they know how to stick to them. But most people look around to see what is happening. They want to be part of the flow and hip to what\u2019s going on. These days that can be very dangerous, and it\u2019s is going to get worse.\n\nMy advice is PAY ATTENTION!!. Stop, as Dr. Kahneman says, and Think Slowly. Choose your own future.", 
        "title": "Pay Attention! #1 \u2013 Choosing Our Future \u2013"
    }, 
    {
        "url": "https://medium.com/@mslavescu/https-twitter-com-gtarobotics-status-798881778022973447-73e128e3294d?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Very nice collection of #DeepLearning resources, including some that could be applied to\u2026"
    }
]