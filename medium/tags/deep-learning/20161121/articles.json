[
    {
        "url": "https://hackernoon.com/finding-the-genre-of-a-song-with-deep-learning-da8f59a61194?source=tag_archive---------0----------------", 
        "text": "The average library is estimated to have about 7,160 songs. If it takes 3 seconds to classify a song (either by listening or because you already know), a quick back-of-the-envelope calculation gives around 6 hours to classify them all.\n\nIf you add the time it takes to manually label the song, this can easily go up to 10+ hours of manual work. No one wants to do that.\n\nIn this post, we\u2019ll see how we can use Deep Learning to help us in this labour-intensive task.\n\nHere\u2019s a general overview of what we will do:\n\nFirst of all, we\u2019re going to need a dataset. For that I have started with my own iTunes library\u200a\u2014\u200awhich is already labelled due to my slightly obsessive passion for order. Although it is not as diverse, complete or even as big as other datasets we could find, it is a good start. Note that I have only used 2,000 songs as it already represents a lot of data.\n\nThe first observation is that there are too many genres and subgenres, or to put it differently, genres with too few examples. This needs to be corrected, either by removing the examples from the dataset, or by assigning them to a broader genre. We don\u2019t really need this Concertos genre, Classical will do the trick.\n\nOnce we have a decent number of genres, with enough songs each, we can start to extract the important information from the data. A song is nothing but a very, very long series of values. The classic sampling frequency is 44100Hz\u200a\u2014\u200athere are 44100 values stored for every second of audio, and twice as much for stereo.\n\nThis means that a 3 minute long stereo song contains 7,938,000 samples. That\u2019s a lot of information, and we need to reduce this to a more manageable level if we want to do anything with it. We can start by discarding the stereo channel as it contains highly redundant information.\n\nWe will use Fourier\u2019s Transform to convert our audio data to the frequency domain. This allows for a much more simple and compact representation of the data, which we will export as a spectrogram. This process will give us a PNG file containing the evolution of all the frequencies of our song through time.\n\nThe 44100Hz sampling rate we talked about earlier allows us to reconstruct frequencies up to 22050Hz\u200a\u2014\u200asee Nyquist-Shannon sampling theorem\u200a\u2014\u200abut now that the frequencies are extracted, we can use a much lower resolution. Here, we\u2019ll use 50 pixel per second (20ms per pixel), which is more than enough to be sure to use all the information we need.\n\nNB: If you know a genre characterized by ~20ms frequency variations, you got me.\n\nHere\u2019s what our song looks like after the process (12.8s sample shown here).\n\nTime is on the x axis, and frequency on the y axis. The highest frequencies are at the top and the lowest at the bottom. The scaled amplitude of the frequency is shown in greyscale, with white being the maximum and black the minimum.\n\nI have used use a spectrogram with 128 frequency levels, because it contains all the relevant information of the song\u200a\u2014\u200awe can easily distinguish different notes/frequencies.\n\nThe next thing we have to do is to deal with the length of the songs. There are two approaches for this problem. The first one would be to use a recurrent neural network with wich we would feed each column of the image in order. Instead, I have chosen to exploit even further the fact that humans are able to classify songs with short extracts.\n\nWe can create fixed length slices of the spectrogram, and consider them as independent samples representing the genre. We can use square slices for convenience, which means that we will cut down the spectrogram into 128x128 pixel slices. This represents 2.56s worth of data in each slice.\n\nAt this point, we could use data augmentation to expand the dataset even more (we won\u2019t here because we aready have a lot of data). We could for instance add random noise to the images, or slightly stretch them horizontally and then crop them.\n\nHowever, we have to make sure that we do not break the patterns of the data. We can\u2019t rotate the images, nor flip them horizontally because sounds are not symmetrical.\n\nE.g, see those white fading lines? These are decaying sounds which cannot be reversed.\n\nAfter we have sliced all our songs into square spectral images, we have a dataset containing tens of thousands of samples for each genre. We can now train a Deep Convolutional Neural Network to classify these samples. For this purpose, I have used Tensorflow\u2019s wrapper TFLearn.\n\nWith 2,000 songs split between 6 genres\u200a\u2014\u200aHardcore, Dubstep, Electro, Classical, Soundtrack and Rap, and using more than 12,000 128x128 spectrogram slices in total, the model reached 90% accuracy on the validation set. This is pretty good, especially considering that we are processing the songs tiny bits at a time. Note that this is not the final accuracy we\u2019ll have on classifying whole songs (it will be even better). We\u2019re only talking slices here.\n\nSo far, we have converted our songs from stereo to mono and created a spectrogram, which we sliced into small bits. We then used these slices to train a deep neural network. We can now use the model to classify a new song that we have never seen.\n\nWe start off by generating the spectrogram the same way we did with the training data. Because of the slicing, we cannot predict the class of the song in one go. We have to slice the new song, and then put together the predicted classes for all the slices.\n\nTo do that, we will use a voting system. Each sample of the track will \u201cvote\u201d for a genre, and we choose the genre with the most votes. This will increase our accuracy as we\u2019ll get rid of many classifications errors with this ensemble learning-esque method.\n\nWith this pipeline, we can now classify the unlabelled songs from our library. We could simply run the voting system on all the songs for which we need a genre, and take the word of the classifier. This would give good results but we might want to improve our voting system.\n\nThe last layer of the classifier we have built is a softmax layer. This means that it doesn\u2019t really output the detected genre, but rather the probabilities of each. This is what we call the classification confidence.\n\nWe can use this to improve our voting system. For instance, we could reject votes from slices with low confidence. If there is no clear winner, we reject the vote.\n\nSimilarly, we could leave unlabelled the songs for which no genre received more than a certain fraction -70%?- of the votes. This way, we will avoid mislabeling songs, which we can still label later by hand.\n\nIn this post, we have seen how we could extract important information from redundant and high dimensional data structure\u200a\u2014\u200atracks. We have taken advantage of short patterns in the data which allowed us to classify 2.56 second long extracts. Finally, we have used our model to fill in the blanks in a digital library.\n\nYou can play with the code here:\n\nIf you want to go further on audio classification, there are other approaches which yield impressive results, such as Shazam\u2019s fingerprinting technique or dilated convolutions.\n\nThanks for reading this post, stay tuned for more\u00a0!", 
        "title": "Finding the genre of a song with Deep Learning \u2014 A.I. Odyssey part. 1"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/five-levels-of-capability-of-deep-learning-ai-4ac1d4a9f2be?source=tag_archive---------1----------------", 
        "text": "Arend Hintze has a good short article on \u201cUnderstanding the four types of AI, from reactive robots to self-aware beings\u201d where he outlines the following types:\n\nReactive Machine\u200a\u2014\u200aThe most basic type that is unable to form memories and use past experiences to inform decisions. They can\u2019t function outside the the specific tasks that they were designed for.\n\nLimited Memory\u200a\u2014\u200aAre able to look into the past to inform current decisions. The memory however is transient and aren\u2019t used for future experiences.\n\nTheory of Mind\u200a\u2014\u200aThese systems are able to form representations of the world as well as other agents that it interacts with.\n\nI like his classification much better than the \u201cNarrow AI\u201d and \u201cGeneral AI\u201d dichotomy. This classification makes an attempt to break down Narrow AI into 3 categories. This gives us more concepts to differentiate different AI implementations. My reservation though of the definition is that they appear to come from a GOFAI mindset. Furthermore, the leap from limited memory able to employ the past to theory of mind seems to be an extremely vast leap.\n\nI however would like to take this opportunity to come up with my own classification, more targeted towards the field of Deep Learning. I hope my classification is a bit more concrete and helpful for practitioners. This classification gives us a sense of where we currently are and where we might be heading.\n\nWe are inundated with all the time with AI hype that we fail to good conceptual framework for making a precise assessment of the current situation. This may simply be due to the fact that many writers have trouble keeping up with the latest development in Deep Learning research. There\u2019s too much to read to keep up and the latest discoveries continue to change our current understanding. See \u201cRethinking Generalization\u201d as one of those surprising discoveries.\n\nThis level includes the fully connected neural network (FCN) and the convolution network (CNN) and various combinations of them. These system take a high dimensional vector as input and arrive at a single result, typically a classification of the input vector. You can consider these systems as being stateless functions, meaning that their behavior is only a function of the current input. Generative models are one of those hotly researched areas and these also belong to this category. In short, these systems are quite capable by themselves.\n\nThis level includes memory elements incorporated with the C level networks. LSTMs are example of these with the memory units are embedded inside the LSTM node. Other variants of these are the Neural Turing Machine (NMT) and the Differentiable Neural Computer (DNC) from DeepMind. These systems maintain state as they compute their behavior.\n\nThis level is somewhat similar to the CM level, however rather than raw memory, the information that the C level network is able to access is a symbolic knowledge base. There are actually three kinds of symbolic integration that I have found, a transfer learning approach, a top-down approach, a bottom up approach. The first approach uses a symbolic system that acts as a regularizer. The second approach has the symbolic elements at the top of the hierarchy that are composed at the bottom by neural representations. The last approach has it reversed, where a C level network is actually attached to a symbolic knowledge base.\n\nAt this level, we have a system that is built on top of CK, however is able to reason with imperfect information. An example of this kind of system would be AlphaGo and Poker playing systems. AlphaGo however does not employ CK but rather CM level capability. Like AlphaGo, these kind of systems can train itself by running simulation of it against itself.\n\nThis level is very similar to the \u201ctheory of mind\u201d where we actually have multiple agent neural networks combining to solve problems. Theses systems are designed to solve multiple objectives. We actually do se primitive versions of this in adversarial networks, that learn to perform generalization with competing discriminator and generative networks Expand that concept further into game-theoretic driven networks that are able to perform strategically and tactically solving multiple objectives and you have the making of these kind of extremely adaptive systems. We aren\u2019t at this level yet and there\u2019s still plenty of research to be done in the previous levels.\n\nDifferent level bring about capabilities that don\u2019t exist in the previous level. C level systems for example are only capable of predicting anti-causal relationships. CM level systems are capable of very good translation. CIK level systems are capable of strategic game play.\n\nWe can see how this classification somewhat aligns with Hinzte classification, with the exception of course of self-awareness. That\u2019s a capability that I really have not explored and don\u2019t intend to until the pre-requisite capabilities have been addressed. I\u2019ve also not addressed zero-shot or one-shot learning or unsupervised learning. This is still one of the fundamental problems, as Yann LeCun has said:\n\nLeCun has also recently started using the phrase \u201cpredictive learning\u201d in substitution of \u201cunsupervised learning\u201d. This is an interesting change and indicates a subtle change in his perspective as to what he believes is required to implement the \u201ccake\u201d. In LeCun\u2019s view, the foundation needs to be built before we can make substantial progress in AI. In other words, building off current supervised learning by adding more capabilities like memory, knowledge bases and cooperating agents will be a slog until we are all able to build that \u201cpredictive foundational layer\u201d. In the most recent NIPS 2016 conference he posted this slide:\n\nOne accelerator technology in all of this however is that when the capabilities are used in a feedback loop. We actually have seen instance of this kind of \u2018meta-learning\u2019 or \u2018learning to optimize\u2019 in current research. I cover these developments in another article \u201cDeep Learning can Now Design Itself!\u201d The key take away with meta-methods is that our own research methods become much more powerful when we can train machines to actually discover better solutions that we otherwise could find.\n\nThis is why, despite formidable problems in Deep Learning research, we can\u2019t really be sure how rapid progress may proceed.\n\nTo understand better how Deep Learning capabilities fit with your enterprise, visit Intuition Machine or discuss on the FaceBook Group on Deep Learning.\n\nhttp://csc.ucdavis.edu/~cmg/papers/et1.pdf Computational Mechanics of Input-Output Processes: Structured transformations and the e-transducer", 
        "title": "The Five Capability Levels of Deep Learning Intelligence"
    }, 
    {
        "url": "https://gab41.lab41.org/feature-engineering-is-just-easier-1928d935ed17?source=tag_archive---------2----------------", 
        "text": "The staggering proliferation of deep learning architectures in the past few years is evidence for the maxim that architecture engineering is the new feature engineering. It used to be that every new advance in machine learning relied on some clever feat of feature engineering\u200a\u2014\u200aa tweak taking the raw data and exposing its characteristics in some way that a learning algorithm could exploit. Everyone had access to the same few learning algorithms, so feature engineering was the easiest\u200a\u2014\u200aand often the only\u200a\u2014\u200away to differentiate yourself from the pack.\n\nThe pendulum has, however, swung sharply away from hand-guided feature engineering. Convolutional neural networks, to take the best-known example, can learn feature transformations for all sorts of computer vision tasks (among others), given only a dataset and an objective function. Turns out, convolutional neural networks are a good fit for image in part because their performance is \u201ctranslation-invariant\u201d\u200a\u2014\u200ai.e., they do the same thing no matter where in the input they are looking. This property has stood them in good stead on problem domains outside of vision, as well, leading to some rather bullish declamations:\n\nSimilarly, recurrent neural network architectures have been used to do feature learning for NLP and tackle hard problems like machine translation. These architectures reflect the nature of the problem and elegantly exploit the structure of the data, and this is one reason they work so well. More and more, the \u201cright\u201d way to tackle many machine learning problems is to define a network architecture and objective function that conforms with the problem and the shape of the data.\n\nHere\u2019s the (not so) dirty secret: this kind of \u201carchitecture engineering\u201d is hard\u200a\u2014\u200aand by hard I mean expensive. Finding the \u201cright\u201d network for the job, especially if one doesn\u2019t exist yet (but make sure to try ResNets first!) is not, strictly speaking, an engineering effort. Fiddling with deep learning architectures is still definitely in the realm of research\u200a\u2014\u200athere are few established best practices, the risk of failure is high, and personnel with the necessary expertise are rare and costly. Deep learning slays the competition in object detection, image captioning, and machine translation, but small deformations of these more commonly researched problems can make duds of even the dearest of deep learning darlings. And doubly worth noting given all the deep learning hype is that manual feature engineering still provides an edge in image retrieval and tagging, among a plethora of other tasks.\n\nIn two of Lab41\u2019s recent endeavors, D*Script and Pythia, we pursued machine learning solutions to real-world problems\u200a\u2014\u200aidentifying writers of handwritten documents and detecting novel documents in large corpora, respectively. In both projects, some combination of \u201chand-engineered\u201d features rose to the top of the heap, despite a lot of experimentation with deep learning-based approaches. In D*Script, I bet we could have found more competitive solutions using deep learning if we had spent more time looking, or if we had hired a whole team of recent NYU PhDs. But that would have been prohibitively expensive. And when you add in constraints like \u201cnot much data available,\u201d it becomes harder to say whether deep learning will ever be able to do the job for you.\n\nFeature engineering, on the other hand, is cheaper than ever. As Anna pointed out in her post on experiment logging with Sacred, firing off thousands of experiments testing different feature configurations is nearly trivial. And though we aren\u2019t half as clever as the many Kaggle winners whose hair-raising feature engineering exploits would make any machine learning enthusiast\u2019s heart skip a beat, calculating a time lag feature or rolling mean takes on the order of minutes, and assessing its usefulness scarcely longer. Devising, training, and testing an end-to-end deep learning framework takes a bit more time than that.\n\nSo what deep learning has introduced isn\u2019t so much the death of hand-tuned features, but instead a richer continuum along the risk-reward axis between feature engineering and feature learning. Hand-tuned features combined with versatile, robust learners like XGBoost are a reasonably low-cost effort that can often yield satisfactory results\u200a\u2014\u200aand if they don\u2019t, who cares? In the upper stratosphere of academic and industrial machine learning, deep learning has almost entirely taken over, but it\u2019s no accident that the field is dominated by a few large companies, and almost everyone involved has a PhD from one of a handful of programs. It\u2019s still an expert\u2019s game\u200a\u2014\u200aand these days it does make more sense to have the experts spend their time designing sensible network architectures instead of chasing down the One True Feature.\n\nArchitecture engineering is getting cheaper, too. Though there\u2019s still a long way to go, efforts such as Keras have done a lot to make deep learning more accessible and tinkerable. And it is only going to get easier. In the meantime, hand-engineering features for your problem isn\u2019t necessarily some rearguard action, undertaken on behalf of a desperate ancien regime that doesn\u2019t know anything else. Sometimes, feature engineering is just the correct, economical choice.", 
        "title": "Feature engineering is just easier \u2013"
    }, 
    {
        "url": "https://medium.com/transmission-newsletter/self-driving-shuttles-model-3-spy-shots-photonic-neural-networks-lip-reading-with-deep-learning-84e185d5955e?source=tag_archive---------3----------------", 
        "text": "Speaking of Tesla, the Model 3 might just be the first fullyself-driving car available to the mass market. Many of the autonomy enthusiasts I know are one of the 400,000 people to have Model 3 on pre-order, but have yet to actually seethe car in anything but press shots. It turns out there\u2019s tons of Model 3 spy shots out there, so I curated a bunch. View the photos here\u2026\n\nThe key to building a successful self-driving vehicle startup is to ship. The more you ship, the more data you can collect. Last week, I was excited to see my friends at Auro Robotics launch their self-driving shuttle at Santa Clara University. Read more about their launch. Oh, and they\u2019re hiring!\n\nThe Open Source Car Control Project enables engineers to build their own self-driving development vehicle using existing by-wire technologies on the 2014-or-later Kia Soul. OSCC can be integrated into a new or used vehicle for less than $1,000. I\u2019m super excited to see what this enables in the industry. Read more\u2026\n\nEarly results, but exciting! \u201cThe results show just how fast photonic neural nets can be. \u201cThe effective hardware acceleration factor of the photonic neural network is estimated to be 1,960 \u00d7 in this task,\u201d say Tait and co. That\u2019s a speed up of three orders of magnitude.\u201d Read more\u2026\n\nUniversity of Oxford researchers continue to tackle the problem of reading lips using deep learning, and have published two potentially novel contributions: first, a pipeline for fully automated large-scale data collection from TV broadcasts; second, CNN architectures that are able to effectively learn and recognize hundreds of words from this large-scale dataset. Read the paper\u2026\n\nAn excellent post by @zaydenam, discussing just how different machine learning is to traditional software engineering: \u201cWhat is unique about machine learning is that it is \u2018exponentially\u2019 harder to figure out what is wrong when things don\u2019t work as expected\u201d Read more\u2026\n\nThe goal of DeepLearningKit is to support using pre-trained Deep Learning models on all Apple\u2019s devices that have GPUs. It is developed in Swift and Metal\u200a\u2014\u200ato efficiently use on-device GPU to ensure low-latency Deep Learning calculations. DeepLearningKit sounds very useful! Read more\u2026", 
        "title": "Self-Driving Shuttles, Model 3 Spy Shots, Photonic Neural Networks, Lip Reading with Deep Learning\u2026"
    }, 
    {
        "url": "https://medium.com/@dsouza.amanda/multi-channel-cnn-for-text-699713aa98a7?source=tag_archive---------4----------------", 
        "text": "Here is an implementation of the paper by Kim et al. for sentence classification using multi-channel CNNs.\n\nI won\u2019t go into details of the text input/preprocessing steps etc. since they are standard pipelines already available on several blogs, as well as Keras\u2019 examples. [Check this post]\n\nThe multi-channel architecture constructs 2 channels of word embeddings using Glove/Word2Vec for the input sentences.\n\nOn one channel, pre-trained vectors with static weights are used, while in the other, pre-trained vectors with learnable weights are used. It would be interesting to see if having Glove & Word2Vec vector embeddings as the two channels improves the model.\n\nThese two channels are fed to a CNN model.\n\nHere\u2019s a simple model that does this:", 
        "title": "Multi-channel CNN for text \u2013 Amanda Dsouza \u2013"
    }, 
    {
        "url": "https://blog.deepomatic.com/why-i-hate-quitting-deepomatic-e1cfe8cef888?source=tag_archive---------5----------------", 
        "text": "Every startup has many firsts during its early years. We have known our share of exciting firsts: first employee, first fundraising, first signed contract, first office outside of the incubator, first discussion with France\u2019s Prime Minister, first seminar abroad, first profitable month,\u2026 This month, I am the first employee to resign. Although it might not be as exciting for Deepomatic, it gives me the occasion to reflect on the year I spent here.\n\nNear the end of my PhD thesis in logics, I read that Deepmind had created a program which could learn to play atari games from pixels. At that moment, I became convinced that artificial intelligence would play a huge role in the following years and I wanted to be a part of it. That is why I applied to join Deepomatic, to learn as much as I could about machine learning, and to use this knowledge on challenging problems.\n\nI was not disappointed. Vincent\u200a\u2014\u200aour CTO who did his PhD in computer vision\u200a\u2014\u200ashared his knowledge with me, pointing to important resources and explaining the most tricky parts. I had the occasion to work on many different problems using various machine learning techniques: ensembles methods to improve accuracy, distillation, autoencoders, contrastive loss, weakly-supervised, semi-supervised and active learning, topic modelling, personality clustering on social networks,\u2026 I had both the occasion to read and implement recent papers, and do original research. Many among us were interested in machine learning and artificial intelligence advances, even outside the scope of Deepomatic. During the Tuesday research meetings, we had many stimulating discussions about scientific papers. Papers we wanted to implement, theoretical papers, or just papers we found interesting.\n\nWhile this has been an interesting year, I am mostly interested in very long term research, my dream being to contribute to the first artificial general intelligence (AGI). This is not the purpose of Deepomatic, so I applied to Vicarious (a company aiming to build a unified algorithmic architecture to achieve human-level intelligence) and I was accepted. Because Vicarious is a high profile and very selective company, this attests that Deepomatic was a great place to grow in competence both in terms of code and machine learning. Even more knowing that I had never programmed in python before joining Deepomatic and that my machine learning knowledge was minimal.\n\nSo, Deepomatic wants to hire a new data scientist. They will also need to find a replacement for my other job there. Do you think you have what it takes to be our new Beer Captain? Every Thursday afternoon, we have a team meeting. We discuss the latest business news (new contracts, prospects), our progress on current objectives (with demos), our next objectives, and any subject people want to bring up. And, to combine work and pleasure, we taste craft beers. As the next Beer Captain, you will have to remember everyone\u2019s preferences (small tip: get white beer for Zo\u00e9, and IPA for David). The two positions are obviously not linked, you can apply to be a data scientist without being Beer Captain material.\n\nThis is one of the small pleasures which, because we shared it together, brought the team closer. So did the chocolate croissant of monday morning, the footsie games, doing sport together (from jogging to tennis), the team seminar we had in Morocco, the Thursday meals Nolwenn cooks. We also have every quarter a \u201cKing for the Week\u201d: an employee gets to decide the priorities for a week. Kevin got the lucky charm in Epiphany\u2019s king cake and we focused on small projects we had delayed for too long, especially concerning our new office (for instance, we can now open the building door with our smartphone instead of bothering others with the intercom). Alexis was the next King for the Week and we focused on getting familiar with each other\u2019s code and tools. More generally, all those fun examples are the manifestation of the genuine attention of our founders, towards us, our opinions and our well-being.\n\nWhile my dreams are bringing me to other places, I leave Deepomatic with a heavy heart, because I had a great year there, and I will miss those colleagues who became friends.\n\nIf you are interested in the newest developments in deep learning and/or want to be a part of Paris\u2019 coolest start-up, be sure to visit this page\u00a0:)!", 
        "title": "Why I hate quitting Deepomatic \u2013"
    }, 
    {
        "url": "https://machinelearnings.co/machine-learnings-18-does-facebook-suck-at-machine-learning-7614f2e6713c?source=tag_archive---------6----------------", 
        "text": "#Awesome \n\n\u201cAs machine intelligence improves, the value of human prediction skills will decrease because machine prediction will provide a cheaper and better substitute. However, this does not spell doom for human jobs. When the cost of prediction falls, demand for judgment rises. We\u2019ll want more human judgment.\u201d\u200a\u2014\u200aAjay Agrawal, Professor of Entrepreneurship @ University of Toronto\n\n#Not Awesome\n\n\u201cAI and Machine learning are moving so quickly that any notion of ethics is lagging decades behind. This might explain a new study, which says computers can tell whether you will be a criminal based on nothing more than your facial features.\u201d\u200a\u2014\u200aSam Biddle, The Intercept_ Reporter\n\n1/ Facebook is doing a terrible job of using machine learning to detect fake news. Learn More on Elad Blog >\n\n2/ Every resource for non-technical people to learn about machine learning and AI is now in one place. Learn More on Medium >\n\n3/ A machine learning algorithm that classifies sensitive emails could save the U.S. State Department $16 billion. Learn More on MIT Technology Review >\n\n4/ Giant corporations are hoarding AI talent and screwing startups in the process. Learn More on Wired >\n\n5/ Researchers at Google DeepMind are using a classic video game to teach AI how to \u2018dream.\u2019 Learn More on Inverse >\n\n6/ OpenAI partners with Microsoft to use specialized chips that will help push the boundaries of artificial intelligence. Learn More on WIRED >\n\n7/ A student\u2019s machine learning app lets visually impaired people take a picture of an object and receive an audible description\u00a0. Learn More on TechCrunch >\n\nDo you ever sketch or doodle when you\u2019re bored? \ud83d\udd8c\ud83c\udfa8\n\nWell, Google just built a game, with machine learning, that tries to guess what you\u2019re drawing. It isn\u2019t perfect at first, but it gets better at guessing as you draw more. Go ahead and draw something!\n\nIs artificial intelligence beneficial or dangerous to humanity? \ud83e\udd16+\ud83d\udc71=\ud83d\udc80?\n\nEric Schmidt from Alphabet weighed in on this question on Twitter a few hours ago, and the responses have been pretty wild. See his take and everyone\u2019s response.", 
        "title": "\u201cDoes Facebook suck at machine learning?\u201d \u2014 #18 \u2013"
    }, 
    {
        "url": "https://medium.com/ai-for-founders/ai-for-technical-founders-day-0-why-how-and-what-8314a94ae7c2?source=tag_archive---------7----------------", 
        "text": "There is an industrial revolution taking place\n\nLately AI based startups and products have been dominating our social feeds, Product Hunt, news, and Ted Talks, occupying our own apartments with conversational assistants like Alexa and soon will be taking over a ton of jobs\u2026\n\nWe are on the verge of an industrial revolution. We, as founders, should take on the responsibility to act on what the future holds to inspire the world around us. More realistically, for the love of god, can we please put all that high-school/college math to use?\n\nThe right amount of passion & material\n\nJust like any other subject you want to pursue properly, you need the right amount of passion and the material to begin your journey. Here are few things which got me inspired:\n\nAn awesome blog post by Per Harald Borgen\u00a0: Machine Learning in a Year\n\nAfter talking to a few friends in the industry, reading a ton of blog posts, searching Amazon and sites like Udacity and Udemy, here is a list of relevant material I ended up collecting/buying:\n\nAn open source project, which will help Fundraising, PR and BD using AI\n\nMy goal is to create a tool for founders to help them with Fundraising, while learning/polishing my skills. Often times the most challenging part of fundraising is identifying the right investors. AngelList, Crunchbase, Linkedin and your network is great, but I want to be able to answer these questions:\n\nI am sure the roadmap will change as we go along, but for now at least I have a goal and a set mission to go after.", 
        "title": "AI for Technical Founders, Day 0. Why, How and What\u2026"
    }, 
    {
        "url": "https://medium.com/the-quarter-espresso/introduction-of-neural-redis-part-3-9d59012120a?source=tag_archive---------8----------------", 
        "text": "If we are going to implement a classifier that has a set of 2 inputs, 1 hidden layer with 3 units, and the set of 2 types as below:\n\nLet\u2019s create the neural network (named as ) via the following command:\n\nHere is an easier way to calculate the total number of tunable parameters. Assume U is the number of units in the layer, the total number of tunable parameters will be:\n\nIn this case the it will be:\n\nIf we want the classifier to label the set of inputs as below:\n\nThus the classifier will actually infer the tunable parameters, to get the following equations:\n\nLet\u2019s insert one of the training examples:\n\nWe can have several training examples inserted:\n\nLet\u2019s see how it classifies the set of inputs via :\n\nThe output of is the number of type it labeled.\n\nWe can also see the actual output for each types via :\n\nObviously, the set of inputs is labeled to the type that as the highest score.\n\nPart 4 will show you some examples using neural-redis.", 
        "title": "Introduction of neural-redis, part 3 \u2013 The Quarter Espresso \u2013"
    }, 
    {
        "url": "https://medium.com/@enterlol/until-death-do-us-part-6548267c913b?source=tag_archive---------9----------------", 
        "text": "Does your car \u201clove you? In the future, it might.\n\nAs deep learning algorithms get smarter and more complex to pilot autonomous cars\u200a\u2014\u200aa simple dilemma might disrupt everything:\n\nIf your autonomous car while driving encounters four children suddenly running across the road just before you enter a tunnel, would your car swerve to protect the children and hit the side of tunnel to potentially kill you? Or would it run over the children?\n\nThe future of autonomous car will be fixated on this dilemma. The autonomous car sales man and sales woman will be selling the \u201cbond\u201d between you and car\u200a\u2014\u200amore than selling the features, looks, and speed. Car sales women and men will be selling \u201caffinity\u201d\u200a\u2014\u200aand doing matchmaking of riders to cars.\n\nMore than auto repair shops, there might also arise a time where automobile \u201cattitude adjustment\u201d shops pop up to tweak the \u201cbond\u201d or \u201caffinity\u201d to the owner and passengers of the car. Of course these will null and void insurance policies if discovered and these illegal data science shops to increase correlation of decision making towards those on the inside of the car than those on the outside\u200a\u2014\u200awould appear as factory errors and defects. Those that do this face potential blacklisting as future autonomous car customers and employable data scientists.\n\nThis is akin to currently buying a cutting edge smart phone say in the USA and having a hacker \u201ccrack it\u201d to be used in Asia. It null and voids the warranty and occasionally \u201cbricks\u201d the phone from being useful.\n\nAs a child my favorite Disney movie featured \u201cHerbie the Love Bug\u201d\u200a\u2014\u200anot to be confused with the one with Lindsey Lohan.\n\nHerbie was a 1969 Volkswagen Bug or Beetle painted while with a single blue and red racing stripe and the number fifty-three encircled in black on both the drives, passenger side door and hoed. Herbie would \u201cbond\u201d with his owner, sometimes even getting jealous of potential new cars, and remarkably even for being a Volkswagen Bug be able to win car races\u200a\u2014\u200aincluding the Monte Carlo Rally. Herbie was an autonomous car who loved to improvise. In the original Herbie was sawed or cut in two and won 1st and 3rd place in the same race.\n\nBut never in the series of films including \u201cHerbie Goes Bananas\u201d set in Mexico did Herbie have to make a decision to keep the driver alive or safe over others.\n\nIn the future, if an accident happens that kills or injures a passengers or driver\u200a\u2014\u200ait might \u201cpsychologically\u201d block that owner from ever purchasing that brand again.\n\nWhich things up another dilemma\u200a\u2014\u200ahow long would the normal \u201clife\u201d of a car last? Would it be similar to the life of a pet? Seven years?\n\nWhat about used cars? As drivers and passengers are different from household to household will some choose to keep the existing \u201csoul\u201d or reboot with a \u201cnewborn\u201d? Is it cheaper to keep the existing one?\n\nAutonomous cars and their \u201cdeep learning souls\u201d could be passed down as heirlooms from generation to generation much like the family name.\n\nAs Aretha Franklin once sang, \u201dWelcome to the Freeway of Love.\u201d", 
        "title": "Autonomous Cars: Until Death Do Us Part \u2013 enterlifeonline \u2013"
    }
]