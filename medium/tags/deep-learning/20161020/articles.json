[
    {
        "url": "https://medium.com/emergent-future/emergent-future-preparing-for-our-ai-future-smarter-smartphones-self-driving-cars-12a1c0e71177?source=tag_archive---------0----------------", 
        "text": "You Might Have Heard: President Obama sat down with MIT Media Lab director Joi Ito and WIRED to talk about neural nets, self-driving cars, and the future of the world.\n\nIn this must read interview Obama and Ito discuss how AI will shape our world over the next 50 years more than any other single technology.\n\nBut Did You Know? The Obama Administration releasedPreparing for the Future of Artificial Intelligence, a report focused on the opportunities, considerations, and challenges of artificial intelligence on society.\n\nThe new report coincided with the White House Frontiers Conference in Pittsburgh, where some of the world\u2019s top innovators came together to discuss how science and technology can help improve lives, such as using machine learning to build intelligent traffic systems, improve medical diagnoses, and protect wildlife.\n\nWorried about your job? Don\u2019t be. While artificial intelligence will kill some jobs, it\u2019ll create others.\n\nALSO: Obama challenged America to send humans to Mars by the 2030s, and safely return them to Earth, with the ultimate ambition to one day make human settlement of space a reality.\n\nGoogle aims to win the AI revolution by building a personal assistant into everything.\n\nTim Cook, meanwhile, says the iPhone\u2019s future is in AI, which is why Apple\u2019s hired a prominent artificial intelligence researcher from Carnegie Mellon University.\n\nApple\u2019s penchant for secrecy has burdened efforts to improve their AI offerings and hire the best talent. Researchers in the field like to publish their findings, something Apple has historically frowned upon.\n\nWalt Mossberg says Apple wasted its lead with Siri and now has an intelligent assistant that is too limited and unreliable to be an effective tool in coming AI wars.\n\nIt\u2019s a bit ironic then that Samsung recently acquired Viv, the next-gen AI assistant built by the creators of Apple\u2019s Siri. \ud83d\udd25\n\nIn the end, the only difference between the next generation of smartphones will be their AI assistants.", 
        "title": "Emergent // Future: Preparing for Our AI Future, Smarter Smartphones, Self-Driving Cars"
    }, 
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-session-4-proofs-of-key-neural-net-properties-4213b46a561c?source=tag_archive---------1----------------", 
        "text": "Yesterday evening, untapt hosted yet another session of our Deep Learning Study Group.\n\nOur recommended preparatory reading this time around was the fourth and fifth chapters of Michael Nielsen\u2019s interactive textbook on neural networks. We took a break from applying techniques for this one session to speed our progress through the textbook theory, but we\u2019ll get back on track for the next session. Below are my notes on what we covered.\n\nNeural nets can compute any function (i.e., they are universal), assuming that (1) we accept they are an approximation (that can be improved by the inclusion of additional hidden neurons) as opposed to an exact solution, and (2) the function they are explaining is continuous (e.g., has no sharp jumps).\n\nFor the first time in our study session, we moved from whiteboarding to a projector to cover this content. This is because, in his fourth chapter, Michael Nielsen did a tremendous job of developing thematically-coherent, interactive Java applets that facilitate a clear visual understanding of this proof, and we wanted to be able to talk through these demonstrations.\n\nA fair bit of our discussion centered on the practicalities of expanding the proof beyond two inputs features into n-dimensional space.\n\nWe primarily discussed the causes, implications, and methods to mitigate unstable gradients, which in deep neural nets tend to vanish but in some cases can instead explode.\n\nWe also touched on other factors that can make deep nets difficult to train, e.g., the propensity for sigmoids to saturate, the risks of random weight initialization.\n\nThomas Balestri introduced us to Jason Yosinski\u2019s breathtaking Deep Visualization Toolbox for developing an understanding of how individual layers contribute to a convolutional NN.\n\nAt our next meeting, we\u2019ll discuss finishing off Nielsen\u2019s textbook and working through a TensorFlow implementation of a convolutional neural net that was built for visual recognition.\n\nFor curated data science resources, including suggested paths for getting started with deep learning, visit my site.", 
        "title": "Deep Learning Study Group Session #4: Proofs of Key Neural Net Properties"
    }, 
    {
        "url": "https://medium.com/@vcjha/ais-language-problem-1fa1cbdc858f?source=tag_archive---------2----------------", 
        "text": "About halfway through a particularly tense game of Go held in Seoul, South Korea, between Lee Sedol, one of the best players of all time, and AlphaGo, an artificial intelligence created by Google, the AI program made a mysterious move that demonstrated an unnerving edge over its human opponent.\n\nOn move 37, AlphaGo chose to put a black stone in what seemed, at first, like a ridiculous position. It looked certain to give up substantial territory\u200a\u2014\u200aa rookie mistake in a game that is all about controlling the space on the board. Two television commentators wondered if they had misread the move or if the machine had malfunctioned somehow. In fact, contrary to any conventional wisdom, move 37 would enable AlphaGo to build a formidable foundation in the center of the board. The Google program had effectively won the game using a move that no human would\u2019ve come up with.\n\nAlphaGo\u2019s victory is particularly impressive because the ancient game of Go is often looked at as a test of intuitive intelligence. The rules are quite simple. Two players take turns putting black or white stones at the intersection of horizontal and vertical lines on a board, trying to surround their opponent\u2019s pieces and remove them from play. Playing well, however, is incredibly hard.\n\nWhereas chess players are able to look a few moves ahead, in Go this isn\u2019t possible without the game unfolding into intractable complexity, and there are no classic gambits. There is also no straightforward way to measure advantage, and it can be hard for even an expert player to explain precisely why he or she made a particular move. This makes it impossible to write a simple set of rules for an expert-level computer program to follow.\n\nAlphaGo wasn\u2019t told how to play Go at all. Instead, the program analyzed hundreds of thousands of games and played millions of matches against itself. Among several AI techniques, it used an increasingly popular method known as deep learning, which involves mathematical calculations inspired, very loosely, by the way interconnected layers of neurons fire in a brain as it learns to make sense of new information. The program taught itself through hours of practice, gradually honing an intuitive sense of strategy. That it was then able to beat one of the world\u2019s best Go players represents a true milestone in machine intelligence and AI.\n\nA few hours after move 37, AlphaGo won the game to go up two games to nothing in the best-of-five match. Afterward Sedol stood before a crowd of journalists and photographers, politely apologizing for letting humankind down. \u201cI am quite speechless,\u201d he said, blinking through a storm of flash photography.\n\nAlphaGo\u2019s surprising success points to just how much progress has been made in artificial intelligence over the last few years, after decades of frustration and setbacks often described as an \u201cAI winter.\u201d Deep learning means that machines can increasingly teach themselves how to perform complex tasks that only a couple of years ago were thought to require the unique intelligence of humans. Self-\u00addriving cars are already a foreseeable possibility. In the near future, systems based on deep learning will help diagnose diseases and recommend treatments.\n\nDeep learning means that machines can increasingly teach themselves how to perform complex tasks that only a couple of years ago were thought to require the unique intelligence of humans.\n\nYet despite these impressive advances, one fundamental capability remains elusive: language. Systems like Siri and IBM\u2019s Watson can follow simple spoken or typed commands and answer basic questions, but they can\u2019t hold a conversation and have no real understanding of the words they use. If AI is to be truly transformative, this must change.\n\nEven though AlphaGo cannot speak, it contains technology that might lead to greater language understanding. At companies such as Google, Facebook, and Amazon, as well as at leading academic AI labs, researchers are attempting to finally solve that seemingly intractable problem, using some of the same AI tools\u200a\u2014\u200aincluding deep learning\u200a\u2014\u200athat are responsible for AlphaGo\u2019s success and today\u2019s AI revival. Whether they succeed will determine the scale and character of what is turning into an artificial-\u00adintelligence revolution. It will help determine whether we have machines we can easily communicate with\u200a\u2014\u200amachines that become an intimate part of our everyday life\u200a\u2014\u200aor whether AI systems remain mysterious black boxes, even as they become more autonomous. \u201cThere\u2019s no way you can have an AI system that\u2019s humanlike that doesn\u2019t have language at the heart of it,\u201d says Josh Tenenbaum, a professor of cognitive science and computation at MIT. \u201cIt\u2019s one of the most obvious things that set human intelligence apart.\u201d\n\nPerhaps the same techniques that let AlphaGo conquer Go will finally enable computers to master language, or perhaps something else will also be required. But without language understanding, the impact of AI will be different. Of course, we can still have immensely powerful and intelligent software like AlphaGo. But our relationship with AI may be far less collaborative and perhaps far less friendly. \u201cA nagging question since the beginning was \u2018What if you had things that were intelligent in the sense of being effective, but not like us in the sense of not empathizing with what we are?\u2019\u201d says Terry Winograd, a professor emeritus at Stanford University. \u201cYou can imagine machines that are not based on human intelligence, which are based on this big-data stuff, and which run the world.\u201d\n\nA couple of months after AlphaGo\u2019s triumph, I traveled to Silicon Valley, the heart of the latest boom in artificial intelligence. I wanted to visit the researchers who are making remarkable progress on practical applications of AI and who are now trying to give machines greater understanding of language.\n\nI started with Winograd, who lives in a suburb nestled into the southern edge of Stanford\u2019s campus in Palo Alto, not far from the headquarters of Google, Facebook, and Apple. With curly white hair and a bushy mustache, he looks the part of a venerable academic, and he has an infectious enthusiasm.\n\nBack in 1968, Winograd made one of the earliest efforts to teach a machine to talk intelligently. A math prodigy fascinated with language, he had come to MIT\u2019s new AI lab to study for his PhD, and he decided to build a program that would converse with people, via a text prompt, using everyday language. It didn\u2019t seem an outlandish ambition at the time. Incredible strides were being made in AI, and others at MIT were building complex computer vision systems and futuristic robot arms. \u201cThere was a sense of unknown, unbounded possibilities,\u201d he recalls.\n\nNot everyone was convinced that language could be so easily mastered, though. Some critics, including the influential linguist and MIT professor Noam \u00adChomsky, felt that the AI researchers would struggle to get machines to understand, given that the mechanics of language in humans were so poorly understood. Winograd remembers attending a party where a student of Chomsky\u2019s walked away when he heard him say that he worked in the AI lab.\n\nBut there was reason to be optimistic, too. Joseph \u00adWeizenbaum, a German-born professor at MIT, had built the very first chatbot program a couple of years earlier. Called ELIZA, it was programmed to act like a cartoon psychotherapist, repeating key parts of a statement or asking questions to encourage further conversation. If you told the program you were angry at your mother, for instance, it would say, \u201cWhat else comes to mind when you think about your mother?\u201d A cheap trick, but it worked surprisingly well. Weizenbaum was shocked when some subjects began confessing their darkest secrets to his machine.\n\nThere\u2019s an obvious problem with applying deep learning to language. It\u2019s that words are arbitrary symbols, and as such they are fundamentally different from imagery.\n\nWinograd wanted to create something that really seemed to understand language. He began by reducing the scope of the problem. He created a simple virtual environment, a \u201cblock world,\u201d consisting of a handful of imaginary objects sitting on an imaginary table. Then he created a program, which he named SHRDLU, that was capable of parsing all the nouns, verbs, and simple rules of grammar needed to refer to this stripped-down virtual world. SHRDLU (a nonsense word formed by the second column of keys on a Linotype machine) could describe the objects, answer questions about their relationships, and make changes to the block world in response to typed commands. It even had a kind of memory, so that if you told it to move \u201cthe red cone\u201d and then later referred to \u201cthe cone,\u201d it would assume you meant the red one rather than one of another color.\n\nSHRDLU was held up as a sign that the field of AI was making profound progress. But it was just an illusion. When Winograd tried to make the program\u2019s block world larger, the rules required to account for the necessary words and grammatical complexity became unmanageable. Just a few years later, he had given up, and eventually he abandoned AI altogether to focus on other areas of research. \u201cThe limitations were a lot closer than it seemed at the time,\u201d he says.\n\nWinograd concluded that it would be impossible to give machines true language understanding using the tools available then. The problem, as Hubert Dreyfus, a professor of philosophy at UC Berkeley, argued in a 1972 book called What Computers Can\u2019t Do, is that many things humans do require a kind of instinctive intelligence that cannot be captured with hard-and-fast rules. This is precisely why, before the match between Sedol and AlphaGo, many experts were dubious that machines would master Go.\n\nBut even as Dreyfus was making that argument, a few researchers were, in fact, developing an approach that would eventually give machines this kind of intelligence. Taking loose inspiration from neuroscience, they were experimenting with artificial neural networks\u200a\u2014\u200alayers of mathematically simulated neurons that could be trained to fire in response to certain inputs. To begin with, these systems were painfully slow, and the approach was dismissed as impractical for logic and reasoning. Crucially, though, neural networks could learn to do things that couldn\u2019t be hand-coded, and later this would prove useful for simple tasks such as recognizing handwritten characters, a skill that was commercialized in the 1990s for reading the numbers on checks. Proponents maintained that neural networks would eventually let machines to do much, much more. One day, they claimed, the technology would even understand language.\n\nOver the past few years, neural networks have become vastly more complex and powerful. The approach has benefited from key mathematical refinements and, more important, faster computer hardware and oodles of data. By 2009, researchers at the University of Toronto had shown that a many-layered deep-learning network could recognize speech with record accuracy. And then in 2012, the same group won a machine-vision contest using a deep-learning algorithm that was astonishingly accurate.\n\nA deep-learning neural network recognizes objects in images using a simple trick. A layer of simulated neurons receives input in the form of an image, and some of those neurons will fire in response to the intensity of individual pixels. The resulting signal passes through many more layers of interconnected neurons before reaching an output layer, which signals that the object has been seen. A mathematical technique known as backpropagation is used to adjust the sensitivity of the network\u2019s neurons to produce the correct response. It is this step that gives the system the ability to learn. Different layers inside the network will respond to features such as edges, colors, or texture. Such systems can now recognize objects, animals, or faces with an accuracy that rivals that of humans.\n\nThere\u2019s an obvious problem with applying deep learning to language. It\u2019s that words are arbitrary symbols, and as such they are fundamentally different from imagery. Two words can be similar in meaning while containing completely different letters, for instance; and the same word can mean various things in different contexts.\n\nIn the 1980s, researchers had come up with a clever idea about how to turn language into the type of problem a neural network can tackle. They showed that words can be represented as mathematical vectors, allowing similarities between related words to be calculated. For example, \u201cboat\u201d and \u201cwater\u201d are close in vector space even though they look very different. Researchers at the University of Montreal, led by Yoshua Bengio, and another group at Google, have used this insight to build networks in which each word in a sentence can be used to construct a more complex representation\u200a\u2014\u200asomething that Geoffrey Hinton, a professor at the University of Toronto and a prominent deep-learning researcher who works part-time at Google, calls a \u201cthought vector.\u201d\n\nBy using two such networks, it is possible to translate between two languages with excellent accuracy. And by combining this type of network with one designed to recognize objects in images, it is possible to conjure up surprisingly plausible captions.\n\nSitting in a conference room at the heart of Google\u2019s bustling headquarters in Mountain View, California, one of the company\u2019s researchers who helped develop this approach, Quoc Le, is contemplating the idea of a machine that could hold a proper conversation. Le\u2019s ambitions cut right to the heart of why talking machines could be useful. \u201cI want a way to simulate thoughts in a machine,\u201d he says. \u201cAnd if you want to simulate thoughts, then you should be able to ask a machine what it\u2019s thinking about.\u201d\n\nGoogle is already teaching its computers the basics of language. This May the company announced a system, dubbed Parsey McParseface, that can look at syntax, recognizing nouns, verbs, and other elements of text. It isn\u2019t hard to see how valuable better language understanding could be to the company. Google\u2019s search algorithm used to simply track keywords and links between Web pages. Now, using a system called RankBrain, it reads the text on pages in an effort to glean meaning and deliver better results. Le wants to take that much further. Adapting the system that\u2019s proved useful in translation and image captioning, he and his colleagues built Smart Reply, which reads the contents of Gmail messages and suggests a handful of possible replies. He also created a program that learned from Google\u2019s IT support chat logs how to answer simple technical queries.\n\nMost recently, Le built a program capable of producing passable responses to open-ended questions; it was trained by being fed dialogue from 18,900 movies. Some of its replies seem eerily spot-on. For example, Le asked, \u201cWhat is the purpose of life?\u201d and the program responded, \u201cTo serve the greater good.\u201d \u201cIt was a pretty good answer,\u201d he remembers with a big grin. \u201cProbably better than mine would have been.\u201d\n\nThere\u2019s only one problem, as quickly becomes apparent when you look at more of the system\u2019s answers. When Le asked, \u201cHow many legs does a cat have?\u201d his system answered, \u201cFour, I think.\u201d Then he tried, \u201cHow many legs does a centipede have?\u201d which produced a curious response: \u201cEight.\u201d Basically, Le\u2019s program has no idea what it\u2019s talking about. It understands that certain combinations of symbols go together, but it has no appreciation of the real world. It doesn\u2019t know what a centipede actually looks like, or how it moves. It is still just an illusion of intelligence, without the kind of common sense that humans take for granted. Deep-learning systems can often be wonky this way. The one Google created to generate captions for images would make bizarre errors, like describing a street sign as a refrigerator filled with food.\n\nLe asked, \u201cWhat is the purpose of life?\u201d and the program responded, \u201cTo serve the greater good.\u201d\n\nBy a curious coincidence, Terry \u00adWinograd\u2019s next-door neighbor in Palo Alto is someone who might be able to help computers attain a deeper appreciation of what words actually mean. Fei-Fei Li, director of the Stanford Artificial Intelligence Lab, was on maternity leave when I visited, but she invited me to her home and proudly introduced me to her beautiful three-month-old baby, Phoenix. \u201cSee how she looks at you more than me,\u201d Li said as Phoenix stared at me. \u201cThat\u2019s because you are new; it\u2019s early facial recognition.\u201d\n\nLi has spent much of her career researching machine learning and computer vision. Several years ago, she led an effort to build a database of millions of images of objects, each tagged with an appropriate keyword. But Li believes machines need an even more sophisticated understanding of what\u2019s happening in the world, and this year her team released another database of images, annotated in much richer detail. Each image has been tagged by a human with dozens of descriptors: \u201cA dog riding a skateboard,\u201d \u201cDog has fluffy, wavy fur,\u201d \u201cRoad is cracked,\u201d and so on. The hope is that machine-learning systems will learn to understand more about the physical world. \u201cThe language part of the brain gets fed a lot of information, including from the visual system,\u201d Li says. \u201cAn important part of AI will be integrating these systems.\u201d\n\nThis is closer to the way children learn, by associating words with objects, relationships, and actions. But the analogy with human learning goes only so far. Young children do not need to see a skateboarding dog to be able to imagine or verbally describe one. Indeed, Li believes that today\u2019s machine-learning and AI tools won\u2019t be enough to bring about real AI. \u201cIt\u2019s not just going to be data-rich deep learning,\u201d she says. Li believes AI researchers will need to think about things like emotional and social intelligence. \u201cWe [humans] are terrible at computing with huge data,\u201d she says, \u201cbut we\u2019re great at abstraction and creativity.\u201d\n\nNo one knows how to give machines those human skills\u200a\u2014\u200aif it is even possible. Is there something uniquely human about such qualities that puts them beyond the reach of AI?\n\nCognitive scientists like MIT\u2019s Tenenbaum theorize that important components of the mind are missing from today\u2019s neural networks, no matter how large those networks might be. Humans have the ability to learn very quickly from a relatively small amount of data and have a built-in ability to model the world in 3-D very efficiently. \u201cLanguage builds on other abilities that are probably more basic, that are present in young infants before they have language: perceiving the world visually, acting on our motor systems, understanding the physics of the world or other agents\u2019 goals,\u201d \u00adTenenbaum says.\n\nIf he is right, then it will be difficult to re-create language understanding in machines and AI systems without trying to mimic human learning, mental model building, and psychology.\n\nNoah Goodman\u2019s office in Stanford\u2019s psychology department is practically bare except for a couple of abstract paintings propped against one wall and a few overgrown plants. When I arrived, Goodman was typing away on a laptop, his bare feet up on a table. We took a stroll across the sun-bleached campus for iced coffee. \u201cLanguage is special in that it relies on a lot of knowledge about language but it also relies on a huge amount of common-sense knowledge about the world, and those two go together in very subtle ways,\u201d he explained.\n\nGoodman and his students have developed a programming language, called Webppl, that can be used to give computers a kind of probabilistic common sense, which turns out to be pretty useful in a conversation. One experimental version can understand puns, and another can cope with hyperbole. If it is told that some people had to wait \u201cforever\u201d for a table in a restaurant, it will automatically decide that the literal meaning is improbable, and they most likely just hung around for a long time and were annoyed. The system is far from truly intelligent, but it shows how new approaches could help make AI programs that talk in a more lifelike way.\n\nAt the same time, Goodman\u2019s example also suggests just how difficult it will be to teach language to machines. Understanding the contextual meaning of \u201cforever\u201d is the kind of thing that AI systems will need to learn, but it is a rather simple and rudimentary accomplishment.\n\n\u201cI want a way to simulate thoughts in a machine,\u201d he says. \u201cAnd if you want to simulate thoughts, then you should be able to ask a machine what it\u2019s thinking about.\u201d\n\nStill, despite the difficulty and complexity of the problem, the startling success that researchers have had using deep-learning techniques to recognize images and excel at games like Go does at least provide hope that we might be on the verge of breakthroughs in language, too. If so, those advances will come just in time. If AI is to serve as a ubiquitous tool that people use to augment their own intelligence and trust to take over tasks in a seamless collaboration, language will be key. That will be especially true as AI systems increasingly use deep learning and other techniques to essentially program themselves.\n\n\u201cIn general, deep-learning systems are awe-inspiring,\u201d says John Leonard, a professor at MIT who researches automated driving. \u201cBut on the other hand, their performance is really hard to understand.\u201d\n\nToyota, which is studying a range of self-driving technologies, has initiated a research project at MIT led by Gerald Sussman, an expert on artificial intelligence and programming language, to develop automated driving systems capable of explaining why they took a particular action. And an obvious way for a self-driving car to do this would be by talking. \u201cBuilding systems that know what they know is a really hard problem,\u201d says Leonard, who is leading a different Toyota-backed project at MIT. \u201cBut yeah, ideally they would give not just an answer but an explanation.\u201d\n\nA few weeks after returning from California, I saw David Silver, the Google \u00adDeepMind researcher who designed AlphaGo, give a talk about the match against Sedol at an academic conference in New York. Silver explained that when the program came up with its killer move during game two, his team was just as surprised as everyone else. All they could see was AlphaGo\u2019s predicted odds of winning, which changed little even after move 37. It was only several days later, after careful analysis, that the Google team made a discovery: by digesting previous games, the program had calculated the chances of a human player making the same move at one in 10,000. And its practice games had also shown that the play offered an unusually strong positional advantage.\n\nSo in a way, the machine knew that Sedol would be completely blindsided.\n\nSilver said that Google is considering several options for commercializing the technology, including some sort of intelligent assistant and a tool for health care. Afterward, I asked him about the importance of being able to communicate with the AI behind such systems. \u201cThat\u2019s an interesting question,\u201d he said after a pause. \u201cFor some applications it may be important. Like in health care, it may be important to know why a decision is being made.\u201d\n\nIndeed, as AI systems become increasingly sophisticated and complex, it is hard to envision how we will collaborate with them without language\u200a\u2014\u200awithout being able to ask them, \u201cWhy?\u201d More than this, the ability to communicate effortlessly with computers would make them infinitely more useful, and it would feel nothing short of magical. After all, language is our most powerful way of making sense of the world and interacting with it. It\u2019s about time that our machines\u00a0\n\ncaught up.", 
        "title": "AI\u2019s Language Problem \u2013 Vikram Jha \u2013"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/deep-learning-takes-on-gifs-fashion-doodles-and-more-at-acm-multimedia-df0379c3e8d1?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep Learning Takes on GIFs, Fashion, Doodles and More at ACM Multimedia"
    }
]