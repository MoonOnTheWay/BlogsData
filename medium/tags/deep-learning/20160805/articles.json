[
    {
        "url": "https://medium.com/@philjama/how-tensors-advance-human-technology-3831bff0906?source=tag_archive---------0----------------", 
        "text": "Simply put, a tensor is a multi-dimensional matrix, or array of numbers. More generally, tensor analysis spans a set of mathematical tools used to quantify and model a diversity of systems spanning physics, psychology, evolutionary biology, and modern artificial intelligence.\n\nLike many, I was curious to understand more about what tensors have to do with machine learning.\n\nHow are tensors used in modern approaches to artificial intelligence and machine learning?\n\nI first came across tensors in the form of continuum mechanics while studying engineering.\n\nLet me tell you a fascinating story about mathematics. A mathematical exploration of the meaning and origins of tensors.\n\nThis story tells some of the fascinating applications of tensor analysis\u200a\u2014\u200aones that have helped shape and revolutionize humanity\u2019s technological progress.\n\nIn the field of solid mechanics, tensors are used to quantify forces throughout a material.\n\nLet\u2019s take, for instance, consider a piece of cylindrical chalk used to write on blackboards in school. Imagine pulling, bending, and twisting this solid and predicting when and how it would break.\n\nApplying a combination of external forces on a simple object (like chalk) creates an array of stresses throughout the material.\n\nThese stresses can be modeled using a set of 9 stress forces, exerted on virtually every particle within the object.\n\nThe stresses throughout a solid can be modeled at every point using a a tensor known as the Cauchy stress tensor.\n\nThis type of analysis yields some curious consequences.\n\nFor instance, it turns out that if you apply a purely torsional force (twist) a piece of chalk until it breaks, then it will consistently snap along a 45-degree angle.\n\nWhereas bending the chalk will break chalk along a 90-degree angle.\n\nMaterials such as chalk and metals often break predictably depending on:\n\nStress forces, like in the chalk example, can be modeled by tensors in an area of mathematics called continuum mechanics.\n\nUnderstanding when things will break helps us design things that won\u2019t break unexpectedly.\n\nThe development of mathematics to model physical forces has allowed humans to make tremendous leaps in technological progress.\n\nLet\u2019s consider that for most of civilization, the tallest structures created by human were piles of stones\u200a\u2014\u200apyramids and eventually cathedrals.\n\nThe tensor mathematics used to model stresses in materials such as steel was developed in the mid 1800\u2019s by Augustin-Louis Cauchy. These tensor equations helped advance humanity\u2019s engineering from tall piles of stones to the Eiffel Tower, and far beyond.\n\nIn fact, for his contributions, Cauchy\u2019s name is one of the seventy-two names of scientists, engineers, and mathematicians engraved near the base of the Eiffel Tower.\n\nThe consequences of this can be staggering if you think that every time you drive over a large bridge or ride a tall elevator, that your life relies (at least in some small way) on these equations of mathematical physics and continuum mechanics. Engineering design calculations apply principles of tensors to ensure that the roof doesn\u2019t fall in on our heads.\n\nAfter the technological progress of the nineteenth century, another mathematician named Hermann Minkowski, took this direction of mathematics further by describing Einstein\u2019s theory of Special Relativity in terms of a tensor field representing a four-dimensional manifold known as spacetime.\n\nIn General Relativity, the Riemann Curvature Tensor associates a tensor to each point in space used to describe the bending of spacetime due to gravity.\n\nTensors also appear in the study of electromagnetism\u200a\u2014\u200aMinkowski simplified Maxwell\u2019s equations (four vector calculus equations) into two tensor field equations.\n\nThere are plenty of other examples of tensor mathematics spanning from fluid mechanics, to quantum mechanics.\n\nIt\u2019s fascinating to think that tensors appear in both the mathematics used to model spacetime (time-dilation due to gravity) as well as electromagnetism. This has laid the foundations for further leaps in technology, which have allowed humans to create AM/FM radio, satellite communications (such as GPS) and hopefully (one day) interstellar space travel.\n\nSo what do tensors mean in the context of AI and machine learning? Particularly deep learning.\n\nTensor mathematics comes up in a class of machine learning models that involve hidden variables.\n\nIn these models, the latent (hidden) state of data cannot be observed directly, but instead, their effects are indirectly observed in correlated variables.\n\nFor instance, let\u2019s consider Gaussian mixture models\u200a\u2014\u200awhere random samples are drawn from several normal distributions (or bell curves) with unknown parameters. The challenge becomes estimating the parameters of the probability distributions that generated the data samples.\n\nTraditionally we would solve for these unknown parameters using iterative approaches such as the expectation\u2013maximization algorithm. Taking a random initial estimate of the parameters, the algorithm would iterate repeatedly to converge on a solution.\n\nHowever, for a set of problems with many contributing factors, these iterative techniques become prohibitive and computationally expensive.\n\nThe modern approach to estimating these unknown parameters, known as the method of moments, was originally developed by a British biostatistician, Karl Pearson, who outlined the technique in his 1894 paper, Contributions to the Mathematical Theory of Evolution.\n\nPearson recognized that certain processes in nature were the result of multiple random factors. He remarks that the statistical distribution of various \u201cbiological, sociological, and economic measurements\u201d are not a result of a single Gaussian, but rather \u201cIt may happen that we have a mixture of 2,3,\u2026,n homogeneous groups, each of which deviates about its own mean symmetrically and in a manner represented with sufficient accuracy by the normal curve.\u201c\n\nWhat does this have to do with artificial intelligence? It turns out that Pearson was on the right path over 120 years ago. Solving for unknown statistical parameters using the method of moments approach has lead to breakthroughs in computation of various models in AI research.\n\nA key research paper on the topic, Tensor Decompositions for Learning Latent Variable Models, was published in 2014. One of the authors, Anandkumar wrote the following in a discussion on Quora:\n\nStandard iterative approaches, such as expectation-minimization mentioned earlier, tend to be computationally prohibitive. The breakthrough of these statistical tensor analysis techniques arises when we consider the computational efficiency of this approach.\n\nThe process of estimating model parameters is reduced to a problem of extracting a decomposition of a symmetric tensor derived from the moments. Furthermore, these decomposition problems tend to be amenable to efficient methods, such as gradient descent and the power iteration method.\n\nConsequently, these computationally-efficient methods lead to tremendous advancements in the field of machine learning enabling researchers to build much larger models in order to tackle far more complex problems.\n\nAdmittedly, current machine learning applications pale in comparison to the complexity of a single human brain. However with the exponential pace of improvements in technology and advancements in algorithmic techniques, some believe that a technological singularity may occur within decades.\n\nRecent years have already yielded exciting results in fields such as image recognition by detecting diabetic retinopathy in human eyes, brain computer interfacing (BCI) by identifying hand motions from EEG scans, and a plethora of applications in voice recognition, facial recognition, text analysis, and so forth.\n\nGoogle Research had a remarkable result where an unsupervised learning algorithm, trained from still frames from unlabeled YouTube videos, learned the visual concept of a cat. They managed to scale their computation across 16,000 CPU cores to train a model with over 1 billion connections!\n\nThe exciting feature of these algorithms (beyond the ability to identify cats) is their scalability. Rather than having computation limited to a single sequential calculation, modern approaches allow the computation to be shared among many, many processors.\n\nWhat applications will arrive with these mathematical and computational advancements? What are the limitations of tensor decomposition?\n\nThis feels like, in many ways, like an Eiffel Tower moment in history where all past efforts to build large-scale AI has been the equivalent of stacking large piles of stones. Now we could be developing the tools to make the next technological leap forward.", 
        "title": "How Tensors Advance Human Technology \u2013 Philip Jama \u2013"
    }, 
    {
        "url": "https://gab41.lab41.org/exploring-pipelines-68f900625bd4?source=tag_archive---------1----------------", 
        "text": "This summer I have taken on the formidable task of creating a \u2018good\u2019 data ingest and machine learning pipeline for Pythia. If you didn\u2019t know, aside from effectively transporting water and sludge, a pipeline can tactically link different aspects in projects to create one cohesive framework. As I quickly discovered, there is more to building a pipeline than meets the eye. After lots of tinkering, slogging, and construction, the Pythia pipeline was born.\n\nI cannot say I know the guy, but I have heard he made computers a long time ago. I have also found that a framework is a critical component to the success of a project (next Michael Dell perhaps?).\n\nFirst, some background on Pythia. Many people spend much of their day reading articles, trying to find the important ones or perhaps some new nugget of information. A majority of these articles may rehash the same information, so being able to eliminate \u201cduplicate\u201d articles would vastly improve these peoples\u2019 jobs. The Pythia project is exploring a variety of machine learning and deep learning techniques in order to detect novelty in text. We are defining novelty as a document that has something new to say about a topic, when compared to a corpus of already processed documents about that topic. For example, if \u201cI had a sandwich for lunch\u201d is the contents of the corpus (\u201clunch\u201d being the topic) a new document containing \u201cI ate a good sandwich\u201d has low novelty, whereas a new document containing \u201cAfter my sandwich, I took a nap\u201d has high novelty. To detect novelty, Pythia uses a wide range of features and algorithms, and each of these can interact with one another in distinct ways.\n\nAs I began working on Pythia, it quickly became apparent that there would be a lots of trial and error required to find the best features and algorithms to detect novelty for any given corpus, and that \u201chard coding\u201d them into Pythia would soon become cumbersome. I realized that it was vital to develop a flexible framework that allows our project to experiment on different combinations of features and algorithms and one that is easily extensible to incorporate new techniques.\n\nAfter building our pipeline, I have learned that there are major advantages of having a well-designed framework in three key parts of the data science workflow: development, experimentation and deployment.\n\nAs far as I can tell, the main reason people don\u2019t spend time on their framework is the urge to get started. In the moment, it seems easier simply to hard code the specific functionality desired than to spend time planning and developing a general structure. For truly one-off projects, this may be the right approach. However, for larger or more sustained efforts, a solid framework is essential to the project team\u2019s productivity.\n\nWith a well structured pipeline, implementing and integrating a new feature or algorithm becomes straightforward. Instead of having to consider how to work in the new technique, the sole focus can be on the technique itself because the code will easily slot into the larger framework.\n\nThe basic structure of Pythia\u2019s pipeline is the following:\n\nEach step of the pipeline is built to allow a broad array of functionality, facilitating the flexibility desired in each stage of the project\u2019s workflow. For example, the prepossessing module collects any necessary information and parameter settings from the data needed for the given features. In Pythia, we needed a vocabulary, encoder and decoder or a topic model, depending on the feature, and the pipeline allows us to incorporate and potentially change any of these items.\n\nWhile all the steps in the workflow benefit from a flexible pipeline, feature extraction and algorithm development benefit the most. As the pipeline moves to feature extraction, it is not unusual for a project to apply multiple feature extraction techniques using vectors. Each vector is a different way to represent a group of text, storing different information. The Pythia team is currently looking at bag of words vectors, skip-thought vectors, and latent Dirichlet allocation (LDA) vectors.\n\nFor each feature, we calculate the vector representation of the document we are analyzing and the vector representation of the corpus to which it is being compared. There are many ways to compare these two vectors, and the pipeline is set up to implement any combination of them. For example, the most classic comparison of two vectors is cosine similarity, which calculates the angle between two vectors. While this gives a single figure as a comparison point, some of the algorithms do better with more information, which is why the feature module can also calculate the difference and product of the document vector and the corpus vector, or merely concatenate the two. The feature module connects all the specified features into one vector for each datapoint, so it is ready to be fed into the classifiers.\n\nSetting up the pipeline with this kind of flexibility helps speed up development and eliminate errors because the generation of observations and the comparison of the vectors is all extracted from the individual feature calculation. To add a new feature to the pipeline, only a few if statements are required. This allows focus on the implementation of the feature itself, accelerates the integration into the pipeline, and requires less code and therefore reduces the chances for errors. As a proof point, adding skip-thoughts as a feature to our pipeline was less than an hour\u2019s work.\n\nThe three machine learning algorithms we currently have implemented are logistic regression, support vector machine (SVM) and Extreme Gradient Boosting (XGBoost). Each of these algorithms takes in the training data in the form of two lists: one with the feature vectors and the second with their corresponding label (duplicate or novel), and returns a classifier. By standardizing the input and output, any algorithm can run on any combination of features, simplifying algorithm implementation while allowing maximum flexibility.\n\nAs we move forward, we are working to implement additional deep learning techniques, such as one-hot convolutional neural networks and dynamic memory networks. These techniques can be easily added to the framework, as they follow the same workflow of generating features then building the classifier.\n\nAnother key advantage of setting up a strong framework is the smooth transition to the experimentation stage of the project. There are a few aspects of the pipeline that enable easy, repeatable, experimentation both with different combinations of features and algorithms themselves and also with the hyper-parameters of the features and algorithms.\n\nThe framework\u2019s parsing module reads JSON files full of text in an established format and stores them internally, randomly splitting all the document information into training and testing data given a configurable seed. Therefore, the raw data of a given dataset must be parsed separately, but once it is structured in compliance with the JSON format it can be run through the pipeline. This allows for easy experimentation on any number of datasets with little overhead.\n\nIn addition, since all of the modules, (features, algorithms, and other necessary information) are already connected, actually running tests all the way through to the end is easily achievable. The pipeline allows for any combination of any feature, and even any combination of different comparisons within the feature, as well as running any combination of algorithms. In Pythia, we exposed all of the important hyper parameters for our features and algorithms, such as vocabulary size for bag of words and kernel type for SVM, so that experimenting is even easier. Therefore, the same structure that helped us develop the pipeline also allows for immediate experimentation with absolute flexibility.\n\nIn a project like Pythia, any given dataset may require a unique combination of available NLP techniques and hyper-parameters in order to detect text novelty. Since we might have multiple clients who want to detect text novelty, and we don\u2019t have access to the specific data that our clients will be applying, it is important that our project is easily adjustable to the client\u2019s specific use case. Because the pipeline wraps up all of Pythia\u2019s functionality, a client can easily fiddle with parameters without any reconstruction or complications. All of the adjustments are centralized to one location, and the rest of the pipeline runs accordingly based on the specifications.", 
        "title": "Exploring Pipelines \u2013"
    }, 
    {
        "url": "https://towardsdatascience.com/machine-learning-for-hackers-b60bfb9bbade?source=tag_archive---------2----------------", 
        "text": "This week I gave a talk entitled \u201cIntro to Machine Learning for Hackers\u201d at the Dev-ES meetup.\n\nMachine Learning is a super cool subject with applications varying from speech recognition and recognizing objects in pictures to building self-driving cars. Deep Learning, which is a subfield of Machine Learning, is playing a crucial role in the business model of tech companies like Google, Facebook and Microsoft.\n\nMy main goal was to present an approach to studying Machine Learning that is mainly hands-on and abstracts most of the math for the beginner.\n\nMany people get discouraged from books and courses that tell them as soon as they can that multivariate calculus, inferential statistics and linear algebra are prerequisites. Even though these things are (very) important, we might be able to get more Machine Learning Engineers out there by engaging them early with high-level code and only later presenting them with what is going on under the hood.\n\nWe tried to make it as simple as possible by using Python notebooks and scikit-learn for the first couple of examples and then TFLearn on top of TensorFlow for recognizing handwritten digits.\n\nHere\u2019s the code on GitHub (warning: some things might be in Portuguese):\n\nTo hook people up even more I made sure to follow up with cool stuff like transfer learning, generative models and adversarial networks.\n\nAgain, if you understand Portuguese, feel free to browse the presentation:\n\nLet me know if you\u2019d like me to go through some of the code here on the blog too.\n\nA Portuguese version of this post has been published on the Dev-ES community blog.", 
        "title": "Machine Learning for Hackers \u2013"
    }, 
    {
        "url": "https://write.100nakedwords.com/lessons-learnt-through-conversations-37d257f567da?source=tag_archive---------3----------------", 
        "text": "", 
        "title": ""
    }, 
    {
        "url": "https://medium.com/emergent-future/apple-pivots-car-project-seymour-papert-has-died-ai-for-office-apps-and-machine-learning-in-3256e1bcd316?source=tag_archive---------4----------------", 
        "text": "Issue 18\n\n\u00a0This week we look at Apple\u2019s autonomous car project, reflect on the passing of AI pioneer Seymour Papert, check out the AI updates to Microsoft Office apps, and review some uses of machine learning in science. Plus, projects to try at home, and our top reads from the past week.\n\nNot a subscriber? Join the Emergent // Future newsletter here.\n\nYou might have heard: Apple is shifting the focus of their car projectfrom building a self-driving, electric car to developing an autonomous driving system.\n\nThe news comes after Apple tapped Bob Mansfield to oversee car project, dubbed Project Titan.\n\nApple isn\u2019t giving up on a car, however, but the company is now diving into the software that could power the next generation of carswith a focus on the user experience.\n\n\u201cWe have focused our AI efforts on the features that best enhance the customer experience,\u201d Tim Cook explained. \u201cA killer user experience that is integrated across their lives I think becomes more important, and I think that really plays to our advantage. I also think that the deployment of AI technology is something that we will excel at because of our focus on user experience, and so I like that.\u201d\n\nBut did you know: Tesla is buying SolarCity for $2.6 billion.\n\nA major part of Tesla CEO Elon Musk\u2019s master plan involves developing solutions to generate, store, and enable mass consumption of solar energy.\n\nTesla is also racing to finish the \u201cGigafactory\u201d before the Model 3 rollout in 2018. Musk anticipates that the plant could produce 105 gigawatt hours of battery cells by 2020.\n\nPLUS: Delphi hopes to make self-driving taxis a reality in Singapore by 2022. Delphi was the first car company to complete a cross-country road trip using autonomous technology, driving from San Francisco to New York back in April 2015.\n\nProfessor Emeritus Seymour Papert, a pioneer of constructionist learning, is dead at 88.\n\nThe world-renowned mathematician, learning theorist, and educational-technology visionary was a founding faculty member of the MIT Media Lab.\n\nWell before the advent of the personal computer, Papert foresaw children using computers as instruments for learning and enhancing creativity. Papert\u2019s \u201cconstructionist\u201d theory of education held that kids learn best by building things and making things happen.\n\nPapert was the co-author of the now-classic work on artificial intelligence: \u201cPerceptrons: An Introduction to Computational Geometry. The perceptron was one of the first artificial neural networks to be created. The algorithm uses pattern recognition based on a two-layer computer learning network.\n\nMicrosoft Word, Outlook, and PowerPoint are getting intelligent updates to help improve productivity and performance.\n\nWith the addition of machine learning and natural language processing, Microsoft believes the updates to Office 365 apps will \u201csave you time and produce better results.\u201d\n\nMicrosoft is adding a new Researcher feature to Word, which uses the Bing Knowledge Graph to find content from the internet and pull it straight into Word.\n\nEmergent Future is a weekly, hand-curated dispatch exploring technology through the lens of artificial intelligence, data science, and the shape of things to come. Subscribe here.", 
        "title": "Emergent // Future Weekly: Apple Pivots Car Project, Seymour Papert Has Died, AI for Office Apps\u2026"
    }, 
    {
        "url": "https://medium.com/@carmelitajkeeling/data-science-deep-learning-in-python-1d0e43babfdb?source=tag_archive---------5----------------", 
        "text": "This course will get you started in building your FIRST artificial neural network using deep learning techniques. Following my previous course on logistic regression, we take this basic building block, and build full-on non-linear neural networks right out of the gate using Python and Numpy. All the materials for this course are FREE.\n\nWe extend the previous binary classification model to multiple classes using the softmax function, and we derive the very important training method called \u201cbackpropagation\u201d using first principles. I show you how to code backpropagation in Numpy, first \u201cthe slow way\u201d, and then \u201cthe fast way\u201d using Numpy features.\n\nNext, we implement a neural network using Google\u2019s new TensorFlow library.\n\nYou should take this course if you are interested in starting your journey toward becoming a master at deep learning, or if you are interested in machine learning and data science in general. We go beyond basic models like logistic regression and linear regression and I show you something that automatically learns features.\n\nThis course provides you with many practical examples so that you can really see how deep learning can be used on anything. Throughout the course, we\u2019ll do a course project, which will show you how to predict user actions on a website given user data like whether or not that user is on a mobile device, the number of products they viewed, how long they stayed on your site, whether or not they are a returning visitor, and what time of day they visited.\n\nAnother project at the end of the course shows you how you can use deep learning for facial expression recognition. Imagine being able to predict someone\u2019s emotions just based on a picture!\n\nAfter getting your feet wet with the fundamentals, I provide a brief overview of some of the newest developments in neural networks\u200a\u2014\u200aslightly modified architectures and what they are used for.\n\nIf you already know about softmax and backpropagation, and you want to skip over the theory and speed things up using more advanced techniques along with GPU-optimization, check out my follow-up course on this topic, Data Science: Practical Deep Learning Concepts in Theano and TensorFlow.\n\nI have other courses that cover more advanced topics, such as Convolutional Neural Networks, Restricted Boltzmann Machines, Autoencoders, and more! But you want to be very comfortable with the material in this course before moving on to more advanced subjects.\n\nAll the code for this course can be downloaded from my github: /lazyprogrammer/machine_learning_examples\n\nMake sure you always \u201cgit pull\u201d so you have the latest version!\n\nHARD PREREQUISITES / KNOWLEDGE YOU ARE ASSUMED TO HAVE:\n\nTIPS (for getting through the course):", 
        "title": "Data Science: Deep Learning in Python \u2013 Carmelita J Keeling \u2013"
    }
]