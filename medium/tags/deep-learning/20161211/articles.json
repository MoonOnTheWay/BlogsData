[
    {
        "url": "https://medium.com/@mslavescu/amazing-work-for-open-source-self-driving-car-1a86e2ed6c6a?source=tag_archive---------0----------------", 
        "text": "Would be great if you can join our efforts there:\n\nSee here my call to:", 
        "title": "Amazing work for Open Source Self Driving Car! \u2013 Marius Slavescu \u2013"
    }, 
    {
        "url": "https://medium.com/bbm406f16/week3-eat-count-5ee4db6c0c58?source=tag_archive---------2----------------", 
        "text": "In this week, we searched some deep learning frameworks for our project. We needed a framework that satisfies our requirements such as easy to find sources about the framework, flexible, good at performance, suitable for work at Python etc. We found some frameworks, Caffe, Tensorflow, Theano and Torch [1]\u00a0. Each framework has advantages and disadvantages.\n\nFirst, we considered to not use Torch Framework because it is not suitable for Python. When we consider performance last three frameworks all uses the gpu for evaluation, then all of them were good. In Caffe Framework, language modelling were low level, but on the other hand, in Tensorflow and Theano Frameworks, we can abstract our architecture with using symbolic graphs. So that they are easier to use for us compare to Caffe. At last, when we compare these two frameworks, Theano is hard to navigate, debug and refactor because of whole code packaged as python string. But in Tensorflow, the code converted into C++ code and it is more cleaner modular architecture. In the direction of these results, we decided to use Tensorflow Framework and we started to our experiments with this framework.\n\nIn Tensorflow architecture, it represents computations as graphs. Every layers are made up of computations. At the beginning, we create a computation graph and run it using sessions.\n\nAs a beginning, we followed its own tutorials [2] and we used MNIST dataset to classify handwritten digits. We tried a network with 2 CONV layers and got about %99.02 accuracy with it. Tensorflow allows us to use high-leveled built in function and they made it easy us to handle back propagation and forward pass.\n\nOn our next blog, we will get some experiments on our dataset which is Food-101 and we will be talking about the results of these experiments.", 
        "title": "[Week3 \u2014 Eat&Count] \u2013 bbm406f16 \u2013"
    }, 
    {
        "url": "https://medium.com/@sexywee/yup-deep-work-is-important-1c2436974523?source=tag_archive---------3----------------", 
        "text": "Yup, Deep Work is important. I have read Dr Cal Newport\u2019s book\u200a\u2014\u200aDeep Work, and this article is a good quick 5 minutes summary of the Dr Newport\u2019s idea of isolating oneself from distraction to create high quality work.\n\nIf you\u2019ve watched his TED Talk video, he is also an advocate for anti-social media. In his book, he also mentioned how social media pulls us away from the optimal state of mind to produce deep work. He also seeked to prove that one would be able to survive without the use of social media. Facebook, Twitter, Youtube, Snapchat and many other social networking tools may provide you with transient entertainment but most of the times these platforms do not return knowledge or information that is worth your time. He did however say that if you are able to weigh why using these tools may give you more pros than cons, then you are justified to use them freely.\n\nOh well I can go on to elaborate more than his book but you did great on pointing out the useful pointers! Thanks for sharing and all the best!", 
        "title": "Yup, Deep Work is important. \u2013 Wong Seng Wee \u2013"
    }, 
    {
        "url": "https://medium.com/imagine-that-tech/learning-a-language-is-putting-ai-to-the-test-cb3cb7bb764c?source=tag_archive---------5----------------", 
        "text": "Artificial intelligence is taking our world by storm. Through deep learning and neural networks it is already capable of many things we, human beings can do. Securing companies from cyber attacks is one example. Writing simple sport recaps and reports is another example. Through natural language processing, companies like IBM, Google and others now try to master what we humans are so good at: learning languages.\n\nThink about the last conversation you had. Either face-to-face, through a phone call or a messaging app. What was it about? Who did you speak to? Did you think a lot about what you wanted to say? How and what you say probably took a couple of seconds, but didn\u2019t slow down the conversation. Using the right grammar didn\u2019t make you hesitate and the context of the conversation was clear from the start.\n\nSee what I did there? I made you think about a skill that is natural for us human beings: understanding and speaking languages. Language is a natural thing that we don\u2019t have to think about and that we can learn pretty easily. It is also a pretty big part of our culture. In fact \u2018culture\u2019 won\u2019t exist the way we know it without any form of language. And yet, it is one of the most ambiguous things in our society. Academics still haven\u2019t figured out the origin of languages and speaking languages mostly depends on context. This becomes very clear later in our lives. When we learn a second or third language.\n\nWant to learn Spanish? Go to a small village in the Andes and try to survive for a month without speaking a world of English. This is called \u2018immersion\u2019 and it is how we learnt our mother tongue as well. Our human brain can process and acquire the many rules by listening to other people speaking that language. But still, we can not explain why we use a certain word in a certain context. When we get older, learning these rules gets harder. Learning an extra language besides your mother tongue isn\u2019t that natural anymore and it demands many hours of studying to be able to order your meal in a restaurant during your holiday abroad. Choosing your words takes longer, it demands a lot of effort to understand people and you make mistakes, all the time.\n\nLearning a new language ourselves gives us a clear look in the struggle artificial intelligence has when trying to master languages and conversing with human beings. To understand this process, we, humans should, first of all, path ourselves on the back. We are the masters of language learning. Robots are even mimicking our brain trying to master languages. It is the last piece of land of human intelligence that artificial intelligence has yet to conquer. Winning the last battle won\u2019t be easy. It is no longer merely about data processing, it is about context and common sense.\n\nThe field of study that focuses on the interactions between human language and computers is called Natural Language Processing, or NLP for short. It sits at the intersection of computer science, artificial intelligence, and computational linguistics. Google is the leading force behind this field of study and made a lot of progress already (more on that in a bit). Other leading organisations are the Stanford NLP Group, the University of Toronto and the University of Oxford.\n\nLet\u2019s take a look at what this \u2018Natural language processing\u2019 is all about. Basically it can be divided into two components: natural language understanding (NLU) and natural language generation (NLG). For both components the input and output can be written text or speech. NLU processes the input, NLG is charged with producing phrases and sentences in the form of natural language. This is where the ambiguity of language comes into play. And that on many levels: on a word level (is \u2018fly\u2019 used as a noun or as a verb), on a syntax level (there are 100s of ways to interpret the same sentence) and on a \u2018reference\u2019 level (what is related to whom, in a conversation).\n\nInterested in the more technical stuff? Find more about it here\n\nGoogle was one of the first to take a big step forward in the processing of languages. It has developed world\u2019s most accurate parser (analysis of words in the sentence for grammar and arranging words in a manner that shows the relationship among the words) called SyntaxNet. This open source neural network framework is situated in the NLU part of natural language processing. Out of this framework, Google has developed a parser that can help you analyse the English language with 94% accuracy. Its name: Parsey McParseface. The most beautiful part is that Google has made this open source, kickstarting the many possibilities of language processing for developers (Think about text-based personal assistants in messaging app or Google\u2019s Assistant in the Pixel phone).\n\nAnother notable development in the field of natural language processing is probabilistic programming languages. Although described by Noah D. Goodman and Andreas Stuhlm\u00fcller as a computer language that helps at representing and using uncertain knowledge. The implementations of this form of programming languages mostly lie in the processing of language. A great example of PPL is the Webppl language, which is capable of sensing hyperboles or puns in a conversation. Indeed, one day robots will be better at sensing sarcasm better than Sheldon from The Big Bang Theory does.\n\nBack in 1966, MIT professor Joseph Weizenbaum developed a chatbot called ELIZA that acted like a cartoon psychotherapist. It repeated key parts of statements, encouraging its \u201cclient\u201d to continue the conversation. It was part of a project to make natural language conversation possible with a computer. AI avant la lettre, you could say, as ELIZA was not capable of learning any new scripts. Those must be added in real-time to be able to continue the conversation. Weizenbaum wrote down is findings in the book \u2018Computer Power and Human Reason: From Judgment to Calculation\u2019. In this book, He stressed the fact that a computer is not capable to grasp people\u2019s emotions (yet).\n\n\u201cThis is all very well, but what it\u2019s in for us?\u201d you might ask yourself. Natural language processing is a pretty complex technology about a pretty commonplace thing. That\u2019s why, you won\u2019t sense this type of technology until something goes wrong. Siri does not understand your question, or your Amazon Echo plays the wrong song. That\u2019s a difficult position to be in as an emerging field of study. But yet without us knowing it, it slowly is taking over small parts of our lives.\n\nInspired by the all-in-one-app approach of WeChat, Facebook is developing its \u2018M\u2019 personal assistant for Messenger. A bot will be capable of understanding what you are looking for and then (in the initial stage of the software) will send this demand to a group of Facebook employees that order your shoes, book a hotel or cancel your restaurant reservation.\n\nLast week The Next Web reported that AI will be concluding 60% of the customer service workload. How? Through a combination of data analytics and improving the work of customer service reps through AI (companies like Assist.ai and Digital Genius). But yet again, it looks like AI will be on the predictive part of the equation. Thus, this will result in better product development and more happy clients. Robots won\u2019t steal our jobs they will make them easier.\n\nBut basically, AI has implementations in every conversation based service where there are certain patterns and easy yes/no scripts. The only boundaries that is yet to be on the full force level are the specific aspects that make our way of conversation \u2018natural\u2019. The future and most of all the productive implementation of natural language processing software relies on those aspects. Basically what will happen in the next few years can be reduced to the question: if AI will be a gimmick or a productive tool that improves our lives on a day to day basis.\n\nSomething is heating up. Research in natural language processing is growing and receiving more funding. But it will take a while until we\u2019ll have a conservation with a robot the same way we talk to our friends and family. Do we really want do?\n\nOver to you now.\n\nWould it be a good thing that robots can understand and speak the same way we do?\n\nWould it improve our lives?\n\nAnd what are the consequences of this type of development?", 
        "title": "Learning a language is putting AI to the test \u2013 Imagine That Tech \u2013"
    }
]