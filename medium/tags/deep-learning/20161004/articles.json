[
    {
        "url": "https://medium.com/autonomous-agents/how-to-tame-the-valley-hessian-free-hacks-for-optimizing-large-neuralnetworks-5044c50f4b55?source=tag_archive---------0----------------", 
        "text": "Let\u2019s say you have the gift of flight (or you are riding a chopper). You are also a Spy (like in James Bond movies). You are given the topography of a long narrow valley as shown in the image and you are given a rendezvous point to meet a potential aide who has intelligence that is helpful for your objective. The only information you have about the rendezvous point is as follows:\n\nHow do you go about finding the lowest co-ordinate point? More so, how do you intend to find it in a stipulated time period?\n\nWell, for complex Neural Networks which has very large parameters, the error surface of the Neural Network is very similar to the long narrow valley of sorts. Finding a \u201cminima\u201d in the valley can be quite tricky when you have such pathological curvatures in your topography.\n\nIn the past posts, we used Gradient Descent algorithms while Back-propagating that helped us minimize the errors. You can find the techniques in the post titled \u201cBackpropagation\u200a\u2014\u200aHow Neural Networks Learn Complex Behaviors\u201d\n\nThere is nothing fundamentally wrong with a Gradient Descent algorithm [or Stochastic Gradient Descent (SGD) to be precise]. In fact we have proved that it is quite efficient for some of the Feed Forward examples we have used in the past. The problem of SGD arises when we have \u201cDeep\u201d Neural Networks which has more than one hidden layer. Especially when the Network is fairly large.\n\nHere are some illustrations of a non-monotonic error surface of a Deep Neural Network to get an idea.\n\nNote that there are many minima and maxima in the illustration. Let us quickly look at the weight update process in SGD\n\nThe problem with using SGD for the illustrations is as follows:\n\nWe need a better method to work with large or Deep Neural Networks.\n\nSGD is a first order optimization problem. First order methods are methods that have linear local curves. In that we assume that we can apply linear approximations to solve equations. Some examples of first-order methods are as follows:\n\nThere are methods called the second-order methods which considers the convexity or curvature of the equation and does quadratic approximations. Quadratic approximations is an extension of linear approximations but provide an additional variable to deal with which helps create a quadratic surface to deal with a point on the error surface.\n\nThe key difference between the first-order and second-order approximations is that, while the linear approximation provides a \u201cplane\u201d that is tangential to a point on a error surface, the second-order approximation provides a quadratic surface that hugs the curvature of the error surface.\n\nThe advantage of a second-order method is that, it shall not ignore the curvature of the error surface. Because of the fact that the curvature is being considered, second-order methods are considered to have better step-wise performance.\n\nThe following are some second-order methods\n\nLet\u2019s take a look at Newton\u2019s method which is a base method and is bit more intuitive compared to others.\n\nNewton\u2019s Method, also called Newton-Raphson Method is an iterative method approximation technique on the roots of a real valued function. This is one of the base method\u2019s used in any second-order convex optimization problems to approximate functions.\n\nLet\u2019s first look at Newton\u2019s method using first-derivate of a function.\n\nLet\u2019s say we have a function f(x) = 0, and we have some initial solution x_0 which we believe is sub-optimal. Then, Newton\u2019s method suggest us to do the following\n\nReally that simple. The caveat is that the method does not tell you when to stop so we add a 5th step as follows:\n\n5. If x_n (the current value of x) is equal to or lesser than a threshold then we stop.\n\nHere is the image that depicts the above:\n\nHere is an animation that shows the same:\n\nHere is the math for a function which is a first degree polynomial with one-dimension.\n\nNow, we can work on Newton approximation for a second degree polynomial (second-order optimizations) function with one-dimension (before we get to multiple dimensions). A second degree polynomial is quadratic in nature and would need a second-order derivative to work with. To work on the second-derivative of a function, let\u2019s use the Taylor approximation as follows:\n\nSuppose that we are working on a second degree polynomial with multiple dimensions, then we work with the same Newton\u2019s approach as we found above but replace the first-derivatives with a gradient and the second-derivatives with a Hessian as follows:\n\nA Hessian Matrix is square matrix of second-order partial derivatives of a scalar, which describes the local curvature of a multi-variable function.\n\nSpecifically in case of a Neural Network, the Hessian is a square matrix with the number of rows and columns equal to the total number of parameters in the Neural Network.\n\nThe Hessian for Neural Network looks as follows:\n\nNow, the second-order optimization using the Newton\u2019s method of iteratively finding the optimal \u2018x\u2019 is a clever hack for optimizing the error surface because, unlike SGD where you fit a plane at the point x_0 and then determine the step-wise jump, in second-order optimization, we find a tightly fitting quadratic curve at x_0 and directly find the minima of the curvature. This is supremely efficient and fast.\n\nBut\u00a0!!! Empirically though, can you now imagine computing a Hessian for a network with millions of parameter? Of course it gets very in-efficient as the amount of storage and computation required to calculate the Hessian is of quadratic order as well. So, though in theory, this is awesome, in practice it sucks.\n\nWe need a hack for the hack\u00a0! And the answer seems to lie in Conjugate Gradients.\n\nActually, there are several quadratic approximation methods for a convex function. But Conjugate Gradient Method works quite well for a symmetric matrix, which are positive-definite. In fact, Conjugate Gradients are meant to work with very-large, sparse systems.\n\nNote that a Hessian is symmetric around the diagonal, the parameters of a Neural Network are typically sparse, and the Hessian of a Neural Network is positive-definite (Meaning, it only has positive Eigen Values). Boy, are we in luck?\n\nThe easiest way to explain the Conjugate Gradient (CG) is as follows:\n\nYou can check most of the hairy-math around arriving at a CG equation by the paper cited above. I shall directly jump to the section of the algorithm of the conjugate gradient:\n\nFor solving a equation Ax=b, we can use the following algorithm:\n\nGiven that we know how to compute the Conjugate Gradient, let\u2019s look at the Hessian Free optimization technique.\n\nNow that we have understood the CG algorithm, let\u2019s look at the final clever hack that allows us to be free from the Hessian.\n\nHere we need to find the best delta_x and then move to x+delta_x and keep iterating until converge. In other words, the steps involved in Hessian-free optimization is as follows:\n\nThe crucial insight: Note that unlike in the Newton\u2019s method where a Hessian is needed to compute x_n+1, in Hessian-free algorithm we do not need the Hessian to compute x_n+1. Instead we are using the Conjugate Gradient.\n\nClever Hack: Since the Hessian is used along with a vector x_n, we just need an approximation of the Hessian along with the vector and we do NOT need the exact Hessian. The approximation of Hessian with a Vector is far faster than computing the Hessian itself. Check the following reasoning.\n\nTake a look at the Hessian again:\n\nHere, the i\u2019th row contains partial derivates of the form\n\nWhere \u2018i\u2019 is the row index and \u2018j\u2019 is the column index. Hence the dot product of a Hessian matrix and any vector:\n\nThe above gives the directional derivative of \u2018e\u2019 with respect to \u2018w\u2019 in the direction \u2018v\u2019.\n\nUsing finite differences, we can then optimize the above as following:\n\nWith this insight, we can completely skip the computation of a Hessian and just focus on the approximation of the Hessian to a vector multiplication, which tremendously reduces the computation and storage capacity.\n\nTo understand the impact of the optimization technique, check the following illustration.\n\nNote that with this approach, instead of bouncing off the side of the mountains like in SGD, you can actually move along the slope of the valley before you can find a minima in the curvature. This is quite effective for very large Neural Networks or Deep Neural Networks with million of parameters.\n\nApparently, It\u2019s not easy to be a Spy\u2026", 
        "title": "How to tame the valley \u2014 Hessian-free hacks for optimizing large #NeuralNetworks"
    }, 
    {
        "url": "https://medium.com/huggingface/launching-a-deep-learning-for-nlp-study-group-60ae8aca48ac?source=tag_archive---------1----------------", 
        "text": "The application of Deep learning to Natural langage processing, in particular to text understanding, is the next frontier of AI: deep learning is now proven and mature enough for other modes such as computer vision and speech recognition\u200a\u2014\u200afor text, it\u2019s just getting started.\n\nMy friend Thomas Wolf and I are starting a study group to collectively follow Richard Socher\u2019s CS224d class at Stanford (\u201cDeep learning for NLP\u201d), which was last offered in the spring quarter this year.\n\nThe goal is to assemble a group of 6 to 10 people who are passionate about the subject enough to commit to devote 6-8 hours per week over 4 months, to study the class material and programming assignments (see syllabus below)\u200a\u2014\u200athe idea being that you will study more efficiently as part of a group than you will by yourself.\n\nPrerequisites are: intro to machine learning (Coursera course or equivalent), fluency in Python, and 6\u20138 hours of available time per week.\n\nWe will work remotely, via a Facebook group, however we might have a few physical get-together meetups in Paris and/or NYC.\n\nWe also might have a few featured, recognized outside lecturers (teaser\u2026\ud83e\udd13).\n\nWe aim to stick rather closely to the CS224d syllabus, meaning that we will start with some theory (word vectors, neural nets, RNNs, LSTMs, CNNs) but also dive pretty deep into implementation. One of the goals is to build and ship public prototypes implemented in this field\u200a\u2014\u200ait\u2019s a great topic to ship PoCs and experiments on\u2013, so bonus points if you have an entrepreneurial spirit.\n\nIf you\u2019re interested or want to know more, send me an email\u200a\u2014\u200awe\u2019re going to have a great time!", 
        "title": "Launching a Deep learning for NLP study group \u2013 HuggingFace \u2013"
    }, 
    {
        "url": "https://medium.com/@Francesco_AI/what-you-are-too-afraid-to-ask-about-artificial-intelligence-part-ii-neuroscience-c34ea89add0c?source=tag_archive---------2----------------", 
        "text": "This is the second of three parts on recent developments in AI\n\nAlong with the advancements in pure machine learning research, we have done many steps ahead toward a greater comprehension of the brain mechanisms. Although much has still to be understood, we have nowadays a slightly better overview of the brain processes, and this might help to foster the development of an AGI.\n\nIt seems clear that try to fully mimic the human brain is not a feasible approach, and is not even the correct one. However, drawing inspiration from how the brain works is a completely different story, and the study of neuroscience could both stimulate the creation of new algorithms and architectures, as well as validate the use of current machine learning research toward a formation of an AGI.\n\nMore in detail, according to Numenta\u2019s researchers AI should be inspired to the human neocortex. Although a common theoretical cortical framework has not been fully accepted by the scientific community, according to Numenta a cortical theory should be able to explain:\n\ni) how layers of neurons can learn sequences;\n\nv) how brain regions model the world and create behaviors; and finally,\n\nvi) the hierarchy between different regions.\n\nThese can be seen then as the six principles any biological or artificial intelligence should possess to be defined as such. Intuitively, it sounds a reasonable model, because the neocortex learns from sensory data, and thus it creates a sensory-motor model of the world. Unfortunately, we do not fully comprehend how the neocortex works yet, and this demands a machine intelligence be created flexible as much as robust at the same time.\n\nIn a more recent work, Hawkins and Ahmad (2016) turned their attention on a neuroscientific problem who is though crucial to the development of an AGI. They tried to explain how neurons integrate inputs from thousands of synapses, and their consequent large-scale network behavior. Since it is not clear why neurons have active dendrites, almost every ANNs created so far do not use artificial dendrites at all, and this would suggest that something is probably missing in our artificial structures.\n\nTheir theory explains how networks of neurons work together, assumed all the many thousands of synapses presented in our brain. Given those excitatory neurons, they proposed a model for sequence memory that is a universal characteristic of the neocortical tissue, and that if correct would have a drastic impact on the way we design and implement artificial minds.\n\nRocki (2016) also highlighted few aspects specifically relevant for building a biologically inspired AI\u200a\u2014\u200aspecifically, the necessary components for creating a general-purpose learning algorithm. It is commonly assumed that humans do not learn in a supervised way, but they learn (unsupervised) to interpret the input from the environment, and they filter out as much data as possible without losing relevant information (Schmidhuber, 2015).\n\nSomehow, the human brain applies a sort of Pareto\u2019s rule (or a Minimum Description Length rule otherwise) to information it gathers through sensory representations, and keeps and stores only the information that can explain the most of what is happening. According to Rocki, unsupervised learning regularizes and compresses information making our brain a data compactor (Bengio et al., 2012; Hinton and Sejnowski, 1999).\n\nIn addition to being unsupervised, Rocki hypotheses that the architecture of a general-learning algorithm has to be compositional; sparse and distributed; objectiveless; and scalable. Human brain learns sequentially, starting from simpler patterns and breaking up more complex problems in terms of those simpler bricks it already understood\u200a\u2014\u200aand this type of hierarchy and compositional learning is indeed well captured by deep learning.\n\nAs already pointed out by Ahmad and Hawkins (2015), sparse distributed representations are essential, and they are much more noisy-resistant than their dense counterparts. However, there are much more peculiarities that make SDRs preferable: there are no region-specific algorithms in the brain, but the cortical columns act as independent feature detectors. Each column becomes active in response to a certain stimulus, and at the same time, it laterally inhibits other adjacent columns, forming thus sparse activity patterns. Since they are sparse, it is easier to reverse engineer a certain external signal and extract information from it (Cand\u00e8s et al., 2006). The property of being distributed helps instead in understanding the causes of patterns variations. SDRs also facilitates the process described above of filtering out useless information. They represent minimum entropy-codes (Barlow et al., 1989) that provide a generalized learning mechanism with simpler temporal dependencies.\n\nThe reason why the learning process should not have a clear stated objective is slightly controversial, but Rocki\u200a\u2014\u200aand Stanley and Lehman (2015) before him\u200a\u2014\u200asupport this argument as the only way to achieve and form transferrable concepts. Moreover, Rocki states scalability as fundamental for a general-learning architecture. The brain is inherently a parallel machine, and every region has both computational and storing tasks (and this is why GPUs are much more efficient than CPUs in deep learning). This would suggest an AI to have a hierarchical structure that separates local learning (parallel) from higher-order connections (synapses updates), as well as a memory that can itself compute, in order to reduce the energy cost of data transfers.\n\nRocki eventually concludes with some further functional rather than structural ingredients for the formation of an AI, namely: compression; prediction; understanding; sensorimotor; spatiotemporal invariance; context update; and pattern completion.\n\nWe discussed the importance of compression and sensorimotor before, and we can think of AGI as a general purpose compressor that forms stable representations of abstract concepts\u200a\u2014\u200aalthough this point is controversial according to the no free lunch theorem (Wolpert and Macready, 1997) that indirectly states that this algorithm cannot exist. We can also see prediction as of a weak form of spatiotemporal coherence of the world, and then we can argue learning to predict to be equivalent to understanding. Finally, we need to incorporate a continuous loop of bottom-up predictions and top-down contextualization to our learning process, and this contextual spatiotemporal concept would also allow for a disambiguation in the case of multiple (contrasting) predictions.\n\nAhmad, S., Hawkins, J. (2015). \u201cProperties of sparse distributed representations and their application to hierarchical temporal memory\u201d. arXiv preprint arXiv:1503.07469\n\nBengio, Y., Courville, A.C., Vincent, P. (2012). \u201cUnsupervised feature learning and deep learning: A review and new perspectives\u201d. CoRR abs/1206.5538.\n\nHawkins, J., and Ahmad, S. (2016). \u201cWhy Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex\u201d. Frontiers in Neural Circuits, 10.\n\nStanley, K.O., Lehman, J. (2015). Why Greatness Cannot Be Planned\u200a\u2014\u200aThe Myth of the Objective. Springer International Publishing.", 
        "title": "What you are too afraid to ask about Artificial Intelligence (Part II): Neuroscience"
    }, 
    {
        "url": "https://medium.com/emergent-future/emergent-future-weekly-historic-ai-partnership-mission-to-mars-chatbots-aws-p2-gpus-c70bc0d56baf?source=tag_archive---------3----------------", 
        "text": "You Might Have Heard: Amazon, Facebook, Google, IBM, and Microsoft have formed a partnership to advance the public understanding of artificial intelligence.\n\nThe five tech giants are tasked with establishing best practices for dealing with the challenges and opportunities that AI presents. They\u2019ve released eight guiding principles, which evoke Isaac Asimov\u2019s original \u201cThree Laws of Robotics.\u201d\n\nTechCrunch points out that \u201cData is becoming an important currency for the modern world. The data\u2019s value is rooted in its applications to artificial intelligence. Whichever company owns the data, effectively owns AI. Right now that means companies like Facebook, Amazon, Alphabet, IBM and Microsoft have a ton of power.\u201d\n\nThis is why it\u2019s important to democratize access to state-of-the-art algorithmic intelligence.\n\nBut, why form an AI coalition at all? tl;dr people are afraid of a robot takeover.\n\nOh, and Apple refused to join new AI club. Go figure.\n\nBut Did You Know? Microsoft formed a new 5,000-person AI division. The combined AI and Research group and will include both the Cortana and Bing teams.", 
        "title": "Emergent // Future Weekly: Historic AI Partnership, Mission to Mars, Chatbots, AWS P2 GPUs"
    }, 
    {
        "url": "https://medium.com/@BonsaiAI/welcome-to-bonsai-f57119b0df91?source=tag_archive---------4----------------", 
        "text": "As the newest member of Bonsai\u2019s Berkeley staff, I\u2019ve entered into this new and uncharted territory of developer tooling for artificial intelligence, and I\u2019m absolutely fascinated with what I (and the computers!) could learn. Coming to this from a developer tools background, I\u2019m eager to use the Bonsai platform to craft my own artificial intelligence models (we call them BRAINs, short for Basic Recurrent Artificially Intelligent Network) and add the wonder of AI to my own projects. Come along and we\u2019ll explore this brave new world of notional training using concepts, lessons, and curricula to break down the barriers to entry into machine learning! But first, Python. As a scientific and mathematical modeling language, we looked at Python as a natural choice for the first implementation of our tooling. The initial implementation of our simulator integration library is built with it (more languages coming soon), as well as our CLI and tools. The popularity of Python provides an easily comprehensible starting point and ensures there will be ample resources to draw upon in the greater community of developers. Fun fact: the entire Bons.ai service stack, both front and back end, are built on Python.\n\nBefore I got started with Inkling and the Bonsai CLI, I had to get my Python, pip, and virtualenv into shape. The general idea is that virtualenv allows me to isolate a specific Python environment (in our case the Python that I use with Bonsai) from the Python I use for everything else. To do this, it will be represented by a group of files that are created in the context it\u2019s called from containing only the packages I\u2019ve called for this virtual environment. Pip, in turn, will obtain and maintain your packages in this virtual environment. Full documentation can be found at the PyPA website, but here\u2019s some pro tips to make sure your virtual environment stays in good condition:\n\nArtificial intelligence and machine learning are complex and difficult to get started with, even for seasoned developers. The only way we\u2019ll realize our vision of AI for Everyone will be to break down these barriers to entry that prevent the world\u2019s developers from using and deploying modern artificially intelligent systems. We\u2019ve already seen this approach earlier in the history of computer science with the development of compiled and interpreted languages. To see how Bonsai leveraged the DQN algorithm against the Breakout Simulator problem published by University of Toronto in just a few dozen lines of code, check out our three-part blog post on the topic: Concepts and Schemas,Curricula and Lessons, and The BRAIN Server. The current state of AI and ML is similar to the earliest days of computing, when the only method of representing logic to a computer was through the use of assembly code or machine language. Over time, some enterprising developers wrote a compiled language to express this complexity in a (somewhat) human-readable fashion. While learning this new \u201cC\u201d language was a large commitment, it took the world by storm because it abstracted the details into a modular, interchangeable system. By now, C is one of the most widely used languages in the world, with assembly code relegated to #8. When taken together with its descendants C++ and C#, the C family is in the #1 spot. We can catalyze the same transformation in the AI space, and make AI approachable to many more developers, by creating the tooling and paradigms that will unlock the power of AI for everyone.\n\nIn the upcoming weeks, I\u2019ll be reporting on the people and ideas that drive our design, implementation, and architecture as we bring artificial intelligence and machine learning to the masses with Inkling as our language and compiler, trained BRAIN models, and approachable concepts using a modular approach. Next week, I\u2019ll chat with Megan Adams, our Senior Engineer writing the Inkling language specification and compiler, to get more details. She\u2019ll give us an insider\u2019s view of the challenges and motivations around building a special purpose language for AI.", 
        "title": "Welcome to Bonsai \u2013 Bonsai \u2013"
    }, 
    {
        "url": "https://medium.com/@simons.brenna/artificial-intelligence-terminology-you-need-to-know-86c4a0b13747?source=tag_archive---------5----------------", 
        "text": "Artificial intelligence is a large and varied field with many sub-fields and lots of confusing terminology. But as artificial intelligence becomes more mainstream and begins to have more of a role in our day-to-day lives, it\u2019s going to become more important to have a general understanding of it. Here are some AI terminology basics you need to know.\n\nArtificial intelligence is actually a sub-field of computer science tasked with creating computers that can perform tasks usually relegated to humans. Computers can do many things that humans can\u2019t but there are many things that humans can do that computers can\u2019t, and AI is about changing that.\n\nWeak AI refers to AI applications that perform a single task very well and there are already many weak AI programs out there. Strong AI is still science fiction at this point; it\u2019s an AI system that can do virtually anything that a human could do.\n\nThe term \u201cmachine learning\u201d is often uses interchangeably with artificial intelligence though it\u2019s actually a sub-field of AI. Machine learning is concerned with programming machines to learn how to perform a task themselves as opposed to having to be programmed to do that task.\n\nThis is another sub-field of AI though the term is sometimes used interchangeably. NLP a branch of AI that focuses solely on language whether written or spoken. A crucial obstacle that must be overcome in developing strong AI is the inability of computers to comprehend written and spoken speech.\n\nDeep learning is a sub-field of machine learning and is focused on programming machines to \u201cthink\u201d in a way similar to the human brain: that is on multiple levels simultaneously.\n\nArtificial Intelligence News brought to you by artificialbrilliance.com\n\nSource: forbes.com/sites/quora/2016/09/23/what-are-the-differences-between-ai-machine-learning-nlp-and-deep-learning/#759012a736f0", 
        "title": "Artificial Intelligence Terminology You Need to Know"
    }, 
    {
        "url": "https://medium.com/@ravigbo/intelig%C3%AAncia-artificial-na-medicina-b58aaacdeaf9?source=tag_archive---------6----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Intelig\u00eancia Artificial na Medicina \u2013 Ravi Gandhi Blumenthal de Oliveira \u2013"
    }
]