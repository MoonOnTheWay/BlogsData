[
    {
        "url": "https://medium.com/@Hisaaki/chainer%E3%81%A7word2vec%E3%82%92%E5%8B%95%E3%81%8B%E3%81%97%E3%81%A6%E7%B5%8C%E9%81%8E%E3%82%92%E8%A6%8B%E3%81%A6%E3%81%BF%E3%82%8B-62299105eb38?source=tag_archive---------0----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Chainer\u3067word2vec\u3092\u52d5\u304b\u3057\u3066\u7d4c\u904e\u3092\u898b\u3066\u307f\u308b \u2013 hisaaki \u2013"
    }, 
    {
        "url": "https://medium.com/@TianJianJiang/a-slanting-viewpoint-of-translation-in-acl-ijcnlp-2015-126233807789?source=tag_archive---------1----------------", 
        "text": "The annual meeting of the Association for Computational Linguistics, ACL in short, is one of the top conferences in natural language processing. IJCNLP, stands for International Joint Conference on Natural Language Processing, is one of the most important activities among Asian language technology research society, held by the Asian Federation of Natural Language Processing. They just had a joint conference along with the co-located event CoNLL, the SIGNLL (the ACL Special Interest Group on Natural Language Learning) Conference on Computational Natural Language Learning, on this July, and I quote, \u201cFor the first time, the annual meeting of the Association for Computational Linguistics (ACL) takes place in Mainland China,\u201d so it may justify my Asia-specific perspectives. Speaking of translation, it might be worth mentioning that ACL was originally named the Association for Machine Translation and Computational Linguistics (AMTCL) in 1962 and renamed six years later as now it is, so one may safely see (machine) translation as remaining an influential aspect of it.\n\nJudging by the number of machine translation papers, it may not seem likely at first glance. After all, only about 29 out of 329 papers in total. However, if only looking at the two best papers, the fact that one of them is titled as \u201cImproving Evaluation of Machine Translation Quality Estimation\u201d couldn\u2019t speak louder for the importance, and if looking at it closer with \u201cQuality Estimation,\u201d one may take it for something probably useful to the translation industry, which is one topic this article would like to address soon. Before that, let\u2019s take a step back to have an outlook.\n\nYou may be already aware of the recent trend of Deep Learning and predicting that it also buzzes in ACL-IJCNLP 2015. You are right. The first session is exactly about machine translation based on neural networks. Despite neural networks being studied for almost a decade, Deep Learning based on convolutional neural network or recurrent neural network is relatively immature, not to mention the systems have adapted it, such as neural machine translation. While it\u2019s refreshing to see a new paradigm other than phrase-based machine translation emerging, researchers have encountered limitations. A major one is that neural machine translation has to trade the complexity with the size of target vocabulary. If ever played with the demo mentioned in my last article, http://lisa.iro.umontreal.ca/mt-demo, one may have noticed that it worked well with sentences but didn\u2019t work at all with single words (Figure 1). So the authors of the above system, and other pioneers, have been pushing the frontier a bit further \u201cOn Using Very Large Target Vocabulary for Neural Machine Translation\u201d (Figure 2) and \u201cAddressing the Rare Word Problem in Neural Machine Translation.\u201d Give it time, neural machine translation may have a breakthrough, especially when some of these researchers are working with BOLT (DARPA, IBM), Matecat (EU), Cosmat (SYSTRAN, INRIA), etc. J\n\nAs for the outlook, it\u2019s uneasy to let so many other interesting papers slip away, but in order to keep this article compact, I\u2019m afraid that the best I can offer for the time being is to highlight some keywords from titles: \u201cStatistical Machine Translation Features with Multitask Tensor Networks,\u201d \u201cSyntax-based Simultaneous Translation through Prediction of Unseen Syntactic Constituents,\u201d \u201cEfficient Top-Down BTG Parsing for Machine Translation Preordering,\u201d \u201cOnline Multitask Learning for Machine Translation Quality Estimation,\u201d \u201cA Context-Aware Topic Model for Statistical Machine Translation,\u201d \u201cEvaluating Machine Translation Systems with Second Language Proficiency Tests,\u201d \u201cRepresentation Based Translation Evaluation Metrics,\u201d \u201cExploring the Planet of the APEs: a Comparative Study of State-of-the-art Methods for MT Automatic Post-Editing,\u201d \u201cMT Quality Estimation for Computer-assisted Translation: Does it Really Help?\u201d \u201cContext-Dependent Translation Selection Using Convolutional Neural Network,\u201d \u201cWhat\u2019s in a Domain? Analyzing Genre and Topic Differences in Statistical Machine Translation,\u201d \u201cimproving pivot translation by remembering the pivot,\u201d etc., in hope for the readers to find insight by and for themselves.\n\nSince the key phrase \u201cquality estimation\u201d popped up frequently, it\u2019s about time to define it. According to the Workshop on Statistical Machine Translation (WMT), it\u2019s \u201cfor estimating (predicting) the quality (based on several quantities in terms of post-editing) of machine translation output at run-time, without relying on reference translations.\u201d (Words in parentheses are mine.) The two critical conditions here are \u201cno reference translations\u201d and \u201cin terms of post-editing.\u201d Unlike typical automatic evaluation metrics of machine translation, quality estimation works like another predictive modeling (or data science, if one prefers): whenever a machine translation output produced, how confident are we do recommend it to the post-editors, or even the customers? Thanks to years of WMT shared tasks, we have just about sufficient data to observe and learn. For example, what would the correlation between machine translation quality and post-editing time be? The readers shall be able to find all the criteria on http://www.statmt.org/wmt14/quality-estimation-task.html. Imagine that there will be an oracle identifying the best machine translation result for you, so the burden and frustration to post-editors and customers can be minimize. Too good to be true? Here\u2019s where the best paper \u201cImproving Evaluation of Machine Translation Quality Estimation\u201d kicks in. As its introduction said, \u201cthis also produces conflicting results and raises the question which method of evaluation really identifies the system(s) or method(s) that best predicts translation quality.\u201d WMT-14 even faced a serious issue that English-to-Spanish has so many tied matches resulted in 22 winning systems. Feels like being back to square one. Fortunately, the paper figured out that the previous evaluation metrics are somewhat flawed, and pointed out a more appropriate choice should be the Pearson correlation. Since there are so many lessons can be learnt from the paper, the readers are encouraged to read it for themselves. This article will just provide a small (crazy?) idea as the final words.\n\nAn estimation being not relying on reference translations doesn\u2019t have to limit itself to predict the quality of machine translation. What if the target is crowd-translation? For example, Gengo has been doing quality assurance for a while and gathered some (open) data, yet it is probably not convincing to rest the customer assured. Could it be more trustworthy by taking further steps to apply quality estimation on crowd-translations and then notify proofreaders when the prediction raises the red flag? I couldn\u2019t speak for the customers, but I know I\u2019m sold.", 
        "title": "A slanting viewpoint of translation in ACL-IJCNLP 2015"
    }
]