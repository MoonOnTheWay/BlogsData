[
    {
        "url": "https://medium.com/@awjuliani/visualizing-neural-network-layer-activation-tensorflow-tutorial-d45f8bf7bbc4?source=tag_archive---------0----------------", 
        "text": "I am back with another deep learning tutorial. Last time I showed how to visualize the representation a network learns of a dataset in a 2D or 3D space using t-SNE. In this tutorial I show how to easily visualize activation of each convolutional network layer in a 2D grid. The intuition behind this is simple: once you have trained a neural network, and it performs well on the task, you as the data scientist want to understand what exactly the network is doing when given any specific input. Or, in the case of visual tasks, what the network is seeing in each image allows it to perform the task so well. This technique can be used to determine what kinds of features a convolutional network learns at each layer of the network. The technique I describe here is taken from this paper by Yosinski and colleagues, but is adapted to Tensorflow.\n\nLike my other tutorials, all code is written in Python, and we use Tensorflow to build and visualize the model. Hopefully it is helpful!", 
        "title": "Visualizing Neural Network Layer Activation (Tensorflow Tutorial)"
    }, 
    {
        "url": "https://medium.com/@jdwittenauer/assignments-from-google-s-deep-learning-class-posted-6ad72f8fbcd9?source=tag_archive---------1----------------", 
        "text": "A quick note for anyone interested in deep learning\u200a\u2014\u200aI\u2019ve been working through Google\u2019s deep learning class on Udacity and posted (most of) the assignments on my IPython Github repo. There are links in the README to rendered versions of the notebooks. I said I posted \u201cmost of\u201d the assignments because I haven\u2019t finished the last one yet, and it may be a while before I circle back to it. It turns out that implementing recurrent nets and sequence-to-sequence models from scratch using low level primitives, with no prior experience or theoretical foundation, is pretty fucking hard =) I fully intend to work on it, I just need to spend a lot more time studying and playing around with the code.\n\nI\u2019m planning to do a future blog post where I\u2019ll go more in depth on this class, and deep learning in general, but for now I\u2019ll just offer a few initial impressions of the class in case anyone else is planning to give it a try.\n\nThe video content, while informative, is very light on substance. After doing a number of MOOCs on Coursera, I was a bit disappointed by the lack of depth in Udacity\u2019s course format. I get that a lot of the Coursera content is developed by universities and adapted from real semester-long courses, and by contrast, many Udacity courses (such as Google\u2019s course) are developed by companies with a somewhat limited investment in the curriculum. But there\u2019s a pretty stark difference between the depth of a class like Andrew Ng\u2019s machine learning class and this one.\n\nAs a direct consequence of this, don\u2019t expect to learn everything you need to complete the assignments from watching the videos. You really need to be willing to go out and read research papers on relevant topics to meaningfully progress on the labs. If that\u2019s not your thing then you can just watch the videos and skip the assignments but all of the \u201cmeat\u201d of the class is in the notebook code posted with the labs. You\u2019re expected to be reasonably comfortable building fairly complex computation graphs in Tensorflow (or at least comfortable enough to learn as you go). The course itself doesn\u2019t teach you any of this so you\u2019ll have to seek that knowledge out yourself.\n\nIf you can get past those hurdles, there\u2019s quite a lot of value in the code that Google provides with the labs. I found it to be fairly complex and hard to follow at times, but if you\u2019re willing to invest the time there\u2019s a lot to learn. It\u2019s amazing how much effort can go into just pre-processing, formatting, and batching data to get it ready for input to a neural net, and there are some great of tips and tricks to learn from the code. Likewise, the example code in Tensorflow is very helpful and sets a good foundation to build on with the labs.\n\nOverall I enjoyed the class but wouldn\u2019t recommend it as a first step into deep learning. It seems much better suited for individuals with prior conceptual knowledge of the subjects that are looking to develop hands-on experience. If you\u2019re totally unfamilar with deep learning and just looking to get started, I\u2019d recommend some of the free online books linked in Awesome Deep Learning as a starting point.", 
        "title": "Assignments From Google\u2019s Deep Learning Class \u2013 John Wittenauer \u2013"
    }, 
    {
        "url": "https://medium.com/@mitchdover/do-computers-still-need-us-7dad090ca3c2?source=tag_archive---------3----------------", 
        "text": "Do computers still need us?\n\nOne of the most exciting things going on in the world of tech as we approach 2016 is the advancement of artificial intelligence technology. One of the biggest steps taken in the industry is the improvement of so-called \u201ccognitive computing.\u201d Cognitive computing is when a computer is capable of interpreting human meaning out of questions spoken in natural language. In other words, it doesn\u2019t need to be given specified commands. They aren\u2019t programmed to give a set response to a predetermined question posed in a specific way. They \u201chear\u201d human speech, interpret what is being asked or said and determine an appropriate response.\n\nDoes that make us obsolete?\n\nAs with any major scientific advancement, the idea of artificial intelligence is scary to a lot of people. One of the fears, and our literature and movies often depict this fear, is that artificial intelligence could grow smart enough to rise up and overthrow humanity. That level of artificial intelligence is probably very far away still.\n\nA more legitimate fear, and a fear that could be realized very soon, is the idea that artificial intelligence can grow smart enough that it no longer needs us. If we create something autonomous, it can take our jobs, do them better than we can, and make us obsolete. For some, that may be a fate worse than death at the hands of a robot apocalypse.\n\nAccording to developers, the answer is \u201cno\u201d\n\nIn a recent survey of 529 artificial intelligence developers, 47% said that machine learning software still requires human input some of the time. Just 2.6% reported that human input wasn\u2019t required at all. That means that approximately 97.4% of the most advanced computers in the world still need the human touch.\n\nYou can expect the cognitive computing industry to continue to grow in the coming years and expect artificial intelligence to become even more advanced. But according to the leading researchers in the field who understand the concept of artificial intelligence more than anyone else, even as these computers and programs are designed and released into the world, they will still need a team of humans to keep them working properly. It seems that humans don\u2019t need to worry about losing their jobs to artificial intelligence anytime soon.\n\nBut now that computers can interpret and respond to questions, it\u2019d be interesting to see how they\u2019d answer the question: \u201cDo you still need us?\u201d\n\nArtificial Intelligence News brought to you by artificialbrilliance\u00a0.com", 
        "title": "Do computers still need us? \u2013 Mitchelle Dover \u2013"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/a-2-billion-chip-to-accelerate-artificial-intelligence-646bdb33f714?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "A $2 Billion Chip to Accelerate Artificial Intelligence"
    }
]