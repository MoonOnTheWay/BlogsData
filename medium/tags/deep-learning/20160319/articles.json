[
    {
        "url": "https://medium.com/@AskSensai/is-ai-being-oversold-e6351650ae2c?source=tag_archive---------0----------------", 
        "text": "By Monica Anderson, Co-founder and CTO of Sensai.\n\nA headline in a Scientific American article of March 2016 asks that question.\n\nIt was oversold in the past. It\u2019s easy to see why\u200a\u2014\u200athe potential gains were (and still are) enormous and any indication you were on the right track meant people would throw money at you. And if you then couldn\u2019t deliver anything monetizable, the money people would lose faith in AI. This lead to not one but two separate \u201cAI Winters\u201d where AI funding dried up.\n\nBut this time we\u2019re over the hump. Deep Learning (DL) is a new machine learning technology based on Neural Networks that has achieved near-human performance in a half-dozen problem domains such as signal processing, low-level speech understanding, image understanding, text understanding, Atari video games, and Go. Google\u2019s Go playing machine spectacularly beat Korean Go champion Lee Se-Dol by 4\u20131 in a recent match televised worldwide.\n\nDL is not yet worthy of the name \u201cAI\u201d but it is the only thing we have that is on the right track. And it actually works much better than anything we have ever tried in several \u201cAI-level\u201d problem domains. Companies are also noticing that \u201chuman level\u201d AI is not necessary in order to disrupt a market and make a huge profit. \u201cCattle-level Understanding Machines\u201d that Understand things like human languages but don\u2019t yet know how to Reason are on track to open dozens of billion-dollar markets. Understanding Machines are like a simpler, more harmless kind of AI, suitable for AI beginners like humans. The horror stories about AIs getting super-intelligent overnight and taking over the world are pure fiction based on old AI theories that we believed in until people like Geoff Hinton made DL work 2006\u20132012. Intelligence is much simpler than we ever expected, and the algorithms are so simple and the code required to implement them so small that there will soon not be much room for algorithm improvement. At that point, AIs will continue to improve almost entirely by learning more and by running on better hardware. But this will be a slow and controlled improvement process.\n\nFrom now until then we can devise incremental improvements to our algorithms and we can reliably measure our progress. The potential gains are as large as ever, and large companies like Google with hundreds of data scientists on their payroll are using these new methods everywhere. Others have taken notice and are scrambling to get on that bandwagon. They are trying to hire data scientists and are buying Deep Learning hardware\u2026 both of which are in very short supply in Silicon Valley. NVIDIA makes the vital GPU boards and is betting heavily on this market. Their top-of-the-line graphics board for gamers used to cost $700. They added some extra memory to make it possible to run deeper neural networks and now charge $4000. They have much stronger boards in the pipeline. Gamers may well become the minority customer as Google and other companies are buying boards by the truckload for their server farms.\n\nI\u2019m expecting over a thousand DL based apps (for phones, laptops, as web apps, and as cloud components) to appear in the next four years. We could discuss general application domains ripe for a Deep Learning takeover but generalities are hard to understand, and the full range of AI level problem domains which could be attacked using these methods is unknown.\n\nSo instead I\u2019d like to mention a few speculations about my own favorite applications from various domains, as more concrete examples, just off the top of my head. These are just from the domains where we already have demonstrated DL to work, so these kinds of applications are virtually guaranteed to work and to be released in the near to medium future.\n\nDeep Speech 2 (from Baidu that converts spoken Mandarin or English into text) indicates voice input will become very common, especially for mobile use and especially in Asia, where large character sets make typing difficult. This will quickly lead to a world much like the one depicted in Spike Jonze\u2019s movie \u201cHer\u201d where computers had displays but no keyboards. You will be able to speak to your cellphone all day long and it will talk back and make sense. In fact, if you are using Android phones then you have already been using DL based voice understanding since 2012. These are existing and real products.\n\nMachine translation will achieve high levels of perfection, allowing for close captioning of all live TV\u200a\u2014\u200asuch as news\u200a\u2014\u200ain real time, from any language to any language, and will provide full access to the entire internet in any language you want. I can\u2019t wait for the impact on the Arabic speaking countries.\n\nThe music makers of the world will get tools like perfect recording-to-midi-to-sheet-music analysis tools, allowing unprecedented precision and flexibility for mashups, music-minus-one style practice software, music teaching software that listens to you play or sing, and re-synthesis mashups which will allow Beyonce to sing a duet with Caruso. You let the system analyze your favorite symphony and you can get individual MIDI files for each kind of instrument. To see how far we have gotten, google for \u201cDeep Karaoke\u201d.\n\nVideo games will have NPCs (computer-controlled non-player characters like shop merchants) that actually understand human languages and produce speech that makes sense. They will move and fight much more gracefully, as if they understood what they were doing in the context of the game. And one day, they will.\n\nMicrophones used on video cameras will be able to record conversations from across the room and remove room acoustics artifacts like reverberation and noises. Classical recordings will be restored to noise-free quality and harmonics outside of original recording frequency range (live violins sound much better than recorded ones) can be synthesized. Movie special effects will be taken to new levels. Humphrey Bogart will appear in new movies. Space operas will correctly depict weightlessness. Spam filters and email programs will classify your email perfectly. You will be able to sort all files on your computer by topic and summarize any subset of documents into a few paragraphs.\n\nLegal search and legal discovery becomes cheap. Take the example of canvassing a company\u2019s email archive looking for evidence of illegal activities that may jeopardize the company. If there is one single email among billions that has \u201cillegal\u201d content, a classifier based on full language understanding will find it. Paralegal-in-a-box will become available and we may see a Lawyer-in-a-box that has passed the bar. Open source movement will kill any effort to make, say, lawyer-in-a-box charge as much as lawyers normally do.\n\nFurther down the road\u2026 Doctor-in-a-box, accountant-in-a-box, and you-name-it-profession-in-a-box will be available to download or buy for your laptop or use or rent in the cloud. Again, open source efforts may be able to gather the materials to use for training.\n\nProgramming itself transforms into \u201cCurriculum Selection\u201d or \u201cCorpus Curation\u201d. Excellent explainers (teachers, authors, even parents experienced with home schooling) will be in high demand; possibly more in demand than programmers. Once you have an AI, you don\u2019t program it. You ask it to do something and it will know what you mean. Misunderstandings\u200a\u2014\u200aa staple of genie-in-the-bottle fiction\u200a\u2014\u200awon\u2019t be more common than when dealing with human contractors.\n\nAny of these products will disrupt their respective markets. And there will be thousands of them. A handful of retired lawyers and a teenager with some computer skills will be able to produce a decent lawyer-in-a-box in a couple years once the learning tools become simple enough to use.\n\nIn DL, all tasks are learned from examples and many won\u2019t require much additional world knowledge beyond a training corpus of text. Some do. The difference will basically be a matter of degree in the ability of the systems to truly understand their problem domains. And this is largely a question of available training materials\u2026 and we\u2019ll be amazed at what sources they will be able to learn from.\n\nThese systems may start out by providing new capabilities that are controlled 100% by humans but will (with subsequent software upgrades) start taking on more and more of the human\u2019s responsibilities. We will get used to living with Artificial Intelligences through a kind of \u201cGentle Seduction\u201d. There will be changes, but we\u2019ll have a decade to get used to them, as individuals, as a society, and as a species.\n\nAnd them to us.", 
        "title": "Is AI being oversold? \u2013 Sensai \u2013"
    }, 
    {
        "url": "https://medium.com/@damian.wilbur/this-serves-as-a-reminder-that-we-as-technologists-should-approach-interfaces-not-purely-from-a-d69c02d709f3?source=tag_archive---------1----------------", 
        "text": "This serves as a reminder that we as technologists should approach AI interfaces not purely from a probabilistic vantage point, but also from a humanist\u2019s perspective. Bringing more voices and perspectives into the development process would go a long way to building sensitivity into our AI from the outset.\n\nDeep learning-backed approaches such as Siri are more than up to handling the task you describe. What you are identifying here are precisely the set of concerns that foster a deeper, more general suspicion of AI.\n\nIt seems like a fundamental tenet, but sensitivity should be a prominent feature in any AI purposed to act in the interest of human needs.\n\nSensitivity, not in the way the learning model responds to input, but sensitivity in the way that the fruits of the endeavor are conveyed to a human being.", 
        "title": "This serves as a reminder that we as technologists should approach AI interfaces not purely from a\u2026"
    }
]