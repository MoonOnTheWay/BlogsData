[
    {
        "url": "https://medium.com/@the1ju/simple-logistic-regression-using-keras-249e0cc9a970?source=tag_archive---------0----------------", 
        "text": "This post basically takes the tutorial on Classifying MNIST digits using Logistic Regression which is primarily written for Theano and attempts to port it to Keras.\n\nThis is what the official Keras site says.\n\nSo, what better way to put that claim to the test than to write some code!\n\nKeras comes with great documentation. One can really get up and running in a matter of minutes. Everything needed to accomplish the goal can be found on the Guide to Sequential Model page (assuming of course the initial setup and configuration is all taken care of).\n\nKeras also offers a collection of datasets that can be used to train and test the model. The MNIST set is a part of the available datasets and can be loaded as shown below.\n\nAnd that\u2019s it! Full source code available here", 
        "title": "Simple Logistic Regression using Keras \u2013 veej.alur \u2013"
    }, 
    {
        "url": "https://medium.com/@fendicase/want-to-know-deep-learning-of-convolution-on-the-gpu-to-optimize-it-big-god-134b39190b47?source=tag_archive---------2----------------", 
        "text": "Want to know deep learning of convolution on the GPU to optimize it Big God\n\nLei feng\u2019s network (search for \u201cLei feng\u2019s network\u201d, public interest) by: this article is reprinted from the NVIDIA NVIDIA business solutions, public. Zhao Kaiyong, Department of computer science, Hong Kong Baptist University PhD Candidate heterogeneous computing laboratory, have long been engaged in research on high performance computing, CPU, GPU heterogeneous computing has many years of research experience. Mr Zhao Kaiyong organization is involved in a number of research units and users of high performance high performance projects, served as a wave of GPU Computing consultant, has repeatedly served as NVidia CUDA judges in China. He has also published the GPU CUDA in high performance computing, second edition of the translation of the actual programming massively parallel processors. One of the earliest extension of GPU computing research. VR Shinecon headset\n\nMentioned areas of artificial intelligence, now one of the hottest words is the Deep depth learning Learning (hereinafter DL). In recent years, the depth of learning became absolute in academic circles and the industrial areas of Visual Computing mainstream. In addition to traditional computer rendering, professional, medical, life sciences, energy, financial services, automotive, manufacturing and entertainment industries have to deep learning, application and technological optimization, avoiding defeat in the competition. In technology to compete in the crowded, GPU plays an essential role, associated with optimization is also the focus of attention of researchers.\n\n\u201cDeep learning technology can be used to solve real problems, rather than remain in Demo presentation stage\u201d\n\nAs Microsoft\u2019s voice and Google cat recognition and face recognition to the recent fire, and automatic driving and so on, these are the typical application of deep learning. Alibaba, Baidu, Facebook, and Google, IBM and other large companies in the DL has a great deal of input. Here it must be emphasized that a sentence that many mainstream DL framework and algorithms were developed by Chinese-dominated, DL recovery without Chinese researchers.\n\n\u201cBefore the in-depth training you need to consider two things\u201d\n\nA software framework, such as Caffe,Tensorflow,Mxnet, and so on.\n\nAnother is the hardware. Hardware, there are several isomeric forms, CPU,FPGA,DSP, and so on, but most mainstream GPU, is that really in DL fast combat CUDA hardware (NVIDIA GPU) +CUDA DL learning software (cuDNN), which is NVIDIA the result of years of development and cultivation.\n\nNow mainstream DL development training platform with a NVIDIA video card like NVIDIA TITAN series is a very good tool. In order to accelerate the pace of training, general training in high performance computer equipped with multiple GPU or cluster above, trained transplant network is also very easy to use NVIDIA Tegra processor embedded platform such as NVIDIA Jetson TX1, they have the same structure, so transplants are going to be very convenient.\n\nJetson TX1 NVIDIA Tegra-based X1 processor to create, using identical Maxwell and super computer core GPU architecture 256, can provide up to 1T-Flops of computing performance and full support for CUDA (Compute Unified Device Architecture) technology to meet the development tools preinstalled, very suitable for deep learning-based intelligent embedded devices to create. Shortly before the Jetson users equipped with Tegra processor platform, test gardens if cats in.\n\nTest videos of kittens into the garden through a network, this application using Convolutional neural networks (Convolutional Neural Network, called CNN) categories, on the desktop or on a cluster, and then porting (transplant) on the Tegra, played a key role in CNN algorithm. CNN is the most critical part of convolution. Image recognition and image classification most CNN will work in the areas, the key is the convolution. It evolved from two aspects, one network for voice processing delay time, a feature extraction algorithm of image processing. For the latter, convolution filter is the image, simply put, is to do some feature extraction. Common Sobel edge detection, and hog, Gaussian filter, and so on, these are two-dimensional convolution.\n\nThough now you do convolution is square, but in fact this is defined, you can not follow this standard, can be replaced with a different shape of convolution, to better adapt to your way of operation, especially when the convolution kernel is larger, this is also a way of optimization to the convolution. In General, the popular CNN networks, convolution part takes more than 70% of computing time, and optimize convolution part is necessary. You need from algorithms, parallel perspectives, and GPU hardware features, and many other considerations. GPU is a programmable parallel computing architecture, it has a lot of very good algorithms, but NVIDIA also provides a tool, that helps you to optimize.\n\n7. the parallel property for hardware, more efficient use of the network and hairstyles\n\nMemory that is used to change the time\n\nIf depth learning DL in the each layer of volume product are is for same Zhang pictures, so all of volume product nuclear can with on this Zhang pictures for volume product operation, then again respectively storage to different of location, this on can increased memory of using rate, once loaded pictures, produced times of data, and not need times access pictures, this is with memory to for time.\n\nConvolution is the multiplication of a small area, and then add it in the field of parallel computing is very mature.\n\nDiagram above, for example, the left is a picture on the right is the convolution kernel. We can expand into a convolution kernel and multiple convolution kernel can be arranged into multiple lines, then the image is expanded in a similar way, you can transform a convolution into multiplication problems. This is multiplied by a column of a row, is one result. So while doing some operations, but for calculated speed will improve a lot.\n\nUnderstanding as well as IO IO access performance;\n\nTime overlap between the IO and parallel computing\n\nFor NVIDIA\u2019s GPU, memory access is some characteristics of continuous merging access bandwidth can make good use of the hardware. As you can see, the GPU NVIDIA latest schema, which may not have a significant increase in the number, the architecture doesn\u2019t seem to have changed much, but in the middle of several computing stream processors increase cache improves the performance of large, for IO access this piece is a big optimization.\n\nAbove is a classic memory and threading models, shared memory and registers are the fastest access speed of memory, memory access compared with calculated, is too slow, so try to put more data into high-speed cache.\n\nTo this figure, for example, when we start from the c matrix results, each c need a row and a column b to calculate, using the GPU features, we can store the result of zero in the registers, then we can divide the 64\u00d72 thread, to calculate the thread.\n\nIn the photocopying section c, so multiple threads can have 64\u00d72 on one of my visits, you can storage 64\u00d72 data. You can make each thread 64\u00d72 thread is stored 16 or 32 data, then, we can use 64x2x16 64\u00d72 thread storage (32) data. So more data can be stored in the memory, when multiple read and write, and speed quickly. Meanwhile, we consider access to the matrices a and b, the corresponding data matrix b, put a lot of shared memory, and thus improve the commonality of shared memory. In this way, entire AxB according to these threads can read globalmemory (a matrix), you can merge access, you can follow each row of 32, 32 reads, you can speed up the merger visit =C. This optimization of the entire matrix sorted out.\n\nAbove is a response to deep learning convolution in the GPU, and Jetson TX1 platform some optimization ideas.", 
        "title": "Want to know deep learning of convolution on the GPU to optimize it Big God"
    }
]