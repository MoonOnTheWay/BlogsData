[
    {
        "url": "https://medium.com/intuitionmachine/nips-2016-cake-rocket-ai-gans-and-the-style-transfer-debate-708c46438053?source=tag_archive---------0----------------", 
        "text": "Or, if you put them all together, my experience of the NIPS conference can be summarized as the image below. Read along if you prefer words, details and creative applications of AI. Thanks Prisma for the style transfer and my stepdad for the somewhat GANy\u00a0cake Mentioned in Yann LeCun\u2019s invited talk on Monday, the cake became the meme of NIPS 2016, appearing in presentations throughout the conference and workshops. Turns out, machine learning researchers must really like cake, sometimes with extra cherries (oh RL!).\n\nThe real cherry on the NIPS cake was of course the launch of Rocket AI. I was there, still not sure what they do, but impressive team, private residence in the embassy district and an on-call police force for when the parties get too rowdy (the team need their beauty sleep!). Yup, next year I\u2019m betting on Temporally Recurrent Optimal Learning becoming the new GAN. And now more seriously\u2026 This was my first NIPS and it was probably the best conference I had ever been to. So much inspiration, energy and positivity from the AI community amongst the chaotic frenzy of invited talks from industry titans, hundreds of posters, artistic demos, company parties, workshops and satellite events for the almost 6,000 machine learning researchers assembled from all over the world. Read on for my NIPS week highlights. Organised by Xavier Serra at Universitat Pompeu Fabra on the Sunday before NIPS, the seminar was a great music-focussed start to the week. I only managed to catch the final few presentations, but it was a delight to hear what was happening industryside with presentations from Aaron van den Oord on the Wavenet audio generation model, Oriel Nieto on music recommendation models at Pandora and Colin Raffel on using the Lakh MIDI dataset. Lines, grids and reflections at the CCIB conference venue In its 11th year, the WIML workshop welcomed 570 participants to its event on Monday, featuring invited speakers, contributed talks and mentorship roundtables on academic and career topics. I was invited to mentor on music applications and building your professional brand; we had some fascinating discussions on separating professional and personal lives online as well as figuring out what our dream applications of AI in music would be (music tutors, music-to-image and improved recommendation systems were all mentioned). Despite the fact that WIML numbers doubled from the previous year, women still made up only 15% of the almost 6,000 attendees. Judging by the number of women-specific sponsor events hosted on Sunday before the workshop, it is clear that companies are trying to address the gender balance. I do wonder if they\u2019re going about it in the right way, but that is a topic for another article. Real life sea demo found just outside the conference centre Monday, Tuesday and Wednesday evenings are reserved for posters with 200 on display every day and only 3.5 hours to see them all. Alongside the posters, there were 10 demos each on Tuesday and Wednesday allowing you to play music, figure out your personality, generate text interactively and turn frowns into smiles. They brought the research to life and were great fun to play around with. Some favourites are listed below. Memo Akten\u2019s Real-time interactive sequence generation and control with Recurrent Neural Network ensembles: a system to gesturally \u2018conduct\u2019 the generation of text. What was particularly entertaining here is watching the generated text change as you moved your hand across the screen to increase the presence of the style of the Bible, Trump or the Linux source code. Tom White\u2019s Neural Puppet: How can we understand and use the structured latent space of generative networks? This uses a neural network to remove or add smiles to a photograph.\n\nMagenta, the music generation project from Google Brain, Interactive Musical Improvisation with Magenta. The short video is of Magenta playing the bassline with Sageev Oore playing over it. Winner of the best demo award. Anh Nguyen et al\u2019s Plug & Play Generative Networks: Conditional Iterative Generation of Images in Latent Space. Like a never-ending film\u00a0, generating image after image in response to a word prompt. Generative Adversarial Networks (GANs) were one of the hottest trends this year, with a dedicated tutorial from Ian Goodfellow on Monday, a workshop on Adversarial Training on Friday and pop-up appearances pretty much anywhere and everywhere else. I\u2019m still in awe of the artistic potential of these models. Below are some slides from Ian Goodfellow\u2019s tutorial, with some results described as \u201cproblems\u201d because of incorrect perspective, global structure, and counting. What is a bug in the quest for realistic image generation can be a feature in creative explorations. With the amount of interest in the field, I can\u2019t wait to see how the technical models will develop and how they will be used creatively. Get in touch if you spot some cool creative uses of GANs.\n\nDuring Friday lunch, I was pleasantly surprised to find a workshop on public engagement. Titled \u2018 People and machines: Public views on machine learning, and what this means for machine learning researchers\u2019, it featured presentations from Sabine Hauert, Zoubin Ghahramani and Katherine Gorman. Sabine Hauert gave an overview of the public opinion insights gained from the survey conducted by The Royal Society earlier this year. Turns out that only 9% of people know the term ML, but 76% know NLP and 75% driverless cars as possible applications. Meanwhile, Zoubin Ghahramani presented an overview of the main research and public opinion challenges facing the machine learning community.\n\nRoss Goodwin was one of the invited speakers and I can\u2019t help but include this definition of love generated by his lexiconjure bot which was trained on the OED. Ross is also the writer of writer (best job title ever?) for the science fiction short film Sunspring, which was screened at the workshop. Style transfer is only cool to computer scientists?! Ever since the Leon Gatys et al paper on A Neural Algorithm of Artistic Style in August 2015, the so-called neural style transfer has been growing in popularity and exploded this summer with the launch of the mobile app Prisma. Here is Magenta\u2019s version of live video style transfer with the option of combining multiple styles and tweaking their individual strength. Pretty cool, right? Turns out, only if you\u2019re a computer scientist (or journalist!). Simon Colton from Goldsmiths delivered a controversial talk on computational creativity with heavy criticism of the current experiments in style transfer carried out by the computer science community due to their excessive focus on the so-called \u201cpastiche generation\u201d. He also gave some additional uses of style transfer\u200a\u2014\u200amore acceptable to the art world in his view - including style exploration and style invention. The discussion continued on twitter. With that, I sign off. What was your experience of NIPS? What are the most creative and unusual applications, papers and projects that you saw? Anything major I missed? Tweet me @elluba or comment below.", 
        "title": "NIPS 2016: Cake, Rocket AI, GANs and the Style Transfer Debate"
    }, 
    {
        "url": "https://medium.com/@memoakten/collaborative-creativity-with-monte-carlo-tree-search-and-convolutional-neural-networks-and-other-69d7107385a0?source=tag_archive---------1----------------", 
        "text": "Models of creativity and collaboration aren\u2019t the purpose of this research, but very loosely, the system is based on ideas of: Creativity: as an efficient method of searching a large space of actions or decisions, Imagination: as the the ability to simulate many different scenarios and outcomes, Evaluation: as the ability to evaluate both the current situation, as well as the outcome of many imagined actions and scenarios, against some desired criterion, Collaboration: as the ability to respond\u200a\u2014\u200asomewhat intelligently and creatively\u200a\u2014\u200ain realtime, to some kind of external input, such as the actions of another user. This is an incredibly brief description of these topics, and I can\u2019t go into more detail now. But suffice to say it forms the basis of the implementation explained below. The basic idea is that the agent is a bit like a Logo Turtle, it can move forward or rotate left or right, and it draws as it moves. It uses Monte Carlo Tree Search (MCTS) for planning and decision making (the same algorithm that forms the backbone of AlphaGo\u200a\u2014\u200aI wrote about that here). More details on the implementation can be found in the paper, but briefly: At every timestep, MCTS does many simulations of random actions. I.e. The agent \u2018imagines\u2019 what would happen if it took particular actions. These simulations are run for an arbitrary depth (i.e. timesteps into the future). Each of the simulated trajectories are rendered into a texture, fed into an image classifier\u200a\u2014\u200asuch as a Convolutional Neural Network\u200a\u2014\u200aand the probability returned from the classifier is back-propagated up the partially expanded decision tree as a reward. This process of \u2018simulating and evaluating\u2019 is repeated many times per timestep. Because of the probabilistic nature of MCTS, and the balance between exploration vs exploitation, the system converges towards imagining actions and trajectories which are more likely to produce the desired outcome. When a predetermined time budget is reached (e.g. 100ms\u200a\u2014\u200athis way the system manages to remain realtime, at interactive rates), MCTS stops simulating new actions and trajectories, and picks the most robust child, i.e. the most visited (i.e. \u2018promising\u2019) action. The agent then performs that action (i.e. makes that move), and at the next timestep, the whole process of \u2018imagining and evaluating\u2019 is repeated. I tested this with a number of models. Because it\u2019s compulsory in the Machine Learning world, I first tested the system on MNIST, a dataset of handwritten digits. The first model I trained and tested is a simple Multinomial Logistic Regression.\n\nThe task of this agent is to draw a \u20183\u2019. And as you can see, it\u2019s working pretty well. The top left viewport shows the outcome of what the agent is drawing. The top centre viewport is what the classifier sees, i.e. the drawing scaled down to 28x28. The top right viewport shows all of the simulated (i.e. \u2018imagined\u2019) paths (all of the bright white squiggly lines). So at every timestep, each one of those white squiggly lines are being imagined by the MCTS, and evaluated by the classifier. Those imagined paths are random, but not totally random (i.e. not random from a uniform distribution). As the MCTS imagines more and more paths, it starts to get an idea of what performs better and what doesn\u2019t, so it leans towards picking actions that produces paths that are closer to the desired outcome (but it always tries to keep a balance between exploiting what it knows to work, vs exploring new territory). The red bar that\u2019s rising is the probability (i.e. confidence) of the image classifier that the drawing is of the desired class, in this case, a \u20183\u2019. Note this is not a deep model. It\u2019s simply a linear transformation on the pixels of the image (i.e. Wx+b, where x is the vector of pixels, W and b are respectively the weights matrix and bias vector), put through a softmax (nice tutorial on this here for tensorflow, and here for theano). I also trained and tested a deeper, Convolutional Neural Network (CNN)\u200a\u2014\u200aa network architecture inspired by the visual cortex (particularly based on the work of Hubel and Wiesel), specialising in image processing\u200a\u2014\u200asimilar to LeNet5 (second half of this tutorial here for tensorflow, or here for theano). I\u2019ll skip the results on this for now as I\u2019ll summarise it in the conclusion. I then tried a much deeper CNN. Specifically, I downloaded Google\u2019s 2015 state of the art architecture and classification model inception-v3, pre-trained on ImageNet, millions of images in 1000 categories. Here\u2019s the agent trying to draw a \u2018meerkat\u2019 using this model as the classifier, and much to my disappointment, failing miserably. It looks nothing like a meerkat, but just some kind of noise.\n\nInterestingly, in both of these latter two cases, the confidence of the classifier is very high (higher than 99.9+% in fact) that the outcome is of the desired class. You can just about see this in the videos as the very thin red line that slowly rises (the other blue lines at the bottom of the screen are the probabilities for the remainder of the 1000 classes). So the failure in this case is not necessarily a failure of the planning done by MCTS, or the way the MCTS is integrated with the classifier. In fact, the first MNIST study is a good demonstration that the overall system architecture and concept works. The failure in this case is because Google\u2019s very deep model trained on ImageNet is almost 100% confident that this random-looking noise is indeed a meerkat (in the first example) or a white wolf (in the second example). So the classifier is feeding an undesired reward signal into the MCTS (\u2018undesired\u2019, because clearly the results are not as we would like them). Turns out these networks are really easy to fool. This isn\u2019t necessarily related to this particular model by Google, but deep CNNs trained for classification in general. They\u2019re discriminative models, trained to classify images. So they have all kinds of tricks in their architecture (weight shared convolution layers, maxpool layers) to provide translational, scale (and a little bit of rotational) invariance. While the networks do classify natural images correctly with super-human performance, they also classify noise, and \u2018random shapes\u2019 incorrectly, with equally high confidence. In short they produce lots of false positives, and the manifold of what the network thinks a meerkat looks like, includes what meerkats really look like, as well as a whole bunch of other junk. Actually this was discovered by others around the same time I started working on this idea. They used a very different method, evolving input pixels to maximise desired neuron activations\u200a\u2014\u200aa very non-realtime, non-interactive process (in fact, this same research is what gave rise to Deepdream). In the past few years there has been some work overcoming these problems using natural image priors and adversarial networks\u200a\u2014\u200awhich I showed at the start of this article. These are methods I\u2019m now also looking at to incorporate into my MCTS driven agent, to try and constrain its output to the sub-manifold of more natural looking images. Alternatively, I\u2019m also looking at other classifiers altogether, perhaps not trained on ImageNet, that might not produce as many false positives. (For a hopefully not very technical intro to Manifolds and Latent Spaces see the relevant sections in this very long post). NB. It\u2019s very interesting to note, that while the output of the first ImageNet study looks like complete junk noise to us, and the classifier is almost 100% confident that the image is that of a meerkat, its remaining top 5 guesses are: mongoose, hyena, badger, cheetah\u200a\u2014\u200aarguably all meerkat-like animals. In the second ImageNet study the output again looks like complete junk noise to us, but it clearly looks different to the meerkat noise. Again the classifier is almost 100% confident that the image is a white wolf, but again its remaining top 5 guesses are timber wolf, arctic fox, samoyed, west highlands terrier\u200a\u2014\u200aall are dogs (or dog-like animals). So clearly there is something wolf-like about this particular distribution of noise, and something meerkat-like in the other distribution of noise. I wish I had tried this on non-animal classes like church, or submarine or tennis racket before I put this research on hold to move onto other things. As soon as I find the source I will try it out again!", 
        "title": "Collaborative creativity with Monte\u00ad Carlo Tree Search and Convolutional Neural Networks (and other\u2026"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/what-the-skeptics-say-about-deep-learning-4e1571638763?source=tag_archive---------2----------------", 
        "text": "Every hyped up technology needs a healthy dose of skeptics. Deep Learning is not an exception. To keep us all level set, let\u2019s see what a few of these skeptics are saying. Here are three recent articles from three different data science/machine learning experts.\n\nThis AI Boom Will Also Bust\n\nAll these criticism have come in the past month. They are all pretty tame as compared to this blog post in 2014 \u201cGet off the deep learning bandwagon and get some perspective\u201d:\n\nonly to come back later and write in his disclaimer:\n\nInteresting arguments all of them, but are any of them valid?\n\nTo understand Deep Learning better, please see my 2017 predictions for Deep Learning or sign up with Intuition Machine to get the conversation going.", 
        "title": "What Skeptics Say About Deep Learning \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/@kendenman/great-read-auren-b8b170a53942?source=tag_archive---------3----------------", 
        "text": "Great read, Auren! You\u2019ve got your finger on a challenge that will consume a good part of the AI space for the next decade, at least. One thing I\u2019ve observed in the computer vision space is that, the collecting, curation and management of the data is certainly a critical part of success, but the training set is relatively small, compared to all data collected. The point of diminishing returns is often reached much sooner than I would have guessed. At that point, careful culling of better false positives and false negatives from the data is the art and science. Disciplined curation to decide what of the morass of data is additive to the predictive success becomes all consuming. The problem turns on it\u2019s head in some way. Trying to see the trees for the forest, if you will. Edge cases become disproportionately valuable. Being able to cull through a huge database can be valuable, of course. But finding the right data is ultimately the challenge. I\u2019m reminded of poorly written code vs. elegantly written code. One is bloated and the other seems sparse. An analogy might be that while Tesla has far more miles of data on assisted driven cars, I\u2019m guessing that Google has far more elegant data sets to train against, given the depth of team, time on target and better understanding of the nature of deep learning.", 
        "title": "Great read, Auren! \u2013 Ken Denman \u2013"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/deep-learning-machine-listens-to-bach-then-writes-its-own-music-in-the-same-style-1a099cf1a449?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep-Learning Machine Listens to Bach, Then Writes Its Own Music in the Same Style"
    }, 
    {
        "url": "https://medium.com/@devadattaj/can-a-i-deep-learning-help-cancer-care-professionals-for-precise-diagnosis-personalized-6a87c4169d1a?source=tag_archive---------5----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Can A.I & deep learning help cancer care professionals for precise diagnosis & personalized\u2026"
    }, 
    {
        "url": "https://medium.com/@nikitay/dialogue-with-the-invisible-jeeves-36c6c0b72c26?source=tag_archive---------6----------------", 
        "text": "Dialogue between humans and machines is a reality\n\nof our life at present. But where is it taking us? Will technological developments make robo-advisors and personal assistants assume quasi-human characteristics eventually\u200a\u2014\u200afrom artificial intelligence at its core, to the pleasant voice modulations of a good actress or actor inviting emotional attitudes. The answers are not clear yet, but we can foresee that the ability to place basic human needs and interests at the centre of the entire ecosystem, and serving them most effectively in terms of cost, speed, and quality will shape the future course of both technological advancement and market developments. Most importantly, it will unfold as the learning process in a dialogue with us through a Voice User Interface (VUI).\n\nWhat is this dialogue about\n\n\u2013 an invisible and rather unobtrusive Jeeves responding to daily needs of a typical modern-day Wooster? Most of the projections focus on automating lifestyle services such as basic financial planning\u200a\u2014\u200afrom moving savings to an account with higher interest rates to balancing the portfolio in line with risk profiling and analysis, ordering food and household goods, or using a VUI for conversational programming of IoT[i] objects, booking hotels and arranging things for dining out with friends, travelling, going for sports, using health services, planning and paying for education, insurance, etc. Even advising on the style of a dress or gift is a possibility. Constant background analysis of personal choices and preferences as well as smart use of statistics and big data will stand behind the higher cost/quality ratio than the best human PA could deliver. The psychology of communication, sentiment analysis, and conversational excellence will probably be among other methods of achieving excellence.\n\nMachine dialogue is with us already\n\nIt starts from a simple voice-driven IVR[ii], where a customer of a telecom or a bank does not have to wait on the line listening for all the complex menu options of an old-style DTMF[iii] IVR forgetting the first few options by the time of getting to the last ones, and pressing buttons randomly to get to a human operator (unless the connection is interrupted suddenly by the call centre, which can be truly maddening). Instead, you simply say what you want and machine deals with it, as it has been trained specifically to understand all the possible ways and forms in which you can formulate, articulate, and voice the reasons why you are calling. It can also get a sense of whether you are happy with the service or not. Even if you are mad, it will transfer you to a specialist who can help (it\u2019s not a joke).\n\nWhat makes this dialogue possible:\n\nspeech recognition and analytics, semantic interpretation techniques enabling contextual understanding, voice biometrics to verify human identity for processing highly personal and confidential data, a VUI layer for universal chatbots and AI platforms based on machine learning, 4\u20135G and computing power, big data, and importantly\u200a\u2014\u200apsychological preparedness to communicate with the machine based on an increasingly smooth customer experience.\n\nWhat are the main obstacles:\n\naccuracy of speech recognition, technical imperfections (noise filtering and, generally, managing acoustic environments)? As it stands today, it is increasingly about innovation in algorithms and the right amount of data for machine learning. Technologies like those of Spitch AG will deliver the greatest accuracy if trained on a massive amount of client audio data from the same very end-users that the products based on these technologies will be serving.\n\n(1) can the machine/AI harm humans, and (2) can we harm ourselves by treating the machine as a slave. This invites a more philosophical question of whether artificial intelligence, when/if it fully materializes, is a form of semi-autonomous artificial life and personality, essentially involving a new kind of ethics, or human relationships ethics would apply. In the foreseeable future AI will remain fully dependent on human programming. Chatbots will too, by default. The danger, therefore, can only be rationally conceptualized as being associated with human mistakes. The powerful image of \u201cSkynet\u201d terminating life on Earth or mismanaging your pension funds and deleting all the records is disquieting enough to make us think about real but less conspicuous and more distant threats. The old notion of prevention being better than cure is valid and important in this context.\n\nWhere are banks and money in this picture?\n\nKPMG is envisioning an \u201cinvisible bank\u201d of 2030:\n\n\u201c\u2026 there is no \u2018banking app\u2019\u200a\u2014\u200aaccess to money is interwoven with health, time management, leisure and friendship. Visiting a bank will be as alien a concept as picking up the landline (\u201cWhat\u2019s that?\u201d my five-year-old son said recently). Banks will be just as invisible, but just as vital, as the manufacturers of 4G base stations are today\u201d[iv].\n\nThe KPMG vision for retail banking in 2030 is one of a disaggregated industry\u200a\u2014\u200awith three distinct components. The first layer, represented by universal personal assistants like EVA in the KPMG example, is the Platform Layer. Together with Product and Process layers, the banking industry is set for a period of significant structural reform. At present, one can see only incremental improvements in customer experience such as contactless payments, faster onboarding processes etc. Genuine, transformational innovation is rare.\n\n\u201cBut customers are increasingly using other channels to fulfill functions previously dominated by banks. The arrival of services such as Apple Pay\u200a\u2014\u200ato which most banks have signed up\u200a\u2014\u200ahints at a future where financial brands are hidden behind devices\u2026\n\nBank brands remain highly trusted. Some would argue they could develop lifestyle layers to compete in this Platform space. This is one possible scenario\u2026\u201d[v].\n\nOther scenarios are dependent on the evolution of smartphones and other personal mobile devices and their \u2018always-on\u2019 voice user interfaces enabling access to essential banking services, robots, and PAs surpassing the scope of human capability as far as the speed and volume of data that they can process.\n\nAt Spitch AG we are excited to be taking part in the effort to transform this space for you and your customers alike. We wonder what are you thinking about it? Please let us know!", 
        "title": "Dialogue with the Invisible Jeeves \u2013 Nikita Y \u2013"
    }
]