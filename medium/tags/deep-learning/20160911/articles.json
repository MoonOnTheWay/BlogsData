[
    {
        "url": "https://medium.com/@deepnarainsingh/vanishing-gradients-while-training-neural-network-42d6d5d83cdf?source=tag_archive---------0----------------", 
        "text": "One of the difficulty which is faced while training deep neural network with gradient based method is the vanishing gradient problem. Back propagation algorithm is used to train the neural net and the fundamental problem which we face in this technique is the vanishing gradient and some time exploding gradients and this causes increase in training time and accuracy suffers. This problem is caused by certain activation functions and it becomes difficult to tune the parameters of different layers in neural network and as the number of layers in neural network increases this problem become more significant.\n\nLet\u2019s explain it with an example of the sigmoid activation function.\n\nSigmoid function takes any input and squeezes its value between (0,1). By seeing the graph we can interpret that the gradient near the asymptotes is tending towards zero. This makes the training of network extremely slow as the update in the network parameters is so small\u00a0. We can represent this situation as saturated neuron or dead neuron where the network is no more learning.\n\nLet\u2019s go into detail and take derivative of sigmoid function and plot it.\n\nThe maximum value of the function is\u00a0.25 and asymptotic value is touching zero as the value of x increases on positive or negative side. The range of the derivative of sigmoid is between (0,.25)\u00a0.\n\nConsider a neural net with input x1\u00a0, a hidden layer with single neuron and a output layer. Error will be the difference of actual value and predicted value.\n\nTaking the derivative of error with respect to the weight and by the chain rule we will get.\n\nSo if we see this term we are multiplying by the derivative of sigmoid two times and the value of derivative of sigmoid lies between (0,.25). The product of number multipled together when it\u2019s value lies between 0 and 1 gives a smaller value. This is the case for one hidden layer and imagine if its a deep neural network then the product of derivative will reduce the gradient very fast and hence we will face the problem of vanishing gradient.\n\nWe have to select the activation function where in we can avoid the situation when we face saturated or dead neurons. One solution is to use relu functions and it\u2019s different versions which avoid the vanishing gradient problem. More on this in next post.", 
        "title": "Vanishing Gradients while training Neural Network \u2013 deepnarainsingh \u2013"
    }
]