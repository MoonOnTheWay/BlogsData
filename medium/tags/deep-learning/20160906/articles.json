[
    {
        "url": "https://medium.com/autonomous-agents/mathematical-foundation-for-noise-bias-and-variance-in-neuralnetworks-4f79ee801850?source=tag_archive---------0----------------", 
        "text": "Neural Nets are quite powerful models in machine learning which are used for learning the behavior of high dimensional data. Typically, data is not always present in its purest form, the signal. Most of the data that is made available for machines during training or runtime prediction do have some amount of noise. During training, the Neural Nets can get highly sensitive to the noise in the input and can start overfitting the learning to the noise in the input.\n\nIn the previous post titled \u201cAlgorithms to Improve NeuralNetwork Accuracy\u201d, we learnt about the overfitting problem in Neural Nets and how to break it using L1/L2 regularizers, Weight penalties decay and constraints. Also in the post title \u201cCommittee of Intelligent Machines\u201d\u00a0, we learnt how to stack different models to improve accuracy and generalize the prediction better.\n\nIn this post, I would like to introduce the fundamental math on understanding Noise, Bias and Variance during Neural Net training and also use Noise as a regularizer for generalizing the Neural Nets.\n\nIt\u2019s important to understand noise in machine learning because this is a fundamental underlying concept that is present in all datasets.\n\nLet\u2019s start with simple understanding of Noise. Noise is a distortion in data, that is unwanted by the perceiver of data. Noise is anything that is spurious and extraneous to the original data, that is not intended to be present in the first place, but was introduced due to faulty capturing process.\n\nNoise gets into data in many ways:\n\nGiven this, we need to always account for noise during machine learning.\n\nNow, let\u2019s say we have some data that we are using to train our Neural Net models. Let\u2019s say that the input vectors {x1, x2,\u2026 xn} has outcomes {y1, y2\u2026yn} associated with it. Then the way to think about Noise is as follows:\n\nWhere f(x) is some underlying function on the independent variable x (input features) and y is the outcome.\n\nHere, epsilon is the noise in the data that has a functional relation with the input. Lets also assume that epsilon has a zero-mean and a standard-variance.\n\nThe objective of the Neural Nets is to learn the underlying function f(x) to predict the outcomes. As you may guess, since there is a noisy functional relation \u2018epsilon\u2019 between the underlying function f(x) and the outcome, the probability of the Neural Nets to learn about the noise is high.\n\nThe trick of the trade is that, if the Neural Nets can learn about the noise \u2018epsilon\u2019 as a separate functional variant, then we can claim that the Neural Nets are balanced. Typically, Neural Nets models are unable to differentiate noise from the input and lands up learning about noise as part of the input function, f(x+epsilon). This is when the Neural Nets have overfitted. We also call this state as high-variance state. The opposite of a high-variance state is the high-bias state, where the Neural Nets are unable to come up with any learning at all (as in, the Neural Net is not able to find any relation between the input vector and the outcomes). Let\u2019s study this further\u2026\n\nLet\u2019s mathematically decompose the input function to understand the concept of Bias and Variance, given that there is noise in the signal.\n\nLet\u2019s say we train the Neural Nets to find a underlying function f_cap(x) that tries to approximate the underlying function f(x) while noise \u2018epsilon\u2019 is present. Now, we can say that the Neural Network has learnt well if the mean squared error between the expected outcome \u2018y\u2019 and the output of the learnt function f_cap(x) is minimal or near to zero. Note that it should be minimal not only for the training set, but also for any new data that is provided during validation.\n\nLet\u2019s define the concept of bias and variance now:\n\nBias: As we said, bias is an inability of the Neural Nets to learn any co-relation between the input vector and the output state. Mathematically it can be expressed as follows:\n\nI am using the < > notation to denote the expected-value here. If the expected value of the difference between the learnt-function and the underlying function is high, then we state that the learnt function is highly-biased.\n\nVariance: Variance is the sensitivity of the Neural Nets to small changes in the inputs. In other words, any minor noise in the input gets picked up by the learning functions of the model and tries to overfit the noise as if they are signal. This causes overfitting and produces poor accuracy during validation. Mathematically it can be expressed as follows:\n\nWe can also generalize the expected value of a variance function for any given random variable X as follows:\n\nWe said that the mean squared error between the outcome \u2018y\u2019 and the learnt-function f_cap(x) should be minimal or near zero. In other words, we said:\n\nIn order to decompose the mean-squared error to its Bias and Variance parts, lets work on the equation as follows:\n\nAnd hence, the above equation provides the relation between Variance and Bias for a mean-squared-error between the learnt-function and the signal.\n\nSo, whenever we say that our Neural Net models are overfitting, we are stating that there is a high variance in the learnt-function.\n\nOne of the ways to reduce overfitting is to actually add more noise while training Neural Nets!!\n\nIf a Neural Net is displaying high variance, it is possibly because it is overfitting \u201cf(x) + epsilon\u201d of the training-set nearly well. So when a validation-set during test is provided, the model does not know how to predict accurately.\n\nOne of the ways to break this is actually to add noise into f(x) so that the model is not able to co-relate the input vector {x1\u2026xn} tightly to its output class {y1\u2026.yn}. In other words, we are trying to reduce variance of the system by breaking its ability to tightly fit f_cap(x) to \u201cf(x) + epsilon\u201d.\n\nNote: For people who have used ensemble methods, this is NOT the same as bootstrapping in ensemble methods where multiple subsets of data is created from the same data-set using noise. Instead this concept is similar to Additive White Gaussian Noise, or AWGN.\n\nA controlled noise like Gaussian is good to modulate any noise that exists in the input data.\n\nHere, \u2018N\u2019 represents the Gaussian noise with zero-mean and Gaussian variance.\n\nMathematically, the effect can be understood as follows:\n\nNote that the sigma-square turns out to be Gaussian penalty on the cost function, and is very similar to a L2-penalty which you learnt from the post titled \u201cAlgorithms to improve Neural Network Accuracy\u201d.", 
        "title": "Mathematical foundation for Noise, Bias and Variance in #NeuralNetworks"
    }, 
    {
        "url": "https://chatbotsmagazine.com/alterra-ai-launches-slack-bot-travel-agent-fe094e256dc9?source=tag_archive---------1----------------", 
        "text": "Our Alterra.ai today announces the launch of its AI-powered virtual travel agent on the Slack platform. It is the first true Slack bot in the travel category.\n\nAlterra is a virtual travel agent. She can book flights and hotels. She can also recommend where to go for a leisure trip, and what to see there:\n\nAlterra has been on Facebook and Telegram for quite a while. (You can read here about the A.I. behind it.)\n\nAnd now she is on Slack. Users can DM her, or add to a channel. The team can then collectively research and book a trip, with Alterra acting as just another team member.\n\nSurprisingly, Alterra seems to be the first true Slack bot in the travel category.\n\nThere are other virtual travel agents on Slack. Some of them, however, are technically apps, not bots. You cannot chat with them; they only understand a small set of predefined slash-commands. They are apps with command-line interface. Others do understand natural language but are Mechanical Turks, not automata. There are humans inside, and thus it often takes them hours to reply.\n\nAlterra is a true chatbot: she can maintain a dialog in natural language and is 100% powered by A.I. algorithms. No humans are involved, whatsoever.", 
        "title": "Alterra.ai launches Slack bot \u2014 travel agent \u2013"
    }, 
    {
        "url": "https://medium.com/@kglushak/thank-you-brandon-reynolds-for-this-great-article-16b575ff1e?source=tag_archive---------2----------------", 
        "text": "Thank you Brandon Reynolds for this great article. I wonder what other products of SalesForce can we expect in the near future, based on the principles from this article?\n\nSameerkumar Sarma, if we talk of chat-bots, there are several awesome blog posts on the web. Please check these 2 to start with: http://www.wildml.com/2016/04/deep-learning-for-chatbots-part-1-introduction/\u00a0, https://blog.acolyer.org/2016/06/29/a-neural-conversation-model/\u00a0. Here you will be able to find some basic principles on how chat-bots are built. You might also want to check my blog post on text categorisation, using some A.I. approaches, if you\u2019d like: https://tech.smartling.com/text-categorization-the-hackathon-project-b8c33ae94c24", 
        "title": "Thank you Brandon Reynolds for this great article. \u2013 Konstantin Glushak \u2013"
    }, 
    {
        "url": "https://medium.com/@dollyfisher/japanese-farmers-invention-demonstrates-the-importance-of-ai-200ce68aef49?source=tag_archive---------3----------------", 
        "text": "Makoto Koike of Japan grew up a cucumber farmer. But he always had bigger problems. Eventually he wound up in the automobile industry as a computer systems designer. There he learned to integrate artificial intelligence and automation into vehicle computer systems and became quite proficient with different machine learning systems. Inspired by this work, he decided to return to his parent\u2019s cucumber farm in 2015 with an idea that would help them out tremendously.\n\nOn the surface, it might seem like sorting through cucumbers to find the best ones wouldn\u2019t be so difficult, but there are many factors to take into consideration. Each cucumber has a different shape, color, quality, and freshness. It takes employees of Koike\u2019s parents months to learn the system for sorting cucumbers and it\u2019s very labor intensive and costly.\n\nThat\u2019s when Koike had the idea of creating a program that would automate the sorting process and decrease expenses while maximizing profits. Though he had never used deep learning artificial intelligence technology before, he started out by experimenting with TensorFlow, Google\u2019s open source deep learning platform. He discovered that by simply inputting images of cucumbers, TensorFlow was able to sort the vegetables with great accuracy. He continued to input images and fine tune the software until it was capable of sorting the vegetables at least as accurately as humans and much more quickly. Finally, he took the software and turned it into a fully-operational sorting machine that completely automates the process of cucumber sorting for his parent\u2019s farm.\n\nCucumber sorting is hardly the greatest problem facing humanity, but the fact that Koike was able to harness the power of AI to solve it demonstrates the importance of such technology. There\u2019s no limit to what it can do for humankind as it continues to be developed and applied to different problems.\n\nArtificial Intelligence News brought to you by artificialbrilliance. com", 
        "title": "Japanese farmer\u2019s invention demonstrates the importance of AI"
    }, 
    {
        "url": "https://medium.com/@dataaspirant/in-this-best-data-science-articles-section-we-were-going-to-present-you-the-top-most-popular-e036f46a8fb2?source=tag_archive---------4----------------", 
        "text": "In this best data science articles section, we were going to present you the top most popular interviews of data scientists, who have done a great work in kaggle competition. Solutions of kaggle problems, in addition to these we were also presenting you the most popular data science articles to read.", 
        "title": "In this best data science articles section, we were going to present you the top most popular\u2026"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/all-74-service-ai-service-map-2016-summer-ai-you-want-to-take-advantage-of-you-quickly-find-70151ff37945?source=tag_archive---------5----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "All 74 Service! \u201cAI Service Map 2016 Summer \u201c \u2014 AI You Want to Take Advantage of You Quickly Find"
    }
]