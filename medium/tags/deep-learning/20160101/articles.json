[
    {
        "url": "https://medium.com/@ilblackdragon/tensorflow-tutorial-part-3-c5fc0662bc08?source=tag_archive---------0----------------", 
        "text": "In the previous Part 1 and Part 2 of this tutorial, I introduced a bit of TensorFlow and Scikit Flow and showed how to build various models on Titanic dataset.\n\nIn this part, let\u2019s make a more complex model, something that can handle categorical variables.\n\nUsually in machine learning, handling of categorical variables requires creating a one-hot vector for each category. In deep learning, there is an alternative solution for that\u200a\u2014\u200adistributed representations or embeddings.\n\nUsing embeddings, you can represent each category as a vector of floats of the desired size, which can be used as features for the rest of the model. Note, that because of the fully differentiable nature of the TensorFlow components (and other Deep Learning frameworks), this allows to \u201ctrain\u201d the most optimal representation for your task. This has shown been the most powerful tool in the Deep Learning toolkit as it removes need to do manual feature engineering.\n\nThis brings most value when you have a lot of categories or discrete sparse values\u200a\u2014\u200ae.g. hundred and thousands. Then you get compression of the input from N categories to fixed size embedding.\n\nWhen you have a small number of categories it still works by using the embedding space to model one-hot vectors per category (e.g. by spreading categories around without any semantic).\n\nLet\u2019s continue with our Titanic dataset and try a simple example of using just Embarked field as categorical variable for prediction:\n\nFirst, we select only \u201cEmbarked\u201d column, for as our features. We then follow regular 20% train/test split.\n\nIt\u2019s always useful to analyze what kinds of values features have:\n\nThis is passed to CategoricalProcessor\u200a\u2014\u200aa helper class that maps categorical variables to ids. In this case it will create a vocabulary of S->1, C->2, Q->3 and unknonw/nan -> 0 and remap this column to integers.\n\nThe final model is simple, it leverages another helper function skflow.ops.categorical_variable which creates an embedding matrix of size n_classes by embedding_size and looks up ids from input in it. This is a similar to skflow.ops.one_hot_matrix but instead returning a learnable distributed representations for given categories.\n\nFinally train model and predict on a test dataset and voila, we got a model using distributed representations for categorical variables.\n\nAfter using embeddings, there is a simple model to compare with one-hot vector representation. It will map from class 1, 2, 3 into a vector with one at the position of the class and zero everywhere else. E.g. class C (2) will be mapped into [0, 0, 1, 0] vector.\n\nIn this case, given only one feature with 3 classes for prediction, results end up been exactly the same as using embeddings.\n\nYou can learn how to combine categorical and continues values in Part 4.\n\nAdditionally, as I mentioned above, the most value in using distributed representation coming from categorical features with large number of classes. In the Part 5 you can use this method of representing categorical variables for Natural Language Understanding tasks, like Document classification.", 
        "title": "TensorFlow Tutorial \u2014 Part 3 \u2013 Illia Polosukhin \u2013"
    }, 
    {
        "url": "https://medium.com/@fabmilo/how-to-compile-tensorflow-with-cuda-support-on-osx-fd27108e27e1?source=tag_archive---------1----------------", 
        "text": "With the release of Tensorflow 1.0 and the official OSX GPU package the preferred way to install Tensorflow with CUDA on Mac is now to use download directly from the Python Repository. I always advice to use the Anaconda Python distribution as your python environment.\n\nFor more tensorflow tips and updates follow me @fabmilo", 
        "title": "How to compile tensorflow with CUDA support on OSX (Updated Feb 19 2017)"
    }, 
    {
        "url": "https://medium.com/@tedsta/deep-learning-in-rust-7e228107cccc?source=tag_archive---------2----------------", 
        "text": "Meta Edit 2/2/2016: I wrote this post when deeplearn-rs was only a week old. This is more of a journal post where I reflect on things. To see the latest deeplearn-rs, the readme on the github repo is probably the best place to look.\n\nLast semester[1] I flailed around trying to get into deep learning. I felt kind of guilty for jumping onto the hype train[2]. Deep Learning is a guilty pleasure. I went through tutorials to relearn error backpropagation[3]. This course in particular helped me a lot.\n\nAfter gaining a concrete understanding of the mechanics of deep learning, I wanted to experiment with real data and build real networks. I read lots of papers, but struggled to get even a few lines of code out. This was frustrating, because I\u2019m the type of guy that churns code out. I was also kind of burning myself out, because my main project at the time was a ZFS driver for Redox. Dividing my brain between the behemoth that is ZFS and the still-young field of Deep Learning was exhausting and unproductive. So I shelved Deep Learning for a bit.\n\nI was excited for winter break. I told myself I was going to get stuff done. I got some things done. I merged a massive PR for ZFS on Redox that I had been working on for about a month. My friend and I made the beginnings of a crappy stop motion fighting game. I played around with Tensorflow, went through some of their tutorials, and extended the language model tutorial code, but I wanted more. I struggled because I\u2019ve never designed and carried out a full deep learning experiment with a modern framework. Nothing truly satisfying got done until the very end of winter break.\n\nI found a nice compromise in a project that involved both deep learning and systems programming\u200a\u2014\u200aa deep learning framework in Rust that uses OpenCL to do all the heavy lifting. Luckily, I had already written an OpenCL matrix math library[4]!\n\nI started hacking on this a week ago, and I\u2019m pretty satisfied with how far it\u2019s come. You can construct and (manually) train a deep computation graph, and it all runs on the GPU via OpenCL. Take a gander:\n\nNot as easy on the eyes as Theano or Keras or Tensorflow, I know. It\u2019s hard to beat python in the readability department[5], though. Anyway, this is lowest layer. I intend to write a `GraphBuilder` that makes constructing these networks a little more cushy and a little less error prone.\n\nI\u2019ll just throw the deeplearn-rs github here.\n\nIf you don\u2019t know about deep learning and computation graphs, I\u2019ll try to elaborate. It turns out that artificial neural networks can be represented in a more general framework\u200a\u2014\u200acomputation graphs. Conveniently, artificial neural networks can even be represented by much more compact (in terms of node and edge count) graphs of matrix operations. For example, fully connected layers use matrix multiplication to multiply the weights and inputs and sum the products for each node. For each layer, all the inputs are represented as one matrix, and all the weights are represented as another. So convenient! Praise the math gods! It turns out that this makes it easy for us to implement super fast deep neural networks by running our (highly parallelizable) matrix operations on (highly parallel) GPUs.\n\nRust has strict ownership and borrowing rules. To make a long story short, you can\u2019t just dole out pointers to things like you can in most other languages[6]. So, instead I use indices into arrays. This way I don\u2019t have to deal with all of the headaches brought by ubiquitous use of `Rc<RefCell<T>>`[7]. Let\u2019s see what a node looks like:\n\nEvery matrix[8] in a computation graph is represented as a variable. Variables are accessed using a corresponding `VarIndex`. All variables are stored in the graph\u2019s `VarStore`. VarStore uses `RefCell` to hide variable mutability so that multiple variables can be borrowed simultaneously, regardless of mutability. There is never a reason to borrow the same variable more than once at a time for our purposes.\n\nNote the `out_grad` field is a list of `OutGrad`. OutGrad is a sort of CoW (Copy on Write) VarIndex to a gradient variable. When calculating gradients during the backward pass of the graph, if a variable is forked, (if a variable is used as input to multiple operations) there will be multiple gradients on the variable. If there are multiple gradients, they need to be summed to produce the final gradient, and a variable must be made to store that sum. If there is only one gradient, we can just use that gradient variable. OutGrad will only allocate a new variable if it needs to.\n\nLet\u2019s look at the code for `Graph`, which ties everything together.\n\nThe most interesting function here is `Graph::add_node`. You give it an operation and a list of inputs and it\u2019ll handle the rest. It creates the output variables and creates a new gradient for each of the inputs.\n\nLastly, let\u2019s look at the implementation of matrix multiplication:\n\nOverall, I like the design so far. The purpose of deeplearn-rs is to provide a framework to build trainable matrix computation graphs that are configurable at runtime.\n\nMoving forward, some things I\u2019d like to do:\n\n[1] Fall 2015\n\n[2] But then less guilty because I was into artificial neural networks back in high school, and deep learning is a generalization of those ideas\n\n[3] Error backpropagation never really rolled off the tongue (so many syllables!)\n\n[4] The matrix math library also got some sweet upgrades\n\n[5] Nigh impossible\n\n[6] In safe Rust you can\u2019t, but in unsafe Rust, anything is possible\u2026\n\n[7] `Rc<T>` is a reference counted pointer, but it demands that the inner type be immutable. `RefCell` hides mutability at compile time and ensures that all mutable references are exclusive at run time. Any number of immutable borrows from a value can exist at any given time, but a mutable borrow must be the only borrow for the duration of its lifetime.\n\n[8] Whether an input, a weight, a gradient, etc.", 
        "title": "Deep Learning in Rust: baby steps \u2013 Theodore DeRego \u2013"
    }, 
    {
        "url": "https://medium.com/machine-intelligence-report/machine-deep-learning-in-an-artistic-context-441f28774bcc?source=tag_archive---------3----------------", 
        "text": "The key aspect of ML is supplying a learning algorithm with training data. In the most simple terms, during the training phase, the learning algorithm analyses the training data and builds a model. After training, during the prediction phase, new input data is presented to the model, the model processes the input and produces an output, a decision or prediction.\n\nThe input to the model is a vector (or more broadly speaking, a tensor, with dimensions and shape depending on the problem). The output of the model is also a vector (or tensor), which may or may not have the same dimensions as the input, again depending on the problem.\n\nThe model is parameterised by model parameters, and it\u2019s these model parameters that the learning algorithm tries to learn. Training the model consists of specifying an Objective function (also known as Loss, Cost or Energy function) and the learning algorithm tries to find the set of model parameters which minimise the Objective function over the training data (or it may be a Utility function which the algorithm tries to maximise).\n\nIn reality, ML implementations may be a lot more complicated than this, sometimes without such clear distinctions between training and prediction phases (e.g. in Online Learning, Transfer Learning, Reinforcement Learning etc where the model is updated continually as new data becomes available), but these concepts are at the root of it all.\n\nIn Supervised Learning, the model is trained on labelled data, where each training example is an input-target pair. During training, the learning algorithm tries to learn the model parameters which effectively implement a function that maps the input of each training pair, to the associated target. These targets can also be thought of as a supervisory signal. For classification problems, the target is usually a discrete class label, often represented as a one-hot vector (i.e. a vector where all elements are zero, except for the entry for the desired class, which is one). For regression problems, the target is usually a real-valued vector (or tensor). Having input-target pairs makes it relatively straight forward to specify the objective function, thus right now Supervised Learning is one of the most popular and successful branches of ML. However, the training pairs often need to be manually associated by people. This makes them cumbersome and very time consuming to prepare. Online crowd-sourcing platforms such as Mechanical Turk have helped accelerate the preparation of large labelled datasets which is why we\u2019re starting to see more success in this field (LeCun, 2014).\n\nIn Unsupervised Learning, training is performed on unlabelled data. Without an external supervisory signal, it can be more ambiguous as to how to specify the Objective function, and unsupervised learning is currently one of the big open problems in ML (Le et al. 2012). One of the common training objectives of unsupervised learning is found in an Auto-Encoder, in which the target is the same as the input, and the learning algorithm tries to learn how to compress and decompress each training example with minimal loss. If successful, it finds regularities in the training data, and learns more compact and meaningful representations. Another common objective is clustering, in which the learning algorithm tries to organise the training data into groups based on similarities that it tries to learn. When new inputs are presented to a model trained with one of these unsupervised learning methods, the model can transform the input data to one of the more compact representations, or predict how it relates to other data based on the patterns it has already found.\n\nA combination of the two\u200a\u2014\u200aSemi-supervised Learning\u2014 is used when some of the data is manually labelled, and some of it isn\u2019t. Upon training, the model learns how to classify the data, and associate the unlabelled items with the correct labels supplied in the training.\n\nIn Reinforcement Learning (RL), the model is not trained with labels associating inputs with targets. It is neither supervised (with a direct supervisory signal) nor unsupervised (with no supervisory signal), but instead there is a delayed reward signal (Kaelbling, Littman, & Moore, 1996). The terminology is slightly different in RL, where decisions or predictions are called actions, and the decision making (or action taking) entity is called an agent. This is because RL is based on a Markov Decision Process (MDP) (Bellman, 1957), where there is a notion of time, and at each time-step, agents take actions and move between different states. In RL, at every time-step, the agent receives a reward from the environment. But this reward might not be an immediate reward for the last action, but could be a delayed reward for an action or series of actions taken much earlier on, or it could even be related to \u2018random\u2019 events outside of the agent\u2019s control or knowledge. Part of RL is to solve this attribution problem of delayed rewards. The general objective of the algorithm is to learn what the optimal decisions are by maximising its long term reward. This process also involves a balance between exploration (of new actions which haven\u2019t yet been made) vs exploitation (of actions which are known to reward higher than others). RL can also be thought of as learning by trial and error.\n\nDeep Learning (DL) is the name given to a family of architectures and methods that aims to minimise or eliminate domain-specific feature engineering by learning a sequence of non-linear feature transformations and hierarchical representations (LeCun, 2014).\n\nTraditional machine learning techniques\u200a\u2014\u200asuch as support vector machines or shallow neural networks\u200a\u2014\u200aare unable to work with highly complex, high dimensional data. When working with such models, it is necessary to pre-process the data, reduce dimensions and extract hand-crafted, domain-specific features, a process called feature engineering [LeCun 2012]. Collectively, these features are called a representation (of the data). The learning algorithm is then trained on this hand-crafted representation. The process of feature engineering is often quite difficult, time-consuming and requires skill [Ng 2013]. Furthermore, the success of the training is highly dependent on this chosen representation [Bengio et al. 2013]. As a result, feature engineering approaches can provide inconsistent, unreliable results.\n\nUsing deep learning techniques, the pre-processing and feature extraction steps can be skipped. Instead, the model is fed the higher dimensional, raw data. The deep learning model is a stack of parameterised, non-linear feature transformations that learn hierarchical representations [LeCun 2014]. During training, each layer learns what kinds of transformations to apply to the previous layer, i.e. what kinds of features to extract from it. As a result, the deep learning model is a hierarchy of representations with an increasing level of abstraction.\n\nThis makes deep learning very powerful in handling real world data. However, with many more model parameters to learn, often in the millions, this comes at the cost of more complex implementations, higher computational requirements, and larger training sets [LeCun et al. 1998].\n\nIn summary: A machine learning model requires a representation of the world. In shallow learning, this representation needs to be hand crafted using domain-specific pre-processing and feature extraction. In deep learning, the raw data can be directly fed into the deep model and the algorithms learn a hierarchy of representations.\n\nHowever, even though for deep learning domain-specific feature engineering is not necessary, representation of the input data is still important. A key component in implementing a successful deep learning system, is the relationship between the representation and architecture of the model, they need to be compatible so that the relevant features can be extracted efficiently from the inputs [Bengio et al. 2013]. In the seminal research [Krizhevsky et al. 2012], the authors found that removing any one of their convolutional layers\u200a\u2014\u200aeach of which contained no more than 1% of their 60 million parameters\u200a\u2014\u200aresulted in inferior performance.\n\nDL researchers are very aware of these limitations and architecture design is a very active research area. Thus currently DL is far from the silver bullet / universal learning algorithm that the media hype sometimes proposes it to be.\n\nAn in-depth survey of DL and related algorithms is not within the scope of this text and we\u2019ll only focus on recent major milestones relevant to this research. For an in-depth survey please see (Schmidhuber, 2015).\n\nIn [LeCun et al. 1989] Yann LeCun used a Convolutional Neural Network (CNN)\u200a\u2014\u200aa deep learning neural network with many layers and connections inspired by those found in biological systems\u200a\u2014\u200ato recognise handwritten digits. However the computers of the era were not powerful enough to run the operations required by the network, so an additional DSP board was needed. Over the next twenty years LeCun\u2019s research showed that CNNs had the ability to learn and recognise patterns in image and speech recognition [LeCun and Bengio 1995][LeCun et al. 1995][LeCun et al. 1998][LeCun et al. 2004].\n\nHowever, DL algorithms require a great deal of computing power. Especially when dealing with large inputs such as images, in which case the amount of computation scales linearly with the number of pixels [Mnih et al. 2014]. CNNs were not practical for real world applications until highly parallel Graphical Processing Units (GPU) became available [Raina et al. 2009]. This led to an explosion of DL implementations. Quite famously, Quoc Le et al developed software that learned how to detect faces and cats and extract their features by sampling random frames from 10 million YouTube videos [Le et al. 2011].\n\nWith so many parameters to train, CNNs require massive datasets. In the absence of such data, CNNs were being outperformed by older, handcrafted, specialist pattern recognition algorithms. With the introduction of large datasets with millions of labelled images in thousands of categories, such as ImageNet [Deng et al. 2009], this balance shifted. In 2012 Geoffrey Hinton\u2019s students Alex Krizhevsky and Ilya Sutskever designed a new deep CNN architecture with 60 million parameters and trained it across two GPUs [Krizhevsky et al. 2012]. Their model outperformed traditional image recognition methods by a large margin. This was a turning point that led to many improvements over recent years, dramatically decreasing errors in predictions.\n\nA similar pattern can be seen in the adoption of Recurrent Neural Networks (RNNs)\u200a\u2014\u200adeep neural networks with cyclic connections, able to store internal states, allowing them to process sequential, temporal data. In the field of speech recognition, hand-crafted Gaussian Mixture Model\u200a\u2014\u200aHidden Markov Model approaches were consistently outperforming other methods, including DL. Once large datasets and computing power became available, RNNs started to outperform GMM-HMM and become more widespread in speech recognition [Hinton et al. 2012][Deng et al. 2013].\n\nAs described above, a significant aspect of the successful application of DL involves the recognition of complex patterns.\n\nRecent research has demonstrated that deep learning approaches are useful beyond classification and recognition tasks. In [Ilya Sutskever, Oriol Vinyals 2014], the authors used DL to translate text. This involved the considerable challenge of dealing with input and output sequences of variable length. The authors used Long Short-Term Memory (LSTM) [Hochreiter and Schmidhuber 1997] RNN, and their approach is known to outperform previously known methods for solving such problems. In [Graves 2013], Alex Graves demonstrated using LSTM networks to generate a variety of sequential outputs including long chunks of text and hand-writing. Further research in the field has shown promise with respect to uses of deep learning with artistic, creative output.\n\nIn [Erhan et al. 2009], the authors were curious about methods of qualitative analysis of network architectures, particularly the effects of varying inputs on specific neuron activity in hidden layers of deep models. They achieved this by using gradient ascent on image classification deep models. With this method they were able to generate inputs which maximised neuron activity on the layers of interest in Stacked Denoising Autoencoders and Deep Belief Networks.\n\nIn 2013 Researchers at Oxford University [Simonyan et al. 2013] used a similar gradient ascent method to generate images that maximise a class score in a convolutional network trained on ImageNet. They used a similar technique to also generate a class saliency map, given an input image and a class.\n\nThe following year, Oxford University researchers [Mahendran and Vedaldi 2014] were frustrated at the lack of knowledge of the internals of image representation used in convolutional networks trained for image classification. They applied gradient ascent to \u2018invert\u2019 a CNN, reconstructing images for the various different hidden layers and neurons. They found that a CNN trained on images, such as ImageNet, stored photographic information on some layers, and other more abstract features such as edges, shapes and gradients. The visual outputs of these methods include abstract\u200a\u2014\u200abut recognisable\u200a\u2014\u200arepresentations of the trained images and classes.\n\nIn 2015, Alexey Dosovitskiy and Thomas Brox devised a new method of \u2018inverting\u2019 CNNs\u2019 to visualise the internal representations of a convolutional neural network by using a second convolutional network [Dosovitskiy and Brox 2015].\n\nAlso in 2015, Anh Nguyen et al were curious how recognition in deep image classification models differed to visual recognition in humans. They used convolutional networks trained on ImageNet and the MNIST handwriting dataset [LeCun et al.], combined with evolutionary algorithms and gradient ascent to generate images that scored highly for specific classes, but that were unrecognisable to humans [Nguyen et al. 2015]. They found that in some cases they could generate images that would satisfy particular classes with 99.99% accuracy by the CNN, but be completely unrecognisable by humans (e.g. detecting a cheetah in white noise, a starfish in wavy lines etc.). Interestingly, they submitted the output images to an art contest, and were among the 21.3% of the submitted artworks selected for exhibition.\n\nAgain in 2015 Google researchers [Mordvintsev et al. 2015] released code for a research they called #DeepDream #Inceptionism, which went viral on social media. They used similar gradient ascent to generate images that maximised activity on particular hidden neurons, but then fed the generated output back into the input to create feedback loops that acted to amplify activity. Combined with image transformations on every iteration, this created endless fractal-like animations and \u2018hallucinations\u2019 of abstract\u200a\u2014\u200abut subtly recognisable\u200a\u2014\u200aimagery. They used their own GoogLeNet convolutional network architecture for this research, details of which can be found in [Szegedy et al. 2014].\n\nAlso in 2015 [Gatys et al. 2015] released similar research that was also highly shared on social networks, called #StyleNet. This research extracts the artistic style of an image\u200a\u2014\u200afor example, a painting by Van Gogh or Edvard Munch, and applies it to another image, such as a photograph. Techniques of applying artistic styles to images have been researched for many years\u200a\u2014\u200aas a subset of nonphotorealistic rendering, a 2013 survey can be found in [Kyprianidis et al. 2013]. However in most cases the algorithms are hand-crafted to resemble each particular style (an exception to this can be seen in [Mital et al. 2013]). In Gatys et al.\u2019s research, the authors found that they could use convolutional neural networks to separate the content and the style of the image, storing different representations for each. Doing so enabled them to apply different transformations, or even mix and match representations from different images\u200a\u2014\u200ae.g. applying the style of Van Gogh\u2019s \u2018Starry Night\u2019 to a photograph. This technique works remarkably well even on very abstract styles such as Mark Rothko, Jackson Pollock, or Piet Mondrian.\n\nSimilar developments have also taken place with sequenced data using recurrent neural networks. In 2015 Andrej Karpathy released an open-source Recurrent Neural Network implementation for training on character level language models called char-rnn [Karpathy 2015a] based on [Graves 2013]. The software takes a single text file, and generates similar text based on character sequence probabilities. This software has been used by a number of people to generate text in the style of Shakespeare, cooking recipes, rap lyrics, Obama speeches, the bible and more [Karpathy 2015b]. This char-rnn library was also used by [Sturm 2015] to generate midi notes in the style of folk music. Due to it being a simple implementation using a text sequence, it is limited to monophonic output. Other examples of composing music using recurrent networks include [Eck and Schmidhuber 2002][Boulanger-Lewandowski et al. 2012].\n\nIn addition, in 2013, DeepMind technologies (recently acquired by Google) developed a system which is capable of learning how to play Atari games simply by observing the images on the screen [Mnih et al. 2013]. Given no prior knowledge of the game rules or controls, with only the screen pixels as input, the system developed strategies within a few days of playing. In some cases these strategies outperformed other AIs, and in other cases even outperformed human players. They achieved this by implementing a deep reinforcement learning algorithm training convolutional networks with Q-learning, an approach they call Deep Q Networks (DQN). In another recent research [Guo et al. 2014], the authors investigated methods of improving on DQN\u2019s performance using Monte Carlo Tree Search methods [Browne and Powley 2012]. Not having access to the internal game-state, the researchers look to train a CNN with offline UCT using the screen pixels as input, and then use the trained CNN at runtime as a policy to select actions. They found that the UCT trained CNN is currently state of the art realtime AI and beat the score of the DQN. However in order to achieve those results there was a very long offline training period.\n\nThese recent developments using deep learning networks to generate images, sounds or actions show incredible potential for the application of deep learning for creative output. There are still many unexplored territories however, and the performance is far from realtime, and not interactive.\n\nIn this section I take a slight digression to acknowledge algorithmic art prior to Deep Learning. An indepth survey is out of scope for this review, but this serves as a brief summary of the area. A more comprehensive review can be found in [Grierson 2005] and [Levin 2000].\n\nThe use of computers for the purposes of making art dates back at least as far as the 1950s and 1960s, most notably with John Whitney\u2019s DIY analog computers built from World War II M5 and M7 targeting computers and anti-aircraft machinery [Alves 2005]. Whitney\u2019s works were not only pioneering technically, leading to the birth of computer graphics and special effects in scenes such as Stanley Kubrick\u2019s 2001: A Space Odyssey, but in addition they pioneered the field of computer-aided audio visual composition [Grierson 2005]. His work continues in the tradition of experimental abstract animators and filmmakers such as Normal McLaren and Oskar Fischinger. He was joined shortly after by software artists such as Paul Brown, Vera Molnar, Manfred Mohr, Frieder Nake, Larry Cuba and many more.\n\nHowever it was Harold Cohen\u2019s AARON software from 1973 [Cohen 1973] which first introduced artificial intelligence into computer art. AARON was ostensibly a piece of software, written to understand colour and form. Cohen often talks about \u2018training\u2019 his software [Cohen 1994]. However he uses the term rhetorically. The \u2018learning\u2019 in AARON is not the machine learning mentioned in previous sections. AARON does not learn by looking at data. Instead, whenever Cohen wants AARON to learn something new, he has to analyse it himself, and implement the sets of rules required to replicate that behaviour. Often these are very complex sets of rules that take years for Cohen himself to learn before he can program them [Cohen 2006].\n\nOther computer graphics artists working with artificial intelligence shortly after include William Latham [Todd and Latham 1992], Karl Sims [Sims 1994], and Scott Draves [Draves 2005]. Inspired by Darwinian evolution by natural selection, these artists primarily explored Evolutionary Algorithms (EA)\u200a\u2014\u200aalso known as Genetic Algorithms (GA)\u200a\u2014\u200ain the creation of algorithmic art, eventually known as evolutionary art. Also during this period, David Cope developed an algorithm for composing music. His \u2018Experiments in Musical Intelligence (EMI)\u2019 began in 1981, and he developed it over the decades until he eventually patented the algorithm \u2018Recombinant music composition algorithm\u2019 [Cope 2010]. Using this algorithm he generated musical sequences in the style of many classical composers and styles, such as Bach, Vivaldi, Beethoven, Mozart, Chopin and Debussy. His latest software \u2018Emily Howell\u2019 has albums being released under its name.\n\nStarting in the 1960s Myron Kruger developed gesturally interactive computer artworks and Responsive Environments, culminating in his seminal Artificial Reality environment \u2018Videoplace\u2019 [Krueger et al. 1985]. Videoplace tracked users with cameras, enabling them to interact with virtual objects in the scene using projectors. First developed in 1986, David Rokeby\u2019s \u2018Very Nervous System\u2019 explores similar themes of gestural full body interaction\u200a\u2014\u200ausing hand built cameras\u200a\u2014\u200ain this case to generate music [Rokeby 1986]. Other notable artists working with similar ideas in this era include Ed Tannenbaum, Scott Snibbe, Michael Naimark, Golan Levin and Camille Utterback.\n\nWith the introduction of creative coding tools and open-source communities there has been exponential growth in this field over the last decades. Some of these tools which have global communities include Processing, openFrameworks, Cinder, vvvv, Max/MSP/Jitter, PureData, SuperCollider, TouchDesigner, QuartzComposer, Three.js and many smaller bespoke ones.\n\nThis process driven creative art form can be traced back to a wider rule-based generative art movement that includes composers such as Steve Reich, John Cage, Terry Riley, Brian Eno and artists such as Sol Lewitt and Nam June Paik.\n\nA sub-field of AI which is related to this area is Computational Creativity. Whereas AI questions whether a machine can think, or exhibit intelligent behaviour [Turing 1950], Computational Creativity questions whether a machine can be creative, or exhibit creative behaviour.\n\nComputational Creativity research is not only concerned with the creative output of the algorithms or technical implementation details, but is equally\u200a\u2014\u200aif not more\u200a\u2014\u200aconcerned with the philosophical, cognitive, psychological and semantic connotations of machines exhibiting creative behaviour, or acting creative. In [McCormack and d\u2019Inverno 2012] and related papers [McCormack and D\u2019Inverno 2014], the authors ask\u200a\u2014\u200aand attempt to answer\u200a\u2014\u200aquestions regarding computers and creativity, creative agency and the role of creative tools.\n\nAs part of the philosophical angle of computational creativity research, there is often an emphasis on fully autonomous systems. The field includes research into software which exhibits intentionality, and is able to justify the decisions it makes when creating a piece of work by framing information in the context of the work [Colton and Wiggins 2012]. This can be thought of as analogous to an artist making deliberate, purposeful decisions at every step of the creative process. This aspect of computational creativity is sometimes referred to as strong computational creativity [Al-rifaie and Bishop 2015]\u200a\u2014\u200aanalogous to John Searle\u2019s strong (vs weak) AI [Searle 1980]. The field is also accompanied by certain formalisms, proposed models and theories of creativity to ensure the systems\u2019 behaviour complies with what is thought to be \u2018creative behaviour\u2019 [Colton et al. 2011].\n\nWithin this context there has been research into systems that conceive fictional concepts [Cavallo et al. 2013], design video games [Cook et al. 2014], write poetry [Colton et al. 2012], and other inherently \u2018creative\u2019 tasks.\n\nNB: While the algorithmic techniques used for content generation in Computational Creativity is within the scope of my research, the formalisms and models of creativity are not. It can be said that my research is interested in weak computational creativity\u200a\u2014\u200aparticularly semi-autonomous, collaborative creativity where human interaction in the content creation process is not only relevant, it is essential\u200a\u2014\u200ato create interactive systems where the human user can guide the computationally creative system in realtime.\n\nIn the previous sections I reviewed a non-exhaustive range of relevant literature in areas of algorithmic image and sound generation, ranging from simple algorithms to the latest developments in deep learning. Some of these have been non-realtime, for example where content is generated through the application of deep learning, while others have been realtime, even interactive. This section will cover Artistic Expressive Human Computer Interaction (AEHCI)\u200a\u2014\u200aHuman Computer Interaction for artistic expression.\n\nIn [Dourish 2001] Paul Dourish proposes new models for interactive system design. Embodied Interaction is interaction embodied in the environment, not just physically, but as a fundamental component of the setting. It is an interaction design that takes into consideration the ways we experience the everyday world. This philosophy is particularly applicable when designing gestural interfaces for artistic expression.\n\nAs mentioned before, Myron Kruger was also interested in exploring Responsive Environments, in which \u2018interaction is a central, not peripheral issue\u2019 [Krueger et al. 1985]. He saw potential in this area for the arts, education, telecommunications as well as general human-machine interaction and was motivated by creating playful environments which explore the perceptual process we use to navigate the physical world.\n\nHuman computer interaction for music\u200a\u2014\u200aor Musician-Computer Interaction (MCI) [Gillian 2011]\u200a\u2014\u200ais one of the more academically established fields related to AEHCI, more so than gestural human computer interaction for visual composition. However, many of the requirements for interaction design and particularly gesture recognition are similar. Both require low-latency, realtime systems that can be configured on-the-fly. They need to be capable of detecting a wide range of gestures, some AEHCI systems might concentrate on subtle finger movements, while others track whole bodies of multiple people. Furthermore, the ability to detect and respond to subtle variations in gestures is essential to convey expressivity [Caramiaux et al. 2014]. Also, in performance situations, gesture recognition need not be generalized across different people, but training can be specific to the performing individual to maximise personal expression [Gillian 2011].\n\nDue to these similarities, in this research Musician-Computer Interaction is taken as a base model for expressive gestural interaction, and will be built on for general AEHCI.\n\nA survey of definitions of gesture, especially in relation to music can be found in [Cadoz and Wanderley, 2000]. The authors conclude that the many proposed definitions do not adapt to gesture in music, but they purposefully avoid providing a new definitions, focusing instead on which aspects of the various definitions might apply.\n\nIn [Camurri et al., 2004] the authors define Expressive Gesture as \u201cresponsible of [sic] the communication of information that we call expressive content\u201d where \u201cExpressive content concerns aspects related to feelings, moods, affect, intensity of emotional experience\u201d. This is the definition of Expressive Gesture that is used in this research, complemented with the \u201cnatural, spontaneous gestures made when a person is telling a story\u201d, as described in [Cassell and Mcneill, 1991]. Particularly those with the semiotic classification of metaphoric, indicating abstract ideas [McNeill and Levy, 1980]. A wider study of gesture expressivity and its dimensions\u200a\u2014\u200aespecially in the context of musical performance and human computer interaction can be found in [Caramiaux, 2015].\n\nThis research is not concerned with detecting emotion within gesture as in [Cowie et al., 2001] or [Zeng et al., 2009]. Instead it is concerned with finding correlations between various parameters of a gesture, and parameters of the generative output model. It will map expressive gesture to trained models of artistic content synthesis and manipulation. Inspired by research in embodied cognition and the relationship between action and perception [Kohler et al., 2002, Metzinger and Gallese, 2003, Leman, 2007], in [Caramiaux et al., 2009] the authors investigate similar relationships by analysing motion capture data of participants performing free hand movements while listening to short sound samples.\n\nOne of the significant challenges in executing gestural interaction is reading relevant information from the user\u200a\u2014\u200arecognising their positions, movements and gestures. A further challenge is extrapolating their motivations and intentions from those gestures.\n\nThere are many hardware devices and sensors which can support this process: accelerometers and inertial measurement units (IMU), myoelectric sensors, ultrasonic and infra-red range finders, 2d cameras / depth cameras / computer vision (CV), radar, lidar etc. Surveys of gesture recognition technology and research can be found in [Gillian 2011] and [LaViola Jr. 2013].\n\nMy research does not investigate new modes of sensing. It focuses on emerging consumer technology, and applications of algorithmic image and sound synthesis and manipulation within that context. This is to remain applicable to hardware and environments that relate to commercial gaming and mainstream use. Primarily this will involve depth cameras similar to Microsoft\u2019s Kinect 2 and LeapMotion, as well computer vision with traditional 2D cameras.\n\nFurthermore, there is an increasing trend in consumer devices to combine multiple sensors for more varied data, contributing to higher accuracy in estimations of pose and movement. Past examples of this include Nintendo\u2019s Wiimote controller combining accelerometer with infrared sensor (and an additional gyroscope with the MotionPlus addon ), and Sony\u2019s PSMove controller combining a 6-axis IMU with magnetometer and high speed computer vision. Both controllers also feature buttons and D-pad as well as vibration based haptic feedback. Microsoft\u2019s Kinect also combines an RGB camera, IR camera / depth sensor, microphone array, and accelerometer (to detect device orientation) into a single, affordable consumer device.\n\nNext generation devices are combining increasing numbers of sensors. Project Tango by Google\u2019s Advanced Technology And Projects Group (ATAP) [Google Advanced Technology And Projects 2014] is an example of this trend. Project Tango is a next generation mobile device with a depth sensor, motion tracking camera and 9-axis IMU enabling it to calculate its position and orientation in space, while simultaneously scanning and building a 3D map of its environment. A very recent research project from a different team at the same group have announced Project Soli [Google Advanced Technology And Projects 2015]. Project Soli uses radar to track hand and finger movements at sub-millimetric precision and high speed, enabling natural, intuitive interfaces for small wearable devices.\n\nThese are all examples of the kinds of devices that will be used for this research. But such devices share a common problem: extracting meaningful information from the data for gesture recognition is a challenging task. Currently, machine learning is a very popular and successful research area in this field.\n\nGestural interaction (and gesture recognition) is a very broad field. This section will focus on applications within AEHCI, particularly in context of machine learning. Wider surveys can be found in [Mitra and Acharya 2007][Gillian 2011] and [LaViola Jr. 2013].\n\nArtificial Neural Networks (ANN) are particularly useful for AEHCI as they are able to map m-dimensional input vectors to n-dimensional output vectors with a learned non-linear function, allowing them to control complex parameter-sets simultaneously. This is especially useful in regression tasks, when manipulating continuous parameters of a generative visual or sonic model. They can be equally successful in classification tasks, for recognising gestures and triggering desired visual or sonic outputs. In 1992 Michael Lee et al used ANNs inside the MAX/MSP musical programming environment to investigate adaptive user interfaces for realtime musical performance [Lee et al. 1992]. They were able to successfully recognise gestures from a number of devices including a radio baton and a continuous space controller. In 1993, Sidney Fels and Geoffrey Hinton used ANNs to map hand movements captured via a data-glove, to a speech synthesiser [Fels and Hinton 1993]. They achieved realtime results with a vocabulary of 203 gestures-to-words demonstrating the potential of neural networks for adaptive interfaces. Now, with many open-source implementations, and also integrated into Rebecca Fiebrink\u2019s Wekinator and Nick Gillian\u2019s GRTGui, ANNs are widely used for creative gestural interaction.\n\nMany other machine learning techniques have been used for gesture recognition, with different specific use cases. These include K-Nearest Neighbour, Gaussian Mixture Models, Random Forests, Adaptive Na\u00efve Bayes Classifiers and Support Vector Machines to classify static data; Dynamic Time Warping and Hidden Markov Models can be used to classify temporal gestures; Linear Regression, Logistic Regression and Multivariate Linear Regression can be used for real-valued outputs as opposed to classifying the input. A survey of machine learning techniques and applications for musical gesture recognition can be found in [Caramiaux and Tanaka 2013].\n\nAs mentioned previously, in an artistic, performative context, detecting subtle variations of gestures is vital to conveying expressivity. In [Bevilacqua and Muller 2005][Bevilacqua et al. 2009], Bevilacqua et al design continuous gesture followers that allow temporal gesture recognition in realtime while the gesture is still being performed. This algorithm returns time progression information and likelihood, enabling performers to alter speed and accuracy of the gesture to control parameters of their generative model.\n\nIn [Caramiaux et al. 2014][Caramiaux 2015] Caramiaux et al develop systems that go beyond classification of the gestures, to characterise the qualities of the gesture\u2019s execution. They use computational adaptive models for identifying temporal, geometric and dynamic variations on the trained gesture. Returning this information in realtime to the performer as they are executing gestures, enables the performer to map the variations to parameters such as time-stretching samples, modulations, and volume or custom synth parameters.\n\nIn [Kiefer 2014] Kiefer investigates the use of Echo State (Recurrent Neural) Networks (ESN) as mapping tools, to learn sequences of input gestures, and non-linearly map them to multi-parameter output sequences. The research concludes that ESNs demonstrate good potential in pattern classification, multi-parametric control, explorative and nonlinear mapping, but there is room for improvement to produce more accurate results in some cases.\n\nAs discussed above, ML is a very successful technique for pattern and gesture recognition. However using machine learning can be difficult because of the technical knowledge and time required in building classifiers and setting up the signal processing pipeline [Fails et al. 2003].\n\nInteractive Machine Learning (IML) is a field which looks at the process of using machine learning, through the lens of human computer interaction research [Fiebrink 2011].\n\nWhile ML brings huge advancements to the fields of data analysis and pattern recognition, IML seeks to improve how ML systems can be used. Particularly, expanding its user-base from dedicated computer scientists and closely related disciplines, to a much wider audience. One of the ways in which this is made possible is via a Graphic User Interface (GUI) front end to a ML backend, with data streamed live to and from the ML backend. The training and predictions can be performed in realtime, without writing any code making it a perfect choice for performance and AEHCI.\n\nRebecca Fiebrink et al\u2019s previously mentioned Wekinator software released in 2009 is an example of such an IML system aimed at musical performance [Fiebrink et al. 2009]. Using a GUI, users are able to setup, train and modify parameters of an ANN. The software also allows other applications\u200a\u2014\u200asuch as existing music software, visual software, or other custom generative software\u200a\u2014\u200ato stream data to the Wekinator using a UDP-based protocol commonly used in inter-app and inter-device communications called Open Sound Control (OSC) [Wright and Freed 1997]. As the Wekinator receives this data, it runs it through a machine learning model and streams back predictions in realtime. Wekinator also has a number of built-in sensor input and feature extraction capabilities such as edge detection from a webcam. Using this tool, artists, musicians, dancers, performers and researchers from other fields can train and map gestures to arbitrary outputs, such as notes, effects, images and sounds; with no programming or need for any other computer vision software.\n\nNick Gillian\u2019s Gesture Recognition Toolkit (GRT) from 2011 [Gillian 2011] provides similar functionality but with more emphasis on the signal processing / gesture recognition pipeline. It lacks built-in input functionality such as webcam or microphone inputs, but has a number of built-in pre-processing, feature extraction and post-processing algorithms. Examples for these are Fast Fourier Transform, Principal Component Analysis, various filters, derivatives, dead zones and more. In addition to being an open-source application, the underlying codebase is released as a C++ framework allowing it to be integrated into bespoke applications.\n\nRecently NVIDIA released a similar GUI based application\u200a\u2014\u200aDeep Learning GPU Training System (DIGITS)\u200a\u2014\u200aallowing researchers to use deep learning in a similar interactive fashion [NVIDIA 2015]. The software uses the popular open-source deep learning framework Caffe [Jia et al. 2014], and is designed to take full advantage of GPU acceleration, scaling up automatically in multi-GPU systems.\n\nIn 2015, during my research I required a similar Multi-Model IML system. One in which I could dynamically create and train new models while leaving existing models intact. I also needed to be able to access multiple models simultaneously, feeding each model different inputs and receiving the associated predictions. For this I developed msaOscML [Akten 2015a]. msaOscML is a Multi-Model Interactive Machine Learning tool. It is inspired by and similar to Rebecca Fiebrink\u2019s Wekinator and Nick Gillian\u2019s GRT. However, while those tools are aimed at a non-technical audience, and have a user- friendly Graphical User Interface (GUI), msaOscML is currently aimed at creative developers who would like to add machine learning capabilities to their creative software suite, with a focus on self-running installations and performances where they would like to minimise hands-on operation of the software. For this reason msaOscML has no GUI. It runs in the background as a server with only a console to indicate status and provide visual feedback to the user if desired. Similar to Wekinator and GRT, it can be interacted with (input and output) via the Open Sound Control (OSC) protocol (Wright & Freed, 1997). The main purpose of msaOscML, and difference over Wekinator and GRT, is that it can not only train and predict multiple independent models simultaneously, but also create, save and load multiple models directly via its OSC interface without requiring a human operator at any point. This allows a host application (e.g. software such as Max MSP, Ableton Live, or custom software generating sound or visuals) to be able to control multiple models directly, and the system can be unmanned throughout a long performance or installation. Cross platform and written in C++, msaOscML uses an abstraction layer for ML implementation, which I refer to as the Machine Learning Implementation Abstraction Layer (MLIAL). This allows different machine learning libraries to plug in as a backend with a minimal MLIAL wrapper. Currently there are MLIAL wrappers for Gillian\u2019s GRT framework, and Steffen Nissen\u2019s Fast Artificial Neural Network Library (FANN) (Nissen, 2003). msaOscML was written for and used on an R&D interactive dance project called Pattern Recognition (Akten, 2015c).\n\nTools like these enable both technical and non-technical users to quickly setup, train and test models for gesture recognition and gestural interaction. Without writing any code, users can start streaming input data from their sensors, and receive predictions in their application of choice, enabling them to gesturally create, manipulate and perform audio-visual content in realtime. An example of Fiebrink\u2019s Wekinator can be seen in the band 000000Swan\u2019s audio-visual shows gesturally driven using a Microsoft Kinect and commercially available sensor bow [Schedel et al. 2011]. In addition, it has also been applied in contexts such as workshops with people with learning and physical disabilities [Katan et al. 2015].\n\nThe field of algorithmically generating images and sound is a very rich and well-established field. Expressive interaction\u200a\u2014\u200aparticularly for music\u200a\u2014\u200ais also well-established with new advanced techniques emerging as the field is maturing. Deep learning is going through an almost revolutionary revival with many recent developments. The gaming, entertainment and media industries are converging as next generation multi-modal interaction and virtual, augmented and mixed reality technologies are becoming mainstream.\n\nWithin this context, there are still many unexplored territories with a lot of artistic potential, especially at the intersections of these trends. These include ways of generating content using deep learning, particularly in realtime and interactively. Also applications of expressive interactions to the generation of the content, particularly with next generation consumer devices set in mixed reality environments.\n\nAKTEN, M. 2015a. msaOscML. https://github.com/memo/msaOscML.\u00a0\n\nAKTEN, M. 2015b. \u2018Pattern Recognition\u2019 Dance Performance. http://www.memo.tv/pattern-recognitionwip/.\u00a0\n\nAKTEN, M., STEEL, B., MCNICHOLAS, R., ET AL. 2011. Sony PlayStation VideoStore Mapping.\u00a0.\u00a0\n\nAL-RIFAIE, M.M. AND BISHOP, J.M. 2015. Weak and Strong Computational Creativity. In: Computational Creativity Research: Towards Creative Machines. 0\u201314.\u00a0\n\nALVES, B. 2005. Digital Harmony of Sound and Light. Computer Music Journal 29, 4, 45\u201354.\u00a0\n\nBENGIO, Y., COURVILLE, A., AND VINCENT, P. 2013. Representation Learning: A Review and New Perspectives. Tpami 1993, 1\u201330.\u00a0\n\nBEVILACQUA, F. AND MULLER, R. 2005. A gesture follower for performing arts. Proceedings of the International Gesture\u00a0\u2026, 3\u20134.\u00a0\n\nBEVILACQUA, F., ZAMBORLIN, B., SYPNIEWSKI, A., SCHNELL, N., GU\u00c9DY, F., AND RASAMIMANANA, N. 2009. Continuous realtime gesture following and recognition. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 5934 LNAI, 73\u201384.\u00a0\n\nBOULANGER-LEWANDOWSKI, N., VINCENT, P., AND BENGIO, Y. 2012. Modeling Temporal Dependencies in HighDimensional Sequences: Application to Polyphonic Music Generation and Transcription. Proceedings of the 29th International Conference on Machine Learning (ICML-12) Cd, 1159\u20131166.\u00a0\n\nBROWNE, C. AND POWLEY, E. 2012. A survey of monte carlo tree search methods. Intelligence and AI 4, 1, 1\u201349.\u00a0\n\nCADOZ, C. AND WANDERLEY, M. 2000. Gesture-music. Trends in gestural control of music, 71\u201394.\u00a0\n\nCAMURRI, A., MAZZARINO, B., RICCHETTI, M., TIMMERS, R., AND VOLPE, G. 2004. Multimodal analysis of expressive gesture in music and dance performances. In: Gesture-based {C}ommunication in {H}uman-{C}omputer {I}nteraction, {LNAI} 2915. 20\u201339.\u00a0\n\nCARAMIAUX, B. 2015. Motion Modeling for Expressive Interaction A Design Proposal using Bayesian Adaptive Systems. International Workshop on Movement and Computing (MOCO), IRCAM.\u00a0\n\nCARAMIAUX, B., BEVILACQUA, F., AND SCHNELL, N. 2009. Towards a gesture-sound cross-modal analysis. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 5934 LNAI, 158\u2013170.\u00a0\n\nCARAMIAUX, B., DONNARUMMA, M., AND TANAKA, A. 2015. Understanding Gesture Expressivity through Muscle Sensing. ACM Transactions on Computer-Human Interaction 0, 0, 1\u201327.\u00a0\n\nRealtime image & sound synthesis & expressive manipulation using DL & RL in responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\u00a0\n\n21\u00a0\n\nCARAMIAUX, B., MONTECCHIO, N., TANAKA, A., AND BEVILACQUA, F. 2014. Adaptive Gesture Recognition with Variation Estimation for Interactive Systems. ACM Transactions on Interactive Intelligent Systems (TiiS) (In Press) V, 212.\u00a0\n\nCARAMIAUX, B. AND TANAKA, A. 2013. Machine Learning of Musical Gestures. Proceedings of the International Conference on New Interfaces for Musical Expression, 513\u2013518.\u00a0\n\nCASSELL, J. AND MCNEILL, D. 1991. Gesture and the Poetics of Prose. Poetics Today 12, 3, 375\u2013404.\u00a0\n\nCAVALLO, F., PEASE, A., GOW, J., AND COLTON, S. 2013. Using Theory Formation Techniques for the Invention of Fictional Concepts. 176\u2013183.\u00a0\n\nCIRESAN, D., MEIER, U., AND MASCI, J. 2011. Flexible, high performance convolutional neural networks for image classification. International Joint Conference on Artificial Intelligence, 1237\u20131242.\u00a0\n\nCOHEN, H. 1973. Parallel to perception: some notes on the problem of machine-generated art. Computer Studies, 1\u201310.\u00a0\n\nCOHEN, H. 1994. The Further Exploits of Aaron, Painter.\u00a0.\u00a0\n\nCOHEN, H. 2006. AARON, Colorist: from Expert System to Expert.\u00a0.\u00a0\n\nCOLLOBERT, R., WESTON, J., BOTTOU, L., KARLEN, M., KAVUKCUOGLU, K., AND KUKSA, P. 2011. Natural language processing (almost) from scratch. The Journal of Machine Learning Research 1, 12, 2493\u20132537.\u00a0\n\nCOLTON, S., GOODWIN, J., AND VEALE, T. 2012. Full-FACE Poetry Generation. Proceedings of the Third International Conference on Computational Creativity (ICCC\u201912), 95\u2013102.\u00a0\n\nCOLTON, S., PEASE, A., AND CHARNLEY, J. 2011. Computational creativity theory: The FACE and IDEA descriptive models. Proceedings of the Second International Conference on Computational Creativity, 90\u201395.\u00a0\n\nCOLTON, S. AND WIGGINS, G. A. 2012. Computational creativity: The final frontier? Frontiers in Artificial Intelligence and Applications 242, 21\u201326.\u00a0\n\nCOOK, M., COLTON, S., AND GOW, J. 2014. Automating Game Design In Three Dimensions. AISB Symposium on AI and Games, 3\u20136.\u00a0\n\nCOPE, D.H. 2010. Recombinant music composition algorithm and method of using the same.\u00a0.\u00a0\n\nCOUPRIE, C., NAJMAN, L., AND LECUN, Y. 2013. Learning Hierarchical Features for Scene Labeling. Pattern Analysis and Machine Intelligence, IEEE Transactions on 35, 8, 1915\u20131929.\u00a0\n\nCOWIE, R., DOUGLAS-COWIE, E., TSAPATSOULIS, N., ET AL. 2001. Emotion recognition in human-computer interaction. Signal Processing Magazine, IEEE 18, 1, 32\u201380.\u00a0\n\nRealtime image & sound synthesis & expressive manipulation using DL & RL in responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\u00a0\n\n22\u00a0\n\nCRUZ-NEIRA, C., SANDIN, D., AND DEFANTI, T. 1993. Surround-screen projection-based virtual reality: the design and implementation of the CAVE.\u00a0\u2026 of the 20Th Annual Conference on\u00a0\u2026, 135\u2013142.\u00a0\n\nDENG, J.D.J., DONG, W.D.W., SOCHER, R., LI, L.-J.L.L.-J., LI, K.L.K., AND FEI-FEI, L.F.-F.L. 2009. ImageNet: A large-scale hierarchical image database. 2009 IEEE Conference on Computer Vision and Pattern Recognition, 2\u20139.\u00a0\n\nDENG, L., HINTON, G., AND KINGSBURY, B. 2013. New Types of Deep Neural Network Learning for Speech Recognition and Related Applications\u00a0: an Overview. 8599\u20138603.\u00a0\n\nDOSOVITSKIY, A. AND BROX, T. 2015. Inverting Convolutional Networks with Convolutional Networks. 1\u201315.\u00a0\n\nDOURISH, P. 2001. Where the Action Is: The Foundations of Embodied Interaction. Where the action is the foundations of embodied interaction 36, 233. http://books.google.com/books?id=DCIy2zxrCqcC&pgis=1.\u00a0\n\nDRAVES, S. 2005. The Electric Sheep screen-saver: A case study in aesthetic evolution. Proc. EvoMUSART, 458\u2013467.\u00a0\n\nECK, D. AND SCHMIDHUBER, J. 2002. A first look at music composition using lstm recurrent neural networks. Istituto Dalle Molle Di Studi Sull Intelligenza\u00a0\u2026.\u00a0\n\nERHAN, D., BENGIO, Y., COURVILLE, A., AND VINCENT, P. 2009. Visualizing higher-layer features of a deep network. Bernoulli 1341, 1\u201313.\u00a0\n\nFAILS, J.A., OLSEN, JR., D.R., AND OLSEN, D.R. 2003. Interactive Machine Learning. Proceedings of the 8th International Conference on Intelligent User Interfaces, ACM, 39\u201345.\u00a0\n\nFELS, S.S. AND HINTON, G.E. 1993. Glove-talk: a neural network interface between a data-glove and a speech synthesizer. IEEE Transactions on Neural Networks 4, 1, 2\u20138.\u00a0\n\nFIEBRINK, R., TRUEMAN, D., AND COOK, P.R. 2009. A metainstrument for interactive, on-the-fly machine learning. Proc. NIME 2, 3.\u00a0\n\nFIEBRINK, R.A. 2011. Real-time Human Interaction with Supervised Learning Algorithms for Music Composition and Performance. Imagine January, 376.\u00a0\n\nGATYS, L. A., ECKER, A.S., AND BETHGE, M. 2015. A Neural Algorithm of Artistic Style. 3\u20137.\u00a0\n\nGILLIAN, N.E. 2011. Gesture Recognition for Musician Computer Interaction. Social Sciences March.\u00a0\n\nGOOGLE ADVANCED TECHNOLOGY AND PROJECTS. 2014. Project Tango. https://www.google.com/atap/projecttango/.\u00a0\n\nGOOGLE ADVANCED TECHNOLOGY AND PROJECTS. 2015. Project Soli. https://www.google.com/atap/projectsoli/.\u00a0\n\nRealtime image & sound synthesis & expressive manipulation using DL & RL in responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\u00a0\n\n23\u00a0\n\nGRIERSON, M. 2005. Audiovisual composition. http://www.strangeloop.co.uk/Dr. M.Grierson\u200a\u2014\u200aAudiovisual Composition Thesis.pdf.\u00a0\n\nGUO, X., SINGH, S., LEE, H., LEWIS, R., AND WANG, X. 2014. Deep Learning for Real-Time Atari Game Play Using Offline Monte-Carlo Tree Search Planning. Advances in Neural Information Processing Systems (NIPS) 27 2600, 3338\u20133346.\u00a0\n\nGUZELLA, T.S. AND CAMINHAS, W.M. 2009. A review of machine learning approaches to Spam filtering. Expert Systems with Applications 36, 7, 10206\u201310222.\u00a0\n\nHINTON, G., DENG, L., YU, D., ET AL. 2012. Deep Neural Networks for Acoustic Modeling in Speech Recognition. Ieee Signal Processing Magazine November, 82\u201397.\u00a0\n\nHOCHREITER, S. AND SCHMIDHUBER, J. 1997. Long short-term memory. Neural computation 9, 8, 1735\u201380.\u00a0\n\nILYA SUTSKEVER, ORIOL VINYALS, Q.V. LE. 2014. Sequence to Sequence Learning with Neural Networks. Nips, 1\u20139.\u00a0\n\nJIA, Y., SHELHAMER, E., DONAHUE, J., ET AL. 2014. Caffe\u00a0: Convolutional Architecture for Fast Feature Embedding. arXiv preprint arXiv:1408.5093.\u00a0\n\nJONES, B., SHAPIRA, L., SODHI, R., ET AL. 2014. RoomAlive: Magical Experiences Enabled by Scalable, Adaptive Projector-camera Units. Proceedings of the 27th annual ACM symposium on User interface software and technology\u200a\u2014\u200aUIST \u201914, 637\u2013644.\u00a0\n\nJONES, B.R., BENKO, H., OFEK, E., AND WILSON, A.D. 2013. IllumiRoom: peripheral projected illusions for interactive experiences. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems\u200a\u2014\u200aCHI \u201913, 869.\u00a0\n\nKARPATHY, A. 2015a. char-rnn. https://github.com/karpathy/char-rnn.\u00a0\n\nKARPATHY, A. 2015b. The Unreasonable Effectiveness of Recurrent Neural Networks.\u00a0.\u00a0\n\nKARPATHY, ANDREJ, L.F.-F. 2015. Deep Visual-Semantic Alignments for Generating Image Descriptions. Cvpr.\u00a0\n\nKATAN, S., GRIERSON, M., AND FIEBRINK, R. 2015. Using Interactive Machine Learning to Support Interface Development Through Workshops with Disabled People. CHI \u201915 Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems.\u00a0\n\nKIEFER, C. 2014. Musical Instrument Mapping Design with Echo State Networks. Proceedings of the International Conference on New Interfaces for Musical Expression, 293\u2013298.\u00a0\n\nKOHLER, E., KEYSERS, C., UMILT\u00c0, M.A., FOGASSI, L., GALLESE, V., AND RIZZOLATTI, G. 2002. Hearing sounds, understanding actions: action representation in mirror neurons. Science (New York, N.Y.) 297, 5582, 846\u2013848.\u00a0\n\nRealtime image & sound synthesis & expressive manipulation using DL & RL in responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\u00a0\n\n24\u00a0\n\nKRIZHEVSKY, A., SUTSKEVER, I., AND HINTON, G.E. 2012. ImageNet Classification with Deep Convolutional Neural Networks. Advances In Neural Information Processing Systems, 1\u20139.\u00a0\n\nKRUEGER, M.W., GIONFRIDDO, T., AND HINRICHSEN, K. 1985. VIDEOPLACE\u200a\u2014\u200a-an artificial reality. ACM SIGCHI Bulletin 16, 4, 35\u201340.\u00a0\n\nKYPRIANIDIS, J.E., COLLOMOSSE, J., WANG, T., AND ISENBERG, T. 2013. State of the \u2019Art: A taxonomy of artistic stylization techniques for images and video. IEEE Transactions on Visualization and Computer Graphics 19, 5, 866\u2013885.\u00a0\n\nLAVIOLA JR., J.J. 2013. 3D Gestural Interaction: The State of the Field. ISRN Artificial Intelligence 2013 2013, 2, 1\u201318.\u00a0\n\nLE, Q. V., RANZATO, M.A., MONGA, R., ET AL. 2011. Building high-level features using large scale unsupervised learning. International Conference in Machine Learning, 38115.\u00a0\n\nLEAPMOTION. LeapMotion. https://www.leapmotion.com/.\u00a0\n\nLECUN, Y. 2012. Learning invariant feature hierarchies. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 7583 LNCS, PART 1, 496\u2013505.\u00a0\n\nLECUN, Y. 2014. The Unreasonable Effectiveness of Deep Learning. Facebook AI Research & Center for Data Science, NYU.\u00a0\n\nLECUN, Y. AND BENGIO, Y. 1995. Convolutional networks for images, speech, and time series. The handbook of brain theory and neural networks 3361, 255\u2013258.\u00a0\n\nLECUN, Y., BOSER, B., DENKER, J.S., ET AL. 1989. Backpropagation Applied to Handwritten Zip Code Recognition. Neural Computation 1, 541\u2013551.\u00a0\n\nLECUN, Y., BOTTOU, L., BENGIO, Y., AND HAFFNER, P. 1998. Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 11, 2278\u20132323.\u00a0\n\nLECUN, Y., CORTES, C., AND BURGES, C.J.C. The MNIST Database. http://yann.lecun.com/exdb/mnist/index.html.\u00a0\n\nLECUN, Y., HUANG, F.J.H.F.J., AND BOTTOU, L. 2004. Learning methods for generic object recognition with invariance to pose and lighting. Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004. 2.\u00a0\n\nLECUN, Y., JACKEL, L., BOTTOU, L., ET AL. 1995. Comparison of learning algorithms for handwritten digit recognition. International Conference on artificial neural networks, 53\u201360.\u00a0\n\nLEE, M., FREED, A., AND WESSEL, D. 1992. Neural networks for simultaneous classification and parameter estimation in musical instrument control. Proceedings of SPIE 1706, 244\u2013255.\u00a0\n\nRealtime image & sound synthesis & expressive manipulation using DL & RL in responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\u00a0\n\n25\u00a0\n\nLEMAN, M. 2007. Embodied Music Cognition and Mediation Technology.\u00a0.\u00a0\n\nLEVIN, G. 2000. Painterly Interfaces for Audiovisual Performance. Media, 1\u2013151.\u00a0\n\nMAHENDRAN, A. AND VEDALDI, A. 2014. Understanding Deep Image Representations by Inverting Them.\u00a0.\u00a0\n\nMCCORMACK, J. AND D\u2019INVERNO, M. 2012. Computers and Creativity: The Road Ahead. Computers and Creativity, 421\u2013424.\u00a0\n\nMCCORMACK, J. AND D\u2019INVERNO, M. 2014. On the Future of Computers and Creativity.\u00a0.\u00a0\n\nMCNEILL, D. AND LEVY, E. 1980. Conceptual representations in language activity and gesture.\u00a0.\u00a0\n\nMETZINGER, T. AND GALLESE, V. 2003. The emergence of a shared action ontology: Building blocks for a theory. Consciousness and Cognition, 549\u2013571.\u00a0\n\nMISTRY, P. AND MAES, P. 2009. SixthSense: a wearable gestural interface. ACM SIGGRAPH ASIA 2009 Sketches, ACM.\u00a0\n\nMITAL, P.K., GRIERSON, M., AND SMITH, T.J. 2013. Corpus-based visual synthesis. Proceedings of the ACM Symposium on Applied Perception\u200a\u2014\u200aSAP \u201913 July, 51\u201358.\u00a0\n\nMITCHELL, T.M. 1997. Machine Learning. McGraw Hill.\u00a0\n\nMITRA, S. AND ACHARYA, T. 2007. Gesture Recognition\u00a0: A Survey. IEEE Transactions On Systems, Man, And Cybernetics\u200a\u2014\u200aPart C: Applications And Reviews 37, 3, 311\u2013324.\u00a0\n\nMNIH, V., HEESS, N., GRAVES, A., AND KAVUKCUOGLU, K. 2014. Recurrent Models of Visual Attention. Nips, 1\u2013 12.\u00a0\n\nMNIH, V., KAVUKCUOGLU, K., SILVER, D., ET AL. 2013. Playing Atari with Deep Reinforcement Learning. arXiv preprint arXiv:\u00a0\u2026, 1\u20139.\u00a0\n\nMORDVINTSEV, A., OLAH, C., AND TYKA, M. 2015. Deepdream inceptionism. http://googleresearch.blogspot.ch/2015/06/inceptionism-going-deeper-into-neural.html.\u00a0\n\nNG, A. 2013. Machine Learning and AI via Brain Simulations. Stanford University.\u00a0\n\nNGUYEN, A, YOSINSKI, J., AND CLUNE, J. 2015. Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images. Cvpr 2015.\u00a0\n\nNISSEN, S. 2003. Fast Artificial Neural Network Library. http://leenissen.dk/fann/wp/.\u00a0\n\nNVIDIA. 2015. Deep Learning GPU Training System (DIGITS). https://developer.nvidia.com/digits/.\u00a0\n\nRAINA, R., MADHAVAN, A., AND NG, A.Y. 2009. Large-scale deep unsupervised learning using graphics processors. Icml 9, 873\u2013880.\u00a0\n\nRealtime image & sound synthesis & expressive manipulation using DL & RL in responsive environments. Memo Akten, IGGI, Literature Review 30/09/2015\u00a0\n\n26\u00a0\n\nROKEBY, D. 1986. Very Nervous System.\u00a0.\u00a0\n\nSCHEDEL, M., FIEBRINK, R., AND PERRY, P. 2011. Wekinating 000000Swan\u00a0: Using Machine Learning to Create and Control Complex Artistic Systems. Proceedings of the International Conference on New Interfaces for Musical Expression June, 453\u2013456.\u00a0\n\nSCHMIDHUBER, J. 2014. Deep Learning in Neural Networks: An Overview. arXiv preprint arXiv:1404.7828, 1\u201366.\u00a0\n\nSEARLE, J.R. 1980. Minds, Brains, and Programs. Behavioral and Brain Sciences 3, 1\u201319.\u00a0\n\nSIMONYAN, K., VEDALDI, A., AND ZISSERMAN, A. 2013. Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. arXiv preprint arXiv:1312.6034, 1\u20138.\u00a0\n\nSIMS, K. 1994. Evolving virtual creatures. Siggraph \u201994 SIGGRAPH \u2019, July, 15\u201322.\u00a0\n\nSTURM, B. 2015. Recurrent Neural Networks for Folk Music Generation. https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folkmusic-generation.\u00a0\n\nSZEGEDY, C., LIU, W., JIA, Y., ET AL. 2014. Going Deeper with Convolutions. arXiv preprint arXiv:1409.4842, 1\u201312.\u00a0\n\nTHRUN, S., MONTEMERLO, M., DAHLKAMP, H., ET AL. 2006. Stanley: The Robot That Won the DARPA Grand Challenge. Journal of Field Robotics 23, 9, 661\u2013692.\u00a0\n\nTODD, S. AND LATHAM, W. 1992. Evolutionary art and computers. Academic Press, Inc.\u00a0\n\nTURING, A. 1948. Intelligent Machinery.\u00a0.\u00a0\n\nTURING, A. 1950. Computing Machinery and Intelligence. Mind 59, 433\u2013460.\u00a0\n\nWRIGHT, M. AND FREED, A. 1997. Open Sound Control: A new protocol for communicating with sound synthesizers. Proceedings of the International Computer Music Conference (ICMC).\u00a0\n\nZENG, Z., PANTIC, M., ROISMAN, G.I., AND HUANG, T.S. 2009. A survey of affect recognition methods: Audio, visual, and spontaneous expressions. IEEE Transactions on Pattern Analysis and Machine Intelligence 31, 1, 39\u201358.", 
        "title": "Review of machine / deep learning in an artistic context"
    }, 
    {
        "url": "https://medium.com/@abduljaleelkavungal/deep-machine-learning-libraries-and-frameworks-5fdf2bb6bfbe?source=tag_archive---------4----------------", 
        "text": "At the end of 2015, all eyes were on the year\u2019s accomplishments, as well as forecasting technology trends of 2016 and beyond. One particular field that has frequently been in the spotlight during the last year is deep learning, an increasingly popular branch of machine learning, which looks to continue to advance further and infiltrate into an increasing number of industries and sectors. Here are a list of Deep Learning libraries and frameworks that will gain momentum in 2016.\n\n1. Theano is a python library for defining and evaluating mathematical expressions with numerical arrays. It makes it easy to write deep learning algorithms in python. On the top of the Theano many more libraries are built.\n\n\u00b7 Keras is a minimalist, highly modular neural network library in the spirit of Torch, written in Python, that uses Theano under the hood for optimized tensor manipulation on GPU and CPU.\n\n\u00b7 Pylearn2 is a library that wraps a lot of models and training algorithms such as Stochastic Gradient Descent that are commonly used in Deep Learning. Its functional libraries are built on top of Theano\n\n\u00b7 Lasagne is a lightweight library to build and train neural networks in Theano. It is governed by simplicity, transparency, modularity, pragmatism\u00a0, focus and restraint principles.\n\n\u00b7 Blocks a framework that helps you build neural network models on top of Theano.\n\n2. Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors. Google\u2019s DeepDream is based on Caffe Framework. This framework is a BSD-licensed C++ library with Python Interface.\n\n3. nolearn contains a number of wrappers and abstractions around existing neural network libraries, most notably Lasagne, along with a few machine learning utility modules.\n\n4. Gensim is deep learning toolkit implemented in python programming language intended for handling large text collections, using efficient algorithms.\n\n5. Chainer bridge the gap between algorithms and implementations of deep learning. Its powerful, flexible and intuitive and is considered as the flexible framework for Deep Learning.\n\n6. deepnet is a GPU-based python implementation of deep learning algorithms like Feed-forward Neural Nets, Restricted Boltzmann Machines, Deep Belief Nets, Autoencoders, Deep Boltzmann Machines and Convolutional Neural Nets.\n\n7. Hebel is a library for deep learning with neural networks in Python using GPU acceleration with CUDA through PyCUDA. It implements the most important types of neural network models and offers a variety of different activation functions and training methods such as momentum, Nesterov momentum, dropout, and early stopping.\n\n8. CXXNET is fast, concise, distributed deep learning framework based on MShadow. It is a lightweight and easy extensible C++/CUDA neural network toolkit with friendly Python/Matlab interface for training and prediction.\n\n9. DeepPy is a Pythonic deep learning framework built on top of NumPy.\n\n10. DeepLearning is deep learning library, developed with C++ and python.\n\n12. ConvNet Convolutional neural net is a type of deep learning classification algorithms, that can learn useful features from raw data by themselves and is performed by tuning its weighs.\n\n13. DeepLearnToolBox is a matlab/octave toolbox for deep learning and includes Deep Belief Nets, Stacked Autoencoders, convolutional neural nets.\n\n14. cuda-convnet is a fast C++/CUDA implementation of convolutional (or more generally, feed-forward) neural networks. It can model arbitrary layer connectivity and network depth. Any directed acyclic graph of layers will do. Training is done using the backpropagation algorithm.\n\n15. MatConvNet is a MATLAB toolbox implementing Convolutional Neural Networks (CNNs) for computer vision applications. It is simple, efficient, and can run and learn state-of-the-art CNNs\n\n16. eblearn is an open-source C++ library of machine learning by New York University\u2019s machine learning lab, led by Yann LeCun. In particular, implementations of convolutional neural networks with energy-based models along with a GUI, demos and tutorials.\n\n17. SINGA is designed to be general to implement the distributed training algorithms of existing systems. It is supported by Apache Software Foundation.\n\n18. NVIDIA DIGITS is a new system for developing, training and visualizing deep neural networks. It puts the power of deep learning into an intuitive browser-based interface, so that data scientists and researchers can quickly design the best DNN for their data using real-time network behavior visualization.\n\n20. N-Dimensional Arrays for Java (ND4J)is scientific computing libraries for the JVM. They are meant to be used in production environments, which means routines are designed to run fast with minimum RAM requirements.\n\n21. Deeplearning4j is the first commercial-grade, open-source, distributed deep-learning library written for Java and Scala. It is designed to be used in business environments, rather than as a research tool.\n\n22. Encog is an advanced machine learning framework which supports Support Vector Machines,Artificial Neural Networks, Genetic Programming, Bayesian Networks, Hidden Markov Models, Genetic Programming and Genetic Algorithms are supported.\n\n23. Convnet.js is a Javascript library for training Deep Learning models (mainly Neural Networks) entirely in a browser. No software requirements, no compilers, no installations, no GPUs, no sweat.\n\n24. Torch is a scientific computing framework with wide support for machine learning algorithms. It is easy to use and efficient, fast scripting language, LuaJIT, and an underlying C/CUDA implementation. Torch is based on Lua programming language.\n\n25. Mocha is a Deep Learning framework for Julia, inspired by the C++ framework Caffe. Efficient implementations of general stochastic gradient solvers and common layers in Mocha could be used to train deep / shallow (convolutional) neural networks, with (optional) unsupervised pre-training via (stacked) auto-encoders. Its best feature include Modular architecture, High-level Interface, portability with speed, compatibility and many more.\n\n26. Lush(Lisp Universal Shell) is an object-oriented programming language designed for researchers, experimenters, and engineers interested in large-scale numerical and graphic applications. It comes with rich set of deep learning libraries as a part of machine learning libraries.\n\n28. Accord.NET is a\u00a0.NET machine learning framework combined with audio and image processing libraries completely written in C#. It is a complete framework for building production-grade computer vision, computer audition, signal processing and statistics applications\n\n29. darch package can be used for generating neural networks with many layers (deep architectures). Training methods includes a pre training with the contrastive divergence method and a fine tuning with common known training algorithms like backpropagation or conjugate gradient.\n\n30. deepnet implements some deep learning architectures and neural network algorithms, including BP,RBM,DBN,Deep autoencoder and so on.", 
        "title": "Deep Machine Learning libraries and frameworks \u2013 Abdul Jaleel Kavungal \u2013"
    }, 
    {
        "url": "https://medium.com/jim-fleming/implementing-lstm-a-search-space-odyssey-7d50c3bacf93?source=tag_archive---------5----------------", 
        "text": "This week I read LSTM: A Search Space Odyssey. It\u2019s an excellent paper that systematically evaluates the different internal mechanisms of an LSTM (long short-term memory) block by disabling each mechanism in turn and comparing their performance. We\u2019re going to implement each of the variants in TensorFlow and evaluate their performance on the Penn Tree Bank (PTB) dataset. This will obviously not be as thorough as the original paper but it allows us to see, and try out, the impact of each variant for ourselves.\n\nTL;DR Check out the Github repo for results and variant definitions.\n\nWe\u2019ll start with a setup similar to TensorFlow\u2019s RNN tutorial. The primary difference is that we\u2019re going to use a very simple re-implementation for the LSTM cell defined as follows:\n\nThis corresponds to the \u201cvanilla\u201d LSTM from the paper. Each equation defines a particular component of the block: block input (z), input gate (i), forget gate (f), cell state (c), output gate (o) and block output (y). Both g and h represent the hyperbolic tangent function and sigma represents the sigmoid activation function. The circle dot represents element-wise multiplication.\n\nHere\u2019s the same thing in code:\n\nBe sure to check out the full source for the rest of the cell definition. Mostly we create a new class inheriting from RNNCell and use the above code as the body of __call__. The nice part about this setup is that we can utilize MultiRNNCell to stack the LSTMs into multiple layers.\n\nNotice that we initialize all of our parameters using get_variable. This is necessary so that we can reuse these variables for each time step rather than creating new parameters at each step. Also, all parameters are transposed from the paper\u2019s definitions to avoid additional graph operations.\n\nThen we define each equation as operations in the graph. Many of the operations have reversed inputs from the equations so that the matrix multiplications produce the correct dimensionality. Other than these details we\u2019re directly translating the equations.\n\nNote that from a performance perspective, this is a na\u00efve implementation. If you look at the source for TensorFlow\u2019s LSTMCell you\u2019ll see that all of the cell inputs and states are concatenated together before doing any matrix multiplication. This is to improve performance, however, since we\u2019re more interested in taking the LSTM apart, we\u2019ll keep things simple.\n\nRunning this vanilla LSTM on the included notebook we obtain a test perplexity (e^cost) of less than 100. So far so good. This will serve as our baseline to compare to the other variants. Below is the cost (average negative log probability of the target words) on the validation set after each epoch:\n\nThe most helpful bits for implementing each of the variants can be found in appendix A3 of the paper. The gate omission variants such as no input gate (NIG), no forget gate (NFG), and no output gate (NOG) simply set their respective gates to 1 (be sure to use floats, not integers, here):\n\nThe no input activation function (NIAF) and no output activation function (NOAF) variants remove their input or output activation functions, respectively:\n\nThe no peepholes (NP) variant removes peepholes from all three gates:\n\nThe coupled input-forget gate (CIFG) variant sets the forget gate like so:\n\nThe final variant, full gate recurrence (FGR), is the most complex, essentially allowing each gate\u2019s previous state to interact with each gate\u2019s next state:\n\nIn many of the variants, we can remove parameters no longer needed to compute the cell. The FGR variant, however, adds significantly more parameters (9 additional square matrices) which also increases training time.\n\nTo implement each, we\u2019ll simply duplicate our vanilla LSTM cell implementation and make the necessary modifications for the variant. There are too many to show here but you can view the full source for each variant on Github. To train each, we\u2019ll use the same hyperparameters from the vanilla LSTM trial. This probably isn\u2019t fair and a more thorough analysis (as performed in the paper) would try to find the best hyperparameters for each variant.\n\nThe NFG and NOG variants fail to converge to anything useful while the NIAF variant diverges significantly after around the 8th epoch. (This divergence could probably be fixed with learning rate decay which I omitted for simplicity.)\n\nIn contrast, the NIG, CIFG, NP and FGR variants all converge. The NIG and FGR variants do not produce great results while the NP and CIFG variants perform similarly to the vanilla LSTM.\n\nFinally the NOAF variant. Its poor performance is likely due to the lack of clamping from the output activation function so its cost explodes:\n\nHere are the test perplexities for each variant:\n\nOverall it\u2019s been fun dissecting the LSTM. Feel free to try out the code yourself and if you\u2019re interested in taking this further I recommend running comparisons with GRUs, looking at fANOVA or extending what\u2019s here with more thorough analysis.\n\nFollow me on Twitter for more posts like these. If you\u2019d like help with production NLP, I do consulting.", 
        "title": "An LSTM Odyssey \u2013 Jim Fleming \u2013"
    }, 
    {
        "url": "https://medium.com/machine-intelligence-report/three-reasons-deep-learning-rocked-2015-e7e63d8b8ee0?source=tag_archive---------6----------------", 
        "text": "DNNResearch\u2019s Alexnet (2012) is to Nirvana\u2019s Nevermind (1991), as deep learning in 2015 is to grunge rock in 1994. In a span of a few years, deep learning (DL) has gone from crashing the party to hosting it.\n\nThat DL rocked software in 2015 has been well documented. The giants of tech snapped up talent and produced a slew of mainstream apps like Moments and Translator. At Google alone, more than 2,000 projects and 70 teams featured DL, producing such 2015 gems as SmartReply, Photos, and revamped voice search & text search. Deepnet-paloozas such as ICLR and NIPS saw explosions in attendance. The keynotes at NVIDIA\u2019s tech conference all focused on DL. Funding reached new heights via startups like Clarifai, Enlitic, and Nervana. It was breathtaking and fun.\n\nOver the year, I read\u200a\u2014\u200aok, scanned\u200a\u2014\u200aover 1,000 new articles on advancements and investments in DL. In my attempt to summarize what emerged in 2015, I noticed a few dominant themes.\n\nWhat became DeepDream produced such reverberations that non-technical friends were sharing its fruits with no mention of the underlying tech. It even spawned costumes for Halloween and a new species: The puppyslug.\n\nThis inceptionism was part of a big trend of generative algorithms that learn from data to produce creative and useful variations of reality. It was usually in the form of images\u200a\u2014\u200aof faces, interiors, and stylized paintings\u200a\u2014\u200abut also seen in music, storytelling, written characters, and 3D models & graphics.\n\nAlso in 2015, Artomatix won NVIDIA\u2019s startup challenge, DeepMind released DRAW, Facebook Eyescream, U. T\u00fcbingen style transfer, and U. Toronto generating images from novel captions. GANs were everywhere.\n\nLearning how to decompose data into meaningful compositional factors is captivating and important. There are obvious applications in digital arts, game design, 3D design tools, and video & audio compression. Coupled with physical synthesis technology like nanoscale 3D printing and genome editing, such techniques should eventually lead to the manifestation of new, useful materials, medicines, and organisms. Pet puppyslugs, anyone?\n\nGoogle\u2019s hesitant self-driving cars are a common sight here in the Valley, but in 2015 Tesla put something kick-ass and slightly dangerous right into the crowd\u2019s hands. Autopilot: A collaborative learning system, designed to both learn from drivers and to help them drive while they find some good music.\n\nSuch human-in-the-loop learning systems became a common reprise from researchers to investors, who finally got that simply incanting \u201cdeep learning\u201d over a pile of data won\u2019t get you far in the real world. Interactive systems afford faster learning and yield copious and meaningful data, which increases in value as the learning algorithms continue to improve.\n\nAlong these lines, 2015 also brought robotics startup Osaro, interactive approaches for image annotation (producing sweet datasets including visual genome and LSUN), and methods to adaptively leverage crowdsourced input as a machine progressively assumes responsibility. Intelligent systems should be designed to be interactive and interpretable, leading the human-machine hybrid to ever-advancing understanding and capability.\n\nGrunge bloomed in Seattle because of the mutual inspiration and collaboration between bands in the local scene. Similarly, deep learning owes its rapid rise to the unprecedented openness of its researchers, many of whom work in large corporations.\n\nOpen software toolkits for DL were already numerous, but 2015 incredibly saw Facebook, Google, Microsoft, Samsung, and NVIDIA open-source their libraries, along with accomplished startup Nervana, and popular Keras. The toolkits now number at least fifty. Facebook and NVIDIA even shared their DL hardware designs.\n\nDespite openness, corporate researchers could still be influenced by their companies\u2019 profit motives. To address this danger that grows with the power the AI that they aspire to build, Elon Musk, Sam Altman, and others committed $1 billion in founding the non-profit OpenAI. With their funding, strong research team, and mission of openness and benevolence, OpenAI should at least prove to be a formidable recruiter for DL R&D talent. Some additional thoughts and questions about OpenAI can be found here.\n\nPredictions are mostly wrong, useless, or boring. Yes, of course, much of the current R&D will continue and advance\u200a\u2014\u200abut what unexpected, game-changing developments might there be in 2016?", 
        "title": "Three reasons deep learning rocked 2015 \u2013 Machine Intelligence Report \u2013"
    }, 
    {
        "url": "https://medium.com/@joshdotai/deep-reinforcement-learning-papers-a2167c136fc7?source=tag_archive---------7----------------", 
        "text": "Josh.ai is an artificial intelligence agent for your home. If you\u2019re interested in learning more, visit us at https://josh.ai.\n\nLike us on Facebook, follow us on Twitter.", 
        "title": "Deep Reinforcement Learning \u2014 Papers \u2013 Josh \u2013"
    }, 
    {
        "url": "https://medium.com/machine-intelligence-report/personal-views-on-the-future-of-artificial-intelligence-509c5db276fc?source=tag_archive---------8----------------", 
        "text": "On January 11th, I attended the first and public day of the NYU Symposium on the Future of AI, hosted by our colleagues and neighbors at NYU. Here are a few notes and thoughts about the event.\n\nI could not find any website describing the event. Here is a scan of the program of the first day.\n\nKeep in mind that I am not an AI person. My PhD is in databases. Take everything I say with a grain a salt. Also, the field is moving very fast: since January 11th, the Google Deep Mind team beat the EU Go champion using its AlphaGo software.\n\nKey industry players were present at the conference, including Google, Facebook and Microsoft. The overall message from industry can be summarized as:\n\nIndustry repeatedly acknowledged the fact that the next good idea is more likely to come from academia. This might be true in theory. In practice, the most successful AI techniques require an enormous amount of data, data that academia usually does not have access to.\n\nAt the technical level, the research directions mentioned were around (a) integration with reasoning, attention, planning, memory; (b) combination of supervised, unsupervised and reinforcement learning; and (c) effective unsupervised learning.\n\nAt the meta-level, challenges included (1) provably safe AIs (Microsoft), (2) AIs that can interact with humans \u2013 aka AIs like us \u2013 and also (3) better platforms that can bring AI to the many (Google).\n\nIf like me, you have not been following the field very carefully, see Richard Mallah's overview of 2015 top AI achievements [5].\n\nSelf-driving cars have been a boon for the AI community. Pun intended, this is really the killer (or rather un-killer) app and it keeps pushing the envelope for the entire field. First, driving is a great test case for AI challenges such as perception, learning and planning. Second, the auto industry produces cars en masse with a good track record of quality and reliability. Third, transportation is a very big industry with a clear impact on humans and their environment. When you combine these three elements, you get a great recipe for real innovation, i.e. the \u00ab\u00a0successful creation and delivery of a new or improved product or service in the marketplace\u00a0\u00bb as defined in [1].\n\nHaving realized that, the interesting question you should ask yourself \u2013 if you are not doing AI \u2013 is:\n\nAdvances in hardware like GPU had a strong impact on AI algorithms especially for deep learning, making them orders of magnitude faster (60x according to NVIDA). The R&D for these GPUs was mostly funded by the video game industry. It is ironic to see AI being applied to learning how to play video games, as demonstrated by Deep Mind [2,3] for Atari games. AI is also starting to disrupt other games like poker [4].\n\nThe Deep Mind team motivated their research interest for games by the fact this is a domain where you can easily gets lots of data.\n\nAdvances in neuroscience also had a strong impact on AI algorithms. Neural networks take their inspiration from the way our brain works. An interesting application of AI mentioned during the conference was \u00ab\u00a0AI assisted science\u00a0\u00bb where AI can be used to help and guide scientists doing their work. Think medicine, physics, astronomy, etc.\n\nSo, it seems that we have two AI virtuous cycles:\n\nWith AI playing an increasingly important role in our lives\u200a\u2014\u200adriving our cars, scheduling our lives, etc.\u200a\u2014\u200a, making it do the right thing becomes critical. Science fiction is a good inspiration for horror stories or predictions in this space, e.g. SkyNet, Iron Man\u2019s Jarvis, Spike Jonze\u2019s Her, or Asimov\u2019s Three Laws of Robotics.\n\nEric Horvitz from Microsoft Research mentioned some ethical and safety related requirements, like being able to\n\nNote that for self-driving cars, fallback cases (aka disengagements) must be reported to the US Department of Transportation.\n\nAssuming the AI system does faithfully what it is supposed to do, it is not always clear what this \u201cit\u201d should be. My guess is that something like the trolley problem in the context of self-driving cars will keep AI experts, entrepreneurs, ethicists, lawyers and policy makers agitated for some time.\n\nThank you to NYU for organizing this great event, with key players from the field, both from industry and academia.\n\nI am not familiar enough with the field of AI but it felt to me more like \u201cThe Future of Deep Learning\u201d than \u201cThe Future of AI\u201d. Also, the choice of speakers and panelists did not scream \u201cdiversity\u201d to me, with only one woman present. I am sure we want the future of AI to be more diverse.\n\nMy 3 personal take aways from the day are:\n\nTo best summarize my current and personal point of view about the future of AI, I will borrow a quote from @KJ_Hammond at Next:Economy conference:\n\nAnd nobody wants to be any AI's bully.\n\nSpecial thanks to Nikolai, Gideon, David and Hanna for comments on early versions of this post.\n\n[1] Innovation: The Five Disciplines for Creating What Customers Want, Curtis R. Carlson and William W. Wilmot, 2006.\n\n[4] Poker-CNN: A Pattern Learning Strategy for Making Draws and Bets in Poker Games, Yakovenko et al., 2015.", 
        "title": "Personal Views on the Future of Artificial Intelligence"
    }, 
    {
        "url": "https://medium.com/@kidargueta/show-me-the-faces-collecting-faces-with-emotional-expression-for-deep-learning-eeb54526ab93?source=tag_archive---------9----------------", 
        "text": "Teaching a machine how to recognize objects (cars, houses, and cats) is a difficult task, teaching it to recognize emotions is another story. If you have been following my posts, you then know that I want to teach machines to recognize human emotions. One important way in which machines can detect our feelings is by reading our faces. Teaching a machine to read faces has many challenges, and now that I started to tackle this problem I have encountered my first big one.\n\nDeep Learning, a powerful tool used to teach machines seems promising for the task at hand, but in order to make use of it I needed to find the materials to teach the machine. Let me use an analogy to explain. For humans to learn to recognize objects, or in our specific case recognize facial expressions, a person has to be exposed to many faces. That\u2019s not a big deal as we see faces everywhere from the second we are born. On the other hand, we don\u2019t really have tools to take a computer into the wild and let it learn. So my big challenge was finding pictures or videos of people showing emotions in their face, to feed it to the machine and let it learn.\n\nCompanies like Google and Facebook, and some big labs in prestigious universities, have access to an enormous amount of data (just think of how many faces people tag on Facebook). However, mere mortals like me have to find not straightforward ways to collect humble amounts of data to teach our machines. So let me start by defining exactly what is the data I wanted to collect. To teach my machine to recognize emotions from facial expressions, I needed to collect pictures of faces expressing some emotion (angry faces or happy faces), and at the same time I need to explicitly tell the machine what the emotion is (this face shows anger). To be more exact, what I need to feed the machine is a collection of data pairs of the form [picture, emotion]. The question now is how to obtain such data?\n\nFirst let me quickly tell you how you should not obtain this data. Many, including me, would first think about manually collecting thousands of pics from different sources (personal photos, Facebook, etc\u00a0\u2026), use a photo app to crop the faces (the learning is more efficient if the pic contains just the face), and manually define the emotion tag. This is time consuming, and not scalable. Let me explain what I did instead\u00a0.First, many companies offer some automatic ways to pull data from their servers. The obvious choice for pics then might be Instagram (not Facebook as the data is not public). Now the problem with Instagram is that it\u2019s not easy to specify that you want pics with faces. So in order to get exactly what I needed (faces with emotional expressions) my best choice was Google.\n\nGoogle offers the Custom Search API, a tool to let programs pull data based on queries, much like humans would using the Google website. This was perfect for me, to understand why try the query scared look on Google (then go to images). So now I had an automatic way to get faces expressing emotions and I did not have to manually identify the emotion (it comes from the query). But wait, what about this picture:\n\nThe image was obtained with the query angry look, and it clearly has an angry face in it, but it also has an upper body, a gun, and many watermarks. This is not good as it will confuse my machine. How about this picture obtained with the query sad person:\n\nIt clearly has no sad person, it has no person at all as it\u2019s just a table. So while in most cases (when using appropriate queries) you will obtain faces with the intended emotion (like the angry man), it will most likely come with extra noise, or sometimes even not have a face at all. Again, the best way to deal with this is not manually, but using Computer Vision tools to remove the noise automatically.\n\nAfter submitting many queries and downloading a few thousands pics (due to rate limitations this might span a few days), I automatically processed all the pics using the popular Computer Vision library OpenCV (free if you wonder). OpenCV comes pre-loaded with a set of nice filters to detect faces and other features (eyes, mouth,\u00a0\u2026) in pics. The results are magical:\n\nOpenCV automatically detected a square region containing the face, and with additional commands, my program was able to automatically crop and reduce the face to a size and format appropriate to feed to my machine. Now what happened to the image without the face? OpenCV did not detect any face in it so it was automatically ignored. And Voila, that\u2019s how I could efficiently (and free) start building a descent dataset of faces to later teach a computer how to detect our emotions.\n\nTo conclude, very often (depending on the query) you will find friends like this in the pictures:\n\nand OpenCV will of course return you this beauty:\n\nWhether this is bad or not for the trained machine I still don\u2019t know. I will find out when I move to the training process. Worst case, I have to manually remove a few faces (and other possible wrongfully detected objects). Best case, I have a machine that can know if my kids are watching happy cartoons.", 
        "title": "Show Me The Faces: Collecting Faces With Emotional Expression for Deep Learning"
    }
]