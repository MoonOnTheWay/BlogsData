[
    {
        "url": "https://medium.com/@majortal/bitcoin-apocalypse-ea89041f24c4?source=tag_archive---------0----------------", 
        "text": "Go Anguilla??? Sanjay asked? WTF?\n\nI turned around and showed him the back of my shirt.\n\n\u201cIf you work at a startup with a weird domain extension, you have to root for that country at the Olympics. It\u2019s the law.\u201d Sanjay burst our laughing.\n\nYou actually printed that? What kind of entrepreneur are you? Don\u2019t you have better things to do with your time?\n\nI didn\u2019t really. bonz.ai was just a skeleton now after I fired the last of the developers. We had a good run but you need to know when to move on.\n\nWe were bored, hanging out at his place. TV was full of politics and the olympics, neither of which we really cared about. No new Mr. Robot episodes, which was probably our only common denominator for streaming. Hacking required too much thinking and we were not in the mood.\n\nBlaspha.me Sanjay said. Taken.\n\nShangri.la I said. Taken as well.\n\nWe were taking turns coming up with creative startup names. I\u2019m pretty sure most companies start off that way, I mean what good is a company if you don\u2019t have a proper Twitter handle and domain name. I was sitting at the computer and Sanjay was on his back on the sofa, playing catch with himself with a basketball, trying to NOT hit the ceiling yet succeeding almost every time. Hey, check this out, I said\u200a\u2014\u200a\u201cco.in\u201d is the TLD for companies in India.\n\nSo, he said?\n\nCoin! Don\u2019t you get it? Cryptocurrencies?\n\nCute, he said. It\u2019s still your turn though.\n\nDaenerys.co.in I said.\n\nHah hah, they are going to sue the pants off of you for violating their trademark and you know it. Fine. gold.co.in.\n\nI smiled and looked at this 300 pound Indian with a thick gold chain around his neck. Not exactly how I would picture a scientist on the Google Brain team (not that I knew anyone else who worked there).\n\nPfff. Taken, I said. But many of the short domain names are free for \u201cco.in\u201d, I said after a couple of minutes. I ran a short script.\n\nCross reference them with Wikipedia and search for money expressions on the page, Sanjay suggested. Hmmm, I replied, already coding it.\n\nA couple of minutes later I found it\u200a\u2014\u200azuz.co.in.\n\nWhat\u2019s a Zuz, Sanjay asked?\n\nAn ancient Jewish silver coin struck during some ancient revolt against the Roman empire, I read from the Wikipedia page. Now it was Sanjay\u2019s turn to humm. It was used in parallel to the Roman Denarius, I kept reading from the page. That actually got his attention.\n\nThere was a coin called Daenerys?\n\nDenarius, you idiot. I spelled it.\n\nHe laughed that huggable bear infectious laugh of his that got me into trouble at school so many times.\n\nStill, he said. Is Denarius.co.in taken?\n\nNope. I said a second later.\n\nCool, he said. He was sitting up on the sofa now. Everybody is going to call it Daenerys Coin and we can get away with it because Denarius really is a coin!\n\nHe was dribbling now, his eyes unfocused. Oh, I knew that look.\n\nDone and done and done, I said. That broke his focus and he actually missed the ball.\n\nOh I\u2019m sorry, did I break your concentration? I smiled, never to miss the Pulp Fiction quote opportunity.\n\nWhat did you do? he asked.\n\nNothing, I said. I just bought the domain and I already secured the Twitter handle AND the Facebook page.\n\nYou bought the domain? he asked again.\n\nYyyyes, I said slowly. That\u2019s what we fake entrepreneurs do, you know. We act on stuff. Seize opportunities? And the domain was only $16\u200a\u2014\u200athat\u2019s a no brainer. You\u2019ve chosen names before, right? For your projects?\n\nYeah, I guess, Sanjay said. But they were all Neural This, or Deep That, nothing very creative.\n\nWe were quiet for a few minutes.\n\nThe fuck are you going to do with this domain, he asked. I didn\u2019t know yet. We were quiet for a few more minutes.\n\nRemember Auroracoin? I asked? He didn\u2019t.\n\nI told you about them about, say, a year ago.\n\nI\u2019m pretty sure you didn\u2019t, he replied.\n\nWell, some guy set up a web page and forked a crypto coin, and said he is going to AIR DROP a bunch of them for every citizen in Iceland as an alternative to the heavily restricted kr\u00f3na.\n\nOk, he said. And how did that work out for them?\n\nPfff, I said, not very well. I\u2019m pretty sure it was some guy\u2019s Ponzi scheme, but that doesn\u2019t mean it cannot work. The bootstrapping thought process, with lots of people being semi-forced into involvement is kinda cool, and I think I can take it one step further. I was thinking out loud but blocks were clicking into place. What if we pre-distribute the coins to people? In Iceland, people had to prove they were citizens with some kind of local ID number, but if we eliminate the friction by creating wallets and actually distributing the coins to many people, maybe to celebrities, that could really boost the coin. I\u2019m thinking, I paused again, You know those Twitter official accounts? They are all celebrities and we can give them all Daenerys coins! See what they do with it!\n\nWouldn\u2019t they all just cash out at the first possible moment? Sanjay asked.\n\nThey might, they might, but it could become like the official Twitter coin or something. Worst case scenario is that we repeat Dogecoin.\n\nWhat\u2019s the best case scenario? Sanjay asked.\n\nThe best case scenario for what? Sky asked as she walked in the room and I nearly died.\n\nSky?!\n\nI was a teen and Sky was his older sister and she was so amazing I completely ignored her every time I came over. Her hair had thick brown curls which gradiented to blond down her back except that one curl which was purple and oh-my-god I have dreamed about her for years. She sat on the sofa reading some book and her legs were the most beautiful thing I could not look at even if I wanted to. Sky! She had a nose ring and she changed her name at 15 and she was so cool it froze my adolescent brain.\n\nAnd I have not seen her in 20 years.\n\nOwen? She said, smiling, enunciating the syllables, pronouncing it \u201cOh, When?\u201d I was semi dazed as we kissed hello and we talked and we talked and we sat on the once-purple futon-couch and we talked some more.\n\nI\u2019m forking Bitcoin Sanjay said, still at his computer, but it barely registered. Go for it, I said, cryptocurrency as far from my mind as medieval paleography.\n\nA few hours later I went home. I watched some porn. I fell asleep.", 
        "title": "Bitcoin Apocalypse \u2013 Tal Weiss \u2013"
    }, 
    {
        "url": "https://gab41.lab41.org/flexible-image-tagging-with-fast0tag-681c6283c9b7?source=tag_archive---------1----------------", 
        "text": "One of the goals of multimodal embedding is to make it easier to expand machine learning models into new contexts. Deep architectures are typically trained on large, diverse datasets, because without a significant quantity of labeled data pairs (on the order of 10,000s to 1,000,000s), it can be quite a challenge to produce robust results. Typically, models will require dozens if not hundreds of exemplars in order to learn a new class. Even after learning on thousands of images, their architecture still only allows them to make predictions on the original label set. That means that, if the network was trained on \u201cplane\u201d but not \u201cairplane\u201d or \u201c747\u201d, there isn\u2019t any straightforward way of teaching it how to use those labels without re-training the entire network with new samples.\n\nSpeaking on this problem, the recent paper \u201cReturn of Frustratingly Easy Domain Adaptation\u201d, by Baochen Sun et al., noted:\n\nIf we can find a way to flexibly align features from different modalities, it opens up the possibility for networks that generalize to new sources.\n\nHere at the Lab, the Attalos challenge has been chipping away at multimodal embedding. We take in labeled datasets of images and their associated tags, and after extracting features from the images using off-the-shelf deep convolutional networks and the tags using word embedding models, we\u2019ve been exploring intelligent ways to project both encodings into the same vector space. From that point (no pun intended), it is much easier to assess similarity and find related content.\n\nOver the past few years, we\u2019ve seen a proliferation of neural learning architectures\u2014of the convolutional flavor, to be exact\u2014with better performance on image tasks coming from increasingly deep networks. Instead of reinventing the wheel, we realized that it was possible to use these models pre-trained on image classification tasks to extract a representation of images. This basically means stepping in the middle of the network and extracting a dense, feature-rich encoding of our images.\n\nWe\u2019ve posted on text embedding methods before (here, here, and here), so I won\u2019t go into too much detail, but suffice it to say that there are a couple of options available, all of which perform well. Many authors use word2vec, a neural embedding model devised at Google in 2013, as the semantic embedding model. Others have taken advantage of embeddings pre-trained using Global Vectors for Word Representation (GloVe). For our purposes, though, any semantic embedding will do, so long as it preserves the relationship where semantically-similar words are closer to each other than they are to dissimilar ones.\n\nThe last (and most important) step in the process is translating our encoded image and text features into the same vector space. This is where we need to do the most heavy lifting, since there is no standard method for integrating multiple modalities in an extensible way. Luckily, there\u2019s a decent amount of literature on different areas where joint embedding can be helpful. Researchers have successfully done joint embedding of shapes with images\n\n, sounds with transcripts, two different languages, and even videos with images and summaries. So long as you can create an embedding of both modalities, a vector space model can reveal how each modality relates to the other.\n\nThe heart of the challenge has been figuring out the best way to meaningfully project these image and text features into the same vector space. While there are a number of feature extraction architectures that all work fairly well, the difference between a good joint embedding method and a bad one is often pretty stark. Here\u2019s one method that we\u2019ve found promising!\n\nEarlier this year at CVPR, researchers from the University of Florida presented a new approach to zero-shot learning that achieved surprising performance through a relatively simple method. In their paper, \u201cFast Zero-Shot Image Tagging\u201d, they approach the issue of image-tag embedding from a new angle.\n\nThe key insight is this: what we want to know is what visual features make a difference to the meaning of the image. In order to translate image features into text features, the algorithm finds the direction in our semantic space that ranks tags in order of how appropriate they are to the image. Then we can simply grab that ranking for a given image and take the top-n tags to find its best tags. Or in reverse, we can take a group of tags and find the image whose direction maps closest to that group.\n\nIf you\u2019re curious about the details, the authors of the method, which they call \u201cFast0Tag\u201d, defined a cost function that optimizes for the best directions, equivalent to the following:\n\nIn (slightly) plainer English, the cost is lowest when the network outputs a direction that\u2019s aligned with the positive examples and away from the negative ones. With this, you can build a sequential model of fully connected layers (they used Theano + Keras, but TensorFlow is just as good) using the above equations in the loss function. And that\u2019s it! After it trains, you should have a model that can accommodate multi-tag search, semantic clustering, and other multimodal tasks.\n\nWe trained the Fast0Tag model on some of the datasets we\u2019ve been working with for the Attalos Challenge. The network architecture itself is straightforward: three layers, two with ReLU activations and one linear. It takes in the output from our pre-trained Inception image feature extractor and outputs a vector in the GloVe 200-dimensional word embedding space. Here are a few examples from testing the model on the IAPR TC-12 dataset after training it on the ESP Game dataset and vice versa:\n\n.\u00a0.\u00a0. while the model has some difficulty on a few images.\n\nAlternatively, you can input a set of tags and find images related to those tags:\n\nRemember: not only are these images from a dataset the network had never seen before, but the labels are a different set from the tags it had trained on.\n\nFor example, the tag \u201cocean\u201d isn\u2019t part of the ESP Game dataset, but using our Fast0Tag model, we can use it as a query and examine the top results:\n\nOverall, we found that Fast0Tag was able to get upwards of 40% overall precision, recall and F1 scores within a single corpus and 20% between corpora. It turns out that the model tends to perform a bit worse when trained on the ESP Game dataset, likely because the set of images contain a number of labels that are mostly unrelated to the visual features of the image. Tags like \u201cposter\u201d and \u201cgame\u201d are difficult to deduce from the extracted features alone, especially when we\u2019ve trained our model on a set of tags that are mostly annotating outdoor scenes. This really shows how important training on a broad tag set is to good model generalization.\n\nAlthough the results of Fast0Tag are promising, the method is not without flaws. For us, there\u2019s a major bottleneck in the original implementation. In the paper, the authors computed the distances in the second equation from the network\u2019s output for every tag in the dataset. Since they were training on the tiny NUS-WIDE training set (91 tags), that wasn\u2019t an issue, but it becomes a major headache when dealing with even slightly larger datasets such as IAPR TC-12 (291 tags), not to speak of the giant tag sets from Visual Genome or Yahoo!-Flickr 100M (+100,000 tags).\n\nInstead of optimizing for all positive and negative tags for every image, we\u2019d like to choose a handful of negative tags for each, to accompany the positive tags from the training data. This would allow us to approximate their cost function, using negative sampling without the overhead of redundant computations. For instance, if we feed in an image of a forest, there\u2019s no reason to compare against both \u201cmarine\u201d and \u201creef\u201d, especially since those two will already be close in word2vec space. Much better to randomly select one of them as a negative tag example every time we see that image. We\u2019re working on a TensorFlow implementation of this negative sampling method.\n\nSo what does Fast0Tag mean for our quest to build a cross-modal embedding of features?\n\nFirst, I think it provides yet another proof of concept that deep/wide networks can perform well at transferring knowledge between domains. It also makes me hopeful for developments in zero/one-shot learning over the next few years. Finding a successful joint embedding method would mean being able to swap test corpora and still get quality results, which is critical in the real world since it allows for dynamic application of prediction in new contexts without having to train another network to map between the two domains. To run your embedding on in a new context, just apply the same feature extractor and\u00a0.\u00a0.\u00a0. voil\u00e0! Automatic tagging and image retrieval.\n\nHumans have a remarkably strong capacity for abstraction. From a handful of examples we can pull out high-level regularities that we can extend to classify or produce novel examples. Moreover, we can often generalize to new classes with an incomplete data point or even zero examples from that class. By extracting general features from examples and translating them into a new domain, we can transfer existing knowledge to new problem sets.\n\nTake early hominid fossils as an example. Digs rarely uncover a full head-to-toe skeleton in this area; anthropologists are lucky if they can find even partial remains. However, they\u2019re able to use prior knowledge about hominid physiology to extrapolate from important features (dental structure, femur size, etc.) to a general understanding of how the individual looked and behaved.\n\nWith the flurry of literature on multimodal embeddings and their usefulness, I think that new models may draw from both mathematics and neuroscience to figure out how to formalize our insights about how humans integrate data from multiple modalities. We\u2019re already taking a look at whether we can improve performance through localization (i.e. attention)! With a bit more refinement, networks that can learn to generalize to new cases could be on our horizon.", 
        "title": "Flexible Image Tagging with Fast0Tag \u2013"
    }, 
    {
        "url": "https://medium.com/autonomous-agents/part-1-error-analysis-how-not-to-kill-your-puppy-with-neuralnetwork-66a766b6b406?source=tag_archive---------2----------------", 
        "text": "Do you remember getting your first puppy (or kitten or baby) which started responding to your commands. Do you remember that the initial responses were just some random reactions? You tell the puppy to \u201csit\u201d and she is all curious, shaking her butt vigorously in the hope that we will play with her! You say \u201croll-over\u201d and she rotates in the same place chasing her own tail endlessly? (and, you face palm saying \u201cI said *roll*, not rotate\u201d)\n\nWell, welcome to training the Neural Networks.\n\nJust like puppies, you cannot cut open a Neural Network to see what is happening inside that black-box to fix it. If you land up visualizing the innards of the Neural Nets, you shall just see some nodes and connections and some numbers that are associated with it. There is no code, rules, labels, documentation or perceptible schematics that shall allow you to program it.\n\nIn this post, I intend to get a little more deeper in \u2018training ourselves\u2019, the humans, to better observe the behavior of the Neural Network, its error rates, its overall conditioning and change our training techniques to get a desired response from the Neural Network.\n\nBut in Part-1, lets focus on understanding the Error scores.\n\nBefore we start training the Machines, let\u2019s lay out some golden rules (or principles) for us humans first.\n\nWhy these rules and disciplines?\n\nTake a print out of the rules and stick this on your cubicle actually. Ok, enough of analogies, let\u2019s learn about the nature of errors that the Neural Nets commit.\n\nThere are many types of errors. Let\u2019s start with the simplest. Notice that in the wine tasting post, here, we saw the following output\u00a0:\n\nLet\u2019s delve a bit deeper into understanding what the scores are.\n\nAs a recap, the wine tasting example had 3 outputs or classes into which the wine was classified. Given a set of 13 input features, the network choose the most probable cultivar from which the wine came from. The way the network choose a cultivar, was by turning ON or OFF a specific output neuron which was assigned to a cultivar.\n\nThe ON/OFF is considered as a binary state, and the process is called binary classification in statistics. The ON/OFF, 1/0, TRUE/FALSE is a predicted probability state based on some input, activities and threshold as we understood.\n\nIn the wine dataset, we had 178 different wines that was created by 3 different cultivars. We set aside 65% of that dataset which is about 115 wines for training the model. During training, we supervised the outputs to see if the correct cultivar is predicted, and if not, we backpropagated the error to fix the knowledge weights on the network to predict the correct weight on the next iteration. There, we used ESS (Sum of Squared Error) to keep the cost function in check during training. We spent 600 iterations to train the puppy.\n\nAfter full training was complete, we ran the remaining 35% (without backpropagation or any other network optimizations) to validate if the model is predicting correctly. Now, just like ESS was the cost function during training, we need some error mechanism for the validation phase as well. Accuracy, Precision, Recall and F1-scores are just that.\n\nLets first understand some base statistical errors which are components of the scores.\n\nFalse-positives (or Type-1 error): A false-positive is a error when your model wrongly identifies a cultivar as positive for a wine. In other words, if wine-1 is from cultivar-A, then the positive set for wine-1 shall be {cultivar-A} and negative set for wine-1 shall be {cultivar-B, cultivar-C}. When the model is asked to predict the cultivar for wine-1 and the model predicts a class from the negative set (either cultivar-B or C) then we state that we have a false-positive error.\n\nFalse-negative (or Type-2 error): When a model rejects a true positive as a negative, then its a false-negative or type-2 error. In the wine example, if cultivar-A for wine-1 (from the positive set) was not chosen (not predicted correctly), then the rejection of cultivar-A from the output is called a false-rejection or a false-negative.\n\nThe opposite of False-positive and False-negative, is True-positive (A correct cultivar for a wine is predicted) and True-negative (The in-correct cultivars for the wine is rejected).\n\nUsing these base statistics, lets understand the error scores in the validation set now. We have 13 wines from cultivar-A, 26 wines from cultivar-B, and 24 wines from cultivar-C (from the above illustration).\n\nLet\u2019s take a hypothetical scenario of the 13 wine predictions which came from cultivar-A as follows:\n\nWhile we must have 13 {1,0,0} predictions (because all the 13 wines came from cultivar-A), let\u2019s say we have the above predictions instead.\n\nThen, we have the following types of base measures identified:\n\nNow comes the scores.\n\nPrecision is a measure that shows, among all the items that are selected how many are relevant. It is also called the Positive Predictive Value or PPV.\n\nIn our scenario we have 13 predicted positives (Since in this example, we shall get a prediction for every input) and only 8 of them are true-positive. hence our precision score should be\n\nRecall is a measure which portrays among all the possible relevant items that should have been selected, how many are really selected. It is also called True Positive Rate or TPR sometimes also called sensitivity.\n\nIn our case, the relevant items is also 13 (This is because, we have a mutually-exclusive multivariate, and we are predicated a value for every input). So our recall score is as follows:\n\nAccuracy is a measure of proximity of predicted values to its true values. Statistically stated, its the distance between the true value as a reference to the mean of the probability density of the predicted value. The equation of Accuracy is as follows:\n\nThe total population is the sum of all the grid\u2019s that needs to be predicted for each wine, which in our case is 13 relevant classes for cultivar-A and 26 not relevant classes for cultivar-B + cultivar-C. Hence:\n\nHere is a illustration to show the difference between accuracy and precision\n\nPrecision is a measure of repeatability or reproducibility of the prediction. If the model is reproducing similar response to similar inputs each time you predict a response for your input, then its precise (it may not be a accurate prediction where the prediction is close to its true value). Here is another illustration for accuracy and precision\u00a0:\n\nThe F1-score, F-score or the F-measure is a harmonic mean of precision and recall. It is a true measure of the overall performance of the model which considers both precision and recall. You can consider it as a weighted average of precision and recall.\n\nHence in our wine scenario it shall be:\n\nWell, the value of precision, recall and F1-score for a multivariate class, which are \u201cmutually exclusive\u201d and a \u201cprecisely-rounded\u201d where a prediction is available for every input shall all be equal.\n\nBut, in a Neural Network, the value shall not be \u201cprecisely-rounded\u201d to a set of {1,0,0} or {0,1,0} or {0,0,1} in a softmax activation. Instead the value is a probability as shown\n\nHere, the sum of the k-dimensional set adds upto 1 (as the result of the quashing function of the softmax) hence there shall be variance in precision, recall and f1-scores when you run the model. Technically speaking you may see results as following:\n\nNote in the above result, there are 2 wrong predictions denoted by the output \u201cExamples labeled as 1 classified by model as 0: 2 times\u201d\n\nWe should strive for the overall performance of the system to get better (F1-score).\n\nThere are cases or business needs where a high accuracy is demanded while they can be relaxed with precision. In such cases, you strive to improve the \u201crecall\u201d of the system.\n\nAn example can be, a medical diagnostic system which wants all people who have diseases to be predicted correctly by the system that they do indeed have a disease. Maybe, such a medical facility can tolerate false-positives (model predicting that a person has a disease while he does not) because, a drug administered to a normal person who does not have a disease is not as harmful versus not administering a drug to a person who truly had a disease, but the model predicting he does not, may turn out critical.\n\nHence, the choice of which score is more important for predictions is driven by the business case as against trying to improve the model in vacuum.\n\nThis post has established the base scores for analyzing the error on your validation test after the model is trained. In the next posts, we shall see how to improve upon the errors and other numerical conditioning of the Neural Network.\n\nRemember, its not the puppy you train\u2026", 
        "title": "Part-1: Error Analysis \u2014 How not to kill your Puppy with #NeuralNetwork."
    }, 
    {
        "url": "https://medium.com/@ivydatascience/landscape-of-deep-learning-frameworks-aae34564cab?source=tag_archive---------3----------------", 
        "text": "TensorFlow is the newly open sourced deep learning library from Google. It is their second generation system for the implementation and deployment of large-scale machine learning models. Written in C++ with a python interface, it is borne from research and deploying machine learning projects throughout a wide range of Google products and services\u200a\u2014\u200asee Figure 1 below for adoption. Google and the open source community are constantly adding changes including releasing a version that runs on a distributed cluster.\n\nTorch is a neural network library written in Lua with a C/CUDA interface originally developed by a team from the Swiss institute EPFL. At the heart of Torch are popular neural network and optimization libraries which are simple to use, while being flexible in implementing different complex neural network topologies. Finally, Theano is a deep learning library written in python and popular for its ease of use. Using Theano, it is possible to attain speeds rivaling hand-crafted C implementations for problems involving large amounts of data.\n\nSo what are the various metrics we can use to compare open source software libraries in general, and these deep learning libraries in particular? The most common ones are speed of execution, ease of use, languages used (core and front-end), resources (CPU and memory capacity) needed in order to run the various algorithms, GPU support, size of active community of users, contributors and committers, platforms supported (e.g., OS, single devices and/or distributed systems), algorithmic support, and number of packages in their library. Various benchmarks and comparisons are available here, here and here.\n\nOf course, these libraries are not static, but rather dynamic, living repositories, constantly evolving as the user base adds to and modifies them. As Google report in their white paper, published to support the release of TensorFlow, \u201cWe will continue to use TensorFlow to develop new and interesting machine learning models for artificial intelligence, and in the course of doing this, we may discover ways in which we will need to extend the basic TensorFlow system. The open source community may also come up with new and interesting directions for the TensorFlow implementation.\u201d\n\nOver the course of 2015/2016, Google, IBM, Samsung, Microsoft, Nervana, Baidu and others all open sourced their machine learning frameworks. Suffice to say, there are many open source deep learning libraries out there for people to use. Frameworks familiar to researchers and developers in this space include Caffe, CuDNN, Deeplearning4J, CNTK and MXnet. In fact, there is such a plethora of machine learning libraries that many are beginning to ask how do we decide which ones to use, and should we start thinking about combining them to remove confusion and add efficiencies? This is an interesting space to be working in right now, with refinements being added seemingly every day providing a constantly evolving landscape.\n\nIt is certainly an interesting journey we are on as these developments help bring us towards the holy grail of artificial general intelligence, intelligence that can truly multitask just as biological intelligence can. I can\u2019t help sometimes but to step back and watch in awe and wonder as the field of artificial intelligence unfolds and to contemplate the ramifications that go along with this progress.\n\nUpdate August 2017: New frameworks that have appeared in the interim include PyTorch, Caffe2 and Keras.", 
        "title": "Landscape of Deep Learning Frameworks \u2013 Ivy Data Science \u2013"
    }, 
    {
        "url": "https://medium.com/pocket-hrms/what-hr-can-expect-from-deep-learning-in-2016-and-beyond-cf94c342b396?source=tag_archive---------4----------------", 
        "text": "As we are half past 2016, all eyes are locked on forthcoming and future accomplishments. One particular tech trend that\u2019s making a lot of noise these days is Deep Learning, which is a popular branch of machine learning. Deep learning is expected to influence a number of sectors and industries in the near future. Let\u2019s figure out how.\n\nAs we know that computers these days are getting better and better with groundbreaking innovations such as cloud computing, image recognition, 3D printing, language translation, etc., whereas a decade ago, things were complete different. Of all the technological innovations, Deep Learning is one such tech trend that has been making consistent and quick advances since its inception.\n\nHere are a few cool things that you might want to know about Deep Learning and how this emerging tech phenomenon will be affecting HR and businesses in the future:\n\nFirstly, it uses the same algorithms as used in several cognitive domains such as recommendation engines, image processing and language.\n\nAnother prominent aspect of deep learning is that there is no need to command the algorithm for solving the problem, as you simply give the system a whole bunch of examples and it would figure out a solution on its own.\n\nSo can Deep Learning and Artificial Intelligence (AI) really affect the lives of HR?\n\nHere are certain core areas where Deep Learning could enhance the human performances in businesses.\n\nSimply put, Deep Learning would help us identify individuals, who are not a good fit for your organisation. Good hire is all an HR craves and AI is set to make this feat a cakewalk for human resource personnel in the future.\n\nThe vast amount of knowledge around people\u2019s management in businesses is remarkable, but the amount of data in question is often excessively gigantic to keep in our heads and come up with decisions at the speed of light, maybe AI can help us out here.\n\nTo read more such cool articles on HR and technology, contact us here. You can also give us a shout-out at Twitter and Facebook or write to us at sales@sagesoftware.co.in or simply SMS SAGE to 56767.", 
        "title": "What HR can expect from Deep Learning in 2016 and Beyond"
    }
]