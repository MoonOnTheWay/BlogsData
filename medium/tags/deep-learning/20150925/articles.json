[
    {
        "url": "https://chronicles.mfglabs.com/rbm-and-recommender-systems-3fa30f53d1dc?source=tag_archive---------0----------------", 
        "text": "Literature about Deep Learning applied to recommender systems is not very abundant. At MFG, we\u2019ve been working on Salakhutdinov, Mnih and Hinton\u2019s article \u2018Restricted Boltzmann Machines for Collaborative Filtering\u2019 ([1]) and on its possible extension to deep networks such as Deep Belief Networks (DBN) ([2]). It has proven to be competitive with matrix factorization based recommendations. But how could we improve it in order to obviously outperform matrix factorization\u00a0? That\u2019s a great challenge that could be a breakthrough for our activity. ICML was the opportunity for us to catch work in progress in deep learning techniques from universities all around the world and from applications far from recommender systems. We were especially interested in a talk given about RBM and DBN application to genomic. Indeed, constraints that come from genomic representations could find their counterpart in Facebook data recommendation.\n\nIn the following, we just focus on RBM in order to see how to improve the unsupervised training. DBN is just the stacking of RBM pretraining and a fine-tuning that we\u2019re not discussing here.\n\nLet\u2019s first see how to apply RBM to recommender systems.\n\nOur data is a Facebook likes matrix L with N users in lines and M items in columns with coefficient (u,i) being 1 if user u likes item i, 0 otherwise. This matrix is obviously sparse. We pick out randomly n users and m items and then split this matrix in a (n,M) training set and a (N-n,M) test set. The submatrix of likes we wish to predict is (N-n,M-m).\n\nRBM are stochastic neural networks with two layers only\u00a0:\n\n- a layer of I visible units v, which is both designed for input and output\u00a0;\n\nThe number of visible units is the dimension of examples\u00a0: I = M. The two layers are fully interconnected, but there is no connection within each layer. Neurons have binary response.\n\nEach neuron is designed by its activation probability, which depends from the former layer in a sigmoid manner\u00a0:\n\n- c the bias of hidden neurons.\n\nRBM are an energy-based model\u00a0: we can link to each state of the network an energy E(v,h) defined by\u00a0:\n\nThis energy allows us to define a joint probability\u00a0:\n\nSo that we have the marginal\u00a0:\n\nWe learn W, b and c by applying gradient descent to log-likelihood maximization. For instance, we learn the network\u2019s weights by\u00a0:\n\n- The first term, called positive, is easily computed with the empirical visible data and the hidden layer directly resulting from them.\n\n- The second term, called negative, can\u2019t be computed analytically. That\u2019s the key point when studying RBM. We approximate the negative term using a method called Contrastive Divergence. This method lies on Gibbs sampling to evaluate the negative term. For k Gibbs steps, we follow the following picking process\u00a0:\n\nFinally, after a few calculations, we get\u00a0:\n\nRecall that within the test set not all likes are known and that we we wish to predict unknown likes based on known ones. After having trained our network on all items, we predict iteratively for each user the probability of liking the next item. In other words, based on the m known likes, we predict the visible unit m+1. Then we consider this visible unit as a known like and, based on these m+1 known likes, we predict the visible unit m+2. And so on.\n\nSo we just have to compute the probability of picking a visible unit m+1 equal to 1 given the former m visible units\u00a0:\n\nSo we have a method to predict likes based on RBM. Can we improve it using the binary nature of data and their sparsity\u00a0?\n\nIn their paper \u2018Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions\u2019 ([3]), Taehoon Lee and Sungroh Yoon design a new way of performing contrastive divergence in order to fit to binary sparse data.\n\nThe goal of the paper is to identify some DNA fragments. Recall that DNA is a sequence of four types of nucleotides\u00a0: Adenine (A), Cytosine (C), Guanine (G) and Thymine (T). In order to give DNA sequence to a RBM as input, they use orthogonal encoding\u00a0: more precisely, each nucleotide is encoded on 4 bits. A, C, G and T are encoded by 1000, 0100, 0010 and 0001. That\u2019s why their data are binary, but also why they are sparse\u00a0: for example, the simple AGTT sequence is encoded by the 16-dimensional vector 1000001000010001. So they wish to incorporate this prior knowledge on sparsity.\n\nThey convert a DNA sequence of m nucleotides into a binary vector of 4m elements v that is given in input of the RBM. In the computation of the CD, v(0) and v(k) are the original input and its reconstruction using the RBM. Their idea is that the trained RBM should be able to reconstruct precisely the original input. So they design a constraint that fit their specific original input\u00a0: they add a regularization term that penalizes the deviation of the sum of 4 visible units from 1. They call this term categorical gradient. The minimization problem thus becomes\u00a0:\n\nWe can deduce from this problem new update rules for the network parameters.\n\nCould this innovation be applied to recommender systems\u00a0? The easiest way would be to penalize the deviation of the total sum of the reconstruted input from the original one, that is to say, to penalize the user\u2019s reconstructed number of likes from his actual one\u00a0:\n\nBut it should be possible to go further. We could for instance design macro-items, that is to say cluster of items, and, for each user, represent his relation to a macro-item by the array of his likes on this macro-items. Then we would be able to penalize the deviation of each reconstruted macro-like to the actual one. We let you imagine the formula.\n\nThese are ways to explore a generalization of categorical gradient to recommender systems.\n\nWe would like to conclude assessing that, owing to its multiple applications, research in machine learning should always be multidisciplinary. A method used for classification (RBM) may be useful for recommender systems, but also for genomic. And the discoveries made in genomic could in return be of great help for recommender systems. That\u2019s why it is important for us, MFG Labs, to be backing such events as ICML to get the newest ideas and try to enrich our toolbox of machine learning methods. Deep learning is amongst them and deep learning is ever increasing. So let\u2019s keep on learning deep\u00a0!\n\n1 SALAKHUTDINOV, Ruslan, MNIH, Andriy, et HINTON, Geoffrey. Restricted Boltzmann machines for collaborative filtering. In\u00a0: Proceedings of the 24th international conference on Machine learning. ACM, 2007. p. 791\u2013798.\n\n2 SALAKHUTDINOV, Ruslan et HINTON, Geoffrey E. Deep boltzmann machines. In\u00a0: International Conference on Artificial Intelligence and Statistics. 2009. p. 448\u2013455.\n\n3 LEE, Taehoon, KR, A. C., et YOON, Sungroh. Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions.\n\nThanks to Alain Soltani for his contribution to this work.", 
        "title": "RBM and recommender systems \u2013"
    }, 
    {
        "url": "https://chronicles.mfglabs.com/restricted-boltzmann-machines-intelligent-barbies-and-more-in-the-programmable-edition-29-2f270fd65f37?source=tag_archive---------1----------------", 
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter.", 
        "title": "Restricted Boltzmann Machines, intelligent Barbies and more in the Programmable Edition #29"
    }
]