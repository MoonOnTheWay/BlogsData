[
    {
        "url": "https://medium.com/machine-intelligence-report/three-reasons-deep-learning-rocked-2015-e7e63d8b8ee0?source=tag_archive---------0----------------", 
        "text": "DNNResearch\u2019s Alexnet (2012) is to Nirvana\u2019s Nevermind (1991), as deep learning in 2015 is to grunge rock in 1994. In a span of a few years, deep learning (DL) has gone from crashing the party to hosting it.\n\nThat DL rocked software in 2015 has been well documented. The giants of tech snapped up talent and produced a slew of mainstream apps like Moments and Translator. At Google alone, more than 2,000 projects and 70 teams featured DL, producing such 2015 gems as SmartReply, Photos, and revamped voice search & text search. Deepnet-paloozas such as ICLR and NIPS saw explosions in attendance. The keynotes at NVIDIA\u2019s tech conference all focused on DL. Funding reached new heights via startups like Clarifai, Enlitic, and Nervana. It was breathtaking and fun.\n\nOver the year, I read\u200a\u2014\u200aok, scanned\u200a\u2014\u200aover 1,000 new articles on advancements and investments in DL. In my attempt to summarize what emerged in 2015, I noticed a few dominant themes.\n\nWhat became DeepDream produced such reverberations that non-technical friends were sharing its fruits with no mention of the underlying tech. It even spawned costumes for Halloween and a new species: The puppyslug.\n\nThis inceptionism was part of a big trend of generative algorithms that learn from data to produce creative and useful variations of reality. It was usually in the form of images\u200a\u2014\u200aof faces, interiors, and stylized paintings\u200a\u2014\u200abut also seen in music, storytelling, written characters, and 3D models & graphics.\n\nAlso in 2015, Artomatix won NVIDIA\u2019s startup challenge, DeepMind released DRAW, Facebook Eyescream, U. T\u00fcbingen style transfer, and U. Toronto generating images from novel captions. GANs were everywhere.\n\nLearning how to decompose data into meaningful compositional factors is captivating and important. There are obvious applications in digital arts, game design, 3D design tools, and video & audio compression. Coupled with physical synthesis technology like nanoscale 3D printing and genome editing, such techniques should eventually lead to the manifestation of new, useful materials, medicines, and organisms. Pet puppyslugs, anyone?\n\nGoogle\u2019s hesitant self-driving cars are a common sight here in the Valley, but in 2015 Tesla put something kick-ass and slightly dangerous right into the crowd\u2019s hands. Autopilot: A collaborative learning system, designed to both learn from drivers and to help them drive while they find some good music.\n\nSuch human-in-the-loop learning systems became a common reprise from researchers to investors, who finally got that simply incanting \u201cdeep learning\u201d over a pile of data won\u2019t get you far in the real world. Interactive systems afford faster learning and yield copious and meaningful data, which increases in value as the learning algorithms continue to improve.\n\nAlong these lines, 2015 also brought robotics startup Osaro, interactive approaches for image annotation (producing sweet datasets including visual genome and LSUN), and methods to adaptively leverage crowdsourced input as a machine progressively assumes responsibility. Intelligent systems should be designed to be interactive and interpretable, leading the human-machine hybrid to ever-advancing understanding and capability.\n\nGrunge bloomed in Seattle because of the mutual inspiration and collaboration between bands in the local scene. Similarly, deep learning owes its rapid rise to the unprecedented openness of its researchers, many of whom work in large corporations.\n\nOpen software toolkits for DL were already numerous, but 2015 incredibly saw Facebook, Google, Microsoft, Samsung, and NVIDIA open-source their libraries, along with accomplished startup Nervana, and popular Keras. The toolkits now number at least fifty. Facebook and NVIDIA even shared their DL hardware designs.\n\nDespite openness, corporate researchers could still be influenced by their companies\u2019 profit motives. To address this danger that grows with the power the AI that they aspire to build, Elon Musk, Sam Altman, and others committed $1 billion in founding the non-profit OpenAI. With their funding, strong research team, and mission of openness and benevolence, OpenAI should at least prove to be a formidable recruiter for DL R&D talent. Some additional thoughts and questions about OpenAI can be found here.\n\nPredictions are mostly wrong, useless, or boring. Yes, of course, much of the current R&D will continue and advance\u200a\u2014\u200abut what unexpected, game-changing developments might there be in 2016?", 
        "title": "Three reasons deep learning rocked 2015 \u2013 Machine Intelligence Report \u2013"
    }
]