[
    {
        "url": "https://medium.com/@tedsta/deep-learning-in-rust-a-walk-in-the-park-fed6c87165ea?source=tag_archive---------0----------------", 
        "text": "This is a sequel to my previous post several months ago. Last time, I introduced a shoddy library named deeplearn-rs that allowed you to build and (manually) train neural networks that run on your GPU via OpenCL. I told you to \u201ctake a gander\u201d at some atrocious code that constructed a rather useless XOR network and then didn\u2019t bother to train it. For reasons unknown, I still got positive feedback. Thanks, internet\u00a0:)\n\nDeeplearn-rs has come a long way since January!\n\nLet\u2019s see what it looks like before we dive into the details:\n\nThis snippet builds a batched 2-layer network with biased fully connected layers, ReLU activations, and MSE (mean squared error) loss. I\u2019m really pleased with how the API turned out. Check out the full MNIST example to see how it\u2019s trained and validated.\n\nThe Operation (matrix multiplication, in this case) needs the shapes of the input variables and access to the GPU context so that it can create the intermediate GPU buffers it needs. The Graph needs to know the input variables and the shapes of the output variables. It was pretty ridiculous to ask the poor API user to supply of all of these.\n\nThe solution was to separate Operation descriptions from their implementations. I introduced an OpBuilder trait and an OpDescriptor struct.\n\nUnder this system, there are two structs associated with every operation.\n\nBelow is the new matrix multiplication implementation:\n\nNote that <MatMul as OpBuilder>::build figures out the shapes of the input variables, checks for dimension errors, and constructs the MatMulImpl struct, which actually runs the forward and backward passes. The new system is slightly more complex, but much more convenient for the end user.\n\nEven with the improved convenience of node creation, building layers was still too inconvenient for my tastes.\n\nManually creating the weights, adding the nodes, and getting the output variables can be such a drag! I want to build layers with one-liners!\n\nI thought long and hard about how to simplify layer creation for the user. I thought about complicating the operation system further by allowing you to make composite operations out of other operations. And then complicating it even further by giving OpBuilder another associated type \u201cVariables\u201d, which would be a tuple of some VarIndexes for the Graph::add_node to return to the user. Then we\u2019d have to build some notion of which variables to build automatically in OpBuilder::build (i.e. weights). But MatMul seems like a pretty general operation, what if the user doesn\u2019t want their weights made for them? All of this is wayyy too complicated.\n\nInstead, I opted to place layer creation at a higher level of abstraction. I just wrote some functions that build common layer types. Here\u2019s the declaration for the function that adds a biased dense layer:\n\nYou supply the graph, the input to the layer, the layer size (number of neurons), and the initializers for the weights and bias (maybe a normal distribution, for example). You get back the VarIndexes for the output, weights, and biases. Nice!\n\nThere\u2019s now a Trainer struct. Right now, it just backpropagates the gradients to the learnable variables and adds the gradients to the variables at each epoch. I guess that makes it Stochastic Gradient Descent. I intend to implement fancier trainers like RMSprop and Ada Grad eventually. The trainer takes your graph, the number of epochs you want to run, an update function, and your training data. The update function is called every epoch and is where you can read the values of your different variables so that you can monitor your network\u2019s progress.\n\nThe MNIST example runs in 75.6 seconds on my Toshiba Satellite laptop with an Intel Integrated IvyBridge graphics card running the Beignet OpenCL driver. It achieves 89.8% validation accuracy. Pretty terrible, but here I am just stoked that it all works.\n\nNext, I\u2019m going to write a LSTM layer and build a simple char-rnn model.\n\nAfter that, the rough plan is:\n\nI\u2019m graduating from undergrad this coming Saturday (WOOT). Once I find a job and get some money, I\u2019d like to buy a beefy desktop with a real graphics card that can run OpenCL 2.0. Rust\u2019s OpenCL bindings haven\u2019t gotten a lot of love lately, and I\u2019d like to write my own bindings with OpenCL 2.0 support. Then, I\u2019d like to take advantage of the OpenCL 2.0 support in gpuarray-rs and deeplearn-rs. I think device-side kernel enqueuing would be really useful for things like generic broadcast/reduce operations. Stuff for another blog post on another day.", 
        "title": "Deep Learning in Rust: a walk in the park \u2013 Theodore DeRego \u2013"
    }, 
    {
        "url": "https://medium.com/latelier-verrocchio/lintelligence-artificielle-et-l-avenir-de-notre-monde-partie-1-2-5904d164f4cf?source=tag_archive---------1----------------", 
        "text": "Les hommes les plus brillants de cette plan\u00e8te (Elon Musk, Bill Gates ou encore Stephen Hawking) s\u2019inqui\u00e8tent tr\u00e8s fortement de l\u2019\u00e9mergence des Intelligences Artificielles, et je pense que nous devons tous nous y int\u00e9resser de plus pr\u00e8s pour en \u00eatre, au moins, conscient des enjeux.\n\nL\u2019intelligence artificielle est la science dont le but est de faire faire par une machine des t\u00e2ches que l\u2019homme accomplit en utilisant son intelligence. Ce n\u2019est pas qu\u2019un simple concept de sciences fictions associ\u00e9 \u00e0 Terminator ou Star Wars. Elles sont d\u00e9j\u00e0 partout autour de nous, et elles se d\u00e9veloppent \u00e0 une vitesse exponentielle. On peut les classer en 3 grandes cat\u00e9gories en fonction de leur niveau d\u2019intelligence\u00a0:\n\nLe Deep Learning n\u2019est pas la simple cons\u00e9quence de l\u2019augmentation de la puissance de calcul des machines et de l\u2019expansion de nos connaissances en neurosciences\u00a0: c\u2019est \u00e9galement une r\u00e9sultante de la tr\u00e8s forte croissance des donn\u00e9es, des Big data. Ces big data ne sont ni plus ni moins que des informations, que nous cr\u00e9ons chaque jour, et nous en cr\u00e9ons toujours plus. En 2013, 90% des donn\u00e9es et informations disponibles dans le monde dataient d\u2019il y a moins de 2 ans. Tout ce que nous faisons, les 4.2 millions de choses que nous likons sur Facebook et les 500 000 photos que nous partageons sur Snapchat chaque minutes dans le monde, viennent alimenter ces intelligences. Et c\u2019est gr\u00e2ce \u00e0 cela qu\u2019elles se nourrissent, qu\u2019elles apprennent \u00e0 apprendre.\n\nPour faire simple, plus le progr\u00e8s avance, plus il acc\u00e9l\u00e8re\u00a0: en anglais, cela s\u2019appelle the Law of accelerating returns.\n\nEn Mars 2016, l\u2019Intelligence Artificielle d\u00e9velopp\u00e9e par Google, Alpha Go, a battu le meilleur joueur du monde au jeu de Go. On a tous plus ou moins entendu parler de cette nouvelle, sans comprendre r\u00e9ellement ce que celle-ci impliquait. Cette victoire, anecdotique en apparence, est une \u00e9tape suppl\u00e9mentaire vers les formes d\u2019intelligences plus \u00e9volu\u00e9es, vers les Artificial General Intelligence et les Artificial Superintelligence. Si l\u2019on interrogeait les experts mondiaux quelques mois auparavant en leur demandant quand est-ce qu\u2019une IA battra le meilleur joueur de Go du monde, leur r\u00e9ponse aurait \u00e9t\u00e9 \u201cenviron une dizaine d\u2019ann\u00e9e\u201d. Cela s\u2019est finalement produit en quelques mois.\n\nLe Go est un jeu infiniment plus complexe que les \u00c9checs qui n\u00e9cessite des techniques de r\u00e9flexion beaucoup plus \u00e9labor\u00e9es de la part des IA. A titre d\u2019indication, il y a environ 10\u2078\u2070 atomes dans l\u2019univers, et 10\u00b9\u2077\u2070 combinaisons possibles au Go. Comment AlphaGo a t-il fait pour gagner\u00a0? En analysant des millions de parties professionnelles et en y jouant lui m\u00eames des millions de fois. Le Deep Learning. (Si vous souhaitez comprendre comment cette prouesse a pu \u00eatre possible, je vous invite \u00e0 aller regarder cette vid\u00e9o).\n\nLes experts affirment ainsi que nous atteindrons probablement les AGI d\u2019ici 2030 et les ASI devraient suivre vers 2045. Nous serons donc amen\u00e9 \u00e0 cohabiter avec une nouvelle esp\u00e8ce, au moins aussi d\u00e9velopp\u00e9e et intelligente que la notre, d\u2019ici une quinzaine d\u2019ann\u00e9es. Et l\u2019Histoire de notre monde nous a bien appris une chose\u00a0: en parvenant \u00e0 dominer son environnement, l\u2019esp\u00e8ce la plus intelligente devient toujours l\u2019esp\u00e8ce dominante. Tout comme l\u2019Homme domine actuellement le monde animal. Vision du future assez inqui\u00e9tante, logiquement c\u2019est l\u00e0 qu\u2019on commencetous \u00e0 flipper. Qui sait de quoi seront capables ces machines lorsqu\u2019elles seront 10 fois, 100 fois, 1 000 fois plus intelligentes que nous\u00a0? Probablement de progr\u00e8s jamais vu \u00e0 notre si petite \u00e9chelle.\n\nQue feront les chauffeurs de taxi quand les voitures rouleront toutes seules\u00a0? Que feront les employ\u00e9s de bureaux moyens quand les IA seront capables de r\u00e9pondre au t\u00e9l\u00e9phone\u00a0? Que feront les m\u00e9decins quand les IA feront des diagnostics plus efficaces et pertinents\u00a0? Que ferons-nous des avocats qui, en soit, ne font que lire et interpr\u00e9ter des textes de lois\u00a0? Toutes ces choses l\u00e0 ne concernent que des t\u00e2ches bien pr\u00e9cises. C\u2019est \u00e0 dire des ANI, que nous ma\u00eetrisons d\u00e9j\u00e0\u00a0!", 
        "title": "L\u2019intelligence Artificielle et l\u2019avenir de notre monde [Partie 1/2] \ud83d\udcbb"
    }, 
    {
        "url": "https://medium.com/@n4utilius/the-data-pub-abril-2016-segunda-parte-591da9c5d29c?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "The Data Pub \u2014 Abril 2016 [Segunda parte] \u2013 Kevin Martinez \u2013"
    }
]