[
    {
        "url": "https://medium.com/planet-stories/this-company-is-using-timely-satellite-imagery-and-deep-learning-to-predict-a-67-billion-u-s-7346bd0f3643?source=tag_archive---------0----------------", 
        "text": "A data analysis company named Descartes Labs can predict the percentage of farms in the United States that will grow soy or corn next year\u200a\u2014\u200aand this year, their predictions may be more accurate than the US Government\u2019s forecast.\n\nThat may not seem like a big deal, but consider the value of a prediction like this\u200a\u2014\u200acorn is a $67 billion business in America. To understand why and how Descartes Labs made this prediction, it\u2019s good to know a bit about land use maps and how they\u2019re used in the agriculture business.\n\nIn any given square mile of the U.S., land might be covered by concrete for a road, cement for a strip mall, or corn for crops. Using traditional methods, it takes a lot of time to map out all of these different land uses; the process is arduous and must be done by hand, by analysts who pore over historical satellite imagery, ground observations, and surveys. Economic forecasters for agribusiness companies need to know this information\u200a\u2014\u200ahow much of a crop is being planted\u200a\u2014\u200ato predict seasonal outputs. It\u2019s a big business, so they pay for the analysts\u2019 time. And this is where Descartes Labs steps in: They\u2019re able to simplify the process\u200a\u2014\u200asaving time and money\u200a\u2014\u200aby processing a massive amount of data, efficiently and accurately, with the help of Planet Labs imagery.\n\nThe farmlands of United States are some of the highest producing crops in the world.\n\nToday, the benchmark for metrics on land use come in the form of yearly national Cropland Data Layer (CDL) maps published by the U.S. Department of Agriculture (USDA). These maps use year-old data and are inherently not up-to-date when released. They also use freely available, but lower resolution imagery from Landsat 8, a satellite operated by NASA. Landsat 8 collects an image of an area every sixteen days. In short, this means that economic forecasters are using low-resolution imagery that is a year old to predict crop outputs\u200a\u2014\u200aand that leaves a lot of room for improvement.\n\nWith land use changing frequently and a greater need for up-to-date and accurate maps, Descartes Labs is using deep learning technology to analyze imagery faster and predict the future.", 
        "title": "This Company is Using Satellite Imagery & Deep Learning to Predict a $67B Corn Market"
    }, 
    {
        "url": "https://medium.com/machine-intelligence-report/3-favorite-machine-learning-papers-406165f251e5?source=tag_archive---------1----------------", 
        "text": "Well, because 1) these papers are broadly applicable and has a fair chance to influence your product 2) are not loaded with random greek alphabets and useless theorems 3) it\u2019s unlikely you\u2019ll come up with all of it by yourself.\n\nPaper #1 \u201cA Few Useful Things To Know About Machine Learning\u201d Pedro Domingos, 2012.\n\nIf you have been working on machine learning for a little while and are familiar with the basics then this paper delivers great value. It brings together a lot of insights that you may be beginning to feel, but are not sure of. For example, you may see that there are as many ML algorithms as there are ML PhDs in the world, but for your problem it doesn\u2019t matter which variation you pick. When it works, all of them work within few percentages of each other and when they don\u2019t, all of them fail simultaneously. This paper articulates a lot of these kind of observations in a proper way (\u201cMore data beats a cleverer algorithm\u201d).\n\nThis is more of a machine learning system design paper. This is a hard paper because a lot of the things it talks about are hard to appreciate unless you have made the same mistakes yourself. Still it\u2019s highly recommended because once you are forewarned, it\u2019s easier to spot and accept your mistake. It\u2019s very likely that you\u2019ll be coming back to this paper multiple times.\n\nPaper #3 \u201cEfficient Estimation of Word Representations in Vector Space\u201d Mikolov et al., 2013.\n\nThe bulk of applied machine learning is feature engineering. This paper is interesting because it talks about one of the ultimate feature engineering tricks: converting sparse features to dense features. This tutorial based on the paper is also a great place to start. The consequences of this is far reaching, including better generalization of your sparse features and getting more out of your precious labeled data by augmenting it with unlabeled data.\n\nEmbedding is closely related to matrix factorization, which in turn is related to collaborative filtering, as this paper explains (Note:this is additional info and good to know after mastering the embedding technique).\n\nOkay, now you have three more interesting links to bookmark for future reading\u00a0:-)", 
        "title": "3 Favorite Machine Learning Papers \u2013 Machine Intelligence Report \u2013"
    }, 
    {
        "url": "https://medium.com/machine-intelligence-report/what-is-deep-machine-learning-8d4feffd18ba?source=tag_archive---------2----------------", 
        "text": "I\u2019d like to start this one with a quote:\n\nThe linguist refers to Machine Learning and its fixed structure. That means, Deep learning takes out the EXPERT to improve overall performance.\n\nDeep Learning provides for an inherently fascinating subject, and one that I just love to geek out upon. In my last LinkedIn post I spoke of Deep Learning in and as it relates to Web Development (you can read that particular blog post here Deep learning and Web development) today however I want to take a look at deep machine learning. (There are to many synonyms, indeed!)\n\nDeep Learning has provided for amazing achievements within realms that span from speech perception to object recognition. But it\u2019s far from a branch of commuter science that makes for light, easy to understand reading. So here\u2019s my definition of deep machine learning in its most simplistic sense.\n\nMachine learning is the art form of making computers act free from explicit instructions (such as following a specific line of code for each and every action within a program).\n\nDeep machine learning is then the science of programming a computer to learn in much the same way that the neural networks within a human brain works\u200a\u2014\u200athrough many layers and parameters. It is then very much our closest form to truly achieving artificial intelligence.\n\nDelving into Deep machine learning further we find three core concepts: compositions of layers, end-to-end learning and distributed representation. Let\u2019s take a close look at each.\n\nMultiple layers of representation can, again, perhaps best be understood from how a human brain learns, and the neural networks within it that operates upon multiple understandings from a single sensory input, such as the sound of a word (this analogy within the field of deep learning is a pretty consistent occurrence, after all much of what has already been achieved is working towards Artificial Intelligence). And from the speaking of a word to the brain\u2019s understanding there are multiple hidden layers that connect the two, which is exactly the same as with any deep machine learning process.\n\nPerhaps one of the most important things to appreciate about multiple layers of representation is that it\u2019s overcome much of the previous issues faced when computer scientists modelled neuron networks and today instead of simply classifying data they can instead generate the data models for themselves.\n\nPrior to deep machine learning, and within much of the earliest work upon speech recognition, there lay a problem with how densely connected the layers of representations were. This issue, even experienced by the Google speech API, additionally resulted in such systems overfitting (which, to you and I, are plenty of random errors).\n\nToday deep machine learning features end-to-end learning that can allow a computer to learn free from intermediaries and significant human expertise. And as a perfect example of this is the way in which speech recognition advanced with deep learning free from the previously cumbersome (yet necessary) phonetic representations.\n\nAgain we can go back to our human brain analogy to consider how a brain develops over time and the way that these changes affect our extraction of information from the stimuli in question. Take Primary school children as the perfect example\u200a\u2014\u200athey learn their first words phonetically. This then compares to fully fledged adults who are fluent in English who then are free from working out words through a drawn out process that begins with phonetics.\n\nAlgorithms for deep learning are, in essence, applications of the concept of a distributed representation. The idea behind a distributed representation is that observed information is the result of a multitude of factors that work together to produce the outcome. It transforms what would otherwise be humongous amounts of data into a streamlined team of data combinations.\n\nLet\u2019s take a simple example: you want a computer to store data about a collection of vehicles.\n\nWorking this way you would need to store each of these to a single computer memory unit. And of course with each Disney car, BMW or any other vehicle you add this is going to add up to a whole load of data units. Not very efficient, right?\n\nLet us then imagine a fresh way of storing these vehicles. We could use three memory units: one to describe the size (small, medium, and large massive and Disney Cars), the next to store the colour and the third to store the make. We can then store all the vehicles we wanted, all with the very same three memory units. This resourceful way of working is an example of distributed representation. In the most basic sense this then represents groups of neurones working together. And it\u2019s another seriously efficient example of deep machine learning in action.\n\nSo, if you\u2019re ready to partner with a web developer who can truly harness the potential of deep learning then let\u2019s connect.\n\nWant to know about the way in which I work? Great, you can get started with an overview of just how I approach a project right here.", 
        "title": "What is Deep (Machine) Learning? \u2013 Machine Intelligence Report \u2013"
    }, 
    {
        "url": "https://medium.com/@phunter/deep-learning-for-hackers-with-mxnet-2-neural-art-284f3f5ac3ab?source=tag_archive---------3----------------", 
        "text": "Special thanks to Eric Xie for fixing the MXnet cuDNN problem. MXnet can fully utilize cuDNN for speeding up neural art.\n\nNeural art is a deep learning algorithm which can learn the style from famous artwork and apply to a new image. For example, given a cat picture and a Van Gogh artwork, we can paint the cat in Van Gogh style, like this (Van Gogh Self-portrait in 1889 wikipedia):\n\nNeural art comes from this paper \u201cA Neural Algorithm of Artistic Style\u201d by Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge http://arxiv.org/abs/1508.06576. The basic idea is leveraging the power of Convolution Network (CNN) which can learn high level abstract features from the artwork and simulating the art style for generating a new image. These generative network is a popular research topic in deep learning. For example, you might know Google\u2019s Inception which generates a goat-shape cloud image by given a goat image and a cloud image. Facebook also has their similar Deep Generative Image Models\u00a0, inspired by this paper Generative Adversarial Networks where one of the authors, Bing Xu, is also the main author of the great MXnet.\n\nNeural art algorithm has many implementations and applications, for example these two Lua/Torch7 ones LINK1 LINK2. The paper\u2019s gitxiv also includes many interesting applications, for example, generating a neural art gif animation. All of these implemtations uses the VGG model of image classification as mentioned in the paper. With popular demand on github, MXnet has just published its fast and memory efficient implementation. Let\u2019s have fun with MXnet!\n\nHave you clicked the \u201cstar\u201d and \u201cfork\u201d on MXnet github repo? If not yet, do it now! https://github.com/dmlc/mxnet\n\nThe neural art example is under mxnet/example/neural-style/. Since this example needs much computing work, GPU is highly recommended, please refer to my previous blog \u201cDeep learning for hackers with MXnet (1) GPU installation and MNIST\u201d for detailed installation guide. For sure, one can use CPU anyway since mxnet supports seamless CPU/GPU switch, just a reminder, it may take about 40\u201350 minutes for generating an image with CPU.\n\nOptional: MXnet can speed up with cuDNN! cuDNN v3 and v4 both work with mxnet, where v4 is 2\u20133 seconds faster than v3 on my GTX 960 4GB. Please go to https://developer.nvidia.com/cudnn and apply for nVidia Developer program. If approved, one can install CuDNN with CUDA as simple as this (Reference: Install Caffe on EC2 from scratch (Ubuntu, CUDA 7, cuDNN)))\u00a0:\n\nAnd please turn on USE_CUDNN = 1 in make/config.mk and re-compile MXnet for CuDNN support, if not previously compiled. Please also update the python installation if necessary.\n\nFor readers who don\u2019t have an GPU-ready MXnet, the market has these free or paid services and apps for trying Neural Art. Since neural art needs a lot of computing, all these paid or free services need to upload the images to servers, and wait for a long time for finishing processing, usually from hours (if lucky) to weeks:\n\nFor my dear readers who are lucky to have a GPU-ready MXnet, let do it with MXnet! I am going to use an image from my sister\u2019s cat \u201cpogo\u201d and show every single detail of generating an art image, from end to end.\n\nMXnet needs a VGG model. We need to download it for the first time running using download.sh. MXnet version of this VGG model takes about several MB where the Lua version of the same model costs about 1 GB. After having the model ready, let\u2019s put the style image and the content image into the input folder. For example, I give mxnet the cat image as content, and Van Gogh\u2019s painting as style:\n\nAfter 1\u20132 minutes, we can see the output in the output folder like this:\n\nLet\u2019s try painting the cat \u201cPogo\u201d in a modern art style. By replacing Van Gogh with \u2018Blue Horse\u2019 Modern Equine Art Contemporary Horse Daily Oil Painting by Texas Artist Laurie Pace (https://www.pinterest.com/pin/407223991276827181/)\u00a0, pogo is painted like this:\n\nIn the python script run.py, there are some fine tune parameters for better results, and each of them is explained as following:\n\nSince the runtime memory cost is proportional to the size of the image. If\u200a\u2014\u200amax-long-edge was set too large, MXnet may give this out of memory error:\n\nTo solve it, one needs to have smaller\u200a\u2014\u200amax-long-edge: for 512px image, MXnet needs 1.4GB memory; for 850px image, MXnets needs 3.7GB. Please notices these two items:\n\nIf the image size is larger than 600 to 700 pixels, the default workspace parameter in model_vgg19.py may not be enough, and MXnet may give this error:\n\nThe reason is MXnet needs a buffer space which is defined in model_vgg19.py as workspace for each CNN layer. Please replace all workspace=1024 in model_vgg19.py with workspace=2048.\n\nIn this benchmark, we choose this Lua (Torch 7) implementation https://github.com/jcjohnson/neural-style and compare it with MXnet for learning Van Gogh style and painting pogo the cat. The hardware include a single GTX 960 4GB, a 4-core AMD CPU and 16GB memory.\n\nMXnet has efficient memory usage, and it costs only half of the memory as that in the Lua/Torch7 version.\n\nLua/Torch 7 is not able to run with 850px image because of no enough memory, while MXnet costs 3.7GB memory and finishes in 350 seconds.\n\nMemory Runtime\n\nMXnet (w/o cuDNN) 3670MB 350s\n\nMXnet (w/ cuDNN) 2986MB 320s\n\nLua Torch 7 Out of memory Out of memory\n\nWith some invaluable discussion from reddit, and special thanks to alexjc (the author of DeepForger) and jcjohnss (the author of Lua Neural-artstyle), I have this updated benchmark with MXnet\u2019s new magic MXNET_BACKWARD_DO_MIRROR to squeeze memory (github issue). Please update to the latest MXnet github and re-compile. To add this magic, one can simply do:\n\nThe mirror magic slows down a little bit and gains memory saving. With this Mirror magic, a 4GB GPU can process up to 1024px image with 3855MB memory!\n\nSome comments about improving the memory efficiency: currently in the market, the Lasagne version (with Theano) is the most memory efficient Neural Art generator (github link, thanks to alexjc) which can process 1440px images with a 4GB GPU. antinucleon, the author of MXnet, has mentionedthat, gram matrix uses imperative mode while symbolic mode should save more memory by reusing it. I will update the benchmark when the symbolic version is available.\n\nIn short, MXnet can save more memory than that in the Lua version, and has some speed up with CuDNN. Considering the price difference between a Titan X (1000$) and a GTX 960 4GB (220$), MXnet is also eco-friendly.\n\nA note about the speed comparision: Lua version uses L-BFGS for the optimal parameter search while MXnet uses SGD, which is faster but needs a little bit tune-ups for best results. To be honest, the comparision above doesn\u2019t mean MXnet is always 2x faster.\n\nFor readers who want to know MXnet\u2019s secret of efficient memory usage, please refer to MXnet\u2019s design document where all dark magic happens. The link is http://mxnt.ml/en/latest/#open-source-design-notes\n\nTill now, my dear readers can play with Neural art in MXnet. Please share your creative artwork on twitter or instagram with #mxnet and I will check out your great art!\n\n\u201cStyle\u201d itself doesn\u2019t have a clear definition, it might be \u201cpattern\u201d or \u201ctexture\u201d or \u201cmethod of painting\u201d or something else. People believe it can be described by some higher order statistical variables. However, different art styles have different representations, and for a general approach of \u201clearning the style\u201d, it becomes very difficulty to extract these higher order variables and apply to some new images.\n\nFortunately, Convolution Network (CNN) has proved its power of extracting high-level abstract features in the image classification, for example, computers can tell if a cat is in the image by using CNN. For more details, please refer to Yann Lecun\u2019s deep learning tutorial. The power of \u201cextracting high-level abstract features\u201d is used in Neural Art: after couple of layers of convolution operations, the image has lost its pixel-level feature, and only keeps its high-level style. In the following figure from the paper, the author has defined a 5-layer CNN, where the staring night by Van Gogh keeps some content details in the 1st, 2nd and 3rd layer, and becomes \u201csomething looks like staring night\u201d in the 4th and 5th layer:\n\nAnd the author has reached the \u201cAha!\u201d moment: if we put a Van Gogh image and one more other image to the same CNN network, some clever adjustment may make the second image closer to Van Gogh, but keeps some content in the first 3 layers. It is the way to simulate Van Gogh painting style! Moreover, there is a VGG model for image classification in the market for it!\n\nNow the problem becomes an optimization problem: I want the generated picture looks like my cat (the content feature should be kept for the first 3 layers), and I want Van Gogh style (the style feature for the 4th and 5th layer), thus the solution is an intermediate result which has a similar content representation to the cat, and a similar style representation to Van Gogh. In the paper, the author uses a white noise image for generating a new image closer to the content using SGD, and the other white nose image for being closer to the style. The author has defined a magical gram matrix for describing the texture and has used this matrix to defind the loss function which is a weighted mixture of these two white noise image. Mxnet uses SGD for converging it into a image which meets both of the content and style requirement.\n\nFor exmaple, in these 200+ steps of painting pogo the cat, the generated image changes like this:\n\nwhere we can see, in the first 50 epoches, the generated image looks like a simple texture overlap in between the content and the style; with more epoches, the program gradually learns the color, the pattern etc, and becomes stable around 150th epoches, and finally paints pogo the cat in Van Gogh style.\n\nNeural art is not the only method of simulating artwork style and generating new images. There are many other computer vision and graph research papers, for example:\n\nNeural art is a nice demo for convolution network, and people can generate artwork from their own images. Let\u2019s have fun with MXnet neural art. Please share your creative artwork on twitter or instagram and add hashtag #mxnet.\n\nA reminder about the style: if the content image is a portrait, please find a portrait artwork for learning the style instead of a landscape one. It is the same with landscape images, always landscape to landscape. Because the landscape artwork uses different paiting techniques and it doesn\u2019t look good on portrait images.\n\nIn the next blog, I will have detailed introduction to the convolution network for image classification, a.k.a, the dog vs the cat.", 
        "title": "Deep learning for hackers with MXnet (2): Neural art"
    }, 
    {
        "url": "https://medium.com/@ravlondon/understand-latest-trends-in-deep-learning-in-20-minutes-62ab787e7b4?source=tag_archive---------4----------------", 
        "text": "Nice 20 minute presentation from the Yoda of Machine Learning, Andrew Ng.\n\nParticularly liked the \u2018Rocket\u2019 analogy to describe why Deep Learning is taking off right now as well as how Baidu Light can help the blind.", 
        "title": "Understand latest trends in Deep Learning in 20 minutes"
    }
]