[
    {
        "url": "https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2?source=tag_archive---------0----------------", 
        "text": "In this article I want to provide a tutorial on implementing the Asynchronous Advantage Actor-Critic (A3C) algorithm in Tensorflow. We will use it to solve a simple challenge in a 3D Doom environment! With the holidays right around the corner, this will be my final post for the year, and I hope it will serve as a culmination of all the previous topics in the series. If you haven\u2019t yet, or are new to Deep Learning and Reinforcement Learning, I suggest checking out the earlier entries in the series before going through this post in order to understand all the building blocks which will be utilized here. If you have been following the series: thank you! I have learned so much about RL in the past year, and am happy to have shared it with everyone through this article series.\n\nSo what is A3C? The A3C algorithm was released by Google\u2019s DeepMind group earlier this year, and it made a splash by\u2026 essentially obsoleting DQN. It was faster, simpler, more robust, and able to achieve much better scores on the standard battery of Deep RL tasks. On top of all that it could work in continuous as well as discrete action spaces. Given this, it has become the go-to Deep RL algorithm for new challenging problems with complex state and action spaces. In fact, OpenAI just released a version of A3C as their \u201cuniversal starter agent\u201d for working with their new (and very diverse) set of Universe environments.\n\nAsynchronous Advantage Actor-Critic is quite a mouthful. Let\u2019s start by unpacking the name, and from there, begin to unpack the mechanics of the algorithm itself.\n\nAsynchronous: Unlike DQN, where a single agent represented by a single neural network interacts with a single environment, A3C utilizes multiple incarnations of the above in order to learn more efficiently. In A3C there is a global network, and multiple worker agents which each have their own set of network parameters. Each of these agents interacts with it\u2019s own copy of the environment at the same time as the other agents are interacting with their environments. The reason this works better than having a single agent (beyond the speedup of getting more work done), is that the experience of each agent is independent of the experience of the others. In this way the overall experience available for training becomes more diverse.\n\nActor-Critic: So far this series has focused on value-iteration methods such as Q-learning, or policy-iteration methods such as Policy Gradient. Actor-Critic combines the benefits of both approaches. In the case of A3C, our network will estimate both a value function V(s) (how good a certain state is to be in) and a policy \u03c0(s) (a set of action probability outputs). These will each be separate fully-connected layers sitting at the top of the network. Critically, the agent uses the value estimate (the critic) to update the policy (the actor) more intelligently than traditional policy gradient methods.\n\nAdvantage: If we think back to our implementation of Policy Gradient, the update rule used the discounted returns from a set of experiences in order to tell the agent which of its actions were \u201cgood\u201d and which were \u201cbad.\u201d The network was then updated in order to encourage and discourage actions appropriately.\n\nThe insight of using advantage estimates rather than just discounted returns is to allow the agent to determine not just how good its actions were, but how much better they turned out to be than expected. Intuitively, this allows the algorithm to focus on where the network\u2019s predictions were lacking. If you recall from the Dueling Q-Network architecture, the advantage function is as follow:\n\nSince we won\u2019t be determining the Q values directly in A3C, we can use the discounted returns (R) as an estimate of Q(s,a) to allow us to generate an estimate of the advantage.\n\nIn this tutorial, we will go even further, and utilize a slightly different version of advantage estimation with lower variance referred to as Generalized Advantage Estimation.\n\nIn the process of building this implementation of the A3C algorithm, I used as reference the quality implementations by DennyBritz and OpenAI. Both of which I highly recommend if you\u2019d like to see alternatives to my code here. Each section embedded here is taken out of context for instructional purposes, and won\u2019t run on its own. To view and run the full, functional A3C implementation, see my Github repository.\n\nThe general outline of the code architecture is:\n\nThe A3C algorithm begins by constructing the global network. This network will consist of convolutional layers to process spatial dependencies, followed by an LSTM layer to process temporal dependencies, and finally, value and policy output layers. Below is example code for establishing the network graph itself.\n\nNext, a set of worker agents, each with their own network and environment are created. Each of these workers are run on a separate processor thread, so there should be no more workers than there are threads on your CPU.\n\n~ From here we go asynchronous ~\n\nEach worker begins by setting its network parameters to those of the global network. We can do this by constructing a Tensorflow op which sets each variable in the local worker network to the equivalent variable value in the global network.\n\nEach worker then interacts with its own copy of the environment and collects experience. Each keeps a list of experience tuples (observation, action, reward, done, value) that is constantly added to from interactions with the environment.\n\nOnce the worker\u2019s experience history is large enough, we use it to determine discounted return and advantage, and use those to calculate value and policy losses. We also calculate an entropy (H) of the policy. This corresponds to the spread of action probabilities. If the policy outputs actions with relatively similar probabilities, then entropy will be high, but if the policy suggests a single action with a large probability then entropy will be low. We use the entropy as a means of improving exploration, by encouraging the model to be conservative regarding its sureness of the correct action.\n\nA worker then uses these losses to obtain gradients with respect to its network parameters. Each of these gradients are typically clipped in order to prevent overly-large parameter updates which can destabilize the policy.\n\nA worker then uses the gradients to update the global network parameters. In this way, the global network is constantly being updated by each of the agents, as they interact with their environment.\n\nOnce a successful update is made to the global network, the whole process repeats! The worker then resets its own network parameters to those of the global network, and the process begins again.\n\nTo view the full and functional code, see the Github repository here.\n\nThe robustness of A3C allows us to tackle a new generation of reinforcement learning challenges, one of which is 3D environments! We have come a long way from multi-armed bandits and grid-worlds, and in this tutorial, I have set up the code to allow for playing through the first VizDoom challenge. VizDoom is a system to allow for RL research using the classic Doom game engine. The maintainers of VizDoom recently created a pip package, so installing it is as simple as:\n\nOnce it is installed, we will be using the environment, which is provided in the Github repository, and needs to be placed in the working directory.\n\nThe challenge consists of controlling an avatar from a first person perspective in a single square room. There is a single enemy on the opposite side of the room, which appears in a random location each episode. The agent can only move to the left or right, and fire a gun. The goal is to shoot the enemy as quickly as possible using as few bullets as possible. The agent has 300 time steps per episode to shoot the enemy. Shooting the enemy yields a reward of 1, and each time step as well as each shot yields a small penalty. After about 500 episodes per worker agent, the network learns a policy to quickly solve the challenge. Feel free to adjust parameters such as learning rate, clipping magnitude, update frequency, etc. to attempt to achieve ever greater performance or utilize A3C in your own RL tasks.\n\nI hope this tutorial has been helpful to those new to A3C and asynchronous reinforcement learning! Now go forth and build AIs.\n\nIf you\u2019d like to follow my writing on Deep Learning, AI, and Cognitive Science, follow me on Medium @Arthur Juliani, or on twitter @awjuliani.\n\nIf this post has been valuable to you, please consider donating to help support future tutorials, articles, and implementations. Any contribution is greatly appreciated!", 
        "title": "Simple Reinforcement Learning with Tensorflow Part 8: Asynchronous Actor-Critic Agents (A3C)"
    }, 
    {
        "url": "https://medium.com/@waleedka/traffic-sign-recognition-with-tensorflow-629dffc391a6?source=tag_archive---------1----------------", 
        "text": "This is part 1 of a series about building a deep learning model to recognize traffic signs. It\u2019s intended to be a learning experience, for myself and for anyone else who likes to follow along. There are a lot of resources that cover the theory and math of neural networks, so I\u2019ll focus on the practical aspects instead. I\u2019ll describe my own experience building this model and share the source code and relevant materials. This is suitable for those who know Python and the basics of machine learning already, but want hands on experience and to practice building a real application.\n\nIn this part, I\u2019ll talk about image classification and I\u2019ll keep the model as simple as possible. In later parts, I\u2019ll cover convolutional networks, data augmentation, and object detection.\n\nThe source code is available in this Jupyter notebook. I\u2019m using Python 3.5 and TensorFlow 0.12. If you prefer to run the code in Docker, you can use my Docker image that contains many popular deep learning tools. Run it with this command:\n\nNote that my project directory is in ~/traffic and I\u2019m mapping it to the /traffic directory in the Docker container. Modify this if you\u2019re using a different directory.\n\nMy first challenge was finding a good training dataset. Traffic sign recognition is a well studied problem, so I figured I\u2019ll find something online.\n\nI started by googling \u201ctraffic sign dataset\u201d and found several options. I picked the Belgian Traffic Sign Dataset because it was big enough to train on, and yet small enough to be easy to work with.\n\nYou can download the dataset from http://btsd.ethz.ch/shareddata/. There are a lot of datasets on that page, but you only need the two files listed under BelgiumTS for Classification (cropped images):\n\nAfter expanding the files, this is my directory structure. Try to match it so you can run the code without having to change the paths:\n\nEach of the two directories contain 62 subdirectories, named sequentially from 00000 to 00061. The directory names represent the labels, and the images inside each directory are samples of each label.\n\nOr, if you prefer to sound more formal: do Exploratory Data Analysis. It\u2019s tempting to skip this part, but I\u2019ve found that the code I write to examine the data ends up being used a lot throughout the project. I usually do this in Jupyter notebooks and share them with the team. Knowing your data well from the start saves you a lot of time later.\n\nThe images in this dataset are in an old\u00a0.ppm format. So old, in fact, that most tools don\u2019t support it. Which meant that I couldn\u2019t casually browse the folders to take a look at the images. Luckily, the Scikit Image library recognizes this format. This code will load the data and return two lists: images and labels.\n\nThis is a small dataset so I\u2019m loading everything into RAM to keep it simple. For larger datasets, you\u2019d want to load the data in batches.\n\nAfter loading the images into Numpy arrays, I display a sample image of each label. See code in the notebook. This is our dataset:\n\nLooks like a good training set. The image quality is great, and there are a variety of angles and lighting conditions. More importantly, the traffic signs occupy most of the area of each image, which allows me to focus on object classification and not have to worry about finding the location of the traffic sign in the image (object detection). I\u2019ll get to object detection in a future post.\n\nThe first thing I noticed from the samples above is that images are square-ish, but have different aspect ratios. My neural network will take a fixed-size input, so I have some preprocessing to do. I\u2019ll get to that soon, but first let\u2019s pick one label and see more of its images. Here is an example of label 32:\n\nIt looks like the dataset considers all speed limit signs to be of the same class, regardless of the numbers on them. That\u2019s fine, as long as we know about it beforehand and know what to expect. That\u2019s why understanding your dataset is so important and can save you a lot of pain and confusion later.\n\nI\u2019ll leave exploring the other labels to you. Labels 26 and 27 are interesting to check. They also have numbers in red circles, so the model will have to get really good to differentiate between them.\n\nMost image classification networks expect images of a fixed size, and our first model will do as well. So we need to resize all the images to the same size.\n\nBut since the images have different aspect ratios, then some of them will be stretched vertically or horizontally. Is that a problem? I think it\u2019s not in this case, because the differences in aspect ratios are not that large. My own criteria is that if a person can recognize the images when they\u2019re stretched then the model should be able to do so as well.\n\nWhat are the sizes of the images anyway? Let\u2019s print a few examples:\n\nThe sizes seem to hover around 128x128. I could use that size to preserve as much information as possible, but in early development I prefer to use a smaller size because it leads to faster training, which allows me to iterate faster. I experimented with 16x16 and 20x20, but they were too small. I ended up picking 32x32 which is easy to recognize (see below) and reduces the size of the model and training data by a factor of 16 compared to 128x128.\n\nI\u2019m also in the habit of printing the min() and max() values often. It\u2019s a simple way to verify the range of the data and catch bugs early. This tells me that the image colors are the standard range of 0\u2013255.\n\nWe\u2019re getting to the interesting part! Continuing the theme of keeping it simple, I started with the simplest possible model: A one layer network that consists of one neuron per label.\n\nThis network has 62 neurons and each neuron takes the RGB values of all pixels as input. Effectively, each neuron receives 32*32*3=3072 inputs. This is a fully-connected layer because every neuron connects to every input value. You\u2019re probably familiar with its equation:\n\nI start with a simple model because it\u2019s easy to explain, easy to debug, and fast to train. Once this works end to end, expanding on it is much easier than building something complex from the start.\n\nTensorFlow encapsulates the architecture of a neural network in an execution graph. The graph consists of operations (Ops for short) such as Add, Multiply, Reshape,\u00a0\u2026etc. These ops perform actions on data in tensors (multidimensional arrays).\n\nI\u2019ll go through the code to build the graph step by step below, but here is the full code if you prefer to scan it first:\n\nFirst, I create the Graph object. TensorFlow has a default global graph, but I don\u2019t recommend using it. Global variables are bad in general because they make it too easy to introduce bugs. I prefer to create the graph explicitly.\n\nThen I define Placeholders for the images and labels. The placeholders are TensorFlow\u2019s way of receiving input from the main program. Notice that I create the placeholders (and all other ops) inside the block of with graph.as_default(). This is so they become part of my graph object rather than the global graph.\n\nThe shape of the images_ph placeholder is [None, 32, 32, 3]. It stands for [batch size, height, width, channels] (often shortened as NHWC)\u00a0. The None for batch size means that the batch size is flexible, which means that we can feed different batch sizes to the model without having to change the code. Pay attention to the order of your inputs because some models and frameworks might use a different arrangement, such as NCHW.\n\nNext, I define the fully connected layer. Rather than implementing the raw equation, y = xW + b, I use a handy function that does that in one line and also applies the activation function. It expects input as a one-dimensional vector, though. So I flatten the images first.\n\nI\u2019m using the ReLU activation function here:\n\nIt simply converts all negative values to zeros. It\u2019s been shown to work well in classification tasks and trains faster than sigmoid or tanh. For more background, check here and here.\n\nThe output of the fully connected layer is a logits vector of length 62 (technically, it\u2019s [None, 62] because we\u2019re dealing with a batch of logits vectors).\n\nA row in the logits tensor might look like this: [0.3, 0, 0, 1.2, 2.1,\u00a0.01, 0.4,\u00a0\u2026.., 0, 0]. The higher the value, the more likely that the image represents that label. Logits are not probabilities, though\u200a\u2014\u200aThey can have any value, and they don\u2019t add up to 1. The actual absolute values of the logits are not important, just their values relative to each other. It\u2019s easy to convert logits to probabilities using the softmax function if needed (it\u2019s not needed here).\n\nIn this application, we just need the index of the largest value, which corresponds to the id of the label. The argmax op does that.\n\nThe argmax output will be integers in the range 0 to 61.\n\nChoosing the right loss function is an area of research in and of itself, which I won\u2019t delve into it here other than to say that cross-entropy is the most common function for classification tasks. If you\u2019re not familiar with it, there is a really good explanation here and here.\n\nCross-entropy is a measure of difference between two vectors of probabilities. So we need to convert labels and the logits to probability vectors. The function sparse_softmax_cross_entropy_with_logits() simplifies that. It takes the generated logits and the groundtruth labels and does three things: converts the label indexes of shape [None] to logits of shape [None, 62] (one-hot vectors), then it runs softmax to convert both prediction logits and label logits to probabilities, and finally calculates the cross-entropy between the two. This generates a loss vector of shape [None] (1D of length = batch size), which we pass through reduce_mean() to get one single number that represents the loss value.\n\nChoosing the optimization algorithm is another decision to make. I usually use the ADAM optimizer because it\u2019s been shown to converge faster than simple gradient descent. This post does a great job comparing different gradient descent optimizers.\n\nThe last node in the graph is the initialization op, which simply sets the values of all variables to zeros (or to random values or whatever the variables are set to initialize to).\n\nNotice that the code above doesn\u2019t execute any of the ops yet. It\u2019s just building the graph and describing its inputs. The variables we defined above, such as init, loss, predicted_labels don\u2019t contain numerical values. They are references to ops that we\u2019ll execute next.\n\nThis is where we iteratively train the model to minimize the loss function. Before we start training, though, we need to create a Session object.\n\nI mentioned the Graph object earlier and how it holds all the Ops of the model. The Session, on the other hand, holds the values of all the variables. If a graph holds the equation y=xW+b then the session holds the actual values of these variables.\n\nUsually the first thing to run after starting a session is the initialization op, init, to initialize the variables.\n\nThen we start the training loop and run the train op repeatedly. While not necessary, it\u2019s useful to run the loss op as well to print its values and monitor the progress of the training.\n\nIn case you\u2019re wondering, I set the loop to 201 so that the i % 10 condition is satisfied in the last round and prints the last loss value. The output should look something like this:\n\nNow we have a trained model in memory in the Session object. To use it, we call session.run() just like in the training code. The predicted_labels op returns the output of the argmax() function, so that\u2019s what we need to run. Here I classify 10 random images and print both, the predictions and the groundtruth labels for comparison.\n\nIn the notebook, I include a function to visualize the results as well. It generates something like this:\n\nThe visualization shows that the model is working\u00a0, but doesn\u2019t quantify how accurate it is. And you might\u2019ve noticed that it\u2019s classifying the training images, so we don\u2019t know yet if the model generalizes to images that it hasn\u2019t seen before. Next, we calculate a better evaluation metric.\n\nTo properly measure how the model generalizes to data it hasn\u2019t seen, I do the evaluation on test data that I didn\u2019t use in training. The BelgiumTS dataset makes this easy by providing two separate sets, one for training and one for testing.\n\nIn the notebook I load the test set, resize the images to 32x32, and then calculate the accuracy. This is the relevant part of the code that calculates the accuracy.\n\nThe accuracy I get in each run ranges between 0.40 and 0.70 depending on whether the model lands on a local minimum or a global minimum. This is expected when running a simple model like this one. In a future post I\u2019ll talk about ways to improve the consistency of the results.\n\nCongratulations! We have a working simple neural network. Given how simple this neural network is, training takes just a minute on my laptop so I didn\u2019t bother saving the trained model. In the next part, I\u2019ll add code to save and load trained models and expand to use multiple layers, convolutional networks, and data augmentation. Stay tuned!", 
        "title": "Traffic Sign Recognition with TensorFlow \u2013 Waleed Abdulla \u2013"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-20-e45a87a8a01f?source=tag_archive---------2----------------", 
        "text": "The service lets you start a conversation with up to 100 participants and display translations of everything you say in their preferred language in real-time, though it\u2019s not perfect yet and works best in one-to-one conversations.\n\nSkymind Academy gives software engineers and data scientists the knowledge necessary to build deep learning applications, covering the math, best practices, enterprise data pipelines and deployment\n\nMicrosoft dataset aims to help researchers create tools to answer questions as well as people\n\nMicrosoft has released a set of 100,000 questions and answers that artificial intelligence researchers can use in their quest to create systems that can read and answer questions as well as a human.", 
        "title": "\u2014 Issue #20 \u2013"
    }, 
    {
        "url": "https://medium.com/@vatsals/on-the-beauty-of-technology-c77de82ef0a7?source=tag_archive---------3----------------", 
        "text": "This is the beauty of technology. Teams build software to solve a particular problem (or a set of problems) in a specific domain. And it ends up enabling applications and empowering people in ways the creators would never have imagined in their wildest dreams!", 
        "title": "Silicon Serendipity \u2026 \u2013 Vatsal H. Shah \u2013"
    }
]