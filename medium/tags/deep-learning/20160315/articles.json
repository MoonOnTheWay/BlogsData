[
    {
        "url": "https://chatbotnewsdaily.com/getting-real-with-deep-learning-3b7ef698766d?source=tag_archive---------0----------------", 
        "text": "It was nearly 30 years ago that I first got infatuated with Artificial Intelligence (AI) and I ended up focusing both my undergraduate and graduate engineering research on applications of Artificial Neural Networks (ANNs). My first two jobs after graduate school stayed in the same groove; over 6 years I developed AI and machine learning techniques to address real world problems that ranged from recognizing human speech and natural language, to converting handwriting to searchable digitized text, and to streamlining maintenance procedures in nuclear reactor cores. So it is with a mix of amazement and amusement that I am soaking up the resurgence of AI and machine learning as the buzzword-du-jour: \u201cDeep Learning\u201d.\n\nDeep Learning is very visible in the high hopes we hold for driverless cars and in the triumph of machines over chess champions. It is less conspicuously and more frequently used in the form of Apple\u2019s Siri, Amazon\u2019s Echo, playlists generated on Spotify, that auto-tag feature on Facebook Photos, the voice assistant that answers the phone when you call your bank, or when your fingerprint is recognized by a machine.\n\nOver the past three decades, several developments in the world of computing have resulted in machine learning moving out of esoteric science into mainstream applications.\n\nApologies for my rough whiteboard sketch below, but this is my attempt to show how Moore\u2019s law has delivered on a cascade of technologies and put more advanced advanced science into the hands of consumers.\n\nA Google search on \u201cDeep Learning\u201d produces 30M+ results. For the technicians among us there are pretty good overviews online [see Wikipedia]. To keep it relatively simple for most readers, I will break this down as follows.\n\nDeep Learning is a computational technique whereby you train a computer with examples of correlated data. For example, we present two different images of the letter \u201ca\u201d and associate the images with the English letter \u201ca\u201d\u200a\u2014\u200anote that the two image examples below are fairly different in appearance and yet map to the same letter.\n\nThe more examples of images we show the computer, the better it gets at building that association until we find that when it sees a totally new image of the letter \u201ca\u201d it can correctly identify it as being a letter \u201ca\u201d and not a letter \u201cb\u201d. This is inspired by what occurs in early childhood learning in humans. In practice you would also show it images of other letters so the Deep Learning software can correctly recognize a set of hand written letters via say a camera on your smartphone.\n\nTo demonstrate how this same model works with other data sets, assume you have weather data for the past five years. You could train the model above to associate historical weather patterns on a day in the past with pressure, temperature, wind and precipitation data for the previous day along with date, time and seasonal information. You could then use the trained system to project what tomorrow\u2019s weather will be based on similar data for today.\n\nDeep Learning is based on recognizing/classifying patterns in data: at its core, a deep learning system uses statistical / probabilistic methods, or a weighted graph to develop knowledge of dependencies between input data and outcomes. Based on the data in the training set (say the 5 years of historical weather data in the example above), the calculations within the statistical model or interdependencies in graph are adjusted so known inputs produce known outputs. The theory then extends to the fact that a trained Deep Learning system will correctly recognize the pattern when looking at new examples of it. Some learning systems will recognize the existence of a pattern and others will categorize it as one of many.\n\nAnother quick-and-dirty sketch below attempts to demonstrate the over simplified innards of a Neural Network based classifier as it gets trained by examples of the letter \u201ca\u201d: certain connections in the graph get reinforced for the correct output (the letter \u201ca\u201d) and other connections are reinforced when presented with images of other letters. The process is hypothesized to be similar to neurons firing in our brains in response to specific stimuli.\n\nDeep Learning is not straightforward: As easy as the teams at Google\u2019s Tensor Flow, Kaggle, etc., are trying to make it for everybody to use deep learning, there are a few important features of deep learning that one needs to bear in mind:\n\nBessemer\u2019s Amit Karp put out a good post yesterday that presents one perspective on the Deep Learning Opportunity (quoted below).\n\nMy view breaks down the opportunity into three realms:\n\nApplications that leverage Deep Learning: Amit\u2019s post has some depth on this. Additionally, any application that leverages data science, can also leverage deep learning (hence Kaggle plays here?).\n\nInfrastructure for deep learning: although Google\u2019s Tensorflow, open source modules and other platforms by large players like Facebook, Apple, and Amazon are making the technology more accessible, there remains ample room for PaaS and IaaS plays to make Deep Learning easier, more scalable and more robust.\n\nServices: Every milestone technology boom (mobility, big-data, e-commerce, cloud computing, client-server computing, etc.) has presented a huge opportunity in the form of niche services vendors. Deep Learning services vendors today are ostensibly IBM and a few other tech integrators, but the \u201cMu Sigma of Deep Learning\u201d is yet to emerge.\n\nThe risks? I am already seeing unrealistic expectations, hype, overuse and the lack of talent.\n\nI am happy to share detailed views on the opportunities in this space if you get in touch with me, but this post is probably too long already.", 
        "title": "Getting real with Deep Learning \u2013"
    }, 
    {
        "url": "https://medium.com/@hanaken/alphago-%E3%81%94%E3%81%A8%E3%81%8D-%E3%81%A7%E9%A8%92%E3%81%8E%E3%81%99%E3%81%8E-ba40f860165?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "AlphaGo\u201c\u3054\u3068\u304d\u201d\u3067\u9a12\u304e\u3059\u304e \u2013 \u82b1\u7530\u8ce2\u4eba \u2013"
    }, 
    {
        "url": "https://blog.init.ai/joining-init-ai-to-build-deep-learning-for-conversational-apps-623545ada595?source=tag_archive---------2----------------", 
        "text": "As I write, DeepMind\u2019s algorithm is surpassing the world\u2019s best Go players in ability. AlphaGo plays against Lee Se-dol, the world\u2019s second best Go player, in a set of five matches this week. Soon, the same deep learning technologies will approach human-level natural language ability. That means we, as software users, will be able to converse with our computers in a natural way. Our utterances towards our machines will have fewer constraints, yet still be correctly interpreted.\n\nWe at init.ai are bringing deep learning technology to conversational apps. We are studying the research in the field that is openly published every day. We have seen the steady performance gains on tasks in vision and language over the past several years. It is time to start building upon these proven technologies which are now available.\n\nWho are you?\n\nWhy is this now possible? Deep learning describes a set of trainable components that stack together in many layers. These components, neural networks, are fully differentiable. Their gradient, or derivative, may be calculated throughout the network given an error signal. An error signal communicates how much incorrect behavior the network has, and how to reduce the error.\n\nIt appeared, for many years, that these components were too difficult to optimize. Altogether, they lack a mathematical property that simpler machine learning algorithms have: convexity. Convexity ensures that an algorithm always makes progress during training. But recent research shows for large non-convex models, training is as possible as for convex models.\n\nAlong an axis, a critical point on the function can be a minimum, maximum or saddle point. Critical points of high dimensional models are increasingly unlikely to be local minima. Thus, an optimization algorithm almost-always has an escape route to reach an accurate score. Deep learning research formulates training schemes to bypass saddle points via these routes.\n\nNow that training any given model is likely, the important choice is to shape your model architecture to solve your problem. This is the differentiable programming paradigm: specify a model shape but learn the logic from data.\n\nMany model shapes have arisen in the research, and several apply to language understanding. The word2vec algorithm represents a word with a dense vector of learned parameters. The vectors are derived from the word\u2019s contexts within a relevant dataset. The contextual knowledge word vectors provide is like the visual context AlphaGo exploits to reduce its game search tree.\n\nGiven a word vector model, the next advancement is the sequence model. The recurrent neural network is the simplest, and the long short-term memory model and the gated recurrent unit model build upon it. These models can process sequences of inputs and perform a task on each token in the sequence, or on the entire sequence.\n\nThe sequence-to-sequence model goes further. It digests an input sequence into a summary, which it expands into an output sequence. Sequence-to-sequence can perform language translation.\n\nFurther developments have two prominent branches: attention-based models and automata-augmented models. Each performs both sequence and sequence-to-sequence tasks.\n\nAttention-based models review inputs and cached intermediate states to weight their relative importance. These models are able to perform text and image question answering tasks.\n\nAutomata augmented models give recurrent neural networks a differentiable memory unit. These units could be a stack, queue, deque or random access memory array, as in the Neural Turing Machine. Such models are algorithmically capable. They can learn to perform a sequence of steps to implement some simple algorithms.\n\nThe Neural GPU may be one of the most impressive. It improves upon the Neural Turing Machine by parallelizing its computations through convolution operations. Essentially, it learns cellular automata to implement algorithms.\n\nAt init.ai, we will be employing some of these and other relevant models for powering truly conversational apps.", 
        "title": "Joining Init.ai to build deep learning for conversational apps."
    }, 
    {
        "url": "https://gab41.lab41.org/it-has-been-interesting-to-watch-deep-learning-evolve-over-the-past-four-years-aa4000d7d7e?source=tag_archive---------3----------------", 
        "text": "It has been interesting to watch deep learning evolve over the past four years. Deep learning has made some significant advances, but the progress in unsupervised learning has caught my eye recently. I was academically birthed from the womb of a Frequentist, but the impact of deep Bayesian models cannot be ignored. At the Lab we recently completed the D*Script challenge on handwriting author identification. I found myself delving into deep generative models and actually enjoying it. However, I got hung up on some of the key components that differentiate generative architectures from discriminant architectures. The purpose of this post is to highlight some of the differences between generative and discriminative auto-encoders, and how one could use generative models (specifically DRAW) to solve the problem of writer identification in handwritten documents.\n\nThe main method for deep unsupervised learning is the auto-encoder. The traditional auto-encoder (AE) is typically made up of dense, fully connected layers of a feed-forward neural network. AEs are unsupervised in that their targeted output is the same as their input. The goal of AEs is to compress the input data and then attempt to recreate it (see figure below), much like crushing a soda can and then trying to bend it back into its original form.\n\nThe first half of the AE is called the encoder, because it compresses the original input and encodes it into a latent space. Often this latent space has a lower dimensionality than the input. The second half is called the decoder, and its job is to undo and recreate what the encoder compressed. The loss function simply measures how much information was lost when comparing the original input to the reconstruction; the better the reconstruction, the lower the loss. The encoder and decoder are arbitrary functions. More recently other types of auto-encoders have been innovated.\n\nThe variational auto-encoder (VAE) was innovated with beautiful Bayesian theory to support its existence. VAEs differ from traditional AEs in that they have a generative component and want to say something about the distribution of the latent space P(z|x). After data are compressed by the encoder, the encoding is used to define the parameters of a latent posterior distribution. In other words, we travel from a prior to a posterior in light of the data. From this distribution we can randomly draw a sample z_i\u00a0, conditioned on input x, for the decoder to reconstruct back into the original input. The decoder is the generative portion of the VAE that makes sense of the latent sample. If we return to the crushed can analogy, this is like crushing a soda can, having it engulfed by The NeverEnding Story\u2019s the \u201cNothing\u201d, having manifest itself on Earth in one of many forms of human sorrow and suffering, and then reconstructing that back into a soda can (see figure below).\n\nTwo significant benefits emerge from this:\n\n1) The input no longer has a direct route from beginning to end as with the AE, thus the decoder must learn how to disentangle the randomly drawn latent sample. This allows the decoder to model complex and even multi-modal distributions.\n\n\u00a02) Given a randomly drawn sample z_i, not conditioned on input x, the decoder can \u201challucinate\u201d a reconstruction from the randomness. This allows it to generate samples that look like real world data.\n\nHowever, there are a few obstacles to overcome with the latent posterior P(z|x) from which z_i is drawn. First, you won\u2019t be able to find it in the forest, because it is untrackable. Second, in practice, exact inference of the posterior is usually intractable. The later means we cannot differentiate what we need to and must use approximate inference. One method is to approximate P with a distribution we know, call it Q. How close we come using Q is evaluated using the Kullback-Leibler Divergence. This is the first part of the final loss function: the latent loss. But we still can\u2019t back-propagate through this architecture. Enter the Diederik (Kingma). In the enlightening paper Auto-Encoding Variational Bayes, Kingma et. al developed the \u201creparameterization trick\u201d that transforms z_i from being defined probabilistically as z_i ~ Q(z|x) to being defined deterministically as z_i = \u03bc + \u03c3*\u03b5, where \u03b5 is a random noise from a standard normal distribution (Note: This specific reparameterization is for the normal distribution. Others exist for the other families of distributions.). \u03bc and \u03c3 are parameters learned through back-propagation. The encoder helps define the parameters of Q(z|x), while the decoder attempts to recreate the input from the random sample z_i. How well we reconstruct the latent sample provides the second part of the VAE\u2019s final loss function: the reconstruction loss.\n\nSo far we have talked about auto-encoders in an unsupervised setting. How do we make the leap to supervised or semi-supervised learning using the auto-encoder architecture? The traditional way to use an AE in the supervised setting is to encode the data and use the latent representation, call it h_i, as a feature-rich replacement for the input. Then all of the h\u2019s are used as a dataset to train a separate classifier. This approach uses two separate architectures (AE and classifier) and hence two separate loss functions. The VAE, on the other hand, already has two parts to its loss function in its single architecture. In another Kingma et. al paper titled Semi-Supervised Learning with Deep Generative Models, a third loss is added to the final loss function: the classification loss (here is a presentation by Kingma and Welling that adds more clarity and visuals to their paper). This three-loss architecture is well poised to tackle handwriting challenges. But before we jump into the possible VAE end-to-end solution, let\u2019s briefly explore the handwriting authorship challenge.\n\nAuthor identification is a very difficult problem in handwriting which often gets confused with handwriting optical character recognition (OCR). OCR is concerned with which letter is written, whereas author identification is concerned with how a letter is written. There are a few obstacles that stand in the way of analyzing writing styles. First, unique features need to be captured about each author. Without an automated process and handwriting experts, this can be a very time intensive problem. Second, any noise (lines, watermarks, stains, etc.) can throw off even the most sophisticated algorithm. Data for many of the handwriting competitions are blank white paper with handwriting in black ballpoint pen. See the ICDAR competition for an example. Third, in practice you don\u2019t always know who wrote what. Many handwritten documents in the wild don\u2019t have a signature, and even if they do there is no guarantee it belongs to the name signed. Thus, there is a classification part and a data retrieval part to this problem. So, what do VAEs have to do with all of this? I believe generative models have the end-to-end answer.\n\nThere are two VAE papers that could Captain Planet their powers together for an end-to-end solution. The first is DRAW, and the second is the Semi-Supervised VAE already mentioned. DRAW is a VAE with an attention mechanism, a very clever attention mechanism. It attends to the most important part of the image while iteratively reading the data and writing the data onto a \u201ccanvas.\u201d If you haven\u2019t seen the video of DRAW in action, please click here. Attention in a VAE could be valuable, because it could be taught to attend to the features that separate one writer from another. The DRAW paper has an example of classification, but it only uses the encoder with attention. It achieves excellent results without the decoder or reconstruction, but the reconstruction loss can add a lot of value. One value-add is noise removal. Auto-encoders have long been used to remove imperfections from images (see here and here for examples). One could take pristine competition data, add noise (lines, watermarks, stains, etc.) to the inputs, and at reconstruction time the model would see the clean target image it should have recreated, thus learning what to ignore. In order to preserve the reconstruction with attention in the DRAW network, Kingma et. al add the semi-supervised (or supervised, if that\u2019s what your heart and data desire) to the VAE in a way that it contributes to the whole system. It allows for writing styles to be incorporated into the model, which is something of tremendous worth to the problem at hand. Adding in the latent loss, this three-loss DRAW architecture could be well poised to address the handwriting writer identification problem.", 
        "title": "Deep Generative Models are a Good Idea for Handwriting"
    }
]