[
    {
        "url": "https://medium.com/@DonDodge/how-is-google-using-machine-learning-adff7bfdc333?source=tag_archive---------0----------------", 
        "text": "Machine Learning will impact everything we do at Google. You can already see the results in Google products you use today. Artificial Intelligence, Machine Learning, Deep Learning, and Big Data are buzzwords we hear every day. What do these words mean? How will Machine Learning make my life better? Let\u2019s take a look.\n\nWhat is Machine Learning? Isn\u2019t that Artificial Intelligence? Not exactly. Let\u2019s spend a minute defining the terms. AI is a high level term used to describe any approach to make a computer smart. AI started out as a programmed set of rules that could quickly sort through mountains of data to find the desired answer. But, AI rules couldn\u2019t learn or adapt to new data. You could add more \u201crules\u201d to handle new data, but there was no \u201clearning\u201d. Machine Learning (ML) is a new type of Artificial Intelligence (AI) that lets computers learn without being explicitly programmed. ML is a set of classifiers and algorithms that can teach themselves to grow and adapt when exposed to new data. Machine Learning can learn. This is a BIG deal. Deep Learning (DL) is a particular technique within Machine Learning. It uses an Artificial Neural Network with many layers to learn optimal model parameters. DL creates an algorithm that will automatically decide which features work best to accomplish a task. Finally, Big Data is a term used to describe LOTS of seemingly unrelated data, that upon further analysis could become very useful.\n\nWe are already seeing Machine Learning embedded in services like Gmail, Search, Maps, and even in what ads we see. Let\u2019s look at some examples in Gmail. Priority Inbox automatically identifies your important incoming messages and separates them out from everything else. It learns over time what is important to you, and what isn\u2019t. Smart Reply is another Inbox feature that suggests up to three responses based on the emails you get. For those emails that only need a quick response, it can take care of the thinking and save time spent typing. This is especially helpful on mobile phones. The responses get better over time as the system learns.\n\nGoogle Search and Google Maps employ Machine Learning too. When you start typing in the search box it automatically anticipates what you might be looking for and provides suggested search terms. The suggestions could be based on past searches, what is popular now, or where you are at the time.\n\nGoogle Assistant is a new example of Machine Learning on Android, helping you with everyday tasks. Click on the video at left to see what it can do for you. Google Assistant makes it easy to buy movie tickets while on the go, to find a perfect restaurant for your family to grab a quick bite before the movie starts, and then help you navigate to the theater. Today, Google Assistant is available on the new Google Pixel phone. Older phones have an earlier version called Google Now.\n\nSelf Driving Cars are probably the most sophisticated example of Machine Learning in action. If you drive around Mountain View, California you will likely see a Google self-driving car. They have been on campus for several years, and have logged 700,000 miles of accident-free autonomous driving. Watch this video to see what the onboard computers \u201csee\u201d and how they react as they drive the car. It is truly amazing!\n\nMachine Learning will radically transform and improve all kinds of applications. It is already seeping into things like; Security/threat detection, Fraud detection, Recommendation engines for eCommerce, NLP for legal documents, emails, patent searches, Video/voice/speech recognition, Health screening, Cancer detection, Fact checking, Weather or Financial models, and many more.\n\nIn an effort to accelerate innovation, Google is making the core Machine Learning framework Tensor Flow available as Open Source. Google Cloud is also providing access to Machine Learning APIs for Natural Language Processing, Speech Recognition, Language Translation, and Vision/Image Recognition.\n\nIf you are interested in becoming a Google Partner to take advantage of these and other Google technologies, sign up here.\n\nNext time I will take a look at Virtual Reality and Augmented Reality (AR/VR) and how it will change the world. Click the Follow button to be notified when my next post is available.\n\nDisclosure: Thoughts and opinions expressed here are my own, and do not necessarily reflect those of my employer.\n\nBio: Don Dodge works on the Google Cloud Technology Partner team. Don is a veteran of five start-ups including Forte Software, AltaVista, Napster, Bowstreet, and Groove Networks. Prior to Google, Don was Director of Business Development for Microsoft\u2019s Emerging Business Team. You can follow Don on Medium, on Twitter @DonDodge or on Facebook", 
        "title": "How is Google using Machine Learning? \u2013 Don Dodge >Root Access \u2013"
    }, 
    {
        "url": "https://medium.com/@NathanBenaich/news-in-artificial-intelligence-and-machine-learning-48aa618cab1b?source=tag_archive---------1----------------", 
        "text": "Intel CEO, Brian Krzanich, announced that Intel Capital will invest $250m in the next two years in the autonomous vehicle (AV) ecosystem, focused on problems in connectivity, communication, context awareness, deep learning, security and safety. When viewed in the context of the fund\u2019s short-lived intention to sell $1bn worth of portfolio holdings in March this year (it was cancelled in May), I think this shows Intel is serious on going long with AI. Indeed, the company purchased recently Nervana Systems and Movidius, which could help it\u2019s larger AV program and the race against NVIDIA.\n\nNauto, the startup offering a direct to consumer network of cloud-connected dashboard cameras applied to car insurance, inked a data sharing agreement and investment from Toyota Research Institute, BMW iVentures and Allianz Ventures (thanks Moritz for sharing!). One of the reasons for the immense progress in AI is data crowdsourcing. Collaborations between startups and incumbents who have mature products and distribution scale makes a lot of sense. Others to watch: Nexar (dashboard camera for road safety) and Mapillary (crowdsourced street maps).\n\nIt\u2019s been a phenomenal Q3 for NVIDIA, which blew public market analysts away by recognising $2bn in revenue, up 54% from last year. The share price popped 30% in a single day, which tells me that public markets have yet to truly appreciate the impact of AI. While the lioonshare of the company\u2019s revenue came from gaming, it launched the Drive PX 2 platform, a partnership with Baidu for their AVs and a data collection partnership with TomTom. So much more to come I\u2019m sure!\n\nUniversity of Michigan, which has a simulated urban and suburban environment for testing automated and connected vehicles, launched open-access automated cars for academics and industry partners to advance driverless research. The cars are powered by PolySync middleware (check out the open source project here).\n\nAfter an audacious talk at TC Disrupt announcing a driverless car kit, Comma.ai founder George Hotz received a special order from the National Highway and Traffic Safety Administration to answer detailed questions on safety, testing and performance. Instead of complying, George stated that \u201cdealing with regulators and lawyers\u2026 isn\u2019t worth it\u201d and pulled the product entirely. Unsurprisingly cowboy-like!\n\nAuto OEMs are working with an Automotive Grade Linux operating system for their connected car projects without the involvement of Apple or Google.\n\nNew York\u2019s Mount Sinai Hospital will be using an electronic patient record data processing platform built by CloudMedx to identify patients at risk of developing congestive heart failure, a condition affecting an estimated 5 million Americans.\n\nHBR walks through how a visitor to an AI-infused hospital might be like. It helps frame how AI introduces a new user experience paradigm centered around contextual awareness, personalisation and seamless interaction with the physical and digital world.\n\nA Harvard research group has developed a system that uses generative deep learning models trained on chemical structures to output novel structures using representations of learned chemical knowledge. This is an exciting means to supercharge exploration of a complex search space.\n\nSlack, the business messaging platform used by 4 million people every day, will be stepping up its game with AI-driven productivity tools. Stuart Butterfield made the case for building an application that draws from enterprise resource planning, marketing, sales, business intelligence and other enterprise systems to answer complex queries that otherwise require painfully inefficient searching through troves of data. This will probably be the fruits of Noah Weiss\u2019s \u201cSearch, Learning and Intelligence\u201d team set up in January, which now counts two dozen machine learning engineers.\n\nRus Salakhutdinov, Associate Professor of Machine Learning at CMU, whose group publishes widely in deep learning was hired by Apple as their inaugural Director of AI Research. His work has explored transfer learning (the ability for models to train on one task/data type and be used on a different one), reinforcement learning and unsupervised learning. The team is growing!\n\nFei Fei Li, Director of the Stanford Artificial Intelligence Lab and Stanford Vision Lab, was hired by Google to lead their Google Cloud Machine Learning group along with Jia Li, who is Head of Research at Snap Inc. and did her PhD work with Fei Fei Li. Both were involved in building ImageNet, the large-scale image database that helped catalyse breakthroughs in computer vision.\n\nGoogle announced a many updates to their Cloud Platform offering. First, they launched a Cloud Jobs API which uses machine learning to understand how job titles and skills relate to one another and what job content, location, and seniority are the closest match to a jobseeker\u2019s preferences. Second, developers will be able to access NVIDIA and AMD GPUs in the cloud starting from 2017! Third, the company dropped pricing for the Cloud Vision API as a result of running the models on their proprietary TPU hardware. Fourth, the Cloud Natural Language API is pushed publicly and the Translation API is live with their state of the art Neural Translation Machine. The company also announced a $4.5m grant to the Montreal Institute for Learning Algorithms, which notably includes Yoshua Bengio. The Montreal office will also open a deep learning and AI research group.\n\nThe Backchannel features a piece on Google\u2019s Assistant product, which runs across multiple of the company\u2019s products including Home and the Pixel phone. Namely, it talks about The Transition\u200a\u2014\u200aa two year period of AI training that will help Google \u201cmove from systems that are explicitly taught to ones that implicitly learn.\u201d Fernando Pereira, who leads Google\u2019s projects in natural language understanding, likens the launch of the Assistant to that of Search: \u201cIt\u2019s going to be way more fluent, more able to help you do what you want to, understand more of the context of the conversation, be more able to bring information from different sources together.\u201d\n\nBryan Johnson, founder of Braintree and OSFund, opined a piece on his newest venture, Kernel, which seeks to (wait for it\u2026) build the world\u2019s first implantable neural prosthetic for human intelligence enhancement. This will be a long but fascinating journey: Bryan suggests that \u201ceach market approved product we create will require approximately $200M and 7\u201310 years\u201d. Without any details on roadmap and how it works, hard to say much! Watch this space.\n\nMicrosoft signs a cloud partnership with OpenAI, which will see the research institute run their experiments on NVIDIA Tesla and Pascal GPUs on the Azure cloud. This follows Microsoft\u2019s mission to democratise access to AI\u200a\u2014\u200athe company is picking up some serious steam.\n\nGoogle DeepMind and Blizzard announced a collaboration to open up StarCraft II as a complex testing environment for AI research. The game requires exploration of partially observable environments, long-term planning, memory and multi-agent collaboration, making it rather fascinating. More resources on StarCraft in AI are available on GitHub here.\n\nThe US National Science and Technology Council\u2019s Subcommittee on Machine Learning and Artificial Intelligence publish a whitepaper entitled \u201cPreparing for the future of artificial intelligence\u201d. It explores the current state of AI, its existing and potential applications, and the questions that are raised for society and public policy by the progress in AI. Here are some important recommendations for US Federal agencies (page 40):\n\nOn the topic of accountability, this piece sets out five key principles for technologists to onboard: responsibility, explainability, accuracy, auditability and fairness. Nature magazine run two pieces (here and here) exploring this black box problem.\n\nIn the EU, the General Data Protection Regulation that will come into effect in 2018 prohibits any automated decision that \u201csignificantly affects\u201d EU citizens. This includes techniques that evaluate a person\u2019s \u201cperformance at work, economic situation, health, personal preferences, interests, reliability, behavior, location, or movements.\u201d What\u2019s more, the rule gives the right to EU citizens to review how a particular service made a particular algorithmic decision.\n\nOutgoing President Obama sat down with MIT Media Lab Director, Joi Ito, to discuss AI. The technology, in his words, \u201cpromises to create a vastly more productive and efficient economy. If properly harnessed, it can generate enormous prosperity and opportunity.\u201d\n\nBritish Prime Minister Theresa May announced an ambitious Industrial Strategy Challenge Fund to help Britain \u201ccapitalise on its strengths in cutting-edge research like AI and biotech\u201d, as well as further tax credits and government investment worth \u00a32 billion per year by 2020 for R&D.\n\nWIRED\u2019s Rowland Manthorpe runs a profile piece on the Leverhulme Centre for the Future of Intelligence, a think tank that seeks to explore the nature and impact of AI. Projects include trust and transparency (i.e. interpretability) of AI models, policy and responsible innovation and kinds of (general) intelligence.\n\nHere\u2019s a chart from the World Economic Forum on the change in share of jobs from 1980 to 2012, which shows that many of the jobs that AI is suggested to automate away have indeed already fallen.\n\nFacebook\u2019s News Feed came under significant scrutiny over the proliferation of fake content and the echo chambers that it can create, namely in the context of the Trump/Clinton election campaign (see Blue Feed, Red Feed). Tim O\u2019Reilly explores the problem of editorial curation in a world of infinite information and limited attention. In the piece, Tim and Matt Cutts, former head of the web spam team at Google, rightly state that while Facebook\u2019s pursuit of engagement on its content (vs. link quality for Google search) might optimise for revenue, it ends up producing \u201cshady stories, hoaxes, incorrect information, or polarizing memes as an unintended consequence.\u201d", 
        "title": "News in artificial intelligence and machine learning"
    }, 
    {
        "url": "https://medium.com/@dsouza.amanda/unraveling-a-keras-model-30021c7db497?source=tag_archive---------2----------------", 
        "text": "Keras is a great library for hands-on on neural networks, and it has a ton of great examples that makes it very easy to create ANNs & DNNs. So easy in fact, that you could even build one without knowing what\u2019s going on.\n\nI used the CNN model from this Keras blog post to create a simple sentiment analysis model. But to fully understand what I had just done, I had to dig a little deeper.\n\nThe basic model outlined in the post is using pre-trained word embeddings of the text to train a CNN for sentiment analysis. I have shown it below, with a few minor changes to padding sizes (border_mode=\u2019same\u2019), so that the convolution output size stays the same as its input (for simplicity).\n\nFirst, to understand the intuition behind this model, you can refer to Kim et al.\u2019s paper on using CNN\u2019s for sentence classification, as well as this Quid blog post. To summarize briefly, the convolution operation over N word embedding vectors (where N\u200a\u2014\u200afilter size) can be said to check for the presence of absence of N-grams in the input word vectors, and that along with a maxpooling layer results in the most salient features being extracted as a feature for training the fully connected layers.", 
        "title": "Unraveling a Keras model \u2013 Amanda Dsouza \u2013"
    }, 
    {
        "url": "https://medium.com/@mslavescu/i-think-the-deep-learning-wikipedia-description-is-pretty-accurate-especially-this-section-c257e76880a3?source=tag_archive---------3----------------", 
        "text": "I think the Deep learning Wikipedia description is pretty accurate,\u00a0especially\u00a0this\u00a0section:\n\nUnfortunately deep learning became lately more of a buzzword for all the cool results shown in many areas from image/voice recognition to self driving cars.\n\nWhat most people don\u2019t know is that this method became popular in the last few years, because of advances to the decision components in neural networks (the basis of deep learning) and the fact that GPUs started to be used\u00a0for\u00a0computation, which accelerated a lot the required computing\u00a0tasks\u00a0and\u00a0allowed\u00a0for\u00a0large datasets\u00a0processing.\n\nDeep learning is a very effective supervised and (kind of) unsupervised machine learning method, for certain areas, for example very good at fuzzy recognition of objects/patterns in images.\n\nThe main concern I have is that there is this belief (even across some experts) that this method will solve a lot of (all) the problems,\u00a0maybe\u00a0in\u00a0next\u00a020\u00a0years,\u00a0but\u00a0not\u00a0now,\u00a0and\u00a0see\u00a0bellow\u00a0why.\n\nBeing a fuzzy (or probabilistic)\u00a0method, it is not really safe (just by itself) to use it in all situations, especially where the robustness and precision of the output is critical.\n\nSo if high safety is a requirement, unless there are very good safeguards in place, a deep neural network (DNN) may not be the best idea, and I\u2019m looking at self driving car space for now, although any precise control of a robot in unknown environments fits the same category.\n\nThe other main problem with deep learning is its speed of convergence/learning, which currently may not give us the most efficient way to learn things, but because it is easy to use, we just throw more computing resources at it, and at the end it will provide the expected (or unexpected) behavior.", 
        "title": "I think the Deep learning Wikipedia description is pretty accurate, especially this section:"
    }, 
    {
        "url": "https://medium.com/@mslavescu/join-us-in-shaping-the-future-self-driving-cars-7ff0626ce0ce?source=tag_archive---------4----------------", 
        "text": "To get involved in this initiative see the links at the bottom of this article.\n\nThank you Asim Wagan for sharing this excellent document:\n\nThey are open to comments and suggestions, so we should really voice out our ideas and concerns,\u00a0and\u00a0through\u00a0the\u00a0Open Source\u00a0(OS) Self\u00a0Driving\u00a0Car (SDC)\u00a0project\u00a0provide\u00a0safe and solid\u00a0solutions.\n\nHere\u00a0is an\u00a0excerpt\u00a0from\u00a0the\u00a0document\u00a0above,\u00a0calling\u00a0for general public engagement in ensuring that the best and most comprehensive policies\u00a0are\u00a0created:\n\nThe report is from Sept 2016, so it is the right time for all of us to think hard on these policies and provide our best ideas and feedback.\n\nFor a lively and interactive discussion\u00a0on\u00a0these\u00a0policies, I invite you to join the #sdc_policies_and_regu channel\u00a0part\u00a0of\u00a0ND013\u00a0Slack\u00a0team,\u00a0that Udacity created for their SDC Nanodegree, Open Source SDC and SDC challenges:\n\nIf you\u2019ll like to get involved in OS SDC development see also my previous articles:\n\nI\u2019ve started the Open Source Self Driving Car Initiative in Toronto/GTA, the project page and details are here:\n\nJoin the discussion and participate on the project\u2019s OSSDC Slack team here:", 
        "title": "Join us in shaping the future Self Driving Cars \u2013 Marius Slavescu \u2013"
    }, 
    {
        "url": "https://medium.com/@mslavescu/carlos-thanks-for-your-interest-be6cc5a857f4?source=tag_archive---------5----------------", 
        "text": "Carlos, thanks for your interest!\n\nI already provide Amazon AMIs with the same software stack, I explain in my instructions, on GitHub\u00a0how\u00a0to\u00a0use\u00a0them, but I\u2019ll write more about that in next parts of this series.\n\nSo those who don\u2019t have a good GPU can run DNN training there.\n\nBecause for OS SDC the datasets are very big and we plan to collect a lot of them, running on AWS will become very expensive.\n\nFor example I\u2019ve been running training for 9 hours on a p2.xlarge (the minimum for serious work) and it cost me $12. And in 9 hours you do very little work.\n\nFor anyone who wants to get seriously into OS SDC development, I recommend to invest in at least a mid-range deep learning machine, this will help also the community as we are trying to build a shared infrastructure with our own GPUs.\n\nI\u2019ll write more about all of these in the next parts of this series.", 
        "title": "Carlos, thanks for your interest! \u2013 Marius Slavescu \u2013"
    }, 
    {
        "url": "https://medium.com/@mslavescu/thats-exactly-what-i-felt-when-started-to-deep-dive-into-self-driving-cars-sdc-development-about-976b01e7a8ba?source=tag_archive---------6----------------", 
        "text": "That\u2019s exactly what I felt when started to deep dive into self driving cars (SDC) development about 5 weeks ago.\n\nIt is not easy to do it, but through my series:\n\nand with the work done as part of ND013 community we can advance this area and ensure everyone will benefit and be safe while riding SDC in the near future.", 
        "title": "That\u2019s exactly what I felt when started to deep dive into self driving cars (SDC) development about\u2026"
    }, 
    {
        "url": "https://medium.com/@mslavescu/excellent-article-about-what-we-will-see-in-the-near-future-98830e405ac9?source=tag_archive---------7----------------", 
        "text": "Excellent article about what we will see in the near future.\n\nIt may happen even earlier\u00a0:-)\n\nRobotics and AI will be more and more used in all aspects of our life, and we need to prepare ourselves for that time.\n\nOne very good way to #LearnTeachInventMake and get ready, is to get involved in Open Source Self Driving Car development, this being the area of robotics where AI is promoted a lot and it will affect us all.\n\nTo get started check my series on:\n\nAnd\u00a0I\u2019m\u00a0very serious,\u00a0when\u00a0I\u00a0say anyone can and should get involved!", 
        "title": "Excellent article about what we will see in the near future."
    }
]