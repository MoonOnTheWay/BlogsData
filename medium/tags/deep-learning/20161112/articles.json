[
    {
        "url": "https://blog.insightdatascience.com/isee-removing-eyeglasses-from-faces-using-deep-learning-d4e7d935376f?source=tag_archive---------0----------------", 
        "text": "How long does it usually take you to pick out a new pair of glasses at the store? 10 minutes? 30? When left unsupervised, I\u2019ve admittedly taken over an hour. Head tilt. Half smile. Side shot. Next pair. It\u2019s a big deal, as it is scientifically established that the type of glasses you wear impacts perception of your intelligence, success, and attractiveness.\n\nIt\u2019s 2016; there must certainly be some sort of technology that has solved this problem. Of course there is! DITTO technologies developed a virtual mirror that allows customers to try on hundreds of products from the comfort of their homes. Center your face, turn left, turn right, and you\u2019re done. You can sit back and evaluate which glasses pairs your face with your style and character.\n\nThere is, however, one small caveat for those of us who are optically challenged\u200a\u2014\u200ayou have to remove your glasses to use the virtual mirror. In case you\u2019ve never had the experience, it\u2019s even more difficult to choose glasses if you need your original glasses to see. You can only hope that the small, rectangular blobs you see in the mirror have perfectly framed your blurred visage.\n\nWouldn\u2019t it be great if people could leave their glasses on, and the software automatically removed them? Imagine walking into a retail store and having a virtual mirror remove your glasses and replace them with different products in real-time. Implementing this vision for DITTO is the challenge that I faced as an Insight Fellow.\n\nThe task of removing eyeglasses from faces is not a new one, by far. A hefty amount of scientific literature documents a variety of image processing algorithms to remove eyeglasses, often for the goal of improving facial recognition technologies. Using some thoughtful math with features such as contrast, edges, and congruency, these techniques typically detect and subtract the image pixels containing the glasses and then synthesize the obfuscated facial region through smoothing or inference. Despite the ingenuity, these algorithms can fall short at the recognition of the glasses and/or the reconstruction of the face. They can also notably struggle with generalizing across different skin tones and correcting for shadows, magnification, and glare caused by the frames and lenses.\n\nSo why on earth would anyone think that, in 4 weeks, they could improve upon something that groups of specialists have devoted years to developing? The answer is that there has been a recent insurgence of algorithms, open-source code and tools, and GPU computing that have opened the floodgates for application of deep learning to an endless array (or should I say\u2026tensor) of high-dimensional data problems. The power of deep learning is that you don\u2019t have to design and optimize an algorithm based on the features that you think are important for your task; you only have to provide examples and the neural network identifies and weights the features that are relevant. Rather than engineering an algorithm to identify glasses, one to remove them, and then one to reconstruct faces, you can simply train one neural network to do all-of-the-above as one computation. Not only does this require much less time and domain knowledge, but the resulting network can generalize across a much broader range of inputs.\n\nThe most critical components for the success of a deep learning project is having the right data, having a lot of it, and having hardware that can handle it. While there are a number of cloud computing options, I was fortunate to have access to PC with a Tesla K40 graphics card that was generously donated to Insight from NVIDIA. On the data front, DITTO was able to supply to an incredibly unique dataset: thousands of faces with without glasses (I should note at this point that the faces shown in this article belong to DITTO employees, not their customers). Armed with access to DITTO\u2019s API, and the IDs of 20,000 customers, I used their technology to project glasses onto the customers\u2019 faces and thus create a very large, labeled dataset. The only missing component at this point was designing a neural network to remove the glasses.\n\nWhile I knew that a convolutional neural network would be the best choice for recognizing an abstract object such as glasses, and is robust to spatial variance, it wasn\u2019t immediately clear to me how to go about removing the glasses and more importantly reconstructing the faces. My original inspiration came from an online article about a man who trained a network on \u201cBlade Runner\u201d and then had it watch \u201cA Scanner Darkly.\u201d The article included a link to the dissertation of Terrance Broad, called \u201cAutoencoding Video Frames,\u201d which was a total gold mine of relevant information. Another valuable source was an article by Insight Alumnus TJ Torres at StitchFix, which also centered around the concept of using a convolutional neural network as an autoencoder.\n\nAn autoencoder is a network that is trained to reconstruct an input, in our case an image, and in doing so it compresses the relevant features of inputs into a lower dimensional space (labeled \u201cz\u201d below). The cost function used for backpropagation is the mean squared error (MSE) between the input image and the reconstructed image, which is the output. A cartoon of this approach would be something like this:\n\nGiven enough training, an autoencoder would eventually learn to recreate the original image. For Terrence and TJ, this approach was useful for creating generative models from the latent or \u201cz\u201d space. For my application, however, I didn\u2019t need a generative model because I already had the outputs that I wanted. I simply needed the network to take an image of a face with glasses as an input and construct that same face without the glasses as an output. To do this, I started where anyone who is rapidly iterating on a new project should start, with open-source code. I found code for a convolutional neural network autoencoder in TensorFlow, written by Parag K. Mital, and redefined the cost function to be the difference between the desired output (the face without glasses) and the reconstructed image, but still provided the face with glasses as the input. The network was thus provided with tens of thousands of examples of the input (face with glasses) and desired output (face, no glasses) and was trained to perform that computation. As a technical aside, doing so means that the network is no longer really an autoencoder anymore\u200a\u2014\u200ait\u2019s just a convolutional neural network with a symmetric topology and a linearized layer in the middle.\n\nThis method, coined \u201ciSee,\u201d worked quite well. That is, of course, after considerable tuning of the hyperparameters. The images below show the model\u2019s performance on faces of three DITTO employees, which were not included in the training set. The network had never seen their faces before but was still able to reconstruct the area of their faces that had been obfuscated by the glasses. It was also able to remove the glasses, which varied in size and position across the images. In contrast to more traditional image processing techniques found in the literature, this method can work for different facial structures and skin tones as long as similar faces are provided in the training data.\n\nYou might notice that the images above have been flattened to grayscale and are low in resolution. This was necessary because the amount of RAM (64GB) on the PC that I used placed limitations on number of weights/parameters that can be held in memory at once, and thus placed constraints on the total size of the network. It\u2019s important to note that the total amount of data was not a limiting factor, as TensorFlow permits batch processing, which makes it possible to work with a large dataset on a home computer. In order to implement the iSee method, I needed a network that was both deep (many layers) and wide (many filters per layer). For the example shown above, I used 6 convolutional layers, each with 90 3x3 filters per layer, and the original images were cropped and downsampled down to 66x66 pixels. The network was trained over 250 epochs in batches of 20 images. As a point of reference, the images below were generated using the same hyperparameters except that only 70 filters were used per layer and the network was only trained over 50 epochs in batches of 25 images.\n\nTuning hyperparameters can be incredibly time-consuming; not only are there many of them, but they are often related to each other. For example, increasing the size of the convolutional filters from 2x2 to 3x3 improved the network\u2019s ability to remove the glasses, but more filters per layer were then needed to produce a high-resolution reconstruction. While there are no universal default hyperparameters, there are a few options available that can help with this process. I used the quick and dirty approach: downsample images (with Python Imaging Library), reduce batch size, and manually tweak parameters. The next level of sophistication would be to preprocess the images more thoughtfully. For me, this would involve identifying the pixels that contain glasses across all images and then only training the network on that smaller region, rather than forcing the network to reconstruct the entire face and background. Products such as SigOpt, which uses an ensemble of optimization algorithms, can be helpful in automating the hyperparameter space search. Finally, as summarized in this blog post by Alex Gude, an Insight alumnus at Lab41 (an In-Q-Tel lab), pruning, clustering, and encoding can all be used to reduce the size and ultimate RAM costs of deep learning algorithms.\n\n\u201cIt \u2018actually works\u2019!\u201d exclaimed Ilya Sutskever during a presentation on his infoGAN work at a scaled machine learning conference in August of 2016. As a novice to the field, it\u2019s both exciting and reassuring to hear that even the experts can be surprised by what can be accomplished with deep learning. This is because deep learning is an empirical science, which is prime and open for commercial and creative applications. The innovation for the iSee method is the idea that synthetic data can be used to train a neural network to identify and remove an abstract object from an image. More work needs to be done to show that the algorithm can successfully remove a real glasses, shadows, and lens magnification. If robust, the approach can be extended to a wider variety of applications. The doors have opened, the tools are here, and I am delighted to see what our generation of data scientists and engineers will create.", 
        "title": "iSee: Using deep learning to remove eyeglasses from faces"
    }, 
    {
        "url": "https://medium.com/self-driving-cars/gpus-are-eating-the-world-4ccfcfc5ce3d?source=tag_archive---------1----------------", 
        "text": "Our partners at NVIDIA just announced an amazing third-quarter, which cycled (see what I did there?) their stock price up 30%.\n\nThe bulk of NVIDIA\u2019s present growth is in their bread and butter gaming business, where they sold $1.24 billion worth of GPUs in just the third quarter.\n\nHeadlines then mention NVIDIA\u2019s datacenter business, where they sell GPUs to companies like Google and Facebook, which use the GPUs not for gaming, but rather for high-powered deep learning.\n\nGPUs employ massive parallelism to stream games to computer monitors. One way to think of it is that every pixel on a monitor is doing pretty much the same thing, just with different inputs, which is how the colors change.\n\nThat massive parallelism turns out to be equally helpful for deep neural networks, in which every unit in the network is doing pretty much the same thing, just with different inputs.\n\nThe third and fastest-growing unit of NVIDIA\u2019s business is automotive, which grew 61% year-over-year. Every automotive company in the world is pulling NVIDIA chips, particularly the DRIVE PX2, into their autonomous vehicles. These chips enable deep learning and other parallelized computations that help the car process data in real-time.", 
        "title": "GPUs Are Eating the World \u2013 Self-Driving Cars \u2013"
    }, 
    {
        "url": "https://medium.com/@manku_timma1/part-1-g2-2xlarge-gpu-basics-805ad40a37a4?source=tag_archive---------2----------------", 
        "text": "A Graphics Processing Unit is a specialized processor usually sitting on a special card with its own memory. It does not have interrupts or virtual memory.\n\nFor example EC2\u2019s g2.2xlarge machine has the following graphics card.\n\nGRID K520 belongs to Nvidia\u2019s Tesla card series and has GRID support (which allows GPU sharing). It plugs into a PCI slot on the motherboard. Each such card has 2 GK104GL GPUs, two 4GB RAM units, and 2x1536 CUDA cores. I will explain in detail below.\n\nThe RAM is special\u200a\u2014\u200aGDDR5. GDDR can request and receive data in the same memory clock cycle. It also has a much wider memory bus than the normal DDR RAM. Consequently the memory bandwidth of the GPU is 160 GBps which is at least 3 times that available to the E5\u20132670 CPU on the same machine.\n\nThe CPU clock is 2.5GHz while the GPU clock is 800MHz. So each CUDA core is at least a third slower than each cpu hyperthread. Basically each CUDA core is slower but there are many many more of them, ala Agent Smith.\n\nThe GPU has a 8 SMX\u200a\u2014\u200astreaming multiprocessors\u200a\u2014\u200aeach having 192 CUDA cores. The 192 cores are arranged in 6 warps of 32 cores each. A warp is a group of cores all running a single instruction. A core is pretty simple\u200a\u2014\u200acontains FPU+IPU+logic unit+branch unit. Overall 1536 CUDA threads can be running simultaneously on a GPU.", 
        "title": "Part 1: g2.2xlarge GPU basics \u2013 Manku Timma \u2013"
    }, 
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-5-how-deep-convolutional-neural-networks-work-and-how-to-improve-them-3d0db8c34f8f?source=tag_archive---------3----------------", 
        "text": "On Thursday evening, our Deep Learning Study Group convened for the fifth time. The recommended preparatory work was reading the final chapter of Michael Nielsen\u2019s introductory text on neural networks. This particular chapter was focused on using convolutional neural nets for classifying images, e.g., the MNIST digits\u200a\u2014\u200aor pretty well anything else!\n\nWe covered the three key properties of convolutional neural nets, i.e.:\n\nIn particular, we noted how these properties in concert facilitate the location-invariant detection of image features (e.g., edges, curves). In turn, feeding convolutional-pooling layers into a dense (fully-connected) layer enables the individual feature representations to be assimilated into more complex representations (e.g., the digit \u201c8\u201d, or\u200a\u2014\u200ain deeper networks\u200a\u2014\u200aa school bus, or a cat\u2019s face).\n\nWe went on to discuss architecture changes that can improve the image-classification accuracy of ConvNets. These include:\n\nI committed a Jupyter notebook to GitHub here to illustrate in detail how you can implement these improvements with Theano. Outputs include classification accuracy after each epoch of training.\n\nGiven the issues discussed in previous sessions of our study group (particularly the vanishing gradient problem), how does ConvNet training work? Well:\n\nIn addition to ConvNets, we touched on other popular deep neural network architectures:\n\nWe also discussed implementations of TensorFlow for Poets, which makes it trivial to leverage the powerful neural net image-classification architecture of Inception v3. A number of us worked through its Dockerized demo, with special mention to study-group member Thomas Balestri who quickly trained it into an image-classification tool for consumer products.\n\nWith this session, we completed Nielsen\u2019s textbook! Up next for us are the notes and lectures from the Stanford CS231n (Convolutional Neural Networks for Visual Recognition) class.\n\nFor curated data science resources, including suggested paths for getting started with deep learning, visit my site.", 
        "title": "Deep Learning Study Group #5: How Deep Convolutional Neural Networks Work and How to Improve Them"
    }, 
    {
        "url": "https://medium.com/@atveit/note-this-was-first-posted-as-a-blog-post-on-https-amundtveit-com-2016-11-12-deep-learning-for-6857658230c2?source=tag_archive---------4----------------", 
        "text": "note: This was first posted as a blog post on: https://amundtveit.com/2016/11/12/deep-learning-for-natural-language-processing-iclr-2017-discoveries/\n\nThe 5th International Conference on Learning Representation (ICLR 2017) is coming to Toulon, France (April 24\u201326 2017), and there is large amount of Deep Learning papers submitted to the conference, looks like it will be a great event.\n\nThis blog post gives an overview of Natural Language Processing related papers submitted to ICLR 2017.", 
        "title": "Deep Learning for Natural Language Processing \u2014 ICLR 2017 Discoveries"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-15-7b3fe05db812?source=tag_archive---------5----------------", 
        "text": "How Feasible Is the Rapid Development of Artificial Super-intelligence?\n\nTwo crucial questions in discussions about the risks of artificial super-intelligence are: 1) How much more capable could an AI become relative to humans, and 2) how easily could superhuman capability be acquired?\n\nFor the first time, a \u201cone stop shop\u201d of the machine intelligence stack is coming into view\u200a\u2014\u200aeven if it\u2019s a year or two off from being neatly formalized.\n\nAndrew Ng: What AI Can and Can\u2019t Do\n\nIf a task takes you less than one second of thought, a machine can probably do it.\n\nStartups in Machine Learning are are tackling verticals ranging from autonomous vehicles to behavioural monitoring within the enterprise. Third wave AI, with its focus on deep specialization, represents a dramatic shift from generalist approaches in both strategy and talent needs.", 
        "title": "\u2014 Issue #15 \u2013"
    }, 
    {
        "url": "https://medium.com/@MilenaMartinez/niantic-made-players-furious-on-recent-update-f59903d31265?source=tag_archive---------6----------------", 
        "text": "Our new innovative apps deliver the speed, breadth and insight of Times Latest Update News journalism in comprehensive news streams across different platforms.\n\nTen years ago today, Nvidia launched its G80 architecture, a fundamentally new approach to both PC gaming and GPGPU computing that delivered huge performance improvements and performance-per-watt gains. We sat down with two of the Nvidia engineers responsible to discuss its success, the challenges, and how the G80 influenced the GPUs we use today, up to and including Pascal itself.\n\nUSB-C has the potential to replace a huge number of peripheral connections\u200a\u2014\u200abut right now, there are plenty of problems around compatibility and cable quality. If you\u2019re going to invest in the USB-C ecosystem right now, you\u2019ll need to do your research. And Latest Update News says the hurricane is still on course to hit Florida with powerful winds, storm surges and heavy rain today, according to the US National Hurricane Centre. The problems keep on coming. Now V6 Audis (VW owns Audi) may emit excessive emissions\u200a\u2014\u200aon both diesel and gasoline cars. While you will continue to be able to read MNT as normal, your actual experience may not be exactly as we intended and you will not be permitted to log-in to, or register for an MNT account.\n\nBringing youthe very latest Hurricane Matthew path updates,Storm track models, Weather forecasts, tracking maps and videos as the biggest Atlantic hurricane in a decade approaches the United States.Starting TipsIf you\u2019re in the market for an affordable big-screen laptop, take a look at the 17.3-inch Inspiron 17 5000 series from Dell. With a Skylake CPU and a full HJD display, this laptop is a steal when you use today\u2019s coupon code.\n\nContent on this website is for information only. It is not intended to provide medical or other professional advice. It reads: Due to unforeseen circumstances the delivery of the 12z High Resolution Forecast and the BC-High Resolution Forecast (both atmosphere and wave) will be late. We are anticipating the delivery of the products to take place approx. 60 minutes later than the normal schedule. For more information visit the site http://latestupdatenews.com/.", 
        "title": "Niantic Made Players Furious On Recent Update \u2013 MilenaMartinez \u2013"
    }
]