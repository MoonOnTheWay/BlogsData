[
    {
        "url": "https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724?source=tag_archive---------0----------------", 
        "text": "After a weeklong break, I am back again with part 2 of my Reinforcement Learning tutorial series. In Part 1, I had shown how to put together a basic agent that learns to choose the more rewarding of two possible options. In this post, I am going to describe how we get from that simple agent to one that is capable of taking in an observation of the world, and taking actions which provide the optimal reward not just in the present, but over the long run. With these additions, we will have a full reinforcement agent.\n\nEnvironments which pose the full problem to an agent are referred to as Markov Decision Processes (MDPs). These environments not only provide rewards and state transitions given actions, but those rewards are also condition on the state of the environment and the action the agent takes within that state. These dynamics are also temporal, and can be delayed over time.\n\nTo be a little more formal, we can define a Markov Decision Process as follows. An MDP consists of a set of all possible states from which our agent at any time will experience . A set of all possible actions from which our agent at any time will take action . Given a state action pair , the transition probability to a new state is defined by , and the reward is given by . As such, at any time in an MDP, an agent is given a state , takes action , and receives new state and reward .\n\nWhile it may seem relatively simple, we can pose almost any task we could think of as an MDP. For example, imagine opening a door. The state is the vision of the door that we have, as well as the position of our body and door in the world. The actions are our every movement our body could make. The reward in this case is the door successfully opening. Certain actions, like walking toward the door are essential to solving the problem, but aren\u2019t themselves reward-giving, since only actually opening the door will provide the reward. In this way, an agent needs to learn to assign value to actions the lead eventually to the reward, hence the introduction of temporal dynamics.\n\nIn order to accomplish this, we are going to need a challenge that is more difficult for the agent than the two-armed bandit. To meet provide this challenge we are going to utilize the OpenAI gym, a collection of reinforcement learning environments. We will be using one of the classic tasks, the Cart-Pole. To learn more about the OpenAI gym, and this specific task, check out their tutorial here. Essentially, we are going to have our agent learn how to balance a pole for as long as possible without it falling. Unlike the two-armed bandit, this task requires:\n\nTo take reward over time into account, the form of Policy Gradient we used in the previous tutorials will need a few adjustments. The first of which is that we now need to update our agent with more than one experience at a time. To accomplish this, we will collect experiences in a buffer, and then occasionally use them to update the agent all at once. These sequences of experience are sometimes referred to as rollouts, or experience traces. We can\u2019t just apply these rollouts by themselves however, we will need to ensure that the rewards are properly adjusted by a discount factor\n\nIntuitively this allows each action to be a little bit responsible for not only the immediate reward, but all the rewards that followed. We now use this modified reward as an estimation of the advantage in our loss equation. With those changes, we are ready to solve CartPole!\n\nLet\u2019s get to it!\n\nAnd with that we have a fully-functional reinforcement learning agent. Our agent is still far from the state of the art though. While we are using a neural network for the policy, the network still isn\u2019t as deep or complex as the most advanced networks. In the next post I will be showing how to use Deep Neural Networks to create agents able to learn more complex relationships with the environment in order to play a more exciting game than pole balancing. In doing so, I will be diving into the kinds of representations that a network learns for more complex environments.", 
        "title": "Simple Reinforcement Learning with Tensorflow: Part 2 - Policy-based Agents"
    }, 
    {
        "url": "https://medium.com/@earndotcom/how-to-use-21-to-create-and-host-a-machine-payable-api-on-heroku-or-aws-31245850386d?source=tag_archive---------1----------------", 
        "text": "In our last post, we open sourced 21, a library which makes it easy to get bitcoin, create machine-payable APIs, list them in a marketplace, and purchase these APIs with BTC. Today we\u2019ll see how 21 can be used to host your own machine-payable APIs using industry standard tools such as Amazon Web Services, Heroku, and Django.\n\nIf you want to skip right ahead to the technical tutorials, go here, here, and here. In this blog post, though, the overall idea we want to illustrate is that the open source version of 21 is now unlocked. You can install the 21 software on any client/server pair, load the client wallet with bitcoin, and then execute machine-to-machine transactions.\n\nOn the server side, it\u2019s your choice as to whether to list your machine-payable endpoint in the 21 Marketplace with the publish command. By doing so, you allow clients with the 21 software to automatically discover and purchase the endpoint. However, clients with the 21 software can buy your endpoint even if it\u2019s not listed in the 21 Marketplace by using either on-chain transactions or micropayment channels.\n\nOn the client side, it\u2019s your choice as to exactly how to load up your client wallet with bitcoin. 21 provides five different convenient ways of doing that (including a simple faucet), with the sixth being to simply just send yourself bitcoin from an external wallet by running \u201cwallet payoutaddress\u201d after installing 21 and sending bitcoin to the displayed address.\n\nWe illustrate both the client and server concepts here by showing how to build a fairly sophisticated machine-payable API with 21 that uses deep learning to build logos for bitcoin.\n\nInfrequent but computationally intensive applications like deep learning-powered algorithms are ideal candidates for turning into machine-payable APIs.\n\nOur solution (tutorial, source) is to just set up a 21-powered API that accepts bitcoin micropayments from the client. For long-running server jobs, the server then generates a cryptographic token which the client can redeem for their output upon job completion.\n\nThis obviates the concept of customer signup or API keys, as your money is now your API key. You still need a bitcoin wallet on the client and server side, but there are many forces pushing for that to happen and it can now be accomplished by a convenient install in a few seconds\u200a\u2014\u200amuch faster than setting up a corporate bank account!\n\nBy way of demonstration, we\u2019ve implemented a bitcoin-payable endpoint that uses deep learning to perform artistic style transfer. A style transfer algorithm applies the \u201cstyle\u201d of one image to another image in a more subtle way than a brute force copy/paste. It\u2019s useful for logo generation, photo filtering, digital art, movie special effects, and more besides.\n\nAs an example, here we\u2019ve used a 21-powered style transfer to render the 21 logo in the style of Yayoi Kusama\u2019s Infinity Mirrored Room:\n\nThis API is only about 200 lines of Python code, not including boilerplate. Our implementation launches a fresh EC2 instance on each request, computes the requested style transfer, and then uploads the output to Imgur, returning you a link.\n\nYou can use the API in three different ways:\n\nRegarding that last point, here is a sample API call to the 21 Marketplace that executes deep learning for bitcoin using the 21 command line tools:\n\nNote you can get bitcoin to execute this purchase through a variety of methods, including simply sending it to your 21 wallet by generating an address with wallet payoutaddress and send bitcoin to it.\n\nBecause style transfer algorithms take around 15 minutes to run on specialized hardware, this command returns a token which you can redeem when the computation is finished, like this:\n\nAnd here\u2019s the redeem call using that token, which you can run 15 minutes later:\n\nNote that this can only be used once. Here\u2019s the result of the redemption:\n\nTo see what this looks like visually, here is the input content image:\n\nAnd here is the style image that we used for the transfer:\n\nFinally, here\u2019s the output image which we got back after running the redeem command:\n\nPretty cool! And that\u2019s how to buy a bitcoin-payable API that uses deep learning behind the scenes from the 21 Marketplace. The full documentation for the API is available here.\n\nIf you want to take the next step and learn how to make APIs like this yourself, please check out these links:\n\nIf you have questions or want to join the discussion, join our Slack channel at slack.21.co. We can\u2019t wait to see what you build!", 
        "title": "How to use 21 to create and host a machine-payable API on Heroku or AWS"
    }, 
    {
        "url": "https://medium.com/this-week-in-machine-learning-ai/dueling-neural-networks-at-icml-plus-training-a-robotic-housekeeper-twiml-2016-06-24-this-week-4086022a9896?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Dueling Neural Networks at ICML, Plus Training a Robotic Housekeeper \u2014 June 24, 2016"
    }, 
    {
        "url": "https://medium.com/@Knoyd/deep-learning-on-sport-forum-discussions-c80bcef3fcc3?source=tag_archive---------3----------------", 
        "text": "Having some well-earned spare time between finishing big projects (e.g. building fintech solutions from scratch for FinAccel) is an awesome opportunity to relax\u2026 or in our case an awesome opportunity to try out new things! One thing that we wanted to try out for a while was how well deep learning works on languages other than english. Character level learning got quite a bit of attention thanks to Andrej Karpathy\u2019s work described in The Unreasonable Effectiveness of Recurrent Neural Networks using his char-rnn and Recurrent Neural Networks to learn generative language models. Many people used it for many different things from automatically generating Irish folk songs (here), click-bait headlines (here) or Obama speeches (here).\n\nWe took data from the sport community website sport.sk from Slovakia, infamous for very heated discussions in the forums, which frequently turn into serious trash-talking. It was interesting to see, whether we could teach a model to generate these comments automatically from all the past history we got and if it was going to learn the structures and nuances used there.", 
        "title": "DEEP LEARNING ON SPORT FORUM DISCUSSIONS \u2013 Knoyd \u2013"
    }, 
    {
        "url": "https://medium.com/@dfeehely/found-this-week-8-aa30f6416431?source=tag_archive---------4----------------", 
        "text": "Apple have released a swift and objective-c API to allow developers to run pre-trained neural networks on device, they call them BNNS. Conceivably, some of the neural network features described in the AI primer video in last week\u2019s post could now be run on your iPhone?", 
        "title": "Found This Week #8 \u2013 Daryl Feehely \u2013"
    }
]