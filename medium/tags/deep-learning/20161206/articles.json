[
    {
        "url": "https://hackernoon.com/deep-learning-cheat-sheet-25421411e460?source=tag_archive---------0----------------", 
        "text": "Deep Learning can be overwhelming when new to the subject. Here are some cheats and tips to get you through it.\n\nIn this article we will go over common concepts found in Deep Learning to help get started on this amazing subject.\n\nThe gradient is the partial derivative of a function that takes in multiple vectors and outputs a single value (i.e. our cost functions in Neural Networks). The gradient tells us which direction to go on the graph to increase our output if we increase our variable input. We use the gradient and go in the opposite direction since we want to decrease our loss.\n\nAlso known as back prop, this is the process of back tracking errors through the weights of the network after forward propagating inputs through the network. This is used by applying the chain rule in calculus.\n\nA function used to activate weights in our network in the interval of [0, 1]. This function graphed out looks like an \u2018S\u2019 which is where this function gets is name, the s is sigma in greek. Also known as the logistic function\n\nThe sigmoid function has an interval of [0,1], while the ReLU has a range from [0, infinity]. This means that the sigmoid is better for logistic regression and the ReLU is better at representing positive numbers. The ReLU do not suffer from the vanishing gradient problem.\n\nTanh is a function used to initialize the weights of your network of [-1, 1]. Assuming your data is normalized we will have stronger gradients: since data is centered around 0, the derivatives are higher. To see this, calculate the derivative of the tanh function and notice that input values are in the range [0,1].The range of the tanh function is [-1,1] and that of the sigmoid function is [0,1]. This also avoids bias in the gradients.\n\nTypically found in Recurrent Neural Networks but are expanding to use in others these are little \u201cmemory units\u201d that keep state between inputs for training and help solve the vanishing gradient problem where after around 7 time steps an RNN loses context of the input prior.\n\nSoftmax is a function usually used at the end of a Neural Network for classification. This function does a multinomial logistic regression and is generally used for multi class classification. Usually paired with cross entropy as the loss function.\n\nThese regularization methods prevent overfitting by imposing a penalty on the coefficients. L1 can yield sparse models while L2 cannot. Regularization is used to specify model complexity. This is important because it allows your model to generalize better and not overfit to the training data.\n\n[1] \u201cIt prevents overfitting and provides a way of approximately combining exponentially many different neural network architectures efficiently\u201c(Hinton). This method randomly picks visible and hidden units to drop from the network. This is usually determined by picking a layer percentage dropout.\n\n[1] When networks have many deep layers there becomes an issue of internal covariate shift. The shift is \u201cthe change in the distribution of network activations due to the change in network parameters during training.\u201d (Szegedy). If we can reduce internal covariate shift we can train faster and better. Batch Normalization solves this problem by normalizing each batch into the network by both mean and variance.\n\nAlso known as loss function, cost function or opimization score function. The goal of a network is to minimize the loss to maximize the accuracy of the network.\n\nThe loss/cost/optimization/objective function is the function that is computed on the prediction of your network. Your network will output a prediction, y_hat which we will compare to the desired output of y. This function then is used in back propagation to give us our gradient to allow our network to be optimized. Examples of these functions are f1/f score, categorical cross entropy, mean squared error, mean absolute error, hinge loss\u2026 etc.\n\nA measure of how accurate a model is by using precision and recall following a formula of:\n\nPrecise: of every prediction which ones are actually positive?\n\nRecall: of all that actually have positive predicitions what fraction actually were positive?\n\nUsed to calculate how far off your label prediction is. Some times denoted by CE.\n\nCross entropy is a loss function is related to the entropy of thermodynamics concept of entropy. This is used in multi class classification to find the error in the predicition.\n\nThe learning rate is the magnitude at which you\u2019re adjusting your weights of the network during optimization after back propagation. The learning rate is a hyper parameter that will be different for a variety of problems. This should be cross validated on.\n\nAs always check out my other articles at camron.xyz", 
        "title": "Deep Learning Cheat Sheet \u2013"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/10-deep-learning-trends-and-predictions-for-2017-f28ca0666669?source=tag_archive---------1----------------", 
        "text": "I used to write predictions for the upcoming year in my previous blog. The last one I recall writing was \u201cSoftware Development Trends and Predictions for 2011\u201d. That\u2019s quite a long time ago. Just to recap, out of 10 predictions, I gather that I got 6 accurately (i.e. Javascript VM, NoSQL, Big Data Analytics, Private Clouds, Inversion of Desktop Services, Scala), however the remaining 4 have not gained enough traction (i.e. Enterprise AppStores, Semantic Indexing, OAuth in the Enterprise, Proactive Agents). Actually, AppStores and OAuth doesn\u2019t happen in big enterprises, however, small companies have adopted this SaaS model in full force. I\u2019ll chalk the prediction failure to not being able to predict how slow enterprises actually change! The remain two predictions, that of Semantic Indexing and Proactive Agents, have unfortunately not progressed as I had originally projected. I may have overly estimated the AI technology at that time. Deep Learning had not been invented back then.\n\nMy Deep Learning predictions will not be at the same conceptual level as my previous predictions. I\u2019m not going to predict enterprise adoption but I rather am going to focus on research trends and predictions. Without a doubt, Deep Learning will drive AI adoption into the enterprise. For those still living underneath a rock, it is a fact that Deep Learning is the primary driver and the most important approach to AI. However, what is not so obvious is what kind of new capabilities will arise in 2017 that will lead to exponential adoption.\n\nSo here come my fearless predictions for 2017.\n\nThis, of course, is entirely obvious if you track developments at Nvidia and Intel. Nvidia will dominate the space throughout the entire 2017 simply because they have the richest Deep Learning ecosystem. Nobody in their right mind will jump to another platform until there is enough of an ecosystem developed for DL. Intel Xeon Phi solutions are dead on arrival with respect to DL. At best they may catch up in performance with Nvidia by mid-2017 when the Nervana derived chips come to market.\n\nIntel\u2019s FPGA solutions may see adoption by cloud providers simply because of economics. Power consumption is the number one variable that needs to be reduced. Intel\u2019s Nervana based chip will likely clock in at 30 teraflops by mid-2017. That\u2019s my guesstimate, but given that Nvidia is already at 20 teraflops today, I wouldn\u2019t bet on Intel having a major impact until 2018. The only big ace that Intel may have is in 3D XPoint technology. This will help improve the entire hardware stack but not necessarily the core accelerator capabilities considering that GPUs use HBM2 that\u2019s stacked on top of the chip for performance reasons.\n\nAmazon has announced its FPGA based cloud instance. This is based on Xilinx UltraScale+ technology and are offering 6,800 DSP slices and 64 GB of memory on a single instance. That\u2019s impressive capability however, the offering may be I/O bound by not offering the HBM version of UltraScale+. The lower memory bandwidth solution as compared with Nvidia, Intel, and even AMD may give developers pause as to whether to invest in a more complicated development process (i.e. VHDL, Verilog etc).\n\nIn late breaking news, AMD has revealed its new AMD Instinct line of Deep Learning accelerators. The specifications of these are extremely competitive versus Nvidia hardware. This offering is scheduled to be available early 2017. This is probably should be enough time for AMDs ROCm software to mature.\n\nCNNs will be the prevalent bread-and-butter model for DL systems. RNNs and LSTMs with its recurrent configuration and embedded memory nodes are going to be used less simply because they would not be competitive to a CNN based solution. Just like GOTO disappeared in the world of programming, I expect the same for RNNs/LSTMs. Actually, parallel architectures trump sequential architectures in performance.\n\nDifferentiable Memory networks will be more Common. This is just a natural consequence of architecture where memory will be refactored out of the core nodes and just reside as a separate component from the computational components. I don\u2019t see the need for forget, input and output gates for LSTM that can be replaced by auxiliary differentiable memory. We already see conversation about refactoring the LSTM to decouple memory (see Augmented Memory RNN).\n\n3. Designers will rely more on Meta-Learning\n\nWhen I began my Deep Learning journey, I had thought that optimization algorithms, particularly ones that were second-order would lead to massive improvements. Today, the writing is on the wall, DL can now learn the optimization algorithm for you. It is the end of the line for anybody contemplating a better version of SGD. The better version of SGD is the one that is learned by a machine and is the one that is specific to the problem at hand. Meta-learning is able to adaptively optimize its learning based on its domain. Further related to this is whether alternative algorithms to backpropagation will begin to emerge in practice. There is a real possibility that hand tweaked SGD algorithm may be in its last legs in 2017.\n\n4. Reinforcement Learning will only become more creative\n\nObservations about reality will always remain imperfect. There are plenty of problems where SGD is not applicable. This just makes it essential that any practical deployment of DL systems will require some form of RL. In addition to this, we will see RL used in many places in DL training. Meta-Learning, for example, is greatly enabled by RL. In fact, we\u2019ve seen RL used to find different kinds of neural network architectures. This is like Hyper-parameter optimization on steroids. If you happen to be in the Gaussian Process business then your lunch has just been eaten.\n\n5. Adversarial and Cooperative Learning will be King\n\nIn the old days, we had monolithic DL systems with single analytic objective functions. In the new world, I expect to see systems with two or more networks cooperation or competing to arrive at an optimal solution that likely will not be in analytic form. See \u201cGame Theory reveals the future of Deep Learning\u201d. There will be a lot of research in 2017 in trying to manage non-equilibrium contexts. We already see this now where researchers are trying to find ways to handle the non-equilibrium situation with GANs.\n\n6. Predictive Learning or Unsupervised Learning will not progress much\n\n\u201cPredictive Learning\u201d is the new buzzword that Yann LeCun in pitching in replacement to the more common term \u201cUnsupervised Learning\u201d. It is unclear whether this new terminology will gain adoption. The question though of whether Unsupervised or Predictive Learning will make great strides in 2017. My current sense is that it simply will not because there seems to be a massive conceptual disconnect as to how exactly it should could work.\n\nIf you read my previous post about \u201c5 Capabilities of Deep Learning Intelligence\u201d, you get the feeling that Predictive Learning is some completely unknown capability that needs to be shoehorned into the model that I propose. Predictive Learning is like the cosmologists Dark Matter. We know it is there, but we just don\u2019t know how to see it. My hunch is that it has something to do with high entropy or otherwise randomness.\n\nAndrew Ng thinks this is important, I think so too!\n\n8. More Applications will use Deep Learning as a component\n\nWe saw this already in 2016 where we see Deep Learning used as a function evaluation component in a much larger search algorithm. AlphaGo employed Deep Learning in its value and policy evaluations. Google\u2019s Gmail auto-reply system used DL in combination with beam searching. I expect to see a lot more of these hybrid algorithms rather than new end-to-end trained DL systems. End-to-end Deep Learning is a fascinating area of research, but for now hybrid systems are going to be more effective in application domains.\n\nDeep Learning is just one of those complex fields that need a conceptual structure. Despite all the advanced mathematics involved, there\u2019s a lot of hand waving and fuzzy concepts that can best be captured not by formal rigor but rather with a method that has been proven to be effective in other complex domains like software development. I predict practitioners will finally \u201cget it\u201d with regards to Deep Learning and Design Patterns. This will be further motivated by the fact that Deep Learning architectures are becoming more modular rather than monolithic.\n\nThe background of researchers and the mathematical tools that they employ are a breeding ground for a kind of bias in their research approach. Deep Learning systems and Unsupervised Learning systems are likely these new kinds of things that we have never encountered before. Therefore, there is no evidence that our traditional analytic tools are going to be any help in unraveling the mystery as to how DL actually works. There are plenty of dynamical systems in physics that have remain perplexed about for decades, I see the same situation with regard to dynamical learning systems.\n\nThis situation, however, will not prevent the engineering of even more advanced applications despite our lack of understanding of the fundamentals. Deep Learning is almost like biotechnology or genetic engineering. We have created simulated learning machines, we don\u2019t know precisely how they work, however that\u2019s not preventing anyone from innovating.\n\nI\u2019ll come back to these predictions in a year from now. Wish me luck!", 
        "title": "10 Deep Learning Trends and Predictions for 2017 \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/predictive-learning-is-the-key-to-deep-learning-acceleration-93e063195fd0?source=tag_archive---------2----------------", 
        "text": "Yann LeCun in his many talks this year has repeatedly hammered away at this analogy:\n\nLeCun at NIPS 2016, has now started using the phrase \u201cpredictive learning\u201d in substitution of \u201cunsupervised learning\u201d. LeCun says:\n\nThis is an interesting change and indicates a subtle change in his perspective as to what he believes is required to build up the \u201ccake\u201d. In LeCun\u2019s view, the foundation needs to be built before we can make accelerated progress in AI. In other words, building off current supervised learning by adding more capabilities like memory, knowledge bases and cooperating agents will be a slog until we are all able to build that \u201cpredictive foundational layer\u201d (see: \u201cFive Capability Levels of Deep Learning\u201d ). At the conference he posted this slide:\n\nWhich emphasizes the formidable task that is ahead of us. Predictive learning is clearly requires machines to be able to not just learn without human supervision but learn a predictive model of the world. It is very important to emphasize this point and why LeCun is attempting to change our perspective of the canonical taxonomy of AI ( i.e. unsupervised, supervised and reinforcement learning).\n\nRuslan Salakhudinov, recently hired by Apple to lead their AI research, has a good survey talk of Unsupervised Learning (now to be rechristened as Predictive Learning) where he provides this taxonomy:\n\nAt the right corner of the slide he mentions Generative Adversarial Networks (GANs). GANs consists of competing neural networks, a generator and discriminator, the former tries to generate fake images while the later tries to identify real images. The interesting feature of these systems is that a closed form loss function is not required. In fact, some systems have the surprising capability of discovering its own loss function! A disadvantage of adversarial networks are they are difficult to train. Adversarial learning consists in finding a Nash equilibrium to a two-player non-cooperative game. Yann Lecun, in a recent lecture on unsupervised learning, calls adversarial networks the \u201cthe coolest idea in machine learning in the last twenty years\u201d.\n\nElon Musk\u2019s OpenAI research has a big focus on Generative Models, their motivation can be summarized by Richard Feynman\u2019s quote \u201cWhat I cannot create, I do not understand.\u201d Feynman is here alluding to his \u201cFirst Principles\u201d method of thought were he needs to be able to build up understanding by composing proven concepts. The basic idea here is that perhaps if a machine is able to generate models with high realism then perhaps ( a big leap here ) it develops an understanding of the predictive model. Here are some images of the state-of-the-art of this technique:\n\nThese are images generated by the DL system given the word provided. This indeed in quite impressive. I wouldn\u2019t expect many humans to be able to draw this well! Now, this system is not perfect, as evidence by this failure set:\n\nBut, hey, I\u2019ve seen many people do much worse while playing Pictionary!\n\nThe current consensus though are that these generative models aren\u2019t able to capture the semantics of the the task. They don\u2019t understand the meaning of an ant, volcano or redshank. They are however very good a mimicry and in fact prediction. These images are not recreations of images that the machine was previously trained on. Rather, the machine has come up with some generalized model that allows it to extrapolate a very realistic result.\n\nThis approach of using adversarial networks is different from the more classical approach of machine learning. Here we have two competing neural networks (i.e. discriminator and generator) that seem to work synergistically to accomplish a kind of generalization (see: \u201cRethinking Generalization\u201d). In the classical ML world one would define a objective function that one would fire up one\u2019s favorite optimization algorithm. However, in this research area, the correct objective function is quite unclear. Even more surprising is that these systems are able to learn their own objective function!\n\nThe fascinating realization here is that DL systems are extremely malleable. The classic ML notions that the objective function and constraints are fixed or the notion that the optimization algorithm is fixed do not apply in DL. Even more surprising is that a meta-level approach can be used. That is, DL systems can learn how to learn (see: \u201cMeta-learning\u201d).", 
        "title": "\u201cPredictive Learning\u201d is the New Buzzword in Deep Learning"
    }, 
    {
        "url": "https://blog.insightdatascience.com/nips-2016-day-1-6ae1207cab82?source=tag_archive---------3----------------", 
        "text": "Want to learn about applied Artificial Intelligence from leading practitioners in Silicon Valley or New York? Learn more about the Insight Artificial Intelligence Fellows Program.\n\nPacked. That is one way to describe the thirtieth annual Neural Information Processing Systems (NIPS) Conference. Packed with excitement, packed with results, and packed with people. This year over 2500 top quality papers where submitted, and over 5000 people are in attendance. We too are in attendance for the week, and are excited to talk with the top scientists and teams to hear what their currently doing.\n\nKicking off Day 1 of NIPS was a series of tutorials, focused on bringing participants up to speed on the current state of the art for a number of topics. This year NIPS has ran three tutorial sessions simultaneously, so sadly we weren\u2019t able to catch them all. However, the ones we did see were quite fantastic.\n\nDavid Blei (Columbia University) and company gave an in-depth primer on Variational Inference, which has seen a number of advances recently. Of these perhaps the most influential is the reparameterization trick, which allows backprop through random variables and helped to unlock recent advances in Variational Autoencoders. On a more applied front, Andrew Ng (Baidu Research) laid out his best practices roadmap for building out learning systems in industrial settings. In another session, Francis Bach brought us up to date on recent advances in (non)convex optimization, where algorithms like SAGA are simply crushing BFGS. We think such results could see significant adoption in data science and applied machine learning settings, once rolled out to commonly used libraries.\n\nThe star, in our opinion, was the tutorial on Generative Adversarial Networks by Ian Goodfellow. In 2014, Goodfellow introduced GANs to the Deep Learning community with extensive interest and praise. In his conference opening keynote speech, Yann LeCun heralded GANs as \u201cthe most exciting idea [in the field] in the last 20 years.\u201d In his talk, Goodfellow clearly described the concepts and current advances around GANs, including tips and tricks as well as current research frontiers. Much of his talk highlighted recent advances in training and using GANs, which we have been excited about for some time. At the end, Goodfellow amazed the audience with recent results from Plug & Play Generative Networks which produced\u200a\u2014\u200afor the first time\u200a\u2014\u200arealistic computer generated images (see below) which are similar to ImageNet data (example here from Andrej Karpathy).\n\nWrapping up the day was an exciting poster session. Over 170 awesome submissions were present, and well known contributors like Diederik Kingma, David Blei, and Yoshua Bengio joined the crowd and explained their posters. The quality of the submissions were really high, and it is a shame we can\u2019t talk about them all. Some of our highlights included:\n\nAll in all, it was a amazing first day at NIPS 2016 and we are eager to see what lies ahead.", 
        "title": "NIPS 2016 \u2014 Day 1 Highlights \u2013"
    }, 
    {
        "url": "https://medium.com/@hekonsek/when-to-dsstne-df831e46fa25?source=tag_archive---------4----------------", 
        "text": "So you have heard about DSSTNE and wondering if its a match for your machine learning problem? Let me help you then.\n\nDSSTNE has been recently open sourced by Amazon. It is a C library for running deep learning neural networks algorithms against laaarge sparse input vectors on GPU hardware.\n\nComputing deep learning against laaarge sparse input vectors on cluster of GPU hardware.\n\nSpeaking in human language\u200a\u2014\u200aassume you have many very large input records with many empty values and you need to calculate machine learning algorithm on this data in reasonable time. In such situation your regular machine learning solution running on regular CPU-based machine may be too slow for your needs. So you need to use a cluster of computers with GPU units, as GPU units are blazing fast when it comes to arithmetic operations. DSSTNE can do a heavy lifting for you\u00a0:)\u00a0.\n\nIf you operate on really large feature vectors with many sparse space between the values (i.e. vectors with many zeros and only bunch of values) and your Apache Spark ML solution does\u2019t scale so well for this kind of data\u200a\u2014\u200ayou should consider using DSSTNE.\n\nYou can run DSSTNE as a docker container (which hides all the twisted C libraries complexity from you) or as a regular C library.\n\nI\u2019m personally really impressed how easy it is to run DSSTNE in Docker. That could be a perfect match to wire the former into your Kubernetes infrastructure.\n\nThere is also Python support coming to DSSTNE, to make it more civilized\u00a0;)\u00a0.\n\nI\u2019m DigitalOcean fanboy, but unfortunately DigitalOcean doesn\u2019t support GPU droplets yet (I hope yet). Please don\u2019t cry however, as Amazon EC2 allows you to create GPU instances.", 
        "title": "When to DSSTNE? \u2013 Henryk Konsek \u2013"
    }, 
    {
        "url": "https://medium.com/@andressasivolella/deep-learning-nunca-vi-nem-comi-mas-j%C3%A1-ouvi-falar-13bab3883c75?source=tag_archive---------5----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep Learning: nunca vi, nem comi mas j\u00e1 ouvi falar!"
    }, 
    {
        "url": "https://medium.com/@mza/ai-research-scale-6c4f253b1e1c?source=tag_archive---------6----------------", 
        "text": "At Re:Invent last week, James gave a great presentation on \u2018innovating at scale\u2019\u200a\u2014\u200adetailing some of the things we do on under the hood relating to networking, data center infrastructure and power at Amazon. It\u2019s a fascinating talk which covers everything from private trans-oceanic network, to custom ASICs, to storage racks which weigh in at 2,778lbs.\n\nAll these areas of innovation are driven by customer workloads running at huge scale on AWS today: web and mobile apps like Airbnb, Pinterest and Slack, large IoT deployments, broad usage of analytics and, an area close to my heart, AI research. I had the good fortune to join James on stage to talk about this, and this post is a summary of my presentation (which you can also watch you YouTube).\n\nAI research\u200a\u2014\u200athe development and training of new machine learning models\u200a\u2014\u200ais enjoying new found momentum and focus across virtually every industry driven by the perfect storm of three things: algorithms (some of which are actually pretty old; published in the last decade but enjoying new found usage with the advent of\u2026), huge data collections (modern apps and businesses are fantastic data generators; where collection, storage and analytics were once boxed in by fixed data center walls that couldn\u2019t move, in the cloud these walls melt away\u200a\u2014\u200aalong with the constraints of fixed capacity or limited capabilities), and the availability of computational power (especially GPUs), at almost unlimited scale, priced as a utility. The intersection of these three areas enables anyone with an AWS account and an idea to build more sophisticated models to be trained on larger data sets.\n\nData scientists, researchers and academics are able to harness algorithms, and then train them at scale.\n\nOne group of algorithms have really benefited from this perfect storm. Deep learning\u200a\u2014\u200aa set of statistical machine learning techniques which perform feature detection using a hierarchy of multiple layers, trained with artificial neural networks\u200a\u2014\u200ahave shown great success in solving some of the hardest problems in computer science: natural language processing, image analysis, voice recognition, personalization and autonomy.\n\nHowever, real world deep learning algorithms are large and complex, often involving thousands of different layers, with an almost insatiable hunger for computational resources for both training and inference. The challenges in building new models break down into three pieces\u200a\u2014\u200athe \u2018Three P\u2019s of data science: programmability (how easy it is to compose and manipulate the network models), portability (how easy it is to move models between platforms, from mobile devices to robotics), and performance (how efficiently models scale in training and inference).\n\nIn a way, this isn\u2019t news: customers have been using AWS to build novel AI features (or entirely new products), for years\u200a\u2014\u200ato name but a few\u2026 Pinterest have a really smart visual image search in their mobile app; Netflix make movie and show recommendations; FINRA look for trade anomalies; Hudl improve sports teams performance through video analysis; Stanford have trained deep learning models on fundus images to help early detection of diabetic retinopathy; Wolfram have built Alpha, a computational knowledge engine; Amazon created Echo and Alexa; and TuSimple have created some of the world\u2019s best performing autonomous driving algorithms based on deep learning-trained computer vision.\n\nMany of these new applications use deep learning frameworks, such as TensorFlow, Torch, Caffe, Theano or MXNet, running on AWS, to help address the \u2018Three Ps\u2019.\n\nWhile we\u2019ll make sure that all of these frameworks run really well on AWS (take a look at our Deep Learning AMI and CloudFormation template which come pre-configured to run all of these frameworks), one really stood head and shoulders above the others: MXNet. It\u2019s what powers Wolfram Alpha (and in turn provides factual answers to voice assistants such as Siri, S Voice and Iris), TuSimple\u2019s autonomous driving systems; it\u2019s the framework of choice at AWS and the foundation of our future AI services. We love it, and our customers love it, precisely because it addresses the \u2018Three P\u2019s so well.\n\nMXNet has two really nice programmability features. Firstly, it provides a great breadth of support for different programming models and languages, supporting everything from Python and Javascript, to R and Matlab. It\u2019s tailor made to support the programming languages that researchers are already using, but compiles this code down using a high performance back end to give reliable, high performance in training and inference. Secondly, it provides the benefit of imperative programming (the ability to script and code with loops and conditionals which are easier to develop with), mixed with the benefits of declarative models (which provide more opportunity for optimization, just like with SQL). In fact, the name \u2018MXNet\u2019, is derived from this: it\u2019s allows for mixed network models.\n\nMXNet is incredibly efficient when it comes to using and allocating memory: a 1000-layer network can fit inside 4 gig of memory, which means that networks can be trained at scale in the cloud, and then moved from one platform to the next: from mobile and web clients, to robots and drones (TuSimple have computer vision systems which will run on TX1 boards on drones to track and follow objects automatically).\n\nDriven in part by the compiled back-end, opportunities for optimization in a mixed imperative/declarative model, and efficiency in memory usage, and because of automatic parallelization of both serial code across CUDA cores on GPUs, and clever data parallelization, MXNet scales extremely well to multiple cores on a single GPU, to multiple GPUs in a single instance, and multiple GPUs on multiple instances.\n\nIn our own benchmarking, we saw 91% efficiency in scaling the Inception v3 image analysis algorithm across 16 GPUs on a P2.8xl; and 88% efficiency scaling up to 256 GPUs on 16 P2.8xl.\n\nThat\u2019s just a 3% decrease in efficiency for 16x the scale, across over 600k CUDA cores.\n\nBut don\u2019t take my word for it! One of the benefits of the cloud is that you are able to perform your own due diligence using either your own applications, or\u00a0. We want these benchmarks to be reproducible, and to provide a starting point for your own performance evaluations, so we have made available open source deep learning benchmark code. Just spin up a P2 cluster using the CFN template, log into the master node, and start running these benchmarks with a single command.\n\nThis is an exciting area, especially as it\u2019s so early in the advent of how we can apply deep learning. I\u2019m looking forward to an exciting year ahead.\n\nStarting to experiment with other platforms; cross-posting new articles to Medium from my blog. The original is here.", 
        "title": "AI Research @ Scale \u2013 Matt Wood \u2013"
    }, 
    {
        "url": "https://medium.com/@tryolabs/the-major-advancements-in-deep-learning-in-2016-98de0feb164a?source=tag_archive---------7----------------", 
        "text": "Deep Learning has been the core topic in the Machine Learning community the last couple of years and 2016 was not the exception. In this article, we will go through the advancements we think have contributed the most (or have the potential) to move the field forward and how organizations and the community are making sure that these powerful technologies are going to be used in a way that is beneficial for all.\n\nOne of the main challenges researchers have historically struggled with has been unsupervised learning. We think 2016 has been a great year for this area, mainly because of the vast amount of work on Generative Models.\n\nMoreover, the ability to naturally communicate with machines has been also one of the dream goals and several approaches have been presented by giants like Google and Facebook. In this context, 2016 was all about innovation in Natural Language Processing (NLP) problems which are crucial to reach this goal.\n\nUnsupervised learning refers to the task of extracting patterns and structure from raw data without extra information, as opposed to supervised learning where labels are needed.\n\nThe classical approach for this problem using neural networks has been autoencoders. The basic version consists of a Multilayer Perceptron (MLP) where the input and output layer have the same size and a smaller hidden layer is trained to recover the input. Once trained, the output from the hidden layer corresponds to data representation that can be useful for clustering, dimensionality reduction, improving supervised classification and even for data compression.\n\nRecently, a new approach based on generative models has emerged. Called Generative Adversarial Networks, it has enabled models to tackle unsupervised learning. GANs are a real revolution. Such has been the impact of this research that in this presentation, Yann LeCun (one of the fathers of Deep Learning) said that GANs are the most important idea in Machine Learning in the last 20 years.\n\nAlthough introduced in 2014 by Ian Goodfellow, it is in 2016 that GANs have started to show their real potential. Improved techniques for helping training and better architectures (Deep Convolutional GAN) introduced this year have fixed some of the previous limitations, and new applications (we list some of them later) are revealing how powerful and flexible they can be.\n\nImagine an aspiring painter who wants to do art forgery (G), and someone who wants to earn his living by judging paintings (D). You start by showing D some examples of work by Picasso. Then G produces paintings in an attempt to fool D every time, making him believe they are Picasso originals. Sometimes it succeeds; however as D starts learning more about Picasso style (looking at more examples), G has a harder time fooling D, so he has to do better. As this process continues, not only D gets really good in telling apart what is Picasso and what is not, but also G gets really good at forging Picasso paintings. This is the idea behind GANs.\n\nTechnically GANs consist of a constant push between two networks (thus \u201cadversarial\u201d): a generator (G) and discriminator (D). Given a set of training examples (such as images), we can imagine that there is an underlying distribution (x) that governs them. With GANs, G will generate outputs and D will decide if they come from the same distribution of the the training set or not.\n\nG will start from some noise z, so the generated images are G(z). D takes images from the distribution (real) and fake (from G) and classifies them: D(x) and D(G(z)).\n\nD and G are both learning at the same time, and once G is trained it knows enough about the distribution of the training samples that it can generate new samples that share very similar properties:\n\nThese images were generated by a GAN trained with CIFAR-10. If you pay attention to the details, you can see they are not indeed real objects. However, there is something to them that captures a certain concept that can make them look real from a distance.\n\nRecent developments have extended the GANs idea to not only to approximate the data distribution, but also to learn interpretable, useful vector representations of the data. These desired vector representations need to capture rich information (same as in autoencoders) and also need to be interpretable, meaning that we can distinguish parts of the vector that contribute to a specific type of shape transformation in the generated outputs.\n\nThe InfoGAN model proposed by OpenAI researchers in August addresses this issue. In a nutshell, InfoGAN is able to generate representations that contain information about the dataset in an unsupervised way. For instance, when applied to the MNIST dataset it is able to infer the type of number (1, 2, 3,\u00a0\u2026), the rotation and the width of the generated samples without the need for manually tagged data.\n\nAnother extension of GANs is a class of models called Conditional GAN (cGAN). These models are able to generate samples taking into account external information (class label, text, another image), using it to force G to generate a particular type of output. Some applications that have recently surfaced are:\n\nYou can check more about generative models in this blog post or in this talk by Ian Goodfellow.\n\nIn order to be able to have fluent conversations with machines, several issues need to be solved first: text understanding, question answering and machine translation.\n\nSalesforce MetaMind has built a new model called Joint Many-Tasks (JMT) with the objective of creating a single model able to learn five common NLP tasks:\n\nPart-of-speech tagging Assign parts of speech to each word, such as noun, verb, adjective. Chunking Also called shallow parsing. Involves a range of tasks, like finding noun or verb groups. Dependency parsing Identify syntactic relationships (such as an adjective modifying a noun) between words. Semantic relatedness Measure the semantic distance between two sentences. The result is a real-valued score. Textual entailment Determine whether a premise sentences entails a hypothesis sentence. Possible classes: entailment, contradiction, and neutral.\n\nThe magic behind this model is that it is end-to-end trainable. This means it allows collaboration between different layers, resulting in improvements on lower layers tasks (which are less complex), with the results from higher layers (more complex tasks). This is something new compared to older ideas, which could only use lower layers to improve higher level ones, but not the other way around. As a result, this model achieves state of the art results in all but POS tagging (where it came out in second place).\n\nMetaMind also presented a new model called Dynamic Coattention Network (DCN) for the question answering problem, which builds on a pretty intuitive idea.\n\nImagine I was going to give you a long text and ask you some question. Would you prefer to read the text first and then be asked the question, or be given the question before you actually start reading the text? Naturally, knowing in advance what the question will be conditions you so you know what to pay attention to. If not, you would have to pay equal attention and keep track of every detail and dependencies, to cover for all possible future questions.\n\nDCN does the same thing. First, it generates an internal representation of the documents conditioned on the question that it is trying to answer, and then starts iterating over a list of possible answers converging to the final answer.\n\nIn September, Google presented a new model used by their translation service called Google Neural Machine Translation (GNMT). This model is trained separately for each pair of languages like Chinese-English.\n\nA new GNMT version was announced in November. It goes a step further, training a single model that is able to translate between multiple pairs of languages. The only difference with the previous model is that it now GNMT takes a new input that specifies the target language. It also enables zero-shot translation meaning that it is able to translate a pair of language that it wasn\u2019t trained to.\n\nGNMT results show that training it on multiple pairs of languages is better than training on a single pair, demonstrating that it is able to transfer the \u201ctranslation knowledge\u201d from one language pair to another.\n\nSeveral corporations and entrepreneurs have created non-profits and partnerships to discuss about the future of Machine Learning and making sure that these impressive technologies are used properly in favor of the community.\n\nOpenAI is a non-profit organization that aims to collaborate with the research and industry community, and releasing the results to public for free. It was created in late 2015, and started delivering the first results (publications like InfoGAN, platforms like Universe and (un)conferences like this one) in 2016. The motivation behind it is to make sure that AI technology is reachable for as many people as possible, and by doing so, avoiding the creation of AI superpowers.\n\nOn the other hand, a partnership on AI was signed by Amazon, DeepMind, Google, Facebook, IBM and Microsoft. The goal is to advance public understanding of the field, support best practices and develop an open platform for discussion and engagement.\n\nAnother aspect worth highlighting is the openness of the research community. Not only can you find almost any publication on sites like Arxiv (or Arxiv-Sanity) for free, but you can also now replicate their experiments by using the same code. One useful tool is GitXiv, which links Arxiv papers with their open source project repository.\n\nOpen source tools are everywhere (as we highlighted in our 10 main takeaways from MLconf SF blogpost). They are used and created by researchers and companies. Here is a list of the most popular tools in 2016 for Deep Learning:\n\nIt is a great time to be part of the recent Machine Learning developments. As you can see this year has been particularly exciting; the research is moving at such a rapid pace that it\u2019s hard to keep up with latest advancements. We are truly lucky to be living in an era where AI has been democratized.\n\nAt Tryolabs we are working in some very interesting projects with these technologies. We promise to keep you all posted with our findings and continue sharing experiences with the industry and all the interested developers out there.\n\nWe reviewed a lot in this post, but there were many other great developments that we had to leave out. If you feel we have not done enough justice to some of these, please feel free to say so in the comments below!\n\nUpdate (12/09/2016): there have been really good contributions on the discussion of this post on Reddit and HackerNews (see links below). In particular, the arguments about AlphaGo (Deep Reinforcement Learning), ByteNet, WaveNet, PixelRNN / PixelCNN and some others are very interesting. Make sure you read through and enlighten yourself!", 
        "title": "The major advancements in Deep Learning in 2016 \u2013 Tryolabs \u2013"
    }, 
    {
        "url": "https://medium.com/fusion-by-fresco-capital/the-emergence-of-data-over-oil-93b45f0c794e?source=tag_archive---------8----------------", 
        "text": "If oil dominated the last century, data is the leading candidate to dominate this one. An often repeated phrase is that \u201cdata is the new oil\u201d. But beyond the simple similarity of being important, is it useful to use oil as an analogy for data? How are they different?\n\nOil is a scarce resource. Technology innovations related to discovering and processing oil merely help to contain price increases. Data is not just abundant, it is a cumulative resource. Technology innovations lead to a collapse in the cost of collecting and manipulating data, and our new data builds on top of our existing data. Personalization is an obvious application of this idea. To date, companies have mostly used this for targeted content and commerce. Going forward, the bigger opportunity is in areas like personalised learning and medicine.\n\nIf oil is being used, then the same oil cannot be used somewhere else because it is a rival good. This results in a natural tension about who controls oil. If data is being used, the same data can be used elsewhere because it is a non-rival good. It is up to us to appreciate this difference and embrace the potential. An obvious example is the power of open source. Our Internet would not exist in its current form without the positive impact of open source. It\u2019s also clear that our intellectual property laws have not fully understood the implications of data being a non-rival good.\n\nAs a tangible product, oil faces high friction, transportation and storage costs. These costs place limits on the applications of oil. As an intangible product, data has much lower friction, transportation and storage costs. The result is a much wider range of applications due to fewer physical restrictions. The exponential growth in content and media is an obvious result of the fact that data is intangible. Less obvious is the transformational opportunity for a globally distributed manufacturing supply chain which would be connected by data rather than the current system of shipping and flying physical goods everywhere.\n\nThe lifecycle of oil is defined by process: extraction, refining and distribution. This process is relatively stable and predictable. The lifecycle of data is defined by relationships: with other data, with context and with itself via feedback loops. These relationships are dynamic and uncertain, requiring an entirely different approach to building value. This highlights the difference between complicated and complex systems. Oil and industrial assets benefit from the use of ideas like six sigma to enhance efficiency. Data benefits from the use of technology like deep learning to learn from the data itself, a form of exploration.\n\nA fixed amount of oil results in a predictable amount of output. There is no possibility of non-linear upside surprise. A fixed amount of knowledge can create huge value in a non-linear way. The laws of physics really do limit the benefits we can derive from a given amount of oil. On the contrary, the concept of zero is a core building block supporting our entire digital technology infrastructure. A completely non-linear benefit from a deceptively simple idea.\n\nEach of the above differences is valid individually, but taken together they multiply in importance and reflect the emergent nature of systems based on data. While our current oil based system has some features of emergence, the new data based system has vastly more potential for new and unpredictable applications.\n\nTo be clear, the impact of these applications is not always going to be good for society, certainly at an individual level, and sometimes at the aggregate level. Even with a much smaller level of emergence, we\u2019ve already seen the damage that our oil based system can leave to our planet.\n\nWe should not be blindly following data and need to appreciate that the emergent properties of data make it much more powerful than oil. As data becomes increasingly important, it\u2019s critical to understand the differences between oil and data. To fully enjoy the benefits of data, we need to fundamentally update all of our global ecosystems as soon as possible across business, government, and society. This is not something that can be done by any single person, company, or country. For those interested to build a stronger global ecosystem together, let\u2019s connect and make it happen.", 
        "title": "The Emergence of Data Over Oil \u2013 Fusion by Fresco Capital \u2013"
    }, 
    {
        "url": "https://medium.com/re-write/amazon-go-2bede154bc46?source=tag_archive---------9----------------", 
        "text": "Amazon just launched Go, a shopping experience that requires no lines and no check-out\u2026 you just go. Full of grab-and-go options and grocery essentials, you can pick up everything you\u2019d need without the hassle. Amazon claims to use computer vision, sensor fusion, and deep learning to create what they call \u201cJust Walk Out\u201d technology. Sign into the app before you walk in the store and it automatically detects when products are taken from or returned to the shelves and then keeps track of them in a virtual cart on your phone. Using computer vision the Amazon Go store recognizes you and your actions, making it unnecessary scan each item individually (or have a cashier!). After you\u2019re all done shopping, you just walk out\u200a\u2014\u200ano line, no hassle. Shortly thereafter, Amazon charges your account and sends you a receipt.\n\nThe first store is in Seattle, of course, at 2131 7th Ave, Seattle, WA, on the corner of 7th Avenue and Blanchard Street. Amazon is hoping to open stores to the public in early 2017, but it\u2019s currently only open to Amazon employees that are in their beta program. You can sign up to be the first to know when a store opens near you here.\n\nWhat do stores like this mean for retail jobs? We\u2019re not quite sure yet. But to be honest, this move doesn\u2019t surprise me. There have been rumors of Amazon creating more retail experiences around the globe. By eliminating most of the staff needed to operate a store and the time it takes to checkout, Amazon keeps costs lower than traditional supermarket competitors, moving people through the space faster and more efficiently. Not only has Amazon started Go, but it\u2019s opened a plethora of new businesses and leveraged incredible technology to deliver packages via drone with Amazon Prime Air, offer home grocery delivery with Amazon Fresh, let you upload data by the truckload\u200a\u2014\u200aliterally\u200a\u2014\u200awith Amazon\u2019s Snowmobile, and deliver a new shopping experience through new digitized experiences. The idea is to reimagine the shopping experience with users in mind and grow an understanding of shopper interests in order to engage with them more effectively across their shopping journey.\n\nAmazon, a retail powerhouse, has been slowly expanding its offerings over the years. After opening its first Amazon Books store last year and a variety of pop-up stores around the US, they\u2019ve been pushing brick and mortar locations as extensions of their Amazon.com experience and new retention channels for customers. On the surface, these stores may look like normal retail stores, but the real magic is what happens under the hood. In Amazon Books stores, reviews are up front and real-time, customers are encouraged to price check online. The entire experience delivers data back to your Amazon account, which helps create a custom profile for you and your preferences.\n\nWith the backing of Amazon\u2019s web services analytics and data platforms, Amazon is able to rethink the in-store retail experience with transparency in mind. Amazon can track which books are best-selling locally, nationally or internationally, upkeep inventory and track purchases, geo-target shoppers accurately, and think of the buying process as a journey from an online search to the physical purchase. But Amazon Books is just one example, with the collected data around you, and a bunch of other people like you. Amazon can know what you\u2019re interested in and what you\u2019re going to purchase before you do, leveraging opportunities to morph their brand into our lives, creating loyalty and instilling trust, not resentment. Amazon Go, and a multitude of other offerings from Amazon, are changing the way we live.", 
        "title": "Amazon Go \u2013 RE: Write \u2013"
    }, 
    {
        "url": "https://medium.com/@wschae/why-deep-learning-is-suddenly-changing-your-life-by-roger-rarloff-54d32cb07a59?source=tag_archive---------10----------------", 
        "text": "I am reading this article on Fortune. Some quotes with my thoughts:", 
        "title": "\u201cWhy deep learning is suddenly changing your life\u201d By Roger Rarloff"
    }, 
    {
        "url": "https://medium.com/@rakesh.botadkar/very-aptly-said-from-my-personal-experiences-i-have-realised-every-day-is-a-new-day-of-learning-305c9072773a?source=tag_archive---------11----------------", 
        "text": "Very aptly said\u2026 from my personal experiences, i have realised every day is a new day of learning. Although i can say with a degree of confidence that i know the tricks of the trade for my business and as an entrepreneur, However, it somehow amazes me, how there is so much to learn in terms of leadership, quality, time and people management. All these soft skills will only come with learning and interaction, I suppose!", 
        "title": "Very aptly said\u2026 from my personal experiences, i have realised every day is a new day of learning."
    }
]