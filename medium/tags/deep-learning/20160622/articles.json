[
    {
        "url": "https://medium.com/@sidd_reddy/mobile-friendly-deep-convolutional-neural-networks-part-3-e7b1bd779a21?source=tag_archive---------0----------------", 
        "text": "Read part one here and part two here.\n\nThe amount of research being done on improving the run time of CNNs and compressing their models is staggering. I\u2019m going to cover two major techniques being used for the same in this article.\n\nBecause CNNs run on images and each kernel in a layer is independent of the others, they are parallelizable and are the perfect candidates for hardware acceleration. Since each kernel in a layer is independent of the other kernels, they can be run on different cores or even different machines. Having dedicated hardware means power and speed improvement at the cost of area and price. But the effectiveness of deep learning is motivating hardware manufacturers to invest in developing such hardware. Qualcomm zeroth and Nvidia Digits are examples of such accelerators.\n\nSince a convolutional neural network is basically a series of tensor operations, we can use tensor rank decomposition techniques to decrease the number of operations that need to be done for each layer. The paper \u201cCompression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications\u201d uses Variational Bayesian Matrix Factorization for rank selection and tucker-2-decomposition to split each layer into three layers.\n\nThis type of architecture is also found in ResNet, SqueezeNet and the inception layers in GoogleNet. It can be intuitively justified by considering that the input layers are correlated. So their redundancy can be removed by properly combining them with 1 x 1 layers. After the core convolution, they can be expanded for the next layer. The loss in accuracy due to this operation is compensated using fine-tuning.\n\nAlternatively, techniques like pruning and weight sharing are used to compress the model thereby decreasing the model size as detailed in Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding. The author claims that the network learns redundant connections during training. So he proposes to remove such connections and keep only the most informative ones. He does this by removing connections with weights below a certain threshold and finetuning the remaining weights. Using this technique he achieves a 9x reduction in parameters for AlexNet. He also uses k-means clustering to identify weights that can be shared in a single layer.\n\nWith this, I conclude my three part series on making deep learning \u201cmobile friendly\u201d. Please let me know if I you have any suggestions.", 
        "title": "\u201cMobile friendly\u201d deep convolutional neural networks \u2014 Part 3"
    }, 
    {
        "url": "https://medium.com/@garyclifton/im-yet-to-read-an-account-of-the-potential-for-irrational-belief-in-ai-fb324991ec98?source=tag_archive---------1----------------", 
        "text": "I\u2019m yet to read an account of the potential for irrational belief in AI. It seems assumed that the product of rational minds (AI developers) will follow the rules based way of thinking that they value and the potential for AI to create a concept of Silicon Heaven is not considered. Given that the antecedents of black box neural network outputs can be very difficult to determine, it could very soon be beyond our ability to understand the justifications for AI beliefs.\n\nThe flash-crash stock trading of recent years are salient examples of this. Many work-hours of analysis are required to understand their triggers, and this lag will only increase with added complexity.\n\nWhat is the danger of our AI developing a God complex?", 
        "title": "I\u2019m yet to read an account of the potential for irrational belief in AI."
    }
]