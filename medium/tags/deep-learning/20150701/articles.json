[
    {
        "url": "https://medium.com/technology-invention-and-more/how-to-build-a-multi-layered-neural-network-in-python-53ec3d1d326a?source=tag_archive---------0----------------", 
        "text": "In my last blog post, thanks to an excellent blog post by Andrew Trask, I learned how to build a neural network for the first time. It was super simple. 9 lines of Python code modelling the behaviour of a single neuron.\n\nBut what if we are faced with a more difficult problem? Can you guess what the \u2018?\u2019 should be?\n\nThe trick is to notice that the third column is irrelevant, but the first two columns exhibit the behaviour of a XOR gate. If either the first column or the second column is 1, then the output is 1. However, if both columns are 0 or both columns are 1, then the output is 0.\n\nSo the correct answer is 0.\n\nHowever, this would be too much for our single neuron to handle. This is considered a \u201cnonlinear pattern\u201d because there is no direct one-to-one relationship between the inputs and the output.\n\nInstead, we must create an additional hidden layer, consisting of four neurons (Layer 1). This layer enables the neural network to think about combinations of inputs.\n\nYou can see from the diagram that the output of Layer 1 feeds into Layer 2. It is now possible for the neural network to discover correlations between the output of Layer 1 and the output in the training set. As the neural network learns, it will amplify those correlations by adjusting the weights in both layers.\n\nIn fact, image recognition is very similar. There is no direct relationship between pixels and apples. But there is a direct relationship between combinations of pixels and apples.\n\nThe process of adding more layers to a neural network, so it can think about combinations, is called \u201cdeep learning\u201d. Ok, are we ready for the Python code? First I\u2019ll give you the code and then I\u2019ll explain further.\n\nAlso available here: https://github.com/miloharper/multi-layer-neural-network\n\nThis code is an adaptation from my previous neural network. So for a more comprehensive explanation, it\u2019s worth looking back at my earlier blog post.\n\nWhat\u2019s different this time, is that there are multiple layers. When the neural network calculates the error in layer 2, it propagates the error backwards to layer 1, adjusting the weights as it goes. This is called \u201cback propagation\u201d.\n\nOk, let\u2019s try running it using the Terminal command:\n\nYou should get a result that looks like this:\n\nFirst the neural network assigned herself random weights to her synaptic connections, then she trained herself using the training set. Then she considered a new situation [1, 1, 0] that she hadn\u2019t seen before and predicted 0.0078876. The correct answer is 0. So she was pretty close!\n\nYou might have noticed that as my neural network has become smarter I\u2019ve inadvertently personified her by using \u201cshe\u201d instead of \u201cit\u201d.\n\nThat\u2019s pretty cool. But the computer is doing lots of matrix multiplication behind the scenes, which is hard to visualise. In my next blog post, I\u2019ll visually represent our neural network with an animated diagram of her neurons and synaptic connections, so we can see her thinking.", 
        "title": "How to build a multi-layered neural network in Python"
    }, 
    {
        "url": "https://medium.com/@memoakten/deepdream-is-blowing-my-mind-6a2c8669c698?source=tag_archive---------1----------------", 
        "text": "The poetry is blowing my mind at every step of the process\u2026\n\nWhen an artificial neural network receives an input such as an image, it tries to make sense of it based on what it already knows. The image data flows through the network, \u2018activating\u2019 neurons. Effectively the image is ripped apart and scanned for features that the network recognises. This can be thought of as asking the network \u201cBased on what you already know, can you see anything here that you recognise?\u201d.\n\nThis of course is how we make sense of the world. It\u2019s analogous to asking us to recognise objects in clouds or ink / Rorschach tests. But I don\u2019t even mean just visually. We try to frame everything that we see, hear or learn within the context of what we already know, and we build on top of that. This can be purely visual like seeing faces in clouds. Or it can be more critical as it affects how we learn, interpret information, make decisions, construct theories or develop prejudices based on the limited knowledge that we have. If we don\u2019t have sufficient information, the assumptions we make are likely to be incorrect, as are the decisions we make as a result of them.\n\nWhen the network is processing the image, some of these recognitions might be weak firings within the network. These weak neural firings can be thought of as almost sub-conscious level \u201cI think I see a little bit of a lizard-like texture over here, perhaps something that resembles a bridge over there\u201d. But if these are very weak activations in the deep layers, they\u2019ll dissipate within the network and won\u2019t elevate to higher layers, or influence the final output.\n\nBut in the case of #deepdream, we choose a particular group of neurons dedicated to detecting particular features\u200a\u2014\u200ae.g. those which respond to lizard like features\u200a\u2014\u200aand we take a snapshot image, from inside the network. Whatever features a particular group of neurons respond to, will be dominant in the snapshot image created from those neurons. (NB. Technically speaking, by \u2018take a snapshot\u2019 I mean we choose a group of neurons, and we modify the input image such that it amplifies the activity in that neuron group. See my other article for more non-technical technical info).\n\nThis snapshot shows what that particular group of neurons are responding to, or \u2018thinking about\u2019.\n\nWhen we feed that snapshot image back into the network as a new input, the network recognises those exact same features but with more confidence, because those patterns in the new image are now stronger, so those same neurons fire stronger. And when we take another snapshot of the same neurons, and feed that back in, it becomes even stronger. What was an initial \u201cmaybe I see inklings of little lizard-like features over here\u201d on a deep sub-conscious level, starts to become \u201cyea, I think they might be lizard-like features\u201d, to \u201coh definitely, that\u2019s a lizard-skin puppy-slug\u201d at a well defined, visible high level. These activations are now strong enough to not dissipate and disappear in the depths of the network, and can propagate to higher levels, potentially even affecting the final output or decision.\n\nThis creates a positive feedback loop, reinforcing the bias in the system. Building confidence with each iteration. Transforming what was subtle, unnoticeable trends deep within the network, to strong, visible, defining biases that affect the decisions of the network.\n\nThis is almost like asking you to draw what you think you see in the clouds, and then asking you to look at your drawing and then draw a new image of what you think you are seeing in your drawing. And repeating this.\n\nBut that last sentence was not even fully accurate. It would be accurate, if instead of asking you to draw what you think you saw in the clouds, we scanned your brain, looked at a particular group of neurons which we know responds to a particular pattern, then we reconstructed an image based on the firing patterns of those neurons, and gave that image to you to look at. And then we scanned the same neurons again to produce a new image and showed you that etc.\n\nThe critical difference is, if we\u2019d asked you what you saw in the clouds, we\u2019d be representing the final conscious decision you made regarding what you saw. Whereas by scanning and extracting the snapshot from a group of neurons, we\u2019re preying on and amplifying a particular thread of thought to create a strong bias. Like an indoctrination on a neurological level.\n\nThis of course is analogous to so many aspects of how our mind functions already. We see the world through the filter of a biased mind. A product of our upbringing, everything we've ever seen or learnt, the culture in which we live or come from. We project this bias onto everything we perceive, and if we\u2019re not very careful, everything we perceive will in turn reinforce the very bias that shaped it.\n\nThe face in the clouds looks more and more like a face the more we think it\u2019s a face. The shadow in the alley looks more and more like a mugger the more afraid we become. The image of the virgin mary on a piece of toast is more tangible the more we want to believe in it. The more convinced we are of a certain hypothesis, the more inclined we are\u200a\u2014\u200asubconscious or not\u200a\u2014\u200ato find that every piece of evidence confirms that hypothesis.\n\nInterestingly, even if you don\u2019t agree with my previous points, you've probably already confirmed them. If you see a human or ape like face in the image below; or [bird, slug, reptile, worm, puppy, sloth]-like creatures; then you have just demonstrated it. Perhaps you see something else? Something I can\u2019t see? Then you've confirmed my point even stronger.\n\nThere are no faces, birds, slugs, reptiles, worms, puppies, sloths in the image above.\n\nYour mind is projecting those meanings, trying to recognise patterns based on what it already knows, what it\u2019s been trained on. Neurons in your brain stimulated by different features of these abstract shapes are trying to make sense of what you\u2019re seeing and frame it in context of something familiar. (NB. Also see See apophenia and pareidolia).\n\nJust like the #deepdream neural network.\n\nYou\u2019re looking into a mirror of your own mind.\n\nEven more interestingly, remember that these images generated by the #deepdream process are not what the network is seeing on a high level. These images are extracts from inside the network. Abstract representations from the depths of its memory. Snapshots from inside the AI\u2019s brain.\n\nThese were weak neural firings, that we amplified. Without us interfering, these firings might not have even elevated to higher levels, and would have remained latent in the network. On a higher level the AI might not even be aware of these features. But deep inside the artificial neural network, certain neurons fired weakly, responding to certain features that the network recognised.\n\nAnd then you look at these images. You find patterns in them. You recognise the exact same features that the artificial neural network had recognised, and amplified. You project meaning onto this noise, the abstract representations extracted from inside the hidden depths of the artificial neural network. In recognising these forms, you are confirming what the #deepdream neural network recognises but doesn't know that it\u2019s seeing. The same neurons which fired weakly in the depths of the artificial neural network, are now firing in your brain.\n\nYou are completing this cycle of recognition and meaning in your mind.", 
        "title": "#Deepdream is blowing my mind \u2013 Memo Akten \u2013"
    }, 
    {
        "url": "https://medium.com/@memoakten/background-info-for-deepdream-is-blowing-my-mind-1983fb7420d9?source=tag_archive---------2----------------", 
        "text": "An Artificial Neural Network (ANN) can be thought of as analogous to a brain (immensely, immensely simplified. Nothing like a brain really).\n\nIt\u2019s not really like a brain, it\u2019s just metaphorically similar. And initially was inspired by (what we think we thought we knew about) how the brain works. But there are some similarities on a very high level.\n\nAn ANN consists of \u2018neurons\u2019 and \u2018connections\u2019 between neurons. The neurons are usually organized in layers. See this image from Wikipedia:\n\nData flows in one side of this neuron network (via input nodes), gets processed along the network, and something is output on the other side via output nodes. (NB. In this context \u2018Data\u2019 and \u2018Information\u2019 mean the same thing, numbers. Long sequences of numbers)\n\nEach connection (i.e. the arrows in the image above) between two nodes has a weight associated with it. This number is the strength of that connection. (NB. When data is fed to input nodes, they get passed down all of the arrows, multiplied by the \u2018weight\u2019 of each connection. The receiving nodes add up all of the numbers they receive from all their connections, put them through a little function called an \u2018activation function\u2019, and sends the results down their own arrows to the next nodes. This is repeated throughout, all the way to the output nodes -> RESULT)\n\nIn short an ANN processes and maps an arbitrary number of inputs, to an arbitrary number of outputs. In this way the ANN acts like (and is often said to \u2018model\u2019) a mapping function.\n\nAnd most importantly: Information (the function it models) is stored in the network as \u2018weights\u2019 (strengths) of connections between neurons.\n\nIf we feed a network some inputs, and the output depends on these connection weights, how do we know what weights we should use?\n\nThat\u2019s where training comes in.\n\nIn what\u2019s known as supervised learning, you provide the network with a bunch of training examples, in simple terms: input-output pairs. (NB. There are other types of learning too. Like unsupervised, where you don\u2019t provide training examples, you just give the network a bunch of data, and it tries to extract patterns and relationships. Or semi-supervised learning which is a mixture of both.)\n\nYou would effectively say \u201cFor this input A, I want this output X; For this input B, I want this output Y; for this input C, I want this output Z\u201d. This is analogous to pointing to pictures of animals with a toddler going \u201cCAT\u201d, \u201cDOG\u201d etc. You\u2019re associating inputs (pictures of cats and dogs) with outputs (the words \u201cCAT\u201d and \u201cDOG\u201d).\n\nThen you say LEARN! And a long iterative process tries to solve the network. The problem it\u2019s trying to solve is: what are the weights I need on each connection, such that when I feed in the inputs of the training examples, I get the corresponding outputs of the training examples. (NB. In reality you will never be able to train the network such that you get the exact same outputs for the training inputs. So it\u2019s more about trying to minimise the error.)\n\nAfter you've trained the network, the network has (hopefully) optimum weights on each connection. Such that if you were to feed it the inputs from the training examples, you (hopefully) get the same (or near) results as the corresponding outputs.\n\nWhere it gets interesting and potentially useful (and potentially wrong and scary), is if you feed it new input data that it hasn't seen before, and it tries to interpolate / extrapolate / calculate / predict relevant new output based on the patterns it\u2019s found from the training data. Current models predict on a spectrum ranging from spectacularly accurate, to spectacularly wrong.\n\nSo how the hell do you use this for complex tasks like image or voice recognition?\n\nThe network image from Wikipedia above is a very simple network. Suited to simple (i.e. low dimensional) problems. But to solve complex problems with it, you\u200a\u2014\u200aas the trainer of the network\u200a\u2014\u200awould need to be very specific in the data you feed it. You couldn't just feed it raw image data (i.e. millions of pixels), the network wouldn't be able to cope with the immense amount of information found in a raw image. You would need to do a hell of a lot of manual, handcrafted feature extraction first to reduce the dimensions (i.e. amount of information). Instead of feeding it raw images, you might need to run filters on the image first, find edges, break it down into simpler shapes, etc. And then feed those reduced, simplified features into the network for training and processing. This process of manually identifying and extracting features\u200a\u2014\u200acalled feature engineering\u200a\u2014\u200ais a major bottleneck. It\u2019s difficult, time-consuming and requires domain specific knowledge and skill.\n\nYou can feed deep networks complex, raw inputs. And they will do the feature extraction for you, so you don\u2019t have to. Which features do they extract? Whatever they need, they learn that too! They figure it out based on the data. The deep learning model is a essentially a stack of parameterised, non-linear feature transformations that can be used to learn hierarchical representations. During training, each layer learns which transformation to apply\u200a\u2014\u200ai.e. it learns which feature to extract\u200a\u2014\u200aand how. As a result, the deep learning model stores a hierarchy of features with an increasing level of abstraction. It\u2019s pretty insane. (NB. This is why Deep Neural Networks are often referred to as \u2018black boxes\u2019. You just feed them inputs and they give you outputs. It\u2019s quite difficult to get an intuition of what\u2019s happening inside. That\u2019s why people are trying to invert them and visualise layers, more on that below).\n\nIt\u2019s worth pointing out a caveat to this: we don\u2019t need to manually handcraft the features, but instead the network architecture needs to be manually handcrafted depending on the type input data. So the network is very domain specific. E.g. usually for image classification very specific convolutional networks are used. Yann Lecun\u2019s LeNet was the original. AlexNet, GoogLeNet, VCGNet are some modern ones. For speech recognition completely different Recurrent Neural Networks are used. And there\u2019s loads of different options. This is quite a bottleneck, and we\u2019re still a long way from having a single, universal, general purpose learning algorithm. (Though Jeff Hawkins\u2019s team is working on it).\n\nAn important point to make clear is: An artificial neural network does not store any of the training data. The training data was only used to learn the weights for the connections, and once trained, the data is not necessary.\n\nImagine a complex neural network is trained on millions of images (e.g. http://image-net.org)\n\nThis network just stores the weights of the connections required to recognise images. In doing so it is storing abstract representations of various image features that it has learnt is required to identify the different categories.\n\nThis is worth re-iterating: E.g. for #deepdream, the original images that the network was trained on, is over 1,200,000 MB (1.2 TB) whereas the trained network itself is only ~50 MB\u200a\u2014\u200aand that\u2019s all you need to make predictions.\n\nNormally you feed data into the input layers of a neural network, that data is fed through the network being processed, and results come out of the output layer.\n\nThere is a recently discovered process called \u2018inverting the network\u2019.\n\nIn this case you are effectively asking the network \u201cWhat type of input do you need, to give this particular output\u201d. Of course the network doesn't explicitly know that. But there are ways of manipulating an input image, such that we get the desired results. This is effectively running the network backwards.\n\nA very crude way of putting this is you give the network a completely random picture that looks nothing like a cat and you ask it \u201cdoes this look like a cat?\u201d, the network says \u201cno\u201d. You make a few random changes and ask \u201cwhat about this?\u201d, and the network says \u201cno\u201d. And you keep repeating. If the network says \u201cyea, maybe that looks a bit more like a cat\u201d you say \u201caha! ok so I\u2019ll make more changes in that direction, how about this?\u201d. It\u2019s actually not exactly like that, but you get the idea. And you can see why it takes so long.\n\nThe key thing is, the network doesn't actually know what a cat is, it only recognizes one when it sees it.\n\nThe above technique can be applied to not only end results. I.e. \u201cdoes this input look like a cat\u201d. But also intermediary (hidden) layers, by looking for inputs which maximize activity on specific neurons. So you can pick a neuron in the network, and evolve the input such that you get maximum activity on that particular neuron. Then effectively we are finding what that neuron responds to, i.e. what it represents. (NB. This is similar to how neuroscientists figure out which neurons correspond to which types of inputs. e.g. Feeding specific images to the eye and measuring what happens in the brain, they can see how some neurons respond to diagonal lines, or vertical lines, or to light then dark, or dark then light, horizontal movement, vertical movement etc.)\n\nOne interesting feature, is that the lower layers (close to the inputs) respond (i.e. represent) low-level, abstract features. Like corners, edges, oriented lines etc. Moving up the layers represent higher level, more defined features. This is remarkably analogous to how information is thought to be processed in our own brain (and the mammalian cerebral cortex). A hierarchy of features, starting at low-level abstract features close to the input, and layers building on top, establishing higher and higher level representation with each layer. (NB. Of course it\u2019s way more complex in the brain, and no one really knows absolutely for sure, but most accepted theories point this way).", 
        "title": "Background info for \u201c#Deepdream is blowing my mind\u201d"
    }, 
    {
        "url": "https://blog.alexlenail.me/ethics-are-too-complicated-for-words-85597f86b841?source=tag_archive---------3----------------", 
        "text": "Ethics fundamentally concern themselves with the rightness of decisions, so an ethical system can be formalized as some function on the domain of the infinite set of scenarios and decisions that maps each of those (scenario, decision) coordinates to a level of righteousness.\n\nEthical discussions traditionally concern themselves with the morally hazardous (situation, decision) subspaces and boundary conditions. The trolley problem, for example, is such a subspace, which helps us gauge the general shape of the \u2018ethics\u2019 function a person abides by.\n\nUnder this analogy, if we (temporarily) assume all decisions are either right or wrong, (discretizing the z-axis), learning ethics becomes isomorphic to the common task of binary classification from Machine Learning. Let\u2019s extend this analogy a little further and see what we find.\n\nDeontological Ethics, recall, are a simple analysis of the space of decisions: in any situation, some set of decisions is always wrong, independent of outcome. In the analogy to machine learning this system of ethics maps to the set of decision-stump-based models (decision trees, boosting, etc\u2026), some of the most na\u00efve models.\n\nUtilitarianism, evaluates likely outcomes of each decision, and ascertains which one to take based on an analysis of those predictions. That maps by analogy to a transform on the input space, and choosing between outcomes (and their probabilities) instead of actions. In machine learning, we call this a feature transformation, where the moral rectitude of a decision is nonlinear in the the input space but linear in the transformed space (which in this case is the predicted outcome of the decision from the situation).\n\nOnce you know whether a (situation, decision) will promote \u2018good\u2019, the ethics become very simple, according to Utilitarianism.\n\nBut decisions aren\u2019t made in either of these ways, nor should they. Neither of these models accurately nor satisfactorily approximates the distinction between right and wrong, or describes how humans face ethical choices. There\u2019s something missing, some amount of subtlety which these models don\u2019t seem to capture: they seem far too mechanical to cope with all of the grey areas of human existence.\n\nThe way humans respond to The Trolley Problem sheds some light on the nature of the gap between human decision-making and these two famous ethical approaches. Recall the first scenario: The Switch, in which the decision to save five results in the death of one. What would you do?\n\nMost people will flick the switch. However, hardly anyone claims they will throw the innocent Fat Man to his death, even though from a consequentialist standpoint, the outcomes are identical. If most would flick the switch (anti-Kant) but not push the fat man to his death (anti-utilitarian), then although both of these models seem plausible, a majority of polled humans don\u2019t abide by either of them. Many people devote themselves to coming into alignment with one of these systems, but I claim that misguided endeavor results from old, broken ethical rhetoric being allowed to retain a primacy it should have ceded long ago. These two systems, endlessly debated, do not actually drive our decision-making, nor should they.\n\nHow might a more complex machine learning model approach the task of discriminating between right and wrong (situation, decision) coordinates? Imagine for a moment a Neural Network\u2019s approach.\n\nWhat are the hidden units, the internal nodes, in this metaphorical algorithm? They are each unique transformations of the features of the (situation, decision) into deeper, less articulable, and yet more salient features. They are the concepts and ideals which as a whole represent the inexpressible notions of right and wrong.\n\nWhen confronted with the Trolley Problem, humans envision the scenario, pull from any experiences which are whatsoever similar by a variety of analogies and approximate values for a huge number of feature variables, making a great many predictions/assumptions about a great many potential decisions at once, and then choose from those the decision which they are most drawn to by some inarticulable intuition, which they may subsequently seek to justify, perhaps even using Utilitarian or Deontological rhetoric.\n\nHumans flick The Switch but refuse to push the Fat Man for reasons they can hardly express. We don\u2019t have the declarative semantic structures to handle all the complexity of our decision-making so we use metaphorical language, describing situations as \u201cgrey areas\u201d.\n\nBut we usually make very good decisions the vast majority of the time, because decision-making is a task for which we have an extraordinary aptitude, greater than any mechanistic model, greater than we can even describe in words. Deontological Ethics and Utilitarianism are not only wrong, but they dramatically undersell human potential.\n\nOur behavior has been meticulously refined for our entire lives performing an optimization algorithm machine learning researchers have yet to invent. Given a situation, we perform a series of deeply non-linear transformation of the input we receive from our senses; combinations of contextual cues and instinctual and learned motives and ideas provide an intuition of rightness, the source of our decisions.\n\nJeremy Bentham and Immanuel Kant were pioneers of ethical thinking in their time, but the fact that humans have flouted their teachings for centuries indicates to me that they must have missed something. Abstract Ethical arguments around these two simplistic paradigms only serve to undersell our own capacity for righteous action, and may lead us to worse decisions.\n\nMachine Learning, and the Neural Network algorithm in particular, provide a framework for re-examining the brain from a mathematically regimented standpoint, and furthermore, a language to grapple with some of the more sophisticated phenomena it produces, such as ethical decision-making.\n\nIn a parallel sense to how the brain inspired the Neural Network algorithm, the algorithm can inspire ways of understanding the brain.\n\nNext time you make a decision which causes you to think twice, try to examine the inner workings of your own mind, both conscious and unconscious, which might be driving your thinking. It might surprise you.", 
        "title": "The way we talk about Ethics is broken \u2013"
    }, 
    {
        "url": "https://engineering.huew.co/introducing-deep-learning-studio-d70d8694c05b?source=tag_archive---------4----------------", 
        "text": "Computer Science (or CS) is at the edge of a revolution; machine intelligence has dramatically improved over last few years and is poised to surpass human intelligence!\n\nWhat is artificial (or machine) intelligence?\n\nHumans are exceptionally better at executing several complex tasks. Take for example, recognizing a person you met 2 years back from a crowd of hundred people at a mall, or identifying a unique music genre from a song playing in the background. Machines (or software programs), however, lagged behind (until now!) in solving such complex problems. We became better at machine learning to solve computationally complex problems (traditional big-data or text mining). However, there was a large gap in these machines solving complex tasks where humans are significantly (and inherently) better at.\n\nThis gap is narrowing down at a fast rate, and machines are becoming as intelligent as human beings! Taking inspirations from the workings of human brain, computer programs are getting better every day in learning patterns and executing complex tasks. Machines can now look at millions of images and identify unique images consisting of a rare dog-breed, machines can now watch videos and interpret different objects and their interactions, machines can now listen to your speech and understand your mood and sentiment. We have entered the age of intelligent machines.\n\nAt the center of this revolution is the ability of CS researchers to build large-scale \u201cdeep neural networks\u201d (DNNs). DNNs, in a nutshell, model the ability of human brain to solve complex problems. The past two years have seen exciting research outcomes that have helped CS researchers and engineers to build and apply DNNs to image, speech, video and text analytics; tasks at which machines lagged behind human intelligence. No more! Changing the landscape dramatically, DNNs are becoming better at mimicking neural-activities in human brain and bringing ultra-sophisticated intelligence to software programs.\n\nWhy is there so much buzz around artificial-intelligence (AI) now?\n\nThe buzz in AI is largely surrounding \u201cdeep learning\u201d. Deep Learning is about learning multiple levels of representations and abstractions that help in making sense of data such as images, sound, and text. Deep learning arranges artificial neurons (each neuron stores some information) in a sequence of layers and enables these layers to collectively learn (or model) a classification or clustering task.\n\nThere are two key trends defining innovations in deep learning: (a) availability of huge amounts of data which makes it easier for software to learn by examples (learning can\u2019t happen unless we tell machine what is \u2018good\u2019 and what is \u2018bad\u2019), and (b) advances in computation infrastructure which makes a reality for large-scale models to learn better and faster (what used to take months earlier now takes hours). These trends are making it possible for deep learning to truly define the future of computer industry.\n\nAs a result, analyzing rich media and big data comprising of images, videos, speech and text is no more about writing custom hand-written algorithms; we just need to unleash DNNs! DNNs automatically and accurately learn latent features (or representations) which empowers them distinguish between say breeds of different animals, or genres of music, or sentiments in speech, text data.\n\nWhy are we passionate about using \u201cdeep learning\u201d?\n\nWe are excited about the growing scale of user-generated image (or photo), speech, video and text (collectively called \u201crich media\u201d) data. This opens up tremendous opportunities for us to contribute and solve some of the pressing problems faced by today\u2019s businesses. At Huew, our goal is to scale, simplify and manage rich media of businesses that crucially rely on the utility of images, videos, speech or text data. Let us see a real-world example and understand \u2018why and how\u2019 deep learning can help to solve business problems.\n\nLet us think about the need and scale of Internet-based marketplaces (buyer and seller platforms for second hand goods) to make sense out of streams of tens of thousands of photos coming onto their platforms every day. It is well-known that photos on product listings have tremendous impact on buyer experience, click-through-rate (CTR), buyer engagement and conversion ratios. However, most of these websites suffer from \u201cphoto pollution\u201d; despite having an army of manual laborers to approve new listings, a significant fraction of photos ingested by their platforms have poor quality and irrelevant photos.\n\nFor instance, a significant fraction of these photos turn out to be stock photos, many times there is a repetition of photos within and across listings, and photos sometimes are uploaded under unintended categories. Such poor visual quality of listings adversely impacts experience of buyers and affects eventual conversions. Not to mention the problem of filtering huge amount of profane photos. A pool of manual labor can only do so much!\n\nUsing deep learning, we have made it possible for a machine to automatically and accurately identify a profane photo from an acceptable photo. We can now accurately classify a stock photo vs a user-generated photo. Furthermore, using models trained using deep learning, we can automatically categorize a photo among 100 different categories while filtering duplicate photos, without any manual intervention. We feel that the possibilities are endless. We have further applied deep learning to extract objects out of photos and show \u201cvisually similar\u201d listings. We are truly excited about these capabilities of deep learning.\n\nWe have been building platforms, algorithms and products to innovate and apply deep learning based frameworks to various businesses. Our core platform makes it very easy to ingest large amount of image, speech and text data and run deep neural networks. We call this platform \u2018deep learning studio\u2019 (DLS). We have been using DLS to process vast amount of data from the Internet and apply GPU infrastructure on popular cloud-data-centers to learn large-scale DNNs. We have built several APIs on top of DLS that can be seamlessly invoked to utilize the machine-learning models learned underneath.\n\nWe believe that machine intelligence has got the Midas touch to transform industries, businesses and services that pervades every aspect of our daily activities. We believe that machine intelligence can enable societies to lead smarter, simpler and better lives. At Huew, we continuously push boundaries of technologies to solve complex real-world business problems and build superior quality machine intelligence.\n\nIn the next few posts, we will give insights on how we are training, validating and testing deep neural networks on DLS, specifically convolution neural networks, to solve classification and clustering problems for photo-analytics.", 
        "title": "Introducing Deep Learning Studio \u2013"
    }, 
    {
        "url": "https://medium.com/@iguchijp/%E5%BF%85%E8%A6%81%E3%81%AA%E3%81%AE%E3%81%AF%E3%81%8A%E3%81%97%E3%82%83%E3%81%B9%E3%82%8A%E3%81%A0%E3%81%91-%E3%81%AE%E4%B8%96%E7%95%8C%E8%A6%B3-c926a0006c56?source=tag_archive---------5----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u5fc5\u8981\u306a\u306e\u306f\u304a\u3057\u3083\u3079\u308a\u3060\u3051\uff01\u306e\u4e16\u754c\u89b3 \u2013 takahito iguchi \u2013"
    }, 
    {
        "url": "https://blog.hyperverge.co/hyperverge-2-0-part-ii-the-journey-a82d0c0118fe?source=tag_archive---------6----------------", 
        "text": "In our previous post, which also happens to be our first post on the blog, we introduced you to our new brand identity. In this post, we will take you through our branding journey, starting from where we were, how we got here, and where we are headed.\n\nHyperVerge was born out of the Computer Vision Group (CVG) at IIT Madras. We started the Computer Vision Group with a vision of creating a group of students who could develop deep expertise in the technology of computer vision. The members of CVG worked towards creating different modules that would form key components of entries to various international robotics competitions. Apart from working on different academic problem statements, CVG also focused on solving real-life problems faced by industries through consulting assignments. In 2012\u20132013 the team worked on pilot projects for the Indian Railways (Overhead Line Inspection System), ITC (Biscuit Inspection System), MRF (High speed inspection of tyres), etc.\n\nOur CVG logos were developed using MS PowerPoint (which became our magical go-to tool for all our designs henceforth).\n\nIn summer of 2013, Vignesh (my co-founder) was interning at Microsoft Research, Redmond. While he was there, he constantly kept pinging me and asking me to set up a Skype call. I thought he wanted to discuss about a new Kinect or some new technology that he might have seen at his office. When we finally did get on that call, he said, \u2018Listen, I can work for them anytime in the future. We need to start our company now\u2019. I tried scaring him and Kishore (another co-founder) about startups as much as I could (I was already running Lema Labs then). They refused to listen to my reasoning. With a lot of excitement and a bit of fear, we decided to take the plunge.\n\nWe needed to decide a name for our company. Like most naming exercises, this one became interesting only late at night. We thought of Mawobe, Latele, Kahuna and lot more. Finally we decided on HyperVerge. It stands for going beyond the limit\u200a\u2014\u200aHyper- (beyond) and Verge (the limit). We felt that this name best captured our journey so far; reaching out towards a crazy goal that was then way beyond our abilities, and somehow making it work!\n\nWe secured our first contract not long after registering the company. It was while preparing this contract that we realized we needed a company logo for it. We could no longer use the CVG one. Frantic work late night, font borrowed from Lema Labs and MS PowerPoint (again!) efforts resulted in us creating a very simple logo in White and Blue.\n\nSoon we decided to focus on the problem of photo organization and using technology of deep-learning to solve the same. As our first prototypes were getting ready, we went to the Bay area last summer (July 2014) to pull off a Kickstarter campaign. We returned with close to a million dollars in funding from some of the top Bay Area VCs!\n\nOne of the first decisions we took after receiving funding was to get a new brand identity. Right away we got in touch with Beard Design. We were always very impressed by their work. In the past we had unsuccessfully tried very hard to convince them to work for a promising young startup\u200a\u2014\u200aone that is going to be the next big thing but has no cash.\n\nWork with Beard Design started with a tremendous amount of excitement. They sent us an elaborate questionnaire which asked us briefly about the history of the company, what our company stood for, and what our expectations in terms of a logo were.\n\nAs they sent over the first concepts, we went through the standard process that usually happens after hiring any consultant (High excitement\u200a\u2014\u200aLook at initial concepts\u200a\u2014\u200aShock\u200a\u2014\u200aAre we paying so much money for this?!\u200a\u2014\u200aOk, these guys know what they are talking about)\n\nMost design stages were met with a \u2018What?!\u2019 reaction from the entire team. With a heavy heart, I called up Beard Design to express our shock and reject the design. Abhisek (no-nonsense founder of Beard design) explained the concept so me in greater detail, and managed to convince me why these designs were right for us (Design 101 tutorials). In turn, I convinced the team. The process moved forward.\n\nFinally we were very happy with what Beard Design delivered. You can see our logo and identity here. We love their work and would recommend them to anyone who is looking for an amazing brand identity.\n\nThanks to Beard, we now have this cool logo that is not made in PowerPoint. It symbolizes in many ways what HyperVerge stands for. Futuristic, Simple and a Brand that is there to stay. This is just the very first step\u200a\u2014\u200avisual representation. We still have a long way to go in establishing our brand in terms of our products and technology. Our brand is also in the process of developing a personality, one that will grow and evolve in numerous ways.\n\nAll said and done, what I most hope for is that we as a team stay true to the weird idea we started out with\u200a\u2014\u200aA team of inspired people that will continue to reach out for the impossible and has dogged determination to make it happen!", 
        "title": "2.0 \u2014 Part II, the Journey \u2013"
    }, 
    {
        "url": "https://medium.com/@niland/deep-learning-and-the-future-of-music-listening-experiences-c0f11dcb614c?source=tag_archive---------7----------------", 
        "text": "In recent years, our musical horizons have expanded dramatically with unprecedented access to a panoply of songs via the Internet. But, how do we wade through this melodic overload to get a truly personalised listening experience?\n\nThe secret seems to lie in the bounding advances made in deep learning.\n\nDeep Learning refers to a sub-branch of Artificial Intelligence that gets machines to extract patterns and find rules. By using neural networks, which simulate the billions of neurons in our brains, computers can \u201clearn\u201d how to understand German or recognise when they see a cat (as opposed to a dog or rabbit).\n\nWith more powerful computers and huge amounts of data, companies like Google and Microsoft can harness its potential on a bigger and more viable scale. Although the focus is currently on speech and images\u200a\u2014\u200afor example, words errors were reduced by 25% when deep-learning-based speech recognition was added to Android smartphones\u200a\u2014\u200athis approach could and is being applied to different fields\u2026like music.\n\nBut let\u2019s not get ahead of ourselves. Before we look at deep learning in music, let\u2019s take a look at the music industry as it stands.\n\nThe much talked about decline in physical music sales has made way for the rise of the internet as the place to \u201cget\u201d music\u200a\u2014\u200awhether legally or illegally, by paying a subscription or sitting through lots of ads.\n\nYou can now access music whenever you want and in massive, gluttonous quantities.\n\nIn this highly competitive context, how can music sites stand out from the crowd and add genuine value?\n\nBy developing ever more sophisticated options to customise your listening or, in the words of Spotify chief executive Daniel Ek, by \u201ccreating a true soundtrack to your life\u201d.This digital music giant has just launched\u201cNow\u201d, which gives you musical recommendations based on whether you just got up or you\u2019re in need of an after-lunch boost.\n\nThis is certainly an exciting step in the right direction.\n\nYet, for the most part, this new \u201ccontextualisation\u201d draws on songs humanly curated to appeal to the majority. The same can be said for Apple Music which mainly targets people who have no idea what to play. This means that the latent recommendation power of the millions of long tail playlists uploaded to YouTube, Google Play or SoundCloud is left untapped.\n\nFinding exciting new music is highly rewarding, but the effort that goes into it can frustrate even the most determined of us. Too many new songs, too many new talents, too much \u201cnoise\u201d.\n\nNiland was set up in 2013 by a group of young researchers from IRCAM specializing in machine learning.\n\nBasically, we use what we know about machine listening and deep-learning algorithms to capture all the musical, cultural and emotional characteristics of songs based on their acoustic features\u200a\u2014\u200aand not just their popularity. This way we can provide the right sound for the right user at the right time.\n\nThe easy-to-use API \u201cunderstands\u201d the mood and content of each song.\n\nThe first version of this technology won the MIREX competition in 2011 and these yet-to-be-beaten results have been honed over the last 2 years through relentless R&D. Our service is currently being used to provide music licensers like SongFreedom, TBWA and Jamendo with the perfect song for adverts and films. Automatic classification helps them automate a costly process, while the music-search-similarity option builds a completely new client experience.\n\nAnd, now, we want to apply this technology to help you provide meaningful contextual playlists and recommendations.", 
        "title": "Deep Learning and the Future of Music Listening Experiences"
    }, 
    {
        "url": "https://medium.com/@yoninachmany/freudian-ai-v0-wordhack-1b20d262dcb2?source=tag_archive---------8----------------", 
        "text": "Here are 49 poems, in 4 chapters, from what Clarifai\u2019s \u201cartificial neural network\u201d sees in Google\u2019s neural net \u201cdreams\u201d\u200a\u2014\u200awhat I call Freudian AI (for more photos and descriptions, see https://goo.gl/photos/3HiBVoFNwkfJsiTd7).\n\nfine art, painting, medieval, nobody, renaissance, halo, roman, religion, robe, two, ancient, headgear, symbol, people, and ceiling\n\npeople, adult, two, group, men, spear, clothing, war, military, soldier, mammal, exploration, fine art, headgear, one, battle, painting, and many", 
        "title": "Freudian AI \u2013 Yoni Nachmany \u2013"
    }, 
    {
        "url": "https://medium.com/world-wcloud-baby/all-you-need-is-a-conversation-658e2895ce19?source=tag_archive---------9----------------", 
        "text": "We\u2019ve created a conceptual video in which Cloud Baby plays an active role. The story is based on the ideal image of the world that Doki Doki wants Cloud Baby to create. The main theme is creativity on a global scale. In the first scene, a housewife is humming. Then, that act of humming transforms into a symphony with Baby\u2019s help.\n\nThe interactive artificial sapience, Baby, encouraged people to naturally form an orchestra to create a symphony. Afterwards, the Babies themselves will communicate with each other to carry out a collective brainstorming session. Our video is an outcome of imagining a creative act that was impossible to be carried out, no matter how much effort people had put in to make it happen.\n\nIn 2015, most of the conversations are still carried out between humans. However, I believe that in the near future, more and more people will be accustomed to communicate with a machine (i.e. Artificial Intelligence). That is the moment when a kind, interactive artificial sapience needs to come on stage to hold hands with humans, and to form a good relationship with them. Such vision of the futuristic world is the kind of dream that Doki Doki wants to create and to be part of.\n\nAs you have probably realised already, in this video, Baby is only having a small conversation with the doctor and the housewife. In order to carry out such communication method with a computer, people still need to physically face the computer and to control the conversation themselves.\n\nWith Baby, we want to replace such tedious act with a casual chat. When the time comes, I believe that nurturing the individual value and life will be more appreciated than how efficiently you can handle a computer. When I think about it, the availability of the first generation computers was limited to specialists only. This shift (shift of computers becoming more available to wider range of people) seems only a natural progression to me.\n\nFor example, with cars, there was a time when only professionals were allowed to ride them. From now on, robots and drones are going to replace various types of occupation. Artificial Intelligence is going to encourage human progress even more, and will relief people\u2019s burden by absorbing daily routines. Baby, a kind computing system that is able to have a natural conversation, might be able to take in all the knowledge that scholars possess, only to free them to the world.", 
        "title": "All you need is a conversation. \u2013 World Wide Cloud Baby \u2013"
    }
]