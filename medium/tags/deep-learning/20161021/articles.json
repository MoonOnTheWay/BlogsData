[
    {
        "url": "https://medium.com/@aniketvartak/nuts-and-bolts-of-applying-deep-learning-summary-84b8a8e873d5?source=tag_archive---------0----------------", 
        "text": "Due to revival of deep learning techniques, work flow of a typical machine learning practitioner is changing for good. Following are some topics that the practitioner needs to keep in mind in his/her day-to-day job to use deep learning tools more effectively.\n\nDeep learning in the era of data availability explosion\n\nAs shown in the diagram above, the expectation of performance boost with a deep neural network increases linearly with amount of data available. This is in contrast to some of the more traditional ML algorithms. Thus, we can always expect incremental gain in performance if we just get more data for the task at hand. Although, this was known to the community for long time, the availability of huge amounts of data is what is changing. Note that near the \u201csmall data regime\u201d in the left side of the x-axis, the difference between the different methods is minuscule, but this changes quickly as we have more access to the data. This explosion in data availability is naturally coming from corresponding explosion in mobile usage numbers, IoT and so on.\n\nOne practical thing to note is: it is one thing to obtain a large set of data for your task, and it is entirely a new thing to maintain this dataset in a scaleable form. Thus for commercial applications it is recommended to maintain two teams, a systems team and an algorithms team.\n\nThe next theme Andrew Ng talked about was the types of deep networks that are out there today. This is bucketed in four broad categories.\n\nEven if the first three buckets are driving growth and products in this the industry now, one needs to watch out for the last bucket as far as future growth is concerned.\n\nThe whole field of deep learning was revived in 2012 when \u201cAlexNet\u201d beat traditional CV algorithms at ImageNet challenge by a huge margin. The slew of research papers that followed this were mostly focused on this straight forward class prediction task (although not simple by any means itself).\n\nWhat is more exciting and interesting is the innovations in deep learning systems that output much more complex and interesting information than a single number (i.e. a class prediction). One example is Image captioning where CNNs are used to produce a feature vector from input image. This feature vector is then used as a priming input to an RNNs system which produces word-by-word caption for the image. Other examples worth mentioning are audio transcription, speech translation and image generation.\n\nA Machine learning engineer has a lot of decisions to make while building a ML system. These include how much data is appropriate for the task at hand and when to go out and get some more? Whether to train the model for longer time? When is it time to re think the architecture? When to introduce/remove regularization terms? To answer these questions in systematic manner, Ng brings the good old bias/variance analysis in the mix. (Side note: A very intuitive explanation of these terms and how they are related to machine learning models is available in this blog). Looking at these factors from the lens of bias and variance lends itself to a flow-chart of decisions.\n\nHere is a recipe one can follow:\n\nFirst, collect following metrics for your problem:\n\nNow when you train your model, one of the following usually happens:\n\nIn this case, the network does not have enough freedom to explore different solutions and gets stuck in sub-optimal solutions quickly. Increasing the capacity of the network would be the next step here.\n\nHere we have the opposite effect going on. The network is perfectly (actually too perfectly) modeling the training data. This leads to high variance in model. Usual next steps are to add (or crank up) regularization. This restricts the degrees of freedoms of the large capacity model. Adding more data would also help.\n\nhigh bias and variance in the model\n\nHere is a case where you are way off from the ideal performance goal of the human level error rates. It is time to rethink your architecture, and add more data.\n\nFollowing workflow nicely describes different paths to take:\n\nAs this work flow indicates, in the deep learning era, there is always some next step we can take to get out of the issue we are facing. This was not true in the non-deep learning era, as one couldn\u2019t just increase the model size and throw more data at the model to resolve some of the issues.\n\nHow to leverage already existing (probably large size) data to build a ML system for your application?\n\nImagine the following scenario. We want to build an image classifier system for 10 classes. We have access to a large (e.g. ImageNet size) dataset to play with. We also have collected a much smaller dataset (due to practical reasons such as budget, project deadlines etc.) that covers the exact 10 classes you want your final system to predict accurately. This is described in the diagram below.\n\nThe hope from a deep learning system is that to build a generic feature learner that \u201csorts all the variabilities\u201d out on its own. So then how do you train a system that performs well for your problem?\n\nHere are some tips:\n\nWe can still reserve a small chunk of data as train-dev set out of the training set itself. With this, in addition to bias and variance characterization of the system we can also characterize our train-test data-mismatch error.\n\nUsing the above, the previous workflow can be refined as follows:\n\nHuman level performance as a benchmark: why has this become important?\n\nObservation: Teams would make rapid progress over time to reach human level performance. The progress usually tapers off beyond this level of performance.\n\nThe following could be the reasons for this:\n\nWe don\u2019t have processes to build AI products as yet. In order to build an AI product, usually product managers think about what an AI system CAN do. To help this thinking, following rules of thumb on what AI can actually do could be useful\u00a0:\n\nThese couple of guidelines could be a good starting point for anyone looking to building new and exciting AI solutions.\n\nAdvice on how to build a career in ML/DL?!", 
        "title": "Nuts and Bolts of Applying Deep Learning \u2014 Summary \u2013 Aniket \u2013"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/a-pattern-language-for-deep-learning-30de291434e1?source=tag_archive---------1----------------", 
        "text": "Pattern Languages are languages derived from entities called patterns that when combined form solutions to complex problems. Each pattern describes a problem and offers solutions. Pattern languages are a way of expressing complex solutions that were derived from experience such that others can gain a better understanding of the solution.\n\nPattern Languages were originally promoted by Christopher Alexander to describe the architecture of businesses and towns. These ideas where later adopted by Object Oriented Programming (OOP)practitioners to describe the design of OOP programs, these were named Design Patterns. These were extended further into other domains like SOA (http://www.manageability.org/blog/stuff/pattern-language-interoperability/view) and High Scalability (http://www.manageability.org/blog/stuff/patterns-for-infinite-scalability/view).\n\nIn the domain of Machine Learning (ML) there is an emerging practice called \u201cDeep Learning\u201d. In ML there are many new terms that one encounters such as Artificial Neural Networks, Random Forests, Support Vector Machines and Non-negative Matrix Factorization. These however usually refer to a specific kind of algorithm. Deep Learning (DL) however is not really one kind of algorithm, rather it is a whole class of algorithms that tend to exhibit similar \u2018patterns\u2019. DL systems are Artificial Neural Networks (ANN) that are constructed with multiple layers (sometimes called Multi-level Perceptrons). The idea is not entirely new, since it was first proposed back in the 1960s.. However, interest in the domain has exploded with the help of advancing hardware technology (i.e. GPU). Since 2011, DL systems have been exhibiting impressive results in the field.\n\nThe confusion with DL arises when one realizes that there actually many implementations and it is not just a single kind of algorithm. There are the conventional Feed forward Networks (aka. Fully Connected Networks), Convolution Networks (ConvNet), Recurrent Neural Networks (RNN) and less used Restricted Boltzmann Machines (RBM). They all share a common trait in that these networks are constructed using a hierarchy of layers. One common pattern for example is the employment of differentiable layers, this constraint on the construction of DL systems leads to an incremental way to evolve the network into something that learns classification. There are many such patterns that have been discovered recently and it would be very useful for practitioners to have at their disposal a compilation of these patterns. In the next few weeks we will be sharing more details of this Pattern Language.\n\nPattern languages are an ideal vehicle for describing and understanding Deep Learning. One would like to believe the Deep Learning has a solid fundamental foundation based on advanced mathematics. Most academic research papers will conjure up high-falutin math such as path integrals, tensors, Hilbert spaces, measure theory etc. but don\u2019t let the math distract oneself from the reality that our understanding is minimal. Mathematics you see has its inherent limitations. Physical scientists have known this for centuries. We formulate theories in such a way that the structures are mathematically convenient. The Gaussian distribution for example is prevalent not because its some magical construct that reality has gifted to us. It is prevalent because it is mathematically convenient.\n\nPattern languages have been leveraged in many fuzzy domains. The original pattern language revolved around the discussion of architecture (i.e. buildings and towns). There are pattern languages that focus on user interfaces, on usability, on interaction design and on software process. These all don\u2019t have concise mathematical underpinnings yet we do extract real value from these pattern languages. In fact, the specification of a pattern language is not too far off from the creation of a new algebra in mathematics. Algebras are strictly consistent but they are purely abstract and may not need to have any connection with reality. Pattern languages are however connected with reality, however consistency rules are more relaxed. In our attempt to understand the complex world of machine learning (or learning in general) we cannot always leap frog into mathematics. The reality may be such that our current mathematics are woefully incapable of describing what is happening.", 
        "title": "Design Patterns for Deep Learning \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/@VentureScanner/artificial-intelligence-market-overview-q4-2016-569be6b7f8b1?source=tag_archive---------2----------------", 
        "text": "The above sector map organizes the Artificial Intelligence sector into 13 categories and shows a sampling of companies in each category.\n\nDeep Learning/Machine Learning (Platforms): Companies that build computer algorithms that operate based on their learnings from existing data. Examples include predictive data models and software platforms that analyze behavioral data.\n\nDeep Learning/Machine Learning (Applications): Companies that utilize computer algorithms that operate based on existing data in vertically specific use cases. Examples include using machine learning technology to detect banking fraud or to identify the top retail leads.\n\nNatural Language Processing: Companies that build algorithms that process human language input and convert it into understandable representations. Examples include automated narrative generation and mining text into data.\n\nSpeech Recognition: Companies that process sound clips of human speech, identify the exact words, and derive meaning from them. Examples include software that detects voice commands and translates them into actionable data.\n\nComputer Vision/Image Recognition (Platforms): Companies that build technology that process and analyze images to derive information and recognize objects from them. Examples include visual search platforms and image tagging APIs for developers.\n\nComputer Vision/Image Recognition (Applications): Companies that utilize technology that process images in vertically specific use cases. Examples include software that recognizes faces or enables one to search for a retail item by taking a picture.\n\nGesture Control: Companies that enable one to interact and communicate with computers through their gestures. Examples include software that enables one to control video game avatars through body motion, or to operate computers and television through hand gestures alone.\n\nVirtual Assistants: Software agents that perform everyday tasks and services for an individual based on feedback and commands. Examples include customer service agents on websites and personal assistant apps that help one with managing calendar events, etc.\n\nSmart Robots: Robots that can learn from their experience and act autonomously based on the conditions of their environment. Examples include home robots that could react to people\u2019s emotions in their interactions and retail robots that help customers find items in stores.\n\nPersonalized Recommendation Engines: Software that predicts the preferences and interests of users for items such as movies or restaurants, and delivers personalized recommendations to them. Examples include music recommendation apps and restaurant recommendation websites that deliver their recommendations based on one\u2019s past selections.\n\nContext Aware Computing: Software that automatically becomes aware of its environment and its context of use, such as location, orientation, lighting, and adapts its behavior accordingly. Examples include apps that light up when detecting darkness in the environment.\n\nSpeech to Speech Translation: Software which recognizes and translates human speech in one language into another language automatically and instantly. Examples include software that translates video chats and webinars into multiple languages automatically and in real-time.\n\nVideo Automatic Content Recognition: Software that compares a sampling of video content with a source content file to identify the content through its unique characteristics. Examples include software that detects copyrighted material in user-uploaded videos by comparing them against copyrighted material.\n\nWe are currently tracking 1464 Artificial Intelligence companies in 13 categories across 73 countries, with a total of $8.5 Billion in funding. Click here to learn more about the full Artificial Intelligence landscape report and database.", 
        "title": "Artificial Intelligence Market Overview \u2014 Q4 2016 \u2013 Venture Scanner \u2013"
    }, 
    {
        "url": "https://medium.com/@getartwork/lets-unleash-the-creative-power-of-picasso-kandinsky-vangogh-derain-lichtenstein-cross-5d7e3c618222?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Lets unleash the creative power of Picasso + Kandinsky + Vangogh + Derain + Lichtenstein + Cross"
    }, 
    {
        "url": "https://medium.com/@PeterAllan/what-tesla-really-revealed-1ef38496c173?source=tag_archive---------4----------------", 
        "text": "Everyone who followed Tesla\u2019s recent announcement will probably be impressed with the technology presented. We finally got a glimpse on what Peter Hochholdinger was referring to in this recent interview by stating that the product is seven years beyond everything he\u2019s seen before (and he\u2019s seen a lot).\n\nBut what\u2019s the real game changer here from an automotive-industry perspective?\n\nWhile the industry is still worrying about take rates and trying to optimize their operations to accommodate an unthinkable variety, Tesla is just installing autonomous hardware in every car. Full self-driving capability will set you back $ 8,000. Adequately priced from my perspective. Car buyers were always willing to spend on capabilities. Tomorrow\u2019s cars most desired capabilities will be the advanced software.\n\nBut why would you put all this hardware in a car without knowing if buyers tick those two boxes in the Design Studio or upgrade later. What else could be the rationale behind all those sensors and the advanced computing unit in every single car and that two years early?\n\nGetting to full autonomy is simply put a software challenge. And to develop great software you need to get it out in the field and iterate quickly. But how to do it safely?\n\nWhat if you could deploy and run your algorithm continuously and compare its decisions with real drivers? What if you could upload all these insights to a central server. Deep learning algorithms need training and this system is going to receive a hell of a lot of it. Several billion miles by 2018. What if all this data allows you to prove to regulators that your system is safe?\n\nI bet this is exactly what Tesla will be doing.", 
        "title": "What Tesla really revealed \u2013 Peter Allan \u2013"
    }
]