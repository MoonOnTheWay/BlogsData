[
    {
        "url": "https://medium.com/@pilooch/a-machine-learning-api-to-rule-them-all-caffe-xgboost-and-tensorflow-are-in-a-boat-ee2a2f6ac4ef?source=tag_archive---------0----------------", 
        "text": "A year ago, I was building up my fourth Machine Learning API while hiking alone for days through one of the beautiful inner jungles of Taiwan. With no paper handy, and walking through the rainy season, I had to get it clear in my mind first.\n\nA year later, the result is DeepDetect, an Open Source API and server for deep learning, and more. And for the first time, I am happy with the result, especially with the API. I feel it is intuitive, easy to use, generic and malleable at the same time.\n\nI was interested in building up a useful and generic API to serve a set of complementary techniques. With the help from a handful of contributors, I was able to integrate Caffe, XGBoost, and soon Tensorflow, with no modification to either the server or the API.\n\nIn practice XGBoost\u2019s gradient boosted trees are a great alternative or complement to deep models. Tensorflow allows both models and data distribution, has good support for LSTM and RNNs, while Caffe shines in production on images and text data. Having a generic API allows painless switches among these libraries.\n\nThe following is an attempt to write down the main principles that were used when building this particular machine learning API. Hopefully it would benefit others and/or generate some thoughts and criticism, leading to improvements.\n\nI have been working with machine learning, deep learning, reinforcement learning and Markov decision processes for over ten years and I know the joys and pains. I have been developing my own tools and custom systems, most of them Open Source, for a variety of industries, from NASA Mars rovers activity planning to Airbus cybersecurity and other industrial automated systems.\n\nSo a year ago, my focus is on commoditizing the current addition to the AI toolbox, deep learning, the neural net renaissance. Many great libraries are available out there, and what a great surprise, most of them are fully Open Source, transparent, contributor friendly, and most of all, up-to-date with the state-of-the-art of publications. For the first time I am witnessing that the code was being trusted into the hands of practitionners before the papers had even hit the conference\u2019s reviewers!\n\nI had dealt for a sufficiently long time with various industries and research institutes to know that only a small portion of enterprises and developers could ingurgitate such a massive and fast moving experimental codebase. My focus was then definitely on commoditizing machine learning, and to start with, the main deep learning algorithms and architectures.\n\nWhile walking among the macaques and bears, I slowly organized the various elements in my mind. To start with, I knew that developers and practitioners would get used to the existing libraries, and that they\u2019d certainly start building around them. On the other side, existing businesses would ponder how to make good use of the new technology, in a pragmatic way, with clear returns and without compromising the existing working codebases. What would they have in common then\u00a0?\n\nAn Open Source project that does fit both needs in another context, that of search, is Elasticsearch. It started as a scalable search backend, a clear REST API and full JSON input/output data structures. Though I believe their API is a bit bloated nowadays, I still like the product, and its growth was quite spectacular.\n\nSo what would a deep learning API with an integrated server backend look like then\u00a0? I needed to structure it a bit more:\n\nIf these elements could be brought together into a generic machine learning server with a simple while powerful API, it would have to be malleable enough to accomodate both developers and enterprise needs. That is, the seamless switch from development to production (and back). It would have to speak JSON and unify several deep learning and machine learning libraries around a single framework and API while hiding most of their inner complexity.\n\nSo starting with a Machine Learning API, the core elements would be the resources and the data input / output structure.\n\nResources were considered the server\u2019s resources, not just the machine learning services. This design was favored because GPU and memory would be scarce resources over which to POST machine learning service\u2019s jobs. Also, it was simpler to memorize. Let\u2019s see the core resources:\n\nSo services hold the machine learning services, train and predict are the resources associated to the two main operations on statistical models. Note that there\u2019s no difference between supervised and unsupervised services at this stage.\n\nMain parameters for machine learning are input acquisition & pre-processing, statistical processing, and final output. Thus the natural data structure that comes to mind is very simple: input, mllib and output. mllib corresponds to the specific parameters of each supported library, the other two are self-explanatory. Let\u2019s see an example that creates an image classification service:\n\nThe breakdown of parameters into input, mllib, output is generic, it typically covers both supervised and unsupervised settings by adapting the output connector. The input connector deals with input formats, from CSV, libsvm, to text, including character-based features, and images. The mllib component embeds the machine learning library parameters at service creation, training and prediction time. It is very convenient as it allows to refer to each of the libraries original documentation for parameters, as these remain identical when used through the DeepDetect API.\n\nLet\u2019s see an input connector for a CSV format:\n\nPretty straightforward and independent from the machine learning library. Let\u2019s see a typical output connector for a training job:\n\nHere again, the metrics apply to all supervised services. Now let\u2019s look at a trick with the output. An output template in Mustache format can be set so that the standard JSON output can be transformed into any other format:\n\nThe template above allows supervised classification results to be directly fed and indexed into Elasticsearch, see http://www.deepdetect.com/tutorials/es-image-classifier/ for the full details. Also take note of the network object, that holds the output server to POST to. This object could be used within the input connector as well, to connect remote sources.\n\nThe template above is to be matched with at a typical supervised classification JSON output from the DeepDetect server:\n\nThis trick allows to get rid of glue code when integrating into existing pipelines, and nicely fits many enterprise use cases.\n\nLet\u2019s now give a quick look to the mllib component, with Caffe, then XGBoost:\n\nWith Caffe, the server is instructed to use the GPU, and other parameters set the solver, the learning rate, etc\u2026 With XGBoost, the number of iterations and the objective are set. In both cases, these parameters are from the respective machine learning libraries, making it easier for users with some prior knowledge.\n\nNow, importantly, the prediction resource remains in practice independent from the mllib component. And this is of importance to our observation that the lifetime of a machine learning service is mostly spent predicting from data:\n\nThe mllib component is omitted. Sometimes it can be useful however, typically when extracting features from a deep net. In our API jargon, this is akin to unsupervised learning, since the output is a tensor, not a class or a regression objective:\n\nIn summary the core principles learnt from the elaboration of this machine learning API are:\n\nLet\u2019s finish this short review with the last two points.\n\nThe presented API is available over HTTP and RESTful. However, it is independent from the network and can be used from straight C++, here is an example:\n\nThe important construct in the code above is APIData. It is the server\u2019s internal representation of all of the API data structures. It is equivalent to JSON but for some ugly performance reason it is not exactly JSON.\n\nHowever, it reads and converts to JSON. APIData allows to write machine learning services, train and predict with Caffe, XGBoost and others, using still the same API, but instead of JSON, the exact same parameters are fed to APIData objects.\n\nThe point above is of importance, as it means that changes to the API do not affect the inner server. Typically, adding a new parameter to the API, handling it from the machine learning code, does not modify the server itself.\n\nThe second and last point is \u201cfictionality\u201d, and remains my favorite. This is what allows to preempt the changes to the API in a simple manner. It also allows users to intuitively navigate the API without going back the documentation every few minutes.\n\nI love to try it when pondering the future additions to the API. Here is a first one, the (fictionized) ability to schedule lists of jobs to the server, let\u2019s fictionize a new resource named chain that would allow to chain up any list of API operations:\n\nThis basically takes the JSON calls on existing resources as objects and chain them up. And here is a second one, let\u2019s say we would like to use the chain resource to implement ensembling over multiple predictions:\n\nWe could change the chain call for supporting different types of ensembling:\n\nThis machine learning API is still young, and much work remains. But as a fourth attempt, I believe it has hit the right level, at least the one I was looking for.\n\nDespite being young, the API and the server are robust because they are simple. As such they already powers several in production services in both startups and large corporations. And that was quick, or at least it has felt that way.\n\nMuch remains to be done, and a malleable, simple and generic API is a great way to update a product in a lean manner. Better waste time fictionizing new additions while hiking than losing weeks or months of hard work in front of the screens.\n\nOr at least it is how I do prefer my time to be spent.", 
        "title": "A Machine Learning API to rule them all: Caffe, XGBoost and Tensorflow are in a boat\u2026"
    }, 
    {
        "url": "https://medium.com/@Avarex/how-deep-is-your-mind-alphago-af70efd568a6?source=tag_archive---------1----------------", 
        "text": "March 15, 2016, was the happy day for all AI enthusiasts and probably a sad day for the rest of the humans. This day AI-powered AlphaGo program (designed and trained by Google DeepMind lab) defeated Lee Sedol, one of the world\u2019s top players with 9-dan rank, by four games to 1.\n\nMany lances have been broken discussing what does this mean to all of us. Indeed, it is the first time in history when AI stood over the human in such variative and abstract game as Go. However, for engineering guys like us, it is more interesting to see what is under the hood of AlphaGo.\n\nThis post is based on Mastering the Game of Go with Deep Neural Networks and Tree Search from Google Deep Mind.\n\nThe fundamental difference between AlphaGo and previous AI monsters likeIBM Deep Blue is that Deep Blue, as well as most chess-playing computers, used Monte Carlo Tree Search(MCST) algorithm. This is kind of brute-force approach (for sure, with a lot of smart tweaks), recursively computing the optimal move for every game state, that can\u2019t be efficiently used in case of Go.\n\nThe game of Go has long been viewed as the most challenging of classic games for artificial intelligence due to its enormous search space and the difficulty of evaluating board positions and moves.\n\nAs it widely accepted, the number of possible combinations in Go exceeds the number of atoms in the universe. Indeed, the number of combinations in Go can be estimated as1.74\u00d710172 (compare with an estimation of 1080atoms for the observable universe).\n\nDue to this, the search over all possible positions is mostly infeasible. However, AlphaGo has gracefully solved this issue with the smart combination of MCST and complicated Deep Learning.\n\nAlphaGo uses two types of neural networks\u200a\u2014\u200avalue network to evaluate board positions andpolicy networks to select moves.\n\nSince theoretical breads of Go (number of possible moves) is enormous, thepolicy networktries to minimize it by predicting most probable opponent\u2019s move based on previous training results.\n\nValue network gives an estimation of current position strength or in other words, rapidly estimates the probability of game winning for every given position.\n\nAs a result, AlphaGo evaluates thousands of times fewer positions than Deep Blue did by selecting positions more intelligently (using the policy network) and evaluating them more precisely (using the value network).\n\nBoth neural networks are trained by a combination of supervised learning and reinforcement learning from games of self-play. So let\u2019s look how this training is done.\n\nThe architecture of the policy networks is 12 convolutional layers. A finalsoftmax layer outputs a probability distribution over all legal moves. Since the purpose of the network is to predict the next opponent\u2019s move it was trained on the database of the real Go games available online (~150K real games, ~30 million positions). Interesting, that this dataset did not include the games of top players but rather the games of Go-enthusiasts playing onKGS Go Server.\n\nThe network takes 48 inputs as described below\n\nAchieved prediction accuracy is about 57% on a held-out test set, using all input features, and 55.7% using only raw board position and move history as inputs. By the way, these relatively poor results simply mean that humans are the subjects hard to predict.\n\nNext step is reinforcement learning (RL). On this stage, various versions of the policy networks play against each other. Reinforcement learning improves the SL policy network by optimising the final outcome of games of self-play. This adjusts the policy towards the correct goal of winning games, rather than maximizing predictive accuracy.\n\nThis significantly improves the strength of RL-policy network over initial implementation of SL-policy network.\n\nThe final stage of the training pipeline focuses on position evaluation predicting the outcome from the current position of the game. Value network has a similar architecture to the policy networks but outputs a single prediction instead of a probability distribution.\n\nThe naive approach of predicting game outcomes from data consisting of complete games leads to overfitting because successive positions are strongly correlated, differing by just one stone, but the regression target is shared for the entire game. When trained on the real games dataset, the value network memorised the game outcomes rather than generalising to new positions.\n\nTo mitigate this problem, AlphaGo\u2019s team generated a new self-play dataset consisting of 30 million distinct positions, each sampled from a separate game. Each game is played between the RL policy network and itself until the game terminated.\n\nFinally, AlphaGo combines the policy and value networks in an MCTS algorithm.\n\nTo efficiently combine MCTS with deep neural networks, AlphaGo uses an asynchronous multi-threaded search that executes simulations on CPUs, and computes policy and value networks in parallel on GPUs. The final version of AlphaGo used 40 search threads, 48 CPUs, and 8 GPUs.", 
        "title": "How deep is your mind, AlphaGo? \u2013 Avarex Group \u2013"
    }, 
    {
        "url": "https://medium.com/@mikethebbop/separating-natural-language-processing-from-artificial-intelligence-in-chat-bots-and-personal-e174f3347e66?source=tag_archive---------2----------------", 
        "text": "Arguably the computer tech theme known as \u201cChatBots\u201d has moved into first place in the ongoing race for most hyped marketing notion, edging out \u201cmachine learning\u201d and \u201cbig data\u201d. Even the CEOs of Facebook and Microsoft last week directly spoke to this theme during, respectively, the Facebook F8 and Microsoft Build conferences.\n\nBut this position is not to be envied. The general public (which, in 2016 exhibits the special type of thick mental callous created only by efforts to \u201cdumb down\u201d technology, year after year, device after device) is particularly gullible. Sure, the notion of \u201cchatbots\u201d seems to have surfaced. But the theme is actually still entirely opaque, hidden in a mirage of misunderstanding, which, in turn, is the result of what one COULD call misrepresentation on the part of the ISVs behind this new theme.\n\nSo why the discomfort with this phenomenon? The reason is tech marketers crusading this kind of vaporware are 1) doing a disservice to consumers and 2) handicapping their own odds of success once authentic code becomes available.\n\nHere is an example: Ms. Jenna Wortham wrote a story titled \u201cWhat ChatBots Reveal About Our Own Shortcomings\u201d, which was published in the online New York Times Magazine yesterday, Thursday, April 21, 2016. At the end of the first paragraph of her story, Ms. Wortham defines \u201cchatbots\u201d. She posits they are \u201clittle artificial-intelligence programs that work like personalized assistants.\u201d\n\nPerhaps on paper, but not in the real world and, further, not likely to be found in the real world for sometime (meaning years). There are not only real technical barriers to anyone coming to market with a conversational artificial intelligence (AI) program any time soon. There are also serious privacy considerations blocking this development. Even if this software really existed right now, there is zero chance it would be packaged in a freemium business plan.\n\nSo, if we are not looking at anything near a functional example of an AI program when we talk about the \u201cchatbots\u201d Mark Zuckerberg demonstrated during his conference, what are we talking about? Perhaps it is much safer to simply claim this software is an example of intense efforts to master Natural Language Processing (NLP). This claim makes more sense. After all, the Tay.ai debacle was all about an effort to feed the program with the \u201cTwitter fire hose\u201d so it could quickly sample as many variants on semantic communication, wasn\u2019t it?\n\nLet\u2019s keep AI out of the conversation, at least for now. Let\u2019s also inform the public of the likely amount of time it will take for this software to \u201chit the road\u201d and pass by anyone\u2019s home. I wouldn\u2019t hold my breath.", 
        "title": "Separating Natural Language Processing from Artificial Intelligence in ChatBots and Personal\u2026"
    }
]