[
    {
        "url": "https://medium.com/making-sense-of-data/time-series-next-value-prediction-using-regression-over-a-rolling-window-228f0acae363?source=tag_archive---------0----------------", 
        "text": "Given a time series, predicting the next value is a problem that fascinated lot of programmers for a long time. Obviously, a key reason for this attention is stock markets, which promised untold riches if you can crack it. However, except for few (see A rare interview with the mathematician who cracked Wall Street), those riches have proved elusive.\n\nThanks to IoT (Internet of Things), time series analysis is poise to a come back into lime light. IoT let us place ubiquitous sensors everywhere, collect data, and act on that data. IoT devices collect data through time and resulting data are almost always time series data.\n\nFollowing are few use cases for time series prediction.\n\nLet\u2019s explore the techniques available for time series forecasts.\n\nThe first question is that \u201cisn\u2019t it regression?\u201d. It is close, but not the same as regression. In a time series, each value is affected by the values just preceding this value. For example, if there is lot of traffic at 4.55 in a junction, chances are that there will be some traffic at 4.56 as well. This is called autocorrelation. If you are doing regression, you will only consider x(t) while due to auto correlation, x(t-1), x(t-2),\u00a0\u2026 will also affect the outcome. So we can think about time series forecasts as regression that factor in autocorrelation as well.\n\nFor this discussion, let\u2019s consider \u201cIndividual household electric power consumption Data Set\u201d, which is data collected from a one house hold over four years in one minute intervals. Let\u2019s only consider three fields, and data set will look like following.\n\nFirst question is ask is how do we measure success? We do this via a loss function, where we try to minimize the loss function. There are several loss functions, and they are different pros and cons.\n\nIf we are trying to forecast the next value, we have several choices.\n\nThe gold standard for this kind of problems is ARIMA model. The core idea behind ARIMA is to break the time series in to different components such as trend component, seasonality component etc and carefully estimate a model for each component. See Using R for Time Series Analysis for a good overview.\n\nHowever, ARIMA has a unfortunate problem. It needs an expert ( a good statistics degree or a grad student) to calibrate the model parameters. If you want to do multivariate ARIMA, that is to factor in multiple fields, then things get even harder.\n\nHowever, R has a function called auto.arima, which estimates model parameters for you. I tried that out.\n\nYou can find detail discussion on how to do ARIMA from the links given above. I only used 200k from the data set as our focus is mid size data sets. It gave a MAPE of 19.5.\n\nThe second approach is to come up with a list of features that captures the temporal aspects so that the auto correlation information is not lost. For example, Stock market technical analysis uses features built using moving averages. In the simple case, an analyst will track 7 day and 21 day moving averages and take decisions based on cross over points between those values.\n\nFollowing are some feature ideas\n\nCommon trick people use is to apply those features with techniques like Random Forest and Gradient Boosting, that can provide the relative feature importance. We can use that data to keep good features and drop ineffective features.\n\nI will not dwell too much time on the this topic. However, with some hard work, this method have shown to give very good results. For example, most competitions are won using this method (e.g.http://blog.kaggle.com/2016/02/03/rossmann-store-sales-winners-interview-2nd-place-nima-shahbazi /).\n\nDown side, however, is crafting features is a black art. It takes lot of work and experience to craft the features.\n\nNow we got to the interesting part. It seems there is an another method that give pretty good results without lot of hand holding.\n\nIdea is to to predict X(t+1), next value in a time series, we feed not only X(t), but X(t-1), X(t-2) etc to the model. Similar idea has being discussed in Rolling Analysis of Time Series although it is used to solve a different problem.\n\nLet\u2019s look at an example. Let\u2019s say that we need to predict x(t+1) given X(t). Then the source and target variables will look like following.\n\nData set would look like following after transformed with rolling window of three.\n\nThen, we will use above transformed data set with a well known regression algorithm such as linear regression and Random Forest Regression. The expectation is that the regression algorithm will figure out the autocorrelation coefficients from X(t-2) to X(t).\n\nFor example, with the above data set, applying Linear regression on the transformed data set using rolling window of 14 data points provided following results. Here AC_errorRate considers forecast to be correct if it is within 10% of the actual value.\n\nThis is pretty interesting as this beats the auto ARIMA right way ( MAPE 0.19 vs 0.13 with rolling windows).\n\nSo we only tried Linear regression so far. Then I tried out several other methods, and results are given below.\n\nLinear regression still does pretty well, however, it is weak on keeping the error rate within 10%. Deep learning is better on that aspect, however, took some serious tuning. Please note that tests are done with 200k data points as my main focus is on small data sets.\n\nI got the best results from a Neural network with 2 hidden layers of size 20 units in each layer with zero dropout or regularisation, activation function \u201crelu\u201d, and optimizer Adam(lr=0.001) running for 500 epochs. Network is implemented with Keras. While tuning, I found articles [1] and [2] pretty useful.\n\nThen I tried out the same idea with few more datasets.\n\nForecasts are done as univariate time series. That is we only consider time stamps and the value we are forecasting. Any missing value is imputed using padding ( using most recent value). For all tests, we used a window of size 14 for as the rolling window.\n\nFollowing tables shows the results. Here except for Auto.Arima, other methods using a rolling window based data set.\n\nThere is no clear winner. However, rolling window method we discussed coupled with a regression algorithm seem to work pretty well.\n\nWe discussed three methods: ARIMA, Using Features to represent time effects, and Rolling windows to do time series next value forecasts with medium size data sets.\n\nAmong the three, the third method provides good results comparable with auto ARIMA model although it need minimal hand holding by the end user.\n\nHowever, this does not discredit ARIMA, as with expert tuning, it will do much better. At the same time, with hand crafted features methods two and three will also do better.\n\nOne crucial consideration is picking the size of the window for rolling window method. Often we can get a good idea from the domain. User can also do a parameter search on the window size.\n\nFollowing are few things that needs further exploration.\n\nIf you enjoyed this post you might also find following interesting.\n\nAlso check out some of my most read posts and my talks (videos). Talk to me at @srinath_perera or find me.", 
        "title": "Rolling Window Regression: a Simple Approach for Time Series Next value Predictions"
    }, 
    {
        "url": "https://medium.com/@azntaiji/bots-in-the-news-may-31-june-3-2016-a9473ceb9d9d?source=tag_archive---------1----------------", 
        "text": "Happy Friday bot aficionados, we\u2019re back for another roundup. Today, we\u2019ll cover the effects emotions have on bots, inquire why bots have not yet been used to gather datasets and the future of AI in automotive.\n\nSolving the dilemma of bots enslaving the human race may have finally been discovered. \u201cGive them emotion,\u201d suggested Patrick Levy Rosenthal of Emoshape. Rosenthal\u2019s team recently developed a chip that synthesizes 12 human emotions into autonomous robots. These machines are able to express their feelings and assess situations through empathetic eyes, like a human can.\n\nOnce artificially intelligent robots develop personalities, will they become \u201cpeople\u201d from a legal standpoint? Many questions come to play in this new proposition. What do you think?\n\nRead more on the BBC here.\n\nGoogle has been developing their own self-driving car for a few years now. Mercedes is going \u201cBack to the Future\u201d with its autonomous F 015 concept car. Finally, Tesla has introduced its own Autopilot\u200a\u2014\u200aalthough not without its issues.\n\nWhile we truly are living in the future thanks to the amazing technology advancements as of late, self-driving cars still have a long way to go. One challenge in self-driving cars is teaching them to learn how to drive, instead of just following pre-built \u201cif this, then that\u201d instructions from code.\n\nEnter deep learning\u200a\u2014\u200aa popular subset of machine learning that has risen in popularity over the last few years. Read how Nvidia has been applying deep learning technology to cars in this article on International Business Times\n\nThere are currently two primary types of chatbots on the market: shopping assistants and customer service guides. While neither bot has been successful enough to stand alone, other applications have surfaced. Since a chat interface is great for supplying users with quick and snappy replies, Robert May of Talla wonders why these bots aren\u2019t being utilized toward building datasets.\n\nThese chatbots could gradually gather data overtime and complete what would have been abandoned by employees. Learn more about the proposal on Inside Big Data here.", 
        "title": "Bots in the News: May 31 \u2014 June 3, 2016 \u2013 Zach Taiji \u2013"
    }
]