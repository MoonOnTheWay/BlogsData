[
    {
        "url": "https://medium.com/@cdixon/comma-ai-e62eea5fa8d2?source=tag_archive---------0----------------", 
        "text": "I wrote a blog post last month highlighting some of the exciting trends in the computing industry. One trend I discussed is the rapid progress in a branch of artificial intelligence called deep learning. Big tech companies are making significant investments in deep learning, but there are also opportunities for startups:\n\nYou might have seen recent press coverage of a software developer named George Hotz who built his own self-driving car.\n\nI first met George a few months ago, and, like a lot of people who had seen the press coverage, I was skeptical. How could someone build such an advanced system all by himself? After spending time with George, my skepticism turned into enthusiasm. I tested his car, and, along with some of my colleagues and friends with AI expertise, dug into the details of the deep learning system he\u2019d developed.\n\nI came away convinced that George\u2019s system is a textbook example of the \u201cWhatsApp effect\u201d happening to AI.\n\nGeorge is certainly brilliant (he\u2019s a famous hacker for a reason), and he\u2019s no longer alone: he\u2019s now working with a small team of machine learning experts. But he\u2019s also riding a wave of exponentially improving hardware, software, and, most importantly, data. The more his system gets used, the more data it collects, and the smarter it becomes.\n\nToday we are announcing that a16z is leading a $3.1M investment in George\u2019s company, Comma.ai. This investment will help them continue to build their team (they\u2019re hiring), and bring their technology to market. Expect more announcements from Comma in the next few months. We are very excited to support George and his team on this ambitious project.", 
        "title": "Comma.ai \u2013 Chris Dixon \u2013"
    }, 
    {
        "url": "https://medium.com/@awjuliani/recognizing-sounds-a-deep-learning-case-study-1bc37444d44d?source=tag_archive---------1----------------", 
        "text": "In the recent years, image classification has become an increasingly popular machine learning task, utilized in large-scale applications such as Google Photos, and Facebook tagging. With the advent of fast and reliable convolutional neural networks, sets of thousands of images with hundreds of classes can be easily classified with high accuracy (Kirzhevsky & Hinton, 2012). The success of these networks in the image classification domain begs the question of its applicability to other domains where discreet objects exists. We can image one of these domains being audition, where there are discreet sounds that happen over time. This is analogous to image recognition, where discreet objects exist across space. As such, it is an ideal domain to explore.\n\nFor image recognition, there exist large curated databases such as IMAGE-NET and MS-COCO, which contain millions of labeled examples to use for training a network. For audition, there is no such expansive database. As such, I had to develop a novel database from scratch. When thinking about the kinds of sounds which we typically associate as being discreet objects, instruments immediately come to mind. A note played by an individual instrument is something that exists for a fixed period in time, and contains an auditory signature that most individuals can distinguish from one another. This is at least true on the inter-instrument level where the sound of a drum can be told apart from a guitar for example. They were also a fun dataset to obtain, as I was able to spend time working with my musician friends.\n\nSounds were recorded by two friends of mine playing a drum kit and guitar. Within the drum kit, there were eight individual percussive sub-instruments: Snare, Rim, Hi-hat, Ride, Kick, Small-tom, Mid-tom, and Floor-tom. Three notes (3rd fret E string, 7th fret E string, and 9th fret D string) and two chords (A# D F and B B D#) were recorded from the guitar, for a total of 13 sound object classes. Each of these sound objects were captured at 240bpm for two minutes in order to obtain 480 samples each. The musician kept time using a metronome, and the data was checked to ensure each sound occurred within the designated 0.25 second window. The samples were taken using a single channel microphone recording at 44100hz.\n\nData was then randomized and split into 70% training set, 20% validation set, and 10% test set. With raw audio data, there are a number of possible ways to represent this information. For the experiments in the current study, I choose to use the sound-pressure information at each sample time, and a spectrogram of the audio for each sample which represented the data in a frequency space. When represented visually, both methods produce patterns which can be told apart by an ordinary observer. Furthermore, the spectrogram contains frequency and amplitude information over time, something considered essential in most analyses of acoustic information. I had a hunch this would be the better representation, but I wanted to see whether a neural network was capable of learning from the pure sound-pressure information as well. The sound-pressure dataset was initially 11025 dimensional, however this was reduced to 1024 dimensions in order to match the size of the frequency space information, which was represented in a 32 x 32 matrix. By doing so, the models could be compared directly.\n\nI chose to compare three different model architectures of increasing complexity in order to learn what model features may be needed to successfully distinguish between the different sound objects in the dataset. The first model was the simplest neural network possible: a softmax regression, which consisted of a single linear layer. This architecture was chosen for its simplicity, and generally impressive performance on a number of tasks, including image recognition in certain contexts, such as MNIST handwritten digit recognition. The second chosen architecture was a neural network with a single hidden layer. The size of the hidden layer was adjusted as a hyper-parameter, and optimal tuning parameters are presented in the Results section. A neural network was chosen as an intermediate because it allows for the introduction of nonlinearity into the model in a way that could be controlled, while maintaining interpretable weight interpretations.\n\nThe final chosen architecture was a convolutional neural network with a design similar to that of Le-Net (LeCun et al., 1998). It is here that the deep learning comes in, since this model contained multiple hidden layers. Specifically, a model with the following feed-forward architecture (32x32x1) -> CONV (32x32x32) -> POOL (16x16x32) -> CONV (16x16x64) -> POOL (8x8x64) -> FC (4096x1024) -> FC (1024x13) -> OUT was used. A convolutional network was chosen as the third model for its success in image recognition tasks, as well as its similarity to biological models of the nervous system. Convolutional layers in particular capture the property of receptive fields, which are essential to the operation of both the human visual and auditory system (Kandel et al., 2000). All model architectures were implemented using TensorFlow, and dataset training was performed on an NVIDIA Geforce GTX 970. Adam gradient descent was utilized for all training regiments, and the loss was always a softmax loss. Adam was chosen for its improved performance when compared to traditional gradient descent methods (Kingma & Ba, 2014).\n\nAll models were trained for 500 iterations regardless of convergence. I first compared learning accuracy between the two data representation formats. The three model architectures were trained using each kind of representation, and the results show unanimously better accuracy for networks trained using the frequency-space representation than the sound pressure representation. After 500 iterations none of the three architectures were able to achieve greater than 20% accuracy on the latter representation. In contrast both the neural network and convolutional network architectures achieved greater than 80% accuracy using the frequency-space represented data.\n\nWithin the neural network architecture, I examined the effect of accuracy when the size of the hidden layer was adjusted. Given the main result suggesting the much worse accuracy when models were trained using the sound-pressure space data representation, I only conducted this analysis on the frequency-space data. Accuracy scaled roughly linearly with an increase in layer size until 50 units. At this point the training set accuracy for both the 40-unit model and 50-unit model was not appreciably different after 500 iterations. The validation set accuracy was higher for the 50-unit model, and as such it was chosen as the model used in overall comparisons. Given the likely minimal returns using a neural network with more than 50 hidden units, additional models with larger hidden layers were not constructed.\n\nComparing between the model architectures, a clear ordering of training accuracy appears: specifically, we find that the softmax architecture fails to successfully classify the training data, and performs at chance regardless of number of iterations. Next we find that the neural network architecture is comparable to the convolutional architecture in performance using the sound-pressure representation, but worse in the frequency-space representation. After roughly half of the total iterations the convolutional neural network achieves an accuracy of over 97% on the test dataset, making it the most successful of the algorithms compared. For all architecture and data pairings there was no evidence of overfitting, as the validation accuracy tracked training accuracy for the duration of the training regimen.\n\nThese results show that it is indeed possible to use machine learning methods to classify subtle differences in sound objects both between and within instruments. This was made possible through the healthy training set size, and the general internal consistency of the sound objects. When comparing representation type, frequency space was overwhelmingly the more successful choice. This was expected, given that most acoustic analyses are performed in frequency space, and not on the raw sound pressure information. Given the ability for convolutional networks to learn higher representations from raw image pixel information in the image classification domain however, it is disappointing that a convolutional neural network was unable to approximate such a transformation in the auditory domain. Of course, the network would have to learn the equivalent of a Fourier transformation, and the LeNet architecture may not be suited to such a task (but maybe a more complex network is).\n\nWhen comparing the architectures themselves, the convolutional network was the most successful in achieving high classification accuracy. With enough training iterations however, the neural network performed surprisingly well, considering how much simpler a representation it learned. The softmax architecture failed to successfully learn either data representation. This is likely due to the inherent nonlinearity in the problem domain. Within a 0.25 second window, the sound will likely not appear in the same place every time, thus preventing a simple linear model from learning to represent the sounds in an invariant way. The other important difference between the two models was training time. The convolutional network took more than twice as long to train. In a production environment the convolutional network would likely take longer to classify new sounds as well, given its reliance on computationally more expensive convolutional layers. Taking all of this into account however, the high accuracy would likely outweigh any overhead downsides of the larger network.\n\nKrizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (pp. 1097\u20131105).\n\nLeCun, Y., Bottou, L., Bengio, Y., & Haffner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11), 2278\u20132324.", 
        "title": "Recognizing Sounds (A Deep Learning Case Study) \u2013 Arthur Juliani \u2013"
    }, 
    {
        "url": "https://medium.com/@doitarv/becoming-a-learning-city-85d08efa7afb?source=tag_archive---------2----------------", 
        "text": "What would a learning city be like? How can Jaipur become a learning city? What is your idea of such a place? How to know where do we have to direct our efforts for making it a reality?\n\nWhy not bring people from various walks of life, activists, artists, social entrepreneurs, storytellers, makers, etc at one common platform, and discus, take suggestions, and maybe design a route-map while giving everyone just a glimpse of how amazingly vibrant a learning city can be\u00a0!\n\nThe purpose of organizing the 1st Jaipur Learning City Unconference was the same. And the way this whole event turned out, we can simply say, the foundation has been a solid one.\n\nWhat an unconference is and what it is not\u00a0! Better to know, is it not\u00a0?\n\nWhat Unconference is not\u200a\u2014\u200aIt is not about PowerPoint Presentations and less exciting Keynote Speeches. It is not about those few people who are considered as superior knowledgeable beings for a day. It is not about the procedural drama, suits and ties, and everything that makes us little uncomfortable.\n\nMoving forward\u2026. the 1st Jaipur Learning City Unconference which took place on 12th March was an example of beautifully executed Unconference -\n\nJust when you pass the main entrance, and enter the lawn, what do you see? You see a bunch of people in a large circle, and doing things which you don\u2019t get to see every other morning\u00a0! Then you join them, and soon you are just as weird (or awesome) as them.\n\nUnconference started with games. What a fun way to urge people to show their true colors and begin a charismatic event\u00a0! More and more people joined. Laughs became louder, followed by more games with weird names and even weirder way they were being played.\n\nAnyone could give a talk and people were eager to listen [ YOU LEAD TALKS]; some folks came in pajamas; everyone was at ease, they were smiling, participating, sharing, laughing, and learning; there were dance workshops, laughing workshops, art and craft workshops consisting of bottle decoration, organic color making, etc\n\nThere were food stalls serving juices, homemade muffins and chocolates, to organizers, visitors, and participants. There were stalls of arts and crafts items ranging from dream catchers to paper earrings.\n\nOne of the things that highlighted the essence of the event was the \u201cDariya Dil ki Dukan \u201d, where one could buy anything present in the stall with a smile. Yes, a smile. Smile, not money, is the currency of exchange here\u00a0!\n\nA room was dedicated to DIY Film Festival showing short movies throughout the day.\n\nFor lunch, everyone sat on the ground and ate delicious Organic Food.\n\nShikshantar Andolan, Startup Oasis, and Pravah Initiative were the main organizers, apart from many other individuals who ran around gathering everything required, promoted the event on social networks, and gave life to a delicate idea in a fanciest possible way. Samarth School was the host of the Unconference.\n\nThis event did something special which most people might still not have realized. It sowed the seeds of an Idea in our minds. An idea so powerful that it won\u2019t let us settle till we make it a reality. The idea of a seeing Jaipur as a learning city. The idea of accepting diversity in human beings. The idea of a community that celebrates learning. This unconference helped us think in this direction\u00a0! However crazy it might sound, but people who came for the unconference can now imagine of such a place.\n\nThis Unconference was just the beginning. Beginning of Jaipur becoming a Learning City.", 
        "title": "WHAT WOULD A LEARNING CITY BE LIKE \u2013 Aarav \u2013"
    }, 
    {
        "url": "https://medium.com/@iambee/too-much-notifications-there-s-a-solution-for-that-cb5aa2e1e3ab?source=tag_archive---------3----------------", 
        "text": "Dear Apple, Google and Facebook. A little food for thoughts.\n\nMy last post about distraction focused on the steps one could take to reduce the number of information that passively reached an eyeball. Whether it originated from a Facebook feed or a Twitter timeline, from a Whatsapp notification or an Instagram event, whether it was just a missed call or just yet-another-text message, your devices keep on turning themselves on and off distracting you from your present task. There's no filter around what you would like to receive and when and how. The information is chronological and turned-on all the time.\n\nLet's be honest, it's tiring and more important than that, 75% of those notifications are just a waste of your precious time.\n\nSo how do we solve that today? Turning off notifications is one of the first steps to reduce the busy notification line-up. Deleting the irrelevant apps once and for all from our smartphones and keeping the ones that are most relevant and important is a bit more radical. In both cases, it's an all-in all-out situation, no middle ground here. Some would also install a plugin to help them block certain websites when they are trying to focus, a remix of parental control right? We could enable \"Do not disturb\" and put our smartphone on \"Silence\" all day long and check out the notifications we received in the mean time\u00a0\u2026 oh well, it's a vicious circle.\n\nIt shouldn't be THAT difficult really. Apple and Google are maybe (should be) working on it and it's based on AI! Siri is funny, it can beatbox, it can start your music, it can, like Google's \"OK Google\", respond to your voice. But we're just at the premises of this technology.\n\nWhat AI lacks today is context. Once these softwares integrate context and deep-learning about your habits and routine, then will we be able to control how, when and what we want to interact with, with our devices and notifications. That doesn't mean you cannot check back your feeds whenever you want, it just helps simplify your life and deliver information to you in a smarter way when and if you need it depending on how busy your day is. Isn't easing up your life, the whole promise of smartwatches, the IoT dream and the everlasting quest of a more connected world?\n\nImagine the following. You can choose to wake up at a time of your choosing with a fixed alarm or command your smart light to wake you up gently by switching the lights on. You could even ask your device to study your sleeping rhythm and wake you up at the best time by cross-referencing your agenda and the transit time to get to your office on time.\n\nYour device would know when to switch itself from a home, transit, restaurant or office mode. It would know how to save energy by turning on and off Wi-Fi or Bluetooth connectivity depending on its location. Your Apple watch could sync using WiFi instead of Bluetooth. Your music would switch automatically from your headphones to your home Wi-Fi as soon as you step home.\n\nYour device would know when you are in transit and would cross-reference your agenda knowing if you're going to work, to meet a client, or if you're on a day off and going just to have lunch with a friend. It would remind you to dress accordingly and take essential water supplies on your next run or bike ride depending on your training plan and the weather forecast.\n\nIf you're headed to the airport, it would scan the area around you for taxis around your block and compute the best time to leave depending on the transit time.\n\nYour kitchen experience could also use some AI. It would suggest to you your next meals depending on your habits, the season and the availability of fresh vegetables and fruits. Your groceries list would either be directly shipped to you or you could use a walk to the right shop with your list and buy what you need.\n\nThese are just a few thoughts of what would happen if your life met AI and deep learning.\n\nMany IoT devices offer these services on a proprietary basis. Some open up their APIs to help cross-integration, others don't. Privacy is also a rising concern, will we be willing to enable deep-learning on our devices when the feature will be available in order to have a consolidated digital vision of our lives? Some would argue on the rise of the machines but on the other hand we see the benefits of easing up our lives with the help of AI and robots.\n\nSo Google, Facebook, Apple if you are reading this post, let's bring out some context to AI and help consolidate all the isolated information available today through the multiplicity of gadgets and devices. It's time we make better and more sustainable choices instead of a prolific, non constructive one.\n\nKeep it simple, they said.", 
        "title": "From distraction to efficiency \u2013 Bee \u2013"
    }
]