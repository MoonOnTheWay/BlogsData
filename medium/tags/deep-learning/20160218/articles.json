[
    {
        "url": "https://medium.com/@cbeta9702_1/project-based-school-%ED%95%99%EB%B6%80%ED%98%95%EC%9D%B4-%EB%B3%B8-ebs-%EB%8B%A4%ED%81%90-%ED%94%84%EB%9D%BC%EC%9E%84-%EA%B3%B5%EB%B6%80%EC%9D%98-%EC%9E%AC%EA%B5%AC%EC%84%B1-1-c28d79d0f1ef?source=tag_archive---------0----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Project-Based School \ud559\ubd80\ud615\uc774 \ubcf8 \u2018EBS \ub2e4\ud050 \ud504\ub77c\uc784-\uacf5\ubd80\uc758 \uc7ac\uad6c\uc131\u2019(1)"
    }, 
    {
        "url": "https://medium.com/pankajmathur/what-is-anaconda-and-why-should-i-bother-about-it-4744915bf3e6?source=tag_archive---------1----------------", 
        "text": "In this Article we will be installing Anaconda, managing python packages, creating individual conda environments and sharing them via conda YAML file. We will be covering most of these topics in following order:\n\nFirst of All, What is Anaconda & Why Should I bother about it?\n\nYou probably already have Python installed and will be wondering why you need this at all. Firstly, since Anaconda comes with a bunch of data science packages, you\u2019ll be all set to start working with data. Secondly, using conda to manage your packages and environments will reduce future issues dealing with the various libraries you\u2019ll be using.\n\nIn most of the real world Data Science projects, conda based package and environments are widely used and I personally preferred conda based package installation and maintenance of project then installing and maintaining directly PIP based packages.\n\nSo, Why Anaconda?\n\nAnaconda is a distribution of packages built for data science. It comes with conda, a package, and environment manager. We usually used conda to create environments for isolating our projects that use different versions of Python and/or different version of packages. We also use it to install, uninstall, and update packages in our project environments. When you download Anaconda first time it comes with conda, Python, and over 150 scientific packages and their dependencies. Anaconda is a fairly large download (~500 MB) because it comes with the most common data science packages in Python, for people who are conservative about disk space, there is also Miniconda, a smaller distribution that includes only conda and Python. You can still install any of the available packages with conda, that comes by default with the standard version. Conda is a program we will be using exclusively from the command line, so if you aren\u2019t comfortable using it, check out these learn by doing videos on Lynda.com command prompt tutorial for Windows and Linux Command Line Basics for Mac OSX/Linux\n\nInstalling Anaconda\n\nAnaconda is available for Windows, Mac OS X, and Linux. You can find the installers and installation instructions at https://www.continuum.io/downloads If you already have Python installed on your computer, this won\u2019t break anything. Instead, the default Python used by your scripts and programs will be the one that comes with Anaconda. Choose the Python 3.5 version, you can install Python 2 versions later. Also, choose the 64-bit installer if you have a 64-bit operating system, otherwise go with the 32-bit installer. Go ahead and choose the appropriate version, then install it. Continue on afterward!\n\nAfter installation, you\u2019re automatically in the default conda environment with all packages installed which you can see below. You can check out your own install by entering conda list into your terminal.\n\nCreating Environments via Conda\n\nconda can be used to create environments to isolate your projects. To create an environment, use\n\nin your terminal. Here -n env_name sets the name of your environment (-n for the name) and list of packages is the list of packages you want to be installed in the environment. For example, to create an environment named my_env and install numpy in it, type\n\nWhen creating an environment, you can specify which version of Python to install in the environment. This is useful when you\u2019re working with code in both Python 2.x and Python 3.x. To create an environment with a specific Python version, do something like\n\nfor Python 3.3.\n\nOnce you have an environment created, use\n\nto enter it on OSX/Linux. On Windows, use\n\nWhen you\u2019re in the environment, you\u2019ll see the environment name in the terminal prompt. Something like\n\nThe environment has only a few packages installed by default, plus the ones you installed when creating it. You can check this out with command\n\nInstalling packages in the environment is the same as we saw before:\n\nOnly this time, the specific packages you install will only be available when you\u2019re in the environment. To leave the environment, type\n\nOn OSX/Linux:\n\nManaging Packages via Conda\n\nOnce you have Anaconda installed, managing packages are fairly straightforward. To install a package, type\n\nin your terminal. For example, to install numpy, type\n\nSaving & Loading Environments\n\nYou can install multiple packages at the same time. Something like\n\nwill install all those packages simultaneously. It\u2019s also possible to specify which version of a package you want by adding the version number such as\n\nConda also automatically installs dependencies for you. For example, scipy package depends on numpy, as it uses and requires numpy. So, If you install just scipy\n\nConda will also install numpy if it isn\u2019t already installed.\n\nMost of the commands are pretty intuitive. To uninstall, use\n\nIf you want to update all packages in an environment, which is often useful, use\n\nAnd finally, to list installed packages, it\u2019s again\n\nIf you don\u2019t know the exact name of the package you\u2019re looking for, you can try searching with\n\nFor example, if you want to install a package that read and write excel files, but if you are not sure of the exact package name. you can try searching for excel keyword\n\nIt returns a list of the excel writer packages available with the appropriate package name, which I personally recommend, XlsxWriter.\n\nSaving, Listing, Sharing & Removing Environments\n\n\n\nSaving and Sharing Environments:\n\nA really useful feature is sharing environments so others can install all the packages used in your code, with the correct versions. You can save the packages to a YAML file with\n\nThe first part writes out all the packages in the environment, including the Python version.\n\nAbove you can see the name of the environment and all the dependencies (along with versions) are listed. The second part of the export command\n\nwrites the exported text to a YAML file environment.yaml. This file can now be shared and others will be able to create the same environment you used for the project.\n\nTo create an environment from an environment file use\n\nThis will create a new environment with the same name listed in environment.yaml.\n\n\n\nListing Environments:\n\nIf you forget what your environments are named (happens to me sometimes), use\n\nto list out all the environments you\u2019ve created. You should see a list of environments, there will be an asterisk next to the environment you\u2019re currently in. The default environment, the environment used when you aren\u2019t in one, is called root.\n\nRemoving Environments:\n\nIf there are environments you don\u2019t use anymore\n\nwill remove the specified environment (here, named env_name).\n\nBest Practices\n\n\n\nFirst good practice:\n\nWhile using anaconda is having 2 separate environments one for Python 2 and other for Python 3.\n\nfor example, you can use\n\nto create two separate environments, py2env and py3env. Now you can have a general use environment for each Python version. In each of these 2 separate python version environments, you should install most of the standard data science packages numpy, scipy, pandas, matplotlib, etc.\n\nSecond best practice:\n\nWhen sharing your code on GitHub, is to make an environment file and include it in the repository. This will make it easier for people to install all the dependencies for your code. Ideally, you should also include a pip requirements.txt file, for people who are not using conda, by using\n\nThat\u2019s all for a quick start. Key is to keep practicing using above mentioned commands.\n\nHopefully, this article will help you to cut down your time spent during python package management in half, and help you to jumpstart in using anaconda virtual environments and package management for your day to day python project environment.\n\nPlease do let me know your thoughts, questions under the comments section. I would really appreciate getting some feedback on this article & ideas to improve it.\n\nIn the meanwhile, Happy Thinking\u2026", 
        "title": "What is Anaconda and Why should I bother about it? \u2013 Pankaj Mathur \u2013"
    }, 
    {
        "url": "https://medium.com/@WaarSpiegel/can-artificial-intelligence-make-us-laugh-5c3847e3f4af?source=tag_archive---------2----------------", 
        "text": "2016 could be the year that redefines the boundaries of artificial intelligence - Rumours from the bay area indicate that Silicon Valley giants such as Google, Twitter, Facebook and Amazon are joining forces and forming the Coalition for Unified Learning Technologies. The team behind this exciting new initiative has set itself an ambitious goal - Taking artificial intelligence to the next level.\n\nCurrently, artificial intelligence algorithms recognize complicated patterns by going through massive amounts of example data. This technique, known as deep learning, is the engine behind face recognition, product recommendations, analyzing purchasing habits and more. \u201cWe\u2019ve had great success with deep earning, but at Silicon Valley that\u2019s never enough\u201d, Says Pierre La-Devie, the Executive Senior Lead Innovator of the coalition. \u201cRecommending a product based on your personal data alone is so 2015. We are now working on the next wave of world-transforming ideas - Algorithms that understand the complexity of human emotions, and recommend the best purchasing opportunity based on your personality and current mood\u201d.\n\nThe first challenge taken by Pierre and his team is to develop an algorithm that is self-aware. \u201cAt the moment, self-awareness is not one of our core competencies\u201d, he admits, but his team works day and night (and weekends), developing new artificial intelligence techniques. Since \u201cEmotions are very different than rational thinking\u201d, the team had to apply some unconventional methods. \u201cFirst of all, 50% of our team are women. We also developed a new technology, Brains As Low-Level Systems, which mimics brain activity more accurately\u201d. So far, the results seem promising, \u201cWe can detect anxiety and depression with fairly high certainty\u201d, yet there is still a long way to go\u200a\u2014\u200a\u201cSarcasm, humour and empathy are still elusive to us\u201d.\n\nWhen we ask Pierre about future possibilities, he excitingly replies \u201cImagine a world where you can meet the love of your life, without even leaving your office chair. No more swiping through pictures on Tinder, or trying to impress girls at the bar with your Googler swag. Very soon technology will be able to match you with someone based on your self-esteem, circumstances and opinions\u201d. He looks at his phone for a second, verifying that there are no new messages, and concludes \u201cArguments, disagreements and personal differences will all be mitigated by unified learning technologies. This is my view of a better world for 2016\u201d.", 
        "title": "Can Artificial Intelligence make us laugh? \u2013 Waar Spiegel \u2013"
    }, 
    {
        "url": "https://medium.com/development-of-things/yes-i-understand-the-stakes-are-really-high-but-that-is-only-because-we-look-for-minimal-failure-d4d4ab544516?source=tag_archive---------3----------------", 
        "text": "Yes, I understand the stakes are really high, but that is only because we look for minimal failure systems which can replace the human diagnostician. While this is a nice future goal, today we could make use of existing tech to give a helping hand to doctors overwhelmed with data and new science that pops up every week.\n\nIn my view medicine is a social concern, it is probably the best bet of the idea of greater good. Deep learning systems fed constantly with journal publication\u2019s data, constant feeds of correct human made diagnostics, along with symptoms and working medicine, even some non patented pharma data coming out of clinical trials and lab testing, such deep learning systems can offer suggestions, exhaustive suggestions, so that medics don\u2019t skip over possibilities because of normal human memory retrieval and the limits of a biological brain, busy with not forgetting to buy milk three hours later.\n\nYou are probably right in the sense that legal stakes are very, very high, and we keep preferring to cover the so hard to prove \u201cmalpractice\u201d, than to risk wrong machine suggestions where the liability is direct and A.I. data is traceable and easy to use in court. Basically we wait on infallible systems because money hungry lawyers and a legal and ethical system that keeps lagging behind technological progress.\n\nThat\u2019s my problem with Watson. And all the systems that attempt to get marketed at physicians only. The successful products will be \u201chealth platforms\u201d, end to end software that binds three things: patients, medical professionals and medical academia, and all this binding is done at data level:\n\nA health platform manages pathological history, medical history, physiological history, treatment history but also as many other details as the patients offer permission for the platform to gather.\n\nUnfortunately for now we\u2019ll see iPad apps like Watson Paths:\n\nAnd these apps are built like the classical software was built: self contained products and it, apparently at least, has the same issue articulated as:\n\nwhich is not a problem by itself, but lacking patient data and patient history the suggested paths are too wide in scope to be directly useful.", 
        "title": "Yes, I understand the stakes are really high, but that is only because we look for minimal failure\u2026"
    }
]