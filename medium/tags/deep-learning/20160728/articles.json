[
    {
        "url": "https://medium.com/artists-and-machine-intelligence/a-journey-through-multiple-dimensions-and-transformations-in-space-the-final-frontier-d8435d81ca51?source=tag_archive---------0----------------", 
        "text": "These are eigenfaces, a very simple demonstration of this. Going into detail as to how they\u2019re generated is beyond the scope of my talk right now, but I\u2019ll spend just a couple of minutes to give a very rough overview for the technically curious.\n\nWhat does that mean?\n\n(This bit can be skipped if a. you\u2019re already comfortable with PCA or b. you don\u2019t really care. If this section isn\u2019t clear, don\u2019t worry about it as afterwards I\u2019ll summarise the conceptual significance, which is the bit that really matters).\n\nImagine we have a bunch of 3D data, and we plot them in 3D space, we get something that resembles a point cloud.\n\nNow this point cloud might be perfectly spherical, but it\u2019s more likely to be kind of elongated and blobby like in this image. The point cloud in this image is quite elliptical and is oriented in a particular direction. The directions in which the data point cloud is elongated in are called the principle components (or eigenvectors). And Principle Component Analysis (PCA) is a method of finding these directions in which the data is most \u2018elongated\u2019. Then we can define these directions as new axes, and project our data into that new axis system. I.e. transform it.\n\nBut here\u2019s an important detail: PCA finds the directions of elongations (eigenvectors), and how big each \u2018elongation\u2019 is (eigenvalue) along that direction. We can then choose to omit any directions (eigenvectors) where the elongation (eigenvalue) isn\u2019t that significant i.e. If it\u2019s quite flat in a particular directions.\n\nE.g. Imagine we plot a bunch of data in 3D space, and it turns out to be (almost) flat like a piece of cardboard, but tilted at an angle. If we can calculate that angle, we can transform our coordinate system, and reduce it to 2D. That\u2019s exactly what we can do with PCA: reduce dimensions by transforming the data to a new axis system, one which potentially represents the data more optimally.\n\nAnd as always, this works in any number of dimensions. If we have 100D data in 100D space, it might also have elongations. In fact, because it\u2019s in high dimensions, it will probably have many elongations in many different directions and dimensions. PCA will find all of these elongations. In fact PCA will return the same number of elongations as there are original dimensions. I.e. For a 100D space, PCA will return a new set of 100 directions (axes). But these 100 axes will be rotated to fit our data more optimally. Most importantly, because we know how much elongation there is on each direction (the eigenvalue corresponding to the eigenvector) we can sort the axes by elongation amount. The first axis will have the most elongation (pointing in the direction of most variance), second axis will have the second most elongation (pointing in the direction of second most variance), etc. and the last (100th) axis will have the least amount of elongation. This means that we can choose an arbitrary cutoff point (for amount of variance), and just ignore the axes (dimensions) beyond that cut off point. The same way that we can transform 3D data that is \u2018flat\u2019 into 2D (by finding the most \u2018important\u2019 set of 2D axes), we can transform 100D data into say 20D data\u200a\u2014\u200aby finding the most \u2018important\u2019 set of 20D axes.\n\nIf this wasn\u2019t very clear, it doesn\u2019t matter. Understanding what exactly PCA does isn\u2019t the purpose of my talk. This bit was only for those who were interested and might have already seen this before. The important thing is to understand the implications of this which I will explain next.\n\nLet\u2019s take our dataset of face images (which we assume to be 32 x 32 pixels, so that it ties in with our previous discussion). Remember that every single 32 x 32 BW image is a single point in 1024D pixel space. If we plot all of our face images, we get a point cloud in 1024D. We can run PCA on this 1024D dataset and choose an arbitary number of dimensions (i.e. axes) to reduce it to.\n\nE.g. If we were to choose the top 24 dimensions, we might get something like this\n\nEach one of these \u2018face\u2019 images, is an eigenvector of this dataset, i.e. the \u2018directions\u2019 in which our dataset point cloud is most elongated in 1024D. These are the new axes which represent our data set more optimally, in a more compact manner.\n\nWhat does it even mean for \u2018an image to be an axis\u2019? Well, remember that in our 1024D space each point is an image. So each of these images here, is also a point in 1024D space. It\u2019s a vector. And eigenface image 1 is our new axis 1, eigenface image 2 is our new axis 2, eigenface image 3 is our new axis 3\u2026 eigenface image 24 is our new axis 24 etc.\n\nAnd this is conceptually really significant. Because first we discussed a 1024D pixel space. In that space, each axis (i.e. feature) corresponds to a pixel in a 32 x 32 grid\u200a\u2014\u200ai.e. the features of the space are pixels.\n\nNow (after PCA / Eigenfaces) we have a new coordinate system (i.e. new axes, which are somehow rotated in space) to fit our particular dataset better. These new axes constitute a 24D latent space\u200a\u2014\u200aI call it latent space because it\u2019s features (i.e. axes) are not directly observable. And these latent features are how much an input image resembles the eigenfaces. I.e. these are what the axes of this new latent space represent.\n\nI\u2019ll give an example to try and make this a bit clearer.\n\nThis image of Hedy Lamarr has a pixel representation, a 1024D vector of pixel values. How do we transform it from 1024D pixel space to 24D latent space? How do we find its representation in this latent space? i.e. the 24D vector of latent features? How do we encode it?\n\nWith this particular latent space (i.e. the one we constructed via eigenfaces and PCA), it\u2019s very simple.\n\nWe take the image (cropped and resized to 32 x 32, so it\u2019s a 1024D vector) and dot product it with the first eigenface (which is also a 1024D vector), that will give us a number, how much the image \u2018resembles\u2019 the first eigenface. That\u2019s the value of our first latent feature. i.e. if we were to plot this 24D representation as a point in 24D latent space, that\u2019s the distance we would go along the first of the 24 axes. We then dot product the image with the second eigenface, that number will give us the second latent feature, i.e. the distance to go along the second axes. Etc. All the way to the 24th eigenface and the last latent feature (i.e. axis).\n\nIf you\u2019re not familiar with dot products etc and this bit wasn\u2019t clear, it doesn\u2019t matter. The most crucial thing here is:\n\nIt might turn out that this image of Lamarr is 24% 1st eigenface, 12% 2nd eigenface, -31% 3rd eigenface,\u00a0\u2026, 17% 24th eigenface etc. Or in a more compact syntax: [0.24, 0.12, -0.31,\u00a0\u2026 0.17]. That\u2019s only 24 numbers! We would call each of these 24 numbers, the latent features of this image (in this particular latent space), and the vector (i.e. list) of 24 numbers is a representation of this image in (this particular) latent space.\n\nIf we have a representation of an image in this 24D latent space, i.e. a vector of 24 latent features, how can we reconstruct the original image? I.e. transform from 24D latent space back to 1024D pixel space? I.e. decode it?\n\nRemember that the latent features in this space are simply how much an image resembles each eigenface. So we simply multiply each of the 24x eigenfaces with the value of the corresponding latent feature, and add them up. I.e. for each pixel, we do:\n\nThat\u2019s it. The resulting 1024D vector is the pixel representation.\n\nThis is a huge compression of information.\n\nIf I want to send you a picture of a face, I don\u2019t need to send you all of the pixels of the image, i.e. a pixel representation, i.e. a vector of 1024 pixel features. I can just transform my image from 1024D pixel space into 24D latent space. I encode it. And then I can send you just the 24 numbers, the latent representation, a vector of 24 latent features. Of course you need a way of decoding those 24 numbers, transforming from latent space back to pixel space. If you already have these eigenfaces handy, then you can easily transform back to pixel space as I described before.\n\nBut if you don\u2019t have the eigenfaces, then I\u2019d need to send them to you first, and that would be very inefficient for just one picture.\n\nBut if you don\u2019t have the eigenfaces, and I want to send you a million 32 x 32 face images, sending pixel representations for all images would take up 1GB (1,000,000 images * 1,024 pixels per image, assuming 1 byte per pixel). Alternatively I could send you the pixel representations of the eigenfaces first which would be 24KB (24 images * 1,024 pixels per image). Then I could send the latent representations for each of the million faces which would be roughly 24MB (1,000,000 images * 24 latent features per image, assuming 1 byte per latent feature). A massive compression.\n\nThere is a catch associated with this. This is a lossy compression. Very lossy. If we take an image (e.g. Hedy Lamarr) and encode it, i.e. transform from 1024D pixel space to 24D latent space, we will end up with a 24D vector of latent features, a representation in 24D latent space. If we decode that, i.e. transform it from latent space back into 1024D pixel space, we will end up with an image again. We could call this a reconstructed image. But the reconstructed image will not necessarily be identical to the original input image (e.g. Lamarr). The \u2018difference\u2019 between the original input image and the reconstructed image is the error (of this encoding-decoding). There are many different ways of measuring this \u2018difference\u2019, and it depends on the domain. For an image like this, we could simply take the difference between all of the pixels and add them up (L1 Norm) or we could measure the euclidean distance in 1024D space (L2 Norm) etc. (For a more complicated, probabilistic model it\u2019s more common to look at the \u2018difference\u2019 between probability distributions, e.g. using something like KL divergence).\n\nThere are two main reasons for this error:\n\nLet\u2019s also remember that this 24D representation, the vector of 24 latent features, are coordinates in a 24D space. So each 32 x 32 pixel face image can be thought of as a point in 24D latent space (in addition to being a point in 1024D pixel space). Now what happens when we perform geometric operations in this 24D space? If we average two points (i.e. two latent representations of face images)?\n\nThe method I just described was using PCA and eigenfaces. PCA is a method dating back to 1901, and was applied to faces in 1987. It\u2019s quite old, really not state of the art at all. Also PCA is a linear dimensionality reduction technique. I.e. the new (latent) features are linear combinations of the original features (e.g. pixels).\n\nwhere all Kx_y are constants. PCA\u2019s job is to find those constants.\n\nSo PCA won\u2019t find complicated, intricate manifolds (i.e. crumpled pieces of paper, or intricate mountain ridges), or even slightly curved manifolds (like the surface of bowl). It will only find completely \u2018flat\u2019 manifolds (like a flat piece of cardboard). So even though finding midpoints of multiple image representations in this new latent space will be more interesting than doing it in the 1024D space, the results will still be linear combinations and not terribly exciting. However\u2026\n\nI only showed and spent so much time on PCA / Eigenfaces because they\u2019re relatively easier to visualise and understand what\u2019s going on under the hood (compared to the \u2018black-box\u2019 of neural networks).\n\nThere are many other, totally different methods which essentially do what we want here, which is to\u2026\n\nIn the next sections I\u201dll talk about a few other methods which are considerably more complicated under the hood, so I won\u2019t go into so much detail on how they work. I\u2019ll focus mainly on the end result and how they work on a conceptual level.\n\nBut first I want to underline a few things:", 
        "title": "A journey through multiple dimensions and transformations in SPACE"
    }, 
    {
        "url": "https://medium.com/@bastienpetit/trying-tensorflows-image-recognition-script-811f37e3cbde?source=tag_archive---------1----------------", 
        "text": "I followed this guide to install TensorFlow on my machine and try its image classification tool. I\u2019m sharing some of the results here.\n\nFor each image, TensorFlow returns a list of 5 keywords along with a score indicating a level of confidence. With this gorilla picture, the result is very clear.\n\nSome mistakes are quite understandable, like when it hesitates between an ox and an ibex to describe this yak.\n\nIt does fail sometimes, like for this violet-backed starling which cannot be identified. In these cases, TensorFlow only returns labels with a low confidence score.\n\nScores tend to get lower when the picture contains multiple subjects, like in this picture from Noah\u2019s Ark Animal Sanctuary.\n\nI tried to isolate the subjects to understand if TensorFlow was struggling with one of them. Submitted individually, it identifies the bear without any doubt. I suspect that it\u2019s not listed with the original image because of the 5 keywords limit.\n\nTensorFlow is sometimes surprisingly good at identifying something from a small part of it. This picture is one of the best examples that I could find:\n\nIf we crop the image to exclude most of the animal, TensorFlow remains highly confident about the fact that there is an armadillo in the picture.", 
        "title": "Trying TensorFlow\u2019s image recognition script \u2013 Bastien Petit \u2013"
    }, 
    {
        "url": "https://medium.com/making-sense/analytics-automation-6407aec95e91?source=tag_archive---------2----------------", 
        "text": "Today companies face many challenges with analytics: time to insights, overwhelmed with data, and complex crowded tools marketplace. Automation is needed in Enterprise Analytics to drive the next phase of value creation from data. Techniques and architectures used in AI research mainly Deep Learning can help organizations achieve Analytics Automation by speeding up the cumbersome process of feature engineering. Analytics Automation is implementing systems, which find the signals from the massive troves of data and provide predictions and prescriptions to transform an enterprise.\n\nThere are several key themes that put us on the door step of Analytics Automation: continued exponential growth in high dimensional data, challenges in the current data science process & inefficiency of existing data mining & machine learning algorithms.\n\nEvery 60 seconds, 150 hours of video is uploaded on YouTube! According to the networking giant Cisco, 85% of Internet traffic is pixel based\u200a\u2014\u200aImages and Videos, add to that data generated from 3D environments measured in Voxels. New data coming as raw material for data scientists is increasingly high dimensional. The combination of growing interconnectedness around the world and explosive growth in mobile devices and sensors capturing not only pixel data but also data from biological, chemical & physical objects has brought us to the era of highly dimensional data; the much hyped Internet of Things (IoT). According to research firm Gartner, these networked sensors are estimated to double the human population and reach 13.2 billion by year 2020. Simple way of thinking about high dimensionality is a single observation having a large number of attributes. Combine the number of devices emitting observations of the world around them with the high dimensional nature of the observations and it quickly becomes a major challenge for Data Scientists.\n\nSpeaking of Data Scientists, let\u2019s take a step back and review the current data science process, in somewhat loose terms, it typically starts with transforming raw data for feature engineering, followed by testing of newly derived feature(s) using various algorithms to improve baseline model performance and finally collaborating with data engineering for update to production model using new feature(s). The problem with this process when working with high dimensional data is the feature engineering becomes difficult and model complexity increases. Using current machine learning & data mining techniques, Data Scientists will lose sleep worrying about their models drifting astray due to variability from new data. It becomes difficult to separate the signal that has predictive power from the noise. If lucky the exercise is not futile and it is possible to generate good models, feature engineering surely is time consuming for the highly sought after Data Scientist.\n\nGiven this challenging environment of analytics in the enterprise, automation is needed to offload some of the burden back to the machines. In order to do so, we need to understand how humans are able to analyze highly dimensional data at high speeds, learn new concepts, detect patterns and make decisions. To mimic this human like behavior in machines, computer scientists set path towards solving the quest of Artificial Intelligence many decades ago.\n\nAI researchers looked to build a system that resembles or perhaps even a replica of the human neural network, giving birth to the concept of Artificial Neural Networks. Conceptually there were many break throughs in the first few decades but no viable practical applications resulted from the research, driving the movement into hibernation: popularly known as AI Winter. Well Winter may last forever in some fictional set of books and a tv series but it is safe to say AI Winter is over! There have been three key developments that have ushered this spring of Artificial Intelligence: The curse of high dimensionality is also a blessing due to the abundance of data to feed these algorithms, computing economies of scale and public-private collaboration in practical applications (loosely speaking the open source movement). All three developments can be credited as dividends of the Internet era, which has led to rise of large technology companies, increased sharing of knowledge among researchers and lowered computing costs.\n\nTo drive Analytics Automation in this AI spring, the area known as Deep Learning has the nearest term application. Deep Learning is the general umbrella under which most of the current AI research is conducted. In order to understand Deep Learning, let\u2019s take a quick detour to understand a simple neural network. In a simple neural network there is a stimulus, processing of the stimulus and the resulting output reaction. Some simple examples of sensory stimulus are tickle generating a laughter response, touching hot cup of coffee generating pain response. The process of converting a stimulus to an output can be called a layer in a neural network, perhaps it\u2019s as simple of one layer of stimulus, processing and response or it could be many layers filtering a complex stimulus to a simplified understandable response. Deep Learning, simply speaking, is applying multiple layers of processing on input data (stimulus) to understand the meaning of the input data. It is a way to model the physical world and extract meaning from the complex high dimensional physical world inputs. Back from the detour and revisiting the why, the need, of Analytics Automation, you can see that applying deep learning techniques on the highly dimensional data to reduce or eliminate feature engineering can be a significant upgrade to the current data science process. Deep Learning facilitates a movement towards feature learning for model development.\n\nIn order to accomplish Analytics Automation via Deep Learning, there are three key requirements for an organization:\n\nLet\u2019s start from the last one first, in what can be described as completely opposite of any game theory or competitive economic theory, there is significant collaboration and perhaps a friendly rivalry among the leading Deep Learning researchers many of who are employed by corporations who most would see as competitors. This collaborative environment has resulted in open availability of software libraries, guides and tutorials that lower the barriers to entry into Deep Learning. Some notable libraries and packages are Google\u2019s TensorFlow, Facebook\u2019s Torch, Microsoft CNTK, Amazon\u2019s DSSTNE, UC Berkeley\u2019s Caffe & Universit\u00e9 de Montr\u00e9al\u2019s Theano. These libraries perform complex tasks that make it easier to build Deep Learning models.\n\nDeep Learning also requires new computing architectures, the one size fits all computing model of CPUs (Central Processing Units) is not a good fit for the large scale learning performed by the above mentioned libraries and packages. Most of the quench for this memory bandwidth thirsty computation is solved by cutting edge GPUs (Graphics Processing Units), however on the rise are Application Specific Integrated Circuits (ASIC) chips. Google has built such a chip called Tensor Processing Unit (TPU) to meet their Deep Learning needs. Organizations looking to enter the age of Analytics Automation have several options: build machines with off the shelf GPUs, buy pre-built systems with GPUs and the easiest way to test out use cases is rent the infrastructure from one of the major cloud vendors.\n\nThat leaves the Enterprise AI adopters with quite possibly the hardest challenge towards Analytics Automation, availability of large datasets. Many businesses are evolving their business models around massive data those that are not can look towards the area of simulation to design agents that generate the data needed to build the learning models required for Analytics Automation.\n\nIn a series of articles, I will dive into further details on how companies can tackle each of the three Analytics Automation enablers mentioned above.\n\nJust as the first two stages of Enterprise Analytics have accelerated growth and disrupted industries, the era of Analytics Automation should not be dismissed by companies. There is an enormous amount of hype in the fields of Artificial Intelligence and Cognitive Computing, however measured approach to Analytics Automation can significantly alter the future outlook of an organization.", 
        "title": "Analytics Automation \u2013 Making sense \u2013"
    }
]