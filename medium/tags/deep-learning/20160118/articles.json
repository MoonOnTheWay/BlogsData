[
    {
        "url": "https://medium.com/@joshdotai/deep-reinforcement-learning-papers-a2167c136fc7?source=tag_archive---------0----------------", 
        "text": "Josh.ai is an artificial intelligence agent for your home. If you\u2019re interested in learning more, visit us at https://josh.ai.\n\nLike us on Facebook, follow us on Twitter.", 
        "title": "Deep Reinforcement Learning \u2014 Papers \u2013 Josh \u2013"
    }, 
    {
        "url": "https://medium.com/@kidargueta/show-me-the-faces-collecting-faces-with-emotional-expression-for-deep-learning-eeb54526ab93?source=tag_archive---------1----------------", 
        "text": "Teaching a machine how to recognize objects (cars, houses, and cats) is a difficult task, teaching it to recognize emotions is another story. If you have been following my posts, you then know that I want to teach machines to recognize human emotions. One important way in which machines can detect our feelings is by reading our faces. Teaching a machine to read faces has many challenges, and now that I started to tackle this problem I have encountered my first big one.\n\nDeep Learning, a powerful tool used to teach machines seems promising for the task at hand, but in order to make use of it I needed to find the materials to teach the machine. Let me use an analogy to explain. For humans to learn to recognize objects, or in our specific case recognize facial expressions, a person has to be exposed to many faces. That\u2019s not a big deal as we see faces everywhere from the second we are born. On the other hand, we don\u2019t really have tools to take a computer into the wild and let it learn. So my big challenge was finding pictures or videos of people showing emotions in their face, to feed it to the machine and let it learn.\n\nCompanies like Google and Facebook, and some big labs in prestigious universities, have access to an enormous amount of data (just think of how many faces people tag on Facebook). However, mere mortals like me have to find not straightforward ways to collect humble amounts of data to teach our machines. So let me start by defining exactly what is the data I wanted to collect. To teach my machine to recognize emotions from facial expressions, I needed to collect pictures of faces expressing some emotion (angry faces or happy faces), and at the same time I need to explicitly tell the machine what the emotion is (this face shows anger). To be more exact, what I need to feed the machine is a collection of data pairs of the form [picture, emotion]. The question now is how to obtain such data?\n\nFirst let me quickly tell you how you should not obtain this data. Many, including me, would first think about manually collecting thousands of pics from different sources (personal photos, Facebook, etc\u00a0\u2026), use a photo app to crop the faces (the learning is more efficient if the pic contains just the face), and manually define the emotion tag. This is time consuming, and not scalable. Let me explain what I did instead\u00a0.First, many companies offer some automatic ways to pull data from their servers. The obvious choice for pics then might be Instagram (not Facebook as the data is not public). Now the problem with Instagram is that it\u2019s not easy to specify that you want pics with faces. So in order to get exactly what I needed (faces with emotional expressions) my best choice was Google.\n\nGoogle offers the Custom Search API, a tool to let programs pull data based on queries, much like humans would using the Google website. This was perfect for me, to understand why try the query scared look on Google (then go to images). So now I had an automatic way to get faces expressing emotions and I did not have to manually identify the emotion (it comes from the query). But wait, what about this picture:\n\nThe image was obtained with the query angry look, and it clearly has an angry face in it, but it also has an upper body, a gun, and many watermarks. This is not good as it will confuse my machine. How about this picture obtained with the query sad person:\n\nIt clearly has no sad person, it has no person at all as it\u2019s just a table. So while in most cases (when using appropriate queries) you will obtain faces with the intended emotion (like the angry man), it will most likely come with extra noise, or sometimes even not have a face at all. Again, the best way to deal with this is not manually, but using Computer Vision tools to remove the noise automatically.\n\nAfter submitting many queries and downloading a few thousands pics (due to rate limitations this might span a few days), I automatically processed all the pics using the popular Computer Vision library OpenCV (free if you wonder). OpenCV comes pre-loaded with a set of nice filters to detect faces and other features (eyes, mouth,\u00a0\u2026) in pics. The results are magical:\n\nOpenCV automatically detected a square region containing the face, and with additional commands, my program was able to automatically crop and reduce the face to a size and format appropriate to feed to my machine. Now what happened to the image without the face? OpenCV did not detect any face in it so it was automatically ignored. And Voila, that\u2019s how I could efficiently (and free) start building a descent dataset of faces to later teach a computer how to detect our emotions.\n\nTo conclude, very often (depending on the query) you will find friends like this in the pictures:\n\nand OpenCV will of course return you this beauty:\n\nWhether this is bad or not for the trained machine I still don\u2019t know. I will find out when I move to the training process. Worst case, I have to manually remove a few faces (and other possible wrongfully detected objects). Best case, I have a machine that can know if my kids are watching happy cartoons.", 
        "title": "Show Me The Faces: Collecting Faces With Emotional Expression for Deep Learning"
    }, 
    {
        "url": "https://medium.com/pankajmathur/learn-linear-regression-a-must-have-for-deep-learning-machine-learning-any-kind-learnings-you-will-126319773595?source=tag_archive---------2----------------", 
        "text": "In this simple project, We will create Linear regression algorithm and build a simple but robust model from scratch in python. After that, we will use this model to predict Blood Sugar of Diabetes Patient from their BMI data.\n\nRead more about Body Mass Index and how it impacts people health in various ways, and ultimately how it affects Blood Sugar among diabetes patient Here: https://en.wikipedia.org/wiki/Body_mass_index\n\nData taken from http://www4.stat.ncsu.edu/~boos/var.select/diabetes.tab.txt is showing Blood Sugar of 442 diabetes patient with 10 baseline variables or features, age, sex, body mass index, Blood Pressure, and six blood serum measurements. Out of these variables or features, we will be using only 1 variable BMI, and assume that it has a direct linear relationship with Blood Sugar. We have already done all the data munging and have prepared data for you with required columns. Please see the data file on GitHub Repo:\n\nhttps://github.com/pankymathur/simple-linear-regression-using-python-only/blob/master/diabetes.csv\n\nBefore you move forward, Yes, you heard right in Title of this article, we will use no state of the art SciKit Learn but will be building Linear Regression model with our own hand from scratch.. and be explaining things on the way\n\nWe will be covering:\n\nPlease note, This article does not undermine status of SciKit-Learn or GraphLab anyhow and that these packages are unnecessary (because they are totally absolutely wonderful, widely used in industry and much-needed packages for any real-life machine learning or data science projects)\n\nHowever, the Goal is to show math and beauty of python, and the reasoning that all these packages are somehow using similar math and logic behind the doors. In addition, these packages provide many other features, and very highly sustainable Machine Learning models, via simple function calls. Read more about SciKit Learn here, http://scikit-learn.org/stable/ & https://turi.com\n\nOk, enough background, lets\u2019 kickoff this nano project.\n\nIn this nano project we are using Simple line equation y = mX +b to build supervised Linear Regression Model.\n\nBased upon given data, we are taking y (target) as Blood Sugar and X (feature) as BMI of diabetes patients, Look at the data here, (there are no name columns) but do note, that first column is BMI and second column is Blood Sugar. https://github.com/pankymathur/simple-linear-regression-using-python-only/blob/master/diabetes.csv\n\nOur model assumes that there is relationship between y (Blood Sugar) and X (BMI) and they are following a pattern that is represented by equation of line y = mx +b\n\nHere, \u2018m\u2019 is the slope between \u2018X\u2019 (BMI) and \u2018y\u2019 (Blood Sugar), i.e. relationship factor, and in comparison to our data, it means that for every unit of BMI (X), Blood Sugar(y) will be changed by m times.\n\n\u2018b\u2019 is y-intercept (a constant value), y-intercept, it\u2019s a default value of y when x = 0 and is always constant for any values of x > 0, and in comparison to our data, it means that whether any value of BMI (X) is given or not to our model, there is always a Blood Sugar value (y) which is equal to b\u00a0.\n\nWho will have ever known that AI would prefer a strict healthy lifestyle\u2026.\u00a0:)\n\nLet\u2019s move forward, but please note, values of \u201cm\u201d & \u201cb\u201d will be generated during model training or sometimes called in machine learning world \u2018model fitting process\u2019. (All SciKit-Learn models used\u00a0.fit() method to train a model from data)\n\nLong Story short, They way Training or model fitting works (at least in my understanding..), is that we will use equation type y = mX + b, on the given data via trying to fit all values of X in relation to \u201cy\u201d and by finding correct slope of \u201cm\u201d and adding \u201cb\u201d (y-intercept a constant value) to it.\n\nThis will be achieved by choosing any random value of m and b for each row of data and using X value into equation (y = mX +b ), which will give us a new y. This new y aka y_predicted will be compared to actual y from the same row of data, and then we will take the square of the difference of errors, this process will be repeated for each row of data and then we will calculate total error by summing up error of each row and normalized (average) it by dividing it by count of all data rows. See below python code, for computing error for all data rows, using error function f(x) = ((y_initial\u200a\u2014\u200ay_predicted)\u00b2) / Number of data rows\n\nfor example, if you open diabetes.csv https://github.com/pankymathur/simple-linear-regression-using-python-only/blob/master/diabetes.csv you will see in the first row, with the first column as BMI (X) = 41.3 and second column as Blood Sugar (y) = 346 mg/dl\n\nlet's say we use randomly m = 1 and b = 1. Then we put these values into equation y_predicted = mX +b.\n\nnow actual y was 103 mg/dl, so error function result is f(x) = ((y_initial - y_predicted)^2) / Number of data rows = ((341 -41 ) ^2)/442 = ((300)^2 ) / 442 = (90,000) / 442 = ~204 mg/dl of Error. That Sound like a huge unforgiving error for predicting a person Blood Sugar. However, this is where the logic of self-learning, learning from your mistake will come to the picture.\n\nNow, we will keep an eye on this error and try to minimize this error by choosing the different value of \"m\" and \"b\". How do we know, which value of \"m\" and \"b\" is correct at this point, actually we don't, however, we know that which direction we can take, meaning, we know, that by choosing m = 1 and b = 1 we are short of predicting correct Blood Sugar of 346 mg/dl. so let's go in the direction of choosing higher \"m\" and \"b\".\n\nhow about m = 10 and b = 10\u00a0, so new y_predicted = (m)(X) + b = (10)(41.3) + 10 = 423 mg/dl.\n\nSo, error function result is f(x) = ((y_initial - y_predicted)^2 ) / Number of data rows = ((341 - 423 ) ^2) / 442 = ((-82 )^2 ) / 442= (6724 ) / 442 = ~15 mg/dl of Error. (you saw why we choose to square the errors, so we have positive error value..)\n\n15 mg/dl error is much better than 423 mg/dl but still can be improved.\n\nNow looking at y_predicted and error, we can see we are slightly over of predicting correct Blood Sugar, so we need to move gradually down... in direction of lower m and b.\n\nhow about m = 8 and b = 8\u00a0, so new y_predicted = (m)(X) + b = (8)(41.3) + 8 = ~338 mg/dl. so, error function result is f(x) = (y_initial - y_predicted)^2 = ((341 - 338 ) ^2 ) / 442 = ((-2.6 ) ^2) / 442 = ~0.02 mg/dl of Error.\n\nNot bad at all, we just achieve an error of only 0.02 mg/dl in just 3 steps of starting with random values of m and b and then learning from error which direction we need to go to minimize error. Now, that's sound something like self-learning.\n\nand here is python code for calculating new_m and new_b using m_gradient and b_gradient we calculate above:\n\nNow, We will use b_gradient and m_gradient which could be positive or negative value and we will apply them to the previous value of \"b\" and \"m\", in order to come up with the new value of \"b\" and \"m\". However, instead of adding directly b_gradient and m_gradient, we will add a learning rate factor to these gradients. Then we will run this same process over many iterations to come up with best fitted b_gradient and m_gradient.\n\nIn Machine Learning or Deep Learning lingo, Learning Rate and Iterations are called Hyper parameters. These hyper parameter are used to fine tune model by fitting model more accurately around the data, however, beware that Hyper Parameters are two-sided sword, you want to use them to fine tune your model on training data but you don't want to over fit the model so that it will do poor prediction on any new test result. Again, in Machine Learning/ Deep Learning lingo it mean Generalized model vs Over fitted model.\n\nHope it all make sense to you... Here is our final code on GitHub.\n\nAlso, check out my GitHub Page for more simple projects like this.\n\nThat's it, I hope this detailed article, will help you to understand complex math behind linear regression & gradient descent algorithms.\n\nPlease do let me know your feedback, thoughts, questions under the comments section.", 
        "title": "Learn Linear Regression (A must have for Deep Learning, Machine Learning & any kind learnings you will do)"
    }
]