[
    {
        "url": "https://medium.com/@josemarcialportilla/hi-there-im-jose-portilla-and-i-teach-data-science-online-and-in-person-1c93298d5145?source=tag_archive---------0----------------", 
        "text": "Choose a Name, and then select Type: Linux, and Version: Ubuntu (64-bit) or whatever version you downloaded from Step 2.\n\nThis step will depend on your computer\u2019s RAM, but you\u2019ll want to give access to a reasonable amount of RAM here for you to operate the Ubuntu virtualization without any trouble. Depending on your applications choose somewhere between 4\u20138 GB\u00a0, if you only have 8GB total on your computer, then just give the VirtualBox access to 4GB (4000 MB).\n\nChoose \u201cCreate a virtual hard disk and the default 8GB (we will change this later)\n\nChoose \u201cFixed Size\u201d, dynamically allocated can slow down your input output speed. Then choose a decent amount of space, probably 20\u201350 GB. This will take a minute or two (or for SSD it will be much faster).\n\nDepending on your computer, allow access the maximumpa CPUs in the green zone. You can also exceed this is you want, it really depends on what you plan to do\n\nPoint your virtual machine to wherever you saved your Ubuntu iso file and select Start.\n\nDon\u2019t worry this only erases the disk space you set aside, not your actual Windows computer.\n\nIf this takes too long (>10 minutes) go ahead and just reset the Virtual Machine with VirtualBox\n\nI recommend just using the Anaconda version of the instructions:", 
        "title": "Setting up TensorFlow on Windows using VirtualBox \u2013 Jose Marcial Portilla \u2013"
    }, 
    {
        "url": "https://blog.insightdatascience.com/exploring-deep-learning-on-satellite-data-a17bf11781dc?source=tag_archive---------1----------------", 
        "text": "This is a guest post, originally posted at the Fast Forward Labs blog, by Patrick Doupe, now at the Arnhold Institute for Global Health. In this post Patrick describes his Insight project, undertaken in consultation with Fast Forward Labs during the Winter 2016 NYC Data Science session.\n\nMachines are getting better at identifying objects in images. These technologies are used to do more than organize your photos or chat your family and friends with snappy augmented pictures and movies. Some companies are using them to better understand how the world works. Be it by improving forecasts on Chinese economic growth from satellite images of construction sites or estimating deforestation, algorithms and data can help provide useful information about the current and future states of society.\n\nIn early 2016, I developed a prototype of a model to predict population from satellite images. This extends existing classification tasks, which ask whether something exists in an image. In my prototype, I ask how much of something not directly visible is in an image? The regression task is difficult; current advice is to turn any regression problem into a classification task. But I wanted to aim higher. After all, satellite images appear different across populated and non populated areas.\n\nThe prototype was developed in conjuction with Fast Forward Labs, as my project in the Insight Data Science program. I trained convolutional neural networks on LANDSAT satellite imagery to predict Census population estimates. I also learned all of this, from understanding what a convolutional neural network is, to dealing with satellite images to building a website with four weeks of support and mentorship from Fast Forward Labs and Insight Data Science. If I can do this within a few weeks, your data scientists too can take your project from idea to prototype in a short amount of time.\n\nCounting people is an important task. We need to know where people are to provide government services like health care and to develop infrastructure like school buildings. There are also constitutional reasons for a Census, which I\u2019ll leave to Sam Seaborn.\n\nWe typically get this information from a Census or other government surveys like the American Community Survey. These are not perfect measures. For example, the inaccuracies are biased against those who are likely to use government services.\n\nIf we could develop a model that could estimate the population well at the community level, we could help government services better target those in need. The model could also help governments that facing resources constraints that prevent the running of a census. Also, if it works for counting humans, then maybe it could work for estimating other socio-economic statistics. Maybe even help provide universal internet access. So much promise!\n\nSatellite images are huge. To keep the project manageable I chose two US States that are similar in their environmental and human landscape; one State for model training and another for model testing. Oregon and Washington seemed to fit the bill. Since these states were chosen based on their similarity, I thought I would stretch the model by choosing a very different state as a tougher test. I\u2019m from Victoria, Australia, so I chose this glorious region.\n\nSatellite images are also messy and full of interference. To minimize this issue and focus on the model, I chose the LANDSAT Top Of Atmosphere (TOA) annual composite satellite image for 2010. This image is already stitched together from satellite images with minimal interference. I obtained the satellite images from the Google Earth Engine. I began with low resolution images (1km) and lowered my resolution in each iteration of the model.\n\nFor the Census estimates, I wanted the highest spatial resolution, which is the Census block. A typical Census block contains between 600 and 3000 people, or about a city block. To combine these datasets I assigned each pixel its geographic coordinates and merged each pixel to its census population estimates using various Python geospatial tools. This took enough time that I dropped the bigger plans. Best get something complete than a half baked idea.\n\nThe problem I faced is a classic supervised learning problem: train a model on satellite images to predict census data. Then I could use standard methods, like linear regression or neural networks. For every pixel there is number corresponding to the intensity of various light bandwidths. We then have the number of features equal to the number of bandwidths by the number of pixels. Sure, we could do some more complicated feature engineering but the basic idea could work, right?\n\nNot really. You see, a satellite image is not a collection of independent pixels. Each pixel is connected to other pixels and this connection has meaning. A mountain range is connected across pixels and human built infrastructure is connected across pixels. We want to retain this information. Instead of modeling pixels independently, we need to model pixels in connection with their neighbors.\n\nConvolutional neural networks (hereafter, \u201cconvnets\u201d) do exactly this. These networks are super powerful at image classification, with many models reporting better accuracy than humans. For the problem of estimating population numbers, we can swap the loss function and run a regression.\n\nUnfortunately convnets can be hard to train. First, there are a lot of parameters to set in a convnet: how many convolutional layers? Max-pooling or average-pooling? How do I initialize my weights? Which activations? It\u2019s super easy to get overwhelmed. My contact at Fast Forward Labs suggested using VGGNet as a starting base for a model. For other parameters, I based the network on what seemed to be the current best practices. I learned these by following this winter\u2019s convnet course at Stanford.\n\nSecond, convnets take a lot of time and data to train, we\u2019re talking weeks here. I want fast results for a prototype. One option is to use pre-trained models, like those available at the Caffe model zoo. I was writing my model using the Keras python library, which at present doesn\u2019t have as large a zoo of models. Instead, I chose to use a smaller model and see if the results pointed in a promising direction.\n\nTo validate the model, I used data from on Washington, USA and Victoria, Australia. I show the model\u2019s accuracy on the following scatter plot of the model\u2019s predictions against reality. The unit of observation is the small image-observation used by the network and I estimate the population density in an image. Since each image size is roughly the same area (complication: the earth is round), this is the same as estimating population. Last, the data is quasi log-normalized.\n\nWe see that the model is picking up the signal. Higher actual population densities are associated with higher model predictions. Also noticeable is that the model struggles to estimate regions of zero population density. The R^2 of the model is 0.74. That is, the model explains about 74 percent of the spatial variation in population. This is up from 26 percent in the four weeks in Insight.\n\nA harder test is a region like Victoria with a different natural and built environment. The scatter plot of model performance shows the reduced performance. The model\u2019s inability to pick regions of low population is more apparent here. Not only does the model struggle with areas of zero population, it predicts higher population for low population areas. Nevertheless, with an R^2 of 0.63, the overall fit is good for a harder test.\n\nAn interesting outcome is that the regression estimates are quite similar for both Washington and Victoria: the model consistently underestimates reality. In sample, we also have a model that underestimates population. Given that the images are unlikely to have enough information to identify human settlements at current resolution, it\u2019s understandable that the model struggles to estimate population in these regions.\n\nLANDSAT-landstats was an experiment to see if convnets could estimate objects they couldn\u2019t \u2018see.\u2019 Given project complexity, the timeframe, and my limited understanding of the algorithms at the outset, the results are promising. We\u2019re not at a stage to provide precise estimates of a region\u2019s population, but with improved image resolution and advances in our understanding of convnets, we may not be far away.\n\nInterested in transitioning to a career in data science? Find out more about the Insight Data Science Fellows Program in New York and Silicon Valley, apply today, or sign up for program updates.\n\nAlready a data scientist or engineer? Find out more about our Advanced Workshops for Data Professionals. Register for two-day workshops in Apache Spark and Data Visualization, or sign up for workshop updates.", 
        "title": "Exploring Deep Learning on Satellite Data \u2013"
    }, 
    {
        "url": "https://medium.com/@Francesco_AI/13-forecasts-on-artificial-intelligence-82761b7a0f6d?source=tag_archive---------2----------------", 
        "text": "We have discussed some AI topics in the previous posts, and it should seem now obvious the extraordinary disruptive impact AI had over the past few years. However, what everyone is now thinking of is where AI will be in five years time. I find it useful then to describe a few emerging trends we start seeing today, as well as make few predictions around machine learning future developments. The following proposed list does not want to be either exhaustive or truth-in-stone, but it comes from a series of personal considerations that might be useful when thinking about the impact of AI on our world.\n\n1. AI is going to require fewer data to work. Companies like Vicarious or Geometric Intelligence are working toward reducing the data burden needed to train neural networks. The amount of data required nowadays represents the major barrier for AI to be spread out (and the major competitive advantage), and the use of probabilistic induction (Lake et al., 2015) could solve this major problem for an AGI development. A less data-intensive algorithm might eventually use the concepts learned and assimilated in richer ways, either for action, imagination, or exploration.\n\n2. New types of learning methods are the key. The new incremental learning technique developed by DeepMind called Transfer Learning allows a standard reinforcement-learning system to build on top of knowledge previously acquired\u200a\u2014\u200asomething humans can do effortlessly. MetaMind instead is working toward Multitask Learning, where the same ANN is used to solve different classes of problems and where getting better at a task makes the neural network also better at another. The further advancement MetaMind is introducing is the concept of dynamic memory network (DMN), which can answer questions and deduce logical connections regarding series of statements.\n\n3. AI will eliminate human biases, and will make us more \u201cartificial\u201d. Human nature will change because of AI. Simon (1955) argues that humans do not make fully rational choices because optimization is costly and because they are limited in their computational abilities (Lo, 2004). What they do then is \u201csatisficing\u201d, i.e., choosing what is at least satisfactory to them. Introducing AI in daily lives would probably end it. The idea of becoming once for all computationally-effort-independent will finally answer the question of whether behavioral biases exist and are intrinsic to the human nature, or if they are only shortcuts to make decisions in limited-information environment or constrained problems. Lo (2004) states that the satisficing point is obtained through an evolutionary trial and error and natural selection\u200a\u2014\u200aindividuals make a choice based on past data and experiences and make their best guess. They learn by receiving positive/negative feedbacks and create heuristics to solve quickly those issues. However, when the environment changes, there is some latency/slow adaptation and old habits don\u2019t fit the new changes\u200a\u2014\u200athese are behavioral biases. AI would shrink those latency times to zero, virtually eliminating any behavioral biases. Furthermore, learning over time based on experience, AI is setting up as a new evolutionary tool: we usually do not evaluate all the alternatives because we cannot see all of them (our knowledge space is bounded).\n\n4. AI can be fooled. AI nowadays is far away to be perfect, and many are focusing on how AI can be deceived or cheated. Recently a first method to mislead computer vision has been invented, and it has been called adversarial examples (Papernot et al., 2016; Kurakin et al., 2016). Intelligent image recognition software can indeed be fooled by subtle modifying pictures in such a way the AI software would classify the data point as belonging to a different class. Interestingly enough, this method would not trick a human mind.\n\n5. There are risks associated with AI development. It is becoming mainstream to look at AI as potentially catastrophic for mankind. If (or when) an ASI will be created, this intelligence will largely exceed the human one, and it would be able to think and do things we are not able to predict today. In spite of this, though, we think there are few risks associated to AI in addition to the notorious existential threat. There is actually the risk we will not be able to understand and fully comprehend what the ASI will build and how, no matter if positive or negative for the human race. Secondly, in the transition period between narrow AIs and AGI/ASI, there will be generated an intrinsic liability risk\u200a\u2014\u200awho would be responsible in case of mistakes or malfunctioning? Furthermore, there exists, of course the risk of who will detain the AI power and how this power would be used. In this sense, we truly believe that AI should be run as a utility (a public service to everyone), leaving some degree of decision power to humans to help the system managing the rare exceptions.\n\n6. Real general AI will likely be a collective intelligence. It is quite likely that an ASI will not be a single terminal able to make complex decisions, but rather a collective intelligence. A swarm or collective intelligence (Rosenberg, 2015; 2016) can be defined as \u201ca brain of brains\u201d. So far, we simply asked individuals to provide inputs, and then we aggregated after-the-fact the inputs in a sort of \u201caverage sentiment\u201d intelligence. According to Rosenberg, the existing methods to form a human collective intelligence do not even allow users to influence each other, and when they do that they allow the influence to only happen asynchronously\u200a\u2014\u200awhich causes herding biases. An AI on the other side will be able to fill the connectivity gaps and create a unified collective intelligence, very similar to the ones other species have. Good inspirational examples from the natural world are the bees, whose decision-making process highly resembles the human neurological one. Both of them use large populations of simple excitable units working in parallel to integrate noisy evidence, weigh alternatives, and finally reach a specific decision. According to Rosenberg, this decision is achieved through a real-time closed-loop competition among sub-populations of distributed excitable units. Every sub-population supports a different choice, and the consensus is reached not by majority or unanimity as in the average sentiment case, but rather as a \u201csufficient quorum of excitation\u201d (Rosenberg, 2015). An inhibition mechanism of the alternatives proposed by other sub-populations prevents the system from reaching a sub-optimal decision.\n\n7. AI will have unexpected socio-political implications. The first socio-economic implication usually associated with AI is the loss of jobs. Even if from one hand this is a real problem (and opportunity from many extents), we believe there are several further nuances the problem should be approached from. First, the job will not be destroyed, but they will simply be different. Many services will disappear because data will be directly analyzed by individuals instead of corporations, and of the major impact AI will have is fully decentralizing knowledge. A more serious concern in our opinion is instead the two-fold consequence of this revolution. First of all, using always smarter systems will make more and more human beings to lose their expertise in specific fields. This would suggest the AI software to be designed with a sort of double-feedbacks loop, which would integrate the human and the machine approaches. Connected to this first risk, the second concern is that humans will be devoted to mere \u201cmachine technicians\u201d because we will believe AI to be better at solving problems and probably infallible. This downward spiral would make us less creative, less original, and less intelligent, and it will augment exponentially the human-machine discrepancy. We are already experiencing systems that make us smarter when we use them, and systems that make us feeling terrible when we do not. We want AI to fall into the first category, and not to be the new \u201csmartphone phenomenon\u201d which we will entirely depend on. Finally, the world is becoming more and more robo-friendly, and we are already acting as interfaces for robots rather than the opposite. The increasing leading role played by machines\u200a\u2014\u200aand their greater power to influence us with respect to our ability to influence them\u200a\u2014\u200acould eventually make the humans be the \u201cglitches\u201d.\n\nOn a geopolitical side instead, we think the impact AI might have on globalization could be huge: there is a real possibility that optimized factories run by AI systems which control operating robots could be relocated back to the developed countries. It would lack indeed the classic economic low-cost rationale and benefits of running businesses in emerging countries, and this is not clear whether it will level out the countries\u2019 differences or incrementing the existing gaps between growth and developed economies.\n\n8. Real AI should start asking \u201cwhy\u201d. So far, any machine learning system is pretty good in detecting patterns and helping decision makers in their processes, and since many of the algorithms are still hard-coded they can still be understood. However, even if already clarifying the \u201cwhat\u201d and \u201chow\u201d is a great achievement, AI cannot understand the \u201cwhy\u201d behind things yet. Hence, we should design a general algorithm able to build causal models of the world, both physical and psychological (Lake et al., 2016).\n\n9. AI is pushing the limits of privacy and data leakage prevention. AI is shifting the privacy game on an entirely new level. New privacy measures have to be created and adopted, more advanced than simpler secure multi-party computation (SMPC) or faster than homomorphic encryption. Recent researches show how differential privacy can solve many of the privacy problems we are facing on a daily basis, but there are already other companies looking one step ahead\u200a\u2014\u200aan example is Post-Quantum, a quantum cybersecurity computing startup.\n\n10. AI is changing IoT. AI is allowing IoT to be designed as a completely decentralized architecture, where even single nodes can do their own analytics (i.e., \u201cedge computing\u201d). In the classic centralized model, there is a huge problem called server/client paradigm. Every device is identified, authenticated, and connected through cloud servers\u200a\u2014\u200athat entails an expensive infrastructure. A decentralized approach to IoT networking or a standardized peer-to-peer architecture can solve this issue, reduce the costs, and prevent a single node failure to break down the entire system.\n\n11. Robotics is going mainstream. I believe that AI development is going to be constrained by advancements in robotics, and I also believe the two connected fields have to go pari passu in order to achieve a proper AGI/ASI. Looking at following figure, it is clear how our research and even collective consciousness would not consider an AI as general or super without having a \u201cphysical body\u201d.\n\nOther evidence that would confirm this trend are: i) the recent spike in robotic patent application, which according to IFI Claims reached more than 3,000 applications in China, and roughly the same number spread across USA, Europe, Japan, and South Korea; ii) the price trend for the Robo Stox ETF, as shown in next figure.\n\n12. AI might have a real barrier to development. The real barrier for running toward an AGI today is not the choice of algorithms or data we used (not only at least) but is rather a mere structural issue. The hardware capacities, as well as the physical communications (e.g., the internet) and devices power, are the bottlenecks for creating an AI fast enough\u200a\u2014\u200aand this is why I believe there exist departments such as Google Fiber. This is also why quantum computing is becoming extremely relevant. Quantum computing allows us to perform computations that Nature does instantly although they would require us an extremely long time to be completed using traditional computers. It relies on properties of quantum physics, and it is all based on the idea that traditional computers state every problem in terms of strings of zeros and ones. The qubits instead identify quantum states where a bit can be at the same time zero and one. Hence, according to Frank Chen (partner at Andreessen Horowitz), transistors, semiconductors, and electrical conductivity are replaced by qubits\u200a\u2014\u200athat can be represented as vectors\u200a\u2014\u200aand new operations different from traditional Boolean algebra.\n\nA common way to explain the different approach of traditional vs. quantum computing is through the phonebook problem. The traditional approach for looking for a number in a phonebook proceeds through scanning entry by entry in order to find the right match. A basic quantum search algorithm (known as Grover\u2019s algorithm) relies instead on what is called \u201cquantum superposition of states\u201d, which basically analyzes every element at once and determines probabilistically the right answer.\n\nBuilding a quantum computer would be a scientific revolutionary breakthrough, but it is currently extremely hard to build according to Chen. The most relevant issues are the elevated temperature needed for superconducting materials the computer will be built with; the small coherence time, which is the time window in which the quantum computer can actually perform calculations; the time for performing single operations; and eventually, the energy difference between the right and the wrong answers is so small to be hard to be detected. All these problems shrink the market space to no more than a few companies working on quantum computing: colossus such as IBM and Intel are working on it since some years, and startups such as D-Wave Systems (acquired by Google in 2013); Rigetti Computing; QxBranch; 1Qbit; Post-Quantum; ID Quantique; Eagle Power Technologies; Qubitekk; QC Ware; Nano-Meta Technonoliges; and Cambridge Quantum Computing Limited are laying the foundations for quantum computing.\n\n13. Biological robot and nanotech are the future of AI applications. We are witnesses of a series of incredible innovations lying at the intersection of AI and nanorobotics. Researchers are working toward creating creatures entirely artificial as well as hybrids, and they even tried to develop biowires (i.e., electrical wires made by bacteria) and organs on chips (i.e., functional pieces of human organs in miniature made by human cells that can replicate some of the organ functions - Emulate is the most advanced company in this space). Bio-bots research is also testing the boundaries of materials, and soft-robots have been recently created with only soft components. BAE Systems corporation is also pushing the limits of computing trying to create a \u201cchemical computer (the Chemputer)\u201d, a machine that would use advanced chemical processes \u201cto grow\u201d complex electronic systems.\n\nKurakin, A., Goodfellow, I. J., Bengio, S. (2016). \u201cAdversarial Examples in the Physical World\u201d. Technical report, Google, Inc. Available at arXiv: 1607.02533.\n\nLake, B. M., Ullman, T. D., Tenenbaum, J. B., Gershman, S. J. (2016). \u201cBuilding Machines That Learn and Think Like People\u201d. Available at arXiv:1604.00289.\n\nLo, A. W. (2004). \u201cThe Adaptive Markets Hypothesis: Market Efficiency from an Evolutionary Perspective\u201d. Journal of Portfolio Management 30: 15\u201329.\n\nRosenberg, L. B. (2015). \u201cHuman Swarms, a real-time method for collective intelligence\u201d. Proceedings of the European Conference on Artificial Life: 658\u2013659.\n\nRosenberg, L. B. (2016). \u201cArtificial Swarm Intelligence, a Human-in-the-Loop Approach to A.I.\u201d. Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI-16): 4381\u20134382.\n\nSimon, H. A. (1955). \u201cA Behavioral Model of Rational Choice\u201d. The Quarterly Journal of Economics, 69 (1): 99\u2013118.", 
        "title": "13 Forecasts on Artificial Intelligence \u2013 Francesco Corea \u2013"
    }, 
    {
        "url": "https://medium.com/krol-institute/%D1%87%D1%82%D0%BE-%D1%81%D0%B4%D0%B5%D0%BB%D0%B0%D0%BB%D0%BE-%D0%BD%D0%B0%D1%81-%D0%BB%D1%8E%D0%B4%D1%8C%D0%BC%D0%B8-%D0%B8-%D1%87%D1%82%D0%BE-%D0%B4%D0%B0%D1%81%D1%82-%D1%80%D0%B0%D0%B7%D1%83%D0%BC-%D0%BA%D0%BE%D0%BC%D0%BF%D1%8C%D1%8E%D1%82%D0%B5%D1%80%D1%83-4e707ba73047?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u0427\u0442\u043e \u0441\u0434\u0435\u043b\u0430\u043b\u043e \u043d\u0430\u0441 \u043b\u044e\u0434\u044c\u043c\u0438, \u0438 \u0447\u0442\u043e \u0434\u0430\u0441\u0442 \u0440\u0430\u0437\u0443\u043c \u043a\u043e\u043c\u043f\u044c\u044e\u0442\u0435\u0440\u0443?"
    }, 
    {
        "url": "https://medium.com/@demidovs/ml-1-logistic-function-vs-softmax-44c73399c3c4?source=tag_archive---------4----------------", 
        "text": "Hey everyone. What do you know about the classification task as the ML task and what do you know about softmax and logistic function? If you no nothing about that, you are welcome.\n\nSo, the classic task of the ML is classification. According to Wikipedia, classification (supervised learning) is the task of elaborating inferring function from the labeled data.\n\nTo keep it simple, you want to make the mechanism that is capable of labeling the input against the finite number of known labels. Say we have a picture of a cat and we want the computer to tell us that it is a cat.\n\nAll the images were taken from the Pixabay, which offers amazing images free of charge for commercial usage.\n\nAs usual there are two cases: the simpler and the harder one.\n\nLet\u2019s take the simpler case first. The task is to make classification against only two labels, or in other words to understand whether there is something on the image or not. Say we have two labels: \u201ccat\u201d and \u201cnot a cat\u201d\n\nTo this moment let\u2019s consider the Neural Network (NN) as the black box. Frankly, this article is not about NN at all, but about the important final part of the whole NN computation\u200a\u2014\u200athe part which is responsible for labeling input data.\n\nIn short, the Black Box model is the model of any object, which internal structure is not known for us, and the only thing that we know about it are results that it provides to the given input. For more information refer to the Wikipedia article about that. In contrary, there is the White Box model which provides us with the internal structure of the object under modeling, as well as expected inputs and outputs. There is also Wikipedia article about that.\n\nSo, NN computation block is a Black Box for us. We give it the image and get the list of results.\n\nSo, we get the vector of results:\n\nTo recognize the output and label it with one of two labels we can apply logistic function, which has the following representation:\n\nWhen applied to that input vector it gives us the following results:\n\nSo, we get the following result:\n\nThen, let\u2019s refer to the logistic function curve:\n\nSo, as we see, this function can be successfully used for the classification with two labels: those that are bigger than 0.5 are considered as \u201c1\u201d and those that are smaller than \u201c0.5\u201d are considered as \u201c0\u201d.\n\nNow, let\u2019s apply it to the output we have:\n\nWhat about the softmax function? As we have mentioned before it is used for the tasks of classification against the set of labels. In particular it helps to understand the probabilities of the input to be labeled by this or that name.\n\nConsider this example:\n\nSo, the question is: How probable is that the given input is of each class? We assume that each input can have only one label, i.e. they are inclusive.\n\nTo understand that we use the softmax function:\n\nThe implementation of this function is relatively straightforward:\n\nThe overall structure was inspired by the Udacity video on the Softmax function.\n\nAs a result we get the probabilities for the given vector:\n\nNotice that as these probabilities give the sum of 1 they were calculated properly.\n\na) Softmax function outputs probabilities by the given arbitrary vector and can be used for multi-labeling tasks;\n\nb) Logistic function outputs values of sigmoid function by the given arbitrary vector and can be used for binary labeling tasks.\n\nPlease don\u2019t hesitate to tell if something is wrong, I\u2019d love to hear it and learn more from your comments.", 
        "title": "#ML-1. Logistic function vs Softmax \u2013 Alexander Demidovskij \u2013"
    }, 
    {
        "url": "https://medium.com/@teamrework/the-ai-update-from-re-work-c709a1df45ef?source=tag_archive---------5----------------", 
        "text": "The worlds of AI, Deep Learning and Machine Intelligence are rapidly evolving and it\u2019s hard to keep up! To help you get up-to-date we\u2019ve rounded up the latest must-see AI news, interviews and videos.\n\nRE\u2022WORK Blogs\n\nNeural Attention: Machine Learning Meets Neuroscience\n\nNeural attention has been applied successfully to a variety of different applications including NLP, vision, and memory. We spoke to Brian Cheung, from UC Berkeley & Google Brain, to learn more about the intersection of neuroscience and machine learning.\n\nMachine Intelligence Is Transforming Genomics & Precision Medicine\n\nNew applications for deep learning are rapidly emerging, with healthcare often touted as the industry to be most disrupted by AI. We interviewed Alice Gao, Research Scientist & Engineer at Deep Genomics, to learn more.\n\nVisual Question Answering Problems: Reasoning With Deep Learning\n\nIlija Ilievski is a PhD student at the National University of Singapore, studying interdisciplinary research in the intersection of vision and language. He believes question answering over multimodal data is the next frontier of deep learning.\n\nPredicting Future Human Behavior With Deep Learning\n\nCarl Vondrick\u2019s work has received a lot of media attention, including features in Forbes, Wired, CNN and Popular Science, when he built a vision system for AI to learn and understand human behaviour and interactions, teaching it with popular TV shows like The Office, Desperate Housewives, and YouTube videos.\n\nHow We Built a Chatbot for Real People\n\nThe world of apps and chatbots will go through an exponential transformation in the next few years, thanks to the pervasive use of instant messaging. Roberta Lucca shares her top 5 tips for building a successful chatbot company.\n\nWhat Did You Miss at the Deep Learning Summit Last Week?\n\nMedia attending the 2016 Deep Learning Summit in London included BBC News, The Guardian, The WSJ, Bloomberg, VentureBeat, FT and more, with coverage focusing on a range of topics from robotics, chatbots, machine vision, big data to startup acquisitions.\n\nFurther Afield\n\nWhy Data Is the New Coal\n\nDeep learning needs to become more efficient if it is going to move from using data to categorise images of cats to diagnosing rare illnesses. Alex Hern reports on revelations in this area from Neil Lawrence, newly appointed Senior Principal Scientist at Amazon.\n\nWhy Facebook and Microsoft Say Chatbots Are the Talk of the Town\n\n\u2018Chatbots are the new apps,\u201d said Microsoft\u2019s CEO Satya Nadella earlier this year. He\u2019s not the first senior tech exec to make this claim. Software programmed to interact with humans is hot property in Silicon Valley, with potential benefits for businesses, consumers\u200a\u2014\u200aeven the bereaved.\n\nInside DeepMind\u2019s Attempts to Achieve a General Artificial Intelligence\n\nGoogle DeepMind is working on progressive neural networks to bring us closer to a multi-functional \u2018general\u2019 artificial intelligence, but we are still some distance away.\n\nThe AI Revolution: Why Deep Learning Is Suddenly Changing Your Life\n\nNeural nets aren\u2019t new. The concept dates back to the 1950s, and many of the key algorithmic breakthroughs occurred in the 1980s and 1990s. So what\u2019s changed today to allow computer scientists to harness their power?\n\nYou Too Can Become a Machine Learning Rock Star! No PhD Necessary\n\nWhat if you could get the benefits of AI without having to hire those hard-to-find and expensive-to-woo talents? What if smart software could lower the bar? Could you get deep learning with a shallower talent pool?\n\nHonglak Lee is Assistant Professor of Computer Science & Engineering at the University of Michigan. His research interests span deep learning, unsupervised, semi-supervised, and supervised learning, transfer learning, graphical models, and optimization. He also works on application problems in computer vision, audio recognition, robot perception, and text processing.\n\nWatch more video presentations and interviews on the RE\u2022WORK video hub and our YouTube channel.\n\nFollow us on Twitter and LinkedIn for more news and articles!\n\nView upcoming RE\u2022WORK events here.", 
        "title": "The AI Update, from RE\u2022WORK \u2013 RE\u2022WORK \u2013"
    }, 
    {
        "url": "https://medium.com/@drmani/lenses-for-life-de6bc38a4207?source=tag_archive---------6----------------", 
        "text": "We view the world around us\u200a\u2014\u200aand the people in it\u200a\u2014\u200athrough lenses.\n\nOur focus is often limited to a tiny part of the whole. We see snippets and tidbits of other people\u2019s lives.\n\nBut we have a choice. To zoom out with our lenses to learn more about them\u2026 or zoom in tighter to view their actions in greater depth and detail.\n\nBoth give us more insight. Richer data. Wider perspective. And help us develop empathy.\n\nBecause now, we actually begin to understand them better.\n\nSo next time you see somebody, think about the lens through which you\u2019re looking. And then, reach out for the lever that controls its zoom\u200a\u2014\u200aand twist it.\n\nYou\u2019ll be amazed at what you see!\n\nSpeaking of lenses, my book doesn\u2019t have any.", 
        "title": "Lenses for Life \u2013 Dr Mani \u2013"
    }
]