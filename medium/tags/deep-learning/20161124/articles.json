[
    {
        "url": "https://medium.com/emergent-future/why-deep-learning-matters-and-whats-next-for-artificial-intelligence-5c629993dc4?source=tag_archive---------0----------------", 
        "text": "Why Deep Learning Matters and What\u2019s Next for Artificial Intelligence\n\nIt\u2019s almost impossible to escape the impact frontier technologies are having on everyday life.\n\nAt the core of this impact are the advancements of artificial intelligence, machine learning, and deep learning.\n\nThese change agents are ushering in a revolution that will fundamentally alter the way we live, work, and communicate akin to the industrial revolution\u200a\u2014\u200amore specifically, AI is the new industrial revolution.\n\nThe most exciting and promising of these frontier technologies is the advancements happening in the deep learning space.\n\nWhile still nascent, it\u2019s deep learning percolating into your smartphone, driving advancements in healthcare, creating efficiencies in the power grid, improving agricultural yields, and helping us find solutions to climate change.\n\nJust this year a handful of high-profile experiments came into the spotlight, including Microsoft Tay, Google\u2019s DeepMind AlphaGo, and Facebook M and highlight the versatility of deep learning and the application of AI.\n\nFor instance, Google DeepMind has been used to master the game of Go, cut their data center energy bills by reducing power consumption by 15%, and even working with NHS to fight blindness.\n\n\u201cDeep Learning is an amazing tool that is helping numerous groups create exciting AI applications,\u201d Andrew Ng says, Chief Scientist at Baidu and chairman/co-founder of Coursera. \u201cIt is helping us build self-driving cars, accurate speech recognition, computers that can understand images, and much more.\u201d\n\nThese experiments all rely on a technique known as deep learning, which attempts to mimic the layers of neurons in the brain\u2019s neocortex. This idea\u200a\u2014\u200ato create an artificial neural network by simulating how the brain works\u200a\u2014\u200ahas been around since the 1950s in one form or another.\n\nDeep learning is a subset of a subset of artificial intelligence, which encompasses most logic and rule-based systems designed to solve problems. Within AI, you have machine learning, which uses a suite of algorithms to go through data to make and improve the decision making process. And, within machine learning you come to deep learning, which can make sense of data using multiple layers of abstraction.\n\nDuring the training process, a deep neural network learns to discover useful patterns in the digital representation of data, like sounds and images. In particular, this is why we\u2019re seeing more advancements for image recognition, machine translation, and natural language processing come from deep learning.\n\nOne example of deep learning in the wild is how Facebook can automatically organize photos, identify faces, and suggest which friends to tag. Or, how Google can programmatically translate 103 languages with extreme accuracy.\n\nIt\u2019s been more than a half-century since the science behind deep learning was discovered, but why is it just now starting to transform the world?\n\nThe answer lies in two major shifts: an abundance of digital data and access to powerful GPUs.\n\nTogether, we are now capable of teaching computers to read, see, and hear simply by throwing enough data and compute at the problem.\n\nThere\u2019s a special kind of irony reserved for all of these new breakthroughs that are really just the same breakthrough: deep neural networks.\n\nThe basic concept of deep learning reach back to the 1950s, but were largely ignored till the 1980s and 90s. What\u2019s changed, however, is the context of abundant computation and data.\n\nWe now have access to, essentially, unlimited computational power thanks to Moore\u2019s law and the cloud. On the other side, we\u2019re creating more image, video, audio, and text data everyday than before due to the proliferation of smartphones and cheap sensors.\n\n\u201cThis is deep learning\u2019s Cambrian explosion,\u201d Frank Chen says, partner at the Andreessen Horowitz.\n\nFour years ago, Google had just two deep learning projects. Today, the search giant is infusing deep learning into everything it touches: Search, Gmail, Maps, translation, YouTube, their self-driving cars, and more.\n\n\u201cWe will move from mobile first to an AI first world,\u201d Google\u2019s CEO, Sundar Pichai said earlier this year.\n\nIn a very real sense, we\u2019re teaching machines to teach themselves.\n\n\u201cAI is the new electricity,\u201d Ng says. \u201cJust as 100 years ago electricity transformed industry after industry, AI will now do the same.\u201d\n\nDespite the breakthroughs, deep learning algorithms still they can\u2019t reason the way humans do. That could change soon, though.\n\nYann LeCun, Director of AI Research at Facebook and Professor at NYU, says deep learning combined with reasoning and planning is one area of research making promising advances right now, he says. Solving this in the next five years isn\u2019t out the realm of possibilities.\n\n\u201cTo enable deep learning systems to reason, we need to modify them so that they don\u2019t produce a single output, say the interpretation of an image, the translation of a sentence, etc., but can produce a whole set of alternative outputs. e.g the various ways a sentence can be translated,\u201c LeCun says.\n\nYet, despite plentiful data, and abundant computing power, deep learning is still very hard.\n\nOne bottleneck is the lack of developers trained to use these deep learning techniques. Machine learning is already a highly specialized domain, and those with the knowledge to train deep learning models and deploy them into production are even more select.\n\nFor instance, Google can\u2019t recruit enough developers with vast deep learning experience. Their solution is to simply teach their developers to use these techniques instead.\n\nOr, when Facebook\u2019s engineers struggled to take advantage of machine learning, they created an internal tool for visualizing machine and deep learning workflows, called FBLearner Flow.\n\nBut, where does that leave the other 99% of developers that don\u2019t work at one of these top tech company?\n\nVery few people in the world know how to use these tools.\n\n\u201cMachine learning is a complicated field,\u201d S. Somasegar says, venture partner at Madrona Venture Group and the former head of Microsoft\u2019s Developer Division. \u201cIf you look up the Wikipedia page on deep learning, you\u2019ll see 18 subcategories underneath Deep Neural Network Architectures with names such as Convolutional Neural Networks, Spike-and-Slab RBMs, and LTSM-related differentiable memory structures.\u201d\n\n\u201cThese are not topics that a typical software developer will immediately understand.\u201d\n\nYet, the number of companies that want to process unstructured data, like images or text, is rapidly increasing. The trend will continue, primarily because deep learning techniques are delivering impressive results.\n\nThat\u2019s why it\u2019s important for the people capable of training neural nets are also able to share their work with as many people as possible. In essence, democratizing access to machine intelligence algorithms, tools, and techniques.\n\nGPUs on-demand and running in the cloud, eliminate the manual work required for teams and organizations to experiment with cutting-edge, deep learning algorithms and models, which allows them to get started for a fraction of the cost.\n\n\u201cDeep learning has proven to be remarkably powerful, but it is far from plug-n-play,\u201d Oren Etzioni says, CEO of the Allen Institute for Artificial Intelligence. \u201cThat\u2019s where Algorithmia\u2019s technology comes in\u200a\u2014\u200ato accelerate and streamline the use of deep learning.\u201d\n\nWhile GPUs were originally used to accelerate graphics and video games, more recently they\u2019ve found new life powering AI and deep learning tasks, like natural language understanding, and image recognition.\n\n\u201cWe\u2019ve had to build a lot of the technology and configure all of the components required to get GPUs to work with these deep learning frameworks in the cloud,\u201d Kenny Daniel says, Algorithmia founder and CTO. \u201cThe GPU was never designed to be shared in a cloud service like this.\u201d\n\nHosting deep learning models in the cloud can be especially challenging due to complex hardware and software dependencies. While using GPUs in the cloud are still nascent, they\u2019re essential for making deep learning tasks performant.\n\n\u201cFor anybody trying to go down the road of deploying their deep learning model into a production environment, they\u2019re going to run into problems pretty quickly,\u201d Daniel says. \u201cUsing GPUs inside of containers is a challenge. There are driver issues, system dependencies, and configuration challenges. It\u2019s a new space that\u2019s not well-explored, yet. There\u2019s not a lot of people out there trying to run multiple GPU jobs inside a Docker container.\u201d\n\n\u201cWe\u2019re dealing with the coordination needed between the cloud providers, the hardware, and the dependencies to intelligently schedule work and share GPUs, so that users don\u2019t have to.\u201d\n\nMost commercial deep learning products use \u201csupervised learning\u201d to achieve their objective.\n\nFor instance, in order to recognize a cat in a photo, a neural net will need to be trained with a set of labeled data. This tells the algorithm that there is a \u201ccat\u201d represented in this image, or there is not a \u201ccat\u201d in this photo. If you throw enough images at the neural network, it will, indeed, learn to identify a \u201ccat\u201d in an image.\n\nProducing large, labelled datasets is an achilles heel for most deep learning projects, however.\n\n\u201cUnsupervised learning,\u201d on the other hand, is how deep learning works and enables us to discover new patterns and insights by approaching problems with little or no idea what our results should look like.\n\nIn 2012, Google and Stanford let a neural net loose on 10 million YouTube stills. Without any human interaction, the neural net learned to identify cat faces from the YouTube stills, effectively identifying patterns in the data and teaching itself what parts of the images might be relevant.\n\nThe important distinction between supervised and unsupervised learning is that there is no feedback loop with unsupervised learning. Meaning, there\u2019s no human there correcting mistakes or scoring the results.\n\nThere\u2019s a bit of a gotcha here: we don\u2019t really know how deep learning works. Nobody can actually program a computer to do these things specifically. We feed massive amounts of data into deep neural nets, sit back, and let the algorithms learn to recognize various patterns contained within.\n\n\u201cYou essentially have software writing software,\u201d says Jen-Hsun Huang, CEO of GPU leader NVIDIA says.\n\nWhen we master unsupervised learning, we\u2019ll have machines that will unlock aspects about our world previously out of our reach.\n\n\u201cIn computer vision, we get tantalizing glimpses of what the deep networks are actually doing,\u201d Peter Norvig, research director at Google says. \u201cWe can identify line recognizers at one level, then, say, eye and nose recognizers at a higher level, followed by face recognizers above that and finally whole person recognizers.\u201d\n\nIn other areas of research, Norvig says, it has been hard to understand what the neural networks are doing.\n\n\u201cIn speech recognition, computer vision object recognition, the game of Go, and other fields, the difference has been dramatic,\u201d Norvig says. \u201cError rates go down when you use deep learning, and both these fields have undergone a complete transformation in the last few years. Essentially all the teams have chosen deep learning, because it just works.\u201d\n\nIn 1950, Alan Turing wrote \u201cWe can only see a short distance ahead, but we can see plenty there that needs to be done.\u201d Turing\u2019s words hold true.\n\n\u201cIn the next decade, AI will transform society,\u201d Ng says. \u201cIt will change what we do vs what we get computers to do for us.\u201d\n\n\u201cDeep learning has already helped AI make tremendous progress,\u201d Ng says, \u201cbut the best is still to come!\u201d", 
        "title": "Why Deep Learning Matters and What\u2019s Next for Artificial Intelligence"
    }, 
    {
        "url": "https://blog.getrevue.co/the-most-read-ai-vr-articles-on-revue-c93768dbc114?source=tag_archive---------1----------------", 
        "text": "We thought we will continue with the tech theme and write up an article that resembles a digest. Talk about eating our own dog food and all! A while back, we created a piece on the most popular AI & VR digests on Revue. Since we have a growing community that is becoming more and more engaged with those topics, we thought it would be cool to see which pieces have sparked the most interest throughout various digests. So, let\u2019s take a look!\n\nThis piece was featured in the Wild Week In AI. This is not really an article on it\u2019s own, it\u2019s more of a \u201cplayground\u201d that presents a neutral network for you to fiddle with. If you open it, you\u2019ll see a system with a choice of different data sets, features & hidden layers that you can adjust so you can properly try it out. Also, underneath you\u2019ll be able to find instructions about what you can do with this nifty tool! Last but not least, you can even re-purpose it since it\u2019s open sourced on GitHub.\n\nThis article was featured in Azeem Azhar\u2019s newsletter- The Exponential View. If you\u2019re not familiar with him, he knows a lot about tech and he has a massive following. Naturally, we thought that he must have one of the most clicked links since he\u2019s such a skilled curator. This is one of them. The piece is about the significant technological advances that have been made in the field of AI. To be more specific, this article is discussing the event of a computers becoming better at playing Go and being able to predict next moves, which until recently wasn\u2019t considered possible.\n\nThis piece has been one of the most clicked ones in Nathan Benaich\u2019s newsletter. It starts off by discussing how Google has changed since its launch all those years ago. The author mentions the mission that Google has when it comes to information-\u201cto organize the world\u2019s information and make it universally accessible and useful\u201d. The great thing about this article is that it talks about how Google has incorporated VR & AI into their products to build a different, better future. It\u2019s all about how those specific products are solving different problems from our daily lives faster & more efficiently because of the big technological advances that already exist these days.\n\nWith everything amazing that is happening in the field of AI it was about time that self-driving cars made their way to the open roads. And guess who made this possible- Google, of course! Well, it\u2019s not like you\u2019ll randomly see Google self-driving cars next to you in a traffic jam at least in the next year or so. But Google made a prototype of this car and worked on a project to test out how this vehicle will perform in the real world. Ironically enough, the little machine got stopped by a police officer for driving too slow. Even though there has been research that many of those self-navigating cars can be a reason for road accidents, Google is set to disprove this by improving their product.\n\nThis article has been part of Peter Rojas\u2019s newsletter VR Links. It is about how virtual Reality hardware, like a VR headset e.g.\u00a0, can be one of the most exciting gadgets out there, especially for enthusiasts. This is becoming a lot more popular. The best news is, that most of all, it\u2019s becoming more accessible. But the main focus here is on what went wrong with Google Glass and why Oculus Rift can and probably will be very different. Maybe one of the most important takeaways here is that VR is all about experiences. That being said, the article also touches upon how smartphones fit into the VR environment. For example, Google\u2019s Project Tango just might be a massive game changer.", 
        "title": "The most read AI & VR articles on \u2013"
    }, 
    {
        "url": "https://medium.com/ai-finance-start-ups-life/future-of-work-e3ffdbb1fa05?source=tag_archive---------2----------------", 
        "text": "\u201cA double shot espresso for John\u201d, the barista at Starbucks calls out your name while you are reading your email on your phone. \u201cShould I review Solar City\u2019s account one more time before I head down there?\u201d \u201cI still have several hours before the noon meeting\u201d Then, a notification comes up. Upon tapping on it, a neighborhood map is displayed with all the accounts. While some of them are familiar customers, a lot of them, marked with \u201cGold Star\u201d, are not. These are the accounts that your other sales team members have been laboring to close in the last month. \u201cI have done deals with these guys before. They know me. I have several hours now. What if I just pay them a visit before the afternoon meeting? I am sure that will help\u201d Then, you tab on the account and dial\u2026.\n\nCarl, your CEO, was scheduled to make a presentation to the board in Q2. He sent you an email suggesting that he himself, Joe ( VP of Marketing ), and yourself ( VP of Sales ) should work together on the slides in April in New York, where the headquarter office is based. Eager to get started on preparing for the meeting, you tab on the \u201csuggested records\u201d button. Your sales data for the past quarter\u200a\u2014\u200aOpportunities, Contracts, Email exchanges\u200a\u2014\u200aare neatly displayed. \u201cI\u2019d better start planning for the trip to New York. \u201cLast time I booked the flight late, and it has costed me an extra thousand dollars!\u201d, you pondered. Scrolling beyond the sales documents, you tab on the \u201cplan your travel\u201d action. There, you enter the itinerary information\u200a\u2014\u200atravel dates, preferred hotel and airline, and the information goes straight to the travel system, and a thousand dollars saved!\n\nYou are reviewing the email from Danaher, a customer at the final stage of discussion. You notice that there will be a demo meeting next month. Darwin, your sales engineer, is preparing for a technical presentation for that meeting. Launching an SMS session with Darwin, you want to work with him to ensure that everything is on track. Unlike any other chat session, there is a bot also listening in. To give Darwin more information about the deal, you order the bot to fetch the contracts which were just exchanged with Danaher\u2019s legal. After Darwin replied, you ask the bot to fetch the opportunities. The chat continues\u2026\n\nIs this vision\u200a\u2014\u200agetting to the right information at the right time, while you work\u200a\u2014\u200atoo far fetched?\n\nFor us at SmarterMe, the answer is no. In fact, we believe this is the future of work. And the future is already here!\n\nThanks to advances in AI and Deep Learning, SmarterMe is realizing this vision now. And customers are reaping the benefits and are becoming extremely productive. In my next blog post, I will reveal the technologies that are powering the vision. Stay tuned!", 
        "title": "Future of Work \u2013 AI, Finance, Start-ups, Social \u2013"
    }, 
    {
        "url": "https://medium.com/@mslavescu/community-is-key-and-i-see-it-the-only-way-to-collect-and-analyze-useful-data-effectively-53bc35e09130?source=tag_archive---------3----------------", 
        "text": "Join me in revolutionizing this approach, through simplifying the collection and analysis of data in such a way that a lot of people will be able to do it themselves and share the research/code/data/results in an open way through open source:", 
        "title": "Community is key! And I see it the only way to collect and analyze useful data effectively"
    }
]