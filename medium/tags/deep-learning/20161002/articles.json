[
    {
        "url": "https://medium.com/@Francesco_AI/what-you-are-too-afraid-to-ask-about-artificial-intelligence-part-i-machine-learning-d3151300ebe?source=tag_archive---------0----------------", 
        "text": "What you are too afraid to ask about Artificial Intelligence (Part I): Machine\u00a0Learning\n\nAI is moving at a stellar speed and is probably one of most complex and present sciences. The complexity here is not meant as a level of difficulty in understanding and innovating (although of course, this is quite high), but as the degree of interrelation with other fields apparently disconnected.\n\nThere are basically two schools of thought on how an AI should be properly built: the Connectionists start from the assumption that we should draw inspiration from the neural networks of the human brain, while the Symbolists prefer to move from banks of knowledge and fixed rules on how the world works. Given these two pillars, they think it is possible to build a system capable of reasoning and interpreting.\n\nIn addition, a strong dichotomy is naturally taking shape in terms of problem-solving strategy: you can solve a problem through a simpler algorithm, which though it increases its accuracy in time (iteration approach), or you can divide the problem into smaller and smaller blocks (parallel sequential decomposition approach).\n\nUp to date, there is not a clear answer on what approach or school of thoughts works the best, and thus I find appropriate to briefly discuss major advancements in both pure machine learning techniques (Part I) and neuroscience (Part II) with an agnostic lens.\n\nMachine learning techniques can be roughly divided into supervised methods and unsupervised methods, with the main difference of whether the data are labelled (supervised learning) or not (unsupervised). A third class can be introduced when we talk about AI: reinforcement learning (RL). RL is a learning method for machines based on the simple idea of reward feedback: the machine indeed acts in a specific set of circumstances with the goal of maximizing the potential future (cumulative) reward. In other words, it is a trial-and-error intermediate method between supervised and unsupervised learning: the data labels are indeed assigned only after the action, and not for every training example (i.e., they are sparse and time-delayed). RL usually comes with two major problems, namely the credit assignment problem and the explore-exploit dilemma\u200a\u2014\u200aplus a series of technical issues such as the curse of dimensionality, non-stationary environments, or partial observability of the problem. The former one concerns the fact that rewards are, by definition, delayed, and you might need a series of specific actions in order to achieve your goal. The problem is then to identify which of the preceding action was actually responsible for the final output (and to get the reward then), and if so to what degree. The latter problem is instead an optimal searching problem: the software has to map the environment as accurately as possible in order to figure out its reward structure. There is an optimal stop problem\u200a\u2014\u200aa sort of satisficing indeed: to what extent the agent should keep exploring the space to look for better strategies, or start exploiting the ones it already knows (and knows that work)?\n\nIn addition to the present classification, machine learning algorithms can be classified based on the output they produce: classification algorithms; regressions; clustering methods; density estimation; and dimensionality reduction methods.\n\nThe new AI wave encouraged the development of innovative ground-breaking techniques, as well as it brought back to the top a quite old concept, i.e., the use of artificial neural networks (ANNs).\n\nArtificial Neural Networks are a biologically-inspired approach that allows software to learn from observational data\u200a\u2014\u200ain this sense sometimes is said they mimic the human brain. The first ANN named Threshold Logic Unit (TLU) was introduced in the Forties by McCulloch and Pitts (1943), but only forty years later Rumelhart et al. (1986) pushed the field forward designing the back-propagation training algorithm for feed-forward multi-layer perceptrons (MLPs).\n\nThe standard architecture for any ANNS is having a series of nodes arranged in an input layer, an output layer, and a variable number of hidden layers (that characterize the depth of the network). The inputs from each layer are multiplied by a certain connection weight and summed up, to be compared to a threshold level. The signal obtained through the summation is passed into a transfer function, to produce an output signal that is, in turn, passed as input into the following layer. The learning happens in fact in the multiple iterations of this process, and it is quantitatively computed by choosing the weighting factors that minimize the input-output mapping error given a certain training dataset.\n\nANNs do not require any prior knowledge to be implemented, but on the other side, they can still be fooled because of it. They are often also called Deep Learning (DL), especially for the case in which there are many layers that perform computational tasks. There exist many types of ANNs up to date, but the most known ones are Recurrent Neural Networks (RNNs); Convolutional Neural Networks (CNNs); and Biological Neural Networks (BNNs).\n\nRNNs use sequential information to make accurate prediction. In traditional ANNs, all the inputs are independent one from the other. RNNs perform instead a certain task for every element of the sequence, keeping a sort of memory of the previous computations. CNNs try instead to mirror the structure of the mammalian visual cortex and they have every layer working as detection filters for detecting specific patterns in the original data (and this is why they are really suitable for object recognition). Finally, BNNs are more a sub-field of ANNs rather than a specific application. The best example of this class is in our opinion the Hierarchical Temporal Memory (HTM) model developed by Hawkins and George of Numenta, Inc, which is a technology that captures both the structural and algorithmic properties of the neocortex.\n\nIn spite of the big hype around deep learning possibilities, all that glitters is not gold. DL is for sure a great step ahead toward the creation of an AGI, but it also presents limitations. The greatest one is the exceptional amount of data required to work properly, which represents the major barrier to a wider cross-sectional application. DL is also not easy to debug, and usually, problems are solved by feeding more and more data into the network, which creates a tighter big-data-dependency. Furthermore, DL is quite useful to bring to light hidden connections and correlations but is not informative at all regarding the causation (the why of things).\n\nThe data need imposes a considerable amount of time to train a network. In order to reduce this time, networks are often trained in parallel, either partitioning the model across different machines on different GPU cards (model parallelism) or reading different (random) buckets of data through the same model run on different machines to tune the parameters (data parallelism).\n\nBecause of the limitations just mentioned, a series of other tools have been developed over the years. Particle Swarm Optimization (PSO) is a computational method that iteratively improves candidate solution to optimize a certain problem (Kennedy and Eberhart, 1995). The initial population of candidates (namely dubbed particles) is moved around in the search-space, and it has single particles that optimize their own position both locally and with respect to the entire search-space\u200a\u2014\u200acreating then an optimized swarm. Agent-based Computational Economics (ACE) is an additional tool that lets agents interacting according to pre-specified rules into simulated environments (Arthur, 1994). Starting from some initial condition imposed by the modeler, the dynamic systems evolves over time as interactions between agents occur (and as they learn from previous interactions).\n\nEvolutionary Algorithms (EA) are instead a broad class of techniques that find solutions to optimization problems through concepts borrowed from natural evolution, i.e., selection, mutations, inheritance, and crossover. An example of EA is the Genetic Algorithm (GA), which is an adaptive search heuristic that attempts to mimic the natural selection process (Holland, 1975). It is an evolutionary computing search optimization method that starts from a base population of candidate solutions and makes them evolving according to the \u201csurvival of the fittest\u201d principle. Genetic Programming (GP) is an extension of GA (Koza, 1992) because it basically applies a GA to a population of computer programs. It creates the chromosomes (i.e., the initial population of programs) made by a predefined set of functions and a set of terminals, and it randomly combines them into a tree-structure. In this context, the previous terminology acquires a slightly different connotation: reproduction means copying another computer model from existing population; cross-over means randomly recombining chosen parts of two computer programs, and mutation is a random replacement of chosen functional or terminal node. Evolutionary Polynomial Regressions (EPRs) are instead hybrid regressions that use GA to select the exponents of the polynomial, and a numerical regression (i.e., least square regression) to compute the actual coefficients (Giustolini and Savic, 2006). A final interesting model is called Evolutionary Intelligence (EI) or Evolutionary Computation (EC), and it has been recently developed by Sentient Technologies, LLC. It begins randomly generating trillions of candidate solutions (called genes) that by definition would probably perform poorly. They are then tested against training data, and a fitness score allowed the software to rank the best solutions (and eliminates the worst). Parts of the emerging candidates are then used to reassemble new populations, and the process restarts until a convergence is achieved.\n\nTo conclude this section, two additional approaches are worthy to be acknowledged. First, Generative Models (GMs) have been initially proposed by Shannon (1948), but recently brought back to the top by OpenAI, a non-profit AI research institute based in San Francisco (Salimans et al., 2016; Chen et al., 2016). This class of models is intuitively defined as those models we can randomly generate data for, assumed some hidden parameters. Once the data are feed, the system specifies a joint probability distribution and label sequences of data.\n\nSecond, Cao and Yang (2015) proposed a new method that converts the learning algorithm into a summation form, instead of proceeding directly from each training data point. It is called Machine Unlearning (MU), and it allows the systems to \u201cforget\u201d unwanted data. They actually introduce an intermediate layer of summation between the algorithm and the training data points, such that they will not depend on each other anymore, but only on the summations themselves.\n\nIn this way, they learning process is much faster, and it can be updated incrementally without training again the model from scratch\u200a\u2014\u200awhich is quite time-intensive and costly. Hence, if some data and its lineage want to be eliminated, the system does not need to recompute the entire lineage anymore\u200a\u2014\u200aa term coined by the two authors to indicate the entire data propagation network\u200a\u2014\u200abut it can simply recompute a small number of summations.\n\nCao, Y., Yang, J. (2015). \u201cTowards Making Systems Forget with Machine Unlearning\u201d. 2015 IEEE Symposium on Security and Privacy: 463\u2013480.\n\nKoza, J.R. (1992). Genetic Programming: On the Programming of Computers by Means of Natural Selection. MIT Press.\n\nMcCulloch, W. S., Pitts, W. (1943). \u201cA Logical Calculus of the Ideas Immanent in Nervous Activity\u201d. Bulletin of Mathematical Biophysics, 5: 115\u2013133.", 
        "title": "What you are too afraid to ask about Artificial Intelligence (Part I): Machine Learning"
    }, 
    {
        "url": "https://medium.com/@kurtisanderton/fail-fail-fail-win-eb17d27c46c?source=tag_archive---------1----------------", 
        "text": "Learning the hard way sucks!. I\u2019ve been reminded of this every time a try a new business idea. Fortunately these mistakes have been so important for me becoming who I am today.\n\nSome go to school and learn from those who\u2019ve gone before them. Some have mentors to guide them every step of the way. I on the other hand like to fail a hundred times and then realize what was in front of me the whole time.\n\nA lot of people might think that I\u2019m going about this whole thing totally wrong. Maybe I am, but I\u2019m learning a heck of a lot.\n\nShaper.com was this revolutionary idea my brother and I came up with that was going to change the fitness industry forever. The plan was to take paper right out of the client orientation process and replace it with software. This was the golden idea. The one that was going to change our lives and create that income we\u2019ve always dreamed of. This was the big one!\n\nDuring the launch of this site, I talked to over 20 personal trainers about shaper and how it was going to make business much more efficient. They agreed and couldn\u2019t wait for the site to be launched. This was all I needed to hear to make this project a reality and start making money.\n\nAfter over a hundred hours of coding and brainstorming done by my brother and I we finally completed the site. It was done and ready for the world.\n\nI sent it to our email list of trainers and waited for the money transactions to come through. For some weird reason not one person paid. They \u201cLiked\u201d the idea but had no intention of paying for it. Turns out they didn\u2019t mind using paper and transferring everything over online was going to be too much work.\n\nI was floored with frustration and shocked I made this mistake. What did I do wrong?\n\nThis was a huge experience for me. I learned that it\u2019s not enough that people like it. They have to NEED it. If we created something that those trainers needed and had to have, then paying for it wouldn\u2019t have been a question.\n\nThe next venture was a membership site that my brother created that offered Wordpress landing page themes. We made a FaceBook ad to validate the idea. We offered 50+ Themes for $9 a month. People loved this offer so much that we had over 50 people sign up and pay for the service before it was even complete.\n\nWe learned from our last mistake and made something people needed and would pay for up front. The mistake we made with this project was offering to many pages at the beginning. We realized it would be to hard to get all 50 themes done in time for our launch date. We decided to launch with 25 themes and give them a free month for signing up early.\n\nWe over promised and under delivered. This was another vital learning experience for us. Everyone was disappointed that they didn\u2019t get access to 50 themes right away and everyone slowly walked away from our service.\n\nWe needed to communicate more and next time offer 20+ themes for $9 a month and then launch with 50 themes. The best feeling ever is when you pay for something and get way more than you paid for.\n\nThis was an awesome few months of creation and hustling to get these projects launched. I will always remember these two projects. They are a big part of who I am today. I won\u2019t be making those mistakes again that\u2019s for sure!\n\nEventually we will get the right balance and create something people have to have. Until then I am going to keep trying, failing and growing.", 
        "title": "FAIL, FAIL, FAIL, WIN! \u2013 The Define Startup Show \u2013"
    }
]