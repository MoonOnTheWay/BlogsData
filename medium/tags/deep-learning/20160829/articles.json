[
    {
        "url": "https://medium.com/autonomous-agents/committee-of-intelligent-machines-unity-in-diversity-of-neuralnetworks-8a6c494f089c?source=tag_archive---------0----------------", 
        "text": "Have you noticed that the best fitness functions that most creatures adopt for survival is to work in collectives? School of fishes, Hive of bees, the nest of ants, horde of wildebeests or flock of birds all have something in common. They co-operate to survive. This is an example of Unity in Uniformity (when the same species act as a collective to form a fitness function)\n\nWhat is even more perplexing about nature is the ecological inter-dependence of different species, collectively surviving to see a better day. This fitness function is a sum of averages of sorts which enables a different form of collective strength. It\u2019s called Unity in Diversity. In essence, this signifies the \u2018unity without uniformity and diversity without fragmentation\u2019 when it comes to ecological fitness at any given time.\n\nHow do we apply this learning to AI? Specifically Neural Networks?\n\nIn the previous posts, we discovered how Neural Networks learn, and also observed that the models can begin overfitting the data. We learnt that the L1/L2 weight penalties, early stopping and weight constraints provide a mechanism to improve prediction accuracy while reducing overfitting.\n\nIn this post, we shall learn about better generalization functions based on how nature works, which is, Unity, or collaborative prediction. This is the first post in series and will establish the fundamentals of \u2018Committee of Machines\u2019 model.\n\nOne of the ways to reduce overfitting and improve prediction accuracy is to have a collection of Neural Networks (similar to nature) predicting the target value. The idea is to average the prediction errors in order to get a better prediction.\n\nAs shown in illustration, if each model predicts an output \u2018y\u2019 with an error distance d1 from the target (due to the absolute prediction value, or the weight of the ball, as a analogy), then it\u2019s possible to combine the variance in the distance to arrive at a better prediction.\n\nJust like in nature,\n\nThe idea behind combining various models is to reduce the variance in resulting prediction. We know that \u2018variance\u2019 is a function of the squared error distance from the mean prediction. The reason we look at squared error distance is because, we want predictions that are farther away from the target to produce a higher standard deviation and predictions which are closer to produce a lower standard deviation.\n\nSquaring moves the value of prediction errors into the realm of positive numbers (to the right of the target).\n\nThe illustration gives the idea that when you have more than one model trained on the same dataset used to predict the outcomes, the squared error distance shall produce, on average, a better output than any individual output.\n\nYou might wonder, why we are not retaining the model that is closer to the target and throwing the rest of the models away? Note that the same model can produce different errors in prediction for different inputs. The model may produce better results for some inputs more accurately than others. Similarly, you shall have other models producing different predictions for the same input. The idea is to get, on average, good predictions for all inputs.\n\nThe prediction vector for all inputs for a given model may be pointing to different directions in the resultant vector sub-space. The combination of all the directions of the prediction vector across all models achieve equilibrium by combining the average predictions as illustrated:\n\nTo find the average prediction, we apply the variance function as follows:\n\nThe above equation can be used as a error function to train the models. This concept is called co-operative error optimization.\n\nNote that we are comparing the average of all predictions with the target value to improve accuracy of prediction. This helps reduce variance and improve model accuracy.\n\nBut, this has a bad side-effect though. As the models co-operate to improve the average prediction, the equation lands up overfitting higher on average as well. This is because, we are taking the combined squared error over \u2018all models collectively\u2019. So this is not a good outcome for reducing overfitting.\n\nInstead, there is a better approach to improve generalization while improving accuracy. The concept is called a \u201cMixture of Experts\u201d.\n\nA better approach for improving prediction accuracy while improving model generalization is based on the concept of specialization. Instead of comparing the average of all predictors with a target value as a cost function, Hinton et al, proposed a better model where each predictor is compared individually and separately with the target value and average-out the \u2018probability\u2019 of selecting a expert based on the \u2018class\u2019 of input.\n\nOne of the ways to classify an input before prediction, is to use the internal representation of the input vector and cluster them based on \u2018un-supervised' learning. There are several techniques to cluster the input data as follows:\n\nOnce a class of the input is obtained, we can keep track of the probability of accuracy of a model participating in the \u2018Committee of Machines\u2019 and come up with a better cost function that can be used to train a Neural Net that is participative in the Committee.\n\nThe setup for a Committee of Machines looks as follows:\n\nHere, P1, P2, P3 are the probabilities assigned to the input class. The number of probabilities are equal to the number of experts. An expert can be any machine learning model that can predict the target value, given the input feature vector. Subsequently, y1, y2, y3 are the resultant prediction.\n\nThe manger is used for allocating the input vector based on its class to different specialized expert (a machine learning model) and is done by the softmax gating network. A softmax gating network can be a Neural Network which simply takes the raw inputs and spits out \u2019N\u2019 probabilities based on the \u2019N\u2019 experts we have. Note that N is just the number of experts and is not co-related to the experts in any other way.\n\nThe softmax function is a normalized exponential function that squashes a k-dimensional vector to a vector with real values between {0 and 1} which adds upto 1, and hence is a good gating function to identify the internal representation of the input vector and come up with a probability distribution over N.\n\nThe base loss function is as follows:\n\nNow, if you need a signal to train each expert, we have to differentiate the error w.r.t the output of each expert as follows:\n\nIf you need a signal to train the softmax gating network (which is supervised learning Neural Network), we have to differentiate the error w.r.t the output of the softmax gating network as follows:\n\nIn fact, Hinton et al > here, proposes a better way of coming up with the probability distribution for predicting the target value. They state that, we can think of the prediction made by each expert as a Gaussian distribution around their outputs with unit-variance. In that case, they propose:\n\nThen, the probability of a target can be thought of:\n\nIn fact, drop-outs as regularizers works on a similar concept as Committee of Machines. Instead of using different network for predicting the target value, it is proposed that for each training example:", 
        "title": "Committee of Intelligent Machines \u2014 Unity in Diversity of #NeuralNetworks"
    }, 
    {
        "url": "https://medium.com/@gansai9/tensorflow-post-deep-learning-with-tensorflow-ead67385b414?source=tag_archive---------1----------------", 
        "text": "Neural Network is all about learning to arrive at a function from the data. It tries to understand the data, so that, given some input, it arrives at the output, using the learned function. For example, the input could be some set of raw pixels of an image and the output could be a word, \u2018cat\u2019.\n\nDeep Learning involves automatically generating many layers of abstraction, during the course of learning the function. There are neurons in each layer, and each neuron implements a simple mathematical function. But the composition of millions and billions of these functions within this deep learning model is what makes it powerful.\n\nEach neuron in a layer, is connected to a bunch of neurons. They form the layers in the models.Higher Layers can form interesting combination of lower level of features that have been detected.The neurons in the machine learning models, learn to cooperate to accomplish a complicated task.\n\nBased on input raw set of pixels, some of the neurons in different layers will get activated and in the final layer, it might make a guess that its a \u2018cat\u2019. But with training, we make little adjustments of the neurons in different layers, so that finally, the model learns, in basically, the mathematical functions are adjusted, in order to almost correctly predict the correct answer.\n\nFunctions are simple\u200a\u2014\u200athey are like matrix multiplier or vector-dot product. The composition of mathematical functions, upto millions and billions of these, form a powerful deep learning model, which helps in recognizing some insights from the input- be it an image, an audio, or a video or simple text. So, these models (mathematical functions, in turn) have lots of floating point operations, they are computationally intensive.\n\nThe results of the models tend to get better if we train them with\n\nNew insights, Better algorithms and Good techniques always help to get amazing results from neural networks. But, most importantly, to work with large datasets, scaling is what is important at the core.\n\nIf a machine learning model is arrived at in a domain, it could be reused in another domain. For example, while solving some image processing problems, if a machine learning model is arrived at, it could be used in speech processing domain, with little bit of training and arrive at good results.\n\nIt is a system built by Google to express machine learning ideas, train them, deploy them in real world products. It is open source, has an apache license 2.0, released in Nov 2015. Caffe, Theano, Torch are other machine learning frameworks which have been around quite a while.\n\nTensorFlow has created a standard way to express machine learning ideas and computations. It is built out of DistBelief, older machine learning system, used within Google, scalable and production-deployable system, less flexible than TensorFlow.\n\nAt the core, TensorFlow is a graph execution engine. It is designed in such a way, that you can run these graphs on a variety of devices. It seamlessly allows you to move from one device to another, for example, you would have trained your model in a data center with lot of GPUs, but would actually run the same model by deploying it in an app in the smartphone. The core of tensorflow is written in C++.\n\nThere are different front-ends which are driving the computation\u200a\u2014\u200acurrently, it has C++ Frontend and a Python Frontend. There are various front-ends that are in the works, majorly contributed by the open source community.\n\nBasically, you build a graph to compute a neural network inference. mnist is kind of the \u2018hello world\u2019 of machine learning, the dataset where in there are bunch of images with letters and numbers and you build a machine learning model which recognize these letters and alphabets.\n\nTensorFlow has a feature\u200a\u2014\u200awhich is automatic differentiation, for example, we would have a machine learning model and we would want to optimize it with respect to a loss function, basically, we want to minimize the loss function. So, you can symbolically, differentiate the graph, so that you will know what updates need to be done to your machine learning model, so that, next time, likely, the model will actually be able to predict that it is a \u2018dog\u2019. For example, you train the model with \u2018gradient descent optimizer\u2019 in order to minimize the \u2018entropy loss\u2019.\n\nTensorFlow computation is nothing but a graph and what TensorFlow does, is that it executes this graph. So, basically, it\u2019s a graph execution engine. Each node in the graph is an operation, as you see the circular node. And each edge which flow across these nodes are nothing but the tensors\u200a\u2014\u200amulti-dimensional arrays with various kinds of types\u200a\u2014\u200alike 2d array of floating point numbers, 3d array of integers.\n\nThe underlying system can decide that, this part of the computational graph, can be submitted to a GPU and another part of the graph can be computed by another different CPU, as shown in above image.\n\nTensorFlow seamlessly, inserts the send and receive nodes as part of the computation dataflow graph, so that some tensors are sent to different devices and some are received at different devices, these send/receive node/operation makes tensorflow distributed. And these send/receive operation manages all the communication underneath and we don\u2019t get to really see them, as a user of the system.\n\nBecause of the ability to decompose the tensorflow computational dataflow graph, there is the possibility to run the same model on different kinds of machines, for example, smartphones, single machines (CPUs or GPUs), or a distributed systems of 100s of machines or GPU cards. OR you could have a custom machine learning hardware which is capable enough to perform the level of computation you need.\n\nSome of the uses of Deep Learning:-", 
        "title": "TensorFlow Post \u2014 Deep Learning with TensorFlow \u2013 GSP \u2013"
    }
]