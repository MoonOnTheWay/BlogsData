[
    {
        "url": "https://medium.com/@devnag/pointer-networks-in-tensorflow-with-sample-code-14645063f264?source=tag_archive---------0----------------", 
        "text": "The SQuAD task is a nice step on the way to linguistic AI; you basically get a medium-length text passage, a variety of questions about that passage, and then text answers. The catch \u200a\u2014\u200aand what makes this task more \u2018feasible\u2019 than full-fledged Q&A\u200a\u2014\u200ais that the answer has to be a contiguous sequence of letters or words in the original passage. In other words, the answer to any of the questions has to be a set of two pointers : one pointer to the start of the \u2018answer range\u2019 and one pointer to the end (say, from character 47 to character 65, or word 14 through word 17).\n\nI recently read a paper that described a new state-of-the-art result on the Stanford Question Answering Dataset (SQuAD) . The performance (F1 of 70%) was impressive, but especially interesting was an architectural capability that I hadn\u2019t seen when it came out a year earlier in the literature\u200a\u2014\u200athe ability to compute variable-length probability-distributions with a fixed architecture, over arbitrary inputs .\n\ntl; dr: Deep learning networks can be applied to variable-length targets, meaning you can index into arbitrary text, time series, or any sequence of data your selfish little heart desires\n\nObviously, not all questions can be answered in such a way, so this is something of an \u201ceasy subset\u201d of all possible, relevant questions (\u201ceasy\u201d being relative, of course). But if you actually peruse the dataset over a chilled glass of Pinot Grigio, it\u2019s fascinating just how many meaningful questions fit into this merest haiku of a linguistic task.\n\nNow, let\u2019s describe the naive way that you might try to attack this problem. Typically, your output layer will be a vector that\u2019s either a distributed representation (maybe a so-called \u2018sentence vector\u2019 or \u2018thought vector\u2019) or a one-hot representation (representing a probability distribution over the set of vector slots/dimensions).\n\nYou have to pick an output size. You have to train that final matrix with some fixed output dimensions (whether 10 or 10,000) and that\u2019s the size of the output vector you\u2019re going to get.\n\nHow do we apply this to SQuAD? What if one passage is 300 words long and another is 3,000 words long? Maybe you should pick the longest length passage you\u2019ll accept (say, 10,000) and just hope that 1.) you won\u2019t be wasting too much computation time training shorter passages on a giant architecture and that 2.) learning will actually transfer well across different passage lengths inside this fixed-length box. Unfortunately, this approach is just as fragile as it sounds.\n\nThe state-of-the-art SQuAD paper described above (Wang & Jiang, 2016) used some common designs (heavy use of bidirectional LSTMs, alignment matrices as used in translation tasks, etc.) but also mentions a technique called Pointer Networks that is, indeed, that better way.\n\nHere\u2019s the basic idea, architecturally: we\u2019ll train a fixed-size network but map it over variable-length input to get variable-length output (pointers).\n\nTo do this, we start with the sequence-to-sequence design pattern explained here, folding the input sequence (whatever the length) into a fixed-size hidden state. Then, we\u2019ll unfold that hidden state into a series of soft \u2018pointers\u2019\u200a\u2014\u200aprobability distributions over the input sequence. In the SQUAD example above, as well as our coming example, there are two pointers (start and end), so we unfold the hidden state twice\u200a\u2014\u200athe first time, to get the \u2018start pointer\u2019 probability distribution over all the inputs, and the second time, to get the \u2018end pointer\u2019 probability distribution over all the inputs. (If we had a different problem that required one or three or fourteen pointers, we would have to unfold the hidden state one or three or fourteen times.)\n\nHere\u2019s a schematic of the whole pipeline:", 
        "title": "Pointer Networks in TensorFlow (with sample code) \u2013 Dev Nag \u2013"
    }, 
    {
        "url": "https://gab41.lab41.org/850k-images-in-24-hours-automating-deep-learning-dataset-creation-60bdced04275?source=tag_archive---------1----------------", 
        "text": "Normally this computer vision adventure would start with the protagonist scouring the internet to find dataset owners. These individuals have already gone through the trouble of amassing a large number of images, looked at each image, applied labels and/or tags for each image. This individual would have packaged things up for their own purposes and probably had the labeling work performed by indentured graduate students.\n\nIf you are lucky some small percentage of each corpus located relates to the feature/item that you yourself are attempting to amass.\n\nWash, rinse, repeat until you have amassed a corpus large enough to satisfy your personal data lust.\n\nI would like to bypass corpus searching, downloading, and triage by automagically making my own labeled data. I want to make a dataset and only spot check a small number of images.\n\nI am building a classifier and need a healthy number of images in a short amount of time. The classifier will be used in an upcoming project named Pelops. The classifier will take an image and determine the body type of a vehicle i.e. subcompact, compact, car, van, SUV, crossover, etc.\n\nI initially started thinking of sites with published APIs to call, like Edmonds, where you can specify the make and model of a vehicle. This approach would allow me to get images by make/model and I could use another site to ask for images by body type.\n\nSecondly I thought of hierarchical sites similar to Craigslist, where I could grab images and mine the posting for make and model. (Mining the posting was something I was not looking forward to.)\n\nIt became apparent that I would need to work across several sites to get the number of images I wanted. Each site would need an interface to the API or customized scraping code to bring in the images.\n\nIt was getting close to coding time; I needed to channel my inner lazy developer and think of ways of getting images on my terms. I definitely wanted to hand a list of attributes to something and have it return a series of images. A colleague and I thought that this is perfect occasion to abuse a search engine. I can supply color, make, and model; the search engine would mostly return images related to my query.\n\nRunning with this idea I found an article of the top 10 car colors, along with a GitHub repo where I could quickly extract around 1000 makes and models of vehicles.\n\nWriting code to query/scrape a search engine then download images from a search engine took less than a half hour. Running the script, creating the list of images to download took 5 hours (search engines get persnickety if scraped too quickly).\n\n(100 images * 1046 make/models * 10 colors ) would mean I would hopefully amass around a million images. Whether they would be useful images was something for future me to worry about.\n\nThe actual image downloading took over 10 hours. Some sites were unresponsive or slow to download.\n\n[NOTE: Some of the material you download may be copyrighted]\n\nEven though I asked for images, sometimes you don\u2019t always get what you want. I was not performing file type checking at download time. Spot checking showed I was sometimes downloading HTML instead of the image. The number of non-images was small compared to the number of images, though.\n\nI needed a program to detect if a file was indeed an image or not. The program was pretty speedy and cut through the million plus images in around 7 minutes.\n\nRemoving the not-images led to the next issue; some of the downloaded files were not images of cars. Not wanting to personally Mechanical Turk over 950k images, I turned to using a pre-trained computer vision model to help me out.\n\nI made use of Keras and a pre-trained neural network (resnet50) to determine if a car was in the image. If one of the following terms [\u2018car\u2019, \u2018truck\u2019, \u2018suv\u2019, \u2018ambulance\u2019,\u00a0\u2026] was in the top 4 labels returned for the image I kept the image.\n\nRunning the images through a GPU assisted deep learning model took around 8 hours. The model evaluated approximately 2000 images/minute on a single Titan X.\n\nAfter the final pass, about 840k images were remaining. A cursory look through the data showed mostly cars.\n\nMy final lazy idea for supplementing image labels comes via even more search engine abuse. I wanted labels of car, SUV, truck and/or van for each vehicle. To see if a specific make model of a vehicle should have the label, simply search for it and count the results.\n\nAttempting to classify the Ford Mustang I would perform 4 searches:\n\nI would then capture the number of results returned by the search engine.\n\nYou could take the largest value as the label, or use the array as vector to describe the class. I bet the vector would be better because of cars like the Chevrolet El Camino.", 
        "title": "850k Images in 24 hours: Automating Deep Learning Dataset Creation"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/8-exponential-hockey-stick-charts-for-deep-learning-74bba7a0284c?source=tag_archive---------2----------------", 
        "text": "Some people just don\u2019t grok it, no matter how hard you bang the table.\n\nBut maybe some hockey stick graphs will help.\n\nIt all begins with this chart:\n\nIn 2010 image recognition would fail at least on fourth of the time, but by 2015, image recognition surpassed human recognition! What is driving this? Something definitely is brewing.\n\nSomething called \u201cDeep Learning\u201d began emerging in 2012. So let\u2019s check Google search trends that track interest:\n\nIt turns out that the academic community has something really brewing.\n\nhere is another conference but with some commentary:\n\nBut is this all theory? Let\u2019s take a peak inside Google:\n\nWhat\u2019s the demand for talent like?\n\nWhat about what the 1%ers are investing in?\n\nWhat are the companies acquiring?\n\nIs Deep Learning the main driver of Artificial Intelligence?\n\nNow for the most important trend:\n\nNow, the fact that you are reading this tells me that you\u2019ve heard of \u201cDeep Learning\u201d. It is likely 99.9% of the population have no clue as to what it is. This gives you a head start and I do recommend that you take action on this knowledge advantage.\n\nSign up at Intuition Machine to get the conversation going or discuss at FaceBook or LinkedIn.", 
        "title": "Deep Learning Exponential Growth Trends \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/@mslavescu/how-to-build-a-robot-that-sees-with-100-with-raspberry-pi-and-tensorflow-e2f4feceed53?source=tag_archive---------3----------------", 
        "text": "Want to build your toy open source self driving car, this is a very affordable, approachable and feature rich way\u00a0to\u00a0start!\n\nRead this whole amazing story here.\n\nJoin me in building the future through the Open Source Self Driving Car (toy\u00a0and\u00a0real\u00a0cars) Initiative:\n\nFollow our progress on twitter at #OSSDC and @gtarobotics", 
        "title": "How to build a robot that \u201csees\u201d with $100 (with Raspberry Pi) and TensorFlow"
    }, 
    {
        "url": "https://medium.com/@sanjeevn72/a-i-nlp-is-changing-banking-for-good-for-everyone-640a56129160?source=tag_archive---------4----------------", 
        "text": "You are on a two week long business trip across multiple locations. Its been 5 days and you are wondering how much you current trip has cost you yet. You tap on your banks app on your phone and type in\u200a\u2014\u200a\u2018What\u2019s my spend in New York last week?\u2019\n\nA bot answers back \u2018a total of $343.54\u2019 followed by \u2018would you like to expense it?\u2019\n\nYou say, yes. The bot promptly marks it to your expense list to submit to your company and sends you an email report of the same.\n\nYou follow up by asking\u200a\u2014\u200awhats the status of that loan disbursement? The bot answers, Its scheduled to be disbursed this Wednesday.\n\nLets now switch to another time.\n\nYou ask the bot for options on investments, and the bot responds to you \u2018based on your profile and spending patterns, the gold saver plan is the ideal investment option. I\u2019m assuming you are looking at medium term options?\u2019\n\nA more urgency driven scenario is one where you notice a transaction that was not authorised by you. You initiate the conversation with the bot\u200a\u2014\u200a\u2018I did not make the last transaction on my card\u2019 The bot replies- Okay Tim, please do confirm your identity by naming your dog and your first school.\n\nBot: thank you for confirming your ID, I have marked the last transaction as fraudulent and will update you on our findings on the same.\n\nBot: If you still have your card with you, I recommend that you choose the OTP option for all future transactions on your card.\n\nThe interactions between you and your bank (bot) are extremely personalised, and relevant to your needs. More importantly, its banking made easy and on the go. This is where digital banking can be at its best.\n\nEveryone has been talking about the next big revolution in IT which is being driven by artificial intelligence (AI); mainly natural language processing (NLP) and machine learning. Doomsday predictions of mass layoffs and machines taking over jobs are prevalent. However these predictions are quite misplaced. The technology is here to enable us to do things better, increase efficiency, productivity, and build on the quality of core services overall.\n\nBanking is an area that touches everyones lives, but is mostly viewed as a critical/necessary yet intimidating, difficult task that has to be dealt with. The key reasons being lack of understanding of the financial terms, the intricacies in the product or service offerings, and the related processes and norms that need to be followed to make the most of your investment or savings.\n\nIT investments in Big Data systems have resulted in huge amounts of historical and real time data that is not yet been fully utilised to build for scale and efficiency.\n\nAI powered businesses can process \u201cbig data\u201d far more efficiently than humans, deep diving with the capabilities to recognise speech, images, text, patterns of online behaviour, for example to detect fraud, identify social influencers, product feedback and more. AI powered \u2018intelligent\u2019 technology can leverage available data to derive customer insights that can help enhance service offerings, thus bringing the digital experience closer to real world human interactions for every consumer.\n\nOn the point of efficiency and scale, A single web assistant implemented in a bank in Europe achieved an average of 32,000 conversations per month and first-contact resolution of 78% in its first three months, handling over 350 different customer questions and answers. And this instance is based on just the supervised learning capabilities of AI. Time taken for customer feedback to get to business leaders is down to milliseconds if not lesser.\n\nMachine learning technology has advanced rapidly over the last ten years, and there are now more flexible and cost-effective solutions that banks can implement, even with their often legacy-burdened IT systems. Computers can quickly analyse new information and compare it with existing data to look for patterns, similarities and differences. By repeating the activity, the machine improves its ability to predict and classify information making it easier to make data-driven decisions.\n\nBanks and fintech companies already use machine learning to detect fraud by flagging unusual transactions, as well as for other purposes. Its far more efficient than manual monitoring and is soon to become the norm in banking and finance. Even with the latest in technology, we still have human powered customer support centres falling prey to fishers and hackers as seen in the video.\n\nWhy will it work? Consumers, particularly millennials, increasingly prefer digital banking instead of phone or walk-in as they are most adoptive of AI in other areas of their lives\u200a\u2014\u200acase in point being voice assistants on their home devices and phones. AI applied to customer servicing is also a big opportunity for retail banks to increase automation and reduce the cost of serving customers, which makes it an attractive option. Also, first contact management limitations in terms of peak traffic is a thing of the past.\n\nFor customers, the technology will simplify money management and will also offer suggestions and recommendations on relevant and new services that are matched by algorithms.\n\nMost of the AI implementations behind the scenes are made accessible to consumers via BOTS and Smart / NLS\u200a\u2014\u200awhat is not evident to most is the huge technology push towards making the bots smarter by doing things such as analysing data, making personal recommendations and semantically understanding human language and emotion, which in turn leads to even better user experiences across every user interaction touch point with the business.\n\nThe growth of automated services, AI and robotics has heightened the need for traditional banks, financial services and payment providers to work closely with experience designers, coders, developers and marketers to ensure new concepts are identified, developed and commercialised professionally and effectively.\n\nBottomline: The advantages cannot be ignored.\n\nTo know more about how Light Information Systems builds and deploys NLPBOTS to help banks, insurance companies, logistics and e-commerce companies run intelligent automated customer engagement solutions, feel free to get in touch.", 
        "title": "A.I. NLP is changing banking for good. For everyone."
    }
]