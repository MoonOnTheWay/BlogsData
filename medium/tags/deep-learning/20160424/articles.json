[
    {
        "url": "https://hackernoon.com/chat-live-with-the-characters-from-silicon-valley-show-on-luka-4f7efbd91fed?source=tag_archive---------0----------------", 
        "text": "Chat live with the characters from \u201cSilicon Valley\u201d show on\u00a0Luka\n\nWhat makes you you? What makes your friends know immediately that it\u2019s you texting them and not someone else? It\u2019s not just things you say\u200a\u2014\u200ait\u2019s the choice of words, the length of the sentences, whether you send short texts in a row or one long message, the emotion that can be sensed in your words.\n\nOne of the reasons we started Luka was a desire to create an AI with personality. A huge part of our experiments with neural networks was to learn how to recreate it.\n\nToday we are launching AI bots that will allow you to have a conversation with the characters of \u201cSilicon Valley\u201d show (its season 3 is premiering tonight): the incubator owner Erlich Bachmann, Russ \u201c3 commas\u201d Hanneman and Pied Piper founder Richard Hendricks.\n\nThey are all powered by a neural network dialogue model that we developed here at Luka. We trained our deep neural network to understand the sense of sentences and generate the most relevant responses based on subtitles and tweets.\n\nWe picked these three because we love the show, but actually they were harder to recreate than other celebrities\u200a\u2014\u200amainly for a very small amount of available data. Everything that Russ Hanneman (Pied Piper investor) tweeted and said on the show only comes to a little over a thousand lines\u200a\u2014\u200aand we were still able to recreate his personality. Think about those who tweet, speak and write every day\u200a\u2014\u200atheir AIs will be significantly more powerful.\n\nThese bots are our first steps exploring personality as part of conversational AI. We are working on features that will allow AI to evolve more in conversations with people. You will be able to help AI develop the features you like, teach it facts you want it to know, it will pick up your style and your emotions better.\n\nThere has been a lot happening around bots recently -but most of them don\u2019t even understand natural language. We, on the other hand, can\u2019t wait to see what is the true power of a conversation. Or, as CEO of Hooli once said, \u201cI don\u2019t know about you people, but I don\u2019t want to live in a world where someone else makes the world a better place better than we do\u201d.\n\nTalk to Erlich, Russ and Richard AIs here and write us what you think!", 
        "title": "Chat live with the characters from \u201cSilicon Valley\u201d show on Luka"
    }, 
    {
        "url": "https://medium.com/@ericflo/why-i-made-an-open-source-model-zoo-d9bda255b7d6?source=tag_archive---------1----------------", 
        "text": "\u201cThe Matrix is the best movie ever created.\u201d It\u2019s a line I say to get a reaction, because that\u2019s fun and it usually does, but it also happens to be my honest opinion. The truth of the matter is, since the time I first saw it, I\u2019ve been fascinated by virtual reality and how it will affect our society.\n\nIn college circa 2007, I took an advanced computer vision class, which was both fascinating and sad. I was saddened to learn that, at that time, it seemed like mostly parlor tricks and hacks that barely worked\u200a\u2014\u200aeach algorithmic tool in the computer vision toolbelt was painstakingly hand-coded and tested until it mostly worked. Mostly.\n\nBut today I work in the nascent virtual reality industry, and through the lens of that, have noticed the wide and varied successful applications of computer vision for things like position tracking, geometry reconstruction, pose estimation, etc. How is all this possible using those parlor tricks we learned about in school?\n\nStanford recently open sourced their latest machine learning class, CS231n: Convolutional Neural Networks for Visual Recognition, and not only put all the videos and lecture notes online, but made the homework standalone in a way that you can go through it yourself without checking in with the classroom. I devoured this entire course. It\u2019s absolutely fascinating. Please check it out.\n\nIf you\u2019re reading an article with this title though, you probably know that something interesting happened in 2012, a renewed interest in convolutional neural networks (CNNs) began which has not yet ended and is reshaping the field of computer vision. I wanted to try my own hand at building a CNN, and chose to attempt stereo reconstruction from a monoscopic camera. In essence, to try to extract 3D from 2D.\n\nI set up a simple Keras model and started it training. First it would just produce static. After a while the static started to take shape, starting to have a ghostly form. A day later and you could tell that shape was looking like the input photo. A second day of training later and it was still making progress. The next day when I woke up, my computer was shut down. There had been a power surge the night before, and the latest saved weights file was corrupt. Time to start over.\n\nThis time, I decided, let\u2019s start with a pre-trained VGG or AlexNet and fine tune from there\u200a\u2014\u200athat will surely improve training time and probably produce better results. There were lots of models out there, but the weights were on random dropbox accounts, all of which seemed to be over their capacity. I managed to find one weights file, but there were no instructions on how to initialize the model with these weights. I eventually found out the weights needed to be processed first.\n\nThis whole process is busted. There should be an easy way to periodically save weights and have them shuttled to the cloud, available for safe use later. Once they\u2019re on the cloud, let\u2019s make a standard way of loading them up, easy enough to bootstrap your own efforts. This should be simple stuff.\n\nNow it is: introducing Gradientzoo, an open source model zoo with integrations so far for Keras, Lasagne, Tensorflow, and plain Python. This is definitely an MVP: all it does is provide a unified API, it saves and loads files, keeps several versions of them, and periodically cleans out old versions based on your project settings.\n\nEverything in the project is fully open source, meaning you can run this model zoo on premise as a private model zoo, or you can run a public one on the open internet. But I hope you choose to support the one at gradientzoo.com, because I believe if we unify and are more open with these trained models. That if it\u2019s easier to make a running start training new models composed of existing pre-trained ones, we\u2019ll progress much faster and further as an industry.\n\nLet me know what you think\u200a\u2014\u200aI\u2019ll read all responses to this post and any e-mail that comes in on thoughts@gradientzoo.com. I\u2019m especially interested to hear what you think about pricing and how to make the service sustainable!", 
        "title": "Why I made an open source model zoo \u2013 Eric Florenzano \u2013"
    }
]