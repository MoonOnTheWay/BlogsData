[
    {
        "url": "https://hackernoon.com/how-to-run-text-summarization-with-tensorflow-d4472587602d?source=tag_archive---------0----------------", 
        "text": "Text summarization problem has many useful applications. If you run a website, you can create titles and short summaries for user generated content. If you want to read a lot of articles and don\u2019t have time to do that, your virtual assistant can summarize main points from these articles for you.\n\nIt is not an easy problem to solve. There are multiple approaches, including various supervised and unsupervised algorithms. Some algorithms rank the importance of sentences within the text and then construct a summary out of important sentences, others are end-to-end generative models.\n\nEnd-to-end machine learning algorithms are interesting to try. After all, end-to-end algorithms demonstrate good results in other areas, like image recognition, speech recognition, language translation, and even question-answering.\n\nIn August 2016, Peter Liu and Xin Pan, software engineers on Google Brain Team, published a blog post \u201cText summarization with TensorFlow\u201d. Their algorithm is extracting interesting parts of the text and create a summary by using these parts of the text and allow for rephrasings to make summary more grammatically correct. This approach is called abstractive summarization.\n\nPeter and Xin trained a text summarization model to produce headlines for news articles, using Annotated English Gigaword, a dataset often used in summarization research. The dataset contains about 10 million documents. The model was trained end-to-end with a deep learning technique called sequence-to-sequence learning.\n\nCode for training and testing the model is included into TensorFlow Models GitHub repository. The core model is a sequence-to-sequence model with attention. When training, the model is using the first two sentences from the article as an input and generates a headline.\n\nWhen decoding, the algorithm is using beam search to find the best headline from candidate headlines generated by the model.\n\nGitHub repository doesn\u2019t include a trained model. The dataset is not publicly available, a license costs $6000 for organizations which are not members of Linguistic Data Consortium. But they include a toy dataset which is enough to run the code.\n\nYou will need TensorFlow and Bazel as prerequisites for training the model.\n\nThe toy dataset included into the repository, contains two files in \u201cdata\u201d directory: \u201cdata\u201d and \u201cvocab\u201d. The first one contains a sequence of serialized tensorflow.core.example.example_pb2.Example objects. An example of code to create a file with this format:\n\n\u201cvocab\u201d file is a text file with the frequency of words in a vocabulary. Each line contains a word, space character and number of occurrences of that word in the dataset. The list is being used to vectorize texts.\n\nRunning the code on toy dataset is really simple. Readme on GitHub repo lists a sequence of commands to run training and testing code.\n\nYou can run TensorBoard to monitor training process:", 
        "title": "How to Run Text Summarization with TensorFlow \u2013"
    }, 
    {
        "url": "https://medium.com/@felipeampuerosalinas/es-google-photos-el-pr%C3%B3ximo-google-de-google-d75742b643aa?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u00bfEs Google Photos el pr\u00f3ximo Google de Google? \u2013 Felipe Ampuero Salinas \u2013"
    }, 
    {
        "url": "https://medium.com/@mentificium/open-source-strong-ai-cognitive-architecture-5bd81f69e07d?source=tag_archive---------2----------------", 
        "text": "The cognitive architecture of MindForth and the Perl Webserver Strong AI is depicted in the following diagram.\n\nAbove we see a description of the planned development of the AI Minds in Forth and in Perl. Some parts of the MindGrid have not yet been fleshed out in hardware or software.\n\nIt is noteworthy that the Volition module first calls the Emotion module so that emotions may influence the function of the Think module, which provides the ideas for the FreeWill to command the Motorium module.\n\nThe VisRecog module for visual recognition needs robot embodiment with a camera for an eye, so that the thinking AI Mind may think and talk about the external world. The above diagram shows how the AI would generate the sentence \u201cI see birds\u201d by taking the recognition of birds from the VisRecog module and filling in the direct object of the verb \u201csee\u201d with the word \u201cbirds\u201d in the NounPhrase module.", 
        "title": "Open-Source Strong AI Cognitive Architecture \u2013 A.T. Murray \u2013"
    }, 
    {
        "url": "https://medium.com/@sugarsteroni/embodying-ais-28e0e749bf21?source=tag_archive---------3----------------", 
        "text": "People learn by being embodied in the world. There has been a long-running misunderstanding across science that the goal of the brain is to build representations of the world. From an evolutionary perspective this would be extremely inefficient.\n\nMinds don\u2019t build representations of the world, but representations of a body\u2019s interaction with the world. One of the very handy properties of the world is that when you turn your back on it, it stays in the same place, and you can quite dependably forget all about it, because when you go back everything will be just where it was. It\u2019s the things which move or change which you need to keep a track of, and that\u2019s what intelligence and awareness does.\n\nDreams are interactions of the mind with itself. One common difference to waking experience, along with a lack of awareness, is their lack of embodiment in the world. In the real world, a tree is fixed and rarely moves. But that\u2019s not true in dreams. You can turn around and when you turn back, the tree is a house or a wardrobe or very curiously-shaped elephant.\n\nAI has made leaps and bounds in the last years, powered by exponentially increasing processing power, data availability and algorithmic improvements. Yet, whether the field is computer vision, machine translation or text processing, the paradigm remains constant: AI is required to learn from the data exhaust of humans. To learn vision, AI trawls through millions of photos. To learn to write down speech, a convolutional net is trained on thousands of hours of transcribed speech. To learn to translate, recurrent nets are trained on millions of couple of sentences of speech in one and out the other.\n\nThis is completely different from embodying an AI in a real\u200a\u2014\u200aor simulated\u200a\u2014\u200aworld. A baby learns that the world is invariant to its movements, or learns to model the things which are not. In deep learning, we spend a lot of time thinking about how to expose the net to the right variants of images. Typically thousands of variants are required to learn to recognise an object with a satisfactory level of accuracy, and still a long way from human accuracy. We spend a lot of time thinking about how to generate the correct training data. A lot of machine learning researchers think that one of largest constraints is access to training data. Shifting the paradigm from reading the entrails of human data to embodying an AI in a world could change all of that.", 
        "title": "Embodying AIs \u2013 Robin Allenson \u2013"
    }, 
    {
        "url": "https://medium.com/@smartwatch/how-baidu-just-paddle-platform-for-open-source-be75e540a69e?source=tag_archive---------4----------------", 
        "text": "How Baidu just Paddle platform for open source\n\nPaddle depth, Baidu today open source learning platform has raised very much developer interest in the field of artificial intelligence, including on-the-job training in Tensorflow and Caffe until some developers. But given the deep learning open source platform is not much, masses are as keen to eat melon as a development priority, just want to know\u2013how about this platform? What others think of this platform? And this platform different from Tensorflow as well as Caffe?\n\nHow do \u258e the platform itself\n\nHave existed before the Paddle itself in open source, beginning in 2013, when Baidu depth depth laboratory realized in neural network training, along with computational advertising, text, images, voice training, such as the rapid growth of data, traditional training platform based on single-GPU has been unable to meet demand, led by Xu Wei, the lab built Paddle (Parallel Asynchronous Distributed Deep Learning) how parallel GPU this training platform.\n\nBut today, open source simple model of the Paddle is not 3 years ago, Paddle 3 years ago may also be a deep learning platform-independent, not well supported from other platforms, data access needs. But today\u2019s Paddle is already stressed, it features a Spark with the PADDLE is coupled, a depth of heterogeneous distributed system based on Spark. And after and Baidu-related business of \u201ctight friction\u201d, it has iteration two versions: from the Spark on Paddle schema version 1.0, to Spark on PADDLE schema version 2.0. According to the platform open source rules, presumably in Baidu is very handy for internal repairs after a series of bug lab was finally going to Spark on PADDLE and heterogeneous computing platforms are open source. As to why Baidu to raise revenue, and the reason we all know\n\nDeep learning platform, there are many bug\u2013to attract more developers to try and use advanced learning technology, certainly help to improve the grade of Paddle.\n\n\u258e Evaluation of outsiders on the platform\n\nKnow Jia Qingyang answer, is now more positive evaluations.\n\n3. the design is clean, not too much abstraction, it is much better than TensorFlow. Juicy Couture\n\n4. high speed RDMA part seems to be no open source (possibly because the RDMA for cluster design has certain requirements): Paddle/RDMANetwork.h at master \u00b7 baidu/Paddle \u00b7 GitHub\n\n5. design thinking more like first generation DL framework, but the paddle has been for years, also has historical reasons.\n\n5.1 config is hard-code protobuf message, the extension could have an influence.\n\n5.2, you can see many interesting designs similar to the legacy: using the STREAM_DEFAULT macro, and then directed to a non-default TLS way stream:Paddle/hl_base.h at 4fe7d833cf0dd952bfa8af8d5d7772bbcd552c58 \u00b7 baidu/Paddle \u00b7 GitHub (Paddle off-the-shelf Mac are not supported? \uff09 Juicy iPhone 5 case\n\n5.3 in the gradient using the traditional coarse-grain forward/backward design (similar to the Caffe). One might say \u201cso paddle does not auto gradient generation\u201d, this is not the way, autograd existence regardless of the size weight and op. In fact, the TensorFlow aware of fine-grained operator super slow speed, gradually back to coarse-grain on the operator. Currently only see here. All in all was a very solid framework, Baidu development skills are good.\n\nEstimated evaluation of many people read Jia Qingyang, one Baidu data before we post the following engineers perspective CTO Ying Ning\u2019s evaluation\n\nLooked at from design concepts and Caffe is like, but the network model is not as easy to define as Caffe. Maximum contribution is made, distributed, improves the model\u2019s speed. Details will have to look at the code and get started again.\n\nAnother with the above two opinions contrast a deep learning scholars\n\nTensorflow schema can be thought of as an upgraded version of theano, theano years earlier than the Caffe, but Caffe first train out, and released a successful of convolutional neural network model to get more attention. Relationship between Tensorflow and Caffe fine much, could learn from the Caffe some implementation techniques, essentially nothing. Baidu is likely to be seen after successful implementation of Caffe, much imitated Caffe, while trying to change some things to make it look different from Caffe.\n\nI assume Caffe who directed it, who uses other tools (tensorflow, keras, theano, torch,mxnet) are not into it, say for a few days and then\u00a0\u2026\u00a0\u2026 Github attention look at it in a month and the amount of code on github to find someone else to write what you know whether he could spray (later no one can see him attend kaggle or other games or in scientific publishing). Now each company released its own deep learning framework (or machine learning framework), such as Microsoft, Amazon, Yahoo, seems to have been no major movement.\n\n\u258e Different from the platform with Tensorflow and Caffe\n\nLei feng\u2019s network (search for \u201cLei feng\u2019s network\u201d, public interest) applied for a Paddle public beta today, are also being reviewed, although not direct download experience, but the difference between the other two platforms and is not without a trace. Our Caffe before, Tensorflow understand, as well as Paddle data released today.\n\n1) Caffe can be said to be the first industrial grade deep learning tool, was founded at the end of 2013 prepared by UC Berkely Jia Yangqing development language a CNN implementation of excellent features, Caffe in computer vision area is still one of the most popular Kit.\n\nCaffe development language C++ and Cuda, very fast, but due to some remnants of historic architecture, its flexibility is not strong enough. And support for recurrent neural networks and language modeling is very poor. Caffe supports all major development system, use a moderate difficulty level.\n\n2) Tensorflow is a second generation Google open source advanced learning technology, RNN API implementation is an ideal, it uses the symbol method for vector operations, develop speed quickly.\n\nTensorflow better system only supported by various Linux systems and OSX, but its support for the language is fairly comprehensive, and includes Python, C++ and Cuda, developers document is not as comprehensive as Caffe, so use more difficult.\n\n3) and this time Baidu\u2019s Paddle, as the depth of heterogeneous distributed system based on Spark, through the use of GPU and FPGA heterogeneous computing upgrade data-processing capacity per machine, temporarily access to the industry\u2019s \u201cfairly simple, clean design, stable, fast, occupying smaller video memory. \u201cEvaluation of it by using the GPU and FPGA each machine data processing and heterogeneous computing capability has important contacts. But how, still need to wait a few days to observe people experience.", 
        "title": "How Baidu just Paddle platform for open source \u2013 Smart Watch \u2013"
    }
]