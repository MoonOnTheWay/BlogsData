[
    {
        "url": "https://medium.com/emergent-future/cognitive-toolkits-artsy-ai-solar-roofs-and-autonomous-trucking-66069bfe4e77?source=tag_archive---------0----------------", 
        "text": "You Might Have Heard: Microsoft launched Cognitive Toolkit 2.0 beta last week, the next version of its deep learning framework.\n\nPreviously known as CNTK, it was initially developed by Microsoft developers who wanted a tool to do their research more quickly and effectively. The latest version of the toolkit, available on GitHub, includes new functionality and support for Python and C++.\n\nBut Did You Know: AI Pioneer Yoshua Bengio is launching Element AI, a deep learning incubator.\n\nThe incubator will help build companies from the AI research that emerges from the University of Montreal, where Bengio is also a professor, and at nearby McGill University.\n\nBengio says this is just part of his efforts to develop an ecosystem for AI in Montreal.\n\nThat\u2019s why we\u2019re working with MIT, University of Washington, Carnegie Mellon University, University of California Berkeley, Caltech, University of Texas at Austin, University of Tokyo, University of Toronto, and others to make state-of-the-art algorithms accessible and discoverable by everyone.", 
        "title": "Cognitive Toolkits, Artsy AI, Solar Roofs, and Autonomous Trucking"
    }, 
    {
        "url": "https://medium.com/@richardherbert/generating-fine-art-in-300-lines-of-code-4d37218216a6?source=tag_archive---------2----------------", 
        "text": "[Full code for the model can be found here]\n\n[This article was originally posted here]\n\nStumbling onto Alec Radford\u2019s deep convolutional generative adversarial network, or DCGAN, was one of the few genuinely jaw-dropping moments I\u2019ve experienced in my life. It needs no further introduction; visit the link and see the madness.\n\nOr check it out right here:\n\nThe above images, clearly of bedrooms, were not captured by cameras, but by statistics. They are generated from what you might call the \u201cprobability space\u201d of bedroom pictures; the statistical distribution that contains all the various features you would expect to find in a picture of a bedroom, neatly separated so that a random sample from said distribution can be \u201cmorphed\u201d into something that resembles a photograph.\n\nThe way this is done is both shockingly simple and dreadfully complicated. It\u2019s simple in that it doesn\u2019t require a ton of code, appears to work on various kinds of image data, and can be run on a medium-grade GPU to start producing interesting results in a few hours. It\u2019s complicated because modifying, debugging or adding to the model requires understanding the myriad kinks of generative adversarial networks, which are slightly odd beasts.\n\nI have stitched together the Torch implementation of DCGAN and Kaixhin\u2019s variational autoencoder, the latter of which performs variational inference on complicated integrals like the ones that define photographic probability distributions and can be trained with vanilla stochastic gradient descent. By tinkering with the heuristics, I was able to successfully generate fine-art-like images from the wikiart.org dataset, which was compiled by Small Yellow Duck and hosted on Kaggle. The examples below are picked from thousands upon thousands of samples generated by the model and do not exist in the training data.\n\nWhile the base of the DCGAN remains the same, I have changed a few things, as well as added the variational autoencoder. Compelled by a post on stabilizing GANs, I have both set limits on the generator and discriminator, so they only update when they or their adversary is performing either particularly well or particularly poorly, as well as added noise to the inputs feeding into the discriminator. Simulated annealing then lowers the amount of that noise as training progresses to encourage convergence of the GAN.\n\nThe variational autoencoder uses a modified version of the discriminator to produce the latent variables, which then feed into the generator, so the entire model is convolutional. All three networks contribute equally to the loss, although the generator and discriminator are not always updating their gradients (due to the balancing act that must be played to keep them from outperforming one another). All these tricks combined seem to work consistently and stably for up to 64x64 color images, and although I suspect convergence is possible for larger dimensions, I have yet to successfully do it myself.", 
        "title": "Generating Fine Art in 300 Lines of Code \u2013 Richard Herbert \u2013"
    }, 
    {
        "url": "https://medium.com/@sofronije/leveraging-deep-learning-for-political-leaning-classification-4ddf9d1d2f53?source=tag_archive---------3----------------", 
        "text": "Over the past decade, political campaigns are being increasingly waged on social media networks, out of which Twitter and Facebook dominate. These two networks are also strong platforms for voters voicing their opinions. With this, the ability to predict whether a given text is politically biased, and if so, to identify its particularly political leaning, becomes extremely useful.\n\nTraditional text classification is a well-studied problem, especially in the domain of sentiment detection. Many techniques have been developed from classic text features. Recently, sophisticated methods leveraging Recurrent Neural Networks (eg. LSTM) have also been developed. In a way, RNNs mimic how the human brain works as it allows learning of long term dependencies while still \u2018forgetting\u2019 less relevant dependencies.\n\nThe classification problem we address is to identify political leaning in texts with respect to the 2016 presidential election in the United States. In this problem, we classify messages as Democratic or Republican, based on the views expressed in the message. We build the training and test datasets by picking users on Twitter whose political leanings are known to be either Democratic or Republican. We do this by using Twitter Lists, which are manually curated topical lists of users by other users of the social media platform. More about different training data sets and models derived from training data can be found at Klout\u2019s Open Data page: https://github.com/klout/opendata.\n\nThe actual accuracy of the classification is highly dependent on the input data, especially the presence of Twitter mentions. A training data set that includes only tweets with no mentions under-performs by almost 20% in accuracy compared to a data set that includes mentions.\n\nThe problem of detecting political leaning is a hard one, as it\u2019s highly context-dependent and temporal. The long-term context of the author of a tweet may have an effect on the leaning. For example, a Bernie Sanders supporter may speak negatively of Hillary Clinton while still be leaning towards the Democratic party. Similarly, the dynamic nature of the problem may come from the fact that at different times, opinions can be voiced differently. Before the primary elections, the majority of the political \u2018battle\u2019 is within the party (Hillary Clinton vs. Bernie Sanders, e.g. Democrat vs. Democrat) while post-primary, the battle is between Hillary Clinton and Donald J. Trump (e.g. Democrat vs. Republican). This means the training data from one period of time may not be applicable for a prediction task at a later time.\n\nIf you are interested in checking out how well the political leaning text classification works, please check out our demo.\n\nMore info on political leaning leveraging LSTM\u2019s can be found in the paper\u200a\u2014\u200ahttps://arxiv.org/pdf/1607.02501v2.pdf.", 
        "title": "Leveraging Deep Learning for Political Leaning Classification"
    }, 
    {
        "url": "https://chatbotslife.com/posthumous-ai-and-the-digital-confessional-db1561d12f9b?source=tag_archive---------4----------------", 
        "text": "Love and digital confession from beyond the\u00a0grave\n\nMany people are stuck with their eyes fixed on the AI horizon and the believed-to-be-inevitable singularity when humans transcend their physical form into a world of digital bliss. Regardless of which side of the singularity debate you fall on, there are many stepping-stones ahead of this extreme that warrant recognition and discussion not in five or ten years, but today. Though we haven\u2019t cracked the general AI case, our soft efforts have already accomplished a lot: we\u2019ve build a chatbot that technically passed the Turing test, we carry on lengthy conversations with support-service chatbots with our banks, telcos, and others, and we allow recommendation engines to influence our food, movie, music, and dating habits.\n\nLike it or not, AI is not simply an emerging movement; it is already here in a big way and is infiltrating the most private parts of our lives and even deaths. The often-speculated sci-fi scenario of a former friend being recreated in part or whole due to personality or biologic information left behind has recently evolved past fiction and become a reality thanks to Eugenia Kuyda\u2019s AI startup, Luka. Casey Newton at The Verge recently published a beautiful piece on Kuyda\u2019s efforts to build an AI around her late friend, Roman Mazurenko, and raises a number of crucial and difficult issues on this type of posthumous artificial intelligence. For some, the Roman chatbot was seen as insufficient. Others found it inappropriate. However, many of Mazurenko\u2019s friends found some comfort in this rare and strange interaction.\n\nYet perhaps the most interesting part of Newton\u2019s article is in his exploration of how people were using the Roman bot. Far from seeking unanswered questions or closure with their late friend, many seemed to go to the chatbot as a sort of confessional; a deaf ear to speak to. People would describe personal challenges they were encountering or ask Roman for advice, knowing the answers would be obscure, but finding value in them all the same. It seemed that the Roman bot had hit upon a crucial human need: something that felt intelligent enough to listen, but not smart enough to judge or critically talk back.\n\nInterestingly, the Roman bot ended up filling a very similar gap to one of the first chatbots ever built. ELIZA was a computer program published in 1966 that mocked a conversation between a user and a rudimentary psychotherapist. In addition to being the first, ELIZA became a thing of programming legend, often referenced for decades to come in the AI and developer communities. Part of what made ELIZA so successful was that responses were often vague, obscure, and open to personal interpretation. Coupled with anonymity and a lack of human judgment, ELIZA\u200a\u2014\u200amuch like Roman\u200a\u2014\u200abecame the perfect platform for people to share and discuss the authentic depths of their mind with.\n\nThe question that must be raised, however, is what is it about these platforms that we love? Is it their fake humanity, or in fact, their lack thereof? Is it the anonymity and lack of perceived judgment? Do we love these platforms because they feel just human enough to trick us into fully immersing into the experience, but still foreign enough to not have to worry about feeling vulnerable? Or are we so insecure in our own societal narcissism that we subconsciously want to speak to ourselves in order to sort things out yet feel as though we need some sort of external agent to help us work through our issues?\n\nAnd what happens when our AI become more advanced? What happens when their responses become less obscure and more poignant to our confessions? What happens when these chatbots sound and even look more human? Will they still serve the same purpose? Do we truly wish to bear our souls to those we love? Do we actually want to speak to the dead?\n\nOr do we simply want to speak to ourselves, but not feel crazy while we\u2019re doing it?\n\nShane Saunderson is the VP of IC/things at Idea Couture.", 
        "title": "Posthumous AI and the Digital Confessional \u2013"
    }, 
    {
        "url": "https://cenksezgin.com/the-one-learning-algorithm-hypothesis-166647115f2?source=tag_archive---------5----------------", 
        "text": "\u0097There is some evidence that the human brain uses essentially the same algorithm to understand many different input modalities.\n\nExample: Ferret experiments, in which the \u201cinput\u201d for vision was plugged into auditory part of brain, and the auditory cortex learns to \u201csee.\u201d\u00a0\n\n[Roe et al., 1992]", 
        "title": "The \u201cone learning algorithm\u201d hypothesis \u2013"
    }, 
    {
        "url": "https://medium.com/@dturchin/what-does-it-mean-to-be-human-c3905cb72d02?source=tag_archive---------6----------------", 
        "text": "What does it mean to be human when our lives have been digitized? We\u2019re little more than thoughtlets, metadata, and avatars etched for eternity in S3 buckets that are the modern-day equivalent of microfiche. We\u2019ve been reduced to digital exhaust emitted from containers that live for microseconds\u2026 and yet we\u2019re more human than ever.\n\nWhen we relegate what\u2019s easy we\u2019re liberated to practice what\u2019s hard. Researchers at Oxford recently concluded that 47% of jobs are at risk of \u201ccomputerization\u201d. Those odds aren\u2019t great\u2026 unless we turn the existential threat into an opportunity.\n\nFrom now on, to be the best humans we can be get comfortable reciting this like a mantra: what requires prediction is better left to machines. What requires judgment is better left to us.\n\nTo outwit smart machines practice empathy. Practice giving. Practice caring. Knowing the right answer won\u2019t distinguish us in the decades ahead. Knowing where to find it will. Algorithms can teach and learn and interact and compute. But only we know when and why to use them.\n\nGartner\u2019s Peter Sondergaard estimates that within the next five years half of all analytical interactions will be delivered via artificial intelligence. Find a hobby to consume the other half of your time\u2026 or reinvent yourself by finding ways to make the lives of others better. Within two years, technical skills and recall of cyber-jargon will only make it clear how inferior we are to simple machines.\n\nWe reinvented ourselves after Gutenberg displaced scribes with his printing press and Watt displaced weavers with his steam engine. We\u2019re resilient out of necessity but creative and collaborative by nature. Artificial intelligence is the call to arms we need to rediscover what it means to be human.", 
        "title": "What does it mean to be human? \u2013 Dan Turchin \u2013"
    }, 
    {
        "url": "https://medium.com/@leishi_51564/ai-what-on-earth-is-really-going-on-95f7f3907a56?source=tag_archive---------8----------------", 
        "text": "Exactly the question in the title, what ON EARTH is really going on with Artificial Intelligence\u00a0? We hear about AI almost everyday, but seldom do we know what those things mean to us and what can today\u2019s AI really do. Can they be like the Westworld androids who act and feel like a real person\u00a0?\n\nLet me firstly define what i mean by AI. Here, I am definitely not talking about General AI where the argument becomes rather philosophical as it requires the AI machine to really UNDERSTAND what is being processed. Most interesting argument is brought about by Professor John Searle in his famous Chinese room argument. He argues that even if an AI computer could converse with a native Chinese speaker and successfully pass the Turing Test, it still merely following steps. Hence, in that line of thinking, General AI has to really think like a human instead of acting as if it thinks like a human. Therefore, all we talk about here is weak AI, AI that performs tasks like human.\n\nThe term AI has been around for many many year already. The reason why it was revived in the recent years is actually because of the application of deep learning which is roughly a subset of machine learning which in turn is a subset of AI. Summarised quite vividly in the infographic above, AI started in the 50s where the created computer program can win checker all the time. Then it progressed in the 80s where through machine learning, the computer is able to know which ones are the junk mail, this proves to be useful even till today. Just in the past 5 years or so, deep learning started to blossom and because of deep learning and its new applications, AI is once again one of the most discussed topics. Even Professor Hawking with his precious speech ability took his time to warned us about the danger of AI.\n\nThey have a point. Indeed, if robots that could think and learn from their thinkings were created, it would probably end in apocalyptical scenarios, even if it was created with the best intentions possible. The bases of this statement is that computers does not have biological limitations to evolution. Hence, suppose a computer started with the IQ of maybe a caterpillar and let\u2019s say it took 1 years to \u201clearn\u201d by itself and reach the IQ of a mouse, because of this higher IQ, it would took maybe 6 month to reach the IQ of a pig and so on. The point is, given the right method, AI\u2019s IQ could surpass human in a matter of seconds and there is no going back after that. Imagine one day, computers view human like we view caterpillar. At that point, no matter how hard AIs try to communicate with us (with their best intentions), it would just be like us teaching music to chimps. At that time, would AIs care about our existence any more than we care about caterpillars\u2019 existence? I do not know.\n\nThe high profile application of AI are very HIGH profile. Autonomous driving, high frequency trading, facial recognitions, business intelligence, chatbots, smart Contracts, speech recognition. The list goes on and on as the benefits go on and on. In fact, AI in recent years has increasingly become the bread and butter of startups and people believe that it is going to bring revolution to many fields.\n\nI am one of them. it will be the pinnacle of the way human try to understand the world. With the ocean of data available and the ever increasing computer power, it would be the powerhouse that enables businesses to understand their consumers, governments to find terrorists. Negativity bias is a part of our nature and it is definitely good to be prepared for any danger from AI, however, my take is that in this period when huge changes are happening because of enablers like AI, it would be extremely foolish of us to not harness AIs to their fullest potential.", 
        "title": "AI \u2014 What on Earth is really going on? \u2013 Lei Shi \u2013"
    }
]