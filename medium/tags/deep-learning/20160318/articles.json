[
    {
        "url": "https://medium.com/@kumar0ashish/deep-learning-xgboost-winning-it-hands-down-14621e3bad2c?source=tag_archive---------0----------------", 
        "text": "Earlier it was Random forest\u00a0, the go-to algorithm for classification problems in most of the data science competitions. Correctly formulated problem\u00a0, with smart feature engineering and minimal tuning of the RF algorithm ( ntree, mtry) using grid search could get you past the bulk of the crowd\u00a0.\n\nThen came Xgboost and it soon became the hot favorite. It isn\u2019t very tough to say Deep learning is running the show at the moment. Although, GPU powered deep learning frameworks, weren\u2019t accessible to everyone\u00a0. The ones who could use it were reaping the benefits.\n\nThen arrived H2o, bringing deep learning to R with ease; (although Darch and deepnet were already available in R, not as popular though)\u00a0.\n\nHere is a demonstration of how deep learning made the lunch of the classic MNIST dataset, A digit recognition data, being used since long in the academic and research arena. A one-liner R code running a deep learning algorithm with 3 hidden layers each having 1024,1024,2048 neurons respectively\u00a0, the non-linear differentiable activation function being rectifier with dropout; achieved an error rate of 0.83 % on the test data\u00a0! A world record on the data set\u00a0: No distortion, no convolution, no ensemble, no unsupervised learning\u00a0!\n\n2. Airbnb competition (reward\u00a0: potential interview at airbnb)\u00a0: On Going\u200a\u2014\u200a10 days left \n\nIn this recruiting competition, Airbnb challenges you to predict in which country a new user will make his or her first booking. Kagglers who impress with their answer (and an explanation of how they got there) will be considered for an interview for the opportunity to join Airbnb\u2019s Data Science and Analytics team.\n\nA lazily built xgboost algorithm gets you a 0.86 score on the leaderboard\u00a0, while the leading team is at barely 0.88.\n\nWant to get better score? If are not feeling lazy\u00a0, you gotta do some hyper parameter tuning. A grid search\u00a0? I know\u00a0, computationally expensive and too slow to wait for long\u00a0. I cut it down in 2 minutes, as there was no motivation\u00a0. Lol\n\nSet the grid search to tune the hyper-parameters.\n\nSet the train control parameter to use k-fold cross validation.\n\nTrain the model on the tuned hyper parameters that gave the best accuracy as per the cross validation.\n\nWait, You could even try Optunity to optimize the hyper parameter tuning and achieve even better results with xgboost model.\n\nUse grid search to find the max-depth that maximizes AUC-ROC in twice iterated 5-fold cross-validation:\n\nHopefully it will get you closer to the finish line, who knows may be a call from airbnb\u00a0;)\n\nOriginally published by the author on linkedin.com", 
        "title": "Deep learning & XgBoost : Winning it hands down ! \u2013 Ashish Kumar \u2013"
    }, 
    {
        "url": "https://medium.com/@tianlongwang/deep-learning-to-draw-with-style-4f1aa01149dc?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep Learning to draw with style \u2013 Tianlong Wang \u2013"
    }
]