[
    {
        "url": "https://medium.com/@akashg/predicting-poker-hand-using-neural-networks-83ed7d0bfc6a?source=tag_archive---------0----------------", 
        "text": "Neural networks are powerful learning system which can be used to approximate any function.They can be used to transform the features so as to form fairly complex non linear decision boundaries. They are primarily used for classification problems.\n\nNow the problem of predicting poker hand using the cards has a deterministic solution. So we won\u2019t go into that. Rather we try to model it using neural network.\n\nYou can read about Neural Networks and how they work on wikipedia.\n\nData consists of two parts testing data and training data.\n\nEach card has two attributes a) card number and b) card type\n\nCard number is represented using number from 1 to 13 for Ace to king.\n\nSimilarly card types are represented using numbers 1 to 4. This training data consists of description of 5 cards using 10 numbers and and an output number from 0 to 9 indicating the poker hand.\n\nA simplistic neural network is to directly rescale the data into the range 0 to 1. Then use a neural network with 10 inputs and 1 output and few hidden layer. Since this is not a very complex function one might expect that using 2 hidden layers with 20 neurons each might be suffice. It turns out that it performs very badly. Even after using upto 100 neurons and upto 7 layers it does not performs very well. Therefore this approach works very badly.\n\nLets look at another architecture. Here we first do some feature engineering. Each card has two properties. The number of a card can be represented using a vector of 13 zeros and ones. Vector for number n has nth number 1 and rest 0. Similarly for suites we vector of length 4. Each card has 17 features out of which exactly 2 are 1. A set of cards can be represented using a vector of length 85. We do the same transformation for output which can take 10 values and thus represented using vector of length 10. Value i is represented using unit vector e_(i+1).\n\nOne we have done this. The rest is very easy. We use a 4 layer architecture as shown below.\n\na) The first layer is input layer which contains 85 neurons.\n\nc) The hidden layer 2 consists of 10 neurons.\n\nThe input layer takes 85 features of the cards as vector and output layer outputs 10 real numbers. Suppose ith number is highest then this set makes (i-1)th hand.\n\nThe train data can be found here and test data here.\n\nHere is the python code:\n\nWe have used the standard back propagation algorithm for training the neural network.\n\nThis simple algorithm achieves an accuracy of 94%, on test data which is a great start!", 
        "title": "Predicting Poker hand using neural networks \u2013 Akash Garg \u2013"
    }, 
    {
        "url": "https://medium.com/@spawwk/adhdeep-learning-9828ed8a13ba?source=tag_archive---------1----------------", 
        "text": "Once upon a time there was supervised learning and unsupervised learning and unsupervised learning was the future because there\u2019s no way we could learn everything we need to know from only labels.\n\nGetting labels is a lot of work, you need someone to look at your data and tell you what\u2019s going on! It\u2019s so much easier to get unlabelled data, and a label only gives you a few bits of information per example anyway. Unsupervised learning is obviously the only way to go.\n\nThen Alex Krizhevsky happened and we all got stuck with this truncated convnet diagram that only approximates what we call AlexNet today.\n\nWe also found out that you actually can learn all kinds of fancy stuff from labels if you have enough of them. You just make your network really big (buy a gpu) and push a lot of data through it and everything just works out. Sometimes you even get neat part based decomposition of objects just by predicting labels!\n\nLabels are still expensive but we have this thing called ImageNet that has 1 million of them (and incidentally has a whole lot more images with no labels but that is so 2010). Just train your network on 224x224 crops from ImageNet (because that\u2019s what Alex did) and fine tune on whatever your real task is.\n\nAnd that was The Order of Things for a few years.\n\nAt some point people figured out how to make really efficient parameterizations of Markov chains and then Jurgen showed up to remind everyone that he thought of them first and that we should also pay attention to cappuccino monkeys. But this wasn\u2019t unsupervised learning because language models are actually useful for things.\n\nThen Dirk Kingma reminded everyone that autoencoders are pretty cool and putting a KL constraint on your latent noise means this is totally not the same thing that people were doing in 2010.\n\nThe Bayesians were also really excited beacuse this was their \u201cin\u201d with deep learning.\n\nMeanwhile Ian Goodfellow came up with a really good loss function for images and Alec Radford made it work really well. The Bayesians were less excited about this because just about everything they care about is intractable with GANs. Everyone else thought it was cool though, because now they could make images that don\u2019t look like ghosts.\n\nSomewhere along the way most people lost sight of old idea of unsupervised learning for features and started focusing on the old idea of unsupervised learning for making cool pictures except now they had cooler things to show than Gabors.\n\nAround the same time this was happening some people at Google figured out how to train really deep networks. It turns out if you have a whole lot of layers you can do an even better job of recognising cats than Alex did.\n\nThese were cool for a while but then Kaiming He started making networks that are so absurdly deep that I can\u2019t even post a picture of one.", 
        "title": "ADHDeep Learning \u2013 sub cmdr spawk \u2013"
    }
]