[
    {
        "url": "https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b?source=tag_archive---------0----------------", 
        "text": "When we offered CS231n (Deep Learning class) at Stanford, we intentionally designed the programming assignments to include explicit calculations involved in backpropagation on the lowest level. The students had to implement the forward and the backward pass of each layer in raw numpy. Inevitably, some students complained on the class message boards:\n\nThis is seemingly a perfectly sensible appeal - if you\u2019re never going to write backward passes once the class is over, why practice writing them? Are we just torturing the students for our own amusement? Some easy answers could make arguments along the lines of \u201cit\u2019s worth knowing what\u2019s under the hood as an intellectual curiosity\u201d, or perhaps \u201cyou might want to improve on the core algorithm later\u201d, but there is a much stronger and practical argument, which I wanted to devote a whole post to:\n\n> The problem with Backpropagation is that it is a leaky abstraction.\n\nIn other words, it is easy to fall into the trap of abstracting away the learning process\u200a\u2014\u200abelieving that you can simply stack arbitrary layers together and backprop will \u201cmagically make them work\u201d on your data. So lets look at a few explicit examples where this is not the case in quite unintuitive ways.\n\nWe\u2019re starting off easy here. At one point it was fashionable to use sigmoid (or tanh) non-linearities in the fully connected layers. The tricky part people might not realize until they think about the backward pass is that if you are sloppy with the weight initialization or data preprocessing these non-linearities can \u201csaturate\u201d and entirely stop learning\u200a\u2014\u200ayour training loss will be flat and refuse to go down. For example, a fully connected layer with sigmoid non-linearity computes (using raw numpy):\n\nIf your weight matrix W is initialized too large, the output of the matrix multiply could have a very large range (e.g. numbers between -400 and 400), which will make all outputs in the vector z almost binary: either 1 or 0. But if that is the case, z*(1-z), which is local gradient of the sigmoid non-linearity, will in both cases become zero (\u201cvanish\u201d), making the gradient for both x and W be zero. The rest of the backward pass will come out all zero from this point on due to multiplication in the chain rule.\n\nAnother non-obvious fun fact about sigmoid is that its local gradient (z*(1-z)) achieves a maximum at 0.25, when z = 0.5. That means that every time the gradient signal flows through a sigmoid gate, its magnitude always diminishes by one quarter (or more). If you\u2019re using basic SGD, this would make the lower layers of a network train much slower than the higher ones.\n\nTLDR: if you\u2019re using sigmoids or tanh non-linearities in your network and you understand backpropagation you should always be nervous about making sure that the initialization doesn\u2019t cause them to be fully saturated. See a longer explanation in this CS231n lecture video.\n\nAnother fun non-linearity is the ReLU, which thresholds neurons at zero from below. The forward and backward pass for a fully connected layer that uses ReLU would at the core include:\n\nIf you stare at this for a while you\u2019ll see that if a neuron gets clamped to zero in the forward pass (i.e. z=0, it doesn\u2019t \u201cfire\u201d), then its weights will get zero gradient. This can lead to what is called the \u201cdead ReLU\u201d problem, where if a ReLU neuron is unfortunately initialized such that it never fires, or if a neuron\u2019s weights ever get knocked off with a large update during training into this regime, then this neuron will remain permanently dead. It\u2019s like permanent, irrecoverable brain damage. Sometimes you can forward the entire training set through a trained network and find that a large fraction (e.g. 40%) of your neurons were zero the entire time.\n\nTLDR: If you understand backpropagation and your network has ReLUs, you\u2019re always nervous about dead ReLUs. These are neurons that never turn on for any example in your entire training set, and will remain permanently dead. Neurons can also die during training, usually as a symptom of aggressive learning rates. See a longer explanation in CS231n lecture video.\n\nVanilla RNNs feature another good example of unintuitive effects of backpropagation. I\u2019ll copy paste a slide from CS231n that has a simplified RNN that does not take any input x, and only computes the recurrence on the hidden state (equivalently, the input x could always be zero):\n\nThis RNN is unrolled for T time steps. When you stare at what the backward pass is doing, you\u2019ll see that the gradient signal going backwards in time through all the hidden states is always being multiplied by the same matrix (the recurrence matrix Whh), interspersed with non-linearity backprop.\n\nWhat happens when you take one number a and start multiplying it by some other number b (i.e. a*b*b*b*b*b*b\u2026)? This sequence either goes to zero if |b| < 1, or explodes to infinity when |b|>1. The same thing happens in the backward pass of an RNN, except b is a matrix and not just a number, so we have to reason about its largest eigenvalue instead.\n\nTLDR: If you understand backpropagation and you\u2019re using RNNs you are nervous about having to do gradient clipping, or you prefer to use an LSTM. See a longer explanation in this CS231n lecture video.\n\nLets look at one more\u200a\u2014\u200athe one that actually inspired this post. Yesterday I was browsing for a Deep Q Learning implementation in TensorFlow (to see how others deal with computing the numpy equivalent of Q[:, a], where a is an integer vector\u200a\u2014\u200aturns out this trivial operation is not supported in TF). Anyway, I searched \u201cdqn tensorflow\u201d, clicked the first link, and found the core code. Here is an excerpt:\n\nIf you\u2019re familiar with DQN, you can see that there is the target_q_t, which is just [reward * \\gamma \\argmax_a Q(s\u2019,a)], and then there is q_acted, which is Q(s,a) of the action that was taken. The authors here subtract the two into variable delta, which they then want to minimize on line 295 with the L2 loss with tf.reduce_mean(tf.square()). So far so good.\n\nThe problem is on line 291. The authors are trying to be robust to outliers, so if the delta is too large, they clip it with tf.clip_by_value. This is well-intentioned and looks sensible from the perspective of the forward pass, but it introduces a major bug if you think about the backward pass.\n\nThe clip_by_value function has a local gradient of zero outside of the range min_delta to max_delta, so whenever the delta is above min/max_delta, the gradient becomes exactly zero during backprop. The authors are clipping the raw Q delta, when they are likely trying to clip the gradient for added robustness. In that case the correct thing to do is to use the Huber loss in place of tf.square:\n\nIt\u2019s a bit gross in TensorFlow because all we want to do is clip the gradient if it is above a threshold, but since we can\u2019t meddle with the gradients directly we have to do it in this round-about way of defining the Huber loss. In Torch this would be much more simple.\n\nI submitted an issue on the DQN repo and this was promptly fixed.\n\nBackpropagation is a leaky abstraction; it is a credit assignment scheme with non-trivial consequences. If you try to ignore how it works under the hood because \u201cTensorFlow automagically makes my networks learn\u201d, you will not be ready to wrestle with the dangers it presents, and you will be much less effective at building and debugging neural networks.\n\nThe good news is that backpropagation is not that difficult to understand, if presented properly. I have relatively strong feelings on this topic because it seems to me that 95% of backpropagation materials out there present it all wrong, filling pages with mechanical math. Instead, I would recommend the CS231n lecture on backprop which emphasizes intuition (yay for shameless self-advertising). And if you can spare the time, as a bonus, work through the CS231n assignments, which get you to write backprop manually and help you solidify your understanding.\n\nThat\u2019s it for now! I hope you\u2019ll be much more suspicious of backpropagation going forward and think carefully through what the backward pass is doing. Also, I\u2019m aware that this post has (unintentionally!) turned into several CS231n ads. Apologies for that\u00a0:)", 
        "title": "Yes you should understand backprop \u2013 Andrej Karpathy \u2013"
    }, 
    {
        "url": "https://medium.com/@amarbudhiraja/moocs-for-machine-learning-5a2f2c6cdcfe?source=tag_archive---------1----------------", 
        "text": "There are many-many MOOCs online related to Machine Learning. Be it stats or machine learning models or even core math, there is a lot of content available online and for someone who is just starting with machine learning on their own, it can become pretty daunting, like it became for me.\n\nIts been 3\u20134 years since I started into ML as an student and I have gone through a lot of lectures online and in class to learn. Through this blog post, I want to help people who want to learn machine learning online.\n\nI have had the chance to interview with certain machine learning giants recently and one thing that I have learnt\u200a\u2014\u200aYou don\u2019t need to know everything. You just need to know basics and some advance stuff, but you need to know it very well. Keeping that in mind, I have prepared a list of machine learning MOOCs from Coursera, EdX and Udacity. I have also considered a level \u2018O\u2019 for people who don\u2019t know programming.", 
        "title": "MOOCs for Machine Learning \u2013 Amar Budhiraja \u2013"
    }, 
    {
        "url": "https://aboveintelligent.com/a-i-threat-or-opportunity-fbcfc68e0cae?source=tag_archive---------2----------------", 
        "text": "As a youth, my friends and I read magazines like \u201cYoung Technician\u201d, \u201cQuantum\u201d, \u201cScience and Life\u201d, along with sci-fi novels by Sheckley, Bradbury and Asimov. Even so we could not imagine how quickly artificial intelligence (AI) would become such an integral part of our daily lives.\n\nI always liked to do something with my hands, and once in grades 4\u20135, I decided to build a robot. It was made of plywood, and almost my height. The robot was equipped with a vibration motor (brushes for clothes as his feet plus eccentric motors), so that he could move. His eyes were light bulbs with a conical reflector made from a tin can. He raised his hand when you came closer to him, thanks to a photo diode reacting to changes in lighting which was built into his chest.\n\nAnd most importantly, he was able to speak\u200a\u2014\u200awith my voice! To do this I disassembled my parents\u2019 state-of the-art cassette recorder, a Sputnik 401 (terribly expensive and scarce in those days) and embedded it into the robot. When I recall how I sawed and soldered, I wonder whether the thought occurred to me that I would be involved in the creation of artificial intelligence technology when I grew up?\n\nThe world\u2019s first tentative steps in artificial intelligence\u200a\u2014\u200atechnology that thinks and makes decisions like a human\u200a\u2014\u200awere made in the middle of the 20th century. Today, AI is present in many and various forms: speed cameras, robotic vacuum cleaners, voice assistants, sensors for unmanned vehicles, text and voice recognition software.\n\nThe Associated Press, the international information and news agency, is already making 3000 financial statements per quarter with the help of AI. Analysts estimate that by 2025 robots that act as financial advisors, and will manage a budget of 7 trillion dollars. More and more technologies are emerging each day based on neural networks and deep machine learning.\n\nProgress in the field of deep learning, deep neural networks and parallel computing has played a major role in the breakthrough development of artificial intelligence. Since 2010\u20132012, the development of deep neural networks has been skyrocketing. Hundreds of scientific articles are published on a daily basis, and technologies which only appeared 6 months ago are already considered obsolete.\n\nNew achievements in AI are announced literally every month. More recently, Google AlphaGo beat the world champion in Go, a very complex ancient Chinese board game. 10 million people professed their love for the girl, Xiaoice, a chat-bot created by Microsoft, which was later invited onto China\u2019s Morning News TV show.\n\nIt is believed that the desktop PC will rival the human brain for its productivity by 2030\u20132040. The technological singularity will be achieved, when a computer will become smarter than a human, and therefore will begin to improve itself faster and better than a human.\n\nA reasonable question that worries many people is whether there will still be a need for human professions, when AI learns to perform more tasks and takes over many roles? According to scientists at Oxford University, about 47% of US jobs are at risk of being fully replaced by technology by 2025.\n\nWhite House financial experts forecast that AI will replace all workers that make less than $20 per hour. Thus, it seems like there will be 350 million production or warehouse employees out of work. There are already 30,000 robots working in Amazon\u2019s warehouses, saving the company both time and money.\n\nThe American bank Merrill Lynch predicts that automation will reduce labor costs by 9 trillion dollars. Another 8 trillion will be saved in the processing industry and healthcare; autonomous vehicles and drones will bring 2 trillion more. In general, the influence of AI on the economy due to production automation is estimated at $US33 trillion\u00a0. According to report from Bank of America (http://bit.ly/2a42LiQ), robots and AI may increase the productivity by 30% in many industries, while reducing labour costs by 18\u201333%.\n\nAll of the above excites one\u2019s imagination. Currently, there are hundreds of thousands of engineers all over the world working to improve machines and AI technologies. In Silicon Valley, investor interest in these technologies is unparalleled.\n\nIn the second quarter of 2016, the share of investments in AI increased in comparison with other major areas of focus (e.g., digital medicine). By June 2016, more than 200 companies focused on AI had attracted more than $US1.5 billion. The number of investment transactions funding such startups has increased 6-fold over the past few years, from 70 in 2011 to 400 in 2015.\n\nAI technology is encouraging the creation of a vast number of brand new products and markets.\n\nExamples are easy to find. Thanks to Realty Editor, we can control physical objects, such as lighting devices, technology, or a vehicle, with our smartphone. All we need to do is point the smartphone camera at an object created on the Open Hybrid platform.\n\nOne very interesting project, The AIVC, is actually the first venture investment fund that evaluates startup ideas using AI technology!\n\nAnother example, Kensho responds to financial questions and analyses the market. IBM Watson analyses patient data, compares it with the accumulated medical knowledge base and helps doctors determine the correct diagnosis. On 05 August 2016, the Japanese Cancer Center reported that Watson established a diagnosis of a rare form of leukemia, a task that professional doctors failed to accomplish.\n\nApi.ai is a service that provides all the tools for implementing communication platforms to third-party applications. Alterr\u0430 chooses a country for your next vacation, while startup Luka generates a dialogue with users on the basis of neural networks. Cubic robotics is an intelligent butler for house management.\n\nThe hugely popular Prisma app stylises photographs as if they were made by famous artists, and has won the hearts of 50 million users around the world in just a few months.\n\nMy company, ABBYY, is also into AI development. Text recognition technologies are already being used to extract relevant data from documents and speed up the filling in of fields. ABBYY Compreno is able to understand the meaning of text. It allows banks to receive a borrower\u2019s information and make sound credit decisions. It speeds up the technical support process and can respond to a client directly (or direct a client to a proper tech support engineer). It helps HR with routine work such as reviewing CVs, extracting required potential candidate information and matching data with job descriptions.\n\nThe Findo project, one that I am working on currently, is also AI-based and helps to find necessary files, letters, and personal documents based on their description in natural language (for example, \u201cfind me a healthy diet presentation from the person I met in Boston\u201d).\n\nThe system aims to solve the eternal problem: you know you have a file, yet cannot find it. Findo is an intelligent search agent that will not just recognise language (at the moment, it is English only) but also act as an assistant, for instance to notify you that you received an urgent request from your boss and haven\u2019t responded to it yet.\n\nSo, is it likely that half us will be unemployed in less than 10 years? I don\u2019t think so. Even if machines take over some of the current functions performed by humans, the demand for labour will remain. In the 1980s, the United States introduced barcode scanners and POS terminals. This reduced labour costs in stores, but the cashiers are still around. Between 1980 and 2013, their employment grew by more than 2%.\n\nThere is still a huge gap between what many people are doing at work, and what robots and AI can do. It is relatively easy to automate everyday tasks, but there are many non-standard challenges which machines cannot manage.For example, there are many routine tasks which seem easy for humans, but are impossible for a machine. This phenomenon is known as Moravec\u2019s paradox. To fold a towel, a person needs only a few seconds. In 2010, a robot performed this task and it took nearly 25 minutes. From this we may conclude that that cooks, gardeners, plumbers, and dentists won\u2019t be replaced by AI in the near future. All of these jobs are associated with sensorimotor skills and many of them require brainstorming, recognition of huge volumes of images, and making insights.\n\nIt is quite difficult for AI to perform work that requires the development of interpersonal relationships. Machines have weak social skills. They will not be able to perform tasks associated with human interaction, such as, for example, motivation, psychological support and care. This means that the jobs of sales staff, managers, and entrepreneurs, nurses and kindergarten teachers are probably safe.\n\nMankind has always been distinguished by the ability to think creatively, a difficult quality to teach to a machine. While AI technologies may take on mundane tasks, we will continue to think creatively and, for instance, write poetry. Digital technologies may complement this, but in no way will it supplant it.\n\nAccording to Eric Brynjolfsson, an MIT economist and one of the authors of \u201cThe Second Machine Age\u201d, these are the best of times for creative people, those who develop something innovative, whether it\u2019s a song, film, service or software.\n\nAI may automate routine tasks but it also opens up new markets which previously could not be imagined.\n\nThe US Labor Department predicts that 1.2 million new jobs will emerge by 2020, related solely to computer technology. Someone needs to develop and maintain these machines. The Harvard Business Review has named data scientist as the most attractive and in-demand profession of the 21st century.\n\nIn addition to industry-specific professions, new jobs will add to the technological ecosystem. For example, a lawyer in robo-ethics, a human body designer, an engineer for natural environment restoration, a space guide, and many others.\n\nToday, teachers train humans in programming and development, but there will also be a need for those who train machines. It is not enough to train a machine to use AI to perform the work of a human. New operating instructions must be thoroughly described\u200a\u2014\u200aa task that can only be performed by those whose jobs will be subsequently replaced by robots. So new roles will evolve.\n\nAccording to McKinsey Global Institute (MGI) analysts, society will develop 10 times faster and 300 times more extensively in the near future, thanks to achievements in AI development.\n\nThis will provide unlimited opportunities to develop new products, services, businesses, and revolutionise workplaces since people will have new tasks and challenges.\n\nWhat an interesting time we live in! We just need to adapt to modern times, prepare to adapt to new, more popular professions, and learn key competencies that no robot or algorithm can replace. Ultimately, I believe that despite all the technological advances, humans will always remain humans. Our main role is to continue our own DNA.\n\nDavid Yang is a co-founder of ABBYY, the leading developer of document recognition, data capture and linguistic software, and Findo, a smart search assistant. He holds an M.S. in Applied Mathematics and Physics, is the author of a large number of scientific publications and holds many patents.", 
        "title": "A.I. Threat or Opportunity \u2013"
    }, 
    {
        "url": "https://cicero.ai/good-ai-resources-baa8d5222306?source=tag_archive---------3----------------", 
        "text": "The modern (post-Deep Learning) Artificial Intelligence is a blend of Machine Learning, Natural Language Processing, Logical and Statistical AI of yore and Data Science, the latter a practice of building data-driven business processes. The list here reflects the predilections of the Institute.\n\nThe Deep Learning Book by Goodfellow, Bengio, and Courville\n\ndeeplearning4j\u200a\u2014\u200athe most comprehensive library for Deep Learning on the JVM from Skymind. The Skymind Academy helps you learn all about it.", 
        "title": "Good AI Resources \u2013"
    }, 
    {
        "url": "https://medium.com/the-data-intelligence-connection/business-must-adapt-artificial-intelligence-to-survive-677fe66a372c?source=tag_archive---------4----------------", 
        "text": "Artificial Intelligence is no longer just hype: it is a reality. Artificial Intelligence-based approaches, such as Natural Language Processing (NLP), Machine Learning (ML) and Deep Learning, are becoming more realistic within the technology industry. We have today very efficient NLP engines as well as powerful ML and deep learning algorithms available. In a recent article on WIRED, I remember reading about the death of code (programs and programming) and how we will soon be training systems like we train our pets.\n\nMachine Learning involves learning from examples and experiences: it\u2019s all about digesting huge volumes of data. IBM and Memorial Sloan Kettering are training Watson in Oncology using the massive amount of patient medical records across the world. Watson learns from how doctors treat cancer patients worldwide, similar to how a medical student learns but on a larger scale.\n\nAnother example of machine learning can be found in Japan: here, farmers cultivate fresh and crispy cucumbers, with many prickles on them. Straight and thick cucumbers with a vivid color and a lots of prickles are considered to be of premium grade. Each cucumber has a different color, shape, quality, and freshness. Cucumbers are sorted into nine different classes based on size, thickness, color, texture, small scratches, whether or not it is crooked, and the number of prickles on it. There is no well-defined instruction set for classification of cucumbers.\n\nMakoto Koike studied this problem while helping his parents to classify the cucumber on their farm.", 
        "title": "Business must Adapt Artificial Intelligence to survive"
    }, 
    {
        "url": "https://medium.com/@YvesMulkers/business-must-adapt-artificial-intelligence-to-survive-a86f269e4456?source=tag_archive---------5----------------", 
        "text": "Artificial Intelligence is no longer just hype: it is a reality. Artificial Intelligence-based approaches, such as Natural Language Processing (NLP), Machine Learning (ML) and Deep Learning, are becoming more realistic within the technology industry. We have today very efficient NLP engines as well as powerful ML and deep learning algorithms available. In a recent article on WIRED, I remember reading about the death of code (programs and programming) and how we will soon be training systems like we train our pets.\n\nMachine Learning involves learning from examples and experiences: it\u2019s all about digesting huge volumes of data. IBM and Memorial Sloan Kettering are training Watson in Oncology using the massive amount of patient medical records across the world. Watson learns from how doctors treat cancer patients worldwide, similar to how a medical student learns but on a larger scale.\n\nAnother example of machine learning can be found in Japan: here, farmers cultivate fresh and crispy cucumbers, with many prickles on them. Straight and thick cucumbers with a vivid color and a lots of prickles are considered to be of premium grade. Each cucumber has a different color, shape, quality, and freshness. Cucumbers are sorted into nine different classes based on size, thickness, color, texture, small scratches, whether or not it is crooked, and the number of prickles on it. There is no well-defined instruction set for classification of cucumbers.\n\nMakoto Koike studied this problem while helping his parents to classify the cucumber on their farm.", 
        "title": "Business must Adapt Artificial Intelligence to survive"
    }, 
    {
        "url": "https://medium.com/@exastax/how-deep-learning-is-transforming-the-future-of-technology-aab7cf516351?source=tag_archive---------6----------------", 
        "text": "How Deep Learning is Transforming the Future of Technology?\n\nThere is no doubt that Big Data has been one of the most popular topics among marketers and tech enthusiasts for several years. Within the big data domain one of the most promising fields is deep learning which has evolved into one of tech\u2019s most exciting and promising disciplines in the field of AI (Artificial Intelligence).\n\nThe popularity of deep learning peaked in March 2016, when Google\u2019s DeepMind AI program called AlphaGo bested Lee Sedol, the celebrated player of the board game \u201cGo\u201d, by winning four out of five games. After the match it was revealed that a relatively new AI technique called \u201cdeep learning\u201d was responsible for the victory. According to scientists, deep learning technology has the potential to transform the entire AI area.\n\nDeep learning can be defined as a new area of machine learning working to improve areas like voice search, image and language processing or solve unstructured data challenges. We might not be aware of deep learning, but it is everywhere around us. It\u2019s giving us personalized recommendations on Amazon, it\u2019s on our smartphones by the name of Siri (or another voice-activated assistant). Deep learning can enable us to constructing our digital marketing activities by transforming the contents of our websites and mobile applications into tailor made offers.\n\nDeep Learning is transforming the future of technology in many ways. Here are some of the most crucial areas;\n\nOne of the most popular usage areas of deep learning is voice search & voice-activated intelligent assistants. With the big tech giants having already made significant investments in this area, voice-activated assistants can be found on nearly every smartphone. Apple\u2019s Siri is in the market since October 2011. Google Now, the voice-activated assistant for Android, was launched less than a year after Siri. The newest of the voice-activated intelligent assistants is Microsoft Cortana.\n\nMajor companies such as Netflix, Amazon, Google, and Facebook have been using recommendation systems on their web and mobile applications for years. These tech giants have access to a vast wealth of user generated data and they strive to use these data in order to add value to both users and their business bottom line. For instance, 35% of Amazon\u2019s sales are generated from recommendations, and two thirds of the movies users watch on Netflix are recommended by Netflix\u2019s recommendation engine.\n\nAnother popular area regarding deep learning is image recognition. It aims to recognize and identify people and objects in images as well as to understand the content and context. Image recognition is already being used in several sectors like gaming, social media, retail, tourism, etc\u2026\n\nFor instance the video below shows how image recognition and AI might potentially transform the music industry.\n\nAdvertising is another key area that has been transformed by deep learning. It has been used by both publishers and advertisers to increase relevancy of their ads and boost the return on investment of their advertising campaigns. For instance, deep learning makes it possible for ad networks and publishers to leverage their content in order to create data-driven predictive advertising, real-time bidding (RTB) for their ads, precisely targeted display advertising, and more.\n\nDeep learning is becoming an important area for all types of economy sectors and the tech giants are heavily investing in deep learning in terms of R&D and M&A. For instance, Apple acquired an AI startup VocalIQ (a company which used deep learning to understand the context in which words are spoken) to make Siri smarter and more humane. Similarly, Facebook acquired Wit.ai (a startup whose deep learning technology enables developers to add natural language processing to apps) in order to develop smarter personal assistants.\n\nTo wrap up, deep learning can be evaluated as a disruptive technology that is being used by more and more companies to build innovative products and to even spawn new business models.", 
        "title": "How Deep Learning is Transforming the Future of Technology?"
    }
]