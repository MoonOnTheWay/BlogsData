[
    {
        "url": "https://medium.com/machine-learning-world/keras-lstm-to-java-a3124402d69?source=tag_archive---------0----------------", 
        "text": "We have lot of amazing frameworks for deep learning which allow us easy and fast prototyping and learning complex architectures even not thinking about what happening inside of them. But sometimes you need to deploy your model somewhere\u2026 let\u2019s say where you can\u2019t use your favorite\n\nI recently faced this problem, when I had to deploy recurrent neural network for action recognition trained in Keras in Java. My client doesn\u2019t want to use some microservices architecture, he wants everything in Java and basta cosi\u00a0:)\n\nSo, let\u2019s see how we can do it.\n\nFirst, I trained 2-layers LSTM model with softmax on the top, classifying in 3 classes:\n\nFirst, let\u2019s load our weights from\u00a0.hdf5 file and see the structure:\n\nOutput looks like this:\n\nparam_0, param_1 doesn\u2019t look very representative, I can\u2019t understand what these weights are responsible for. But output of layer.trainable_weights shows us exactly what we want. And if we check one of the most popular tutorials in LSTMs\u2026 We are just lucky! Notations of weight matrices are the same! We can understand, that param_0 is just W_i, param_1\u200a\u2014\u200aU_i and so on.\n\nNow let\u2019s save matrices in easy to read format:\n\nNow they are nicely stored as\u00a0.txt files looking like:\n\nI am going to follow mentioned above tutorial for implementing LSTM. For all details check out code on Github, here are just some parts of it.\n\nFirst, I decided to use jblas for matrices routines, we will use them a lot.\n\nWe also need classes for:\n\nYou can check them out on Github, here I just post code for forward propagation routine in LSTM. Everything is pretty straightforward, just step-by-step matrices multiplications (carefully think about dimensions!).\n\nTo build our network we can use next approach (yeah-yeah, just making it look prettier, adding layers like in Keras\u00a0:D)\n\nJust left to check results and compare them to Keras output. They are the same\u00a0:)\n\nActually, it was a nice exercise to code some math, especially when you are used to use \u201cout-of-box\u201d instruments. Also it shows, that LSTMs are not that complicated in implementation and in case of need you can always port it to any language to make it work on any devices.\n\nThank you for attention!", 
        "title": "Keras LSTM to Java \u2013 Machine Learning World \u2013"
    }, 
    {
        "url": "https://medium.com/@Lidinwise/the-revolution-of-depth-facf174924f5?source=tag_archive---------1----------------", 
        "text": "Deep Residual Networks (ResNet) were recently proposed by Microsoft team with an immediate impact in machine learning community. They were the first to achieve super-human performance (which lies somehow around 5%) on ImageNet competition with an error rate of only 3.7%\u200a\u2014\u200acode is available on github.\n\nResNet are based on a simple idea: feed the output of two successive convolutional layers and bypass the input to the next layers by adding the input x_l of layer l to the output h_ln of layer l+N. With this simple trick the team were able to train networks of up to 1000 layers deep with remarkable results - thus getting rid of the gradient vanish curse once and for all.\n\nResNet uses a 7x7 conv layer at input level followed with a pool of two layers\u200a\u2014\u200ain contrast with more complex formats used by Google team with Inception V3 and V4. In ResNet the input of the layers is fed to many modules in parallel and the output of each modules is serially connected. ResNet can be thought as an ensembles of parallel and serial machines operating at smaller depth modules\u00a0.\n\nThis very simple idea proved to be extremely powerful, an in 6 months their publicatio already has more than 200 references.\n\nRecently they presented a video with a demo on the network classifying objects in a the streets (watch the youtube video) with mind blowing accuracy.", 
        "title": "The Revolution of Depth \u2013 Armando Vieira \u2013"
    }, 
    {
        "url": "https://medium.hashin.me/catching-the-thoughts-neural-networks-2695e0932b27?source=tag_archive---------2----------------", 
        "text": "I have been following the Google research blog for quiet sometime tuning on to the latest developments from the tech giant.\n\nAnd then I came across something interesting today. The folks at Google call it \u201cInceptionism\u201d and it left me baffled.\n\nBefore I write about what it is, let me give a primer on what neural networks are.\n\nWe all know what Computer programs do\u200a\u2014\u200aprovide step by step instructions on how a particular task is completed. Depending upon the complexity of the task at hand, a computer program can be very simple or can get really really complex.\n\nBut one thing stays the same\u200a\u2014\u200athe programmer instructs the computer what to do and it complies. Almost all the technological brilliance we see around us is built this way\u200a\u2014\u200aby telling the computers what to do and making them do exactly that.\n\nThe so called \u2018software\u2019 companies have invested billions of dollars into creating systems and processes that ultimately churn out the code that make systems tick.\n\nWhatever it does, the workflow is the same\u200a\u2014\u200aThe code knows what to do. This code is again converted to simple arithmetic operations that a computer understand and the computer systematically executes these simple instructions (at insanely insane speeds) to do whatever we want it to do.\n\nSo an onlooker might be tempted to ask,\n\n\u201cSo if you wanna make a computer do difficult tasks, you might umm\u2026 well write a complex program to make it do that. Nothing is impossible, right!!!\u201d\n\nBut it turns out some of the tasks we humans easily do are insanely difficult for computers to replicate. These include natural language processing (learning and understanding a human tongue), detecting objects from image (like a cat in a youtube video), logical thinking based on ontologies etcetera.\n\nThe reason being that the \u2018steps\u2019 that go behind these processes are very difficult to decipher. If you ask me how to make coffee, I will be able to give you a set of steps or an algorithm that might help you to make it again.\n\nBut if you ask me to give a step by step description of how I detected a cat\u2019s face in that video, umm well\u2026 I can\u2019t!!\n\nIt turns out that it\u2019s insanely hard to decipher \u2018the small steps\u2019 involved in say learning a language or looking at a cat\u2019s image and understand that it\u2019s actually a cat.\n\nSo the computer scientists had to look for ways to make computers do such trivial tasks.\n\nThe traditional understanding of Computer Science wouldn\u2019t help us build the tech to do them easily. But what to do, we are a species that is never cowed down by obstacles.\n\nOur people kept thinking on how to tackle this and build systems that will \u2018act like humans\u2019.\n\nSo if we want a computer to mimic human brain, we should make them work like one. So people started looking up to a field of science that tried to understand the human decision making system.\n\nWe humans, despite being rational beings make some insanely foolish decisions at times. We seem to weigh in a lot of factors before taking a decision.\n\nIf we weigh in the right parameters with the right amount of importance to each of them, we are more likely to end up with a better decision.\n\nIf we screw them up, we are not to be surprised when our final decision is screwed up like anything.\n\nSo a lot of research went into this field trying to understand our decision making process and build systems that mimic our decision making. They all were researched and refined under the umbrella of Artificial Neural Networks (ANNs).\n\nHere, I am presenting an over simplified view of what goes under them.\n\nSo these systems are networks that essentially contain nodes that make small decisions based on our inputs and the desired result for this inputs (an optimal or desired output).\n\nSo properties of this nodes are set such that outputs comes as close as possible to the desired results.\n\nBut how do we set these properties?\n\nThis is very similar to asking ourselves on how do we make ourselves ready for an examination after a failed test.\n\nWhat do we do?!!\n\nYes, we make the network learn from its experience and reset the \u2018properties\u2019 so that the next time we give the same input, we get an output that is optimal.\n\nSo we keep training (teaching! ^_^) our networks with huge datasets so that our network gets better and better in what it does.\n\nThis is very similar to how we learned to walk. We tried and we failed. Then we tried again and we failed. The mistakes made us stronger, we learnt to walk at last!\n\nThe input was our desire to walk. The properties of our system (node) was the forces we applied on different point on our body. Output was if we were able to balance ourselves or not.\n\nIf we got our properties wrong, we fall. But we readjust them so that we are better off next time. Slowly we pick this up and at the end we know when and were the right amount of force is to be applied. Viola! We have our properties right, just like a neural network.\n\nSo after the laborious process of learning, practicing it comes quite easy. The same is true for neural networks as well. The entire literature in Neural Network is mostly around how to effectively make your Neural Network learn and how we design Neural Networks that give good results once they are \u2018trained\u2019.\n\nSo now we understand Neural Networks and how they make decisions and gets things done. We also know that they learn from their experiences rather than act on some rigid instructions like a trivial computer program.\n\nBut why did I go to these depths to talk about them all of a sudden? What made me think about them other than my occasional flirtations with Tenserflow and other neural networking toolboxes?\n\nSo what is Inceptionism? It was an attempt to understand neural networks.\n\nAt this point, we know that neural networks learn from their \u2018experience\u2019 and take decisions based on that. If the neural network is trained a lot, we can check it\u2019s efficacy by testing it with a sample data set.\n\nIf the neural network gets a lot of decisions correct we say that it\u2019s a good one. It means we got our design correct. If it fails a lot, we have to redesign them again.\n\nThe problem is that in the process of designing a network, we have no idea about how the \u2018properties\u2019 of the end network will look like. These properties have to be picked up by the network during the \u2018learning\u2019 process.\n\nIn short, the designing and refining of a neural network is a largely trial and error process with a lot of \u2018guidelines\u2019 we have from our past experience designing neural networks. We are still largely unclear about how to design them effectively and get them right every time.\n\nA huge amount of research goes behind this and I found myself interested in finding out how they work. Then I stumble upon this article at the Google research blog.\n\nIt contains some really good images. You can have a look at them here:", 
        "title": "Catching the thoughts : Neural Networks. \u2013"
    }, 
    {
        "url": "https://medium.com/@ai.at.cumc/ai-camp-open-camps-ai-bootcamp-mirix-6613a7bfa23d?source=tag_archive---------3----------------", 
        "text": "AI Camp at Open Camps\n\nThe United Nations is hosting a technology conference from July 8\u201317. It is a way to connect technologist and the advancements in IT to the real world problems that the UN is tackling everyday. A couple of members of the AI community in New York, including myself, are assisting with developing the AI @ Open Camps (Tuesday July 12th, 2016) for the event. We are looking for speakers, volunteers, sponsors and people to come to the event. This will be a yearly event and although this year might be small, next year it will be huge.\u00a0\n\n\u00a0\n\n\u00a0What: Open Camps\u00a0\n\n\u00a0Where: United Nations\n\n\u00a0When: July 8th\u200a\u2014\u200a17th.\n\n\u00a0Time: All Day\n\n\u00a0Info: http://opencamps.org/\n\n\u00a0\n\n\u00a0What: AI Camp @ Open Camps\n\n\u00a0When: United Nations\n\n\u00a0Day meet: 9am\u200a\u2014\u200a5pm\n\n\u00a0Night meet: 6pm\u200a\u2014\u200a9pm\n\n\u00a0Info: http://ai.camp/\n\n\u00a0(FYI, I am working on the website, so if anyone wants to help, give me a holler).\n\n\u00a0\n\n\u00a0We will be looking for more partnerships to develop this group and AI @ Open camps. If your company is interested, or you know anyone who would like to participate, please email me at:\n\n\u00a0AI.at.CUMC@gmail.com\u00a0\n\n\u00a0anthony.albertorio@gmail.com\n\nAI Bootcamp\n\nAs you know, the mission of AI @ CUMC is to mix the coffeehouse culture of the Enlightenment with the ethos of the modern day hackerspace to shape AI research. Since artificial intelligence will be one of the pillars that will define the 21st century, it is essential that this technology is defined by the people, for all the people. Overall, the idea is the connect the philosophy, mathematics and engineering towards the development of artificial intelligence.\u00a0\n\n\u00a0\n\nThe AI bootcamp that we are developing will serve as the backbone for the engineering component. The goal of the bootcamp is to create the technical framework to teach people in the community how to use, deploy, and research AI in the real economy. We working on partnering up with the Community League of the Heights (which is literally right next door to Word Up and has a large conference room full of 25+ networked computers) to develop this further. Preliminary discussion has placed a launch date of around late August/Early September. Our proposal can be found here:\n\nSee:\u00a0\n\nArtificial Intelligence training proposal for The Artificial Intelligence Foundation aka The Foundation.\n\nCommunity League of the Heights \n\n\u00a0\n\nBest of all, it will be FREE! This will keep in line with the Enlightenment philosophy and the pervious works of Peter Cooper (founder of Cooper Union), Townsend Harris (founder of City College of New York), which advocated that education must be accessible to all, so that the diffusion of knowledge allows for all classes to participate and shape the political economy. There is also the idea to create a technological commons and strengthening connections between people, so that value added technologies can be iterated and adopted quickly. \n\n\u00a0\n\nWe are looking for volunteers to help create and structure the project, including the curriculum. If you have any ideas, please feel free to highlight the parts in questions and comment with your name. Also, the citations on the footnotes are off. There isn\u2019t much style. Feel free to comment on the best way to do that: MLA? APA? Chicago style? Harvard style?\n\nMIRIx\n\n\u00a0We have also partnered up with the prestigious Machine Intelligence Research Institute (MIRI) to create a MIRIx affiliated site. This will allow the community to develop the mathematical skills to contribute to AI research outlined by MIRI, as well as serving as a recruitment tool for the main organization who are looking top talent. So, if you want to live around Silicon Valley do cutting edge AI research, and be paid for it, this can be one way in. Here is looking at you Systems Biology and Computational Neuroscience students at Columbia.\u00a0\n\n\u00a0\n\n\u00a0Info on MIRIx:\n\n\u00a0https://intelligence.org/mirix/\n\nProject \u201cTech Tree\u201d\n\nDon\u2019t know a lick of math, but are hungry to learn and want to contribute to AI research? You might be in luck. Currently, I am working on a \u201ctech tree\u201d of skills that you will need to know to engage in research. This project will eventually evolve into an online tool that tracks a person\u2019s progress on learning about AI and skills related to AI research. Think Khan Academy\u2019s math tech tree and the interactive examples for it. The idea is having this as one of our internal tools for the AI bootcamp and it will be completely open sourced.\u00a0\n\n\u00a0\n\n\u00a0If anyone wants to volunteer give me a holler.\n\nVideo:\n\nVia Intelligence Squared Debates: Don\u2019t Trust the Promise of Artificial Intelligence\u00a0\n\nThere are many interesting questions are brought up here:\n\nWhat are the pitfalls of overestimating the utopian vision of AI society?\u00a0\n\nDoes a utopian vision of a technology conflate church and state, that is does belief begin to trump reason in science?\u00a0\n\nWhere do we draw the line between our hopes and beliefs in science and actual fact?\u00a0\n\nDoes the tech world really have AI or is it just one big mechanical turk economy, where the labour of millions of people are being expropriated and fed into algorithms, while devaluing their contribution?\n\nVia Bloomberg: Watch This AI Baby in Action\n\n\n\nVia Bloomberg and Ashlee Vance: Hello World Episode, Meet the Real-Life Tech Wizards of Middle Earth\n\nTools:\n\nVia First Timer\u2019s Only: First timers to Open Source Contributions\n\nPapers:\n\nVia Future of Life Institute: AI papers from the Future Of Life Institute grant recipients", 
        "title": "AI Camp @ Open Camps + AI Bootcamp + MIRIx \u2013 AI at Columbia University Medical Center \u2013"
    }
]