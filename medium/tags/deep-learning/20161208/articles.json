[
    {
        "url": "https://medium.com/intuitionmachine/deconfusing-ai-and-deep-learning-20473d7578c0?source=tag_archive---------0----------------", 
        "text": "We hear and read in the popular media about Artificial Intelligence (AI) all the time. We have movies about them. We hear about Elon Musk and Stephen Hawking warning us about AI\u2019s apocalyptic consequences. We hear from the World Economics Forum about AI\u2019s effect on taking away our jobs. We hear about how disruptive AI will be for businesses. However, when we listen to experts speak about this, they bring about an entirely different phrase: \u201cDeep Learning\u201d.\n\nDeep Learning, two simple words that we all can understand, but yet in the context of Artificial Intelligence, when these words are combined they become inscrutable to the uninformed. In fact, even for the informed it is inscrutable. That\u2019s because decades worth of statistical training have become a liability in understanding what it means.\n\nHere are 10 points that you need to understand Deep Learning. It is simple, you just need to understand what it is not, and understand what it is.\n\nExpert systems, semantic web and deductive logic systems are examples of systems that are based on symbolic logic. These systems are typically associated with AI. They all do work, however they have one shortcoming: They are unable to effectively learn from the data.\n\n2. Deep Learning is Radically Different from Machine Learning\n\nMachine Learning in its most basic distillation is \u201ccurve fitting\u201d. That is, if you have an algorithm that is able to find the best fit of your mathematical model with observed data, then that\u2019s Machine Learning. DL at its earlier incarnation was about \u201ccurve fitting\u201d, however it has progressed beyond that in recent years. Deep Learning Meta-Learning should be a big indicator to anyone that this is indeed very different.\n\nThe architecture of DL have are nowhere close to a biological neuron in structure. Even in behavior they are different. Biological neurons work on spiking behavior, DL system work in a continuous dynamical system. Some DL systems use Artificial Neural Networks, but that is just historic terminology that exists to this day. Anyone explaining DL in terms of biological neurons really doesn\u2019t know what they are talking about. DL isn\u2019t designed to \u2018mimic\u2019 biology, DL just happens to be a computational architecture that learns surprisingly well.\n\nDL can do some fantastic things like cross translate between different human languages and read out captions from images. However, the intelligence is really specialized and narrow. Sure DL can drive cars, but that\u2019s nowhere near the capability of AGI.\n\nThere was a Wired article titled \u201cDeep Learning isn\u2019t a Dangerous Genie, it is Just Math\u201d. This is really the most vacuous statement I\u2019ve heard! It is like saying that computers are just boolean circuits or brains are just made up of neurons or DL are made up of layers that are described using mathematical functions. It doesn\u2019t explain the emergent complex behavior you find in computers, brains and DL systems.\n\nClassical statistics is about analyzing data using aggregate measures. DL systems however work in a domain that statistical methods do not apply. That is high-dimensional data with high mutual information among the variables. Simplifying i.i.d. (i.e. Independent and identically distributed) assumptions are simply not applicable.\n\nBig Data is a technology that is based on the idea that if you are able to store and compute through a massive amount of data, typically hosted in hundreds or thousands of off-the-shelf computers, then you can gain insight. DL is an algorithm that can sit on a single machine and can incrementally, special emphasis on incrementally, process your data to learn from it. Big Data can crunch massive amounts of data, but just because you can process a lot of data doesn\u2019t mean you can derive insight or learn from the data. One last point, unlike Big Data, DL doesn\u2019t need a lot of data to be useful.\n\n8. Deep Learning is not understood by Data Scientists\n\nData Scientists are trained to do modeling of data, feature engineering and data analysis. DL just does what a Data Scientist does but without a human in the loop. This is actually a bit of an exaggeration. The reality is that most Data Scientists trained in other methods have not come up to speed with DL techniques.\n\n9. Deep Learning is not just Artificial Neural Networks or Multi-Level Perceptrons\n\nANN or MLPs were developed way back in the 1950s. DL systems originate from this earlier work, however in recent years they\u2019ve evolved to new kinds of models like Convolution networks, Long Short Term Memory, Residual Networks etc. The field has a much richer collection of concepts than existed when you studied it in graduate school.\n\n10. Deep Learning is the reason for the current AI hype\n\nFinally, this is where the greatest confusion exists. On a daily basis, the press continues to report the amazing progress of AI. Furthermore, you hear about firms like Google and Microsoft changing their entire software DNA to move into AI. The reason for this massive migration is because of Deep Learning. The big problem for the majority of the readers is that, the phrase itself \u201cDeep Learning\u201d is just too difficult to comprehend.\n\nI hope this gives you a frame of reference, a lay of the land, a sketch of where exactly Deep Learning does not fit. You might still be perplexed about reading this considering I haven\u2019t defined Deep Learning it all! My apologies, but unfortunately it\u2019s a complex subject, however more detail can be found at Design Patterns for Deep Learning or start a conversation at FaceBook or LinkedIn.\n\nHowever, if you are pressed for time and need one sentence to describe Deep Learning, it is just \u201cNon-Equilibrium Information Dynamics\u201d", 
        "title": "9 Misconceptions About Deep Learning \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://gab41.lab41.org/nips-2016-review-day-1-6e504bcf1451?source=tag_archive---------1----------------", 
        "text": "Good morning, fellow machine learners. A few of us from Lab41 recently jumped the pond over to Barcelona, Spain, to see what machine learning and artificial intelligence stuffs we could glean from eager minds. We found former colleagues, former students, and more deep learning algorithms than the number of cat pictures on the internet.\n\nThe organizers came out before the keynote (Yann LeCun) to introduce us to NIPS 2016. They pulled together some statistics from the tags that submissions self-identified with. According to the picture below, the distribution of papers is heavy tailed, and the spread of topics makes for a rich problem set. That\u2019s a first order statement, since there seems to be high correlation between topics in a given paper. (I\u2019m sure large scale learning can be applied to computer vision, and they\u2019re using deep learning to do it.) Still, there\u2019s a wide variety of things you can see here.\n\nEver the scientists, the two organizers justified their choice on the program committee by maintaining that they want to grow the number submissions while decreasing bias and variance. They treated the problem with unknown ground truth of what the \u201cbest papers\u201d were,\n\nKeynote\u200a\u2014\u200aYann LeCun gave the keynote on \u201cPredictive Learning\u201d, an ambiguous title of a talk he must have presented a million times by now. It\u2019s the one where he makes an analogy to parts of a cake: unsupervised learning is the filling of the cake\u2026the big kahuna, supervised learning is frosting on the cake, and reinforcement learning is the cherry on top. It seems like he\u2019s been burned by this, and was apologetic, saying that he\u2019ll make it up (to Deep Mind, maybe?) because reinforcement learning, if aided by unsupervised approaches, is also a big chunk of where research should lie. He pretty much gave an overview of what he viewed as important topics. Among other big ideas, he said that Generative Adversarial Networks were the most important innovation in machine learning in recent times, and he credited Ian Goodfellow. High praise from one of the elders of Deep Learning.\n\nIntelligent Biosphere\u200a\u2014\u200aGoogle Deep Mind\u2019s invited talk to kick off the first day on the Intelligent Biosphere. Drew Purves from Deep Mind is the speaker, and it\u2019s a refreshing insight into how AI can be used for social good. The premise is that AI can help Nature, but on a less intuitive note, Nature can help AI. On the former, if you think hard about it, you\u2019ll know that statistics and machine learning can aid policy makers on producing less waste in farming, agriculture, and other efficiency saving measures. Besides augmenting efficiency, breakthroughs can occur to make new innovations that can aid humanity. On the latter, he made the distinction between natural versus artificial and real versus simulated. The world is scale-less, cyclic on many levels, fuzzy, and just plain hard to work with. On the other hand, everything we train has been scoped. We can take cues to build better simulations of the real world.\n\nThere was a whole bunch of other stuff, but the idea is how to make sure machine learning algorithms applied to the real and natural, and so they were proud to introduce their real-world simulator. And\u2026their slides are, by far, the most stylish.\n\nOne fun note is that Purves pointed to two high school students getting a head start on things. These guys had started playing around with Tensorflow, and it\u2019s encouraging to see the future talent come. I was slightly disappointed that there wasn\u2019t any applause, but Purves moved on quickly and needed to wrap up.\n\nBest Paper Award: Value Iteration Networks\u2014 The best paper award went to Aviv Tamar, who talked about Value Iteration Networks, from Berkeley\u2019s Artificial Intelligence Research Laboratory. It was a new look at reinforcement learning. He wanted to build a neural network that can learn to plan a policy rather than follow a totally reactive policy. He also wanted it to be model-free, and what better way to do that than to use CNNs? His paper is on ArXiV and will appear in proceedings.\n\nSome big themes that I noticed, people were either using batch normalization or layer normalization, it has become somewhat of a mantra. I noticed at the posters, there was something called Weight Normalization too; I\u2019ll report on that tomorrow when it\u2019s in an oral.\n\nThe themes of the conference were:\n\nOn more specific themes, I happen to be working on an audio project currently, so maybe my thoughts are a bit skewed on which things are most interesting. If you have alternative views of what was going on in the conference, please do send me a note.\n\nWho knew that one dimensional signals would give us so much trouble. Time series and audio were very present at this year\u2019s NIPS, the hard problems, of course, being non-stationary signals.\n\nAapo Hyvarinen (Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA) was first in the unsupervised track with nonlinear ICA, where he asserted that independent components in nonlinear systems are difficult to obtain because the inverse problem (of a deep learning function) is impossible to solve. He proposes using Time-Contrastive Learning, which seems like another \u201csomething\u201d-2vec at first glance.\n\nOne talk, Using Fast Weights to Attend to the Recent Past, was on fast and slow weights for LSTMs. There is slow varying, longer term information and then there\u2019s fast weight rapid learning also decaying rapidly, storing specific temporary information. Solution? Add a layer to attend to the fast stuff.\n\nOther works dealt with compressing neural networks and providing approximate bounds on them (a la Supervised learning through the lens of compression). The concept was simple: you add an epsilon to your hypothesis, and say that you\u2019re compressing alright if you match your truth+ epsilon instead of just truth.\n\nPhased LSTMs (PLSTMs)\u200a\u2014\u200aAs if LSTMs aren\u2019t complicated enough, we\u2019d like to put more stuff in there. It seems like a sampler, but sampling at different phase. They built this to deal with really long sequences, and they sampled at regular frequencies (think Wavelets style: at low, medium, high frequencies) by putting a gates inside the LSTM.\n\nSRNNs\u200a\u2014\u200aThis one got some flak from Li Deng and others: use recurrent neural networks (RNNs) in combination with State Space Models (SSMs). RNN\u2019s are good at long-term dependencies. And SSMs are good at modeling uncertainty. So, let\u2019s put\u2019m together! Then, model non-stationary stuff.\n\nLots of GANs, here. Also, Ian Goodfellow gave a shout out to plug and play generative networks, which I saw at the posters. The idea was to synthesize images, but do it with a prior so that it doesn\u2019t look so creepy. Here are some of the images.\n\nOne of the practical papers on seeding k-means and improving upon k-means++ was Fast and Provably Good Seedings for k-Means by Oliver Bachem. The idea is to use MCMC to jump around the clusters in an efficient manner and seed the k-means with good clusters.\n\nHaving a discussion with another conference-goer, I learned that European conferences have a reputation for cutting registration right off. I contrasted with my experience at ICML in New York, where they booked an entirely different venue to supplement the overflow of conference participants. Up and downsides.\n\nI don\u2019t know if it\u2019s now commonplace, but WiFi is intermittent, which is frustrating. But on the positive side, the conference app, \u201cWhova\u201d, is neat. The forum is worth checking out. You should note that \u201cWhova\u201d actually doesn\u2019t need the internet, so this mitigates the fact that not everyone got hard-copy programs. The poster session was packed; it was clear that this was one of the largest conferences I\u2019ve been to in a while!\n\nI personally attended Nonstationary Time Series and Generative Adversarial Networks tutorials, where it was so crowded, people stood on the sides. On this point, I must apologize to the twenty people that I had to step over to get to my seat. I promise that I would have been just fine sitting on the sides, but twenty minutes after the lecture started, the security guard kicked everyone off from the edges, and made us all find seats. For good reason though, I think it\u2019s good that there\u2019s strict adherence to their fire codes in mind.\n\nIan Goodfellow delivered the GANs tutorial. So\u2026Las Fallas is a celebration in Spain in March, but there were some fireworks today. I\u2019m not going to gossip, but there was some contention about who came up with what first. Goodfellow shut it down pretty hard and quick.\n\nPlease let us know if there\u2019s anything that I missed, since I was severely jet lagged. More fun tomorrow! Please let us know what other fun things were at NIPS if you\u2019ve attended via e-mail (kni@iqt.org). Tune in tomorrow for the second blog post on NIPS 2016. It\u2019s a blast out here in Barcelona, and we love sharing what we\u2019re seeing!", 
        "title": "NIPS 2016 Review, Days 0 & 1 \u2013"
    }, 
    {
        "url": "https://medium.com/@amarbudhiraja/towards-weight-initialization-in-deep-neural-networks-908d3d9f1e02?source=tag_archive---------2----------------", 
        "text": "This post will be more technical compared to the other two. In this post, I will try to analyze how different sets of weights influence training and accuracy. The main motivation is to explore how well are Glorot[1] weight initialization compared to other trivial options for initialization.\n\nIn this post, I will try to compare the following weight initializations:\n\nFor the validation, I decided to use a custom convolutional neural network similar to VCG-Net and ran it on MNIST dataset.\n\nResults are something as follows (surprising):", 
        "title": "Towards Weight Initialization in Deep Neural Networks"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-have-we-lost-control-of-the-objective-function-fda51b075350?source=tag_archive---------3----------------", 
        "text": "Classical Machine Learning (ML) is based on setting a system with an objective function and finding a minimal (or maximal, depending on which direction you are lookin) solution to this objective function. A set of data is not included in the search (or optimization ), with the purpose of verifying generalization or avoidance of overfitting. Ideally, we would like this objective function to be in analytic form (meaning you can write down the math!).\n\nWhat if the objective function was implicit, in other words, you did not know how to calculate the gradient? You can still get away with this using a finite difference equivalent or some other surrogate approximation. However, at least we still know in which direction the optimization is supposed to proceed (i.e. minimization or maximization).\n\nWhat if however, that we did not even know the direction of the optimization?\n\nIf you read my earlier piece on \u201cGame Theory and Deep Learning\u201d, you\u2019ll get a sense of where Deep Learning is evolving towards. In this new world of multiple agent neural networks and multiple objectives (also see: \u201cThe 5 Capability Levels of Deep Learning Intelligence\u201d) you find yourself in the twilight zone where the machine itself is tasked with not only finding an equilibrium state but even learning an objective function!\n\nHere are two recent research papers with some intriguing ideas. The first one is \u201cImproved Techniques for Training GANs\u201d the authors (Goodfellow et al.) discuss various heuristics. The write:\n\nThe first quote implies the need to understand the appropriate direction of an optimization to achieve equilibrium. The second alludes to the missing objective function.\n\nAnother paper, \u201cImage-to-Image Translation with Conditional Adversarial Networks\u201d, interprets the implicit objective function in terms of \u201cLearning a Objective Function\u201d:\n\nA picture is worth a thousand words, so here are their surprising results. This is not for many neural networks, it is just one network that seems to be able to selectively determine its own objective function:\n\nWelcome to the twilight zone.", 
        "title": "Crafting Deep Learning Objective Functions now Obsolete"
    }, 
    {
        "url": "https://medium.com/@Francesco_AI/insights-on-insurance-and-ai-e8934a7ee401?source=tag_archive---------4----------------", 
        "text": "Artificial Intelligence (AI) is revolutionizing every industry, and insurance will be affected as well. As already stated in previous posts, AI today is perceived in three different ways: it is something that might answer all your questions, with an increasing degree of accuracy (\u201cthe Oracle\u201d); it could do anything it is commanded to do (\u201cthe Genie\u201d), or it might act autonomously to pursue a certain long-term goal (\u201cthe Sovereign\u201d). My personal definition is the following one:\n\nAn artificial intelligence is a system that can learn how to learn, or in other words a series of instructions (an algorithm) that allows computers to write their own algorithms without being explicitly programmed for.\n\nAn artificial engine can also be classified in three ways: a narrow AI, which is nothing more than a specific domain application or task that gets better by ingesting further data and \u201clearns\u201d how to reduce the output error. An example here is DeepBlue for the chess game, but more generally this group includes all the functional technologies that serve a specific purpose. These systems are usually quite controllable because limited to specific tasks.\n\nWhen a program is instead not programmed for completing a specific task, but it could eventually learn from an application and apply the same bucket of knowledge to different environments, we face an Artificial General Intelligence (AGI). This is not technology-as-a-service as in the narrow case, but rather technology-as-a-product. The best example for this subgroup is Google DeepMind, although it is not a real AGI in all respects.\n\nThe final stage is instead called Superintelligent AI (ASI): this intelligence exceeds largely the human one, and it is able of scientific and creative thinking; it is characterized by general common wisdom; it has social skills and maybe an emotional intelligence.\n\nRegardless of the current stage of AI development, the insurance industry will be disrupted from several perspectives: first of all, every process that is done manually today will be automatized in a smart way (e.g., claims processing and management) to reduce costs and improve the UX. This would turn into a better fraud detection as well as more efficient loss prevention. The second block is obviously telematics and Internet of Things applications because innovative devices collect new data that can widen the horizon of insurable risks as well as refine the customized pricing. More generally, underwritings and more granular pricing will be better defined using machine learning techniques that spot out unknown meaningful correlations. Finally, customer acquisition and experience in a sector which is historically reserved for human agents will become completely digital: chatbots, more effective customer classification and targeting, and personalized contents and policies are the main immediate benefits from investments in AI technologies.\n\nHistorically, insurance is sold, not bought. But AI is coming, and it will undermine this principle. Is the insurance industry ready for such a change?", 
        "title": "Insights on Insurance and AI \u2013 Francesco Corea \u2013"
    }, 
    {
        "url": "https://blog.kelsus.com/actually-one-more-point-id-like-to-make-even-though-i-just-told-you-that-you-should-just-try-1cb7006be14c?source=tag_archive---------5----------------", 
        "text": "Actually one more point I\u2019d like to make\u200a\u2014\u200aeven though I just told you that you should just try this and see if it works, there\u2019s another model that seems to approximate what you\u2019re trying to do a little more closely than plain old RNNs. Take a look at what people are accomplishing with WaveNets: https://deepmind.com/blog/wavenet-generative-model-raw-audio/\u00a0.. Here\u2019s a Tensorflow implementation of one: https://github.com/ibab/tensorflow-wavenet\n\nThe reason I think wavenets might be closer to what you\u2019re looking to do is that audio waves just intuitively feel more like stock market data to me than text strings. Predicting what\u2019s coming next in an audio wave might be very similar to predicting what\u2019s coming next in a stock price. Especially if it just so happens that patterns in stock price shape are have a similar kind of importance to patterns in audio wave shape.\n\nYou might also be able normalize your data and feed it into a wavenet with less weird number crunching and matrix manipulation that you\u2019d have to do in your market-to-vec plan.", 
        "title": "Actually one more point I\u2019d like to make \u2014 even though I just told you that you should just try\u2026"
    }, 
    {
        "url": "https://blog.kelsus.com/definitely-some-interesting-thoughts-here-2fe2b2bed0a2?source=tag_archive---------6----------------", 
        "text": "Definitely some interesting thoughts here. I too have wondered whether there are underlying patterns in the stock prices themselves that could be teased out with the right deep learning model. A lot of people are quick to point out that correlating the prices to something else (news, tweets, etc) would be more likely to succeed, but no one actually gave you evidence that what you\u2019re going to do doesn\u2019t work. Only opinions.\n\nThat\u2019s what\u2019s great about machine learning today. You can spend a couple months on the problem and have a concrete answer and it may surprise you! No need to listen to the experts that tell you you\u2019re probably not right.", 
        "title": "Definitely some interesting thoughts here. \u2013"
    }, 
    {
        "url": "https://medium.com/@andrew.d.wilkie/self-driving-car-engineer-diary-1-33cf9f8ff1cd?source=tag_archive---------7----------------", 
        "text": "Hi. This is the 1st in a series of short posts where I will capture what I found most interesting during my education journey to become a qualified Self-Driving Car Engineer. Hope you enjoy the ride as much as me!", 
        "title": "Self-Driving Car Engineer Diary \u2014 1 \u2013 Andrew Wilkie \u2013"
    }, 
    {
        "url": "https://medium.com/@maedabr/introdu%C3%A7%C3%A3o-para-a-s%C3%A9rie-deep-learning-a-jornada-dafa25202d8f?source=tag_archive---------8----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Introdu\u00e7\u00e3o para a s\u00e9rie Deep Learning \u2014 A Jornada \u2013 Andherson Maeda \u2013"
    }, 
    {
        "url": "https://medium.com/bitviu/nos-reemplazar%C3%A1-la-inteligencia-artificial-parte-1-beedceb1d112?source=tag_archive---------9----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u00bfNos reemplazar\u00e1 la Inteligencia Artificial? Parte 1"
    }
]