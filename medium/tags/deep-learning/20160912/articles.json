[
    {
        "url": "https://medium.com/axiomzenteam/an-honest-guide-to-machine-learning-2f6d7a6df60e?source=tag_archive---------0----------------", 
        "text": "One investor joked that adding AI to the end of your startup\u2019s name would increase investment by 10%\u200a\u2014\u200aand while he was exaggerating, the truth is that artificial intelligence is on everyone\u2019s minds. What exactly do we mean when we talk about artificial intelligence? It doesn\u2019t necessarily mean a computer that thinks exactly like a human\u200a\u2014\u200aalthough that is the end goal of most AI research. Artificial intelligence is a smart agent that has input from sensors and, based on incoming signals, internal knowledge, and some decision making systems, can decide on its next action. Fulfilling this simple goal resulted in a vast area of research and development, answering questions like: Where does knowledge come from? How should these smart agents talk to each other? Currently, researchers are using machine learning to tackle the third problem: knowledge. This can be done by mapping input signals to human added knowledge, or by automatically collecting knowledge from different sources of input. When we talk about artificial intelligence that can solve only one problem, we call it narrow AI. Narrow AI is in our phones; it\u2019s in our messaging services; it\u2019s trading stocks, designing websites, and making even basic programs run more smoothly. Machine learning is the path we\u2019re taking to try to achieve AI, and it\u2019s becoming harder and harder to design software without using some form of machine learning. This is leaving a generation of engineers and developers without the tools they need to excel in their field. While there are plenty of guides out there to machine learning\u200a\u2014\u200aintroductions, how tos, 101s\u200a\u2014\u200athe problem is that even \u201csimple guides\u201d bombard readers with advanced mathematical formulas. So what do you do if you want to learn how to get into machine learning, but you don\u2019t have a math degree? This is where we come in. We want to create a guide to machine learning without the math, and without the magic. Over the next few weeks, we\u2019ll be detailing a journey from layperson to expert with An Honest Guide to Machine Learning. Learn the ins and outs, the whys and wherefores, and do it all without once breaking out your calculator. Today we\u2019re starting things off nice and simple: with a basic history of what machine learning is, and why we\u2019re all so gungho about it. Welcome to most machine learning papers. The cute language and casual tone do nothing to disguise the info dump of formulas. That terrifying figure above? It\u2019s explaining the basic idea of a machine learning technique called Support Vector Machines, and it can be summed up in two thoughts: \u201cDo not completely trust your data because of the noise, and do your best to generalize your model to cover future data.\u201d\n\nOur first attempt to mimic human intelligence in a computerized form involved copying the human brain. We wanted to see if we could make an artificial version, which would fire the same way neurons fire in the brain. We ran into two problems with this branch of research: the first was that the human brain was simply too complicated. With 100 billion brain cells linking to tens of thousands of possible connections, the computational power required to mimic that flow was simply beyond us. It couldn\u2019t be done. The second problem was the limitations in our understanding of how the brain works. We still have so much to learn about how our minds operate\u200a\u2014\u200ahow can we copy something we so imperfectly understand?\n\nThe next stage in AI research involved, for the most part, eliminating the attempt to recreate the brain. Instead it broke down a brain as if it were a computer, and attempted to model its behaviour. Input became sight, sound, smell, and touch, and once they passed through the brain into signals, they became output: decisions, communication skills, and categorizations. This model was very popular, but far too complicated. Trying to find one algorithm that could understand sight (categorization) and sound (natural language processing), for instance, seemed like a pipe dream. So the different branches of AI were born, and began to change the way we looked at AI. There are still \u201cpurists\u201d who feel that narrow AI is not AI at all, and many a cerebral argument has been had on the differences between narrow and general AI, and what the definition of intelligence really is. This was a great step in the right direction, but input can be narrowed into only five or six basic sources\u200a\u2014\u200atext, audio, and visual (we have yet to conquer the input of scent or touch). A new way to narrow the focus of problems was needed. So instead of dividing by input, we began dividing by output. This made it much easier to create focus, narrow solutions. Instead of focusing on the input of vision, for instance, we could divide into focusing on facial recognition, or focusing on finding similar images. This was much more effective, and gave us excellent narrow AI. Technology like this is still being used by Google\u2019s search engines, for instance, or Facebook\u2019s tagging algorithms. The problem is that it doesn\u2019t scale up: applying it to anything outside of the incredibly narrow focus you\u2019ve defined doesn\u2019t work. If I teach an algorithm to tell the difference between a cat and a dog, and then I show it a picture of a tree? It will classify it as either a cat or dog, because it doesn\u2019t know there are any other options. This was clearly not the way of the future. Some researchers tried to solve this using rule-based solutions, sometimes called Classic or Symbolic AI. To create classic AI, researchers create a list of logical rules, and a reasoning system to draw conclusions. The problem is that writing a rule is not trivial\u200a\u2014\u200ait\u2019s incredibly time consuming, and sometimes it isn\u2019t clear what is true and what isn\u2019t, which can result in arguments about what is considered a fact. While it \u201cworks,\u201d it isn\u2019t sustainable as a commercial solution to all of our problems.\n\nOther researchers used statistics and pattern recognition to let computers find the rules they need. Called Statistical Machine Learning, this is where giant chalkboards full of mathematical formulas come from. Looks pretty impressive, no? The problem is that once you\u2019ve found the formula, it still needs to be optimized\u200a\u2014\u200athis is a step, but not an end point. So, where do you head? Back to your math teacher to optimize the formula. This is where we\u2019ve landed today\u200a\u2014\u200awe use the objective function to extract patterns, and then we optimize the objective function. This has both strengths and weaknesses. It\u2019s one of the best solutions we\u2019ve found, but it\u2019s still imperfect. Optimization isn\u2019t always possible, and getting enough input data can be a real challenge! All those articles about sample bias corrupting machine learning? That\u2019s because there simply isn\u2019t enough input data to eliminate any, even stuff that\u2019s biased. Unfortunately, our computers still don\u2019t have the computational power necessary to work as quickly and efficiently as we want them to, and while we\u2019re now making amazing strides at solving the small tasks\u200a\u2014\u200athat tree that was being categorized as a dog or a cat? That\u2019s still a problem. Our incredible work into narrow AI has nothing little (or nothing) to help us solve the problem of creating a general (true) artificial intelligence. The End of the Beginning Hopefully you now understand more about artificial intelligence, its connection to machine learning, and the different approaches to machine learning. Throughout the rest of the series we\u2019ll talk about high level information machine learning tasks, and techniques for each type of input. Part 2 of our Honest Guide to Machine Learning\n\nPart 3 of our Honest Guide to Machine Learning", 
        "title": "An Honest Guide to Machine Learning: Part One \u2013 Axiom Zen Team \u2013"
    }, 
    {
        "url": "https://gab41.lab41.org/lab41-reading-group-squeezenet-9b9d1d754c75?source=tag_archive---------1----------------", 
        "text": "The next paper from our reading group is by Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, Song Han, William J. Dally and Kurt Keutzer. This paper introduces a small CNN architecture called \u201cSqueezeNet\u201d that achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. As you may have noticed with one of our recent posts we\u2019re really interested in learning more about the compression of neural network architectures and this paper really stood out.\n\nIt\u2019s no secret that much of deep learning is tied up in the hell that is parameter tuning. This paper makes a case for increased study into the area of convolutional neural network design in order to drastically reduce the number of parameters you have to deal with. Unlike our previous post on \u201cdeep compression\u201d, this paper proposes making a network smaller by starting with a smarter design versus using a clever compression scheme. The authors outline 3 main strategies for reducing parameter size while maximizing accuracy. I\u2019ll walk you through them now.\n\nThis strategy reduces the number of parameters 9x by replacing a bunch of 3x3 filters with 1x1 filters. At first this seemed really confusing to me. By moving 1x1 filters across an image I would think that each filter has less information to look at and would thus perform more poorly, however that doesn\u2019t seem to be the case! Typically a larger 3x3 convolution filter captures spatial information of pixels close to each other. On the other hand, 1x1 convolutional filters zero in on a single pixel and capture relationships amongst its channels as opposed to neighboring pixels. If you are looking to learn more about the use of 1x1 filters check out this blog post.\n\nThis strategy reduces the number of parameters by basically just using fewer filters. The systematic way this is done is by feeding \u201csqueeze\u201d layers into what they term \u201cexpand\u201d layers as shown below:\n\nTime to define some terms that are specific only to this paper! As you can see above, \u201csqueeze\u201d layers are convolution layers that are made up of only 1x1 filters and \u201cexpand\u201d layers are convolution layers with a mix of 1x1 and 3x3 filters. By reducing the number of filters in the \u201csqueeze\u201d layer feeding into the \u201cexpand\u201d layer, they are reducing the number of connections entering these 3x3 filters thus reducing the total number of parameters. The authors of this paper call this specific architecture the \u201cfire module\u201d and it serves as the basic building block for the SqueezeNet architecture.\n\nNow that we have talked about ways to reduce the sheer number of parameters we are working with, how can we get the most out of our remaining set of smaller parameters? The authors believe that by decreasing the stride with later convolution layers and thus creating a larger activation/feature map later in the network, classification accuracy actually increases. Having larger activation maps near the end of the network is in stark contrast to networks like VGG where activation maps get smaller as you get closer to the end of a network. This different approach is very interesting and they cite a paper by K. He and H. Sun that similarly applies a delayed down sampling that leads to higher classification accuracy.\n\nSo how does this all fit together?\n\nSqueezeNet takes advantage of the aforementioned \u201cfire module\u201d and chains a bunch of these modules together to arrive at a smaller model. Here are a few variants of this chaining process as shown in their paper:\n\nOne of the surprising things I found with this architecture is the lack of fully-connected layers. What\u2019s crazy about this is that typically in a network like VGG, the later fully connected layers learn the relationships between the earlier higher level features of a CNN and the classes the network is trying to identify. That is, the fully connected layers are the ones that learn that noses and ears make up a face, and wheels and lights indicate cars. However, in this architecture that extra learning step seems to be embedded within the transformations between various \u201cfire modules\u201d. The authors derived inspiration for this idea from the NiN architecture.\n\nEnough with the details! How does this perform?\n\nThe authors of the paper show some impressive results.\n\nTheir SqueezeNet architecture was able to achieve a 50X reduction in model size compared to AlexNet while meeting or exceeding the top-1 and top-5 accuracy of AlexNet. But perhaps the most interesting part of this paper is their application of Deep Compression (explained in our previous post) to their already smaller model. This application of Deep Compression created a model that was 510x smaller than AlexNet! These results are really encouraging because it shows the potential for combining different approaches for compression. As next steps I would love to see how this type of design thinking can apply to other neural network architectures and deep learning applications.\n\nIf this paper seems interesting to you, definitely check out their open source code for SqueezeNet on Github. I only had time to cover the highlights but their paper is full of in depth discussions on parameter reducing CNN design.", 
        "title": "Lab41 Reading Group: SqueezeNet \u2013"
    }, 
    {
        "url": "https://medium.com/innoarchitech/artificial-intelligence-deep-learning-neural-networks-explained-22a3cc132580?source=tag_archive---------2----------------", 
        "text": "Originally published at innoarchitech.com here on September 1, 2016.\n\nArtificial intelligence (AI), deep learning, and neural networks represent incredibly exciting and powerful machine learning-based techniques used to solve many real-world problems. For a primer on machine learning, you may want to read this five-part series that I wrote.\n\nWhile human-like deductive reasoning, inference, and decision-making by a computer is still a long time away, there have been remarkable gains in the application of AI techniques and associated algorithms.\n\nThe concepts discussed here are extremely technical, complex, and based on mathematics, statistics, probability theory, physics, signal processing, machine learning, computer science, psychology, linguistics, and neuroscience.\n\nThat said, this article is not meant to provide such a technical treatment, but rather to explain these concepts at a level that can be understood by most non-practitioners, and can also serve as a reference or review for technical folks as well.\n\nThe primary motivation and driving force for these areas of study, and for developing these techniques further, is that the solutions required to solve certain problems are incredibly complicated, not well understood, nor easy to determine manually.\n\nIncreasingly, we rely on these techniques and machine learning to solve these problems for us, without requiring explicit programming instructions. This is critical for two reasons. The first is that we likely wouldn\u2019t be able, or at least know how to write the programs required to model and solve many problems that AI techniques are able to solve. Second, even if we did know how to write the programs, they would be inordinately complex and nearly impossible to get right.\n\nLuckily for us, machine learning and AI algorithms, along with properly selected and prepared training data, are able to do this for us.\n\nSo with that, let\u2019s get started!\n\nIn order to define AI, we must first define the concept of intelligence in general. A paraphrased definition based on Wikipedia is:\n\nWhile there are many different definitions of intelligence, they all essentially involve learning, understanding, and the application of the knowledge learned to achieve one or more goals.\n\nIt\u2019s therefore a natural extension to say that AI can be described as intelligence exhibited by machines. So what does that mean exactly, when is it useful, and how does it work?\n\nA familiar instance of an AI solution includes IBM\u2019s Watson, which was made famous by beating the two greatest Jeopardy champions in history, and is now being used as a question answering computing system for commercial applications. Apple\u2019s Siri and Amazon\u2019s Alexa are similar examples as well.\n\nIn addition to speech recognition and natural language (processing, generation, and understanding) applications, AI is also used for other recognition tasks (pattern, text, audio, image, video, facial,\u00a0\u2026), autonomous vehicles, medical diagnoses, gaming, search engines, spam filtering, crime fighting, marketing, robotics, remote sensing, computer vision, transportation, music recognition, classification, and so on.\n\nSomething worth mentioning is a concept known as the AI effect. This describes the case where once an AI application has become somewhat mainstream, it\u2019s no longer considered by many as AI. It happens because people\u2019s tendency is to no longer think of the solution as involving real intelligence, and only being a application of normal computing.\n\nThis despite the fact that these applications still fit the definition of AI regardless of widespread usage. The key takeaway here is that today\u2019s AI is not necessarily tomorrow\u2019s AI, at least not in some people\u2019s minds anyway.\n\nThere are many different goals of AI as mentioned, with different techniques used for each. The primary topics of this article are artificial neural networks and an advanced version known as deep learning.\n\nThe human brain is exceptionally complex and quite literally the most powerful computing machine known.\n\nThe inner-workings of the human brain are often modeled around the concept of neurons and the networks of neurons known as biological neural networks. According to Wikipedia, it\u2019s estimated that the human brain contains roughly 100 billion neurons, which are connected along pathways throughout these networks.\n\nAt a very high level, neurons interact and communicate with one another through an interface consisting of axon terminals that are connected to dendrites across a gap (synapse) as shown here.\n\nIn plain english, a single neuron will pass a message to another neuron across this interface if the sum of weighted input signals from one or more neurons (summation) into it is great enough (exceeds a threshold) to cause the message transmission. This is called activation when the threshold is exceeded and the message is passed along to the next neuron.\n\nThe summation process can be mathematically complex. Each neuron\u2019s input signal is actually a weighted combination of potentially many input signals, and the weighting of each input means that that input can have a different influence on any subsequent calculations, and ultimately on the final output of the entire network.\n\nIn addition, each neuron applies a function or transformation to the weighted inputs, which means that the combined weighted input signal is transformed mathematically prior to evaluating if the activation threshold has been exceeded. This combination of weighted input signals and the functions applied are typically either linear or nonlinear.\n\nThese input signals can originate in many ways, with our senses being some of the most important, as well as ingestion of gases (breathing), liquids (drinking), and solids (eating) for example. A single neuron may receive hundreds of thousands of input signals at once that undergo the summation process to determine if the message gets passed along, and ultimately causes the brain to instruct actions, memory recollection, and so on.\n\nThe \u2018thinking\u2019 or processing that our brain carries out, and the subsequent instructions given to our muscles, organs, and body are the result of these neural networks in action. In addition, the brain\u2019s neural networks continuously change and update themselves in many ways, including modifications to the amount of weighting applied between neurons. This happens as a direct result of learning and experience.\n\nGiven this, it\u2019s a natural assumption that for a computing machine to replicate the brain\u2019s functionality and capabilities, including being \u2018intelligent\u2019, it must successfully implement a computer-based or artificial version of this network of neurons.\n\nThis is the genesis of the advanced statistical technique and term known as artificial neural networks.\n\nArtificial neural networks (ANNs) are statistical models directly inspired by, and partially modeled on biological neural networks. They are capable of modeling and processing nonlinear relationships between inputs and outputs in parallel. The related algorithms are part of the broader field of machine learning, and can be used in many applications as discussed.\n\nArtificial neural networks are characterized by containing adaptive weights along paths between neurons that can be tuned by a learning algorithm that learns from observed data in order to improve the model. In addition to the learning algorithm itself, one must choose an appropriate cost function.\n\nThe cost function is what\u2019s used to learn the optimal solution to the problem being solved. This involves determining the best values for all of the tunable model parameters, with neuron path adaptive weights being the primary target, along with algorithm tuning parameters such as the learning rate. It\u2019s usually done through optimization techniques such as gradient descent or stochastic gradient descent.\n\nThese optimization techniques basically try to make the ANN solution be as close as possible to the optimal solution, which when successful means that the ANN is able to solve the intended problem with high performance.\n\nArchitecturally, an artificial neural network is modeled using layers of artificial neurons, or computational units able to receive input and apply an activation function along with a threshold to determine if messages are passed along.\n\nIn a simple model, the first layer is the input layer, followed by one hidden layer, and lastly by an output layer. Each layer can contain one or more neurons.\n\nModels can become increasingly complex, and with increased abstraction and problem solving capabilities by increasing the number of hidden layers, the number of neurons in any given layer, and/or the number of paths between neurons. Note that an increased chance of overfitting can also occur with increased model complexity.\n\nModel architecture and tuning are therefore major components of ANN techniques, in addition to the actual learning algorithms themselves. All of these characteristics of an ANN can have significant impact on the performance of the model.\n\nAdditionally, models are characterized and tunable by the activation function used to convert a neuron\u2019s weighted input to its output activation. There are many different types of transformations that can be used as the activation function, and a discussion of them is out of scope for this article.\n\nThe abstraction of the output as a result of the transformations of input data through neurons and layers is a form of distributed representation, as contrasted with local representation. The meaning represented by a single artificial neuron for example is a form of local representation. The meaning of the entire network however, is a form of distributed representation due to the many transformations across neurons and layers.\n\nOne thing worth noting is that while ANNs are extremely powerful, they can also be very complex and are considered black box algorithms, which means that their inner-workings are very difficult to understand and explain. Choosing whether to employ ANNs to solve problems should therefore be chosen with that in mind.\n\nDeep learning, while sounding flashy, is really just a term to describe certain types of neural networks and related algorithms that consume often very raw input data. They process this data through many layers of nonlinear transformations of the input data in order to calculate a target output.\n\nUnsupervised feature extraction is also an area where deep learning excels. Feature extraction is when an algorithm is able to automatically derive or construct meaningful features of the data to be used for further learning, generalization, and understanding. The burden is traditionally on the data scientist or programmer to carry out the feature extraction process in most other machine learning approaches, along with feature selection and engineering.\n\nFeature extraction usually involves some amount dimensionality reduction as well, which is reducing the amount of input features and data required to generate meaningful results. This has many benefits, which include simplification, computational and memory power reduction, and so on.\n\nMore generally, deep learning falls under the group of techniques known as feature learning or representation learning. As discussed so far, feature extraction is used to \u2018learn\u2019 which features to focus on and use in machine learning solutions. The machine learning algorithms themselves \u2018learn\u2019 the optimal parameters to create the best performing model.\n\nParaphrasing Wikipedia, feature learning algorithms allow a machine to both learn for a specific task using a well-suited set of features, and also learn the features themselves. In other words, these algorithms learn how to learn!\n\nDeep learning has been used successfully in many applications, and is considered to be one of the most cutting-edge machine learning and AI techniques at the time of this writing. The associated algorithms are often used for supervised, unsupervised, and semi-supervised learning problems.\n\nFor neural network-based deep learning models, the number of layers are greater than in so-called shallow learning algorithms. Shallow algorithms tend to be less complex and require more up-front knowledge of optimal features to use, which typically involves feature selection and engineering.\n\nIn contrast, deep learning algorithms rely more on optimal model selection and optimization through model tuning. They are more well suited to solve problems where prior knowledge of features is less desired or necessary, and where labeled data is unavailable or not required for the primary use case.\n\nIn addition to statistical techniques, neural networks and deep learning leverage concepts and techniques from signal processing as well, including nonlinear processing and/or transformations.\n\nYou may recall that a nonlinear function is one that is not characterized simply by a straight line. It therefore requires more than just a slope to model the relationship between the input, or independent variable, and the output, or dependent variable. Nonlinear functions can include polynomial, logarithmic, and exponential terms, as well as any other transformation that isn\u2019t linear.\n\nMany phenomena observed in the physical universe are actually best modeled with nonlinear transformations. This is true as well for transformations between inputs and the target output in machine learning and AI solutions.\n\nAs mentioned, input data is transformed throughout the layers of a deep learning neural network by artificial neurons or processing units. The chain of transformations that occur from input to output is known as the credit assignment path, or CAP.\n\nThe CAP value is a proxy for the measurement or concept of \u2018depth\u2019 in a deep learning model architecture. According to Wikipedia, most researchers in the field agree that deep learning has multiple nonlinear layers with a CAP greater than two, and some consider a CAP greater than ten to be very deep learning.\n\nWhile a detailed discussion of the many different deep-learning model architectures and learning algorithms is beyond the scope of this article, some of the more notable ones include:\n\nIt\u2019s worth pointing out that due to the relative increase in complexity, deep learning and neural network algorithms can be prone to overfitting. In addition, increased model and algorithmic complexity can result in very significant computational resource and time requirements.\n\nIt\u2019s also important to consider that solutions may represent local minima as opposed to a global optimal solution. This is due to the complex nature of these models when combined with optimization techniques such as gradient descent.\n\nGiven all of this, proper care must be taken when leveraging artificial intelligence algorithms to solve problems, including the selection, implementation, and performance assessment of algorithms themselves. While out of scope for this article, the field of machine learning includes many techniques that can help with these areas.\n\nAI is an extremely powerful and exciting field. It\u2019s only going to become more important and ubiquitous moving forward, and will certainly continue to have very significant impacts on modern society.\n\nArtificial neural networks (ANNs) and the more complex deep learning technique are some of the most capable AI tools for solving very complex problems, and will continue to be developed and leveraged in the future.\n\nWhile a terminator-like scenario is unlikely any time soon, the progression of artificial intelligence techniques and applications will certainly be very exciting to watch!\n\n\u2192 Click here to visit my GitHub repo of resources related to data science, machine learning, artificial intelligence (AI), big data, and more!\n\nAbout the Author: Alex Castrounis founded InnoArchiTech. Sign up for the InnoArchiTech newsletter and follow InnoArchiTech on Twitter at @innoarchitech for the latest content updates.", 
        "title": "Artificial Intelligence, Deep Learning, & Neural Networks Explained"
    }, 
    {
        "url": "https://medium.com/@kimsvandenberg/visual-thinking-explore-your-mind-by-drawing-49d520d17c80?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Visual thinking \u2014 explore your mind by drawing \u2013 Kim S van den Berg \u2013"
    }, 
    {
        "url": "https://medium.com/extremetech-access/intel-snaps-up-movidius-to-accelerate-its-ai-and-realsense-vision-efforts-49966657b51a?source=tag_archive---------4----------------", 
        "text": "Custom silicon vendor Movidius has attracted a lot of attention for its high-performance, low-power chips that have powered vision applications like Google Tango, as well as making machine learning possible on mobile devices. Now it has received the ultimate compliment. Chip giant Intel has acquired it to help accelerate its RealSense project and other efforts to provide computer vision and deep learning solutions. Intel is expecting to see Movidius technology deployed in drones, robots, and VR headsets\u200a\u2014\u200ain addition to more traditional mobile devices such as smartphones and tablets.\n\nPower requirements are the traditional Achilles heel of mobile solutions that require substantial computation, with vision and machine learning being two of the most extreme cases. By creating optimized, custom silicon\u200a\u2014\u200aits Myriad chip family\u200a\u2014\u200aMovidius has reduced the power needed to run machine learning and vision libraries by well over an order of magnitude compared to a more-general-purpose GPU.\n\nAfter a lot of initial excitement, Intel\u2019s first-generation RealSense products\u200a\u2014\u200adesigned to provide devices with a 3D view of their surroundings to support mapping, navigation, and gesture recognition\u200a\u2014\u200afaltered due to technical shortcomings. However, Intel has more than re-doubled its efforts, and is aiming to make RealSense the eyes and ears of the Internet of Things, which Intel believes will comprise over 50 billion devices by 2020. Intel Senior VP Josh Walden likens vision processors such as Movidius\u2019s Myriad to the \u201cvisual cortex\u201d of IoT devices.\n\nThis move takes Intel further into Nvidia\u2019s home turf. Nvidia has bet big on high-performance computing for AI, self-driving cars, vision, and VR\u200a\u2014\u200athe exact markets Intel is trying to move into with its RealSense platform, and now the Movidius acquisition. This pits Nvidia\u2019s strategy of providing the most possible general computing power per watt versus Intel\u2019s custom silicon.\n\nOn paper, the advantages of each are fairly straightforward. General purpose GPU (GPGPU) computing provides the most flexibility and adaptability, while custom silicon can be more efficient when running a specific task or library\u200a\u2014\u200aonce it has been developed. In the market, expect to see plenty of design wins for both Intel and Nvidia, and some leapfrogging of each other as subsequent product generations roll out from each.", 
        "title": "Intel snaps up Movidius to accelerate its AI and RealSense vision efforts"
    }
]