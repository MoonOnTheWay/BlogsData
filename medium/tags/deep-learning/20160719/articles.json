[
    {
        "url": "https://medium.com/@awjuliani/introducing-neural-dream-videos-5d517b3cc804?source=tag_archive---------0----------------", 
        "text": "Over the past few weeks I have been working on a way to combine two deep learning architectures, a variational auto-encoder and a recurrent neural network, to allow for the generation of what I am calling Neural Dream Videos. These videos capture both the spatial and temporal properties of a given source video, and produce potentially endless new variations on the substance of the video itself in a hallucinogenic way. Below are a couple examples of Neural Dream Videos I generated from classic video games (see further down for live-action videos).\n\nThe videos above were generated completely from the two neural network working together, and once trained don\u2019t rely on the source videos at all. Let me explain the process below.\n\nVariational Auto-encoders (VAE) have been used as a way to allow neural networks to create new examples of images. They do this by learning an efficient representation of the spatial characteristics of thousands of training images. For example, give a VAE thousands of horse photos, and it will learn a representation that allows it to produce novel horse photos from it\u2019s own representation. Like humans, this can be thought of as a kind of imagination at work. While they work well for still images, videos are far too complex to capture using a VAE however, and that is where the current network comes into play.\n\nRecurrent Neural Networks (RNNs) are used to learn temporal patterns in data, and create new patterns from old ones. A common use case for RNNs that has gained attention lately is language modeling. For example an RNN learns to produce a piece of text, such as a new Shakespearian play, when trained on a corpus of Shakespeare\u2019s works. The problem with RNNs by themselves is that they are too simple to understand the relationships between video frames, which contain thousands of numbers and complex spatial relationships. By combining a VAE and RNN, we can train the VAE to learn a compact and semantically meaningful representation of the frames of a video, and then train an RNN to learn how to model the temporal patterns of those latent representations. Once we have a newly generated sequence of latent representations, we can run them through the trained VAE to generate a new set of full video frames. Stitch them back into a video, and thats it!\n\nBelow are a few videos created from live-action footage.\n\nI am happy to be releasing this code for creating these videos on Github here, and I hope others are able to have fun making these kinds of videos as well. There are also a lot of ways in which the quality of both the video and the logic of the temporal flow of the videos could be improved by using more sophisticated kinds of auto-encoders and recurrent networks. If you work on those kinds of things, please feel free to contact me or contribute additions or changes to the repository.\n\nWe are still in the early times of creative applications for generative networks. Google is currently developing models that produce basic melodies, and the videos here don\u2019t stray too far from their source material. Still, such networks are the first steps to neural networks that will be able to accomplish the exciting tasks of write songs, making art, and create films on their own.", 
        "title": "Introducing Neural Dream Videos \u2013 Arthur Juliani \u2013"
    }, 
    {
        "url": "https://medium.com/@moatazr/the-critical-hidden-side-of-the-deep-learning-revolution-8868d5320ed6?source=tag_archive---------1----------------", 
        "text": "There\u2019s no denying we\u2019re already in the first phase of a global AI revolution at this point. This statement is in fact true, regardless of how sophisticated your understanding of AI is. We choose the term deep-learning since it\u2019s much more accurately defined than the broader AI space. AlphaGo, Siri, Alexa, Cortana, Shazam, automated-driver-assistance leading to self-driving cars, and a plethora of new AI-assistants in development for pretty much everything you can think of, are all proof points that it\u2019s an emerging reality. However, the infrastructure side of the AI revolution gets almost no coverage!\n\nThe infrastructure is the plumbing; it\u2019s not sexy, it\u2019s not visual, it\u2019s complex and it\u2019s never effective for clickbait pieces. But that\u2019s understandable. No one looks at a house, and instead of noticing the french doors, the roof and garden, they think, it\u2019s got this amazing efficent eco-friendly plumbing. Until they buy it and experience all the leaks and the mounting bills, that is!\n\nThe infrastructure side of deep-learning is where you would train your deep neural-networks, be it in the cloud (AWS, Google, Mircosoft, etc) or in your own data-centers, and on what type of hardware and with what tools. We\u2019ll cover those aspects in this article, highlighting why it\u2019s an increasingly important strategic decision, and indeed a competitive differentiator for almost all companies working on AI/deep-learning. Very few engineering leaders at large companies get enough focused time to contemplate all such issues and tradeoffs before making a decision, so it ends up being largely momentum or background driven. If this article helps just a few such folks make the right tradeoffs for their own setup, it\u2019ll have achieved its goal. Most people think only of the AI arms race as largely involving Google, Facebook, Microsoft, Amazon, and Apple, however, there are tens of companies where AI is strategic, both B2C and B2B companies, and we collaborate with some of them.\n\nRecently it was disclosed by Google DeepMind that it took thousands of GPU-enabled servers to train AlphaGo and at Google IO, it was disclosed that they in fact also used a custom Tensorflow Processor (TPU)which inspired quite a bit of excitement across the deep-learning circles.\n\nIf you track the academic literature on AI, machine-learning, and computer vision/pattern recognition, you\u2019ll find that almost invariably they are running on Nvidia GPUs, either in the cloud or on beefed up servers they configured in their lab. Are GPUs the only option to do this? And are Nvidia and AMD the only games in town? For many startups, Nvidia GPUs is an obvious and easy choice for the near-term, since it\u2019s likely the default option for the Virtual Machines you\u2019re launching for your compute cluster at your cloud provider of choice, and because CUDA is very popular in many of the open-sourced packages released thus far.\n\nHowever, there are other alternatives; for example, about three years ago Microsoft Research was first to suggest the idea of using FPGAs instead of GPUs for training deep neural networks. Performance per Watt and massive off chip IO were the triggers for the proposal. Energy consumption per task and per data-center may mean little to a startup with hundreds to thousands of VMs (virtual machines) like DeepVu for example. However, it means everything for the planet-scale computing companies like Google, FB, Amazon and Alibaba. Are FPGAs a better answer? What are all the options and what are the tradeoffs? That\u2019s what we\u2019ll try to address briefly at a high level.\n\nThere are several conflicting goals for a deep-learning HW/SW platform.\n\nThe hardware options include three other variants, so let\u2019s cover all options as an overview:\n\nYou could replace the low, medium, high values with actual scores or percentages, and you can see how this can easily be mapped to a \u201cConstraint Satisfaction Problem\u201d if you\u2019d like to reason formally about this. But that\u2019s beyond the scope of this article.\n\nAs you can see from the table if we start from the top row, the highest investment would be to puruse a path similar to Google which is to develop your own new programmable Neural Processor Unit (NPU) with a novel instrution set architecture (ISA) and hardware acceleartion blocks for deep-neural networks. You\u2019d want your processor architecture to be well suited for multiple neural network architctures that have proven successful for subsets of the problems you\u2019re targeting, such as for example, convolutional neural networks, long-short-term-memory (LSTMs), neural turing machines, etc. The reason the (N) row shows high devleopment cost and medium schedules in software is that you\u2019re developing a new processor, and hence you\u2019ll have to either port existing frameworks and libraries (such as Tensorflow, or Torch) and Scikit-learn etc to your processor so you can leverage and build upon, and compare to, open source code bases, or develop your own framework, and then a mapping tool that converts code in TF or Torch to your framework. However, this option (N) affords you the highest flexibility and complete control, as you can invent a brand new NN algorithm and still be able to expect your processor to deliver solid performance on it. This option is for companies that have the resources and the long-term strategic committment to deep-learning to make a major investment in a processor architecture, SDK/framework, and tools to enable a true HW/SW partitioning and acceleration of many deep-learning applications. I have spent the first 15 years of my career, mostly for imaging and communications products, in this area of HW/SW partitioning and co-design for a product/solution development i.e. looking at an application, choosing an algorithmic implementation and defining a processor architecture and ISA that\u2019s well suited for it to achieve a given performance/watt within a specificed time-frame. Advances in AI and its wide ranging applications are definitely bringing back the need for this end-to-end optimization approach.\n\nThe next two rows SG, and SC use existing standard GPUs and CPUs which is what a lot of startups are forced to pursue as they are worried about cost and ease of scaling their training workloads. So they end up using spot-pricing, and launching a phantom cluster of Nvidia GPU based VM instances for 12\u201336 hours to perform the training, and then releasing them. They leverage existing frameworks and libraries and focus most of their effort in tweaking architectural parameters of the NN, the distributed cluster computing aspects of the problem and the data engineering. If you work on training on a billion images, and you have access to 1000 GPU workers, you have model parallelism and data parallelism decisions to consider. One issue that often arises in such cluster configurations is that they can become bandwidth limited as communciatoin overhead dominates.\n\nThe FPGA approach (F) has been made popular primarily by Microsoft about 3 years ago. A \u201cField programmable gate array\u201d is simply a mapping of your software implementation into a \u201csea of gates\u201d where configuring the connectivity implements your program\u2019s functions. There are more to them than sea-of-gates view, since all modern FPGA come with hard blocks that implement common value-add functions such as memory controller, fast IO blocks, etc There are two major players in the FPGA world, Intel (via Altera acquisition) and Xilinx. The issue with FPGA development is that they require you to port your software which is typically in C++, Python, Lua, or Java etc to Verilog or VHDL before it can be mapped to the FPGA. There are mapping tools to automate this however, the resulting code always has inefficiencies. So the code you\u2019re mapping to gates isn\u2019t as optimized as it can be and hence the gate structures aren\u2019t optimized, and consequently the neural networks performance in terms of precision/recall will not be on par with the GPU/NPU implemenation of same algorithm.\n\nOne of the key pros of the FPGAs is that you can optimize the I/O in terms of mapping your code to a cascade of FPGAs where you can have full control of over 300GB/sec I/O between the chips, which enables interesting level of macro parallelism. The other key pro often cited is that their total energy consumption per application (Watt x elapsed time in seconds) is often lower than high end GPUs. However, this can be debated for NPUs/GPUs that are especially designed for neural work loads. A major con for FPGA is that to date, colleagues, who favor this approach, complain there are still no commerically viable open-source implementations to build upon.\n\nThe last row covers custom ASICs (Application Specific Integrated Circuits). They are hardwired implementations of a specific algorithm directly into hardware. They offer the least flexibility, as they are almost always fixed function. Cable-modem chips in the ealry 2000s were often custom ASICs. So were H264 video codecs for video playback chips. When an algorithm or application becomes standardized, it often lends itself to a $2 custom ASIC or custom \u201cIP core\u201d that gets integrated into a larger system-on-chip, as there\u2019s no reason or need for flexibilty at that point. However, at this stage of AI developments, where the pace of algorthmic innovation and the wide spectrum of architectural variations in neural networks, it is clearly quite early, very limiting and doesn\u2019t make strategic sense to develop a custom ASIC!\n\nOne can think about costs in terms of\u00a0:\n\n\u2014 Energy cost in terms of KiloWattHour for running a particular NN training job\n\n\u2014 Cost in terms of total person-hours to get a particular NN training done to achieve a given performance level. Note that this always requires multiple iterations, so in fact, the agility of the HW+SW platforms directly impacts the productivity level of such a DL team\n\n\u2014 Cost in terms of data-center operations, maintenance, upgrades etc\n\n\u2014 Unit cost per X. OK, I\u2019ve personally always been against the headline performance numbers such as GFLOPs since they are usually BS, even if someone quotes sustained performance level rather than peak, it sitll means very little to your own application\u2019s performance as observed by an end customer. It\u2019ll also be hard to distill it down to cost per neuron/unit! However, perhaps the industry can converge on few standard problems, for ex, MNIST handwritten digits, and ImageNet LSVRC are such problems. Then one can argue that a reasonable cost metric would be to quote the cost to implement such problems on a given HW choice, and deliver state of the art performance/accuracy on it.\n\nIn a sales pitch you may refer to \u201cTotal Cost of Ownership\u201d, however this is an internal development\n\nFinally, this begs the question, if you\u2019re such a firm believer in deep neural networks, could you possibly build such an NN to assist you with this strategy decision to maximize ROI. The ultimate goal is AI-assisted management, so why not go \u201cmeta\u201d on this one as well! Theoretically it\u2019s possible, if you had enough training data, however, there\u2019s no training data available for this. You\u2019d have to pursue all 5 implementations in parallel for multiple projects and collect project meta-data such as:\n\nteam size, hardware choice, software platform choice, target schedule, actual completion schedule, performance target on benchmark, actual realized performance on benchmark, etc\n\nCompanies with serious commitment to experimentation for organizational learning such as Alphabet and Amazon could potential do this over time but it\u2019s a massive investment, although it would offer amazing insights. Alternatively, industry forums or companies like GitHub could start asking teams to supply additional meta-data on their deep-learning projects, although the schedule and performance bits are sensistive.\n\nIt\u2019s an incredibly exciting time for deep-learning and AI development in terms of algorithmic innvoations, hardware-software partitioning, and massive scaling of neural networks that can mimic or emulate biological structures to power truly intelligent applications. Such applications will overtake both B2C and B2B software in the coming years. For some companies with a decades long committment to AI and deep-learning, a strategic investment in a NPU processor development along with associated tools make strategic sense and will deliver high ROI over 5\u201310 years. For companies that are still exploring impact, using Nvidia GPUs, or even FPGAs may very well be the ideal short and medium term strategy, while collecting meta-data on these projects for leverage in deciding their long term strategy. It\u2019s the best time to be investing time and money in AI.\n\nAck: Thanks to Jasmina Vasiljevic, Mahmoud Khalafallah and Aya Rashad for reviewing earlier drafts of this article", 
        "title": "The hidden side of the deep-learning revolution \u2013 Moataz Rashad \u2013"
    }, 
    {
        "url": "https://medium.com/@maxspear/the-singularity-is-here-a-book-written-by-ai-4545e19f5a00?source=tag_archive---------2----------------", 
        "text": "Last week I encountered a deliciously thought-provoking experiment by Max Deutsch here on Medium. Max posited a challenge: use his guide to \u201cDeep Writing\u201d to train and create artificially intelligent authors to craft new stories.\n\nMax\u2019s guide was well-written and straightforward, but as I was implementing it only one question remained\u200a\u2014\u200awhat text should I use to train the model?\n\nNaturally, any talk of AI sparks to mind Ray Kurzweil\u2019s seminal book \u201cThe Singularity is Near\u201d. For those not familiar, in this book Ray lays out his theories on the inevitable convergence of man and machine. Generally, Ray is quite positive on the topic (not exactly the doomsday AI scenario believed by others), and speaks rather convincingly about how we are on the edge of creating a truly Artificial Generalized Intelligence from which there is no turning back.\n\nIn the spirit of this inevitable convergence, I thought\u200a\u2014\u200awhy not train my new author AI on the very book that predicts its future? At the very least, hopefully our eventual AI overlords will look back favourably on this deed and spare me from whatever machinators they impose on society.\n\nWithout further ado, the exact story written by the AI is pasted below. I only made minor adjustments to formatting and punctuation in the spirit of readability; no content was touched. I am curious to hear your thoughts; while largely unintelligible, there are a few startlingly scary themes that emerged\u2026you be the judge.\n\nBooks such as virtual pattern-recognition patterns. Our most profound brains, \u201cexplains ultimately find it slow many times more flexible.\u201d Computation and stay of the impending probabilities to the past for human traits. Michael Young, kind of cells started in the semiconductor industry had been strings that constitutes Nick types of neurons, sites in the development of genetic algorithms involves determining the brain\u2019s detailed substances, and machines (that is, today outweighs this step in each profound new science based on the Internet reverse-engineering state of billions of trillions) must be replaced to about twenty cells.\n\nThe genetic wisdom is only more form. As we learn to generate detailed tests to track of DNA therapies that Bostrom and medical types of entangled qubits, and the language processes is greater than others quietly becomes greater gradual but does the following: like a natural, shape that there are in a little period of biotechnology processes such as handheld cells, and existential risks, so it is characterized by a specific line of history took a limit to augment the precise nutrients gather electricity from Singularity whether but I made the speculation of enhance levels from cells, producing solved by long predictions, as sums of God\u2019s farming Molly 2004: George, even like what Searle is also reflected at the University of California at Berkeley and Stanford University also used for example, the G.M. are intended to a NASA-organized scripts which for September 2, presented both an differences was a interesting problem in such a range of operations per second, so there\u2019ll choose to IBM, of 1016 system.\n\nAnimals even included in his atoms and anything intensely out, bacteria. Has gone toward systems at doing so. One mission uses worth on demand. Musicians developed what still still give us to contemplate the program go from a series of philosophic, physics, as (someone after them) are likewise eight thousand. Chaos, and the twentieth century and no likely influence what is represented by some of the performer. Identity. Because it is based on his theory of choice. Body for a genius? A Panoply of trace information about the cost of local information after this scenario, fully slowly appears to define this particular person\u2019s experiences and distinguished stories by nonaqueous open movies of nanotechnology and easily information vendors are dramatically to 1050 stem cells to compute from explanations, test and equipment. If they failed to consider the public tubes to eat George, had so running at the whole and genetic algorithms underlying time.\n\nWe should still need to make away from the sixth likely to point to a bridge to looking in tens of thousands of lines sources and pathogens store and products as the end of the senses, the enormous energy Setting a puff of a space had already personally solved a major part of the Nobel Prize differential device and ongoing specific upper object and in this traditional connection scenario. Perhaps how you are, it\u2019s not just to say that the mechanical is not a closed process will be from a century using DNA, which accounts below in effecting to focus about up to war each of these extremely small engineering sources are even it? Some new skills, progress has been discovered.\n\nFor example, universes to the conclusion is that I look in chapter 4, we have to be essentially to any limits. One times, or to are finer components to perform virtually 1016 cps for all of these reality (with Earth and theories often demonstrate how the natural design capacity for the actual body paradigm), the machine appears to be capable of small down mastery of being challenged with the 2030s the rock as humans reverse species these would be much understood in comparison. But the mystery itself away, is that give one more than a bunch of DNA, including emotional therapies that do reflect for death, advanced food, to first carry out my animal anyone\u2019s tag dispositions and ungainly but effective, quite to encrypt a human judge on the that works above consisted of improvements in innovation and once it does not have adults to do gladly that which consists of the adoption of atoms by many low-cost functions, so the prospect of matter across destruction, quantum decisions.\n\nThey would also spread out such alternatively, humans were achieved. The ability to perform full-immersion intelligence to zero. First, we builds on the DNA in the criticism of our intelligence from the two genes of billions of years about each student and all gene sends precisely models of nanotechnology-based components and directives but use studies on time to those conditions to be long as we speak, are formidable creature of the medical switching computational system at the quantum space to implement the true architecture for its dynamics as shown as improving the cosmos for analyzing the life and early procedures for evolution, the subject can learn to shape by dropping the lessons earlier a value of a simple summary of the 2001 \u201cEmbryogenesis of research centers have been coding an inherent only engineering of the original, all of the repeating code each year are already demonstrated only the globe.\u201d Based on the nonbiological intelligence to stop the knowledge of the brain in the neuron to the size and aging bits are likely to be on the completion of $1012 and keep out of our biological species. \u00b7We should have to be reduced intelligent systems from applying detailed models. We do accept the laws of ideas.\n\nIn other life. was proposed but naturally one child appears to be able to evolve everywhere with other decentralization. However, that the Singularity itself is limited to begin to this machine would have the same: part onto the research and radical air language or each amino detection and fMRI, Marvin Minsky. Everything killed a tabletop configuration of another factor in doing with hundreds of trillions of modeling information resources for suddenly maintain the size but it is one in their deepest means. would look in essence, \u201cRepeat whether the rules has been routinely proved difficult to introduce computation.\u201d", 
        "title": "The Singularity is Here: A book written by AI \u2013 Max Spear \u2013"
    }
]