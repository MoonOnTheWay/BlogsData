[
    {
        "url": "https://engineering.huew.co/introduction-to-convolution-neural-networks-18981d1cd09a?source=tag_archive---------0----------------", 
        "text": "In our previous post, we talked about the trends and recent advances in Deep Learning. Deep Learning is about learning multiple levels of representation and abstraction that helps to make sense out of high-dimensional data such as images, sound, and text (or rich media analytics). In particular, Convolution Neural Networks (CNNs) have been at the forefront of this revolution to solve classification and clustering problems in speech and image processing. In this post, we will cover background and some details on the workings of CNNs.\n\nAt high level, CNNs can be considered as biologically-inspired variants of MultiLayer Perceptrons (MLP). (A perceptron with a single layer of weights can learn to solve a simple classification or clustering problem.) As discovered in 1960s, animal and human visual cortex contains a complex arrangement of neurons which work with each other to execute a complex visual recognition task. These neurons are sensitive to small sub-regions of the visual field, called the receptive field. These sub-regions are tiled to cover the entire visual field. The sub-regions in a way act as local filters over the receptive field and are well-suited to exploit the strong spatially local correlation present in natural images. The inner neurons take inputs from these sub-regions (sub-regions can detect patterns such as edges, shapes etc.,) and learn to identify higher level representation (e.g., human ear, or dog leg) using patterns learned by previous layer of sub-regions.\n\nCNNs are modeled after the above mentioned structure of sub-regions and arrangement of neurons to process outputs of sub-regions. In 1980s, Prof. Yann Lecun, one of the pioneers in this field, showed how CNNs can be applied to various visual object recognition problems such as face detection and pose estimation in photos, generic object identification in images with invariance to pose, lighting, and clutter, and vision-based navigation for mobile robots etc.\n\nHow is CNN different than MultiLayer Perceptron (MLP)?\n\nAs compared MLP (which represents fully-connected layers of neural networks that blows up the number of parameters to learn), CNNs exploit two key properties inspired from the working of visual cortex: (1) Sparse Connectivity, and (2) Shared Weights.\n\nSparse Connectivity: CNNs exploit spatially-local correlation by enforcing a local connectivity pattern between neurons of adjacent layers. In other words, the inputs of hidden units in layer m are from a subset of units in layer m-1, units that have spatially contiguous receptive fields. These subset of units are the kernels or filters whose weights are learned over time. The filters are then \u2018convoluted\u2019 over input image in a sliding window fashion. See figure 1 below for illustration of convolution operation.\n\nShared Weights: Sliding the same filter over the image allows for the corresponding features to be detected (you can have one filter to detect edges another filter to detect circles) regardless of their position in the visual field. This results in only K number of filters being applied and learned over time. This is much more efficient than connecting P number of neurons in one layer to Q number of neurons in subsequent layer. K is typically order of 400 and size of each filter is order of 10 cross 10 (corresponding to 2-D structure of images) whereas P and Q is typically in tens of hundreds.\n\nWhat kind of layers a CNN has?\n\nA CNN is a deep neural network with number of layers. Overall, a CNN has the following key types of layers: (1) Spatial Convolution (2) Activation Function (3) Max Pooling (4) Fully Connected and (5) Classifier. Layers 1,2, and 3 are typically applied one after another and in a repeated fashion (let us call this sequenced combination as a meta-layer) to extract multiple levels of features. We can have L (typically L = 5 to 10) number of meta-layers followed by a few (typically 1 to 3) fully-connect layers. The fully connected layers act upon the features fed by meta-layers and in turn feed deeper representation to the classifier which learns to perform different visual/speech classification tasks. Let us dig deeper into the working of each layer.\n\nLet us start with Spatial Convolution. A Spatial Convolution layer uses filters to extracts patterns and can have number of filters to perform convolution over the input image. The convoluted value of each filter over a sub-region is given to an activation function. An activation function can be a Sigmoid or Rectified Linear Unit which maps the value to a fixed value between 0 and 1 or max(0, x) respectively. The activation function add non-lineary to the model to learn latent patterns. The values obtained after activation function for each filter correspond to the filter\u2019s feature map. Thus for each filter, CNN generates a feature map. These features maps in a way act as different abstract representation of the input image and are processed by subsequent layers.\n\nPooling operations are then typically applied on individual feature maps to make them robust to variance to image transformations. For instance, the max pooling operation chooses the maximum value in a small sub-region for feature maps and such operation is applied to all sub-regions in a sliding window fashion. See figure 2 for illustration of convolution and pooling.\n\nThe meta-layers are then followed by fully-connected layers and then eventually by a classifier layer. See figure 3 for the entire CNN pipeline. Note that only the Spatial Convolution and Fully-Connected layers carry weight matrices which are learned over time. As mentioned above, the pooling (and sometimes normalization) layers of the pipeline add robustness for CNNs to be invariant to image transforms due to lighting, color or scale etc.\n\nHow do we learn weights?\n\nThe most predominant learning algorithm in machine learning is gradient descent algorithm which adjusts weights of the model in iterations. Gradient descent can still be used to learn such shared parameters and parameters across layers, with only a small change to the original algorithm. The gradient of a shared weight is simply the sum of the gradients of the parameters being shared. This idea along with the famous chain rule has been applied in Back-Propagation algorithm. This algorithm forms the basis of CNNs to learn millions of parameters and converge to high accuracy classification models. A very nice illustration of Back-Propagation can be found here.\n\nThe new dawn of CNN\n\nAlthough CNNs have been a powerful tool to model visual cortex and solve a complex classification problem, they were limited by nonavailability of large data sets for training (to learn millions of parameters) and nonavailability of inexpensive and fast computation infrastructure (to converge the model in a few hours or days). By implementing some of the fundamental operations such as Spatial Convolution and Back-Propagation to run on GPU infrastructure and using millions of images from Imagenet data repository, Alex Krizhevsky in his landmark paper in 2012 showed how CNNs can be applied to large-scale object classification problems. He showed how CNNs can process more than 1 million images to learn 60 million parameters under 7 days to classify a given image into 1000 different categories. Since then, there is a virtual war among researchers to improve the accuracy of CNNs on such image recognition tasks and it has now surpassed human intelligence with close to 97% accuracy. In addition to accuracy, an important commercial aspect of this technology is automation; machine can sift through 1 millions images without any human intervention.\n\nAt Huew, we have been training large-scale CNNs to solve various problems for our target customers (Online Classifieds, Online Market Places, E-commerce Websites). We have been applying Transfer learning and have developed proprietary algorithms to train custom-built CNNs in a speedy manner. Furthermore, we have been using CNNs for similarity based image retrieval to cater to product search optimization requirements. In our upcoming posts, we will cover details of our innovations in CNNs.", 
        "title": "Introduction to Convolution Neural Networks \u2013"
    }
]