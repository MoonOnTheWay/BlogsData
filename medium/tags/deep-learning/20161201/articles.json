[
    {
        "url": "https://towardsdatascience.com/machine-learning-how-black-is-this-black-box-f11e4031fdf?source=tag_archive---------0----------------", 
        "text": "Software is much better now than it was a few decades ago. Machine learning is better than it was. The machine learning software libraries are now mature and well tested. The technology that surrounds it is also much better. As an engineer I became used to working with more high level and abstract libraries, frameworks that obscured the underlying operations and quite frequently I used total black boxes. By black boxes I mean tools where I have little concern about what is going on inside.\n\nArtificial neural networks (ANNs) have a big advantage by not requiring physical pre-information before modelling a system. When a new computation is run on network it ends up with a different new weight matrix. This is a result of starting with small random values that are then adjusted as the program runs. This is why ANN\u2019s are black boxes.\n\nAside from the random starting point a reason that it is difficult to describe, mathematically at least, what a network is doing; is that the network is in a state of change, and maths are in effect stateless. That is we can use maths to describe; a start, an end, and a point in the process, but the learning process is a sequence of results from many operations and is not stateless.\n\nA few steps in a simple network with very few layers can be expressed by the function that fires and the adjustment in weights that result. Add more layers and a different type and scale of representation is required. Representing a wave of energy in a small amount of liquid is ok but it is no way to describe an ocean.\n\nAside from the density of ANNs these are systems which allow random variables and the incorporation of arbitrary black box deterministic functions. They enable the smart initialisation of inference algorithms and nested optimisation.\n\nArtificial neural networks generate rules by learning from examples. To learn they use a process of adjusting weights and measures to match a known result. They create their own example based rules on the fly. The way they operate does not detail how they evaluate and adjust the weights using something such as backpropagation.\n\nNeural networks are systems that arrive at complex answers. To do so they generate their own questions and rules. The lack of transparency on how the rules come about is seen by some as an issue. Some argue that the lack of transparency, the Black Box problem, could impede the usefulness of artificial neural networks. It is felt that if we rely on ANNs for answers they need to be able to explain themselves and tell us how they arrived at their answers.\n\nI am still happy to use a calculator on my phone to find the square root of a number or use a built in math function of whatever programing language I am using at the time. All of these use different approximations to solve the square root problem. The truth of the matter however is that, for most purposes, I am happy if the method I use to find a square root comes up with a close estimate. Should we be comfortable putting our faith in black boxes?\n\nOne way of seeing what a neural network is doing would be to probe the network using test inputs and measuring the impact of input variables on the outputs where the anticipated results are known. A bit like the interrogation technique of asking a subject a question to which you know the answer so as to test the veracity of the subject. With ANNs that use backpropagation this could be done by tracking error terms during the backpropagation step and then measuring the amount that each input impacts the output.\n\nAnother way to learn to trust network derived knowledge is to extract the rules that the network uses. The weights arise from transformations along a non linear path between inputs and outputs. All the values that comprise the knowledge are just a series of numerical values. Another process for extracting rules would be to translate the numerical values into symbolic forms by extracting rules through turning the learning process on itself.\n\nViewing the rules need not require examining the weight matrices directly but instead rendering the rules symbolically. Extracting rules by comparing the input / output mappings and forming decision trees. Decision trees based on classifier searches seeking a series of rules that intelligently organize a given dataset.\n\nA neural network takes inputs and outputs knowledge. The network uses a series of inputs and then creates outputs, together they provide a new dataset. There is a possibility of forming both a symbolic and a visual representation of the process. By passing the inputs and the new outputs to a decision tree classifier we could disclose how the network derived its knowledge. As well as disclosing the rules, that the network used, it gives us a way of visualising those rules.\n\nWhy look for the way the network builds it\u2019s rules at all? Is it a redundant effort if the network has been trained properly in the first place? Yet the ability to extract symbolic knowledge has some potential advantages. The knowledge obtained from the network can lead to new insights into patterns and dependencies within the data. From symbolic knowledge, it is easier to see which features of the data are important to the results.\n\nIt is not too difficult to program a machine learning model to represent what it is doing as another model. The trouble with doing this is it could tend to exist at a mundane or trivial level. It could be time consuming and not really suitable for understanding large complex structures. There is also the problem that the derived model would not provide a clearer understanding. If it reduced the rules the target network created, it may be too terse or give an incomplete description but the suspected danger is it would amplify the rules and further obfuscate the understanding sought.\n\nA different approach would be to work with the black box function and look at the importance of the inputs to that function. Apply it to quantify the effect of algorithm hyperparameter optimisation.\n\nThere are a software libraries to resolve blackbox optimisation problems. These use sequential model based optimization (Bayesian optimization), which is an iterative method for solving such problems. AutoWeka is one example designed to provide automatic model selection and hyperparameter optimization in WEKA Weka being an easy to use collection of generally available machine learning algorithms for data mining tasks using machine learning\u00a0.\n\nThe other way of dealing with black boxes is to make the network even more dense by adding more layers. This is entering the place where deep networks reside. The land of the varied architectures of deep networks including: Convolutional Neural Networks (CNN\u2019s), Recurrent Neural Networks (RNN), Recursive Neural Network, and the newer models of Decoupled Neural Interfaces (DNI) and Differentiable Neural Computers (DNCs).\n\nDeep networks are very different from their traditional ANN relatives. One way to visualise how different deep networks are is that they, by their very nature, overfit to the data they learn from. This is the opposite of more traditional ANN\u2019s which tend to aim for a more close fitting curve.\n\nObtaining a good fit works when there are not too many parameters. Deep learning can take multiple inputs and compute them through multiple layers. Even with random weights the hidden layers of a deep network are very powerful and able to represent highly nonlinear functions.\n\nMost deep learning algorithms employ some form of optimisation to minimise or maximise a function f(x) by adjusting the value for x. The function to be adjusted is known as the objective, loss, error or cost function. Deep learning employs these functions to measure and adjust how close results (predictions, classifications, etc) are to the inputs. The aim is to minimise the loss function.\n\nThe derivative of the function\n\ngives the slope of f(x) at the point x which speci\ufb01es the scale of a small change in the input to match a corresponding change in the output.\n\nSimple solutions to reduce loss functions work well when a large number of repetitions and layers are available. One recurring problem is that large training sets are required to provide reliable generalisations and thus require a lot of compute resource.\n\nRather than looking at all of the weights in a network and recalculating them gradient descent is a method of pathfinding that shrinks the relevant weight space, and therefore the number of updates and required computation.\n\nThe Stochastic Gradient Descent (SGD) method is used for many machine learning models and is the prime algorithm used on ANN\u2019s, it also provides a simple way to produce accurate results in deep networks. The stochastic gradient descent is able to give an approximate estimate of loss using a small set of samples lessening the amount of compute required.\n\nWhere X is the set of input values of Xi\u00a0,W is set of the importance factors(weights) of every value Xi. A positive weight means that that risk factor increases the probability of the outcome, while a negative weight means that that risk factor decreases the probability of that outcome. The target output value, \u03b7 is the learning rate (the role of the learning rate is to control the level to which the weights are modified at every iteration. f(z) is the output generated by the function that maps a large input domain to a small set of output values in this case.\n\nThe function f(z) is the logistic function:\n\nlooking to estimate the parameter w which minimises Q(w).\n\nwhere \u03b7 is the learning rate.\n\nA stochastic gradient descent at any single point is approximated as\n\nSo the initial vector w at learning rate \u03b7 is iterated i number of times with random shuffles of the training data. By making the connections random faster training times are possible as there is no need to train the weights of the hidden layers.\n\nWhile the level of understanding of how convergence occurs, or how sound generalizations are derived is vague, deep learning is best applied to tasks where we know what answers to expect but it is tiresome to find them. Common examples of such tasks are; speech recognition, image classification, anomaly detection and character recognition are not dealing with unknowns.\n\nIf a network recognises a character as an \u2018a\u2019 or a drawing as a fish we can make an immediate assessment of its potential correctness. If the network recognises a random grey texture as a parrot we can quickly see it has been fooled and is foolish. We do not need to know how it was fooled because we use empiricism to test the networks accuracy. Knowing it happens allows us to look at ways to add such features as weight regularisation.\n\nThe reassurance of knowing how the hidden layers are working can be replaced by another knowledge. The performance of deep learning models and their scope replaces; the need for knowing the innards with; pragmatic evaluation. Being able to view the results and performance achieved from using deep reinforcement learning gives a somewhat pragmatic way of seeing what is occurring.\n\nThere is a another way to view a black box. When using neural networks to undertake language tasks such as translation and natural language processing vector representations of words or even phrases are used, these are know as word embeddings.\n\nVector space models (VSMs) represent (embed) words in a continuous vector space where semantically similar words are mapped to nearby points (\u2018are embedded nearby each other\u2019). These stem from the theory that words that appear in the same contexts share semantic meaning. This is the way that programs like Word2Vec use a two-layer neural net to reconstruct the linguistic contexts of words within a multidimensional vector space; giving a Vector Representations of Words.\n\nThe hidden layer of the network learns to transform the input data and represent it in a multidimensional way. It gives a value for the closeness of one word to another within space. It is very difficult to visualise this space but the technique of using dimensionality reduction to visualise high-dimensional representations would also provide a way of inspecting the models used by the network.\n\nWord2Vec mathematically detects the similarities of words against other words that are close to them in the input corpus. It gives numerical representations of word features such as the context. It trains words against other words from the inputs. There are two methods for doing this. One uses the context to predict a target word (a method known as continuous bag of words). The other uses a word to predict a target context (known as skip-gram).\n\nThe program is an example of a neural network that delivers relationships between words defined as vectors, these can be mapped in space. This means the deep learning process has a representational output, even if it is difficult to visualise. That difficulty is somewhat overcome by the embedding projector that Google have open sourced.\n\nChristopher Colah writing about \u201cUnthinkable Thoughts, Incomprehensible Data\u201d and how being \u201cas a machine learning researcher (his) job is basically to struggle with data that is incomprehensible\u200a\u2014\u200aliterally impossible for the human mind to comprehend\u200a\u2014\u200aand try to build tools to think about it and work with it\u201d. Visualisation then becomes an ideal tool to understand what deep learning networks are actually doing.\n\nAnother solution to overcoming the black box fear of the unknown is to develop a model that has knowledge of itself, a model that can recall, recount, reason and reflect, and self-analyse; a model with memory. One that is self aware and learns from what it has done historically. A model that has the ability to represent variables and data structures and to store data over long timescales. Several models of neural networks (Attention and Augmented Recurrent Neural Networks) provide a way of joining networks together allowing one network to explore another.\n\nOne aspect of the black box problem is that most networks to date disregard the knowledge they acquire during each cycle of learning. By allowing multiple copies of the same process to be held one form of recurrent neural network (RNN) architecture using long short-term memory (LSTM) allows cycles of learning to persist long enough to improve the next cycle. Decoupling the interfaces between networks would allow networks to communicate with each other.\n\nAdding read write memory to a network enables learning machines that can store knowledge Differentiable neural computers (DNCs) are just that.While more complex to build architecturally by providing the model with an independent read and writable memory DNCs would be able to reveal more about their dark parts. So the way to deal with black boxes is to make them a little blacker and give them ways to learn the dark arts.\n\nScience has a long tradition of learning from doing, that is using experiments to form theory. It is also an area where making propositions or proposing a thesis for testing is a way forward. Much of the material referenced in this article is at the cutting edge of machine learning. As Ryan Lowe, the author of How Machines Learn, commented on a recent review paper that he was a co author of \u201cWe are of the opinion that keeping the \u2018good\u2019 unpublished while waiting for the \u2018perfect\u2019 is not a productive way forward for science.\u201d.\n\nAccess to data is another way of opening up ways of peeking into what black boxes are doing, being able to replicate experiments using the same data. A significant move is the open sourcing not just of the design of algorithms but also the data sets they use to train and feed them. Google, Microsoft, Facebook, Apache, scientific communities, government bodies and many others have open sourced not just software but also training data, and labs.\n\nSimilarities between deep learning and physics have been used to describe why deep neural networks are so efficient. Sharing simplifying properties tracing back to the laws of physics and using certain hierarchal forms of data that are prevalent to both indicates that the structure of the subject matches well to the task the learning process is being asked to perform.\n\nConsider the two ends of sub molecular physics Gauge Theory and the Large Hadron Collider. Both are used to seek to find the similar enlightenment. Scientists are discovering surprising results using the LHC and the string theorists predict surprising particles. Both converge, at times, to seek and at times reveal joined up insights, with one informing the other. On occasion astounding theories, such as the idea that gravity is an entropic force and not a fundamental interaction as Einstein suggested sit around for years and gain credence as the tests the theory suggest are completed.\n\nTheoretical understanding and practical applications are paths which are not always identical yet they may lead to the same places. The architectures, algorithms and models of machine learning follow a similar path. On one hand theoretical knowledge follows that gained from experimentation on the other theory shows where experiments need to go.", 
        "title": "MACHINE LEARNING: How Black is This Beautiful Black Box"
    }, 
    {
        "url": "https://medium.com/self-driving-cars/tensorflow-on-windows-17329aa1d7fa?source=tag_archive---------1----------------", 
        "text": "TensorFlow is the main deep learning library we are using in the Udacity Self-Driving Car Engineer Nanodegree Program, and it\u2019s been a little bit painful because of the lack of Windows support.\n\nWe\u2019ve had to work with our Windows users to set up Docker containers in which to run TensorFlow, and frankly we haven\u2019t done as good a job with that as we should.\n\nSo thank goodness Google announced yesterday that they\u2019re releasing Windows support for TensorFlow.\n\nIt looks to be early stages and I\u2019m not sure if we can safely point our students there yet, but hopefully it means we can get there soon.\n\nAnd it\u2019s also another step toward TensorFlow becoming the library of choice for deep learning.", 
        "title": "TensorFlow on Windows \u2013 Self-Driving Cars \u2013"
    }, 
    {
        "url": "https://medium.com/@julsimon/amazon-polly-hello-world-literally-812de2c620f4?source=tag_archive---------2----------------", 
        "text": "Today, AWS announced a new text-to-speech service, called Polly. Well\u2026 I had to try it\u00a0:D\n\nHere\u2019s a *very* basic example in Python. There is much more to Polly, but this should get you started. You can list all available voices with \u2018aws polly describe-voices\u2019.\n\nAnd yes, I\u2019m sure there are more clever ways to play sound files in Python, but they\u2019re beyond my weak skills, so there\u00a0;)\n\nVery fun service. I see a lot of chatty build servers on the horizon, yelling in German at careless developers. Oh yes, pure bliss\u00a0:D", 
        "title": "Amazon Polly: \u201cHello World\u201d\u2026 literally! \u2013 Julien Simon \u2013"
    }, 
    {
        "url": "https://chatbotslife.com/deep-learning-will-consume-these-fpgas-like-candy-c99188e54d4d?source=tag_archive---------3----------------", 
        "text": "A new service on AWS to offer FPGAs\u200a\u2014 Field Programmable Gate Array\u200a\u2014 basically highly reconfigurable hardware (at gate level) with a lot of parallel\u00a0computing capabilities.\n\nThis comment on Hacker News explains what performance you can get out of these machines, the title of this post is from a comment from same user.", 
        "title": "Deep Learning will consume these FPGAs like candy \u2013"
    }, 
    {
        "url": "https://medium.com/@mslavescu/great-resource-for-getting-into-machine-learning-aa01cf4fc72f?source=tag_archive---------4----------------", 
        "text": "Also the larger list Nam Vu posted in a comment above.\n\nI started a few weeks ago a practical approach (for everyone) to machine learning and robotics by diving into self driving car development.\n\nYou can find two of my articles in this area here:\n\nI hope to get tonight the next article in the series.\n\nYou can also follow my journey in this and more on Twitter @gtarobotics", 
        "title": "Great resource for getting into machine learning \u2013 Marius Slavescu \u2013"
    }, 
    {
        "url": "https://medium.com/jmtorres/deep-learning-en-el-ii-congreso-de-estudiantes-de-ingenier%C3%ADa-inform%C3%A1tica-6c4734342051?source=tag_archive---------5----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep Learning en el II Congreso de Estudiantes de Ingenier\u00eda Inform\u00e1tica"
    }, 
    {
        "url": "https://medium.com/@sixsamuraisoldier/thats-because-no-one-wants-to-spend-30-million-to-develop-an-asic-that-s-only-going-to-be-around-e480f4fbc261?source=tag_archive---------6----------------", 
        "text": "That\u2019s because no one wants to spend 30 million to develop an asic that\u2019s only going to be around 10x faster than a gpu, and all tf,caffe,keras,etc code has to be ported to new software. Also I thought you said for video games? Why would anyone want to buy a specialized card when it could just run on their gpu? Also, if it gets popular what do you do when nvidia puts a similar asic on die?\n\nAlso, nervana\u2019s chips are super expensive because they use hbm2", 
        "title": "That\u2019s because no one wants to spend 30 million to develop an asic that\u2019s only going to be around\u2026"
    }, 
    {
        "url": "https://chatbotslife.com/the-nightmare-machine-how-ai-is-taking-fear-to-the-next-level-f682a05d9208?source=tag_archive---------7----------------", 
        "text": "Disclaimer: More Robopocalypse talk coming. Rest assured that despite the constant Matrix-like scenarios, we\u2019re actually big fans of AI technology. But we\u2019re also sci-fi geeks, so we have to get it out somewhere.", 
        "title": "The Nightmare Machine: How AI is Taking Fear to the Next Level"
    }, 
    {
        "url": "https://medium.com/@dan.jeffries/because-you-are-assuming-that-gpus-are-the-ideal-way-to-run-the-calculations-79377b1f5b34?source=tag_archive---------8----------------", 
        "text": "Because you are assuming that GPUs are the ideal way to run the calculations. GPUs are used now simply because they are the most parallel chips but that does not mean they are even close to being optimize for the calculations they are running. That\u2019s why Google Built custom ASICs to run TensorFlow and why Nervana built custom chips for deep learning that Intel bought. There are tons of optimizations possible. And that is only one type of AI. The types of operations specific to games are different and could certainly be optimized in silicon.", 
        "title": "Because you are assuming that GPUs are the ideal way to run the calculations."
    }, 
    {
        "url": "https://medium.com/@mslavescu/i-cant-wait-for-more-automation-in-our-cars-3bd50d0c36d3?source=tag_archive---------9----------------", 
        "text": "Sorry for your accident, until we experience an accident we don\u2019t really know what it means.\n\nThe same thing is happening with self driving cars today, they are thought how to drive, but until they experience all the road situations we cannot really trust them.\n\nA better approach would be to, transfer the experience we acquired over years to the onboard computer.\n\nDeep learning can help, but a solution with a more deterministic behavior will be preferred.\n\nWe (everyone) need to think hard and come up with creative solutions\u00a0(not\u00a0only\u00a0technology\u00a0oriented), instead of just waiting for the car to learn.", 
        "title": "I can\u2019t wait for more automation in our cars \u2013 Marius Slavescu \u2013"
    }
]