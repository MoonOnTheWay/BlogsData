[
    {
        "url": "https://gab41.lab41.org/across-the-network-ai-week-in-review-sept-16-1d65b22cc532?source=tag_archive---------0----------------", 
        "text": "Across the Network\u200a\u2014\u200aAI Week in Review Sept\u00a016\n\nFor the longest time, I was a Google Reader guy. I had a list of sites that I wanted to follow, and Reader was the way that I could quickly triage the news that I cared about. I\u2019m almost embarrassed to admit it because RSS feeds are so last decade, but I like being in control of what I read and what I don\u2019t read. And I don\u2019t trust recommendations from Flipboard style applications or from my Facebook feed.\n\nWhen Google Reader was unceremoniously shut down, I shuffled back and forth between different RSS readers until I found Feedly. Feedly makes a solid product, but I\u2019ve never fully bought in. As a result, I no longer actively prune the sites on there (case in point: Feedly still pulls feeds from a number of watch related sites from the short-lived time when I thought that my life goal ought to be to own both a Lange & Sohne and Patek Philippe Perpetual Calendar Chronograph). The end result of all of this is that RSS is no longer the way that I get my news.\n\nThe way that I keep up-to-date on news these days (particularly on work-related topics) is by checking the General and Random channels on Lab41\u2019s Slack. We have a large enough team these days that there are at least 2 or 3 gems in there every day. It has completely replaced my HackerNews habit. It got me thinking that others might be interested in the curated list of papers, articles, tutorials, and reviews that our team has put together. So, without further adieu, I present\u2026\n\nWelcome to first edition of Across the Network: Lab41\u2019s weekly review of the most important happenings in the Artificial Intelligence universe.\n\nWaveNet\u200a\u2014\u200aStrictly speaking, this post is from the end of last week\u200a\u2014\u200abut who\u2019s counting anyways. The fine folks at Google DeepMind bring to us a generative model that produces raw audio waveforms. What does that mean? Well, the most practical application of this waveform generation is speech synthesis\u200a\u2014\u200awhen computers speak words to you. You know how you can tell that Siri is a computerized voice? With WaveNet\u2026you can still tell it is a computer. But it sounds more like a human. How much more? /shrug.\n\nProsthetic Knowledge\u200a\u2014\u200aStaying on the topic of generative models (in this case, a Generative Adversarial Network), the folks at MIT have built a neural network that can predict frames in a video from a single image. The layperson in me is usually not very impressed with these types of efforts (because the predicted frames are obviously not very real looking), but the geek in me can\u2019t help be amazed at how well this network performs.\n\nDiversity in Language Analysis\u200a\u2014\u200aThis is a really interesting article that discusses the datasets that are commonly used to train Natural Language Processing (NLP) models. The authors\u2019 contention is that NLP models are always trained with \u201cpristine\u201d language. But if you train your model with data from the New York Times or Wall Street Journal, there are entire classes of English speakers whose speech and writing will be unintelligible. Seems reasonable. The authors suggest that NLP researchers should broaden their data sources in order to create more comprehensive models of the English language.\n\nEvolution in Action\u200a\u2014\u200aThis link has nothing to do with AI. But I think that everyone should watch this video. The researchers behind MEGA-plate wanted to see if they could videotape evolution in action. So they set up a 4ft wide dish with agar jelly (bacteria like to eat this stuff) and segmented areas with different concentrations of antibiotics. Over the course of several days, they show in video how quickly the bacteria evolve to be resistant to the antibiotic. It is stunning.\n\nStacked Approximated Regression Machines\u200a\u2014\u200aThe big news this week was that a paper that purported to have discovered a \u201csimple deep learning approach,\u201d was withdrawn from arXiv after some loud complaining in academic circles (and by loud complaining I mean shade being thrown in the quiet hallways of academic institutions). What did SARM claim? They claimed to get VGG-like performance using stacked layer-wise operations without back-propagation. I\u2019m not actually sure about the details because the paper was pulled! If I find out, I\u2019ll let you know.\n\nReinforcement Learning meets Starcraft\u200a\u2014\u200aFinally, an AI that does something valuable! Playing Starcraft. True story\u200a\u2014\u200athe first thing that my wife ever said to me (this was the second day of freshman year of college) was \u201cDo you play Starcraft?\u201d I didn\u2019t. She didn\u2019t talk to me again for 5 years. The folks from Facebook AI Research have just the answer for me though. High-level Starcraft play is all about micromanagement of resources. They wanted to see if an AI could do a good job. The answer\u200a\u2014\u200atheir solution is world-class, but can\u2019t do everything a human can.\n\nData Science Masters\u200a\u2014\u200aInterested in becoming a Data Scientist? The kind team at Luminant have put together a set of training resources that an aspiring Data Scientist can look at. It\u2019s remarkably complete, and I found some things on the list that I hadn\u2019t seen before. Definitely worth a quick peak, even if you can already explain the difference between Momentum and AdaGrad.\n\nThe Neural Network Zoo\u200a\u2014\u200aI love this page. Every day, you read about a new neural network architecture. After all, every Computer Science graduate student in the world right now who wants to maximize their income is becoming a Deep Learning expert (haven\u2019t you heard\u200a\u2014\u200athere is a shortage of experts). This page intends to maintain a \u201ccomplete\u201d chart of all of the neural network architectures and simple descriptions about how they differ. I plan to use this site frequently.\n\nLab41 Reading Group\u200a\u2014\u200aMy colleague Abhi just wrote up an article explaining an academic paper that he read called SqueezeNet by Forrest Iandola et al. I suggest you take a look!\n\nThat\u2019s all I\u2019ve got. We hope to see you again next week on Across the Network.", 
        "title": "Across the Network \u2014 AI Week in Review Sept 16 \u2013"
    }, 
    {
        "url": "https://medium.com/@iamgaurav/using-keras-for-mnist-af6fb519a63c?source=tag_archive---------1----------------", 
        "text": "Keras is an awesome Deep Learning Library for TensorFlow and Theano. It provides a framework for high level implementation Deep Learning methods.This post describes the Hello World of Deep Learning, the classic MNIST Digit Classification.\n\nMNIST is a database for handwritten digits. Keras can be used to classify these digit images. Following description is based on the example code provided in the keras distribution.\n\nThe general approach to the problem follow few simple steps. Load the data. Make a model. Train the model. MNIST dataset is a standard and keras provides API to download it for convenience. This example uses Convolutional Neural Net (CNN) as the hidden layers to extract features from the digit images. CNN generates a smaller representation of a entire image. In keras training is a single line code.\n\nAssuming the necessary imports, the following loads the MNIST dataset. The Dataset is segregated into Training and Testing examples.\n\nConvolutionLayer expects number of channel to be specified, so reshaping is done ensure it is single channel.\n\nThe predefined output needs to be in one-hot encoding format.\n\nThe model comprises of two 2D Convolutional Layers followed by Maxpooling and a Dropout Layer.\n\nFirst 2D Convolutional Layer generates nb_filters=32 feature maps. Think of it as a new image which has 32 channels 26X26 and is smaller in size as compared to single channel original single channel 28x28 Image. So as result of 2D Conv Layer the Image gets transformed from 1x28x28 to 32x26x26. The following ReLU Activation layer clears off the negative values and brings in the nonlinearity in the flow.\n\nThe image dimension is further reduced after the second 2D Conv Layer.The MaxPooling Layer further reduces the image size. It returns the maximum of the pixel value for a region.It reduces the size to one fourth based on pool size.After these layers a 32 channel deep 12x12 image is obtained. It is also called feature maps or activation maps.The Dropout layer acts a regularizer to prevent over fitting of the model.\n\nPrevious was mostly the feature detection part and now comes the classification part.\n\nFlatten layer combines all the 32x12x12 values into a single vector of 4608 length. The Dense Layer is the fully connected layer which connects each of the 4608 units to its 128 units.It is followed by ReLU Activation and the Dropout. The Final Dense Layer is the fully connected layer which has the same number of output nodes as there are classses to be determined.In our case 10. The final Activation is softmax which maps the values returned into range 0 to 1.\n\nFollowing this is the compilation Step and the training step. The Compile step converts the model in efficient native code. The training process begins Model.fit.\n\nKeras is really easy to use framework to experiment with Deep Learning.", 
        "title": "Using Keras for MNIST \u2013 Gaurav Gupta \u2013"
    }, 
    {
        "url": "https://chatbotslife.com/deep-nlp-from-deep-learning-summer-school-cc23b523d1db?source=tag_archive---------2----------------", 
        "text": "What does it mean to build an agent that can understand natural language? It just means building a better language model. Widely adopted approaches include:\n\nLinguistics\u200a\u2014\u200ause theory and structure inherent in a language to model understanding:\n\nThe input question uses X syntax and Y grammar, so it must mean Z. Problematic because language is too fluid and the \u201crules\u201d for English are not very rigid. Perhaps more accurately, the rules for English are not followed very rigidly. This is especially true in the age of the internet and tweets when highly unstructured text is being generated at an astounding rate. The data does not represent formal English sentences, so trying to interpret the snippet as a normal sentence just falls apart.\n\nWhat is the probability of output response Y given the input question X?Using conditional probability, now you can simply model the sentence by trying to maximize the expected (log) probability of a series of words. i.e. What is the probability of each words given the probability of all the words that came before it?\n\nN-grams simplifies the model a bit by saying rather than looking at all previous words, let\u2019s just look back at n previous words, so if n = 5, then we only look back at the last 5 tokens preceding the word we are trying to predict\n\nThe problem is that if a certain sequence never occurs in the corpus, then that particular n-gram product goes to 0 because multiplying anything by 0 goes to 0.\n\nFor example, let\u2019s say you are using a trigram model You are trying to predict the likelihood of p(we, were, on, fire, last, night). This is the product of the probabilities:\n\nLet\u2019s make the reasonable assumption that the phrases \u201cwe were on\u201d, \u201c we were\u201d and \u201cfire last night\u201d all have high likelihood of occurring. However, let\u2019s also assume that \u201con fire last\u201d was never in the corpus, then p(x) = 0%. Now the entire sentence is rated as 0% even though the other phrases have high chance of occurring.\n\nNo matter how big your corpus, it won\u2019t cover all examples, so there is a Data Sparsity problem. To over come this issue, conventional solutions include:\n\nThis is still unsatisfactory because there is still a lack of generalization. For example, assume that the corpus includes (chases, a, dog) (chase, a, cat), (chases, a, rabbit) but does not include (chases, a, llama).\n\nEven after applying the smoothing or backoff, you still have either 0% or at the best, a workable suboptimal model. Thus, we move onto more sophisticated models\u00a0\u2026\n\nFeedforward neural nets\u200a\u2014\u200ause a parametric function approximator, which is to say we take a linear combination, but then we weight each factor differently according to their importance.\n\nFirst, create vector embeddings using one-hot encoding. Then join these to create a continuous-space word representation. This goes into any number of non-linear representation (a tanh or ReLU), aka an activation function. Then finally a softmax for prediction\n\nTo see why this model in an improvement, let\u2019s look at an example. Assume that your corpus includes three sentences:\n\nHow likely is \u201cgroups\u201d followed by the word \u201cthree\u201d? Under the normal n-gram model, the answer would be 0% chance since the phrase \u201cthree groups\u201d has never occurred in the corpus. This can be done a little smarter using NER and replacing \u201cthree\u201d with 3, \u201cfour\u201d with 4, and then tagging both as numbers. Then tagging people, places, things in a similar fashion. This is essentially make n-grams smarter by using feature engineering to add an extra layer of meta-data during training. At the end of the day though, this is still suboptimal method of representing the language space. Just as decision trees can be augmented with dozens of extra tweaks, sometimes it\u2019s best to just move onto another algorithm.\n\nIn this case, the neural network model has a strong advantage because doesn\u2019t follow a strict multiplication of seen vs. unseen occurrences. In particular, the softmax calculates a non-linear, normalized probability that is optimized through a cost function (or optimization function. This cost function calculates a soft distance between predicted (y-hat) vs actual (y).\n\nMost importantly, these penalty \u201ccosts\u201d are SUMMED together, rather than multiplied. Thus, wildly off prediction are heavily penalized, but never pushed the probability to zero.\n\nThe distance of these items are normally based off the euclidean distance of the vector embeddings. (Surprisingly, even just a matrix factorization with SVD or PCA of simple one-hot encoding can get reasonable results).\n\nSo from our example, we will see that \u201cthree\u201d will be projected to a space close to \u201cfour\u201d because \u201cthree teams\u201d and \u201cfour teams\u201d are both phrases that occur. And since the phrase \u201cfour groups\u201d also occur, the measure associated with \u201cthree groups\u201d becomes more likely (and certainly non-zero).\n\nOf course, even smarter vector embeddings such as word2vec will often yield even better results. Altogether, this means that Neural Nets can be great for generalization, while non-parametric approaches (i.e. n-gram modeling) can often be great for memorization of the data. One way to inspect this progress is to visualize the word space, using techniques such as t-SNE. We saw that this vector really convinced others of the efficacy of the model\n\nRecurrent Modeling (aka. Neural Machine Translation)\u200a\u2014\u200abreaks from the Markov assumption of looking at the n-grams before a word (t-4) to predict the current word (t), which has a core issue that dependencies beyond the context window are ignored.\n\nFor example, \u201cThe bright red umbrella which had been left at the restaurant over the last four weeks was finally returned to its owner who used it immediately for cover from the rain.\u201d\n\nIn this sentence, we learn that an \u201cumbrella\u201d is a type of object that can be used for \u201ccover\u201d, but the distance between the two words is so high that the relationship is never derived.\n\nWe made this simplification to ease our training, but if we train through another method, then perhaps this simplification is no longer necessary.\n\nOne way is to extend the length to much longer 5 \u2192 32 \u2192 64. And then apply zero-padding to get the lengths of all words to be relatively similar. An even better way is to use a RNN, namely a recurrent neural net. An RNN allows the user to input one word at a time, and at each time step the function approximator takes into account the current word, but also the interactions of all past words.\n\nIn this way, rather than feeding in a continuous concatenated word vector, we feed in one word at a time. Similar to how CNNs give an advantage over vanilla NNs by taking spatial information into consideration, RNNs give an advantage over vanilla NNs by taking temporal ordering into consideration.\n\nIt keeps track of weights using (h_t), which is calculated by each cell unit, and then passed back to the same cell unit in the next time step. This internal hidden state is also referred to as the memory of the network. Not to be confused with attention (Bandahau) or storage (Graves).\n\nRecurrent neural network example: p(eating|the, cat, is)\u200a\u2014\u200asame as above except the function (f) is a neural network, the final result is just the product of each output\n\nFinally, in both example, you Read, Update, Predict next time step (until we predict a EOF token). Now this is just a LSTM or GRU, that takes care of the vanishing gradient problem. (Recall: you can just do gradient clipping to avoid the exploding gradient problem)\n\nThe growth of LSTMs, GRU, and computing power presents many new opportunities in NLP based on Neural Nets. Originally we were worried that:\n\nBut it turns out that none of these fears were legitimate:\n\nThis is useful for many reasons. Only English has spaces and clear \u201cwords\u201d. In Arabic, there are single \u201cwords\u201d that mean \u201cand to his vehicle\u201d In Chinese, many ideas are represented by two \u201cword\u201d phrases In Finnish, one word \u201cthree\u201d is strictly less than a compound word \u201cthree thousand kilowatts per hour\u201d\n\nPut another way, words were never the most ideal representation of language. Breaking down further always gives more accuracy, but technical constraints held this back. Now that we no longer have these technical constraints, we should go ahead and dig deeper down into the character level.\n\nNLP is easier now since word segmentation (a.k.a. tokenization) is no longer needed. Furthermore, we don\u2019t even need to do this on the source (or target) side either. The model just knows how to string together characters to make up words!", 
        "title": "Deep NLP from Deep Learning Summer School \u2013"
    }
]