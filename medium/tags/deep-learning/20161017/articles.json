[
    {
        "url": "https://medium.com/the-mission/up-to-speed-on-deep-learning-september-part-2-and-october-part-1-d72d7e5df1ea?source=tag_archive---------0----------------", 
        "text": "Continuing our series of deep learning updates, we pulled together some of the awesome resources that have emerged since our last post on September 20th. In case you missed it, here are our past updates: September part 1, August part 2, August part 1, July part 2, July part 1, June, and the original set of 20+ resources we outlined in April. As always, this list is not comprehensive, so let us know if there\u2019s something we should add, or if you\u2019re interested in discussing this area further.\n\nOpen Sourcing 223GB of Driving Data by Oliver Cameron of Udacity. 223GB of image frames and log data from 70 minutes of driving in Mountain View on two separate days. Log data includes latitude, longitude, gear, brake, throttle, steering angles and speed. GitHub repo here.\n\nImageNet 2016: Large Scale Visual Recognition Challenge results (ILSVRC 2016). A yearly seminal competition where teams correctly classify and detect objects and scenes in images. The teams operate at the bleeding-edge of image recognition\u200a\u2014\u200alearn about the teams here.\n\nGenerating Faces with Deconvolution Networks by Michael Flynn. Neural networks that generate and interpolate between faces, based on a large publicly available dataset. Inspired by this paper on image generation. GitHub repo here.\n\nYoutube-8M Dataset by Google. 8 million video IDs and associated labels from over 4800 visual entities (e.g. vehicle, concert, music video, etc.), making it possible to advance research & applications of video understanding. Blog post here.\n\nDeep3D: Automatic 2D-to-3D Video Conversion with CNNs by Eric Junyuan Xie. 3D videos are typically produced in one of two ways: shooting with a special 3D camera or shooting in 2D and manually convert to 3D\u200a\u2014\u200aboth are hard. This project demonstrates automatic 2D-to-3D conversion, so you could potentially take a 3D selfie with an ordinary smartphone.\n\nOpen Sourcing a Deep Learning Solution for Detecting NSFW Images by Yahoo. An open-source classifier for identifying NSFW content, based on a CNN architecture and implemented with Caffe. GitHub repo here.\n\nAnticipating Visual Representations from Unlabeled Video by MIT. Anticipating actions and objects via computer vision is hard (e.g. if someone is gesturing forward to shake hands). Humans do this through extensive experiential knowledge and inference\u200a\u2014\u200ait\u2019s much harder for a machine. This implementation trains deep neural networks to predict the visual representation of images in the future. Forbes article here.\n\nTensorFlow in a Nutshell by Camron Godbout. A three part series that explains Google\u2019s deep learning framework TensorFlow. The guides cover the basics, hybrid learning, and an overview of supported models. Part 1, part 2, and now, part 3.\n\nThe Neural Network Zoo by Fjodor Van Veen. A cheat sheet that covers many of the popular neural network architectures. Great way to keep track various architectures and their underlying structures and relations. The cheat sheet has descriptions of each architecture and links to their original academic papers.\n\nTorch Video Tutorials by Alfredo Canziani. A video collection of intro tutorials on leveraging Torch, providing an overview of Lua, Torch, neural networks, CNNs, and relevant Torch packages. RNNs coming soon.\n\nShow and Tell: image captioning open sourced in TensorFlow by Google Brain. Chris Shallue and his team make available their image captioning system. It\u2019s faster to train, more detailed, and more accurate than past iterations. GitHub repo here. Original paper here.\n\nThe Alexa Prize by Amazon. A new annual competition for university students to advance the field of conversational AI. Participants develop a bot that converses coherently with humans for 20 minutes. The application process closes October 28, 2016\u200a\u2014\u200aapply here.\n\nBay Area Deep Learning School held at Stanford in late September and organized by Pieter Abbeel, Samy Bengio, and Andrew Ng. Speakers included Yoshua Bengio, Hugo Larochelle, Russ Salakhutdinov, and many others. All slide decks here and live stream videos from day 1 and day 2 are available.", 
        "title": "Up to Speed on Deep Learning: September, Part 2 and October, Part 1"
    }, 
    {
        "url": "https://medium.com/@Francesco_AI/who-is-best-positioned-to-invest-in-artificial-intelligence-a-descriptive-analysis-a3fd3f0f63f7?source=tag_archive---------1----------------", 
        "text": "Who is best positioned to invest in Artificial Intelligence?\n\nIt seems to me that the hype about AI makes really difficult for experienced investors to understand where the real value and innovation are. I would like then to humbly try to bring some clarity to what is happening on the investment side of the artificial intelligence industry.\n\nWe have seen as in the past the development of AI has been stopped by the absence of funding, and thus studying the current investment market is crucial to identify where AI is going. First of all, it should be clear that investing in AI is extremely cumbersome: the level of technical complexity goes out of the pure commercial scope, and not all the venture capitalists are able to fully comprehend the functional details of machine learning. This is why the figures of the \u201cAdvisors\u201d and \u201cScientist-in-Residence\u201d are becoming extremely important nowadays. Those roles would also help in setting the right expectations level, and figuring out what is possible and what is not.\n\nAI investors are also slightly different from other investors: they should have a deep capital base (it is still not clear what approach will pay off), and a higher than usual risk tolerance: investing in AI is a marathon, and it might take ten years or more to see a real return (if any). The investment so provided should allow companies to survive many potential \u201cAI winters\u201d (business cycles), and pursue a higher degree of R&D even to the detriment of shorter term profits. An additional key element of this equation is the regulatory environment, which is still missing and needs to be monitored to act promptly accordingly.\n\nWhen it comes to AI hardware or robotic applications then, few extra points are advised\u200a\u2014\u200ainvestors do not have to suffer from the sunk cost fallacy bias, and technical milestones should be clear a priori to track real progress.\n\nAll these characteristics are motivated by a series of AI-specific problems: first, as above-mentioned the technical complexity makes often AI startups black boxes. Secondly, it is quite hard to show proof of concepts. Some narrow AI prototype might be easier to be built, but in general, the difficulty of creating GAI-resembling software and the opaque benefit-costs analysis make hard to attract initial funding\u200a\u2014\u200aand in my opinion, this is where the governments should intervene in. The concern then about what kind of milestone is deemed investable (revenues, open source communities, etc.) is tangible, and I would suggest considering investable only those companies showing some degree of technical innovation, either actual (MVPs) or potential (academic publications), or with data virtuous cycle (a mixture of unique datasets and users).\n\nOn the hardware side instead, other considerations have to be added: they are way more expensive than AI software developments, and victim of higher obsolescence and replacement costs. Hence, the tradeoff cost/reliability/speed/full control adds a further layer of complexity in the investing game. In particular, it is interesting to notice that if we would be able to work in the robotics space at much lower costs, this would shift completely our risk aversion perception, and it would encourage investors to risk more given the lower cost.\n\nHaving identified all these characteristics, we can try to draw a rough profile of companies that might represent (ex-ante) good investment opportunities: an early sign of good potential investment is definitely the technical expertise of the founders/CEOs. You should prove to have the right mix of technical understanding, technology exposure, access to a wider network, and vision leadership in order to convince brilliant researcher to work for your AI company. The second point of interest is the diverse and multidisciplinary team: it does not sound impressing having all the co-founders or research team to come from the same school or previous research lab, but rather quite the opposite. Finally, startups that are people-centric are ex-ante more likely to succeed. The ability to create and supporting a developer community, as well as making products that are designed to be easily understandable have more probability to be adopted without frictions.\n\nIt is not a coincidence indeed that all the features so far highlighted were observable in early-stage success such as DeepMind. However, as we already emphasized earlier when discussing new business models, DeepMind has not only innovated from a strategic point of view, but it also stressed out the major points of interest for any AI startup. First, always aim to a general-purpose intelligence: the value DeepMind is proving to own is the ability to apply their general research in the same way to medical problems or energetic issue. Second, do not be afraid of public exposure to failure: challenging Lee Sedol on a live worldwide recording was risky, but the brand reward and resonance obtained from winning vastly overcame the effects from a (potential) public failure.\n\nIn order to study more deeply what the AI environment looks like, it has been created a unique customized dataset listing 13,833 startups, tracking down in financial news and SEC filings companies operating in artificial intelligence; machine learning; big data; analytics; robotics; and drones. Data about the company so selected have been filled using mainly Crunchbase[1] dataset, and major incumbents (Table 1) have been excluded.\n\nIdeally, we should spot some common features belonging to all the successful startups operating in the AI space, because the sector of activity largely influences the companies\u2019 structure. However, the artificial intelligence landscape is not mature yet, and it might be hard to reach strong conclusions. The novelty of the space can be noticed immediately looking at the companies\u2019 age distribution. In Fig. 1, it is clear how the majority of startups were born over the past 5\u20136 years (the peak has been reached in 2014), certainly because of the reasons we specified earlier and the recent AI wave.\n\nThe geographic concentration of AI companies gives us another insight. Fig. 2 shows that North America plays as expected the most important role in this sector, followed by Europe that accounts for less than a half of the American amount. The Asian ecosystem comes after, and most of the companies operating in Asia are rather in the hardware and robotics businesses. If we look instead at the countries\u2019 breakdown, it is not surprising that the USA represents more than 57% of the worldwide AI community. It is relevant though that the English and Indian landscapes are the other important AI clusters (Fig. 3).\n\nDigging one further layer (Fig. 4), we notice how San Francisco represents almost one-sixth of the entire market, but other cities such as London or Bangalore are important pieces of the puzzle as well. In fact, if we do not take into account San Francisco and New York that are clearly the two major worldwide startups centers, London and Boston occupy the third and fourth positions. This is not a coincidence, and I believe that these two cities have many similarities. First of all, they are in the middle of strong scientific academic triangles (Harvard, MIT, and Boston University from one hand, and Oxford, Cambridge, Imperial College from the other hand). This fosters the commercialization of academic spin-off and encourages the entrepreneurial culture between students and professors as well. This entrepreneurial wave turned over the past few years into the birth of manifold accelerators and incubators, which are essential for both the ideas generating process and the early development. Finally, the amount of venture funding is critical for the success of the ecosystem. Hence, talent, money and infrastructure are the main reasons of success for London and Boston ecosystem.\n\nThere are other two aspects that should be considered in analyzing the environment, namely the financing and the operational sides. From a financial point of view, we can firstly study the time series for the rounds of financing. Looking at Fig.5, which shows the breakdown of the round of financings by year over the past 16 years, we can notice how earlier stages of financing are decreasing on percentage in the last five years and the funding is redistributed to later stage. Despite that, Fig. 6 illustrates that the total amount of funding has drastically increased in the last 2\u20133 years. Those two insights suggest that usually, AI startups ask for lower rounds of financings (Fig. 7), and often they even barely reach rounds C or higher, either because they are not able to deliver what has been promised or because they are acquired by big players.\n\nWe can confirm this intuition looking at the number and types of exits for AI companies. Fig. 8 shows that a quite high number of startups exits being acquired, while a lower number raises funds in the public market.\n\nThe operational point of view can be instead faced through studying concrete variables of growth. First of all, we can notice in Fig. 9that the majority of AI startups are composed by up to ten people, and in some cases, they reach forty or even fifty elements. Even if the employees average is around 69 people, the median is rather around 8 persons per company\u200a\u2014\u200athat on the occasion of exits often means a range valuation per employee of $2.5\u200a\u2014\u200a$10 million. Furthermore, usually the first ten hired are mostly engineers or technical people, and just after the first round of financing further horizontal layers are added.\n\nAs growth measure, we looked instead at the employee growth on a monthly and semester basis (Fig. 10). Even if on a monthly basis it is quite normal to oscillate the total number of employees between -10% and +20%, on a longer time period many startups reach exponential growth rates close to 40%-50%.\n\nThe second proxy of growth is the impact the startups have through social media channels such as Twitter, Facebook, and Linkedin (Fig. 11). Month-by-month social media exposure grows between -10% and +20%, and this validates the idea that AI is gradually being socially acknowledged and used.\n\nThese are only a few common features we can find in AI startups, but there are for sure characteristics we are missing. A point of concern, for instance, is the average equity stake investors ask for funding AI startups. My hypothesis is that the average equity required is lower than what is asked in other sectors and that control is less relevant for those investments. The insight behind it is that the technical difficulty in understanding the product makes the venture capitalists contribution less effective from a product standpoint, and it is just limited to UX and market strategies.\n\nThe second concern is the funding resiliency to the business cycle and to the level of optimism of the mass about this technology. A negative phase might, in fact, pull back all the investments made, because as we explained before it is often hard to see profits in the short term. This might negatively impact the sector overall since the whole AI environment has been pushed for venture funding and corporate acquisition (see table below)\u200a\u2014\u200ainterestingly enough, all big technology players performed poorly in 2015, and this has been already noticed in their tighter acquisition strategies.\n\nAnother problem then is whether venture funding, accelerators, and incubators (Table 2) are actually accelerating AI real growth, or if they are just inflating expectations and AI short term impact. It might be the case in fact that venture funding is not accelerating the development of a strong AI, but rather the \u201ctime-to-exit\u201d. Data, in fact, confirms that on average the time-to-exit shrank to a period of 3\u20135 years for average AI startups.\n\nThe final point to highlight is the way in which the AI ecosystem is developing. We are observing a predetermined pollination process: the startup creates an MVP, and maybe launch a first version on the market. It needs for liquidity, and usually get one or two rounds of funding\u200a\u2014\u200aand thus grows, and hires as a consequence. As soon as the startup is getting some real traction, employees start leaving and they create their own things, encouraged by the idea of being operationally backed and financially supported by VCs. We are not able to conclude with a positive or negative feedback on this mechanism, but it looks at a first glance a bit unstable to lay the foundations for the research of a general artificial intelligence.\n\n[1] http://www.crunchbase.com. I have obtained a Crunchbase License that allowed me to complete the dataset with relevant missing information of several companies.", 
        "title": "Who is best positioned to invest in Artificial Intelligence?"
    }, 
    {
        "url": "https://medium.com/emergent-future/the-fault-in-our-approach-what-youre-doing-wrong-while-implementing-recurrent-neural-network-lstm-929fbe17723c?source=tag_archive---------2----------------", 
        "text": "I started to delve into the field of Machine Learning few months back and after making a few projects, I thought to myself, \u201cthis isn\u2019t really tough\u201d. That was until I encountered Deep Learning. A whole new field of study, Deep Learning requires a vast amount of mathematical as well as analytical knowledge.\n\nAs I was preparing to get hands on with Neural Nets, I realized that it is so overwhelming. There are so many complex concepts that cannot be just \u201clearnt and implemented\u201d. Recurrent Neural Networks, for example, requires a different level of understanding than Convulation Neural Networks. My learnings were marred by the struggle of not being able to implement Neural Nets.\n\nI was focusing too much on implementing RNN with LSTM that I totally overlooked the implication of the model. So here\u2019s my first advice: Do not be a robot and jump straight into coding. Modelling a RNN requires patience and understanding. How can you create a net without being able to determine the input dimensions?\n\nI was given the job to pick up a case study and apply RNN model on it. The mistake I commited here was to start coding right away without even giving a single thought to the data! I chose to model a Stock Market Prediction Engine as my case study.", 
        "title": "The Fault in our Approach: What you\u2019re doing wrong while implementing Recurrent Neural\u2026"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/a-development-methodology-for-deep-learning-2ce515158bb7?source=tag_archive---------3----------------", 
        "text": "The practice of software development has created development methodologies such agile development and lean methodology to tackle the complexity of development with the objective of improving the quality and efficiency of software creation. Although Deep Learning is built from software it is a different kind of software and therefore a different kind of methodology is needed. Deep Learning differs most from traditional software development in that a substantial portion of the process involves the machine learning how to achieve objectives. The developer is not completely out of the equation, but is working in concert to tweak the Deep Learning algorithm.\n\nDeep Learning is sufficiently rich and complex a subject that a process model or methodology is required to guide a developer. The methodology addresses the necessary interplay of the need for more training data and the exploration of alternative Deep Learning patterns that drive the discovery of an effective architecture. The methodology depicted as follows:\n\nWe begin first we some initial definition of the kind of architecture we wish to train. This will of course be driven by the nature of the data that we a training from and the kind of prediction we seek. The latter is guided by Explanatory Patterns and the former by Feature Patterns. There are a variety ways to optimize our training process, this is guided by the Learning Patterns.\n\nAfter the selection of our network model and the data we plan on training on, the developer is then tasked with answering the question as to whether adequate labeled training set is available. This process goes beyond conventional machine learning process that divides the dataset into three sets. The machine learning convention has been to create a training set, a validation set and a test set. In the first step of the process, if the training remains high there are several options that can be pursued. The first is to try to increase the size of the model, a second option is perhaps train a bit long (alternatively perform hyper-parameter tuning) and if all fails then the developer tweaks the architecture or attempts a new architecture. In the second step of the process, a develop validates the training against a validation set, if the error rate is high indicating overfitting then the options are to find more data, apply different regularizations and if all fails attempt another architecture. The observations here that differs from conventional machine learning is that Deep Learning has more flexibility in that a developer has the additional options of employing either a bigger model or using more data. One of the hallmarks of deep learning is its scalability in performing well when trained with large data sets.\n\nTrying a larger model is something that a developer has control over, unfortunately finding more data poses a more difficult problem. To satisfy this need for more data one can leverage data from different contexts. In addition one can employing data synthesis and data augmentation to increase the size of training data. These approaches however lead to domain adaptation issues, so a slight change in the traditional machine learning development model is called for. In this extended approach, the validation and training sets are required to belong to the same context. Furthermore, to validate training from this heterogeneous set, another set called the training-validation set is set aside to act as additional validation. This basic process model, inspired by a talk by Andrew Ng, serves as a good scaffolding to hang off the many different patterns that we find in Deep Learning.\n\nAs you can see, there are many paths of exploration and many alternatives models that may be explored to reach to a solution. Furthermore, their is sufficient Modularity in Deep Learning that we may compose solutions from other previously developed solutions. Autoencoders, Neural Embedding, Transfer Learning and bootstrapping with pre-trained networks are some of the tools that do provide potential shortcuts that reduce the need to train from scratch.\n\nThis is no means complete, but it is definitely a good starting point to guide the development of this entirely new kind of architecture.", 
        "title": "A Development Methodology for Deep Learning \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://towardsdatascience.com/advancements-in-ai-3c56c6b5ea9?source=tag_archive---------4----------------", 
        "text": "Ivy Data Science is an applied A.I. company that offers: A.I. platform for Healthcare/ Finance/ Energy/ Retail, Consulting and Advisory, the world\u2019s first 12-week A.I. Immersive Bootcamp, and an AI startup incubator.\n\nThe last few years have seen some truly dramatic developments in the field of artificial intelligence (AI). Hardly a week goes by these days without some announcement of a new record being broken by clever artificial intelligence algorithms. So what is AI and why now? Intelligence is an agent\u2019s ability to adapt to and to achieve goals in its environment. Artificial simply means non-biological. However, the lines are starting to blur between biological and non-biological (machine) intelligence. In fact, it\u2019s become pretty clear that there is no fundamental difference between the two, and that they are in fact, fast converging. The important concept seems to be computation.\n\nSo why now? There are several reasons for this. They involve the amount of labelled data available, tremendous increase in compute power as well as advances in algorithms, the software that processes all this new data. In November, for example, Google stunned the machine learning community by open sourcing it\u2019s prized framework TensorFlow, that it uses in over one hundred of its internal projects, making this software freely available to the world overnight. This was followed almost instantly by a flurry of companies all open sourcing similar frameworks, including Microsoft and Samsung.", 
        "title": "Advancements in AI \u2013"
    }, 
    {
        "url": "https://machinelearnings.co/machine-learnings-13-rotting-organs-and-self-aware-machines-78e543f678c0?source=tag_archive---------5----------------", 
        "text": "1/ Cognitive science professor argued rare disease is key to determining if machines can be self aware. link\n\n2/ Advocates for the visually impaired pushed for development of autonomous vehicles that can be driven by blind people. link\n\n3/ President Obama addressed concerns over machine learning and the impacts it will have on our planet. link\n\n4/ OpenAI started using Reddit to teach an artificial intelligence how to speak. link\n\n5/ Researchers stressed the need for agreed methods to assess the effects of autonomous systems on human populations. link\n\n6/ The first concert consisting almost entirely of music composed by AI took place in North East London. link\n\n7/ The White House laid out a strategic plan for federally-funded research and development in AI. link\n\n8/ Researchers at Google DeepMind broadened the abilities of today\u2019s best AI systems by giving them a working memory. link\n\n10/ A team of MIT PhDs and Google[x]\u2019ers raised $5.25M to build a self-driving car startup, Optimus Ride. link\n\nMachine Learning is set to change our world as we know it, and you have made the decision to stay informed as the news unfolds. You are awesome. Keep it a secret, or invite one of your friends.", 
        "title": "Rotting organs and self aware machines \u2014 #13 \u2013"
    }, 
    {
        "url": "https://medium.com/mit-initiative-on-the-digital-economy/pedro-domingos-in-search-of-the-master-algorithm-for-machine-learning-c1893bdb2c70?source=tag_archive---------6----------------", 
        "text": "Philosophers have said that to know your present, look to the past, then, imagine the future. This aphorism holds true when it comes to understanding how machine learning works. First, we need to understand the underlying principles of where knowledge comes from, and how humans learn. But which principles should we follow? Figuring out the best approach falls to scholars like Pedro Domingos.\n\nDomingos, a professor of computer science at the University of Washington and the author of The Master Algorithm (Basic Books, 2015), said that in the past few decades, five schools of thought have dominated the understanding of machine learning, each with its own master algorithm and each with its own flaws.\n\nAt a recent MIT IDE seminar, he explained these \u201cfive tribes of machine learning,\u201d and how they each contribute to the ultimate goal of a unified, \u201cmaster algorithm\u201d that will combine many parts into a scalable model. In other words, to fully grasp the potential of artificial intelligence, it is necessary to deconstruct human intelligence and learning.\n\nDomingos said he agrees with the sentiment of Yann LeCun, director of AI at Facebook, that: \u201cMost of the knowledge in the future will be extracted by machines and will reside in machines.\u201d But we have a long way to go to perfect that learning, he said.\n\nStudying human learning patterns, according to Domingos, requires a broad, multidisciplinary approach. In fact, for much of the 20th Century, philosophy and logic vied with biology, neuroscience, statistics and psychology to explain how humans learn and how computers can emulate that process.\n\nThese five fields have led to distinct paradigms and schools of research that became known as:\n\nEmergence of each concept was a major advance, and each one exceeds the others by orders of magnitude, Domingos said. Computer learning is already taking leaps forward and is building on these theories for different types of applications. Baysean thinking has led to Spam filters that use machine learning to distinguish what is spam, while medical diagnosis are often based on analogy-based algorithms, for instance.\n\nAnd one application of symbolic learning is a robotic biologist that learns about yeasts and uses inverse deduction to study growth; robot scientists that use logic and symbolic learnings are being tested today as well.\n\nAt Google and Microsoft, artificial models of neural networks are being developed to create Deep Learning environments based on some of the work of Connectionists. Yet, deep learning has many challenges, Domingos said, and producing a whole network of these neurons is difficult to achieve.\n\nAs his book explains, \u201cin the world\u2019s top research labs and universities, the race is on to invent the ultimate learning algorithm: one capable of discovering any knowledge from data, and doing anything we want, before we even ask.\u201d When will we find it? It\u2019s hard to predict, because scientific progress is not linear. It could happen tomorrow, or it could take many decades.", 
        "title": "Pedro Domingos: In Search of the Master Algorithm for Machine Learning"
    }, 
    {
        "url": "https://medium.com/@sderymail/deep-preach-sunday-sermon-oct-16-8c2d6d0732d0?source=tag_archive---------7----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep Preach \u2014 Sunday Sermon (Oct 16) \u2013 Sebastien Dery \u2013"
    }, 
    {
        "url": "https://medium.com/@BeckyHolmes/audit-yourself-a592c60b930a?source=tag_archive---------8----------------", 
        "text": "So you think you\u2019re self-aware?\n\nThe best thing you can do to make the most of your life is to become self-aware. The ironic misfortune is that those who are not self-aware, are less able to see that they are not self-aware.\n\nThose who are self-aware are great at diving deep into their souls and pulling out gems of information\u200a\u2014\u200athose who are not as self-aware are often unable to see beyond their basic \u201clikes\u201d and \u201cdislikes\u201d and get confused when they face deep questions around the greater picture of their motives and desires. They\u2019re not stupid. It\u2019s that they never thought to think in that way. It wasn\u2019t intuitive and no one ever taught them that they should either. These are the people who wake up 50 years later wondering how they got there. How can you pursue the right thing if you don\u2019t truly know yourself?\n\nIt\u2019s easy to get caught up in superficial interactions and quick hits with society. It\u2019s easy to get caught up in \u201cArticle B is due by Friday\u201d, and \u201cGet this order out in two hours\u201d, and \u201cMom needs you to pick up the groceries today\u201d. But when do you step outside of it all?\n\nLet\u2019s say your job is to hack down trees with an axe all day. Many people would spend their 9 to 5 hacking the tree. While others take an hour to pull back and sharpen their axe or think about how they could reengineer the axe to be more efficient.\n\nLet\u2019s say your job is to fly a plane and you hit a storm that is tossing your plane wildly around. You could just stare straight ahead and assume you\u2019ll arrive at the destination regardless or you could take a moment to recalibrate and call air traffic to ensure you\u2019re on the right path again.\n\nWhen do you audit yourself? When do you step back? When do you take out time to stop and think? Deeply think. If you have to\u200a\u2014\u200aschedule it out. Maybe you need to stop and recalibrate once a week, maybe once a month, or once a quarter. Figure it out and set aside the time.\n\nHere\u2019s a quick internal and external audit for your self-awareness benefit. Enjoy.\n\nNot just the standard job interview \u201chard-working, team-player\u201d medley\u200a\u2014\u200abut your real strengths. The things that set you apart from Anna & Juan. Be specific. Instead of one word adjectives, come up with sentences that set you in an experience.\n\nThink, \u201cListens and combines input from all members of a team before making decisions\u201d instead of, \u201cteam player\u201d\n\nThink, \u201cHas the ability to make all people feel important and worthy through providing a safe space to share\u201d instead of, \u201cnice\u201d\n\nThink, \u201cIs intuitive and foresees times & circumstances when others need them and acts upon it\u201d instead of, \u201chelpful\u201d\n\nGo ahead. Make a list. Think of the characteristics that come out no matter the circumstance. When and in what way are you naturally inclined to take action.\n\nSorry you can\u2019t use a pseudo weakness that\u2019s actually a strength to answer this question. What are your weaknesses? When do you always come up short? Be specific, just like in step 1.", 
        "title": "Audit Yourself. \u2013 Becky Holmes \u2013"
    }, 
    {
        "url": "https://resources.trainingdata.com/artificial-intelligence-machine-out-plays-gamers-in-video-game-1645d0f45034?source=tag_archive---------9----------------", 
        "text": "When it comes to gaming, Carnegie Mellon students are usually the ones beating the computer. But this time, the computer beat the students. Carnegie Mellon University computer science students, Devendra Chaplot and Guillaume Lample, recently made an artificial intelligence (AI) agent in the video game Doom that outplays computer-generated agents and human gamers. They accomplished this by applying deep-learning techniques that taught their agent, Arnold, to manipulate the game\u2019s 3-D design. \u201cThe work is purely a result of our passion for artificial intelligence and video games,\u201d Chaplot said. \u201cGames have been a testbed for advancement of AI since decades, like Chess, Poker, 2-D Atari Games, Go, 3D FPS Games, and this development can be viewed as a very small step towards creating a general Artificial Intelligence\u2026", 
        "title": "Artificial intelligence machine out-plays gamers in video game"
    }
]