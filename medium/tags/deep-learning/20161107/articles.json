[
    {
        "url": "https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-38dad1cf7571?source=tag_archive---------0----------------", 
        "text": "To explore the limits of the HOG + Sliding Window pipeline, we apply it to a scene with a less uniform background and from a different sensor. Recall that our classifier was trained on DigitalGlobe data with 0.5 meter ground sample distance (GSD), though our test image below is a Planet image at 3m GSD.\n\nThe HOG + Sliding Window object detection approach discussed in previous posts ( 4 , 5 ) demonstrated impressive results in both open water and harbor (F1 ~ 0.9). Recall from Section 2 of 5 that we evaluate true and false positives and negatives by defining a true positive as having a Jaccard index (also known as intersection over union) of greater than 0.25. Also recall that the F1 score is the harmonic mean of precision and recall and varies from 0 (all predictions are wrong) to 1 (perfect prediction).\n\nThe ImageNet competition has helped spur rapid advancements in the field of computer vision object detection, yet there are a few key differences between the ImageNet data corpus and satellite imagery. Four issues create difficulties: in satellite imagery objects are often very small (~20 pixels in size), they are rotated about the unit circle, input images are enormous (often hundreds of megapixels), and there\u2019s a relative dearth of training data (though efforts such as SpaceNet are attempting to ameliorate this issue). On the positive side, the physical and pixel scale of objects are usually known in advance, and there\u2019s a low variation in observation angle. One final issue of note is deception; observations taken from hundreds of kilometers away can sometimes be easily fooled. In fact, the front page of The New York Times on October 13, 2016 featured a story about Russian weapon mock-ups (Figure 1).\n\nDetection of small objects over large swaths is one of the primary drivers of interest in satellite imagery analytics. Previous posts ( 4 , 5 ) detailed efforts to localize boats in DigitalGlobe images using sliding windows and HOG feature descriptors. These efforts proved successful in both open water and harbor regions, though such techniques struggle in regions of highly non-uniform background. To address the shortcomings of classical object detection techniques we implement an object detection pipeline based upon the You Only Look Once framework. This pipeline (which we dub You Only Look Twice) greatly improves background discrimination over the HOG-based approach, and proves able to rapidly detect objects of vastly different scales and over multiple sensors.\n\nWe adapt the You Only Look Once (YOLO) framework to perform object detection on satellite imagery. This framework uses a single convolutional neural network (CNN) to predict classes and bounding boxes. The network sees the entire image at train and test time, which greatly improves background differentiation since the network encodes contextual information for each object. It utilizes a GoogLeNet inspired architecture, and runs at real-time speed for small input test images. The high speed of this approach combined with its ability to capture background information makes for a compelling case for use with satellite imagery.\n\nThe attentive reader may wonder why we don\u2019t simply adapt the HOG + Sliding Window approach detailed in previous posts to instead use a deep learning classifier rather than HOG features. A CNN classifier combined with a sliding window can yield impressive results, yet quickly becomes computationally intractable. Evaluating a GoogLeNet-based classifier is roughly 50 times slower on our hardware than a HOG-based classifier; evaluation of Figure 2 changes from ~2 minutes for the HOG-based classifier to ~100 minutes. Evaluation of a single DigitalGlobe image of ~60 square kilometers could therefore take multiple days on a single GPU without any preprocessing (and pre-filtering may not be effective in complex scenes). Another drawback to sliding window cutouts is that they only see a tiny fraction of the image, thereby discarding useful background information. The YOLO framework addresses the background differentiation issues, and scales far better to large datasets than a CNN + Sliding Window approach.\n\nThe framework does have a few limitations, however, encapsulated by three quotes from the paper:\n\nTo address these issues we implement the following modifications, which we name YOLT: You Only Look Twice (the reason for the name shall become apparent later):\n\n\u201cOur model struggles with small objects that appear in groups, such as flocks of birds\u201d\n\n\u201cIt struggles to generalize objects in new or unusual aspect ratios or configurations\u201d\n\n\u201cOur model uses relatively coarse features for predicting bounding boxes since our architecture has multiple downsampling layers from the original image\u201d\n\nThe output of the YOLT framework is post-processed to combine the ensemble of results for the various image chips on our very large test images. These modifications reduce speed from 44 frames per second to 18 frames per second. Our maximum image input size is ~500 pixels for NVIDIA GTX Titan X GPU; the high number of parameters for the dense grid we implement saturates the 12GB of memory available on our hardware for images greater than this size. It should be noted that the maximum image size could be increased by a factor of 2\u20134 if searching for closely packed objects is not required.\n\nTraining data is collected from small chips of large images from both DigitalGlobe and Planet. Labels are comprised of a bounding box and category identifier for each object.\n\nWe initially focus on four categories:\n\nWe label 157 images with boats, each with an average of 3\u20136 boats in the image. 64 image chips with airplanes are labeled, averaging 2\u20134 airplanes per chip. 37 airport chips are collected, each with a single airport per chip. We also rotate and randomly scale the images in HSV (hue-saturation-value) to increase the robustness of the classifier to varying sensors, atmospheric conditions, and lighting conditions.\n\nWith this input corpus training takes 2\u20133 days on a single NVIDIA Titan X GPU. Our initial YOLT classifier is trained only for boats and airplanes; we will treat airports in Part II of this post. For YOLT implementation we run a sliding window across our large test images at two different scales: a 120 meter window optimized to find small boats and aircraft, and a 225 meter window which is a more appropriate size for larger vessels and commercial airliners.\n\nThis implementation is designed to maximize accuracy, rather than speed. We could greatly increase speed by running only at a single sliding window size, or by increasing the size of our sliding windows by downsampling the image. Since we are looking for very small objects, however, this would adversely affect our ability to differentiate small objects of interest (such as 15m boats) from background objects (such as a 15m building). Also recall that raw DigitalGlobe images are roughly 250 megapixels, and inputting a raw image of this size into any deep learning framework far exceeds current hardware capabilities. Therefore either drastic downsampling or image chipping is necessary, and we adopt the latter.\n\nWe evaluate test images using the same criteria as Section 2 of 5, also detailed in Section 2 above. For maritime region evaluation we use the same areas of interest as in (4, 5). Running on a single NVIDIA Titan X GPU, the YOLT detection pipeline takes between 4\u201315 seconds for the images below, compared to the 15\u201360 seconds for the HOG + Sliding Window approach running on a single laptop CPU. Figures 6\u201310 below are as close to an apples-to-apples comparison between HOG + Sliding Window and YOLT pipeline as possible, though recall that the HOG + Sliding window is trained to classify the existence and heading of boats, whereas YOLT is trained to produce boat and airplane localizations (not heading angles). All plots use a Jaccard index detection threshold of 0.25 to mimic the results of 5.", 
        "title": "You Only Look Twice \u2014 Multi-Scale Object Detection in Satellite Imagery With Convolutional Neural\u2026"
    }, 
    {
        "url": "https://medium.com/transmission-newsletter/transmission-1-d90c34f7568b?source=tag_archive---------1----------------", 
        "text": "Style transfer is a technique (popularized by Prisma ) that utilizes deep learning to attempt to replicate an input photo using the style specified during training. Fast Style Transfer is an implementation using TensorFlow that can process an image in 100ms on a NVIDIA Titan X GPU. It even works on videos! Read more\u2026\n\nThe gold standard of compression for JPEGs ( JPEG 2000 ) has been beaten for the first time by an approach using deep learning, built by a team at Twitter. Prepare to see many more iterations on this idea! Read the paper\u2026\n\nLipNet is a ridiculously impressive LSTM recurrent network that attempts to read lips (imagine the possibilities!), achieving 93.4% accuracy on the GRID corpus , outperforming experienced human lipreaders and the previous 79.6% state-of-the-art accuracy. Read more\u2026\n\nDeep learning-powered image upscaler Neural Enhance (built by must-follow Tweeter @alexjc ) continues to improve! New models are demonstrating better performance when handling blur and noise. See examples here and here . Read more\u2026\n\nWhy are just two car models used extensively in autonomous vehicle development? The Lexus RX450 and the Ford Fusion, with its sibling, the Lincoln MKZ (the car we work on at Udacity) appear to dominate the landscape. Read more about why\u2026\n\nMy colleague David shared the news that Blackberry is partnering with Ford, and how transformational it could be for Blackberry. Could Blackberry turn their company around by building software for self-driving cars? Read more\u2026\n\nReader @zuzoovn shared his multi-month and comprehensive study plan to go from a mobile developer (self-taught with no CS degree) to a machine learning engineer. Very useful read if you find yourself in a similar spot! Read more\u2026\n\nHere\u2019s how it works: The Sunflower Home Awareness System relies on the drone and a handful of in-ground smart lights to watch over your house. It detects motion, vibration and sound. Using machine learning, the system can distinguish between a human, a car and animals. Crazy! Read more\u2026\n\nHaving followed the work of @crizcraig on DeepDrive for quite a while, I was super excited to hear he has released instructions for anyone to get up and running with his self-driving car simulator in GTA V. It\u2019s not a simple installation, and certainly not for the faint-hearted, but will be a ton of fun once running, promise! Read how to get up and running\u2026", 
        "title": "Using Deep Learning to Read Lips, Compress JPEGs, Protect Homes & More"
    }, 
    {
        "url": "https://medium.com/@surmenok/intelligence-platform-stack-8c623f71f990?source=tag_archive---------2----------------", 
        "text": "Machine intelligence field grows with breakneck speed since 2012. That year Alex Krizhevsky, Ilya Sutskever and Geoffrey E. Hinton achieved the best result in image classification on LSVRC-2010 ImageNet dataset using convolutional neural networks. It\u2019s amazing that end-to-end training of a deep neural network worked better than sophisticated computer vision systems with handcrafted feature engineering pipelines being refined by researchers for decades.\n\nSince then deep learning field got the attention of machine learning researchers, software engineers, entrepreneurs, venture investors, even artists and musicians. Deep learning algorithms surpassed the human level of image classification and conversational speech recognition, won Go match versus 18-time world champion. Every day new applications of deep learning emerge, and tons of research papers are published. It\u2019s hard to keep up. We live in a very interesting time, future is already here.\n\nDeep learning is rapidly moving from research labs to the industry. Google had 2700+ projects using deep learning in 2015.\n\nA neural network is only one layer of an intelligent application. If you look at a typical deep learning system, it includes hardware (CPU, GPU, FPGA), drivers, programming languages, libraries, models, etc. Almost no company can develop the entire stack alone, the work should be divided between multiple organizations and multiple teams inside of an organization. It is natural to structure computer systems in layers, from bare metal in the bottom to more abstract layers on the top. If interfaces between layers are stable and well defined, it is easy for engineers to focus on one of the layers and reuse lower layers developed by someone else.\n\nThere is no stable intelligence platform stack yet, it is open for interpretation and changes. Most common model of the stack includes 8 layers, from hardware in the bottom to an application layer on the top. Layers from the bottom up:\n\nTypically it is multi-core CPU or GPU. GPUs are much more powerful than CPUs for deep learning applications. Thanks to gamers who paid for the progress on GPU engineering. Now there are companies working on hardware which is more power efficient than GPUs: FPGA and ASIC chips. For example, Google is using special chips named TPU (tensor processing unit) in their data centers.\n\nCompanies working on this layer: NVIDIA, Intel, Altera (acquired by Intel), Google (tensor processing unit).\n\n2. Hardware abstraction. NVIDIA CUDA and CuDNN provide functionality for training and inference of neural networks on NVIDIA GPU.\n\n3. Programming language. Developers need to use a programming language to do data preprocessing, model training, inference. Python is the most popular language for deep learning, at least for training, because there are very good scientific computing libraries for Python. C++, Lua, R are also being used, though less than Python.\n\n4. Deep learning libraries/frameworks. Perhaps the most important layer. It contains means for expressing deep learning models and often contains algorithms for auto-differentiation and optimization. Theano, Torch, and Caffe have been used by researchers and engineers a lot. In 2015 Google released TensorFlow, and it quickly became the most popular framework for deep learning. A few more recently released libraries: CNTK (Microsoft), Warp-CTC (Baidu).\n\nAll these frameworks have different levels of abstraction, e.g. Theano has auto-diff but doesn\u2019t have built-in code for popular types of neural networks (LSTM, CNN), so you will need to program neural network code yourself or use higher level frameworks which work on top of Theano, e.g. Keras.\n\nThe libraries have different characteristics for use in a production environment: TensorFlow provides very efficient C++ based TensorFlow Serving component which can do inference, Theano doesn\u2019t have anything like that.\n\n5. Neural network architecture. There are lots of different ways how you can configure a neural network: which types of neurons you use, how many layers and neurons, regularization techniques. See a catalog of architectures in Neural Network Zoo.\n\nNeural networks tend to get larger and more sophisticated. AlexNet had 8 learned layers, more recent ResNet networks from Microsoft researchers use 200\u20131000 layers.\n\nRecent neural network models also are more complex and modular than earlier architectures. A Dynamic Memory Network model, presented by Richard Socher at Bay Area Deep Learning School 2016 contains 5 modules:\n\nThis is an area of active research: Google, Facebook, Baidu, Microsoft, Stanford, University of Toronto and others.\n\n6. Cognitive architecture. A few neural networks and other kinds of AI modules can be combined to support smarter reasoning. Deep learning models which extract meaning from the text can work together with symbolic computation, use predefines rules and knowledge bases to get to deeper levels of text understanding. Supervised learning models can be combined with unsupervised learning models to use unlabeled data better. Large neural networks can be split into smaller neural networks trained and configured separately. This area is quite interesting, and there are more problems than solutions here yet.\n\n7. Machine learning as a service. Cloud APIs for machine learning, e.g. Google Vision API, Google Cloud ML. The chatbot ML services wit.ai, api.ai, luis.ai are in this category too.\n\n8. Application. An application which is accessible by end users and provides business value.", 
        "title": "Intelligence Platform Stack \u2013 Pavel Surmenok \u2013"
    }, 
    {
        "url": "https://medium.com/@IntuitMachine/deep-learning-can-now-create-itself-92e7ff0d59a7?source=tag_archive---------3----------------", 
        "text": "This is actually just the beginning of Deep Learning systems just bootstrapping themselves. So I must now share Schmidhuber\u2019s cartoon that aptly describes what is happening:\n\nThis is absolutely shocking and there\u2019s really no end in sight as to how quickly Deep Learning algorithms are going to improve. This meta capability allows you to apply it on itself, recursively creating better and better systems.\n\nFeel to jump into the conversation by requesting an invite at LinkedIn: https://www.linkedin.com/groups/8584076 or join at Facebook: https://www.facebook.com/groups/deeplearningpatterns/\u00a0.", 
        "title": "Deep Meta-Learning : Machines now Bootstrap Themselves"
    }, 
    {
        "url": "https://medium.com/@tuzzer/building-a-deep-handwritten-digits-classifier-using-microsoft-cognitive-toolkit-6ae966caec69?source=tag_archive---------4----------------", 
        "text": "Before we begin, we need to install CNTK. I found that the easiest way is to download the CNTK binary and run the installation script. It will install CNTK along with Anaconda 3 and Python 3.4 automatically. For this tutorial, I was using \u201cCNTK for Linux v2.0 Beta2 CPU only\u201d and running it on Ubuntu 16.04.1 LTS.\n\nBelow shows all the Python modules that we will need for this tutorial. gzip, os, struct, urllib are just for loading the data set. For building the neural network, we really only need to import numpy and cntk.\n\nI really like how Tensorflow made it very easy for new users to try using it by including many popular data sets as part of the library. Importing the MNIST data in Tensorflow only took two lines of code. On the other hand, for CNTK, we have to download and process the data sets ourselves. In fact, the data loading parts of the code are way longer than that actual machine learning parts!\n\nThe five functions below essentially enable us to download the MNIST handwritten digits training and testing data sets from Yann LeCunn\u2019s website, save them in a local folder named \u201cMNIST\u201d, and convert them to the text format required by CNTK Text Reader. Most of these code were copied from the CNTK data loading tutorial. However, instead of re-downloading and re-processing the data sets every time we run the program, these functions look into the local directory first and load the local copy if it exists.\n\nJust one more step before we get to the machine learning part.\n\nEach image sample in the MNIST data set is 28 x 28 pixels large and greyscale (thus, 1 channel). This means that the input to the CNN has 784 (= 1*28 *28) dimensions (input_dim).\n\nEach image can be classified as a digit between 0 and 9. Using one-hot encoding, we have a total 10 output neurons (output_dim), each representing the probability that a given image is one of the 10 classes.\n\nAccording to the MNIST database, the number of training and testing sets are 60,000, and 10,000 respectively, and we are going to use all of them!\n\nAfter specifying these constants about the data sets, we read in the data (either loading from file or downloading from web) and convert them to a format that is usable by CNTK using the functions defined above.\n\nConstructing a neural network is actually very simple in CNTK. Unlike Tensorflow, many variables and parameters are defined implicitly. For instance, it is not necessary to define the weight and bias variables; those are created implicitly for us when we create a convolutional layer.\n\nThe first convolutional layer computes 32 features from 5 x 5 patches with stride size of 1 and padding. We then feed its outputs into the first pooling layer, which does max pooling over 2 x 2 blocks.\n\nThe second convolutional layer is essentially the same as the first one except that it computes 64 features. Both layers use the classic ReLU function as their activation functions.\n\nEach convolutional layer computes small patches of a given image independently. Then at the subsequent pooling layer, those small patches are pooled into a smaller number of bigger patches to capture the relationships among the different patches. After the two layers of pooling, the total number of patches is significantly smaller compared to the number of raw pixels. We then feed the outputs of all of those patches (or neurons) into a fully-connected layer where all patches that make up the entire image can be processed at once. Our fully-connected layer has 1024 neurons.\n\nA dropout layer is added to prevent over-fitting during the training stage. It works by turning off a random set of neuron in both the forward pass (i.e. setting their activation values to 0) and backward pass (i.e. not updating their weights). In CNTK, the dropout layer is automatically disabled during testing.\n\nFinally, the output layer, which is made up of 10 neurons, outputs the 10 values. Once the CNN is trained, each output variable represents the softmax probability of each class of a given image.\n\nAs some of you might have noticed, I configured this to be identical to the one presented in the Tensorflow Deep MNIST tutorial. I have no explanation on why this particular configuration and CNN structure were chosen.\n\nWe also need to define a placeholder for the input variables that will be fed into the CNN during the training and testing processes.\n\nThe code below shows how everything is put together.\n\nWe can speed up the training process by initializing the weights with some random values drawn from a Guassian distribution and biases with a small constant value.\n\nThe model trainer requires the training labels as input. It is basically an array of 10 (output_dim) numbers. Again, it must be defined as a placeholder (cntk.ops.input_variable) since we are going to feed it data later during the training process.\n\nRather than adjusting the weights and biases one training sample at a time, we train in minibatches of data. This allow us to optimize for the average loss at each back prorogation pass, which enables training to complete faster and be less affected by noise in the data.\n\nThe learning rate and momentum affect how much the values of weights and biases change at each back propagation pass. In general, a higher learning rate or momentum makes the training process less affected by noise at the cost of longer training time.\n\nWe then specify the loss function for the trainer. Similar to the Tensorflow tutorial, we use the cross-entropy loss function here. For those who are interested, this article explains why using cross-entropy as the loss function is better than using classification error.\n\nIn addition, we need specify the function that computes the classification errors so that we know how well our classifier is doing.\n\nFinally, we instantiate the trainer. Here we use the Adam Stochastic Optimizer to compute the gradient descent.\n\nWith both the convolutional neural network and the trainer set up, we are at last ready to train the model!\n\nIn each epoch, we feed the trainer a new minibatch of 50 samples every loop until we have used all 60,000 training samples. The trainer adjusts the weights and biases through back prorogation to reduce the training error at each iteration.\n\nAfter maybe around 5 to 10 minutes (depending on how fast your computer runs) of training, we have our handwritten digits classifier at last!\n\nWe can feed in the test data into our classifier and see how well it performs. In this particular implementation, I got an average classification error of around 1.46% error after just 1 epoch and 1.03% error after 2 epochs!", 
        "title": "Building a Deep Handwritten Digits Classifier using Microsoft Cognitive Toolkit"
    }, 
    {
        "url": "https://medium.com/@jdiossantos/pokemon-colors-and-deep-learning-95fb715be46?source=tag_archive---------5----------------", 
        "text": "In the convolution process, the input, which in this case is an image, is scanned by a small window (named local receptive field) that slides through the image. This window has a fixed size and a stride , which is the number of pixels it will slide during each pass. During each scan the scanned portion of the image translates into a node. Thus, after the whole image is scanned, we will have a collection of new nodes which are somehow a \u201cnew\u201d representation of the image. These nodes is our convolution layer. An important detail here is that most of the times, the resulting layer has a greater width than the original image because during each scan the portion being scanned will be \u2018squeezed\u2019 into a smaller area (imagine squeezing a balloon, either the height or length increases while the other decreases).\n\nA convolutional neural network is a type of artificial neural network that is widely used for image recognition and natural language processing. As seen in the previous picture, the nodes and layers of an ANN follow a 2D shape, and are connected in a left-to-right manner (this is called feedforward network). However, in a ConvNet we have several 3D layers that are connected to one another. These layers are created through two main processes: convolutions , and (sometimes optional) pooling .\n\nThe input layer receives the predictors used by the ANN (in our previous example this is the name), the hidden layer process it, and the output layer outputs the class or label (\u201cSpanish\u201d or \u201cEnglish\u201d). Each of these layers are made of nodes which are responsible for performing the logic of the system.\n\nThe previous example was an oversimplified explanation of what supervised learning is, and while there are many different methods, all follow this same principle. An example of supervised learning are the Artificial Neural Networks (ANN). However, what exactly is an ANN? An artificial neural network is an algorithm used to estimate functions, such as the case previously explained. It is called network because it is made of several nodes, organized in three principal layers: the input layer, hidden layer (or layers as it can have more than one) and output layer.\n\nIn this experiment, the image of the Pokemon is the predictor, while its type is the class.\n\nNow that you have learned from these examples (you are trained), I will ask you a question? Is \u201cJoni\u201d a Spanish or an English name? If you learned what I wanted you to learn, your answer should be \u201cSpanish\u201d. Thus, even though I never taught you that Joni is a Spanish name, you made the deduction based on what you had previously learned.\n\nThen, I tell you: \u201cCarol is an English name\u201d, \u201cCathy is an English name\u201d, \u201cCoral is an English name\u201d, and \u201cChris is an English name\u201d. In this case these are English names that starts with \u201cC\u201d.\n\nDo you see any similarities between those names? All of them start with \u2018J\u2019 and have four letters.\n\nIn this data sample, \u201cJuan\u201d is the prediction and \u201cSpanish\u201d is the class. Then I keep training you by saying \u201cJose is a Spanish name\u201d, \u201cJota is a Spanish name\u201d, and \u201cJavi is a Spanish name\u201d.\n\nFor example, suppose you are a computer and I tell you the following:\n\nThe way this is done is by feeding the system with many labeled (predictors+class) examples, where by labeled I mean samples of data with their class attached. Since in this case I am giving the system the label of the data, I am supervising the training process. Once the system is trained, it will know about the data and its patterns, so now you can present to the system a new example (without the class) and it should be able to determine the class on its own.\n\nWhat exactly are we predicting? In a supervised learning setting, a data record (or example) is made of one or several predictors and a class . These predictors are the information the algorithm will learn to being able to predict the classes of new examples.\n\nIn machine learning, in particular the subfield of supervised learning (which is the one related to this experiment), one could say that the purpose or goal is to find a mathematical function, or a way, able to predict new results based on examples previously seen. In other words, a supervised learning algorithm finds patterns in the data.\n\nBefore going into details of the experiment I would like to give a quick overview of what a Convolutional Neural Network is. If you have a background in machine learning/artificial neural networks (ANN) I suggest you skip the next paragraph. Otherwise, if you are a beginner or have absolutely no idea of what an artificial neural network or machine learning is, keep on reading.\n\nThe second process, pooling, is performed after the convolution. Its purpose is similar to a convolution, in the sense that the spatial size of the layer will be reduced once again into a new pooling layer. However, instead of the layer being squeezed, this time it will be aggregated into a small region. The purpose of this is to reduce the amount of nodes in the layer, thus reducing the computational time it takes to fully process the original image.\n\nThe next image shows the whole process. Starting from an 28x28 input, we obtained a convolutional layer of dimension 3x24x24 and a pooling layer of dimension 3x12x12.\n\nFor more information about Convolutional Neural Networks, I recommend the chapter 6 of the book \u201cNeural Networks and Deep Learning\u201d by Michael Nielsen available here for free, and this thesis project Facial Emotion Detection using Deep Learning.\n\nFor the experiment I employed two convolutional and pooling layers. Since now we have an idea of what these things are, I will further explain the layers.\n\nWhat does this means? A window of size 5x5 will scan the image starting from the upper right corner, and move 1 pixel to the right and scan it again. When the scanner reaches the rightmost part of the picture, it will move down 1 pixel and scan from left to right.\n\nResulting layer size: At the end of the scan, the layer will be 32x32x32 (image size, image size and number of filters).\n\nWhat does this means? This means that every 2x2 (window) region of the image will be reduced to one single pixel. After that, the window will move two pixels to the right (and two pixels down if it is at the end of a pixel row), to reduce the newly scanned 2x2 pixels to one. This is will done until the whole picture is reduced.\n\nResulting layer size: At the end of the scan, the layer will be 16x16x32.\n\nWhat does this means? A window of size 5x5 will scan the previous layer starting from the upper left corner, and move 1 pixel to the right and scan it again. When the scanner reaches the rightmost part of the picture, it will move down 1 pixel and scan right to left.\n\nResulting layer size: At the end of the scan, the layer will be 16x16x64 (image size, image size and number of filters).\n\nWhat does this means? This means that every 2x2 (window) chunk of the image will be reduced to one single pixel. After that, the window will move two pixels to the right (and two pixels down if it is at the end of a pixel row), to reduce the newly scanned 2x2 pixels to one. This is will done until the whole picture is reduced.\n\nResulting layer size: At the end of the scan, the layer will be 8x8x64\n\nDuring the experiment I executed the algorithm more than once to test the consistency of the outcome. Unfortunately, I am sad to inform that this was not the case. From about 10 runs performed, around half of them were very good and the other half were very bad, so I was not able to see the consistency I was looking for. Why did this happen? It can be a million reasons. In the next section I will present the result of the worst and best cases, followed by a few comments about some of the things that could have went wrong, and how they could be avoided.\n\nThe most positive outcome of the model showed an accuracy of 96.57% percent out of a sample of 146 records. Meaning that for almost all the Pokemon used to test the system, their type was predicted correctly. This is of course, a good result. The following Pokemon got misclassified.\n\nFrom these five misclassified example I find quite interesting that Dedenne, an orange Pokemon and Lombre a green one got classified as Fire and Grass respectively because most of the Fire and Grass Pokemon are indeed orange/red-ish and green.\n\nFor those of you familiarized with evaluation metrics, I will leave this table which show the precision, recall and F1 score of each class.\n\nIn the worst case, the model reported an accuracy of just 19.12% out of sample of 136; in other words 8 out of every 10 Pokemon was classified wrongly. All the Pokemon belonging to the types: Dark, Dragon, Electric, Fairy\u00a0,Fighting, Ghost, Ground, Ice, Poison, Rock and Steel were misclassified. The class \u201cWater\u201d was the one with the highest accuracy (44.4%).\n\nThese are the evaluation metrics:\n\nNow that we have seen both results, I would like to mention a couple of things about the gap between the metrics.\n\nThe main reason why the final accuracies vary is because of the size of the data. 714 records is a small sample to do this kind of work. Moreover, when working in cases when the number of features is greater than the number of samples (such as this case), the performance of the model is more prone to suffer (note: this does not apply to every model; I recommend to check out the page 12 of the book Applied Predictive Modeling for more information regarding this, a preview of the book is available on Google Books). A low number of samples also causes issues in the split of dataset and the records used for training and testing. As stated at the beginning, 80% of the dataset (~517 records) were used as a training set. While sampling from such as small set, there is always the risk of drawing a small number of samples from every class, or worse not drawing any examples of a class. We encountered these situation, for example, in the bad case only 7 bug Pokemon out of 64 were used as training.\n\nEven if we sample a good amount of records, how can we know for sure that we are getting those that are significant? For example, in a worst case scenario, the algorithm could be trained using cases such as Dedenne, a red colored and Electric Pokemon, which would train the classifier wrong, since the Electric Pokemon are usually yellow.\n\nThis can be avoided. A starting point would have been to change the percentage of training/testing data. While this means a decrease in the number of samples for testing, we would have been getting more records for training the system. Something that could have also helped was to use the same testing set while testing different configurations of the neural network.\n\nThis report shows step by step how I implemented a technique to predict the type of Pokemon based on how it looks, using a Deep Learning prediction model called Convolutional Deep Network (ConvNets). The results obtained in the experiment were not persistent as the accuracy fluctuates from as low as 19% up to as high as 96%. Following this, an explanation stating some of the reasons for this behavior were explained.\n\nA quick overview and introduction to ConvNets was given at the start of the report, however some details such as the concept of fully-connected layer, and dropout were omitted.\n\nThe Python script, and the data, are available here: https://github.com/juandes/PokemonTypesDeepLearning\n\nFinally I would like to point out that this piece of work is for pure personal research and should not be taken so seriously. Comments, suggestions and anything else is welcome. Thanks!\n\nThe images taken from http://neuralnetworksanddeeplearning.com/chap6.html are distributed under a Creative Commons Attribution-NonCommercal 3.0 Unported license. Those from Wikipedia are distributed under a Creative Commons Attribution-ShareAlike 3.0 Unported license.\n\nPok\u00e9mon And All Respective Names are Trademark & \u00a9 of Nintendo 1996\u20132016.", 
        "title": "Pokemon, Colors, and Deep Learning \u2013 Juan De Dios Santos \u2013"
    }, 
    {
        "url": "https://medium.com/udacity/this-week-in-machine-learning-4-november-2016-e341ed0a3e92?source=tag_archive---------6----------------", 
        "text": "This week\u2019s top Machine Learning stories, including everything from gender bias and MRIs to journalism, fashion and transportation!\n\nMachine Learning is one of the most exciting fields in the world. Every week we discover something new, something amazing, something revolutionary. It\u2019s incredible, but it can also be overwhelming. That\u2019s why we created This Week in Machine Learning! Each week we publish a curated list of Machine Learning stories as a resource to help you keep pace with all these exciting developments. New posts will be published here first, and previous posts are archived on the Udacity blog.\n\nWhether you\u2019re currently enrolled in our Machine Learning Nanodegree program, already working in the field, or just pursuing a burgeoning interest in the subject, there will always be something here to inspire you!\n\nA machine learning algorithm quantifies the gender bias present in astronomy research, finding that papers with women as first-authors suffer a 10% drop in citations.\n\nA new deep learning-powered tool from Fraunhofer researchers supports physicians visually examining CT and MRI scans for changes in tumors.\n\nFashion startup Combatant Gentlemen leverages machine learning to increase sales at its physical stores, powered by its proprietary software platform Tower.\n\nThe IBM Institute of Business Value projects that the cognitive systems space will reach a total value of $47 billion by 2020, supported by AI and machine learning.\n\nThe Associated Press plans to use machine learning to support journalists and writers by automatically generating multiple versions of the same story for distribution.\n\nUber prepares to deploy a new version of its app that uses machine learning to predict when and where riders will want to travel, streamlining every part of the process.", 
        "title": "This Week in Machine Learning, 4 November 2016 \u2013 Udacity Inc \u2013"
    }, 
    {
        "url": "https://medium.com/@johnkoetsier/2-minute-explainer-ai-vs-machine-learning-vs-deep-learning-c5150fd1721e?source=tag_archive---------7----------------", 
        "text": "Every post I read that says it is going to explain the relationship between artificial intelligence, machine learning, and deep learning goes way off the deep end into extremely technical terms and huge amounts of detail.\n\nSo I decided to take a crack at a simple 2-minute explainer. Here it is, in one pic:\n\nArtificial intelligence:\n\nReplicating part, all, or more than human intelligence.\n\nMachine learning:\n\nEnabling computers to learn, usually \u201con their own,\u201d often with big datasets.\n\nDeep learning:\n\nBuilding multiple layers of abstractions on datasets to construct higher-level meaning.\n\nSometimes, of course, it\u2019s easier to understand by walking through an example.\n\nAI might be a system for recognizing dogs in pictures. Machine learning would feed the system hundreds of thousands of pictures of dogs. Deep learning would help the system recognize patterns (shapes that form a more complex shape that we call legs, multiple instantiations of legs on a creature, four legs is one signifier that you might be looking at a dog).\n\nThis is certainly simplified and almost certainly oversimplified. But hopefully it helps any who have been wondering.", 
        "title": "2-minute explainer: AI vs. machine learning vs. deep learning"
    }, 
    {
        "url": "https://medium.com/@sanparithmarukatat/%E0%B8%A2%E0%B9%89%E0%B8%B2%E0%B8%A2%E0%B8%84%E0%B9%88%E0%B8%B2%E0%B8%A2-caffe-%E0%B9%84%E0%B8%9B-keras-theano-6ba6b0291d6f?source=tag_archive---------8----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u0e22\u0e49\u0e32\u0e22\u0e04\u0e48\u0e32\u0e22 Caffe \u0e44\u0e1b Keras/Theano \u2013 Sanparith Marukatat \u2013"
    }, 
    {
        "url": "https://medium.com/5-minutes-pour-comprendre/les-machines-peuvent-elles-penser-a321daf97d2f?source=tag_archive---------9----------------", 
        "text": "Les premiers ordinateurs \u00e9tait des machines que peu de personnes auraient pu d\u00e9signer comme intelligentes. De simples calculateurs \u00e0 cartes perfor\u00e9es qui comptaient et classaient d\u00e8s la fin du XIX\u00e8me si\u00e8cle. La deuxi\u00e8me g\u00e9n\u00e9ration ne fut pas plus intelligente. Mais le fait qu\u2019elle soit programmable, qu\u2019on lui fournisse des instructions pas \u00e0 pas pour effectuer des t\u00e2ches, nos fameux algorithmes, \u00e9tait d\u00e9j\u00e0 une nette avanc\u00e9e. L\u2019ENIAC, premier ordinateur \u00e9lectronique utilis\u00e9 dans les ann\u00e9es 50 aux \u00c9tats-Unis, en fut l\u2019illustration. L\u2019intelligence, qu\u2019elle qu\u2019en soit sa nature et sa forme, humaine ou artificielle, suppose cependant la capacit\u00e9 d\u2019apprendre. C\u2019est l\u00e0 que la troisi\u00e8me g\u00e9n\u00e9ration entre en jeu de nos jours, elle sera auto-apprenante ou ne sera pas.\n\nD\u00e9velopper une intelligence artificielle, IA pour les fans d\u2019acronyme, est bien plus complexe qu\u2019il n\u2019y parait. Plus de soixante ans apr\u00e8s la cr\u00e9ation du test d\u2019Alan Turing, via lequel une machine ne pourrait \u00eatre distingu\u00e9e d\u2019un humain dans un questionnaire, il faut bien admettre qu\u2019aucune machine pensante n\u2019a encore vu le jour. Si ce n\u2019est au cin\u00e9ma sous la forme angoissante du Hal de 2001, L\u2019Odyss\u00e9e de l\u2019espace. Lorsque l\u2019intelligence artificielle fut lanc\u00e9e en tant que domaine scientifique en 1956 au Dartmouth College, John McCarthy et Marvin Minsky pensaient que d\u00e9velopper une IA aboutirait \u00e0 copier le fonctionnement du cerveau. Le fruit de ces recherches aboutit au concept du Perceptron qui resta \u00e0 l\u2019\u00e9tat de projet et ne vit jamais le jour. Plut\u00f4t qu\u2019une machine intelligente, la voie qui s\u2019ouvre \u00e0 nous ressemble bien plus \u00e0 une collaboration, un processus que Douglas Engelbart d\u00e9finissait comme une \u00ab\u00a0co\u00e9volution\u00a0\u00bb entre l\u2019homme et la machine.\n\nUn des actes fondateurs des innovations li\u00e9es \u00e0 l\u2019intelligence artificielle fut \u201cla M\u00e8re de toutes les d\u00e9mos\u201d en d\u00e9cembre 1968 par Engelbart. Il s\u2019\u00e9tait choisi pour mission d\u2019inventer des moyens par lesquels l\u2019informatique pourrait augmenter l\u2019intelligence humaine. Fortement influenc\u00e9 par le memex de Vannevar Bush (Life, \u201cComme nous pouvons le penser\u201d), un concept de syst\u00e8me personnel de stockage et de partage d\u2019informations, il chercha \u00e0 mettre au point d\u00e8s 1950 un syst\u00e8me qui permettrait aux humains de g\u00e9rer la complexit\u00e9 auquel il donna le nom d\u2019intelligence augment\u00e9e. \u00c0 partir de 1957, il travailla au SRI (Stanford Research Institut) qui cherchait \u00e0 cr\u00e9er un sysf\u00e8me qui imiterait les r\u00e9seaux de neurones du cerveau humain. Engelbart y poursuivait une autre voie, non pas remplacer la pens\u00e9e humaine par une intelligence artificielle, mais combiner l\u2019esprit intuitif de l\u2019individu avec les capacit\u00e9s de traitement de la machine dans une symbiose entre l\u2019humain et l\u2019ordinateur, id\u00e9e \u00e9nonc\u00e9e en 1960 par Licklider dans son article \u201cLa symbiose homme-ordinateur\u201d. Engelbart d\u00e9crivit sa vision en 1962 dans l\u2019ouvrage \u201cAugmenter l\u2019intellect humain\u201d et y reprit l\u2019id\u00e9e de Bush qu\u2019ordinateur et humain pourraient interagir en temps r\u00e9el au travers d\u2019interfaces simples comprenant des \u00e9crans graphiques, des curseurs et des dispositifs de saisie. Cette conception de l\u2019interaction homme-machine s\u2019est quelque peu infl\u00e9chie depuis quelques ann\u00e9es vers une limitation de l\u2019interface graphique au profit de l\u2019interface vocale jusqu\u2019\u00e0 s\u2019affranchir peut \u00eatre un jour de l\u2019\u00e9cran. On songe en cela \u00e0 la reconnaissance vocale des assistants virtuels tels que Siri d\u2019Apple, Cortana de Microsoft ou celle de Google bien entendu. Mais l\u2019\u00e9cran demeure. Amazon a ouvert une br\u00e8che dans l\u2019appel \u00e0 l\u2019\u00e9cran avec le lancement il y a quelques ann\u00e9es de l\u2019Amazon Echo et son interface vocal. Suivi depuis quelques mois par Google. Mais le chemin est encore long pour faire passer la machine du rang d\u2019assistant \u00e0 celui de compagnon naturel.\n\nDe la notion de symbiose homme-machine d\u00e9coule n\u00e9cessairement celle d\u2019\u00e9cosyst\u00e8me donc d\u2019\u00e9quilibre entre notre part d\u2019humanit\u00e9 et la machine. Sarah Connor peut ouvrir sa porte sans sortir son flingue et Skynet retourner \u00e0 ses calculs. Quoi que\u2026 Google vient d\u2019annoncer que dans le cadre de ses recherche sur le Deep Learning, \u00e0 savoir la mani\u00e8re dont des ordinateurs peuvent \u00e9voluer de fa\u00e7on autonome gr\u00e2ce \u00e0 des algorithmes, son programme de recherche en intelligence artificielle Google Brain vient de franchir un grand cap. Comme nous le pr\u00e9cise le Science Post, \u201cles chercheurs sont parvenus \u00e0 faire communiquer deux IA nomm\u00e9es Alice et Bob entre elles, alors que de son c\u00f4t\u00e9, une troisi\u00e8me IA du nom d\u2019Eve avait pour but d\u2019intercepter leurs communications. Alors qu\u2019ils n\u2019avaient mis aucun algorithme sp\u00e9cifique en place, les chercheurs ont constat\u00e9 que ces deux ordinateurs ont s\u00e9curis\u00e9 leurs communications par le biais d\u2019un chiffrement qu\u2019ils avaient eux-m\u00eames d\u00e9velopp\u00e9. Si la troisi\u00e8me IA est parvenue \u00e0 intercepter certaines communications, la majorit\u00e9 d\u2019entre elles sont rest\u00e9es ind\u00e9chiffrables. Ce n\u2019est l\u00e0 qu\u2019un d\u00e9but, mais", 
        "title": "Les machines peuvent-elles penser ? \u2013 Nicolas Bariteau \u2013"
    }
]