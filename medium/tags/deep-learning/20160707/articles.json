[
    {
        "url": "https://blog.init.ai/teaching-machines-to-talk-my-role-in-innovating-machine-understanding-449e58d88c77?source=tag_archive---------0----------------", 
        "text": "We are participating in a world where the limit to what computers can do is less bounded than ever. Firmly past the AI winters, speculation about where machine learning can take technology is reaching new heights of not only optimism but tangible results. In Alan Turing\u2019s fever dream of 2016, it seems like computers can learn just about anything. We are the Jetsons!\n\nNot quite. But the tools we have for artificial intelligence are powerful, certainly. Big tech players, Google and Apple, have recognized that and are pouring money and effort into beefing up their ML chops. The good news is, you don\u2019t have to be a multi-billion-dollar corporation to put machine learning to use for you. Do you have a vested interest in interacting with users via text in an intuitive, natural way? An impatience for the clicking-around user interfaces of yesteryear? Machine learning will work for you.\n\nDevelopers like me are starting with the tool that ushered in the current AI boom\u200a\u2014\u200athe neural network. From simple to complex, neural networks take their inspiration from the way human neurons pass signal to learn. If you\u2019re curious about how that works or what we\u2019re learning about neural networks at init.ai, my coworker, Robert, has written some great overviews.\n\nSo if this tool is so powerful, what can it do?\n\nNeural networks can pick out dogs, sure, but they can also parse, analyze, classify, and generate natural language. Machine learning aims to create computers that don\u2019t just execute commands when it comes to those tasks but figure out for themselves how input spaces pattern. We give machines the ability to make inferences about language data and adjust based on their errors\u200a\u2014\u200ato understand their input. This is great news for anyone looking to automate seamless text interaction with a user base.\n\nUser interaction with conversational interfaces is moving far beyond the Moviefone clunkiness of years past. You might have heard of chat bots, but with experimentation applied to trusted machine learning techniques, we can far surpass them. Foundations in canonical machine learning strategies create the opportunity for innovation and experimentation in training data collection, data scope, and model engineering.\n\nAt init.ai, it\u2019s my job to use my experience studying natural language and natural language processing to create conversational interfaces that demonstrate what it means for machines to really understand the world around them. I apply my intuitions about language and experience with machine learning models to make better AI for you.", 
        "title": "Teaching machines to talk: My role in innovating machine understanding"
    }, 
    {
        "url": "https://medium.com/@niland/teaching-machines-how-to-feel-the-music-in-a-human-way-part-1-1645305a398e?source=tag_archive---------1----------------", 
        "text": "Building an algorithm able to listen to music and say: \u201cyou might love this track because you played that one before\u201d is more than compiling some mechanical rules. Actually, it\u2019s rather like reproducing a part of your brain!\n\nMusic recommendation has gained increased relevance in recent years. While online music services are still trying to prove the viability of their business models, they have learned that pure access isn\u2019t enough. They need to push their product\u2019s boundaries. Recommendation and discovery features have became their value proposition to captivate more users. On the artist\u2019s side, music recommendation is crucial to gain exposure and meet the audience. Musicians need to cut through the noise while listeners want to discover tracks they love.\n\nHowever, as a scientific topic, music recommender systems have been around for decades. We can distinguish two kinds of technologies exploring this field\u00a0: data & statistic analysis (ex collaborative filtering) and content-based analysis (aka acoustic analysis ).\n\nStatistic techniques like collaborative filtering are great to capture the wisdom of the crowd. These systems recommend items based on similarity measures between users and/or items. The items recommended to a user are those preferred by \u201csimilar\u201d users (ex: Amazon). This kind of techniques can achieve great results when you\u2019ve got many millions of users and tons of data interactions.\n\nA year ago, Spotify launched its Discover Weekly feature mostly based on this approach. This playlist features songs Spotify thinks users will love, based on advanced data analysis and human curation. In less than 10 months, it became one of the company\u2019s most successful products. Every competitor on the market is currently trying to bring their own copy (Apple, Soundcloud, Pandora). This approach works very well to put users in taste clusters and select a pool of customized tracks. It is based on an elegant expansion of the user\u2019s established taste, by linking it with listening data from other users with similar taste profiles. It unveils what music you will like (as Netflix\u2019 recommendation engine), but not how you will like it.\n\nThis approach tends to teach to machines what humans like to listen to, without understanding what is recommended. It is a deaf approach that\u2019s trying to mimic the record dealer\u2019s behavior. It\u2019s not a DJ that builds a listening experience. It doesn\u2019t capture what the soundtrack of your life is. Collaborative filtering also tends to make predictable and familiar recommendations. This favors the rise of a popular artist dictatorship, harmful for the beautiful versatility of music. It also has some well-known issues like the cold-start problem.\n\nThe other objective of recommender system is to pursue the powerful emotional meanings in music. And here, it\u2019s time to put the spotlight on content-based recommendation techniques. Acoustic analysis, signal processing or machine listening are general fields comprising systems that try to teach machines to understand audio like humans do.\n\nAt Niland, we believe that managing the mountains of music data is already a problem of the past. Understanding the music content itself is the missing link to develop more sophisticated options to customize your listening. The new generation\u2019s listening experience lies in the understanding of music and context awareness.\n\nAnalyzing music directly from the audio is far from a new idea. The signal processing discipline started when Pythagoras discovered the foundations of musical tuning. In the second part of the last century, it established itself as the science behind our digital lives. The main challenge of audio analysis for music recommendation lies in the translation between audio features and attributes affecting listening preference: this is referred to as the semantic gap.\n\nThe next part of this blog post will introduce you to the state-of-the-art recommendation systems based on audio.\n\nIn the last (coming soon), we will explain how Niland\u2019s technology compares to others and goes beyond traditional audio analysis thanks to deep learning techniques.", 
        "title": "Teaching machines how to feel the music in a human way \u2014 part 1"
    }
]