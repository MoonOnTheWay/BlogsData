[
    {
        "url": "https://medium.com/@vzkuma/4-steps-for-learning-deep-learning-86f11fcee54?source=tag_archive---------0----------------", 
        "text": "Firstly, if you need some basic information or convincing on why Deep Learning is having a significant impact, check out the following video by Andrew Ng\n\nStart with Andrew Ng\u2019s Class on machine learning https://www.coursera.org/learn/machine-learning. His course provides an introduction to various Machine Learning algorithms are out there and, more importantly, the general procedures/methods for machine learning, including data preprocessing, hyper-parameter tuning, etc.\n\nRead the NIPS 2015 Deep Learning Tutorial by Geoff Hinton, Yoshua Bengio, and Yann LeCun for an introduction at a slightly lower level.\n\nMy learning preference is to watch lecture videos and thankfully there are several excellent courses online. Here are few classes I liked\n\nIf you are more into books, here are some excellent resources. Go ahead and check them out, I won\u2019t judge.\n\nIdentify what you are passionate about and go deeper. The field is vast, so this list is in no way a comprehensive list.\n\nDeep learning has transformed this area. Stanford\u2019s CS231N course by Andrej Karpathy\u2019s course is the best course I have come across; CS231n Convolutional Neural Networks for Visual Recognition. It teaches you the basics and up to covnets, as well as helping you to set up GPU instance in AWS. Also, check out Getting Started in Computer Vision by Mostafa S. Ibrahim\n\nUsed for machine translation, question and answering, sentiment analysis. To master this field, an in-depth understanding of both algorithms and the underlying computational properties of natural languages is needed. CS 224N / Ling 284 by Christopher Manning is a great course to get started. CS224d: Deep Learning for Natural Language Processing, another Stanford class by David Socher (founder of MetaMind)is also an excellent course which goes over all the latest Deep learning research related to NLP. For more details see How do I learn Natural Language Processing?\n\nRecent work in combining attention mechanism in LSTM Recurrent Neural networks with external writable memory has meant some interesting work in building systems that can understand, store and retrieve information in a question & answering style. This research area got its start in Dr. Yann Lecun\u2019s Facebook AI lab at NYU. The original paper is on arxiv: Memory Networks. There\u2019re many research variants, datasets, benchmarks, etc that have stemmed from this work, for example, Metamind\u2019s Dynamic Memory Networks for Natural Language Processing\n\nMade famous by AlphaGo, the Go-playing system that recently defeated the strongest Go players in history. David Silver\u2019s (Google Deepmind) Video Lectures on RL and Professor Rich Stutton\u2019s Book is a great place to start. For a gentle introduction to LSTM see Christopher\u2019s post on Understanding LSTM networks & Andrej Karpathy\u2019s The Unreasonable Effectiveness of Recurrent Neural Networks\n\nWhile discriminatory models try to detect, identify and separate things, they end up looking for features which differentiate and do not understand data at a fundamental level. Apart from the short-term applications, generative models provide the potential to automatically learn natural features; categories or dimensions or something else entirely. Out of the three commonly used generative models\u200a\u2014\u200aGenerative Adversarial Networks (GANs), Variational Autoencoders (VAEs) and Autoregressive models (such as PixelRNN)\u00a0, GAN\u2019s are most popular. To dig deeper read\n\nDoing is key to becoming an expert. Try to build something which interests you and matches your skill level. Here are a few suggestions to get you thinking:\n\nFor more inspiration, take a look at CS231n Winter 2016 & Winter 2015 projects. Also keep an eye on the Kaggle and HackerRank competitions for fun stuff and the opportunities to compete and learn.\n\nHere are some pointers to help you with continuous learning\n\nSee ChristosChristofidis/awesome-deep-learning, a curated list of awesome Deep Learning tutorials, projects and communities for more fun", 
        "title": "4 Steps for Learning Deep Learning \u2013 Vivek Kumar \u2013"
    }, 
    {
        "url": "https://medium.com/huggingface/chatting-with-a-deep-learning-brain-fff7a8656c4b?source=tag_archive---------1----------------", 
        "text": "Last year, Google researchers published a widely-discussed paper titled \u201cA Neural Conversational Model\u201d which triggered a frenzy of reactions and articles such as Wired\u2019s Google Made a Chatbot That Debates the Meaning of Life or Motherboard\u2019s Google\u2019s New Chatbot Taught Itself to Be Creepy.\n\nSince then, conversational AI has been the hottest of topics in academia as well as in large tech companies like Facebook or Microsoft (remember Tay? we do too \ud83d\ude01). Right now, we\u2019re at the stage where PhDs have written really good papers, open source implementations have started to emerge and mature, and startups can start building on this body of research to ship new products. Exciting times! \ud83d\ude0e\n\nA solid implementation of Google\u2019s paper called neuralconvo was started by Marc-Andr\u00e9 Cournoyer last December and is picking up steam fast.\n\nLike Google\u2019s bot, neuralconvo can be trained on the Cornell Movie-Dialogs Corpus, a widely-used NLP dataset made of 220,579 conversational exchanges between 10,292 pairs of movie characters.\n\nHowever, there\u2019s not been an easy way to try out the resulting chatbot online. Selected excerpts from the research papers are hilarious or creepy (or both), but there\u2019s not an easy way to run it and see for yourself.\n\nSo we built a simple interface to chat with the trained model here. The interface will feel familiar, as it looks like messaging products that we all love and use daily: http://neuralconvo.huggingface.co \ud83e\udd13", 
        "title": "Chatting with a Deep learning brain \u2013 HuggingFace \u2013"
    }, 
    {
        "url": "https://blog.cloudsight.ai/deep-learning-image-recognition-using-gpus-in-amazon-ecs-docker-containers-5bdb1956f30e?source=tag_archive---------2----------------", 
        "text": "Scaling up a web service was once a nightmare among DevOps. Provisioning and maintaining \u2018N\u2019 machines, handling failures gracefully, and tracking down trouble gets infinitely more complicated as the service grows. With today\u2019s platforms like Amazon Web Services, we can scale up and down with ease, and tools like Docker make it even easier with containerization.\n\nAt CloudSight, we utilize a lot of GPUs with our Deep Learning Neural Networks, and since Docker is primarily designed to abstract hardware from the container, we experienced a lot of challenges in scaling up our API on Amazon\u2019s Elastic Container Service platform. NVIDIA gives us their convenient nvidia-docker tool, which exposes the GPU to the running Docker container, thereby making it easy for the software inside to utilize it for various tasks. Yet, on Amazon\u2019s Elastic Container Service, their ecs-host agent interacts directly with Docker, preventing us from just swapping the existing docker command line tool binary for the GPU enabled, nvidia-docker one.\n\nAfter digging around a bit, I discovered that the NVIDIA tool does some basic wiring and discovery in order to expose the GPU device and drivers to the running container. However, besides just linking the /dev/nvidia* devices, it also copies the drivers from the host machine into the running container. Sounds simple, but it took a bit of tinkering to get this to work just right outside of the nvidia-docker utility.\n\nThe following example assumes you\u2019ve done some basic work with Amazon\u2019s ECS platform and also Docker. To begin, create and test your CUDA/CUDnn container with the nvidia-docker tool, since the production environment will essentially expose the same environment to the container. This is the part that\u2019s a bit \u201cun-Docker\u201d about using GPUs through CUDA and CUDnn, in that we\u2019re locking down the running environment to specific host requirements. With care, however, consistent results can still be achieved.\n\nThe first step is to get the host machine running with the basic NVIDIA drivers and ECS. If you\u2019ve never set up an ECS instance before, I\u2019d recommend looking here first, or just launch an ECS-optimized AMI. If you\u2019re just using EC2 to manage your machines, you can simply run the following commands to prepare your instances or, if you\u2019re using OpsWorks, you can use the recipe in this cookbook.\n\nOnce your image is built and tested, set up a basic task description as in the example below and register it with ECS.\n\nThe most important parts to the task description above are the volume maps that expose the drivers from /usr and /lib (on the host) into the running container, and finally the privileged flag, which exposes the /dev devices. That way, our setup script can copy the files into the proper place in the container.\n\nIn the running container we use a script (stored in something like /usr/local/bin/startup.sh) that copies these files into the appropriate locations on the host when it starts. If you have some setup processes that run when your container starts, this would be a great place to include the script, otherwise you can set it as the CMD for the container, and add your essential container process with an exec line at the end (used in example).\n\nFinally, a Dockerfile describing the container one might run on the cluster would look like the following:\n\nThat\u2019s it! Build and push the image to your registry and you\u2019ll be running your NVIDIA/CUDA enabled app in an ECS managed Docker container, allowing you to use all the conveniences of the AWS Elastic Container Service like any other Docker container.", 
        "title": "Deep Learning Image Recognition Using GPUs in Amazon ECS Docker Containers"
    }, 
    {
        "url": "https://insights.untapt.com/deep-learning-study-group-session-2-the-backpropagation-algorithm-54124f87c79e?source=tag_archive---------3----------------", 
        "text": "Yesterday evening, untapt hosted the second of a series of workshops on Deep Learning.\n\nAs detailed in our study group\u2019s GitHub repository, the theoretical focus of this session was backpropagation, a principal algorithm for reducing error within artificial neural networks. In particular, econometrician Katya Vasilaky led an illuminating, real-world example-filled discussion on its:\n\nIn addition, we reviewed our initial efforts at developing familiarity with the high-level deep learning library Keras, including playing with some of their simpler networks for classifying MNIST digits and Reuters news items.\n\nFor our next session, we\u2019ll be tackling techniques and Keras-based tools for broadly improving the way neural networks learn.", 
        "title": "Deep Learning Study Group Session #2: The Backpropagation Algorithm"
    }
]