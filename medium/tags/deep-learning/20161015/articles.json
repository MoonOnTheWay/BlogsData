[
    {
        "url": "https://medium.com/@ml.at.berkeley/demo-day-september-2016-252bcd8b395?source=tag_archive---------0----------------", 
        "text": "One of our main goals here at ML@B is to help students understand how to use machine learning in real-world situations. This semester, we\u2019ve teamed up with Github, Grand Rounds, SAP, and Intuit to work on solving some of their problems through machine learning. In addition, we have members working on their own independent research projects with groups such as the International Computer Science Institute.\n\nJust this Friday, we had our very first demo day\u200a\u2014\u200aa day where project members got to present what they\u2019d been up to. Here\u2019s a brief summary of what they had to show:\n\nAsk any programmer, and they\u2019ll tell you the worst part of programming is dealing with bugs. Oftentimes, software engineers will spend hours trying to fix broken pieces of code, only to find that they forgot a semicolon on the 342nd line. That\u2019s where MIT\u2019s Prophet comes in. Prophet is an algorithm to go through lines of code and to automatically fix bugs in the code.\n\nBut if we can fix code, does that mean we can take it one step further and begin to actually generate code? That\u2019s what ML@B wants to know. The Code Synthesis project is focused on implementing Prophet, and then generalizing to try to begin generating code from scratch.\n\nThe thing with machine learning is that, most of the time, data needs to be labeled. For example, if we wanted to classify dogs and cats, we first need a very large dataset of pictures of dogs and cats, each labeled being either a dog or a cat. But labeling data can be tedious. Wouldn\u2019t it be great if we could minimize the amount of labeled data we need?\n\nThat\u2019s what the Active Learning Team is trying to do. They\u2019re working in a field of machine learning called active learning, so called because algorithms here \u201cask\u201d for labeled data only when they absolutely need it. Of course, there\u2019s a trade off. The less labeled data an algorithm asks for, the more inaccurate it gets. The team\u2019s current goal is to find a way to improve accuracy as much as possible for a certain amount of labeled data.\n\nEvery day, millions of files are uploaded onto Github, a coding repository where users share code and collaborate on projects. But how do you tell what language those files are written in?\n\nIdentifying programming languages are surprisingly hard. Symbols used in one language often have different meanings in another. For example, \u2018#\u2019 in python indicates a comment, while in C it indicates a preprocessor command. Even worse, code in one language can actually contain code in different languages, such as HTML, which can contain CSS and/or Javascript.\n\nAt the moment, Github uses a giant checklist to identify unique quirks in the language. For example, if the code contains \u201c:- module\u201d, then it\u2019s probably Mercury. However, most languages simply don\u2019t have enough unique quirks for this method to be accurate enough.\n\nThe current solution the Github team at ML@B came up with is to use a machine learning algorithm called a Na\u00efve Bayes Classifier. To optimize the program, they used Github\u2019s checklist as a guideline for choosing the correct language rather than a hard-and-fast rule. Currently, the team is working on scraping Rosetta Code, a large repository of code in hundred of different programming languages, for data that they can use for training the program and testing its accuracy. Eventually, they will want to see if implementing other models, such as neural networks, can improve accuracy.\n\nGrand Rounds is a health-technology company that matches patients with a specific medical problem with specialists who can solve that problem. Through analyzing a patient\u2019s medical records, as well as a doctor\u2019s history, Grand Rounds will recommend and actually schedule appointments with the most qualified specialists.\n\nML@B is helping Grand Rounds perfect their machine learning algorithms by identifying patterns in the company\u2019s medical datasets. However, one of the biggest challenges is the size of the dataset. In it\u2019s entirety, the dataset is more than 100GB (that\u2019s more or less a billion pages of text) so it\u2019s very difficult to analyze, let alone extract any data. Currently, the team is in the early stages of writing scripts to preprocess data and extract the data in an efficient manner.\n\nOther interesting work that the team is pursuing is analyzing the dataset for medical fraud and writing scripts to compile a dictionary of medical terms.\n\nNeural networks are models based on neuroscience. They are designed to mimic how neurons fire in actual brains. And central to almost all state-of-the-art neural networks is the backpropagation algorithm\u200a\u2014\u200aessentially an algorithm to \u201cupdate\u201d the network and thereby \u201clearn.\u201d However, there\u2019s one gigantic problem. Backpropagation doesn\u2019t actually happen in real life. The problem with backpropagation is that it updates the whole neural network at once. In real life, when an actual brain learns something new, not every neuron in the brain changes simultaneously. Instead, individual neurons change gradually, based only on its neighboring neurons. Imagine that. We have a beautiful, mathematical model based on real biological brains, and an algorithm that couldn\u2019t possibly happen in a real brain. This is the problem OpenBrain wants to solve. How can we develop an algorithm that is both biologically feasible, updates locally, and is still capable of learning? The team has already made lots of progress. They have a working, locally updating learning algorithm. They are now trying to adjust it to learn even faster and more accurately than the traditional backpropagation algorithm.\n\nRecommending music to users is a billion-dollar problem and probably one of the most visible applications of machine learning. Current state-of-the-art music recommendation algorithms suggest songs in two ways. Either by using a song\u2019s metadata\u200a\u2014\u200athat is, it\u2019s composer, record label, release date, and so on\u200a\u2014\u200aor by looking at who else listened to that song, and recommending their playlists.\n\nHowever, no algorithm exists that actually uses the songs themselves to recommend music. Thus, this semester ML@B has a team dedicated a team to developing an algorithm to do just that.\n\nAs with many things in life, analyzing digital audio files is not a walk in the park. Although humans may find it easy to understand music, computers have it much harder. This is what a computer sees as music:\n\nThis graph, called a spectrogram, plots frequency on the vertical axis, time on the horizontal axis, and amplitude as brightness. What the team is currently working on right now is taking a spectrogram, and separating it into each of the voices in a piece of music. The idea is that by getting a separate track for bass, drums, guitar, voice and so on, analyzing a piece of a music will be much easier.\n\nIntuit is a company that helps small businesses and individuals with finances and taxes. The thing with taxes though, is that filling out tax returns requires tons of personal and life details. ML@B is helping the company build a program that analyzes users\u2019 emails for important life events to help Intuit personalize its software. For example, if you got a new job, there would probably be a chain of emails between you and your new employer. By considering factors such as frequency of emails from a person, or the length of email correspondences, the will be able to program build a timeline of life events that are important to the user. This information will then help Intuit provide better financial services.\n\nHowever, much of a person\u2019s emails are either irrelevant or just plain spam. Nobody wants a 50% off deal on coffeemakers in their life\u2019s timeline. One important problem the team is trying to solve is to categorize emails as \u201cimportant\u201d or \u201cnot-important.\u201d The team will be investigating natural language processing and pattern recognition techniques to classify emails.\n\nICSI, the International Computer Science Institute, is working with a team at ML@B to improve neural networks by using complex numbers. This research has direct application in audio recognition. This is because while sound waves can be represented as real numbers in something called the \u201ctime domain,\u201d they can also be represented as complex numbers in the \u201cfrequency domain.\u201d In the frequency domain, complex numbers contain a lot of information obscured in the time domain. For example, complex numbers explicitly encode the frequency and phase of a wave.\n\nProcessing phase in addition to frequency (pitch) and amplitude (loudness) allows the algorithm to extract more information from the audio, such as where it\u2019s coming from, how the sound interacts with itself, and any changes in the sound\u2019s \u2018tone\u2019.\n\nTo accomplish this, the team is using \u2018Complex Caffe\u2019, a novel version of Berkeley\u2019s own deep learning framework \u2018Caffe,\u2019 which is optimized to learn and analyze sound and images. By writing new layer functions that use phase, frequency, and amplitude in their convolutional neural network, the team hopes to be able to identify ambient sounds or changes in the tone of a music more easily.\n\nMachine learning works best when there\u2019s a huge amount of data to work with, and one area that generates a tremendous amount of data is the stock market. Almost a century\u2019s worth of data on hundreds of thousands of tickers is available to analyze. However, with massive amounts of data comes the complexity of having to handle it all efficiently. SAP provides a solution to this problem through the HANA Vora framework. HANA Vora provides a platform to quickly query databases. The project team will use this to increase data throughput and quickly train a machine learning trading strategy.\n\nIn a brief review of literature, the team has discovered that researchers primarily use feedforward neural networks\u200a\u2014\u200athe simplest type of neural network\u200a\u2014\u200ato predict changes in stock values. However, the team believes that by utilizing recent advancements in neural network architectures, they can create more advanced, and more accurate models.\n\nFor example, a first step for the team will be to explore LSTM RNNs, a more complex type of neural network that works well on data taken over a period of time.\n\nMachine learning is great at identifying patterns. Thus, predicting things over time and identifying anomalies are natural extensions of its capabilities. At first glance identifying anomalies and predicting the future seem like two separate things, but they\u2019re actually quite similar. When we flip a coin, we predict that it has a 50% chance of landing on heads and 50% of landing on tails. Similarly, when a computer predicts the future using data gathered over time, it predicts that there is an xx% chance that A will happen, a yy% chance that B will happen, and so on. So when anomalies happen, they are by definition very unlikely to happen again. Therefore, when the computer tries to predict the future, it identifies anomalies as a side effect.\n\nMost algorithms analyze the data by considering overlapping segments of data at a time. For example, they would look at data A, B, C. Then, they move one data segment over and consider B, C, D, and then C, D, E, and so on. This is called a sliding window algorithm. However, the team is trying to make this process more efficient using their recurrent neural network, which can remember previous inputs like the sliding window algorithm. They are also displaying the probabilities of all possibilities (such as 90% chance of increase in stock price and 10% chance of decreasing), allowing the user to have a more complete picture of what will happen in the future.", 
        "title": "Demo Day: September 2016 \u2013 ML@Berkeley \u2013"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-11-f292c6679241?source=tag_archive---------1----------------", 
        "text": "Nathan Benaich gives deep insights on what problems startups can focus on in AI, how we can apply AI technologies in today\u2019s market, how the investment climate is, what challenges are considered by operators and investors other interesting views\n\nTensorFlow implementations of several Neural Style techniques described in various papers along with techniques for semantic segmentation and multiple style transfer.\n\nHow to Use t-SNE Effectively\n\nAlthough extremely useful for visualizing high-dimensional data, t-SNE plots can sometimes be mysterious or misleading. By exploring how it behaves in simple cases, we can learn to use it more effectively. Checkout the beautiful visualizations!\n\nAside from style, which framework helps you get results fast? Which is easier to program? Which produces models which train quickly? Which on balance provides the best blend of performance and productivity? Read on!", 
        "title": "\u2014 Issue #11 \u2013"
    }
]