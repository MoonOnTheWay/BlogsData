[
    {
        "url": "https://gab41.lab41.org/i-need-an-ai-bs-meter-27e94d48c8c1?source=tag_archive---------0----------------", 
        "text": "We talk to a lot of analysts at Lab41. \u200bA recurring theme of these conversations is what they frequently refer to as \u201cresult provenance.\u201d Translation\u200a\u2014\u200a\u201cAre these results any good? Can I trust them? I don\u2019t have a whole lot of time to research them any further, so will these results hold up under scrutiny?\u201d\n\nThese questions aren\u2019t new. The old adage about lies, damn lies, and statistics has been around for just about forever. These questions become a lot harder to answer though with the advent of semi-automated analytic techniques such as Machine and Deep Learning. Few data scientists, let alone your average analyst can answer these questions in a world dominated by such concepts as cold-start, back-propagation, hyper-parameters, sigmoid smoothing functions, batch normalization, yada, yada, yada. The immediate gratification of rapid results are tempered, and complicated by the reality that it is hard to know what data was used to train the network. Familiarity with the given data also may not be sufficient as one of Deep Learning\u2019s greatest attractions is how it discerns discriminating features humans were not aware of. Validating that an autonomous vehicle is adhering to rules of the road is one thing. Knowing that the model chose the right peptide sequence from a set with many thousands of subtle protein differences may be an entirely different matter. These issues can only worsen, as ML/DL techniques become more pervasive. This is not to disparage ML and DL, quite the contrary. The concern here is more about how best to deal with the side effects of increased complexity and opacity that accompany these powerful technologies.\n\nOne way experts try to deal with uncertainty is to publish various types of quality/trust scores with the results. This type of self-attestation has limited value, however. As has been documented repeatedly in the cyber security field, entities vouching for themselves cannot always be trusted to tell the truth. This is how a lot of malware propagates across a network. Ideally such self-attestation should be corroborated by a third party (or multiple).\n\nSo what is an analyst to do?\u200b How do they know if the models they are using are the right ones for the data at hand? How do they detect and compensate for training bias or model over fitting? At Lab41 we have developed frameworks such as Circulo (where we built a tool to help analysts make better decisions on which community detection algorithm to use) and studied recommender systems to see if we can do a better job of matching data and algorithms. Recent research focused on what a neural net is paying attention to at inference time also looks very promising and hopefully the technique will transfer over to other data types.\n\nMy goal here is not to come up with an answer. Rather, what I hope I have done is sufficiently piqued your interest in this aspect of ML and DL that we want to mitigate.\n\nLab41 is a place where experts from the U.S. Intelligence Community (IC), academia, industry, and In-Q-Tel come together to gain a better understanding of how to work with\u200a\u2014\u200aand ultimately use\u200a\u2014\u200abig data.", 
        "title": "I need an AI BS-Meter \u2013"
    }, 
    {
        "url": "https://medium.com/@jrodthoughts/the-case-for-and-against-deep-learning-chips-89e6c6ba5d39?source=tag_archive---------1----------------", 
        "text": "Deep learning has become of the most relevant trends in modern software technology. From a conceptual standpoint, deep learning is a discipline of machine learning that focuses on modeling data using connected graphs with multiple processing layers. In the last few years, deep learning has become a pivotal technology to power uses cases such as image recognition, natural language processing or even powering some of the capabilities of self-driving vehicles. The popularity of deep learning has expanded beyond just software and now the industry is starting to talk about the first generation of hardware with deep learning capabilities: a deep learning chip.\n\nA few months ago, at its I/O Conference, Google announced the design of an application-specific integrated circuit (ASIC) focused on deep learning capabilities and neural nets. Google called this chip the Tensor Processing Unit (TPU) because it underpins TensorFlow, Google\u2019s open source deep learning framework. While Google\u2019s TPU is not the first industry attempt to create a deep learning chip it is certainly the most famous one. However, is a deep learning chip a good idea?\n\nThe answer is related to the current time in the evolution of deep learning technologies. While moving deep learning capabilities of hardware is certainly a great concept, there are some doubts whether this is the right time in the evolution of deep learning technologies to pursue such an endeavor. Looking beyond the hype, we can identify solid argument in favor and against the creation of deep learning chip at this moment in the industry.\n\nThe explosion of deep learning technologies has been possible in part because of the breakthroughs in GPU technologies of the last decade. From an execution standpoint, deep learning is an intrinsically parallel model in which an algorithms are based in the parallel execution of concurrent tasks. Before GPUs, it was almost impossible to efficiently execute complex deep learning algorithms using mainstream hardware. GPUs made possible the execution of highly parallelizable tasks and opened the door to the evolution of deep learning.\n\nA great example of how GPUs have helped deep learning can be found in the the ImageNet Large Scale Visual Recognition Challenge, which has been running since 2010 and which has seen the error rate for image classification drop dramatically as the use of GPUs has risen.\n\nAs mentioned before, nobody doubts that deep learning chips are going to be a trend in the future but the question remains whether this is the right timing in deep learning technologies to make that transition. Some of the arguments in favor of a deep learning chip include:\n\n\u00b7 Everything works faster in silicon: A deep learning chip can really optimize the execution of deep learning algorithms and optimize it for specific devices.\n\n\u00b7 Eventually we want deep learning capabilities in our smartphones: A deep learning chip should be a catalyst to execute deep learning algorithms directly in mobile phones which opens the door to many interesting applications.\n\n\u00b7 Powering the next generation of deep learning hardware: We have to assume that in the future there will be entire hardware infrastructures focused on executing deep learning processes. If that\u2019s the case, a deep learning chip can be a key component of those infrastructures.\n\nThere is a segment of the deep learning community believes deep learning chips are a bit ahead of its time. Some of the most common arguments against this approach include:\n\n\u00b7 Support for unsupervised machine learning: Most of the deep learning models still rely on supervised models that need to be trained. A deep learning chip is better suited for unsupervised models that can learn independently.\n\n\u00b7 Algorithms are changing too fast: The rapid pace of evolution of deep learning technologies poses a challenges for a deep learning chip as the hardware might not be optimized for future algorithms.\n\n\u00b7 We don\u2019t know the winner algorithms yet: Complementing the previous point, the deep learning industry is still in a relatively nascent state in which we still don\u2019t have clear winners that can benefit from hardware level optimizations. From that perspective, the creation of a deep learning chip feels like an optimization for problems that we don\u2019t know if need to be optimized yet\u00a0;)", 
        "title": "The Case For and Against Deep Learning Chips \u2013 Jesus Rodriguez \u2013"
    }, 
    {
        "url": "https://medium.com/@VISEO/intelligenceartificielle-5-questions-%C3%A0-cloderic-mars-cto-de-craft-ai-95b88e6613e3?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "#IntelligenceArtificielle, 5 questions \u00e0 Cloderic Mars \u2014 CTO de craft ai"
    }, 
    {
        "url": "https://medium.com/@vmayoral/reinforcement-learning-in-robotics-d2609702f71b?source=tag_archive---------3----------------", 
        "text": "We humans, have a direct sensori-motor connection to our environment that can perform actions and witness the results of these actions on the environment itself. The idea is commonly known as \u201ccause and effect\u201d, and this undoubtedly is the key to building up knowledge of our environment throughout our lifetime.\n\nA variety of different problems can be solved using Reinforcement Learning. Because RL agents can learn without expert supervision, the type of problems that are best suited to RL are complex problems where there appears to be no obvious or easily programmable solution. Two of the most popular ones are:\n\nA nice and relatively recent (at the time of writing) example of reinforcement learning is presented at DeepMind\u2019s paper Human-level control through deep reinforcement learning.\n\nAs J. Kober, J. Andrew (Drew) Bagnell, and J. Peters point out in Reinforcement Learning in Robotics: A Survey:\n\nwhich tend to be most of the existing ones in the real world. As a rule of thumb, someone can probably think that those tasks that are complex for a human could probably be easily done by a robot while things that we humans do easily and with little effort, tend to be highly complex for a robot. This leads many to think that robots excel at solving complexity but perform poorly on trivial tasks (from a human perspective). In other words:\n\nLet\u2019s take a simple example to illustrate this claim. Image we have a robot manipulator with three joints on top of a table repeatedly performing some task. Traditionally, robotic engineers in charge of the specific task will either design the whole application or use existing tools (produced by the manufacturer) to program such application. Regardless of the tools and complexity level, at some point someone should\u2019ve derived its inverse kinematics accounting for possible errors within each motor/joint, included sensing to create a closed loop that enhances the accuracy of the model, designed the overall control scheme, programmed calibration routines,\u00a0\u2026 all to get a robot that produces deterministic movements in a controlled environment.\n\nBut the fact is that (real) environments are not controllable and robots nowadays pay the bill (someone that has ever produced robot demonstrations should know what i mean).\n\nWhich actually makes a lot of sense!. Think of how we learn about speficic tasks. Let\u2019s take me shooting a basketball:\n\nBellman coined the term \u201cCurse of Dimensionality\u201d in 1957 when he explored optimal control in discrete high-dimensional spaces and faced an exponential explosion of states and actions. As the number of dimensions grows, exponentially more data and computation are needed to cover the complete state-action space.\n\nFor example, let\u2019s take a 7 degree-of-freedom robot arm, a representation of the robot\u2019s state would consist of its joint angles and velocities for each of its seven degrees of freedom as well as the Cartesian position and velocity of end effector. That accounts for 2 \u00d7 (7 + 3) = 20 states and 7-dimensional continuous actions.\n\nIf we assume that each dimension of the state-space is discretized into ten levels, we have 10 states for a one-dimensional state-space. In our robot arm example we\u2019ll have 10^{20} unique states.\n\nAs pointed by J. Kober et al.:\n\nThe curse of real-world samples is covered in Kober\u2019s paper. A summary of some relevant aspects is presented below:\n\nSimulation with accurate models could potentially be used to offset the cost of real-world interaction. Such seems to be the case with robots nowadays using the Gazebo simulator. Supported by the Open Source Robotics Foundation, Gazebo is compatible with different physics engines and has proved to be a relevant tool in every roboticists toolbox.\n\nIn an ideal setting, this approach would allow to learn the behavior in simulation and subsequently transfer it to the real robot. Unfortunately, creating a sufficiently accurate model of the robot and its environment is challenging and often requires very many data samples. As small model errors due to this under-modeling accumulate, the simulated robot can quickly diverge from the real-world system.\n\nSimulation transfer to the real robots is generally classified in two big scenarios:\n\nTaking into account the aforementioned challenges for Robot Reinforcement Learning, one can easily conclude that a naive application of reinforcement learning techniques in robotics is likely to be doomed to failure. In order for robot reinforcement learning to leverage good results the following principles should be taken into account:\n\nThe following sections will summarize each one of these principles which are covered in more detail at J. Kober et al.\u2019s paper.\n\nMuch of the success of reinforcement learning methods has been due to the clever use of approximate representations. The need of such approximations is particularly pronounced in robotics, where table-based representations are rarely scalable.\n\nExperience collected in the real world can be used to learn a forward model (\u00c5str\u00f6m and Wittenmark, 1989) from data. Reduced learning on the real robot is highly desirable as simulations are frequently faster than real-time while safer for both the robot and its environment. In robot reinforcement learning, the learning step on the simulated system is often called mental rehearsal.\n\nThe core issues of mental rehearsal are: simulation biases, stochasticity of the real world, and efficient optimization when sampling from a simulator. These issues are addresses using techniques such as Iterative Learning Control, Value Function Methods with Learned Models or Locally Linear Quadratic Regulators.\n\nSimulation biases Given how hard is to obtain a forward model that is accurate enough to simulate a complex real-world robot system, many robot RL policies learned on simulation perform poorly on the real robot. This is known as simulation bias. It is analogous to over-fitting in supervised learning\u200a\u2014\u200athat is, the algorithm is doing its job well on the model and the training data, respectively, but does not generalize well to the real system or novel data. It has been proved that simulation biases can be addressed by introducing stochastic models or distributions over models even if the system is very close to deterministic.\n\nPrior knowledge can dramatically help guide the learning process. These approaches significantly reduce the search space and, thus, speed up the learning process.", 
        "title": "Reinforcement learning in robotics \u2013 V\u00edctor Mayoral Vilches \u2013"
    }, 
    {
        "url": "https://blog.stamplay.com/introducing-ai-integrations-for-flows-clarifai-textrazor-api-ai-3016f4b985c2?source=tag_archive---------4----------------", 
        "text": "Traditional programming throughout the years has operated under the concept of the developer thinking for the computer 100% of the time. But\u2026what if a program had the ability to learn without being explicitly programmed? (Gasp!)\n\nIn comes Machine learning, the exciting field of programming that focuses on the development of computer programs that can teach themselves to grow and change when exposed to new data. However, coding this \u2018fuzzy\u2019 logic within an app is no easy task and will definitely eat up a lot of your precious time & effort.\n\nNow there is an easy way to add AI to your app with these new API integrations!\n\nWith api.ai you can easily implement natural language processing in a Flow! This gives you the ability to train your program to recognize patterns and behaviors of a user and react intelligently. For example, you could create an app like Spotify where you speak into the mic and request a song to be played and it displays \u201cPlaying {{songname}}\u2026\u201d on your screen or even create your own virtual assistant like Siri! The possibilities are endless!\n\nTextRazor allows you to peel back layers of text/markup and perform text analysis for entities, topics, words, phrases, dependency-trees, relations, entailments, and senses within text.\n\nThe Clarifai integration allows deep learning (a type of artificial intelligence that handles certain kinds of data)\u00a0. Using this API in a Flow, you can submit a url of an image or video and receive detailed data with specific tags related to that image/video! For example, if you submit a picture of a food dish and it will return tags like \u201ccusinine\u201d or \u201cdining\u201d.\n\nNew tutorials centered around the AI APIs will be released on our blog in the next couple of weeks, so stay tuned\u2026\n\nWe look forward to seeing what you create with these new APIs!", 
        "title": "Introducing AI integrations for Flows \u2014 Clarifai, TextRazor, & api.ai"
    }, 
    {
        "url": "https://medium.com/@Brady.Evan.Walker/this-is-why-deep-learning-is-your-future-8d4d14b0cef9?source=tag_archive---------5----------------", 
        "text": "While experts disagree on the timeline, few seem to doubt that deep learning will one day reach a level of sophistication that will change the world\n\nDeep learning powers the next generation of uncanny tech: image recognition auto-tagging you in places you shouldn\u2019t be. AI assistants that order pizza, beatbox, and tell politically correct dad jokes. Badasses kicking butt at chess, Jeopardy, and Go.\n\nBut what is it?\n\nMachine learning is based on the idea that a single algorithm can give insight into a dataset without needing to code for every problem. In other words, the algorithm creates and builds upon its own logic. An everyday example is a spam detector that \u201clearns\u201d common features of junk mail.\n\nThis ability is called feature extraction. The feature could be simple, like color or musical tones. It could be much more complex, like facial recognition.\n\nFor marketers, one of the most promising deep learning technologies available is Ditto Labs. Ditto offers a detection system to identify brands and logos in social media photos. Their software determines salient features of the environment in which brands appear. It even analyzes the sentiment of the people photographed.\n\nTo explain deep learning, let\u2019s first talk about Artificial Neural Networks, a computer software model based loosely on brain structure. Each node or \u201cneuron\u201d is a statistical learning model that estimates functions or features.\n\nIn an Artificial Neural Network, each node tackles simple functions. The nodes are organized into a web and feed into each other to increase processing efficiency and to tackle high volumes of input. In a deep learning model, each node is assigned a very small feature extraction task. The combined effort results in a more comprehensive computer education.\n\nInstead of hand-coding commands for holding a conversation, for instance, Google teaches its AI assistant with reams of screenplay dialogue from which to extract, interpret, and learn.\n\nThere are tons of things we\u2019d like computers to understand that cannot be boiled down to objective functions. But the current uses are increasingly astonishing.\n\nAffectiva, a deep-learning powered image analysis technology, can report back on the emotions on a person\u2019s face in any given photo, which is used primarily for market research. It has other applications, though. Hershey is currently using their technology in store for \u201cSmile Sampler\u201d promotion, a vending machine that doles out confectionary only for sincere smiles. That\u2019s right. The program knows if you\u2019re faking.\n\nSuch technology could, one day, power brick and mortars in more integrated ways.\n\nWould a computerized image analysis program spot you as you walk in the door of your favorite store? Then boom, your profile with purchase history pops up on a customer service tablet?\n\n\u00a0\n\n\u00a0According to an analysis of your social media profile, the sales clerk knows you wear primarily black, knows your size, knows you never wear sandals, and that you generally wear a bolo but forgot to put it on this morning. This may sound creepy but with the right mix of technology and customer-centric attitude, great companies will make this feel like a differentiated, personalized service.\n\nBoston-based Indico offers deep-learning text and image analysis tools. Their text analysis works as you type to analyze sentiment, politically charged language, and other topics and keywords. It can even predict Twitter engagement.\n\nThis is a powerful tool for PR and copywriting. Writers of all stripes (like me) already use programs like Grammarly and Hemingway to supplement their instincts. Indico and similar technologies are the next step in computerized mentoring for writers.\n\nMarketers also have a fresh opportunity to join forces with deep learning platforms, just as Domino\u2019s Pizza was the first company to integrate with Amazon Echo smart home assistant. But it\u2019s not just a matter of being buddies. As TechCrunch reports:\n\nSo even if you don\u2019t have the budget for your own deep learning investments, you can push your innovation budget to expand customer service and accessibility in the digital realm. Otherwise, you\u2019ll be left behind. The digital revolution is still turning.\n\nWhile experts disagree on the timeline, few seem to doubt that deep learning technology will one day reach a level of sophistication that will change the world, powering everything from hyper-intelligent personal assistants to self-driving cars, inhabiting the offices of knowledge workers and running entire factories and warehouses. It\u2019s already changing the face of marketing.", 
        "title": "This is Why Deep Learning is Your Future \u2013 Brady Evan Walker \u2013"
    }
]