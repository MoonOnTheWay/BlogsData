[
    {
        "url": "https://medium.com/jim-fleming/notes-on-hierarchical-multiscale-recurrent-neural-networks-7362532f3b64?source=tag_archive---------0----------------", 
        "text": "State-of-the-art on PTB, Text8 and IAM On-Line Handwriting DB. Tied for SotA on Hutter Wikipedia.\n\nLots of prior work with hierarchy (hierarchical RNN / stacked RNN) and multi-scale (LSTM, clockwork RNN) but they all rely on pre-defined boundaries, pre-defined scales, or soft non-hierarchical boundaries.\n\nUses parameterized binary boundary detectors at each layer. Avoids \u201csoft\u201d gating which leads to \u201ccurse of updating every timestep\u201d.\n\nDiscrete (binary) decisions are difficult to optimize due to non-smooth gradients. Uses straight-through estimator (as an alternative to REINFORCE) to learn discrete variables. The simplest variant uses a step function on the forward pass and a hard sigmoid on backward pass for gradient estimation.\n\nThe slope annealing trick on the hard sigmoid compensates for the biased estimator but minimal improvement from experimental results. Also introduces more hyperparameters.\n\nImplemented as a variant of LSTM (HM-LSTM) with custom operations above. No experimental results for variant with regular RNN (HM-RNN).\n\nLearns useful boundary detectors, visualized in the paper.\n\nLatent representations possibly imperfect, or at least, not human: spaces, tree breaks, some bigrams, some prefix delineation (\u201cdur\u201d: during, duration, durable).\n\nOnly results on character-level compression tasks and handwriting, no explicit NLP tasks, e.g. machine translation, question-answering, or named entity recognition.\n\nThanks to those who attended the reading group session for their discussion of this paper! Lots of good insights from everyone.\n\nFollow me on Twitter for more posts like these. If you\u2019d like help deploying similar models in production, I do consulting.", 
        "title": "Notes on Hierarchical Multiscale Recurrent Neural Networks"
    }, 
    {
        "url": "https://gab41.lab41.org/my-first-convergence-796718bc0104?source=tag_archive---------1----------------", 
        "text": "So, you have decided you no longer want to simply plug-n-play your data into someone else\u2019s deep learning architecture. You want more. You want to become the deity of your own neural net creation. Good for you! These appetites and desires are completely natural and should not be bridled. But, before beginning the transition from apprentice to journeyman I would like to give a word of warning. From scratch neural networks can be finicky and don\u2019t always converge on the first try\u2026 like, ever. To ease your coming of age I offer a few things to check while training your homemade model in order to get it to its first convergence.\n\nAn easy entry into creating your own neural network is to use someone else\u2019s architecture and modify from there. But, if there isn\u2019t a paper that fits your problem, then be prepared to follow the deep learning mantra for making your own architecture: \u201cfollow your heart\u201d. How many layers should I have? Follow your heart. How many neurons should I have in each layer? Follow your heart. Which optimizer should I use? Follow your heart. Should I call in sick today to watch the Saved by the Bell marathon? Absolutely.\n\nFirstly, in order to achieve your first model convergence you should start with a small sample of your data with the goal of using your model to overfit the data. Overfitting is bad, but doing it on a small dataset can quickly tell you if your architecture is set up properly and has enough capacity (amount it can learn).\n\nNext, assuming you know the difference between the types of layers and you have selected the appropriate layers, you should decide how many layers and nodes to include. When deciding this, I have found Geoffrey Hinton\u2019s philosophy to be effective: add layers until you overfit then use a regularizer (L1, L2, dropout, etc.). In other words, follow your heart. If it doesn\u2019t immediately converge please don\u2019t go pouring salt water on your computer. You are brilliant, and the model you constructed is fantastic! Let\u2019s look at a few of the major network knobs you can turn while dance punching out your rage.\n\nAdjusting the learning rate is your fastest path to model convergence. Too large of a rate and you might miss any local minimums. Too small, and it may take days to look like it is starting to converge. Play around with large and small learning rates to start. If your experiment looks like it is converging with a static learning rate, you can then switch to something a little more sophisticated. That can either be with a learning function that has an adaptive learning rate (e.g. RMSprop or Adam), or decaying a large initial learning rate. The Theano code for learning rate decay looks like this:\n\nSometimes convergence doesn\u2019t happen immediately because you initialized your weights either too small or too large. If the weights are initially too small the model has to spend many, many, many iterations building up the magnitude of the weights in order to reach its best capacity. The same is for too large of weights in that the model has to spend a lot of time chopping down their magnitudes. If you are initializing with something like isotropic Gaussian initialization, follow your heart and play with standard deviations (SD) between 0.5 and 0.005. Typically, the larger the network is the smaller the SD should be.\n\nOften, I see gradient shrinking (or less often explosion) as the error is being back-propagated. This is sadness. Ideally, the gradient should be equally (-ish) passed back through all of the layers. On your first run with your newly constructed architecture it is always a good idea to check the norm of the gradients at each layer. The Theano code looks like this:\n\nIf the gradient norm of the last layer is 23.0 and the norm of the first layer is 0.000012, then the weights in your last layer are changing a lot, while your first layer weights hardly move. You can clip the norms to say 1, unless otherwise directed by your heart. Here is code for clipping the gradients:\n\nThe clipping function should be added to your learning function. I really like Adam. The gradient clipping is executed right after the gradients are calculated:\n\nYour first choice of c should be something like 1e10. Most likely you won\u2019t be clipping anything with a choice that high. We do this in order to check the unclipped gradient norms during training. You don\u2019t want to see orders of magnitude difference. I usually end up with c in the range [1, 8].\n\nBatch normalize the heck out of everything. Enough said.\n\nObviously, there are many more things to check or add (residual blocks, regularization, choice of activation function, hyper-parameter searching, etc.) in order to help your first creation converge, but these are the major ones for me. For a reference to other tips and tricks see Yoshua Benjio\u2019s paper\u00a0, this blog summary from DL summer school 2016\u00a0, and chapter 11 of the eBook deeplearningbook.org. May the convergence be with you!", 
        "title": "My First Convergence \u2013"
    }
]