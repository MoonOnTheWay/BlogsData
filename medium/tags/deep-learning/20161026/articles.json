[
    {
        "url": "https://medium.com/@shiyan/caffe-c-helloworld-example-with-memorydata-input-20c692a82a22?source=tag_archive---------0----------------", 
        "text": "So for this blog post, I\u2019d like to provide a simple example of using Caffe with a MemoryData layer as the input to solve the xor problem. Hopefully, this is a better helloworld for those who want to learn Caffe.\n\nNeural Network is known to be able to solve the XORproblem well, even the XOR operation is nonlinear. The XOR operation can be summarized in the following table. So our goal is creating a neural network, with two binary numbers a and b as the inputs and one binary number c as its output, c should be equal to a xor b if the network works as expected.\n\nThere are many materials explaining this problem in more detail. For example:\n\nThe model I\u2019m using is similar to that of the above link:\n\nThe only difference is that my model has biases. This one doesn\u2019t.\n\nA Caffe model or a Caffe neural network is formed by connecting a set of blobs and layers. A blob is a chunk of data. And a layer is an operation applied on a blob (data). A layer itself could have a blob too, which is the weight. So a Caffe model will look like a chain of alternating blobs and layers connecting with each other, because a layer needs blobs as its input and it generates new blobs to become the inputs for the next layer.\n\nOverall, my model looks like this (model.prototxt):\n\nThe first 2 layers are the input layers. I use 2 input layers, because one is for training, the other is for interference. As you can see, they are MemoryData layers, because we want to provide the training and testing data directly from memory as they are generated. The only difference between these two layers is the batch size. For training, I use 64 batch size. For testing, the batch size is 4. Because I only need to test these 4 cases: 0 xor 0, 0 xor 1, 1 xor 0, 1 xor 1.\n\nNotice that this MemoryData doesn\u2019t allow you to specify the size of you labels. It has to be 1. I think this is another shitty thing about Caffe (What\u2019s worse is that they don\u2019t document this. You have to debug Caffe\u2019s source code to find out.). While 1 is indeed the label size for the xor problem, for other problems, you will have to put all your data and labels into the same piece of memory and use a SplitLayer to cut them into data and labels. I may show an example later of how to use SplitLayer.\n\nThe next layer is the first hidden layer, corresponding to the yellow neurons in the above diagram. The filters, according to Caffe document, can randomize the initial neural network, otherwise the initial weights will be zeros. Since the model is a fully connected network, the layer type is InnerProduct here. In my previous experiment with my own Neural network implementation, sigmoid activations gave good results. So here I\u2019m just using sigmoid.\n\nThe next layer is corresponding to the green output neuron in the diagram, which is also an InnerProduct:\n\nBut for activation, I created two layers, one for training and one for testing.\n\nThe layer for training is a SigmoidCrossEntropy layer, where sigmoid is the activation and cross entropy is the cost function. The reason for combining sigmoid and cross entropy together as a single layer is because calculating their derivatives is easier this way. Since for testing, there is no need to produce the loss, using just the sigmoid layer should be fine.\n\nAnd then, we have the solver config file (solver.prototxt):\n\nThe learning rate is 0.02 to start with and decreases by 50% for every 500000 steps. The overall iteration count is 5000000.\n\nNow comes the C++ program.\n\nFirst I generate 400 sets of training data. Each training data has the batch size of 64.\n\nBasically, I just random 2 binary numbers a and b and calculate their xor value c. And I save both a and b saved together as the input data and c saved into a separate array as the label.\n\nAnd then I create a solver parameter object and load solver.prototxt into it:\n\nNext, I create the solver out of the solver parameter:\n\nThen I need to obtain the input MemoryData layer from the solver\u2019s neural network and feed in my training data:\n\nThe reset function of MemoryData allows you to provide pointers to the memory of data and labels. Again, the size of each label can only be 1, whereas the size of data is specified in the model.prototxt file. 25600 is the count of training data. It has to be a multiply of 64, the batch size. 25600 is 400 * 64. Basically we generated 400 training data with batch size of 64.\n\nNow, call this line, the network will be trained:\n\nOnce it is trained, we need to test it. Create another network, with the same model, but pass TEST as the phase. And load the trained weights cached inside XOR_iter_5000000.caffemodel:\n\nSimilar to training, we need to obtain the input MemoryData layer and pass the input to it for testing:\n\nNotice that the name for this input layer is test_inputdata, whereas the input layer for training is inputdata. Remember we created 2 input layers in the model file. These names are corresponding to the 2 layers. Their difference is the batch size.\n\nThen we do the following to calculate the neural network output:\n\nOnce this is done, we need to obtain the result by accessing the output blob:\n\nWe know the output size is 4, and we save the outputs into the result vector.\n\nIn the end we just print the results:\n\nThe complete source code is here:\n\nHere are the test results for this simple neural network:\n\nSo given 0 and 0, the expected output should be 0, and the neural network generated 0.0005, which is very close. Given 0 and 1, while the expected output is 1, the neural network gave 0.99368, which is also good enough.", 
        "title": "Caffe c++ helloworld example with MemoryData input \u2013 Shi Yan \u2013"
    }, 
    {
        "url": "https://medium.com/bigdatarepublic/image-analysis-with-deep-learning-6a160d116810?source=tag_archive---------1----------------", 
        "text": "To put this goal into practice we define the inputs and outputs of the global scene understanding system. We want the input of our system to be an image and a natural language query, which can be anything, as long as it\u2019s about the image. The output of our system should be the answer to the question, taking the context of the image into account. Some example image-question pairs are shown below. \u00ad\u00ad\u00ad\u00ad\n\nSince recent years, deep learning has been increasingly popular in computer vision, text understanding and speech recognition. It is especially in areas that were previously thought to be extremely hard for computers to do that deep learning has risen to deliver state-of-the-art performance. Over the past few years, significant progress has been made in image recognition tasks, but this progress mainly focuses on very specific problems, such as object classification or face detection. Recently research has steered more towards global scene understanding, in which we\u2019re interested in the broader goal of understanding the contents and activities of a complete image scene instead of focusing on a specific area.\n\nIn October 2015, a huge dataset was released containing over 200,000 images and 600,000 questions about these images: the Visual Question Answering dataset. The example shown above was taken from this dataset. Together with the release of the dataset a competition was held aiming to achieve high answering accuracy. Considering its recent developments, deep learning was the main strategy for most contestants.\n\nDeep learning comes across like a buzzword, and in a sense it is, but the buzz around deep learning is not without merit. It\u2019s a solid strategy for high-dimensional data such as video, audio and text. A perfect fit for visual question answering! Let\u2019s see what we need to build a visual question answering system (VQA) and how we can apply deep learning to it.\n\nPut simply, we require three building blocks: image understanding, text understanding and answer generation. Deep learning for image understanding can be performed using convolutional neural networks, while for text understanding recurrent neural networks are often used. Generating answers comes naturally to neural networks, as we\u2019ll soon see.\n\nThe images in the VQA-dataset have an average size of 224 x 224 pixels. The images are in color, so they have three channels: red, green and blue. This means that each image has a total of 224 x 224 x 3 = 150,528 features. This definitely qualifies as high-dimensional data! If we were to use this data in a conventional algorithm we would first have to scale the feature-space down using dimensionality reduction techniques. Essentially, this is what a convolutional neural network does as well. It transforms the huge feature-space into a reduced set of features that still capture the same information as the original feature-space. This is possible since the 150,528 different features are highly correlated (e.g. a picture of the sky will be mainly blue and white; many pixels are similar), meaning that it\u2019s possible to retain most information while reducing the dimensionality of the data. This concept is not new, but with deep neural networks, the dimensionality reduction happens gradually instead of instantaneously. Using multiple layers, the feature-space is reduced in dimensionality by a little bit every time. A welcome side-effect of using convolution to do dimensionality reduction is that the system becomes translation invariant, meaning the position of objects in the image has no effect on the final feature vector. To illustrate a convolutional neural network, see the figure below.\n\nIn this figure, we see the image that we try to understand on the left side. Its dimensionality is 224 x 224 x 3. In the first step, a convolutional layer is applied to the data, in which a sliding window moves across the entire contents of the image, aggregating a set of neighboring pixels into a single new value, dependent on the weights of the network called the kernel (these weights are trained using supervised learning). If we were to do this a single time, the result would be a 224 x 224 feature set. However, we move the sliding window across the image multiple times, each time with a different kernel. In the example above, this is done 64 times, for a total resulting feature set of 224 x 224 x 64 values. But wait, that\u2019s more than we originally had! How does this reduce the amount of features? By using a pooling layer, of course! After a set of convolutional layers (in the example two convolutional layers were used after each other, generating a 224 x 224 x 128 grid), a pooling layer is applied, which downscales the resulting features in the spatial dimensions by a certain factor (often by 2). In the example above, the total feature set is thus reduced to 112 x 112 x 128. As you can see, by applying convolutional layers and pooling layers consecutively, we reduce the spatial dimensions of the image, while increasing the third dimension: the feature maps. At the end of the chain, we end up with only 1 x 1 x 1000 = 1000 features! These can be easily processed using conventional methods (such as logistic regression or a multilayer perceptron). A combination of a specific set of values within these 1000 features can often already indicate certain concepts, such as the presence of a person or a car in the image.\n\nSo we now have a feature vector which contains properties of the image. We need to do the same for the question. To represent a string of text often a bag-of-words approach is used, where a vector is created which represents the counts of each word in the string. However, by using this approach you lose all syntactical information, because the word order information is thrown out. With recurrent neural networks the syntactical information is kept. A recurrent neural network processes the string word-by-word but it contains memory cells to keep track of the representation of words that came before the current one. Depending on the memory, a new word can have a different effect on the vector representation of the string. In the end, you get a vector in the same fashion as with the convolutional neural network.\n\nFor the system to generate an answer it first needs to combine the image representation and the question representation. This can be done by element-wise multiplication of the two vectors, concatenation, or by translating one of the vectors to a matrix and performing a matrix-vector multiplication (as shown by Noh et al.). The latter has the advantage that all image features are multiplied with all question features, enabling interaction between all features. This is not necessarily the case for element-wise multiplication and concatenation. More advanced methods for fusing modalities exist and this is an active area of research. To generate the final answer, usually an activation is computed for every answer present in the training set. Then, a softmax function is applied and the answer with the highest activation is chosen as the final answer.\n\nYes, but performance is still quite low for real-world applications. The winners of the VQA challenge were able to attain an answering accuracy of 66.7%. Although that seems low, the number of possible answers is huge (over 10000). Selecting the correct answer out of this huge set of answers is hard and an accuracy of 66.7% is actually quite high when taking this into account. Still, once every three questions an incorrect answer is given. Performance has to improve significantly before being a valuable addition to natural language query interpreters such as Google Now and Siri.", 
        "title": "Image analysis with Deep Learning \u2013 bigdatarepublic \u2013"
    }
]