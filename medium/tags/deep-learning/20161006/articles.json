[
    {
        "url": "https://medium.com/@wchccy/install-theano-cuda-8-in-ubuntu-16-04-bdb02773e1ea?source=tag_archive---------0----------------", 
        "text": "First we install then by the following steps:\n\nIf the system require you to reboot, just do it and Reboot Ubuntu after finished, and login Ubuntu with x-window. Run the last command again to finish the installing. Then open the X-window by\n\n4. Install Theano according to official tutorials:\n\nFinally check if your Theano can use the GPU by running the example files in the official tutorial.\n\nThis tutorial mainly refer to several resources such as a post in askUbuntu, and a blog.", 
        "title": "Install Theano, Cuda 8 in Ubuntu 16.04 \u2013 \u9648\u6625\u9633 \u2013"
    }, 
    {
        "url": "https://medium.com/@Francesco_AI/what-you-are-too-afraid-to-ask-about-artificial-intelligence-part-iii-technologies-19c68c7af568?source=tag_archive---------1----------------", 
        "text": "What you are too afraid to ask about Artificial Intelligence (Part III): Technologies This is the last of three parts on recent developments in\u00a0AI\n\nAs we explained before, the recent surge of AI and its rapidly becoming a dominant discipline are partially due to the exponential degree of technological progress we faced over the last few years. What it is interesting to point out though is that AI is deeply influencing and shaping the course of technology as well.\n\nFirst of all, the Graphics Processing Units (GPUs) have been adapted from traditional graphical user interface applications to alternative parallel computing operations. NVIDIA is leading this flow and is pioneering the market with the CUDA platform and the recent introduction of Telsa P100 platform (the first GPU designed for hyperscale data center applications). On top of P100, they also created the first full server appliance platform (named DGX-1), which will bring deep learning to an entirely new level. Very recently, they also released the Titan X, which is the biggest GPU ever built (3,584 CUDA cores).\n\nIn general, the most impressive developments we observed are related to chips, especially Neuromorphic Processing Units (NPUs) ideated to emulate the human brain. Specific AI-chips have been created by major incumbents: IBM has released in 2016 the TrueNorth chip, which it is claimed to work very similarly to a mammalian brain. The chip is made of 5.4 billion transistors, and it is able to simulate up 1 million neurons and 256 million neural connections. It is equipped with 4,000 cores that have 256 inputs lines (the axons) and as much output lines (neurons), which send signals only when electrical charges achieve a determined threshold.\n\nThis structure is quite similar to the Neurogrid developed by Stanford, although the academic counterpart is made of 16 different chips instead of the single one proposed by the software colossus.\n\nGoogle, on the other hand, announced the introduction design of an application-specific integrated circuit (ASIC) thought and tuned specifically for neural networks\u200a\u2014\u200athe so-called Tensor Processing Unit (TPU). The TPU optimizes the performance per watt specifically for machine learning problems, and it both powers RankBrain (i.e., Google Search) and DeepMind (i.e., AlphaGO).\n\nIntel is working on similar chips as well, i.e., the Xeon Phi chip series, and the latest release has been named Knights Landing (KNL). KNL has up to 72 cores, and instead of being a GPU, it can be a primary CPU that reduces the need to offload machine learning to co-processors.\n\nEven Qualcomm has invested enormous resources in the Snapdragon 820, and eventually into the deep learning SDK Snapdragon Neural Processing Engine and their Zeroth Machine Intelligence Platform.\n\nThe cost for all those chips is huge (on the order of billions for R&D, and hundred thousand dollars as selling cost), and they are not viable for retail consumers yet but only thought for enterprise applications. The main exception to this major trend is the mass-scale commercial AI chip called Eyeriss, released earlier in 2016 by a group of researchers at MIT. This chip\u200a\u2014\u200amade of 168 processing engines\u200a\u2014\u200ahas been built on a smartphone\u2019s power budget and thus is particularly energy-friendly, but it presents anyway computational limitations.\n\nEven though this is a cost-intensive game, several startups and smaller companies are considerably contributing to the space: Numenta open-source NuPIC, a platform for intelligent computing, to analyze streaming data. Knowm, Inc. has brought memristor chips to the market, which is a device that can change its internal resistance based on electrical signals fed into it (and used as a non-volatile memory). KnuEdge (and its subsidiaries KnuPath) created LambaFabric, which runs on a completely innovative architecture different not only from traditional GPUs but also from TPUs. Nervana Systems released an ASIC with a new high-capacity and high-speed memory technology called High Bandwidth Memory. Horizon Robotics is another company actively working in the space, as well as krtkl, which has produced a new low-cost dual-core ARM processor (FPGA, Wi-Fi, Bluetooth) named Snickerdoodle.\n\nA final note has to be made in favor of Movidius, which introduced a completely new concept, i.e., an all-in-one USB for deep learning. Codenamed Fathom Neural Compute Stick, it contains a chip called Myriad 2, which has been thought in partnership with Google specifically to tackle down any advanced image recognition issue (but it has been used also to power drones and robots of a diverse kind).", 
        "title": "What you are too afraid to ask about Artificial Intelligence (Part III): Technologies"
    }, 
    {
        "url": "https://medium.com/nextgenlearning/learner-ecosystems-5e79af28780c?source=tag_archive---------2----------------", 
        "text": "Imagine each child with a learner ecosystem. This learner ecosystem would consist of a set of opportunities that guide the learner towards competency mastery. The opportunities would be cataloged by competency and available at no cost to all learners. Each learner would be assigned a coach to help guide them through the process. Bricks and mortar schools would still exist but coaches would work with students individual and in groups to support connection towards competencies and/or instruction. The learner ecosystem would expand in scale as the student mastered more and more competencies. With a specific competency set around designing learning, a gradual release of responsibility to the student would happen as the student progressed\u200a\u2014\u200aultimately designing their own path, either selected from the existing ecosystem or added to the existing ecosystem.", 
        "title": "Learner ecosystems \u2013 NextGenLearning \u2013"
    }
]