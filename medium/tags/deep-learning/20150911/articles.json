[
    {
        "url": "https://chronicles.mfglabs.com/rolling-in-the-deep-learning-4302bd5c06da?source=tag_archive---------0----------------", 
        "text": "Deep Learning has been getting a lot of press lately, and is one of the hottest buzz terms in Tech these days. Just check out one of the few recent headlines from Forbes, MIT Tech Review and you will surely see these words pop up at least once.\n\nBut what is this strange concept everybody is talking about\u00a0?\n\nIs it just a fleeting craze, that everybody will forget in a few years (or maybe months)\u00a0?\n\nWhat is all the hype about\u00a0?\n\nWe will try to answer these questions, and a few more, in the following post.\n\nLet us first give a quick definition of Deep Learning:\n\nSome of these terms may be unfamiliar to you. The aim is to give an overview of every one of these concepts, so that by the end you will have a pretty clear picture of what it is all about.\n\nMachine Learning (yet another buzz term) boils down to learning a mapping from an input space to an output space in an automated manner, using available data.\n\nIn the case of what we call supervised learning, the output space is a response that we are trying to predict by using examples of this mapping as training.\n\nExample: Learning whether a person is likely or not to default on their loan\n\nIn the case of what we call unsupervised learning, the output space is often a simpler representation of the input space, that is more structured, we are not given explicit examples of this mapping, so we need to learn it by exploiting the internal structure in the input data.\n\nExample: Clustering customers by their purchase habits\n\nThese examples seem simple to understand and reason about, and the representation of the input space is rather natural.\n\nBut a lot of other tasks fall in the Machine Learning framework, and some are quite complicated.\n\nIn these examples, finding a suitable representation of the input data is more complicated.\n\nWe can divide the Machine Learning process into, essentially, two steps:\n\nThe first step is usually the most time consuming, and the most task specific, that\u2019s where Deep Learning comes in.\n\nBefore we elaborate on this, let us first introduce the family of models on which most Deep Learning techniques are based: Artificial Neural Network.\n\nIn its most basic form, an artificial neuron is a simple computational unit that outputs a weighted sum of its inputs.\n\nIt is somewhat inspired by its biological cousin (the \u201creal\u201d neuron), but it doesn\u2019t replicate at all its inner workings.\n\nData in the form of (input, output) couples is used to adjust the weights of this neuron, to make the output as close as possible to the expected output. Let\u2019s illustrate this on a toy example:\n\nLet\u2019s say we wanted to classify a point in this 2D space, as being part of the red curve, or blue curve. If we use our simple neuron to solve this problem, the task comes down to finding the best separating line between the two curves.\n\nNot bad. The separating line does a pretty good job, but we can clearly see that a line will never separate the two curves perfectly, the problem is just a bit too complex. What if we add an extra layer to our simple network\u00a0? i.e.\n\nLet\u2019s try this and see what happens:\n\nBetter\u00a0! We can now separate our two curves perfectly. What is happening in the extra layer, that made this possible\u00a0?\n\nNo\u00a0.. Not really\u00a0\u2026 We are just bending and twisting our space, so that we can better separate our two curves. If we sneak a peak into what\u2019s happening, to see how the input space is transformed, we can see that our extra layer computes a new representation, that makes the two curves linearly separable:\n\nThis is one of the things Deep Learning is all about.\n\nTo better show how learning multiple levels of new, more useful representations looks like, we will make the (huge) jump to the world of human face detection. If we train a Deep Learning model on a dataset of face images, we can see that we naturally have a hierarchy of representations that is learned (object edges -> parts of faces -> whole faces )\n\nOk, so that\u2019s how a neural network works (basically).\n\nWe can argue that if we make our neural networks deeper, by adding more layers (that\u2019s where the Deep is Deep Learning comes from), we can learn more and more complex representations of our input, and we can potentially capture more complicated structures (as in images or text data).\n\nWait\u00a0\u2026 That\u2019s what took everybody several decades to figure out\u00a0?\n\nWell not exactly. These ideas were here a while ago, but (until 2006) people just couldn\u2019t properly train these Deep Networks.\n\nThis Deep Learning renaissance is due to a mix of three things:\n\nThe reason everyone (or almost everyone) is so excited about Deep Learning, is that it achieved really promising results on a variety of tasks and in a relatively short period of time.\n\nDeep Learning models are now the state-of-the-art methods on this task.\n\nIf we take for instance the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which is a annual competition where the task is classifying several tens of thousands of images into one of 1000 different object categories, this competition was dominated by Deep Learning Models in the last few years.\n\nCheck out http://www.image-net.org/challenges/LSVRC/ for more info.\n\nFacebook\u2019s DeepFace model is closely approaching human level performance on the task of recognizing faces (the dataset used is http://vis-www.cs.umass.edu/lfw/ ), achieving an accuracy 97.25%.\n\nAnother really cool application. This model generates image captions automatically by learning correspondences between language and visual data.\n\nCheck out the results here: http://cs.stanford.edu/people/karpathy/deepimagesent/\n\nToday, each time you speak to your android phone, so you can send a text message, call someone, or do a google a search, there is a large Deep Learning model that is processing your voice and translating it into text.\n\nThe use of Deep Learning models helped reduce the error rate of this system dramatically.\n\nSomething that\u2019s been showing some promise in recent years, is the use of Deep Learning to better understand the human language.\n\nNeural Machine Translation is an interesting application of such models, where using just an aligned corpus on the sentence level (like the Euro Parliament corpus (http://www.statmt.org/europarl/ ), we can learn to translate new sentences, and even align words and groups of words across languages.\n\nThere\u2019s a lot more applications, and it\u2019s hard to give an exhaustive view of every one of them.\n\nOk, so all of this seems great, the results are promising, the models are powerful. But there must be some flaw in these models\u00a0? They are certainly not perfect in every way\u00a0? Are they\u00a0?\n\nWell, the answer is no, they are not perfect, and they do have some flaws, which brings us to our next section.\n\nWe argued earlier, that Deep Learning models have a lot of advantages:\n\nWhich make them the tool of choice for a lot of modern problems.\n\nBut there are some downsides to using these models\n\nResolving all of these problems is still an active area of research, and people are trying to make implementing and tuning these models easier. Some people will even argue that these flaws are inherent to a lot of Machine Learning methods and not only Deep Learning.\n\nThere are also some flaws which are even more severe, and have more profound consequences.\n\nYour model is as good as your data\n\nDeep Learning models learn representations that capture the structure of the data. So if we want to learn useful features, we need to make sure our data is large and diverse enough to represent the real world.\n\nOne particular example comes to mind. (http://googleresearch.blogspot.fr/2015/06/inceptionism-going-deeper-into-neural.html)\n\nIf we ask a Deep Learning model trained on a computer vision task to generate what it thinks most resembles a dumbbell, here\u2019s what we obtain:\n\nThe network thinks a dumbbell always has an arm attached to it, because it was the case in most of the training examples it saw. The model is still far from being \u201cintelligent\u201d, it failed to completely distill the essence of a dumbbell.\n\nSome researchers found that Deep Learning models are quite easily fooled by noise in the input data.\n\nThey took an image of an object that was recognizable by the network and added just a little bit of noise, so little that it was basically identical to the human eye. The network completely mislabelled these examples.\n\nThe reverse was also done, taking images that were, to the human eye, just random pixels on a canvas, yet the network labelled them as objects with near certainty.\n\nSome examples of this:\n\nIt makes us question whether these networks are really robust, and understand the essence of what an object is.\n\nMore details and examples of this can be found here\n\nYou probably noticed that most of these examples are related to Computer Vision, not because it\u2019s the only application domain or the most important (which is not), but because we can \u201csee\u201d what is happening under the hood, and that\u2019s not the case with every task.\n\nThis is a problem with these kinds of model, they are more like black boxes, and it\u2019s kind of hard to see what\u2019s exactly happening inside. Sure, it\u2019s ok if all we care about is accuracy and performance, but that\u2019s rarely the case, sometimes we do need to open the box.\n\nWe tried to give a brief overview of Deep Learning, so you can see and understand what is happening beyond the hype.\n\nThree key points to remember are:\n\nYou should follow us on Twitter: @mfg_labs", 
        "title": "Rolling in the Deep (Learning) \u2013"
    }, 
    {
        "url": "https://medium.com/@NathanBenaich/this-week-s-news-in-artificial-intelligence-and-machine-learning-18d5299602c9?source=tag_archive---------1----------------", 
        "text": "Anything else catch your eye? Prefer this as a weekly email? Drop me a line on @nathanbenaich.\n\nPlayfair Capital is actively seeking out entrepreneurs building companies that change the way we live, work and play using deep learning, machine learning, artificial intelligence, computer vision, speech and NLP.", 
        "title": "This week\u2019s news in artificial intelligence and machine learning"
    }, 
    {
        "url": "https://chronicles.mfglabs.com/rolling-in-the-deep-learning-shoes-startups-martian-nuclear-blast-and-more-in-the-programmable-5d4984172f8a?source=tag_archive---------2----------------", 
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter.", 
        "title": "Rolling in the Deep (Learning), shoes startups, Martian nuclear blast and more in The Programmable\u2026"
    }
]