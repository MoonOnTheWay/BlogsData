[
    {
        "url": "https://medium.com/@sentimentron/faceoff-theano-vs-tensorflow-e25648c31800?source=tag_archive---------0----------------", 
        "text": "Theano needs a fairly standard set of tools and is easily installed via pip. TensorFlow is just as straightforward: just follow the instructions. However, when I first evaluated TensorFlow several months ago, I quickly found out that they\u2019d hard-coded a dependency on the CUDA 7.0 SDK into their build files, making it very difficult to run on system using a later version of CUDA. In the last few weeks, the TensorFlow team have released version 0.8, which corrects that problem. It also now supports Python 3\u200a\u2014\u200awhich I\u2019m gradually switching to\u200a\u2014\u200abut for the rest of this post I\u2019m using 2.7.\n\nOnce installed, I tested it out using an example script from Google, reproduced below:\n\nThese two little scripts actually encapsulate a great deal of the difference between the two frameworks. Each starts off with two identically-generated numpy arrays describing the input, and the target output (it\u2019s mathematically z = 0.1 * x + 0.2 * y). Next, comes model initialisation:\n\nNote that TensorFlow doesn\u2019t require any special treatment of the x and y variables: they just exist in their native forms. Theano meanwhile requires some additional plumbing to say that the variables are symbolic inputs to the function. The syntax defining what the b and W variables look like is also, in my opinion, a little nicer.\n\nNext up is the way of actually learning: gradient descent.\n\nFinally, we get to the body of the training itself:\n\nAgain, the function of these fragments is essentially identical: but TensorFlow\u2019s philosophy of encapsulating the graph execution in the Session object does feel conceptually cleaner than Theano\u2019s.\n\nStep 1 of Dracula is to take two arrays per batch as input: one represents the character indices for each tweet, and the other describes how the characters are laid out into words that we want to classify. Here\u2019s what the embeddings matrix code looks in Theano:\n\nAnd here\u2019s how it looks in TensorFlow:\n\nTensorFlow doesn\u2019t have a flatten operation like Theano does (for those looking\u200a\u2014\u200apass -1 into the reshape function\u2019s shape parameter), but actually that turned out not to be necessary: gather does the equivalent operation by slicing up the embeddings matrix Wemb into slices defined by the matrix of indices x. Whilst TensorFlow\u2019s written documentation is generally excellent\u200a\u2014\u200aif a bit confusing\u200a\u2014\u200ait\u2019s online documentation is especially excellent: whilst I was groping around for gather, TensorFlow gave me really excellent error messages to help me figure out what was going on. Since I first ported some of the code, 0.8 introduces a new function which improves handling of very large embeddings matrices distributed across multiple machines.\n\nAs I port each layer of Dracula over to TensorFlow, I\u2019m adding and revising tests as I go along. I use Python\u2019s unittest module because it\u2019s convenient and built-in, but it\u2019s more interesting to discuss how TensorFlow fits in. Here\u2019s the test for the above:\n\nStraightforward stuff, set up some numpy arrays, declare the embeddings matrix as a tf.Variable (since in the model it\u2019s learned) and declare the input index array as a constant for evaluation. We then run through the session boilerplate, execute the embeddings_layer function to get a Tensor, and then use eval to turn it into a numpy array for assertion.\n\nPer-word averaging is a convolution operation that takes the embeddings or letter-level LSTM output for each character, and turns it into a discrete word embedding that can then be fed to higher recurrent layers. Getting this bit right was the hardest part of creating the first version of the model and underwent several revisions to improve performance. The final version uses a (max word index, max char index, batch size, embedding size)-sized tensor from the embeddings layer and then averages out the second dimension, leaving a 3D (max word index, batch size, embedding size) tensor. The mask is used to communicate to the algorithm which bits of the input are defined (i.e. covered by actual letters in the input), so the input is masked off via element-wise multiplication and then summed to produce the divider. The divider is normalised so that it contains no zeros (if a zero\u2019s on the bottom, then there should be a zero on top too). The per-dimension total is then element-wise divided by this result. Theano\u2019s code is relatively straightforward:\n\nThe dimshuffle calls aren\u2019t actually unnecessary, and I picked this up during the port (they\u2019ll be removed in a future version). Note than in Theano, operations like sum are often attached to the tensors themselves, indicating that the result is derived from the previous step in the computational graph. Operations which bring multiple parts of the compute graph together (like eq) are still defined within Theano\u2019s main namespace. Tensorflow\u2019s version looks very similar, but moves everything into the main namespace.\n\nThe softmax layer is used to produce a probability distribution across the possible labels. Both frameworks assume that the input to softmax is 2D, so we have to use the frameworks\u2019 iteration constructs to scan across each 2D slice of the 3D output of Dracula. Both final softmax layers use dropout during training to try to fight overfitting.\n\nAgain, there\u2019s an issue with undefined input, which should always be given a particular label. The way this is accomplished by Dracula in Theano is to create a default tensor with the probability distribution of the undefined label, and then setting the defined areas to the output of softmax. Theano\u2019s scan function is used to convert the (max word index, batch size, embedding size) input into a (batch size, embedding size) slice.\n\nTensorflow\u2019s implementation is a little more complicated because I opted to use the same implementation to produce both the probability distributions and the raw activations for computing the cost. This is needed because of the cost function I chose, which for some inexplicable reason does softmax internally (\u201cfor efficiency\u201d).\n\nOverall it\u2019s very straightforward: TensorFlow has a built in drop-out function, and map_fn takes care of creating the 2D slices. Because TensorFlow\u2019s indexing capability is limited, it\u2019s not possible to mask undefined regions of the input in TensorFlow like in Theano: but it\u2019s possible to work around it in this case by using the correct cost function.\n\nDracula supports a variable number of LSTM layers at both the word and character levels to perform sequence tagging. This\u200a\u2014\u200amore than anywhere else\u200a\u2014\u200ais where the port differs from Theano. The Theano implementation applies the modifed form of the LSTM presented in the sentiment analysis demo multiple times, and supports both 2D and 3D versions (thanks to broadcasting). The forward and backward iterations are summed to produce the final output.\n\nThe TensorFlow version has only one single activation weight and bias, and the multiple layers are natively supported within the framework. TensorFlow requires two 2D scans (both per-word and per-letter) to compute the output.\n\nThis was surprisingly one of the more difficult pieces to port. In Theano, the cost is computed with via mean squared-error based on the negative log probabilities, and looks like this:\n\nTensorFlow\u2019s got a function which does much the same thing, and (guess what!) it only operates on 2D tensors. So to port this code, it\u2019s necessary to iterate in much the same way, with the caveat that indexing via lists, TensorFlow Variable objects and other symbolic tensors is not supported. As a result, it\u2019s not possible do a map_fn on a tf.range and access the labels and computed probabilities within the scanning function like in Theano, and it\u2019s not possible to pass more than one tensor in either.\n\nThe solution was to combine the tensors representing the reference labels (y) and the unscaled activations (pred_logits) are combined and then unpacked within the map routine. This is a little gross, but it works.\n\nInstead of needing numpy to save Theano\u2019s parameters, TensorFlow has the capability to save tensor values built-in, so that\u2019s great. The only catch is that all variables must be specified in advance (otherwise TensorFlow will complain when model parameters are reloaded), meaning that hyper-parameter optimisations where layers are added or removed have to be considered in advance.\n\nSaving the state of the graph is as easy as creating a tf.train.Saver object with some tf.Variables, including it in the session and then calling it\u2019s save method. The Saver object also supports incremental checkpointing, making it a good fit for environments where training can stop and start (like Amazon EC2 Spot Instances).\n\nTensorFlow has acquired a reputation for being sluggish, and that certainly bears out in my testing. For the initial pre-training stage (with no LSTM layers, just embeddings, averaging and softmax), TensorFlow takes 163 seconds per epoch to train on my single NVidia GTX 980 consumer-class GPU. With letter layer LSTMs, it takes 5503 seconds per epoch. In Theano, the first stage takes 95 seconds per epoch and the second requires 1709, despite doing more work in the LSTM layers. Whilst there is likely some optimisation work to do, a significant amount of this difference is probably down to Theano\u2019s more aggressive compilation and optimisation strategy, which can result in long startup times for the model. Without optimisation enabled, the model starts up as quickly under Theano as it does under TensorFlow. The good news is that TensorFlow\u2019s performance is improving, and for certain workloads it\u2019s quite competitive with other specialized frameworks. But for single-machine workloads, it\u2019s quite hard to recommend it at this stage.\n\nThe big unknown is whether Theano can turn into TensorFlow (e.g. by adding multi-GPU support) faster than TensorFlow can turn into Theano. I wouldn\u2019t count on it: with a fully managed cloud platform already available, its growing adoption inside Google, custom hardware, Google\u2019s significant developer resources and mind-share, and the promise to bring the core engine to other platforms, there\u2019s no chance of TensorFlow going anywhere in the near future. Right now, assuming the compute time involved in training is cheap, there may be small productivity improvements under TensorFlow\u200a\u2014\u200ait has built-in optimizers, more built in neural network capabilities, and tightly integrated check-pointing\u200a\u2014\u200abut its performance and its potential just hasn\u2019t arrived yet. Whilst I\u2019m not officially releasing or supporting the code behind this article (which definitely bears the scars of battle), I\u2019m going to keep it around and update it for TensorFlow 0.9 (hopefully due soon).", 
        "title": "Theano vs Tensorflow \u2013 Richard Townsend \u2013"
    }
]