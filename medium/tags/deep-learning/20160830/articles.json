[
    {
        "url": "https://medium.com/@drive.ai/introducing-drive-ai-and-a-new-vision-for-self-driving-4334ed967c62?source=tag_archive---------0----------------", 
        "text": "Today, we\u2019re officially launching our company: Drive.ai. We\u2019re building artificial intelligence for self-driving vehicles. We think we have the technology to power this new era, and an important vision to make cars that are safe, trustworthy, and even fun.\n\nBut first, a bit on what drives us.\n\nEvery day, drivers and pedestrians are subject to a persisting danger: human error. Nearly 1.3 million people worldwide die in vehicle accidents every year, 90% of which are caused by human error. Over 33,000 of these deaths are in the US alone. That\u2019s equivalent to a large passenger airplane crashing every single day. And someday, not too long from now, those deaths could be preventable. We started Drive.ai because we believe there\u2019s a real opportunity to make our roads, our commutes, and our families safer.\n\nWe\u2019ve now reached a point where every machine that can get smarter\u200a\u2014\u200afrom your phone to your refrigerator\u200a\u2014\u200awill get smarter. Cars are at the forefront of this tectonic shift. We think that making them smarter and thereby safer is not a novel new feature, but an urgent mandate. And recently, an incredible new form of artificial intelligence has been moving through the technology industry, surpassing every benchmark and outperforming every metric. We think this new technology is the key to a future of self-driving vehicles. And today, we\u2019re here to share our vision of this future, where transportation is safer, friendlier, and more reliable.\n\nMore than three years ago, six of our eight founders were graduate students studying at Stanford\u2019s Artificial Intelligence Lab. They were looking at how deep learning\u200a\u2014\u200aknown as the most effective form of artificial intelligence\u200a\u2014\u200acould be applied to the problem of self-driving cars.\n\nDeep learning emerged onto the tech scene just a few years ago, when Google and other companies already had robust autonomous vehicle projects. Unlike other forms of AI, which involve programming many sets of rules, a deep learning algorithm learns more like a human brain. You provide examples, tagged and labeled by an expert, and the system starts to learn for itself\u200a\u2014\u200acreating its own rules.\n\nSay you\u2019re driving down the road, and you see a bicyclist ahead of you. You know it\u2019s a bicyclist\u200a\u2014\u200anot because you count two wheels, and identify all the spokes, and see the handlebars. You know it\u2019s a bicyclist because you\u2019ve seen hundreds of bicyclists before, and you just know what they look like. That\u2019s how our brains work, and it\u2019s why we can drive cars and recognize faces, tasks that the many powerful rule-based algorithms struggle with.\n\nThe situations a car faces on the road are almost infinite. As we\u2019ve been out testing our vehicles, we\u2019ve seen people doing cartwheels and running around the car in circles. One time we saw a dog on a skateboard, wearing a helmet, trailing a person on roller blades. Using a traditional, rule-based approach, you\u2019d have to program thousands of scenarios like these into your algorithm; miss one, and that could be the one that causes an accident.\n\nBut deep learning is different, and that\u2019s what makes it so powerful. It has shown tremendous success understanding nuanced, variable situations. That\u2019s why voice recognition products like Siri and Alexa use deep learning. Google uses it for image and web search, to understand the complexities of pictures and identify specific objects and people. And now, it\u2019s time to apply it to driving.\n\nAt Drive.ai, we\u2019re building the brain of the self-driving car. And in doing so, we aim to fundamentally reimagine the relationships between people, cars, and the world around them.\n\nOur founders put their PhD studies on hold and founded Drive.ai for a simple reason. They knew that deep learning was the right technology to enable self-driving. This isn\u2019t easy stuff; deep learning comprises some of the most complex, challenging algorithms in the engineering world.\n\nWe are at the forefront of the self-driving technology field because of years of experience developing deep learning software. We\u2019re pushing this technology forward\u200a\u2014\u200afrom perception to decision making. Our goal is to empower the car to understand the world holistically, make the best decisions in a given situation, and communicate that decision to the driver and people outside the car. Moreover, we see a better, more accessible approach by taking full advantage of more cost-effective sensors, like cameras. From there, we take on perception, motion planning and controls in a smarter, more efficient way. Our approach to labeling and annotating data makes the learning semi-autonomous, removing the need for impractical manual tagging.\n\nWe think this approach fundamentally sets us apart from the rest of the industry. We\u2019re not building the self-driving car piece by piece; instead, we wanted to reimagine the solution from the ground up.\n\nThe next era of transportation\n\nOur company is built on the belief that deep learning is the safest, most effective way to teach cars to navigate the world. But getting from point A to point B is only part of the battle. Self-driving vehicles will maneuver through a world of nuance, inconsistency, and unpredictability. In other words: the human world.\n\nThink about the four way intersection, when two cars arrive at the same time. Or the family that is hesitant to cross the street in front of a car at a stop sign. Or the highway lane that ends, forcing you to merge in front of someone. All of these situations require human-to-human communication. And there are so many more: bikers, parking garages, emergency stops, kids in the street, careless pedestrians, distracted drivers\u2026\n\nWe humans have a language to navigate these situations: nods, hand waves, eye contact, polite honks\u2026 less polite honks. You simply can\u2019t drive safely in the world without it. But with self-driving cars, everything changes. That\u2019s why we\u2019re focused on building a totally new language: one of trust and transparency. We shouldn\u2019t fear self-driving cars. We should understand them, and feel safe around them, knowing exactly what the car is planning to do.\n\nSelf-driving cars need to build this trust through a user experience that is responsive and clearly communicates the car\u2019s intent. Switching lanes, pulling over, and allowing pedestrians to cross all need to be signaled to riders, other drivers, and pedestrians. You design this experience with deep thought and intention from the start, and then put it out into the world. You can\u2019t slap it on as an afterthought.\n\nAt Drive.ai, human-centered design is baked into everything we do. We don\u2019t want people to just tolerate self-driving vehicles. We want people to love self-driving vehicles. Self driving isn\u2019t just a new feature\u200a\u2014\u200ait\u2019s a once in a generation opportunity to reimagine the fundamental relationships between people, cars, and the world around them. On the way, we can make our roads safer, give people back countless hours of time, empower the disabled and the elderly, transform our urban landscapes, and cut down on CO2 emissions.\n\nAnd just maybe, make cars fun again. That\u2019s what we believe, and today we invite you to come along for the ride.", 
        "title": "Introducing Drive.ai and a New Vision for Self-Driving"
    }, 
    {
        "url": "https://medium.com/the-downlinq/object-detection-in-satellite-imagery-a-low-overhead-approach-part-i-cbd96154a1b7?source=tag_archive---------1----------------", 
        "text": "The promise of detecting and enumerating objects of interest over large areas is one of the primary drivers of interest in satellite imagery analytics. Detecting small objects in cluttered environments over broad swaths is a time consuming task for analysts, particularly given the ongoing growth of imagery data. A number of integrated pipelines using convolutional neural nets (FasterRCNN, YOLO) have proven very successful for detecting objects such as people, cars or bicycles in cell phone pictures. These methods are not optimized to detect small objects in large images, and often perform poorly when applied to such problems. Adapting these methods to the different scales and objects of interest in satellite imagery shows great promise, but is a research area still in relative infancy and one to be explored later. In this paper we discuss a simpler approach using pre-filters and sliding windows paired with histogram of oriented gradient (HOG) based classifiers.\n\nWhile the sliding window approach may be considered a brute force approach, it is also simple to implement and effective in certain problems. Furthermore, all of the work shown here is run on a laptop using open source software [1, 2, 3]; no GPU, high performance computing cluster, or proprietary software licenses required. We apply the sliding window approach to a maritime domain awareness problem and attempt to enumerate ships at anchor as well as in harbor. In later posts we will detail an increasingly sophisticated classification approach and compare it with sliding windows paired with neural network classifiers.\n\nFor demonstration purposes, we choose a 2200 x 2200 pixel subset of the DigitalGlobe (DG) WorldView2 0.5m ground sample distance (GSD) validation image taken near the Panama Canal (see the datasets post, Section 4 for more information). Raw DigitalGlobe cutouts are roughly 50 times larger than this cutout, at ~16,000 x 16,000 pixels. The diverse maritime scene selected contains terrestrial regions, boats in open water, and moored boats. There are 112 boats total, 1/3 of which are in harbor, a handful are anchored together, and the remainder are in open water (Figure 1).\n\nWhen searching for detailed objects the human eye tends to skim over featureless regions and hone in on more complex scenes. Efficient preprocessing techniques should strive to emulate this process by discarding regions with an obvious lack of notable features and flag complex regions for further analysis. Myriad methods exist for such processes: image segmentation, thresholding, masks, corner detection, SURF, gradients, contours, Canny edge detection, etc. We focus on image masks and Canny edge detection as powerful techniques for maritime scenes. Shape files from OpenStreetMap can be used to construct a terrestrial mask, leaving only maritime regions. Open water is relatively featureless when viewed from above, and so Canny edge detection can be used to identify areas of interest (Figure 2). Edge detection can be very useful in sparse scenes, though image noise such as whitecaps or haze (which reduces image contrast) can severely limit the utility of using Canny edges as a pre-filter.\n\nFor boat detection heading classification purposes we use the dataset defined in our datasets post and follow the same approach to building a classifier as that described in the boat heading post. Those posts sought to differentiate boats from background images, and determine the heading of positive results. Our current goal of object localization is slightly different in that we want to draw a bounding box around the entirety of each positive detection. Initial runs of the classifier pipeline defined in the boat heading post yield many partial boats or dock regions classified as positive results. We deem these false positives, feed them back into the classifier defined below, and retrain. This process is called hard negative mining and has the potential to greatly reduce false positives, particularly when applied multiple times. We only run a single iteration of hard negative mining as proof of concept, but in a deployed environment one would likely continually feed in false positives to the system to successively refine the classifier.\n\nThe HOG+PCA+LogReg classifier defined in the boat heading post works well for boat heading classification, yet in initial experiments it struggles to accurately locate boats. The 73-class classifier cannot adequately disentangle docks and partial boats from full boats. To address this issue we train a corpus of 72 binary boat/no-boat random forest (RF) classifiers, one for each 5-degree rotation. Classification accuracy on boat headings (using the exact same training and testing data as the boat heading post) is nearly identical to the HOG+PCA+LogReg classifier, though differentiation between partial boats and full boats is significantly improved. Training the RF classifier set takes 145 seconds on a single CPU (no GPU needed).\n\nRun time is of great importance for object detection over large regions. Extracting the HOG descriptor takes 0.49 milliseconds per window; fitting the multi-class logistic regression classifier takes a mere 0.008 milliseconds per image, while applying the 72 binary random forest classifiers adds 0.33 milliseconds per image. The multi-class classifier obviously is far faster than the 72 binary classifiers, though HOG feature extraction is still the slowest step. These evaluation times are for CPU computation, and should enjoy marked speed improvements if optimized for GPU computation. All told, the HOG+RF classifier takes ~0.8 milliseconds per image to evaluate. For comparison, on the CosmiQ GPU server (CosmiQ is running an NVIDIA DIGITS DevBox with four TitanX GPUs), each evaluation of an AlexNet-based neural network model takes 10 milliseconds.\n\nSliding window approaches are simple in concept, a bounding box of the desired size(s) slides across the test image and at each location applies an image classifier to the current window (see Figures 4, 5).\n\nSatellite imagery is unique from most other imagery types in that pixels remain a static physical size, known as ground sample distance (GSD). For example, if one is searching for a 45m ship in DigitalGlobe data at 0.3m GSD, one need only look for objects of length ~150 pixels. The sliding window box sizes are therefore determined by the scales of objects of interest. Given the diversity of ship lengths in our test image, we select window sizes of [140, 100, 83, 66, 38, 22, 14, 10] meters.\n\nThe total number of sliding windows is a function of the number of scales searched, with smaller scales imparting a much larger computational toll given the larger number of windows. Searching for small objects is computationally expensive, as the number of windows is a quadratic function of window size (one can extract approximately four times as many 10x10 windows as 20x20 windows from a given image).\n\nThe total number of windows also depends heavily on the level of overlap between windows. Effective localization for closely spaced objects requires a high degree of overlap between sliding windows. We find that a step of 1/6 of the bin size works well. The total number of windows to analyze is strongly dependent on the overlap fraction (proportional to the square inverse of the overlap fraction).\n\nThe simple Canny edge detection preprocessing discussed in Section 2 significantly reduces computational requirements. Iterating through the test image and keeping only those images flagged by the edge detection algorithm yields 26,000 bins (a 30x reduction) after a run time of 3.8 seconds on a single CPU. Computing the HOG feature descriptors takes another 12.8 seconds, and classification takes 9.6 seconds, for a total of ~26 seconds for the DG sub-image. This translates to ~15 minutes for each full DigitalGlobe image on a single CPU.\n\nNon-maximum suppression is a method for collapsing overlapping detections into a single detection (see here for an excellent example). The method works by discarding detections with an overlap greater than a specified threshold with previously seen detections. Traditionally, non-max suppression is applied to rectangular bounding boxes oriented in cardinal directions. This method is very fast, and takes a mere 16 milliseconds for our 951 detections.", 
        "title": "Object Detection in Satellite Imagery, a Low Overhead Approach, Part I"
    }, 
    {
        "url": "https://gab41.lab41.org/tell-me-something-i-dont-know-detecting-novelty-and-redundancy-with-natural-language-processing-818124e4013c?source=tag_archive---------2----------------", 
        "text": "Large collections of documents can be hard for humans to sift through on their own. High-quality search can help find what you want, and if you have the resources to annotate documents with tags or taxonomic categories, you can bring some order to an unwieldy corpus. But when the number of potentially relevant documents is high and your time to individually examine them is low, what really matters is finding documents that tell you what you don\u2019t already know.\n\nPythia is Lab41\u2019s exploration of solutions to this problem: can we flag novel and redundant documents on a given topic so that precious human and machine resources can be directed where they are needed?\n\nImagine you have a stream of documents on a particular topic coming into your possession one at a time. For each one that arrives, Pythia is trying to answer the question: does this document tell me something that isn\u2019t in the documents I\u2019ve already seen (novel)? Or does it basically repeat information that has already arrived in the corpus (redundant)?\n\nNIST sponsored a competition on novelty detection in the early 2000s; its \u201cnovelty track\u201d had performers attempting to identify what sentences in a document contained new and relevant information on a given topic, as measured against the assessments of actual humans on the same test. After a couple of hiccups, including annotation snafus that rendered the entire 2002 dataset release almost unusable, the performers in the final 2004 challenge came back with their entries. In tasks focused on retrieving novel sentences, few entries achieved above baseline performance, and none of them by much.\n\nPart of the problem is probably in the data. What does it mean to be novel? Two texts of a given length are almost certain to bear some difference on a semantic level, and even at the single-document level, a sentence is unlikely to convey only given information.\n\nWe decided that novelty has to have some sort of practical meaning related to what people are actually doing with the documents. The Stack Exchange Data Dump was an obvious point of departure. When two questions are getting at exactly the same problem, community members in the Stack Exchange family of sites often flag one as a duplicate of the other, to help route traffic to high-quality questions and the correct answers. Stack Exchange users also often link to related questions that are nevertheless not exactly the same.\n\nThe distinction between related and duplicate questions is a great analogue for novelty vs. redundancy, and we are lucky that the Stack Exchange community has offered up its entire archives under Creative Commons. With some preprocessing and clever querying, we have been able to come up with thousands of query documents and the reference documents they do or do not duplicate.\n\nThis allows us to get to the good part: finding a machine learning algorithm to help us tell the new apart from the old!\n\nTo our minds, the simplest way to compare a document to one or more other documents is to represent the incoming document as a bag-of-words vector and its predecessors also as a bag-of-words vector\u200a\u2014\u200athe sum of all the bags-of-words for the individual predecessor documents. Such a simple featurization scheme leaves a lot of signal on the table, so you may not be surprised to know that, by itself, bag-of-words has performed poorly in our experiments so far.\n\nWe were also intrigued by the notion of \u201ctemporal inverse document frequency,\u201d used for novelty detection in this arXiv paper. The result of the technique is a document-level version of tf-idf, using a discounting function for the age of the document being compared. Unfortunately, the technique yields only a single scalar value for each document, and it hasn\u2019t worked very well on the Stack Exchange corpus. Relative novelty is not really a function of document age in the Stack Exchange data, however, so this is also somewhat unsurprising.\n\nBetween sparse, high-dimensionality bag-of-words vectors and one-dimensional scalars, perhaps there is a way to strike a bargain on both density and dimensionality? Enter \u201cskip-thought\u201d vectors, a sentence encoding technique that tries to represent a phrase in a way that makes the sentences before it and after it easy to predict.\n\nSkip-thought vectors give you a fixed-size representation for each sentence in a document\u200a\u2014\u200ausing those to generate a fixed-size representation of a whole document is a separate proposition. We plan to try:\n\nWe are playing with the code from the paper and have successfully used pre-trained encoders to get vectors for our data. We\u2019re excited about the next step!\n\nMemory networks have been getting a lot of attention in the question answering (QA) literature. The memory part is the new bit\u200a\u2014\u200ayou can expose a network to input, and it learns functions to encode it efficiently and retrieve it when necessary.\n\nIn particular, the dynamic memory networks approach has recently begun to promote itself as a first step towards the NLP Swiss Army knife. Basically, since you can pose any NLP question as a question in natural language, and the answer to such a question itself relies on natural language data, you can show a memory network the text you have a question about and train it to give you the right answer when you ask it.\n\nOur hope is that memory networks, given the correct encoding and retrieval functions, can learn to look at streams of documents and tell us whether arriving documents have fresh information or are just rehashed and warmed-over versions of what we\u2019ve already seen.\n\nWe are actively developing toward a platform to test out these solutions. Our awesome intern Audrey has already published her post on our pipeline for preprocessing, training, and evaluation. And we\u2019re excited by the rapid pace of research in NLP these days\u200a\u2014\u200aif you know of anything you think we should look at, we welcome your thoughts!", 
        "title": "Tell me something I don\u2019t know: Detecting novelty and redundancy with natural language processing"
    }, 
    {
        "url": "https://hojaenblanco.net/machine-learning-medios-de-comunicaci%C3%B3n-y-la-nueva-temporada-e239a1791a8b?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Machine Learning y medios de comunicaci\u00f3n, un terreno f\u00e9rtil pero inexplorado"
    }, 
    {
        "url": "https://medium.com/@dollyfisher/artificial-intelligence-may-soon-be-making-our-decisions-for-us-1122803738ca?source=tag_archive---------4----------------", 
        "text": "Apple was already quite successful when Steve Job\u2019s gut told him that the iPhone would be a major success. But nobody could have predicted just how much the invention of the iPhone would transform the company and the world. That kind of intuition that Jobs displayed is crucial for business owners/leaders whether it\u2019s big or small. Google\u2019s AlphaGo computer displayed similar intuition recently when it defeated the world champion of the complex board game \u201cGo.\u201d The programmers who created AlphaGo fed it every move of 150,000 \u201cGo\u201d matches to give it a feel for the game and the best strategies for achieving victory, and they were just getting started. Next they let the computer play against itself over and over again constantly fine tuning the strategies it had learned. In all, it made billions of self-adjustments before it was ready to play against and defeat the reigning world champion.\n\nThe same intuition that AlphaGo used to learn and eventually master a board game may also soon be used to make business decisions like the ones Steve Jobs had to make when he was the CEO of Apple and pioneering the smart phone.\n\nAs it turns out, humans have a major weakness when it comes to decision-making, they\u2019re prone to making mistakes due to bias. Even though we\u2019re aware, at some level, of our own bias, we\u2019re still incapable of subtracting it from the decision making process no matter how hard we try. AI doesn\u2019t suffer from that problem. Whereas humans have the tendency to discount any information that doesn\u2019t confirm their already-held assumptions or pursue short-term gains despite potential long-term losses, AI can objectively analyze all the relevant data and arrive at the decision that makes the most sense.\n\nBelieve it or not, many corporations are already relying on artificial intelligence machines for some of their decision making. Smaller decisions may be made without any input from humans (this happens on the stock market every day). Larger decisions are still made by humans but are often informed by a vast amount of analysis performed by AI. As AI\u2019s decision-making becomes better, we may see AI taking over more and more of the decision-making responsibilities though it\u2019s safe to say that humans will still need to be involved for many years to come.\n\nArtificial Intelligence News brought to you by artificialbrilliance\u00a0.com\n\nSource: forbes\u00a0.com/sites/sap/2016/08/24/how-to-make-better-bias-free-decisions-with-artificial-intelligence/#32b2b74523c2", 
        "title": "Artificial intelligence may soon be making our decisions for us"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/drive-ai-uses-deep-learning-to-teach-self-driving-cars-and-to-give-them-a-voice-cfa2b19fdb59?source=tag_archive---------5----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Drive.ai Uses Deep Learning to Teach Self-Driving Cars \u2014 And to Give Them a Voice"
    }
]