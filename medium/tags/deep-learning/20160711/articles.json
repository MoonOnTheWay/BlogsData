[
    {
        "url": "https://goberoi.com/comparing-the-top-five-computer-vision-apis-98e3e3d7c647?source=tag_archive---------0----------------", 
        "text": "Over the last six months, Google, Microsoft, and IBM have all announced a suite of \u201cintelligent APIs\u201d that offer various types of image, video, speech, and text recognition. One can, for instance, pass in a photograph of a day at the park, and receive tags that tell you it includes a dog, frisbee, and trees. How good are these out-of-the-box models for general purpose tasks, and can I use them to build interesting new products or features?\n\nIn this post, I compare the image labelling capabilities of five vendors with observations, and suggestions for product builders. I also provide an open-source tool, Cloudy Vision, so you can test your own images and pick the best vendor for you (spoiler: Google was not the clear winner for my corpus). Finally, I share labeling results for 40 example images.\n\nNote that I aim to get a qualitative feel for these APIs, this is not a rigorous accuracy study. I also focus on image labeling, not face detection or OCR [1].\n\nI started by taking a few photos, and running them through the web based testing tools provided by some vendors. My plan was to manually capture results in a spreadsheet. Alas, but this process was so tedious that I found myself fretting over which small set of images I should try out. It was also incomplete because not all vendors have such testing tools (ahem, Google).\n\nI quickly realized that to see side-by-side comparisons of lots of images, my best bet would be to build a little tool. I wrote a Python script to iterate over a directory of images, call the various vendor APIs, cache the responses, and render a web page to see the bits I\u2019m interested in.\n\nThe resulting tool, Cloudy Vision, presents image labeling results from Microsoft, Google, IBM, Clarifai, and Cloud Sight, but is easy to extend to support more vendors (please send me a pull request). If you have a corpus of images and want to explore labeling, this is a good starting point for qualitative assessment, as well as for more rigorous accuracy testing (e.g., compare computed labels with your own training set).\n\nI ran about forty images through five vendors. I recommend viewing that page now to form your own impressions before going further. Here are some of the things I noticed:\n\nI\u2019m impressed with the quality of general classification\u200a\u2014\u200ait\u2019s good enough to get the gist of an image, is fast, and relatively low cost at scale.\n\nThat said, it\u2019s hard to evaluate a solution without a real problem. For instance, what use is general classification (e.g. \u201cthis is food\u201d) if what I need is details for my use-case (e.g., \u201cmy recipe site needs to know if this is a slab of meat, or a vegetable\u201d). With that in mind, let me posit a few hypotheses on when I think you may find these APIs worth your time.\n\nAreas where these APIs may be immediately useful:\n\nSpaces where these APIs won\u2019t be enough:\n\nThanks for reading this far. As a reward, I present you with one of the funnier labeling results: my own profile photo. I know I\u2019m not the tallest guy around, but Google, did you really have to label me as a jockey? Come on\u2026", 
        "title": "Comparing the Top Five Computer Vision APIs \u2013"
    }, 
    {
        "url": "https://gab41.lab41.org/lab41-reading-group-deep-networks-with-stochastic-depth-564321956729?source=tag_archive---------1----------------", 
        "text": "Today\u2019s paper is by Gao Huang, Yu Sun, et al. It introduces a new way to perturb networks during training in order to improve their performance. Before I continue, let me first state that this paper is a real pleasure to read; it is concise and extremely well written. It gives an excellent overview of the motivating problems, previous solutions, and Huang and Sun\u2019s new approach. I highly recommended giving it a read!\n\nThe authors begin by pointing out that deep neural networks have greater expressive power as compared to shallow networks, that is they can learn more details and better separate similar classes of objects. For example, a shallow network might be able to tell cats from dogs, but a deep network has a better chance of learning to tell Husky from a Malamute. However, deep networks are more difficult to train. Huang and Sun list the following issues that appear when training very deep networks:\n\nThere are many solutions to these problems and the authors propose a new one: Stochastic Depth. In essence what stochastic depth does is randomly bypass layers in the network while training. They construct their network of ResBlocks (see image below, and our post for more information) which are a set of convolution layers and a bypass that passes the information from the previous layer through without any change. With stochastic depth, the convolution block is sometimes switched off allowing the information to flow through the layer without being changed, effectively removing the layer from the network. During testing, all layers are left in and the weights are modified by their survival probability. This is very similar to how dropout works, except instead of dropping a single node in a layer the entire layer is dropped!", 
        "title": "Lab41 Reading Group: Deep Networks with Stochastic Depth"
    }
]