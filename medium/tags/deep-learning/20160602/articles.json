[
    {
        "url": "https://medium.com/@asonstrom/beyond-alphago-deep-learning-and-the-neural-network-d2dbc79d8e0f?source=tag_archive---------0----------------", 
        "text": "When you bring up the topic of artificial intelligence to the general public, one of the first things that comes to mind is either Terminator\u2019s Skynet or the robots of the Matrix. But stories of man\u2019s creations rising up against him are not new. In fact, the word Robot was first used in 1920, by Karel Capek in his play, R.U.R. Back then, the word robot didn\u2019t refer to the mechanized automatons we\u2019re used to today, but simply scientifically engineered humanoids.\n\nIn addition to being the originator of the word robot, R.U.R. is also the first story to explore the concept of robots rising up and turning on their creators. And even older than that is Mary Shelley\u2019s classic Frankenstein.\n\nBut instead of being scared away from these stories, we\u2019ve embraced them. In fact, AI experts today are even researching a process called deep learning which intends to bring artificial intelligence out of science fiction and into real life.\n\nDeep learning is a form of machine learning that\u2019s geared towards advancing computer intelligence. In machine learning, a computer is slowly exposed to new data over a period of time and taught to make predictions based on that data. Then, developers go back into the software and make tweaks to the parameters in order to improve prediction quality.\n\nDeep learning, however, uses repeated exposure to multiple data sets, typically images or sound bites, in order to identify key classifiers. The computer then presents a prediction based on those classifiers, and developers provide feedback, either confirming the prediction or providing a correction. It\u2019s a process very similar to human learning, and in fact, deep learning actively attempts to mimic a human mind by using systems referred to as artificial neural networks.\n\nA neural network is a mathematical model which comprises multiple layers. When a data set is sent through a neural network, it travels through each of the layers, known as hidden layers. At each layer it checks against certain parameters, until it reaches out the output layer, where a prediction is made regarding the content of the data. How the neural networks check parameters is defined by something called a learning rule.\n\nWhile there are many different kinds of learning rules, such as the Perceptron learning rule, the Widrow-Hoff learning rule, and the Adaptive Ho-Kashyap (AHK) learning rules, the Delta learning rule is one of the most commonly used rules used by back-propagational neural networks. Now, back-propagational sounds like a really fancy word, but all it really means is that the computers learn by making mistakes, which are then corrected by a developer, much like with human learning.\n\nIf it sounds like this process would take a long time to complete, you would be right. If it\u2019s only one computer going through the process. But many places that are investing in deep learning, like Google, frequently use large amounts of computers at once.\n\nIn June of 2013, Google compiled one of the largest neural networks to date, made up 16,000 computer processors running over a billion connections, and showed it 10 million randomly selected Youtube videos. Without any human input, it was able to positively identify 16% of the content, which doesn\u2019t seem impressive until you realize that this was a 70% increase over previous models. When they recalibrated the system and made the sorting categories more general (down to 1,000 categories from 22,000), the accuracy rate jumped up to 50%.\n\nSo what, you may ask, are we doing with this technology, besides showing it a lot of cat videos off the internet? Well, so far, deep learning processes have increased the accuracy in voice recognition software and there have been great strides in translation technology as well. It\u2019s also been used in robotics to increase dexterity and precision in their movement. Finally, one of the most impressive things we\u2019ve done is teach a computer how to play the Chinese game, Go.\n\nNot only does the computer play this abstract strategy game well, but it has consistently defeated master players time and time again. Of course, this is because the computer, AlphaGo, prioritizes a win condition over gaining points, which is something few humans would consciously chose to do. After all, crushing an opponent by 50 points is far more satisfactory than winning by 2 points, even if the possibility of losing is slightly higher.\n\nAlphaGo\u2019s objectivity is part of the reason why it is able to keep racking up wins against the best of human players, but what does this mean for the future?\n\nWill our Go playing robot overlords decide that our human emotions are the cause of our suffering?\n\nWill they lock us in tubes to serve as batteries for their highly complex and logical society?\n\nOr will they merely declare us obsolete and send mechanical assassins back in time to erase humanity\u2019s resistance leaders from existence in order to crush our inborn need to survive, despite their best efforts to crush us beneath their mighty metallic heels?\n\nOne can only hope that the artificial intelligence of tomorrow is far more benevolent than science fiction likes to depict. It\u2019s hard to deny, though, that the nerds in us are half excited to see what we can make out of our machines, and half preparing to save John Connor.\n\nFor now, though, we\u2019ll stick with losing at board games.", 
        "title": "Beyond AlphaGo: Deep Learning and the Neural Network"
    }
]