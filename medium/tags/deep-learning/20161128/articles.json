[
    {
        "url": "https://medium.com/savants-in-the-levant/the-ai-technology-stack-powering-autonomous-machines-services-147d52862e53?source=tag_archive---------0----------------", 
        "text": "Vertical markets such as the automotive, agriculture and healthcare have long been impervious, if not indifferent to many of the new technology trends sweeping the corporate business world. The absence of early adopters, the dependence on proprietary technology systems, and uncomfortably low product margins made these markets unappealing to cutting edge technology providers that could have transformed them. While each of these vertical markets is distinct, the rise of autonomous machines and services, which perform activities much more efficiently than humans ever could, will disrupt the current way of business in all markets whether on the road, in the field or in the hospital. Autonomous software based on advanced neural networks and deep learning will rapidly find its way into existing business and consumer products, but its potential to catapult traditional vertical industries into the 21 century is what has us most excited. Rather than resist the inevitable, previously conservative market players in non-tech verticals are beginning to engage with startups to be among the first to leverage the promise of autonomous technology.\n\nInstead of looking at just one vertical opportunity, I prefer to focus on the broader trend of autonomous machines and services, which are generally based on the same underlying Artificial Intelligence (AI) technology stack regardless of the vertical market. Although it\u2019s not entirely obvious when we encounter it, AI-powered services already surround us. AI powers many free services we take for granted, whether it is product and music recommendation features, voice command services, facial recognition, real-time translation or virtual digital assistants. But AI\u2019s predictive and learning capabilities are not simply for consumer thrills. It is enlisted by law enforcement to uncover terrorist cells, by banks to detect fraud, and even by Kepler astronomers to decide where we are likely to discover Earth-like exoplanets.\n\nIt\u2019s worth noting that the rise of autonomous machines and services is not only the culmination of decades of research into AI but also the confluence of other established technology trends of the last decade, including big data collection, mobile broadband, cloud computing and SaaS. Irrespective of application, each new autonomous service opportunity has three essential parts: Data Collection & Connectivity, Big Data Cloud, and Deep Learning/AI.\n\nLet\u2019s take a look at each part of this Autonomous Technology Stack.\n\nBecause deep learning demands massive, annotated data sets to train and learn, there is an opportunity, and increasingly a race, to map, measure and monitor everything from physical surroundings and genetic code, to business interactions and machine logs. New and proprietary methods for collecting and building such data sets are essential if autonomous machines and services are to proliferate.\n\nInterestingly, there are already billions of powerful, connected sensors capturing real-time data generated by humans and their electronic possessions, by nature and the environment, and by computers and machines at an ever-accelerating rate. Whether it\u2019s sequencing genomes or 3D scanning of cities or molecules, we are generating incredible amounts of valuable data at a negligible cost. But while the rapid commoditization and proliferation of such sensors may have enabled the autonomous age, there is still a critical need for advanced sensors to feed autonomous machines with ever more precise and detailed data.\n\nBessemer had made three key investments in this area: Oryx, Vayyar and Prospera Technologies. Fully autonomous vehicles will require depth sensors with a range and precision that is an order of magnitude greater than the market can provide today. Oryx uses nano-antennas to build a highly detailed visual representation of a car\u2019s environment as I explained in this recent post.\n\nVayyar developed a radar-on-a-chip, which enables super low-cost 3D imaging for a wide variety of applications, including medical, automotive, manufacturing and industrial. Vayyar\u2019s technology enhances professionals\u2019 ability to collect large amounts of data at a fraction of the traditional price, spreading the ability to the masses. Finally, Prospera Technologies is applying computer vision to the terabyte of picture data it retrieves from greenhouses and crop fields. While it has not developed its own sensor, its ability to capture customer data gives the company a proprietary edge in its autonomous agronomist service.\n\nNeither big data infrastructure nor the public cloud were designed specifically to support autonomous services, but both are now critical elements in the AI technology stack. All of the sensor data described above needs to be synthesized, uploaded and ingested into large databases and distributed computing systems (Hadoop/Spark/Redshift) to identify patterns, trends, and associations. Much of this big data processing is handled by the public cloud vendors, but there are still significant challenges in curating, securing, anonymizing and sharing such large and complex data sets, especially when specific regulatory and industry needs come into play. This explains our investment in Otonomo, which collects data from different automakers to make it useful to service and application providers. And as powerful as the cloud is, a lot of AI processing is still done locally on servers and embedded devices. However, much of this will eventually shift to the cloud.\n\nAs previously mentioned, it is the advent of deep-learning software, which attempts to mimic the numerous layers of neural activity in the brain, that has the most profound implications for AI. These artificial neural networks allow algorithms to find patterns in enormous piles of digital information and make predictions with considerable accuracy and minimal human intervention. Large data sets are used to train deep learning software, enabling it to learn and get smarter, thereby improving over time.\n\nWhile deep learning started in university research labs, it received a considerable boost when AI moved into the web scale data centers of Google, Microsoft, Facebook, IBM, Baidu and Amazon. These companies have unprecedented storage capacity, computing power and ability to fund non-commercial forward-looking projects. More recently, each has attempted to draw developers to their own cloud platforms, similar to what happened with big data infrastructure. As such, they have released deep learning open source libraries, adding to the dozen open source deep learning frameworks developed by academia.\n\nOf course, new applications and services that employ deep learning will still need their own proprietary algorithms tailored to their particular use case. Combined with specialized or proprietary data sets, this is where startups will develop and apply their own IP. For instance, companies that use computer vision to identify and extract detailed data from images have a unique advantage over companies that rely on limited data from a single source. Prospera Technologies is one example of a company utilizing the latest in computer vision and deep learning to deliver an autonomous agronomy solution, which helps farmers and producers extract more yield out of their land with fewer resources.\n\nFinally, not all of the big data infrastructure in the cloud runs on standard CPU servers. The leap forward in autonomous machines and services would not have been possible without re-purposing powerful graphics processors from Nvidia, whose chips can process AI algorithms more than 10x as fast as a typical CPU. This includes cloud-based GPUs and \u201con board\u201d GPUs. However, as AI data sets continue to grow in size and complexity, we see an opportunity on the hardware side. Just as graphics processors emerged to offload heavy video graphics rendering from generic CPUs, we expect to see dedicated AI processors enter the market to facilitate the most computationally intensive AI tasks.", 
        "title": "The AI Technology Stack Powering Autonomous Machines & Services"
    }, 
    {
        "url": "https://medium.com/the-downlinq/you-only-look-twice-multi-scale-object-detection-in-satellite-imagery-with-convolutional-neural-34f72f659588?source=tag_archive---------1----------------", 
        "text": "Rapid detection of objects of vastly different scales over large areas is of great interest in the arena of satellite imagery analytics. In the previous post (6) we implemented a fully convolutional neural network classifier (You Only Look Twice: YOLT) to rapidly localize boats and airplanes in satellite imagery. In this post we detail efforts to extend the YOLT classifier to multiple scales, both at the vehicle level and at infrastructure scales.\n\nRecall that our YOLT training data consists of bounding box delineations of airplanes, boats, and airports.\n\nOur previous post (6) demonstrated the ability to localize boats and airplanes via training a 3-class YOLT model. Expanding the model to four classes and including airports is relatively unsuccessful, however, as we show below.\n\nThere are multiple ways one could address the false positive issue noted in Figure 2. Recall from 6 that for this exploratory work our training set consists of only a few dozen airports and a couple hundred airplanes, far smaller than usual for deep learning models. Increasing this training set size could greatly improve our model, particularly if the background is highly varied. Another option would be to use post-processing to remove any detections at the incorrect scale (e.g.: an airport with a size of 50 meters). Another option is to simply build dual classifiers, one for each relevant scale. We explore this final option below.\n\nWe train a classifier to recognize airports and airstrips using the training data described in 6 of 37 Planet images at ~3m ground sample distance (GSD). These images are augmented by rotations and rescaling in the hue-saturation-value (HSV) space.\n\nOver the entire corpus of airport test images, we achieve an F1 score of 0.87, and each image takes between 4\u201310 seconds to analyze depending on size.\n\nWe are now in a position to combine the vehicle-scale classifier trained in 6 with the infrastructure classifier of Section 3 above. For large validation images, we run the classifier at three different scales: 120m, 200m, and 2500m. The first scale is designed for small boats, while the second scale captures commercial ships and aircraft, and the largest scale is optimized for large infrastructure such as airports. We break the validation image into appropriately sized bins and run each image chip on the appropriate classifier. The myriad results from the many image chips and multiple classifiers are combined into one final image.\n\nOverlapping detections are merged via non-maximal suppression, and all detections above a certain threshold are plotted. The relative abundances of false positives and false negatives is a function of the probability threshold. A higher threshold means that only highly probable detections are plotted, yielding fewer detections and therefore fewer false positives and more false negatives. A lower threshold yields more detections and therefore more false positives and fewer false negatives. We find a detection probability threshold of between 0.3 and 0.4 yields the highest F1 score for our validation images. Figure 5 below shows all detections above a threshold of 0.3.", 
        "title": "You Only Look Twice (Part II) \u2014 Vehicle and Infrastructure Detection in Satellite Imagery"
    }, 
    {
        "url": "https://medium.com/transmission-newsletter/fluid-smoke-simulation-with-deep-learning-evolutionary-algorithms-a-self-driving-car-from-the-77a45ba6f231?source=tag_archive---------2----------------", 
        "text": "Learn more about ALVINN (Autonomous Land Vehicle In a Neural Network), a groundbreaking project from CMU in the late 80s to build an autonomous vehicle powered by a neural network. Read more\u2026\n\nAmazon has adopted MXNet as their deep learning framework of choice for AWS customers, signaling where their focus will be for any deep learning efforts (vs. TensorFlow etc.). Read this note from Werner Vogels (Amazon CTO) for more information\u2026\n\nReal-time simulation of fluid and smoke is a tough problem. This paper proposes using deep learning to obtain both fast and highly realistic simulations. Crazy! Read more and check out the code\u2026\n\nAn exciting paper (with code) on using conditional adversarial networks to predict output images from an input. For example: given a Google Maps tile, output an image of the predicted aerial view. Very cool! Read more\u2026\n\nJapan aims to build a 130 petaflop machine that will be accessible to others (for a fee), that appears to be focussed on deep learning applications. The closest rival is a 93 petaflop machine in China, so this would be a significant leap forward if built. Dubbed ABCI (AI Bridging Cloud Infrastructure), the project will cost around $173 million. Read more\u2026\n\nDeepMind\u2019s WaveNet paper was a revelation to many, and now you can play around with a TensorFlow implementation! Although not a 1:1 replica (the paper has gaps on implementation details), it\u2019s a good way to get started learning. Read more\u2026\n\nAn interesting look at how evolutionary algorithms might evolve (pun intended) to be more significant than machine learning. My take: the best tool for the use-case should win! Read more\u2026\n\nA fantastic and interactive way to learn more about numerical optimization, a core technique used in machine learning. We need more tutorials like this! Try it out\u2026\n\nUber has open sourced deck.gl, a WebGL-powered framework for visually exploring large datasets. Take look at a demo of the mapping integration and prepare to be wow\u2019d! Read more\u2026", 
        "title": "Fluid & Smoke Simulation with Deep Learning, Evolutionary Algorithms, a Self-Driving Car From the\u2026"
    }, 
    {
        "url": "https://medium.com/the-mission/up-to-speed-on-deep-learning-november-update-c93663b59923?source=tag_archive---------3----------------", 
        "text": "Continuing our series of deep learning updates, we pulled together some of the awesome resources that have emerged since our last post on October 17th. In case you missed it, here are our past updates: September part 2 & October part 1, September part 1, August part 2, August part 1, July part 2, July part 1, June, and the original set of 20+ resources we outlined in April. As always, this list is not comprehensive, so let us know if there\u2019s something we should add, or if you\u2019re interested in discussing this area further.\n\nAdvances in Neural Information Processing Systems 29 (NIPS 2016) pre-proceedings by NIPS. The 2016 NIPS Conference, the leading annual machine learning conference, is next week, December 5 to 10, in Barcelona. Let us know if you\u2019ll be there. The list of presenting papers is available here ahead of time.\n\nWhat Artificial Intelligence Can and Can\u2019t Do Right Now by Andrew Ng of Baidu. Overview of what machine learning is actually capable of and where most progress has been made in AI so far\u200a\u2014\u200asupervised learning. And a helpful description of its current potential: If a typical person can do a mental task with less than one second of thought, we can probably automate it using AI either now or in the near future.\n\nThe Next Frontier in AI: Unsupervised Learning by Yann LeCun, Director of AI Research at Facebook. Video discussion of the power of unsupervised deep learning, current challenges, and approaches.\n\nKeras Tutorial: The Ultimate Beginner\u2019s Guide to Deep Learning in Python by EliteDataScience. Learn how to build a convolutional neural network in Python.\n\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville. A new and much-anticipated book by experts in deep learning, providing a broad, technical overview of the field and where it\u2019s headed. Available digitally or hardcover. Elon Musk stated: \u201cWritten by three experts in the field, Deep Learning is the only comprehensive book on the subject.\u201d\n\nAnnouncing GPUs for Google Cloud Platform by John Barrus of Google Cloud Platform. Google announces that in early 2017, Google Cloud Platform will offer GPUs worldwide for Google Compute Engine and Google Cloud Machine Learning users. They will offer AMD FirePro S9300 x2 and NVIDIA Tesla P100 and K80 GPUs.\n\nThe Montreal Institute for Learning Algorithms (MILA) announces funding by Google and others, and leadership by professor Yoshua Bengio, of the University of Montreal and an expert in deep learning. The lab is dedicated to deep learning research and advancement.\n\nReinforcement learning with unsupervised auxiliary tasks by Google DeepMind. DeepMind augments standard deep reinforcement learning methods with two additional tasks for agents to perform during training: The first task involves the agent learning how to control the pixels on the screen; in the second task the agent is trained to predict the onset of immediate rewards from a short historical context. This improves the learning speed and final performance of agents. Original paper here.\n\nNeural Architecture Search with Reinforcement Learning by Google researchers Barret Zoph and Quoc V. Le. Deep learning is engendering a paradigm shift from feature design to architecture design. Neural networks are still hard to design. This research uses reinforcement learning to generate a neural network architecture.\n\nLip Reading Sentences in the Wild by Google DeepMind and University of Oxford. Researchers develop an AI system that performs lip-reading tasks better than professionals, with 46.8% accuracy (compared to professionals\u2019 12.4%). The AI was trained on 5000 hours of video.\n\nPeeking into the neural network architecture used for Google\u2019s Neural Machine Translation by Stephen Merity of Salesforce. A deeper explanation of Google\u2019s architecture for its end-to-end learning approach to automated machine translation. Original paper here.", 
        "title": "Up to Speed on Deep Learning: November Update \u2013 The Mission \u2013"
    }, 
    {
        "url": "https://machinelearnings.co/machine-learnings-19-dirty-minded-ai-developers-b3fc678f97bd?source=tag_archive---------4----------------", 
        "text": "#Awesome \n\n\u201cIn the future, a shopper could walk into Uniqlo and use their iPhone to ask Siri whether a certain down jacket was in stock in light blue\u200a\u2014\u200arather than talking to a human store assistant. As a result, AI has the potential to reduce the need for human shop assistants, reducing overall labour costs for retailers.\u201d\u200a\u2014\u200aLeslie Hook and Lindsay Whipp, Reporters for The Financial Times (behind a paywall)\n\n#Not Awesome\n\n\u201cWhat happens when you make robots that are smart, independent thinkers\u200a\u2014\u200aand then try to limit their autonomy? What happens if we were to launch a robot on the battlefield and all of the sudden it took a more partial liking to the enemy than it did to its human sponsor?\u201d\u200a\u2014\u200aPeter Kalis, Chairman of K&L Gates\n\n1/ A naughty developer creates a machine learning video editor that identifies \u2018specific acts\u2019 within porn scenes. Learn More on Gizmodo > (NSFW)\n\n2/ Google, Facebook, Amazon, and Microsoft are teaching AI classes to employees so the Tech world doesn\u2019t leave them behind. Learn More on WIRED >\n\n3/ Fake news is a major problem for big companies (and society too), but AI is nowhere near ready to solve the problem. Learn More on Quartz >\n\n4/ Large Tech companies are draining top universities of AI professors, and screwing students along the way. Learn More on The Wall Street Journal >\n\n5/ A resource that tells you what business problems can be solved with machine learning methods now exists. Learn More on Harvard Business Review >\n\n6/ Machine learning is now better than your nosiest friend at reading lips. Learn More on MIT Technology Review >\n\n7/ Stitch Fix, an online clothing retailer that expects to haul in $375 million in revenue in 2016, uses machine learning to develop new brand styles. Learn More on Harvard Business Review >\n\nGoogle just announced updates to 9 of their tools that now use machine learning to save you time.\n\nNone of them are sexy, but you will use most of them heavily. My favorite is a Google Calendar feature that suggests times to meet based on your preferences and potential conflicts with participants. \u2709\ufe0f\u2709\ufe0f\ud83d\ude11\ud83e\udd16\ud83d\udcc5\ud83d\ude01\n\n\u201cAI isn\u2019t coming for our jobs, it\u2019s coming for our planet and will one day colonize the galaxy.\u201d\n\nA few weeks ago, an \u2018AI guru\u2019 made that wild claim. Whether or not you agree, the argument that exploded on reddit is worth reading. \ud83e\udd16\ud83c\udf0e\ud83d\udc7d\u270c\ufe0f\ud83c\udf0c\n\nMachine Learning and AI are having a huge impact on our lives. Our mission is to create space for discussion and learning that helps you become prepared to handle these changes as they happen.", 
        "title": "Dirty minded AI developers \u2014 #19 \u2013"
    }, 
    {
        "url": "https://chatbotslife.com/resources-for-deep-learning-6bc24cc1ab2d?source=tag_archive---------5----------------", 
        "text": "From my book \u201cBusiness Application of Deep Learning\u201d (in preparation), a few very good resources essential to understand this very exciting topic:\n\n1. A recent book from Yoshua Bengio et al. is the best and most updated reference on DNN with a strong emphasis on theoretical and statistical aspects of deep neural networks. Although there is no good theory behind these algorithms, the authors do their best to explain why DNN work so well.\n\n2. The online book http://neuralnetworksanddeeplearning.com is also a very good introductory source for those interested in understanding the fundamentals of DL.\n\n3. Fundamentals of Deep Learning, Oreilly (2016) is a book that explains step by step the fundamental concepts of Artificial Neural Networks and Deep Learning.\n\nFor those starting in DL, I recommend the Keras framework\u200a\u2014\u200athat works under both Theano or Tensorflow.\n\n\u00b7 https://www.getrevue.co/profile/nathanbenaich from Nathan Benaich\u200a\u2014\u200amy favorite monthly review on Artificial Intelligence news, research, investments and applications.\n\n\u00b7 Wildml.com is a good blog maintained by Denny Britz with tutorials on DL\u200a\u2014\u200ait has a weekly newsletter.\n\n\u00b7 The exponent view https://www.getrevue.co/profile/azeem News about AI based technology and its impact on society\n\n\u00b7 datascienceweekly.org a weekly summary of new relevant for machine learning and data science\n\n\u00b7 Andrew Karpathy blog is a great source of inspiration for those who want to have a hands on approach of deep learning new tools, from image processing to recurrent neural networks.\n\n\u00b7 https://jack-clark.net/\u200a\u2014\u200aa very good weekly review on deep learning and AI.\n\n\u00b7 KDnuggets is a good blog with a diversity of topics on ML and AI.\n\n\u00b7 Data Science Central provides interesting posts on the business implications of ML\u200a\u2014\u200ahas a daily newsletter\n\n\u00b7 CreativeAI is an excellent blog showcasing works in a confluence of AI and Art.\n\n\u00b7 Arxiv.org the best repository of open publications in many areas, including Computer Science.\n\n\u00b7 Gitxiv.com a blog combining publications on Arxiv with the respective code on Github.\n\n\u00b7 http://www.arxiv-sanity.com/ a site made by A. Karpathy to easily curate content from Arxiv.\n\n\u00b7 Coursera has an excellent online course with the grandfather of ANN Geofrey Hinton https://www.coursera.org/learn/neural-networks\n\n\u00b7 Udacity also has a good course: https://www.udacity.com/course/deep-learning--ud730\n\n\u00b7 The classic, and pioneer, course of Stanford professor Andrew Ng https://www.coursera.org/learn/machine-learning\n\n\u00b7 Jason Brownlee has some excellent tutorials and e-books to start learning machine learning and deep learning models in Python using the Keras framework.\n\n\u00b7 Videolectures.net (for example ICML 2015 and deep learning summer school of 2016)\n\n\u00b7 This week in machine learning and AI is my favourite weekly podcast\u200a\u2014\u200ait gives an overview of recent developments and applications of AI and features a guest interview.\n\n\u00b7 Talking machines is also a very podcast featuring a guest in each episode.\n\n\u00b7 https://github.com/kjw0612/awesome-deep-vision\u200a\u2014\u200alist of resources of DL for computer vision.\n\n\u00b7 Approaching almost any machine learning problem by Abhishek Thakur (Kaggle Grand Master)\u200a\u2014\u200aa realistic overview of most machine learning pipelines.\n\n\u00b7 Kaggle promotes several challenging machine learning contests with prizes up to 100 000 USD. But more than the money it\u2019s about creating reputation as a true data scientist.\n\n\u00b7 This AMA from Geoffrey Hinton at Reddit is extremely helpful in understanding the history behind ANN narrated by one of its grand parents. Jurgen Schmidhuber also has a great AMA.", 
        "title": "Resources for deep learning \u2013"
    }, 
    {
        "url": "https://medium.com/udacity/this-week-in-machine-learning-25-november-2016-7c38b7a6174a?source=tag_archive---------6----------------", 
        "text": "This week\u2019s top Machine Learning stories, including Google Translate automatically writing its own universal language, and more!\n\nMachine Learning is one of the most exciting fields in the world. Every week we discover something new, something amazing, something revolutionary. It\u2019s incredible, but it can also be overwhelming. That\u2019s why we created This Week in Machine Learning! Each week we publish a curated list of Machine Learning stories as a resource to help you keep pace with all these exciting developments. New posts will be published here first, and previous posts are archived on the Udacity blog.\n\nWhether you\u2019re currently enrolled in our Machine Learning Nanodegree program, already working in the field, or just pursuing a burgeoning interest in the subject, there will always be something here to inspire you!\n\nGoogle\u2019s Neural Machine Translation system develops its own \u201cuniversal\u201d language, allowing it to translate between previously-unseen pairs of languages.\n\nClient-side media server Plex introduces automated tagging to its media library tool; it now can automatically tag the contents of pictures, such as the presence of a dog or cat.\n\nStartup Presenso aims to minimize or eliminate work stoppages in factories and machinery by combining distributed sensors with machine learning.\n\nProminent machine learning startup Wise.io, which focuses on supporting customer service through machine learning, is acquired by GE Digital.\n\nMachine learning helps doctors differentiate pathological hypertrophic cardiomyopathy, the leading cause of death in young athletes, and benign physiological hypertrophy.\n\nGoogle\u2019s Quick, Draw! app challenges visitors to draw simple pictures in 20 seconds or less to see if its neural network can recognize the image\u200a\u2014\u200aand improve it if not.", 
        "title": "This Week in Machine Learning, 25 November 2016 \u2013 Udacity Inc \u2013"
    }, 
    {
        "url": "https://medium.com/fusion-by-fresco-capital/nanotech-could-blow-artificial-intelligence-wide-open-heres-how-7c9998884fd?source=tag_archive---------7----------------", 
        "text": "Originally published by Marshall Kirkpatrick at www.getlittlebird.com as part of a series on the intersection of AI and a wide variety of other exponential technologies, as explored by cross-over influencers and experts.\n\nThese \u201ccombinatorial exponential\u201d technologies, as Frank Diana calls them, could have exponential impacts on the world in isolation\u200a\u2014\u200abut when they combine, things will get really interesting.\n\nNanotechnology, the ability to manipulate materials at nano-meter scale, using single atoms to assemble substances and objects, is often discussed in close proximity with artificial intelligence. Think of this as the intersection between materials sciences and computing. Nanotech on its own is generally discussed as a trillion dollar a year industry. Now cross pollinate it with AI.\n\nThere\u2019s a two-way relationship there and it\u2019s likely to only get stronger: AI for the nanotech and nanotech for the AI. It\u2019s such a close relationship that on any given day, like today, the top story on Kurzweil\u2019s Accelerating Intelligence website is about breakthrough methods in nanometer level materials manipulation.\n\nWhat could this intersection of AI and nanotech look like? Some of the gooiest fears have been de-emphasized by the community of people watching nanotech closely over the years, but it\u2019s not uncommon for new voices to raise concerns.\n\nWe can identify some of the 5 to 10 year future possibilities in this intersection of nanotech and AI now, (three possible scenarios are described below) but perhaps even more valuable is identifying some people who are likely sources of future knowledge about possible futures. Who\u2019s paying the most attention to both nanotech and AI?\n\nWe used Little Bird\u2019s influencer discovery and research technology to find out, (as you can use it to find key people in your industry).\n\nThese are 4 people that anyone interested in technology and the future should know.\n\nWe\u2019ll take a look at a few possible scenarios being discussed among experts at the intersection of these fields.\n\nHere are some scenarios identified by subject matter experts in the intersection of these two fields that could unfold over the next 5 to 10 years of nanotech + AI. We asked three questions to get these answers, using a foresight model called Incasting: what could go really well, what could go really badly, and what could just change the day to day?\n\nBy analyzing tens of thousands of connections between subject matter experts and thought leaders in both AI and nanotech, on Twitter, we found a list of respected nanotech thought leaders who appear to be paying particularly close attention to AI thought leaders online. Watch them and you may be one of the first to know about new developments and opportunities at this very important intersection.\n\nCyrus Hodes (top left) is in the UAE and is Founding Director at the Harvard Kennedy School\u2019s AI Initiative @ The Future Society. Almost no-one follows him on Twitter. I just added him to my private Twitter list titled Top People.\n\nHamid Sarraf (top right) is in the US and Czech Republic. He is a nanotech business consultant.\n\nHouston-based Gisele Waters (bottom left) has done many things and has many interests. She\u2019s currently engaged with the nanomedicine industry.\n\nKen Mason (bottom right) is a 3D automated surgery company founding executive and student of leadership.", 
        "title": "Nanotech Could Blow Artificial Intelligence Wide Open: Here\u2019s How"
    }, 
    {
        "url": "https://chatbotslife.com/evidence-of-skynets-rise-2016-11-51a29e514a29?source=tag_archive---------8----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Evidence of Skynet\u2019s Rise, 2016\u201311\u201328 \u2013"
    }, 
    {
        "url": "https://medium.com/@emwalz/there-is-a-hole-in-how-we-learn-and-we-wont-fully-see-it-until-we-fix-it-d2f1c597ff50?source=tag_archive---------9----------------", 
        "text": "There is a hole in how we learn\u200a\u2014\u200aand we won\u2019t fully see it until we fix it.\n\nThe hole in our learning actually has more to do with the whole of our learning than any specific hole. However we won\u2019t be able to address this until we have a bigger conversation. One that gets us to a new vantage point and allows us to properly see the gaps we are leaving in student\u2019s learning. The problem is not in what students are learning. The problem is a lack of awareness of how they are learning. Which in the end comes back to why they are learning.\n\nFor me, when I start with asking why is education important, how we approach education starts to take on a different feel.\n\nWhy does education matter? Education is about empowering individuals to be effective in their worlds, both now and in the future.\n\nHow does this change the way we approach education? By giving students a solid foundation and then teaching them how to teach themselves so that whatever lies ahead they are ready to master the skills and content they need.\n\nWhat does this look like in action? Teaching students to think about how they think, and what that means they can do. It means showing them biology is important not because it\u2019s valuable to know the names of different plants, but because it means they can understand how soil nutrients turn into energy in their bodies so they want to eat healthy or even plant their own vegetables. It means teaching the laws of motion not so they can pass their physics exam but so they can do their own basic fuel efficiency studies.\n\nTeaching metacognition means explaining the why behind the work students are given and the way they are given it. It means connecting what happens in class to the rest of their lives so effectively that next time they will make that connection on their own. It means helping them see how their minds are made to do a multitude of things, all of which they have the power to master on their own.\n\nI believe so strongly in metacognition because my life changed when I realized I\u200a\u2014\u200ameaning my mind\u200a\u2014\u200acould study myself/itself, both internally and externally. And by doing so, I was becoming infinitely smarter than trying to feed myself facts. In essence, I was no longer seeking new fish\u200a\u2014\u200aI was seeking new ways to fish.\n\nI pursued metacognition in various ways, from reflecting on what type of thinking was needed for different tasks to exploring learning theories and maps of the brain. The more I learned the more important this reality became: Our mind is what controls us, but to the extent we can understand them, we can control our minds.\n\nMetacognitive awareness is what lets you work smarter not harder. It is what causes you to pause upon hitting a roadblock in order to assess whether you need to choose a new road, to find a stunt driver, or to get a sledgehammer. It is what allows you to look at a project from start to finish and know what steps will be needed to get there.\n\nOur minds are the most powerful tool we have\u200a\u2014\u200aif we know how to use them well. Education needs to be equipping our minds with an understanding of our minds. Our learning, and teaching, needs to focus on elevating our minds to new levels of thinking in order to expand how we think. Higher education especially should focus less on deeper analysis of more complex topics, in exchange for teaching metacognitive awareness that unlocks higher levels of thinking.\n\nWe are our minds. Let\u2019s learn how to use them.", 
        "title": "There is a hole in how we learn \u2014 and we won\u2019t fully see it until we fix it."
    }, 
    {
        "url": "https://medium.com/@therapistmumble/the-problem-is-not-that-artificial-intelligence-will-become-self-driven-and-powerful-enough-to-take-d135bb36b8c2?source=tag_archive---------10----------------", 
        "text": "The problem is not that Artificial Intelligence will become self-driven and powerful enough to take over the world. The problem is that too many people will cede their decision making abilities to the analytics and algorithms that AI generates. Many people will do this without much realization that it is happening.\n\nIBM Watson machines are doing a great job of suggesting possible diagnoses and treatments for complex medical cases. Yet it should still up to the patient and doctor to determine the best course of action\u200a\u2014\u200aif they have the knowledge to make that determination.\n\nOne of the biggest usages of AI and Big Data has been to target people in order to sell them things. It is also being used to spread ideologies and mind memes. This can be done through ads, video games, emails, TV, Twitter, augmented reality, point of sale suggestions, apps, and probably many other ways that are just below our awareness.\n\nPeople have to be aware that all of these AI innovations are tools to help us. We are not their helpless tools.", 
        "title": "The problem is not that Artificial Intelligence will become self-driven and powerful enough to take\u2026"
    }, 
    {
        "url": "https://medium.com/@hellobly/robots-pourquoi-les-chatbots-vont-t-ils-r%C3%A9volutionner-lindustrie-des-petites-annonces-1d78e992d67c?source=tag_archive---------12----------------", 
        "text": "Depuis, beaucoup de choses ont chang\u00e9, le minitel puis internet ont permis l\u2019essor des petites annonces comme jamais auparavant\u00a0: une petite description, un joli titre, une petite photo et voil\u00e0 que tout le monde peut entrer en contact avec vous. Cependant, \u00e0 l\u2019\u00e8re des SMS et des r\u00e9seaux sociaux, o\u00f9 il est presque mal vu de ne pas \u00eatre pr\u00e9sent sur Facebook ou Twitter, l\u2019industrie des petites annonces s\u2019appuie encore sur le mail et le t\u00e9l\u00e9phone, en misant sur l\u2019anonymat. L\u2019industrie des petites annonces a longtemps arr\u00eat\u00e9 d\u2019innover, et par cons\u00e9quent elle devient de plus en plus obsol\u00e8te, inefficace et dangereuse aussi.\n\nLes services de petites annonces souffrent g\u00e9n\u00e9ralement de deux grands probl\u00e8mes\u00a0: la confiance et l\u2019encombrement. Personne ne sait qui est son interlocuteur jusqu\u2019au moment de la rencontre, de plus la saturation des sites d\u2019annonces rend le processus de d\u00e9couverte difficile, \u00e0 la fois pour l\u2019acheteur et le vendeur.\n\nTous les utilisateurs de chatbots ont d\u00e9j\u00e0 un compte sur un r\u00e9seau social, par cons\u00e9quent on pourrait appliquer des algorithmes sophistiqu\u00e9s pour \u00e9valuer la cr\u00e9dibilit\u00e9 du vendeur/acheteur en se basant par exemple sur son activit\u00e9 sur le r\u00e9seau social, ses transactions pr\u00e9c\u00e9dentes, le prix moyen de l\u2019article sur le march\u00e9 vs le prix demand\u00e9 (on a tous vu des iPhone 6 neufs \u00e0 200 euros). Le bot peut aussi planifier la rencontre entre l\u2019acheteur et le vendeur dans un endroit public, s\u00e9curis\u00e9 (une caf\u00e9t\u00e9ria par exemple) et assez proche des deux parties.\n\nNous pouvons aussi aller un peu plus loin, appliquer du machine learning sur les petites annonces pour d\u00e9tecter les escroqueries r\u00e9pandues et d\u00e9gager de tout \u00e7a un pattern qui prot\u00e9gera les utilisateurs des diff\u00e9rentes menaces potentielles, et puisque les chatbots sont impl\u00e9ment\u00e9s au dessus des plateformes de chat officielles, l\u2019environnement deviendra de plus en plus hostile aux arnaqueurs.\n\nLes petites annonces font presque partie de notre vie quotidienne. En appliquant les technologies actuelles, nous pouvons d\u2019ores et d\u00e9j\u00e0 r\u00e9soudre les probl\u00e8mes de confiance et d\u2019encombrement. T\u00f4t ou tard, l\u2019I.A et les chatbots d\u00e9clencheront une grande r\u00e9volution technologique et changeront totalement notre fa\u00e7on de faire du business autour de nous.", 
        "title": "#Robots : Pourquoi les chatbots vont t-ils r\u00e9volutionner l\u2019industrie des petites annonces ?"
    }
]