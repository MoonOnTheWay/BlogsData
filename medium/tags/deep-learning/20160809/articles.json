[
    {
        "url": "https://medium.com/@bakiiii/microsoft-presents-deep-residual-networks-d0ebd3fe5887?source=tag_archive---------0----------------", 
        "text": "It is clear that deep learning lives its golden era. It is not surprising to see a breakthrough improvement in this field everyday. In addition application areas of the deep learning are getting wider from finance to advertising. This results in that big players like Google, Facebook, Microsoft organized teams to study deep learning.\n\nFor whom like to do some research about these teams\u2019 work, LeNet (1998), AlexNet (2012)\u00a0, GoogleNet(2014), VGGNet (2014), ResNet(2015) are worth to look. Each of these network architectures have unique approach to different problems. For example, AlexNet has parallel two CNN line trained on two GPUs with cross-connections, GoogleNet has inception modules\u00a0,ResNet has residual connections.\n\nOne of the main deductions from these studies is that the depth of the networks is crucial parameter and not easy to decide. Theoretically increasing number of layers should results in increase in representation capacity of the network. This is supposed to enhance the accuracy of the network. However in practice this is not the case due to:\n\nIn some cases some neuron can \u201cdie\u201d in the training and become ineffective/useless. This can cause information loss, sometimes very important information.\n\nIf parameters like weights,biases increases due to increasing depth, training the network becomes very difficult. Even this causes in higher training errors.\n\nHence the problem becomes increasing network depth without affecting from these problems.\n\nOne of the biggest advantages of the ResNet is while increasing network depth, it avoids negative outcomes. So we can increase the depth but we have fast training and higher accuracy. Pretty, right?\n\nIn normal cases, we can have underlying mapping with a nonlinear H(x) function from input to output. Lets say instead of H(x), use nonlinear function F(x) which defined as H(x)-x. At the output of the second weight layer (on the right) we arithmetically add x to the F(x). Then pass F(x)+x through Rectified Linear Unit (ReLU). This enables us to carry important information in the previous layer to the next layers. By doing so we can prevent vanishing gradient problem. Even if this connection looks like an addition to standard CNN approach, surprisingly, it fastens the training of the network.\n\nFor those in the deep learning field, this approach seem familiar. Yes you are right, it is actually similar principle introduced with Long Short Term Memory (LSTM) cells.\n\nThe connection carrying input to the output called shortcut connections. In the main paper published by Microsoft team, you can see the comparison of the two CNN networks, one is normal CNN and the other has residual connections, in terms of training time and accuracy on ImageNet and CIFAR-10 datasets.\n\nAccording to the paper published in 2015, 152-layer ResNet was the deepest network trained on ImageNet at that time. And as promised it has lower parameter than of VGG Net which is 8x times smaller in depth. This has quite impact on faster training performance.\n\nThis improvements results in winning the 1st place in ILSVRC classification competition on ImageNet with 3.57% top 5 error.\n\nI assume that this post gives you brief introduction about ResNet. For detailed information you can read \u201cDeep Residual Learning for Image Recognition\u201d paper. It is available in arxiv.org. The links for both this paper and other helpful references are given below.\n\nFor my other post you can view my profile or my personal website\u00a0: nurbakier.com.\n\n1- Deep Residual Learning for Image Recognition\n\n[1512.03385] Deep Residual Learning for Image Recognition\n\nAbstract: Deeper neural networks are more difficult to train. We present a residual learning framework to ease the\u2026arxiv.org", 
        "title": "Microsoft Presents : Deep Residual Networks \u2013 Baki Er \u2013"
    }, 
    {
        "url": "https://medium.com/@dcvc/standing-on-the-shore-how-ai-is-disrupting-the-worlds-largest-industries-38df7430a543?source=tag_archive---------1----------------", 
        "text": "Artificial Intelligence is undergoing a massive acceleration driven by rapid growth in available data and rapid evolution of algorithms.\n\nIntel\u2019s acquisition this Tuesday of our portfolio company Nervana Systems validates that our companies\u2019 platforms, driving this acceleration, are disrupting the world\u2019s largest industries.\n\nOur thesis is that a) increasingly powerful and inexpensive hardware that is machine learning/deep learning-friendly (lots of multipliers and fast memory; e.g., Nervana Systems), b) a flourishing of learning approaches running on that hardware, and c) large, novel data sets, now inexpensive to acquire and refresh, to train and drive those novel learning algorithms, is fueling a transformation of major global industries right in front of everyone\u2019s eyes.\n\nMission-critical decisions can now be made in the face of huge amounts of even chaotic data, and life-or-death actions can be implemented in the real-world, at large scale, with dramatically less cap-ex and op-ex than ever before, thanks to the speed, clarity, and efficacy of commercially practical AI.\n\nWe\u2019ve been investing in this thesis for the better part of a decade along with a small number of like-minded folks**, and we expect that it will fundamentally disrupt every industry vertical. Let\u2019s take a look at how this combination of the rapid growth in available data (see our post here) and a corresponding rapid evolution of algorithms (see our post here) is playing out in the wild.\n\nIncreasingly powerful hardware, evolving machine learning approaches, and large new data sets are fueling a transformation of major global industries.\n\nDrug discovery is moving from a paradigm of armies of expensive white coats in labs to algorithms in the cloud\u200a\u2014\u200aleading to a drug discovery process that takes places over months rather than years. CapellaBio and Atomwise engage in drug discovery directly. Transcriptic and Vium are make experimental biology and pharma processes elastic and on-demand via fully automated warehouses for wet lab and in vivo experiments, respectively; think of them as AWS for wet labs and animal models, respectively. 3Scan is reinventing pathology as a high throughput, quantifiable, digital science. Omniome is revolutionizing diagnosis using a more complete set of genomic, proteomic, and clinical patient data.\n\nZymergen has integrated proprietary machine learning software solving a problem computationally harder than AlphaGo with robotic lab automation, and a huge storehouse of genomic data, to evolve in silico, test, and deliver microbes for its Fortune 100 global customers that radically change the economics of bio-produced products for the chemicals, health, agriculture and food industries. Atomwise, which uses deep learning at massive scale along with very large chemistry and genomics data sets, enables in silico discovery of non-toxic pesticides and safer agricultural products, as well as pharmaceuticals, for multiple Fortune 100 companies today\n\nInnovation in precision agriculture is occurring from a macro scale based on aerial imagery to a micro scale based on a deeper understanding of bacteria. Descartes Labs is detecting subtle changes in growth patterns from petabytes of orbital data to accurately predict harvests, impacting agriculture worth hundreds of billions of dollars. Pivot Bio has applied proprietary, multi-decade-depth genomics data along with novel machine learning to program networks of soil bacteria to generate fertilizer for plants from the air and soil, potentially eliminating nearly 10% of global energy use, as well as huge costs and toxic run-offs for farmers world-wide.\n\nAs Bill Gates often proclaims, breakthroughs in Materials Science will be one of the biggest accelerators of technology development in other fields. Citrine is a DCVC company that helps discover cheaper, less energy intensive, more environmentally friendly ways to produce existing materials, as well as in silico discovery of new materials that solve a unique product or production requirement, through novel deep learning approaches applied to extremely large and complex data sets.\n\nFrom banking to insurance, data and algorithms are disrupting how we estimate and manage risk, price products, and make financial predictions. LendUp uses machine learning to score risk faster and more accurately, with less data, to unlock credit for the world\u2019s unbanked. Raptor streamlines compliance processes and detects suspected money laundering and terror-financing patterns in huge transaction flows. One of our stealth companies unearths complex dependencies in multi-party financial instruments to prevent another 2008 event. Another stealth startup underwrites cyber insurance policies and provides board-level financial risk assessments for cybersecurity risk. Cape Analytics uses aerial imagery with breakthrough machine vision techniques to facilitate real estate insurance underwriting from above rather than sending someone out to visit buildings manually. Metabiota is the world\u2019s leading company for tracking, qualifying, mitigating and pricing and making efficient markets to manage epidemic risk, applying machine learning to fast-moving, proprietary data sets from its network of epidemic sentinel stations around the world.\n\nWith the increasingly visible cataclysmic manifestations of digital and physical risks, detecting and neutralizing threats is top of mind for the public and private sector. Primer (stealth) uses a novel ensemble of deep learning techniques to ingest petabytes an hour of unstructured data in multiple languages and automatically generating analytical reports to identify patterns of terror or crime. SentinelOne uses compute-efficient machine learning (less than 1MB memory profile, runs in real-time on an ARM) to Identify the behavior of \u201czero-day\u201d cyber-attacks in real time and stops them cold. Kentik ingests terabits an hour of network traffic and automatically detects DDoS attempts and network security risks. PerimeterX finds and stops bot networks in banking and trading, an area where other systems have failed to date. Area1 detects the subtle patterns of phishing attacks and stops them before they start.\n\nAs computation eats capex and opex, the world increasingly yearns for more powerful and specialized compute resources to unlock new possibilities and make existing programs run faster on larger amounts of data. Nervana is a great example of specialized compute\u200a\u2014\u200amassively accelerating deep learning. Rigetti aims to deliver multiple orders of magnitude improvement in machine learning and deep learning in general, and in particular simulation, genomics/proteomics, materials science, and similar computation via its breakthrough quantum compute platform.\n\nWith costs growing at unsustainable rate and a system rife with inefficiencies and poor decisions, healthcare is ripe for technology disruption. From supporting physicians in making more accurate diagnoses to closed-loop optimization by analyzing outcomes and learning to predict the most effective treatments, applications abound for data-driven improved outcomes. CloudMedx synthesizes effective and inexpensive treatment plans from huge amounts of ambiguous health care data. Karius helps avoid the costs of mistaken diagnoses and poorly conceived treatments.\n\nManufacturing and supply chain have been under the optimization microscope since the industrial revolution, and the quest continues. Tradeshift streamlines how corporations manage their suppliers and leverages network effects of supply chain data to provide never before seen supply chain analytics and risk management. Kindred uses AI-driven robotics so that one human worker can do the work of four. SigOpt delivers custom manufacturing and supply chain optimization recommendations, on demand. Rescale enables the on-demand combination of existing industrial supercompute for simulation and manufacturing planning with advanced machine learning for optimization. Most of today\u2019s private aerospace and rocket companies use Rescale to simulate how existing and novel materials will perform in real world systems at full scale.", 
        "title": "Standing on the Shore*: How AI is Disrupting the World\u2019s largest Industries"
    }, 
    {
        "url": "https://gab41.lab41.org/lab41-reading-group-deep-compression-9c36064fb209?source=tag_archive---------2----------------", 
        "text": "Pre-trained convolutional neural networks are too large for mobile devices: AlexNet is 240 MB and VGG-16 is over 552 MB. This seems small when compared to a music library or large video, but the difference is that the networks reside in memory when running. On mobile devices SRAM is scarce and DRAM is expensive to access in terms of energy used. For reference, the authors estimate that a 1 billion node network running at 20 FPS on your phone would draw nearly 13 Watts from just the DRAM access alone. If these networks are going to run on a mobile device (where they could, for example, automatically tag pictures as they are taken) they must be compressed in some manner. In this paper the authors apply three compression methods to the weights of various networks and measure the results. A diagram of the three methods and their results are below; I\u2019ll walk you through them in more depth in the next few paragraphs.\n\nThe next paper from our reading group is by Song Han, Huizi Mao, and William J. Dally. It won the best paper award at ICLR 2016. It details three methods of compressing a neural network in order to reduce the size of the network on disk, improve performance, and decrease run time.\n\nThe first compression method is Network Pruning. In this method a network is fully trained and then any connections with a weight below a certain threshold are removed leaving a sparse network. The sparse network is then retrained to ensure the remaining connections are used optimally. This form of compression reduced the size of AlexNet by a factor of 9, and VGG-16 by a factor of 13. The authors also use a clever data structure that makes use of variably sized integers to store the network after this compression.\n\nThe second compression method is Trained Quantization and Weight Sharing. Here the weights in a network are clustered together with other weights of similar magnitude, and all these weights are then represented by a single shared value. The authors use k-means clustering to group weights for sharing. They explore multiple methods of setting the k centroids and find that a simple linear spacing of the centroids along the full distribution of weight values performs best. This compression method reduces the size of the networks by a factor of 3 or 4. A diagram with an example of this compression technique is shown below.\n\nThe third and final compression method is Huffman Coding. Huffman coding is a standard lossless compression technique. The general idea is that it uses fewer bits to represent data that appears frequently and more bits to represent data that appears infrequently. For more details see the Wikipedia Article. Huffman coding reduces network size by 20% to 30%.\n\nUsing all three compression methods leads to a compression factor of 35 times for AlexNet, and 49 times for VGG-16! This reduces AlexNet to 6.9 MB, and VGG-16 to under 11.3 MB! Unsurprisingly it is the fully connected layers that are the largest (90% of the model size), but they also compress the best (96% of weights pruned in VGG-16). The new, smaller convolutional layers run faster than their old versions (4 times faster on mobile GPU) and use less energy (4 times less). These results are achieved with no loss in performance! A plot showing the energy efficiency and speedups due to compression are shown below:", 
        "title": "Lab41 Reading Group: Deep Compression \u2013"
    }, 
    {
        "url": "https://medium.com/@dcvc/rapid-evolution-of-algorithms-16c0813e2bd9?source=tag_archive---------3----------------", 
        "text": "Machine Learning techniques are applicable to a rapidly growing set of important problems. We have an equally rapid growth of available data to power these applications. Our constant push to invent new formalisms driven by improved performance on practical problems yields the increasingly creative composition of the different learning paradigms that Jordan outlines. Taken together, these motivating factors are driving a boom in novel machine learning research.\n\nThese forces are leading to a boom in both exploring new approaches and refining older approaches that haven\u2019t been practicable until more recently. For example, neural networks have been around since the the 1950s, and deep neural networks have been around since the 1980s. Despite their long history, deep nets have only taken off more recently as the result of a confluence of multiple trends\u200a\u2014\u200alarge available data sets with new hardware (e.g., NVIDIA and Nervana) and approaches to train the networks faster.\n\nDeep learning papers represented only ~0.15% of computer science papers in arXiv published in early 2012 but grew ~10X to ~1.4% by the end of 2014. In 2016, 80% of papers at many top NLP conferences are deep learning papers. Deep nets are now demonstrating state of the art results across applications in computer vision, speech, NLP, bioinformatics, and a growing list of other domains.\n\nAlthough deep nets are currently non-interpretable, we\u2019ve seen interesting work in progress to compose deep nets with classical symbolic AI to treat logic programs as a prior structure on the network. This approach may allow deep nets to learn logic programs that ultimately yield new scientific insights in a range of fields. As Mike Jordan mentions in the quote opening this post, Machine Learning often evolves rapidly when we invent new formalisms driven by insights from practical applications. As deep nets continue to deliver state of the art results for various tasks, efforts to invent new formalisms is an area of focus much like the efforts to understand the statistical properties of boosting lead to deeper understanding of regularization in the mid-2000s.\n\nMachine learning researchers focusing on applications at scale have developed a rich set of techniques for featurizing massive amounts of heterogeneous data to feed into well-worn logistic regression and boosted decision tree models. Since this approach is robust and very fast in production, the \u201crich features with simple models\u201d approach powers many of the largest systems we use every day, like Facebook newsfeed ranking.\n\nWe\u2019ve developed some very clever tricks over the years to turn very hard problems into simpler ones. One important set of such tricks involves techniques like Variational Methods, which formulate intractable problems as approximate convex optimization problems, and then apply well understood optimization algorithms which yield good performance and often have fast parallel and streaming variants.\n\nA second set of sneaky tricks include methods like distant supervision, self training, or weak supervision, for starting with an insufficient dataset and incrementally bootstrapping your way into sufficient data for supervised learning. Using these methods, you may or may not have some labeled data, you definitely have a bunch of unlabeled data, and you have a \u2018function\u2019 (read: a dirty yet clever hack) that assigns noisy labels to the unlabeled data\u200a\u2014\u200aonce you have lots of data with noisy labels, you\u2019ve turned the problem into vanilla supervised learning.\n\nA third bag of tricks arise from the method of Transfer Learning\u200a\u2014\u200aapplying knowledge learned from one problem to a different but related problem. Transfer learning is especially exciting because we can learn from one data-rich domain with a totally different feature space and data distribution, and apply those learnings to bootstrap another domain where we may have much less data to work with.", 
        "title": "Application Drives Formalism: Why Machine Learning is Booming and Three Bags of Tricks"
    }, 
    {
        "url": "https://medium.com/@bakiiii/microsoft-sunar-deep-residual-network-d2970003ad8b?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Microsoft Sunar : Deep Residual Network \u2013 Baki Er \u2013"
    }, 
    {
        "url": "https://chatbotnewsdaily.com/the-rise-of-the-chatbots-7aa0eacae064?source=tag_archive---------5----------------", 
        "text": "I recently read an article that highlighted 25 new start-up\u2019s focused on chatbot solutions; from this list I noted a UK-based start-up called Cyra (http://www.recbo.co/), an AI based recruitment solution that is currently in beta testing phase\u200a\u2014\u200aI have asked to sign-up for the trial and I am currently awaiting a response. Additionally, there was also a USA based start-up offering an automated chatbot to target the rise of freelancer work; called TARA (https://www.tara.ai/).\n\nAmong the start-ups listed, mainly based in the USA, where chatbot solutions for automated assistance, customer service, conversational commerce to encourage you to purchase products via text message, personal shopping assistants, marketing & communications and even a proposition that allows you to \u2018create avatars of themselves that learn their speech patterns and life stories, and can then converse with loved ones after death\u2019.\n\nIt would appear that the integration of AI algorithms and chatbots are on the rise.\n\nThe discovery of the aforementioned start-up was a catalyst for further exploration of AI and/ or chatbot solutions, and more specifically, stories of success. The most obvious AI assistant that is often overlooked is \u2018Siri\u2019, the \u2018intelligent personal assistant\u2019 and \u2018knowledge navigator\u2019 according to Apple, its maker when it introduced Siri to the world in 2011 on the iPhone 4S utilising iOS 5.\n\nApple\u2019s use of a natural language user interface to answer individual\u2019s questions and perform actions, through delegation to a set of web services, then begins to learn an individual\u2019s preferences with the aim of starting to return personalised recommendations. Now to the question, is Siri a success?\n\nIf you ask Apple, definitively, the answer will be \u2018yes\u2019. If you ask the consumer (*you and I), the answer may be slightly more polarised; for Apple fans, you will find numerous forums and blog sites advocating the advances of Apple technology, however, back in 2012 if you asked some English speakers with \u2018distinctive accents\u2019, they would have expressed frustration at the results returned by Siri owing to a lack of being to comprehend the language and intonation.\n\nApple\u2019s response was that \u201cas more people use Siri and it\u2019s exposed to more variations of a language, its overall recognition of dialects and accents will continue to improve, and Siri will work even better.\u201d Not exactly success, however, not exactly a disaster either; also, this was 4 years ago and the current iteration of Siri seems to be an accepted companion with each iPhone release. Personally, I find using Siri useful and the social self-consciousnesses of talking to my phone subsided pretty quickly after a few weeks of use.\n\nThe first word that springs to mind is \u2018trust\u2019\u200a\u2014\u200ado you trust an AI, and not a person, to have access to your bank account? I have to admit to pausing before committing to the sending the \u2018OK\u2019 message I had typed out in Messenger to allow Cleo, an \u2018intelligent assistant for your money\u2019, access to my UK bank account on a \u2018read-only\u2019 basis. Even with the claim that \u2018she\u2019 said she was built with \u2018military-grade security standards and uses 256-bit encryption, I still felt very reserved about allowing an unknown entity access to \u2018my money\u2019.\n\nSo after the \u2018brief pause\u2019 (*the time it took to make a cup of tea and contemplate the loss of all the money in my bank account), the on-boarding experience was initially very engaging (*apart from when I forgot my bank account app password\u200a\u2014\u200aiTouch/Touch ID has completed removed the need for me to remember). Then came the interaction with Cleo once she had access to my account; the responsiveness is a positive and she can see all the information and data my bank app holds\u200a\u2014\u200ahowever, I think that\u2019s the challenge, the quality of the data my bank provides is not contextually sufficient for my needs. With this in mind, I think the increased volumes of in-app purchasing, social media commerce and the ability to book, purchase and bank in messaging services highlights the lack of innovation from established financial service providers.\n\nI think we will see an increased utilisation of chatbots in front line customer services, which I have eluded to in previous posts; the challenge is the ability to return contextually rich results that enhance our experience, however, whether we have the patience to maintain our interest through the \u2018learning phase\u2019 in some cases may be the defining factor in success. What is proven is that chatbots can be very effective focused on a specific subject (\u2018can you tell me how much I have paid out in direct debits this month?\u2019; however, when you broaden the scope of your requests (\u2018who won the English Premier League last season?\u2019), the response are invariable \u2018I didn\u2019t catch that\u2019 with a prompt to ask the question again.\n\nIn short, work in progress, however, solid progress being made.\n\nVenture Radar\u200a\u2014\u200a\u2018\u201925 ChatBot Start-up\u2019s You Should Know About\u2019\u2014 http://blog.ventureradar.com/2016/06/14/25-chatbot-startups-you-should-know/\u200a\u2014\u200aJune 2016\n\nMetz, Cade\u200a\u2014\u200aWired\u200a\u2014\u200a\u201cA New Chatbot Would Like To Help You With Your Bank Account\u201d\u200a\u2014\u200ahttp://www.wired.com/2016/06/new-banking-ai-now-chatbots/\u200a\u2014\u200aJune, 28, 2016", 
        "title": "The Rise of the Chatbots \u2013"
    }, 
    {
        "url": "https://medium.com/@farshchi/a-coming-semiconductor-nervana-ec4b040cea5a?source=tag_archive---------6----------------", 
        "text": "For more than a decade, founders and venture investors have steered clear of semiconductors. Their rationale seemed sound. As Moore\u2019s Law brought exponential growth in performance, the cost of designing new chips turned out to far outpace that rate. Bringing a new chip to market can cost well over $100M\u200a\u2014\u200atherefore, startups, and incumbents alike, need to sell many millions of chips just to break even. Large companies benefit from existing software and product teams to sell these ever-complex products. Startups, however, suffer by having to build up customer support infrastructure for a single product as it slowly ramps. Increasing development costs, compressing margins, slow rates of customer adoption, and industry consolidation have decimated many fledgling companies. Well that\u2019s settled\u200a\u2014\u200anext industry please!\n\nBut investors who did not subscribe to this conventional wisdom (ahem\u2026) were rewarded handsomely with Intel\u2019s acquisition of Nervana, announced earlier today.\n\nI first met Naveen Rao a decade ago in an unsuccessful attempt to recruit him into my Neuroengineering laboratory at UCLA. Naveen had previously spent several years as a chip designer at Sun Microsystems, and later chose to pursue his PhD to learn more about the brain to build brain-inspired chips. He jumped at the opportunity to work with John Donoghue at Brown University, where he built brain-machine interfaces. We were reunited seven years later by my partner Zavain Dar as Naveen set out to raise Nervana\u2019s Series A financing. Along with cofounder Amir Khosrowshahi, Naveen had built a world-class hardware and software team. They aimed to bring powerful software tools to empower developers to bring machine intelligence to the masses\u200a\u2014\u200aenabled by special hardware (powerful servers to be powered by their unique chips) in the cloud. Nervana\u2019s hardware engineers were among the world\u2019s best digital designers, who invented a means of performing the most common operations used in training deep learning algorithms, natively in silicon.\n\nNervana\u2019s story reminded me of the concept of GPUs in the early 1990s; accelerating polygon rendering by doing many multiplications and additions in parallel. Unfortunately, there was a problem: memory was too expensive to realize their full potential. It didn\u2019t take long for memory prices to drop, and the masses were quickly enchanted by the magic of rendering rich polygons in real time, bringing Grand Theft Auto to life. Today, GPUs have outpaced CPUs in PCs, mobile, and gaming, with even more opportunities in new applications in driverless cars, computer vision, natural language processing, and robotic chess players. If Nervana\u2019s chips are what comes after the GPU, why have investors steered clear of semiconductors? To understand how the landscape is different, let\u2019s take a glimpse into the past.\n\nStarting a few decades a go, semiconductor foundries catalyzed the creation of the \u201cfabless\u201d semiconductor startup\u200a\u2014\u200ano longer did a startup need to raise hundreds of millions of dollars to build a cleanroom facility to build chips. Furthermore, third-party packaging and test companies made semiconductor development a matter of a few clicks in CAD, with fully-packaged chips being drop-shipped to customers. Founders and venture capitalists had a field day with fabless semi home runs such as Qualcomm, Marvell, and Broadcom, just to name a few. The telecom and internet bubble of the \u201990s sent their stock prices skyrocketing, leading to a fabless semi startup acquisition feeding frenzy, which prompted more talented founders to start fabless chip companies fueled by venture capitalists eager to generate a quick return.\n\nIn the late \u201990s and early \u201900s, it was a no-brainer for public incumbents to exchange many hundreds of millions in inflated stock for a fabless chip startup, in many cases before they even had a fully-functional test chip. Investors assigned tens of millions of valuation on every PhD doing WiFi, Bluetooth, GPS, SERDES, and fiber-channel. Innovent was sold for hundreds of millions for its Bluetooth know-how. Newport generated a whopping $1B in proceeds after showing a demo chip with less than $5M of invested capital. Unfortunately, the second half of the 2000s flipped the model on its head. The dot-com crash compressed the valuations of the big acquirers. The benefits of Moore\u2019s Law led to skyrocketing development costs, exacerbated by chips getting exponentially more complicated. Competing technologies led to competing standards which took many years to get embraced by big electronics companies\u200a\u2014\u200aWireless GigaBit and Wireless USB being examples where most investors in both protocols lost money, with the former eventually being subsumed by Wi-Fi.\n\nSince the mid-2000s, compressed valuations and pursuit of greater economies of scale have brought on industry consolidation\u200a\u2014\u200acontinuing today with Analog Devices\u2019 pending acquisition of Linear. Although the behemoths had greater leverage than startups on customers such as Apple and Samsung, pressure from Asian competitors pushed chip prices\u200a\u2014\u200ato the extent where Apple captures all the value\u200a\u2014\u200aboth at the retail counter and in the public markets. As a result of the complexities associated with getting larger, the big chipmakers are also getting slower, focusing on iteration rather than fundamental innovation: marginally faster/lower power/cheaper processor and radios. Conventional wisdom accepts chip startups\u2019 fundamental disadvantage, and steers venture dollars away from chip companies altogether.\n\nLux\u2019s quest for an unconventional approach to semiconductor startups also led to the creation of Flex Logix. UCLA Professor Dejan Markovic and his Postdoc Dr. Cheng Wang invented a novel interconnect optimization scheme, that when applied to digital circuits, would yield a configurable logic block that outperforms FPGAs at a fraction of the chip area. Dejan and Cheng aspired to build a company around this invention, but feared the capital intensity of building a full-fledged chip startup, and questioned the viability of simply licensing the configurable blocks to chipmakers. Lux introduced Dejan to Geoff Tate, founding CEO of Rambus, which besides ARM, is one of the only semiconductor IP licensing companies that built a market capitalization in the billions of dollars. Geoff recognized three powerful converging themes that would make a Flex Logix incredibly valuable as a licensing company: 1) chip costs soaring, 2) demand for mass customization, and 3) shortening product life cycles. Since partnering together to co-found Flex Logix, Geoff, Cheng, and Dejan have engaged with major semiconductor companies and attracted National Semiconductor cofounder Pierre Lamond\u200a\u2014\u200ahimself having put many semiconductor behemoths in business as a former Partner at Sequoia Capital\u200a\u2014\u200aas an investor and board member.\n\nLux continues to seek opportunities in semiconductors, albeit by taking some of the lessons learned over the past decade. Here\u2019s what we look for:\n\nI can feel the eyes rolling. Every VC claims to be seeking disruptive technology, while anyone doing anything in semiconductors views themselves as being on the cutting edge. Attention chip designers: The bar is much higher for you. Founders need to find creative ways of leveraging semiconductor technology currently offered by foundries to offer functionality that is fundamentally unique. It could be a novel circuit topology, or unique algorithm implemented with a unique topology, or do far more with far less. The award doesn\u2019t go to the talented designers that achieve better performance, power, and cost for existing functionality\u200a\u2014\u200ait goes to those that offer entirely new functionality. For example, basic physics says that the distance data can travel over copper wires is inversely proportional to the data rate\u200a\u2014\u200aa limitation that light doesn\u2019t apply to light over fiber. As machines got faster, the performance of data centers was limited to power and interconnectivity, resulting from the physical constraints of pushing electrons through copper. Lux-backed Luxtera rose to the challenge of using photonics to connect machines, with the vision of connecting chips, with light. The interconnect bottleneck disappeared, and a new era of high-performance cloud compute will push machine intelligence forward.\n\nIncrementally faster and cheaper is great for consumers but doesn\u2019t stand out as an enabler. Chip founders should think less in terms of quantitative performance and more in terms of capability. What does this chip allow you to do that you otherwise can\u2019t do without it? The challenge here is that many applications probably aren\u2019t obvious until designers do something interesting with the chips\u200a\u2014\u200ain which case I would encourage founders to let their imaginations go wild. Lux helped to lead the spinout of Everspin from private equity-owned Freescale (now NXP) after taking more than $100M of corporate investment to develop its magnetoresistive technology, weaving memory into processing elements. Everspin\u2019s MRAM memory has enabled the world\u2019s fastest solid-state disks, and can empower tomorrow\u2019s in-memory and neuromorphic compute architectures.\n\nMany products we take for granted today would have been nothing short of magical upon their introduction. Broadcom built cheap chips that would push megabits per second over twisted copper wire at a time when conventional technology could barely establish a reliable kilobit per second connection. Qualcomm magically transmitted signals buried in noise with its novel coding techniques. Many engineers have fond memories of the credit-card-like WiFi adapters that magically liberated them from their desks, only later to become standard equipment on all laptops shipped.\n\nAt Lux, we\u2019re seeking the brave founders attracting the world\u2019s best talent solving hard problems across all disciplines, including semiconductors, towards building multi-billion-dollar businesses.\n\nShahin is a partner at Lux Capital. Based in Silicon Valley, he invests in space, robotic, AI, transportation, VR and brain-tech companies; follow him on Twitter @farshchi", 
        "title": "A Coming Semiconductor Nervana? \u2013 Shahin Farshchi \u2013"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-1-3f5caa2e21ed?source=tag_archive---------7----------------", 
        "text": "Google Brain does an AMA on Machine Learning\n\nThey\u2019re gathering questions now and will be answering them on August 11, 2016.\n\nThe erstwhile Dato and GraphLab was founded by Carlos Guestrin, ML professor at University of Washington\n\nAimed at graduate students, industrial engineers and researchers, this year\u2019s edition is organised by Aaron Courville and Yoshua Bengio. You can check out the slides in link.\n\nThe company launched DuSee, an AR platform allowing users to interact with computer-generated visuals through their smartphones.", 
        "title": "\u2014 Issue #1 \u2013"
    }, 
    {
        "url": "https://medium.com/@judy.yero/self-directed-learning-c47e971c1fd5?source=tag_archive---------8----------------", 
        "text": "I just returned from the Alternative Education Resource Organization (AERO) conference. The members and member schools all espouse the methods that you have described. I can assure you that standards and standardized testing are the LAST thing you should worry about. Two keynote speakers\u200a\u2014\u200aDr. Peter Gray (Free to Learn) and Dr. Yong Zhao (World Class Learners) explain why self-directed learning is the most authentic way to help students to develop their unique strengths, which is what we need in today\u2019s rapidly changing world. Peter Gray is an evolutionary psychologist who describes the role of play and self-directed learning in cultures that have not suppressed the drive to learn as has happened in the West.\n\nThe type of schools we have today are based on the Calvinist belief that children are inherently lazy and untrustworthy and that it is the duty of adults to turn them into acceptable members of society. Standards and standardized testing came into existence with the industrial age, when \u201caverage\u201d workers were needed to perform standardized work. And today, even as machines continue to take over that standardized work, we are still trying to produce standardized worker bees.\n\nYong Zhao points out that trying to force all children to study the same (academic) things in the same ways and at the same age does nothing but create mediocrity. We only need consider our own lives to recognize that we are stronger, and more interested, in some areas and less in others. While we might become competent in areas that don\u2019t interest us, we do it at the expense of developing our greatest gifts. Obsessing about academic (verbal, linguistic) achievement to the exclusion of other types of intelligence not only wastes those gifts, but turns children off to their innate drive to learn and convinces them that they are failures.\n\nIn his recent book, The End of Average, Todd Rose provides the historical background and current research on the whole concept of average, standards, and standardized tests. It provides tremendous insight into why the idea of multiple intelligences is only the tip of the iceberg. Reading it will clarify why self-directed learning is the only way to provide \u201cequal access\u201d to opportunity. As Rose states, any system designed to fit the average fits no one! For more background, you can also read this article.\n\nThe current system of education cannot be \u201creformed\u201d into a viable, effective learning environment that will facilitate the development of every child\u2019s innate gifts and talents. This is the antithesis of what traditional schools were designed to do. You can\u2019t \u201creform\u201d a Model-T Ford into a rocket that will travel to Mars and beyond. What we are facing is a paradigm shift equivalent to the change from the geocentric to heliocentric models of the solar system. Genius hours and more choice within a traditional classroom are patches designed to make teachers and administrators feel better about what they are doing to children. At some deep level, they recognize how they have imprisoned the minds of our young.\n\nWhat of the role of the teacher? As you point out, a guide is still a guide. What if the role of adults lay in curating rich learning environments in which children can authentically explore the answers to their own questions, identify their passion, and achieve agency? This already exists in many alternative learning environments. From what I saw at the conference, the education revolution is already underway. The task is to reach the tipping point by helping parents and educators shed the brainwashing of the past.", 
        "title": "Self-Directed Learning \u2013 Judy Yero \u2013"
    }
]