[
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78?source=tag_archive---------0----------------", 
        "text": "Let\u2019s tackle this problem one step at a time. For each step, we\u2019ll learn about a different machine learning algorithm. I\u2019m not going to explain every single algorithm completely to keep this from turning into a book, but you\u2019ll learn the main ideas behind each one and you\u2019ll learn how you can build your own facial recognition system in Python using OpenFace and dlib.\n\nThe first step in our pipeline is face detection. Obviously we need to locate the faces in a photograph before we can try to tell them apart!\n\nIf you\u2019ve used any camera in the last 10 years, you\u2019ve probably seen face detection in action:\n\nFace detection is a great feature for cameras. When the camera can automatically pick out faces, it can make sure that all the faces are in focus before it takes the picture. But we\u2019ll use it for a different purpose\u200a\u2014\u200afinding the areas of the image we want to pass on to the next step in our pipeline.\n\nFace detection went mainstream in the early 2000's when Paul Viola and Michael Jones invented a way to detect faces that was fast enough to run on cheap cameras. However, much more reliable solutions exist now. We\u2019re going to use a method invented in 2005 called Histogram of Oriented Gradients\u200a\u2014\u200aor just HOG for short.\n\nTo find faces in an image, we\u2019ll start by making our image black and white because we don\u2019t need color data to find faces:\n\nThen we\u2019ll look at every single pixel in our image one at a time. For every single pixel, we want to look at the pixels that directly surrounding it:\n\nOur goal is to figure out how dark the current pixel is compared to the pixels directly surrounding it. Then we want to draw an arrow showing in which direction the image is getting darker:\n\nIf you repeat that process for every single pixel in the image, you end up with every pixel being replaced by an arrow. These arrows are called gradients and they show the flow from light to dark across the entire image:\n\nThis might seem like a random thing to do, but there\u2019s a really good reason for replacing the pixels with gradients. If we analyze pixels directly, really dark images and really light images of the same person will have totally different pixel values. But by only considering the direction that brightness changes, both really dark images and really bright images will end up with the same exact representation. That makes the problem a lot easier to solve!\n\nBut saving the gradient for every single pixel gives us way too much detail. We end up missing the forest for the trees. It would be better if we could just see the basic flow of lightness/darkness at a higher level so we could see the basic pattern of the image.\n\nTo do this, we\u2019ll break up the image into small squares of 16x16 pixels each. In each square, we\u2019ll count up how many gradients point in each major direction (how many point up, point up-right, point right, etc\u2026). Then we\u2019ll replace that square in the image with the arrow directions that were the strongest.\n\nThe end result is we turn the original image into a very simple representation that captures the basic structure of a face in a simple way:\n\nTo find faces in this HOG image, all we have to do is find the part of our image that looks the most similar to a known HOG pattern that was extracted from a bunch of other training faces:\n\nUsing this technique, we can now easily find faces in any image:\n\nIf you want to try this step out yourself using Python and dlib, here\u2019s code showing how to generate and view HOG representations of images.\n\nWhew, we isolated the faces in our image. But now we have to deal with the problem that faces turned different directions look totally different to a computer:\n\nTo account for this, we will try to warp each picture so that the eyes and lips are always in the sample place in the image. This will make it a lot easier for us to compare faces in the next steps.\n\nTo do this, we are going to use an algorithm called face landmark estimation. There are lots of ways to do this, but we are going to use the approach invented in 2014 by Vahid Kazemi and Josephine Sullivan.\n\nThe basic idea is we will come up with 68 specific points (called landmarks) that exist on every face\u200a\u2014\u200athe top of the chin, the outside edge of each eye, the inner edge of each eyebrow, etc. Then we will train a machine learning algorithm to be able to find these 68 specific points on any face:\n\nHere\u2019s the result of locating the 68 face landmarks on our test image:\n\nNow that we know were the eyes and mouth are, we\u2019ll simply rotate, scale and shear the image so that the eyes and mouth are centered as best as possible. We won\u2019t do any fancy 3d warps because that would introduce distortions into the image. We are only going to use basic image transformations like rotation and scale that preserve parallel lines (called affine transformations):\n\nNow no matter how the face is turned, we are able to center the eyes and mouth are in roughly the same position in the image. This will make our next step a lot more accurate.\n\nIf you want to try this step out yourself using Python and dlib, here\u2019s the code for finding face landmarks and here\u2019s the code for transforming the image using those landmarks.\n\nNow we are to the meat of the problem\u200a\u2014\u200aactually telling faces apart. This is where things get really interesting!\n\nThe simplest approach to face recognition is to directly compare the unknown face we found in Step 2 with all the pictures we have of people that have already been tagged. When we find a previously tagged face that looks very similar to our unknown face, it must be the same person. Seems like a pretty good idea, right?\n\nThere\u2019s actually a huge problem with that approach. A site like Facebook with billions of users and a trillion photos can\u2019t possibly loop through every previous-tagged face to compare it to every newly uploaded picture. That would take way too long. They need to be able to recognize faces in milliseconds, not hours.\n\nWhat we need is a way to extract a few basic measurements from each face. Then we could measure our unknown face the same way and find the known face with the closest measurements. For example, we might measure the size of each ear, the spacing between the eyes, the length of the nose, etc. If you\u2019ve ever watched a bad crime show like CSI, you know what I am talking about:\n\nOk, so which measurements should we collect from each face to build our known face database? Ear size? Nose length? Eye color? Something else?\n\nIt turns out that the measurements that seem obvious to us humans (like eye color) don\u2019t really make sense to a computer looking at individual pixels in an image. Researchers have discovered that the most accurate approach is to let the computer figure out the measurements to collect itself. Deep learning does a better job than humans at figuring out which parts of a face are important to measure.\n\nThe solution is to train a Deep Convolutional Neural Network (just like we did in Part 3). But instead of training the network to recognize pictures objects like we did last time, we are going to train it to generate 128 measurements for each face.\n\nThe training process works by looking at 3 face images at a time:\n\nThen the algorithm looks at the measurements it is currently generating for each of those three images. It then tweaks the neural network slightly so that it makes sure the measurements it generates for #1 and #2 are slightly closer while making sure the measurements for #2 and #3 are slightly further apart:\n\nAfter repeating this step millions of times for millions of images of thousands of different people, the neural network learns to reliably generate 128 measurements for each person. Any ten different pictures of the same person should give roughly the same measurements.\n\nMachine learning people call the 128 measurements of each face an embedding. The idea of reducing complicated raw data like a picture into a list of computer-generated numbers comes up a lot in machine learning (especially in language translation). The exact approach for faces we are using was invented in 2015 by researchers at Google but many similar approaches exist.\n\nThis process of training a convolutional neural network to output face embeddings requires a lot of data and computer power. Even with an expensive NVidia Telsa video card, it takes about 24 hours of continuous training to get good accuracy.\n\nBut once the network has been trained, it can generate measurements for any face, even ones it has never seen before! So this step only needs to be done once. Lucky for us, the fine folks at OpenFace already did this and they published several trained networks which we can directly use. Thanks Brandon Amos and team!\n\nSo all we need to do ourselves is run our face images through their pre-trained network to get the 128 measurements for each face. Here\u2019s the measurements for our test image:\n\nSo what parts of the face are these 128 numbers measuring exactly? It turns out that we have no idea. It doesn\u2019t really matter to us. All that we care is that the network generates nearly the same numbers when looking at two different pictures of the same person.\n\nIf you want to try this step yourself, OpenFace provides a lua script that will generate embeddings all images in a folder and write them to a csv file. You run it like this.\n\nThis last step is actually the easiest step in the whole process. All we have to do is find the person in our database of known people who has the closest measurements to our test image.\n\nYou can do that by using any basic machine learning classification algorithm. No fancy deep learning tricks are needed. We\u2019ll use a simple linear SVM classifier, but lots of classification algorithms could work.\n\nAll we need to do is train a classifier that can take in the measurements from a new test image and tells which known person is the closest match. Running this classifier takes milliseconds. The result of the classifier is the name of the person!\n\nSo let\u2019s try out our system. First, I trained a classifier with the embeddings of about 20 pictures each of Will Ferrell, Chad Smith and Jimmy Falon:", 
        "title": "Machine Learning is Fun! Part 4: Modern Face Recognition with Deep Learning"
    }, 
    {
        "url": "https://goberoi.com/comparing-the-top-five-computer-vision-apis-98e3e3d7c647?source=tag_archive---------1----------------", 
        "text": "Over the last six months, Google, Microsoft, and IBM have all announced a suite of \u201cintelligent APIs\u201d that offer various types of image, video, speech, and text recognition. One can, for instance, pass in a photograph of a day at the park, and receive tags that tell you it includes a dog, frisbee, and trees. How good are these out-of-the-box models for general purpose tasks, and can I use them to build interesting new products or features?\n\nIn this post, I compare the image labelling capabilities of five vendors with observations, and suggestions for product builders. I also provide an open-source tool, Cloudy Vision, so you can test your own images and pick the best vendor for you (spoiler: Google was not the clear winner for my corpus). Finally, I share labeling results for 40 example images.\n\nNote that I aim to get a qualitative feel for these APIs, this is not a rigorous accuracy study. I also focus on image labeling, not face detection or OCR [1].\n\nI started by taking a few photos, and running them through the web based testing tools provided by some vendors. My plan was to manually capture results in a spreadsheet. Alas, but this process was so tedious that I found myself fretting over which small set of images I should try out. It was also incomplete because not all vendors have such testing tools (ahem, Google).\n\nI quickly realized that to see side-by-side comparisons of lots of images, my best bet would be to build a little tool. I wrote a Python script to iterate over a directory of images, call the various vendor APIs, cache the responses, and render a web page to see the bits I\u2019m interested in.\n\nThe resulting tool, Cloudy Vision, presents image labeling results from Microsoft, Google, IBM, Clarifai, and Cloud Sight, but is easy to extend to support more vendors (please send me a pull request). If you have a corpus of images and want to explore labeling, this is a good starting point for qualitative assessment, as well as for more rigorous accuracy testing (e.g., compare computed labels with your own training set).\n\nI ran about forty images through five vendors. I recommend viewing that page now to form your own impressions before going further. Here are some of the things I noticed:\n\nI\u2019m impressed with the quality of general classification\u200a\u2014\u200ait\u2019s good enough to get the gist of an image, is fast, and relatively low cost at scale.\n\nThat said, it\u2019s hard to evaluate a solution without a real problem. For instance, what use is general classification (e.g. \u201cthis is food\u201d) if what I need is details for my use-case (e.g., \u201cmy recipe site needs to know if this is a slab of meat, or a vegetable\u201d). With that in mind, let me posit a few hypotheses on when I think you may find these APIs worth your time.\n\nAreas where these APIs may be immediately useful:\n\nSpaces where these APIs won\u2019t be enough:\n\nThanks for reading this far. As a reward, I present you with one of the funnier labeling results: my own profile photo. I know I\u2019m not the tallest guy around, but Google, did you really have to label me as a jockey? Come on\u2026", 
        "title": "Comparing the Top Five Computer Vision APIs \u2013"
    }, 
    {
        "url": "https://medium.com/@awjuliani/simple-reinforcement-learning-with-tensorflow-part-3-model-based-rl-9a6fe0cce99?source=tag_archive---------2----------------", 
        "text": "What is a model and why would we want to use one? In this case, a model is going to be a neural network that attempts to learn the dynamics of the real environment. For example, in the CartPole we would like a model to be able to predict the next position of the Cart given the previous position and an action. By learning an accurate model, we can train our agent using the model rather than requiring to use the real environment every time. While this may seem less useful when the real environment is itself a simulation, like in our CartPole task, it can have huge advantages when attempting to learn policies for acting in the physical world.\n\nUnlike in computer simulations, physical environments take time to navigate, and the physical rules of the world prevent things like easy environment resets from being feasible. Instead, we can save time and energy by building a model of the environment. With such a model, an agent can \u2018imagine\u2019 what it might be like to move around the real environment, and we can train a policy on this imagined environment in addition to the real one. If we were given a good enough model of an environment, an agent could be trained entirely on that model, and even perform well when placed into a real environment for the first time.\n\nHow are we going to accomplish this in Tensorflow? As I mentioned above, we are going to be using a neural network that will learn the transition dynamics between a previous observation and action, and the expected new observation, reward, and done state. Our training procedure will involve switching between training our model using the real environment, and training our agent\u2019s policy using the model environment. By using this approach we will be able to learn a policy that allows our agent to solve the CartPole task without actually ever training the policy on the real environment! Read the iPython notebook below for the details on how this is done.\n\nSince there are now two network involved, there are plenty of hyper-parameters to adjust in order to improve performance or efficiency. I encourage you to play with them in order to discover better means of combining the the models. In Part 4 I will be exploring how to utilize convolutional networks to learn representations of more complex environments, such as Atari games.", 
        "title": "Simple Reinforcement Learning with Tensorflow: Part 3 - Model-Based RL"
    }, 
    {
        "url": "https://medium.com/learning-machine-learning/getting-tensorflow-theano-and-keras-on-windows-70c18f2c533b?source=tag_archive---------3----------------", 
        "text": "Updated\u00a0: Since writing this tensorflow for windows came out and my workflow completely changed, so I recommend just using keras on top of Tensorflow for deep learning. I\u2019m also updating the relevant parts with that information in mind. Also, I\u2019m using Python 3.5 currently because Tensorflow is not available for Windows in any other version. Either way, if you need 2.7, just get an instance from AWS, they are pretty cheap or switch your OS ^___^\n\nFrom the official documentation of Theano it\u2019s mentioned that Anaconda installs all dependencies of Theano.\n\nFrom the docs we see\u00a0:\n\nFor the latest stable release 0.7 (as of March 2015) run instead:\n\nEither way, a folder Theano will be created with the library downloaded to it.\n\nNote\u00a0: There\u2019s some test scripts in the documentation too.\n\nNote\u00a0: You must have Python 3.5 for it. I\u2019m using the Anaconda distribution from Continuum in a separate environment.\n\nKeras currently is the official API for tensorflow. Keras can be configured to run with Tensorflow or Theano on the backend. Just follow the instructions from here\u00a0: \u201cSwitching from Tensorflow to Theano\u201d\u00a0. Installation is basically a one-liner.\n\nThe end. This doc will probably not be actively maintained unless needed.", 
        "title": "Getting Tensorflow, Theano and Keras on Windows \u2013 Learning Machine Learning \u2013"
    }, 
    {
        "url": "https://medium.com/all-of-us-are-belong-to-machines/gentlest-introduction-to-tensorflow-part-2-ed2a0a7a624f?source=tag_archive---------4----------------", 
        "text": "Summary: We show in illustrations how the machine learning \u2018training\u2019 process happens in Tensorflow, and tie them back to the Tensorflow code. This paves the way for discussing \u2018training\u2019 variations, namely stochastic/mini-batch/batch, and adaptive learning rate gradient descent. The \u2018training\u2019 variation code snippets presented serve to reinforce the understanding of the role of Tensorflow placeholders.\n\nThis is part of a series:\n\nIn the previous article, we used Tensorflow (TF) to build and learn a linear regression model with a single feature so that given a feature value (house size/sqm), we can predict the outcome (house price/$).\n\nHere is the review with illustration below:\n\nIn machine learning (ML) literature, we come across the term \u2018training\u2019 very often, let us literally look at what that means in TF.\n\nThe goal in linear regression is to find W, b, such that given any feature value (x), we can find the prediction (y) by substituting W, x, b values into the model.\n\nHowever to find W, b that can give accurate predictions, we need to \u2018train\u2019 the model using available data (the multiple pairs of actual feature (x), and actual outcome (y_), note the underscore).\n\nTo find the best W, b values, we can initially start with any W, b values. We also need to define a cost function, which is a measure of the difference between the prediction (y) for given a feature value (x), and the actual outcome (y_) for that same feature value (x). For simplicity, we use least minimum squared error (MSE) as our cost function.\n\nBy minimizing the cost function, we can arrive at good W, b values.\n\nOur code to do training is actually very simple and it is labelled with [A, B, C, D], which we will refer to later on. The full source is on Github.\n\nOur linear model and cost function equations [A] can be represented as TF graph as shown:\n\nNext, we select a datapoint (x, y_) [C], and feed [D] it into the TF Graph to get the prediction (y) as well as the cost.\n\nTo get better W, b, we perform gradient descent using TF\u2019s tf.train.GradientDescentOptimizer [B] to reduce the cost. In non-technical terms: given the current cost, and based on the graph of how cost varies with other variables (namely W, b), the optimizer will perform small tweaks (increments/decrements) to W, b so that our prediction becomes better for that single datapoint.\n\nThe final step in the training cycle is to update the W, b after tweaking them. Note that \u2018cycle\u2019 is also referred to as \u2018epoch\u2019 in ML literature.\n\nIn the next training epoch, repeat the steps, but use a different datapoint!\n\nUsing a variety of datapoints generalizes our model, i.e., it learns W, b values that can be used to predict any feature value. Note that:\n\nYou can train the model a fixed number of epochs or until it reaches a cost threshold that is satisfactory.\n\nIn the training above, we feed a single datapoint at each epoch. This is known as stochastic gradient descent. We can feed a bunch of datapoints at each epoch, which is known as mini-batch gradient descent, or even feed all the datapoints at each epoch, known as batch gradient descent. See the graphical comparison below and note the 2 differences between the 3 diagrams:\n\nThe number of datapoints used at each epoch has 2 implications. With more datapoints:\n\nThe pros and cons of doing stochastic, mini-batch, batch gradient descent can be summarized in the diagram below:\n\nTo switch between stochastic/mini-batch/batch gradient descent, we just need to prepare the datapoints into different batch sizes before feeding them into the training step [D], i.e., use the snippet below for[C]:\n\nLearn rate is how big an increment/decrement we want gradient descent to tweak W, b, once it decides whether to increment/decrement them. With a small learn rate, we will proceed slowly but surely towards minimal cost, but with a larger learn rate, we can reach the minimal cost faster, but at the risk of \u2018overshooting\u2019, and never finding it.\n\nTo overcome this, many ML practitioners use a large learn rate initially (with the assumption that initial cost is far away from minimum), and then decrease the learn rate gradually after each epoch.\n\nTF provides 2 ways to do so as wonderfully explained in this StackOverflow thread, but here is the summary.\n\nTF comes with various gradient descent optimizer, which supports learn rate variation, such as tf.train.AdagradientOptimizer, and tf.train.AdamOptimizer.\n\nAs you have learned previously, if we declare a tf.placeholder, in this case for learn rate, and use it within the tf.train.GradientDescentOptimizer, we can feed a different value to it at each training epoch, much like how we feed different datapoints to x, y_, which are also tf.placeholders, at each epoch.\n\nWe illustrated what machine learning \u2018training\u2019 is, and how to perform it using Tensorflow with just model & cost definitions, and looping through the training step, which feeds datapoints into the gradient descent optimizer. We also discussed the common variations in training, namely changing the size of datapoints the model uses for learning at each epoch, and varying the learn rate of gradient descent optimizer.", 
        "title": "Gentlest Introduction to Tensorflow #2 \u2013 All of Us are Belong to Machines \u2013"
    }, 
    {
        "url": "https://medium.com/artists-and-machine-intelligence/a-journey-through-multiple-dimensions-and-transformations-in-space-the-final-frontier-d8435d81ca51?source=tag_archive---------5----------------", 
        "text": "These are eigenfaces, a very simple demonstration of this. Going into detail as to how they\u2019re generated is beyond the scope of my talk right now, but I\u2019ll spend just a couple of minutes to give a very rough overview for the technically curious.\n\nWhat does that mean?\n\n(This bit can be skipped if a. you\u2019re already comfortable with PCA or b. you don\u2019t really care. If this section isn\u2019t clear, don\u2019t worry about it as afterwards I\u2019ll summarise the conceptual significance, which is the bit that really matters).\n\nImagine we have a bunch of 3D data, and we plot them in 3D space, we get something that resembles a point cloud.\n\nNow this point cloud might be perfectly spherical, but it\u2019s more likely to be kind of elongated and blobby like in this image. The point cloud in this image is quite elliptical and is oriented in a particular direction. The directions in which the data point cloud is elongated in are called the principle components (or eigenvectors). And Principle Component Analysis (PCA) is a method of finding these directions in which the data is most \u2018elongated\u2019. Then we can define these directions as new axes, and project our data into that new axis system. I.e. transform it.\n\nBut here\u2019s an important detail: PCA finds the directions of elongations (eigenvectors), and how big each \u2018elongation\u2019 is (eigenvalue) along that direction. We can then choose to omit any directions (eigenvectors) where the elongation (eigenvalue) isn\u2019t that significant i.e. If it\u2019s quite flat in a particular directions.\n\nE.g. Imagine we plot a bunch of data in 3D space, and it turns out to be (almost) flat like a piece of cardboard, but tilted at an angle. If we can calculate that angle, we can transform our coordinate system, and reduce it to 2D. That\u2019s exactly what we can do with PCA: reduce dimensions by transforming the data to a new axis system, one which potentially represents the data more optimally.\n\nAnd as always, this works in any number of dimensions. If we have 100D data in 100D space, it might also have elongations. In fact, because it\u2019s in high dimensions, it will probably have many elongations in many different directions and dimensions. PCA will find all of these elongations. In fact PCA will return the same number of elongations as there are original dimensions. I.e. For a 100D space, PCA will return a new set of 100 directions (axes). But these 100 axes will be rotated to fit our data more optimally. Most importantly, because we know how much elongation there is on each direction (the eigenvalue corresponding to the eigenvector) we can sort the axes by elongation amount. The first axis will have the most elongation (pointing in the direction of most variance), second axis will have the second most elongation (pointing in the direction of second most variance), etc. and the last (100th) axis will have the least amount of elongation. This means that we can choose an arbitrary cutoff point (for amount of variance), and just ignore the axes (dimensions) beyond that cut off point. The same way that we can transform 3D data that is \u2018flat\u2019 into 2D (by finding the most \u2018important\u2019 set of 2D axes), we can transform 100D data into say 20D data\u200a\u2014\u200aby finding the most \u2018important\u2019 set of 20D axes.\n\nIf this wasn\u2019t very clear, it doesn\u2019t matter. Understanding what exactly PCA does isn\u2019t the purpose of my talk. This bit was only for those who were interested and might have already seen this before. The important thing is to understand the implications of this which I will explain next.\n\nLet\u2019s take our dataset of face images (which we assume to be 32 x 32 pixels, so that it ties in with our previous discussion). Remember that every single 32 x 32 BW image is a single point in 1024D pixel space. If we plot all of our face images, we get a point cloud in 1024D. We can run PCA on this 1024D dataset and choose an arbitary number of dimensions (i.e. axes) to reduce it to.\n\nE.g. If we were to choose the top 24 dimensions, we might get something like this\n\nEach one of these \u2018face\u2019 images, is an eigenvector of this dataset, i.e. the \u2018directions\u2019 in which our dataset point cloud is most elongated in 1024D. These are the new axes which represent our data set more optimally, in a more compact manner.\n\nWhat does it even mean for \u2018an image to be an axis\u2019? Well, remember that in our 1024D space each point is an image. So each of these images here, is also a point in 1024D space. It\u2019s a vector. And eigenface image 1 is our new axis 1, eigenface image 2 is our new axis 2, eigenface image 3 is our new axis 3\u2026 eigenface image 24 is our new axis 24 etc.\n\nAnd this is conceptually really significant. Because first we discussed a 1024D pixel space. In that space, each axis (i.e. feature) corresponds to a pixel in a 32 x 32 grid\u200a\u2014\u200ai.e. the features of the space are pixels.\n\nNow (after PCA / Eigenfaces) we have a new coordinate system (i.e. new axes, which are somehow rotated in space) to fit our particular dataset better. These new axes constitute a 24D latent space\u200a\u2014\u200aI call it latent space because it\u2019s features (i.e. axes) are not directly observable. And these latent features are how much an input image resembles the eigenfaces. I.e. these are what the axes of this new latent space represent.\n\nI\u2019ll give an example to try and make this a bit clearer.\n\nThis image of Hedy Lamarr has a pixel representation, a 1024D vector of pixel values. How do we transform it from 1024D pixel space to 24D latent space? How do we find its representation in this latent space? i.e. the 24D vector of latent features? How do we encode it?\n\nWith this particular latent space (i.e. the one we constructed via eigenfaces and PCA), it\u2019s very simple.\n\nWe take the image (cropped and resized to 32 x 32, so it\u2019s a 1024D vector) and dot product it with the first eigenface (which is also a 1024D vector), that will give us a number, how much the image \u2018resembles\u2019 the first eigenface. That\u2019s the value of our first latent feature. i.e. if we were to plot this 24D representation as a point in 24D latent space, that\u2019s the distance we would go along the first of the 24 axes. We then dot product the image with the second eigenface, that number will give us the second latent feature, i.e. the distance to go along the second axes. Etc. All the way to the 24th eigenface and the last latent feature (i.e. axis).\n\nIf you\u2019re not familiar with dot products etc and this bit wasn\u2019t clear, it doesn\u2019t matter. The most crucial thing here is:\n\nIt might turn out that this image of Lamarr is 24% 1st eigenface, 12% 2nd eigenface, -31% 3rd eigenface,\u00a0\u2026, 17% 24th eigenface etc. Or in a more compact syntax: [0.24, 0.12, -0.31,\u00a0\u2026 0.17]. That\u2019s only 24 numbers! We would call each of these 24 numbers, the latent features of this image (in this particular latent space), and the vector (i.e. list) of 24 numbers is a representation of this image in (this particular) latent space.\n\nIf we have a representation of an image in this 24D latent space, i.e. a vector of 24 latent features, how can we reconstruct the original image? I.e. transform from 24D latent space back to 1024D pixel space? I.e. decode it?\n\nRemember that the latent features in this space are simply how much an image resembles each eigenface. So we simply multiply each of the 24x eigenfaces with the value of the corresponding latent feature, and add them up. I.e. for each pixel, we do:\n\nThat\u2019s it. The resulting 1024D vector is the pixel representation.\n\nThis is a huge compression of information.\n\nIf I want to send you a picture of a face, I don\u2019t need to send you all of the pixels of the image, i.e. a pixel representation, i.e. a vector of 1024 pixel features. I can just transform my image from 1024D pixel space into 24D latent space. I encode it. And then I can send you just the 24 numbers, the latent representation, a vector of 24 latent features. Of course you need a way of decoding those 24 numbers, transforming from latent space back to pixel space. If you already have these eigenfaces handy, then you can easily transform back to pixel space as I described before.\n\nBut if you don\u2019t have the eigenfaces, then I\u2019d need to send them to you first, and that would be very inefficient for just one picture.\n\nBut if you don\u2019t have the eigenfaces, and I want to send you a million 32 x 32 face images, sending pixel representations for all images would take up 1GB (1,000,000 images * 1,024 pixels per image, assuming 1 byte per pixel). Alternatively I could send you the pixel representations of the eigenfaces first which would be 24KB (24 images * 1,024 pixels per image). Then I could send the latent representations for each of the million faces which would be roughly 24MB (1,000,000 images * 24 latent features per image, assuming 1 byte per latent feature). A massive compression.\n\nThere is a catch associated with this. This is a lossy compression. Very lossy. If we take an image (e.g. Hedy Lamarr) and encode it, i.e. transform from 1024D pixel space to 24D latent space, we will end up with a 24D vector of latent features, a representation in 24D latent space. If we decode that, i.e. transform it from latent space back into 1024D pixel space, we will end up with an image again. We could call this a reconstructed image. But the reconstructed image will not necessarily be identical to the original input image (e.g. Lamarr). The \u2018difference\u2019 between the original input image and the reconstructed image is the error (of this encoding-decoding). There are many different ways of measuring this \u2018difference\u2019, and it depends on the domain. For an image like this, we could simply take the difference between all of the pixels and add them up (L1 Norm) or we could measure the euclidean distance in 1024D space (L2 Norm) etc. (For a more complicated, probabilistic model it\u2019s more common to look at the \u2018difference\u2019 between probability distributions, e.g. using something like KL divergence).\n\nThere are two main reasons for this error:\n\nLet\u2019s also remember that this 24D representation, the vector of 24 latent features, are coordinates in a 24D space. So each 32 x 32 pixel face image can be thought of as a point in 24D latent space (in addition to being a point in 1024D pixel space). Now what happens when we perform geometric operations in this 24D space? If we average two points (i.e. two latent representations of face images)?\n\nThe method I just described was using PCA and eigenfaces. PCA is a method dating back to 1901, and was applied to faces in 1987. It\u2019s quite old, really not state of the art at all. Also PCA is a linear dimensionality reduction technique. I.e. the new (latent) features are linear combinations of the original features (e.g. pixels).\n\nwhere all Kx_y are constants. PCA\u2019s job is to find those constants.\n\nSo PCA won\u2019t find complicated, intricate manifolds (i.e. crumpled pieces of paper, or intricate mountain ridges), or even slightly curved manifolds (like the surface of bowl). It will only find completely \u2018flat\u2019 manifolds (like a flat piece of cardboard). So even though finding midpoints of multiple image representations in this new latent space will be more interesting than doing it in the 1024D space, the results will still be linear combinations and not terribly exciting. However\u2026\n\nI only showed and spent so much time on PCA / Eigenfaces because they\u2019re relatively easier to visualise and understand what\u2019s going on under the hood (compared to the \u2018black-box\u2019 of neural networks).\n\nThere are many other, totally different methods which essentially do what we want here, which is to\u2026\n\nIn the next sections I\u201dll talk about a few other methods which are considerably more complicated under the hood, so I won\u2019t go into so much detail on how they work. I\u2019ll focus mainly on the end result and how they work on a conceptual level.\n\nBut first I want to underline a few things:", 
        "title": "A journey through multiple dimensions and transformations in SPACE"
    }, 
    {
        "url": "https://gab41.lab41.org/i-need-an-ai-bs-meter-27e94d48c8c1?source=tag_archive---------6----------------", 
        "text": "We talk to a lot of analysts at Lab41. \u200bA recurring theme of these conversations is what they frequently refer to as \u201cresult provenance.\u201d Translation\u200a\u2014\u200a\u201cAre these results any good? Can I trust them? I don\u2019t have a whole lot of time to research them any further, so will these results hold up under scrutiny?\u201d\n\nThese questions aren\u2019t new. The old adage about lies, damn lies, and statistics has been around for just about forever. These questions become a lot harder to answer though with the advent of semi-automated analytic techniques such as Machine and Deep Learning. Few data scientists, let alone your average analyst can answer these questions in a world dominated by such concepts as cold-start, back-propagation, hyper-parameters, sigmoid smoothing functions, batch normalization, yada, yada, yada. The immediate gratification of rapid results are tempered, and complicated by the reality that it is hard to know what data was used to train the network. Familiarity with the given data also may not be sufficient as one of Deep Learning\u2019s greatest attractions is how it discerns discriminating features humans were not aware of. Validating that an autonomous vehicle is adhering to rules of the road is one thing. Knowing that the model chose the right peptide sequence from a set with many thousands of subtle protein differences may be an entirely different matter. These issues can only worsen, as ML/DL techniques become more pervasive. This is not to disparage ML and DL, quite the contrary. The concern here is more about how best to deal with the side effects of increased complexity and opacity that accompany these powerful technologies.\n\nOne way experts try to deal with uncertainty is to publish various types of quality/trust scores with the results. This type of self-attestation has limited value, however. As has been documented repeatedly in the cyber security field, entities vouching for themselves cannot always be trusted to tell the truth. This is how a lot of malware propagates across a network. Ideally such self-attestation should be corroborated by a third party (or multiple).\n\nSo what is an analyst to do?\u200b How do they know if the models they are using are the right ones for the data at hand? How do they detect and compensate for training bias or model over fitting? At Lab41 we have developed frameworks such as Circulo (where we built a tool to help analysts make better decisions on which community detection algorithm to use) and studied recommender systems to see if we can do a better job of matching data and algorithms. Recent research focused on what a neural net is paying attention to at inference time also looks very promising and hopefully the technique will transfer over to other data types.\n\nMy goal here is not to come up with an answer. Rather, what I hope I have done is sufficiently piqued your interest in this aspect of ML and DL that we want to mitigate.\n\nLab41 is a place where experts from the U.S. Intelligence Community (IC), academia, industry, and In-Q-Tel come together to gain a better understanding of how to work with\u200a\u2014\u200aand ultimately use\u200a\u2014\u200abig data.", 
        "title": "I need an AI BS-Meter \u2013"
    }, 
    {
        "url": "https://chatbotsmagazine.com/alterra-ai-launches-flight-reservation-in-conversational-bot-interface-powered-by-artificial-501ae460a54a?source=tag_archive---------7----------------", 
        "text": "Our Alterra.ai today announces the launch of flight reservations in its Facebook Messenger bot. Surprisingly, it was done virtually without traditional coding\u2026\n\nAlterra\u2019s Marina is a virtual travel agent. She can recommend travelers where to go on vacation and what to see there. She can then book hotels, and now flights:\n\nMarina understands plain English. Users can chat with her as with a human, almost\u00a0:-). However, Marina is 100% powered by AI algorithms. No humans are involved, whatsoever.\n\nTraditionally, natural language understanding involved a lot of coding. You have to anticipate words and phrases a user could say in each context, recognize patterns of speech, extract predefined keywords, etc. In other words: a lot of rules, a lot of hard-coding (this is how we did hotel booking earlier). It takes a lot of time and effort to write and debug such programs. And they quickly break down when the user deviates from the expected path. This is why many bots feel so dumb.\n\nInstead of writing explicit rules you create an artificial neural network and feed it a training corpus\u200a\u2014\u200ain our case transcribed recordings of conversations between travelers and live travel agents\u200a\u2014\u200athis artificial brain learns how to understand the language, similar to how a rat learns how to navigate a maze, or more realistically, how a fruit fly learns how to find bananas. It is how Google\u2019s DeepMind built its famous Go-playing program. It is how we did our flight booking.\n\nConstructing an artificial neural network is not straightforward. You have to experiment with different algorithms and ways to stitch them together. (We ended up with 50-dimensional Glove word embedding, two bi-directional residual LSTMs, and a CRF on top. It achieves 98% per-label precision and 99% recall, with 26 labels.)\n\nOn the bright side, when you\u2019re done you get a machine that magically understands natural language. If you notice that it makes certain mistakes you don\u2019t have to write new code to fix it\u200a\u2014\u200ayou just give it more examples to learn from. You don\u2019t have to rewire the bot\u2019s brain; you teach it.\n\nMarina is able to understand rather complex requests like these:\n\n- Book me a 3 or 4-star hotel room in London from Aug 15 for 5 nights under $200 with free wi-fi and gym\n\n- Please find me 2 roundtrip economy tickets from Chicago to London on Star Alliance next Sunday back on Oct 5 with 1 stop or less\n\n- What to see on Corsica?\n\n- Give me ideas for a 7 day inexpensive comfortable vacation in unspoiled nature with good food & wine\n\nOr you may start with just \u201chotel in London\u201d or \u201cflight from Chicago to London\u201d and Marina will ask you all the necessary questions.\n\nFor fulfillment of bookings, we didn\u2019t reinvent the wheel. Marina connects to existing flight and hotel booking engines. It could be Expedia, Booking.com, United Airlines, Hilton Hotels\u200a\u2014\u200ayou name it. Marina provides a conversational front-end; it is back-end agnostic. (In fact, our business model is to license Marina to these travel brands.)\n\nFor vacation recommendations, however, we did invent our own wheel. We developed a structured data set of major vacation destinations complete with themes, available activities, major attractions, climate, expensiveness, etc. We call it the \u201ctravel genome project\u201d.\n\nAlterra.ai is a Deep Learning start-up, building bots for online travel and beyond. It is headquartered in Palo Alto, CA, and led by Sergei Burkov, PhD, an ex-Googler and serial entrepreneur. His previous start-up was acquired by Google, where he became the first head of its Moscow R&D Center. Our chief algorithms dude is another ex-Googler, Max Ushakov.", 
        "title": "Alterra.ai launches flight reservation in conversational bot interface, powered by artificial\u2026"
    }, 
    {
        "url": "https://gab41.lab41.org/tensorflow-3-ways-46a46bef895d?source=tag_archive---------8----------------", 
        "text": "I recently started my first real project in Tensorflow. The larger effort, Attalos, was aimed at exploring vector spaces containing multiple types (modalities) of data. This post is about work that I did looking into how to project image features into a word vector space. The goal of the project is to be able to do image search more effectively.\n\nThis article isn\u2019t about getting started with Tensorflow and it\u2019s not directly about the projection that I was attempting to do. Instead, this article is a walk down the path I took in implementing a regression with neural networks in Tensorflow. Specifically, it\u2019s about finding some of the higher-level abstractions I loved in Keras in tensorflow.contrib (and yes, I know Keras can run on Tensorflow).\n\nI mentioned above that I was trying to project image features into a word vector space. More specifically I had extracted 2048 dimensional vectors of image features from an Inception v3 model. The task I was completing was to regress those features into a 300 dimensional word vector space based on GloVe.\n\nI ended up doing the same regression three different ways (at different abstraction levels):\n\nThe first approach had its origins in Theano (another deep learning library Lab41 has previously used). In Theano (and Tensorflow) the user is responsible for everything. You define a graph of computation that is merely a thin layer on top of a matrix math library.\n\nIn order to represent our regression in Tensorflow (or Theano) it\u2019s best to first mathematically represent our calculation:\n\nFor the loss function (what we are trying to optimize) I used mean squared error:\n\nIn raw Tensorflow the graph setup looks like the code snippet below. We start by defining our variables (input image features, target word vectors, and the weights). We then create our computation graph (in this case a two layer perceptron). Finally we setup our optimization by defining a loss function and then specifying an optimizer for that loss. All together that creates:\n\nThe above approach worked but I was left thinking that I surely can\u2019t be the only person who was interested in a dense, fully-connected neural network and it seemed silly (and error prone) to start from scratch. (Maybe I was just bitter because I forgot to initialize my network properly and learned that a vector of all zeros is actually a local minimum for my problem).\n\nAfter a little reading I learned about tensorflow.contrib. This is a place where new ideas can get tested before they get integrated into the core library. Try two borrows from the \u201clayers\u201d portion of contrib, which gives us a Keras like abstraction for layers.\n\nOur network from above now looks like this:\n\nAs a non-deep learning expert I can look at this and reason about what\u2019s happening (vs. the code from the 1st approach where I find myself counting sigmoids to figure out the depth of the network).\n\nThe above approach gives you almost all of the flexibility from the 1st approach while avoiding some of the pitfalls and ending up with something a little easier to approach. Try 3 borrows from another part of the contrib section, \u201clearn\u201d which is based on the SkFlow work.\n\nSpecifically I can define the architecture for a neural network based regression:\n\nIf you\u2019ve used scikit-learn before this interface looks familiar. You define your model, call a fit function, and then a predict function after that. Having a scikit-learn like interface to deep learning primitives makes deep learning much more accessible but at the expense of flexibility. This also means that it is easy to replace calls to scikit-learn with calls to \u201clearn\u201d functions.\n\nGoing through this process there were a few things that struck me:\n\nThe three approaches represent different abstraction levels and which is best really depends on your interest and what you are trying to do. In general the \u201clayers\u201d library provides me with the abstraction level that most matches the way I think about neural networks. It gives me the flexibility to connect things way I want without too much boilerplate code. The \u201clearn\u201d library is great if what you\u2019re interested in is DNN classification/regression or if you were already using scikit-learn and wanted to maintain that interface.", 
        "title": "Tensorflow 3 Ways \u2013"
    }, 
    {
        "url": "https://medium.com/@Zelros/how-deep-learning-solved-phase-1-of-the-data-science-game-2712b949963f?source=tag_archive---------9----------------", 
        "text": "The selective phase of the Data Science Game is now over. More than 140 registered teams, from more than 30 countries participated in this first stage, hosted on Kaggle In Class.\n\nThe goal was to classify satellite images of roofs, to identify their orientation (this could be used to estimate the solar energy potential of a country).\n\nWe had the opportunity to meet the two best teams at our office in Paris\u00a0:\n\nHere is what we learned.\n\nWe met at the UPMC Data Science master, and currently work together in the same lab and same office. It was natural to build a team, and it was easy for us to meet physically everyday during the competition to organize our strategy.\n\nWe were all students at Ecole Polytechnique, at different levels. We received a message through Facebook, inviting us to participate. That\u2019s how we initially build the team. And we also used Facebook to communicate during the challenge\u00a0!\n\nAn other team from Polytechnique also finished among the 20 best teams selected for the final challenge. Unfortunately, only one team per university is allowed to be finalist. It\u2019s a pity, because they were very serious candidates for the challenge title\u00a0!\n\nWhen we discovered the data and the challenge, Remi naturally took the lead of the machine learning part\u00a0: indeed he already worked on similar problems of image classification, had the tools and proper code framework to start from. The rest of the team worked on data analysis and visualization, features extraction, data augmentation and stacking.\n\nThe work breakdown was quite natural\u00a0: Guillaume was the most experienced, and had the suitable hardware\u200a\u2014\u200aso he was responsible for running the neural networks itself. The rest of the team was mainly in charge of data preparation, data augmentation, and ensembling (see below).\n\nWe spent the first three weeks brainstorming and reading papers. Our conclusion was that Inception V3 (GoogleNet) seemed to present better transfer learning properties than VGG\u200a\u2014\u200athat\u2019s why we decided to focus on it for the rest of the competition.\n\nWe used a Torch implementation optimized by Remi, and Nvidia cuDNN over a cluster of 4 TitanX GPUs. We started from a model pre-trained on ImageNet, and fine tuned it on the images set of the challenge.\n\nWe augmented the training dataset with images transformations (flips, rotations,\u00a0\u2026), but didn\u2019t use the unlabeled images. Indeed our thought was that labelling these images would not help the models\u00a0: it would provide no extra information (for the images trivially labeled by the models), or noise (for the images incorrectly labels by the models).\n\nWe had a lot of variance in our cross-validation score. To reduce it, we used bagging of bootstrapped models. We also generated models with different hyperparameters, to create diversity. Our final submission was a blend of almost 100 models.\n\nOur cornerstone was the VGG very deep ConvNet, from the University of Oxford (the 19 layers flavour). We started with a network pre-trained on ImageNet, and then fine-tuned the weights by continuing the backpropagation on data provided for the challenge (transfer learning).\n\nTo have more training data, we used data augmentation technics to extend the available images, like flips or 90\u00b0 / 180\u00b0 / 270\u00b0 rotations for example.\n\nWhat\u2019s more, as there were unlabeled images, we used a semi-supervised technic\u00a0: we labelled the unlabeled images with our classifiers, and used this newly labeled data to augment our training test. Surprisingly, this didn\u2019t improved results a much as we would have expected.\n\nUsing GPUs helped us to iterate quicker. For a given set of hyperparameters it took around 30 min to train a model.\n\nTo blend our models, we didn\u2019t use an average of the soft decisions. Instead, we used a majority vote of the hard decisions\u200a\u2014\u200awhich worked better for us.\n\nWe also tweaked the predictions of our final submission to counteract the fact that the train set had unbalanced classes, while the test set was balanced. This improved a bit our score, even though without this trick we still were ranked first.\n\nTuning Deep Neural Networks is a lot about experience and know-how. Even if you know the theory, you need to have tuned dozens of networks before, to get the best of it. Our previous experiments with this technology helped us a lot. But more than that, without GPUs we won\u2019t have been able to reach this performance.\n\nStacking different models also helped us to improve accuracy (at the end we had 84 models\u00a0: 12 root models, trained on 7 different versions of the images). Our best submission was a classifier trained on the predictions of these 84 models.", 
        "title": "How Deep Learning Solved Phase 1 Of The Data Science Game"
    }
]