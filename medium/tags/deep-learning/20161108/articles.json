[
    {
        "url": "https://medium.com/@olivercameron/20-weird-wonderful-datasets-for-machine-learning-c70fc89b73d5?source=tag_archive---------0----------------", 
        "text": "They say great data is 95% of the problem in machine learning. We saw first hand at Udacity that this is the case, with the amazing reception from the machine learning community when we open sourced over 250GB of driving data. But, finding interesting data is really hard, and actively holds the industry back from progress. In trying to learn more about this problem I searched far and wide, and cataloged just a sliver of the datasets I found.\n\nIn the hope that others might find this catalog useful, here\u2019s 20 weird and wonderful datasets you could (perhaps) use in machine learning.\n\nCaveat: I haven\u2019t validated that all of these datasets are actually useful for machine learning (in terms of size or accuracy). Use your own judgement when playing with them (and check licenses)!\n\nI\u2019ve also been fascinated with the militarized interstates disputes dataset, which includes 200 years of international threats and conflicts. It includes the action taken, level of hostility, fatalities, and outcomes.\n\nIf you have any thoughts, questions, or datasets you\u2019d like to share, I\u2019d love to hear from you in Tweet-form. You can follow and message me at @olivercameron.", 
        "title": "20 Weird & Wonderful Datasets for Machine Learning \u2013 Oliver Cameron \u2013"
    }, 
    {
        "url": "https://medium.com/@vivek.yadav/wx-b-vs-xw-b-why-different-formulas-for-deep-neural-networks-in-theory-and-implementation-a5ae6995c4ef?source=tag_archive---------1----------------", 
        "text": "In most neural networks textbooks, neural networks combine features using, y = WX+B or y=W\u2019X+B, however, in tensorflow and theano implementations, neural networks are implemented as y = XW+B. I spent a lot of time investigating the reason for this discrepancy, and came up with this. TL;DR:I think its an implementation issue, computing derivatives for y=XW+B is easier than y=WX+B\n\nTheoretically, W\u2019X+B (or WX+B) is how neural network math is presented in books etc, which is equivalent to XW+B (taking transpose). However, while computing derivatives, the two formulations are not equivalent. Note WX+B is a vector, and we are taking its derivative with respect to a matrix (W). The derivative of a vector with respect to matrix gives a tensor. This tensor is computed by taking each element in y, and taking its derivative with each element in W, and storing that information in i X j X k element of a 3-D tensor, where i is the index of the element in y, and j and k are indices of elements in W. So this tensor results in computing additional matrices that need to be computed and stored. Eq 74 in matrix cookbook, http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf\u00a0.\n\nUsing XW+B, derivative of the function with respect to each element in W is much easier. Its simply X with some elements zero. No need to compute transposes of data and store tensor derivatives, and when ever you want you can compute the derivative by taking a matrix of size weights and putting each element in it equal to the input X with some elements zero.\n\nI think this is the main reason to use XW+B vs WX+B. Below is derivation of derivative for y = XW and WX (dropped bias). I think XW implementation is better suited for numpy and python packages, hence the use of XW instead of WX. Please feel free to add more. If you agree or disagree or have any other opinion, please feel free to add more information.", 
        "title": "WX+b vs XW+b, why different formulas for deep neural networks in theory and implementation?"
    }, 
    {
        "url": "https://medium.com/the-quarter-espresso/introduction-of-neural-redis-part-1-fa13c1faeef1?source=tag_archive---------2----------------", 
        "text": "Assuming you already know about Neural Network.\n\nThe neural-redis module provides an easy way to simulate a Multi-layer Neural Network that can do regression and classification, and it is designed to be native supported by redis server.\n\nIf we are going to implement a neural network model that has 2 inputs, 1 hidden layer with 3 units, and 1 output as below:\n\nHere are the steps:\n\nThe output means the number of tunable parameters in the neural network.\n\nIn the neural network, we have input X, output Y, and hidden H:\n\nNow for easily explanation, considering activation function:\n\nWhich includes the weight A between input layer and hidden layer, weight B between hidden layer and output layer, and the bias/threshold \u0394 for the outputs:\n\nThe total number of these parameters is 13.\n\nPart 2 will show you how to train the neural network.", 
        "title": "Introduction of neural-redis, part 1 \u2013 The Quarter Espresso \u2013"
    }, 
    {
        "url": "https://medium.com/@vivek.yadav/why-is-gradient-descent-robust-to-non-linearly-separable-data-a50c543e8f4a?source=tag_archive---------3----------------", 
        "text": "Clarification: Gradient descent by itself is NOT robust to non-linearly separable data. However, when used with appropriate nonlinear activation functions it is.\n\nThe reason is due to the kernel trick. In kernel trick, we apply a nonlinear transform on the data, so the resulting data set is linearly separable. This is illustrated below. Consider task of classifying blue and red points, they are not linearly separable. But what if we transform this data by adding a third variable (z = x\u00b2+y\u00b2), wecan draw a plane between blue and red points, and separate the two set of points. This is precisely what neural networks also do.\n\nNeural networks\u2019 learning can be viewed as a 2 part process where they learn some nonlinear transform of the data, and how to separate data based on this transform. Consider a 1 layer NN, the output of the network (ignoring bias) is, Y=W phi(Vx) where phi is a nonlinear function. Now what neural networks are in essence doing is applying nonlinear transform on x, via phi (Vx) and then performing linear separation on the transformed data. So learning via gradient descent algorithm is a 2 part process. In first part, it is learning optimal kernel or function (via V), and in the second part it is separating the transformed data using linear methods. This is also illustrated in Andrej Karpathy\u2019s website. Here is the link to a visual display of the model for seeing how neural networks apply kernel trick and then do separation. http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html\n\nFigure below presents kernel trick applied via neural networks, from the website link above.", 
        "title": "Why is gradient descent robust to non-linearly separable data?"
    }, 
    {
        "url": "https://medium.com/@surmenok/deep-learning-for-spell-checking-2ffdbad65554?source=tag_archive---------4----------------", 
        "text": "I use spell checking every day, it is built into word processors, browsers, smartphone keyboards. It helps to create better documents and make communication clear. More sophisticated spell checker can find grammatical and stylistic errors.\n\nHow to add spell checking to your application? A very simple implementation by Peter Norvig is just 22 lines of Python code.\n\nIn \u201cDeep Spelling\u201d article, Tal Weiss wrote that he tried to use this code and found that it is slow. The code is slow because it is brute forcing all possible combinations of edits on the original text.\n\nAn interesting approach is to use deep learning. Just create artificial dataset by adding spelling errors into correct English texts. And you better have lots of text! The author has been using one billion words dataset released by Google. Then train character-level sequence-to-sequence model with LSTM layers to convert a text with spelling errors to a correct text. Tal got very interesting results. Read the article for details.\n\nGood quality spell checkers can be very useful for chatbots. Most of the chatbots rely on simple NLP techniques, and typical NLP pipeline includes syntax analysis and part of speech tagging, which can be easily broken if the input message is not grammatically correct or has spelling errors. Perhaps fixing spelling errors earlier in the NLP pipeline can improve the accuracy of natural language understanding.\n\nIt can be good to try train such spell checker model on another dataset, more conversational.\n\nHave you tried to use any models like this in your apps?\n\nThe article was originally published on http://pavel.surmenok.com/2016/11/08/deep-learning-for-spell-checking/", 
        "title": "Deep Learning for Spell Checking \u2013 Pavel Surmenok \u2013"
    }, 
    {
        "url": "https://medium.com/@ooohiroyukiooo/deep-neural-network-learns-to-judge-books-by-their-covers-a9288fd7dfde?source=tag_archive---------5----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep Neural Network Learns to Judge Books by Their Covers"
    }
]