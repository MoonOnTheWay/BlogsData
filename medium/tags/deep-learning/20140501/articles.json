[
    {
        "url": "https://hackernoon.com/gradient-descent-vs-coordinate-descent-9b5657f1c59f?source=tag_archive---------0----------------", 
        "text": "When it comes to function minimization, it\u2019s time to open a book of optimization and linear algebra. I am currently working on variable selection and lasso-based solutions in genetics. What lasso does is basically minimizing the loss function and an penalty in order to set to zero some regression coefficients and select only those covariates that are really associated with the response. Pheew, the shortest summary of lasso ever!\n\nWe all know that, provided the function to be minimized is convex, a good direction to follow, in order to find a local minimum, is towards the negative gradient of the function. Now, my question is how good or bad is following the negative gradient with respect to a coordinate descent approach that loops across all dimensions and minimizes along each?\n\nThere is no better way to try this with real code and start measuring. Hence, I wrote some code that implements both gradient descent and coordinate descent.\n\nThe comparison might not be completely fair because the learning rate in the gradient descent procedure is fixed at 0.1 (which in some cases might be slower indeed). But even with some tuning (maybe with some linear search) or adaptive learning rates, it\u2019s quite common to see that coordinate descent overcomes its brother gradient descent many times.\n\nThis occurs much more often when the number of covariates becomes very high, as in many computational biology problems. In the figure below, I plot the analytical solution in red, the gradient descent minimisation in blue and the coordinate descent in green, across a number of iterations.", 
        "title": "Gradient descent vs coordinate descent \u2013"
    }
]