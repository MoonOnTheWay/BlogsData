[
    {
        "url": "https://medium.com/jim-fleming/implementing-lstm-a-search-space-odyssey-7d50c3bacf93?source=tag_archive---------0----------------", 
        "text": "This week I read LSTM: A Search Space Odyssey. It\u2019s an excellent paper that systematically evaluates the different internal mechanisms of an LSTM (long short-term memory) block by disabling each mechanism in turn and comparing their performance. We\u2019re going to implement each of the variants in TensorFlow and evaluate their performance on the Penn Tree Bank (PTB) dataset. This will obviously not be as thorough as the original paper but it allows us to see, and try out, the impact of each variant for ourselves.\n\nTL;DR Check out the Github repo for results and variant definitions.\n\nWe\u2019ll start with a setup similar to TensorFlow\u2019s RNN tutorial. The primary difference is that we\u2019re going to use a very simple re-implementation for the LSTM cell defined as follows:\n\nThis corresponds to the \u201cvanilla\u201d LSTM from the paper. Each equation defines a particular component of the block: block input (z), input gate (i), forget gate (f), cell state (c), output gate (o) and block output (y). Both g and h represent the hyperbolic tangent function and sigma represents the sigmoid activation function. The circle dot represents element-wise multiplication.\n\nHere\u2019s the same thing in code:\n\nBe sure to check out the full source for the rest of the cell definition. Mostly we create a new class inheriting from RNNCell and use the above code as the body of __call__. The nice part about this setup is that we can utilize MultiRNNCell to stack the LSTMs into multiple layers.\n\nNotice that we initialize all of our parameters using get_variable. This is necessary so that we can reuse these variables for each time step rather than creating new parameters at each step. Also, all parameters are transposed from the paper\u2019s definitions to avoid additional graph operations.\n\nThen we define each equation as operations in the graph. Many of the operations have reversed inputs from the equations so that the matrix multiplications produce the correct dimensionality. Other than these details we\u2019re directly translating the equations.\n\nNote that from a performance perspective, this is a na\u00efve implementation. If you look at the source for TensorFlow\u2019s LSTMCell you\u2019ll see that all of the cell inputs and states are concatenated together before doing any matrix multiplication. This is to improve performance, however, since we\u2019re more interested in taking the LSTM apart, we\u2019ll keep things simple.\n\nRunning this vanilla LSTM on the included notebook we obtain a test perplexity (e^cost) of less than 100. So far so good. This will serve as our baseline to compare to the other variants. Below is the cost (average negative log probability of the target words) on the validation set after each epoch:\n\nThe most helpful bits for implementing each of the variants can be found in appendix A3 of the paper. The gate omission variants such as no input gate (NIG), no forget gate (NFG), and no output gate (NOG) simply set their respective gates to 1 (be sure to use floats, not integers, here):\n\nThe no input activation function (NIAF) and no output activation function (NOAF) variants remove their input or output activation functions, respectively:\n\nThe no peepholes (NP) variant removes peepholes from all three gates:\n\nThe coupled input-forget gate (CIFG) variant sets the forget gate like so:\n\nThe final variant, full gate recurrence (FGR), is the most complex, essentially allowing each gate\u2019s previous state to interact with each gate\u2019s next state:\n\nIn many of the variants, we can remove parameters no longer needed to compute the cell. The FGR variant, however, adds significantly more parameters (9 additional square matrices) which also increases training time.\n\nTo implement each, we\u2019ll simply duplicate our vanilla LSTM cell implementation and make the necessary modifications for the variant. There are too many to show here but you can view the full source for each variant on Github. To train each, we\u2019ll use the same hyperparameters from the vanilla LSTM trial. This probably isn\u2019t fair and a more thorough analysis (as performed in the paper) would try to find the best hyperparameters for each variant.\n\nThe NFG and NOG variants fail to converge to anything useful while the NIAF variant diverges significantly after around the 8th epoch. (This divergence could probably be fixed with learning rate decay which I omitted for simplicity.)\n\nIn contrast, the NIG, CIFG, NP and FGR variants all converge. The NIG and FGR variants do not produce great results while the NP and CIFG variants perform similarly to the vanilla LSTM.\n\nFinally the NOAF variant. Its poor performance is likely due to the lack of clamping from the output activation function so its cost explodes:\n\nHere are the test perplexities for each variant:\n\nOverall it\u2019s been fun dissecting the LSTM. Feel free to try out the code yourself and if you\u2019re interested in taking this further I recommend running comparisons with GRUs, looking at fANOVA or extending what\u2019s here with more thorough analysis.\n\nFollow me on Twitter for more posts like these. If you\u2019d like help with production NLP, I do consulting.", 
        "title": "An LSTM Odyssey \u2013 Jim Fleming \u2013"
    }, 
    {
        "url": "https://medium.com/the-technews/google-is-offering-a-deep-learning-course-on-udacity-204ac3a07566?source=tag_archive---------1----------------", 
        "text": "Google has launched a deep learning course explaining the machine learning technique that validates so many of its services. Available through Udacity, a for-profit education site-the class is expected to take about three months to complete at a rate of six hours of work per week. It\u2019s not for the utmost beginners if you have some background in machine learning you can tune into this course.\n\nThrough this course, participants will be introduced to TensorFlow, the in-house machine learning software that Google open sourced last November. Being an increasingly popular type of artificial intelligence, deep learning involves training artificial neural networks on lots of data. It is also the leading force behind the company\u2019s speech recognition technology and Google Photo\u2019s search engine.\n\nMr. Vanhoucke, who is teaching the course (part of Udacity\u2019s machine learning engineer Nanodegree program), explained that its aim is to make deep learning more accessible, and TensorFlow certainly met the needs of a wide range of users.\n\nTensorFlow, the powerful piece of deep learning software from the Google Brain Team, will be covered by the Udacity course. And just like the TensorFlow software, the course itself is free. Udacity will also cover convolutional neural networks, recurrent neural networks, and long-short-term memory networks according to Vanhoucke.", 
        "title": "Google is Offering a Deep Learning Course on Udacity"
    }, 
    {
        "url": "https://medium.com/@onlineclothingstore/microsoft-s-deep-learning-toolkit-for-speech-recognition-is-now-on-github-b064c3a71bff?source=tag_archive---------2----------------", 
        "text": "In order to speed up their AI and speech recognition projects, researchers at Microsoft have developed a toolkit that uses deep neural networks and multiple graphics processing chips for quicker results.\n\nThe company has now open sourced the Computational Networks Toolkit (CNTK) and made it available for anyone to use in their own work on GitHub.\n\nMicrosoft claims CNTK is faster than other publicly available deep learning toolkits\n\nIt\u2019s not the first tech giant to release deep learning software for free; last November, Google open-sourced its TensorFlow machine learning systemthat powered its Photos search functionality.\n\nBut according to Microsoft\u2019s chief speech scientist, the company\u2019s CNTK toolkit is \u201cjust insanely more efficient than anything we have ever seen.\u201d\n\nPart of that could be attributed to CTNK\u2019s ability to harness the power of graphics processing units, or GPUs. The company claims that its toolkit is the only publicly available one that can scale beyond a single machine and take advantage of several GPUs on multiple machines for superior performance.\n\nBy pairing CNTK with Microsoft\u2019s networked GPU system, called Azure GPU Lab, the company was able to train deep neural networks for speech recognition in its Cortana virtual assistant 10 times faster than before.\n\nThe researchers made CNTK available to academic researchers last April under a more restricted open-source license. But now that it\u2019s freely available, they believe it could be useful to anyone from deep learning startups to more established companies that are processing a lot of data in real time.", 
        "title": "Microsoft\u2019s deep learning toolkit for speech recognition is now on GitHub"
    }
]