[
    {
        "url": "https://gab41.lab41.org/faster-on-ramp-to-deep-learning-with-jupyter-driven-docker-containers-3483027a067d?source=tag_archive---------0----------------", 
        "text": "In our Learning About Deep Learning! post, we walked through our \u201cUnauthoritative Practical Getting Started Guide\u201d to this set of machine learning techniques. If you\u2019re new to deep learning or just interested in a nice curated set of resources, go back and take a gander. Following up on that overview, we wanted to share our tips and tricks that have made it easier for us to actually use these powerful technologies. More specifically, Docker and Jupyter have been lynchpins for our efforts. Let\u2019s dig a little deeper and uncover exactly why\u2026\n\nA maze of technologies\n\n\u00a0The hype around deep learning has spurred development of several tools for building, training, and deploying neural networks. Some are general-purpose foundational libraries, while others are tailored to specific applications like image processing. The landscape is far from often-mocked Javascriptland, but there are certainly enough frameworks to develop choice paralysis. Since nothing says \u201chype\u201d like a word cloud, here\u2019s a quick detour to visualize several choices:\n\nFor our sentiment analysis project, we needed to work with several of these technologies. Like all decent hackers, we wanted a fast, useful, and repeatable way to use them. Our desire to get our hands dirty with multiple frameworks led to a few key requirements:\n\nFROM Docker\n\n\u00a0Since we\u2019ve been using Docker for quite a while now, we quickly realized each of those requirements could be met by Docker\u2019s wonderful triad of \u201cIsolation! Portability! Repeatability!\u201d At its core, Docker provides the ability to build a runtime environment that not only remains isolated from other running containers, but also can be deployed to multiple locations in a repeatable way. Docker also uses a text document\u200a\u2014\u200aa Dockerfile\u200a\u2014\u200athat contains all the commands to assemble an image, which will meet our need to document the build environment. Finally, Docker\u2019s runtime options enable us to attach GPU devices when deploying on remote servers.\n\nTo recap, Docker meets all of the following needs:\n\nWith that justification for, \u201cWhy Docker?\u201d out of the way, let\u2019s take a look at one of our example Dockerfiles, which shows exactly how to containerize a fairly complex environment:\n\nRuntime specifics\n\n\u00a0For our images, we built off the fantastic work of Kai Arulkumaran aka kaixhin, who has become a leading maintainer of Docker images for various popular deep learning frameworks. In our opinion, this is THE starting point for using any Dockerized deep learning framework. Basically, kaixhin has done the heavy lifting of Dockerizing most major libraries\u200a\u2014\u200aespecially CUDA\u200a\u2014\u200aand maintains nightly builds of many popular frameworks.\n\nWhile kaixhin\u2019s images are the starting point, we still needed to customize our containers with additional libraries, configuration options, and runtime parameters. More specifically, we adopted the following practices when running the containers:\n\nJupyter notebooks for collaboration\n\nInitially, we focused on the primary task of getting everyone various deep learning environments with as little overall effort as possible. Docker\u2019s \u201cbuild once, run anywhere\u201d mantra largely met that need and propelled our team forward. But the wheels weren\u2019t turning as quickly as we liked:\n\nClearly, we needed an extra step to become truly productive\u2026\n\nSince we have previously used Docker to build an IPython-driven Spark deployment, we already knew how the Jupyter notebook paradigm addresses each of the points above:\n\nDo Jupyter notebooks meet all our needs?\n\n\u00a0As is the case with most headlines framed as questions, the answer is \u201cNo.\u201d Most pertinent to this thread is the simple reality that Jupyter notebooks fall a bit outside the typical software development cycle. Some would go as far as to say they encourage poor software development practices, with global variables, un-modular single files, and the potential for nonlinear code execution.\n\nWe won\u2019t get into an opinionated debate here, mainly because the notebooks\u2019 pros (easy interface and collaboration) have far outweighed their cons (barriers to committing and making code modular) in our prototyping efforts. However, the basic need to backup and/or version control our work remains essential. As near as we could tell, that need left us with two options for using git and GitHub to maintain version control over our notebooks:\n\nStay tuned for a follow-up post to see how we built a basic way to do #2 above with a single click. Until then, we hope this post highlighted how Docker is a solid foundation for building deep learning environments, while Jupyter notebooks make them more approachable for developers and data scientists alike. Thanks for reading!", 
        "title": "Faster On-Ramp to Deep Learning With Jupyter-driven Docker Containers"
    }
]