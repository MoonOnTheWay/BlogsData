[
    {
        "url": "https://gab41.lab41.org/learning-about-deep-learning-d39ad4f78c80?source=tag_archive---------0----------------", 
        "text": "With all the coverage on CNN and Fox News lately, Deep Learning has quickly become a household term. Well, not quite, but Deep Learning is definitely all the craze these days for those of us steeped in Machine Learning and big data. We at Lab41 have been trying to find signal in noise over the past few months, and thought it worthwhile to share our \u201cUnauthoritative Practical Getting Started Guide\u201d with others looking to get started with Deep Learning. Before I go on, I must warn you this largely avoids the difficult Greek symbols and other maths underpinning this complex topic. If you\u2019re looking for formal definitions and proofs, you\u2019ll have to follow the links to resources peppered throughout this post. Now let\u2019s get Deeply Learned!\n\nBefore we get started, we have to ask the most basic question: Why is Deep Learning so interesting?\n\nWell, it\u2019s partly because you can be so incredibly flexible and creative with the tasks you want these algorithms to complete:\n\nThese are but a sample of the creative examples posted to places like Hacker News on a seemingly weekly basis, which makes it very exciting how many domains and applications will be improved by this still-bourgeoning field. But the question we asked ourselves is how do we actually get started with understanding deep learning? Since we\u2019re not Ph.D. Machine Learning candidates and don\u2019t work for any of the Google/Facebook/Microsoft brain trusts in the field, we initially felt a bit uneasy about how to tackle the foundations and applications. Luckily for us, people like Karpathy, repos on GitHub, and several other amazing resources are out there if you know where to look. Let\u2019s do just that\u2026\n\nAs we initially approached this question, we realized that there are already a ton of online resources that can help you get started and we have aggregated many of these resources on our Github Wiki.\n\nThis is great news and shifts the focus from \u201cfinding resources\u201d to \u201cfiltering good ones.\u201d If you follow down this path, you\u2019ll quickly find that one of the first resources everyone mentions is Andrew Ng\u2019s Coursera class on machine learning (ML).\n\nWe found the lectures about basic ML concepts like linear and logistic regression to be useful if you don\u2019t have much of a ML background. More specific to Deep Learning, we found the lectures on neural networks to be a great primer for understanding the basic concepts and architectures. Once we got a hang of some of the basic neural net lingo we found a lecture series that Quoc Le (from the Google Brain project) gave at Carnegie Mellon a few years ago to be very effective. That lecture series not only explains what neural networks are, but also shows examples of how a place like Google uses them and why we call this field \u201cdeep learning\u201d.\n\nAfter we watched these different lecture series\u2019, we felt like we were getting a grasp of the field. However, we were still looking for something that tied the math and theory to a practical application in a cohesive way. One of the best resources we evaluated for putting this picture together were the notes by the aforementioned Andrej Karpathy for his class at Stanford \u201cConvolutional Neural Networks for Visual Recognition\u201d. His notes are tremendously clear and understandable (unlike many other deep learning texts we found) and explain how to think of complicated deep learning concepts in simpler terms. We really can\u2019t emphasize enough how his notes (which led us to referring to him as Andrej the Jiant) illuminated so many difficult topics and helped us transition from \u201creading about\u201d to \u201cactually doing\u201d Deep Learning.\n\nOnce we got a good sense for the overall picture with a combination of class notes and lecture videos, we were in a good position to start understanding recent academic literature in this space. We found the transition from more high-level resources like Andrew Ng\u2019s class to more specific example based resources like Andrej Karpathy\u2019s notes to be extremely valuable in solidifying our understanding of key concepts. As we alluded to earlier, the resources mentioned above represent only a few of the resources we used to get started. More detailed resources lists can be found on our Github Wiki, which includes deep learning textbooks and academic papers we\u2019re reading that apply deep learning in interesting domains.\n\nSo where do we plan to go from here? We\u2019ve seen great progress in the computer vision field with people getting higher and higher scores on ImageNet. This is great news for the field and for image-related applications. However, as a recent Nature article from Bengio, Hinton, and LeCun highlighted, Natural Language Processing could be the next frontier to benefit from Deep Learning architectures. Classes like Richard Socher\u2019s at Stanford, \u201cDeep Learning for Natural Language Processing\u201d (the class notes and lectures are freely available!) only reinforce this notion and indicate the growing interest in this area.\n\nSince text processing is central to so many of our challenges at Lab41, we wanted to use NLP as our first hands-on foray into the field. Stay tuned to our blog as we document our exploration of Deep Learning! Our journey will be documented in code and Wiki within our Github repository and with blog entries centered on the following topics:", 
        "title": "Learning About Deep Learning! \u2013"
    }, 
    {
        "url": "https://medium.com/@dr.ferenc.acs/the-implications-are-indeed-exciting-and-frightening-at-the-same-time-480c54bb9ffa?source=tag_archive---------1----------------", 
        "text": "The implications are indeed exciting and frightening at the same time. I remember to have evangelized about the importance of bringing the theories of artificial neural networks and neuroscience together. Beginning of the new millennium was not the right time to do that, at least in Germany. Some years later, around 2007, I met the first scientist in my life who shared this point of view. Prof. Gustavo Deco from Barcelona. He works on the problem of how to model vast neural structures of the human brain with mathematical approximations of artificial neural networks. Because even most advanced machines today still have not enough computing power to simulate the whole brain on a neuron by neuron level. The other two researchers who had important influence on me were Prof. Karl Friston, Prof. Will Penny (UCL, UK) and Prof. Klaas Enno Stephan (ETH Z\u00fcrich, Swizerland). Karl & Will developed a method to explore the interactions in networks of massive neural assemblies in the human brain by fMRI (Dynamic Causal Modelling). Klaas evangelized a lot in the psychological neuroscience community and supported the research about structural connectivity in the mammal brain. However still such methods were regarded as too complicated. Fortunately recently the tide has turned and explorations in brain connectivity seem to have arrived in the neuroscientific establishment. My guess is that deep learning, big data and the success of IT companies using such techniques had an impact on that. Well, I am out of this since some years. But from the perspective of a teacher and former researcher I am satisfied that my visions were finally correct. In fact now it speeds up much faster than I expected!", 
        "title": "The implications are indeed exciting and frightening at the same time."
    }
]