[
    {
        "url": "https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721?source=tag_archive---------0----------------", 
        "text": "Machine learning only works when you have data\u200a\u2014\u200apreferably a lot of data. So we need lots and lots of handwritten \u201c8\u201ds to get started. Luckily, researchers created the MNIST data set of handwritten numbers for this very purpose. MNIST provides 60,000 images of handwritten digits, each as an 18x18 image. Here are some \u201c8\u201ds from the data set:\n\nWe also know that the idea of machine learning is that the same generic algorithms can be reused with different data to solve different problems. So let\u2019s modify this same neural network to recognize handwritten text. But to make the job really simple, we\u2019ll only try to recognize one letter\u200a\u2014\u200athe numeral \u201c8\u201d.\n\nIn Part 2 , we learned about how neural networks can solve complex problems by chaining together lots of simple neurons. We created a small neural network to estimate the price of a house based on how many bedrooms it had, how big it was, and which neighborhood it was in:\n\nBefore we learn how to recognize pictures of birds, let\u2019s learn how to recognize something much simpler\u200a\u2014\u200athe handwritten number \u201c8\u201d.\n\nIn the last few years, we\u2019ve finally found a good approach to object recognition using deep convolutional neural networks. That sounds like a a bunch of made up words from a William Gibson Sci-Fi novel, but the ideas are totally understandable if you break them down one by one.\n\nThe goof is based on the idea that any 3-year-old child can recognize a photo of a bird, but figuring out how to make a computer recognize objects has puzzled the very best computer scientists for over 50 years.\n\nYou might have seen this famous xkcd comic before.\n\nTo feed an image into our neural network, we simply treat the 18x18 pixel image as an array of 324 numbers:\n\nThe answer is incredible simple. A neural network takes numbers as input. To a computer, an image is really just a grid of numbers that represent how dark each pixel is:\n\nThe neural network we made in Part 2 only took in a three numbers as the input (\u201c3\u201d bedrooms, \u201c2000\u201d sq. feet\u00a0, etc.). But now we want to process images with our neural network. How in the world do we feed images into a neural network instead of just numbers?\n\nAll that\u2019s left is to train the neural network with images of \u201c8\u201ds and not-\u201c8\"s so it learns to tell them apart. When we feed in an \u201c8\u201d, we\u2019ll tell it the probability the image is an \u201c8\u201d is 100% and the probability it\u2019s not an \u201c8\u201d is 0%. Vice versa for the counter-example images.\n\nOur neural network is a lot bigger than last time (324 inputs instead of 3!). But any modern computer can handle a neural network with a few hundred nodes without blinking. This would even work fine on your cell phone.\n\nNotice that our neural network also has two outputs now (instead of just one). The first output will predict the likelihood that the image is an \u201c8\u201d and thee second output will predict the likelihood it isn\u2019t an \u201c8\u201d. By having a separate output for each type of object we want to recognize, we can use a neural network to classify objects into groups.\n\nThe handle 324 inputs, we\u2019ll just enlarge our neural network to have 324 input nodes:\n\nWe can train this kind of neural network in a few minutes on a modern laptop. When it\u2019s done, we\u2019ll have a neural network that can recognize pictures of \u201c8\u201ds with a pretty high accuracy. Welcome to the world of (late 1980\u2019s-era) image recognition!\n\nIt\u2019s really neat that simply feeding pixels into a neural network actually worked to build image recognition! Machine learning is magic!\u00a0\u2026right?\n\nWell, of course it\u2019s not that simple.\n\nFirst, the good news is that our \u201c8\u201d recognizer really does work well on simple images where the letter is right in the middle of the image:\n\nBut now the really bad news:\n\nOur \u201c8\u201d recognizer totally fails to work when the letter isn\u2019t perfectly centered in the image. Just the slightest position change ruins everything:\n\nThis is because our network only learned the pattern of a perfectly-centered \u201c8\u201d. It has absolutely no idea what an off-center \u201c8\u201d is. It knows exactly one pattern and one pattern only.\n\nThat\u2019s not very useful in the real world. Real world problems are never that clean and simple. So we need to figure out how to make our neural network work in cases where the \u201c8\u201d isn\u2019t perfectly centered.\n\nWe already created a really good program for finding an \u201c8\u201d centered in an image. What if we just scan all around the image for possible \u201c8\u201ds in smaller sections, one section at a time, until we find one?\n\nThis approach called a sliding window. It\u2019s the brute force solution. It works well in some limited cases, but it\u2019s really inefficient. You have to check the same image over and over looking for objects of different sizes. We can do better than this!\n\nWhen we trained our network, we only showed it \u201c8\u201ds that were perfectly centered. What if we train it with more data, including \u201c8\u201ds in all different positions and sizes all around the image?\n\nWe don\u2019t even need to collect new training data. We can just write a script to generate new images with the \u201c8\u201ds in all kinds of different positions in the image:\n\nUsing this technique, we can easily create an endless supply of training data.\n\nMore data makes the problem harder for our neural network to solve, but we can compensate for that by making our network bigger and thus able to learn more complicated patterns.\n\nTo make the network bigger, we just stack up layer upon layer of nodes:\n\nWe call this a \u201cdeep neural network\u201d because it has more layers than a traditional neural network.\n\nThis idea has been around since the late 1960s. But until recently, training this large of a neural network was just too slow to be useful. But once we figured out how to use 3d graphics cards (which were designed to do matrix multiplication really fast) instead of normal computer processors, working with large neural networks suddenly became practical. In fact, the exact same NVIDIA GeForce GTX 1080 video card that you use to play Overwatch can be used to train neural networks incredibly quickly.\n\nBut even though we can make our neural network really big and train it quickly with a 3d graphics card, that still isn\u2019t going to get us all the way to a solution. We need to be smarter about how we process images into our neural network.\n\nThink about it. It doesn\u2019t make sense to train a network to recognize an \u201c8\u201d at the top of a picture separately from training it to recognize an \u201c8\u201d at the bottom of a picture as if those were two totally different objects.\n\nThere should be some way to make the neural network smart enough to know that an \u201c8\u201d anywhere in the picture is the same thing without all that extra training. Luckily\u2026 there is!\n\nAs a human, you intuitively know that pictures have a hierarchy or conceptual structure. Consider this picture:\n\nAs a human, you instantly recognize the hierarchy in this picture:\n\nMost importantly, we recognize the idea of a child no matter what surface the child is on. We don\u2019t have to re-learn the idea of child for every possible surface it could appear on.\n\nBut right now, our neural network can\u2019t do this. It thinks that an \u201c8\u201d in a different part of the image is an entirely different thing. It doesn\u2019t understand that moving an object around in the picture doesn\u2019t make it something different. This means it has to re-learn the identify of each object in every possible position. That sucks.\n\nWe need to give our neural network understanding of translation invariance\u200a\u2014\u200aan \u201c8\u201d is an \u201c8\u201d no matter where in the picture it shows up.\n\nWe\u2019ll do this using a process called Convolution. The idea of convolution is inspired partly by computer science and partly by biology (i.e. mad scientists literally poking cat brains with weird probes to figure out how cats process images).\n\nInstead of feeding entire images into our neural network as one grid of numbers, we\u2019re going to do something a lot smarter that takes advantage of the idea that an object is the same no matter where it appears in a picture.\n\nHere\u2019s how it\u2019s going to work, step by step \u2014\n\nSimilar to our sliding window search above, let\u2019s pass a sliding window over the entire original image and save each result as a separate, tiny picture tile:\n\nBy doing this, we turned our original image into 77 equally-sized tiny image tiles.\n\nEarlier, we fed a single image into a neural network to see if it was an \u201c8\u201d. We\u2019ll do the exact same thing here, but we\u2019ll do it for each individual image tile:\n\nHowever, there\u2019s one big twist: We\u2019ll keep the same neural network weights for every single tile in the same original image. In other words, we are treating every image tile equally. If something interesting appears in any given tile, we\u2019ll mark that tile as interesting.\n\nWe don\u2019t want to lose track of the arrangement of the original tiles. So we save the result from processing each tile into a grid in the same arrangement as the original image. It looks like this:\n\nIn other words, we\u2019ve started with a large image and we ended with a slightly smaller array that records which sections of our original image were the most interesting.\n\nThe result of Step 3 was an array that maps out which parts of the original image are the most interesting. But that array is still pretty big:\n\nTo reduce the size of the array, we downsample it using an algorithm called max pooling. It sounds fancy, but it isn\u2019t at all!\n\nWe\u2019ll just look at each 2x2 square of the array and keep the biggest number:\n\nThe idea here is that if we found something interesting in any of the four input tiles that makes up each 2x2 grid square, we\u2019ll just keep the most interesting bit. This reduces the size of our array while keeping the most important bits.\n\nSo far, we\u2019ve reduced a giant image down into a fairly small array.\n\nGuess what? That array is just a bunch of numbers, so we can use that small array as input into another neural network. This final neural network will decide if the image is or isn\u2019t a match. To differentiate it from the convolution step, we call it a \u201cfully connected\u201d network.\n\nSo from start to finish, our whole five-step pipeline looks like this:\n\nOur image processing pipeline is a series of steps: convolution, max-pooling, and finally a fully-connected network.\n\nWhen solving problems in the real world, these steps can be combined and stacked as many times as you want! You can have two, three or even ten convolution layers. You can throw in max pooling wherever you want to reduce the size of your data.\n\nThe basic idea is to start with a large image and continually boil it down, step-by-step, until you finally have a single result. The more convolution steps you have, the more complicated features your network will be able to learn to recognize.\n\nFor example, the first convolution step might learn to recognize sharp edges, the second convolution step might recognize beaks using it\u2019s knowledge of sharp edges, the third step might recognize entire birds using it\u2019s knowledge of beaks, etc.\n\nHere\u2019s what a more realistic deep convolutional network (like you would find in a research paper) looks like:\n\nIn this case, they start a 224 x 224 pixel image, apply convolution and max pooling twice, apply convolution 3 more times, apply max pooling and then have two fully-connected layers. The end result is that the image is classified into one of 1000 categories!\n\nSo how do you know which steps you need to combine to make your image classifier work?\n\nHonestly, you have to answer this by doing a lot of experimentation and testing. You might have to train 100 networks before you find the optimal structure and parameters for the problem you are solving. Machine learning involves a lot of trial and error!\n\nNow finally we know enough to write a program that can decide if a picture is a bird or not.\n\nAs always, we need some data to get started. The free CIFAR10 data set contains 6,000 pictures of birds and 52,000 pictures of things that are not birds. But to get even more data we\u2019ll also add in the Caltech-UCSD Birds-200\u20132011 data set that has another 12,000 bird pics.\n\nHere\u2019s a few of the birds from our combined data set:", 
        "title": "Machine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks"
    }, 
    {
        "url": "https://medium.com/@surmenok/character-level-convolutional-networks-for-text-classification-d582c0c36ace?source=tag_archive---------1----------------", 
        "text": "One of the common natural language understanding problems is text classification. Over last few decades, machine learning researchers have been moving from the simplest \u201cbag of words\u201d model to more sophisticated models for text classification.\n\nBag of words model uses only information about which words are used in the text. Adding TFIDF to the bag of words helps to track relevancy of each word to the document. Bag of n-grams enables using partial information about structure of the text. Recurrent neural networks, like LSTM, can capture dependencies between words even if they are far from each other. LSTM learns structure of sentences from the raw data, but we still have to provide a list of words. Word2vec algorithm adds knowledge about word similarity, which helps a lot. Convolutional neural networks can also help to process word-based datasets.\n\nA trend is to learn using raw data, and provide machine learning models with an access to more information about text structure. A logical next step would be to feed a stream of characters to the model and let it learn all about the words. What can be cruder than a stream of characters? An additional benefit is that the model can learn misspellings and emoticons. Also, the same model can be used for different languages, even those where segmentation into words is not possible.\n\nThe article \u201cCharacter-level Convolutional Networks for Text Classification\u201d (Xiang Zhang, Junbo Zhao, Yann LeCun) explores usage of character-level ConvNet networks for text classification. They compare performance of a few different models on several large-scale datasets.\n\nDatasets contain from 120,000 to 3,600,000 samples in the training set, from 2 to 14 classes. The smallest dataset is AG\u2019s News: news articles divided into 4 classes, 30,000 articles for each class in the training set. The largest is Amazon Review Polarity: 2 polarity segments with 1,800,000 reviews for each of them.\n\nCharacter-level ConvNet was compared with state-of-the-art models: Bag-of-words and its TFIDF, Bag-of-ngrams and its TFIDF, Bag-of-means on word embedding, word-based ConvNet, word-based LSTM.\n\nResults are quite interesting. N-gram and N-gram TFIDF models have are the best for smaller datasets, up to several hundreds of thousands of samples. But when dataset size grows to several million we can observe that character-level ConvNet performs better.\n\nConvNet tends to work better for texts which are less curated. For example, ConvNet works better on Amazon reviews dataset. Amazon Reviews are raw user inputs, whereas users could be more careful in writings on Yahoo! Answers.\n\nChoice of alphabet matters. ConvNet works better is not distinguishing between upper and lower case characters.\n\nAnother overview of this paper\n\nA Set of Character-Based Models for Sentiment Analysis, Ad Blocking and other tasks", 
        "title": "Character-level Convolutional Networks for Text Classification"
    }, 
    {
        "url": "https://medium.com/machina-sapiens/aprendizagem-de-m%C3%A1quina-%C3%A9-divertido-parte-3-deep-learning-e-redes-neuronais-convolutivas-879e0ee7ba48?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Aprendizagem de M\u00e1quina \u00e9 Divertido! Parte 3 \u2013 Machina Sapiens \u2013"
    }, 
    {
        "url": "https://medium.com/@jkarnows/inputting-image-data-into-tensorflow-for-unsupervised-deep-learning-f36d05e12591?source=tag_archive---------3----------------", 
        "text": "Everyone is looking towards TensorFlow to begin their deep learning journey. One issue that arises for aspiring deep learners is that it is unclear how to use their own datasets. Tutorials go into great detail about network topology and training (rightly so), but most tutorials typically begin with and never stop using the MNIST dataset. Even new models people create with TensorFlow, like this Variational Autoencoder, remain fixated on MNIST. While MNIST is interesting, people new to the field already have their sights set on more interesting problems with more exciting data (e.g. Learning Clothing Styles).\n\nIn order to help people more rapidly leverage their own data and the wealth of unsupervised models that are being created with TensorFlow, I developed a solution that (1) translates image datasets into a file structured similarly to the MNIST datasets (github repo) and (2) loads these datasets for use in new models.\n\nTo solve the first part, I modified existing solutions that demonstrate how to decode the MNIST binary file into a csv file and allowed for the additional possibility of saving the data as images in a directory (also worked well for testing the decoding and encoding process).\n\nI then reversed the process, making sure to pay attention to how these files were originally constructed (here, at the end) and encoding the information as big endian.\n\nThe TensorFlow tutorials allow you to import the MNIST data with a simple function call. I modified the existing input_data function to support the loading of training and testing data that have been created using the above procedure: input_any_data_unsupervised. When using this loading function with previously constructed models, remember that it does not return labels and those lines should be modified (e.g. the call to next_batch in line 5 of this model).\n\nA system for supervised learning would require another encoding function for the labels and a modified input function that uses the gzipped labels.\n\nI hope this can help bridge the gap between starting out with TensorFlow tutorials with MNIST and diving into your own problems with new datasets. Keep deeply learning!", 
        "title": "Inputting Image Data into TensorFlow for Unsupervised Deep Learning"
    }, 
    {
        "url": "https://medium.com/@orbital_insight/orbital-insight-announces-expanded-product-coverage-in-retail-and-energy-verticals-410476bb3066?source=tag_archive---------4----------------", 
        "text": "Orbital Insight is excited to announce enhancements to both its US Retail Traffic and Worldwide Oil Storage and Production product suites.\n\nOrbital Insight has made a number of enhancements to its US Retail Traffic offerings to give clients a broader understanding of the US retail economy. Orbital Insight recently added 38 chains to the coverage universe of its US Micro Retail Traffic offering, bringing its total coverage to 92 chains (see the entire coverage universe below). In addition, Orbital Insight has released sub-macro US Retail Thematic Baskets for client consumption in areas such as Home Improvement, Big Box, and Auto Parts, which take advantage of Orbital Insight's expanded coverage by aggregating micro-chains into a sector-wide indicator. Regional breakdowns of US Micro Retail and US Retail Thematic Baskets are also available.\n\nOrbital Insight has also expanded its retail coverage to cover major US shopping malls through its US Mall Traffic product and has produced numerous traffic indices for client consumption, including nationwide, regional, by mall type, by mall grade, by mall operator, and by REIT-ownership indices.\n\nIn its Oil Storage and Production vertical, Orbital Insight will be releasing its China Oil Storage backtest data, following the successful rollout of US Oil Storage backtest data to clients. In addition to volume estimates for Chinese floating roof tanks, Orbital Insight has made use of its proprietary Tank Farm Finder to scan China to locate and measure net shell capacity and volume estimates at oil tanks that were previously unknown to industry databases.\n\nNew Micro Chain Coverage: AN, BH, BLMN, BOBE, CAKE, CASY, CATO, CTRN, DAVE, DEG, DENN, DRI, DXLG, EAT, FIVE, FRED, HGG, HIBB, HVT, IMKTA, JACK, JMBA, KMX, LUB, MIK, ODP, ORLY, PLCE, PLKI, RRGB, RT, SONC, SVU, TAST, TXRH, VSI, WEN, WMK", 
        "title": "Orbital Insight Announces Expanded Product Coverage in Retail and Energy Verticals"
    }
]