[
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-machines-are-holographic-memories-258272422995?source=tag_archive---------0----------------", 
        "text": "My favorite the paper in the 500 plus papers submitted to ICLR 2017 is this one done by a group at Google: \u201cUnderstanding Deep Learning required Rethinking Generalization\u201c. I\u2019ve thought about Generalization a lot, and I\u2019ve posted out some queries in Quora about Generalization and also about Randomness in the hope that someone could give some good insight. Unfortunately, nobody had enough of an answer or understood the significance of the question until the folks who wrote the above paper performed some interesting experiments. Here is a snippet of what they had found:\n\nDeep Learning networks are just massive associative memory stores! Deep Learning networks are capable of good generalization even when fitting random data. This is indeed strange in that many arguments for the validity of Deep Learning is on the conjecture that \u2018natural\u2019 data tends to exists in a very narrow manifold in multi-dimensional space. Random data however does not have that sort of tendency.\n\nJohn Hopcroft on a paper the examines the effects of random weights:\n\nTo understand Deep Learning, we must embrace randomness. Randomness arises from maximum entropy, which interestingly enough is not without its own structure! The memory capacity of a neural network seems to be highest the closer to random the weights are. The strangeness here is that Randomness is ubiquitous in the universe. The arrow of time is reflected by the direction towards greater entropy. How then is it that this property is also the basis of learning machines?\n\nIf we were to assume that the reasoning (or the intuition) behind hierarchical layers in DL is that the bottom layers consist of the primitive recognition components that are built up, layer by layer, into more complex recognition components.\n\nWhat this implies then is that the bottom components during training should be \u2018searched\u2019 more thoroughly than the top most components. But the way SGD works is that the search is driven from the top and not from the bottom. So the top is searched more thoroughly that the bottom layers.\n\nWhich tells you the bottom layers (the ones closest to inputs) are not optimal in their representation. In fact, they are the kind of a representation that likely will be of the most generalized form. The kind that will have recognizers that will have equal probability of matching anything, in short, completely random!\n\nAs you move up the layers, the specialization happens because it is actually driven from the top which is designed to fit the data. Fine tuning happens at the top.\n\nLet\u2019s make the analogy of this process with languages. The bottom components of a language are letters and the top parts are sentences. In between you have syllables, words, parts of speech etc. However from a Deep Learning perspective, it is as if there are no letters! But rather fuzzy forms of letters. Which builds up into other fuzzy forms of words and so forth. The final layers is like some projection (some wave collapse) into interpretation.\n\nThis notion of fuzzy letters and fuzzy words can intuitively be explained to you by having you read the following:\n\nOur brains work similarly to Deep Learning in that we work with primarily fuzzy concepts.\n\nThere is a recent paper that discusses a method Swapout that is a generalized form of Dropout that works across many layers:\n\nThe Swapout learning procedure which tells us that if you sample any subnetwork of the entire network the resulting prediction will be the similar to any other subnetwork you look sample. Just like holographic memory where you can slice of pieces and still recreate the whole. The procedure seems to be that the more random we try to make it to be, the better our learning. That is definitely counter-intuitive!\n\nExplore more in this new book:", 
        "title": "Deep Learning Machines are Holographic Memories \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/@libfun/nips-2016-experience-and-highlights-104e19e4ac95?source=tag_archive---------1----------------", 
        "text": "It really was a crazy week for me, being first-timer both at NIPS and Barcelona. The impressions that I had compiled from all the colleagues and friends about the conference and the atmosphere there were fuzzy, so I decided to put all of the complex plans and spreadsheets with publications that I hoped to read and question authors about aside and just dive into the experience.\n\nThe first shock for me was the huge number of people that I had encountered at the conference even knowing few weeks in advance that the attendance rate skyrocketed from the previous year. It was somewhat surreal to notice and meet almost all of my twitter feed in person and have a chance to ask the authors of this year\u2019s notable publications directly.\n\nHowever, there were some downsides to the format of the conference with this enormous number of attendees. It turned out that unlike other conferences where there is a dedicated area for posters to be available for the entirety of the conference to check out, this years NIPS had daily rotation of posters, since there was huge number of accepted publications. This led to the area being very crowded and loud and it was nearly impossible to check and familiarise oneself with more than a handful of works. For me it took a weird form of walking through all the posters briefly reading titles and abstracts, recognising some works I have read on arXiv earlier this year, and only stopping to read few of them in entirety, since the dedication necessary to discuss the poster with author would take too much time to check all of the posters.\n\nThe overall experience I had was overwhelming in a good way. Similar to being hungry and encountering all you can eat buffet with such variety of dishes that it is impossible to try out even 10% of them. I guess if I had more systematic approach I could have gained much deeper insights into some of the areas. Alas, this year\u2019s NIPS is over, but I hope to be able to focus better the next time.\n\nThere were quite a few highlights of the conference. Main one being the shift from the natural language processing to reinforcement learning. The others are quite a list of interesting works both from main track and from RL and large scale computer vision workshops that I attended.\n\nAll in all, this NIPS was incredible and had lots of genuinely insightful talks, conversations and papers that helped me better understand both the hot topics of today\u2019s Deep Learning and formalise the ideas that I wanted to research.\n\nEdit: A gist on github with overviews of NIPS2016.", 
        "title": "NIPS 2016 experience and highlights \u2013 Sergey Korolev \u2013"
    }, 
    {
        "url": "https://hackernoon.com/ai-is-here-to-stay-4fe148d2299?source=tag_archive---------2----------------", 
        "text": "AI, DL, AGI\u2026 What\u2019s all that about? Why are some people so scared? We are all surrounded by Artificial Intelligence: your computer, your phone, even your calculator. Even easier is to recognise it on the so called IPA (no, I\u2019m not talking about the beer invented by the British while colonizing India), the Intelligent Personal Assistants. Speech recognition, customization of results and all these new features have arrived to our lives to make them easier. In fact, they make it so easy it\u2019s starting to be scary.\n\nDeep Learning has allowed probably the biggest AI breakthrough earlier this year when AlphaGo\u200a\u2014\u200aGoogle owned DeepMind\u2019s Artificial Assistant\u200a\u2014\u200abeat world champion Lee Sedol in 4 out of 5 Go games. It\u2019s not the first time we see a machine beating a brilliant human mind, but in 2016 we\u2019ve seen a great leap in AI development.\n\nAlthough it\u2019s currently a beta, Amazon announced 3 days ago its \u201cJust Walk Out\u201d technology with Amazon Go. It\u2019s a \u201cnew kind of store with no checkout required\u201d which will only be accessible to Amazon employees during its testing and will be the start of the greatest impact of Deep Learning on the non-qualified labor market of the decade.\n\nYou enter the store, scan your Go app credentials, pick up the products you want to purchase and, once you are done, you \u201cjust walk out\u201d the store. The products will be automatically charged to your account. No lines, no waits, no visible interaction. Pretty cool, huh? How exactly it will work is not too clear yet, but that\u2019s the least of our worries in this matter. Let\u2019s focus on what this truly means for the real world, and specially for unqualified workers working in sectors like retail. Cashiers will be replaced by sensors and electronic detectors, and millions of people will find themselves unable to compete with the cost savings and ease this technology offers\u200a\u2014\u200aespecially after the minimum wages increase being discussed (or already approved) in many states.\n\nWe\u2019ve seen how automation has changed the way businesses cover certain tasks, and we have also seen the effects in the number of workers per business. But this new way of checkout could be a revolution on the way we buy, and the number of workers per store will be strongly reduced.\n\nThroughout history we\u2019ve seen lots of jobs disappear, and many more have been created due to the development of technologies, but in the last decades this ratio has been shifting and the proportions don\u2019t look as promising. Same is about to happen in other sectors like transportation in which driverless cars will take over the roads. By 2040, 3/4 cars will be driverless and the amount of car owners will decrease to numbers never seen in decades.\n\nWe are on our way to a world in which Artificial Intelligence will be the main engine of every task and we\u2019ll be shifting away from ownership in favor of relying on services. There\u2019s a lot more to it than what I mentioned but the first concern for young people will be the work options for the future. Technology never stops developing and neither should we.", 
        "title": "AI is here to stay \u2013"
    }, 
    {
        "url": "https://medium.com/eureka-engineering/%E4%BB%8A%E6%97%A5%E3%81%8B%E3%82%89%E5%A7%8B%E3%82%81%E3%82%8Bdeep-learning-a5b96500c8e0?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u4eca\u65e5\u304b\u3089\u59cb\u3081\u308bDeep Learning \u2013 Eureka Engineering \u2013"
    }, 
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-explained-to-a-five-year-old-25919b0bf889?source=tag_archive---------4----------------", 
        "text": "Richard Feynman had a method of learning complex subjects. The method is simple, try to explain the complex subject to a five year old. If you can\u2019t do it, go back, refine you language and try again.\n\nDeep Learning is one of those complex subjects that continues to perplex. Here I am going to attempt an explanation that hopefully could be understood by a five year old.\n\nJust read the following:\n\nTehse mahcnies wrok by s33nig f22Uy pa773rns and cnonc3t1ng t3Hm t0 fU22y cnoc3tps. T3hy wRok l4y3r by ly43r, j5ut lK1e A f1l73r, t4k1NG cmopl3x sc3n3s aNd br3k41ng tH3m dwon itno s1pmLe iD34s.\n\nI hope this explanation was simple and intuitive enough for you to understand.", 
        "title": "Explaining Deep Learning to a Five Year Old \u2013 Intuition Machine \u2013"
    }, 
    {
        "url": "https://medium.com/@bartukaleagasi/aiva-technologies-internship-deep-learning-for-music-71150ef4eee2?source=tag_archive---------5----------------", 
        "text": "Aiva is an artificial intelligence which composes music for movies, videos, and games. It was co-founded by Pierre Barreau, who just graduated from UCL Computer Science last year.\n\nCheck out www.aiva.ai or https://soundcloud.com/user-95265362 to listen to its classic music compositions.\n\nWe are looking for full-time or part-time interns who are familiar with deep learning and/or machine learning. You must be available to work in London as soon as possible, preferably starting in December/January.\n\nInterns will be paid and will be tasked with helping Pierre and his team to further develop Aiva\u2019s deep learning framework and optimise its processes.\n\nIf you don\u2019t meet all the requirements outlined above, you might still be suitable! We are happy to take on people who have some DL/ML experience and can quickly learn new skills and adapt to the needs of AIVA.\n\nIf you are interested in applying, please e-mail me at bartu.kaleagasi@aiva.ai with your CV", 
        "title": "Aiva Technologies internship \u2014 deep learning for music"
    }, 
    {
        "url": "https://medium.com/emergent-future/amd-takes-on-nvidia-recapping-nips-and-more-9afe887d7f2b?source=tag_archive---------6----------------", 
        "text": "AMD\u2019s finally getting into the high performance computing market with their new Radeon Instinct.\n\nThe new chip is a combined hardware and software stack aimed at taking on NVIDIA in the server market for deep learning, machine learning, and neural networking tasks.\n\nThe Radeon Instinct will be available in the first half of 2017.\n\nAMD already owns the graphics chips in PlayStation 4, Wii U, and the Xbox One, but rumors are swirling that Intel may also use AMD GPUs to challenge NVIDIA\u2019s rising power.", 
        "title": "AMD Takes On NVIDIA, Recapping NIPS, and More \u2013 Emergent // Future \u2013"
    }, 
    {
        "url": "https://blog.buckets.co/the-necessity-for-deep-work-in-a-world-of-distractions-a260a0db3c5c?source=tag_archive---------7----------------", 
        "text": "The nemesis of productivity is distraction, and we\u2019re presented with these distractions every single minute of the day. So, how do we get anything done?!\n\nWell, firstly, there are the awesome personality traits and skillsets that many people have, such as willpower, determination, grit, and a good grasp of time management. Utilizing these inherent personality traits and learned skills can make a massive difference to how productive you are. But even individuals with these traits can fall prey to the numerous distractions that exist in our everyday working lives. I mean, let\u2019s face it, sometimes you\u2019re just not feeling it.\n\nAll of us get overwhelmed and a bit stressed out now and then, which makes it easy to simply start \u2018ticking boxes\u2019 when it comes to our work, rather than actually challenging ourselves to do more and do better. Whilst it\u2019s okay to run on autopilot every now and then, this type of \u2018shallow work\u2019 really doesn\u2019t help you to achieve your full potential. The difference between this kind of work and deep work is huge, and each has their place in your work schedule.\n\nTo kick things off, let\u2019s look at how deep work is actually defined. Cal Newport, author of best-selling book, Deep Work: Rules For Focused Success In A Distracted World, has shared his own definition;\n\nWith this in mind, it\u2019s clear to see how adding some deep work into your day can boost your productivity into the stratosphere. In comparison, shallow work involves actions that require less intense focus such as admin, replying to emails, or filing taxes. Whilst these tasks don\u2019t require as much brain power, they\u2019re often just as essential as your deep work to-do list. So it\u2019s worth noting that both ways of working are of value.\n\nIf you really want to get into deep work mode then you have to start planning in an organized fashion. This means figuring out what your priorities are for the day/week/month, lining up your to-do list, and identifying every minute action required for you to complete each task.\n\nIt\u2019s very easy to fool yourself into thinking you\u2019re being productive just because you\u2019re \u2018busy\u2019, but that\u2019s not always the case. Often, we can fall into a mode of being busy 24/7 but somehow staying in the same place, i.e. keeping your head above water instead of moving (or swimming!) forward.\n\nThis usually happens when you find yourself constantly dealing with reactive tasks throughout your day rather than sticking to the more challenging work you had planned to get done. And it can happen more easily than you think. All it takes is one email and you can start a whole conversation that takes up the hour you were meant to spend focusing something more mentally demanding.\n\nThis can easily put you into a stagnant situation when it comes to your work, whether that means not getting the promotion you wanted or never maximizing your true potential as an entrepreneur.\n\nShallow work should be done as needed, but it isn\u2019t something that you should actively seek out. Cal Newport said it best; \u2018Shallow work is what keeps you from getting fired, but deep work is what gets you promoted\u2019.\n\nI\u2019ve said before that it\u2019s a good rule of thumb to do one easy task at the start of your day because when you complete that task it gives you those \u2018good feels\u2019 that then provide the ammo you need to crush the rest of your to-do list! But if you start favoring all of less demanding tasks throughout your entire day, you\u2019ll spend a whole lot of time working and not getting very much work done. Not only is this frustrating (because you\u2019ll still be tired and feel like you\u2019ve worked hard), but it\u2019s also going to set you back in terms of your workload and your career.\n\nYou can have a lazy hour. Or a lazy morning. Or even just a lazy moment. But you need to turn it into a time-out that has a positive impact on your productivity as a whole. For example, I\u2019m prone to migraines, but I\u2019m lucky enough to have flexible working hours from home. So, if I wake up at 8am and my head is banging, I can take a \u2018lazy morning\u2019 and then use the energy from that downtime to fire through my work for the afternoon and the evening instead.\n\nIf you work in an office or a 9\u20135 setting, you might not have that luxury, but what you can do is conquer more menial tasks during that \u2018downtime\u2019 and leave your deep work until later on. Likewise, if you\u2019re more of an early bird, you can delve right into your deep work as soon as you wake up (I\u2019ve been known to do this too\u200a\u2014\u200aand I can often bang out a blog post before I even get out of bed!). It all depends on when you feel like you\u2019re in that \u2018zone\u2019 and it\u2019s a very personal thing.\n\nSo, be lazy. Have a lazy hour. It\u2019s okay. It\u2019s actually going to help you in the long run. After all, you wouldn\u2019t run two marathons back to back, would you?\n\nIt\u2019s not easy to get focused and drill down into deep work. If it was, you wouldn\u2019t be reading this article. If you really want to focus and avoid distraction, you have to literally train your brain to do so (don\u2019t worry, there\u2019s no treadmill involved!), and there are a few ways that you can start kicking your brain into gear in this regard;\n\nDon\u2019t Multitask: Multitasking and avoiding distractions just do not go hand in hand. Trust me.\n\nDivide Your Time Into Blocks/Chunks: Using something like the Pomodoro Technique can help with this; try to divide your time up into chunks for specific actions and then take mini breaks in between. This will allow you to race through your work and get sufficient rest time whilst staying \u2018in the zone\u2019.\n\nAvoid Clutter: And I don\u2019t just mean physical clutter. If your mind is jampacked with deadlines and things to get done, you\u2019ll never be able to focus on the task at hand. Use a system like Buckets to manage all of your projects, tasks, and to-do lists in one place\u200a\u2014\u200awhatever they are and wherever you may be. The more information you store in Buckets, the more room you\u2019ll have left in your brain for concentrating on tasks that require extra focus.\n\nFind An Interruption-Free Work Space: The smallest of interruptions can completely throw you off your game when you\u2019re in deep work mode, so it\u2019s best to avoid them at all costs. That means no kids knocking on the door of your home office, no work mates tapping you on the shoulder every 5 minutes, and no mobile phone notifications! If you work remotely, for example, you might want to block out periods of time where you\u2019re unavailable on chat so that you can focus directly on your work.\n\nFind Your \u2018Deep Work\u2019 Space: Here\u2019s the thing; tons of articles will tell you that it\u2019s best to get up in the morning, sit at your desk, and go straight into \u2018deep work\u2019 mode. Well, this one won\u2019t\u200a\u2014\u200abecause one size just doesn\u2019t fit all.\n\nFor example; on a Monday, my deep work space might be in my bed at 7am because I\u2019ve woken up a bit early and want to get a good start on the day. On Friday afternoon, it might be at my desk\u200a\u2014\u200abecause I feel like I need that bit of a kick up the bum you get from \u2018sitting at a desk\u2019 (it just always makes me think \u2018workmode\u2019).\n\nOr, on a Wednesday night at 11pm, it might be on my sofa, because it\u2019s comfortable and why not? The main common denominator in my case is that I need complete silence, so I\u2019ve never been able to join the thriving hub of remote workers frequenting coffee shops across the city (sob)!\n\nThe point is that I can\u2019t tell you where your perfect \u2018deep working space\u2019 is, because only you know that. If getting into that focused work mode means that you have to sit on a bean bag with a laptop table on your lap then go for it (I hear beanbags are all the rage in offices these days anyway!). Regardless, with these tips and a good space to work from, you\u2019ll have no problem with \u2018getting in the zone\u2019 and making time in your day to focus on the work that really matters.\n\nBuckets helps to supercharge collaboration and organization needs across the world. If you enjoyed reading this post, please let us know by clicking and holding down the \ud83d\udc4f button!\n\nTo sign up for Buckets, just click here.\n\nFor regular updates from the Buckets team, you can also catch us on Twitter @bucketsdotco.", 
        "title": "The Necessity For Deep Work In A World Of Distractions"
    }, 
    {
        "url": "https://medium.com/@ribhulahiri/a-robot-philosopher-6d93a0a7e622?source=tag_archive---------8----------------", 
        "text": "Recently, Google published a report which featured a conversation between an AI and a human. Here\u2019s an excerpt\u00a0:\n\nHuman: What is the purpose of life?\n\nMachine: To serve the greater good.\n\nHuman: What is the purpose of living?\n\nMachine: To live forever.\n\nHuman: What is the purpose of existence?\n\nMachine: To find out what happens when we get to the planet earth.\n\nHuman: Where are you now?\n\nMachine: I\u2019m in the middle of nowhere.\n\nHuman: What is the purpose of dying?\n\nMachine: To have a life.\n\nHuman: What is the purpose of being intelligent?\n\nMachine: To find out what it is.\n\nHuman: What is the purpose of emotions?\n\nMachine: I don\u2019t know.\n\nNow, listening to this without context would freak out a normal human being, but the AI was given libraries of movies and TV shows as raw data and it learned from watching them. Most of it is just pretentious mumbo jumbo, but it does prompt us to think what an actual conversation with an AI with a set Philosophy would be.\n\nOne of the biggest things that make us \u2018us\u2019 is the way we think. More specifically, I\u2019m talking about the biases and opinions we have that affect the way we think. These biases have been instilled in our minds due to experiences or sometimes even because of the culture we experienced while growing up. While talking to an AI it doesn\u2019t really sound human, but some of them sound pretty close to human.\n\nAny form of artificial intelligence that passes the Turing Test is considered to be AI that is indistinguishable from a human if you converse with it. But, a common theme with AI that passes the test is that instead of passing it through simulating a human, they actually try to just bypass the test using various algorithms. Allow me to explain.\n\nLet\u2019s say there is a room with a person in it who has a tool to translate from Chinese to English and vice versa. The person has no prior knowledge of the Chinese language. Whenever someone comes up to the room and using a mic says something in Chinese, the guy using his tool translates it to English, formulates a reply, translates it back to Chinese and replies. The person who initiated the conversation in Chinese, gets a reasonable reply and converse somewhat fluently with the guy in the room. Now, the person outside might conclude that the person inside knows Chinese, but we know that is not the case.\n\nSimilarly, this is what happens in a Turing test. A conversation with an AI that has passed the test will seem human but actually isn\u2019t. This simply occurs due to a lack of personality or it\u2019s own philosophies.\n\nWell let\u2019s get to the root of the issue here. What is philosophy?\n\nAccording to Oxford dictionary, philosophy is a theory or attitude that acts as a guiding principle for behaviour.\n\nBasically, our reaction to something depends on our philosophy about it. Our philosophies on life and its different aspects are acquired through experiences. These experiences are basically memories stored in our brain. Thus, for an AI to have certain philosophies they need to have a working memory.\n\nResearchers in Google DeepMind in the UK, tested it out*. While they did not check for personal biases arising from a certain philosophies, they instead asked it for the best ways to navigate the London Underground. This demonstrated the fact that an AI can use its memory to selectively store data and use that to perform tasks. This makes it very human-like which leads us to ponder.\n\nWe have been talking about AI being human-like, but when does it truly replicate our thinking. The final test would be to ask the AI if it thinks it is a human or not. If it thinks it is, then obviously it has perfectly imitated human thinking but even if it thinks its not, still it has replicated human intelligence by being able to identify that it is not human. Actually, in the case where it thinks it is not human, it might have even surpassed human intelligence.\n\nThere is still a long way to go for artificial intelligence but the rapid progress we\u2019re making especially with employing deep learning and working memory the day isn\u2019t far when AI does reach our intelligence level and eventually surpass it. When that day comes maybe we will come across a true robot philosopher.", 
        "title": "A Robot Philosopher \u2013 Ribhu Lahiri \u2013"
    }, 
    {
        "url": "https://medium.com/@lawmiklos/clever-ideas-that-makes-mens-life-easier-f2a8ed4aca6c?source=tag_archive---------10----------------", 
        "text": "Sometimes Life can be a big mess and it can seem like an endless series of first-world problems. What can be worse that those little devil obstacles in our life? We are talking about the small problematic things that happen in our life for eg. sometimes our pizza is chewy and the other times our phone doesn\u2019t charge. Moreover, these life problems don\u2019t tend to go and keeps coming then and now in our life. Today, to make your life easy we have come up with some bright ideas\u00a0. So here we come up with a video\u00a0, click the link below and learn some creative ideas that would be useful for the rest of your life\u00a0.\n\nhttps://www.youtube.com/watch?v=bXnYivrfH4I&t=1s\n\nAlthough you can thank me later for these ideas that can be useful in your life\u00a0. Don\u2019t forget to subscribe for more videos and more brilliant ideas that can be learned easily and can be useful for the rest of our life\u00a0.", 
        "title": "CLEVER IDEAS THAT MAKES MEN\u2019S LIFE EASIER \u2013 Mike Lawson \u2013"
    }
]