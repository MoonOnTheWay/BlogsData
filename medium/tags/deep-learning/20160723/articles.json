[
    {
        "url": "https://medium.com/@lauradonnelly632/learning-b73bcee7b74a?source=tag_archive---------0----------------", 
        "text": "~from When Things Fall Apart by Pema Ch\u00f6dr\u00f6n.\n\nThings seem to regularly fall apart in my life\u200a\u2014\u200ajobs, relationships, plans.\n\nSometimes I spend too much energy trying to fix something that has fallen apart in my life. I remember how it was when it worked, how I relied on it, and the pleasure or joy I received from it. My attempt to \u201ckeep\u201d the thing from falling apart usually arises after it\u2019s already gone.\n\nI made a dance once \u201cThe Transitory Nature of All Things\u201d which was about the impermanence of movement but also life. The 10 dancers each had an individual set of pathways and some directions about speed and movement quality. The rest of the dance was created by the chance meetings caused by the intersections of their paths. This dance was about the flow of movement rather than what happened when people connected. Some audience members were frustrated because it seemed as soon as something was about to develop between two or three dancers they would leave the dynamic of connection and return to completing their individual paths.\n\nI know about transitory\u200a\u2014\u200amuch of my career was spent in dance which is gone the moment you complete the step. Video is a 2D record of the dance but it isn\u2019t the dance, the energy of connection between the performer, the audience, the musicians, and other performers. Dance is a magical thing. For something that exists for only a moment in time it requires energy, practice, and commitment.\n\nLately, I\u2019ve been thinking that the purpose of dance in my life was to teach me about this transitoriness. I\u2019ve circled back to dance as a career three or four times but today I\u2019m feeling like I just may have [finally] learned what I need to know.", 
        "title": "LEARNING \u2013 laura donnelly \u2013"
    }, 
    {
        "url": "https://medium.com/@mats.lundberg/challenges-with-running-deep-learning-frameworks-in-the-cloud-5e2625fb91ab?source=tag_archive---------1----------------", 
        "text": "Notes from talk by Kenny Daniel from Algorithmia at #ddsea16.\n\nSince you want to run Deep learning on GPUs and in the cloud. You need a provider which can provision GPU resources\n\nPythons is your friend! Most popular frameworks run on Python (TensorFlow, Caffe, Theano). One exception is Torch which runs on Lua. And you have Spark MLib, if you are invested into Spark. And really, you want to run deep learning as a service anyhow.\n\nIf you model is many gigs, it\u2019s going to be hard to pass around and startup as necessary.\n\nInteresting research going into this is SqueezeNet. Which has AlexNet performance but the model can take less than 1Mb of space.\n\nSince GPU wasn\u2019t meant to be shared between processes, you are going to run into problems if your Deep learning service has more than concurrent requests.\n\nYou can enable non-exclusive mode, which is a partial fix. Still, if you have many concurrent requests you can get memory overruns which results in weird behavior. Algorithmia fixes this by limiting number of concurrent running processes using semaphores.\n\nRunning a GPU from Docker has two main issues:\n\nNvidia Docker makes life a bit easer, but doesn\u2019t address the two above issues.", 
        "title": "Challenges with running Deep learning frameworks in the Cloud"
    }
]