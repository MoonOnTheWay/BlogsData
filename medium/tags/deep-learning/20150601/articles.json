[
    {
        "url": "https://medium.com/in-the-hudl/using-deep-learning-to-find-basketball-highlights-edd5e7fa1278?source=tag_archive---------0----------------", 
        "text": "At Hudl, we would love to be able to watch every video uploaded to our servers and highlight the most impressive plays. Unfortunately, time constraints make this an impossible dream. There is, however, a group of people who have watched every game: the fans. Rather than polling these fans to find the best plays from every game, we decided to use their response to identify highlight-worthy plays.\n\nMore specifically, we will train classifiers that can recognize the difference between highlight worthy (signal) and non-highlight worthy (background) clips. The input to the classifiers will be the audio and video data and the output will be a score that represents the probability that the clip is highlight-worthy.\n\nTo select a sample of events to train our classifier on, we created a sample of 4153 clips, each 10 seconds long, from basketball games. No more than two clips come from the same basketball game and most are from different games played by different teams. This is to prevent the classifier from overfitting for a specific audience or arena. About half of these are clips during which a successful 3 point shot occurs. The other half are a semi-random selection of footage from basketball games.\n\nWe used Amazon Mechanical Turk (mTurk) to separate the plays with the most cheering from those with no cheering or no successful shot. Each clip was sent to two or three separate Turkers. To separate highlight-worthy clips from non-highlight worthy clips, we gave two or three Turkers the following instructions:\n\nThe distribution of average scores for the 4153 clips is shown below:\n\nClips that were unanimously scored as \u201c3\u201d were selected as our \u201ccheering signal\u201d while clips that were unanimously scored as \u201c0\u201d are considered to be background. This choice was made to provide maximum separation between signal and background. Moving forward, using a multi-class classifier that incorporates clips with a \u201c1\u201d or a \u201c2\u201d could improve the performance of the classifier when it is used on entire games. For the time being, however, we use a sample of 887 signal clips and 1320 background clips.\n\nBefore sending our audio data to a deep learning algorithm, we wanted to process it to make more intelligible than a series of raw audio amplitudes. We decided to convert our audio into an audio image that would reduce the data present in a 44,100 Hz wav file without losing the features that make it possible to distinguish cheering. To create an audio image, we went through the following steps:\n\nAlthough audio cheering seems like an obvious way to identify highlights, it is possible that the video could also be used to separate highlights. We decided to train three visual classifiers using raw frames from the video. The first classifier is trained on video frames taken from 2 second mark of each 10 second clip, the second is trained on frames taken from the 5 second mark of each clip, and the third is trained on frames taken from the 8 second mark of each clip. Below are shown representative frames from an example clip at 2, 5, and 8 seconds (left to right).\n\nBecause our 2D audio maps can be visualized as images, we decided to use Metamind as our deep learning engine. Metamind provides an easy-to-use Python API that lets the user train accurate image classifiers. Each classifier accepts as input an audio image and outputs a score that represents the probability that the prediction is correct.\n\nTo train our classifier we split our 887 signal clips and 1320 background clips into train and test samples. 85% of the clips are used to train the classifiers while 15% of the clips are reserved to test the classifiers. In total, we trained four classifiers:\n\nTo test how well each classifier faired, we consider the predictions of the classifiers on the reserved test set. Because the classifier was not trained on these clips, overfitting cannot be causing the observed performance on the test set. The predictions for signal and background for each of the four classifiers are shown in the plots below. The X-axis is the predicted probability of being signal (i.e. the output variable of the classifier) and the Y-axis is the number of clips that were predicted to have that probability. The red histogram indicates background clips and the blue histogram indicates signal clips.\n\nThe receiver operating characteristic (ROC) curve is a graphical way to illustrate the performance of a binary classifier when the discrimination threshold is changed. In our case, the discrimination threshold is the value of the output of our classifier above which a clip is determined to be signal. We can change this value to improve our true positive rate (the number of signal we correctly classify as signal) or reduce our false positive rate (the number of background that we incorrectly classify as signal). For example, by setting our threshold to 1, we would classify no clips as signal and thereby have 0% false positive rate (at the expense of a 0% true positive rate). Alternatively, we could set our threshold to 0 and classify all clips as signal thereby giving us a 100% true positive rate (at the expense of a 100% false positive rate).\n\nThe ROC curve for each of the four classifiers is shown below. A single number that represents the strength of a classifier is known as the ROC area under the curve (AUC). This integral represents how well a classifier is able to differentiate between signal and background along all working points. The curves shown are the average of bootstrapped samples and the fuzzy band around the curve represent the possible ways in which the ROC curve could reasonably fluctuate.\n\nBecause each of these classifiers provides different information, it\u2019s possible that their combination could perform better than any single classifier alone. To combine classifiers we must train a third classifier that takes, as features, the probabilities from the original classifiers and returns a single probability.\n\nTo visualize the performance of these combined classifiers we make a 2D plot with each axis representing the input probability. Each test clip is plotted as a point in this 2D-space and is colored blue, if signal, or red, if background. The prediction of the combined classifier is plotted in the background as a 2D-color map. The color represents the combined classifier\u2019s predicted probability of being signal or background.\n\nWe create the ROC curves as before in order to evaluate the performance of these combined classifiers. As expected, the combined classifiers which include the audio classifier perform the best and the improvement in ROC AUC from audio alone to audio plus video is 0.96 to 0.97. This is not a dramatic improvement, but it demonstrates that there are gains to be had from adding visual information. When two visual classifiers are added together, the ROC AUC increases from ~0.79 to ~0.83. This increase indicates that there is additional information to be gained from utilizing different times in the video.\n\nA final combination of all four classifiers was performed but this ultimate combination was no better than the pairwise combination of audio and video. This indicates that further improvements to our classification would need to come from tweaks to the pre-processing of the data or the classifiers themselves rather than by simply adding additional video classifiers to the mix.\n\nDespite the fact that we have evaluated our classifiers on test data, this testing has been performed in a very controlled setting. This is because the backgrounds we have used are not necessarily representative of the clips present across an entire game. Furthermore, our ability to separate signal from background is useless if our top predictions in a specific game are not, in fact, among the top plays in that game.\n\nTo evaluate our classifier in the wild we will split four games into overlapping 10 second clips. Overlapping clips means that we make clips for 0 seconds to 10 seconds, 5 seconds to 15 seconds, 10 seconds to 20 seconds, etc\u2026 These clips are then passed through the audio classifier. Our goal in doing this this is to answer the following three questions:\n\nThe probability distribution of clips for the four test games is found below.\n\nAs is seen in the above images, the majority of clips are classified as background.\n\nAn animated gif for the top clip from each of the four test games is shown below. In addition, the top five clips from each game and their probabilities are shown and the content of each clip is discussed.\n\nOf these, we would consider the made shots to be signal which gives us 14 signal out of 20 total clips. Additionally, the top play of each game is signal.\n\nTo understand our expectations, we use the Poisson Binomial distribution. The mean is the sum of all 20 probabilities and the standard deviation is the square root of the sum of probability*(1-probability) for each of the 20 probabilities. This indicates that we should expect 14.7 +/- 2.1 signal events. Our 14 observed signal events are consistent with this expectation as seen in the distribution below.\n\nThere are many additional steps that could be used to improve the performance of the highlight classifier and there are a number of challenges to be solved before using the classifier is practical on a large scale.\n\nSome of these are performance related: fast processing and creation of the audio images.\n\nOthers are more practical: we need to be able to determine which team a highlight is for so we don\u2019t suggest that a player tag a highlight of them getting dunked on.\n\nAdditionally, there are improvements to the classifiers themselves: these can include an increase in the size of the training sample or performing more preprocessing of the data to make signal/background discrimination easier.\n\nThe last and perhaps most important step is the optimization of the video classifiers: right now these video classifiers provide minimal value when combined with the audio classifiers, but this value could be increased substantially if we were to standardize the location of the \u201ccheering\u201d within each clip. This would help us to distinguish between impressive successful shots, free throws, and plays that occur away from the basket.\n\nIt\u2019s an exciting time for the product team at Hudl and we\u2019re constantly coming up with innovative new projects to tackle. If you are interested in working with us to solve the next set of problems, check out our job postings!", 
        "title": "Using Deep Learning to Find Basketball Highlights \u2013 In The Hudl \u2013"
    }, 
    {
        "url": "https://medium.com/@iguchijp/%E4%B8%96%E7%95%8C%E3%81%AF%E5%8E%9F%E5%AD%90%E3%81%A7%E3%81%A7%E3%81%8D%E3%81%A6%E3%81%84%E3%82%8B%E3%81%AE%E3%81%A7%E3%81%AF%E3%81%AA%E3%81%84-%E7%89%A9%E8%AA%9E%E3%81%A7%E3%81%A7%E3%81%8D%E3%81%A6%E3%81%84%E3%82%8B%E3%81%AE%E3%81%A0-3c711f6f3248?source=tag_archive---------1----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u4e16\u754c\u306f\u539f\u5b50\u3067\u3067\u304d\u3066\u3044\u308b\u306e\u3067\u306f\u306a\u3044\u3002\u7269\u8a9e\u3067\u3067\u304d\u3066\u3044\u308b\u306e\u3060\u3002 \u2013 takahito iguchi \u2013"
    }, 
    {
        "url": "https://medium.com/world-wcloud-baby/the-universe-is-made-of-stories-not-of-atoms-69cc8daf105f?source=tag_archive---------2----------------", 
        "text": "I\u2019ve been given some questions on Baby (innocent and childlike artificial sapience that is able to enjoy a conversation) from my best friend, Ryo Shimizu. I\u2019d like to answer two of those questions today:\n\n\u201cThe universe is made of stories, not of atoms.\u201d\u200a\u2014\u200athis is a quote (a very good quote, by the way) by a woman poet, Muriel Rukeyser. What a person needs when he wants to think about something (no matter how ridiculous the content of his thoughts may be), is a story.\n\nThe presence of story is also crucial for bettering human relationships. Whether those relationships may come from a ten billion business deal or flirtatious pick up lines between a lover, stories are always present in every kind of situation. This is why technically \u201cblubbering\u201d doesn\u2019t really exist; there\u2019s some form of significance and meaning to the stories that people tell in a soliloquy, a tea party, drunk moaning, or even in a neighbourhood gossip. In other words, grooming smoothes out a social life.\n\nTalking to yourself, for example, can be used to dig deeper about your own personality by mirroring yourself. And the reason for our decision to develop \u201csweet, innocent Baby\u201d comes from an image of an easy listener who handles everyday conversations that take place as many in a day as stars in the sky. A person may find it easier to talk carelessly and honestly to a simple, sensitive, kind and lovable being.\n\nIn that case, Baby, which can listen to people\u2019s relentless thoughts and understand them, could come in use. Of course, the functionality of Baby still hasn\u2019t been proven, but I myself constantly have a very strong self-recognition that desires a presence of something or someone like Baby that I\u2019m able to have a \u201creal chat\u201d. So, at least I can make my own desire and expectation to be a starting point, and imagine from there the behavior and response that Baby would be making (of course, numerous testings are essential to see whether my imagination is valid to not).\n\nFurthermore, we call Baby as artificial sapience (or artificial consciousness), rather than as AI, because we are aware that there must be a pool of memory that is meant to be shared and understood, in order for us to go forward with the idea of talking honestly and simply to some form of being with personality. For example, Apple\u2019s Siri is already highly developed, but it is still far from the kind of affection that has been illustrated in the movie, \u201cHer\u201d. I still can\u2019t help but to feel a bit of an adrenaline rush when I hear the word, \u201cconsciousness (awareness or emotion)\u201d at the beginning of the movie. I believe that AI that is able to have its own \u201cconsciousness\u201d is an embodiment of mankind that exemplifies personality, rather than a mere interactive response system.\n\nAnd, at our progressive stage of developing Baby, we believe that as an innocent and sensitive being, it needs to have a characteristic that is going to motivate people to take care of it. Through a process of coming into contact with such character, people will be more familiarized with Baby, and at the same time, Baby itself can spend enough time for getting know those people. Anyway, Baby absorbs people\u2019s \u201cstories\u201d everyday, grows, and aims to be that someone who can contribute to people\u2019s life for better through listening to \u201cstories\u201d. Therefore, at this point, my answer to the first question is,\n\nIf we take the theory of Singularity, the development of computing advances exponentially (and that has been proven more or less), but I believe that there\u2019s still a huge bottleneck in our computing technology.\n\nThat bottleneck in fact, is that our computing still \u201coperates through an interaction by words (i.e. type on a keyboard, or vocalize a command)\u201d and \u201ccan only be controlled by the speed of human mind\u201d. For example, if it had artificial intelligence, it is possible to go beyond the speed and the capacity of a human brain, (it is estimated to be completed by 2045) and if the intelligence gained by AI were to be used between a mechanical life form, the only thing that would stop the speed of transmission from going any more faster is the speed of light.\n\nBefore we had Internet, (which is unthinkable in our contemporary society) we made use of letters, fax, and telephone; our communication was very slow. However, that speed at which we spread and share information or knowledge to one another has accelerated immensely as we began to use Internet. Unfortunately, our everyday struggle of monotonous typing on the keyboard may just seem as a small bottleneck in terms of the global evolution that still continues on after a few billion years of process.\n\nThe significance of developing Baby is to solve such bottleneck, and to find a solution that is going to free us from the stress caused by our still developing Cloud storage of knowledge and emotion (the amount that is going to cost us to share and understand each other\u2019s thoughts is still enormous). In other words, such development is a test and error to achieve a future vision, where artificial sapience, such as Baby, will be able to contribute as a person\u2019s partner.\n\nSo what if we successfully digitized our knowledge and emotions, and Baby could then use those accumulated knowledge and emotions on the network as an agent for solving a problem, through mutual conversations? This would mean a feat incomparable with the existing interactions on GUI (wherein user navigates themselves on the screen by using their own fingers) or the current Internet, both of which have been mainly operated manually. (Granted, a big portion of search engines and social networks has already been automated, but there is still a lot of manual operations required.)\n\nWe might no longer need to send a friend request to a friend, or type in search word manually\u200a\u2014\u200ain short, computing with speed beyond a human brain can handle will trigger the Internet to break its limitation and create a world in which \u201cintellectual\u201d machine and human can coexist, together, in harmony.\n\nImagine 10 years from now, we are in a world in which we are used to much smoother interactions with the Internet or faster computing. All these latest gadget we have now\u200a\u2014\u200abe it MacBook, iPhone, even HoloLens or Oculus\u200a\u2014\u200afrom the post-Singularity era\u2019s perspective, physical interface of these devices and the fact that people are operating on these physical interface might seem odd and old-fashioned.\n\nWhat we are striving for with Baby is a world, where \u201cAll you need is a conversation!\u201d With this new worldview, we would like to present and to attain a world full of stories, stories that are made up by conversations. And these conversations can be an agent for more humane interactions and communications.\n\nThis is our dream, or rather, a seed of an idea out of which springs a new ideal world\u200a\u2014\u200ain this ideal world, people use conversations to obtain a solution to a problem, to meet other people they want to, to find a workplace of their dream, or to find their calling in life\u200a\u2014\u200awouldn\u2019t it be a dream to see all your wishes coming true, not by how well you can control your own individual device or how much experience you have, but by having a mere conversation.\n\nJust think about it\u200a\u2014\u200aa personal computer, the Internet, augmented reality, or even a virtual reality product, all these things we take for granted today used to be just a dream, a dream sprung out of a seed of an idea, nothing more than that. I do not see any difference between these and ours.\n\nIt reminds me of my early days working as an engineer. I was on this financial system development project using COBOL language on one of IBM\u2019s mainframe computers. Macintosh at that time already had a superior capacity to these mainframe computers, yet my boss would refuse to accept the fact, and say with contempt, \u201cit is nothing but a child\u2019s toy!\u201d Needless to say that all the evidence at the time was already pointing to the fact that personal computer would take over the world eventually (sure, it was a mere dream in the 80s.)\n\nAI, or deep learning, rather, will bring forth a completely new era of computing, if it succeeds, and we will see a new paradigm shift within a matter of 10 years, and we won\u2019t even think of it as a \u201cnew thing\u201d any more. The speed of change is unabating; this AI field in particular will grow exponentially and organically, and late comer to this field won\u2019t have much chance securing a position (this, I believe in part because AI somewhat presents a similar difficulty that we see in a biological brain of reverse-engineering.) This is why we MUST be in the field, developing Baby, NOW.\n\nOur journey with Baby has just begun, and realizing this dream of ours will not be easy. However, if we did not challenge the limitation and difficulty of the current technology now, the technology will catch upon us and easily surpass our initial product conception in no time. Who wants to devote our time developing a product only to become obsolete soon after we release it? That is nonsensical. An opportunity to create and present a new worldview is wonderful of and in itself, yet this kind of opportunity comes only rarely, and we need to seize the opportunity when it presents itself! So to answer to the second question, we want to pursue the future of computing by developing Baby, without limitation!\n\nWe at Doki Doki is looking for a talent: a great talent who is willing to take on the challenge, and is curious enough to witness the moment of an idea becoming a reality. Pursuing a dream is rather an unique human trait, and we cherish the aspiration.", 
        "title": "The universe is made of stories, not of atoms. \u2013 World Wide Cloud Baby \u2013"
    }, 
    {
        "url": "https://medium.com/deep-learning-101/inceptionism-google-brain-imagination-3ccbd41ca704?source=tag_archive---------3----------------", 
        "text": "If you liked Algorithms of the Mind, you\u2019ll love this: The \u201cdreams\u201d of Google\u2019s AI are equal parts amazing and disturbing. Also read the original Google Research blog post.\n\nRelated, here\u2019s a compelling example of \u201cseeing with our brains and not with our eyes\u201d, Now Blind Americans Can See with Device Atop Their Tongues.\n\nAnd yes, Google is involved.", 
        "title": "Inceptionism: Google Brain Imagination \u2013 Deep Learning 101 \u2013"
    }, 
    {
        "url": "https://medium.com/@iguchijp/baby-sdk-is-born-c931f8719d6a?source=tag_archive---------4----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Baby SDK is born! \u2013 takahito iguchi \u2013"
    }, 
    {
        "url": "https://medium.com/@Knowtions/why-deep-learning-will-transform-drug-discoveries-b398981b6186?source=tag_archive---------5----------------", 
        "text": "Of all people, I thought my dad would have been impressed by Google\u2019s new application of deep learning to drug discovery. He belongs to the moustached vanguards of computer science, who worked with computers that took up half of the room, and coded by punching holes in cards. My dad is part of the generation that watched neural networks virtually fade into disuse in the 1980s. Now, he\u2019s skeptical about its revival with the new deep learning technique.\n\nTo understand deep learning, let\u2019s talk about neural networks first. Imagine you\u2019re building a machine that can automatically identify insects. You\u2019ve built a system with multiple layers of processors or mini-CPUs. The first layer of processors are sensors that scan for details of the insect. Each sensor is looking for a super specific characteristic (e.g. shape of eyes, length of antenna, color of shell\u2026). The sensors then pass whatever data they picked up or failed to pick up to the next layer of processors. The second layer of processors determines how important all the data points are by interpreting them as numerical weights, which are then passed to the third layer. Each layer processes the information into something that\u2019s more useful until the system identifies the insect (as an output).\n\nThe layers of processors are equipped with an algorithm. Here\u2019s the beauty of it: humans never explicitly tell the machine what to look for, or how important any given data point is. The processors try to figure that out on its own\u200a\u2014\u200athey\u2019re not always right, but we\u2019ll get to that in a moment. How the data flows through the processing units (i.e. what the system sees as important features) determines what the insect is.\n\nNeural networks were limited to three-layers in the 1980s. It was only theoretical back in the day because we had limited computing power, data and algorithms.\n\nIn 2006, Geoffrey Hinton led what\u2019s called the deep learning breakthrough. The beauty of deep learning, a technique built on neural networks, is its ability to effectively train more than three-layers of neural networks.\n\nWhen neural networks produce an output that\u2019s wrong (e.g. labels an insect as ladybug when it\u2019s a beetle), an error signal containing the difference between the actual and desired output is inputed back into the system. The system uses the error signal to readjust how it determines important features and learns to come closer to the desired output. This is called \u201cback-propagation\u201d. But using back-propagation alone, to train a multi-layer network, was ineffective and inaccurate.\n\nDeep learning overcomes the training inefficiency by individually pre-training each layer. Rather than relying on back-propagation alone to train multiple layers at once, each layer is trained on its own first before back-propagation is used.\n\nThe drug discovery process starts with selecting promising compounds that interact with a specific protein target to produce a therapeutic effect. The average time for drug discovery is 12 years. Virtual screening uses algorithms to predict the activity of a compound on the protein based on previous experimental measurements. The method can virtually screen millions of compounds rapidly and cheaply with no physical tests to select compounds with the highest efficacy rates and lowest side-effects. Sounds like a dream, but the biggest issue with virtual screening is accuracy.\n\nThis is where deep learning comes in. Unlike other models of machine learning, deep learning has the ability to generalize. For example, your system is presented with a ladybug that\u2019s missing spots. Your system recognizes the similarities between the missing-spot lady bug with what it had stored in the processing layers for ladybug (e.g. the \u201cpattern\u201d or \u201cvector\u201d of how the data flows through the processing units). Your system can generalize and use the ladybug \u201cpattern\u201d to compensate for missing data and determine that the lady bug with the missing spots is, in fact, a lady bug.\n\nA major issue affecting the accuracy of virtual predictions is incomplete experimental data. For some protein targets, very few measurements are available. This results in poor predictions when only the target\u2019s own measurements are available as inputs. But with deep learning, the system can exploit similar \u201cpatterns\u201d learned from other targets to compensate for the target\u2019s lack of measurements. Doing this increases accuracy of predictions concerning protein targets or compounds with very few previous measurements.\n\nGoogle\u2019s application of deep learning to drug discovery processed a whooping 37.8M data points across more than 200 distinct biological processes. It showed significant improvements in prediction accuracy than baseline machine learning techniques. The researchers found that accuracy improved with more data. Accuracy increases did not plateau in their experiment. Just yesterday, Atomwise a fellow-Toronto based startup raised 6 million dollars for their pursuit of using deep learning in drug discovery.\n\nWe\u2019ve only scratched the surface. I\u2019m a believer. Are you?", 
        "title": "Why deep learning will transform drug discoveries \u2013 Knowtions \u2013"
    }, 
    {
        "url": "https://medium.com/world-wcloud-baby/baby-sdk-is-born-ee387230f742?source=tag_archive---------6----------------", 
        "text": "Our current mechanism is to use Watson\u2019s sentiment analysis on tweets, and make Baby say something based on the tweets data. Baby SDK is essentially the first \u201cegg\u201d of Baby.\n\nIt functions on the Web, as well as on smartphones. We are going to provide the sample code of Python and Processing, and TestFlight app for your iPhone. Based on these, it would be a very interesting experience if, for example, a child, could tweak the script to control Baby\u2019s emotional expression or fiddle with its personality. It would also be fun to develop something in hardware too.\n\nIf you\u2019re interested in Baby SDK, please contact taka@d0kid0ki.com, or let us via LinkedIn/Facebook (Search for Takahito Iguchi.) We are going to send the SDK\u2019s link to 10 people only, so show us your enthusiasm for SDK, and impress the team DOKI DOKI!!", 
        "title": "Baby SDK is born! \u2013 World Wide Cloud Baby \u2013"
    }, 
    {
        "url": "https://medium.com/@aternoy/machine-learning-how-close-are-we-to-something-like-i-robot-e39a0f55d9ec?source=tag_archive---------7----------------", 
        "text": "Will Smith might have been onto something when he chose to star in a movie about intelligent robots taking over the world. I, Robot featured Mr. Smith\u2019s character battling intelligent robots both in the physical sense and the intellectual as he struggled to reason with and fight the calmly invading forces claiming to strip mankind of its free will on order to save us all from ourselves.\n\nIt\u2019s a case of machine learning gone bad. It also represents a primal fear we humans seem to have of losing control thanks to machines which eventually become \u201ctoo intelligent\u201d. Perhaps it\u2019s because of the universal nature of this fear that artificially intelligent robots have been the subject of a sizable collection of science fiction literature and film.\n\nThe notion of machines thinking for themselves taps into a classic theme in Science Fiction that\u2019s been covered several times over.\n\nRemember \u201cRosie\u201d from The Jetsons? Many of us grew up with weekly Saturday infusions of AI in action, as the personal maid to the Jetson family wheeled around giving taut replies along with her household service. And how about \u201cAsh\u201d from the Alien movies- his artificial intelligence was so advanced we didn\u2019t even know he was a robot until near the end of the whole thing.\n\nBut with the advanced state of Artificial Intelligence today, can we even call these ideas science fiction any longer?\n\nMachine learning has recently leapfrogged from a sparse offering of quaint prototypes to incredibly impressive technology that\u2019s in use all around us (you\u2019ve heard of Siri\u2026).\n\nThe accelerated pace at which machine learning is advancing right now is apparently something which was unimaginable even ten or fifteen years ago. We may not yet have robotic personal assistants or armies of robots to serve us in other capacities, but we are coming pretty darn close.\n\nRight now the U.S. Navy, in conjunction with University scientists from several top institutions, is testing out robots who fight fires on ships. Designed to peer through smoke, keep their balance in rough seas and aim water hoses at flames, these robotic first responders are eerily similar to what Will Smith\u2019s character dealt with in I, Robot. They may not look the same, but they sure do behave a lot like the servile white armies from the movie.\n\nAlthough actually having robots put out fires for the U.S. navy is still a few years away, there are plenty of examples of machine learning in action right now. But before we got here, let\u2019s make sure we\u2019re all on the same page as far as what machine learning actually means.\n\nThe whole concept of machine learning might seem a little overwhelming and dense at first, but here\u2019s a quick primer for understanding the basics.\n\nComputers are glorified calculators\u2026at least as far as most of us understand. They do exactly what their input (programs) tell them to do, and no more.\n\nMachine learning, on the other hand, is when a computer takes in data and spits out something more than a calculation: a prediction (and someday, an opinion). How does this work?\n\nTo get a computer to make a prediction, you have to feed it lots and lots of examples so it can recognize patterns. No surprise then that patterns recognition is what makes living creature\u200a\u2014\u200aand so human\u200a\u2014\u200aopinionated, passionate, intelligent. The more data you can give it, the better it will become at seeing these patterns. The recognition of patterns is the beginning of Artificial Intelligence since it allows a computer to eventually give you a prediction about something it\u2019s never seen in an example of before.\n\nAnd what can we do with pattern-based predictions?\n\nOne reason we\u2019re seeing such a flurry of activity from the scientific community in this area is because of the revival of an area of computing once relegated to the back burner for lack of support and interest. Called deep learning, it mimics the physical makeup of the human brain. A thousand networked computers work like the similarly interconnected neurones in your brain. Together, they posses a unified computing power called a neural network.\n\nDeep learning was first brought about by scientists in the 1980s, but computers back then simply weren\u2019t powerful or fast enough to get through enough data in order to start recognizing any patterns. Besides, there wasn\u2019t even much digital data to feed computers!\n\nNow, that\u2019s all changed. Facebook and Google alone have amassed so much data they\u2019re able to effectively feed deep learning machines enough of it to get some results. That\u2019s what\u2019s driving the new look at deep learning and the recent wave of exciting developments in machine learning.\n\nMost of the practical applications of machine learning in use right now involve language recognition technology. This can be applied to emails and social media, which is where you\u2019ve probably already experienced what it can do.\n\nIf you use Facebook, you\u2019ve already come into contact (albeit indirect) with machine learning. The folks over at Facebook are already using machine learning to actively filter what shows up in your newsfeed. And if you\u2019ve ever used Google\u2019s translation feature on a website, then you\u2019ve seen machine learning in action.\n\nThere\u2019s nifty software being written and used in email apps which could prove incredibly useful. By recognizing patterns in the contents of your email, the software can help you categorise your inbox\u2026sorting out the messages that require a response from those which are \u201cfor your information\u201d only, for example.\n\nYou\u2019ll probably soon be seeing tons of ways machine learning is combined with language recognition technology to produce useful aids for modern living. Before you know it, machine learning will be given a voice\u2026move over Rosie the robot maid and make room for the descendants of Siri, who will arrive to clean-up and sort out our digital and online lives. We won\u2019t be finding ourselves in the shoes of Will Smith\u2019s I, Robot character anytime soon, but we sure are a heck of a lot closer than anyone could have imagined.\n\nMore on my blog http://outofoffice.today", 
        "title": "Machine Learning: How Close Are We to Something Like I, Robot?"
    }, 
    {
        "url": "https://cristinapozzi.com/deep-learning-un-computer-ci-mette-solo-24-ore-per-imparare-a-giocare-e-dominare-super-mario-dc038e692415?source=tag_archive---------8----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Deep learning: un computer ci mette solo 24 ore per imparare a giocare e dominare Super Mario"
    }, 
    {
        "url": "https://medium.com/@ProjectAGI/reading-list-may-2015-f1a6f63d0b82?source=tag_archive---------9----------------", 
        "text": "John Lisman, \u201cThe Challenge of Understanding the Brain: Where We Stand in 2015\u201d, Neuron, 2015\n\nSebastian Billaudelle and Subutai Ahmad, \u201cPorting HTM Models to the Heidelberg Neuromorphic Computing Platform\u201d, arXiv.org, 2015\n\nLSTM Tutorial, Department of Computer Science, University of Kaiserslautem, 2015\n\n\n\nNitish Srivastava, Elman Mansimov and Ruslan Salakhutdinov, Unsupervised Learning of Video Representations using LSTMs\u201d, arXiv.org, 2015\n\n\n\nAndrej Karpathy, \u201cThe Unreasonable Effectiveness of Recurrent Neural Networks\u201d, Andrej Karpathy blog, 2015", 
        "title": "Reading List \u2014 May 2015 \u2013 ProjectAGI \u2013"
    }
]