[
    {
        "url": "https://medium.com/intuitionmachine/deep-learning-is-non-equilibrium-information-dynamics-b00baa16b135?source=tag_archive---------0----------------", 
        "text": "There are basically several camps studying neural like systems. There are the folks who insist on a biologically inspired approach. These include firms like Numenta, Vicarious and researchers in the Connectome field. The other camp consists of people of the Bayesian religion. People who believe that some theorem, that was invented in the 18th century, would be the key to unlock our understanding of intelligence. There are also the alchemists who don\u2019t really care about theory and are more than happy to conjure out the latest Residual or Attention model. If the results show \u201cstate-of-the-art\u201d then that concoction must be the right approach.\n\nThe present reality of Deep Learning research is that the alchemists are winning and it\u2019s not even a close contest!\n\nWhy is this so? Why are we at such a poor state of comprehension of Deep Learning? Could it be that the biological theorists or the Bayesian zealots are using the wrong toolbox?\n\nOne major shortcoming of our present day mathematical toolbox is that it is relevant only in conditions that are in equilibrium.\n\nUnfortunately, the conditions for learning do not happen in an equilibrium state. Rather they happen at a state of non-equilibrium. It is like trying to take measurements after the fact rather than when it is happening. To measure only when a system is in equilibrium (or assume the central limit theorem) is to make observations only after the entire play is over. To understand Deep Learning, one needs to have a grasp as what happens in non-equilibrium at the transition between order and chaos.\n\nDeep Learning are not biological systems nor are they physical systems. Many researchers derive their intuition from either contexts. However if you have grounded yourself in Newton\u2019s classical mechanics, then the likelihood of you ever discovering Quantum mechanics is next to nil. Unless, you take a close look at the experimental data and realize that your world view is actually flawed. Deep Learning are information systems, not biological and not physical and therefore should be studied as such. That\u2019s why the understanding the dynamics of information is of high importance.\n\nInformation systems (alternatively computational systems) consist of 3 fundamental capabilities. These are:\n\nIt is that simple. The Cellular Automata Rule 110 that I describe in a previous post has all 3 of these capabilities. Universal Machines emerges from these 3 operators.\n\nNow you may be asking yourself that it can\u2019t be this simple! The notion that a complex system requires complex constituents is an entirely false assumption. The key to understanding the capabilities of complex systems in in the 3 operators. In fact, in my previous post about \u201c5 Capability Level of Deep Learning Intelligence\u201d, the levels are just different combinations of these 3 operators and of different levels of sophistication.\n\nDeep Learning systems are of course much more capable that being able to perform universal computation. They are capable of not only learning, but also meta-learning. The two core computational (information modification), capabilities are matching and selection. Deep Learning systems consists of ensembles of self-similar matching and selection units. They consist of multiple layers of this and are routed via signaling (information transfer). To make an analogy with another AI technique, its just like a swarm of simple matching and selection machines.\n\nThe key question however is how do these systems learn? This is a complex research subject, but we certainly know one thing, these systems aren\u2019t learning when they are in equilibrium. In fact if we study biological systems, we know that in the non-equilibrium state that the evolution of a system tends towards minimizing relative entropy. That is, the same optimization direction of minimizing the KL divergence (i.e. a measure of difference between two distributions). Furthermore, we know that phase transitions near high mutual information in models. This implies that all too convenient assumption of i.i.d. needs to be thrown in the dustbin. The study of DL must be in the regime of non-equilibrium states and not in the mathematically convenient regime of equilibrium.\n\nOne final thought, you may be also wondering if physics can be captured in a information dynamics (aka computational mechanics) framework. There actually have been several papers that cover that area, specifically in information theoretic terms. This is possibly where that entire notion of reality being in a simulation comes about. One of those topics that I, like Elon Musk, would also like to avoid!\n\nBTW, the image above is an image of the surface of a liquid in a non-equilibrium state. What does it remind of us that we find in biology?\n\nhttps://arxiv.org/abs/1610.08192 Transfer entropy in continuous time, with applications to jump and neural spiking processes\n\nIt occurs to me that many readers, with an interest in AI, don\u2019t seem don\u2019t seem to understand how mathematics is used to model reality. Math doesn\u2019t model the world, you fit math so that it looks like the world. It is the same idea as curve fitting, you hypothesize that a certain formula fits with the world and if it does then you are lucky.\n\nSo as a matter of convenience though, the math formulas that are easy to work with are the ones that are used. Furthermore, because of the limitation of mathematics, simplified systems are used for analysis. The universe doesn\u2019t have a requirement that a closed form equation exists to model its behavior.\n\nThermodynamics equations are based on empirical observations in that unlike other branches of physics, are not derived from first principles. They are about systems in equilibrium and the variables are aggregate measures of a system. Statistical mechanics is a branch of physics that has techniques to study behavior of large collections of interacting particles. If you think it uses statistics because of its name then that\u2019s also a misconception. Under Statistical Mechanics there is Non-Equilbrium Statistical Mechanics which studies systems outside of equilibrium. This is the regime where Nobel prize winner Prigogine did his work. When you get into this \u2018regime\u2019 then that\u2019s where you biological processes and physics meet.\n\nDL systems however are not biological systems and DL systems are also not physical systems. So the closest thing that can model its behavior and have the properties similar to biology is Information Dynamics in the state of Non-Equilibrium.\n\nExplore more in this new book:", 
        "title": "Deep Learning is Non-Equilibrium Information Dynamics"
    }, 
    {
        "url": "https://medium.com/@amarbudhiraja/supervised-language-identification-for-short-and-long-texts-with-code-626f9c78c47c?source=tag_archive---------1----------------", 
        "text": "In this post, will look at language identification for written text such that some text is given and a set of languages, identify which language it belongs to. To this extent, I use the Genesis dataset from NLTK which has six languages\u00a0: Finnish, English, German, French, Swedish and Portuguese.\n\nApproach 1: Consider Stop Words as Features of Language.\n\nHypothesis 1: For the given corpus, analyze top-K words and see how many of these are in a pre-defined stop words list. The stop-words of the language containing the most top-K words is the language of the document. For experiments, we use the Genesis Dataset (from NLTK) which has text in multiple languages.\n\nHypothesis 1 Verification: For the above approach to work, the frequency distribution of the documents considered should follow Zipf\u2019s law. In the next step, I analyze Frequency Distribution of the dataset for verification.\n\nThe above charts show that the considered languages(corpus) are following a Zipf\u2019s law and hence, will have certain words that are used more frequently the others.\n\nIn NLP literature, these words are termed as Stop-Words. Next we will see how many sentences does each language in the genesis corpus.\n\nNow, to create a dataset, we create paragraphs such that each paragraph has about 50 sentences.\n\nThe following function does the computation.", 
        "title": "Deep Learning for Supervised Language Identification for Short and Long Texts! (With Code)"
    }, 
    {
        "url": "https://medium.com/udacity/this-week-in-machine-learning-9-december-2016-772a4fa3c296?source=tag_archive---------2----------------", 
        "text": "This week\u2019s top Machine Learning stories: searching for aliens, writing music, predicting dementia, and more!\n\nMachine Learning is one of the most exciting fields in the world. Every week we discover something new, something amazing, something revolutionary. It\u2019s incredible, but it can also be overwhelming. That\u2019s why we created This Week in Machine Learning! Each week we publish a curated list of Machine Learning stories as a resource to help you keep pace with all these exciting developments. New posts will be published here first, and previous posts are archived on the Udacity blog.\n\nWhether you\u2019re currently enrolled in our Machine Learning Nanodegree program, already working in the field, or just pursuing a burgeoning interest in the subject, there will always be something here to inspire you!\n\nUniversity of Toronto Scarborough researchers turn recommendation algorithms developed by Google and Netflix to the sky to search for extraterrestrial life.\n\nSan Jose State University researcher Margareta Ackerman develops a machine learning system that can write melodies to fit supplied lyrics.\n\nUber purchases AI startup Geometric Intelligence, leveraging the company as the foundation of its new internal research lab focused on self-driving cars.\n\nUnity hires Dr. Danny Lange, Uber\u2019s former head of machine learning, to lead its efforts to apply AI techniques to new problems in augmented and virtual reality.\n\nUsing big data techniques, doctors in Sweden and Finland develop a risk index for dementia that can predict the onset of the disease ten years in the future.\n\nMachine learning allows researchers to predict the properties of as-yet uncreated 2D materials, helping accelerate the development of such materials.", 
        "title": "This Week in Machine Learning, 9 December 2016 \u2013 Udacity Inc \u2013"
    }, 
    {
        "url": "https://medium.com/@YvesMulkers/why-deep-learning-is-radically-different-from-machine-learning-312c9c7b7b5b?source=tag_archive---------3----------------", 
        "text": "There is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL), yet the distinction is very clear to practitioners in these fields. Are you able to articulate the difference?\n\nThere is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL). There certainly is a massive uptick of articles about AI being a competitive game changer and that enterprises should begin to seriously explore the opportunities. The distinction between AI, ML and DL are very clear to practitioners in these fields. AI is the all encompassing umbrella that covers everything from Good Old Fashion AI (GOFAI) all the way to connectionist architectures like Deep Learning. ML is a sub-field of AI that covers anything that has to do with the study of learning algorithms by training with data. There are whole swaths (not swatches) of techniques that have been developed over the years like Linear Regression, K-means, Decision Trees, Random Forest, PCA, SVM and finally Artificial Neural Networks (ANN). Artificial Neural Networks is where the field of Deep Learning had its genesis from.\n\nSome ML practitioners who have had previous exposure to Neural Networks (ANN), after all it was invented in the early 60\u2019s, would have the first impression that Deep Learning is nothing more than ANN with multiple layers. Furthermore, the success of DL is more due to the availability of more data and the availability of more powerful computational engines like Graphic Processing Units (GPU). This of course is true, the emergence of DL is essentially due to these two advances, however the conclusion that DL is just a better algorithm than SVM or Decision Trees is akin to focusing only on the trees and not seeing the forest.\n\nTo coin Andreesen who said \u201cSoftware is eating the world\u201d, \u201cDeep Learning is eating ML\u201d. Two publications by practitioners of different machine learning fields have summarized it best as to why DL is taking over the world. Chris Manning an expert in NLP writes about the \u201cDeep Learning Tsunami\u201c:\n\nNicholas Paragios writes about the \u201cComputer Vision Research: the Deep Depression\u201c:\n\nIt might be simply because deep learning on highly complex, hugely determined in terms of degrees of freedom graphs once endowed with massive amount of annotated data and unthinkable\u200a\u2014\u200auntil very recently\u200a\u2014\u200acomputing power can solve all computer vision problems.", 
        "title": "Why Deep Learning is Radically Different From Machine Learning"
    }, 
    {
        "url": "https://medium.com/@JoshMangus/big-data-is-not-artificial-intelligence-8f7e7219f32a?source=tag_archive---------4----------------", 
        "text": "Big Data : We should have a pretty good concept of big data by now from the progressing posts. Big data refers to the massive data sets brought in by new technologies and the Internet of Things (IoT). Typically, these data sets are very large and complex, so different tools are used in data mining and companies have quickly emerged that specialize in housing, mining, and modeling big data . Competing in big data as a business is sometimes colloquially called \u201cthe game of picks and shovels\u201d to capture how data is drilled down, examined, and resurfaced with relevant information that can drive firms to make more substantiated decisions. It is important to note that people are the data scientists that make bid data analytics possible and useful in business application.\n\nOftentimes, in discussions about big data, I hear other terminology used and grouped in to how big data will change the way of business. While it may be true, the phrasing used is often incorrect within the context it is used and for what purpose. Normally, this would not be a big deal, but I think it is important to distinguish the following terms and explore how they relate to one another: big data, artificial intelligence, machine learning, and deep learning.\n\nArtificial Intelligence: AI is concept that includes when computers can begin to process data, with little to no human oversight, to make connections in complex relationships between data sets. AI technologies can preform some tasks faster and with more accuracy than human can. It is a popular topic in science fiction and is starting to confirms it\u2019s place in today\u2019s world as well. But what mechanisms can AI use that drives its functionality? This is where we get into machine learning.\n\nMachine Learning: In its most basic form, machine learning deals with using algorithms to parse data, learn from it, and use it to make a prediction or a conclusion. In this iteration, machines are trained using large amounts of data to accomplish a particular task and the machine will learn over time how to complete the task as it increases in complexity. This concept was derived from that same AI crowd and has evolved to include logic programming and network clustering to achieve the general structure of AI. Machine learning approaches have served as the backbone for AI and are continually used for deep learning methodologies.\n\nDeep Learning: Deep learning is another algorithmic approach in which the machine will take unstructured big data and use more cognitive-based computing to learn more over time, based on experience. Deep learning has certainly enabled more practical applications of machine learning and made great contributions to the overall field of AI.\n\nAs the different capabilities are explored, we see the wave of AI quickly following big data. For the first time, companies can use big data to position products and respond to customer needs. They know their methods are working because they have data that proves it. But, if companies want to stay competitive as data growth continues to skyrocket, they will need to scale up a platform using some type of artificial intelligence.\n\nAI will help add a layer of understanding to big data to complete extremely complex analytical task much faster than human.\n\nBig data is not artificial intelligence. Big data is used in artificial intelligence. Data labs will continue to develop amazing technologies using these concepts, but it will take the imaginative nature of people in real-world applications to help turn those theories that will change our professional and personal lives for the better.", 
        "title": "Big Data Is Not Artificial Intelligence \u2013 Josh Mangus \u2013"
    }, 
    {
        "url": "https://deephunt.in/deep-hunt-issue-19-6c1cacac8dd5?source=tag_archive---------5----------------", 
        "text": "With an overwhelming increase in attendance this year, NIPS at Barcelona hosted close to 6000 strong community and around 15% of them were women! You can watch all the talks that happened in the accompanying Women in Machine Learning (WiML) event here.\n\nTreading away from Apple\u2019s policy of secrecy, Ruslan Salakhutdinov, Director of AI Research at Apple announced that they will start publishing research from now!\n\nAs the battle for research platforms gets intense, NIPS saw major announcements by DeepMind, FAIR and OpenAI simultaneously but OpenAI\u2019s Universe was the most popular among the three.\n\nOpenAI releases Universe, a software platform for measuring and training an AI\u2019s general intelligence across a thousand environments including the world\u2019s supply of games like GTA V, websites and other applications.\n\nDeepMind annouces open sourcing of their flagship research platform, DeepMind Lab, with the research community. The company has been using it, known previously as Labyrinth, for some time. Read MIT Tech Review\u2019s take on this simulated world.\n\nFacebook open sources TorchCraft, an interface between Torch and StarCraft.\n\nA library that enables deep learning research on Real-Time Strategy (RTS) games such as StarCraft: Brood War, by making it easier to control these games from a machine learning framework, here Torch. Read more about it in this research\n\nThe joint effort between NVIDIA and Yann LeCun announced the NVIDIA Deep Learning Teaching Kit to help educators give their students hands on experience with GPU-accelerated computing.\n\nMicrosoft Research came up with the best souvenir idea\u200a\u2014\u200aPaperLegend card game with stats on research papers!", 
        "title": "\u2014 Issue #19 \u2013"
    }, 
    {
        "url": "https://medium.com/the-data-intelligence-connection/why-deep-learning-is-radically-different-from-machine-learning-75c18b01d397?source=tag_archive---------6----------------", 
        "text": "There is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL), yet the distinction is very clear to practitioners in these fields. Are you able to articulate the difference?\n\nThere is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL). There certainly is a massive uptick of articles about AI being a competitive game changer and that enterprises should begin to seriously explore the opportunities. The distinction between AI, ML and DL are very clear to practitioners in these fields. AI is the all encompassing umbrella that covers everything from Good Old Fashion AI (GOFAI) all the way to connectionist architectures like Deep Learning. ML is a sub-field of AI that covers anything that has to do with the study of learning algorithms by training with data. There are whole swaths (not swatches) of techniques that have been developed over the years like Linear Regression, K-means, Decision Trees, Random Forest, PCA, SVM and finally Artificial Neural Networks (ANN). Artificial Neural Networks is where the field of Deep Learning had its genesis from.\n\nSome ML practitioners who have had previous exposure to Neural Networks (ANN), after all it was invented in the early 60\u2019s, would have the first impression that Deep Learning is nothing more than ANN with multiple layers. Furthermore, the success of DL is more due to the availability of more data and the availability of more powerful computational engines like Graphic Processing Units (GPU). This of course is true, the emergence of DL is essentially due to these two advances, however the conclusion that DL is just a better algorithm than SVM or Decision Trees is akin to focusing only on the trees and not seeing the forest.\n\nTo coin Andreesen who said \u201cSoftware is eating the world\u201d, \u201cDeep Learning is eating ML\u201d. Two publications by practitioners of different machine learning fields have summarized it best as to why DL is taking over the world. Chris Manning an expert in NLP writes about the \u201cDeep Learning Tsunami\u201c:\n\nNicholas Paragios writes about the \u201cComputer Vision Research: the Deep Depression\u201c:\n\nIt might be simply because deep learning on highly complex, hugely determined in terms of degrees of freedom graphs once endowed with massive amount of annotated data and unthinkable\u200a\u2014\u200auntil very recently\u200a\u2014\u200acomputing power can solve all computer vision problems.", 
        "title": "Why Deep Learning is Radically Different From Machine Learning"
    }
]