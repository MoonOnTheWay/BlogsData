[
    {
        "url": "https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767?source=tag_archive---------0----------------", 
        "text": "In this tutorial I\u2019ll explain how to build a simple working Recurrent Neural Network in TensorFlow. This is the first in a series of seven parts where various aspects and techniques of building Recurrent Neural Networks in TensorFlow are covered. A short introduction to TensorFlow is available here. For now, let\u2019s get started with the RNN!\n\nIt is short for \u201cRecurrent Neural Network\u201d, and is basically a neural network that can be used when your data is treated as a sequence, where the particular order of the data-points matter. More importantly, this sequence can be of arbitrary length.\n\nThe most straight-forward example is perhaps a time-series of numbers, where the task is to predict the next value given previous values. The input to the RNN at every time-step is the current value as well as a state vector which represent what the network has \u201cseen\u201d at time-steps before. This state-vector is the encoded memory of the RNN, initially set to zero.\n\nThe best and most comprehensive article explaining RNN:s I\u2019ve found so far is this article by researchers at UCSD, highly recommended. For now you only need to understand the basics, read it until the \u201cModern RNN architectures\u201d-section. That will be covered later.\n\nAlthough this article contains some explanations, it is mostly focused on the practical part, how to build it. You are encouraged to look up more theory on the Internet, there are plenty of good explanations.\n\nWe will build a simple Echo-RNN that remembers the input data and then echoes it after a few time-steps. First let\u2019s set some constants we\u2019ll need, what they mean will become clear in a moment.\n\nNow generate the training data, the input is basically a random binary vector. The output will be the \u201cecho\u201d of the input, shifted steps to the right.\n\nNotice the reshaping of the data into a matrix with rows. Neural networks are trained by approximating the gradient of loss function with respect to the neuron-weights, by looking at only a small subset of the data, also known as a mini-batch. The theoretical reason for doing this is further elaborated in this question. The reshaping takes the whole dataset and puts it into a matrix, that later will be sliced up into these mini-batches.\n\nTensorFlow works by first building up a computational graph, that specifies what operations will be done. The input and output of this graph is typically multidimensional arrays, also known as tensors. The graph, or parts of it can then be executed iteratively in a session, this can either be done on the CPU, GPU or even a resource on a remote server.\n\nThe two basic TensorFlow data-structures that will be used in this example are placeholders and variables. On each run the batch data is fed to the placeholders, which are \u201cstarting nodes\u201d of the computational graph. Also the RNN-state is supplied in a placeholder, which is saved from the output of the previous run.\n\nThe weights and biases of the network are declared as TensorFlow variables, which makes them persistent across runs and enables them to be updated incrementally for each batch.\n\nThe figure below shows the input data-matrix, and the current batch is in the dashed rectangle. As we will see later, this \u201cbatch window\u201d is slided steps to the right at each run, hence the arrow. In our example below , , and . Note that these numbers are just for visualization purposes, the values are different in the code. The series order index is shown as numbers in a few of the data-points.\n\nNow it\u2019s time to build the part of the graph that resembles the actual RNN computation, first we want to split the batch data into adjacent time-steps.\n\nAs you can see in the picture below that is done by unpacking the columns ( ) of the batch into a Python list. The RNN will simultaneously be training on different parts in the time-series; steps 4 to 6, 16 to 18 and 28 to 30 in the current batch-example. The reason for using the variable names is to emphasize that the variable is a list that represent a time-series with multiple entries at each step.\n\nThe fact that the training is done on three places simultaneously in our time-series, requires us to save three instances of states when propagating forward. That has already been accounted for, as you see that the placeholder has rows.\n\nNext let\u2019s build the part of the graph that does the actual RNN computation.\n\nNotice the concatenation on line 6, what we actually want to do is calculate the sum of two affine transforms in the figure below. By concatenating those two tensors you will only use one matrix multiplication. The addition of the bias is broadcasted on all samples in the batch.\n\nYou may wonder the variable name is supposed to mean. When a RNN is trained, it is actually treated as a deep neural network with reoccurring weights in every layer. These layers will not be unrolled to the beginning of time, that would be too computationally expensive, and are therefore truncated at a limited number of time-steps. In our sample schematics above, the error is backpropagated three steps in our batch.\n\nThis is the final part of the graph, a fully connected softmax layer from the state to the output that will make the classes one-hot encoded, and then calculating the loss of the batch.\n\nThe last line is adding the training functionality, TensorFlow will perform back-propagation for us automatically\u200a\u2014\u200athe computation graph is executed once for each mini-batch and the network-weights are updated incrementally.\n\nNotice the API call to , it automatically calculates the softmax internally and then computes the cross-entropy. In our example the classes are mutually exclusive (they are either zero or one), which is the reason for using the \u201cSparse-softmax\u201d, you can read more about it in the API. The usage is to have is of shape and of shape .\n\nThere is a visualization function so we can se what\u2019s going on in the network as we train. It will plot the loss over the time, show training input, training output and the current predictions by the network on different sample series in a training batch.\n\nIt\u2019s time to wrap up and train the network, in TensorFlow the graph is executed in a session. New data is generated on each epoch (not the usual way to do it, but it works in this case since everything is predictable).\n\nYou can see that we are moving steps forward on each iteration (line 15\u201319), but it is possible have different strides. This subject is further elaborated in this article. The downside with doing this is that need to be significantly larger than the time dependencies (three steps in our case) in order to encapsulate the relevant training data. Otherwise there might a lot of \u201cmisses\u201d, as you can see on the figure below.\n\nAlso realize that this is just simple example to explain how a RNN works, this functionality could easily be programmed in just a few lines of code. The network will be able to exactly learn the echo behavior so there is no need for testing data.\n\nThe program will update the plot as training progresses, shown in the picture below. Blue bars denote a training input signal (binary one), red bars show echos in the training output and green bars are the echos the net is generating. The different bar plots show different sample series in the current batch.\n\nOur algorithm will fairly quickly learn the task. The graph in the top-left corner shows the output of the loss function, but why are there spikes in the curve? Think of it for a moment, answer is below.\n\nThe reason for the spikes is that we are starting on a new epoch, and generating new data. Since the matrix is reshaped, the first element on each row is adjacent to the last element in the previous row. The first few elements on all rows (except the first) have dependencies that will not be included in the state, so the net will always perform badly on the first batch.\n\nThis is the whole runnable program, just copy-paste and run. After each part in the article series the whole runnable program will be presented. If a line is referenced by number, these are the line numbers that we mean.\n\nIn the next post in this series we will be simplify the computational graph creation by using the native TensorFlow RNN API.", 
        "title": "How to build a Recurrent Neural Network in TensorFlow (1/7)"
    }, 
    {
        "url": "https://medium.com/@vivek.yadav/how-neural-networks-learn-nonlinear-functions-and-classify-linearly-non-separable-data-22328e7e5be1?source=tag_archive---------1----------------", 
        "text": "Neural networks are very good at classifying data points into different regions, even in cases when the data are not linearly separable. Linearly separable data is data that can be classified into different classes by simply drawing a line (or a hyperplane) through the data. In cases where data is not linearly separable, kernel trick can be applied, where data is transformed using some nonlinear function so the resulting transformed points become linearly separable. A simple example is shown below where the objective is to classify red and blue points into different classes. Its not possible to use linear separator, however by transforming the variables, this becomes possible.\n\nHere, I show a simple example to illustrate how neural network learning is a special case of kernel trick which allows them to learn nonlinear functions and classify linearly non-separable data. I will use the same example from above.\n\nNeural networks can be represented as, y = W2 phi( W1 x+B1) +B2. The classification problem can be seen as a 2 part problem, one of learning W1 and other of learning W2. Changes in W1 result in different functional transformation of data via phi(W1X+B1), and as the underlying function phi is nonlinear, phi(W1X+B1) is a nonlinear transformation of data X. These nonlinear functions are then combined using linear neurons via W2 and B2. Lets got through this process in steps,\n\nAlthough any nonlinear function can work, a good candicate is Relu. Relu is described as a function that is 0 for X<0 and identity for X>0.\n\nWe typically would compute weights for neurons using a backpropogation scheme, but as the objective is only to illustrate how nonlinear functions transform data, I will set these weights by hand. Consider the case where there are 2 features X1 and X2, and the activation input to relu is given by W1X1+X2. In this case, weight on second neuron was set to 1 and bias to zero for illustration. Figure below shows the effect of changing the weight.Therefore changing weight results in changing the region where the values are retained, and the white is where values of points are zero.\n\nNow we add bias to the special case where output of the neuron is X1+X2+B. The effect of changing B is changing the intercept or the location of the dividing line.\n\nFigures above show that by changing B, the intercept of the line can be changed. Therefore, by changing B and W and having multiple regions, different regions in the space can be carved out to separate red from the blue points above. This is the primary mechanism of how neural networks are able to learn complex nonlinear functions and perform complex nonlinear transformations. Infact, if the activation function is set as a simple linear function, neural networks lose their nonlinear function approximation capabilities.\n\nBy changing weights and biasses, a region can be carved out such that for all blue points w2 relu(W1X+b1)+0.1>0. This is shown in the figure below.", 
        "title": "How neural networks learn nonlinear functions and classify linearly non-separable data?"
    }, 
    {
        "url": "https://twop.agile.esm.co.jp/see-ready-fire-aim-and-dry-principle-through-deep-learning-a6f9c55d9699?source=tag_archive---------2----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "\u30c7\u30a3\u30fc\u30d7\u30e9\u30fc\u30cb\u30f3\u30b0\u3092\u901a\u3058\u3066\u898b\u308b\u3001\u69cb\u3048\u6483\u3066\u72d9\u3048\u3001\u3042\u308b\u3044\u306fDRY\u539f\u5247 \u2013"
    }, 
    {
        "url": "https://medium.com/@callmequeentut/time-25a31f4decf0?source=tag_archive---------3----------------", 
        "text": "Ah, time. Time is one of those things that I start to think about and have to stop myself because it blows my mind. But this time (haha see what I did there) I guess I\u2019ll just let my mind wander while typing because thats what a blog is for, I guess.\n\nFirst of all, I\u2019ve been thinking lately about how we try to bend time. And how it never changes. We are constantly trying to make time speed up or slow down to fit our needs. For me, I want this next week before Thanksgiving break to go by faster than a bat out of hell. But then, I want time to halt itself just when Thanksgiving break starts, so I can spend every minute like it\u2019s an hour until the end of break.\n\nI think maybe I\u2019ve convinced myself I can mold time in a way. By deciding to remember something; a moment, a feeling, or whatever, I can keep that moment going for a longer period of time. So basically I\u2019ll tell my mind to dump all the info I was supposed to remember from that hour lecture, basically erasing that hour of my life. But, I\u2019ll make sure to remember the moment I looked around, driving through the mountains at sunset, listening to good music, and being with some of my favorite people. This way, the moment that is only three seconds will last forever.\n\nSo my question is, why? Why do I try and bend time when I know I cannot change it? A minute is still 60 seconds, an hour still 60 minutes. But somehow, an hour spent watching Netflix seems to go by so much faster than listening to an hour long lecture.\n\nNo, this post isn\u2019t going to solve all your problems or earn you a million dollars for reading it (though I wish it would). But I just encourage you to think about time and how it effects you. What moments last forever in your mind? How do you think our world would be different if we didn\u2019t measure time at all? What if we could actually bend time?\n\nTL;DR- Time is weird. I think about it a lot and maybe you should too.", 
        "title": "Time \u2013 Jenna Tuthill \u2013"
    }, 
    {
        "url": "https://medium.com/the-era-of-apis/moderate-nsfw-images-and-video-with-clarifai-api-tutorial-b2cc222193c8?source=tag_archive---------4----------------", 
        "text": "Let\u2019s face it: nudity is an inevitable part of the internet. Spend enough time online and you\u2019re bound to come across some raunchy imagery (whether you intend to or not!). If you\u2019re building an app or website, it\u2019s important to filter out any \u201cnot safe for work\u201d (NSFW) images or video that may be uploaded. After all, no one likes a surprise!\n\nEnter the Clarifai API! This tool uses deep-learning technology to recognize categories, objects and tags from images and videos.\n\nThe API is free for up to 5,000 calls a month. You can do two important functions with Clarifai\u2019s API: general or custom image tagging. With general image tagging, you rely on Clarifai\u2019s artificial intelligence to recognize items based on Clarifai\u2019s pre-built public models (ex. general,Food, Wedding, Travel, Color, NSFW and detecting age/gender/ethnicity or faces). Custom image tagging means that you create your own model (say, with images of your company logo or product) and train Clarifai\u2019s AI to recognize that specific image.\n\nToday, we\u2019ll be using Clarifai\u2019s Public NSFW Model. The NSFW model recognizes nudity in images. With this functionality, you could\u2026.\n\nWhile you can connect to the Clarifai API directly, today we\u2019ll use our free tool, RapidAPI. Why? The live coding feature means that you can start making calls to the API right away from within the browser. Use RapidAPI\u2019s Clarifai Public Models package to access Clarifai\u2019s existing models and RapidAPI\u2019s Clarifai machine learning package to build a custom model. This tutorial will use the NSFW model in the Clarifai Public Models package.\n\nBy the end of this tutorial, you can upload your own images, call the API and see if it actually passes the NSFW test.\n\nClarifai requires an access token before you start making calls to their API. No worries! It\u2019s easy (and more importantly, free) to get one. Here\u2019s how:\n\nVoila! You should now have your and Access Token for the Clarifai API.\n\nNext, head over to RapidAPI.com to run the API and start testing images! Here\u2019s an overview of what you\u2019ll need to do.\n\nThat\u2019s the overview, but now, for some fun. Let\u2019s try out the Clarifai API with an example.\n\nThe Clarifai API is pretty good at distinguishing safe for work content from the raunchier stuff, even if it\u2019s hard to tell on first glance.\n\nTake this light shade photo, for instance.\n\nThose shadows really make it look like\u2026well, you know what it looks like. Lets see if Clarifai\u2019s API will be fooled!\n\nFirst stop, RapidAPI! We\u2019ll go to RapidAPI.com and find the analyzeImageNSFW endpoint on the Clarifai Public Models API package page.\n\nNext, we\u2019ll put in our Clarifai credentials and upload the image (or use a link).\n\nWhat was the final verdict on the risqu\u00e9 lamps? Clarifai said with 97% certainty that the image is, in fact, safe for work. We couldn\u2019t fool them this time!\n\nGo ahead and try it out. You can either play around with Clarifai\u2019s API on the RapidAPI website or export the code directly into your app. Let us know what you think.", 
        "title": "Moderate NSFW Images and Video with Clarifai API [Tutorial]"
    }, 
    {
        "url": "https://medium.com/@CorvilInc/learning-ecosystems-the-real-deep-learning-ab0610d91ebd?source=tag_archive---------5----------------", 
        "text": "Lynda Donovan, Programme Director eLearning, suggests how technology-enabled learning ecosystems can improve the efficiencies of key organisational processes.\n\nWhile reading an article on Deep Learning, I considered the literal meaning of the phrase, rather than the AI-related meaning. I asked myself: how are smart businesses creating learning ecosystems that support \u2018deep\u2019 learning for the benefit of their employees, their customers and the organization itself? Underpinning such ecosystems is the belief that learning, in all its forms, continuously happens in organizations and should be enabled, captured, and channelled to the right people at the right time.\n\nSmart organizations not only support traditional LMS-based formal learning, they also integrate technology-enabled social and informal learning into the mix. The resulting integrated learning and analytics ecosystems can benefit the organization in several ways.\n\nFor starters, they can afford 360-degree insight into employees for talent management purposes. External talent is in short supply. Businesses need to be able to gain insight into hidden experts within their organization who may quietly and informally be contributing to knowledge repositories such as Communities of Practice and who, with minimal training, may be suitable for a role which the organization has struggled to fill.\n\nWhat\u2019s more, the learning ecosystem can act as a performance-support conduit, enabling product information, learning content, and subject-matter expertise to flow freely throughout the organization and to be available on demand to employees and to customers. In addition, it can harness and preserve invaluable institutional knowledge and memory, cushioning the organization in the event of the departure of key employees.\n\nThe secret sauce of successful learning ecosystems is not the technologies themselves, but in the learning architecture upon which the integrated technology infrastructure is based. Not only does the learning architecture provide specifications for the technologies, it also identifies key indicators of learning within the ecosystem so that learning data, mined from the disparate technologies, can be aggregated and analyzed according to key learning and performance indicators.\n\nWhile the value of technology-enabled learning ecosystems is significant, the adoption of such ecosystems by organizations remains low. Many organizations are still wedded to formal learning, under the impression that it is the only form of learning that is worth enabling, tracking, and measuring. However, by corralling their formal learning content within an LMS and not enabling it to flow freely and naturally through social and informal learning channels, those organizations are limiting the reach and re-use of this valuable asset.\n\nAnother barrier to adoption is that with technology evolving at such a pace, Learning and Development practitioners are finding it difficult to adapt and to change their mindset about meaningful indicators of technology-enabled learning. Many still believe that the number of times a learner logs into the LMS or the time they spend in a formal course are acceptable indicators of learning. The leap for them to identify meaningful learning indicators where multiple technologies are involved is considered a step too far and one which requires a level of upskilling.\n\nAt a time when the workforce is becoming increasingly mobile, businesses need ways to capture and harness their valuable organizational knowledge. Organizations who are brave enough to make the leap to well-designed, technology-enabled learning ecosystems supporting formal, informal, and social learning will be better able to address this and many of the other discussed challenges. So, it\u2019s time to let learning, in all its forms, flow freely and naturally through learning ecosystems so that deep organizational learning can take place!", 
        "title": "Learning Ecosystems: The Real \u201cDeep\u201d Learning \u2013 Corvil \u2013"
    }
]