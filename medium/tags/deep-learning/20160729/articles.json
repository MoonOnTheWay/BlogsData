[
    {
        "url": "https://chatbotsmagazine.com/alterra-ai-launches-flight-reservation-in-conversational-bot-interface-powered-by-artificial-501ae460a54a?source=tag_archive---------0----------------", 
        "text": "Our Alterra.ai today announces the launch of flight reservations in its Facebook Messenger bot. Surprisingly, it was done virtually without traditional coding\u2026\n\nAlterra\u2019s Marina is a virtual travel agent. She can recommend travelers where to go on vacation and what to see there. She can then book hotels, and now flights:\n\nMarina understands plain English. Users can chat with her as with a human, almost\u00a0:-). However, Marina is 100% powered by AI algorithms. No humans are involved, whatsoever.\n\nTraditionally, natural language understanding involved a lot of coding. You have to anticipate words and phrases a user could say in each context, recognize patterns of speech, extract predefined keywords, etc. In other words: a lot of rules, a lot of hard-coding (this is how we did hotel booking earlier). It takes a lot of time and effort to write and debug such programs. And they quickly break down when the user deviates from the expected path. This is why many bots feel so dumb.\n\nInstead of writing explicit rules you create an artificial neural network and feed it a training corpus\u200a\u2014\u200ain our case transcribed recordings of conversations between travelers and live travel agents\u200a\u2014\u200athis artificial brain learns how to understand the language, similar to how a rat learns how to navigate a maze, or more realistically, how a fruit fly learns how to find bananas. It is how Google\u2019s DeepMind built its famous Go-playing program. It is how we did our flight booking.\n\nConstructing an artificial neural network is not straightforward. You have to experiment with different algorithms and ways to stitch them together. (We ended up with 50-dimensional Glove word embedding, two bi-directional residual LSTMs, and a CRF on top. It achieves 98% per-label precision and 99% recall, with 26 labels.)\n\nOn the bright side, when you\u2019re done you get a machine that magically understands natural language. If you notice that it makes certain mistakes you don\u2019t have to write new code to fix it\u200a\u2014\u200ayou just give it more examples to learn from. You don\u2019t have to rewire the bot\u2019s brain; you teach it.\n\nMarina is able to understand rather complex requests like these:\n\n- Book me a 3 or 4-star hotel room in London from Aug 15 for 5 nights under $200 with free wi-fi and gym\n\n- Please find me 2 roundtrip economy tickets from Chicago to London on Star Alliance next Sunday back on Oct 5 with 1 stop or less\n\n- What to see on Corsica?\n\n- Give me ideas for a 7 day inexpensive comfortable vacation in unspoiled nature with good food & wine\n\nOr you may start with just \u201chotel in London\u201d or \u201cflight from Chicago to London\u201d and Marina will ask you all the necessary questions.\n\nFor fulfillment of bookings, we didn\u2019t reinvent the wheel. Marina connects to existing flight and hotel booking engines. It could be Expedia, Booking.com, United Airlines, Hilton Hotels\u200a\u2014\u200ayou name it. Marina provides a conversational front-end; it is back-end agnostic. (In fact, our business model is to license Marina to these travel brands.)\n\nFor vacation recommendations, however, we did invent our own wheel. We developed a structured data set of major vacation destinations complete with themes, available activities, major attractions, climate, expensiveness, etc. We call it the \u201ctravel genome project\u201d.\n\nAlterra.ai is a Deep Learning start-up, building bots for online travel and beyond. It is headquartered in Palo Alto, CA, and led by Sergei Burkov, PhD, an ex-Googler and serial entrepreneur. His previous start-up was acquired by Google, where he became the first head of its Moscow R&D Center. Our chief algorithms dude is another ex-Googler, Max Ushakov.", 
        "title": "Alterra.ai launches flight reservation in conversational bot interface, powered by artificial\u2026"
    }, 
    {
        "url": "https://medium.com/@awjuliani/are-neural-networks-truly-creative-e713ac963f05?source=tag_archive---------1----------------", 
        "text": "In the past couple years there has been an explosion of deep learning applications for the creative arts. There are Neural networks that write plays, compose music, stylize photos, create hallucinogenic imagery, or even produce dream videos. There has been a lot of excitement within the field of machine learning about these new applications, but perhaps less criticality than might be expected for a newly developing creative discipline. In engineering disciplines, defining a problem properly is often an essential means to allowing it to truly be solved. I fear when it comes to creativity and neural networks, many may be settling for a too loosely defined problem.\n\nSimply because something a Neural Network produces looks or sounds interesting or aesthetically pleasing doesn\u2019t necessarily mean it is doing the same thing as those artists we consider great. In fact, I would like to argue that most current neural network algorithms by definition are unable to be truly creative in the critical ways in which humans are, and as such are not yet able to produce great art. I understand this is a somewhat controversial position, so I would like to make the distinction between creativity/art in the strong sense, and in the weak sense. I fully accept the artistic potential in many of the current neural networks, but I\u2019d like to pose the strong sense of the term creative as a means of pushing the field forward.\n\nThe current state of the art algorithms within the field of deep learning are able to learn highly complex models of the patterns inherent in a set of data. The generative models which produce much of the neural artwork being shown off are no different. In order to produce a melody, they are trained using thousands of previous melodies, and the structure inherent in these previous works is then reproduced by the neural network when composing a new piece of music. The same is true for images generation: networks learn to produce images that fit a previous set of image statistics. So too for networks which learn to create new texts, such as poems or plays. They are learning and producing structures that already existed in a previous body of work. These are by no means unimpressive accomplishments. It is a critical aspect of human intelligence to be able to learn the complex patterns of information in the world and reproduce them in meaningful ways.\n\nWhat the architecture behind these networks all miss however is that other critical aspect of human intelligence: the capacity to create things that don\u2019t simply capture the past, but are genuinely new and other to what has come before. The word for this is alterity, and I hope it becomes commonplace to the next generation of deep learning architects. Alterity means not simply new in the sense of a novel recombination of what already exists, but new in the sense of being of a fundamentally different nature than anything that has come before. To a computer scientist such a definition may seem to reflect an impossibility. Surely everything new is simply novel recombinations? But the history of art attests to just the opposite. All great movements in art are predicated on their power to break with the past. This of course doesn\u2019t mean a disregarding the past, or to create with an ignorance of what has come before, but rather to create with a full understanding of the past, and an explicit violation of the patterns that had hitherto been established. This isn\u2019t impossible, but is a much more challenging task.\n\nIn this spirit, I would propose a litmus test for the creative potential of any new piece of artwork (created by a neural network or otherwise). If the art could have been created simply as a novel recombination of what already existed up to that point, then it is not truly creative in our strong sense of the word. If however it would be impossible for the patterns of the past to be exclusively harnessed towards the art\u2019s creation, then we can say it passes our test. This may seem like a high bar to set, but I think we find that most great artwork easily passes. Consider the field of popular music, and the proliferation of music genres from the 1950s onward. In 1950 Hip-hop, Disco, R&B, Punk, Metal, Ambient, Progressive, and countless other genres simply did not exist. Not only didn\u2019t they exist, but there is no way to simply combine the patterns inherent within the music of the time, Classical, Rock n Roll, Jazz, and Folk into any of these new kinds of music. As any music critic will attest, The Velvet Underground and Nico was not simply an album filled with new takes on old ideas, but an act of great alterity in the music world.\n\nAt the beginning of this article I framed the issue as one of posing a problem to the deep learning community. Not as insurmountable gotcha problem, but one I think is most certainly solvable. One that will be essential to helping point those working in this field in the right direction toward truly creative algorithms, not simply impressive imitators. This faith comes from my background in neuroscience, and the belief that while we humans are fantastic creatures, there is nothing cosmic or a-material about our creative capacity. It is something rooted in our neurology and biology. I fully believe computers will be able to truly create in my strong sense of the term one day. We simple need to make sure we push them to do so.", 
        "title": "Are Neural Networks Truly Creative? \u2013 Arthur Juliani \u2013"
    }, 
    {
        "url": "https://medium.com/@Adfab/les-grains-de-la-veille-4-e3a5d229926d?source=tag_archive---------3----------------", 
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out.", 
        "title": "Les Grains de la veille #4 \u2013 Adfab \u2013"
    }, 
    {
        "url": "https://medium.com/@jasikpark/metaneural-network-7003469635bb?source=tag_archive---------4----------------", 
        "text": "a. a network that removes nodes and edges and consolidates them into simpler functions.\n\nb. a network that generates a network based on the string that is input\n\nc. a network that can combine two neural networks to form a new, possibly better, neural network\n\nd. a neural network that learns how to optimize similar to back propogation but better?\n\ne. a reward network that determines whether a generated output is good or bad.\n\nTODO: learn about generative adversarial networks: they seem like this ^^^^", 
        "title": "MetaNeural Network \u2013 Caleb Jasik \u2013"
    }
]