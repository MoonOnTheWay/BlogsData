[
    {
        "url": "https://medium.com/@plusepsilon/unreasonable-effectiveness-of-counting-cf18d15f5a43?source=tag_archive---------0----------------", 
        "text": "Counting is simple and effective. Anyone who\u2019s analyzed data can speak to its ubiquity. It\u2019s useful as a first pass at data as well as inputs to more sophisticated models.\n\nDeep learning is currently the most popular framework for artificial intelligence (AI). In contrast to counting, they are considered opaque black boxes that do some mysterious computations; combine this with analogies to the human brain and you have the perfect storm to titillate peoples\u2019 imaginations.\n\nFortunately, calculus is just counting, as well as data science. So it\u2019s possible that deep learning isn\u2019t so mysterious after all...\n\nLet\u2019s experiment with the difficult task of language modeling.\n\nA canonical example is the phrase: \"how are you\". A language model should score English sounding sentences higher.\n\nThe language model is constructed from probabilities of words and phrases. Probabilities are simply a ratio of counts. The following is the probability of a \u201chow\u201d relative to the entire dataset.\n\nThe word probabilities can be combined to get a sentence-level language model score via simple multiplication.\n\nSince word order isn\u2019t taken into account, prob(you are how) would score equally. Phrase probabilities can be used to take into account word order. The following is the probability of the \u201care\u201d where \u201chow\u201d is the previous word.\n\nThis is the probability of \u201care\u201d given (\u201c|\u201d) the context \u201chow\u201d. This should be much higher than prob(how | are) since no one says \u201care how\u201d, ostensibly.\n\nThese probabilities can be extended to an arbitrary phrase length. At the sentence-level, the probabilities factor out to the following:\n\nThese values can be smoothed out to perform better but conceptually, all language models can be represented as a composition of simple counts.\n\nA recurrent neural network is a flavor of deep learning that approximates the language model. But let\u2019s keep it ominous\u200a\u2014\u200aas a black box.\n\nThe details are left for the readers of Karpathy \ud83d\ude08.\n\nHow does it perform? Pretty well according to the following (outdated) benchmarks.\n\nAwesome, but what about real-world testing?\n\nAt FoxType, we have a dev tool to play around with various features of a language model. To compare, we trained a count and RNN language model in parallel on the same dataset. The dev tool scores an arbitrary sentence for each word.\n\nLet\u2019s say we want to detect unlikely words in the context of the sentence. Stacking the models together gives us a good feel of how the words interact.\n\nWe found that many of the sentences we tested felt similar (or not significantly different enough).\n\nThis is a common thread in machine learning where more complicated models are rivaled by simpler models. Even the venerable word2vec algorithm is well-approximated by count tables.\n\nAn RNN can theoretically capture infinite context so a naive expectation would be to use it as a silver bullet for language. But realistically, an RNN would have to predict each word in the sequence, which is hard. All of a sudden, a tweet could have a search space larger than go (much less chess). This speaks more to the infinite variation of language than the failure of deep learning.\n\nThis is not to say deep learning for language modeling isn\u2019t useful. There are legitimate trade-offs from memory-use to interpretability. The point is, they are still closer to a calculator than actual intelligence. Now that deep learning is so accessible, there is no reason not to try it out.\n\nOne of the strengths of language models is that it generalizes. It\u2019s also a weakness. The sentence above is obviously weird and the language model highlights a few possible words to look at.\n\nWe can complement this information by adding bigram counts. A bigram is a sequence of two words.\n\nWe can stack the bigram counts on top of the language model scores.\n\nThis gives us a different perspective on the data. The counts of \u201cyour trouble\u201d is comparatively low.\n\nEven better, one of the trigrams shows that \u201csolve your trouble\u201d is an absurd sequence of words. This makes it very clear that \u201csolve your trouble\u201d is the problem.\n\nCounts are transparent and it\u2019s very useful to bound the problem. There\u2019s no distinction between \u201ci can solve\u201d and \u201csolve your trouble\u201d in the RNN because the latter is normalized to oblivion.\n\nSo we may cheekily say:\n\nOf course, not every example is this trivial. Deep learning can really shine when there are many interacting variables. But this shows that counting can take you a long way.", 
        "title": "Unreasonable Effectiveness of Counting \u2013 Motoki Wu \u2013"
    }, 
    {
        "url": "https://aiinvestor.com/deep-learning-startups-around-the-world-ef744d1bd1b2?source=tag_archive---------1----------------", 
        "text": "Maverick VC Mike Maples, Jr. has recently been tweeting his angst about his (and my) adopted home town of Austin, Texas.\n\nMike is looking for entrepreneurs who are building category killers. In my mind, almost all of the category killers of the decade ahead will have one thing in common. They will be powered by deep learning.\n\nTo dive deeper into M2Jr\u2019s question, I took a look at startups who are staking a claim in deep learning. Using data from Angellist and from other public sources, I identified 435 funded startups that are developing or leveraging deep learning in their product. For each, I attempted to identify a geography where they are headquartered, and then I mapped these locations.\n\nThe results? It doesn\u2019t look good for ATX. Or for mostly everywhere that doesn\u2019t begin with Bay and end with Area.\n\nI found 118 deep learning oriented startups in the bay area. It\u2019s leads the rest of the country\u200a\u2014\u200aand world\u200a\u2014\u200aby a long shot in both quantity and subjective quality of concept. Around the world, the US and bay area dominate as well. The US holds nearly a 10x advantage over their nearest competitor India with the UK and Europe close behind.\n\nLondon could be seen as a growing hotbed of DL talent, with DeepMind originating at the University College London in 2010. Google acquired the firm for nearly $500M in 2014.\n\nCanada yields an unsurprising 18 startups. After all, the DL family tree originates at Geoff Hinton\u2019s computer science laboratory at the University of Toronto and many Hinton\u2019s research assistants and Postdocs have created startups or are leading AI efforts at larger organizations.\n\nI have not spent the cycles digging into the Indian Deep Learning community. I\u2019ll save that for a future blog post. I started this post focused on Austin, and while it as the bottom of startup hubs focused on DL, one ray of light is Cognitive Scale. The firm was founded by leaders of the IBM Watson division and focuses on delivering a \u201ccognitive cloud\u201d platform. I\u2019m personally a bit averse to adopting IBM messaging, but there are some similarities to Watson / Cognitive Scale, and aspects of Deep Learning.\n\nAt this point, deep learning expertise is closely held in Silicon Valley. The AI Builder platforms\u200a\u2014\u200aTorch, Theano, Tensorflow, Deeplearning4j\u200a\u2014\u200aare all projects of Bay Area teams. It\u2019s no surprise that the startup ecosystem for commercializing those platforms is growing close to where the apple falls from the tree.", 
        "title": "Deep Learning Startups Around The World \u2013"
    }, 
    {
        "url": "https://medium.com/edtech-trends/brewing-beer-with-artificial-intelligence-93d10abbd672?source=tag_archive---------2----------------", 
        "text": "One thing that any self-respecting connoisseur knows is that beer is actually alive. Its flavour is constantly evolving, even after you bottle it, and changes again after you pour it. This is one of the reasons why the micro-brewery movement has taken off in such a big way, as it gives fans of the hoppy stuff (and I count myself among those) a lot more choice.\n\nBut is there a thing as too much choice? A small-scale brewer faced with limited resources and endless choices of what to brew, what ingredients to use, and what methods to employ might well think so. With so many variables, how do you negotiate that uncertainty and figure out what it is your customers want?\n\nThat\u2019s a problem that has long interested Dr. Rob McInerney, who wrote his Doctoral thesis at Oxford on making decisions under uncertainty. After finishing his PhD he then went on to start Intelligent Layer, a Machine Learning company. And it was in the East London WeWork where they were based that he first met 10x founder Hew Leith, a former M&C Saatchi director. In classic start-up style, they eventually realized\u200a\u2014\u200aperhaps after a few beers\u200a\u2014\u200athat they could combine their strengths to fulfill a market need.\n\nThus a mash-up of the two companies\u200a\u2014\u200aIntelligentX\u200a\u2014\u200awas created to make the world\u2019s first \u201cself improving\u201d range of beers that constantly evolve in response to customer feedback data. The system uses machine learning (reinforcement learning and bayesian non-parametrics) to improve itself.\n\nCustomers who try the beer can actually \u201ctalk\u201d to ABI (short for Automated Brewing Intelligence) directly by using a Facebook Messenger bot, giving feedback on the flavours. The AI then crunches all that data and helps fine-tune the next recipe of the brew to best suit their tastes.\n\nIf that all sounds a bit like having a terminator making your drinks for you (hey, I think that would be cool, but it\u2019s not for everybody) never fear. ABI also has a bank of wildcard ingredients, like adding fruit to a recipe, which brings in an element of unpredictability and serendipity, all carefully factored and calculated in, of course.\n\nLeith stresses that this isn\u2019t about machines taking over jobs, but about working closely with brewers to supercharge their intuitive, artistic skills using AI.\n\nMcIrnewey adds that in a situation where you don\u2019t have millions of datapoints to train a deep learning algorithm (like Google\u2019s Deepmind did with AlphaGo) it is better to employ a Bayesian non-parametric approach to use the information you have much more efficiently. This means effectively rewarding the system every time it makes a correct decision. \u201cWe also work very hard to learn as much as possible from the brewer directly, so this really becomes a joint effort between man and machine,\u201d he concludes.\n\nThe beers cost \u00a34.50 a bottle from Ubrew and come in four main styles:\n\nIntelligentX has quite ambitious plans for the future. Not only are they in talks with several Michelin starred restaurants about stocking the AI Beers, but they also have their sights on eventually winning a major brewing award such as CAMRA\u2019s Champion Beer of Britain. So far the beers have already evolved eleven times, so it\u2019s probably only a matter of how many generations it will take to hit that sweet spot of maximum optimisation.\n\nI can certainly see this being a big hit with the Tech City \u201cBeer O\u2019clock\u201d crowd, but there is also a much broader consumer trend emerging here. McInerney and Leith believe this approach of using AI to improve physical products will eventually catch on with all sorts of things, from chocolate and coffee to perfume. Why be stuck with eating the chocolates you don\u2019t like? In future we will all be able to get products fine-tuned to our personal tastes, and it won\u2019t even cost the earth, as properly managed this can actually be a much less wasteful way of manufacturing.", 
        "title": "Brewing Beer with Artificial Intelligence \u2013 Tech Trends \u2013"
    }
]