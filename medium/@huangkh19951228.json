[
    {
        "url": "https://towardsdatascience.com/introduction-to-recommender-system-part-2-adoption-of-neural-network-831972c4cbf7?source=user_profile---------1----------------",
        "title": "Introduction to Recommender System. Part 2 (Neural Network Approach)",
        "text": "Spotlight is a well-implemented python framework for constructing a recommender system. It contains two major types of models, factorization model and sequence model. The former one makes use of the idea behind SVD, decomposing the utility matrix (the matrix that records the interaction between users and items) into two latent representation of user and item matrices, and feeding them into the network. The latter one is built with time-series model such as Long Short-term Memory (LSTM) and 1-D Convolutional Neural Networks (CNN). Since the backend of Spotlight is PyTorch, make sure you have installed proper version of PyTorch before using it.\n\nThe utility matrix is called interactions in Spotlight. To create an implicit interaction, we specify the ids of the users and items for all the pairs of user-item interactions. The additional rating information turns this interaction into an explicit one.\n\nA factorization model takes in an implicit or explicit interaction. We will use the implicit one for easy illustration.\n\nIts idea is very similar to SVD where users and items are mapped into a latent space so that they are directly comparable. Essentially, we use two embedding layers to represent users and items, respectively. The target is the interaction (utility matrix) that we passed in. To compute the score for a user-item pair, we take the dot product of the latent representation for that user and item, and passing it through a sigmoid activation function. By computing the loss (more on that later) for all the user-item pairs with regard to the true interaction, we can back-propagate and optimize the embedding layers. The network structure is shown as the figure below.\n\nWe only need a few lines of code to train such model with Spotlight, which looks very similar to scikit-learn toolkit:\n\nA sequential model treats the recommendation problem as a sequential prediction problem. Given the past interaction, we want to know which item the user is most likely to like in the next time step. For instance, assume user A has interaction with items whose ids are in the sequence of [2, 4, 17, 3, 5]. Then we will have the following expanding window prediction.\n\nThe array on the left stores the past interaction, while the integer on the right represents the item that user A will interacts next.\n\nTo train such model, we simply turn the original interaction object into a sequential interaction object. The remaining are the same.\n\nNote that to_sequence() function pads zeros in front of the sequence whose length is not long enough to ensure that every sequence has the same length.\n\nTherefore, items with id 0 ought to be changed to other arbitrary unused id numbers in order for this function to work.\n\nWhen specifying the model, we have the flexibility to change the loss function. Model with different loss functions may have significant difference in the performance. I will briefly describe two main types of loss function defined in Spotlight.\n\nNow, you have learned how to build a recommender system with Spotlight. It is very simple to use with a decent amount of flexibility to fulfill your need. Though for a majority of the problems, the sequence models outperform the factorization one, it takes a lot longer to train sequence models. In addition, it would not be very helpful if the data does not have a clear sequential correlation when applying sequence model.\n\nThe Item2Vec idea came to my mind last month when I was participating a competition, International Data Analysis Olympia (IDAO). The challenge it gave required participants to build a recommender system for Yandex. Since I was learning Word2Vec then, I thought that perhaps similar concept can also be used in recommender systems. I\u2019m not sure if this idea has been pointed out in any paper or article, but I have not seen any similar application of Word2Vec\u2019s concept in this field. The rough idea behind Word2Vec is that we leverage distributive representation to encode each word. Namely, each word is represented by a vector determined by other words surrounding this word. If you would like to know more about Word2Vec, you can refer to my previous blog post: Word2Vec and FastText Word Embedding with Gensim. Similarly, I tried to use distributive representation to encode each item based on the items that user interacted with before and after interacting with it.\n\nFor each of the user, I first created an item list in chronological order. Then, Gensim\u2019s Word2Vec model was trained on these item list. The trained item vectors were stored in the disk so that we can load it for later use.\n\nAfter that, we load the trained item vectors into an embedding matrix.\n\nWe then define our model for predicting the user\u2019s future interaction. Basically, it is a GRU model accelerated by CuDNN. If you don\u2019t have a Nvidia GPU, don\u2019t worry about it. You can simply replace CuDNNGRU with GRU.\n\nNote that at I loaded the pre-trained embedding matrix into the embedding layer of the model and froze it by setting trainable to be false.\n\nI have tested some of the models that I mentioned in both parts of this series on IDAO\u2019s public board test set. The public board score for each of them is shown in the following table.\n\nIt seems that Neural Network-based models do not necessarily beat traditional method for building a recommender system. While being simple to understand and implement, SVD\u2019s score is on par with the Spotlight Sequence model which takes a much longer time to train. Item2Vec, surprisingly, is the best model among all the models that I have tested out. It is true that we cannot judge all these models with only one test set. This gives you a rough idea of how good each model is.\n\nI have discussed two types of models implemented in Spotlight toolkit and Item2Vec created by myself which relies on Word2Vec concept. I also compared the models that I have mentioned based on IDAO\u2019s public board test set. It turns out that SVD is an efficient solution to a recommender system, and that Item2Vec has demonstrated its ability to recommend items more accurately. Should you have any problem or question regarding to this article, please do not hesitate to leave a comment below or drop me an email: khuangaf@connect.ust.hk."
    },
    {
        "url": "https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c?source=user_profile---------2----------------",
        "title": "Word2Vec and FastText Word Embedding with Gensim \u2013",
        "text": "A traditional way of representing words is one-hot vector, which is essentially a vector with only one target element being 1 and the others being 0. The length of the vector is equal to the size of the total unique vocabulary in the corpora. Conventionally, these unique words are encoded in alphabetical order. Namely, you should expect the one-hot vectors for words starting with \u201ca\u201d with target \u201c1\u201d of lower index, while those for words beginning with \u201cz\u201d with target \u201c1\u201d of higher index.\n\nThough this representation of words is simple and easy to implement, there are several issues. First, you cannot infer any relationship between two words given their one-hot representation. For instance, the word \u201cendure\u201d and \u201ctolerate\u201d, although have similar meaning, their targets \u201c1\u201d are far from each other. In addition, sparsity is another issue as there are numerous redundant \u201c0\u201d in the vectors. This means that we are wasting a lot of space.We need a better representation of words to solve these issues.\n\nWord2Vec is an efficient solution to these problems, which leverages the context of the target words. Essentially, we want to use the surrounding words to represent the target words with a Neural Network whose hidden layer encodes the word representation.\n\nThere are two types of Word2Vec, Skip-gram and Continuous Bag of Words (CBOW). I will briefly describe how these two methods work in the following paragraphs.\n\nFor skip-gram, the input is the target word, while the outputs are the words surrounding the target words. For instance, in the sentence \u201cI have a cute dog\u201d, the input would be \u201ca\u201d, whereas the output is \u201cI\u201d, \u201chave\u201d, \u201ccute\u201d, and \u201cdog\u201d, assuming the window size is 5. All the input and output data are of the same dimension and one-hot encoded. The network contains 1 hidden layer whose dimension is equal to the embedding size, which is smaller than the input/ output vector size. At the end of the output layer, a softmax activation function is applied so that each element of the output vector describes how likely a specific word will appear in the context. The graph below visualizes the network structure.\n\nThe word embedding for the target words can obtained by extracting the hidden layers after feeding the one-hot representation of that word into the network.\n\nWith skip-gram, the representation dimension decreases from the vocabulary size (V) to the length of the hidden layer (N). Furthermore, the vectors are more \u201cmeaningful\u201d in terms of describing the relationship between words. The vectors obtained by subtracting two related words sometimes express a meaningful concept such as gender or verb tense, as shown in the following figure (dimensionality reduced).\n\nContinuous Bag of Words (CBOW) is very similar to skip-gram, except that it swaps the input and output. The idea is that given a context, we want to know which word is most likely to appear in it.\n\nThe biggest difference between Skip-gram and CBOW is that the way the word vectors are generated. For CBOW, all the examples with the target word as target are fed into the networks, and taking the average of the extracted hidden layer. For example, assume we only have two sentences, \u201cHe is a nice guy\u201d and \u201cShe is a wise queen\u201d. To compute the word representation for the word \u201ca\u201d, we need to feed in these two examples, \u201cHe is nice guy\u201d, and \u201cShe is wise queen\u201d into the Neural Network and take the average of the value in the hidden layer. Skip-gram only feed in the one and only one target word one-hot vector as input.\n\nIt is claimed that Skip-gram tends to do better in rare words. Nevertheless, the performance of Skip-gram and CBOW are generally similar.\n\nI will show you how to perform word embedding with Gensim, a powerful NLP toolkit, and a TED Talk dataset.\n\nFirst, we download the the dataset using urllib, extracting the subtitle from the file.\n\nLet\u2019s take a look at what input_text variable stores, as partially shown in the following figure."
    },
    {
        "url": "https://hackernoon.com/introduction-to-recommender-system-part-1-collaborative-filtering-singular-value-decomposition-44c9659c5e75?source=user_profile---------3----------------",
        "title": "Introduction to Recommender System. Part 1 (Collaborative Filtering, Singular Value Decomposition)",
        "text": "Traditionally, there are two methods to construct a recommender system : The first one analyzes the nature of each item. For instance, recommending poets to a user by performing Natural Language Processing on the content of each poet. Collaborative Filtering, on the other hand, does not require any information about the items or the users themselves. It recommends items based on users\u2019 past behavior. I will elaborate more on Collaborative Filtering in the following paragraphs. As mentioned above, Collaborative Filtering (CF) is a mean of recommendation based on users\u2019 past behavior. There are two categories of CF: User-based: measure the similarity between target users and other users Item-based: measure the similarity between the items that target users rates/ interacts with and other items The key idea behind CF is that similar users share the same interest and that similar items are liked by a user. Assume there are m users and n items, we use a matrix with size m*n to denote the past behavior of users. Each cell in the matrix represents the associated opinion that a user holds. For instance, M_{i, j} denotes how user i likes item j. Such matrix is called utility matrix. CF is like filling the blank (cell) in the utility matrix that a user has not seen/rated before based on the similarity between users or items. There are two types of opinions, explicit opinion and implicit opinion. The former one directly shows how a user rates that item (think of it as rating an app or a movie), while the latter one only serves as a proxy which provides us heuristics about how an user likes an item (e.g. number of likes, clicks, visits). Explicit opinion is more straight-forward than the implicit one as we do not need to guess what does that number implies. For instance, there can be a song that user likes very much, but he listens to it only once because he was busy while he was listening to it. Without explicit opinion, we cannot be sure whether the user dislikes that item or not. However, most of the feedback that we collect from users are implicit. Thus, handling implicit feedback properly is very important, but that is out of the scope of this blog post. I\u2019ll move on and discuss how CF works. We know that we need to compute the similarity between users in user-based CF. But how do we measure the similarity? There are two options, Pearson Correlation or cosine similarity. Let u_{i, k} denotes the similarity between user i and user k and v_{i, j} denotes the rating that user i gives to item j with v_{i, j} = ? if the user has not rated that item. These two methods can be expressed as the followings: Both measures are commonly used. The difference is that Pearson Correlation is invariant to adding a constant to all elements. Now, we can predict the users\u2019 opinion on the unrated items with the below equation: Let me illustrate it with a concrete example. In the following matrixes, each row represents a user, while the columns correspond to different movies except the last one which records the similarity between that user and the target user. Each cell represents the rating that the user gives to that movie. Assume user E is the target.\n\nFrom the above table you can see that user D is very different from user E as the Pearson Correlation between them is negative. He rated Me Before You higher than his rating average, while user E did the opposite. Now, we can start to fill in the blank for the movies that user E has not rated based on other users.\n\nAlthough computing user-based CF is very simple, it suffers from several problems. One main issue is that users\u2019 preference can change over time. It indicates that precomputing the matrix based on their neighboring users may lead to bad performance. To tackle this problem, we can apply item-based CF. Instead of measuring the similarity between users, the item-based CF recommends items based on their similarity with the items that the target user rated. Likewise, the similarity can be computed with Pearson Correlation or Cosine Similarity. The major difference is that, with item-based collaborative filtering, we fill in the blank vertically, as oppose to the horizontal manner that user-based CF does. The following table shows how to do so for the movie Me Before You.\n\nIt successfully avoids the problem posed by dynamic user preference as item-based CF is more static. However, several problems remain for this method. First, the main issue is scalability. The computation grows with both the customer and the product. The worst case complexity is O(mn) with m users and n items. In addition, sparsity is another concern. Take a look at the above table again. Although there is only one user that rated both Matrix and Titanic rated, the similarity between them is 1. In extreme cases, we can have millions of users and the similarity between two fairly different movies could be very high simply because they have similar rank for the only user who ranked them both. One way to handle the scalability and sparsity issue created by CF is to leverage a latent factor model to capture the similarity between users and items. Essentially, we want to turn the recommendation problem into an optimization problem. We can view it as how good we are in predicting the rating for items given a user. One common metric is Root Mean Square Error (RMSE). The lower the RMSE, the better the performance. Since we do not know the rating for the unseen items, we will temporarily ignore them. Namely, we are only minimizing RMSE on the known entries in the utility matrix. To achieve minimal RMSE, Singular Value Decomposition (SVD) is adopted as shown in the below formula.\n\nX denotes the utility matrix, and U is a left singular matrix, representing the relationship between users and latent factors. S is a diagonal matrix describing the strength of each latent factor, while V transpose is a right singular matrix, indicating the similarity between items and latent factors. Now, you might wonder what do I mean by latent factor here? It is a broad idea which describes a property or concept that a user or an item have. For instance, for music, latent factor can refer to the genre that the music belongs to. SVD decreases the dimension of the utility matrix by extracting its latent factors. Essentially, we map each user and each item into a latent space with dimension r. Therefore, it helps us better understand the relationship between users and items as they become directly comparable. The below figure illustrates this idea. SVD has a great property that it has the minimal reconstruction Sum of Square Error (SSE); therefore, it is also commonly used in dimensionality reduction. The below formula replace X with A, and S with \u03a3. But how does this has to do with RMSE that I mentioned at the beginning of this section? It turns out that RMSE and SSE are monotonically related. This means that the lower the SSE, the lower the RMSE. With the convenient property of SVD that it minimizes SSE, we know that it also minimizes RMSE. Thus, SVD is a great tool for this optimization problem. To predict the unseen item for a user, we simply multiply U, \u03a3, and T. Python Scipy has a nice implementation of SVD for sparse matrix. >>> from scipy.sparse import csc_matrix\n\n>>> from scipy.sparse.linalg import svds\n\n>>> A = csc_matrix([[1, 0, 0], [5, 0, 2], [0, -1, 0], [0, 0, 3]], dtype=float)\n\n>>> u, s, vt = svds(A, k=2) # k is the number of factors\n\n>>> s\n\narray([ 2.75193379, 5.6059665 ]) SVD handles the problem of scalability and sparsity posed by CF successfully. However, SVD is not without flaw. The main drawback of SVD is that there is no to little explanation to the reason that we recommend an item to an user. This can be a huge problem if users are eager to know why a specific item is recommended to them. I will talk more on that in the next blog post. I have discussed two typical methods for building a recommender system, Collaborative Filtering and Singular Value Decomposition. In the next blog post, I will continue to talk about some more advanced algorithms for building a recommender system. Should you have any problem or question regarding to this article, please do not hesitate to leave a comment below or drop me an email: khuangaf@connect.ust.hk."
    },
    {
        "url": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9?source=user_profile---------4----------------",
        "title": "Introduction to Various Reinforcement Learning Algorithms. Part II (TRPO, PPO)",
        "text": "Advantage is a term that is commonly used in numerous advanced RL algorithms, such as A3C, NAF, and the algorithms that I am going to discuss (perhaps I will write another blog post for these two algorithms). To view it in a more intuitive manner, think of it as how good an action is compared to the average action for a specific state.\n\nBut why do we need advantage? Isn\u2019t Q-value good enough?\n\nI will use an example posted in this forum to illustrate the idea of advantage.\n\nHave you ever played a game called \u201cCatch\u201d? In the game, fruits will be dropping down from the top of the screen. You need to move the basket right or left in order to catch them.\n\nThe above image shows a sketch of the game. The circle on the top represents a fruit, whereas the small rectangle below is a basket. There are three actions, a1, a2, and a3. Apparently, the best action is a2, not moving at all, as the fruit will directly fall into the basket. Now, assume that there is no negative reward for any action. In this case, the agent does not have the incentive to choose the optimal action, a2 in the above scenario. Why? Let\u2019s use Q*(s, a) to denote the optimal Q value for state s and action a. Then we will have:\n\nAssume the discount factor \ud835\udefe is only slightly smaller than 1. We can get\n\nSince there is no negative reward, r(a3) and r(a1) are both greater or equal to 0, implying that Q*(s, a3) and Q*(s, a2) are not very different. Thus, the agent will only have little preference of a2 over a3 in this situation.\n\nTo solve this problem, we can compare the Q-value for each action with the average of them so that we know how good an action is relative to each other. Recall from the last blog that the average Q-value of a state is defined as Value (V). Essentially, we coin a new operator called advantage, which is define by subtracting the Q-value for each of the action with the Value of that state.\n\nDeep Deterministic Policy Gradient (DDPG) discussed in the last post was a break through that allows agent to perform actions in a continuous space while maintaining a descent performance. However, the main issue of DDPG is that you need to pick the step size that falls into the right range. If it is too small, the training progress will be extremely slow. If it is too large, conversely, it tends to be overwhelmed by the noise, leading to tragic performance. Recall that the target for calculating the Temporal Difference (TD) error is the following:\n\nIf the step size is selected inappropriately, the target yi derived from the networks or function estimators will not be good, leading to a even worse sample and worse estimate of the value function.\n\nTherefore, what we need is a way to update parameters that guarantees policy improvement. Namely, we want the expected discounted long-term reward \u03b7 to be always increasing.\n\nSimilar to DDPG, TRPO also belongs to the category of policy gradient. It adopts the actor-critic architecture, but modifies how the policy parameters of the actor are updated.\n\nFor a new policy \u03c0\u2019, \u03b7(\u03c0\u2019) can be viewed as the the expected return of policy \u03c0\u2019 in terms of the advantage over \u03c0, which is the old policy. (Since I cannot find \u03c0 with a curve on it on my keyboard, I will use \u03c0\u2019 in the following paragraphs)\n\nYou might wonder why advantage is used. Intuitively, you can think of it as measuring how good the new policy is with regard to the average performance of the old policy. \u03b7 of the new policy can be rewrite into the following form, where \u2374 is the discounted visitation frequencies.\n\nHowever, the above formula is hard to be optimized since \u2374 is highly dependent on the new policy \u03c0\u2019. Therefore, the paper introduced an approximation to \u03b7(\u03c0\u2019), L\u03c0(\u03c0\u2019):\n\nNote that we replace \u2374\u03c0 with \u2374\u03c0\u2019, assuming state visitation frequency is not too different for the new and the old policies. With this equation, we can combine with the well-know policy update method:\n\nHere, \u03c0_{old} is the current policy, while \u03c0\u2019 is the argument max of the policy that maximizes L_{\u03c0old}. We will then obtain the following theorem (Let\u2019s use Theorem 1 to denote it).\n\nC represents the penalty coefficient, whereas D^{max}_{KL} denotes the maximum KL divergence of the two parameter for each of the state. The concept of KL divergence was originated from information theory, describing the information loss. Simply put, you can view it as how different these two parameters, \u03c0 and \u03c0\u2019, are.\n\nThe above formula implies that the expected long-term reward \u03b7 is monotonically improving as long as the right-hand-side term is maximized. Why? Let\u2019s define the right-hand-side of the inequality to be M_{i}.\n\nWe can then prove the following inequality.\n\nThe first line can be obtained by simply plugging the definition of M_{i} into Theorem 1. The second line holds because the KL divergence between \u03c0_{i} and \u03c0_{i} is 0. Combining the first and the second line, we will get the third line. This shows that as long as M_{i} is maximized at every iteration, the objective function \u03b7 is always improving. (I think the last term atthe end of the third line should be Mi instead of M. Not sure if it is a typo if the paper). Therefore, the complex problem that we are trying to solve now boils down to maximizing Mi. Namely,\n\nThe following graph visually illustrates the approximation of \u03b7 with L:\n\nIn practice, if penalty coefficient is included in the objective function, the step size will be very small, leading to long training time. Consequently, a constraint on the KL divergence is used to allow a larger step size while guarantee robust performance.\n\nThe KL divergence constraint is imposed on every state in the state space, the maximum of which should be smaller than a small number \ud835\udf39. Unfortunately, it is not solvable as there are a infinitely large number of states. The paper proposed a solution which provides a heuristic approximation with the expected KL divergence over states, as opposed to finding the maximum KL divergence.\n\nNow, the objective function becomes the following when we expand the first line:\n\nBy replacing \u03a3 over states with expectation and \u03a3 over the actions with importance sampling estimator, which is equal to the old policy if adopting single path method, we can rewrite the above as:\n\nThe objective function is also called a \u201csurrogate\u201d objective function as it contains a probability ratio between current policy and the next policy. TPRO successfully addresses the problem imposed by DDPG that the performance does not improve monotonically. The subset of region lies within the constraint is called trust region. As long as the policy change is reasonably small, the approximation is not much different from the true objective function. By choosing the new policy parameters which maximizes the expectation subject to the KL divergence constraint, a lower bound of the expected long-term reward \u03b7 is guaranteed. This also implies that you don\u2019t need to worry too much about the step size with TRPO.\n\nAlthough TRPO has achieved great and consistent high performance, the computation and implementation of it is extremely complicated. In TRPO, the constraint imposed on the surrogate objective function is the KL divergence between the old and the new policy.\n\nFisher Information Matrix, a second-order derivative of KL divergence, is used to approximate the KL term. This results in computing several second-order matrixes, which requires a great amount of computation. In the TRPO paper, Conjugate Gradient (CG) algorithm was used to solve the constrained optimization problem so that the Fisher Information Matrix does not need to be explicitly computed. Yet, CG makes implementation more complicated.\n\nPPO gets rid of the computation created by constrained optimization as it proposes a clipped surrogate objective function.\n\nLet rt(\ud835\udf3d) denotes the ratio between the new and the old policy. The surrogate objective function used for approximating long-term reward \u03b7 for TRPO becomes the following. Note the subscript describes the conservative policy iteration (CPI) methods that TRPO is based on.\n\nThe idea of TRPO\u2019s constraint is disallowing the policy to change too much. Therefore, instead of adding a constraint, PPO slightly modifies TRPO\u2019s objective function with a penalty for having a too large policy update.\n\nOn the right you can see that the probability ratio rt(\ud835\udf3d) is clipped between [1- \ud835\udf16, 1+\ud835\udf16]. This indicates that if rt(\ud835\udf3d) causes the objective function to increase to a certain extent, its effectiveness will decrease (be clipped). Let\u2019s discuss two different cases:\n\nIf \u0202t is greater than 0, it means that the action is better than the average of all the actions in that state. Therefore, the action should be encouraged by increasing rt(\ud835\udf3d) so that this action has a higher chance to be adopted. Since the denominator of rt(\ud835\udf3d) is constant, the old policy, increasing rt(\ud835\udf3d) also implies increasing the new policy \u03c0\ud835\udf3d(a, s). Namely, increase the chance for taking that action in the given state. However, because of the clip, rt(\ud835\udf3d) will only grows to as much as 1+\ud835\udf16.\n\nBy contrast, if \u0202t is smaller than 0, then that action should be discouraged. As a result, rt(\ud835\udf3d) should be decreased. Similarly, due to the clip, rt(\ud835\udf3d) will only decreases to as little as 1-\ud835\udf16.\n\nEssentially, it restricts the range that the new policy can vary from the old one; thus, removing the incentive for the probability ratio rt(\ud835\udf3d) to move outside the interval.\n\nIn practice, loss function error and entropy bonus should also be considered during implementation as shown below. However, I am not going into details of them as the most innovative and important part is still the clipped objective function.\n\nComparing the objective function L^{CPI} and L^{CLIP}, we can observe that L^{CLIP} is in fact a lower bound of the former one. It also removes the KL divergence constraint. Consequently, the computation for optimizing this PPO objective function is much less than that of TRPO\u2019s. Empirically, it also proves that PPO\u2019s performance is better than TRPO. In fact, thanks to its lightness and ease of implementation, PPO has become the default RL algorithm of OpenAI (https://blog.openai.com/openai-baselines-ppo/)."
    },
    {
        "url": "https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287?source=user_profile---------5----------------",
        "title": "Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG)",
        "text": "Typically, a RL setup is composed of two components, an agent and an environment.\n\nThen environment refers to the object that the agent is acting on (e.g. the game itself in the Atari game), while the agent represents the RL algorithm. The environment starts by sending a state to the agent, which then based on its knowledge to take an action in response to that state. After that, the environment send a pair of next state and reward back to the agent. The agent will update its knowledge with the reward returned by the environment to evaluate its last action. The loop keeps going on until the environment sends a terminal state, which ends to episode.\n\nMost of the RL algorithms follow this pattern. In the following paragraphs, I will briefly talk about some terms used in RL to facilitate our discussion in the next section.\n\nThe model stands for the simulation of the dynamics of the environment. That is, the model learns the transition probability T(s1|(s0, a)) from the pair of current state s0 and action a to the next state s1. If the transition probability is successfully learned, the agent will know how likely to enter a specific state given current state and action. However, model-based algorithms become impractical as the state space and action space grows (S * S * A, for a tabular setup).\n\nOn the other hand, model-free algorithms rely on trial-and-error to update its knowledge. As a result, it does not require space to store all the combination of states and actions. All the algorithms discussed in the next section fall into this category.\n\nAn on-policy agent learns the value based on its current action a derived from the current policy, whereas its off-policy counter part learns it based on the action a* obtained from another policy. In Q-learning, such policy is the greedy policy. (We will talk more on that in Q-learning and SARSA)\n\nQ-Learning is an off-policy, model-free RL algorithm based on the well-known Bellman Equation:\n\nE in the above equation refers to the expectation, while \u019b refers to the discount factor. We can re-write it in the form of Q-value:\n\nThe optimal Q-value, denoted as Q* can be expressed as:\n\nThe goal is to maximize the Q-value. Before diving into the method to optimize Q-value, I would like to discuss two value update methods that are closely related to Q-learning.\n\nPolicy iteration runs an loop between policy evaluation and policy improvement.\n\nPolicy evaluation estimates the value function V with the greedy policy obtained from the last policy improvement. Policy improvement, on the other hand, updates the policy with the action that maximizes V for each of the state. The update equations are based on Bellman Equation. It keeps iterating till convergence.\n\nValue Iteration only contains one component. It updates the value function V based on the Optimal Bellman Equation.\n\nAfter the iteration converges, the optimal policy is straight-forwardly derived by applying an argument-max function for all of the states.\n\nNote that these two methods require the knowledge of the transition probability p, indicating that it is a model-based algorithm. However, as I mentioned earlier, model-based algorithm suffers from scalability problem. So how does Q-learning solves this problem?\n\n\u03b1 refers to the learning rate (i.e. how fast are we approaching the goal). The idea behind Q-learning is highly relied on value iteration. However, the update equation is replaced with the above formula. As a result, we do not need to worry about the transition probability anymore.\n\nNote that the next action a\u2019 is chosen to maximize the next state\u2019s Q-value instead of following the current policy. As a result, Q-learning belongs to the off-policy category.\n\nSARSA very much resembles Q-learning. The key difference between SARSA and Q-learning is that SARSA is an on-policy algorithm. It implies that SARSA learns the Q-value based on the action performed by the current policy instead of the greedy policy.\n\nThe action a_(t+1) is the action performed in the next state s_(t+1) under current policy.\n\nFrom the pseudo code above you may notice two action selection are performed, which always follows the current policy. By contrast, Q-learning has no constraint over the next action, as long as it maximizes the Q-value for the next state. Therefore, SARSA is an on-policy algorithm.\n\nAlthough Q-learning is a very powerful algorithm, its main weakness is lack of generality. If you view Q-learning as updating numbers in a two-dimensional array (Action Space * State Space), it, in fact, resembles dynamic programming. This indicates that for states that the Q-learning agent has not seen before, it has no clue which action to take. In other words, Q-learning agent does not have the ability to estimate value for unseen states. To deal with this problem, DQN get rid of the two-dimensional array by introducing Neural Network.\n\nDQN leverages a Neural Network to estimate the Q-value function. The input for the network is the current, while the output is the corresponding Q-value for each of the action.\n\nIn 2013, DeepMind applied DQN to Atari game, as illustrated in the above figure. The input is the raw image of the current game situation. It went through several layers including convolutional layer as well as fully connected layer. The output is the Q-value for each of the actions that the agent can take.\n\nThe question boils down to: How do we train the network?\n\nThe answer is that we train the network based on the Q-learning update equation. Recall that the target Q-value for Q-learning is:\n\nThe \u03d5 is equivalent to the state s, while the \ud835\udf3d stands for the parameters in the Neural Network, which is not in the domain of our discussion. Thus, the loss function for the network is defined as the Squared Error between target Q-value and the Q-value output from the network.\n\nAnother two techniques are also essential for training DQN:\n\nAlthough DQN achieved huge success in higher dimensional problem, such as the Atari game, the action space is still discrete. However, many tasks of interest, especially physical control tasks, the action space is continuous. If you discretize the action space too finely, you wind up having an action space that is too large. For instance, assume the degree of free random system is 10. For each of the degree, you divide the space into 4 parts. You wind up having 4\u00b9\u2070 =1048576 actions. It is also extremely hard to converge for such a large action space.\n\nDDPG relies on the actor-critic architecture with two eponymous elements, actor and critic. An actor is used to tune the parameter \ud835\udf3d for the policy function, i.e. decide the best action for a specific state.\n\nA critic is used for evaluating the policy function estimated by the actor according to the temporal difference (TD) error.\n\nHere, the lower-case v denotes the policy that the actor has decided. Does it look familiar? Yes! It looks just like the Q-learning update equation! TD learning is a way to learn how to predict a value depending on future values of a given state. Q-learning is a specific type of TD learning for learning Q-value.\n\nDDPG also borrows the ideas of experience replay and separate target network from DQN . Another issue for DDPG is that it seldom performs exploration for actions. A solution for this is adding noise on the parameter space or the action space.\n\nIt is claimed that adding on parameter space is better than on action space, according to this article written by OpenAI. One commonly used noise is Ornstein-Uhlenbeck Random Process.\n\nI have discussed some basic concepts of Q-learning, SARSA, DQN , and DDPG. In the next article, I will continue to discuss other state-of-the-art Reinforcement Learning algorithms, including NAF, A3C\u2026 etc. In the end, I will briefly compare each of the algorithms that I have discussed. Should you have any problem or question regarding to this article, please do not hesitate to leave a comment below or send an email to me: khuangaf@connect.ust.hk."
    },
    {
        "url": "https://medium.com/@huangkh19951228/predicting-cryptocurrency-price-with-tensorflow-and-keras-e1674b0dc58a?source=user_profile---------6----------------",
        "title": "Predicting Cryptocurrency Price With Tensorflow and Keras",
        "text": "Cryptocurrencies, especially Bitcoin, have been one of the top hit in social media and search engines recently. Their high volatility leads to the great potential of high profit if intelligent inventing strategies are taken. It seems that every one in the world suddenly start to talk about Cryptocurrencies. Unfortunately, due to their lack of indexes, Cryptocurrencies are relatively unpredictable compared to traditional financial instruments. This article aims to teach you how to predict the price of these Cryptocurrencies with Deep Learning using Bitcoin as an example so as to provide insight into the future trend of Bitcoin.\n\nTo run the code below, make sure you have installed the following environment and library:\n\nData for prediction can either collected from Kaggle or Poloniex. To make sure coherence, the column names for data collected from Poloniex are changed to match with Kaggle\u2019s.\n\nData collected from source needs to be parsed in order to send to the model for prediction. The PastSampler class was referenced from this blog for splitting the data into a list of datas and labels. The input size (N) is 256, while the output size (K) is 16. Note that data collected from Poloniex was ticked on a 5 minute basis. This indicates that the input spans across 1280 minutes, while the output covers over 80 minutes.\n\nAfter creating the PastSampler class, I applied it on the collected data. Since the original data ranges from 0 to over 10000, data scaling is needed to allow the neural network to understand the data easier.\n\nA 1D Convolutional Neural Network is expected to capture the data locality well with the kernel sliding across the input data. As shown in the following figure.\n\nThe first model I built is Convolutional Neural Network. The following code set the GPU number \u201c1\u201d to be used (since I have 4, you might set it to any GPU you prefer). Since Tensorflow does not seems to do well when running on multiple GPUs, it is wiser to restrict it to run on only 1 GPU. Don\u2019t worry if you do not have a GPU. Simply ignore these lines.\n\nThe code for constructing CNN model is very simple. The dropout layer is for preventing overfitting problem. The loss function is defined as Mean Squared Error (MSE), while the optimizer is the state-of-the-art Adam.\n\nThe only thing you need to worry about is the dimension of input and output between each layer. The equation for computing the output of a certain convolutional layer is:\n\nAt the end of the file, I added two callback function, CSVLogger, and ModelCheckpoint. The former one helps me to track all the training and validation progress, while the latter one allows me to store the model\u2019s weight for each epoch.\n\nLong Short Term Memory (LSTM) network is a variation of Recurrent Neural Network (RNN). It was invented to solve the vanishing gradient problem created by vanilla RNN. It is claimed that LSTMs are capable of remembering inputs with longer time steps.\n\nLSTM is relatively easier than CNN to implement as you don\u2019t even need to care about the relationship among kernel size, strides, input size and output size. Just make sure the dimension of input and output is defined correctly in the network.\n\nGated Recurrent Units (GRU) is another variation of RNN. Its network structure is less sophisticated than LSTM with one reset and forget gate but getting rid of the memory unit. It is claimed that GRU\u2019s performance is on par with LSTM but more efficient. (which is also true in this blog as LSTM takes around 45 secs/ epoch, while GRU takes less than 40 secs/ epoch)\n\nSimply replace the second line of building model in LSTM\n\nSince the result plotting is similar for the three model, I will only show CNN\u2019s version. First, we need to reconstruct the model and load the trained_weights into the model.\n\nThen, we need to invert-scaled the predicted data, which ranges from [0,1] because of the MinMaxScaler used previously.\n\nBoth Dataframes for the ground true (actual price) and the predicted price of Bitcoin are set up. For visualization purpose, the plotted figure only shows the data from August 2017 thereafter.\n\nPlot the figure with pyplot. Since the predicted price is on a 16 minute basis, not linking all of them up would allow us to view the result easier. As a result, here the predicted data is plotted as red dot, as \u201cro\u201d in the third line indicates. The blue line in the below graph represents the ground true (actual data), whereas the red dots represent the predicted Bitcoin price.\n\nAs you can see from the above figure, the prediction closely resemble the actual price of Bitcoin. To select the best model, I decided to test several kinds of configuration of the network, yielding the below table.\n\nEach row of the above table is the model that derives the best validation loss from the total 100 training epochs. From the above result, we can observe that LeakyReLU always seems to yield better loss compared to regular ReLU. However, 4-layered CNN with Leaky ReLU as activation function creates a large validation loss, this can due to wrong deployment of model which might require re-validation. CNN model can be trained very fast (2 seconds/ epoch with GPU), with slightly worse performance than LSTM and GRU. The best model seems to be LSTM with tanh and Leaky ReLU as activation function, though 3-layered CNN seems to be better in capturing local temporal dependency of data.\n\nAlthough the prediction seems pretty good, there is a concern about overfitting. There is a gap between training and validation loss, (5.97E-06 vs 3.92E-05) when training LSTM with LeakyReLU, regularization should be applied in order to minimize the variance.\n\nTo find out the best regularization strategy, I ran several experiments with different L1 and L2 values. First we need to define a new function that facilitate fitting the data into LSTM. Here, I\u2019ll use bias regularizer that regularizes over the bias vector as an example.\n\nAn experiment is done by repeating training the models for 30 times and each time with 30 epochs.\n\nIf you are using Jupyter notebook, you can see the below table directly from the output.\n\nTo visualize the comparison, we can use boxplot:\n\nAccording to the comparison, it seems that L2 regularizer of coefficient 0.01 on the bias vector derives the best outcome.\n\nTo find out the best combination among all the regularizers, including activation, bias, kernel, recurrent matrix, it would be necessary to test all of them one by one, which does not seem practical to my current hardware configuration. As a consequence, I would leave it as a future plan.\n\nFuture work for this blog would be finding out the best hyper-parameter for the best model, and possibly using social media to help predict the trend more accurately. This is my first time to post in Medium. Should there be any mistakes or questions, please do not hesitate to leave any comments below.\n\nFor more information, please refer to my github."
    }
]