[
    {
        "url": "https://hackernoon.com/benchmarking-kafka-performance-part-1-write-throughput-7c7a76ab7db1?source=user_profile---------1----------------",
        "title": "Benchmarking Kafka Performance Part 1: Write Throughput",
        "text": "This is the first post in a series that explores Kafka performance on multiple public cloud providers.\n\nWe have offered a fully managed Kafka service for some time now, and we are quite often asked about just how many messages can you pipe through a given service plan tier on a selected cloud. So here\u2019s a benchmark we conducted to give you a rough idea on just how well Apache Kafka performs in the public cloud.\n\nApache Kafka is a high-performance open-source stream processing platform for collecting and processing large numbers of messages in real-time. It enables you to accept streaming data such as website click streams, events, transactions or other telemetry in real-time and at scale, and serve it downstream to stream processing applications.\n\n \n\nKafka is built distributed for both scalability as well as fault tolerance. Adding more horizontal nodes to tackle growing loads is fairly straightforward and automatic replication of the data over more than one node maintains availability when nodes fail.\n\n \n\nThe basic concepts in Kafka are producers and consumers.\n\n \n\nA producer is an application that generates data but only to provide it to some other application.\n\n \n\nAn example of a producer application could be a web server that produces \u201cpage hits\u201d that tell when a web page was accessed, from which IP address, what the page was and how long it took to render the page by the web server.\n\n \n\nOn the consumer side there could be multiple systems interested in the same page hit data stream:\n\nKafka suits these kinds of applications very well: it provides a method of getting the data out of the hands of the producing application quickly and safely. Once the producer has written the message to Kafka, it can be sure that its part of the job is done. The producer application does not need to know how the data is used and by which applications, it just stores it in Kafka and moves on.\n\n \n\nOn the consumer side a powerful feature of Kafka is that it allows multiple consumers to read the same messages. In our web page hit example above, each of the consumer applications get their own read cursor to the data and they can process the messages at their own pace, all without causing any performance issues or delays for the producer application.\n\n \n\n Here\u2019s what it roughly looks like:\n\nThe Zookeeper cluster is a critical piece in keeping Kafka healthy and up and running. It maintains Kafka\u2019s metadata and most importantly, a consensus between the Kafka nodes of who is doing what.\n\nAiven Kafka is a a fully managed service based on the Apache Kafka technology. Our aim is to make it as easy as possible to use Kafka clusters with the least amount of operational effort possible. We handle the Kafka and Zookeeper setup and operations for you, so you can focus on value-adding application logic instead of infrastructure maintenance. Aiven Kafka services can be launched in minutes, and we\u2019ll ensure they remain operational, well performing, up-to-date and secure at all times. Nodes are automatically distributed evenly across the available availability zones in order to minimize the impact of losing any of the zones.\n\n \n\nAiven Kafka is available in Amazon Web Services, Microsoft Azure, Google Cloud Platform, UpCloud and DigitalOcean with a total coverage of 53 cloud regions. In this performance comparison we ran the benchmark on all of these except DigitalOcean, where our Kafka offering is limited by the available plans.\n\n \n\nEach Kafka service used in these tests is a regular Aiven-provided service with no alterations to its default settings.\n\nIn this first Kafka benchmark post, we set out to estimate maximum write throughput rates for various Aiven Kafka plan tiers in different clouds. We wanted to use a typical customer message sizes and standard tools for producing load. We also wanted to generate the load from separate systems over the network to make sure the load could mimic the actual customer workloads as closely as possible.\n\n \n\nHigh-level view of the test setup, a single Aiven Kafka service with five nodes, distributed evenly over the availability zones:\n\nWe picked message size of 512 bytes for our tests. Based on our experience, one of the most typical payloads is a JSON encoded message ranging somewhere between 100 bytes to 10 kilobytes in size.\n\n \n\nIn these tests, we use a single topic with the partition count matching the node count of each Aiven plan tier. For more complex topic/partition setups Aiven actively balances the placement of the partitions, trying to achieve a \u201cperfect\u201d distribution of partitions. In the case of this test there is just a single partition for each node, so this is rather simple. We set the replication factor to one (1) in the case of this test, meaning each of the messages only resides on a single Kafka node.\n\n \n\nApache Kafka version used was 0.10.1.1.\n\n \n\nFor load generation, we chose to use librdkafka and rdkafka_performance from the provided examples. We are using default settings for the most part, but bumped up single request timeout to 60 seconds as we expect the Kafka brokers to be under extreme load and request processing to take longer than under a normal healthy load level. Also, since Aiven Kafka services are offered only over encrypted TLS connections, we included the configuration for these, namely the required certificates and keys.\n\n \n\nlibrdkafka defaults to a maximum batch size of 10000 messages or to a maximun request size of one million bytes per request, whichever is met first. In these tests, we did not employ compression.\n\n \n\nproducer.props configuration:\n\nWe ran several instances of rdkafka_performance on multiple VMs on a different cloud provider from the one being tested. So all of the test load was coming from the internet thru the nodes\u2019 public network interfaces.\n\n \n\nWe kept increasing the number of instances until we could find the saturation point and the maximum message rates for each plan.\n\n \n\nEach rdkafka_performance instance was started on the command line with:\n\nFirst set of tests was run on an Aiven Kafka Business-4 plan, which is a three node cluster and a common starting point for many of our customers. Each node in this plan has 4 gigabytes of RAM, a single CPU core and 200 gigabytes of disk on each node, providing a total 600 gigabytes of raw Kafka storage capacity in the cluster.\n\n \n\n Write performance (3 nodes @ 4 GB RAM, 1 CPU, 200 GB disk each):\n\nOn UpCloud, we hit 200,000 messages per second. Azure and Google plans saturated at 120,000 and 130,000 messages per second and the Amazon deployment reached 50,000 messages per second.\n\nThe performance is pretty respectable. The performance on Amazon is a bit behind the others because of the node types available and we will be looking at ways to optimize that in the future. As you will see in the next graph for the test with the bigger plan, the AWS performance is already more in line with the other providers.\n\n \n\nNext, we tested three node clusters but with larger underlying instances using the Business-8 plan. This plan has nodes with 8 gigabytes of RAM, two CPU cores and 400 gigabytes of disk per node, i.e. all the primary resources are doubled when compared to the Business-4 plan. This test indicates how well Kafka scales vertically with increased resources.\n\n \n\nWrite performance (3 nodes @ 8 GB RAM, 2 CPU, 400 GB disk each):\n\nWe see a nice increase in performance, with 320,000 messages per second on UpCloud, 205,000 on Azure, 170,000 on Google and 160,000 messages per second on AWS.\n\n \n\nIn the last test, we wanted to verify how well Kafka scales horizontally. With this test, we went from the Business plan tier to the Premium tier, which bumps the node count from three to five, while keeping the node specs otherwise identical. Also the test setup was updated to utilize a partition count of five (vs. three) for this test.\n\n \n\nWrite performance (5 nodes @ 8 GB RAM, 2 CPU, 400 GB disk each):\n\nThe results here are solid for Kafka: a two-thirds increase in the number of nodes resulted in a straight 2/3 increase in write performance. Awesome!\n\n \n\nAiven Kafka Premium-8 on UpCloud handled 535,000 messages per second, Azure 400,000, Google 330,000 and Amazon 280,000 messages / second.\n\nApache Kafka performs just as well as we expected and scales nicely with added resources and increased cluster size. We welcome you to benchmark your own workloads with Aiven and to share your results.\n\n \n\nWe utilize Kafka as a message broker within Aiven as well as use it as a medium for piping all of our telemetry metrics and logs. We are happy with with our technical choice, and can recommend Apache Kafka for handling all kinds of streaming data.\n\n \n\nFind out more about Aiven Kafka at https://aiven.io/kafka."
    },
    {
        "url": "https://medium.com/@aiven_io/aiven-postgresql-read-only-replicas-7e62e03f0c07?source=user_profile---------2----------------",
        "title": "Aiven PostgreSQL read-only replicas \u2013 Aiven \u2013",
        "text": "Aiven PostgreSQL services allows read-only replica access to all of our PostgreSQL plans that have one or more standby nodes. Utilizing the standby server nodes for read-only queries allows you to scale your database reads by moving some of the load away from the master server node to the otherwise mostly idle replica nodes.\n\nPostgreSQL master node is the primary server node that processes SQL queries, makes the necessary changes to the database files on the disk and returns back the results to the client application.\n\nPostgreSQL standby nodes replicate (which is why they are also called \u201creplicas\u201d) the changes from the master node and try to maintain an up-to-date copy of the same database files that exists on the master.\n\nStandby nodes are useful for multiple reasons:\n\nAiven offers PostgreSQL plans with different number of standby server nodes in each:\n\nThe difference between the plans is primary the behavior during failure scenarios. The are many bad things that can happen to cloud servers (or any server in general): hardware failures, disk system crashes, network failures, power failures, software errors, running our of memory, operator mistakes, fires, floods and so on.\n\nSingle node plans are most prone to data loss during failures. For example, if the server power suddenly goes out, some of the latest database changes may not have made it out from the server into backups. The size of the data loss window is dependent on the backup method used.\n\nSingle node plans are also the slowest to recover back to operation from failures. When the server virtual machine fails, it takes time to launch a new virtual machine and to restore it from the backups. The restore time can be anything from a couple of minutes to several hours, the primary factor in it being the size of the database that needs to be restored.\n\nAdding a \u201chot\u201d standby node helps with both of the above issues: the data loss window can be much smaller as the master is streaming out the data changes in real-time to the standby as they happen. The \u201clag\u201d between the master and standby is typically very low, from tens of bytes to hundreds of bytes of data.\n\nAlso recovery from failure is much faster as the standby node is already up and running and just waiting to get the signal to get promoted as the master, so that it can replace the old failed master.\n\nWhat about having two standby nodes? What is the point in that?\n\nThe added value of having a second standby node is that even during recovery from (single-node) failures, there are always two copies of the data on two different nodes. If another failure strikes after a failover when there is just a single master node running, we again risk losing some of the latest changes written to the database. It takes time to rebuild a new standby node and getting it in sync node after a failover when there is a lot of data in the database, and it often makes sense to protect the data over this time period by having another replica. This is especially important when the database size is large and recreating a replacement node for the faulted one can take hours.\n\nStandby nodes are also useful for distributing the load away from the master server. In Aiven the replica nodes can be accessed by using the separate \u201cRead-only replica URL\u201d visible in the Aiven web console:\n\nUsing the replica URL in a database client application will connect to one of the available replica server nodes. Previously replica node access was only available in our Premium plans (master + two standbys) and now we have enabled it in our Business plans (master + one standby) as well.\n\nSo if you have had high CPU usage on the master node of your Startup plan, it may be worthwhile looking into the possibility of increasing your read throughput by using the replica servers for reads. Of course in addition by using a Business plan you\u2019ll also make the service have better high availability characteristics by having a standby to fail over to.\n\nA good thing to note is that since the PostgreSQL replication used in Aiven PostgreSQL is asynchronous there is a small replication lag involved. What this means in practice is that if you do an INSERT on the master it takes a while (usually much less than a second) for the change to be propagated to the standby and to visible there.\n\nTo start using your read-replica find its database URL and after that you can connect to it by copying the Read-only replica URL:\n\nAfter which you can run any read-only query without slowing down the master.\n\nAlso while connected, PostgreSQL can tell you whether you\u2019re connected to either a master or standby node. To check that out you can run:\n\nIf it returns TRUE you\u2019re connected to the replica, if it returns FALSE you\u2019re connected to the master server.\n\nRemember that trying Aiven is free: you will receive US$10 worth of free credits at sign-up which you can use to try any of our service plans. The offer works for all of our services: PostgreSQL, Redis, InfluxDB, Grafana, Elasticsearch and Kafka!\n\nGo to https://aiven.io/ to get started!"
    },
    {
        "url": "https://medium.com/@aiven_io/aiven-postgresql-connection-pooling-6d41bafac671?source=user_profile---------3----------------",
        "title": "Aiven PostgreSQL connection pooling \u2013 Aiven \u2013",
        "text": "Aiven PostgreSQL connection pooling allows you to maintain very large numbers of connections to a database while keeping the server resource usage low.\n\n \n\nAiven PostgreSQL connection pooling utilizes PGBouncer for managing the database connection and each pool can handle up to 5000 database client connections. Unlike when connecting directly to the PostgreSQL server, each client connection does not require a separate backend process on the server. PGBouncer automatically interleaves the client queries and only uses a limited number of actual backend connections, leading to lower resource usage on the server and better total performance.\n\nEventually a high number of backend connections becomes a problem with PostgreSQL as the resource cost per connection is quite high due to the way PostgreSQL manages client connections. PostgreSQL creates a separate backend process for each connection and the unnecessary memory usage caused by the processes will start hurting the total throughput of the system at some point. Also, if each connection is very active, the performance can be affected by the high number of parallel executing tasks.\n\n \n\n It makes sense to have enough connections so that each CPU core on the server has something to do (each connection can only utilize a single CPU core [1]), but a hundred connections per CPU core may be too much. All this is workload specific, but often a good number of connections to have is in the ballpark of 3\u20135 times the CPU core count.\n\nWithout a connection pooler the database connections are handled directly by PostgreSQL backend processes, one process per connection:\n\nAdding a PGBouncer pooler that utilizes fewer backend connections frees up server resources for more important uses, such as disk caching:\n\nMany frameworks and libraries (ORMs, Django, Rails, etc.) support client-side pooling, which solves much the same problem. However, when there are many distributed applications or devices accessing the same database, a client-side solution is not enough.\n\nAiven PostgreSQL supports three different operational pool modes: \u201csession\u201d, \u201ctransaction\u201d and \u201cstatement\u201d.\n\nFirst you need an Aiven PostgreSQL service, for the purposes of this tutorial we assume you already have created one. A quick Getting Started guide is available that walks you through the service creation part.\n\n \n\n This the overview page for our PostgreSQL service in the Aiven web console. You can connect directly to the PostgreSQL server using the settings described next to \u201cConnection parameters\u201d and \u201cService URL\u201d, but note that these connections will not utilize PGBouncer pooling.\n\nClicking the \u201cPools\u201d tab opens a list of PGBouncer connection pools defined for the service. Since this service was launched, there are no pools defined yet:\n\nTo add a new pool click on the \u201cAdd pool\u201d button:\n\nFor the purposes of this tutorial we\u2019ll name the pool as \u201cmypool\u201d and set the pool size as 1 and the pool mode as \u201cstatement\u201d. Confirming the settings by clicking \u201cAdd pool\u201d will immediately create the pool and the pool list is updated:\n\nClicking the \u201cInfo\u201d button next to the pool information shows you the database connection settings for this pool. Note that PGBouncer pools are available under a different port number from the regular unpooled PostgreSQL server port. Both pooled and unpooled connections can be used at the same time.\n\nWe can use the psql command-line client to verify that the pooling works as supposed:\n\n \n\n From terminal #1:\n\nFrom terminal #2 we open another connection:\n\nNow we have two open client connections to the PGBouncer pooler. Let\u2019s verify that each connection is able access the database:\n\n \n\nTerminal #1:\n\nBoth connections respond as they should. Now let\u2019s check how many connections there are to the PostgreSQL backend database:\n\n \n\n Terminal #1:\n\nAnd as we can see from the pg_stat_activity output the two psql sessions use the same PostgreSQL server database connection.\n\nThe more client connections you have to your database, the more useful connection pooling becomes. Aiven PostgreSQL makes using connection pooling an easy task and migrating from non-pooled connections to pooled connections is just a matter of gradually changing your client-side connection database name and port number!\n\nRemember that trying Aiven is free: you will receive US$10 worth of free credits at sign-up which you can use to try any of our service plans. The offer works for all of our services: PostgreSQL, Redis, InfluxDB, Grafana, Elasticsearch and Kafka!\n\n \n\n Go to https://aiven.io/ to get started! \n\n \n\n Cheers,\n\n Team Aiven"
    },
    {
        "url": "https://medium.com/@aiven_io/monitoring-metrics-collection-and-visualization-using-influxdb-and-grafana-901cffd4495a?source=user_profile---------4----------------",
        "title": "Monitoring, metrics collection and visualization using InfluxDB and Grafana",
        "text": "In addition to providing you the Aiven service, our crew also does a fair amount of software consulting in the cloud context. A very common topic we are asked to help on is metrics collection and monitoring. Here\u2019s a walk through on how to utilize InfluxDB and Grafana services for one kind of solution to the problem. We offer both as a managed Aiven service for quick and easy adoption.\n\nAs an example, here\u2019s a dashboard screenshot from a pilot project we built recently for our customer:\n\nThis particular instance is used to monitor the health of quality assurance system on an industrial manufacturing line. The system being monitored uses IP based video cameras coupled with IO triggers to record a JPEG image of the artifacts passing through various stages of processing steps. The resulting dashboard allows verification that the system is working properly with a single glance.\n\nOn the top of the dashboard you\u2019ll see a simple reading of temperature sensor of the device. Any large deviation from the norm would be a good warning of oncoming hardware fault:\n\nThe next plotted metric is the size of the JPEG compressed image from the imaging device:\n\nInterestingly, this relatively simple metric reveals a lot about the health of both the sensor and any lenses and lighting sources involved. Due to the nature of JPEG encoding, the frame and the size varies slightly even in rather static scenes, so it makes a good quick overall indicator that the component is running fine and returning up-to-date content.\n\nThe two graphs at the bottom track the last update time from each of the camera feeds and each of the IO trigger services respectively:\n\nHere, we expect each of the cameras to update several times a second. The IO triggers are interrogated in a long-poll mode with a timeout of 15 seconds. These limits yield natural maximum allowable limits for monitoring and alerting purposes. In fact, the left hand side shows two readings that correlate with temporary network glitches.\n\nThe visualization and dashboard tool shown above is Grafana. The underlying storage for the telemetry data is InfluxDB. In this case, we utilize Telegraf as a local StatsD compatible collection point for capturing and transmitting the data securely into InfluxDB instance. And finally, we use a number of taps and sensors across the network that feed the samples to Telegraf using StatsD client libraries in Node.js, Python and Java based on the component.\n\nIn this project we are using Aiven InfluxDB and Aiven Grafana hosted services, but any other InfluxDB / Grafana should work more or less the same way.\n\nWe start by launching an InfluxDB service in Aiven:\n\nThe service is automatically launched in a minute or so.\n\nInfluxDB is a time-series database with some awesome features:\n\nNext we will need the connection parameters to our InfluxDB instance. The necessary information required (hostname, username, password, etc.) for connecting our Telegraf collecting agent to InfluxDB can be found from the from the service overview page:\n\nWe typically run a single Telegraf container per environment. In order to make Telegraf talk to our InfluxDB and to accept StatsD input, we will need to modify its configuration file telegraf.conf a little bit and add the following sections:\n\nWe want our InfluxDB connection to be secure against man-in-the-middle attacks, so we have included the service\u2019s CA certificate in the configuration file. This will force the InfluxDB server to prove its identity to our Telegraf client. The certificate can be downloaded from the Aiven web console:\n\nHere\u2019s an example StatsD code blob for Node.js component:\n\nThe StatsD UDP protocol uses super simple textual message format and sending a metric takes few CPU cycles, so even a high request-throughput server can transmit metrics per request processed, without hurting the overall performance. The StatsD receiver in Telegraf parses these incoming metrics messages and consolidates the metrics, typically storing data at a much slower pace in to the metrics databases. This really helps keeping both the source\u2019s software\u2019s and the metrics database\u2019s load levels under control.\n\nIn the above code sample, we use Telegraf\u2019s StatsD extension for tagging support with the source=20 parameter. This handy little feature is what allows us to easily slice and display the collected metrics by each sensor or just plot all metrics, regardless of the source sensor. This is one of the killer features of InfluxDB and Telegraf!\n\nOK, so now we are transmitting metrics from our application thru the Telegraf daemon to our InfluxDB database. Next up is building a Grafana dashboard that visualizes the collected data.\n\nWe launch our Grafana from the Aiven console by creating a new service:\n\nNormally an InfluxDB needs to be manually added as a data source in Grafana, however in this case we can skip that step as InfluxDB and Grafana services launched under the same project in Aiven are automatically configured to talk to each other.\n\nWe like Grafana a lot because it makes it simple to define visually appealing, yet useful graphs and it integrates with InfluxDB well. Grafana has a user-friendly query builder specifically for building queries for InfluxDB, and with a little practice it takes little time to conjure fabulous charts from almost any source data.\n\nThe Grafana web URL, username and password are available on the service overview page:\n\nOpening Grafana in the browser, logging in with the credentials from above and defining a simple graph with an InfluxDB query editor\u2026 PROFIT!\n\nThat\u2019s it for now. Getting application metrics delivered from the application to a pretty dashboard doesn\u2019t take much effort nowadays!\n\nWe use Telegraf, InfluxDB and Grafana extensively in our own Aiven monitoring infrastructure. Our stack also includes a few other components such as Apache Kafka, which we\u2019ll cover in another article. Stay tuned! :-)\n\nOur Aiven Cloud Database service includes hosted InfluxDB and Grafana plans in all major clouds around the world. You can sign up for a free trial at aiven.io."
    }
]