[
    {
        "url": "https://chronicles.mfglabs.com/f8-hacked-google-uber-transparency-and-more-in-the-programmable-edition-92-5e25c2abcbc1?source=---------0",
        "title": "F8, hacked Google, Uber transparency and more in the Programmable Edition #92",
        "text": "Last week was the Facebook annual developer meeting, the now infamous F8. Two big topics there: chatbots, and AR (augmented reality)/MR (mixed reality).\n\nOn the first topic, no groundbreaking evolutions despite the launch of a Messenger Platform V2, but a good signal from Facebook that they still believe in the ability of conversational interfaces to transform the way brands address their consumers.\n\nOn the second one, Facebook showed some work on VR (chatting with weird Mii-like avatars of your friend), but the real and new deal was AR. After copycatting Snapchat on AR features, Facebook now prepares for delivering developers a tooling platform to build AR on top of apps with ease. Their work on SLAM (Simultaneous Localization and Mapping) is pretty spectacular.\n\nWith this acceleration on AR, Facebook gets ready for the time AR enabled products (phones) and soon MR \u2014 mixed reality \u2014 enabled ones (glasses, lenses etc.) will hit the market. It will have the ecosystem (tools and content) in place to lead the battle on these new interfaces with other companies also moving fast in this space, namely Apple and Google."
    },
    {
        "url": "https://chronicles.mfglabs.com/telcos-data-war-chests-challenged-artists-blockchain-in-advertising-and-more-in-the-programmable-4d8e3cd25df4?source=---------1",
        "title": "Telcos data war chests, challenged artists, Blockchain in advertising and more in The Programmable\u2026",
        "text": "At MFG Labs, we follow closely what goes on in the Telco business. After all, they are nothing less than the enabler of all the tech industry! Their business is a typical fixed cost business: they are investing billions to build a network, and then try to monetise it as much as they can. That means going further than selling mobile phones: they started selling TV bundles in some countries (including France who has been a pioneer for a long time), and now some of the biggest telcos are looking at a brand new market: advertising.\n\nAnd they have incredibly valuable assets to succeed in this world, including the most valuable of all: they have ALL the data; Telcos have access to basically everything that happens on both your mobile, your wifi, and even in some cases your TV. And did I mention all the personal information linked to your subscription ?\n\nThey are also offering the dream of online marketing: a 100% reliable cross-device identification of users. You can use your mobile, your laptop, your TV, your operator knows it\u2019s still you.\n\nOnline advertising is very natural for Telcos, and some of the biggest operator are aggressively building their offering: Verizon has invested more than $9 billion to create the 3rd biggest walled garden after Facebook and Google (with the acquisition of AOL and Yahoo). Altice has acquired Teads for a hefty $307 million just a few weeks ago.\n\nThe next frontier in advertising is programmatic TV ads, and Telcos are also uniquely positioned to capture a good share of this market. In some countries, they already have all the infrastructure required to serve TV ads programmatically, and the TV consumption data they have been collecting for years cannot be matched by Facebook or Google.\n\nIf you\u2019re wondering what the next big thing in advertising will be, you should definitely consider Telcos\u2026"
    },
    {
        "url": "https://chronicles.mfglabs.com/stories-and-other-stories-in-the-programmable-edition-90-e8b4ac20ccdc?source=---------2",
        "title": "Stories and other stories in the Programmable Edition #90",
        "text": "When you and I think about hot potatoes between tech giants, we think patents, markets, talents or audience. The huge war over UX concepts is less obvious at first.\n\nThough, recent UX changes on popular digital platforms show that the battle on UX is one of the most violent ones. The most notorious example is the massive convergence around the \u201cStories\u201d format. Launched by Snapchat in 2013, shamelessly copied by Instagram and Messenger in 2016, by Facebook one week ago\u2026 Even Medium adopted the format with its freshly launched new product Series. You get to see them everywhere, to the point some say it might be the new newsfeed: adapted to vertical formats, massively leveraging video (the \u201cnew keyboard\u201d), in short, fully fitted to the evolution of usage. Some other ones, less seriously, predict that even Excel could soon adopt the feature.\n\nAnyway, the point is: UX strongly matters. Contrary to patents, markets, talents, etc. the first mover advantage is smaller in UX: only counts the format, how you take it to a network and put your own spin on it."
    },
    {
        "url": "https://chronicles.mfglabs.com/brain-computer-interfaces-japanese-buses-profitable-pizzas-and-more-in-the-programmable-edition-c5f8dc331529?source=---------3",
        "title": "Brain-computer interfaces, Japanese buses, profitable pizzas and more in the Programmable Edition\u2026",
        "text": "What if we can control machines with a simple thought? Imagine that you can be connected to digital intelligence that reads neural codes. A simple will would make this artificial intelligence design an algorithm to execute your orders.\n\nNeuroscience research combined with A.I technologies have a broad and promising future. A brain-computer interface (BCI) like Neuralink might offer people with neurodegenerative diseases a better life. If the brain is connected to this A.I, a programmed stimulation of specific brain\u2019s parts can help cure not only long-term degenerative disorders like Parkinson and epilepsy but also obesity and anorexia.\n\nEven if BCI technology aims at providing a new pathway of communication for patients suffering from paralysis today, how far it can be integrated in our daily life?"
    },
    {
        "url": "https://chronicles.mfglabs.com/ai-human-balance-smart-coaches-autonomous-vehicles-traps-e48a534fc573?source=---------4",
        "title": "AI / human balance, smart coaches, autonomous vehicles traps",
        "text": "Artificial intelligence is gradually taking shape in all parts of our lives, changing how people interact with technology. Owning transportation equipment like cars will be a thing of the past as ride-sharing in combination with driverless cars will probably change public transit.\n\nIn each domain, AI has started to deliver important benefits but it also raises important social issues, AI technologies have already begun to displace jobs in several sectors and everybody is concerned.\n\nFor example, in legal work, advances in natural language processing has proved useful in scanning and predicting what documents will be relevant to a case making some lawyers worried about their profession, contradicting the assumption that only manufacturing jobs could be computerized.\n\nIn reality AI technology is assisting human work rather than replacing it. Routine tasks with low added value for humans will be automated, but would you fully rely on a computer system for your justice case? Of course not, because efforts that involve strategy, creativity, judgment and empathy cannot yet be automated."
    },
    {
        "url": "https://chronicles.mfglabs.com/ai-disillusion-programmatic-for-the-privileged-and-google-smart-homes-5c46af5a6893?source=---------5",
        "title": "AI disillusion, programmatic for the privileged and Google smart homes",
        "text": "Here at MFG Labs we actively follow the latest news and opinion about Artificial Intelligence. Most articles are about the AI revolution, its miracles, and all the potential it opens to the average person and companies.\n\nIndeed collective thinking is telling us AI is trendy and it will solve any given problem. It makes us believe AI\u2019s miracles will next generate art, compose the news, take care of companies\u2019 strategy or even develop relationships with us.\n\nAt the same time, the Film and TV industries have created science fiction worlds and icons: Star Wars, Her, Terminator\u2026 indirectly generating a lot of expectations.\n\nHowever the ways is still long before reaching SF robots and you won\u2019t meet R2D2 tomorrow. Some voices are rising saying AI is sometimes overkill, or its promises could finally be deceptive. Today AI manages complex tasks such as driving a car, detecting toxic comments on the web and more, but it actually remains just software. The next generation of algorithms will tell us if (human) intelligence can grow out of these advanced softwares we have today."
    },
    {
        "url": "https://chronicles.mfglabs.com/football-science-applied-ai-snaps-mordor-ish-nightmares-and-more-in-the-programmable-edition-86-d604fe7d7734?source=---------6",
        "title": "Football science, applied AI, Snap\u2019s Mordor-ish nightmares and more in the Programmable Edition #86",
        "text": "Every week, we try to select the best articles talking about applied machine learning to build autonomous cars, build personalized news feeds, make search more relevant, or recommend movies you may like. We talk a lot about computer vision, natural language processing, and speech recognition, because breakthroughs in those areas will have huge consequences.\n\nThis week, I want to focus on another shift happening in sports, or one I want to see happening, especially in (European) football. Indeed, we currently only use 5% of the data collected to describe and explain the result of the game. More specifically, as Billy Beane puts it, talking about traditional stats, we \u201conly credit outcome, we don\u2019t credit process\u201d. We count the number of shoot, of passes, of goals. But we filter out everything else, like defensive press, numerical dominance, player attraction or movement intelligence to name of few.\n\nThis is where data science must step in. The goal is to analyse a greater number of events and come up with new metrics to describe the performance of a football player. Eventually, we should be able to \u201cproperly allocate credit or blame to a player\u201d and gain a deeper and wider understanding of the game.\n\nMeanwhile, we can also leverage existing hardware to gather new data. This is what I tried to do by exporting and processing information from a smartwatch that I wore during a game. I also represented my movements on the pitch every second. You can find the article about this project on this link.\n\nMaybe one day, even at the amateur level, we will be able to assert scientifically \u2014 that is backed with data \u2014 who shone and who struggled."
    },
    {
        "url": "https://chronicles.mfglabs.com/global-communities-apple-doppelg%C3%A4ngers-internet-allegories-and-more-in-the-programmable-edition-5c31ba3924fe?source=---------7",
        "title": "Global communities, Apple doppelg\u00e4ngers, Internet allegories and more in the Programmable Edition\u2026",
        "text": "In a 6,000 word letter, Mark Zuckerberg reveals his moving plan to \u2018bring us together in a global community\u2019. While most people now use Facebook to connect with friends and family, Zuckerberg hopes that the social network can encourage more civic engagement, an informed public and community support in the years to come.\n\nWith this manifesto, has Facebook become a globalist movement rather than the ad-driven, highly profitable company we all know?\n\nNot yet, giving the strategy the company has recently adopted. Facebook is indeed conducting a very efficient and aggressive strategy of replicating competitors\u2019 successful features. Snapchat and LinkedIn, for instance, have suffered this strategies for a few months. All things considered, Facebook is not a NGO yet!"
    },
    {
        "url": "https://chronicles.mfglabs.com/back-to-our-first-love-data-science-special-edition-abb31e261a37?source=---------8",
        "title": "Back to our first love: Data Science special edition",
        "text": "The hype around data and AI grew and grew in 2016 pushing the subject in the middle of public debates. Being under the spotlight with all the polls held in 2016 and 2017 is far from a walk in the park.\n\nOne example was the problem of fake news shared on social networks during the presidential election in the USA. This short video clip from Wired and the associated article cover the subject. It will be interesting to see the benefits for AI research of working on such a difficult task. Fake news is not a well defined concept and is hard to distinguish from satirical articles for instance. This makes detecting fake news a way harder problem than beating a human at go or poker.\n\nAccepting science and data can now be seen as a political statement. \n\nWhether by wiping out https://open.whitehouse.gov/browse or by welcoming foreign scientists. The controversial articles from Scott Adams on the subject indicates that more than facts, the ability to convince people is key.\n\nIf fake news can be spread on social networks, so should be popularizing work. \n\nThe growing list of blogs or streaming channels dedicated to history, physics, linguistics and so on is a great indicator of progress in the sharing of the knowledge accumulated by scientists over the world.\n\nThis edito has largely been influenced by the recent death of Hans Rosling."
    },
    {
        "url": "https://chronicles.mfglabs.com/made-in-france-ai-toilet-pictograms-mediaconomics-and-more-in-the-programmable-edition-83-bcdd55b05d94?source=---------9",
        "title": "Made in France AI, Toilet Pictograms, mediaconomics and more in the Programmable Edition #83",
        "text": "Governments have been paying more and more attention to the development of artificial intelligence as recent advances in science and technology play a growing role in our society.\n\nFollowing the US Administration, the French Government has made its move. France has been providing organisations and research labs with top-end engineers and mathematicians for long. Giant tech companies such as Facebook have even opened major research labs in the country.\n\nThis week, Axelle Lemaire (the French Minister for Digital Affairs) launched a plan to build a national strategy in the field, stamped #FranceIA. Report expected end of March. At the same time, the French Senate mandated its commission for the evaluation of scientific and technological choices for an in-depth consultation on the topic. Videos of the audience are available \u2014 in French though : a must watch! (see Part I, and Part II)."
    },
    {
        "url": "https://chronicles.mfglabs.com/flying-taxis-cassette-revival-zuckerberg-2020-and-more-in-the-programmable-edition-82-793d821f7de3",
        "title": "Flying taxis, cassette revival, Zuckerberg 2020 and more in the Programmable Edition #82",
        "text": "We all know the famous quote attributed to Henry Ford. Alright, when collecting insights from future users you\u2019d probably better think about the real need behind the words instead of how people imagine solving it. But it also reveals our intrinsic incapacity to envision the future, because we can\u2019t help projecting mind conceptions from the past \u2014 or, as Benedict Evans would say it, we focus on asking the wrong questions.\n\nJust 40 years ago, BBC broadcasted one of its long-lasting Horizon documentary on chips and the future of the British industry. At that time, no internet (the primary precursor network ARPANET only dates back to 1980s), no personal computers. Just a thriving solid-state electronics industry with its first applications, mainly financed by Defense and Space money. Is software a real industry? Should we (the British Industry) manufacture chips or rather focus on programming them? Can we afford not to invest in this industry? How should governments deal with it?\u2026 \u201cCan we all live on the wealth of automatized factories and the earnings of an elite band of 60,000 software engineers?\u201d\n\nIn the same vein, I was watching (again) Blade Runner the other day. The world has reached singularity in 2016. Replicants have developed free will. Though, Harrison, when zooming in that picture, can\u2019t you just pinch it on the screen with two of your fingers? Do you really have to name the pixels you want the machine (vocal command) to zoom in? Seriously?\n\nI bet we are no better than the people in this documentary (and I certainly am far from Philip K. Dick and Ridley Scott\u2019s genius), asking these questions was already quite visionary at the time. Appart from the obvious fun you get when reading or watching again old prospective pieces, I think it reminds you that you just never know. And as of today, many analysts start to feel that \u201cthe next big thing\u201d could be coming \u2014 so maybe we will all have a chance to see it, and get the opportunity to provide a lot of fun to future generations."
    },
    {
        "url": "https://chronicles.mfglabs.com/home-operating-systems-generous-uber-adtech-duopolies-and-more-in-the-programmable-edition-81-7c137b043240",
        "title": "Home operating systems, generous Uber, adtech duopolies and more in the Programmable Edition #81",
        "text": "Alexa, Amazon\u2019s personal assistant, was the unrivaled star of this year\u2019s CES. Alexa is present inside the homes of the 6 million people who bought an Echo device last year and is now entering the homes of those who buy an ever-growing list of everyday objects and devices. In a 2 year time span, Echo has evolved from a weird novelty to a thing people actually use, through an ecosystem of \u201cskills\u201d which remove frictions in basic tasks.\n\nAlthough Amazon\u2019s competitors have weapons of choice in the assistant war (Google is not bad at search, Microsoft has locked professional usage in, Apple has 1B+ iphones running\u2026), becoming the first company to disseminate its intelligence in home equipment could be a winning move to lock users in (you don\u2019t buy a new fridge every week), and turn Alexa into your home\u2019s operating system."
    },
    {
        "url": "https://chronicles.mfglabs.com/happy-2017-and-more-in-the-programmable-edition-80-8f9b3deffc03",
        "title": "HAPPY 2017 and more in the Programmable Edition #80",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/intelligence-distribution-strategies-avalanches-transitions-and-more-in-the-programmable-edition-e7906de06cf1",
        "title": "Intelligence distribution strategies, avalanches, transitions and more in the Programmable Edition\u2026",
        "text": "Recent industry moves in two sectors \u2014 Intelligent Assistants and Autonomous Vehicles \u2014 highlight the tradeoff AI moguls face when distributing intelligence to the masses.\n\nTake Microsoft; the company announced yesterday it opened up Cortana to developers. This follows similar announcements about Apple\u2019s Siri, Amazon\u2019s Alexa, or Google\u2019s Assistant earlier this year. But it comes with a major difference: it does not launch on a single, proprietary piece of hardware (iPhone/Macs etc., Echo, or Google Home), but allows developers and manufacturers to freely integrate Cortana inside their own devices. Not really surprising from Microsoft, who did the exact same thing with Windows and the PC in the 80s/90s.\n\nThis is a very different strategic approach to intelligence distribution. The tradeoff tech giants face is: owning the final touchpoint but accessing fewer people, versus being the middle man but being accessible no matter the hardware.\n\nSimilar tradeoff is observed in the AV space, with surprising turnarounds: Alphabet, who is probably the most advanced company in autonomous driving systems, started producing the infamous autonomous \u201cBubble car\u201d back in 2014, but seems to have operated a radical U-turn this week. The company reportedly stopped the Bubble car project and transferred all AV activities to a new company, Waymo, that will carry out projects in collaboration with automakers. Waymo\u2019s ambition appears to be the OS of your vehicle, adopting the same strategy Microsoft had with Windows.\n\nOur guess is that similar turnarounds will occur in the next months, in AV and Intelligent Assistant landscape, so stay tuned!"
    },
    {
        "url": "https://chronicles.mfglabs.com/uncertain-ai-projections-rocketmen-amazon-brick-and-mortar-shops-and-more-in-the-programmable-fa6b3181712c",
        "title": "Uncertain AI projections, rocketmen, Amazon brick-and-mortar shops and more in the Programmable\u2026",
        "text": "In 1970, Marvin Minsky, founder of the Artificial Intelligence Laboratory at MIT, back then one of the top scientists working on the subject of AI, declared\n\n45 years later we are still miles away.\n\nIn 1997, on the occasion of Deep Blue beating Gary Kasparov at chess, the NYT published an article that explained how Go was a game very different than chess (it is) and that brute force algorithm that worked for chess could not possibly work for Go (they don\u2019t). They predicted that humans could not be beaten at Go for another one hundred to two hundred years. In 2016, less than 20 years later, Google\u2019s Deep Mind beats the world champion.\n\nMaking prediction on AI, and more generally on computer related topics is extremely hard; as Bill Gates famously said:"
    },
    {
        "url": "https://chronicles.mfglabs.com/dabots-launch-and-more-in-the-programmable-edition-76-7b280cb2bcaa",
        "title": "Dabots launch and more in the Programmable Edition #76",
        "text": "For 6 months now, we have been working hard on defining how conversational interfaces can help brands address their audience in more natural ways. We have learned and experimented a lot, on a very wide variety of topics: AI modules, UX best practices, chatbots frameworks, GAFA platform strategies, etc.\n\nFirst, we are very proud to introduce Nina. Nina is our first commercial chatbot, done in partnership with ticketing leader Digitick. Nina helps you book a ticket for the right event, for you, a friend or family. A media campaign is live to support Nina, so you might have seen her in the Parisian subway or online.\n\nSecond, we have launched a landing page dedicated to Dabots, our chatbot offering, enhancing our visibility. Do not hesitate to share this around you!\n\nSeveral other conversational projects are on their way, so stay tuned, and join the conversation!"
    },
    {
        "url": "https://chronicles.mfglabs.com/zero-ratings-iot-based-ddos-attacks-foodtech-and-more-in-the-programmable-edition-75-9d095950cda7",
        "title": "Zero ratings, IoT-based DDoS attacks, foodtech and more in the Programmable Edition #75",
        "text": "AT&T Inc.\u2019s agreed to buy Time Warner Inc. for $85.4 billion, in one of the biggest deal of the year. This is yet another example of the strategy of distributors to buy content producers. Distributors that own content hope to differentiate their product by offering exclusives. France\u2019s SFR is pursuing this kind of synergies by most notably owning Premier League Football exclusive rights.\n\nThe problem is that distributors (in saturated markets such as AT&T\u2019s) and content producers have opposed business models: distributors have a vertical business, they need to differentiate their offer with exclusive services and content. On the other hand, content companies have horizontal business models: they have fixed cost to produce content, and almost zero distribution costs, thus needing to monetize their content on the broadest customer base possible.\n\nIf the content is meaningful, that means the value you create as a distributor by keeping it exclusive is far lower than the value you destroy by missing on potential customers from other platforms.\n\nThe answer might lie in zero rating: free data usage for certain applications. AT&T could keep Time Warner\u2019s content available on all platform, but if you are on AT&T it does not count in your data plans. AT&T is already doing just that to push DirecTV, a content company it owns.\n\nThis move puts the operator in a very good position, as it differentiate its offering while not shrinking the customer base of its content company; and at the same time puts it in a very good position with competing content providers: if they want to also be zero rated on the network, they need to pay.\n\nThanks and have a great day"
    },
    {
        "url": "https://chronicles.mfglabs.com/public-ai-mckinseys-vision-of-mobility-and-more-in-the-programmable-edition-74-d7f7524555f4",
        "title": "Public AI, McKinsey\u2019s vision of mobility and more in the Programmable Edition #74",
        "text": "One of the biggest human breakthrough that could happen in the next years is the rise of an artificial \u201cgeneral\u201d intelligence, i.e. the intelligence of a machine that could successfully perform any intellectual task that a human being can. Of course there are many very hard obstacles to overcome to get there, but there is no day without a news in that sense.\n\nIf this could make us enter a new era, it also comes with challenges. What is surprising is that the debate on the safest ways and develop AI mainly occur in the private sector; governments and public organisations have been quite discreet on this, and the funding of public AI thinking groups are somewhat rare.\n\nHowever, this month might be a milestone in public sector awakening on AI. First UK launches an AI commission. Second, the White House releases a report on AI, and third, Obama discusses AI with Wired. We can\u2019t wait for French officials\u2019 opinions on this. Might be good presidential campaign theme idea in 2017!"
    },
    {
        "url": "https://chronicles.mfglabs.com/free-lunches-music-blockchain-clean-lumascapes-and-more-in-the-programmable-edition-73-93bd0bd79e77",
        "title": "Free lunches, Music Blockchain, clean LUMAscapes and more in the Programmable Edition #73",
        "text": "A lot of tech companies are naturally gifted at user experience, thus owning the relationship with people. These tech companies have built gigantic sound boxes and sticky ways to interact with their consumers.\n\nBrands are systematically rushing into these ecosystems. They are opportunity for brands to create more tailored, engaging content, and connect with their consumers. Yesterday: Facebook pages and Twitter accounts, today: Snapchat Discovers , Youtube video series, gaming, branded playlists, intelligent assistants and Virtual Reality.\n\nBut these cool ecosystems sometimes function on a \u201cfree lunch\u201d mode. Take the current chatbot rush. Everyone can do whatever it wants. Platforms offer free integration, free tools, free everything. Will it always be like that? Nothing is less certain, as it strongly reminds us what happened for instance with Facebook pages (want to target the audience of your page? Well, now you need to pay me to do that).\n\nOur advice for brands: watch out for baits, and do not place all your eggs in the same basket."
    },
    {
        "url": "https://chronicles.mfglabs.com/psg-new-e-team-pixels-flea-markets-and-more-in-the-programmable-edition-72-b1fa04935533",
        "title": "PSG new e-team, Pixels, flea markets and more in the Programmable Edition #72",
        "text": "\u201cThe next big innovation is going to take place at the intersection between hardware and software, with AI at the center\u201d, said Rick Osterloh on Tuesday at the Google Pixel event (with no big surprise, to be honest).\n\nGoogle indeed unveiled its revamped mobile strategy. But for a supposedly hardware-focused conference, yesterday\u2019s event was overwhelmed by the brain behind all the devices, Google Assistant. Alike its direct competitors (say, Amazon Echo with Alexa inside or Apple\u2019s Siri), Google is opting for a platform strategy. By December, developers will be given the opportunity to program direct actions (\u201cOk Google, turn on the radio in the bathroom\u201d), or to program conversation actions with back-and-forth interactions or even directly embed Google Assistant in their devices.\n\nGoogle indeed wants its assistant to be \u201cuniversal\u201d, but that\u2019s not enough. It has to be \u201can individual Google for each user\u201d. Relying on recent advances in deep learning techniques, Sundar Pichai hopes for the development of more and more human-like capabilities, such as natural expression or human emotions deciphering. In a word, your own assistant will know you enough to adapt itself to your preferences and context.\n\nThis reminds me of a recent conversation I had (with a real person) about the perception we have of the difficulty level for a robot to accomplish certain tasks. Some very stupidly simple things are still years and years of research ahead, but things like unscrambling a human emotional state through its voice seem right at hand. Even I, as a human, can still be really bad at this. Maybe a day will come we will stop talking about machine learning. It will be about human learning, with machines helping us getting better at being humans.\n\nFor the time being, Google Home will come with a much appreciated feature, the mute button, just in case you don\u2019t want your assistant to know too much."
    },
    {
        "url": "https://chronicles.mfglabs.com/british-prophecies-banks-alliances-interplanetary-travel-and-more-in-the-programmable-edition-71-277661cac91d",
        "title": "British prophecies, banks alliances, interplanetary travel and more in the Programmable Edition #71",
        "text": "The 3rd episode of speculative fiction British TV series Black Mirror, \u201cThe Entire History Of You\u201d, features a world where everybody has a chip behind the ear, video and audio recording every single moment in life. This allows memories to be played back, either in front of the person\u2019s eyes, or on a screen, and to be shared.\n\nThe episode focuses on the pitfall of this technology, which is obviously privacy. A husband catches his wife cheating on him by replaying the movie of their life, and the story ends dramatically. Fortunately this is pure fiction.\n\nOr is it?\n\nLast Friday, Snap Inc. released Spectacles, a goggle with built-in video camera, connected to the Snapchat app. While Snap Inc. CEO Evan Spiegel calls them a \u201ctoy\u201d, let\u2019s keep in mind the warning of our modest TV series on privacy: Spectacles might not help catch an unfaithful wife or husband, but they certainly are an advertiser\u2019s dream, who\u2019d love to see what you see\u2026"
    },
    {
        "url": "https://chronicles.mfglabs.com/black-boxes-transportation-revolution-technological-unemployment-and-more-in-the-programmable-b640daea37de",
        "title": "Black boxes, transportation revolution, technological unemployment and more in The Programmable\u2026",
        "text": "We\u2019ve talked about it, the race for self-driving cars is on. Car manufacturers, tech behemoths, ride-hailing companies, computer vision start-ups\u2026 All are now placing their pawns to conquer the next frontier, sometimes turning old friends into foes.\n\nUber has recently invested in Otto (a Google-veterans start-up working on self-driving trucks) and plans to deploy a Volvo fleet for a commercial test in Pittsburgh in the coming weeks. Ford has acquired Snaips, a computer vision start-up. General Motors invested in Lyft, the ride-hailing company. Fiat Chrysler is set to work with Google to create self-driving minivans. Apple confirmed the existence of the car project and invested in Didi Chuxing (China\u2019s giant ride-hailing company), and has views on McLaren. And this is just to name a few.\n\nPolicy makers on their side are struggling to hold Silicon Valley\u2019s horses, but ethical issues do arise. Apart from obvious safety concerns, machines will need a code of conduct as they will have to take decisions our lives could depend on. Try out the trolley problem on your own to get the idea.\n\nAll this is not just about cars or some tech giants\u2019 new obsession, there is a potential Third Transportation Revolution at stake. John Zimmer, Lyft Co-founder, presented his vision of a world built around people, not around cars, where city streets become effervescent public spaces again, shaped by the community who lives in, not by the cars crossing by. A must read for a cheerful week!"
    },
    {
        "url": "https://chronicles.mfglabs.com/the-programmable-edition-69-airpods-master-plan-unexpected-gta-usage-algorithmic-bosses-and-e3dfc94be415",
        "title": "The Programmable Edition #69: AirPods master plan, unexpected GTA usage, algorithmic bosses and\u2026",
        "text": "Back to work for our weekly Programmable Chronicles!\n\nLast week during its traditional September keynote, Apple announced a lot of new stuff. Of course there were the long-awaited iPhone 7 (with no audio jack, for very understandable reasons) and the Apple Watch Series 2, but the real disruptive device was the AirPods.\n\nFar more than wireless headphones, the AirPods are the company\u2019s first ear computer. A very simple computer with one only control, a touch sensor, that activates Siri.\n\nMeanwhile Google, via Deepmind, presents WaveNet, a synthesized speech system that sounds more natural than today technologies.\n\nAnd suddenly you\u2019ll find yourself conversing with an AI\u2013powered voice assistant via a tiny device in your ear.\n\nLooks like something you have seen somewhere in a movie?"
    },
    {
        "url": "https://chronicles.mfglabs.com/red-envelopes-emotional-robots-summer-break-and-more-in-the-programmable-newsletter-68-99db1c7a28c7",
        "title": "Red envelopes, emotional robots, summer break and more in The Programmable Newsletter #68",
        "text": "Last week, Unilever purchased Dollar Shave Club, a Santa Monica based startup that sells razor blades through a subscription service, in a $1 billion dollar deal.\n\nIt may seem expensive, but it is actually very cheap in the world of consumer packaged goods (CPG). Only a few years ago, in 2005, Procter&Gamble (P&G) bought Gillette, the market leader, for a whopping $57 billion.\n\nDollar Shave Club success emphasizes how the internet has shaken the two pillars on which the incumbents built their strength:\n\n- Huge marketing budget. Youtube and Facebook have weakened this barrier, as it is now very easy and almost free to create, publish and share a video with millions of people.\n\n- Control of the distribution. Before the internet, the control of the distribution channels was so crucial that it was almost impossible to compete with incumbents. Now with Amazon, its infinite shelf-space and easy shipping, it just doesn\u2019t matter anymore.\n\nThis cost structure has allowed DSC to offer a much better deal than the incumbent Gillette, with a good-enough product: textbook disruption."
    },
    {
        "url": "https://chronicles.mfglabs.com/elon-musks-master-plan-snapchat-connect-ads-in-pok%C3%A9mon-go-and-more-in-the-programmable-d653868a8698",
        "title": "Elon Musk\u2019s Master Plan, Snapchat connect, ads in Pok\u00e9mon GO and more in The Programmable Edition\u2026",
        "text": "This week was announced the largest acquisition of a European technology company: Masayoshi Son\u2019s SoftBank agreed to buy ARM Holding for \u00a324,3 billion in an all-cash transaction.\n\nARM sure is an interesting company, but calling it a \u2018smartphone chip designer\u2019 is not quite right.\n\nIt has two main activities: one is licensing an Instruction Set Architecture (a language for processing chips) to companies that design their own chips (for example Apple\u2019s A-series); and two is selling IP cores, which is not only the architecture but also the actual logic to implement it. It is true that most of the business is driven by smartphonse now, but the characteristics of the ARM design (low power consumption) apply incredibly well to the internet of things paradigm.\n\nMasayoshi Son has a history of investing very long term, and this deal should be seen more as another of his investments, rather than a synergy-seeking acquisition to its telco activity."
    },
    {
        "url": "https://chronicles.mfglabs.com/snapchat-memories-molotov-tv-minecraft-ai-and-more-in-the-programmable-edition-66-9e031c39cc2c",
        "title": "Snapchat Memories, Molotov TV, Minecraft AI and more in The Programmable Edition #66",
        "text": "Pok\u00e9mon GO is what happens when Nintendo, The Pok\u00e9mon Company and former Google subsidiary Niantic Labs bring Nintendo\u2019s strongest franchise to our smartphones, with an Augmented Reality twist.\n\nDespite servers causing the game to crash, Pok\u00e9mon GO is on the verge of surpassing Twitter in terms of daily active users, and has already been installed on more Android phones than Tinder. And the game is not available worldwide yet.\n\nOnce again, Nintendo has proven the public wrong about its ability to succeed on mobile platforms. The combined use of Augmented Reality and GPS technology brings a new dimension to the gameplay, making it not only more social but also more immersive.\n\nYes, Pok\u00e9mon is just a game. But games are becoming increasingly indistinguishable from reality. So, Pok\u00e9mon will end up being part of our new reality.\n\nLooking at the stock price going up 33% in two days, at least investors and gamers are in line: the bright days are ahead."
    },
    {
        "url": "https://chronicles.mfglabs.com/mfg-labs-shiny-new-website-and-more-in-the-programmable-edition-65-c0f37139223e",
        "title": "MFG Labs shiny new website and more in The Programmable Edition #65",
        "text": "Transforming brands into champions of the digital and data-driven economy is a mission that requires a very wide set of complementary expertise. That is why our team ranges from mathematicians, to data scientists, software engineers and designers. By having all these talents in-house, we provide agility and consistency to our clients.\n\nToday, we are proud to release our new corporate website. Redesigned from the ground up, it is the best example of our capabilities in a critical asset inside our skill set: design. From now on, our site showcases the many talents of our design team: UI, UX and even illustration, that often deliver great value to our clients.\n\nTo celebrate this launch, we wrote a short read about the the most interesting steps of this revamp."
    },
    {
        "url": "https://chronicles.mfglabs.com/angry-taylor-swift-lonely-texans-racist-algorithms-and-more-in-the-programmable-edition-64-3729389434bb",
        "title": "Angry Taylor Swift, lonely Texans, racist algorithms and more in The Programmable Edition #64",
        "text": "Throughout human history, creative destruction has had catastrophic consequences for many low-skilled workers. Paradoxically, it has also created economic growth and improved the overall human condition.\n\nIn the age of artificial intelligence, discussions about whether machine will take our jobs or not, are persistant. We, as humans, will need to find new ways to value work and estimate social contributions.\n\nBut these issues seem a bit far-fetched when looking at the current state of the economy, don\u2019t they?\n\nSo, before worrying about the impact of AI on employment, policymakers would be better off tackling actual issues we are facing today. Adapting employment law, taxation and education to the new rules of the Gig Economy could be the first step towards the construction of a very different economic future."
    },
    {
        "url": "https://chronicles.mfglabs.com/candide-founder-greener-energy-barriers-to-ai-and-more-in-the-programmable-edition-63-8b38412b8114",
        "title": "Candide founder, greener energy, barriers to AI, and more in The Programmable Edition #63",
        "text": "Today, Tesla announced it wanted to buy SolarCity, a solar energy installation provider. For many, the deal to-be made no sense, and Elon Musk (key shareholder of both companies) was acting on his own. But here are some background information.\n\nBy 2030, solar and wind energy will become the cheapest way of producing electricity. According to Bloomberg New Energy report, renewable energy will account for 60% of the $11,4 trillion invested in the industry over the next 25 years. Thus it makes perfect sense for Tesla to investigate opportunities in this sector, and accelerate the energy transition to foster electric cars adoption.\n\nHowever, the strongest argument is that batteries lose efficiency over time. Passed a certain age, they are no longer suitable for your Tesla. The company can either refurbish the batteries at a significant discount, or repackage them into end-to-end energy installations for your house."
    },
    {
        "url": "https://chronicles.mfglabs.com/apple-wwdc-16-sirikit-and-imessage-much-awaited-announcements-and-more-in-the-programmable-edition-9adb8d2eede7",
        "title": "Apple WWDC \u201916 SiriKit and iMessage much awaited announcements and more in The Programmable Edition\u2026",
        "text": "Apple unveiled lots of new things at WWDC \u201916, including iOS 10. The most notable features, SiriKit and iMessages apps, are embracing the conversational paradigm. What it means is that you can now book a ride, make a payment or virtually interact with service provider without leaving the chat interface nor the home screen, by the sole use of text or voice based commands.\n\nThe truth is, we all knew Apple would do that: not because the company became predictable, but because it was necessary. Facebook, Google, Amazon, Microsoft are in fact fiercely battling for the AI assistant supremacy. The stakes are sky-high since voice interfaces will surely become our primary interface with machines.\n\nPlus, US app downloads are declining by more than 20% compared to 2015. This comforts us in our conviction that the paradigm is shifting from an app economy the App Store helped establish, towards the seamless integration of services through conversation.\n\nBut GAFA are not the only ones dreaming of being your voice gateway. Viv (by ex-Siri creators), the new-generation smart assistant platform could be a serious tech challenger."
    },
    {
        "url": "https://chronicles.mfglabs.com/spotifys-edge-game-of-life-brand-new-emojis-and-more-in-the-programmable-edition-61-e10f8e832146",
        "title": "Spotify\u2019s edge, game of life, brand new emojis, and more in The Programmable Edition #61",
        "text": "In 5 years, Snapchat built a larger user base than Twitter did in 10 years. Is it Snapchat\u2019s peculiar design or the vanishing media consumption experience that created the hype?\n\nSome would argue that these the two companies shouldn\u2019t be compared to each other. They have different market positions, products and usage. But the truth is, this shift does actually matter: the proliferation of videos changes what the audience sees, and therefore, where to get the money from.\n\nBut don\u2019t you worry too much about Twitter. It has existed in a near-constant state of chaos, since the beginning. It has lost its hype, its investors and even its CEOs. And hopefully, the only person able to save the company will be the one person who created the magic in the first place, Jack Dorsey.\n\n\u2018What is dead may never die\u2019"
    },
    {
        "url": "https://chronicles.mfglabs.com/tyranny-of-the-popular-mercedes-batteries-useless-dragons-and-more-in-the-programmable-edition-3e27cc320891",
        "title": "Tyranny of the popular, Mercedes batteries, useless dragons, and more in The Programmable Edition\u2026",
        "text": "In Artificial Intelligence, the artificial part can mean quite a number of things.\n\nArtificial could mean handcrafted, human-built: a system designed and maintained by people and that satisfies constraints. Our bridges, and even traditional software solutions, are artificial in that sense. People have been building artificial things for a while now and came up with a reliable process to ensure that every part of it is doing what it is supposed to do. We need to ensure that our AI powered apps are bound to the same principles.\n\nFor some people, artificial can just mean straight up fake: it\u2019s by no means intelligent. Maybe it\u2019s all a marketing hoax and the only cognitive parts about it are the smart engineers that built it.\n\nArtificial can also mean unnatural and some companies have been working on that aspect as well. By making AI less artificial, they are turning it into something that you can talk to and have a conversation with.\n\n\u200b\n\nAt the end of the day, AI powered apps are already changing the way we interact with our phones, our computers and even other people. And if an AI winter comes back, at least Siri will give us the weather report."
    },
    {
        "url": "https://chronicles.mfglabs.com/sneaky-uber-code-as-a-commodity-self-drifting-cars-and-more-in-the-programmable-edition-59-19f64b1eb7b6",
        "title": "Sneaky Uber, code as a commodity, self-drifting cars, and more in The Programmable Edition #59",
        "text": "Major tech companies are shaping the future of human-machine interface.\n\nTo do so, they attract, develop and retain the best mathematicians, and world-class engineers to come up with the \u2018next big thing\u2019.\n\nDuring the last decade, Amazon, Facebook and Google placed large bets on big-data web services, advanced AI, intelligent assistants, and voice interfaces. But what about Apple?\n\nIt is not a big-data-services company. It hasn\u2019t demonstrated strong mastery over the core machine learning skills needed to be the next AI services provider. And finally, its secrecy policy makes it harder for any outsider to keep an unbreakable faith in the company\u2019s success.\n\nWe will listen closely to what Apple has to say during WWDC \u201916 about Siri and the future of their AI services. But in the meantime, Google, Facebook and Amazon seem to be in a far better position to compete in this new paradigm."
    },
    {
        "url": "https://chronicles.mfglabs.com/blockchain-revolution-love-evolution-clean-energy-solution-and-more-in-the-programmable-edition-be858dd9adbb",
        "title": "Blockchain revolution, love evolution, clean energy solution and more in The Programmable Edition\u2026",
        "text": "\u2018I pray every single day for a revolution\u2019 \u2014 What\u2019s up, 4 Non Blondes, 1992\n\nBlockchains. Here comes the new buzzword. Nothing too uncommon though: everyone speaks about it, says it would change our living world and yet, no one but a few know what it is, what it does and how it does it. Some are even trying to regulate it already\u2026 Sounds familiar? Beware of the next bubble.\n\nNo doubt that the adoption of blockchain technology will bring a new set of opportunities for companies as well as a lot of challenges. In fact, the transformative potential of such a technology extends beyond financial services: it has tremendous implications for many, if not all, businesses, setting up the right environment for a sharing economy to emerge. Banks, corporations and governments will have to adapt. The same goes for people.\n\nAnd if blockchain gets as big as it is predicted to be, centralization will be a thing of the past. No more intermediaries, only peers: just data, connected."
    },
    {
        "url": "https://chronicles.mfglabs.com/data-crystal-ball-ai-building-ai-mid-trip-shopping-and-more-in-the-programmable-edition-57-b77307f5af1f",
        "title": "Data crystal ball, AI building AI, mid-trip shopping and more in The Programmable Edition #57",
        "text": "The US government just took public debate about AI to the next level. On Monday 3rd, the White House announced \u2018a new series of workshops and an interagency working group to learn more about the benefits and risks of artificial intelligence\u2019.\n\nWhen AI is about to have such an impact on medicine, employment, education or defense, bringing people to the debate is getting critical. Even if issues raised are quite technical, our future can\u2019t be only decided by tech experts and web giants.\n\nWe have recently been (and still are) shocked by recent law developments towards more surveillance. Not by the content of the law per se, but by the lack of public debate around it. The White House initiative is, in that sense, a great move forward. Let\u2019s hope people take the chance to join in."
    },
    {
        "url": "https://chronicles.mfglabs.com/convcomm-strategic-report-released-and-more-in-the-programmable-newsletter-56-c6ae025b753e",
        "title": "ConvComm strategic report released and more in The Programmable Edition #56",
        "text": "The current \u201cGreat Bot Rush\u201d, largely triggered by Facebook Messenger opening to brands on April 12, is incredibly similar to what happened after Facebook launched the OpenGraph back in 2012: an explosion of new services taking advantage of the feature. Among these, very few provided an improved and meaningful service to consumers. Thus very few are still live today.\n\nIn the prevailing bustle, it is hard for brands to understand the impact of chatbots and conversational commerce, and adopt sustainable strategies. To help separate the signal from the noise, we published earlier this week a strategic report, \u201cIntelligent Assistants, Messaging Platforms & Conversational Commerce\u201d that delivers our convictions on the matter, and a framework to kick-start a ConvComm strategy."
    },
    {
        "url": "https://chronicles.mfglabs.com/magic-reality-internet-of-energy-driverless-buses-and-more-in-the-programmable-edition-55-9f415641668a",
        "title": "Magic reality, internet of energy, driverless buses and more in The Programmable Edition #55",
        "text": "The recent enthusiasm around AI is mainly fueled by breakthroughs in computer vision and reinforcement learning. The family of algorithms used \u2014 Deep Learning \u2014 has been first documented in the 1980s, and is based on concepts from the 1950s\u2026 Why hasn\u2019t this stuff been successful before ?\n\nThe answer is data. And computing power. These algorithms need a tremendous amount of (good) data to be trained, and a lot of computing power. Getting a superstar cluster of machine running now takes just a few clicks thanks to Amazon or OVH: the real progress here is the ability to create very high quality datasets at scale.\n\nThe strategic value of data completely dwarfs that of algorithms. Algorithms are now becoming a commodity: more and more good-enough open source algorithms are being released. Data is the real asset: no surprise that Google open-sourced its secret sauce TensorFlow, and not its precious data.\n\nThanks and have a good day,"
    },
    {
        "url": "https://chronicles.mfglabs.com/silly-bots-google-s-digital-city-nba-legend-and-more-in-the-programmable-edition-54-636928855e6a",
        "title": "Silly bots, Google\u2019s digital city, NBA legend and more in The Programmable Edition #54",
        "text": "English to French translations sometimes hide interesting subtleties. Take the translation of \u2018design\u2019: it translates into \u2018dessin\u2019, meaning drawing, but also into its homonym \u2018dessein\u2019, which means intention or objective. This second meaning is largely ignored in France, but the global misunderstanding is not country-specific. It is perhaps one of the reason creative people have been looked down upon for so long.\n\nBut this has totally changed as great design is improving the way we live and work, and designers are now included at every stage of the life of products and services. MIT professor J. Maeda\u2019s much anticipated DesignInTech Report monitors every year this evolution, from design space M&A activity to the place of design at school. This year\u2019s conclusions strongly comfort us, as we have been making designers work along data scientists and software engineers since 2010, in our fully integrated model choice!"
    },
    {
        "url": "https://chronicles.mfglabs.com/f8-game-changing-announcements-and-more-in-the-programmable-edition-53-bd23b8c8efa9",
        "title": "F8 game-changing announcements and more in The Programmable Edition #53",
        "text": "It\u2019s OUT! Finally, after months of speculations and expectations, Facebook has officially launched the Messenger Platform. During the annual developers\u2019 conference (the \u201cF8\u201d, a gigantic event dedicated to the worldwide community of developers using Facebook technologies) Mark Zuckerberg presented Facebook\u2019s vision for the next ten years. It starts with focusing on product, then ecosystems, and finally technologies that will enable new products, that will in turn federate ecosystems, and so on.\n\nMessenger has now reached the \u201cecosystem building\u201d phase of its developments. Developers can now program bots to interact with users in a conversational manner through the Messenger API. Organisations can already do so on other platforms such as Slack, Kik or Telegram, but Messenger is just the dominant one in our markets. Facebook will cautiously select partners at first, to avoid a complete bot invasion, and let user block annoying bots to avoid spam.\n\nOur main question remains the business model behind it, mainly in the mid-term. If Facebook plans to be the mediation platform between brands and users, it will probably not do it for free. Besides, Apple and Google have not yet entered the battlefield\u2026"
    },
    {
        "url": "https://chronicles.mfglabs.com/drunk-tweets-hidden-musical-identity-reality-shows-and-more-in-the-programmable-edition-52-3ebb04e7fa7f",
        "title": "Drunk tweets, hidden musical identity, reality shows and more in The Programmable Edition #52",
        "text": "One of the ways to understand and measure the impact of machine learning is to keep track of the companies at the forefront of innovation with the potential to disrupt entire industries.\n\nIn an ever-changing world where myriads of companies fight for a spot in the sunlight, knowing which ones matter for each industry is no small feat. Thankfully, Shivon Zilis came up with a great visual, the Machine Intelligence Landscape, to help us build a detailed mental picture of the revolution we are living.\n\nThe second version of the Landscape, published at the end of 2015, is very different from the first one published a year earlier and reflects how quickly change operates in this field. Ultimately, the companies that will revolutionize their industry tomorrow are the ones aiming today to solve problems worth solving, and committed to bring actual business value through learning algorithms."
    },
    {
        "url": "https://chronicles.mfglabs.com/goodbye-novelists-agencies-traffic-lights-and-more-in-the-programmable-edition-51-378c548e9bf1",
        "title": "Goodbye novelists, agencies, traffic lights and more in The Programmable Edition #51",
        "text": "The French mathematician Henri Poincar\u00e9, in his masterpiece Science and Method (1908), penned the following words : \u201cThe scientist does not study nature because it is useful to do so. He studies it because he takes pleasure in it, and he takes pleasure in it because it is beautiful.\u201d\n\nFar from being opposed, Mathematics and Nature are closely related. Hence, we can find the golden ratio in the number of petals in a flower, in snail shells or in spiral galaxies. Likewise, many artists are inspired by the laws of nature and geometry. For instance, Karl Gerstner often uses symmetry as the building block of his works."
    },
    {
        "url": "https://chronicles.mfglabs.com/vrm-ftw-and-more-in-the-50th-issue-of-the-programmable-edition-32763af909fe",
        "title": "VRM FTW and more in the 50th issue of the Programmable Edition",
        "text": "When planning this 50th issue of the Programmable Edition, we swore we\u2019d avoid talking about #bots #messaging #AI #machinelearning #assistant #conversationalcommerce #chatisthenewapp.\n\nWhat is a better occasion than an anniversary to have a deeper look at where all those trends are heading to? We put them all in perspective with an old concept which makes sense today more than ever: VRM. Make sure to read our post here, and thanks to all of you for following us!"
    },
    {
        "url": "https://chronicles.mfglabs.com/messaging-platforms-the-old-chimera-of-vrm-16580173697c",
        "title": "Messaging platforms & the old chimera of VRM \u2013",
        "text": "Marketing evolves at a very high pace, and will continue along our insatiable hunger for technology adoption.\n\nDirect Marketing, One-to-One Marketing, Permission Marketing, and so on, are solely models aiming at engaging a privileged and personalized relationship with the consumer.\n\nIn order to deeply understand where marketing is going, it\u2019s interesting to look at the evolution of the platforms trying to be the gateway between you and your usage of the Internet.\n\nThe first wave of platforms was focused on linking content. Search giants that emerged there were Google and Yahoo. They helped us find content on the web.\n\nThe second wave was about linking people. Social media giants Facebook and Twitter experienced tremendous growth, allowing us to connect through content with the people we know (or not).\n\nToday, I would say our Internet usage is shifting towards services, like transport, personal finance, travel, etc. How to link us up with those services we use everyday in a streamlined and simple way? Messaging platforms (like Facebook Messenger, Slack, Telegram, Kik, etc.) and Agents (like Apple Siri, Google Now, Microsoft Cortana, Amazon Alexa, etc.) are at the forefront of this shift.\n\nSearch, Social Media and Messaging are different layers, but they all aim at one simple goal: being your entry point to the Internet. Because when you are the first door, everyone is knocking at you. To sponsor their business, to sell their services, to distribute their product: you are the platform that rules them all.\n\nAt this game, Messaging should win.\n\nMessaging > Social Media > Search in so many ways. This is by far the best experience to leverage consumer intimacy. This is a race for who will be better to connect every aspect of our life.\n\nIn the following months we\u2019ll observe how the landscape evolves and different moves from the big players, and maybe we\u2019ll witness the awakening of an old marketing chimera which fits perfectly with Messaging platforms: the VRM.\n\nVRM stands for Vendor Relationship Management, as opposed to CRM (Customer Relationship Management). VRM is about buyers finding sellers, not sellers finding \u2014 or \u201ctargeting\u201d \u2014 buyers.\n\nLet\u2019s take a concrete example: a car rental customer should be able to say to the car rental market:\n\n\u2014 and have the sellers compete for the buyer\u2019s business, as Doc Searls brilliantly explained in his essay The Intention Economy.\n\nVRM provides customers with the means to control their own experiences. Imagine, just by a simple conversation, you could organise all your winter trip \u2014 the flight, the ski material renting, the accommodation, etc. \u2014 tailored for you, according to your constraints and your desires, and possibly also the ones from your friends or family.\n\nToday conversations matter more than ever, because we are probably looking for more experiences, than simple transactions.\n\nThe boom of concierge apps, assistants & bots is the sign that we are close to achieve this vision, through Messaging platforms.\n\nMaybe you\u2019ve heard about this rumor of Facebook opening a Bot Store, or all the recent announces from the different Messaging platforms. They all seem to embrace the asian way of life from WeChat or Line, in order to become the ultimate platform, turn upside down markets, and invert the balance power.\n\nI\u2019m quite curious to see if Facebook or others will follow some VRM principles. Is the move to open APIs to let developers reach their massive audience just a first step? Will Facebook M learn from this ecosystem and replace one day all the bots? What about the information master Google? And how will all those Messaging platforms and concierge apps from Slack to Operator compete?\n\nThere is huge opportunities to develop a brand new business model in this area, which would be very interesting to analyze.\n\nMaybe it\u2019s time for brands to seize the opportunity of the Messaging wave, and avoid being disrupted again by a new Internet giant."
    },
    {
        "url": "https://chronicles.mfglabs.com/super-mario-super-uber-super-michelle-and-more-in-the-programmable-edition-49-ba5a32dc7e9e",
        "title": "Super Mario, Super Uber, Super Michelle and more in The Programmable Edition #49",
        "text": "Peter Thiel, the Silicon Valley investor, has called Uber \u00ab the most ethically challenged company in Silicon Valley\u00bb. As a matter of fact, Uber moves the needle in many ways, creating an economic and cultural shift in our societies.\n\nTravis Kalanick, the man behind the only company that has more than 300 A/B tests going on in cities around the world, and occasionally Uber\u2019s CEO, has the clear intention to make an impact when trying to build the future of transportation."
    },
    {
        "url": "https://chronicles.mfglabs.com/short-attention-spans-publishers-wake-up-hipster-armies-and-more-in-the-programmable-edition-f25b0e1ab316",
        "title": "Short attention spans, publishers awakening, hipster armies and more in The Programmable Edition\u2026",
        "text": "We have been using Slack MFG Labs for 6 months now so we are pretty passionate with the current debate among the Slack community: does Slack fragment attention and reduce productivity, and if so, who is to blame: the user or the tool? Some say its design encourages noisy communication, some say the key is reasonable usage. Looks like it is the same story all over again.\n\nWe do not have specific opinion on the matter, partly because attention fragmentation is hard to mesure and must be balanced with benefits (integrations, helpful bots, super fast files transfer, custom emojis :) etc.), and that constant sollicitation is far from only being Slack\u2019s privilege. However what we know for sure is that the main force shaping behaviors is company culture, and that keeping alive live announcements, team lunches, encouraging real-life discussions and empathy, drives right tools usage."
    },
    {
        "url": "https://chronicles.mfglabs.com/google-car-crash-brilliant-ads-scary-bots-and-more-in-the-programmable-edition-47-8bce81355ff5",
        "title": "Google car crash, brilliant ads, scary bots and more in the Programmable Edition #47",
        "text": "Last week, MIT revealed a new revolutionary mobile chip specially designed for AI.\n\nWhile deep learning techniques keep challenging the limits of AI, their adoption has been limited by the high power consumption of the graphics chips they normally use.\n\nTo that purpose, MIT engineers built Eyriss: a new chip able to run neural networks at low power. Specially designed for AI, Eyriss promises to make recent progress more accessible to everyday mobile devices such as smartphones, wearables, self-driving cars or medical portable electronics.\n\nGiven that Apollo 11 was landed on the moon using a computer 1300 times less powerful than an iPhone 6, who knows what our pocket devices will be capable of in the near future."
    },
    {
        "url": "https://chronicles.mfglabs.com/ads-in-messenger-apple-s-secret-weapon-machine-reengineering-and-more-in-the-programmable-edition-e33c6253e318",
        "title": "Ads in Messenger, Apple\u2019s secret weapon, machine reengineering and more in the Programmable Edition\u2026",
        "text": "If I were this Boston Dynamics employee caught on tape bullying a robot with a stick, I would feel real bad right now: the video will obviously be the main evidence robots will use to justify a complete annihilation of the human race in a near future.\n\nJokes asides, it is quite alarming to see AI gaining control over our executive powers. The negative effects of this shift is today blatant in war zones. NSA\u2019s Skynet programme is a perfect example of the flaws of machine learning assistance on life and death decisions. Combine AI extensive usage and drone\u2019s remote and unmanned capabilities and you\u2019ve setup an error-prone environment nicely disconnected from ground reality. It is tempting to imagine terrorist detection algorithms and unmanned action capabilities used to detect and arrest probable dealers or bad drivers.\n\nEventually, seeing the Boston Dynamics bot taking its revenge by knocking at your door and asking you to pay your speed limit fine might not be as zany as that\u2026"
    },
    {
        "url": "https://chronicles.mfglabs.com/facebook-s-empire-rankbrain-spotify-discover-weekly-and-more-in-the-programmable-edition-45-7e3c803e94bd",
        "title": "Facebook\u2019s Empire, RankBrain, Spotify Discover Weekly and more in the Programmable Edition #45",
        "text": "Last Sunday was Valentine\u2019s day\u2026 and our Valentines are the 6 women behind the ENIAC, the world\u2019s first digital computer, unveiled to the world 70 years ago.\n\nNowadays, we live in a world where the ability to learn computer programming is available to the masses. But few recall the courage it took this group of 6 women to become the first computer programmers. Not only their role was minimized at that time, but also they were only seen as the \u2018operators\u2019 of the machine, solely helping men figure out how to use the computer.\n\nToday, thoughts about women being less skilled than men at coding is still present, even though a recent automated study tackling gender bias on Github showed the opposite.\n\nBringing the success story of these young women to light reminds us how important it is to consider obstacles as challenges and go beyond the state of the art knowledge to repel the frontier of innovation."
    },
    {
        "url": "https://chronicles.mfglabs.com/mixed-reality-super-bowl-ad-blockers-and-more-in-the-programmable-edition-44-4b9963999ed2",
        "title": "Mixed Reality, Super Bowl, Ad blockers and more in the Programmable Edition #44",
        "text": "In a previous Programmable Edition, right after the launch of Twitter Moments, we\u2019ve argued Twitter should go a step further both on the user experience side with more personalization, and on the business side with their own AdWords model.\n\nAnd in terms of user experience, Twitter is making an interesting move by ranking your timeline\u2019s tweets. Ultimately, you will never miss important tweets from the people you follow.\n\nAs of today, Facebook has its very own EdgeRank and Google its very own PageRank. Twitter obviously needs one filter of its own in the information overload era. At MFG Labs, we designed, more than 5 years ago the SpreadRank, a visualized algorithm. Here\u2019s an old presentation about it. Sorry you don\u2019t have the voice-over, just shiny slides ;).\n\nI don\u2019t know if Twitter will succeed, but this is definitely a necessary step towards a more sustainable and AdWords-like model for them. Google PageRank is surely not a good approach to information propagation, and some kind of fatigue may gain Facebook users \u2014 or others."
    },
    {
        "url": "https://chronicles.mfglabs.com/twitter-s-death-autonomous-car-testing-center-emojis-ftw-and-more-in-the-programmable-edition-43-2f8cb10959b7",
        "title": "Twitter\u2019s death, autonomous car testing center, emojis FTW and more in the Programmable Edition #43",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/job-automation-cyber-ego-goldman-sachs-leaks-and-more-in-the-programmable-edition-42-2084f3eae040",
        "title": "Jobs automation, cyber-ego, Goldman Sachs leaks and more in the Programmable Edition #42",
        "text": "\u201cWe are being afflicted with a new disease (\u2026), technological unemployment. This means unemployment due to our discovery of means of economising the use of labour outrunning the pace at which we can find new uses for labour\u201d, wrote John Maynard Keynes in the 1930s.\n\nMachines do replace humans. Acknowledged. Interim findings from an ongoing research at Mc Kinsey suggest that 45% of the \u201cactivities\u201d (and 5% of jobs) individuals are paid to perform can actually be automated by adapting current demonstrated technologies. At the same time, we are seeing more and more (welcomed) articles on how automation paradoxically can create jobs as well. Let\u2019s say it transforms the workplace and market. I think the most important word in Keynes\u2019 quote is \u2018pace\u2019. From the 1930s it seems we haven\u2019t fallen into complete chaos. This brings the recurring but always fascinating question: is this time different? Will the pace of (intelligent) automation get so much higher than the pace of the transformation of our jobs? This would indeed lead to the very painful economic period pessimists point at.\n\nBut if we can\u2019t (humbly) renounce to progress or to competitiveness, we can surely accelerate the pace of reinventing ourselves and how we work. Time to be creative!"
    },
    {
        "url": "https://chronicles.mfglabs.com/pedestrian-crowds-as-large-games-6ebaaa9c966",
        "title": "Pedestrian crowds as large games \u2013",
        "text": "This article has been previously published in our blog the 28th of November, 2012.\n\nIn 2006, Lasry and Lions started to work on Mean Field Games. One of the main ideas they had in mind was the modeling of individuals\u2019 behavior in crowded places, e.g. Times Square in New York City.\n\nMean Field Games allow to model a large number of pedestrians which have strategical interactions and are able to anticipate what other people do.\n\nIn a crowd, pedestrians can be seen as competing players who try to find the best strategy to achieve their goal (e.g. reach an exit), given what the others do. The classical game theory is the theoretical framework to be used for such a situation. Nevertheless, the usual equations describing the strategy for the pedestrians (the equilibrium) result to be very hard to solve as the pedestrians become numerous (which by definition always happens in a crowd).\n\nIn other words, if I am a pedestrian in the crowd, I\u2019ll try to optimize my path as follows: \u00ab if I anticipate that many people in the crowd will have to pass by the center of the place, then I\u2019d probably better use another path \u00bb. This is clearly simpler than considering the following: \u00ab if this guy with a red hat and this other guy with pink shoes and this dark hair girl and so on \u2026 will pass by the center, then I\u2019d better use another path \u00bb. There is evidence that considering the global dynamic of the crowd (or its approximation) is more truthful and simplifies the decision process.\n\nAn equilibrium in the mean field game takes place when the anticipated crowd distribution (that pedestrians use to find their optimal path) coincides with the distribution resulting from the optimal paths chosen by the pedestrians when they take this distribution as a forecast. This loop in the decision process is the statistical equivalent of the similar loop in the Nash equilibrium for game with n players.\n\nThe movie above shows an example of pedestrian crowds modeled as a mean field game. We see two groups of many people that are initially on the left hand side corners of a square. The density of pedestrians is colored from blue to yellow and red as it increases. Each group would like to move to the opposite corner, thus having to cross the other group. We can observe that, after spreading all over, one group passes first thru the center of the square and reaches its destination quickly. The other group waits until the road is free again. Here two symmetric equilibria can occur depending on the shared belief of all pedestrians about which group goes thru first. As in classic game theory when there are several Nash equilibria, to answer the question about which equilibrium will appear in real world, one needs more data. Also, if there are too many equilibria it is expected that unless some exogenous coordination is implemented, the crowd is likely to behave chaotically.\n\nOne can for instance try to figure out how all this can be generalized to social network interactions.\n\nTo be continued\u2026 that\u2019s our job."
    },
    {
        "url": "https://chronicles.mfglabs.com/big-bets-snapchat-apple-fail-and-more-in-the-programmable-edition-41-d334ad2c1cc3",
        "title": "Uber\u2019s big bet, Snapchat, Apple fail and more in the Programmable Edition #41",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/supercars-network-effects-electricity-and-more-in-the-programmable-edition-40-15ca4909ab42",
        "title": "Electric supercars, network effects, electricity $$$ and more in The Programmable Edition #40",
        "text": "The year Kodak is trying to get 8mm cameras back, and so business.\n\nThe year designing chatbot personalities is an essential LinkedIn skill for designers.\n\nThe year your segway is also your own personal robot.\n\nThe year an international beauty contest will be judged by an AI.\n\nThe year the first meme was deep learning-ish.\n\nHappy New Year from all of us!"
    },
    {
        "url": "https://chronicles.mfglabs.com/unstoppable-ai-guitar-hero-ids-electricity-ftw-and-more-in-the-programmable-edition-39-909d608ec3a6",
        "title": "Unstoppable AI, Guitar Hero IDs, electricity FTW and more in the Programmable Edition #39",
        "text": "2015 has been a big year for Artificial Intelligence, especially in terms of democratization. First, we\u2019ve seen in a surge in public awareness. AI has made the headlines, mainly on the dark side of its development: technological unemployment, autonomous weapons, or the \u201cexistential threat\u201d we would face, according to Elon Musk. Second, it has been democratized by the booming of companies, projects and potential applications in our every day life: in healthcare, transport, education, to fight cybercrime\u2026 even in art. And finally, it\u2019s been pragmatically speaking democratized, with a wave of open releases of most advanced technologies by major players in the field, to foster innovation and eventually \u201cprotect all of us\u201d. Even your two-year old baby now has access to it right under the Christmas tree, with a new generation of toys that play and learn both with and like your child. No doubt 2016 will confirm this trend \u2014 and see further development of a captivating ethical debate.\n\nPS: the Programmable Edition will be back in January 2016 for new programmable adventures."
    },
    {
        "url": "https://chronicles.mfglabs.com/messaging-going-big-ballmer-inspiration-adele-s-power-and-more-in-the-programmable-edition-38-9cf4c79863a1",
        "title": "Messaging going big, Ballmer inspiration, Adele\u2019s power and more in The Programmable Edition #38",
        "text": "More than 5 years ago, I wrote about how the Information Era we were living in was evolving by looking at the platforms trying to be between you and your usage of the Internet.\n\nThe first wave was about linking content. Search giants that emerged here are Google and Yahoo. They helped us find content on the web.\n\nThe second wave was about linking people. Social media giants Facebook and Twitter experienced tremendous growth, allowing us to connect through content with the people we know (or not).\n\nWhat about the third wave? Linking content and people is great, but what about services? Like transport, personal finance, travel, etc. How to link us up with those services we use everyday in a streamlined and simple way? It\u2019s quite obvious Messaging services are at the forefront of this shift. And this exploding consumer usage is quickly embracing AI \u2014 machine learning \u2014 call it like you want, to better serve our desires.\n\nSearch, Social media and Messaging are different layers, but they all run for one simple goal: being your first door to the Internet.\n\nBecause when you are the first door, everyone is knocking at you. To sponsor their business, to sell their services, to distribute their product: you are the platform that rules them all.\n\nMessaging > Social Media > Search in so many ways. This is by far the best platform to leverage consumer intimacy. This is a race for who will be over over-the-top, who will be better to connect every aspect of our life. This is a race for power.\n\nMaybe it\u2019s time for brands to seize the opportunity of the Messaging wave, and avoid being disrupted again by a new Internet giant."
    },
    {
        "url": "https://chronicles.mfglabs.com/data-vs-pokemons-ad-bubble-cop21-and-more-in-the-programmable-edition-37-b78e0eaf7a0b",
        "title": "Data vs. Pokemons, ad bubble, COP21 and more in the Programmable Edition #37",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/justin-bieber-problem-the-future-of-fb-ml-x-ip-and-more-in-the-programmable-edition-36-b732576dd3c1",
        "title": "Justin Bieber Problem, the Future of FB, ML x IP and more in the Programmable Edition #36",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/small-data-mysterious-car-companies-terminator-vision-and-more-in-the-programmable-edition-35-f30fbbcca50a",
        "title": "Small data, mysterious car companies, Terminator vision and more in the Programmable Edition #35",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/automatic-society-trolley-dilemma-epic-slide-decks-and-more-in-the-programmable-edition-34-6ab47bf21c95",
        "title": "Automatic society, trolley dilemma, epic slide decks and more in the Programmable Edition #34",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/paying-youtube-100-personal-data-sustainable-ad-models-and-more-in-the-programmable-edition-33-a1e4be9eba8f",
        "title": "Paying Youtube, 100$ personal data, sustainable ad models and more in The Programmable Edition #33",
        "text": "Big news this week is YouTube Red. No ads, exclusive content, hookup to Google Play Music, 9.99$/month. The formula is no surprise; but will people pay so much money for \u2014 mainly \u2014 cat videos and Gangnam style parodies? We are not convinced, at least without real original Amazon or Netflix-like content (btw, weird way to recruit exclusive creators\u2026).\n\nBut if the future is a subscription-based model encapsulating all media types, advertising-free, YouTube Red might be a step ahead of its competitors, who also try on their side, to expand to other media. Although this strategy is quite orthogonal to Google\u2019s ad-based model, YouTube execs seem quite confident both models (subscription and ad-funded) can co-exist without cannibalizing each other."
    },
    {
        "url": "https://chronicles.mfglabs.com/big-data-small-data-and-the-role-of-logic-in-machine-learning-c5f9796765e9",
        "title": "Big data, small data, and the role of logic in machine learning",
        "text": "It would be easy to believe that all the interesting machine learning problems involve big data. However, a remarkable aspect of human intelligence is our ability to learn from a single example. For instance, consider a spreadsheet in which the first column contains email addresses of the following form:\n\nNow suppose you want to learn how to populate a second column using the following training example of the form (input,output):\n\nIt is clear that to populate the second column we copy everything in the first column up to the @ symbol, uppercase the initial letter and the letter after the . symbol, and finally remove the . symbol. We learned this solution from a single example. However, most standard machine learning algorithms, such as artificial neural networks and support vector machines, cannot learn from few (~1) training examples, and typically require many (>10k). By contrast, inductive logic programming (ILP), a form of relational machine learning based on formal logic, can [1] learn from a single training example.\n\nWhat is inductive logic programming? What is logic programming?\n\nILP is a form of supervised machine learning based on logic programming. Logic programming is a programming paradigm based on formal logic, typically Horn logic. A logic program is a set of logical formula expressing facts and rules. In contrast to other programming paradigms, such as functional programming, statements in a logic program have no return value and are evaluated to determine their truth or falsity. The field of logic programming is too big to explain fully, but we can illustrate the paradigm with an example using Prolog, the most popular logic programming language, developed by Alain Colmerauer's group in Marseille, France, in the early 70s.\n\nConsider the following Prolog program describing a subset of the Simpsons' kinship relations.\n\nThe first three statements in this program are Prolog facts, expressing things known to be true. The symbols mona, marge, bart, and lisa are constant symbols. The symbols mother, parent, and grandmother are predicate symbols. The fourth and fifth lines are rules where the symbols X, Y, and Z are variables. In first-order logic, variables can be substituted for by constant symbols. In higher-order logic, described later, variables can also be substituted for by predicate symbols. In the Simpsons example, the first rule says that X is the parent of Y if X is the mother of Y. The second rule says that X is the grandparent of Y if X is the parent of some Z and that Z is the parent of Y.\n\nWe can compile this Prolog program and query it to determine the truth or falsity of statements. For instance, we can ask whether marge is the parent of bart:\n\nOr ask whether bart is the parent of marge:\n\nOr ask who is the grandmother of bart:\n\nProlog, and logic programming in general, is based on logical deduction. Logical deduction is the process of reasoning from one or more statements (premises) to reach a logically certain conclusion.\n\nThe typical example of logical deduction is reasoning about the mortality of Socrates, displayed below.\n\nIn this example, we are given two premises (above the line), from which we derive a conclusion (below the line) We can derive this conclusion using a rule of inference named Modus ponens, which is stated as follows:\n\nIn the last Simpsons example, logical deduction is used to infer that mona is the grandmother of bart (i.e. to infer that grandmother(mona,bart) is true).\n\nProlog performs all logical deductions using a single powerful rule of inference named the resolution principle, discovered by John Robinson, in 1965. Resolution is sound, which basically means that it never gives an incorrect answer. Resolution is also complete (well refutation complete), which means that it will always give an answer.\n\nThere is much more to logic programming that I have described here, and for more information I recommend two books. Logic, Programming and Prolog, by Ulf Nilsson and Jan Maluszynski, and John Lloyd's Foundations of Logic Programming. In addition, Bob Kowalski, one of the founders of logic programming, has written an excellent history article.\n\nILP is a form of supervised machine learning which uses logic programming as a uniform representation for background knowledge, examples, and induced theories. This basically means that the inputs and outputs to an ILP systems are logic programs, in contrast to most other forms of machine learning, where the inputs and outputs are typically vectors of real numbers.\n\nBy using logic programming as a uniform representation, ILP has three main advantageous over most textbook machine learning approaches: (1) expressibility, (2) the ability to include background knowledge in learning, and (3) human readable theories.\n\nWe can explain these three advantages through demonstrations. Consider learning the concept of the kinship grandparent relation given the following four positive training examples, denoted as E+:\n\nTo learn this concept, ILP systems can include additional information, known as background knowledge, denoted as B, in the learning task.\n\nGiven the training examples E formed of positive examples E+ and negative examples E- and background knowledge B, the goal of an ILP system is to find (induce) a hypothesis H (a logic program) that explains all of the positive examples and none of the negative examples. In logical terms, we want to find a hypothesis H such that H,B entails E+ and H,B does not entail E-.\n\nNote that we induce a hypothesis, rather than deduce a hypothesis Induction and deduction are two of the three fundamental forms of reasoning characterised by Peirce, the third being abduction.\n\nWe have already mentioned deduction, which can be see as reasoning from the general to the specific and is sound, and it the fundemental form of reasoning in Prolog. Induction, by contrast, reasons from the specific to the general, and is unsound. Induction forms the basis of all of science, in which we try to find some explanation of a set of observations. Although an inductive hypothesis might seem correct, i.e. the theory of evolution, it can never be proven to be absolutely true, and there is always the potential to refute an inductive hypothesis. Indeed, refutability of a hypothesis is one of the main principles behind the scientific method. Abduction, the third form of reasoning characterised by Peirce, is a less well known and more difficult to explain. In abductive reasoning, we try to explain the cause of some observation, and, like induction, is unsound. Sherlock Holmes uses this form of reasoning, contrary to his claims of 'powers of deduction'.\n\nReturning to ILP and the Simpsons kinship example, given the examples and the background knowledge, we would expect an ILP learner to find a hypothesis similar to the following:\n\nThis hypothesis says that X is the grandparent of Y if X is the parent of some Z and Z is the parent of Y. And that X is the parent of Y if X is the mother of Y, or X is the father of Y.\n\nThis example demonstrates a big advantages of ILP in that induced hypotheses are human readable, which is not the case with propositional forms of machine learning. This human readability allows us to use ILP to gain insight into the problem. This readability is especially useful in the sciences, for instance in predicting mutagenesis.\n\nNote that in the above example, where we learned the definition of grandparent, the predicate symbol parent was not in the background knowledge nor in the examples. The introduction of the parent predicate is known in ILP as predicate invention, and is considered an important and challenging problem, although there has been recent progress on this topic [2].\n\nHaving learned this hypothesis, we can use it to make discoveries. For instance, we can query the program to ask who for all the grandparents of maggie.\n\nOne of the most exciting applications of ILP is in program induction, where the goal is to induce a program from input/output examples only. For instance, Lin et al [1] demonstrated an ILP system which can learn string transformation functions from a single training example, similar to the problem given in the introduction. This work was particularily interesting in that it performed a procedure named dependent learning, in which solutions to simple problems were reused to solve more difficult problems, forming a hierarachy of learned logic programs.\n\nILP systems can also learn classical algorithms. For instance, given the following example of a target predicate f:\n\nAn ILP system will induce a program to sort the input to form the output. For instance, our recent IJCAI paper [3] (the grandaddy of AI conferences) demonstrated an ILP system which was able to learn quicksort given only 5 training examples, and, importantly, to prefer the quicksort hypothesis over the less efficient bubble sort hypothesis.\n\nILP is no different from any other form of machine learning in that it involes a search for a suitable hypothesis. There are many ways to perform this search. The approach which I work on is named meta-interpretive learning (MIL) [2], which is novel in its support for predicate invention and the learning of recursive programs (e.g. quicksort). MIL is based on an adapted Prolog meta-interpreter. To those unfamiliar, a meta-interpreter is an interpreter written in the same language as the language to be interpreted. A Prolog meta-interpreter is thus an interpreter for Prolog written in Prolog. The code below shows a 'vanilla' Prolog meta-interpreter.\n\nThis vanilla meta-interpreter basically takes a goal as input and returns true if and only if:\n\nIn logic programming terms, a vanilla Prolog meta-interpreter attempts to prove a goal by repeatedly fetching first-order clauses whose heads unify with a given goal.\n\nA MIL meta-interpreter is different in that it attempts to prove a set of goals by repeatedly fetching higher-order metarules whose heads unify with a given goal. A metarule is a rule which takes predicate symbols as arguments, in contrast to a first-order rule which takes constant symbols as arguments. A meta-substitution is the replacement of a variable in a metarule with a predicate symbol. In the process of attemping to prove a goal, a MIL learner saves the meta-substitutions in an abduction store, which can be reused in later proofs. Following the proof of a set of goals, a hypothesis is formed by applying the meta-substitutions onto their corresponding metarules.\n\nTo demonstrate this technique, suppose the background knowledge consists of the ground atom parent(alice,bob) and our goal is the ground atom child(bob,alice). To prove this goal, a MIL learner fetches the inverse metarule P(X,Y) \u2190 Q(Y,X) and applies the meta-substitution {P/child,Q/parent} to unify the head of the goal with the metarule. The ground atom inverse(child,parent), representing the meta-substitution, is saved in an abduction store, and the learner continues the proof by attempting to prove the body of the metarule. Once a proof is complete, the ground atom inverse(child,parent), saved in the abduction store, is projected onto the corresponding metarule to obtain the clause child(X,Y) \u2190 parent(Y,X). This is a lot to comprehend without knowledge of logic programming, but the key aspect is meta-learning, which, in this case, consists of meta-interpretation and higher-order metarules. Indeed, because the metarules are themselves logical formulae, they can also be reasoned about and learned [4], which is a form of meta-meta-learning. It is also worth noting that a Universal Turing Machine is basically a meta-interpreter, and meta-interpretation can be seen as one of the most fundamental concept in computer science [2].\n\nThis has been a whirlwind overview of logic programming, inductive logic programming, and meta-interpretive learning, and I hope that this post exposes these topics to a few people. I also hope that this post makes it clear that (1) we do not always need vast amounts of training data to learn complex problems, and (2) good old fashioned AI (i.e. symbolic artificial intelligence) is alive and well."
    },
    {
        "url": "https://chronicles.mfglabs.com/programmatic-logistics-meta-curation-crazy-face-reenactment-in-the-programmable-edition-32-edfd50307be7",
        "title": "Programmatic Logistics, meta-curation, crazy face reenactment in The Programmable Edition #32",
        "text": "The main question in logistics these days is no longer whether moving goods from A to B can be automated. It can be. We at MFG Labs do it at this right moment inside a new, specific business line we called \u201cProgrammatic Logistics\u201d. Soon, logistics automation will be empowered with layers of intelligence strong enough to optimize entire chains of supply within all industries, without a single human intervention.\n\nNo, the main question is what comes next. What will this new \u201cProgrammatic Logistics\u201d world be fueled with? Well, if we had to, we\u2019d bet on user experience. Whether you carry people, meals, containers, providing consistent and meaningful user experience to all involved actors will be the main factor of failure or success in an environment where optimization is no longer a matter of debate."
    },
    {
        "url": "https://chronicles.mfglabs.com/jack-dorsey-returning-home-funny-ai-iq-testing-musk-master-plan-and-more-in-the-programmable-45c9a66a1d10",
        "title": "Jack Dorsey returning home, funny AI IQ testing, Musk master plan and more in The Programmable\u2026",
        "text": "This week Jack is back at Twitter as CEO. Some are happy, including me (I don\u2019t know him personally but I know he\u2019s a cartography nerd, which means he\u2019s an objectively good person).\n\nAt the same time, Twitter is going to turn off the public API\n\nand updated its product (finally!) with Twitter Moments, its answer to the information overload problem.\n\nPeople are following more people and more brands, yet none of us have enough time to see everything. Just like Google and their beloved PageRank, we need a filter, a service that helps us navigate this information overload, like our dear SpreadRank.\n\nTwitter Moments is an interesting move, which need to go further both on the user experience side, with personalization (i.e. my own moments about topics I\u2019m interested in, not just globally sports, news, etc.), and on the business side with their own AdWords model.\n\nOn the other hand, Medium, the other information (long form) network, is getting a logo revamp and a brand new publishing API.\n\nIt would be great to see synergies between you guys Ev, Jack\u2026 just sayin\u2019."
    },
    {
        "url": "https://chronicles.mfglabs.com/ad-bots-ad-blockers-potato-shaped-internet-and-more-in-the-programmable-edition-30-b69b7cb5306e",
        "title": "Ad bots, ad blockers, potato-shaped internet and more in The Programmable Edition #30",
        "text": "The Advertising Week XII in NYC is finishing. It was all about the ad blocking threat this year. It\u2019s a good time to step back and think why advertising matters. Besides boosting the economic engine of the world, advertising enables us, and especially the one who can\u2019t afford Pay TV, SVOD, premium streaming services to have access to content, education, entertainment, information for free. It matters, especially for the next 2 Billions of emerging middle class in Africa, Latam and APAC.\n\nWe need to work harder everyday to make it better, more meaningful, less intrusive and more relevant for people. We have to dedicate more resources, time, creativity, to cure its diseases: viewability, bots, spam; and avoid the backlash of ad blocking and ad skipping.\n\nFurther reading on the topic:"
    },
    {
        "url": "https://chronicles.mfglabs.com/restricted-boltzmann-machines-intelligent-barbies-and-more-in-the-programmable-edition-29-2f270fd65f37",
        "title": "Restricted Boltzmann Machines, intelligent Barbies and more in the Programmable Edition #29",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/rbm-and-recommender-systems-3fa30f53d1dc",
        "title": "RBM and recommender systems \u2013",
        "text": "Literature about Deep Learning applied to recommender systems is not very abundant. At MFG, we\u2019ve been working on Salakhutdinov, Mnih and Hinton\u2019s article \u2018Restricted Boltzmann Machines for Collaborative Filtering\u2019 ([1]) and on its possible extension to deep networks such as Deep Belief Networks (DBN) ([2]). It has proven to be competitive with matrix factorization based recommendations. But how could we improve it in order to obviously outperform matrix factorization ? That\u2019s a great challenge that could be a breakthrough for our activity. ICML was the opportunity for us to catch work in progress in deep learning techniques from universities all around the world and from applications far from recommender systems. We were especially interested in a talk given about RBM and DBN application to genomic. Indeed, constraints that come from genomic representations could find their counterpart in Facebook data recommendation.\n\nIn the following, we just focus on RBM in order to see how to improve the unsupervised training. DBN is just the stacking of RBM pretraining and a fine-tuning that we\u2019re not discussing here.\n\nLet\u2019s first see how to apply RBM to recommender systems.\n\nOur data is a Facebook likes matrix L with N users in lines and M items in columns with coefficient (u,i) being 1 if user u likes item i, 0 otherwise. This matrix is obviously sparse. We pick out randomly n users and m items and then split this matrix in a (n,M) training set and a (N-n,M) test set. The submatrix of likes we wish to predict is (N-n,M-m).\n\nRBM are stochastic neural networks with two layers only :\n\n- a layer of I visible units v, which is both designed for input and output ;\n\nThe number of visible units is the dimension of examples : I = M. The two layers are fully interconnected, but there is no connection within each layer. Neurons have binary response.\n\nEach neuron is designed by its activation probability, which depends from the former layer in a sigmoid manner :\n\n- c the bias of hidden neurons.\n\nRBM are an energy-based model : we can link to each state of the network an energy E(v,h) defined by :\n\nThis energy allows us to define a joint probability :\n\nSo that we have the marginal :\n\nWe learn W, b and c by applying gradient descent to log-likelihood maximization. For instance, we learn the network\u2019s weights by :\n\n- The first term, called positive, is easily computed with the empirical visible data and the hidden layer directly resulting from them.\n\n- The second term, called negative, can\u2019t be computed analytically. That\u2019s the key point when studying RBM. We approximate the negative term using a method called Contrastive Divergence. This method lies on Gibbs sampling to evaluate the negative term. For k Gibbs steps, we follow the following picking process :\n\nFinally, after a few calculations, we get :\n\nRecall that within the test set not all likes are known and that we we wish to predict unknown likes based on known ones. After having trained our network on all items, we predict iteratively for each user the probability of liking the next item. In other words, based on the m known likes, we predict the visible unit m+1. Then we consider this visible unit as a known like and, based on these m+1 known likes, we predict the visible unit m+2. And so on.\n\nSo we just have to compute the probability of picking a visible unit m+1 equal to 1 given the former m visible units :\n\nSo we have a method to predict likes based on RBM. Can we improve it using the binary nature of data and their sparsity ?\n\nIn their paper \u2018Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions\u2019 ([3]), Taehoon Lee and Sungroh Yoon design a new way of performing contrastive divergence in order to fit to binary sparse data.\n\nThe goal of the paper is to identify some DNA fragments. Recall that DNA is a sequence of four types of nucleotides : Adenine (A), Cytosine (C), Guanine (G) and Thymine (T). In order to give DNA sequence to a RBM as input, they use orthogonal encoding : more precisely, each nucleotide is encoded on 4 bits. A, C, G and T are encoded by 1000, 0100, 0010 and 0001. That\u2019s why their data are binary, but also why they are sparse : for example, the simple AGTT sequence is encoded by the 16-dimensional vector 1000001000010001. So they wish to incorporate this prior knowledge on sparsity.\n\nThey convert a DNA sequence of m nucleotides into a binary vector of 4m elements v that is given in input of the RBM. In the computation of the CD, v(0) and v(k) are the original input and its reconstruction using the RBM. Their idea is that the trained RBM should be able to reconstruct precisely the original input. So they design a constraint that fit their specific original input : they add a regularization term that penalizes the deviation of the sum of 4 visible units from 1. They call this term categorical gradient. The minimization problem thus becomes :\n\nWe can deduce from this problem new update rules for the network parameters.\n\nCould this innovation be applied to recommender systems ? The easiest way would be to penalize the deviation of the total sum of the reconstruted input from the original one, that is to say, to penalize the user\u2019s reconstructed number of likes from his actual one :\n\nBut it should be possible to go further. We could for instance design macro-items, that is to say cluster of items, and, for each user, represent his relation to a macro-item by the array of his likes on this macro-items. Then we would be able to penalize the deviation of each reconstruted macro-like to the actual one. We let you imagine the formula.\n\nThese are ways to explore a generalization of categorical gradient to recommender systems.\n\nWe would like to conclude assessing that, owing to its multiple applications, research in machine learning should always be multidisciplinary. A method used for classification (RBM) may be useful for recommender systems, but also for genomic. And the discoveries made in genomic could in return be of great help for recommender systems. That\u2019s why it is important for us, MFG Labs, to be backing such events as ICML to get the newest ideas and try to enrich our toolbox of machine learning methods. Deep learning is amongst them and deep learning is ever increasing. So let\u2019s keep on learning deep !\n\n1 SALAKHUTDINOV, Ruslan, MNIH, Andriy, et HINTON, Geoffrey. Restricted Boltzmann machines for collaborative filtering. In : Proceedings of the 24th international conference on Machine learning. ACM, 2007. p. 791\u2013798.\n\n2 SALAKHUTDINOV, Ruslan et HINTON, Geoffrey E. Deep boltzmann machines. In : International Conference on Artificial Intelligence and Statistics. 2009. p. 448\u2013455.\n\n3 LEE, Taehoon, KR, A. C., et YOON, Sungroh. Boosted Categorical Restricted Boltzmann Machine for Computational Prediction of Splice Junctions.\n\nThanks to Alain Soltani for his contribution to this work."
    },
    {
        "url": "https://chronicles.mfglabs.com/programmable-data-scientists-driverless-mercedes-fashion-robots-and-more-in-the-programmable-f633de9658d5",
        "title": "Programmable data scientists, driverless Mercedes, fashion robots and more in The Programmable\u2026",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/learning-to-learn-or-the-advent-of-augmented-data-scientists-20873282e181",
        "title": "Learning to learn, or the advent of augmented data scientists",
        "text": "The job of a data scientist involves finding patterns in data, often in order to automate or augment human decision-making. Is it possible to find patterns in the way data scientists work in order to automate their own job ?\n\nThe concept of automatic machine learning (autoML) is compelling, and we at MFG Labs pay close attention to the development of this field, because the way we work and design our processes might be disrupted by theoretical or practical breakthroughs in this area. Furthermore it incites us to step back, deconstruct our typical workflow, and then question each part of it. At ICML (International Conference of Machine Learning), we had the chance to hear the take of the most brilliant minds on this subject. In this article we will present a brief outlook on how algorithms might replace us data scientists, or most likely assist us in doing our job better. Our intent is by no means to comprehensively survey the field of automatic machine learning, but rather to showcase a couple of specific topics that resonated particularly with our current interests.\n\nThe typical data scientist workflow, when you consider it from defining the problem at hand to debugging a live production system is, in our experience, very intricate and certainly not linear. Thankfully, this process can easily be broken down into distinct parts. Rich Caruana (Microsoft Research) formulates the following pipeline, which feels very familiar for us:\n\nAgain the workflow is usually not followed linearly: for example data quality problems are often brought to light during feature engineering or model tuning, which implies going back to the data collection & cleaning process, improve it, go back to whatever you were doing before, discover another flaw in the data or in the algorithm choice, rinse, repeat.\n\nSome of the necessary steps outlined above look way out of the reach of automation right now: problem definition, data collection, metric selection, deployment, debug\u2026 Those tasks involve a kind of general intelligence only found in humans so far. We\u2019re still far from having machines define a problem relevant for us, nor are we even close to have them industrialize an algorithm from a sandbox-like environment to a live production system.\n\nAutoML focuses only on those tasks that are both time-consuming and automatable. One with which every data scientist should be all too familiar with is data cleaning. In the real world data sets are messy, data specifications are not followed or do not exist, values are missing or mistyped. Currently the way data scientists deal with these issues is very manual. It involves first understanding the dataset and the signification of the features, drawing univariate plots of the data and looking for anomalies. It is however virtually impossible to catch them all in this way. Often data quality issues are still found late in development, for example when debugging the predictions of an algorithm. Surprisingly, not much attention is given by research to tackle this issue.\n\nWe feel though that data cleaning is the kind of task that is definitely automatable because it feels so repetitive. Tools that systematically detect typical anomalies in a dataset and suggest relevant cleaning decisions to the data scientist could be huge time savers. Also going further than looking at univariate distributions in the dataset would help in catching more subtle errors. So far the available tools to perform outlier detection, either assume a given joint distribution (like fitting an elliptic envelope), which is bound to fail on high dimensional complex datasets, or are actually novelty-detection algorithms (like one-class support vector machine), which tends to overfit to the outliers during training. To overcome these issues, research to advance the state of the art in high dimension density estimation (which is a very challenging research area by itself) in the practical context of data cleaning will yield significant benefits for data scientists.\n\nAnother task that data scientists either spend too much time on tinkering manually or completely ignore is the tuning of algorithm parameters (also called hyperparameters). For example when you run a k-NN (k nearest neighbors) classification algorithm, the hyperparameter is k, the number of neighbors considered for classifying a new data point. In this case hyperparameter optimization is straightforward since there is only a single parameter to optimize: it consists of running a cross-validation on many values of k and choosing the value maximizing the generalization performance. This method works fine when there is a single hyperparameter and when model fitting is cheap. When there are several hyperparameters, then running an exhaustive grid search is exponentially expensive with the number of hyperparameters, which is impractical when you work with reasonably-sized datasets.\n\nFor example if you work with a more sophisticated model than k-NN, let\u2019s say random forests, you are dealing with about 6\u20138 hyperparameters, which can be categorical (Gini criterion or entropy? ) or numeric (proportion of features to draw for each tree, maximum depth of the trees). Depending on the way you discretise the hyperparameter space, the number of combinations can easily go up to several hundreds of millions possible choices. This kind of state space is absolutely overwhelming for the mere humans constituting the majority of data scientists.\n\nHowever this choice is absolutely fundamental and should not be expedited too quickly. The algorithm performance varies widely with hyperparameters choices, and it is quite unlikely that you will stumble upon good ones by chance, or that the software default ones will be the best, or even reasonable. Thus most data scientists in industry either use sub-optimal models, for example by using the default parameters suggested by the specific implementation they run, or by optimizing each parameter independently, thus reducing drastically the hyperparameter space but ignoring parameter interactions. Incidentally, comparing algorithms does not make much sense when they are not both correctly tuned, since you tend to negatively bias and discard algorithms whose hyperparameters are difficult to optimize.\n\nTraditionally, hyperparameter tuning was seen as a kind of \u201cblack art\u201d requiring time, experience and often unwritten rules of thumb. Automating this task would improve data scientists\u2019 models and free up time for them to focus on other less automatable tasks. Fortunately, recent research applied a technique from the 70s to this task and actually automates it completely, even performing it better than human experts : Bayesian optimization.\n\nThe algorithm consists in modeling the generalization performance as a smooth function of the hyperparameters, and aims at finding the maxima of this function in as few steps as possible. To do that, it chooses to evaluate the function (which involves running the full machine learning algorithm) on a set of hyperparameters on which the result is the most uncertain (exploration) and that is the most likely to have higher values (exploitation). The uncertainty factor implies modeling the generalization performance as a stochastic process (in practice a Gaussian process is a good choice because of the available closed form formulas for marginal and conditional probabilities), choosing a reasonable prior and updating the posterior via Bayes\u2019 formula each time a new set of hyperparameters is evaluated. This process is typically repeated iteratively until a convergence criterion is met.\n\nThe reason Bayesian optimization vastly outperforms classic grid search for a given computation time is because it carefully chooses the next set of hyperparameters to evaluate at the next iteration. Since the bottleneck in the context of hyperparameter optimization is the function evaluation time (which consists in running a full machine learning algorithm for each iteration), this algorithm saves a lot of computation time by avoiding evaluating hyperparameters that are unlikely to be optimal.\n\nIn practice this technique works very well, as has been shown in many papers and in data science competitions. It can be applied to any complex machine learning algorithm, since they all involve hyperparameters. Efficient distributed implementations like Spearmint or Hyperopt are available for python users. There is thus no reason anymore for a data scientist to spend more time on hyperparameter tuning than is necessary for setting up the Bayesian optimization and run it.\n\nThe automation of hyperparameters selection is a major step forward. What if we could go even further and not even have to choose the specific machine learning algorithm ? Mich\u00e8le Sebag presented at ICML a novel approach for dealing with this subject: ALORS (Algorithmic Recommender System). The main idea is to formulate the problem of algorithm selection as a recommender system problem, a theme we frequently encounter at MFG Labs: a set of users can evaluate or \u201clike\u201d products, and the objective is to predict the unknown ratings of all users for all products from existing ratings. Recommender systems are widely used today (think \u201cyou might also like\u2026\u201d after an Amazon purchase) and have been the subject of increasing academic research and experimentation since the Netflix Prize Competition in 2006.\n\nA convenient way to understand the task of collaborative filtering is to view it as a matrix completion problem. The matrix has users as its rows and products as its columns. The available ratings populate this matrix, which is usually very sparse: typically in practice you could have only about 0.001% of known values in the matrix. To solve this problem, the main idea is to assume the matrix you wish to complete has redundant information (i.e it is of low rank). That makes sense in the context of movie recommendation for example: a user liking one science fiction movie might like other science fiction movies. In this framework collaborative filtering becomes a linear algebra problem, and thus old linear algebra algorithms like Singular Value Decomposition work very well in practice.\n\nIn the context of algorithm selection, users are machine learning algorithms, and products are problem instances. The rating is the generalization performance of an algorithm for a problem instance (after hyperparameter optimization). Algorithm selection in this framework then consists in predicting, given a fraction of the values in the algorithm/problem instance matrix, which algorithm will perform best for each problem. The experimental results of this strategy are promising: on a portfolio of about 30 algorithms and 200 problem instances, the algorithm recommended by the recommender system consistently outperforms the best overall algorithm. The implications are profound. Sticking to a single algorithm regardless of the problem at hand is a bad strategy, even if you choose the best overall algorithm. You need to adapt the algorithm to the problem if you\u2019re targeting optimal performance. The good news is, there is no need to run all algorithms. Thanks to the recommender system, running a couple ones (2\u20133) is enough in many cases, and quite practical if you run them in parallel.\n\nThis kind of research definitely goes in the right direction towards automation of the machine learning workflow. By having an algorithm manipulate a portfolio of other algorithms, we are able to abstract away the algorithm selection phase. This kind of meta-algorithms could be the future of data science, a future in which there are fewer choices to be made once a problem and a performance metric have been defined, because machines could make better choices than us.\n\nAlgorithm selection methods require having a portfolio of algorithms to begin with. Those algorithms have each been designed by humans in order to solve a specific task. They are composed of logical instructions consuming data, and resulting in a mapping between input to output in the framework of a specific task. For example the gradient descent, a very common technique for calibrating weights in machine learning algorithms, implements very specific instructions to minimize a smooth loss function. But depending on the problem, other optimization algorithms can work significantly better, like Nonlinear Conjugate Gradient or L-BFGS. Those algorithms have also been designed by humans. Would it be that far-fetched to imagine that there are better learning algorithms out there, but that we haven\u2019t figured them out yet ? Can we design algorithms to find those algorithms ?\n\nThis problem has seen some research in the last couple of decades. The first approach is genetic programming. It consists in evolving a computer program with biology-inspired evolutions, like crossovers and mutations. A \u201cfitness\u201d of the program is defined, which is usually its ability to perform a given task. Only the fittest programs survive and breed, which leads to even better programs. As you can easily imagine, the bottleneck with this kind of approach is computing power, since you run many programs, many times. Up until the 90s, only very simple problems could be solved with genetic programming. But the exponential growth of CPU power and refinements in the algorithms recently lead to impressive achievements, for example in quantum computing or in soccer-playing (see this link for more examples).\n\nJ\u00fcrgen Schmidhuber, who presented some of these approaches at ICML, went even further with Meta-Genetic Programming: the meta-program modifying the main program can itself be subject to mutations, and these mutations can themselves be modified\u2026 in a potentially infinite recursive loop.\n\nAnother approach, also the subject of substantial research by J\u00fcrgen Schmidhuber and his lab, is even more abstract: fully self-referential learners. This kind of algorithm blurs the line between the main program and the meta-program modifying the main program. In this framework, the learner is conscious of its own code, is able to modify itself, and even improve the codebase responsible for the modifications. To be able to do that, it must have a way of proving that a change in the program will lead to better results. In the G\u00f6del machine, a theoretically optimal self-referential learner designed by Schmidhuber, the starter program basically consists in a set of axioms, describing its own hardware and software (including a theorem prover), and an utility function supplied by the user. Even the utility function could be modified, if the theorem prover manages to deduce that a rewrite would be beneficial, for example if simplifying the utility function would save computing power but still preserve the properties of the original utility. In the framework of self-referential algorithms, the learner can interact with data as well as with an environment, like in reinforcement learning. This algorithm has huge potential, since it is emulating one of the specific features of the human mind: consciousness.\n\nWhat about the feasibility of a G\u00f6del machine implementation? Recent work by Marcus Hutter (former Schmidhuber student) showed that with a specific initialization of the the G\u00f6del Machine, the complexity is O(n^3). However the constant hidden in this complexity is absolutely unmanageable for any reasonable problem. If we\u2019re looking for practical algorithms though, the closest we have to a self-referential learner is the very popular Recurrent Neural Network(RNN). It has been shown that it can behave as a general Turing machine, and the way it references its own weights make it self-referential and explains many of its successes (LSTM, a variation of RNNs, won many machine learning competitions). It is not as ideal as the G\u00f6del machine though, since it is limited in expressiveness by the structure of the network.\n\nMany of the techniques mentioned in this article are not that novel, but are not used widely because they are not well known outside of academic circles, or do not have an easy-to-use implementation. We expect though that in the next decade it will be as easy for a data scientist to apply all of these techniques to their problem instance as it is for them today to run a random forest algorithm. For that to happen, we should increase the communication between industry and the academic (see the inaugurating article of this series) and promote open-source development.\n\nWe are also excited about the development of general self-referential intelligence, which could change the world in so many awesome ways (and probably some appalling ones too)."
    },
    {
        "url": "https://chronicles.mfglabs.com/rolling-in-the-deep-learning-shoes-startups-martian-nuclear-blast-and-more-in-the-programmable-5d4984172f8a",
        "title": "Rolling in the Deep (Learning), shoes startups, Martian nuclear blast and more in The Programmable\u2026",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/rolling-in-the-deep-learning-4302bd5c06da",
        "title": "Rolling in the Deep (Learning) \u2013",
        "text": "Deep Learning has been getting a lot of press lately, and is one of the hottest buzz terms in Tech these days. Just check out one of the few recent headlines from Forbes, MIT Tech Review and you will surely see these words pop up at least once.\n\nBut what is this strange concept everybody is talking about ?\n\nIs it just a fleeting craze, that everybody will forget in a few years (or maybe months) ?\n\nWhat is all the hype about ?\n\nWe will try to answer these questions, and a few more, in the following post.\n\nLet us first give a quick definition of Deep Learning:\n\nSome of these terms may be unfamiliar to you. The aim is to give an overview of every one of these concepts, so that by the end you will have a pretty clear picture of what it is all about.\n\nMachine Learning (yet another buzz term) boils down to learning a mapping from an input space to an output space in an automated manner, using available data.\n\nIn the case of what we call supervised learning, the output space is a response that we are trying to predict by using examples of this mapping as training.\n\nExample: Learning whether a person is likely or not to default on their loan\n\nIn the case of what we call unsupervised learning, the output space is often a simpler representation of the input space, that is more structured, we are not given explicit examples of this mapping, so we need to learn it by exploiting the internal structure in the input data.\n\nExample: Clustering customers by their purchase habits\n\nThese examples seem simple to understand and reason about, and the representation of the input space is rather natural.\n\nBut a lot of other tasks fall in the Machine Learning framework, and some are quite complicated.\n\nIn these examples, finding a suitable representation of the input data is more complicated.\n\nWe can divide the Machine Learning process into, essentially, two steps:\n\nThe first step is usually the most time consuming, and the most task specific, that\u2019s where Deep Learning comes in.\n\nBefore we elaborate on this, let us first introduce the family of models on which most Deep Learning techniques are based: Artificial Neural Network.\n\nIn its most basic form, an artificial neuron is a simple computational unit that outputs a weighted sum of its inputs.\n\nIt is somewhat inspired by its biological cousin (the \u201creal\u201d neuron), but it doesn\u2019t replicate at all its inner workings.\n\nData in the form of (input, output) couples is used to adjust the weights of this neuron, to make the output as close as possible to the expected output. Let\u2019s illustrate this on a toy example:\n\nLet\u2019s say we wanted to classify a point in this 2D space, as being part of the red curve, or blue curve. If we use our simple neuron to solve this problem, the task comes down to finding the best separating line between the two curves.\n\nNot bad. The separating line does a pretty good job, but we can clearly see that a line will never separate the two curves perfectly, the problem is just a bit too complex. What if we add an extra layer to our simple network ? i.e.\n\nLet\u2019s try this and see what happens:\n\nBetter ! We can now separate our two curves perfectly. What is happening in the extra layer, that made this possible ?\n\nNo .. Not really \u2026 We are just bending and twisting our space, so that we can better separate our two curves. If we sneak a peak into what\u2019s happening, to see how the input space is transformed, we can see that our extra layer computes a new representation, that makes the two curves linearly separable:\n\nThis is one of the things Deep Learning is all about.\n\nTo better show how learning multiple levels of new, more useful representations looks like, we will make the (huge) jump to the world of human face detection. If we train a Deep Learning model on a dataset of face images, we can see that we naturally have a hierarchy of representations that is learned (object edges -> parts of faces -> whole faces )\n\nOk, so that\u2019s how a neural network works (basically).\n\nWe can argue that if we make our neural networks deeper, by adding more layers (that\u2019s where the Deep is Deep Learning comes from), we can learn more and more complex representations of our input, and we can potentially capture more complicated structures (as in images or text data).\n\nWait \u2026 That\u2019s what took everybody several decades to figure out ?\n\nWell not exactly. These ideas were here a while ago, but (until 2006) people just couldn\u2019t properly train these Deep Networks.\n\nThis Deep Learning renaissance is due to a mix of three things:\n\nThe reason everyone (or almost everyone) is so excited about Deep Learning, is that it achieved really promising results on a variety of tasks and in a relatively short period of time.\n\nDeep Learning models are now the state-of-the-art methods on this task.\n\nIf we take for instance the ImageNet Large Scale Visual Recognition Challenge (ILSVRC), which is a annual competition where the task is classifying several tens of thousands of images into one of 1000 different object categories, this competition was dominated by Deep Learning Models in the last few years.\n\nCheck out http://www.image-net.org/challenges/LSVRC/ for more info.\n\nFacebook\u2019s DeepFace model is closely approaching human level performance on the task of recognizing faces (the dataset used is http://vis-www.cs.umass.edu/lfw/ ), achieving an accuracy 97.25%.\n\nAnother really cool application. This model generates image captions automatically by learning correspondences between language and visual data.\n\nCheck out the results here: http://cs.stanford.edu/people/karpathy/deepimagesent/\n\nToday, each time you speak to your android phone, so you can send a text message, call someone, or do a google a search, there is a large Deep Learning model that is processing your voice and translating it into text.\n\nThe use of Deep Learning models helped reduce the error rate of this system dramatically.\n\nSomething that\u2019s been showing some promise in recent years, is the use of Deep Learning to better understand the human language.\n\nNeural Machine Translation is an interesting application of such models, where using just an aligned corpus on the sentence level (like the Euro Parliament corpus (http://www.statmt.org/europarl/ ), we can learn to translate new sentences, and even align words and groups of words across languages.\n\nThere\u2019s a lot more applications, and it\u2019s hard to give an exhaustive view of every one of them.\n\nOk, so all of this seems great, the results are promising, the models are powerful. But there must be some flaw in these models ? They are certainly not perfect in every way ? Are they ?\n\nWell, the answer is no, they are not perfect, and they do have some flaws, which brings us to our next section.\n\nWe argued earlier, that Deep Learning models have a lot of advantages:\n\nWhich make them the tool of choice for a lot of modern problems.\n\nBut there are some downsides to using these models\n\nResolving all of these problems is still an active area of research, and people are trying to make implementing and tuning these models easier. Some people will even argue that these flaws are inherent to a lot of Machine Learning methods and not only Deep Learning.\n\nThere are also some flaws which are even more severe, and have more profound consequences.\n\nYour model is as good as your data\n\nDeep Learning models learn representations that capture the structure of the data. So if we want to learn useful features, we need to make sure our data is large and diverse enough to represent the real world.\n\nOne particular example comes to mind. (http://googleresearch.blogspot.fr/2015/06/inceptionism-going-deeper-into-neural.html)\n\nIf we ask a Deep Learning model trained on a computer vision task to generate what it thinks most resembles a dumbbell, here\u2019s what we obtain:\n\nThe network thinks a dumbbell always has an arm attached to it, because it was the case in most of the training examples it saw. The model is still far from being \u201cintelligent\u201d, it failed to completely distill the essence of a dumbbell.\n\nSome researchers found that Deep Learning models are quite easily fooled by noise in the input data.\n\nThey took an image of an object that was recognizable by the network and added just a little bit of noise, so little that it was basically identical to the human eye. The network completely mislabelled these examples.\n\nThe reverse was also done, taking images that were, to the human eye, just random pixels on a canvas, yet the network labelled them as objects with near certainty.\n\nSome examples of this:\n\nIt makes us question whether these networks are really robust, and understand the essence of what an object is.\n\nMore details and examples of this can be found here\n\nYou probably noticed that most of these examples are related to Computer Vision, not because it\u2019s the only application domain or the most important (which is not), but because we can \u201csee\u201d what is happening under the hood, and that\u2019s not the case with every task.\n\nThis is a problem with these kinds of model, they are more like black boxes, and it\u2019s kind of hard to see what\u2019s exactly happening inside. Sure, it\u2019s ok if all we care about is accuracy and performance, but that\u2019s rarely the case, sometimes we do need to open the box.\n\nWe tried to give a brief overview of Deep Learning, so you can see and understand what is happening beyond the hype.\n\nThree key points to remember are:\n\nYou should follow us on Twitter: @mfg_labs"
    },
    {
        "url": "https://chronicles.mfglabs.com/icml-article-series-and-more-in-the-programmable-edition-26-42d14808e438",
        "title": "ICML Article Series and more in The Programmable Edition #26",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/why-you-should-care-about-icml-d6908cd17fd7",
        "title": "Why you should care about ICML \u2013",
        "text": "ICML is one of the top academic conferences on machine learning and, as so, one of the biggest events on the subject. The MFG Labs data team was part of the show as gold sponsor of the conference. It was a great opportunity for us to meet fellow data geeks, discuss with academics and get a feeling of what\u2019s coming next in the field.\n\nBut first, you may wonder what ICML is and why you should care about it. To answer this question, we will first look at what is an academic conference and what\u2019s at stake for academics in such an event. Then we\u2019ll see how these events allow companies and academics to meet and form tight relationships. Finally, we\u2019ll discuss briefly one hot topic of this conference, a consequence of the tight relationships between companies and academia: the fear of a new AI winter.\n\nSay you are a scientist, working with your team, in your lab, on a cool research project. After some time, a few (actually a hell lot of) trials and errors, you happen to find a solution to the problem you were trying to solve. This solution is often a proof of some theorem or the experimental confirmation of a phenomenon. Now, you want to share it with the research community so it can be validated (peer reviews) and other scientists can build upon it. This is probably the most essential rule of academia : any piece of scientific work must be peer validated. Now, with the growing number of scientists in the world, peer reviewing all contributions (outcome of a successful research project) is a challenge. That\u2019s the first role of journals and conferences. The second one is to facilitate open research: it is a way for scientists to gather all contributions on a same topic, so they can keep up with the growing number of contribution made in the world. These two missions are absolutely essential, and rest essentially on academic journals and conferences. So how does it work ?\n\nOnce a research project is successful, the next step is to write a paper that contains the context and the contribution. The context explains why was this project studied, and why it matters. The contribution explains precisely the work produced during the project, so it can be understood and reproduced. Once this paper is written, it is submitted to a conference and/or a journal, so it can be shared, reviewed and validated by other scientists.\n\nWe\u2019re talking about ICML so let\u2019s focus on conferences.\n\nEvery conference is held within particular deadlines. It always starts by a call for paper (CFP) that details the acceptable domains and contributions for the conference. For instance, ICML is a big machine learning conference so the CFP is focused on machine learning. WWW is a conference on web data and is more focused on the web rather than on machine learning. RecSys is a conference on recommender system and is focused on, well, you guessed it, recommender systems. The CFP also defines the submission deadline. Here is for exemple the ICML 2015 CFP. Scientists have to submit their papers before this submission deadline. Once the paper is submitted, then the reviewing begins. Other scientists, from the same domain are randomly selected to review the paper. This review can be done with all names known (reviewers and authors), blind (the name of the authors is visible, reviewer are anonymous) or double blind (authors and reviewer are anonymous). Reviewers\u2019 job is to grade the paper and explain the grade. Papers are then ranked based on the grades and only the best papers are kept as part of the conference program. They are the accepted papers of the conference, they are the papers that will be presented during the talks. Here is the list of accepted papers of ICML 2015.\n\nNow that the conference has a list of accepted papers, the only thing left is to attend it. Everyone can attend the conference (there\u2019s often a fee) and listen to the talks, meet scientists, or if you\u2019re really lucky talk to MFG Labs data scientists. This is why conferences are so vital for the academic world : they provide a solution to the systematic review of scientific contributions, provide an event where to meet fellow scientists and exchange ideas and all accepted papers are gathered together in proceedings so they can easily be accessed by other scientists.\n\nWe explained why these conferences were so essential, and this is why so many scientists attend them. This makes them one of the best places for companies and academics to interact with each others and benefit from each other. The first benefit is that companies can sponsor conferences. This is vital to making the organisation of such conferences even possible. Sponsors of ICML paid a fee to have a booth so they helped the conference and had a nice spot in the main conference hall where they could greet fellow scientists and interact with them. We are proud sponsors of ICML and it is very important for us to be involved in academic research. In machine learning, this close relationship between companies and academics is quite important, and here is why.\n\nThey are roughly two types of contributions to machine learning. Theoretical advances that focus on proofs of theorems like generalization bounds or convergence speed of algorithms, and experimental advances that apply known algorithms to new data and new contexts. Both benefit from each other : the former gives insights on what is to expect when pushing an algorithm to new data, the latter generates theoretical questions that have to be answered. Basically companies help with the latter : they have new datasets and new challenges for the scientists. One perfect illustration is the Netflix Prize: the company provided a dataset (movie reviews), a task (recommend movies) and a financial support for the winner. This challenge drove research on recommender systems further. Netflix benefited from the results as they could integrate the advances made by scientists in their own recommender system. Another benefit of such interactions is the complementarity of scientists from the academic and private sector : companies often hire scientists to work on projects they cannot or do not want to outsource to a company. It provides an opportunity for scientists to work on different, real data and for companies to involve experts in the field.\n\nNow, that\u2019s for the brighter side of this interaction. Sadly enough, there is a darker side that was often discussed at ICML this year. They are two main impacts of the close relationship of academia and private sector. The first one is that, in some domains (like deep neural networks) more and more scientists join private sector companies. As so, reviewers of such domains are likely to belong to private companies. This raises a neutrality issue on the reviewing process for such domains: are these employees biased towards the results of their own company ? The other impact is the so called AI Winter. AI winter is a reference to the 70s where fundings in AI research were really low. This was a consequence of the over confident predictions about the possibilities of AI in the 60s. In those days, the first neural networks (actually only one neuron at the time !) was introduced, as well as many rule learning engines. Companies and scientists advertised there results with great confidence, planning to replace human intelligence in many domains\u2026 but they failed to deliver. As a results, investments on AI dropped drastically. Might history repeat itself (again) ? Today, many companies base their business plans on overconfident AI\u2026 We certainly made a huge leap forward since the 60s (both in hardware, software and theory) but still. The wound inflicted by the first AI winter is still fresh for many and the hype around machine learning is seen with both excitation and fear.\n\nSure, a second AI winter is a risk. How do we avoid it ? Our belief is that it can be avoided by focusing on the benefits of a close relationship between academia and companies while keeping a cool head about what can be done. There are many tasks that can be automated, yet we are still far from reproducing human intelligence. The programmable world is about providing the right information at the right time. This raises many difficult challenges around data analysis and processing and often generates new theoretical questions that drives research further. Academia provides strong knowledge on the behavior of machine learning and helps us understand how and why should a model fit a particular use. A clean communication between academia and private sector is for us the warmest coat to protect ourselves from this winter.\n\nYou should follow us on Twitter: @mfg_labs"
    },
    {
        "url": "https://chronicles.mfglabs.com/a-to-the-t-the-drones-market-and-andr%C3%A8s-iniesta-in-the-programmable-edition-25-2c3ff883db36",
        "title": "A to the T, The Drones Market and Andr\u00e9s Iniesta in The Programmable Edition #25",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/ai-ai-everywhere-in-the-programmable-edition-23-e5136d3325c7",
        "title": "AI. AI everywhere in The Programmable Edition #23 \u2013",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/robocars-nsfw-machines-creepy-ai-chit-chat-and-more-in-the-programmable-edition-22-9724e63a1b4f",
        "title": "Robocars, NSFW machines, creepy AI chat & more in The Programmable Edition #22",
        "text": "Like what you just read? Hit the recommend button so others can find it too and wait for the Programmable Edition of next week.\n\nIn the meanwhile you can follow us on Twitter."
    },
    {
        "url": "https://chronicles.mfglabs.com/divide-et-impera-e2373abecf0f",
        "title": "Divide et impera \u2013",
        "text": "\u00ab Divide and conquer \u00bb. Machiavelli\u2019s maxim is widely spread across the tech world. That\u2019s how engineers think: we break down a complex problem into several smaller and simpler tasks. We like small and agile, not big and strong. Computer science was developed around this idea. In the early 60\u2019s, computers were powerful mainframes shared by dozens of people. After 1969 and the microprocessor revolution that allowed computer miniaturization, we shifted to a distributed architecture: several computers, more and more powerful.\n\nProcessors were developed following the same parallelism principle: GPUs (Graphical Processing Unit) allowed very efficient parallelization of computing, as well as multi-core processors.\n\nWe shifted from an architecture where the intelligence was concentrated in a single point to a distributed architecture where the intelligence is split between the nodes. But how could we take advantage of this tremendous power?\n\nInternet brings the answer in 1972. All these computers become connected, and can join forces to share their computing power. Grid Computing is born. It starts with computers, but soon expands: Stanford University offered Playstation 3 owners the chance to use the console\u2019s powerful processors to help research against Alzheimer or Parkinson.\n\nBut the rise of Cloud Computing and Big Data during the last few years has disrupted this way of doing things.\n\nNew services like Google, Facebook or Amazon needed to collect, store and compute incredible amounts of data, and nothing was available yet for them to handle the task. So they started building huge data centers with tens of thousands of servers, which run 24/7 to fit their needs. Amazon and Google were the first to realize that the architecture and tools they developed could be valuable to other companies. Hadoop was developed as an open source version of Google\u2019s Map-Reduce \u2014 which is by the way totally divide-and-conquerish \u2014 while Infrastructure-as-a-Service was born with Amazon Web Services. Services as big as Netflix or Nasa now entirely rely on it. Our AWS bill at MFG Labs is probably a lot lower than Netflix\u2019s, but we too use the service, as almost every single startup.\n\nBut there is a major problem: these server farms gather in a single node huge amount of computing and storage power, and take off-balance the very vision behind the last 40 years of computer development: we are back to 1962 and the good ole\u2019 Mainframe.\n\nWhere are we heading? Is this just a temporary trend or will it last?\n\nWe think SaaS / IaaS is awesome, and it\u2019s definitely here to stay, but the architecture on which they rely \u2014 data centers \u2014 will disappear.\n\nThey are centralized beings in a distributed world \u2014 the Internet world. They will adapt sooner or later, probably by diluting themselves in the network: every connected device will be a small part of those data centers, dedicating its storage power to host an infinitely small share of the world\u2019s data. This distributed architecture is a lot safer: if one node of the network goes down, it is painless, whereas if tomorrow an asteroid wipes out one of Amazon\u2019s datacenter, it would be a disaster. The data would not be owned by a single entity in a single place either, improving its privacy.\n\nBack to our beloved distributed architecture, the balance in the force will be restored, and us engineers can do once again what we do best: divide and conquer.\n\nYou should follow us on Twitter: @mfg_labs."
    },
    {
        "url": "https://chronicles.mfglabs.com/opensourcing-akka-stream-extensions-fe56faba5790",
        "title": "Opensourcing Akka-Stream Extensions \u2013",
        "text": "MFG Labs is proud to opensource its Akka-Stream Extensions under Apache2 license.\n\nMFG Labs business deeply relies on data, mathematical research and digital strategy. In terms of use case, we have the classic \u201cbatch\u201d one, when dealing with massive data, CPU-intensive and distributable algorithms with re-playable data sources and the well-known solutions around Hadoop/Spark. But we also have another use case where our data flows are real-time with non replayable data-sources. Those computations generally require intensive IO with external services that have their own limitations in terms of bandwidth & connections and that can be error-prone. Those data flows are often required for real-time solutions or for preparing data before sending them to batch processes.\n\nFor this latter use-case, we wanted to be able to:\n\nAkka-Stream seemed to be a really good fit for this use-case providing all the primitives we required, relying on Akka thus bringing the well-known \u201cembrace the failure\u201d philosophy and providing the \u201cback-pressure\u201d mechanism out-of-the-box.\n\nWe have integrated Akka-Stream in our projects and have discovered that Akka-Stream philosophy was about providing all required low-level primitives and encouraging people to build their own structures on top of it. We have started to identify custom primitives common to all our projects and have decided to gather them in a common internal library. We have then spent some time developing streaming extensions for technologies we use everyday like Postgres, ElasticSearch or AWS. Finally, we have evaluated very advanced features trying to enhance the typesafe construction of Akka-Stream flows by relying on Shapeless coproducts.\n\nEverything works very well in production and our initiative is generic enough to opensource this library so that other developers can use our structures, improve our ideas, bring new concepts, develop new extensions. We aim at bringing our little contribution to the very promising Akka-Stream project and contribute to its ecosystem so that it grows faster.\n\nAbout MFG Labs: MFG Labs helps brands and organisations fully embrace the programmable world by unlocking the potential of data through a wide set of services covering strategy, conception, production and data modeling and analysis. It gathers a very wide set of experts, from mathematicians to developers to designers. MFG Labs is growing fast, and is currently seeking talented people to join its team."
    }
]