[
    {
        "url": "https://medium.com/@abhinavr8/best-creators-in-europe-61fb8607f7d1?source=user_profile---------1----------------",
        "title": "Best creators in Europe \u2013 Abhinav Ralhan \u2013",
        "text": "In this blog post, I\u2019d discuss which players (top 5 leagues in Europe) have been crucial to their teams explicitly, i.e, through creating chances, primary assists and secondary assists for their respective teams.\n\nBefore actually discussing the observations, I hope you are familiar with what assists and second assists are.\n\nAn assist is a contribution by a player which helps to score a goal. It can be a direct pass (by foot/headed), a player who wins the penalty which leads to goal, a tackle which led to a chance and eventually a goal. Similarly, a second assist is the pass which leads to the eventual assist and goal.\n\nFor more on this, read here.\n\nComparison of number of chances and their conversion rates across leagues (for all teams) in Europe."
    },
    {
        "url": "https://medium.com/@abhinavr8/xg-model-74c14ede6c58?source=user_profile---------2----------------",
        "title": "xG Model \u2013 Abhinav Ralhan \u2013",
        "text": "In this blog post, I\u2019ll discuss what Expected Goals is, how it works and my first shot at creating one.\n\nOne of the more enthusiastic experts and analysts, Michael Caley, defined the metric in layman\u2019s terms:\n\n\u201cThe idea is to quantify the likelihood of a goal being scored from a particular shot attempt (or other scoring chance). This is an idea that I think is quite intuitive. \u2018We need to create better scoring chances\u2019 is something managers have said forever, and xG is basically just a quantification of that notion. The broad concept has probably been around for a long time in football \u2014 Charles Reep\u2019s notion that \u2018one of every nine shots is scored\u2019 is a sort of early version of xG.\u201d\n\n\u201cIt will also bring a new kind of fan to the game, absolutely. This kind of data overlaps with people who play Football Manager or fantasy league. It\u2019s exactly the same thing, you take information about players and you evaluate them. If you want to be better in fantasy league then you need data. In the US almost everyone grows up with fantasy sports as much as the real thing now.\u201d\n\nThe most common factors used in an xG model are distance and angle of the shot. As you can guess, there are differences even within this. For instance, some people measure distance as where the shot was taken from or even how it was delivered to the location before the shot was taken (11tegen11 and Pleuler). Other factors, may be through-balls, free-kicks, corner kicks, whether it was a header or a normal shot, time of the game and so on. This means the factors available for analysis in a model like this could reach anywhere to 50+ factors I suspect.\n\nThe idea is to be able to predict what will happen to particular chances and events, whether they will be converted or not, using particular attributes of data. I also use this opportunity to understand the importance of features through machine learning. All of this gives us an idea as to whether an event, if it occurs is likely to be a goal or not.\n\nData: Data provision courtesy of Stratabet. Here, I\u2019ve used English Championship, English Premiership, Bundesliga, France, Spain, Italy, division 1 data. It dates from the beginning of season 16\u201317 to the current 17\u201318.\n\nAttributes: For now, I\u2019ve used attributes such as \u2018icon\u2019 (type of event), \u2018shotQuality\u2019 (used values defined by Stratabet), \u2018defPressure\u2019, \u2018numDefPlayers\u2019, \u2018numAttPlayers\u2019, \u2018chanceRating\u2019 (used values as defined by Stratabet), \u2018type\u2019 (defines passage of play). All attributes are encoded to particular values. The \u2018outcome\u2019 variable is binary encoded, ofcourse.\n\nAlthough I\u2019ve used the parameter chanceRating & shotQuality which covers the idea of a shot going in or not, I would also like to incorporate Shot location later on, myself.\n\nNow I\u2019ve implemented it in Python and it gets really ugly exporting all of that code to medium so I implemented it in the form of a Jupyter notebook and it\u2019s pretty heavy. I\u2019ve also added some shot location visualizations as a confirmatory bias. Please take a look here.\n\nThis article was written with the aid of StrataData, which is property of Stratagem Technologies. StrataData powers the StrataBet Sports Trading Platform, in addition to StrataBet Premium Recommendations."
    },
    {
        "url": "https://medium.com/@abhinavr8/self-organizing-maps-ff5853a118d4?source=user_profile---------3----------------",
        "title": "Self Organizing Maps \u2013 Abhinav Ralhan \u2013",
        "text": "A self-organizing map (SOM) is a type of artificial neural network (ANN) that is trained using unsupervised learning to produce a low-dimensional (typically two-dimensional), discretized representation of the input space of the training samples, called a map, and is therefore a method to do dimensionality reduction. Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning (such as backpropagation with gradient descent), and in the sense that they use a neighborhood function to preserve the topological properties of the input space.\n\nSOM was introduced by Finnish professor Teuvo Kohonen in the 1980s is sometimes called a Kohonen map.\n\nWhat really happens in SOM ?\n\nEach data from data set recognizes themselves by competeting for representation. SOM mapping steps starts from initializing the weight vectors. From there a sample vector is selected randomly and the map of weight vectors is searched to find which weight best represents that sample. Each weight vector has neighboring weights that are close to it. The weight that is chosen is rewarded by being able to become more like that randomly selected sample vector. The neighbors of that weight are also rewarded by being able to become more like the chosen sample vector. This allows the map to grow and form different shapes. Most generally, they form square/rectangular/hexagonal/L shapes in 2D feature space.\n\nBest Matching Unit is a technique which calculates the distance from each weight to the sample vector, by running through all weight vectors. The weight with the shortest distance is the winner. There are numerous ways to determine the distance, however, the most commonly used method is the Euclidean Distance, and that\u2019s what is used in the following implementation.\n\nComing to implementation part, there are various Python libraries (minisom, sompy) out there which you could directly use to implement SOM. You could also write your own implementation of it. I\u2019ve implemented both on the iris dataset. Here are the results:\n\nIf the average distance is high, then the surrounding weights are very different and a light color is assigned to the location of the weight. If the average distance is low, a darker color is assigned. The resulting maps show that the concentration of different clusters of species are more predominant in three zones. First figure tells us only about where the density of species is greater (darker regions) or less (lighter regions). The second visualisation tells us how they are specifically clustered.\n\nApplications of the growing self-organizing map, Th. Villmann, H.-U. Bauer, May 1998\n\nPlease find my code here."
    },
    {
        "url": "https://medium.com/@abhinavr8/activation-functions-neural-networks-66220238e1ff?source=user_profile---------4----------------",
        "title": "Activation Functions: Neural Networks \u2013 Abhinav Ralhan \u2013",
        "text": "What is an Activation Function?\n\nIn computational networks, the activation function of a node defines the output of that node given an input or set of inputs. A standard computer chip circuit can be seen as a digital network of activation functions that can be \u201cON\u201d (1) or \u201cOFF\u201d (0), depending on input.\n\nThey are required to produce an output based on the given input signal of a node. That output signal is now used as an input in the next layer of the neural network.\n\nIt is necessary to use an activation function because without it the Neural Network would be a linear regression model. Usually, the activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. We would want our neural networks to work on complicated tasks like language translations and image classifications and that requires a non linear function. Non linear function are great, and they allow us to generate greater number of mappings from the input to the output of the neural network.\n\nThe range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s \u2014 shaped). It is also called the hyperbolic tangent function.\n\nIn general, a sigmoid function is real-valued, monotonic, and differentiable having a non-negative first derivative which is bell shaped.\n\nIn the context of artificial neural networks, the rectifier is an activation function defined as the positive part of its argument:\n\nwhere x is the input to a neuron. The rectifier was certainly the most popular activation function in 2018 for deep neural networks. There are various types of ReLu functions like Leaky ReLu, Noisy ReLu, Exponential Linear Units. Do try and explore these on your own.\n\nSoftplus function: f(x) = ln(1+exp x) , which is called the softplus function. The derivative of softplus is f \u2032(x)=exp(x) / ( 1+exp\u2061 x ) = 1/ (1 +exp(\u2212x )) which is also called the logistic function."
    },
    {
        "url": "https://medium.com/@abhinavr8/goal-chance-analysis-387f7e399102?source=user_profile---------5----------------",
        "title": "Goal Chance Analysis \u2013 Abhinav Ralhan \u2013",
        "text": "In this article, I explore the number of shots/chances that result in goals given their location and quality of chance. Analysis is done on teams based in Chinese Super League, the data is provided by StrataGem, and used Python library Plotly for the visualisations. This analysis was particularly done for a competition hosted by Chance Analytics.\n\nBeing a beginner into Football Analytics, the idea behind this is to create/validate an xG model based on factors like shot location, shot quality, defensive pressure, and a few more. This particular analysis here, helps provide insight into factoring the positional strength of the chance. There are models out there which suggest that the chance percentage of a goal only relies on the position of chance, and while that remains highly true it is not the only factor to take in.\n\nThe data is relatively less and that leaves us susceptible to discrepancies, but the model remains same and can be used for bigger datasets. Current top 5 have been analysed (just for the sake of greater number of chances).\n\nHere, the coordinates (0,0) represent the mid point of a goal while the coordinates (0,44) representing the penalty spot.\n\nThe take away from this analysis is to validate the scoring chance percentage from the scatter plot. Sometimes you would find that a chance with greater chance percentage of scoring is further away from goal than a chance with lesser percentage of scoring. If anything it reinforces my belief that factoring weight provided to all types of chances should be slightly modified while calculating our xG model.\n\nAn excerpt on the same from StrataGem:\n\nThat\u2019s it for now folks."
    },
    {
        "url": "https://towardsdatascience.com/data-preparation-and-exploration-5e09b92cf00e?source=user_profile---------6----------------",
        "title": "Data Preparation and Exploration \u2013",
        "text": "This is a blog for people new to Data Science, like me. I hope we learn together through this process. My personal interests lie heavily in Data Analytics and Visualisation, so keep an eye out for more blogs on the same.\n\nDatasets are like introverted human beings, at first, they might/might not give you a misleading idea but they never lie. Exploratory analysis, noise removal, missing value treatment, identifying outliers and correct data inconsistencies, and more, all are a part of the process called data preparation and exploration. I will talk more about these in detail below.\n\nAnother very important aspect of the process would the identification of variable. The image below speaks for itself\n\nPredictor variables(input) and Target variables(output) define the independent & dependent variables. Knowing whether a variable will consist of a character or a numerical value is also essential to knowledge. Categorical variables can be further divided into: nominal, dichotomous, ordinal variables. They are generally referred to as qualitative variables. Continuous variables have two categories: interval and ratio variables. They are also called quantitative variables.\n\nEDA is used to understand, summarize and analyse the contents of a dataset, usually to investigate a specific question or to prepare for more advanced modeling.\n\nUnivariate Analysis: For continuous variables, it helps us find mean, median, mode, min, max. For categorical variables, we use frequency table to understand distribution of each category. Univariate analysis is also used to highlight missing and outlier values.\n\nBi-variate Analysis: While doing bi-variate analysis between two continuous variables, we should look at scatter plot. The relationship can be linear or non-linear. To find the strength of the relationship, we use Correlation. Correlation varies between -1 and +1.\n\nThere are different models which determine correlation between types of variables:\n\nThere are various ways to deal with each of these missing values, and you should explore that in detail. This step is important because it can cause huge amounts of inconsistency when you look at the averages, sums of various groups of data. It is better to get rid of all those nasty NaN values as soon as possible. We can go ahead with a few models in this one.\n\nDeleting entry, this is where we can remove the missing/null values by removing the entire row of data but also in the process loose quite a lot of data which could create serious amounts of variance.\n\nMean/ Mode/ Median Imputation, we can replace the missing/null values with either of 3 M\u2019s depending on the possible values of the given column. The method consists of replacing the missing data for a given attribute by the mean or median (quantitative attribute) or mode (qualitative attribute) of all known values of that variable.\n\nPrediction model, here we divide the dataset into two datasets, one with no missing values(to train) and one with missing values. This helps us figure out the values to be introduced in the dataset with missing values. This model can still be less precise in cases where there is no relation between missing values and the attributes in the dataset.\n\nIn statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set.\n\nWhile outliers may seem to be an extra unnecessary addition to the dataset at first, from hindsight I can confirm to you it\u2019s not, and they deserve fair treatment as well. Models like deletion, imputing, and individual treatment exist for the same. Transforming and binning also ensures that the variance caused by outliers is reduced.\n\nFeature Engineering involves creation of derived or dummy variables by analysing the rest of the dataset, and existing dependencies between variables in it. Feature engineering can help in validating the correctness of data.\n\nWhen data is collected over time displays random variation, data smoothing techniques can be used to reduce or cancel the effect of these variations. When properly applied, these techniques smooth out the random variation in the time series data to reveal underlying trends.\n\nThat\u2019s it for now folks. This was great fun for me. I hope you enjoyed it as well. Any feedback would be appreciated. Contact me on Twitter about anything, https://twitter.com/abhinavr8.\n\nHere\u2019s a link to some of the Exploratory Analysis I did. There are always various ways to go about the same. Do share your opinions if you feel like it.\n\nAlso, if there\u2019s anything specific you would want me to cover make sure to put that down in the comments."
    }
]