[
    {
        "url": "https://medium.com/explorations-in-language-and-learning/an-introduction-to-speech-recognition-using-wfsts-288b6aeecebe?source=---------0",
        "title": "An Introduction to Speech Recognition using WFSTs \u2013 Explorations in Language and Learning \u2013",
        "text": "Composition, as the name suggests, refers to the process of combining 2 WFSTs to form a single WFST. If we have transducers for pronunciation and word-level grammar, such an algorithm would enable us to form a phone-to-word level system easily.\n\nComposition is done using 3 rules:\n\nAn example of composition is shown below.\n\nAt this point, it may be important to define what \u201csum\u201d means for edge weights. Formally, the \u201clanguages\u201d accepted by WFSTs are generalized through the notion of semirings. Basically, it is a set of elements with 2 operators, namely \u2295 and \u2297. Depending on the type of semiring, these operators can take on different definitions. For example, in a tropical semiring, \u2295 denotes min, and \u2297 denotes sum. Furthermore, in any WFST, weights are \u2297-multiplied along paths (Note: here \u201cmultiplied\u201d would mean summed for a tropical semiring) and \u2295-summed over paths with identical symbol sequence.\n\nSee here for OpenFST implementation of composition.\n\nA deterministic automaton is one in which there is only one transition for each label in every state. By such a formulation, a deterministic WFST removes all redundancy and greatly reduces the complexity of the underlying grammar. But, are all WFSTs determinizable?\n\nThe Twins Property: Let us consider an automaton A. Two states p and q in A are said to be siblings if both can be reached by string x and both have cycles with label y. Essentially, siblings are twins if the total weight for the paths until the states, as well as that including the cycle, are equal for both.\n\nThis is an example of what I said earlier regarding WFSTs being an efficient implementation of the algorithms used in ASR. There are several methods to determinize a WFST. One such algorithm is shown below.\n\nIn simpler steps, this algorithm does the following:\n\nSince this is a local algorithm, it can be efficiently implemented in-memory. To see how to perform determinization in OpenFST, see here.\n\nAlthough minimization is not as essential as determinization, it is still a nice optimization technique. It refers to minimizing the number of states and transitions in a deterministic WFST.\n\nMinimization is carried out in 2 steps:\n\n2. After this is done, we combine those states which have identical paths to any final state. For example in the above WFST, states 1 and 2 have become identical after weight pushing, so they are combined into one state.\n\nIn OpenFST, the implementation details for minimization can be found here.\n\nThe following (taken from [3]) shows the complete pipeline for a WFST reduction."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/how-to-obtain-sentence-vectors-2a6d88bd3c8b?source=---------1",
        "title": "How to obtain Sentence Vectors? \u2013 Explorations in Language and Learning \u2013",
        "text": "In several of my previous posts, I have discussed methods for obtaining word embeddings, such as SVD, word2vec, or GloVe. In this post, I will abstract a level higher and talk about 4 different methods that have been proposed to get embeddings for sentences.\n\nBut first, some of you may ask why do we even need a different method for obtaining sentence vectors. Since sentences are essentially made up of words, it may be reasonable to argue that simply taking the sum or the average of the constituent word vectors should give a decent sentence representation. This is akin to a bag-of-words representation, and hence suffers from the same limitations, i.e.\n\nOther word vector based approaches are also similarly constrained. For instance, a weighted average technique again loses word order within the sentence. To remedy this issue, Socher et al. combined the words in the order given by the parse tree of the sentence. While this technique may be suitable for complete sentences, it does not work for phrases or paragraphs.\n\nIn an earlier post, I discussed several ways in which sentence representations are obtained as an intermediate step during text classification. Several approaches are used for this purpose, such as character to sentence level feature encoding, parse trees, regional (two-view) embeddings, and so on. However, the limitation with such an \u201cintermediate\u201d representation is that the vectors obtained are not generic in that they are closely tied to the classification objective. As such, vectors obtained through training on one objective may not be extrapolated for other tasks.\n\nIn light of this discussion, I will now describe 4 recent methods that have been proposed to obtain general sentence vectors. Note that each of these belongs to either of 2 categories: (i) inter-sentence, wherein the vector of one sentence depends on its surrounding sentences, and (ii) intra-sentence, where a sentence vector only depends on that particular sentence in isolation.\n\nIn this ICML\u201914 paper [1] from Mikolov (who also invented word2vec), the authors propose the following solution: a sentence vector can be learned simply by assigning an index to each sentence, and then treating the index like any other word. This is shown in the following figure.\n\nEssentially, every paragraph (or sentence) is mapped to a unique vector, and the combined paragraph and word vectors are used to predict the next word. Through such a training, the paragraph vectors may start storing missing information, thus acting like a memory for the paragraph. For this reason, this method is called the Distributed Memory model (PV-DM).\n\nTo obtain the embeddings for an unknown sentence, an inference step needs to be performed. A new column of randomly initialized values is added to the sentence embedding matrix. The inference step is performed keeping all the other parameters fixed to obtain the required vector.\n\nThe PV-DM model requires a large amount of storage space since the paragraph vectors are concatenated with all the vectors in the context window at every training step. To solve this, the authors propose another model, called the Distributed BOW (PV-DBOW), which predicts random words in the context window. The downside is that this model does not use word order, and hence performs worse than PV-DM.\n\nWhile PV was an intra-sentence model, skip-thoughts [2] is inter-sentence. The method uses continuity of text to predict the next sentence from the given sentence. This also solves the problem of the inference step that is present in the PV model. If you have read about the skip-gram algorithm in word2vec, skip-thoughts is essentially the same technique abstracted to the sentence level.\n\nIn the paper, the authors propose an encoder-decoder framework for training, with an RNN used for both encoding and decoding. In addition to a sentence embedding matrix, this method also generates vectors for the words in the corpus vocabulary. Finally, the objective function to be maximized is as follows.\n\nHere, the indices i+1 and i-1 represent the next sentence and the previous sentence, respectively. Overall, the function represents the sum of log probabilities of correctly predicting the next sentence and the previous sentence, given the current sentence.\n\nSince word vectors are also precited at training time, a problem may arise at the time of inference if the new sentence contains an OOV word. To solve this, the authors present a simple solution for vocabulary expansion. We assume that any word, even if it is OOV, will definitely come from some vector space (say w2v), such that we have its vector representation in that space. As such, every known word has 2 representations, one in the RNN space and another in the w2v space. We can then identify a linear transformation matrix that transforms w2v space vectors into RNN space vectors, and this matrix may be used to obtain the RNN vectors for OOV words.\n\nThis model, proposed by Kyunghun Cho [3], is also an inter-sentence technique, and is conceptually very similar to skip-thoughts. The only difference is that it uses a BOW representation of the sentence to predict the surrounding sentences, which makes it computationally much more efficient than skip-thoughts. The training hypothesis remains the same, i.e., rich sentence semantics can be inferred from the content of adjacent sentences. Since the details of the method are same as skip-thoughts, I will not repeat them here to avoid redundancy.\n\nThis technique was also proposed in the same paper [3] as FastSent. However, it is essentially an intra-sentence method wherein the objective is to regenerate a sentence from a noisy version.\n\nIn essence, in an SDAE, a high-dimensional input data is corrupted according to some noise function and the model is trained to recover the original data from the corrputed version.\n\nIn the paper, the noise function N uses 2 parameters as follows.\n\nThese are inspired from the \u201cword dropout\u201d and \u201cdebagging\u201d approaches, respectively, which have earlier been studied in some detail."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/online-learning-of-word-embeddings-7c2889c99704?source=---------2",
        "title": "Online Learning of Word Embeddings \u2013 Explorations in Language and Learning \u2013",
        "text": "SGNS is a window-based method with the following training objective: Given the target word, predict all the context words in the window.\n\nSuppose we have a context window where w is the target word and c is one of the context words. Then, skip-gram\u2019s objective is to compute P(c|w), which is given as\n\nBasically, it is just a softmax probability distribution over all the word-context pairs in the corpus, directed by the cosine similarity. However, the denominator term here is very expensive to compute since there may be a very large number of possible context words. To solve this problem, negative sampling is used.\n\nGoldberg and Levy have explained the derivation for the objective function in SGNS very clearly in their note. I will try to provide a little intuition here.\n\nFor the word w, we are trying to predict the context word c. Since we are using softmax, this is essentially like a multi-class classification problem, where we are trying to classify the next word into one of N classes (where N is the number of words in the dictionary). Since N may be quite large, this is a very difficult problem.\n\nWhat SGNS does is that it converts this multi-classification problem into binary classification. The new objective is to predict, for any given word-context pair (w,c), whether the pair is in the window or not. For this, we try to increase the probability of a \u201cpositive\u201d pair (w,c), while at the same time reducing the probability of k randomly chosen \u201cnegative samples\u201d (w,s) where s is a word not found in w\u2019s context. This leads to the following objective function which we try to maximize in SGNS:\n\nIn other words, we push the target vector in the direction of the positive context vector, and pull it away from k randomly chosen (w.r.t. the unigram probability distribution) negative vectors. Here \u201cnegative\u201d means that these vectors are not actually present in the target\u2019s context.\n\nAs is evident from the above discussion, since SGNS is a window-based approach, the training itself is very much in an online paradigm. However, the constraints are in creating a vocabulary and a unigram distribution for negative sampling, which makes SGNS a two-pass method. Further, if additional data is seen later, the distribution and vocabulary would change, and the model would have to be retrained.\n\nEssentially, we need online alternatives for 2 aspects of the algorithms:\n\nWith this background, I will now discuss the two proposed methods for online SGNS."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/sparsity-in-online-learning-with-lasso-regularization-f65f97e08e4e?source=---------3",
        "title": "Sparsity in Online Learning with Lasso Regularization",
        "text": "Note: The method was named Forward Looking Subgradient (FOLOS) in the first draft and later renamed since it was essentially the same as an earlier proposed technique, the Forward Backward Splitting. The authors abbreviated it to FOBOS instead of FOBAS to avoid confusing readers of the first draft.\n\nFirst, a little background. Consider an objective function of the form f(w) + r(w). In the case of a number of machine learning algorithms, the function f denotes the empirical sum of some loss function (such as mean squared error), and the function r is a regularizer (such as Lasso). If we use a simple gradient descent technique to minimize this objective function, the iterates would be of the form\n\nwhere the g\u2019s are vectors from the subgradient sets of the corresponding functions. From the paper:\n\nIn other words, the subgradient approach will result in neither a true minima nor a sparse solution if r is the L1 regularizer.\n\nFOBOS, as the name suggests, splits every iteration into 2 steps \u2014 a forward step and a backward step, instead of minimizing both f and r simultaneously. The motivation for the method is that for L1 regularization functions, true minima is usually attained at the points of non-differentiability. For example, in the 2-D space, the function resembles a Diamond shape and the minima is obtained at one of the corner points. Each iteration of FOBOS consists of the following 2 steps:\n\nThe first step is a simple unconstrained subgradient step with respect to the function f. In the second step, we try to achieve 2 objectives:\n\nSo the first step is a forward step, where we update the coefficient in the direction of the subgradient, while the second is a backward step where we pull the update back a little so as to obtain sparsity by moving in the direction of the non-differentiable points of r.\n\nUsing the first equation in the second, taking derivative w.r.t w, and equating the derivative to 0, we obtain the update scheme as\n\nThis update scheme has 2 major advantages, according to the author.\n\nIn the paper, the authors also prove convergence of the method and show that on setting the intermediate learning rate properly, low regret bounds can be proved for both online as well as batch settings."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/a-short-note-on-gradient-descent-optimization-algorithms-335546c5a896?source=---------4",
        "title": "A short note on Gradient Descent optimization algorithms",
        "text": "I just finished reading Sebastian Ruder\u2019s amazing article providing an overview of the most popular algorithms used for optimizing gradient descent. Here I\u2019ll make very short notes on them primarily for purposes of recall.\n\nThe update vector consists of another term which has the previous update vector (weighted by \u03b3). This helps it to move faster downhill \u2014 like a ball.\n\nIn Momentum optimizer, the ball may go past the minima due to too much momentum, so we want to have a look-ahead term. In NAG, we take gradient of future position instead of current position.\n\nInstead of a common learning rate for all parameters, we want to have separate learning rate for each. So Adagrad keeps sum of squares of parameter-wise gradients and modifies individual learning rates using this. As a result, parameters occuring more often have smaller gradients.\n\nIn Adagrad, since we keep adding all gradients, gradients become vanishingly small after some time. So in RMSProp, the idea is to add them in a decaying fashion as\n\nNow replace G_t in the denominator of Adagrad equation by this new term. Due to this, the gradients are no more vanishing.\n\nAdam combines RMSProp with Momentum. So, in addition to using the decaying average of past squared gradients for parameter-specific learning rate, it uses a decaying average of past gradients in place of the current gradient (similar to Momentum).\n\nThe ^ terms are actually bias-corrected averages to ensure that the values are not biased towards 0.\n\nNadam combines RMSProp with NAG (since NAG is usually better for slope adaptation than Momentum. The derivation is simple and can be found in Ruder\u2019s paper."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/irony-detection-in-tweets-6220f480cc60?source=---------5",
        "title": "Irony detection in tweets \u2013 Explorations in Language and Learning \u2013",
        "text": "There was a SemEval 2018 Shared Task on \u201cirony detection in tweets\u201d that ended recently. As a fun personal project, I thought of giving it a shot, just to implement some new ideas. In this post, I will describe my approach for the problem along with some code.\n\nThe task itself was divided into two subtasks:\n\nWhile the task appears to be a simple text classification job, there are several nuances that make it challenging. Irony is often context-dependent or derived from world knowledge. In sentiment analysis, the semantics of the sentences are sufficient to judge whether the sentence has been spoken in a positive or negative manner. However, irony, by definition, almost always exists when the literal meaning of the sentence is dramatically different from what has been implied. Sample this:\n\nFrom a sentiment analysis perspective, the presence of the phrase \u201cjust great\u201d would adjudge this sentence strongly positive. However, from our world knowledge, we know the nuances of the interplay between a \u201cmobile bill\u201d and \u201ctext.\u201d As a human, then, we can judge that the sentence is spoken in irony.\n\nThe problem is: how can we have an automated system understand this?\n\nThe first idea of a solution came from how the dataset was generated in the first place. To mine tweets containing irony, those tweets were selected which contained the hashtag #not. The idea was that a lot of people explicitly declare their intent at irony through hashtags. For instance, consider the following tweet:\n\nIn this example, let us breakdown the tweet into 2 components:\n\nIt is obvious from the semantics of the 2 components that they imply very different things. As such, it may help to model the interaction between the \u201ctext\u201d and \u201chashtag\u201d components of the tweet and then use the resulting embedding for classification. In this regard, we are essentially treating the problem as that of relation classification, where the entities are the 2 components and we need to identify whether there exists a relation between them (task A), and if yes, of which type (task B).\n\nThe problem, now, is reduced to the issue of how to model the two components and their interaction. This is where deep learning comes into the picture.\n\nThe embeddings to represent the components are obtained simply by passing their pretrained word vectors through a bidirectional LSTM layer. This is fairly simple for the text component.\n\nHowever, in the hashtag component, a single hashtag almost always consists of multiple words concatenated into a single string. Therefore, we first perform word segmentation on the hashtag and use the resulting segments to obtain the embedding.\n\nOnce the embeddings for the two components have been obtained, we use the circular cross-correlation technique (which I have earlier described in this blog post) to model their interaction. Essentially, the operator is defined as\n\nIn Tensorflow, this is implemented as follows:\n\nThe output of this merge is then passed to an XGBoost classifier (whose implementation was used out-of-the-box from the corresponding Python package).\n\nThis model resulted in a validation accuracy of ~62%, compared to ~59% for a simple LSTM model. Time to analyze where it was failing!\n\nThe problem with this idea was that although it performed well for samples similar to the example given above, such samples constituted only about 20% of the dataset. For a majority of the tweets containing irony, there was no hashtag, and as such, modeling interactions was useless.\n\nIn such cases, we have to solely rely upon the text component to detect hashtag, for e.g.\n\nIf an automated system has to understand that the above sentence contains irony, it needs to know that there is nothing fun about driving on a road covered in snow. This knowledge cannot be gained from learning on a few thousand tweets. We now turn to transfer learning!\n\nMIT researchers recently built an unsupervised system called DeepMoji for emoji prediction in tweets. According to the website, \u201cDeepMoji has learned to understand emotions and sarcasm based on millions of emojis.\u201d We hypothesize that if we use this pretrained model to extract features from the text component, it may then be used to predict whether the text contains irony. In a way, we are transfering world knowledge to our model (assuming that the million tweets on which DeepMoji was trained is our world!).\n\nAs expected, concatenating the DeepMoji features with the holographic embeddings resulted in a validation accuracy of ~69%, i.e., a jump of almost 7%. This reinforces our hypothesis that world knowledge is indeed an important ingredient in any kind of irony detection.\n\nIn essence, we identified 2 aspects that were essential to identify irony in tweets:\n\nThe code for the project is available here.\n\nDisclaimer: In the final test phase, the results were disappointing (~50% for task A) especially given the high performance on validation set. This could likely have been due to some implementation error on the test set, and we are waiting for the gold labels to be released to analyze our mistake."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/introduction-to-learning-theory-part-2-48fc1fead6ac?source=---------6",
        "title": "Introduction to Learning Theory \u2014 Part 2 \u2013 Explorations in Language and Learning \u2013",
        "text": "In the first part of this series on learning theory, we looked only at the case of finite hypothesis sets, and derived some generalization bounds using the PAC learning framework. However, in most practical cases, the hypothesis class is usually infinite. To measure the complexity of the class in such cases, 3 different measures are often used \u2014 Rademacher complexity, growth function, and VC dimension. In this article, I will discuss all of these.\n\nGiven a family of functions, one of the ways to measure its complexity is to see how well it can fit a random assignment of labels. A more complex hypothesis set would be able to fit a random noise better, and vice versa. For this purpose, we define m random variables \u03c3i, called Rademacher variables. We then define the empirical Rademacher complexity as\n\nHere the summation term is essentially the inner product of the vector of noise (Rademacher variables) and the labels with some g in G. Intuitively, this term can be taken to represent the correlation between the actual assignment and the random assignment. On taking the supremum over all g in G, we are computing how well the function class G correlates with random noise on S. The expectation of this term over all random noise distributions measures the average correlation.\n\nTherefore, a higher Rademacher complexity would imply that the function class G is able to fit a random assignment of labels well, and vice versa. This is because the more complex a class G is, higher is the probability that it would have some g which correlates well with random noise.\n\nHowever, this is just the empirical R.C. since we are computing the mean on the given sample set. The actual R.C. is obtained by taking the expectation of this value by sampling S from a distribution D consisting of sample sets of size m. Having thus defined the R.C., we can obtain an upper bound on the expected value of an error function g taken from a family of functions G.\n\nNote that if we take the first term on RHS to LHS, the LHS becomes the maximum difference between the empirical and general loss (function value if function is binary-valued). We have access to the empirical values, but not the expectation. So we take 2 sample sets A and B which differ at only 1 point, so that we can use the McDiarmid\u2019s inequality.\n\nThe actual proof then becomes simply manipulating the expectation and supremum using Jensen\u2019s inequality (function of an expectation is at most expectation of the function, if the function itself is convex). I do not go into the details of the proof here since it is readily available.\n\nTill now, we have only computed the bounds on the expectation of the set of loss functions G. We actually need to compute bounds on the general loss on the hypothesis class H, which assigns binary values to given samples. For this, we use the following lemma which is simple to prove.\n\nFrom this and the earlier result, we easily arrive at an upper bound on the generalization error of the hypothesis class in terms of its Rademacher complexity.\n\nHere, computing the empirical loss is simple, but computing the R.C. for some hypothesis sets may be hard (since it is equivalent to an empirical risk minimization problem). Therefore, we need some complexity measures which are easier to compute.\n\nThe growth function of a hypothesis class H for sample size m denotes the number of distinct ways that H can classify the sample. A more complex hypothesis class would be able to have a larger number of possible combinations for any sample size m. However, unlike R.C., this measure is purely combinatorial, and independent of the underlying distributions in H.\n\nThe Rademacher complexity and the growth function are related by Massart\u2019s lemma as\n\nAs soon as we see \u201cexpected correlation,\u201d we should think of the Rademacher complexity. To introduce the growth function, we use the term for the size of the set, since it essentially denotes the size of set containing all possible assignments for a sample.\n\nUsing this relation in the earlier obtained upper bound, we can bound the generalization error in terms of the growth function.\n\nAlthough it is a combinatorial quantity, the growth function still depends on the sample size m, and thus would require repeated calculations for all values m>1. Instead, we turn to the third and most popular complexity measure for hypothesis sets.\n\nThe VC-dimension of a hypothesis class is the size of the largest set that can be fully shattered by it. By shattering, we mean that H can classify the given set in all possible ways. Formally,\n\nIt is important to understand 2 things:\n\nTo relate VC-dimension with the growth function, we use the Sauer\u2019s lemma:\n\nHere, the LHS, which is the growth function, represents the number of possible behaviors that H can have on a set of size m. The RHS is the number of small subsets that are completely shattered by H. For a detailed proof, I highly recommend this lecture (Actually, I would highly recommend the entire course).\n\nUsing some manipulations on the combinatorial, we arrive at\n\nNow we can use this relation with the earlier results to bound the generalization error in terms of the VC-dimension of the hypothesis class.\n\nwhere m is the sample size and d is the VC-dimension."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/introduction-to-learning-theory-part-1-30db513ce3eb?source=---------7",
        "title": "Introduction to Learning Theory \u2014 Part 1 \u2013 Explorations in Language and Learning \u2013",
        "text": "One of the most significant take-aways from NIPS 2017 was the \u201calchemy\u201d debate spearheaded by Ali Rahimi. In the wake of the event, I have been trying to learn more about statistical learning theory, even though the concepts may not be readily applicable to deep neural networks.\n\nOne of the most important concepts in this regard is to measure the complexity of a hypothesis class H. In any machine learning model, the end goal is to find a hypothesis class that achieves a high accuracy on the training set, and has low generalization error on the test set. For this, we require the hypothesis class H to approximate the concept class C which determines the labels for the distribution D. Since both C and D are unknown, we try to model H based on the known sample set S and its labels.\n\nGeneralization error: The generalization error of a hypothesis h is the expectation of the error on a sample x picked from the distribution D.\n\nEmpirical error: This is the mean of the error of hypothesis h on the sample S of size m.\n\nHaving defined the generalization error and empirical error thus, we can state the objective of learning as follows.\n\nThis kind of a learning framework is known as PAC-learning (Probably Approximately Correct). Formally, a concept class C is PAC-learnable if there is some algorithm A for which the generalization error on a sample S derived from the distribution D is very low (less than \u03b5) with high probability (greater than 1- \u03b4). In other words, we can say that for a PAC-learnable class, the accuracy is high with good confidence.\n\nThe PAC-learning framework provides strong guarantees for finite hypothesis sets (i.e., where the size of H is finite). Again, this falls in two categories \u2014 the consistent case, and the inconsistent case. A hypothesis class is said to be consistent if it admits no error on the training sample, i.e., the training accuracy is 100%.\n\nLet us consider a finite hypothesis set H. We want the generalization error to be less than some \u03b5, so we will take a consistent hypothesis h in H, and bound the probability that its error is more than \u03b5, i.e., we are calculating the probability that there exists some h in H, such that h is consistent and its generalization error is more than \u03b5. This is simply the union of all h in H such that it follows the said constraints. By the union bound, this probability will be less than the sum of the individual probabilities i.e.,\n\nFrom the definition of conditional probability, we can write\n\nwhich bounds the required probability P as\n\nThe condition says that the expectation of error of h on any sample is at least \u03b5, so it would correctly classify a sample with probability at most 1- \u03b5. Hence, to correctly classify m training samples with |H| hypotheses, the total probability is given as\n\nOn setting the RHS of the inequality to \u03b4, we obtain the generalization bound of the finite, consistent hypothesis class as\n\nAs expected, the generalization error decreases with a larger training set. However, to arrive at a consistent algorithm, we may have to increase the size of the hypothesis class, which results in an increase in generalization error.\n\nIn practical scenarios, it is very restrictive to always require a consistent hypothesis class to bound the generalization error. In this section, we look at a more general case where empirical error is non-zero. For this derivation, we use the Hoeffding\u2019s inequality which provides an upper bound on the probability that the mean of independent variables in an interval [0,1] deviates from its expected value by more than a certain amount.\n\nIf we take the errors as the random variable, their mean is the empirical error and the expectation is the generalization error. We can then get an upper bound for the generalization error of a single hypothesis h as\n\nHowever, this is still not the general case since the hypothesis h returned by the learning algorithm is not fixed. Similar to the consistent case, we will try to obtain an upper bound on the generalization error for an inconsistent (but finite) hypothesis, i.e., we need to compute the probability that there exists some hypothesis h in H such that the generalization error of h differs from its empirical error by a value greater than \u03b5. Again, using the union bound, we get\n\nUsing the Hoeffdieng\u2019s inequality, this becomes\n\nNow equating the RHS with \u03b4, we can arrive at the result\n\nHere it is interesting to note that for a fixed |H|, to attain the same guarantee as in the consistent case, a quadratically larger labeled sample is required. Let us now analyze the role of the size of hypothesis class. If we have a smaller H, the second term is reduced but the empirical error may increase, and vice versa. However, for the same empirical error, it is always better to go with the smaller hypothesis class, i.e., the famous Occam\u2019s Razor principle."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/unsupervised-approaches-for-nmt-f0b18b12d4d5?source=---------8",
        "title": "Unsupervised approaches for NMT \u2013 Explorations in Language and Learning \u2013",
        "text": "Translation is one of those tasks in language where the arrival of deep learning systems, and in particular sequence-to-sequence, has been something like a boon. In less than 4 years since the first paper on Neural Machine Translation, software giants such as Google and Microsoft have already announced that their translation systems have almost completely shifted from statistical to neural. Gone are the days when researchers mulled over complex word and phrase alignment techniques, and yet fell short on several language combinations. With the latest framework, all you need are a million parallel sentences, and your system can then translate between this pair sufficiently well.\n\nA million parallel sentences \u2014 that\u2019s a little constraining, though! It is often difficult and sometimes even impossible to obtain a bilingual parallel corpus for many pairs of languages. In such cases, using a pivot language for triangulation has been found to be helpful. However, even in such supervised systems, the performance is still constrained by the size of the training corpus.\n\nMonolingual data, on the other hand, is available in abundance, and a number of semi-supervised systems do use these, but mostly for the language modeling part of translation. For example, a naive system may perform word-by-word substitution and use a language model trained on the target language to obtain the most probable word order.\n\nRecently, there have been 2 very similar papers (both currently under review at ICLR \u201918) which propose to perform completely unsupervised machine translation. In this article, I will discuss both of these papers. A similar blog is available here, but I didn\u2019t know of its existence until I was already halfway through this post."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/beyond-euclidean-embeddings-c125bbd07398?source=---------9",
        "title": "Beyond Euclidean embeddings \u2013 Explorations in Language and Learning \u2013",
        "text": "This paper [2] proposes embeddings in hyperbolic spaces, such as the Poincare sphere. Before we get into the method itself, I think it would be best to give a brief overview of hyperbolic geometry itself.\n\nIn his book Elements, Euclid provided a rigourous framework for axioms, theorems and postulates for all geometrical knowledge at the time. He stated 5 axioms which were to be assumed true. The first 4 were quite self-evident, and were:\n\n(1) Any two points can be connected by a line. \n\n(2) Any line segment can be extended indefinitely. \n\n(3) Given a line segment, a circle can be drawn with center at one of the endpoints and radius equal to the length of the segment. \n\n(4) Any two right angles are congruent.\n\nHowever, the fifth axiom, also known as Playfair\u2019s axiom, is much less axiom.\n\nPlayfair\u2019s axiom: Given a line L and a point P, there exists at most one line through P that is parallel to L.\n\nEuclid himself wasn\u2019t very fond of this axiom and his first 28 postulates depended only on the first 4 axioms, which are the \u201ccore\u201d of Euclidean geometry. Even 2000 years after his death, mathematicians tried to derive the fifth axiom from the first 4. While using \u201cproof by contradiction\u201d for this purpose, they assumed the negation of the fifth axiom (Given a line L and a point P not on L, there are at least two distinct lines that can be drawn through P that are parallel to L) and tried to arrive at a contradiction. However, while the derived results were strange and very different from those in Euclidean geometry, they were consistent within themselves. This was a turning point in mathematics as such a bifurcation in geometry had never been expected before. The geometry that arose from these explorations is known as hyperbolic geometry.\n\nWith this knowledge, let us now look at how embeddings may be computed in this new model.\n\nThe Poincare sphere model of hyperbolic space is particularly suitable for representing hierarchies. Consider a knowledge base which can be visualized as a tree. For any branching factor b, the number of leaf nodes increases exponentially as the number of levels increases. If we try to replicate this construction in a Euclidean disk(sphere), it would not be possible since the area(volume) of a disk(sphere) increases only quadratically(cubically) with increase in radius. This requires that we increase the number of dimensions exponentially.\n\nHowever, the Poincare sphere embeds such hierarchies easily: nodes that are exactly l levels below the root are placed on a sphere in hyperbolic space with radius r \u221d l and nodes that are less than l levels below the root are located within this sphere. This type of construction is possible as hyperbolic disc area and circle length grow exponentially with their radius. In the paper, the authors used a sphere instead of disk since more degrees of freedom implies better representation of latent hierarchies.\n\nDistances in the hyperbolic space are given as\n\nHere, hierarchy is represented using the norm of the embedding, while similarity is mirrored in the norm of vector difference. Furthermore, the function is differentiable, which is good for gradient descent.\n\nFor optimization, the update term is the learning rate times the Riemannian gradient of the parameter. The Riemannian gradient itself is computed by taking the product of the Poincare ball matrix inverse (which is trivial to compute) with the Euclidean gradient (which depends on the gradients of the distance function). The loss function used in the paper is a softmax with negative sampling."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/deep-learning-for-multimodal-systems-3dfeec7c263f",
        "title": "Deep Learning for Multimodal Systems \u2013 Explorations in Language and Learning \u2013",
        "text": "Given an album containing several images (which may or may not be similar), the task of Visual Storytelling is to generate a natural language story describing the album. In this EMNLP \u201917 paper [1], the task is decomposed into 3 steps:\n\nFor each of these three components, the paper uses a hierarchically-attentive RNN. The first component is similar to an embedding layer in a text classification setting, wherein a lookup table assigns some pretrained vectors to each word and then an RNN is applied to add sentence-level information to each word vector. In a similar fashion in this paper, the initial embeddings for each image are obtained using a pretrained ResNet101 layer, and then a bidirectional RNN with GRU cells is used to add information pertaining to the entire album in every image embedding.\n\nIn the Photo Selector stage, the selection is treated as a latent variable since we only have end-to-end ground truth labels. As such, we use soft attention to output t probability distributions over all the images in the album, where t is the number of summary images required, i.e., each image has t probabilities associated with it. For this purpose, a GRU takes the previous p and the previous hidden state h as input and outputs the next hidden state. We use a multilayer perceptron with sigmoid activation to fuse the hidden state with the photo vector and obtain the soft attention for the particular image.\n\nFinally, we can obtain t weighted album representations by taking the weighted sum of the photo vectors with the corresponding probability distributions. Each of these vectors is then used to decode a single sentence. For this purpose, a GRU takes the joint input of the album vector at step t, the previous word embedding, and the previous hidden state, and outputs the next hidden state. We repeat this for t steps, thus obtaining the required album summary.\n\nHow do we define loss in such a setting? First, since we already know the correct summary sentences, we can define a generation loss which is simply the sum of negative log likelihoods of the correct words. However, in addition to the words being similar, the story should be temporally coherent, i.e., the sentences themselves should be in a specific order. For this purpose, we apply a max-margin ranking loss as:\n\nThe total loss is just a linear combination of these two losses. This provides a framework for end-to-end training for the system."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/trends-in-semantic-parsing-part-2-bf12fd6301d6",
        "title": "Trends in Semantic Parsing \u2014 Part 2 \u2013 Explorations in Language and Learning \u2013",
        "text": "This paper was published in ACL 2011 [1], back when statistical methods were still being used for NLP tasks. But with the recent forays into generative models, I feel it has again become relevant to understand how such methods worked. The task of frame semantic parsing can be broken down into 3 independent steps:\n\nFrames essentially refer to a semantic representation of predicates (such as verbs), and their arguments are represented as clusters. For sake of convenience, we refer both of these structures as semantic classes. For example, in the sentences:\n\nHere, \u2018defeated\u2019 and \u2018secured a victory\u2019 both belong to the frame WINNING, while \u2018India\u2019 and \u2018Indian team\u2019 are grouped into the cluster labeled WINNER.\n\nThe authors proposed a generative algorithm which makes use of statistical processes to model semantic parsing. We can summarize the model as follows, for a particular sentence:\n\nThis is the essence of the algorithm. Basically we get a semantic frame from the PY process, and then generate the corresponding syntax from a Dirichlet process. This is done recursively, hence the need for a hierarchical PY process. For the details of the stochastic processes, you can look at their Wikipedia pages. For the root level parameters, a stick-breaking construction is used, but I am yet to look into the details of this method. However, I suppose this is similar to the broken-stick technique used to estimate the number of eigenvalues to retain in a principal component analysis."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/the-best-papers-at-iclr-17-9f5695e08532",
        "title": "The best papers at ICLR \u201817 \u2013 Explorations in Language and Learning \u2013",
        "text": "Understanding this paper from researchers at UC Berkeley requires a little background of neural programmer-interpreter (NPI) architectures, which can be found in the paper as well as in the ICLR \u201916 paper in which they were introduced. Basically, an NPI framework consists of a controller (such as an LSTM), which takes as input the environment state and arguments, and returns the next program pointer to be executed. In this way, given a set of sequences that an algorithm must follow to get to the output, the NPI can learn the algorithm itself. In the original paper, the authors learned to perform tasks such as adding, sorting, etc. using NPIs.\n\nIn this paper, the concept of recursion is added to the existing NPI framework, and this makes it capable of performing much more complex tasks such as quick sort. Formally, a function exhibits recursive behavior when it possesses two properties: (1) Base cases \u2014 terminating scenarios that do not use recursion to produce answers; (2) A set of rules that reduces all other problems toward the base cases. In the paper, the author describe how they construct NPI training traces so as to make them contain recursive elements and thus enable NPI to learn recursive programs.\n\nFurthermore, the authors show provably perfect generalization for their new architecture. The theorem states that for the same sequence of step inputs, the model produces the exact same step output as the target program it aims to learn.\n\nTo prove this, we again go to the notions of base case and recursive case. For example, in the addition task, the base case is always a set of small, fixed size step input sequences during which the LSTM state remains constant. So the base case is trivially true. The key in proving the recursive step is to construct the verification set well, so that the step inputs are neither too large so as to be outside the scope of evaluation, nor too small so that the semantics of the problem are not well defined. The actual verification process is simple and can be read in the paper."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/the-last-3-years-in-text-classification-8f408c043a79",
        "title": "The last 3 years in Text Classification \u2013 Explorations in Language and Learning \u2013",
        "text": "While working on my undergrad thesis on relation classification of biomedical text using deep learning methods, I quickly hacked together models in Tensorflow that combined convolutional and recurrent layers in various combinations. While some of these \u201cnetwork architectures\u201d worked superbly (even surpassing state-of-the-art results), I had no clue what was happening inside the model. To gain such an intuition, I read about 20 recent papers on text classification (starting with the first \u201cCNN for sentence classification\u201d paper by Yoon Kim) over the course of a week. Aside from an obvious enlightenment about why my architecture was working the way it was, I also gained valuable insight into how results are presented by experts like Yann LeCunn and Tommi Jaakkola (which would later help me in getting my CoNLL paper accepted as an undergrad).\n\nAnyway, so while reading these myriad of text classification papers, I subconsciously began organizing them under different heads, depending upon the kind of approach used. The common objective across each of these approaches was that they all wanted to model the structural information of the sentence into the sentence embedding. All of this was in March, and ever since, I have wanted to organize the notes I made from my readings into a formal article, so that others may benefit from the insights.\n\nSome background in CNNs and LSTMs is assumed."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/understanding-word-vectors-f5f9e9fdef98",
        "title": "Understanding word vectors \u2013 Explorations in Language and Learning \u2013",
        "text": "The idea is simple. We can treat each element in the vector as a parameter to be updated while training a neural network model. We start with a randomly initialized vector and update it at each iteration. This update is based on the vectors of the context (window) words. The hypothesis is that such an update would ultimately result in similar words having vectors which are closer to each other in the vector space.\n\nHere, I will describe the 2 most popular neural models \u2014 Word2Vec and GloVe.\n\nWord2Vec is actually the name of a tool which internally uses skip-gram or CBOW (continuous bag-of-words) with negative sampling. The objectives for both these models are quite similar, except a subtle distinction. In skip-gram, we predict the context words given the target word, and in CBOW, we predict the target word given the context words. In this article, I will limit my discussion to skip-gram with negative sampling (SGNS).\n\nSuppose we have a context window where w is the target word and c is one of the context words. Then, skip-gram\u2019s objective is to compute P(c|w), which is given as\n\nBasically, it is just a softmax probability distribution over all the word-context pairs in the corpus, directed by the cosine similarity. However, the denominator term here is very expensive to compute since there may be a very large number of context words. To solve this problem, negative sampling is used.\n\nGoldberg and Levy have explained the derivation for the objective function in SGNS very clearly in their note. I will try to provide a little intuition here.\n\nFor the word w, we are trying to predict the context word c. Since we are using softmax, this is essentially like a multi-class classification problem, where we are trying to classify the next word into one of N classes (where N is the number of words in the dictionary). Since N may be quite large, this is a very difficult problem.\n\nWhat SGNS does is that it converts this multi-classification problem into binary classification. The new objective is to predict, for any given word-context pair (w,c), whether the pair is in the window or not. For this, we try to increase the probability of a \u201cpositive\u201d pair (w,c), while at the same time reducing the probability of k randomly chosen \u201cnegative samples\u201d (w,s) where s is a word not found in w\u2019s context. This leads to the following objective function which we try to maximize in SGNS:\n\nOne grievance with skip-gram and CBOW is that since they are both window-based models, the co-occurrence statistics of the corpus are not used efficiently, thereby resulting in suboptimal embeddings. The GloVe model proposed by Pennington et al. seeks to solve this problem by formulating an objective function from probability statistics.\n\nAgain, the original paper is very pleasant to read (section 3 describes their model in detail), and it is interesting to note the derivation for the objective function:\n\nHere, X_ij is the count of the word pair (i,j) in the corpus. The weight function f(x) has 3 requirements:\n\nAgain, please read the paper for details."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/trends-in-semantic-parsing-part-1-ba11888523cb",
        "title": "Trends in Semantic Parsing \u2014 Part 1 \u2013 Explorations in Language and Learning \u2013",
        "text": "Vector semantics have been used extensively in all NLP tasks, especially after word embeddings (Word2Vec, GloVe) were found to represent the synonymy-antonymy relations well in real space.\n\nSimilar to word embeddings, we can try to obtain dense vectors to represent a sentence, and then find some way to obtain the formal representation from it. Ivan Titov (University of Edinburgh) has recently proposed a couple of models which use LSTMs [1] and Graph CNNs [2] for dependency-based SRL task.\n\nI will first explain the task. We work on datasets where the predicates are marked in the sentence, and the objective is to identify and label the arguments corresponding to each predicate. For instance, given the sentence \u201cMary eats an apple,\u201d and the predicate marked as EATS, we need to label the words \u2018Mary,\u2019 \u2018an,\u2019 and \u2018apple\u2019 as agent, NULL, and theme, respectively. Also, since a single sentence may contain multiple predicates, the same word may get different labels for each predicate. Essentially, if we repeat the process once for each predicate, out task effectively reduces to a sequence labeling problem.\n\nLSTM-based approach [1]: LSTMs (which are a type of RNNs that can preserve memory) have been used to model sequences since they were first introduced. In the first model, the sequence labeling is performed as follows.\n\nwhere the vectors in the dot product correspond to randomly initialized embeddings for the predicate lemma and the role, respectively.\n\nGCN-based approach [2]: In a second model, Graph Convolutional Networks (GCNs) have been used to represent the dependency tree for the sentence. In a very crude sense, a GCN input layer encodes the sentence into an mXn matrix based on its dependency tree, such that each of the n nodes of the tree is represented as an m-dimensional vector. Once such a matrix has been obtained, we can perform convolutions on it.\n\nIt is then evident that a one-layer GCN can capture information only about its immediate neighbor. By stacking GCN layers, one can incorporate higher degree neighborhoods. (Please look at Fig. 3 of the paper [2] for the model diagram.)\n\nGCNs and LSTMs are complementary. Why? LSTMs capture long-term dependencies well but are not able to represent syntax effectively. On the other hand, GCNs are built directly on top of a syntactic-dependency tree so they capture syntax well, but due to the limitation of fixed-size convolutions, the range of dependency is limited. Therefore, using a GCN layer on top of the hidden states obtained from a bi-LSTM layer would theoretically capture the best of both worlds. This hypothesis has also been corroborated through experimental results.\n\nEncoder-decoder model [3]: In this paper, the task is broadened into formal representation rather than SRL. If we consider the formal representation as a different language, this is similar to a machine translation problem, since both the natural as well as formal representations mean the same. As such, it might be interesting to apply models used for MT to semantic parsing. [3] does exactly this.\n\nAn encoder converts the input sequence to a vector representation and a decoder obtains the target sequence from this vector."
    },
    {
        "url": "https://medium.com/explorations-in-language-and-learning/metrics-for-nlg-evaluation-c89b6a781054",
        "title": "Metrics for NLG evaluation \u2013 Explorations in Language and Learning \u2013",
        "text": "This is by far the most popular metric for evaluating machine translation system. In BLEU, precision and recall are approximated by modified n-gram precision and best match length, respectively.\n\nModified n-gram precision: First, an n-gram precision is the fraction of n-grams in the candidate text which are present in any of the reference texts. From the example above, the unigram precision of A is 100%. However, just using this value presents a problem. For example, consider the two candidates:\n\nCandidate (i) has a unigram precision of 60% while for (ii) it is 75%. However, it is obvious that (ii) is not a better candidate than (i) in any way. To solve this problem, we use a \u201cmodified\u201d n-gram precision. It matches the candidate\u2019s n-grams only as many times as they are present in any of the reference texts. So in the above example, (ii)\u2019s unigrams \u2018on\u2019, \u2018machine\u2019, and \u2018learning\u2019 are matched only once, and the unigram precision becomes 37.5%.\n\nFinally, to include all the n-gram precision scores in our final precision, we take their geometric mean. This is done because it has been found that precision decreases exponentially with n, and as such, we would require logarithmic averaging to represent all values fairly.\n\nBest match length: While precision calculation was relatively simple, the problem with recall is that there may be many reference texts. So it is difficult to calculate the sensitivity of the candidate with respect to a general reference. However, it is intuitive to think that a longer candidate text is more likely to contain a larger fraction of some reference than a shorter candidate. At the same time, we have already ensured that candidate texts are not arbitrarily long, since then their precision score would be low.\n\nTherefore, we can introduce recall by just penalizing brevity in candidate texts. This is done by adding a multiplicative factor BP with the modified n-gram precision as follows.\n\nHere, c is the total length of candidate translation corpus, and r is the effective reference length of corpus, i.e., average length of all references. The lengths are taken as average over the entire corpus to avoid harshly punishing the length deviations on short sentences. As the candidate length decreases, the ratio r/c increases, and the BP decreases exponentially."
    }
]