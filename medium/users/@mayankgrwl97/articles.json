[
    {
        "url": "https://becominghuman.ai/back-propagation-in-convolutional-neural-networks-intuition-and-code-714ef1c38199?source=user_profile---------1----------------",
        "title": "Back Propagation in Convolutional Neural Networks \u2014 Intuition and Code",
        "text": "I have scratched my head for a long time wondering how the back propagation algorithm works for convolutions. I could not find a simple and intuitive explanation of the algorithm online. So, I decided to write one myself. Hope you enjoy!\n\nAndrej Karpathy wrote in his blog about the need of understanding back propagation coining it as a Leaky Abstraction\n\nThe following figure summarises the use of chain rule for the backward pass in computational graphs.\n\nHere is another illustration which talks about the local gradients.\n\nIf you understand the chain rule, you are good to go.\n\nWe will try to understand how the backward pass for a single convolutional layer by taking a simple case where number of channels is one across all computations. We will also dive into the code later.\n\nThe following convolution operation takes an input X of size 3x3 using a single filter W of size 2x2 without any padding and stride = 1 generating an output H of size 2x2. Also note that, while performing the forward pass, we will cache the variables X and filter W. This will help us while performing the backward pass."
    },
    {
        "url": "https://becominghuman.ai/understanding-and-coding-inception-module-in-keras-eb56e9056b4b?source=user_profile---------2----------------",
        "title": "Understanding and Coding Inception Module in Keras \u2013",
        "text": "A typical Convolutional neural network (CNN) is made up of stacked convolutional layers in combination with max pooling and dropout. For larger datasets such as Imagenet, deeper architectures are used to get better results and dropout is used to prevent overfitting.\n\nGoogLeNet achieved the new state of the art in the ImageNet Large-Scale Visual Recognition Challenge 2014. GoogLeNet was constructed by stacking Inception layers to create a deep convolutional neural network.\n\nIn this blog, I would describe the intuition behind the Inception module. I would also show how one can easily code an Inception module in Keras.\n\nIn a typical CNN layer, we make a choice to either have a stack of 3x3 filters, or a stack of 5x5 filters or a max pooling layer. In general all of these are beneficial to the modelling power of the network. The inception module suggests the use of all of them.\n\nThis means instead of adding a particular filter size layer, we add all 1x1, 3x3, 5x5 filters and perform convolution on the output from the previous layers. Since pooling has been essential for the success of current CNNs, the inception module also includes an additional pooling path.The output of all the filters are concatenated and passed on as input to the next layer.\n\nWe will build a simple architecture with just one layer of inception module using keras. Make sure you have already installed keras beforehand. We will train the architecture on the popular CIFAR-10 dataset which consists of 32x32 images belonging to 10 different classes.\n\nKeras has the functionality to directly download the dataset using the cifar10.load_data() function. Once downloaded the function loads the data ready to use.\n\nEach image is represented as 32x32 pixels each for red, blue and green channels. Each pixel has a value between 0\u2013255. Next, we normalize the values to 0\u20131.\n\nIn order to best model the classification model, we convert y_test and y_train to one hot representations in the form of a binary matrix.\n\nThe latest Keras functional API allows us to define complex models. In order to create a model, let us first define an input_img tensor for a 32x32 image with 3 channels(RGB).\n\nNow, we feed the input tensor to each of the 1x1, 3x3, 5x5 filters in the inception module.\n\nWe learn 64 1x1 filters on the input_img tensor and then we learn 64 3x3 filters on the tower_1 tensor. Similarly, we make tower_2, tower_3 tensors.\n\nNote: We provide input_img tensor to tower_2 and tower_3 as input so all the 3x3, 5x5 filters and the max pooling layers are performed on the same input.\n\nThe padding is kept same so that the output shape of the Conv2D operation is same as the input shape. So, the final output of each filter of tower_1, tower_2 and tower_3 is same. Thus we can easily concatenate these filters to form the output of our inception module.\n\nConcatenate operation assumes that the dimensions of tower_1, tower_2, tower_3 are the same, except for the concatenation axis.\n\nWe flatten the output to a one dimensional collection of neurons which is then used to create a fully connected neural network as a final classifier\n\nThus we obtain a fully connected neural network with final layer having 10 neurons one corresponding to each class.\n\nWe can now create the model\n\nNow we compile and fit the model with SGD optimizer\n\nWe can also store our model in a JSON file and store the results as HDF5 format.\n\nWe can calculate the accuracy of our model for evaluation"
    }
]