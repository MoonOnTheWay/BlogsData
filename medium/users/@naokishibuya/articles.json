[
    {
        "url": "https://towardsdatascience.com/having-fun-with-deep-convolutional-gans-f4f8393686ed?source=user_profile---------1----------------",
        "title": "Having Fun with Deep Convolutional GANs \u2013",
        "text": "This article show Deep Convolutional Generative Adversarial Networks \u2014 a.k.a DCGAN examples using different image data sets such as MNIST, SVHN, and CelebA.\n\nIn Understanding Generative Adversarial Networks, I used a simple GAN to generate images, and the results were merely good enough to prove the concept.\n\nBelow, I used DCGAN to generate the images. There are much less noises, and the shapes are better.\n\nThe generator of the simple GAN is a simple fully connected network.\n\nThe generator of the DCGAN uses the transposed convolution technique to perform up-sampling of 2D image size.\n\nWe can roughly consider the transposed convolution as a reverse operation of normal convolution. The generator network summary below shows that the multiple transposed convolutions increase the image size.\n\nThe final image shape is 28x28 with 1 channel which is the same as the original MNIST digit image size.\n\nFYI \u2014 my article Up-sampling with Transposed Convolution has more details on the concept of transposed convolution.\n\nThis DCGAN example uses The Street View House Numbers (SVHN) Dataset.\n\nThe generated images look pretty descent. If these images are mixed with the real SVHN images, I would not be able to tell which ones are which (I am not a good discriminator\u2026).\n\nThis network was much more difficult to train. No wonder why GANs are notorious for very hard to train.\n\nMy notebook on GitHub contains a few training scenarios with various hyper parameters. I experimented with the learning rate, the leaky ReLU\u2019s alpha and the Adam optimizer\u2019s momentum coefficient (beta 1). Adding / removing the batch normalization, and e.t.c.\n\nI learned some hyper parameter tuning ideas from How to Train a GAN? Tips and tricks to make GANs work. There is a presentation by the author on YouTube which I recommend watching before reading the paper.\n\nThe last (but not least) example uses the Large-scale Celeb Faces Attributes (CelebA) Dataset.\n\nI decided to resize the images into 32x32 as it was taking too long to train the network.\n\nThe generated images look a bit Frankenstein-ish. But look, it\u2019s generating faces from random noises. Isn\u2019t that something?\n\nThis generator network has the similar capacity with the SVHN example.\n\nOne thing I did differently was that I used a different kernel weight initializer as suggested in How to Train a GAN? Tips and tricks to make GANs work. It seems to have improved the quality of images but it is not clear by how much. In any case, the result was good enough to prove that it works. If you play with GANs, you may also experiment with the initializers.\n\nThe concept of GANs is not that hard to understand (i.e., Understanding Generative Adversarial Networks). But implementing them to produce quality images can be tricky. So I recommend anybody who wants to have better insights into GANs to make their hands dirty and tweak parameters.\n\nHaving said that, it takes a long time to train deep networks with tons of images. So, I hope this article and my notebooks in GitHub can provide you with some insights without sweat and tears.\n\n[3] How to Train a GAN? Tips and tricks to make GANs work"
    },
    {
        "url": "https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0?source=user_profile---------2----------------",
        "title": "Up-sampling with Transposed Convolution \u2013",
        "text": "If you\u2019ve heard about the transposed convolution and got confused what it actually means, this article is written for you.\n\nThe content of this article is as follows:\n\nThe notebook is available in my GitHub.\n\nWhen we use neural networks to generate images, it usually involves up-sampling from low resolution to high resolution.\n\nThere are various methods to conduct up-sampling operation:\n\nAll these methods involve some interpolation method which we need to chose when deciding a network architecture. It is like a manual feature engineering and there is nothing that the network can learn about.\n\nIf we want our network to learn how to up-sample optimally, we can use the transposed convolution. It does not use a predefined interpolation method. It has learnable parameters.\n\nIt is useful to understand the transposed convolution concept as it is used in important papers and projects such as:\n\nFYI \u2014 the transposed convolution is also known as:\n\nWe will only use the word transposed convolution in this article but you may notice alternative names in other articles.\n\nLet\u2019s use a simple example to explain how convolution operation works. Suppose we have a 4x4 matrix and apply a convolution operation on it with a 3x3 kernel, with no padding, and with a stride of 1. As shown further below, the output is a 2x2 matrix.\n\nThe convolution operation calculates the sum of the element-wise multiplication between the input matrix and kernel matrix. Since we have no padding and the stride of 1, we can do this only 4 times. Hence, the output matrix is 2x2.\n\nOne important point of such convolution operation is that the positional connectivity exists between the input values and the output values.\n\nFor example, the top left values in the input matrix affect the top left value of the output matrix.\n\nMore concretely, the 3x3 kernel is used to connect the 9 values in the input matrix to 1 value in the output matrix. A convolution operation forms a many-to-one relationship. Let\u2019s keep this in mind as we need it later on.\n\nNow, suppose we want to go the other direction. We want to associate 1 value in a matrix to 9 values in another matrix. It\u2019s a one-to-many relationship. This is like going backward of convolution operation, and it is the core idea of transposed convolution.\n\nFor example, we up-sample a 2x2 matrix to a 4x4 matrix. The operation maintains the 1-to-9 relationship.\n\nBut how do we perform such operation?\n\nTo talk about how, we need to define the convolution matrix and the transposed convolution matrix.\n\nWe can express a convolution operation using a matrix. It is nothing but a kernel matrix rearranged so that we can use a matrix multiplication to conduct convolution operations.\n\nWe rearrange the 3x3 kernel into a 4x16 matrix as below:\n\nThis is the convolution matrix. Each row defines one convolution operation. If you do not see it, the below diagram may help. Each row of the convolution matrix is just a rearranged kernel matrix with zero padding in different places.\n\nTo use it, we flatten the input matrix (4x4) into a column vector (16x1).\n\nWe can matrix-multiply the 4x16 convolution matrix with the 1x16 input matrix (16 dimensional column vector).\n\nThe output 4x1 matrix can be reshaped into a 2x2 matrix which gives us the same result as before.\n\nIn short, a convolution matrix is nothing but an rearranged kernel weights, and a convolution operation can be expressed using the convolution matrix.\n\nThe point is that with the convolution matrix, you can go from 16 (4x4) to 4 (2x2) because the convolution matrix is 4x16. Then, if you have a 16x4 matrix, you can go from 4 (2x2) to 16 (4x4).\n\nWe want to go from 4 (2x2) to 16 (4x4). So, we use a 16x4 matrix. But there is one more thing here. We want to maintain the 1 to 9 relationship.\n\nSuppose we transpose the convolution matrix C (4x16) to C.T (16x4). We can matrix-multiply C.T (16x4) with a column vector (4x1) to generate an output matrix (16x1). The transposed matrix connects 1 value to 9 values in the output.\n\nThe output can be reshaped into 4x4.\n\nWe have just up-sampled a smaller matrix (2x2) into a larger one (4x4). The transposed convolution maintains the 1 to 9 relationship because of the way it lays out the weights.\n\nNB: the actual weight values in the matrix does not have to come from the original convolution matrix. What important is that the weight layout is transposed from that of the convolution matrix.\n\nThe transposed convolution operation forms the same connectivity as the normal convolution but in the backward direction.\n\nWe can use it to conduct up-sampling. Moreover, the weights in the transposed convolution are learnable. So we do not need a predefined interpolation method.\n\nEven though it is called the transposed convolution, it does not mean that we take some existing convolution matrix and use the transposed version. The main point is that the association between the input and the output is handled in the backward fashion compared with a standard convolution matrix (one-to-many rather than many-to-one association).\n\nAs such, the transposed convolution is not a convolution. But we can emulate the transposed convolution using a convolution. We up-sample the input by adding zeros between the values in the input matrix in a way that the direct convolution produces the same effect as the transposed convolution. You may find some article explains the transposed convolution in this way. However, it is less efficient due to the need to add zeros to up-sample the input before the convolution.\n\nOne caution: the transposed convolution is the cause of the checkerboard artifacts in generated images. This article recommends an up-sampling operation (i.e., an interpolation method) followed by a convolution operation to reduce such issues. If your main objective is to generate images without such artifacts, it is worth reading the paper to find out more."
    },
    {
        "url": "https://towardsdatascience.com/understanding-generative-adversarial-networks-4dafc963f2ef?source=user_profile---------3----------------",
        "title": "Understanding Generative Adversarial Networks \u2013",
        "text": "If you see the above image and it does not make much sense, this article is written for you. I explain how GAN works using a simple project that generates hand-written digit images.\n\nI use Keras on TensorFlow and the notebook code is available in my Github.\n\nGAN (Generative Adversarial Network) is a framework proposed by Ian Goodfellow, Yoshua Bengio and others in 2014.\n\nA GAN can be trained to generate images from random noises. For example, we can train a GAN to generate digit images that look like hand-written digit images from MNIST database.\n\nA GAN has two parts in it: the generator that generates images and the discriminator that classifies real and fake images.\n\nThe input to the generator is a series of randomly generated numbers called latent sample. Once trained, the generator can produce digit images from latent samples.\n\nOur generator is a simple fully connected network that takes a latent sample (100 randomly generated numbers) and produces 784 data points which can be reshaped into a 28 x 28 digit image which is the size used by all MNIST digit images.\n\nThe summary output is as follows:\n\nWe use the tanh activation which is recommended in How to Train a GAN? Tips and tricks to make GANs work. It also means that we need to rescale the MNIST images to be between -1 and 1. More details are in my Github.\n\nWithout training, the generator produces garbage images only.\n\nTo train the generator, we need to train a GAN. Before talking about GAN, we shall discuss the discriminator.\n\nThe discriminator is a classifier trained using the supervised learning. It classifies whether an image is real (1) or not (0).\n\nWe train the discriminator using both the MNIST images and the images generated by the generator.\n\nIf the input image is from the MNIST database, the discriminator should classify it as real.\n\nIf the input image is from the generator, the discriminator should classify it as fake.\n\nThe discriminator is also a simple fully connected neural network.\n\nThe summary output is as follows:\n\nThe last activation is sigmoid to tell us the probability of whether the input image is real or not. So, the output can be any value between 0 and 1.\n\nWe connect the generator and the discriminator to produce a GAN.\n\nKeras has an easy way to connect two models as follows:\n\nThe structure of the network is shown below:\n\nNow that we know the generator, the discriminator and the GAN, we shall discuss how to train the generator.\n\nWhen we feed a latent sample to the GAN, the generator internally produces a digit image which is then passed to the discriminator for classification. If the generator does a good job, the discriminator returns a value close to 1 (high probability of the image being real).\n\nWe feed latent samples to the GAN while setting the expected outcome (label) to 1 (real) as we expect the generator to produce realistic image, and we expect the discriminator to say it is real or close to real.\n\nHowever, the generator initially produces garbage images, and the loss value is high. So, the back-propagation updates the generator\u2019s weights to produce more realistic images as the training continues. This is how we train the generator via training the GAN.\n\nThere is one catch in this process of training the generator via the GAN. We do not want the discriminator\u2019s weights to be affected because we are using the discriminator as merely a classifier.\n\nFor this reason, we set the discriminator non-trainable during the generator training.\n\nLet\u2019s not forget that we also need to train the discriminator as well so that it can do a good job as a classifier of real and fake images.\n\nWe train the discriminator and the generator in turn in a loop as follows:\n\nStep 2) Train the discriminator with the real MNIST digit images and the images generated by the generator to classify the real and fake images.\n\nStep 4) Train the generator as part of the GAN. We feed latent samples into the GAN and let the generator to produce digit images and use the discriminator to classify the image.\n\nThe loop should ideally continue until they are both trained well and can not be improved any further.\n\nThe result of the simple GAN is not outstanding. Some of them look pretty good but others are not.\n\nAs it turns out, training a GAN requires lots of hacks as per How to Train a GAN? Tips and tricks to make GANs work such as label smoothing and other techniques.\n\nThere are all sorts of empirical quirks. If I train the discriminator much faster than the generator, the generator gives up learning. In some case, the generator learns to deceive the discriminator and makes the discriminator unable to learn to classify properly.\n\nI tried different hacks and the below plot of the loss values is what I could achieve after about one day experiments (full details in my Github).\n\nThe generator should have lower loss values than the above. I believe we can improve the performance if we use more complex networks like DCGAN (Deep Convolutional GAN).\n\nThe point of this article is to show how GAN works in principle using the simple GAN example. Once you know how, it should be easier to understand other GAN articles and implementations.\n\nMoreover, there are many kinds of GANs (the whole list) and new types of GANs are invented as we speak. So, GANs do work and many people are researching GANs."
    },
    {
        "url": "https://towardsdatascience.com/how-to-reduce-image-noises-by-autoencoder-65d5e6de543?source=user_profile---------4----------------",
        "title": "How to Reduce Image Noises by Autoencoder \u2013",
        "text": "An autoencoder has two parts: an encoder and a decoder.\n\nThe encoder reduces the dimensions of input data so that the original information is compressed.\n\nThe decoder restores the original information from the compressed data.\n\nThe autoencoder is a neural network that learns to encode and decode automatically (hence, the name).\n\nOnce learning is done, we can use the encoder and decoder independently.\n\nSo, an autoencoder can compress and decompress information. Then, can we replace the zip and unzip command with it?\n\nAutoencoders are data specific and do not work on completely unseen data structure. For example, an autoencoder trained on numbers does not work on alphabets.\n\nAnother limitation is that the compression by an autoencoder is lossy. As such, it does not perfectly restore the original information.\n\nThen, what can we do with it?\n\nAutoencoders can be useful for many different things. In this article, I show you how to use an autoencoder for image noise reduction.\n\nNB: the code in this article is based on Building Autoencoders in Keras by Francois Chollet and Autoencoder Examples by Udacity. The notebook code is available in my Github.\n\nWe use MNIST which is a well known database of handwritten digits. Keras has MNIST dataset utility. We can download the data as follows:\n\nThe shape of each image is 28x28 and there is no color information.\n\nThe below shows the first 10 images from MNIST database.\n\nWe start with a simple autoencoder based on a fully connected layers. One hidden layer handles the encoding, and the output layer handles the decoding.\n\nEach of the input images is flatten to an array of 784 (=28\u00d728) data points. This is then compressed into 32 data points by the fully connected layer.\n\nThen, we decode the encoded data to the original 784 data points. The sigmoid will return values between 0 and 1 for each pixel (intensity).\n\nThis whole processing becomes the trainable autoencoder model.\n\nThe autoencoder network compresses and decompresses. So, what\u2019s the point? We talk about that later on.\n\nWe preprocess the MNIST image data so that image data are normalized between 0 and 1.\n\nWe also split the train data into a train set and a validation set.\n\nWe train the autoencoder which compress the input image and then restore to the original size. As such, our training data and label data are both the same image data.\n\nBy training an autoencoder, we are really training both the encoder and the decoder at the same time.\n\nWe can build an encoder and use it to compress MNIST digit images.\n\nWe can confirm the 784 pixel data points are now compressed into 32 data points.\n\nLet\u2019s also build a decoder so that we can decompress the compressed image to the original image size. The decoder takes 32 data points as its input (the size of encoded data).\n\nThe result is as follows. The first row has the original images. The second row has the restored images.\n\nAs can be seen, the decoded images do not completely restore the original image.\n\nWe could add more layers to make the network deeper to improve the performance. But since we are working on images, we could make use of convolutional neural network to improve the quality of compression and decompression.\n\nLet\u2019s examine the layers of the convolutional autoencoder.\n\nThe will repeats the rows and columns twice. Effectively reversing the effect of the .\n\nIn short, the is doubling the height and width.\n\nIf you look closely, you may have noticed that the uses the that reduces the height and width to (14,14) so that we can up-sample again to (28,28) which is the original size of MNIST images.\n\nNow, we reshape the image data to the format the convolutional autoencoder expects for training.\n\nWe just want to see the quality of compression/decompression, for which we do not need to build separate encoder and decoder models. So, we simply feed forward test images to see how the restored digits look like.\n\nAlthough not perfect, the restored digits look better than the ones restored by the simple autoencoder.\n\nLet\u2019s try noise reduction effect using the convolutional autoencoder. We add random noises to the MINST image data and use them as input for training.\n\nWe train a new autoencoder with the noisy data as input and the original data as expected output.\n\nDuring the training, the autoencoder learns to extract important features from input images and ignores the image noises because the labels have no noises.\n\nLet\u2019s pass the noisy test images to the autoencoder to see the restored images."
    },
    {
        "url": "https://towardsdatascience.com/transforming-a-cat-into-an-art-by-convolutional-layers-5a7c0b9b947c?source=user_profile---------5----------------",
        "title": "Transforming a Cat into an Art by Convolutional Layers",
        "text": "The idea of artistic style transfer is to apply the style of one image to another image while keeping the original image content recognizable.\n\nThis article explains how to do the artistic style transfer step by step.\n\nThe code is based on Francois Chollet\u2019s Neural Style Transfer notebook code.\n\nWe use the following libraries:\n\nWe load the original image (which I obtained from Kaggle).\n\nWe load the style image which is The Great Wave Off Kanagawa by Hokusai Katsushika.\n\nThe image size of the style image is different from the content image. As we perform feature by feature comparison, we need to resize the style image to the same as the content image size.\n\nWe use VGG16. It is pre-trained and available in Keras. We need to prepare input into the format expected by VGG16.\n\nWe need three inputs:\n\nWe concatenate the three inputs into one input tensor and give to the VGG16 model.\n\nThe 's shape is (3, height, width, channels or filters).\n\nWe generate the by manipulating the copy of the content image (i.e., cat image) so that it has the style of the style image (i.e., Hokusai).\n\nThe VGG16 model takes the as input. The is set to as we are only interested in the convolutional layers which are the feature extraction layers.\n\nWe are ready to feed forward these three inputs in VGG16, and then compare generated feature values by using multiple cost functions so that we can adjust the generated image input to apply the style while keeping the content intact.\n\nThe available convolutional layers are shown below:\n\nWe pick one of the higher layers for the representation of the content of the image such as shapes. We want to keep the generated image close to the content image. In other words, we want the generated image to have the same content (i.e., shapes) as the original image.\n\nAs such, we define a cost function to keep the content similarity between the content image and the generated image. The content cost is the sum of squared differences between the generated features and the content features.\n\nLet\u2019s use the block5_conv2 as the content representation.\n\nThis is the most difficult part of style transfer. To define the style loss function, we use the Gram matrix which may not be so intuitive for everyone (not to me anyhow).\n\nSuppose a matrix C contains n olumn vectors c1, c2, \u2026, cn, the Gram matrix of C is transpose(C) matrix multiplied by C which is a n\u00d7n matrix. A Gram matrix tells us how column vectors are related to each other. Say , if two vectors has positive dot product, they are pointing to the similar direction. If the dot product is zero, they are orthogonal. If it is negative, they point to opposite directions.\n\nWe use the Gram matrix to see how each filter within a layer relate to each other.\n\nFor example, the block1_conv1 layer has 64 filters. We flatten all 64 filters (matrices) into 64 vectors: v1, v2, \u2026, v64. Then, we calculate the dot products of every combination of the vectors. Each of the dot product tells us how strongly the filter combination (i.e., their activations) is related. This produces a Gram matrix of size 64 by 64.\n\nWhen a style image goes through VGG16 convolutional layers, each layer produces a list of feature matrices. For example, the block1_conv1 produces 64 matrices, each of which is a feature matrix produced by applying a convolution operation on the input image. The same goes for the generated image.\n\nIn short, each layer contains a list of feature matrices. For the style image, we say we have style features in each layer, and for the generated image, we have generated features in each layer. They are nothing but a list of matrices generated as part of activations for each filter in each layer.\n\nWhen style features and generated features have similar Gram matrices, they have similar styles because the Gram matrices are generated from the flatten vectors of feature matrices. If they have similar Gram matrices, the filters are having similar relationships in the same layers.\n\nSo, we calculate two Gram matrixes for each layer: one for the style features and the other for the generated features and calculate the sum of the sqaured differences (with some denominators to adjust the values).\n\nFor example, the block1_conv1 layer, we calculate one Gram matrix for the style features and another Gram matrix for the generated features. We calculate the sum of the squared element wise differences between the two Gram matrices (divided by a denominator which depends on the size of the feature matrices). This tells us how similar the style features and the generate features are in the block1_conv1 layer.\n\nWe choose a list of layers as the representatives of the image style. Then, for each layer, we calculate the Gram matrices: one for the style filters and one for the generated fitlers. We add them up to calculate the style loss.\n\nIf the style loss is low, the two image has similar style.\n\nWe want to make the generated image somewhat smooth, and not jagged. The variation loss is defined based on the difference between the neighboring pixel values to avoid sudden jumps in pixel values.\n\nWe calculate the total variation loss from the generated image.\n\nBased on the combination of the three loss values, we calculate the gradients of it as the generated image as input so that we can nudge the image to reduce the overall cost. As a result, the generated image keeps the content image while applying the style of the style image.\n\nTo display the generated image, we need to do the reverse of the preprocessing.\n\nNow, we are ready to show the generated image.\n\nThe effect is not that obvious here. We should play with the hyper-parameters.\n\nLet\u2019s put all together in one function so that we can do some experiments. Please see the Github for the actual code details.\n\nLet\u2019s use only the style cost function.\n\nDoes the cat look like completely soaked? Or behind a wet glass window? Something waterly for sure.\n\nA bit too much of melting look? Let\u2019s add the content loss to avoid deformation.\n\nLet\u2019s add variation loss as well to make the image smooth like a paint drawing.\n\nIt is hard to predict how the generated image looks like before running the experiments. If you play with the hyper-parameters, be careful not to make them too big which may make the pixel values too small and make the picture completely black.\n\nIn case you are wondering, I drew this image for my soundcloud music \u201cHey Man!\u201d. Let\u2019s use it as the style image.\n\nI chose the lower layers as the style thinking that the color and texture will be captured there. I adjusted other parameters so that it will not go completely black.\n\nThe cat looks angrier. Well, it wasn\u2019t as interesting as I was hoping. I guess the choice of the style image is quite important.\n\nOne more experiment \u2014 instead of starting with the content image, let\u2019s start with a noise image.\n\nIt will take a longer time before seeing any shape from the content image but the end result is quite interesting.\n\nHere is another one with Hokusai style.\n\nI am quite happy with this artistic result. What do you think?"
    },
    {
        "url": "https://towardsdatascience.com/do-filters-dream-of-convolutional-cats-5cd5d1f7e2ff?source=user_profile---------6----------------",
        "title": "Do Filters Dream of Convolutional Cats? \u2013",
        "text": "A convolutional neural network typically has multiple convolutional layers (hence, the name).\n\nConceptually, we understand each convolutional layer extracts spatial features from their inputs. Earlier layer detects low-level features like color, texture, lines, curves, etc. Later layers detect higher abstraction like eyes, tail, etc.\n\nBut we can\u2019t see them visually. Or, can we?\n\nIn this article, I demonstrate how to use a pre-trained convolutional neural network to see what kind of input images strongly activate filters in convolutional layers.\n\nThis notebook code is largely based on the blog article How convolutional neural networks see the world by Francois Chollet.\n\nI use the pre-trained VGG16 available in Keras. The details of VGG 16 is in Very Deep Convolutional Networks for Large-Scale Image Recognition by Karen Simonyan, Andrew Zisserman.\n\nThis gives the following output:\n\nThe output shapes all begin with (None, None, None). It is because we didn\u2019t specify the input shape. In other words, the model can handle any image shape.\n\nThe last dimension of each output shape is the number of filters (channels). For example, the layer block5_conv1 has 512 filters (indexed from 0 to 511).\n\nTypically, the input to the VGG16 model is an image to classify such as cats and dogs. Here, however, we use a randomly generated noise image and feed it to VGG16 model to calculate filter activations and their gradients.\n\nWe can calculate the activations of a filter with the random image. Most likely, the activations are not very strong.\n\nHowever, using the gradients, we adjust the image data to make the activations stronger. In other words, we nudge the input image pixel values to increase the activation using the gradients as our guide. It is a gradient ascent process in that we maximize the activation by adjusting the input image.\n\nAfter some repetition of this process, the output tells us what kind of image triggers the filter to activate strongly, through which we can have some insight into what kind of things the filter detects.\n\nFor details, please see the notebook available in my GitHub. Or Francois Chollet\u2019s original code.\n\nFor example, the first filter of the block4_conv1 layer generated the following image:\n\nIt seems the filter likes an arc shape?\n\nLet\u2019s pick some layers to examine generated images. I will use the first 20 filters of each convolutional layers as examples:\n\nThis is the first layer in VGG16. It seems to detect colors and textures. I do not see much shapes here.\n\nI see more shapes and patterns here. They are definitely higher level of abstractions than the previous two examples.\n\nMore complex shapes are observed here.\n\nEven more complex shapes are here.\n\nIf we throw a cat into this experiment and let a filter to nudge the image, what do we get?\n\nThis is a similar idea used in Inceptionism: Going Deeper into Neural Networks.\n\nI used the first filter in block5_conv3 layer on a cat image from Dogs vs. Cats Redux: Kernels Edition.\n\nDid the filter detect the cat is angry? Maybe not\u2026\n\n[2] How convolutional neural networks see the world: Francois Chollet\n\n[3] Inceptionism: Going Deeper into Neural Networks: Alexander Mordvintsev, Christopher Olah and Mike Tyka"
    },
    {
        "url": "https://towardsdatascience.com/pipelines-mind-maps-and-convolutional-neural-networks-34bfc94db10c?source=user_profile---------7----------------",
        "title": "Pipelines, Mind Maps and Convolutional Neural Networks",
        "text": "The first time I tried training a convolutional neural network myself, I lost track of what changes made the network better or worse. I was overwhelmed by the number of decisions I had to make and the infinite possibilities that I had to explore.\n\nI now use pipelines to experiment with different scenarios, and mind maps to see what I have tried and what else I can try.\n\nIn this article, I will talk about how I use pipelines and mind maps for training convolutional neural networks, using the German Traffic Sign image classification project as an example.\n\nThe sections are as follows:\n\nI do the exploratory data analysis solely to come up with a pipeline plan. Let\u2019s quickly look at the process in three steps:\n\nThe objective is to classify the traffic sign images from the German Traffic Sign image classification into the predefined classes.\n\nVisualization helps me to intuitively understand what I\u2019m dealing with.\n\nI printed the label and shape for each of the randomly selected images. The images come in different sizes.\n\nThe list doesn\u2019t have to be perfect as I can have as many pipeline objects as I want later on.\n\nThis is my pipeline plan. It has two meanings.\n\nFirst of all, it tells me what I have done and what I need to do. Next, it\u2019s a blue print for the actual Pipeline object that I\u2019m going to build.\n\nOut of the 39,209 training images, I reserved 8,000 (20%) for validation. I did this before applying augmentation so that the validation set has original images only.\n\nThat\u2019s it for the exploratory data analysis.\n\nSome people may find it odd that I don\u2019t spend more time on the exploratory data analysis.\n\nThis is because there is no end to it. If I immediately work on ideas as they pop up, I\u2019m letting random thoughts to control my progress. I could easily spend days without producing any results, which can be very discouraging.\n\nOnce I build the first pipeline working, I can experiment more systematically with ideas, and quantitatively measure the effectiveness of changes. This is why I want to finish the initial exploratory data analysis as quick as possible.\n\nI worked on the model first.\n\nThe first choice should be a pre-trained network that works for the same kind of problem (i.e. traffic sign classification). I could use Transfer Learning to reuse the pre-trained model for my project. This would save a lot of time.\n\nThe next choice is to find a well-known model built for the similar purpose, and adapt it to my project. A good network architecture is very hard to come up with. So, this can save a lot of time, too.\n\nThe last resort is to build a network from scratch which can be very time-consuming and risky (i.e., I may not be able to complete the project on time).\n\nI chose to use LeNet by Yann LeCun. It is a convolutional neural network designed to recognize visual patterns directly from pixel images with minimal preprocessing. It can handle hand-written characters very well.\n\nI adapted LeNet to this project which became the network to be used in the first pipeline.\n\nI used Tensorflow and Scikit-Learn\u2019s Pipeline framework to build pipelines.\n\nFirst, I wrote a simple class to easily build a convolutional neural network like below:\n\nHaving such a class was critical for me as I was planning to make many network objects with various configurations.\n\nOnce a network object is created, I use Scikit-Learn\u2019s Pipeline framework to make my network object an estimator (more on this later).\n\nI defined a transformer object to handle the image loading and resizing.\n\nThen, I created a new Pipeline object that combined the transformer (loader) and the estimator (network1).\n\nI can put as many transformer objects as I want into a Pipeline object.\n\nI can train and evaluate the Pipeline object.\n\nFor the first pipeline, I used 5 epochs (50,000 randomly selected samples per epoch) for training.\n\nIt is very likely that the network will see the same images more than once, which can cause overfitting. This will be addressed with augmentation later on.\n\nThe evaluation score is based on the 8,000 validation set.\n\nThis bare-bone pipeline is already performing well. This is encouraging.\n\nIt seems to be overfitting a bit but I could probably improve it by adding regularization. Should I do that now?\n\nIf I change the network while building the preprocessing, it will be harder to see what affects which. This is where I used to lose track of changes and improvements.\n\nI should just note down my observations for later review and move on.\n\nGenerating more image variation is important especially for the minor classes.\n\nThen, I made a new Pipeline object for training and validation.\n\nThere is no overfitting but the performance is much worse. I might have done the augmentation incorrectly.\n\nThis pushed me into thinking: \u201cHow much rotation should I have applied? 10 or 15? How much brightness change? etc, etc..\u201d\n\nCan I experiment with different values?\n\nIt is ok for me to play with the parameters as long as I\u2019m not changing anything in the other boxes of the pipeline plan.\n\nHaving said that, I should examine the result more closely. The training accuracy was similar to the validation accuracy. It indicates these two sets are not completely different in nature.\n\nMoreover, the worse performance shouldn\u2019t be surprising since the model needs to learn from much more training images. The augmentation simply exposed the weakness of the pipeline and/or the model.\n\nThe reason for the bad performance is somewhere else.\n\nThe pipeline objects tested so far did not have any data normalization.\n\nAny machine learning lecturer will tell you that you need to do data normalization before feeding data into your model. But how so?\n\nI wanted to measure how much improvement those normalization can bring.\n\nI created a new Pipeline object for each case.\n\nThe below is 5th epoch result for each case:\n\nis the winner. It\u2019s nice to see the normalization made such a huge difference.\n\nThe images are loaded in RGB format but there are many different color spaces.\n\nI created one Pipeline object for each color space. I used OpenCV function ( ) to convert RGB images into other color spaces.\n\nNote: Gray scale images have only 1 channel, which required the input shape of the input layer to be changed as follows:\n\nThe following is the 5th epoch results:\n\nThe Gray scale and XYZ were both performing about the same as RGB (no conversion). The rest was worse. So, I decided to continue with RGB.\n\nI completed all the boxes in the pipeline plan.\n\nIn doing so, I made 4 different Pipeline objects:\n\nI\u2019ll be using the third pipeline to experiment more with the network design.\n\nMy second network has more filters and neurons.\n\nThe performance is much better than the first network.\n\nI do this for almost all networks I tried. It\u2019s insightful to see how the learning curve differs for various network configurations.\n\nAs this is a classification problem, it could be useful to check the confusion matrix as well.\n\nThe performance is good across the board but some of them are less so. The details for each class is as follows:\n\nI kept creating new network objects to try out different network configuration.\n\nEach network object is trained and evaluated using a pipeline object. I also check the learning curve and confusion matrix for each of them. I noted down my observations for each cases.\n\nFor more details, please see the Jupyter notebook on my Github.\n\nAs I experimented with more and more ideas, it became harder and harder for me to remember what I had tried.\n\nAfter I had worked on the project for more than a few days, I was asking this question too often as I had to scroll up and down in my Jupyter notebook to find answers.\n\nI decided to draw a graph of the important things I tried.\n\nI was able to quickly look up what I had tried, and come up with ideas on what else I may try.\n\nAs of this writing, I re-looked at the map and found it funny that my brain started telling me that I should\u2019ve also tried this and that. It\u2019s a powerful tool.\n\nIf an idea is working, I kept expanding on it. It\u2019s like a greedy search algorithm or DFS (depth first search) but it\u2019s more free style than that.\n\nWhen I scrapped an idea completely, I marked them with \u201cX\u201d. It\u2019s usually because of the bad performance result.\n\nI find it useful to keep track of \u201cT\u201d (training accuracy) and \u201cV\u201d (validation accuracy) next to the network name. I can see how each network was doing and check where overfitting happens.\n\nI don\u2019t need to keep updating the mind map. When I feel a bit lost or need some direction, I update it, look at it and let my mind speak.\n\nI don\u2019t need to make it look beautiful. It\u2019s just a map showing where I am.\n\nI don\u2019t need to put all the experiments in a map. I just put the ones around the most successful path. Sometimes, I shortened it by combining multiple changes. It gives me more space to grow ideas. It\u2019s better to keep the map in one sheet of paper for a quick glance.\n\nI use it to find information quickly and get some inspiration.\n\nI\u2019ve done with the project but it\u2019s far from the end.\n\nI can still find many scenarios that I haven\u2019t tried in the mind map. I could go back and do more experiments thanks to the pipeline mechanism.\n\nIf I want to replace the network with a completely new architecture like GoogLeNet, I can do so by using a new Pipeline object, and draw a new mind map around that network. I\u2019m not stuck with one particular network architecture.\n\nI\u2019m in control with pipelines and mind maps.\n\nThat\u2019s it for now. I hope you find it useful."
    },
    {
        "url": "https://towardsdatascience.com/introduction-to-udacity-self-driving-car-simulator-4d78198d301d?source=user_profile---------8----------------",
        "title": "Introduction to Udacity Self-Driving Car Simulator \u2013",
        "text": "Udacity recently made its self-driving car simulator source code available on their GitHub which was originally built to teach their Self-Driving Car Engineer Nanodegree students.\n\nNow, anybody can take advantage of the useful tool to train your machine learning models to clone driving behavior.\n\nYou can manually drive a car to generate training data, or your machine learning model can autonomously drive for testing.\n\nIn the main screen of the simulator, you can choose a scene and a mode.\n\nFirst, you choose a scene by clicking one of the scene pictures. In the above, the lake side scene (the left) is selected.\n\nNext, you choose a mode: Training Mode or Autonomous Mode. As soon as you click one of the mode buttons, a car appears at the start position.\n\nIn the training mode, you drive the car manually to record the driving behavior. You can use the recorded images to train your machine learning model.\n\nTo drive the car, use the following keys:\n\nIf you like using mouse to direct the car, you can do so by dragging:\n\nTo start a recording your driving behavior, press on your keyboard.\n\nYou can press again to stop the recording.\n\nFinally, use to exit the training mode.\n\nYou can see the driving instructions any time by clicking CONTROLS button in the top right of the main screen.\n\nIn the autonomous mode, you are testing your machine learning model to see how well your model can drive the car without dropping off the road / falling into the lake.\n\nAlthough the driving in the above video may not be the best ever possible, it is quite rewarding to watch a trained model autonomously driving the car without going off the road.\n\nTechnically, the simulator is acting as a server where your program can connect to and receive a stream of image frames from.\n\nFor example, your Python program can use a machine learning model to process the road images to predict the best driving instructions, and send them back to the server.\n\nEach driving instruction contains a steering angle and an acceleration throttle, which changes the car\u2019s direction and the speed (via acceleration). As this happens, your program will receive new image frames at real time.\n\nYou\u2019ll need the following:\n\nUnity is a game development environment in which Udacity self-driving car simulator is built.\n\nYou can install it as follows:\n\nThe Udacity self-driving car simulator uses Git LFS to manage large files.\n\nThe above image is from https://git-lfs.github.com. The below is a quote from the site:\n\nFor Mac, you can use to install it:\n\nOnce installed, you need to enable it for Git on your machine.\n\nFor Windows and Linux, please see https://git-lfs.github.com for details.\n\nNote: I\u2019m assuming you already have the command in your environment. If not, please follow the instruction to install before installing .\n\nThis Github project contains a Unity project for building the simulator.\n\nIf you want to modify the scenes in the simulator, you\u2019ll need to deep dive into the Unity projects and rebuild the project to generate a new executable file.\n\nFor this, I suggest that you first read the README.md in the Self-Driving Car Simulator GitHub to learn the basics of how to navigate in the Unity project.\n\nFor more details on Unity itself, please visit https://unity3d.com.\n\nYou can try a predefined model to see how a deep learning model drives autonomously.\n\nYou can choose a different image quality in the simulator startup screen to see how it affects the car\u2019s driving.\n\nThe has more details of the Deep Learning model. You can modify the model parameters and train it to generate different model files using the saved train data from the Training Mode."
    },
    {
        "url": "https://towardsdatascience.com/raspberry-pi-3-for-the-first-time-50634b115620?source=user_profile---------9----------------",
        "title": "Raspberry Pi 3 for the First Time \u2013",
        "text": "Rasberry Pi is a small computer on which you can install OS (i.e., Linux). In other words, it has no pre-installed OS. You\u2019ll need to install it by yourself if you want it to do something useful.\n\nI think it\u2019s fun to do that but it was a bit tricky for the first time. So, my motivation in this article is to explain how to set up a Raspbian \u2014 a Debian based Linux distribution customized for Rasberry Pi in the following 4 simple steps:\n\nI\u2019m assuming you have a computer with Windows, Linux or Mac, and a SD card reader attached or built-in to your computer. Other than that, you\u2019ll at minimum need the following:\n\nLet\u2019s look at each component in details but focusing only on what we need to know.\n\nAs mentioned earlier, Raspberry Pi is a computer and it has the following interfaces:\n\nThere are different packages of Raspberry Pi, some of which with extra stuff like a touch screen and expansion board. You can also buy a plastic protector so that you won\u2019t touch the eletrical components while connecting cables and avoid dusts accumulating on your Raspberry Pi.\n\nSo, you\u2019ll need to shop around to see what you want. You can buy it online from Amazon, eBay or go to physical shops to check out. I bought it from element14 online store that is a barebone Raspberry Pi 3.\n\nThe cup on the left is for size comparison (not part of Raspberry Pi 3 package in case you are wondering).\n\nThis is where you install OS and all the softwares. 8 GB is the minimum size required. Mine is 64GB. But there is a catch when you format a SD card with more than 32GB (details on this later).\n\nAs mentioned earlier, you\u2019ll need a SD card reader connected or built-in to your computer. My Mac Book Pro from 2012 has a built-in SD card reader. But if your computer does not have one, you\u2019ll need an external SD card reader connected to your computer. Later, we\u2019ll format the SD card and install softwares on it.\n\nI don\u2019t have an external computer monitor at home so I used a TV which has HDMI slots. I have spare HDMI cables at home so I used one of them.\n\nAt home, I have a box full of unwanted cables in that I found Micro USB cables.\n\nI also had a power adapter with an USB interface. If not, I could plug the normal USB side into my Mac Book Pro or an external battery power bank for my mobile phone. As long as you can take power supply from there, it\u2019s ok.\n\nNote: It should be a 5-volt 1 amp power supply. Usually, a power adapter has a seal with those details. Look for \u201coutput\u201d section.\n\nAny keyboard and mouse will do as long as they have a USB interface. I used a logitech\u2019s USB keyboard and mouse.\n\nBut don\u2019t hook up any cables yet. We need to make an OS installer on micro SD card to boot up your Raspberry Pi 3 with it.\n\nRaspberry Pi 3 uses a micro SD card for storage (OS, libraries and user programs). You need at least 8 GB storage on your micro SD card. If you do, place your micro SD card into the full sized SD card adapter and stick it into the SD card reader (built-in or attached to your computer).\n\nThen, all you need is the following two steps to make a bootable OS installation disk.\n\nSDFormatter is a tool available from SD association. You can use it to format your micro SD card to FAT format if it\u2019s 32 GB or less.\n\nOtherwise, it will use exFAT format which does not work with Raspberry Pi (i.e., it will not boot up).\n\nMore details are available here from Raspberry Pi website.\n\nAs my micro SD card is 64GB, I used Disk Utility, one of the standard tools available on macOS to format it with FAT format.\n\nI named my micro SD card as NOOBS. Yours may have a different name.\n\nNOOBS is available for download from here on the Raspberry Pi website. Download it and then extract it.\n\nAll you need is to copy the contents of the extracted folder (not the folder itself) to your micro SD card.\n\nOnce again, you need to copy the contents of the folder (but not the folder itself) to your FAT formatted micro SD card.\n\nOnce the copy is done, safely remove the SD card from your computer (or from your SD card reader). Now, we are ready to boot Raspberry Pi 3.\n\nUsing the micro SD card, we are going to boot up your Raspberry Pi 3.\n\nFirst of all, be careful not to touch any parts on Raspberry Pi 3. As mentioned earlier, you might want to put a plastic protector before you begin this phase.\n\nReady? Place your micro SD card (which has been formatted and has the NOOBS files copied on it) into Raspberry Pi 3\u2019s micro SD card slot.\n\nThere won\u2019t be any clicking sound or anything while attaching the micro SD card to Raspberry Pi 3. If it looks like the above image, it should be ok.\n\nNow, connect all your cables:\n\nYou might have noticed that Raspberry Pi has no power-on button. It will start up booting as soon you connect it to a power supply via your micro USB power supply cable.\n\nIn my place, the power source on the wall has a power-on button. So, I leave it turned off before connecting the power cable to Raspberry Pi 3. Once connected, I made sure that Raspberry Pi is not touching any other metals or things that can carry electricity. After that, I turned the power on (on the wall, that is).\n\nNote: you can also use a mobile power bank to supply electricity to Raspberry Pi or connect it to your computer\u2019s USB port.\n\nIf all is good, you should start seeing the Raspberry Pi start up screen on your monitor (or TV).\n\nIt asks you which software you want to install. You see only Raspbian by default which is fine for our purpose.\n\nIf you specify your wifi details in the Wifi networks section, you\u2019ll get more choices as shown below:\n\nBut we only need Raspbian for now. So, we continue with the default choice. The installation process will take a while.\n\nIt shows you some of the feature highlights on Raspbian so you want to watch it at least once. Once it\u2019s done, you have Raspbian on your Raspberry Pi 3!\n\nYou should enjoy this moment of triumph and explore the menu items and tools. There are default python packages as well."
    },
    {
        "url": "https://towardsdatascience.com/finding-lane-lines-on-the-road-30cf016a1165?source=user_profile---------10----------------",
        "title": "Finding Lane Lines on the Road \u2013",
        "text": "In this project, I used Python and OpenCV to find lane lines in the road images. Full source codes are available on my Github.\n\nThe following techniques are used:\n\nFinally, I applied all the techniques to process video clips to find lane lines in them.\n\nThe test images are shown below.\n\nI use these images to test my pipeline (a series of image processing) to find lane lines on the road.\n\nLines are in white or yellow. A white lane is a series of alternating dots and short lines, which we need to detect as one line.\n\nThe images are loaded in RGB color space. Let\u2019s try selecting only yellow and white colors in the images using the RGB channels (ref: RGB Color Code Chart).\n\nIt looks pretty good except the two in which the yellow lines are not clear due to the dark shade from the tree on the left.\n\nUsing , we can convert RGB image into different color space. For example, HSL and HSV color space.\n\nHow does it look when RGB images are converted into HSV color space?\n\nThe yellow lines are very clear including the ones under the shades but the white lines are less clear.\n\nHow does it look like when images are converted from RGB to HSL color space?\n\nBoth the white and yellow lines are clearly recognizable. Also, the yellow lines under the shades are clearly shown.\n\nLet\u2019s build a filter to select those white and yellow lines. I want to select particular range of each channels (Hue, Saturation and Light).\n\nThe combined mask filters the yellow and white lines very clearly.\n\nThe Canny edge detector was developed by John F. Canny in 1986.\n\nWe want to detect edges in order to find straight lines especially lane lines. For this,\n\nLet\u2019s take a look at each step in details.\n\nThe images should be converted into gray scaled ones in order to detect shapes (edges) in the images. This is because the Canny edge detection measures the magnitude of pixel intensity changes or gradients (more on this later).\n\nHere, I\u2019m converting the white and yellow line images from the above into gray scale for edge detection.\n\nWhen there is an edge (i.e. a line), the pixel intensity changes rapidly (i.e. from 0 to 255) which we want to detect. But before doing so, we should make the edges smoother. As you can see, the above images have many rough edges which causes many noisy edges to be detected.\n\nI\u2019m using a technique called Canny edge detection which make use of the fact that edges has high gradients (how sharply image changes \u2014 for example, dark to white).\n\nWhen finding lane lines, we don\u2019t need to check the sky and the hills.\n\nRoughly speaking, we are interested in the area surrounded by the red lines below:\n\nExcluding everything except for the region of interest:\n\nNow we have lane lines but we need to recognize them as lines. Especially, two lines: the left lane and the right lane.\n\nI\u2019m using Hough Line Transform to detect lines in the edge images.\n\nThere are multiple lines detected for a lane line. We should come up with an averaged line for that.\n\nAlso, some lane lines are only partially recognized. We should extrapolate the line to cover full lane line length.\n\nWe want two lane lines: one for the left and the other for the right. The left lane should have a positive slope, and the right lane should have a negative slope. Therefore, we\u2019ll collect positive slope lines and negative slope lines separately and take averages.\n\nFinally, I\u2019m drawing the lane lines on the video clips. When doing this, I\u2019m calculating the average lane lines across multiple video image frames to make it smoother (i.e. not bumpy / jumping around).\n\nThe project was successful in that the video images clearly show the lane lines are detected properly and lines are very smoothly handled.\n\nIt only detects the straight lane lines. It is an advanced topic to handle curved lanes (or the curvature of lanes). We\u2019ll need to use perspective transformation and also poly fitting lane lines rather than fitting to straight lines.\n\nHaving said that, the lanes near the car are mostly straight in the images. The curvature appears at further distance unless it\u2019s a steep curve. So, this basic lane finding technique is still very useful.\n\nAnother thing is that it won\u2019t work for steep (up or down) roads because the region of interest mask is assumed from the center of the image.\n\nFor steep roads, we first need to detect the horizontal line (between the sky and the earth) so that we can tell up to where the lines should extend."
    },
    {
        "url": "https://towardsdatascience.com/peter-norvigs-sudoku-solver-25779bb349ce?source=user_profile---------11----------------",
        "title": "Sudoku Solver by Peter Norvig \u2013",
        "text": "I recently came across Peter Norvig\u2019s Solving Every Sudoku Puzzle. I was impressed with his concise and beautiful Python code that solves any Sudoku puzzles systematically.\n\nHowever, some people may find it difficult to understand the concise code hence are unable to appreciate the beauty. I break down his article and explain the code in a step-by-step manner.\n\nNote: the original article by Peter uses Python 2 but I\u2019m using Python 3 in this article. In some places, I changed the indentation of Peter\u2019s code but I did not alter his program logic at all.\n\nIf you are not familiar with Sudoku, I recommend that you visit Sudoku Dragon and read their Sudoku rule description.\n\nPeter gives a beautiful summary of its rule in one sentence.\n\nThis may not be very clear at first glance if you don\u2019t know what squares and units are.\n\nSo, C2 is a square at the intersection of the third row (which is C) and the second column (which is 2).\n\nA string can be treated as a list of characters, and you can access each element using for loop.\n\nYou might be wondering why Peter defines cols = digits. Why do we need two names for the same thing? After all, they are both the same character sequence. It will be clear as you read further in his program. You will appreciate that Peter uses digits and cols appropriately for better readability.\n\nFor now, just remember the following. When you see digits in his code, you should think of it as a list of digits from \u20181\u2019 to \u20189\u2019. When you see cols, you should think of it as a list of column identifiers from \u20181\u2019 to \u20189\u2019. Both are exactly the same values but their semantics are different.\n\nA list of squares can be composed in the following code:\n\nIn the above code, a+b is not a mathematical operation. It is a string concatenation operation. The character from a and the character from b are concatenated (i.e. joined). For example, when a=\u2019A\u2019 and b=\u20191', a+b yields \u2018A1\u2019.\n\nWe can use the list comprehension technique to make it more concise.\n\nFinally, the below shows Peter\u2019s code. He defines a function cross to make the code more readable.\n\nTherefore, the cross function generates all cross products of characters from rows and cols. The resulted squares variable points to a list of all the 81 squares.\n\nThere are 81 squares in Sudoku board and each square has exactly 3 units (no less, no more).\n\nFor example, the 3 units of the square C2 is shown below:\n\nThe column, the row, and the box (3x3 area) that contains the square C2 is the 3 units of the square C2.\n\nIn total, there are 27 units in the Sudoku board. They are the 9 column units, 9 row units and 9 box units.\n\nWe can use the cross function to generate the 9 column units:\n\nSimilarly, we can generate the 9 row units:\n\nFor the 9 box units, we break rows and cols into 3 groups and generate cross products.\n\nWe combine all 3 groups of the units into one list. Peter defines unitlist as follows:\n\nLet\u2019s see the number of units:\n\nWe know that there are 3 units for each square. This can be expressed in a Python dictionary as follows:\n\nThis dictionary uses square values as key and a list of units as value. For example, the 3 units of \u2018C2\u2019 can be accessed as follows:\n\nPeter uses much more concise code to achieve the same.\n\nNow that you know squares and units, we are ready to appreciate Peter\u2019s definition of Sudoku rule:\n\nIf it is not yet very clear, you should really visit Sudoku Dragon and read their description of Sudoku rule. Otherwise it will not make any sense reading what comes afer here.\n\nThere is one more terminology we need to introduce before talking about Peter\u2019s Sudoku Solver. It\u2019s called peers.\n\nLet me give an example first. The peers of \u2018C2\u2019 is all squares in the related 3 units except for \u2018C2\u2019 itself.\n\nWe can store all peers for all squares in Python dictionary as follows:\n\nFor example, all peers of \u2018C2\u2019 can be accessed as follows:\n\nThe above code has 3 for loop statements. Can we simplify this using Python\u2019s built-in function?\n\nThe answer is yes but you need to know how the Python list addition works. Let\u2019s take a look at the following example.\n\nThe addition of two Python lists is a combined list. Now, the same thing can be achieved using the built-in sum function.\n\nThe sum function takes two arguments. The first argument is an iterable object and the second argument is the initial value. The sum function will use the second value as the initial value and append each element of the first argument to it. For more details, please refer to the Python online documentation: https://docs.python.org/3/library/functions.html#sum.\n\nThe same result can be achieved by the following:\n\nBut using the sum function is more convenient when you have a list of items.\n\nYou might have noticed that there are two 3s in the combined list. This was deliberate. We can use set to have unique values.\n\nSuppose you want to remove 4 from this set, you can subtract a set that contains only 4.\n\nNow, we are ready to simplify the 3 for loop operations into one statement. Peter uses the following statement to express all peers for all squares in a dictionary.\n\nNote: units[s] returns a list of 3 units, each of which a list of squares. The sum function will combine all 3 unit lists into one list, which is a list of all squares in the 3 units. Using set, we keep the unique square values excluding the original square that is used to fetch all 3 units. Finally, the list of squares is stored in the dictionary using the original square as key.\n\nEach square has exactly 20 peers.\n\nPeter\u2019s Sudoku Solver accepts the various representations as input.\n\nThe following string is one such example:\n\nThe first 9 characters form the first row, the second 9 characters form the second row, and so on. All dots are interpreted as empty square.\n\nThe below is another representation:\n\nHere, all zero characters are interpreted as empty square. New lines are ignored.\n\nThe below is yet another example:\n\nAnything other than \u20180\u2019, \u20181\u2019, \u20182\u2019, \u20183\u2019, \u20184\u2019, \u20185\u2019, \u20186\u2019, \u20187\u2019, \u20188\u2019, \u20189\u2019 and \u2018.\u2019 are ignored such as \u2018|\u2019 and \u2018+\u2019.\n\nSuppose we want to parse the following initial values stored in grid1:\n\nThe following will extract only values we need.\n\nThen, we can store the values in a dictionary using square as key and char values as value.\n\nzip here takes two list and creates an iterable tuples. For more details, please refer to the Python online documentation: https://docs.python.org/3/library/functions.html#zip.\n\nPeter put all the above into a function as follows:\n\nFor example, you can call it like below:\n\nKnowing initial values for all squares (a digit value or an empty value), we can apply constraints to each square by eliminating impossible values for them. Peter lists up two simple rules:\n\nLet\u2019s take one step back and imagine that we don\u2019t know the initial values. In this case, all squares can potentially have any of digits. We can express this state as follows:\n\nThe values is a dictionary that associates each square to all digit values (\u2018123456789\u2019). As soon as we know the initial values, we can eliminate impossible values from other squares.\n\nAs an example of strategy (1), \u2018A1\u2019 is \u20184\u2019 in grid1_values. That means \u2018A1\u2019 can only have \u20184\u2019 as its value, and we can remove \u20184\u2019 from its peers.\n\nAs an example of strategy (2), as none of A2 through I2 except \u2018F2\u2019 has \u20184\u2019 as a possible value in grid1_values, \u20184\u2019 must belong to \u2018F2\u2019 and we can eliminate \u20184\u2019 from the peers of \u2018F2\u2019.\n\nSo, while we assign initial values for each square, we also eliminate it from the peers.\n\nThe outcome of this parsing process leaves only possible values for each square. Peter defines a function to handle this as follows:\n\nIn the above function,\n\nThe assign function is not yet defined here. Other than that, the parse_grid function should be self-explanatory if you read it line by line.\n\nThe function assign(values, s, d) updates the incoming values by eliminating the other values than d from the square s calling the function eliminate(values, s, d).\n\nIf any of the eliminate call returns False, the assign function returns False indicating that there is a contradiction.\n\nSo what does the eliminate function do?\n\nThe function is a bit lengthy but if you have followed the discussion so far, you should be able to understand it.\n\nBy calling parse_grid, we know what are the potential values for each square. Naturally, we want to display the result.\n\nI\u2019m not going to explain the details of this function. If you are curious, play with it by changing here and there and you\u2019ll know how it works.\n\nThe below is an example output.\n\nSo, for example, the square \u2018C2\u2019 has \u201815689\u2019 as the potential values.\n\nCan we enhance the eliminate function further to remove more values by implementing strategies like the naked twins?\n\nIt\u2019s possible to pursue that route. But Peter says otherwise:\n\nSo, what is the alternative to solve every Sudoku puzzles?\n\nFinally, Peter explains how to choose which square to start exploring which is called variable ordering:\n\nFor value ordering (Which digit do we try first for the square?), Peter says:\n\nThe solve function parses the initial representation and calls the search function.\n\nNote: values are copied and passed down to the assign call to avoid book-keeping complexity.\n\nIf one of the attempt succeeds, we solved the puzzle. To check this, the function some is used.\n\nThat\u2019s all. We have a Solver that works for any Sudoku.\n\nFor your reference, I put Peter\u2019s complete Sudoku Solver code here.\n\nThere are results reported in the article which shows how well it works.\n\nIn the end, Peter explains why he did this project:\n\nI hope this article find some use for people having hard time understanding Peter Norvig\u2019s Sudoku solver article which is an excellent one but requires the reader to have a certain level of Python mastery."
    }
]