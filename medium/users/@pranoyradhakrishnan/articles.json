[
    {
        "url": "https://medium.com/@pranoyradhakrishnan/the-neuroscientific-basis-for-convolutional-networks-3aa995919f5a?source=user_profile---------1----------------",
        "title": "The Neuroscientific Basis for Convolutional Networks",
        "text": "The history of convolutional networks begins with neuroscientific experiments long before the relevant computational models were developed.\n\nNeurophysiologists David Hubel and Torsten Wiesel observed how neurons in the cat\u2019s brain responded to images projected in precise locations on a screen in front of the cat.\n\nThe Neurons in the early visual cortex are organized in a hierarchical fashion, where the first cells connected to the cat\u2019s retinas are responsible for detecting simple patterns like edges and bars, followed by later layers responding to more complex patterns by combining the earlier neuronal activities.\n\nConvolutional Neural Network may learn to detect edges from raw pixels in the first layer, then use the edges to detect simple shapes in the second layer, and then use these shapes to deter higher-level features, such as facial shapes in higher layers\n\nThe Visual Cortex of the brain is a part of the cerebral cortex that processes visual information. V1 is the first area of the brain that begins to\n\nperform significantly advanced processing of visual input."
    },
    {
        "url": "https://hackernoon.com/transfer-learning-for-natural-language-processing-bb4669d1c1ff?source=user_profile---------2----------------",
        "title": "Transfer Learning for Natural Language Processing \u2013",
        "text": "Transfer learning is aimed to make use of valuable knowledge in a source domain to help model performance in a target domain.\n\nIn NLP applications, especially when we do not have large enough datasets for solving a task(called the target task T ), we would like to transfer knowledge from other tasks S to avoid overfitting and to improve the performance of T.\n\nTransferring knowledge to a semantically similar/same task but with a different dataset.\n\nTransferring knowledge to a task that is semantically different but shares the same neural network architecture so that neural parameters can be transferred.\n\nThe INIT approach first trains the network on S, and then directly uses the tuned parameters to initialize the network for T . After transfer, we may fix the parameters in the target domain.i e fine tuning the parameters of T.\n\nMULT, on the other hand, simultaneously trains samples in both domains.\n\nWe first pretrain on the source domain S for parameter initialization, and then train S and T simultaneously.\n\nThe Neural Transfer Learning in NLP depends largely on how similar in semantics the source and target datasets are."
    },
    {
        "url": "https://buzzrobot.com/how-to-define-machine-learning-26436aa1c89f?source=user_profile---------3----------------",
        "title": "How to define Machine Learning? \u2013",
        "text": "\u201cA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E.\u201d\n\nIf we want a robot to be able to walk, then walking is the task.\n\n\u201cLearning is our means of attaining the ability to perform the task\u201d\n\nWe could program the robot to learn to walk, or we could directly write a program that specifies how to walk manually.\n\nSome of the most common machine learning tasks include the following:\n\nIn order to evaluate a machine learning algorithm, we must measure its performance.\n\nFor tasks such as classification, we often measure the accuracy of the model.\n\nAccuracy is just the proportion of examples for which the model produces the correct output.\n\nMachine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning process.\n\nUnsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset.\n\nSupervised learning algorithms experience a dataset containing features, but each example is also associated with a label or target."
    },
    {
        "url": "https://towardsdatascience.com/photoshop-2-0-a49990e483?source=user_profile---------4----------------",
        "title": "Photoshop 2.0 \u2013",
        "text": "We can use Photoshop to \u201calter\u201d images like photos, downloaded icons, or scanned artwork. Altering an image includes doing such things as changing the colors within an image, modifying the size and scale of an image, or putting one picture \u201cwithin\u201d another.\n\nHow much time it takes? What about the Perfection? Still you manage to do. That\u2019s good.\n\nGenerative Adversarial Networks are good in translating an image from one domain to another domain.\n\nFor example : Generating cats from user sketches.\n\nAll of us are not good in editing photos. But sometimes we will have to do that ourselves. Suppose if you want to change the color of your hair or to try a different hair style, how to proceed? It\u2019s difficult to transform the image in the way you want, if you are not familiar with the tools.\n\nGANs can convert your rough drawing into realistic images.\n\nThe Generator can translate semantic label maps to realistic-looking images, while the discriminator D aims to distinguish real images from the translated ones.\n\nThe pix2pix method is a conditional GAN framework to model the conditional distribution of real images given the input semantic label maps.\n\nConditional GANs are an extension of the GAN framework. Here we have conditional information Y that describes some aspect of the data. For example, if we are dealing with faces, Y could describe attributes such as hair color or gender.\n\nThe results were unsatisfactory when high resolutions images were generated. So to improve the Photorealism and Resolution, a new architecture is developed.\n\nGlobal generator(G1) takes downsampled input from Semantic Map and generates low resolution output. Local enhancer(G2) combines the feature maps from both the generator and the original label map to produce the final output.\n\nFor synthesizing images at higher resolution, additional local enhancer networks can be used.\n\nDuring training, we first train the global generator and then train the local enhancer in the order of their resolutions. We then jointly fine-tune all the networks together.\n\nMulti-scale discriminators consist of 3 discriminators(D1,D2,D3) that have an identical network structure but operate at different image scales.\n\nThey are trained to differentiate real and synthesized images at 3 different scales.\n\nThe discriminator operating at the coarsest scale has the largest receptive field. On the other hand, the one which operates at the finest scale guides the generator to produce finer details.\n\nA feature matching loss based on the discriminator is incoperated. The features are extracted from multiple layers of the discriminator, and learn to match these intermediate representations from the real and the synthesized image.\n\nAdding low-dimensional feature channels as the input to the generator can produce diverse images and allow instance-level control.\n\nAn encoder network is trained to find a low-dimensional feature vector for each object instance in the ground truth image. We then concatenate the features with the label map to generate the final image. We can produce different outputs by manipulating different features."
    },
    {
        "url": "https://buzzrobot.com/photoshop-2-0-gan-for-photo-editing-3ba4eddceddd?source=user_profile---------5----------------",
        "title": "GAN for Photo Editing \u2013",
        "text": "GANs learn a generative model by training one network, the \u201cdiscriminator,\u201d to distinguish between real and generated data, while simultaneously training a second network, the \u201cgenerator,\u201d to transform a noise vector into samples which the discriminator cannot distinguish from real data.\n\nIf a user has an image of a person with light skin, dark hair, and a widow\u2019s peak, by painting a dark color on the forehead, the system will automatically add hair in the requested area.\n\nSimilarly, if a user has a photo of a person with a closed-mouth smile, the user can produce a toothy grin by painting bright white over the target\u2019s mouth.\n\nLet\u2019s Look at some examples.\n\nThe user uses the brush tools to generate an image from scratch and then keeps adding more scribbles to refine the result. A Trained GAN can generate the most similar real images.\n\nWe can train the Generator to generate a smile(real distribution) whereas the discriminator will distinguish between \u201csmile\u201d(real distribution) and \u201cnot smile\u201d(generated distribution)\n\nAn interesting outcome of the editing process is the sequence of intermediate generated images that can be seen as a new kind of image morphing called \u201cgenerative transformation\u201d.\n\nThe source on the left is transformed to have the shape and color of the one on the right.\n\nThe source on the left is transformed to have shape of the one on the right."
    },
    {
        "url": "https://buzzrobot.com/bias-and-variance-11d8e1fee627?source=user_profile---------6----------------",
        "title": "Bias and Variance in Neural Network \u2013",
        "text": "This is under the assumption that human error ~ 0% and all the data is from the same distribution.\n\nYou can see that that the training error(blue dotted line) keeps on decreasing. In the initial phase, it\u2019s too high (High Bias). Later, it decreases (Low Bias).\n\nHigh Bias means the model is not even fitting on the training data. So, we have to make the bias low.\n\nThe Variance of a model is the difference between validation error and training error . In the figure, you can see that the gap between validation error and training error is increasing. That is, the variance is increasing (Overfitting).\n\nVariance gives us the information about the generalization power of our model.\n\nIf the Variance is high, the model is not performing well on the validation set. We always want a low variance.\n\nLow Bias and High Variance(Overfitting). Since the Variance is greater than bias, this is a Variance problem. We have to lower the variance.\n\nLet\u2019s look at another example.\n\nHigh Bias and Low Variance(Underfitting). Since the Bias is greater than Variance, this is a Bias problem. We have to lower the Bias."
    },
    {
        "url": "https://towardsdatascience.com/what-is-transfer-learning-8b1a0fa42b4?source=user_profile---------7----------------",
        "title": "What is Transfer Learning? \u2013",
        "text": "Transfer learning make use of the knowledge gained while solving one problem and applying it to a different but related problem.\n\nFor example, knowledge gained while learning to recognize cars can be used to some extent to recognize trucks.\n\nWhen we train the network on a large dataset(for example: ImageNet) , we train all the parameters of the neural network and therefore the model is learned. It may take hours on your GPU.\n\nWe can give the new dataset to fine tune the pre-trained CNN. Consider that the new dataset is almost similar to the orginal dataset used for pre-training. Since the new dataset is similar, the same weights can be used for extracting the features from the new dataset.\n\nThe earlier features of a ConvNet contain more generic features (e.g. edge detectors or color blob detectors), but later layers of the ConvNet becomes progressively more specific to the details of the classes contained in the original dataset.\n\nThe earlier layers can help to extract the features of the new data. So it will be good if you fix the earlier layers and retrain the rest of the layers, if you got only small amount of data.\n\nIf you have large amount of data, you can retrain the whole network with weights initialized from the pre-trained network.\n\nWhen you have a new dataset smaller than the orginal dataset used to train the pre-trained model."
    },
    {
        "url": "https://hackernoon.com/attention-mechanism-in-neural-network-30aaf5e39512?source=user_profile---------8----------------",
        "title": "Attention Mechanism in Neural Network \u2013",
        "text": "An Encoder reads and encodes a source sentence into a fixed-length vector.\n\nA Decoder then outputs a translation from the encoded vector.\n\nA potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector.\n\nAttention Mechanism allows the decoder to attend to different parts of the source sentence at each step of the output generation.\n\nInstead of encoding the input sequence into a single fixed context vector, we let the model learn how to generate a context vector for each output time step. That is we let the model learn what to attend based on the input sentence and what it has produced so far.\n\nHere, the Encoder generates h1,h2,h\u2026.hT from the inputs X1,X2,X3\u2026XT\n\nThen, we have to find out the context vector ci for each of the output time step.\n\na is the Alignment model which is a feedforward neural network that is trained with all the other components of the proposed system\n\nThe Alignment model scores (e) how well each encoded input (h) matches the current output of the decoder (s).\n\nThe alignment scores are normalized using a softmax function.\n\nThe context vector is a weighted sum of the annotations (hj) and normalized alignment scores.\n\nThe Decoder generates output for i\u2019th timestep by looking into the i\u2019th context vector and the previous hidden outputs s(t-1).\n\n\u2014 Neural Machine Translation by Jointly Learning to Align and Translate, 2015."
    },
    {
        "url": "https://towardsdatascience.com/image-captioning-in-deep-learning-9cd23fb4d8d2?source=user_profile---------9----------------",
        "title": "Image Captioning in Deep Learning \u2013",
        "text": "Image Captioning is the process of generating textual description of an image. It uses both Natural Language Processing and Computer Vision to generate the captions.\n\nThe dataset will be in the form [image \u2192 captions]. The dataset consists of input images and their corresponding output captions.\n\nThe Convolutional Neural Network(CNN) can be thought of as an encoder. The input image is given to CNN to extract the features. The last hidden state of the CNN is connected to the Decoder.\n\nThe Decoder is a Recurrent Neural Network(RNN) which does language modelling up to the word level. The first time step receives the encoded output from the encoder and also the <START> vector.\n\nThe output from the last hidden state of the CNN(Encoder) is given to the first time step of the decoder. We set x1 =<START> vector and the desired label y1 = first word in the sequence. Analogously, we set x2 =word vector of the first word and expect the network to predict the second word. Finally, on the last step, xT = last word, the target label yT =<END> token.\n\nDuring training, the correct input is given to the decoder at every time-step, even if the decoder made a mistake before.\n\nThe image representation is provided to the first time step of the decoder. Set x1 =<START> vector and compute the distribution over the first word y1. We sample a word from the distribution (or pick the argmax), set its embedding vector as x2, and repeat this process until the <END> token is generated.\n\nDuring Testing, the output of the decoder at time t is fed back and becomes the input of the decoder at time t+1"
    },
    {
        "url": "https://towardsdatascience.com/sequence-to-sequence-learning-e0709eb9482d?source=user_profile---------10----------------",
        "title": "Sequence to Sequence Learning \u2013",
        "text": "In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length.\n\nThe Encoder RNN reads the input sequence and generates the \ufb01xed-size context vector which represents a semantic summary of the input sequence.\n\nThe fixed-size context vector is given as input to the decoder RNN.\n\nThe fixed-size context can be provided as the initial state of the Decoder RNN, or it can be connected to the hidden units at each time step. These two ways can also be combined.\n\nThe number of time steps in the Encoder and Decoder need not to be equal.\n\nOne limitation of this architecture is that it is difficult to summarize a long sequence with a context vector which has a small dimension.\n\nA variable-length context vector can be used instead of a \ufb01xed-size vector.\n\nAn Attention mechanism can be used to produces a sequence of vectors from the encoder RNN from each time step of the input sequence. The Decoder learns to pay selective attention to the vectors to produce the output at each time step."
    },
    {
        "url": "https://towardsdatascience.com/sequence-to-sequence-using-encoder-decoder-15e579c10a94?source=user_profile---------11----------------",
        "title": "Familiarization of Sequence to Sequence model in Deep Learning",
        "text": "Sequence to sequence models are the models which takes sequence input and outputs a sequence.\n\nThe Encoder,which is a RNN, takes input and encodes that to a vector.\n\nThe last hidden state of the encoder gives the encoded vector.\n\nThe encoded vector is repeated n times where n=number of time steps of output.\n\nThe Decoder, which is also a RNN, takes encoded vector and previous states as inputs and gives the output.\n\nWhen you want to generate the output after seeing the whole input sentence, seq2seq model can be used.\n\nFor Example in Machine Translation, the first word in the translated sentence may depend upon the last word in the input sentence.\n\nIn Image Captioning, the textual description of an image is generated.\n\nThe encoder is now replaced with a convolutional neural network. The output from the last hidden state is given to a decoder RNN."
    },
    {
        "url": "https://towardsdatascience.com/introduction-to-recurrent-neural-network-27202c3945f3?source=user_profile---------12----------------",
        "title": "Introduction to Recurrent Neural Network \u2013",
        "text": "There are many deep learning models specialized in solving many tasks. Here we discuss the capability of deep learning models to handle sequences.\n\nIf there is a particular order in which related things follow each other, we call it as a sequence.\n\nDo you think both sentences mean the same? NO! which means the position of words is very important! They are a sequence of words.\n\nThink of a video playing. You can easily predict the next scene if you have already watched that. But consider that you are sleepy, and you don\u2019t remember the position of frames(all jumbled frames in mind). Can you predict the next scene then??? Of course not!!!\n\nThey are like us!!! They can remember sequences. If you tell an RNN to predict the next scene, it can tell you.\n\nWhen the Feedforward Neural Network takes decisions based on only the current input, the RNN takes decisions based on current and previous inputs.\n\nThe Green Box represents a Neural Network. The arrows indicate memory or simply feedback to the next input.\n\nThe first figure shows the RNN. The Second figure shows the same RNN unrolled in time. Consider a sequence [i am a good boy]. We can say that the sequence is arranged in time. At t=0, X0=\u201ci\u201d is given as the input . At t=1, X1=\u201cam\u201d is given as the input. The state from the first time step is remembered and given as input during the second time step along with the current input at that time step.\n\nIn a Feed Forward Neural Network, the Network is forward propagated only once per sample. But in RNN, the network is forward propagated equal to the number of time steps per sample.\n\nGiven a sequence of words we want to predict the probability of each word given the previous words.\n\nMachine Translation is similar to language modeling in that our input is a sequence of words in our source language (e.g. German). We want to output a sequence of words in our target language (e.g. English).\n\nGiven an input sequence of acoustic signals from a sound wave, we can predict a sequence of phonetic segments together with their probabilities.\n\nTogether with convolutional Neural Networks, RNNs have been used as part of a model to generate descriptions for unlabeled images.\n\nChatbots can give reply to your queries. When a sequence of words is given as the input, sequence of words will be generated at the output."
    },
    {
        "url": "https://medium.com/@pranoyradhakrishnan/ways-for-giving-data-to-your-deep-learning-models-fc69db56432f?source=user_profile---------13----------------",
        "title": "Ways for giving data to your Deep Learning Models \u2013 Pranoy Radhakrishnan \u2013",
        "text": "For computers, images are pixels. They are integer values ranging form 0 to 255.\n\nFor classification, we need to create a dataset consisting of inputs and outputs. Here, the inputs consist of images and the outputs consist of labels. For example: Image of cat \u2192\u201dcat\u201d\n\nI have attached my code here: https://github.com/pranoyr/Load-data\n\nNeural networks will not accept text. We have to convert text into integers before feeding into your neural network.\n\nThere are many methods for converting text into integers. You can either go for word embeddings or use corresponding array index as encodings.\n\nFor example: Consider the dictionary [i good boy am nice girl a]. I call this dictionary because i use the index of this array for conversion.\n\nI want to convert [i am a good boy] and [ i am a nice girl] to integers. Use the dictionary for conversion.\n\nSimilary do that for the labels also.\n\nThere is a new approach called word embeddings.\n\nIn the previous example, each word is converted to an integer. The integers does not give any relationship between similar words.\n\nTo capture the relationship between different words, we use word embeddings.\n\nHere, attaching my code for loading data for sentiment analysis"
    },
    {
        "url": "https://medium.com/@pranoyradhakrishnan/different-problem-to-be-solved-with-deep-learning-6a1d5e33c0b8?source=user_profile---------14----------------",
        "title": "Different problems to be solved with deep learning \u2013 Pranoy Radhakrishnan \u2013",
        "text": "There are many problems which we can solve with deep learning.\n\nWe will categorize the problems.\n\nEach sample in the dataset consists of a single input and it\u2019s corresponding class or label.\n\nWe take one image of a cat and label it as \u201ccat\u201d. Similarly do that for the whole dataset.\n\nEach sample in the dataset consists of a single input and the output consists of a sequence.\n\nEach sample in the dataset consists of sequence input and the single output.\n\nEach sample in the dataset consists of sequence input and the sequence output. Number of time steps in the input and output need not be equal.\n\nEach sample in the dataset consists of sequence input and the sequence output. Number of time steps in the input and output should be equal."
    },
    {
        "url": "https://medium.com/@pranoyradhakrishnan/introduction-to-deep-learning-89ca36abaff7?source=user_profile---------15----------------",
        "title": "Introduction to Deep Learning \u2013 Pranoy Radhakrishnan \u2013",
        "text": "Write a program in C++ to print the n Fibonacci numbers? That\u2019s it !!! Right!!!\n\nYour code is the algorithm. Right? YES!!!\n\nYou have written the code(algorithm) by yourself.\n\nHere, you give input to the Algorithm which you have made.\n\nHow about computer write the algorithm for you? Interested? YES!!!\n\nGive computer some sample inputs and sample outputs so that it writes the algorithm(function which maps input and output) for you. WOW!!!\n\nTrain the computer to first build the function(algorithm) for you.\n\nUse the function Later for Prediction!!!\n\nSo, computers have built the functions!!!\n\nYES!!! There are many algorithms written by scientists around the world to make the computers do that!!!\n\nOne such algorithm is NEURAL NETWORK!!!\n\nWe have been saying that our aim is to build f(x).\n\nWe give sample x and y so that W and b is learned(W=0.1,b=0.2) or f(x) is trained.\n\nNow, give x some new values(Prediction)\n\nThis is what is happening in a Neural Network.\n\nw1 to w8 are the weights.\n\nb1 and b2 are the bias.\n\nEach circle represents a neuron where g(Wx+b) is computed.\n\nWhat changes is the complexity.\n\nNumber of neurons and the layers in between input and output layers(Hidden Layers) increases."
    },
    {
        "url": "https://towardsdatascience.com/what-are-hyperparameters-and-how-to-tune-the-hyperparameters-in-a-deep-neural-network-d0604917584a?source=user_profile---------16----------------",
        "title": "What are Hyperparameters ? and How to tune the Hyperparameters in a Deep Neural Network?",
        "text": "Hyperparameters are the variables which determines the network structure(Eg: Number of Hidden Units) and the variables which determine how the network is trained(Eg: Learning Rate).\n\nHyperparameters are set before training(before optimizing the weights and bias).\n\nHidden layers are the layers between input layer and output layer.\n\n\u201cVery simple. Just keep adding layers until the test error does not improve anymore.\u201d\n\nMany hidden units within a layer with regularization techniques can increase accuracy. Smaller number of units may cause underfitting.\n\nDropout is regularization technique to avoid overfitting (increase the validation accuracy) thus increasing the generalizing power.\n\nIdeally, it may be better to use different weight initialization schemes according to the activation function used on each layer.\n\nMostly uniform distribution is used.\n\nActivation functions are used to introduce nonlinearity to models, which allows deep learning models to learn nonlinear prediction boundaries.\n\nGenerally, the rectifier activation function is the most popular.\n\nSigmoid is used in the output layer while making binary predictions. Softmax is used in the output layer while making multi-class predictions.\n\nThe learning rate defines how quickly a network updates its parameters.\n\nLow learning rate slows down the learning process but converges smoothly. Larger learning rate speeds up the learning but may not converge.\n\nMomentum helps to know the direction of the next step with the knowledge of the previous steps. It helps to prevent oscillations. A typical choice of momentum is between 0.5 to 0.9.\n\nNumber of epochs is the number of times the whole training data is shown to the network while training.\n\nIncrease the number of epochs until the validation accuracy starts decreasing even when training accuracy is increasing(overfitting).\n\nMini batch size is the number of sub samples given to the network after which parameter update happens.\n\nA good default for batch size might be 32. Also try 32, 64, 128, 256, and so on."
    },
    {
        "url": "https://medium.com/@pranoyradhakrishnan/when-to-stop-training-your-neural-network-174ff0a6dea5?source=user_profile---------17----------------",
        "title": "When to stop Training your Neural Network? \u2013 Pranoy Radhakrishnan \u2013",
        "text": "If the training accuracy increases(positive slope) while the validation accuracy steadily decreases(negative slope) then a situation of over fitting may have occurred. Time to stop Training.\n\nTraining data is the data given to the network, from which the network build your model.\n\nTraining accuracy tells about how much your model learns to map the input and output.\n\nValidation data is the data with which the training process is validated. Validation accuracy tells about the generalizing power of the network."
    }
]