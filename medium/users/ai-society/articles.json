[
    {
        "url": "https://medium.com/ai-society/gans-from-scratch-1-a-deep-introduction-with-code-in-pytorch-and-tensorflow-cb03cdcdba0f?source=---------0",
        "title": "GANs from Scratch 1: A deep introduction. With code in PyTorch and TensorFlow",
        "text": "In this blog post we\u2019ll start by describing Generative Algorithms and why GANs are becoming increasingly relevant. An overview and a detailed explanation on how and why GANs work will follow. Finally, we\u2019ll be programming a Vanilla GAN, which is the first GAN model ever proposed! Feel free to read this blog in the order you like. For demonstration purposes we\u2019ll be using PyTorch, although a TensorFlow implementation can also be found in my GitHub Repo diegoalejogm/gans. You can check out some of the advanced GAN models (e.g. DCGAN) in the same GitHub repository if you\u2019re interested, which by the way will also be explained in the series of posts that I\u2019m starting, so make sure to stay tuned. Output of a GAN through time, learning to Create Hand-written digits. We\u2019ll code this example! Generative Adversarial Networks (or GANs for short) are one of the most popular Machine Learning algorithms developed in recent times. For those new to the field of Artificial Intelligence (AI), we can briefly describe Machine Learning (ML) as the sub-field of AI that uses data to \u201cteach\u201d a machine/program how to perform a new task. A simple example of this would be using images of a person\u2019s face as input to the algorithm, so that a program learns to recognize that same person in any given picture (it\u2019ll probably need negative samples too). For this purpose, we can describe Machine Learning as applied mathematical optimization, where an algorithm can represent data (e.g. a picture) in a multi-dimensional space (remember the Cartesian Plane? That\u2019s a 2 dimensional field), and then learns to distinguish new multi-dimensional vector samples as belonging to the target distribution or not. For a visual understanding on how machines learn I recommend this broad video explanation and this other video on the rise of machines, which I were very fun to watch. Though this is a very fascinating field to explore and discuss, I\u2019ll leave the in-depth explanation for a later post, we\u2019re here for GANs!\n\nIn short, they belong to the set of algorithms named generative models. These algorithms belong to the field of unsupervised learning, a sub-set of ML which aims to study algorithms that learn the underlying structure of the given data, without specifying a target value. Generative models learn the intrinsic distribution function of the input data p(x) (or p(x,y) if there are multiple targets/classes in the dataset), allowing them to generate both synthetic inputs x\u2019 and outputs/targets y\u2019, typically given some hidden parameters. In contrast, supervised learning algorithms learn to map a function y\u2019=f(x), given labeled data y. An example of this would be classification, where one could use customer purchase data (x) and the customer respective age (y) to classify new customers. Most of the supervised learning algorithms are inherently discriminative, which means they learn how to model the conditional probability distribution function (p.d.f) p(y|x) instead, which is the probability of a target (age=35) given an input (purchase=milk). Despite the fact that one could make predictions with this p.d.f, one is not allowed to sample new instances (simulate customers with ages) from the input distribution directly. \n\nSide-note: It is possible to use discriminative algorithms which are not probabilistic, they are called discriminative functions. GANs they have proven to be really succesfull in modeling and generating high dimensional data, which is why they\u2019ve become so popular. Nevertheless they are not the only types of Generative Models, others include Variational Autoencoders (VAEs) and pixelCNN/pixelRNN and real NVP. Each model has Some of the most relevant GAN pros and cons for the are: They currently generate the sharpest images They are easy to train (since no statistical inference is required), and only back-propogation is needed to obtain gradients GANs are difficult to optimize due to unstable training dynamics. No statistical inference can be done with them. (They belong to the class of direct implicit density models (they can model p(x) without explicitly defining the p.d.f)) Generative models are one of the most promising approaches to understand the vast amount of data that surrounds us nowadays. According to OpenAI, algorithms which are able to create data might be substantially better at understanding intrinsically the world. The idea that generative models hold a better potential at solving our problems can be illustrated using the quote of one of my favourite physicists. \u201cWhat I cannot create, I do not understand.\u201d \u2014 Richard P. Feynman Generative models can be thought as containing more information than their discriminative counterpart/complement, since they also be used for discriminative tasks such as classification or regression (where the target is a continuous value such as \u211d). One could calculate the conditional p.d.f p(y|x) needed most of the times for such tasks, by using statistical inference on the joint p.d.f. p(x,y) that is already available in the generative model. Though generative models work for classification and regression, fully discriminative approaches are usually more successful at discriminative tasks in comparison to generative approaches in some scenarios. Among several use cases, generative models may be applied to: Machine Learning Engineers and Scientists reading this article may have already realized that generative models can also be used to generate inputs which may expand small datasets. I also found a very long and interesting curated list of awesome GAN applications here. Generative Adversarial Networks are composed of two models: The first model is called a Generator and it aims to generate new data similar to the expected one. The Generator could be asimilated to a human art forger, which creates fake works of art. The second model is named the Discriminator. This model\u2019s goal is to recognize if an input data is \u2018real\u2019 \u2014 belongs to the original dataset \u2014 or if it is \u2018fake\u2019 \u2014 generated by a forger. In this scenario, a Discriminator is analogous to the police (or an art expert), which tries to detect artworks as truthful or fraud. How do these models interact? Paraphrasing the original paper which proposed this framework, it can be thought of the Generator as having an adversary, the Discriminator. The Generator (forger) needs to learn how to create data in such a way that the Discriminator isn\u2019t able to distinguish it as fake anymore. The competition between these two teams is what improves their knowledge, until the Generator succeeds in creating realistic data. # Talk about x and z, with their respective distributions Though the GANs framework could be applied to any two models that perform the tasks described above, it is easier to understand when using universal approximators such as artificial neural networks. A neural network G(z, \u03b8\u2081) is used to model the Generator mentioned above. It\u2019s role is mapping input noise variables z to the desired data space x (say images). Conversely, a second neural network D(x, \u03b8\u2082) models the discriminator and outputs the probability that the data came from the real dataset, in the range (0,1). In both cases, \u03b8\u1d62 represents the weights or parameters that define each neural network. As a result, the Discriminator is trained to correctly classify the input data as either real or fake. This means it\u2019s weights are updated as to maximize the probability that any real data input x is classified as belonging to the real dataset, while minimizing the probability that any fake image is classified as belonging to the real dataset. In more technical terms, the loss/error function used maximizes the function D(x), and it also minimizes D(G(z)). Furthermore, the Generator is trained to fool the Discriminator by generating data as realistic as possible, which means that the Generator\u2019s weight\u2019s are optimized to maximize the probability that any fake image is classified as belonging to the real datase. Formally this means that the loss/error function used for this network maximizes D(G(z)). In practice, the logarithm of the probability (e.g. log D(\u2026)) is used in the loss functions instead of the raw probabilies, since using a log loss heavily penalises classifiers that are confident about an incorrect classification. After several steps of training, if the Generator and Discriminator have enough capacity (if the networks can approximate the objective functions), they will reach a point at which both cannot improve anymore. At this point, the generator generates realistic synthetic data, and the discriminator is unable to differentiate between the two types of input. Since during training both the Discriminator and Generator are trying to optimize opposite loss functions, they can be thought of two agents playing a minimax game with value function V(G,D). In this minimax game, the generator is trying to maximize it\u2019s probability of having it\u2019s outputs recognized as real, while the generator is trying to minimize this same value.\n\nThe fundamental steps to train a GAN can be described as following: Sample a noise set and a real-data set, each with size m. Train the Discriminator on this data. Train the Generator on this data. Finally, the moment several of us were waiting for has arrived. \ud83d\ude4c We\u2019ll implement a GAN in this tutorial, starting by downloading the required libraries. In case you haven\u2019t downloaded PyTorch yet, check out their download helper here. Remember that you can also find a TensorFlow example here. We\u2019ll proceed by creating a file/notebook and importing the following dependencies. To log our progress, we will import an additional file I\u2019ve created, which will allow us to visualize the training process in console/Jupyter, and at the same time store it in TensorBoard for those who already know how to use it. You need to download the file and put it in the same folder where your GAN file will be. It is not necessary that you understand the code in this file, as it is only used for visualization purposes. The file can be found in any of the following links: Preview of the file we will use for logging. The dataset we\u2019ll be using here is LeCunn\u2019s MNIST dataset, consisting of about 60.000 black and white images of handwritten digits, each with size 32x32 pixels\u00b2. This dataset will be preprocessed according to some useful \u2018hacks\u2019 proven to be useful for training GANs. **Specifically, the input values which range in between [0, 255] will be normalized between -1 and 1. # Create loader with data, so that we can iterate over it\n\ndata_loader = torch.utils.data.DataLoader(data, batch_size=100, shuffle=True)\n\n# Num batches\n\nnum_batches = len(data_loader) Next, we\u2019ll define the neural networks, starting with the Discriminator. This network will take a flattened image as its input, and return the probability of it belonging to the real dataset, or the synthetic dataset. The input size for each image will be . Regarding the structure of this network, it will have three hidden layers, each followed by a Leaky-ReLU nonlinearity and a Dropout layer to prevent overfitting. A Sigmoid/Logistic function is applied to the real-valued output to obtain a value in the open-range (0, 1): We also need some additional functionality that allows us to convert a flattened image into its 2-dimensional representation, and another one that does the opposite. On the other hand, the Generative Network takes a latent variable vector as input, and returns a 784 valued vector, which corresponds to a flattened 28x28 image. Remember that the purpose of this network is to learn how to create undistinguishable images of hand-written digits, which is why its output is itself a new image. This network will have three hidden layers, each followed by a Leaky-ReLU nonlinearity. The output layer will have a TanH activation function, which maps the resulting values into the (-1, 1) range, which is the same range in which our preprocessed MNIST images is bounded. We also need some additional functionality that allows us to create the random noise. The random noise will be sampled from a normal distribution with mean 0 and variance 1 as proposed in this link. Here we\u2019ll use as the optimization algorithm for both neural networks, with a learning rate of 0.0002. The proposed learning rate was obtained after testing with several values, though it isn\u2019t necessarily the optimal value for this task. The loss function we\u2019ll be using for this task is named Binary Cross Entopy Loss (BCE Loss), and it will be used for this scenario as it resembles the log-loss for both the Generator and Discriminator defined earlier in the post (see Modeling Mathematically a GAN). Specifically we\u2019ll be taking the average of the loss calculated for each minibatch.\n\nIn this formula the values y are named targets, v are the inputs, and w are the weights. Since we don\u2019t need the weight at all, it\u2019ll be set to w\u1d62=0 for all i. If we replace v\u1d62 = D(x\u1d62) and y\u1d62=1 \u2200 i (for all i) in the BCE-Loss definition, we obtain the loss related to the real-images. Conversely if we set v\u1d62 = D(G(z\u1d62)) and y\u1d62=0 \u2200 i, we obtain the loss related to the fake-images. In the mathematical model of a GAN I described earlier, the gradient of this had to be ascended, but PyTorch and most other Machine Learning frameworks usually minimize functions instead. Since maximizing a function is equivalent to minimizing it\u2019s negative, and the BCE-Loss term has a minus sign, we don\u2019t need to worry about the sign. Additionally, we can observe that the real-images targets are always ones, while the fake-images targets are zero, so it would be helpful to define the following functions: def ones_target(size):\n\n '''\n\n Tensor containing ones, with shape = size\n\n '''\n\n data = Variable(torch.ones(size, 1))\n\n return data\n\n\n\ndef zeros_target(size):\n\n '''\n\n Tensor containing zeros, with shape = size\n\n '''\n\n data = Variable(torch.zeros(size, 1))\n\n return data By summing up these two discriminator losses we obtain the total mini-batch loss for the Discriminator. In practice, we will calculate the gradients separately, and then update them together. def train_discriminator(optimizer, real_data, fake_data):\n\n N = real_data.size(0)\n\n # Reset gradients\n\n optimizer.zero_grad()\n\n \n\n # 1.1 Train on Real Data\n\n prediction_real = discriminator(real_data)\n\n # Calculate error and backpropagate\n\n error_real = loss(prediction_real, ones_target(N) )\n\n error_real.backward()\n\n\n\n # 1.2 Train on Fake Data\n\n prediction_fake = discriminator(fake_data)\n\n # Calculate error and backpropagate\n\n error_fake = loss(prediction_fake, zeros_target(N))\n\n error_fake.backward()\n\n \n\n # 1.3 Update weights with gradients\n\n optimizer.step()\n\n \n\n # Return error and predictions for real and fake inputs\n\n return error_real + error_fake, prediction_real, prediction_fake Rather than minimizing log(1- D(G(z))), training the Generator to maximize log D(G(z)) will provide much stronger gradients early in training. Both losses may be swapped interchangeably since they result in the same dynamics for the Generator and Discriminator. Maximizing log D(G(z)) is equivalent to minimizing it\u2019s negative and since the BCE-Loss definition has a minus sign, we don\u2019t need to take care of the sign. Similarly to the Discriminator, if we set v\u1d62 = D(G(z\u1d62)) and y\u1d62=1 \u2200 i, we obtain the desired loss to be minimized. Last thing before we run our algorithm, we want to visualize how the training process develops while our GAN learns. To do so, we will create a static batch of noise, every few steps we will visualize the batch of images the generator outputs when using this noise as input. Now that we\u2019ve defined the dataset, networks, optimization and learning algorithms we can train our GAN. This part is really simple, since the only thing we\u2019ve got to do is to code in python the pseudocode shown earlier on traning a GAN (see Training a GAN). We\u2019ll be using all the pieces we\u2019ve coded already, plus the logging file I asked you to download earlier for this procedure: for epoch in range(num_epochs):\n\n for n_batch, (real_batch,_) in enumerate(data_loader):\n\n N = real_batch.size(0) # Generate fake data and detach \n\n # (so gradients are not calculated for generator)\n\n fake_data = generator(noise(N)).detach() # Display Progress every few batches\n\n if (n_batch) % 100 == 0: \n\n test_images = vectors_to_images(generator(test_noise))\n\n test_images = test_images.data And that\u2019s it, we\u2019ve made it! \ud83c\udf8a In the beginning the images generated are pure noise: Until you get pretty good syntethic images, It is also possible to visualize the learning process. As you can see in the next figures, the discriminator error is very high in the beginning, as it doesn\u2019t know how to classify correctly images as being either real or fake. As the discriminator becomes better and its error decreases to about .5 at step 5k, the generator error increases, proving that the discriminator outperforms the generator and it can correctly classify the fake samples. As time passes and training continues, the generator error lowers, implying that the images it generates are better and better. While the generator improves, the discriminator\u2019s error increases, because the synthetic images are becoming more realistic each time."
    },
    {
        "url": "https://medium.com/ai-society/what-happens-next-on-ai-and-jobs-ad327a2f2d90?source=---------1",
        "title": "What happens next? (a opinion about AI and jobs) \u2013 AI Society \u2013",
        "text": "When you actually sit to think about it, it turns out to be really complex, especially as the effects are starting to be visible right now. I'm talking about AI and its effects on jobs.\n\nBeing immersed in these kinds of topics makes you realize it\u2019s not insane to come to the following conclusion: Sooner or later, a machine will be able to perform any job better than a human. A machine will most likely become a better doctor, a better chef, a better psychologist \u2014yes, a better psychologist \u2013 , a better lawyer, a better programmer, you name it. It\u2019s really just a matter of time.\n\nThe reason I believe this is relatively simple. Firstly, it is worth mentioning the remarkable success that AI has had recently in a large number of fields. Cars can now drive themselves, computers are learning to play some Atari games better than humans, and even read lips better than humans. But more than this, computers are very good at doing mathematical computations, and a lot of the core of machine learning is mathematics. An optimization problem. And while brains are incredible biological machines, a computer really has the potential to take every single variable into account and calculate outputs that are closer to a global optimum than the brain would.\n\nNow, the key difference between yesterday's and today's techniques is that we are designing algorithms that don't implicitly solve a problem, but rather let the program extrapolate and learn what it needs to solve it. Instead of explicitly designing a program to recognize handwritten digits we design a program which, by seeing examples of a lot of different digits and what the answer should be, learns a method (a-priori unknown to the programmer) to recognize them with near-perfect accuracy. This is an incredibly remarkable achievement. Computers are learning to solve problems, rather than following a pre-crafted recipe for solving them. This resembles the way our brains learn, and it's producing absolutely impressive results so far.\n\nBut the purpose of this article is not to convince you that computers will most likely surpass humans at almost every task. So if you are not convinced yet about this there are plenty of other resources than could help. You could also take a look at the following good video from CGP Grey to have a broader view of the situation. Either way, I would also recommend watching it since it complements this article's introduction.\n\nIn summary, I see no reason to doubt that AI will continue to evolve at an ever increasing (exponential) pace. In fact, AI can also help the very research of AI since Machine Learning is becoming one of the core components of Data Science, the art of processing and making sense of data.\n\nAs AI evolves and becomes more capable it starts to displace humans at the tasks that they start to do better (or comparatively good). Why? For one, machines work 24 hours, 365 days a year, without complaining. And, if they do the job better and faster than humans, it's the perfect combination for it to be a huge deal for companies. From their point of view they are being a lot more profitable, they reduce costs and increase their quality (and quantity if applicable). Overall, they are offering better products or services at lower prices. This is not a new thing, as the Industrial Revolution showed us. However, now it's the time for intelectual jobs rather than technical ones.\n\nThis, by itself, is not bad. We are producing better things, and even better knowledge. Just imagine putting machines to think about the most difficult problems humanity has right now, 24/7. The largest and most efficient research center driven by machines that don't sleep, working for the well-being of us and asking nothing in return. We would just set them and wait to see what they discover. Physics, chemistry, medicine, mathematics. In general, any area of knowledge. Envision AI super-computers dedicated to finding the cure to cancer, or to solving the most challenging questions of the universe.\n\nDue to these amazing possibilities I would say research on AI is not going to stop. We are approaching (and already began) a phase of profound change. We already went through the phase of seeing how machines can perform a lot of repetitive jobs better or as good as humans during the Industrial Revolution. But now we are beginning to see how machines can also outperform humans (or do equally or sufficiently good) at tasks we wouldn\u2019t have ever thought, intellectual tasks. As this process happens we start loosing jobs, only this time the new jobs that replace (temporarily) the lost ones will requiere a high level of knowledge, education.\n\nIn the beginning new jobs emerge, as we are seeing today and as we saw during the Industrial Revolution. New jobs in the fields of data science and machine learning have been created recently. However, as progress is made more and more jobs also disappear (exponentially), and at a faster rate than the ones emerging. And this is not something that will happen, it's something that is happening right now as you have probably seen in the news and in your everyday life. Eventually, and as we have discussed earlier, we can get to the point where there are little to no jobs a machine can perform better than a human. If that happens, that would be it: the ultimate revolution.\n\nIndependently of what happens \u2014i.e. if machines outperform humans in any intellectual task\u2014 the economic system is going to suffer. Jacque Fresco, the proposer of something big he called The Venus Project \u2014we'll get to this later\u2014, had a position on this. In one of his talks he remarked precisely on how machines are replacing jobs in multiple sectors of the industry. As we discussed earlier, he said this is going to happen naturally since industries find it far more profitable to have as less humans as possible. No wages or any other legal requirement attached to regular contracts, much faster (and usually better) work, and to top that, no 9\u20135 journeys during weekdays but 24/7 ones 365 days a year.\n\nAs jobs get replaced humans start to make their way out, and as this happens productivity will rise substantially while purchasing power falls drastically: the collapse of the money system. Production is off the charts, but no one has money to buy things. What should we do then? This is my big question and is what gives this article it's title. It's almost certain that economy will collapse, what happens next? We are clearly not prepared for this, and if you think about it, any attempt to prevent it would be dumb, or even unethical.\n\nOne option would be to implement laws to regulate companies and force them to hire people. How should we handle jobs that are no longer doable by humans because of their effort or their precision? How to be fair to all companies? Maybe we come up with some solution to make this work, but, what are we accomplishing? Does it even make sense to force companies to be more inefficient? In the end, machines give companies the potential to produce better products and services \u2014also at a lower price but that's irrelevant since costumers wouldn't be able to afford it.\n\nMaybe another solution then is to stop research, make research on AI illegal, as ridiculous as it sounds. This could certainly stop progress and probably save the economic system. But again, what would we be accomplishing? What is it worth more to us, money, or progress? Should we stop the potential of discovering the cure for cancer in order to save the economic system? Should we agree to let people die in countless driving accidents caused by human error in order to prevent driverless vehicles from taking over the transportation industry? Should we make people waste time in lines in supermarkets to preserve human cashiers? Should we reduce the potential of understanding the biggest mysteries of the universe to prevent AI from evolving?\n\nSo, we will be faced with some controversial decisions, and this is the main point I want to make here. A crisis will come, but this will not be like the rest of the economic crises of the past. This time we will reach a point where money itself stops making sense. Money rewards work, but if there are few to no humans doing work then money loses its very reason of existence. Machines work for us and they don\u2019t ask for anything in return \u2014 hopefully we are smart enough so this doesn\u2019t change.\n\nIf we continue with things as they are today research will be made, technology will improve, and so will services, products, and health. But many jobs will cease to exist and we are already seeing the effect. Naturally, repetitive jobs will be the first to suffer. Technology like Google's Waymo project (Google's version of a self-driving car) and autopilots will displace every human from the transportation industry. And those are not technologies far in the future. We are talking 1\u20132 years before we see comercial, fully autonomous vehicles out there, and you guessed it, they are very profitable for the transportation industry. If you are not convinced, you might take a look at what Uber has already in the works.\n\nJust taking away transportation jobs from humans is enough to create a giant disruption in the system. Where are these people going to go? During the economic crisis that is beginning to happen new, temporary, jobs will be created (temporary since they\u2019ll most likely be replaced later as well). The difference is that these jobs will all have a common denominator: they will be intellectual and require education. Repetitive jobs will come to an end and people will be forced to seek education if they want to survive.\n\nIf you ask me, stopping progress is not a very clever solution, and I would think governments will probably agree. If that is so, an economic crisis will happen, and it will be the ultimate crisis since it will take the monetary system down. I will not disagree, the transition will be very hard, specially for those doing the jobs that will disappear in the first wave. I'm eager to see what people figure out to ease this transition, but something big is about to happen.\n\nAnd so we get to our ultimate question. What happens next? What happens after the crisis? I can't answer this question since only time will tell, but I would say our way of living will see a dramatic change in the coming decades. I think money will lose all of its fundamental value, and that's when we can remove it altogether and start to give things away for free.\n\nIf you want an idea of a solution, the recently died Jacque Fresco thought about one for some time during his life. His response was The Venus Project, a resource-driven economy. He focuses precisely on the idea of removing money from the system and moving on to a way of life in which we are given things for free. Everyone would have a very high standard of life, and this would be possible since machines do mostly everything. In fact, one of Fresco's points is that after removing money we remove most of the problems humanity faces right now: poverty, corruption, most forms of violence and war, among many other problems which derive from money. We could stop worrying about work and focus on doing other things. Is this the right path? Who knows, but what is? We would have to take a lot of care, for example, with respect to the psychological implications this would have on people.\n\nBut my point isn't to discuss The Venus Project in detail. I just referenced it so you could get an idea of what could be next. In summary, the point that I want to make could be summarized as follows. Something big will happen in the coming decades, and this is because it doesn't make much sense to even try to prevent progress. If we have the potential to save lives, shouldn't we? If we can make key advances in medicine, shouldn't we? If we have the potential to solve the most challenging mysteries of the universe, shouldn't we? As a side-effect, money will most certainly stop making sense, and yes, the transition to a money-less system will be hard. Humanity will probably undergo the biggest change in history, and in some decades from now life will be nothing like it currently is. In my humble opinion, this is something worth thinking from now since something big is about to happen, and its going to happen sooner than we realize."
    },
    {
        "url": "https://medium.com/ai-society/why-convolutional-neural-networks-are-a-great-architecture-for-machine-translation-9258ca1263a8?source=---------2",
        "title": "Why Convolutional Neural Networks are a Great Architecture for Machine Translation",
        "text": "Facebook AI Research recently posted a paper in which a Convolutional Neural Network architecture is proposed for machine translation instead of a Recurrent Neural Network architecture, which has been the convention until now. In this post we will explain why a CNN-based architecture might become the standard for machine translation (and even other NLP tasks) in the feature.\n\nConvolutional Neural Networks basics\n\nLet\u2019s first understand how CNNs work in order to explain why they have certain advantages over RNNs. \n\nThe basic concept underlying CNNs is that we compute a vector for every possible phrase. For instance take the string \u201cmy favorite AI blog\u201d. Then we compute the word vector representation for: \u201cmy favorite, favorite AI, AI blog, my favorite AI, favorite AI blog\u201d. Once we have that, we compute all the bigram vectors until we reach a top vector.\n\nThe computation of these bigrams (or n-grams in more complex but convenient CNN architectures) can be parallelized, the time-step-based computations in a RNN architecture can\u2019t. This data structure has been really successful in classifying images; for instance let\u2019s take the image of a cat as an example; it first processes clusters of pixels, then recognizes shapes, then recognizes parts of the image (ears, legs, tail, etc.) and it finally recognizes there is a cat in the picture.\n\nConvolutional Sequence to Sequence Learning\n\nGehring et al., (2017)proposed using CNNs because contrary to RNNs computation can be parallelized, optimization is easier since the number of non-linearities is fixed and independent of the input length and last because they outperform the LSTM accuracy in Wu et al., (2016). In addition to that the algorithm for capturing these dependencies scales in O(n/k) instead of O(n) due to the hierarchical structure.\n\nDespite being known that convolutions have several advantages since the early days, such as the ones presented by Waibel et al., (1989) and LeCun & Bengio, (1995), they solely create representations for fixed sized contexts. RNNs allowed to create representations for variable sized contexts and LSTMs and GRUs tackled the problem of RNNs not capturing long-range dependencies. These were the reasons why RNNs became the standard for machine translation and CNNs became more widely adopted in fixed sized contexts such as image processing. But the convolutional architecture presented by Gehring tackles these problems and outperforms RNNs.\n\nConvolutional Architecture\n\nIn this section of the article we\u2019ll explain in a summarized way what the fully convolutional architecture proposed by Gehring consists of; the first step is embedding input elements in a distributional space and giving a sense of order to the model by embedding the absolute position of input elements, then both vectors are combined and we proceed similarly for the output elements that were already generated by the decoder network.\n\nBased on these input elements, intermediate states are computed both for the encoder and decoder networks. The computation of each of these states is called a block, and each block contains a one-dimensional convolution followed by a non-linearity. Gated Linear Units, as proposed by Dauphin et al., 2016, are the non-linearity that implement a gating mechanism over the output of the convolution. Ultimately, the softmax activation function is used to compute a distribution over the T possible next target elements.\n\nThen we proceed to compute attention. We combine the current decoder state with an embedding of the previous target element to get the current decoder state summary. The current attention for the current decoder layer, state, and source element is computed by taking the dot-product between the decoder state summary and each output of the last encoder block.\n\nNext, the conditional input for the current decoded layer is computed with a weighted sum of the encoder outputs and the input element embeddings. Once this is computed, it\u2019s added to the output of the corresponding decoder layer.\n\nFinally, a normalization strategy and a careful weight initialization are applied in order to ensure that the variance across the network doesn\u2019t change dramatically which results in a stabilized learning. This way we lastly reach our desired translation in the target language.\n\nExperimental Setup and Results\n\n3 major WMT translation tasks were used by Facebook AI Research to compare both architectures. The BLEU algorithm, which is used to evaluate how much correspondence a machine-translated text has with a professional human translation, was used to benchmark translations. For English-Romanian, the convolutional architecture surpassed by 1.8 BLEU. For English-French the difference was 1.5 BLEU. For English-German it was 0.5 BLEU."
    },
    {
        "url": "https://medium.com/ai-society/how-language-translators-will-work-c85a70cc4f3a?source=---------3",
        "title": "Recurrent Neural Networks for Language Translation \u2013 AI Society \u2013",
        "text": "Tools that allow any person to communicate with any other person truly make the world a better place. The Rosetta Stone was the first of such tools, evolving to dictionaries and eventually to sophisticated systems such as a language translator that you\u2019ve probably used before, Google Translate. Deep Learning will soon change how these systems work, and the models that will enable such thing have all kinds of applications in NLP even outside the realm of machine translation (such as building an opinion generator, which part of the AI-Society team will actually hack on, so stay tuned).\n\nLet\u2019s first talk about recurrent neural network (RNN) based language models. Yoshua Bengio proposed using artificial neural network based statistical modeling for computing the probability of a sequence of words occurring. This approach proved to be successful; however, feedforward neural networks don\u2019t allow to receive variable length sequences as an input which limits the power of the model. Since RNNs allow variable length sequences both as an input and as an output, they are naturally the next step in statistical language modeling. [1] The RNN architecture is presented in the diagram below.\n\nIn this model we are given a set of word vectors as an input, we have t time-steps which are equivalent to the number of hidden layers, these layers have neurons (each performing a linear matrix operation on its inputs followed by a non-linear operation). Time-steps generate the output of the previous step and the next word vector in the text corpus is passed as an input to the hidden layer to generate the prediction of a sequence (conditioning the neural network on all previous words).\n\nThese are the basics of RNNs. However simple RNN architectures have problems which were explored by Bengio et al. In practice, simple RNNs aren\u2019t able to learn \u201clong-term dependencies\u201d. Let\u2019s analyze the following example in which we try to predict the last word in a sentence:\n\n\u201cI prefer writing my code in Node JS because I am fluent in ______.\u201d\n\nThe blank could probably be filled with a programming language, and if you know about backend development you might know that the answer is JavaScript. In order for a program to know this the program needs some context about Node JS and JavaScript from somewhere else in the text. Two fancy types of RNNs solve this problem, Long Short-Term Memories (LSTMs) and Gated Recurrent Units (GRUs). The TensorFlow documentation has an amazing tutorial on language modeling with LSTMs so for the purpose of this blog post we will do the same thing with GRUs.\n\nGRUs were introduced by Cho et al. The main idea behind this architecture is to keep around memories to capture long distance dependencies and to allow error messages to flow at different strengths depending on the inputs.\n\nInstead of computing a hidden layer at the next time-step, GRU first computes an update gate (which is another layer) taking the current word vector and hidden state as parameters.\n\nThen a reset gate is computed with the same equation but with different weights.\n\nIf the reset gate is 0, it only stores the new word information in the memory (reset).\n\nThe current time-step combines current and previous time-steps to compute the final memory.\n\nNow that you understand the architecture of a GRU cell, you can do some really interesting things. For instance you could train this model and compare the perplexity that an LSTM yields in comparison with a GRU. You could also modify this piece of code and build your own language translator using GRUs instead of LSTMs.\n\nTraditional machine translation models are bayesian and at a very broad scope what they do is that they align the source language corpus with the target language corpus (usually at a sentence or paragraph level), after many repetitions of the alignment process each block (sentence or paragraph) has many possible translations, and finally the best hypothesis is searched with Bayes\u2019 Theorem.\n\nEDIT: The papers cited in this post are from 2015 and before. On March 2017 -date in which this post was written- we\u2019ve been told that these systems are already used in production, replacing the mentioned bayesian systems.\n\n[1] There are other reasons. For example, the RAM requirement only scales with the number of words."
    },
    {
        "url": "https://medium.com/ai-society/the-lisp-approach-to-ai-part-1-a48c7385a913?source=---------4",
        "title": "The Lisp approach to AI (Part 1) \u2013 AI Society \u2013",
        "text": "If you are a programmer that reads about the history and random facts of this lovely craft, and practice it ad honorem \u2014 just for fun \u2014 , you have found yourself reading about a programming language called Lisp. Some praise it as a software miracle, as the best tool for programming. Some even dare to call Lisp one of the best programming languages ever invented (even if that doesn\u2019t make sense at all). After all, before Python, Scala, Haskell, there was programming, and before Deep Learning there was Artificial Intelligence. Great hackers that love Lisp:\n\nThree of our luminaries, along with Marvin Minsky (the guy referred by Floyd), and John McCarthy (the inventor of Lisp), were awarded with the Turing Award. So why do many CS celebrities talk so good about a simple programming language? Lisp is famous nowadays because of the things others have said about it, but in the early days of AI, Lisp was the de facto language to express ideas related to natural language processing, computer assisted geometry, text generation, AI planning, and automated theorem proving. Yes, there was AI before Machine Learning, indeed, there was an AI winter before the boom of neural networks and statistical approaches to AI, but that\u2019s a topic that deserves an entire single post.\n\nThe progress, development, and evolution of Lisp was tightly related to the early progress, development, and evolution of Artificial Intelligence. Two of the guys mentioned before were pioneers in AI. John McCarthy, the creator of Lisp, coined the term Artificial Intelligence, while Marvin Minsky shaped the content of the new field by founding the AI lab at MIT. Many of their students were the developers of the first digital milestones of artificial intelligence.\n\nPrograms for natural language understanding and generation, game playing (the link contains a paper from the man who introduced the term Machine Learning), theorem proving, early computer vision, symbolic mathematics (specially integration), problem-solving and knowledge representation, were produced at Stanford and MIT using different dialects of Lisp as a tool to express those ideas in. Was it just a coincidence, or is there something special with the idea (not just the language) of Lisp? This is a list of some classic AI programs that were expressed in Lisp.\n\nThe progress of AI in its early days was not because of Lisp, I do think CS subjects should be agnostic of the language they express their ideas in. Lisp was used on the early days of AI because it was flexible enough to allow quick experimentation and prototyping (REPL), and it introduced fundamental ideas that were cool and fresh at the moment (IF-THEN-ELSE construct, recursion, and Garbage Collection). Those features proved themselves to be useful to express the kind of the ideas AI people needed to express. This innovation, and the rapid adoption of Lisp for AI (in labs and projects) helped the language grow and become a standard AI language.\n\nOf course all these programs could have been written in other languages, but Lisp was an accepted and highly praised vehicle to explore and implement these kind of ideas at the moment.\n\nAt this point, you may think that Lisp was just an academic invention to teach and implement symbolic AI programs. But the rapid adoption of Lisp in academia, implied a massive effort to embrace Lisp (or any of its descendants) in real-world production ready software. The following is a collection of some of those programs; most of the programs included in this list are still running on production environments, while the other part of it used to backup large pieces of software in well-known projects or companies.\n\nYou need to know that Lisp and its dialects have evolved a lot since McCarthy defined it for the first time, but most of the original idea of Lisp has been preserved in its descendants. Most of the semantics of Lisp has been an invariant in most Lisp\u2019s implementations that were capable to power or support, in one way or another, the operation of the following projects.\n\nOne of Lisp\u2019s main virtues, is that it enables a programmer to create new linguistic abstractions with ease. So there should be not surprise in the fact that Lisp has influenced many popular programming languages; two of them \u2014 very close to the AI/Data Science/ML community (besides from Lisp itself) \u2014 , which are R and Julia.\n\nMost of the programs mentioned earlier made heavy use of symbolic manipulation. As mentioned by Carlos E. Perez in his post The Many Tribes of Artificial Intelligence, before ML and the Neural Network boom, there were symbolic based approaches to AI that combined symbolic manipulation of some elements, following a collection of rules that were modeled with the purpose to encapsulate the behavior of an intelligent system. The problem those days was not the efficient computation of numerical problems, but the manipulation and synthesis of symbols.\n\nJust as C, C++, and Fortran shine in numerical computation where performance matters the most, Lisp shines in symbolic manipulation. One of Lisp\u2019s greatest strengths is being able to handle efficiently symbols and lists.\n\nLisp is not a perfect language, it has many flaws (lots of dialects, lack of well-known libraries, weird syntax that does not contribute to attract people in, dynamic typing, etc.), but it was a well-suited tool for the problems AI pioneers were trying to tackle at those days, just the same way C/C++, or Fortran are a perfect choice to implement the underpins of a Deep Learning system (TensorFlow is implemented both in C++ and Python). There\u2019s not a single Swiss army knife programming language, we do need to pick a language that suits the most the particular task we\u2019re approaching.\n\nThe whole idea of this series is to use Lisp, more specifically, its dialect Scheme to explore Artificial Intelligence (AI is much more than programming, and AI programming is much more than Lisp) related ideas. The goal is to learn together about classical AI concepts such as general problem solving, text generation, symbolic mathematics problems, knowledge representation, expert systems, search, NLP, logical and stochastic reasoning, game playing, and even \u201ccontemporary\u201d stuff such as neural networks using the Scheme programming language to express those ideas.\n\nLet\u2019s begin our journey exploring Artificial Intelligence using Lisp. Your homework for the next post in the series is to install MIT-Scheme on your machine."
    },
    {
        "url": "https://medium.com/ai-society/my-first-experience-with-deep-reinforcement-learning-1743594f0361?source=---------5",
        "title": "My first experience with deep reinforcement learning",
        "text": "Note: This article assumes previous knowledge on the basics behind neural networks and Q-learning\n\nAbout six months ago I saw myself in the need of deciding a topic for my undergraduate thesis project. Since there wasn\u2019t much of AI in my major\u2019s curriculum I chose to do research in that field to gain some knowledge. Now, I had to decide which AI subtopic I wanted to work on and it clearly became clear to me which one it should be.\n\nI have always been fascinated by neural networks and their ability to learn to approximate any function at all. I have always thought that this is an absolutely remarkable feature since many (if not all) problems can be modeled as a function (i.e. something that takes some input, does some processing, and produces some output). It seems to me that, while we are still far from getting there, neural networks could play a very important role in the path toward the ultimate goal of reaching a general AI.\n\nOn the other side, in the last years a small company called DeepMind \u2014 now owned by Google \u2014 had shown great advances in reinforcement learning, and specifically what it is calling deep reinforcement learning (i.e. combining neural networks with reinforcement learning). In the case of Q-learning the principle behind this is that since neural networks are very good function approximators then why not use them to approximate the Q-function? Deep learning with Q-learning is a very cool concept since other techniques that were used before to approximate the Q-function quickly became unfeasible once the state representation grew in dimensionality. Using the described technique enabled DeepMind to make an algorithm capable of playing many Atari games better than professional human players while not explicitly coding the logic and rules of each game [1]. In other words, the algorithm learned by itself what it was best to do by just looking at the pixels of the game, the score, and given the ability to choose an action (i.e. manipulate the controls of the game) like any human player would be able to.\n\nBut more than this, reinforcement learning is another of the fascinating sides of machine learning since it resembles the way we humans learn. Everything we do in life gets us a reward in return, be it positive or negative. Doing a good job will get us the approval of our colleagues, our boss, money, or even a smile from who we benefited with the job. Those things feel good, our brains release dopamine so we want to do them again, a positive reward. But getting into a car crash doesn\u2019t feel good, so the next time we will try to be more careful since we don\u2019t want that to happen again. We want to maximize our rewards, we learn, and we do it by reinforcement. With experience we get better at doing something, just as reinforcement learning algorithms do.\n\nThat said, with both deep learning and reinforcement learning we can model a huge variety of the problems we as humans face every day, and this is what makes them very interesting. These techniques are what power systems like autonomous cars for example. Could they be the answer to achieving a general AI? Only time will tell, but they are certainly getting us to interesting things.\n\nFrom the results that DeepMind published in one of its papers, one of the graphs looked like this:\n\nThe above graph shows how DeepMind\u2019s algorithm performed with respect to \u201cthe best reinforcement learning methods in the literature\u201d. However, the interesting part is the line in the middle which shows how the algorithm performed in comparison to professional human players. The performance of the algorithms were normalized with respect to the performance of the human players (100% level). As you can see the performance of the algorithm with games like Ms. PacMan was really low. They don\u2019t specifically mention the reason behind this but it seems to be related to the relatively long-term planning that the game requires, combined with the fact that Q-learning as it is commonly implemented is known to have these kinds of temporal limitations.\n\nAfter reading the publication some questions came to me from the approach that DeepMind was having, specifically with the fact that they were were using the very pixels of the game as the state representation. This is remarkable since it is the same information that our brains receive as input, and it is also very good in the sense that it generalizes very well for other games. However, I had the doubt about what would happen if we gave the agent more \u201ccalculated\u201d information, i.e. a state representation composed of information other than the pixels. What kind of impact did the state representation have in the learning process and result? This is when I decided to work with a game (PacMan), write a deep Q-learning agent in Python and look for answers.\n\nTo clarify, I wasn\u2019t pretending to improve the performance that DeepMind achieved in the games below the human line. What I\u2019m trying to show is how the questions that turned into my senior thesis came to be, and it was that \u201chuman-level\u201d line that sparked those questions in me.\n\nNow, finding what was the impact of changing the state representation was the main objective of the research at first, however, one more question arose during the process that I thought was worth investigating, namely: how did (or if) both, varying the topology of the neural network and having a persisted experience replay file before beginning training affected the learning process and the result of the algorithm.\n\nTo expand a little on the second part, experience replay is one of the tricks that has been discovered to be one of the most important optimizations to make that will enable the neural network to learn in a reasonable time (or even converge). This is because this technique breaks the concept of continuity between any two transitions while giving the network a chance to also reinforce its knowledge of previous experiences more efficiently. What I wanted to know was, given that experience replay is helpful, could it be also helpful to have a large pre-populated (persisted) experience replay memory right from the start? Could this help the algorithm to get to convergence faster than having to populate the replay memory each time from zero?\n\nAt this point I would be telling you much of what is written in the report [2], so I would encourage you to read the paper if you find the research interesting. In a nutshell though, I discovered that all three aspects affect the learning process considerably. Firstly, I could see that the state representation should be as simple as possible (but complete) since simpler state representations are considerably easier to train on. Secondly, I found that having a pre-calculated, large, persisted replay memory has the potential to improve learning speed notably but one should take some precautions so having one does not bias what the agent learns (e.g. when past experiences greatly outnumber new experiences). Lastly, I could also see that changing the topology of the neural network does have an important impact in the learning result. The hypothesis that I could extract is that larger networks take considerably longer to train and were not able to train on time, so one should choose an appropriate topology (i.e. one that is complex enough to be able to approximate the Q-function for the particular problem, but not more complex than that).\n\nI had a lot of fun with this project, but what\u2019s more, I learned a lot. Now, from my experience, I think reinforcement learning has the potential to be very powerful, especially in combination with neural networks. However, this combination is what can make the process a little frustrating if you are expecting your model to learn in a matter of a couple of hours and then win every game you play against it. The reality is that neural networks, while very powerful, can require very fine tuning to actually learn something. But more than this, you have to be very careful, for example, with the parameters you choose, the topology, and the activation functions as some of these aspects can represent the difference between a neural network that does a very good job in a reasonable time and a neural network that doesn\u2019t learn anything. In summary, getting a good model can require many optimizations and dedication, however when you achieve one the results can be very surprising (as groups like DeepMind have shown us).\n\nAnother aspect that can get difficult to handle is the computational complexity of training a model of this kind. If you have a good GPU sitting on your desk you can improve training time by quite a lot, since neural networks are really benefitted by massive parallelism. However, not many have a GPU to spare, so testing can become a little tedious as each training session can take several hours or even days depending on the problem.\n\nTo sum up, you will learn a lot from doing different experiments. Nonetheless, if you plan to get immersed in deep reinforcement learning (puns not initially intended) I would recommend to first have a good understanding of neural networks, some patience, and a good machine / GPU could also be very helpful depending on the problem.\n\nSince it\u2019s a game I really like \u2014 I mean, who doesn\u2019t?"
    },
    {
        "url": "https://medium.com/ai-society/hello-gradient-descent-ef74434bdfa5?source=---------6",
        "title": "Hello, Gradient Descent \u2013 AI Society \u2013",
        "text": "With this in mind, we can repeatedly perform these steps in the appropriate direction and we should eventually converge into the (local) minimum. Following our analogy, this is the equivalent of arriving to the bottom of our mountain.\n\nIn other words, think about the function\u2019s surface as a mountain that you are hiking down. You know that your goal is to reach the bottom, and you may think that the fastest way to accomplish this is by proceeding through the path that makes you descend the most. In this case, that path points to the opposite of the steepest mountain direction upwards.\n\nThe inspiration behind Gradient Descent comes directly from calculus. Basically, it states that if we\u2019ve a differentiable function , the fastest way to decrease is by taking steps proportional to the opposite direction of the function\u2019s gradient at any given point. This happens because the gradient points to the steepest direction of the function\u2019s generated surface at the current point.\n\nHi there! This article is part of a series called \u201cHello, <algorithm>\u201d . In this series we will give some insights about how different AI algorithms work, and we will have fun by implementing them. Today we are gonna talk about Gradient Descent, a simple yet powerful optimization algorithm for finding the (local) minimum of a given function.\n\nSo we\u2019ve been talking about taking steps in the right direction, but how can we calculate them? Well, the answer is in the following equation:\n\nThis formula needs some clarification. Let\u2019s say we are currently in a position \u0398\u2070, and we want to get to a position \u0398\u00b9. As we said previously, every step is gonna be proportional to the opposite direction of the function\u2019s gradient at any given point. So this definition of step for a given function J(\u0398) will be equal to \u2212\u03b1\u2207J(\u0398).\n\nRemember that we take steps in the opposite direction of the gradient. So in order to achieve this, we subtract the step value to our current position.\n\nThe symbol \u03b1 is called the Learning Rate. This is a value that will force us to take little steps so we don\u2019t overshoot the (local) minimum. A bad choice for \u03b1 would trap us into one of the following possibilities:\n\nTake a look at the following examples to see what happens when we make a bad choice for the Learning Rate \u03b1:\n\nThe gradient of a function J(\u0398) (denoted by \u2207J(\u0398)) is a vector of partial derivatives with respect to each dimension or parameter \u0398\u1d62. Notational details are given in the equation below:\n\nTo make this definition of gradient clearer, let\u2019s calculate the gradient of the following function:\n\nAs we can see, this function contains three parameters or dimensions. Thus the appropriate way to proceed is by calculating the partial derivative with respect to each param:\n\nNow we can group those values and that will give us the function\u2019s gradient:\n\nAnd that\u2019s it! With this vector, we can get the steepest direction at any given point simply by replacing each parameter with its corresponding value:\n\nAnd now, for the grand finale, we will go through a full example and we will code our own algorithm for gradient descent.\n\nIn this section, we will apply linear regression in order to find the correct function approximation for a given set of points in a plane. The set of points we are trying to predict looks as follows:\n\nAs it\u2019s common, the choice for J(\u0398) will be the least-squares cost function for measuring the error of an approximation:\n\nIn the equation above:\n\nFinally, before beginning to code let\u2019s calculate the gradient vector of our function. You can see that as we\u2019ve got two parameters for \u0398, we will need to calculate two partial derivatives.\n\nOkay, time to proceed. It\u2019s important to mention that our implementation will be in a vectorized form. This means that we will transform all the formulas mentioned above into matrices operations. The advantages of this implementation are that code will be more concise, and with this our computer can take advantage of advanced underlying matrix algorithms.\n\nTo work with the vectorized form, we need to add a dummy variable x0 to each point with a value equal to 1. The reason for this is that when we perform matrix multiplication, the intercept parameter \u03980 will be multiplied with that 1 and it will maintain its value as the defined equations establishes.\n\nBelow you can see the vectorized form of the error function J(\u0398) and its gradient \u2207J(\u0398):\n\nWith every function defined, we can proceed to code our algorithm. The first thing we should do is to declare the points dataset and the Learning Rate \u03b1.\n\nNow we can proceed by defining the error function J(\u0398) and its gradient \u2207J(\u0398). Remember everything will be defined in a vectorized way.\n\nThis is the heart of our code. Here we will perform steps that update \u0398 until we reach the (local) minimum. That is, when all the values of the gradient vector are less than or equal to some specified threshold (1/e\u2075 in this case).\n\nAnd we\u2019re done! You can see the complete code in the snippet below:\n\nNow we can run our algorithm and it will give us the optimal values for \u0398 that minimize the error. Below you can see the answers I obtained after running it on my computer:\n\nThis is the scatter plot we showed before with the line corresponding to the optimal \u0398:\n\nWell, we\u2019ve finished our code and our article. I hope that you\u2019d learned one thing or two about Gradient Descent, and more importantly, that you are now really excited about learning by taking a look at the further reading list."
    },
    {
        "url": "https://medium.com/ai-society/a-concise-recommender-systems-tutorial-fa40d5a9c0fa?source=---------7",
        "title": "An Introductory Recommender Systems Tutorial \u2013 AI Society \u2013",
        "text": "In statistics, the Pearson correlation coefficient is a measure of the linear dependence or correlation between two variables X and Y. It has a value between +1 and \u22121 inclusive, where 1 is total positive linear correlation, 0 is no linear correlation, and \u22121 is total negative linear correlation. In the case of recommender systems, we\u2019re supposed to figure out how related two people are based on the items they both have ranked. The Pearson Correlation Coefficient (PCC) is better understood in this case as a measure of the slope of two datasets related by a single line (we\u2019re not taking into account dimensions). The derivation and the formula itself are harder to find and understand, but by using this method, we\u2019re eliminating the weight of harshness while measuring the relation between two people.\n\nThe PCC algorithm, requires two datasets as inputs, those datasets don\u2019t come from how people ranked the items, but they come from the common ranked items between two people. PCC helps us to find the similarity of a pair of users. Rather than considering the distance between the rankings on two products, we can consider the correlation between the users ratings.\n\nTo clarify the concept of correlation, we include a new dataset and some charts. The dataset, includes few ratings of some remarkable computer scientists to some CS books.\n\nIn order to understand how related are two people, we proceed by plotting their preferences (treating each book as a point, whose coordinates are determined by the rating on this item by both users). Once we have that specific plot, we do need to find the best fit straight line over those points. Finding such a line, requires knowledge of linear regression, a topic that\u2019s out of the scope of this tutorial. While finding the best fit straight line, is not as trivial as it seems, finding the PCC depends just on the data that we already have. This best fit line serves us to explain the concept.\n\nThe plot shows the 2-dimensional space defined by the ratings of Ullman and Carmack, as well as the best fit straight line. The positive slope of the line, shows a positive correlation between those points, then, the PCC for Ullman and Carmack is positive.\n\nThe last plot, shows a negative correlation between Navarro and Norvig.\n\nIf we have one dataset {x[1], x[2], \u2026, x[n]} containing n elements, and another dataset {x[1], x[2], \u2026, x[n]} containing n elements, the formula for the sample PCC is:\n\nA little algebraic manipulation, yield us to the following formula\n\nThis formula, let us write a program to compute the PCC between two people.\n\nBoth similarity measures allow us to figure out how similar two people are. The logic behind a recommender system, is to measure everyone against a given person and find the closest people to that specific person, we can do that by taking a group of the people for whom the distance is small, or the similarity is high.\n\nBy using this approach, we\u2019re trying to predict what\u2019s going to be the rating if our person rates a group of products he has not rated yet. One of the most used approaches to this problem, is to take the ratings of all the other users and multiply how similar they are to the specific person by the rating that they gave to the product. If the product is very popular, and it has been rated by many people, it would have a greater weight, to normalize this behavior, we do need to divide that weight by the sum of all the similarities for the people that have rated the product. The following function implements this approach.\n\nGiven a person included in the index (data), a bound (that is maximum number of items to recommend), and a function to measure the similarity between people (euclidean_similarity, or pearson_similarity), this function gives an estimate on how the person would rate the item according to how similar people rate the item. As an example:\n\nWhile Algorithms and Concurrency are perfect topics to recommend to Alan Perlis, or at least that was what our algorithm found, we should keep Marvin Minsky far from the Databases item. There is a strange phenomenon here, depending on the similarity measure, Marvin Minsky seems to like a lot or dislike a little bit the Programming language theory and Formal methods items. By looking for the scores variable while inspecting the code if you call recommend(\u201cMarvin Minsky\u201d, 5), you can tell that Robin Milner and John McCarthy are the closest to Marvin Minsky, while both Robin Milner and John McCarthy are very different from each other; and also Robin Milner tends to rate a little bit harsher than John McCarthy. That insight clearly taught us that we do need to compare both measures depending on the nature of our data, the election of bound also affects this kind of strange recommendations.\n\nData exploration, and wrangling comes as significant factors while implementing a production recommender system. The more data it can process, the better recommendations we can give our users. While recommender systems theory is much broader, recommender systems is a perfect canvas to explore machine learning, and data mining ideas, algorithms, etc. not only by the nature of the data, but because of the relative ease visualizing and comparing the results."
    },
    {
        "url": "https://medium.com/ai-society/jkljlj-7d6e699895c4?source=---------8",
        "title": "A Comprehensive Introduction to Word Vector Representations",
        "text": "Making a computer mimic the human cognitive function of understanding text is a really hot topic nowadays. Applications range from sentiment analysis to text summary and language translation among others. We call this field of computer science and artificial intelligence Natural Language Processing, or NLP (gosh, please don\u2019t confuse with Neuro-linguistic Programming).\n\nThe \u2018Bag of Words\u2019 model was an important insight that made NLP thrive. This model consists on receiving a list of labeled text corpora, making a word count on each corpus and determining with how much frequency each word (or morpheme [1] to be more precise) appears for every given label. After that, the Bayes\u2019 Theorem is applied on an unlabeled corpus to test which label (a sentiment analysis that labels between positive and negative, perhaps) it has a higher probability of belonging to, based on morpheme frequencies.\n\nEven Though decent (>90%) test scores can be achieved with this method, it has 2 problems:\n\nAnalyzing the context in which a word is used is a transcendental insight to attack this problem. Taking into account a word\u2019s neighboring words is what has made NLP take a quantum leap in the most recent years.\n\nWe will set a parameter \u2018m\u2019 which stands for the window size. In this example we\u2019ll use a size of 1 for educational purposes but 5\u201310 tends to be more common. This means that each word will be defined by its neighboring word to the left as well as the one to the right. This is modeled mathematically by constructing a co-occurrence matrix for each window. Let\u2019s look at the following example:\n\nHere the word \u2018love\u2019 is defined by the words \u2018I\u2019 and \u2018Programming\u2019, meaning that we increment the value both for the \u2018I love\u2019 and the \u2018love Programming\u2019 co-occurrence. We do that for each window and obtain the following co-occurrence matrix:\n\nOnce we have the co-occurrence matrix filled we can plot its results into a multi-dimensional space. Since \u2018Programming\u2019 and \u2018Math\u2019 share the same co-occurrence values, they would be placed in the same place; meaning that in this context they mean the same thing (or \u2018pretty much\u2019 the same thing). \u2018Biology\u2019 would be the closest word to these 2 meaning \u2018it has the closest possible meaning but it\u2019s not the same thing\u2019, and so on for every word. The semantic and syntactic relationships generated by this technique are really powerful but it\u2019s computationally expensive since we are talking about a very high-dimensional space. Therefore, we need a technique that reduces dimensionality for us with the least data-loss possible.\n\nThe idea here is to store only the most \u2018important\u2019 information in order to have a dense vector (eliminating as much 0\u2019s as possible to keep only the relevant values) with a low number of dimensions. The way we do this is by applying a technique borrowed from Linear Algebra called Singular Value Decomposition [2] which in summary is the generalization of the eigendecomposition of a positive semidefinite normal matrix (such as the matrix in the example above, which is a symmetric one with positive eigenvalues).\n\nThis approach generates really interesting semantic and syntactic relationships. Semantically we could visualize things such as \u2018San Francisco\u2019 and \u2018New York\u2019 are at the highest level of similarity possible, at the next level of similarity there\u2019s \u2018Toronto\u2019 and at the next one there\u2019s \u2018Tokyo\u2019. Syntactically we can find words clustered around their respective morphemes; for example \u2018write\u2019, \u2018wrote\u2019 and \u2018writing\u2019 can be clustered together and then far away there\u2019s another cluster with the words \u2018cook\u2019, \u2018cooking\u2019 and \u2018cooked\u2019. With this approach dimensionality has indeed been reduced, however, the computational cost of this approach scales quadratically (O(mn\u00b2) for the nxm matrix) which is something not very desirable. Let us then introduce you to a model that solves this computational complexity runtime issue:\n\nThe way that we are going to finally solve our computational complexity issue is by predicting the surrounding words of every word instead of counting co-occurrences directly. This method is not only more computationally efficient but it also makes it viable to add new words to the model, which in other words means the model scales with corpus size. There are various prediction models but we\u2019re going to talk about one in particular that generates really powerful word relationships, called GloVe: Global Vectors for Word Representation. [3]\n\nThe way these models predict surrounding words is by maximizing the probability of a context word occurring given a center word by performing a dynamic logistic regression. This just means we are going to find the global optimum of a probability function. Review Convex Optimization [4] if this doesn\u2019t sound familiar. Our cost function is the following:\n\nThen something mind-blowing happens. The multi-dimensional plot (represented in 2 dimensions here) understands that what Dollar is to Peso, USA is to Colombia; as well as that what Dollar is to USA, Peso is to Colombia. The most impressive thing about this isn\u2019t that cognitive intelligence assessments test how well can a human build these kind of relations, but that the semantic relation between words turns into a mathematical one. For instance, if you perform the vector operation Peso \u2014 Dollar + USA, you will get Colombia as a result. The reason why this happens is because these words tend to appear in the same context. Imagine we are training a corpora of economic news; you\u2019ll often find fragments such as \u201cThe {Country} {Currency} appreciated\u201d or \u201cFirms that import from {Country1} to {Country2} are worried because the {Currency2} has depreciated with respect to the {Currency1}.\u201d\n\nThis first tutorial has been a lot about the mathematical background behind modern deep learning for NLP techniques. With this notion we can now crack some code to perform sentiment analysis, which we\u2019ll do on our next tutorial.\n\n[1] The smallest meaningful unit of a word. For example; reading, read and readable share the morpheme \u2018read\u2019. Python libraries such as nltk allow you to run an algorithms that reduce each word in a corpus to its morpheme in only a few lines of code.\n\n[2] Here\u2019s a comprehensive tutorial from MIT OCW: https://www.youtube.com/watch?v=cOUTpqlX-Xs If you need a Linear Algebra refresher, please take it.\n\nThanks to Juan C. Saldarriaga, Ana M. G\u00f3mez and Melissa M. Argote for revising the drafts of this text."
    }
]