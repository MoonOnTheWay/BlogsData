[
    {
        "url": "https://medium.com/@abhigoku10/topic-dl02-understanding-backpropagation-ff9a35328638?source=user_profile---------1----------------",
        "title": "Topic DL02: Understanding Backpropagation \u2013 abhigoku10 \u2013",
        "text": "A warm welcome to the second topic of the deep learning understanding the working of backpropagation .\n\nLets start with the understanding below\n\nThe basic concept behind back propagation is to calculate error derivatives. After the forward pass through the net, we calculate the error function, and then update the weights through techniques like gradient decent , stochastic gradient , adam ,rms prop \u2026 The weights that minimize the error function is then considered as the parameters for our model.\n\nWhy do we need Backpropagation ?\n\nWhile designing a Neural Network, the first step is to initialize the weights with some random values . So, it\u2019s not necessary that whatever weight values we have selected is correct or fits our model is the best.Since we have randomly selected some weight values in the beginning , but our model output is way different than our actual output i.e. the error value(loss function) is huge. so we need a method to reduce the error and change the weight parameters such that the error becomes minimum.\n\nLet\u2019s understand the how backpropagation works with an example:\n\nConsider the below table:\n\nNow the output of your model when \u2018W\u201d value is 3:\n\nAs you can observe their is a difference between the actual output and the desired output:\n\nNow lets change the value of \u2018W\u2019 and notice the error when \u2018W\u2019 = \u20184\u2019\n\nNow if you notice, when we increase the value of \u2018W\u2019 the error has increased. So there is no point in increasing the value of \u2018W\u2019 further. But, what happens if we decrease the value of \u2018W\u2019? Consider the table below:\n\nNow,lets see what we did once more :\n\nSo, we are trying to get the value of weight such that the error becomes minimum. Basically, we need to figure out whether we need to increase or decrease the weight value. Once we know that, we keep on updating the weight value in that direction until error becomes minimum. You might reach a point, where if you further update the weight, the error will increase. At that time you need to stop, and that is your final weight value.\n\nConsider the graph below giving you an idea when to increase the weight and when to decrease it\n\nNow lets see how the backpropagation works for a neural netowrk .Consider the below Neural Network:\n\nThe above network contains the following:\n\nBelow are the steps involved in Backpropagation:\n\nWe will repeat this process for the output layer neurons, using the output from the hidden layer neurons as inputs.\n\nNow, let\u2019s see what is the value of the error:\n\nNow, we will propagate backwards. This way we will try to reduce the error by changing the values of weights and biases.\n\nConsider weight W5, we will calculate the rate of change of error w.r.t change in weight W5.\n\nSince we are propagating backwards, first thing we need to do is, calculate the change in total errors w.r.t the output O1 and O2.\n\nNow, we will propagate further backwards and calculate the change in output O1 w.r.t to its total net input.\n\nLet\u2019s see now how much does the total net input of O1 changes w.r.t W5?\n\nNow, let\u2019s put all the values together:\n\nLet\u2019s calculate the updated value of W5:\n\nSince I might not be an expert on the topic, if you find any mistakes in the article, or have any suggestions for improvement, please mention in comments."
    },
    {
        "url": "https://medium.com/@abhigoku10/activation-functions-and-its-types-in-artifical-neural-network-14511f3080a8?source=user_profile---------2----------------",
        "title": "Topic DL01: Activation functions and its Types in Artifical Neural network",
        "text": "An activation function is a very important feature of an artificial neural network , they basically decide whether the neuron should be activated or not\n\nSo lets consider an simple neural network shown below.\n\nIn the above figure,(x1,x2,\u2026xn)is the input signal vector that gets multiplied with the weights(w1,w2,\u2026wn). This is followed by accumulation ( i.e. summation + addition of bias b). Finally, an activation function f is applied to this sum.\n\nAs observed for the above figure when we do not have the activation function the weights and bias would simply do a linear transformation.\n\nA linear equation is simple to solve but is limited in its capacity to solve complex problems and have less power to learn complex functional mappings from data. A neural network without an activation function is just a linear regression model.\n\nThe activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. We would want our neural networks to work on complicated datas like videos , audio , speech etc. Linear transformations would never be able to perform such tasks.\n\nActivation functions make the back-propagation possible since the gradients are supplied along with the error to update the weights and biases. Without the differentiable non linear function, this would not be possible.\n\nSo the functions should be differentiable and monotonic.\n\nThe Activation Functions can be basically divided into 2 types-\n\nAs you can see the function is a line or linear.Therefore, the output of the functions will not be confined between any range.\n\nAs shown in the above figure the activation is proportional to the input. . This can be applied to various neurons and multiple neurons can be activated at the same time. Now, when we have multiple classes, we can choose the one which has the maximum value. But we still have an issue here\n\nThe derivative of a linear function is constant i.e. it does not depend upon the input value x.\n\nThis means that every time we do a back propagation, the gradient would be the same. And this is a big problem, we are not really improving the error since the gradient is pretty much the same. And not just that suppose we are trying to perform a complicated task for which we need multiple layers in our network. Now if each layer has a linear transformation, no matter how many layers we have the final output is nothing but a linear transformation of the input.\n\nThe Nonlinear Activation Functions are the most used activation functions.It makes it easy for the model to generalize or adapt with variety of data and to differentiate between the output.\n\nThe Nonlinear Activation Functions are mainly divided on the basis of their range or curves-\n\nThe Sigmoid Function curve looks like a S-shape.\n\n1.The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points\n\n2.The function is monotonic but function\u2019s derivative is not\n\n1.It gives rise to a problem of \u201cvanishing gradients\u201d, since the Y values tend to respond very less to changes in X\n\n2.Secondly , its output isn\u2019t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n\nThe ReLU is the most used activation function in the world right now\n\nTo solve the ReLU problem we have leaky ReLU\n\nEquation : f(x) = ax for x<0 and x for x>0\n\nThe idea of leaky ReLU can be extended even further. Instead of multiplying x with a constant term we can multiply it with a hyperparameter which seems to work better the leaky ReLU. This extension to leaky ReLU is known as Parametric ReLU.\n\nThe softmax function is also a type of sigmoid function but it is very useful to handle classification problems having multiple classes .\n\nThe softmax function is shown above, where z is a vector of the inputs to the output layer (if you have 10 output units, then there are 10 elements in z). And again, j indexes the output units, so j = 1, 2, \u2026, K.\n\nThe softmax function is ideally used in the output layer of the classifier where we are actually trying to attain the probabilities to define the class of each input.\n\nFrom the above we have seen different categories of activation functions, we need some logic / heuristics to know which activation function has to be should be used in which situation.\n\nBased on the properties of the problem we might be able to make a better choice for easy and quicker convergence of the network.\n\nIn this article, I tried to describe the activation functions commonly used . There are other activation functions too, but the general idea remains the same. Hope this article serves the purpose of getting idea about the activation function , why when and how to use it for a given problem statement"
    }
]