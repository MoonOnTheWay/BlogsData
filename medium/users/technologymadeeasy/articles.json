[
    {
        "url": "https://medium.com/technologymadeeasy/will-the-real-sherlock-holmes-please-stand-up-4e16f3303e0e?source=---------0",
        "title": "Will the real Sherlock Holmes please stand up? \u2013 TechnologyMadeEasy \u2013",
        "text": "More than a century after first emerging into the fog bound gas lit streets of Victorian London, Sherlock Holmes is universally recognizable. Even his wardrobe and accessories are iconic. The Inverness Cape, deerstalker hat and calabash pipe and figures such as his best friend and housemate Dr Watson, arch nemesis Moriarty and housekeeper Mrs Hudson have become part of the popular consciousness.\n\nHis extraordinary infallible powers of deduction utilized in the name of the law, his notorious drug use and his popular catchphrase \u201cElementary my dear Watson\u201d!\n\nAnd yet many of these most recognizable features of Holmes don\u2019t appear in Arthur Conan Doyle\u2019s original stories.\n\nDoyle\u2019s great detective solves crimes in all sorts of ways not just using deduction. He speculates and at times even guesses and regularly makes false assumptions.\n\nFurthermore Mrs Hudson is barely mentioned. No one says \u201cElementary my dear Watson\u201d and the detective and his sidekick live apart for much of the time.\n\nMoriarty, the grand villain only appears in two stories.\n\nThe detective\u2019s drug use is infrequent after the first two novels and Holmes is rarely in thrall to the English legal system. He much prefers in acting his own form of natural justice to sticking to the letter of the law.\n\nFinally many of the most iconic elements of the Holmesian legend aren\u2019t Doyle\u2019s either. The deerstalker cap and Cape were first imagined by Sidney Padgett.\n\nThe story\u2019s initial Illustrator, the curved pipe, was chosen by American actor William Gillette so that audiences could more clearly see his face on stage and the phrase \u201cElementary my dear Watson\u201d was coined by author and humorist. P.G. Woodhouse.\n\nSo who exactly is Sherlock Holmes? Who\u2019s the real great detective and where do we find him?\n\nPurists might answer that the original Sherlock inspired by Arthur Conan Doyle\u2019s University mentor, Dr Joseph Bell is the real one. But the fact remains that that version of Sherlock has been largely eclipsed by the sheer volume of interpretation leaving Doyle\u2019s detective largely unrecognizable.\n\nSo there\u2019s another more complex but perhaps more satisfying answer to the question but to get there we must first consider the vast body of interpretations of the great detective. Since Conan Doyle\u2019s first story in 1887, there have been thousands of adaptations of Holmes making him perhaps the most adapted fictional character in the world.\n\nThat process began with Victorian stage adaptations and accelerated with the emergence of film. There were more than one hundred film adaptations of Holmes in the first two decades of the twentieth century alone. And since then there have been many thousands more in print and on film television stage and radio\n\nHolmes has been reinterpreted by people everywhere, in remarkably different and often contradictory ways. For instance he featured in a number of allied anti Nazi propaganda films during World War 2 and both Winston Churchill and Franklin Delano Roosevelt were avid enthusiasts, the latter even joining the Baker Street Irregulars, a Holmesian appreciation society. Holmes also appeared in various German language film adaptations some of which were said to have been much loved favorites of Adolf Hitler.\n\nSo let\u2019s return to our question! Will the real Sherlock Holmes please stand up?\n\nSherlock is a cultural text repeatedly altered over time as each new interpretation becomes superimposed over those that preceded. This means that Sherlock continually evolves embodying ideas and values often far removed from those found in Conan Doyle. And after each particular story ends, Sherlock rises again, a little changed perhaps with a new face and fresh mannerisms or turns of phrase but still essentially Sherlock, our Sherlock!\n\nIf you enjoyed reading this article, hit the little green heart button to show your love! It gives me the energy to write more :)\n\nTo read more of these interesting stories, please follow :)\n\nAnd if you want your friends to read this too, click share!"
    },
    {
        "url": "https://medium.com/technologymadeeasy/barack-obamas-advice-to-entrepreneurs-a8ea830663e2?source=---------1",
        "title": "Barack Obama\u2019s advice to Entrepreneurs! \u2013 TechnologyMadeEasy \u2013",
        "text": "Great words from the great man himself. Undoubtedly the best thing that happened to me today!\n\nThe primary message I guess I have is that every one of you represent an enormous potential for change because we all know that injustice still exists.\n\nIt exists here in the United States. In every poor neighbourhood and in every inner city in every rural community all across the country. There is quiet desperation. Young people\u2019s lives are filled with sadness and desperation. Anarchy and chaos are obviously all around the world. We see those same symptoms of hopelessness made manifest in all places, in places that too often are forgotten about and not written about until they flare up in tragedy.\n\nSo I hope that all of you who are on the brink of doing extraordinary things decide to channel that talent and that energy and that imagination to figuring out how do you move the process along for a better history. How do you put your shoulder against the wheel and move that boulder up the hill.\n\nI\u2019m absolutely confident that if all of you take up that challenge the world is waiting for you ready to be changed because I think we live in this moment in history where the hunger for change, the hunger for something new is widespread. The desire to break out of the ordinary, the self interested, the petty, the trivial is everywhere and they\u2019re waiting for you. And so I hope that you drive yourself in the wonderful directions in the years to come.\n\nIf you enjoyed reading this article, hit the little green heart button to show your love! It gives me the energy to write more :)\n\nAnd if you want your friends to read this too, click share!"
    },
    {
        "url": "https://medium.com/technologymadeeasy/ethereum-tutorial-series-3-setting-up-the-development-environment-in-under-a-minute-93a008927574?source=---------2",
        "title": "Ethereum Tutorial Series [3] \u2014 Setting up the development environment in under a minute!",
        "text": "The previous 2 articles in the series talk about cryptography, hash functions and blockchain. Things have been explained in a very easy to understand manner. So, if you want a quick recap you can view them here and here!\n\nUpdate: Mix IDE has been discontinued. For learning how to write smart contracts, the best way currently is to use browser-solidity which does not require any installation. In the next few tutorials, we will use browser-solidity to write smart contracts.\n\nTo start developing DApps, we will need to install the Mix IDE. It is currently available for OS X, Ubuntu and Windows.\n\nJust grab the latest binary from here.\n\nIf your platform is not included in the list, you can try building from source (I have not tried it). The detailed instructions are given here.\n\nThis was a short tutorial. Just set up Mix IDE and in the next tutorial I will provide a quick introduction of the Mix IDE workflow by walking you through the creation of a simple DApp.\n\nIf you enjoyed reading this article, hit the little green heart button to show your love! It gives me the energy to write more :)\n\nTo stay updated with new technologies, please follow :)\n\nAnd if you want your friends to read this too, click share!"
    },
    {
        "url": "https://medium.com/technologymadeeasy/blockchains-explained-to-a-5-year-old-child-part-2-of-the-tutorial-series-on-ethereum-dapps-f80b866e72de?source=---------3",
        "title": "BlockChains explained to a 5 year old child! [Part 2 of the tutorial series on Ethereum DApps]",
        "text": "We discussed about Public Key Cryptography, Hash Functions and P2P network in the previous tutorial available here. For those who have not read it, please go have a look as it explains these \u201ctough sounding\u201d concepts in a very simplified manner.\n\nIn this article, we will mainly talk about blockchains. I have taken references from some resources on the internet and rightfully mentioned them after the article. This is just my take on the explanation they have provided. Writing something I have learned helps me retain the knowledge longer and also helps me spread the knowledge to others who might not know about it :)\n\nIn the next article, we will set up our development environment in under a minute! [Available here]\n\nWe are 2 friends sitting in a park. You are Alice, I am Bob.\n\nI have a toy. You have none.\n\nI give the toy to you.\n\nNow you have a toy. I have none.\n\nI had a toy. I gave it to you. We did not need a third person to make the transfer for us. I can\u2019t give the toy to someone else because I don\u2019t have it anymore. Whereas you can give it to whomsoever you wish to. And they can give to whoever they want to and so on.\n\nHow do you know I didn\u2019t give the digital toy as an email attachment to Charlie first? Or I saved a million copies on my laptop and gave it to everyone on the internet?\n\nSo, sending digital toys is not the same as giving physical toys.\n\nThis problem is called the double-spending problem!\n\nWe can maintain a ledger. Similar to what people at shops maintain. An account book, where all the transactions are recorded.\n\nSo, there will be a ledger where all the transactions of digital toys will be recorded. Someone, say Danny will be in charge of it.\n\nBecause how do we know if Danny is not cheating and adding digital toys to his and his girlfriend\u2019s account anytime he wants?\n\nAlso, why do we need to involve Danny in transactions that concern only you and me?\n\nWhat if everyone had a copy of the accounting book on their computer and all the transactions that ever happen are recorded there.\n\nThis is tough to beat. Now Danny can\u2019t add toys that he doesn\u2019t have to his account. Because then it wouldn\u2019t match with everyone else\u2019s book.\n\nEveryone who has a copy of the ledger on their system and maintains it by validating the transactions get digital toys as rewards. This is the way digital toys are ever produced.\n\nThis is blockchain. And these toys that I was talking about are bitcoins or for that matter anything of value.\n\nThe ledger is visible to everyone.\n\nNow we don\u2019t rely on a third person (Danny) to maintain our balances for us.\n\nWe don\u2019t need to worry about the double spending problem.\n\nThis is a very simplified story but if you understand this well, you know more about blockchains than many people in the industry. And to develop DApps this much knowledge is enough for now. Rest we will figure out as we will move forward.\n\nIf you enjoyed reading this article, hit the little green heart button to show your love! It gives me the energy to write more :)\n\nTo stay updated for the next article in the series, please follow :)\n\nAnd if you want your friends to read this too, click share!"
    },
    {
        "url": "https://medium.com/technologymadeeasy/develop-dapps-on-ethereum-tutorial-series-for-beginners-part-1-basic-terminology-866d2ce4cf34?source=---------4",
        "title": "Develop DApps on Ethereum (Tutorial Series for Beginners) [ Part 1 : Basic Terminology]",
        "text": "Want to learn how to develop DApps? Can\u2019t figure out where to start from on the internet? Don\u2019t worry! Stay with me and follow this tutorial series (new articles posted daily) and you will be up and running in no time!\n\nYou don\u2019t need to understand all the crypto economic computer science to start building cool apps. But it will be good to know some of the key terms we will be using and the concepts behind them. So, without wasting any more time, let\u2019s dive into it!\n\nThe next article in the series is available here.\n\nFirst I will give a technical explanation of the concept and then strengthen it with a real life analogy.\n\nThe Public and Private key pair are 2 uniquely related long random numbers.\n\nThe Public Key is what its name suggests \u2014 Public. It is made available to everyone via a publicly accessible repository or directory. On the other hand, the Private Key must remain confidential to its respective owner.\n\nBecause the key pair is mathematically related, whatever is encrypted with a Public Key may only be decrypted by its corresponding Private Key.\n\nAnother important aspect is Digital Signature. A digital signature is a \u201cstamp\u201d someone places on the data. This stamp is extremely difficult to forge. Anyone with the corresponding public key can verify that the document was signed with the private key or not (basically if the document is genuine or not).\n\nAlice has a box with a lock, a very very special lock!\n\nAnd she has 2 SEPARATE keys. The first one can only turn clockwise (from A to B to C) and the second one can only turn anticlockwise (from C to B to A).\n\nAlice picks the key that can turn the lock from A to B to C and keeps it with herself. We will call this as her private key. She makes a thousand copies of the other key, which we call her public key and gives it to her friends, family or anyone who wants to have the key.\n\nNow if someone wanted to send Alice a personal document, they would put it in the box, lock it with the public key (from B to A) and send it to Alice. Since only Alice has the private key which can unlock the box (turn from A to B), there is no threat of anyone else opening it.\n\nNow suppose Alice wants to put a document in the box, lock it with her private key (from B to C) and send it to someone. Why would she want to do this? Because if I get this box which says \u201cFrom Alice\u201d on the label, how can I believe it? But wait, I have Alice\u2019s public key which when I turn clockwise on the box, nothing happens. But if turn it anticlockwise, it opens. This means the document actually came from Alice. This is called Alice\u2019s digital signature!\n\nIt is a function which takes a message as an input and returns a fixed length alphanumeric string, called \u2018hash\u2019 [not that one silly!]. What is the big deal about that?\n\n1. It is super easy to calculate hash of a given message.\n\n2. It is super tough (like impossibly tough) to figure out the message from a given hash.\n\n3. Two messages differing only slightly will have their hashes nowhere close to each other.\n\nFor now knowing these properties is enough. We will talk about how to use them in an upcoming article.\n\nUnlike the Client Server model, peer-to-peer networks consist of networks of computers connected to one another directly without sending requests to any server. All computers taking part in the network are considered to be \u2018peers\u2019 and have equal standing within the network as one another.\n\nAll Ethereum nodes are peers in a distributed network, there\u2019s no centralized server.\n\nCan you think of an example?\n\nThese are the core technologies that are used in Ethereum. In the next lecture (I will post it today itself), we will discuss on the other technologies such as Blockchain, Ethereum Virtual Machine, Node, Miner, Proof of Work, Ether and DApps.\n\nIf you enjoyed reading this article, hit the little green heart button to show your love! It gives me the energy to write more :)\n\nTo stay updated with new technologies, please follow :)\n\nAnd if you want your friends to read this too, click share!"
    },
    {
        "url": "https://medium.com/technologymadeeasy/the-best-explanation-of-convolutional-neural-networks-on-the-internet-fbb8b1ad5df8?source=---------5",
        "title": "The best explanation of Convolutional Neural Networks on the Internet!",
        "text": "CNNs have wide applications in image and video recognition, recommender systems and natural language processing. In this article, the example that I will take is related to Computer Vision. However, the basic concept remains the same and can be applied to any other use-case!\n\nFor a quick recap of Neural Networks, here\u2019s a very clearly explained article series.\n\nCNNs, like neural networks, are made up of neurons with learnable weights and biases. Each neuron receives several inputs, takes a weighted sum over them, pass it through an activation function and responds with an output. The whole network has a loss function and all the tips and tricks that we developed for neural networks still apply on CNNs. Pretty straightforward, right?\n\nSo, how are Convolutional Neural Networks different than Neural Networks?\n\nWhat do we mean by this?\n\nUnlike neural networks, where the input is a vector, here the input is a multi-channeled image (3 channeled in this case).\n\nThere are other differences that we will talk about in a while.\n\nBefore we go any deeper, let us first understand what convolution means.\n\nWe take the 5*5*3 filter and slide it over the complete image and along the way take the dot product between the filter and chunks of the input image.\n\nFor every dot product taken, the result is a scalar.\n\nSo, what happens when we convolve the complete image with the filter?\n\nI leave it upon you to figure out how the \u201828\u2019 comes. (Hint: There are 28*28 unique positions where the filter can be put on the image)\n\nThe convolution layer is the main building block of a convolutional neural network.\n\nThe convolution layer comprises of a set of independent filters (6 in the example shown). Each filter is independently convolved with the image and we end up with 6 feature maps of shape 28*28*1.\n\nSuppose we have a number of convolution layers in sequence. What happens then?\n\nAll these filters are initialized randomly and become our parameters which will be learned by the network subsequently.\n\nI will show you an example of a trained network.\n\nTake a look at the filters in the very first layer (these are our 5*5*3 filters). Through back propagation, they have tuned themselves to become blobs of coloured pieces and edges. As we go deeper to other convolution layers, the filters are doing dot products to the input of the previous convolution layers. So, they are taking the smaller coloured pieces or edges and making larger pieces out of them.\n\nTake a look at image 4 and imagine the 28*28*1 grid as a grid of 28*28 neurons. For a particular feature map (the output received on convolving the image with a particular filter is called a feature map), each neuron is connected only to a small chunk of the input image and all the neurons have the same connection weights. So again coming back to the differences between CNN and a neural network.\n\nParameter sharing is sharing of weights by all neurons in a particular feature map.\n\nLocal connectivity is the concept of each neural connected only to a subset of the input image (unlike a neural network where all the neurons are fully connected)\n\nThis helps to reduce the number of parameters in the whole system and makes the computation more efficient.\n\nI will not be talking about the concept of zero padding here as the idea is to keep it simple. Interested people can read about it separately!\n\nA pooling layer is another building block of a CNN.\n\nIts function is to progressively reduce the spatial size of the representation to reduce the amount of parameters and computation in the network. Pooling layer operates on each feature map independently.\n\nThe most common approach used in pooling is max pooling.\n\nWe have already discussed about convolution layers (denoted by CONV) and pooling layers (denoted by POOL).\n\nRELU is just a non linearity which is applied similar to neural networks.\n\nThe FC is the fully connected layer of neurons at the end of CNN. Neurons in a fully connected layer have full connections to all activations in the previous layer, as seen in regular Neural Networks and work in a similar way.\n\nI hope you understand the architecture of a CNN now. There are many variations to this architecture but as I mentioned before, the basic concept remains the same. In case you have any doubts/feedback, please comment.\n\nYou can follow me to read more TechnologyMadeEasy articles!\n\nAnd if you want your friends to read this too, click share!"
    },
    {
        "url": "https://medium.com/technologymadeeasy/for-dummies-the-introduction-to-neural-networks-we-all-need-part-2-1218d5dc043?source=---------6",
        "title": "For Dummies \u2014 The Introduction to Neural Networks we all need ! (Part 2)",
        "text": "This article is in continuation to the Part1 of this series. If you have not yet read it, I highly recommend you to do that before we dive into multi layered neural networks here!\n\nJust as a recap, I will quickly go through what a single layered neural network basically does. Once a training sample is feeded to the network, each output node of the single layered neural network (also called Perceptron) takes a weighted sum of all the inputs and pass them through an activation function (probably sigmoid or step) and comes up with an output. The weights are then corrected using the following equation,\n\nNote: We drop the derivative function in case the activation function is a step function.\n\nThis process is repeated by feeding the whole training set several times until the network responds with a correct output for all the samples. The training is possible only for inputs that are linearly separable. This is where multi-layered neural networks come into picture.\n\nEach input from the input layer is fed up to each node in the hidden layer, and from there to each node on the output layer. We should note that there can be any number of nodes per layer and there are usually multiple hidden layers to pass through before ultimately reaching the output layer.\n\nBut to train this network we need a learning algorithm which should be able to tune not only the weights between the output layer and the hidden layer but also the weights between the hidden layer and the input layer.\n\nFirst of all, we need to understand what do we lack. To tune the weights between the hidden layer and the input layer, we need to know the error at the hidden layer, but we know the error only at the output layer (We know the correct output from the training sample and we also know the output predicted by the network.)\n\nSo, the method that was suggested was to take the errors at the output layer and proportionally propagate them backwards to the hidden layer.\n\nBelow we will write equation for a 2 layered network but the same concept applies to a network with any number of layers.\n\nWe will follow the nomenclature as shown in the above figure.\n\nThis equation tunes the weights between the output layer and the hidden layer.\n\nFor a particular neuron j in hidden layer, we propogate the error backwards from the output layer, thus\n\nThis equation tunes the weights between the hidden layer and the input layer.\n\nSo, in a nutshell what we are doing is\n\nIf you enjoyed reading this article, hit the little green heart button to show your love!\n\nTo stay updated with new technologies, please follow :)\n\nAnd if you want your friends to read this too, click share!"
    },
    {
        "url": "https://medium.com/technologymadeeasy/for-dummies-the-introduction-to-neural-networks-we-all-need-c50f6012d5eb?source=---------7",
        "title": "For Dummies \u2014 The Introduction to Neural Networks we all need ! (Part 1)",
        "text": "This is going to be a 2 article series. This article gives an introduction to perceptrons (single layered neural networks)\n\nUpdate: Part2 of the series is now available for reading here!\n\nOur brain uses the extremely large interconnected network of neurons for information processing and to model the world around us. Simply put, a neuron collects inputs from other neurons using dendrites. The neuron sums all the inputs and if the resulting value is greater than a threshold, it fires. The fired signal is then sent to other connected neurons through the axon.\n\nThe figure depicts a neuron connected with n other neurons and thus receives n inputs (x1, x2, \u2026.. xn). This configuration is called a Perceptron.\n\nThe inputs (x1, x2, \u2026. xn) and weights (w1, w2, \u2026. wn) are real numbers and can be positive or negative.\n\nThe perceptron consists of weights, summation processor and an activation function.\n\nNote: It also contains a threshold processor (known as bias) but we will talk about that later!\n\nAll the inputs are individually weighted, added together and passed into the activation function. There are many different types of activation function but one of the simplest would be step function. A step function will typically output a 1 if the input is higher than a certain threshold, otherwise it\u2019s output will be 0.\n\nNote: There are other activation functions too such as sigmoid, etc which are used in practice.\n\nAn example would be,\n\nWeighing the inputs and adding them together gives,\n\nHere, the total input is higher than the threshold and thus the neuron fires.\n\nTry teaching a child to recognize a bus? You show her examples, telling her, \u201cThis is a bus. That is not a bus,\u201d until the child learns the concept of what a bus is. Furthermore, if the child sees new objects that she hasn\u2019t seen before, we could expect her to recognize correctly whether the new object is a bus or not.\n\nThis is exactly the idea behind the perceptron.\n\nSimilarly, input vectors from a training set are presented to the perceptron one after the other and weights are modified according to the following equation,\n\nNote: Actually the equation is W(i) = W(i) + a*g\u2019(sum of all inputs)*(T-A)*P(i), where g\u2019 is the derivative of the activation function. Since it is problematic to deal with the derivative of step function, we drop that out of the equation here.\n\nHere, W is the weight vector. P is the input vector. T is the correct output that the perceptron should have known and A is the output given by the perceptron.\n\nWhen an entire pass through all of the input training vectors is completed without an error, the perceptron has learnt!\n\nAt this time, is an input vector P (already in the training set) is given to the perceptron, it will output the correct value. If P is not in the training set, the network will respond with an output similar to other training vectors close to P.\n\nThe perceptron is adding all the inputs and separating them into 2 categories, those that cause it to fire and those that don\u2019t. That is, it is drawing the line:\n\nand looking at where the input point lies. Points on one side of the line fall into 1 category, points on the other side fall into the other category. And because the weights and thresholds can be anything, this is just any line across the 2 dimensional input space.\n\nNot every set of inputs can be divided by a line like this. Those that can be are called linearly separable. If the vectors are not linearly separable, learning will never reach a point where all vectors are classified properly. The most famous example of the perceptron\u2019s inability to solve problems with linearly non-separable vectors is the boolean XOR problem.\n\nThe next part of this article series will show how to do this using muti-layer neural networks, using the back propagation training method.\n\nIf you enjoyed reading this article, hit the little green heart button to show your love!\n\nTo stay updated for the next article in the series, please follow :)\n\nAnd if you want your friends to read this too, click share!"
    },
    {
        "url": "https://medium.com/technologymadeeasy/installing-caffe-on-os-x-el-capitan-the-correct-way-4ecb04ef904c?source=---------8",
        "title": "Installing Caffe on OS X El Capitan (The correct way)",
        "text": "Caffe is a deep learning framework made with expression, speed, and modularity in mind. It is developed by the Berkeley Vision and Learning Center (BVLC) and by community contributors.\n\nHomebrew will come in handy throughout the installation and I recommend using it. Just paste the below mentioned command in the terminal window and the installation will be done.\n\nNext, use the following command to install hdf5 (Ignore, if already installed)\n\nWe will need to edit the OpenCV installation file a bit\n\nOnce this is done run the following commands to install the dependencies\n\nIf everything works fine up till here, proceed to download Caffe\n\nThe most important step in the whole installation process is to edit the Makefile.config file properly. I have posted an example file here.\n\nFor a simple installation, keep the following things in mind:\n\nOnce you have edited the Makefile.config file, run the following commands from the caffe root directory:\n\nIf everything runs perfectly up till here, then you are almost done. There may be a few warnings displayed regarding unused variables and Logs but they can be ignored.\n\nTo install and configure the python interface to Caffe (PyCaffe)\n\nAgain, go to the python folder inside the caffe root directory\n\nYou should see something like\n\nIf you encounter any errors relating to locale, add these lines to the famous ~/.bash_profile\n\nNow you are all set to work with Caffe. For a demo project, you can refer to https://github.com/harshpokharna/CaffeInstallationAndDemo\n\nHit the beautiful heart button if this article helped you. :)\n\nIn case of any doubts/problems faced, feel free to comment."
    }
]