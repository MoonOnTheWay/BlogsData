[
    {
        "url": "https://towardsdatascience.com/information-theory-of-neural-networks-ad4053f8e177?source=user_profile---------1----------------",
        "title": "Information Theory of Neural Networks \u2013",
        "text": "Now, if I say every neural network, itself, is an encoder-decoder setting; it would sound absurd to most.\n\nLet input layer be X and their real tags/classes (present in the training set) be Y. Now we already know Neural Networks find the underlying function between X and Y.\n\nSo X can be seen as High-Entropy distribution of Y. High entropy because X contains the information Y but it also a lot of other information.\n\nNow imagine every hidden layer as a single variable H ( so layers will be named as H0, H1 \u2026.. H(n-1) )\n\nNow every layer becomes a variable and the neural network becomes a Markov Chain. Markov Chain because each variable is only dependent upon the previous layer only.\n\nSo essentially each layer forms a partisan of information.\n\nFollowing is a visualisation of a Neural Network as a Markov Chain.\n\nThe last layer Y_ is supposed to result in least entropy output (with respect to \u2018Y\u2019, the original tag/class).\n\nThis process of obtaining Y_ is squeezing the information in X layer as it flows through H layers and retaining only the information most relevant to Y. This is the information bottleneck."
    },
    {
        "url": "https://hackernoon.com/information-theory-of-neural-networks-c96a0f0a8d9?source=user_profile---------2----------------",
        "title": "Information Theory of Neural Networks \u2013",
        "text": "Now, if I say every neural network, itself, is an encoder-decoder setting; it would sound absurd to most.\n\nLet input layer be X and their real tags/classes (present in the training set) be Y. Now we already know Neural Networks find the underlying function between X and Y.\n\nSo X can be seen as High-Entropy distribution of Y. High entropy because X contains the information Y but it also a lot of other information.\n\nNow imagine every hidden layer as a single variable H ( so layers will be named as H0, H1 \u2026.. H(n-1) )\n\nNow every layer becomes a variable and the neural network becomes a Markov Chain. Markov Chain because each variable is only dependent upon the previous layer only.\n\nSo essentially each layer forms a partisan of information.\n\nFollowing is a visualisation of a Neural Network as a Markov Chain.\n\nThe last layer Y_ is supposed to result in least entropy output (with respect to \u2018Y\u2019, the original tag/class).\n\nThis process of obtaining Y_ is squeezing the information in X layer as it flows through H layers and retaining only the information most relevant to Y. This is the information bottleneck."
    },
    {
        "url": "https://becominghuman.ai/basics-of-neural-network-bef2ba97d2cf?source=user_profile---------3----------------",
        "title": "Basics of Neural Network \u2013",
        "text": "Partial Derivatives are calculated so we know what was the contribution of error by each weight.\n\nThe need of derivatives obvious if you think of it.\n\nFor example think of a neural network trying to find the optimal speed (velocity) of a self driving car. Now if the car finds out the the is either faster or slower than desired speed neural network will change its speed by either accelerating or decelerating the car. What is accelerating/decelerating? Derivatives of speed.\n\nLet\u2019s explain the need of \u2018Partial Derivatives\u2019 with an example as well:\n\nLet\u2019s say that a few kids were asked to throw dart at a dart-board, aiming at the center. The initial results were:\n\nNow if we found total loss and simply subtracted that from all the weights then we generalize the mistakes made by each student. So let\u2019s say a kid aimed too low but we ask all the kids to aim high then it results in:\n\nThe error of a few students might decrease but overall error still increases.\n\nBy finding partial derivatives we find what was the error by each weight individually. Correcting each weight individually results in following results:\n\nWhile neural network is used to automate feature selection, there are still a few parameters that we have to input manually.\n\nLearning Rate is again a very crucial hyper-parameter. If the learning rate is too small then even after training the neural network for long time, it will still be away from the optimal results. Results would look something like:\n\nInstead, if the learning rate is too high then the learner jumps to conclusions too soon. Producing following results:\n\nActivation Function is one of most powerful arsenal, which is responsible for powers Neural Networks advertised to have. Vaguely, it decides which neurons will be activated, in other words what information would be passed to further layers.\n\nWithout activation functions, deep nets lose a bulk of their representation learning power.\n\nThese functions\u2019 non-linearity is responsible for increased degree of freedom of the learners, enabling them to generalize problems of high dimensionality in lower dimensions.\n\nBelow are few examples of popular Activation Functions:\n\nCost Function is at the centre of Neural Network. It is used to calculate loss given the real and observed results. Our aim throughout is to minimise this loss. So Cost Function effectively drives the learning of neural network towards it\u2019s goal.\n\nA cost function is a measure of \u201chow good\u201d a neural network did with respect to it\u2019s given training sample and the expected output. It also may depend on variables such as weights and biases.\n\nA cost function is a single value, not a vector, because it rates how good the neural network did as a whole.\n\nSome of the most famous cost functions are:\n\nRoot Mean Square is the simplest and most used of them all. It is simply defined as:\n\nThe Cost function in NN should satisfy two conditions"
    },
    {
        "url": "https://hackernoon.com/how-to-become-data-scientist-in-one-day-f6c904618190?source=user_profile---------4----------------",
        "title": "How to Become Data Scientist in One Day? \u2013",
        "text": "You see, there are no shortcuts for gaining knowledge. You can just be smart about the learning techniques.\n\nNo tutorials can jump-start your career from novice to pro ( if there is then damn! I wasted life :/ )\n\nYou become a pro by mastering:\n\nClaims like \u201cMaster Neural Networks in One Minute!\u201d will give you an abstract idea about the working of it.\n\nSo you\u2019ll be lost when you are tackling a real life not-so-mainstream problem as default settings won\u2019t work.\n\nLike me, when I used to read the abstract of a research paper and assume I\u2019ve understood all the concepts.\n\nPS : Yeah, the old me wasn\u2019t that bright."
    },
    {
        "url": "https://hackernoon.com/reinforcement-learning-part-1-d2f469a02e3b?source=user_profile---------5----------------",
        "title": "Reinforcement Learning (Introduction) \u2013",
        "text": "You might have observed a level of saturation in Machine Learning recently. Well that\u2019s actually saturation in \u2018Supervised Learning\u2019 actually (poor Kaggle).\n\nMost of us don\u2019t know any other learning algorithm than Back-Propagation. There are a few recent ones like \u2018Equilibrium Propagation\u2019 and \u2018Synthetic Gradients\u2019 but more-or-less fall under similar paradigm as Back-propagation.\n\nWhile, Unsupervised learning and Reinforcement learning remain unscratched (comparatively, in mainstream at least).\n\nIn this blog we\u2019ll be diving into Reinforcement Learning or as I like to call it \u2018Stupidity-followed-by-Regret\u2019 or \u2018What-If\u2019 learning. Those are actually pretty accurate names."
    },
    {
        "url": "https://hackernoon.com/machine-learning-in-brain-120d375eccd1?source=user_profile---------6----------------",
        "title": "Machine Learning in Brain \u2013",
        "text": "Q) Why Algorithmic leaps can be better than Hardware leaps?\n\nAns) Hardware constraints create bottlenecks that are hard to tackle as uncertainty of physics at small scale (nano-meters and less) come into play (electrons start jumping around).\n\nAt this point, ideas (algorithms) can be used to unleash full potential of the feasible hardware.\n\nQ) Why these improvements are so beneficial for Machine Learning?\n\nHence even a small optimisation can have very visible effects, saving you from a few hours to months!\n\nQ) Why refer to the brain for optimisations?\n\nAns) Short answer, without the below mentioned evolution in human brain, we might have looked like:\n\nAns) Neocortex is the largest part of the cerebral cortex which is the outer layer of the cerebrum.\n\nAns) Neocortex, also called the isocortex, is the part of the mammalian brain involved in higher-order brain functions such as sensory perception, cognition, generation of motor commands, spatial reasoning and language (partially).\n\nQ) How is neocortex relevant to machine learning?\n\nAns) Hierarchical temporal memory (HTM) is based on interaction of pyramidal neurons in the neocortex of the human brain.\n\nAt the core of HTM:\n\nWhen applied to computers, HTM is well suited for prediction, anomaly detection, classification and ultimately sensorimotor applications.\n\nThe HTM has a hierarchical topology on account of hierarchies observed in some naturally occurring neural networks, such as those observed in the brain.\n\nHuman Memory hierarchy can be really really complex!\n\nAs one moves up the hierarchy, representations have increased:\n\nAsn) Introspection or metacognition, is self-awareness about one\u2019s thinking or \u2018ability to think about the thinking\u2019. A high-level mental process (very high).\n\nAccurate introspection ables discrimination between correct decisions and incorrect ones.\n\nBasically, a learner developing a conscious. Being self-able to reject or encourage patterns during training. Hence a self-optimising learner, the very necessity of General Learning.\n\nAns) The interior of the central nervous system is organised into gray and white matter.\n\nGray matter consists of nerve cells embedded in neuroglia (nervous tissue made up of large number of nerve cells); it has a gray color.\n\nAns) In humans the gray-matter volume might help clarify the extent to which a person\u2019s confidence about his introspective abilities is supported. Also how accurate his introspective abilities are.\n\nMore gray matter usually signifies more intelligence.\n\nAs Einstein once said \u201cGray Matter bitches!\u201d \u2026.. no, he actually didn\u2019t.\n\nQ) Then why don\u2019t we directly implement the working of gray matter?\n\nAns) Simply because the working of gray matter is very very complex. We do not know the exact correlation between \u2018confidence in\u2019 and \u2018accuracy of\u2019 one\u2019s introspective abilities.\n\nQ) So what do we know?\n\nAns) Introspection is still (after millions years of evolution) a rare abilities even for complex organisms with most organisms lacking this ability completely. This has been one of the most recent development in evolution.\n\nQ) Which part of the brain is responsible for introspection?\n\nPrefrontal cortex (PFC) is the cerebral cortex which covers the front part of the frontal lobe.\n\nTwo parts of this area are of great interest:\n\nAns) Anterior prefrontal cortex has been associated with top-level processing abilities that are thought to set humans apart from other animals.\n\nMost of the gray-matter which plays the role in introspection exists in this region (right anterior prefrontal cortex, to be precise). The structure of neighbouring white matter plays some (unclear) role in introspective capabilities too.\n\nThis brain region has been implicated in planning complex cognitive behaviour, personality expression, decision making, and moderating social behaviour. The basic activity of this brain region is considered to be orchestration of thoughts and actions in accordance with internal goals.\n\nQ) What does that mean in terms of machine learning?\n\nAns) Medial prefrontal cortex (mPFC) is considered to be a part of the brain\u2019s reward system.\n\nThe mPFC is part of the mesocorticolimbic dopaminergic system (basically it generates dopamine, the \u2018happy enzyme\u2019).\n\nEvidence for the involvement of the mPFC in reward-related mechanisms comes mainly from three types of studies:\n\nWhat\u2019s interesting about this reward system is:\n\nQ) What does that mean in terms of machine learning?\n\nIt works something like this:\n\nAns) \u2018Reward\u2019 is what enables to prioritise one decision over other.\n\nIt is a scalar feedback, quantifying how well agent is adapting/interacting with environment.\n\nLike you give treat to your dog if he sits first else no treat (why u no gud boi?!).\n\nThe reinforcement learner simply follows the steps which maximise the cumulative reward.\n\nMultiple reward systems essentially mean that brain has inbuilt multi actor-critic reinforcement learning mechanism.\n\nIn simple words, actor-critic learning involves an actor who takes an action and critic finds pros and cons (criticises) in that action.\n\nQ) Role of reward system sounds very similar to introspection??\n\nAns) Well that\u2019s because reward system does play a major part in one\u2019s introspective abilities. This relation, though, is REALLY COMPLEX.\n\nSo Prefrontal Lobe is frontier in field of General Intelligence!"
    },
    {
        "url": "https://medium.com/@mukulmalik/intelligence-humans-v-s-computers-part-1-7a4ce1fd4f0a?source=user_profile---------7----------------",
        "title": "Brain inspired Computer Architecture \u2013 Mukul Malik \u2013",
        "text": "Ironically we know very less about our abler of knowledge.\n\nBrain, the organ that separates Simple and Rudimentary beings (me) from Complex & Intelligent beings (my maths professors, so they claimed). Brain is result of a large group of nerve cells (neurons) forming a large cluster. Acting in sync to perform varies functions enabling survival of organisms.\n\nBut not all organisms (like Jelly-fish, Sea Star, Sea Cucumber etc) don\u2019t have these clusters, hence we say they have no brain. While, technically true, these organisms still have presence of nerve cells (mostly all over their body with varying density of neurons).\n\nThat\u2019s why we have a pseudo-second brain, the Enteric Nervous System (more on that later).\n\nSo it\u2019s safe all complex organisms do need nervous activities to sustain life (though I don\u2019t trust nature when it comes to surprises).\n\nThe most interesting and intriguing are the brain of Human and Cephalopod. The latter is much less studied so we\u2019ll focus on human brain.\n\nHuman brain is a marvel to say at least (yes even your sibling\u2019s brain, I know you don\u2019t trust me right now).\n\nHave a look at these numbers:"
    },
    {
        "url": "https://medium.com/@mukulmalik/human-brain-s-f426e5bc9964?source=user_profile---------8----------------",
        "title": "Human Brain\u2018s\u2019 \u2013 Mukul Malik \u2013",
        "text": "Enteric nervous system is one of the main divisions of the nervous system and consists of a mesh-like system of neurons that governs the function of the gastrointestinal tract.\n\nIt is now usually referred to as separate from the autonomic nervous system since it has its own independent reflex activity.\n\nRemember Getting nervous before something bad happens of predicting someone is going to change the lane before they do so? Well this is due to the presence of chemical (information) super-highway called Brain-Gut Axis.\n\nThe information this axis shares is not just stored but due to presence of enormous number of neurons, ENS is also able to process this information and feed it\u2019s own responses to our gut (hence anxiety and nausea).\n\nIn fact, ENS contains about 95% of Serotonin which affects our mood.\n\nOur actual brain focuses on only a negligible amount of the overall data it processes. The brain has at least 10x feed-back mechanisms as it has feed-forward mechanisms. So it is able to make summaries of information it processes and some of these summaries are transferred to the ENS for releases different mood affecting chemicals if necessary."
    },
    {
        "url": "https://medium.com/@mukulmalik/how-machine-learning-is-being-abused-91e17bf52363?source=user_profile---------9----------------",
        "title": "How Machine Learning is Being Abused \u2013 Mukul Malik \u2013",
        "text": "The Real Beauty of Machine Learning is the ability to solve much much more complex tasks that have not even been defined yet (general intelligence).\n\nWord2Vec was revolutionary and RNN was Revolutionary. Notice I used past tense? Because they are already mainstream. Yet most of us are going to spend years simply using them, barely understanding why they work.\n\nAlso! Some people are ruining third wave for others\n\nMost people are hence not even close when it comes to solving non trivial problems. Using \u2018black boxes\u2019 like Tensorflow or Keras will hardly ensure you know even 10% of what you are building (Unless you go through their source code)!\n\nPutting different Lego pieces together is easy, the real magic lies in how you create the Lego pieces. Because. With a few types of Lego pieces there is only so much you can do. Whereas if you are able to make your own Lego blocks, the scope is endless!!\n\nI have seen people using CNN for sequential data. Not that the idea is taboo but what is taboo is that they never considered RNN for the task. Even when the problem was the definition problem for the RNN."
    },
    {
        "url": "https://hackernoon.com/effective-parallel-computing-bc8832114b7b?source=user_profile---------10----------------",
        "title": "Effective Parallel Computing \u2013",
        "text": "What are the fundamental instructions that make computing possible and how are they implemented?\n\nWell Starting off with basics:\n\nVector operations execute element by element operations on corresponding elements of vectors (Single or Multi-dimensional). If the operands have the same size, then each element in the first operand gets matched up with the element in the same location in the second operand. If the operands have compatible sizes, then each input is implicitly expanded as needed to match the size of the other.\n\nExample : Adding two Vectors of different sizes\n\nSo we expand the second matrix\n\nMatrix operations follow the rules of linear algebra and are not compatible with multidimensional arrays. The required size and shape of the inputs in relation to one another depends on the operation. For non-scalar inputs, the matrix operators generally calculate different answers than their array operator counterparts.\n\nMultiplication of two matrix using matrix operations will result in\n\nHere \u2018a\u2019, \u2018x\u2019 and \u2018y\u2019 rest in the main memory. An efficient implementation of axpy will load \u2018a\u2019 from memory to a register and will then compute with \u2018x\u2019 and \u2018y\u2019, which must be fetched from memory as well. The updated result, \u2018y\u2019, must also be stored, for a total of about 3n memops for the 2n flops that are executed. Thus, three memops are required for every two flops. If memops are more expensive than flops (as usually is the case), it is the memops that limit the performance that can be attained for axpy.\n\ni.e. the speed of loading content into and from memory forms the bottleneck.\n\nHere \u2018x\u2019 and \u2018y\u2019 are Vectors but \u2018A\u2019 is a matrix.\n\nThis operation involves roughly n^2 data (for the matrix), initially stored in memory, and 2n^2 flops. Thus, an optimal implementation will fetch every element of \u2018A\u2019 exactly once, yielding a ratio of one memop for every two flops. Although this is better than the ratio for the axpy, memops still dominate the cost of the algorithm if they are much slower than flops.\n\nThe term \u2018op(X)\u2019 here signifies that the matrix can either be X or X^T (transpose).\n\nSo what that equation essentially means is that the above equation can take the following forms :\n\nConsider the product C := AB + C where all three matrices are square of order n. This operation involves 4n^2 memops (A and B must be fetched from memory while C must be both fetched and stored) and requires 2n^3 flops for a ratio of 4n^2 /2n^3 = 2/n memops/flops. Thus, if n is large enough, the cost of performing memops is small relative to that of performing useful computations with the data, and there is an opportunity to amortize the cost of fetching data into the cache over many computations.\n\nNow there are Many implementations of GEMM on basis of m,n,k (Dimensions of Matrices)\n\nFollowing are the ways to perform it\n\nGEMM is still one of the most optimizable way of computing with only issue, that it loads a large size of information in memory. So we break matrix into smaller matrices and multiply them in batches.\n\nGaussian Estimation based methods like LU factorisation are commonly used to excel the MatMul problems. When applicable, Cholesky Decomposition can be almost three times more efficient than LU factorisation.\n\nIf you are not familiar with Gaussian Estimation then this video will be very helpful\n\nSo some Deep Learners like CNN (Convolution Neural Networks) are can inherently be parallelised to a much greater extent than others like RNN (Recurrent Neural Network).\n\nIn Case of CNN, let\u2019s take an example of a 4096*4096 size image being analysed. That means there are essentially there are 16,777,216 pixels to be analysed. Here in pooling layer, each block can be viewed as independent computation. Hence each block can be solved independently and just the results can be combined. So Heavy Parallelism can be implemented.\n\nThis also means that it could be broken into batches easily in case if not all 16 million pixels fit in memory. So we could load a small blocks of these 16 million pixels in memory and still optimise them and apply GEMM.\n\nLet\u2019s assume this is an image\n\nThis block will be broken into smaller matrices"
    },
    {
        "url": "https://hackernoon.com/word2vec-part-1-fe2ec6514d70?source=user_profile---------11----------------",
        "title": "Word2Vec (Part 1) \u2013",
        "text": "Ans) I\u2019ll sum it up with three main reasons:\n\n1. Computer cannot do computations on strings.\n\nQ) So what is Explicit information?\n\nAns) Yes, the word itself doesn\u2019t say much about what it represents in real life. Example:\n\nThe string \u201ccat\u201d just tells us it has three alphabets \u201cc\u201d, \u201da\u201d and \u201ct\u201d.\n\nIt has no information about the animal it represents or the count or the context in which it is being used.\n\nAns) Short answer (for now), these vectors can hold Enormous information compared to their size.\n\nA) There are two main categories:\n\nPredictions can be of two types:\n\nAns) Let\u2019s look at an example:\n\nConsider the following sentence \u201cNewton does not like apples.\u201d"
    },
    {
        "url": "https://medium.com/@mukulmalik/natural-language-processing-nlp-933cb7162932?source=user_profile---------12----------------",
        "title": "Natural Language Processing (NLP) \u2013 Mukul Malik \u2013",
        "text": "Ans) NLP is exactly what you are doing right now. Or what you do when you hear your mom scream at you.\n\nAns) Understanding a Natural Language (Languages used by humans to exchange information like English, French, German etc).\n\nQ) Is it related to Artificial Intelligence and Machine Learning?\n\nAns) Machine Learning (ML) is the \u2018I\u2019 (Intelligence) in A.I.\n\nML, at its core, is art of Identifying, Generating, Discriminating and Understanding patterns/relations.\n\nNLP is a subset of ML which deals with patterns/relations of words found in Natural Languages. Language Patterns are sequential patterns. Sequential because in case of languages, a word is connected to other words on its right and its left (definition of sequence).\n\nIn this digital age, Text and Audio are the major forms of communication. Even Audio can be converted into text ( Remember \u201cHey Siri, Who\u2019s your daddy?\u201d ). Hence, making Text the most fertile form of communication to be analysed.\n\nQ) So what can NLP do?\n\nAns) Consider the following text by human:\n\n\u201cMessi and Ronaldo are great football players. Ronaldo is still the better one. He is much more athletic. Though other has natural talent\u201d\n\nWe can infer following from this text\n\nYES! All that from this small Text!\n\nQ) How did we infer all that?\n\nAns) The above inferences are high level conclusions which are mixture of low level inferences. Some low level inferences used above are:"
    },
    {
        "url": "https://medium.com/@mukulmalik/intro-to-big-data-f691c1c2e375?source=user_profile---------13----------------",
        "title": "Introduction To BIG Data \u2013 Mukul Malik \u2013",
        "text": "It\u2019s funny how we can relate to a concept such as Data.\n\nAns) It, simply, is a huge collection of weird data that pushes boundaries of technology one is equipped with.\n\nQ) How do you define it?\n\nAns) It doesn\u2019t really have hard definition, which means something that is considered Big Data to one might not be Big Data for others.\n\nFor instance, 2 TB of text is Big \u201cFreaking\u201d Data for me but it\u2019s just piece of cake for Google.\n\nQ) So no way of defining it?\n\nAns) We can set a few characteristics that are usually seen in Big Data like 3 \u2018V\u2019s to define it:\n\n1) Volume: Huge Size!! Like whole of Wikipedia\n\n2) Velocity: Speed of generation of data is tremendously fast like tracking locations of Billion Android phones simultaneously.\n\n3) Variety: Really Clumpy! No fixed pattern or format. Like Facebook has pictures, videos, gifs etc. and each of them has its own encoding (.JPG, .MP4)\n\nAns) If you can\u2019t open the data file without freezing your computer and it\u2019s not feasible to manually inspect the data, then it has started to get big enough. At time, it requires more than the memory you have on your system, that you have to compress and store it.\n\nAns) You would require heavy parallelism \u2014 Multiple Servers, Load Balancers, Backup nodes etc.\n\nThink of it as catching balls. If it\u2019s just 1 ball, you can easily catch it. If there are 2 balls, it\u2019ll be slightly tough but you still might be able to catch them. Now imagine there are 1000 balls being thrown at 10 balls per second. Then you would need at least 10 hands to have a chances of catching most of those balls.\n\nAns) The thing with diversity is that it induces randomness. If the data is very diverse and you are already aware of the diversity then you can beforehand prepare yourself for it. The problem arises when a format that you had never anticipated comes across, then your system might not know what to do with it.\n\nExample: You have million camera vendors. You ask them to enter the MegaPixels of cameras in order to rank them.\n\nNow you assume that everyone will enter a suitable number but then comes along a genius who writes \u201cVery beautiful\u201d under MegaPixel column. Your system was expecting a number but it got English characters, so now your system won\u2019t be able to comprehend it or know what to do with it.\n\nThus,the larger the data, the more genius answers will exist.\n\nQ) Ok, so is that it? That is how we define Big Data?\n\nAns) Yes, but there are 4 additional \u2018V\u2019s that were added recently:\n\n1) Veracity: The data should be correct\n\n2) Variability: It should have broad horizon of info\n\n3) Visualization: It could be visualized (graphs, charts etc.)\n\n4) Value: It should have huge potential (commercial or scientific use)\n\nBUT AGAIN, these are not the hard definitions. The data might just be huge and diverse and it could still be a Big Data. The above points were just the expectations people have from Big Data.\n\nQ) Ok, so is Big Data all creepy?\n\nAns) No, It\u2019s a sea of knowledge. You just have to analyse it. After that, you can see past its weirdness and look at the potential it has."
    },
    {
        "url": "https://hackernoon.com/introduction-to-machine-learning-cab3c2efeb42?source=user_profile---------14----------------",
        "title": "Introduction To Machine Learning \u2013",
        "text": "Machine Learning, we all have heard it and heard it ears full. Yet we hesitate to get a hang of it.\n\nAns) No, it is used in Rocket Science though.\n\nQ) Why is it that we are scared to take a peek into it?\n\nAns) Maybe what it does seem like a miracle to us. So we assume it is something out of our scope of learning/understanding.\n\nQ) How tough/complex is it?\n\nAns) Anyone who has dared to fight this hydra knows that it is a child\u2019s play (well that was an understatement but you get the idea).\n\nQ) So what is it?\n\nAns) It is an attempt to make things more intelligent. Most of us have come across terms like \u201cArtificial Neural Networks\u201d, it is an attempt to replicate the working of the human brain. Even something like this is not necessarily always complex. At its heart, it is just multiplication and differentiation. Yes, Maths at it again but it\u2019s rather what you learned at school, no different (This coming from a guy who is petrified of maths).\n\nQ) What does intelligent mean?\n\nAns) Understanding concepts or patterns behind the working of something. It could be understanding Emotions, making sense out of Human Languages (Ex: English , Hindi , French) and cool stuff like predictions.\n\nQ) So what can it do?\n\nAns) Well Everything that a human can and a lot more. Some applications are really (Really REALLY !!) cool.\n\nQ) Ok \u2026. ? Like what?\n\n1) Like predicting most relevant option out of a billion choices on eCommerce websites.\n\n2) Remember Tinder? Well for all those who have found a Hot Match, thank you Machine Learning!\n\n3) Netflix uses it to guess your mood and recommend the movie that you will be most interested in.\n\n4) Google uses it to guess the most relevant page out of a billion (even a few hundred billion) results.\n\n5) It is being used in Medical field to predict diseases like Cancer before a person actually gets infected by it. Goosebumps anyone?\n\nSo it funnels down the result something like this:\n\nNow these 5 options are the recommendations it will pitch at you but it doesn\u2019t stop just there.\n\nYou usually watch movies between 6 P.M to 10 P.M -> Schedules recommendation\n\nYou usually watch scary movies before sleeping -> Prioritizes scary movies near 10 P.M slot\n\nQ) Now the question is what value should be given to which rule?\n\nQ) Why am I so excited about not knowing the answer?\n\nAns) Because this is the power of Machine learning! It automates this process.\n\nResults almost made me do the Archimedes.\n\nNow the traditional learning would infer that sentence 2 and 3 have a same word \u2018missed\u2019. Rest no similarities. So 2 and 3 are closer. Stupid, right?\n\nWhereas our brain knows that 1 and 2 are used in same context which is sports or football precisely.\n\nBTW so do our Vectors ;)\n\nThe vector of Ronaldo will have a value much closer to the vector of Messi. So when we find the similarities between the sentences using vectors we get 1 and 2 are the closer ones. Smarter, right?\n\nQ) So a matrix of numbers can understand the language and the context? :O\n\nAns) Yes, rainbows in your eyes and wide open mouth are normal at this point ;)"
    }
]