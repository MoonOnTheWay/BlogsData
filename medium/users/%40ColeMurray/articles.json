[
    {
        "url": "https://medium.freecodecamp.org/building-an-image-caption-generator-with-deep-learning-in-tensorflow-a142722e9b1f?source=user_profile---------1----------------",
        "title": "Building an image caption generator with Deep Learning in Tensorflow",
        "text": "In my last tutorial, you learned how to create a facial recognition pipeline in Tensorflow with convolutional neural networks. In this tutorial, you\u2019ll learn how a convolutional neural network (CNN) and Long Short Term Memory (LSTM) can be combined to create an image caption generator and generate captions for your own images.\n\nIn 2014, researchers from Google released a paper, Show And Tell: A Neural Image Caption Generator. At the time, this architecture was state-of-the-art on the MSCOCO dataset. It utilized a CNN + LSTM to take an image as input and output a caption.\n\nA convolutional neural network can be used to create a dense feature vector. This dense vector, also called an embedding, can be used as feature input into other algorithms or networks.\n\nFor an image caption model, this embedding becomes a dense representation of the image and will be used as the initial state of the LSTM.\n\nAn LSTM is a recurrent neural network architecture that is commonly used in problems with temporal dependences. It succeeds in being able to capture information about previous states to better inform the current prediction through its memory cell state.\n\nAn LSTM consists of three main components: a forget gate, input gate, and output gate. Each of these gates is responsible for altering updates to the cell\u2019s memory state.\n\nIn a sentence language model, an LSTM is predicting the next word in a sentence. Similarly, in a character language model, an LSTM is trying to predict the next character, given the context of previously seen characters.\n\nIn an image caption model, you will create an embedding of the image. This embedding will then be fed as initial state into an LSTM. This becomes the first previous state to the language model, influencing the next predicted words.\n\nAt each time-step, the LSTM considers the previous cell state and outputs a prediction for the most probable next value in the sequence. This process is repeated until the end token is sampled, signaling the end of the caption.\n\nGenerating a caption can be viewed as a graph search problem. Here, the nodes are words. The edges are the probability of moving from one node to another. Finding the optimal path involves maximizing the total probability of a sentence.\n\nSampling and choosing the most probable next value is a greedy approach to generating a caption. It is computationally efficient, but can lead to a sub-optimal result.\n\nGiven all possible words, it would not be computationally/space efficient to calculate all possible sentences and determine the optimal sentence. This rules out using a search algorithm such as Depth First Search or Breadth First Search to find the optimal path.\n\nBeam search is a breadth-first search algorithm that explores the most promising nodes. It generates all possible next paths, keeping only the top N best candidates at each iteration.\n\nAs the number of nodes to expand from is fixed, this algorithm is space-efficient and allows more potential candidates than a best-first search.\n\nUp to this point, you\u2019ve learned about creating a model architecture to generate a sentence, given an image. This is done by utilizing a CNN to create a dense embedding and feeding this as initial state to an LSTM. Additionally, you\u2019ve learned how to generate better sentences with beam search.\n\nIn the next section, you\u2019ll learn to generate captions from a pre-trained model in Tensorflow.\n\nHere, you\u2019ll use Docker to install Tensorflow.\n\nDocker is a container platform that simplifies deployment. It solves the problem of installing software dependencies onto different server environments. If you are new to Docker, you can read more here. To install Docker, run:\n\nAfter installing Docker, you\u2019ll create two files. A requirements.txt for the Python dependencies and a Dockerfile to create your Docker environment.\n\nIf you would like to avoid building from source, the image can be pulled from dockerhub using:\n\nBelow, you\u2019ll download the model graph and pre-trained weights. These weights are from a training session on the MSCOCO dataset for 2MM iterations.\n\nNext, create a model class. This class is responsible for loading the graph, creating image embeddings, and running an inference step on the model.\n\nWhen training an LSTM, it is standard practice to tokenize the input. For a sentence model, this means mapping each unique word to a unique numeric id. This allows the model to utilize a softmax classifier for prediction.\n\nBelow, you\u2019ll download the vocabulary used for the pre-trained model and create a class to load it into memory. Here, the line number represents the numeric id of the token.\n\nTo store this vocabulary in memory, you\u2019ll create a class responsible for mapping words to ids.\n\nTo generate captions, first you\u2019ll create a caption generator. This caption generator utilizes beam search to improve the quality of sentences generated.\n\nAt each iteration, the generator passes the previous state of the LSTM (initial state is the image embedding) and previous sequence to generate the next softmax vector.\n\nThe top N most probable candidates are kept and utilized in the next inference step. This process continues until either the max sentence length is reached or all sentences have generated the end-of-sentence token.\n\nNext, you\u2019ll load the show and tell model and use it with the above caption generator to create candidate sentences. These sentences will be printed along with their log probability.\n\nTo generate captions, you\u2019ll need to pass in one or more images to the script.\n\nYou should see output:\n\nIn this tutorial, you learned:\n\nIf you enjoyed this tutorial, follow and recommend!\n\nInterested in learning more about Deep Learning / Machine Learning? Check out my other tutorials:\n\nOther places you can find me:"
    },
    {
        "url": "https://hackernoon.com/building-a-facial-recognition-pipeline-with-deep-learning-in-tensorflow-66e7645015b8?source=user_profile---------2----------------",
        "title": "Building a Facial Recognition Pipeline with Deep Learning in Tensorflow",
        "text": "You\u2019ll use the LFW (Labeled Faces in the Wild) dataset as training data. The directory is structured as seen below. You can replace this with your dataset by following the same structure.\n\nBelow, you\u2019ll pre-process the images before passing them into the FaceNet model. Image pre-processing in a facial recognition context typically solves a few problems. These problems range from lighting differences, occlusion, alignment, segmentation. Below, you\u2019ll address segmentation and alignment.\n\nFirst, you\u2019ll solve the segmentation problem by finding the largest face in an image. This is useful as our training data does not have to be cropped for a face ahead of time.\n\nSecond, you\u2019ll solve alignment. In photographs, it is common for a face to not be perfectly center aligned with the image. To standardize input, you\u2019ll apply a transform to center all images based on the location of eyes and bottom lip.\n\nHere, you\u2019ll use docker to install tensorflow, opencv, and Dlib. Dlib provides a library that can be used for facial detection and alignment. These libraries can be a bit difficult to install, so you\u2019ll use Docker for the install.\n\nDocker is a container platform that simplifies deployment. It solves the problem of installing software dependencies onto different server environments. If you are new to docker, you can read more here. To install docker, run\n\nAfter installing docker, you\u2019ll create two files. A requirements.txt for the python dependencies and a Dockerfile to create your docker environment.\n\nIf you would like to avoid building from source, the image can be pulled from dockerhub using:\n\nAfter creating your environment, you can begin preprocessing.\n\nYou\u2019ll use this face landmark predictor to find the location of the inner eyes and bottom lips of a face in an image. These coordinates will be used to center align the image.\n\nThis file, sourced from CMU, provides methods for detecting a face in an image, finding facial landmarks, and alignment given these landmarks.\n\nNext, you\u2019ll create a preprocessor for your dataset. This file will read each image into memory, attempt to find the largest face, center align, and write the file to output. If a face cannot be found in the image, logging will be displayed to console with the filename.\n\nAs each image can be processed independently, python\u2019s multiprocessing is used to process an image on each available cpu core.\n\nNow that you\u2019ve created a pipeline, time to get results. As the script supports parallelism, you will see increased performance by running with multiple cores. You\u2019ll need to run the preprocessor in the docker environment to have access to the installed libraries.\n\nBelow, you\u2019ll mount your project directory as a volume inside the docker container and run the preprocessing script on your input data. The results will be written to a directory specified with command line arguments.\n\nUsing Dlib, you detected the largest face in an image and aligned the center of the face by the inner eyes and bottom lip. This alignment is a method for standardizing each image for use as feature input.\n\nNow that you\u2019ve preprocessed the data, you\u2019ll generate vector embeddings of each identity. These embeddings can then be used as input to a classification, regression or clustering task.\n\nYou\u2019ll use the Inception Resnet V1 as your convolutional neural network. First, create a file to download the weights to the model.\n\nBy using pre-trained weights, you are able to apply transfer learning to a new dataset, in this tutorial the LFW dataset:\n\nBelow, you\u2019ll utilize Tensorflow\u2019s queue api to load the preprocessed images in parallel. By using queues, images can be loaded in parallel using multi-threading. When using a GPU, this allows image preprocessing to be performed on CPU, while matrix multiplication is performed on GPU.\n\nWith the input queue squared away, you\u2019ll move on to creating the embeddings.\n\nFirst, you\u2019ll load the images from the queue you created. While training, you\u2019ll apply preprocessing to the image. This preprocessing will add random transformations to the image, creating more images to train on.\n\nThese images will be fed in a batch size of 128 into the model. This model will return a 128 dimensional embedding for each image, returning a 128 x 128 matrix for each batch.\n\nAfter these embeddings are created, you\u2019ll use them as feature inputs into a scikit-learn\u2019s SVM classifier to train on each identity. Identities with less than 10 images will be dropped. This parameter is tunable from command-line.\n\nNow that you\u2019ve trained the classifier, you\u2019ll feed it new images it has not trained on. You\u2019ll remove the is_train flag from the previous command to evaluate your results.\n\nAfter inference is on each image is complete, you\u2019ll see results printed to console. At 5 epochs, you\u2019ll see ~85.0% accuracy. Training @ 25 epochs gave results:\n\nIn this tutorial, you learned about the history of machine learning and how to implement a state of the art pipeline. You utilized docker to manage your library dependencies, offering a consistent environment that is platform agnostic. You used Dlib for preprocessing and Tensorflow + Scikit-learn for training a classifier capable of predicting an identity based on an image."
    },
    {
        "url": "https://hackernoon.com/deep-learning-cnns-in-tensorflow-with-gpus-cba6efe0acc2?source=user_profile---------3----------------",
        "title": "Deep Learning CNN\u2019s in Tensorflow with GPUs \u2013",
        "text": "Convolutional neural networks are the current state-of-art architecture for image classification. They\u2019re used in practice today in facial recognition, self driving cars, and detecting whether an object is a hot-dog.\n\nThe basics of a CNN architecture consist of 3 components. A convolution, pooling, and fully connected layer. These components work together to learn a dense feature representation of an input.\n\nA convolution consists of a kernel (green square above), also called filter, that is applied in a sliding window fashion to extract features from the input. This filter is shifted after each operation across the input by an amount called strides. At each operation, a matrix multiply of the kernel and current region of input is calculated. Filters can be stacked to create high-dimensional representations of the input.\n\nThere are two ways of handling differing filter size and input size, known as same padding and valid padding. Same padding will pad the input border with zeros (as seen above) to ensure the input width and height are preserved. Valid padding does not pad.\n\nTypically, you\u2019ll want to use same padding or you\u2019ll rapidly reduce the dimensionality of your input.\n\nFinally, an activation function (typically a ReLU) is applied to give the convolution non-linearity. ReLU\u2019s are a bit different from other activation functions, such as sigmoid or tanh, as ReLUs are one-sided. This one-sided property allows the network to create sparse representation (zero value for hidden units), increasing computational efficiency.\n\nPooling is an operation to reduce dimensionality. It applies a function summarizing neighboring information. Two common functions are max pooling and average pooling. By calculating the max of an input region, the output summarizes intensity of surrounding values.\n\nPooling layers also have a kernel, padding and are moved in strides. To calculate the output size of a pooling operation, you can use the formula\n\nFully connected layers you are likely familiar with from neural networks. Each neuron in the input is connected to each neuron in the output; fully-connected. Due to this connectivity, each neuron in the output will be used at most one time.\n\nIn a CNN, the input is fed from the pooling layer into the fully connected layer. Depending on the task, a regression or classification algorithm can be applied to create the desired output.\n\nYou\u2019ve now learned about what makes up a convolutional neural network. By passing input through a convolution, you extract highly-dimensional features. Pooling summarizes spatial information and reduces dimensionality. Lastly, this feature representation is passed through fully connected layers to a classifier or regressor."
    },
    {
        "url": "https://medium.com/google-cloud/keras-inception-v3-on-google-compute-engine-a54918b0058?source=user_profile---------4----------------",
        "title": "Deep Learning with Keras on Google Compute Engine \u2013 Google Cloud Platform \u2014 Community \u2013",
        "text": "In this tutorial, you\u2019ll use the pre-trained Inception model to provide predictions on images uploaded to a web server.\n\nBelow, the Inception model is loaded with Keras. Keras holds a cache directory of the models pre-trained weights. On first use, Keras will download these weights into ~/.keras/models/.\n\nYou\u2019ll create a predict function with accepts a base64 encoded image file. Inception V3 requires images to be 299 x 299. After loading the image, it is expanded into a vector and pre-processed.\n\nWith the model loaded, create a web server that can accept base64 encoded images using flask.\n\nYou now have image-recognition as a service! Let\u2019s test it out locally before deploying. First, start the server. Then make a POST request to the prediction service. Sample image\n\nIf everything is working correctly, you\u2019ll see a response back\n\nYou\u2019ve got the model created and generating predictions. Time to deploy the model to Google Compute Engine using Docker. You\u2019ll use a few different technologies Gunicorn, Nginx, and Supervisor. Below, a dockerfile and a few configuration files will set up and serve the prediction api.\n\nAfter creating the docker image, you\u2019ll push it to Google Container Registry.\n\nFor docker to copy the files correctly, structure the directory:\n\nYou\u2019ve built and pushed the docker image to Google Container Registry. From here, create the server and pull down the previously created docker image. First, you\u2019ll enable the API\n\nIf all went well, after a short bit, you\u2019ll have a running prediction service. You can use the curl command above to confirm it\u2019s working. Let\u2019s build a quick front-end to visualize our predictions."
    },
    {
        "url": "https://medium.com/google-cloud/recommendation-systems-with-spark-on-google-dataproc-bbb276c0dafd?source=user_profile---------5----------------",
        "title": "Recommendation Systems with Spark on Google DataProc",
        "text": "In this tutorial, You\u2019ll be learning how to create a movie recommendation system with Spark, utilizing PySpark.\n\nThe tutorial will focus more on deployment rather than code. We\u2019ll be deploying our project on Google\u2019s Cloud Infrastructure using:\n\nYou\u2019ll first need to download the dataset we\u2019ll be working with. You can access the small version of the movielens\u2019 dataset here (1MB). After verifying your work, you can test it with the full dataset here (224MB).\n\nWithin this dataset, you\u2019ll be utilizing the ratings.csv and movie.csv files. Each file provides headers for the columns as the first line entry. You\u2019ll need to remove this before loading the data into CloudSQL.\n\nYou\u2019ll need to create a few SQL scripts to create the db and tables.\n\nCreate a bucket and load the scripts into Google Cloud Storage. Buckets in cloud storage have unique name identifications. You\u2019ll need to replace below with a name of your choosing. Using Google-Cloud-Sdk from terminal:\n\nAfter this step, you can look into GCloud Storage and confirm the files were successfully uploaded.\n\nNext, you\u2019ll create your sql database. Select second-generation. (I\u2019ve disabled backups as this is a project and not necessary, you may choose otherwise):"
    },
    {
        "url": "https://medium.com/google-cloud/node-to-google-cloud-compute-engine-in-25-minutes-7188830d884e?source=user_profile---------6----------------",
        "title": "Node to Google Cloud Compute Engine \u2013 Google Cloud Platform \u2014 Community \u2013",
        "text": "To get started, let\u2019s create a new project on compute engine.\n\nAfter creating your project, navigate to Compute Engine in the Menu.\n\nSweet! We have our VM configured ready for our project.\n\nFor this tutorial, I\u2019ll be using the web ssh client.\n\nWe need to install Node.js and Npm to setup and run our project. SSH into our machine and enter these commands:\n\nAfter installing, confirm everything installed correctly. You should see output similar to below.\n\nWe need to download our source code and task runner for our project.\n\nAfter a minute or two, we\u2019ll have all our dependencies and task runner, gulp, installed. Gulp is our task running we will be using to create our builds. Read more here https://github.com/gulpjs/gulp\n\nNginx will serve as our reverse proxy. This allows our node application to be accessed from port 80\n\nwe can test that it installed correctly by doing\n\nWe now need to configure nginx to serve as our reverse proxy for our node server.\n\nNavigate to Nginx\u2019s sites-available folder. This folder contains configurations for nginx and will be where we create our new configuration.\n\nNow within our default file (/etc/nginx/sites-available/default):\n\nGreat, now we\u2019ve got nginx configured. Our next step is to setup node for production."
    },
    {
        "url": "https://medium.com/@ColeMurray/how-to-track-impressions-e371f0a91f0?source=user_profile---------7----------------",
        "title": "Android RecyclerView Analytics \u2013 Cole Murray \u2013",
        "text": "Our next few code snippets will focus on setting up our RecyclerView. We\u2019ll soon have a glimpse of success!\n\nAdd the recyclerView library into your build.gradle file.\n\nFirst let\u2019s create our recycler view layout in fragment_main.xml:\n\nNow create our item view for our recyclerView product_item_layout.xml:\n\nThis viewholder will be used in our recyclerView. It will hold the view of product_item.layout.xml.\n\nIn this class, we take in an activity (we\u2019ll use this later), a list of data and inflate our ProductViewHolder for each item in the list.\n\nIn our onBindViewHolder method, we take a title from our dataset according to the position in the recycler view and set the title to textview in the view holder. I added in the alternate background to make the view distinction more prominent; it is not necessary.\n\nWe need to find our recycler view in the view hierarchy, and set our adapter to it. In MainActivityFragment.java:\n\nBuild and run the project and you should see this:\n\nAt this point, we\u2019ve got a basic recycler view setup that\u2019s binding our data to views, Sweet. Let\u2019s start wiring up our tracking.\n\nStill in our ImpressionAdapter class, add a member variable for our visibilityTracker and viewPositionMap. We\u2019ll also need to take in our activity as a parameter in the constructor to initialize our tracker.\n\nIn our onBindViewHolder we\u2019ll add two lines. Our first line is creating a mapping of our view to it\u2019s position in the recyclerView.We\u2019ll use this later on in the tutorial. Our second line adds the view to mVisibilityTracker\u2019s tracked views."
    },
    {
        "url": "https://medium.com/@ColeMurray/react-flux-in-es6-pt-1-2-e2a7b4aa074e?source=user_profile---------8----------------",
        "title": "React & Flux In ES6! (Pt 1/2) \u2013 Cole Murray \u2013",
        "text": "I began learning react and flux a short time ago. As I searched the web for react, webpack, and flux, written in ES6, I failed to find a tutorial that had what I truly wanted. A tutorial I could run through start to finish that required minimal effort and met my search criteria.\n\nI was able to string together the bits to form my ideal tutorial. A project that\u2019s familiar, easily configurable, concise, and most importantly\u2026 IN ES6!\n\nGetting things started, we need to initialize a new node project. Open up a shell and navigate to your project\u2019s working directory. Enter command:\n\nNpm will now create a package.json file in our current directory. This file will contain all our dependencies for the project. Npm will ask you a few questions related to the project. You can skip these by pressing enter if you\u2019d like.\n\nHere\u2019s the structure we are aiming to create:\n\nTo create the above folder structure, enter command:\n\nOur project\u2019s folder structure has been created, we\u2019ve initialized our node project, we\u2019re ready to go.\n\nFor this project, we\u2019ll have several dependencies.\n\nIn the root of your projects directory we need to have npm install our dependencies, to do so enter this command:\n\nOur project now has a basic skeleton. Let\u2019s begin to flesh out our server.\n\nLet\u2019s create our webpack configuration file in our project\u2019s root directory:\n\nIt is important to include the .babel.js extension so our webpack configuration can be written in ES6:\n\nEntry: Entry denotes where our application will be started from. In this case, we are pointing to a file called app.js.\n\nOutput: With our output object, we specify to webpack the filename and path for where we would like to create our compiled project. This will create a file called bundle.js in the ./build folder. publicPath refers to the path needed for webpack-dev-server.\n\nLoaders: Loaders pre-process files. In our case, we\u2019d like to pre-process any files that end in .js with the babel loader, excluding any in the node_modules folder. Our other loader is react-hot which allows us to update our modules without requiring a page reload. React-hot saves a lot of time and is a nice addition to the project. \u2018react-hot\u2019 must be on the left of babel. Babel reads loaders right-to-left and we need to pass babel\u2019s output into react-hot.\n\nIf you would like to read more about webpack configuration, see the link here:\n\nhttps://github.com/webpack/docs/wiki/configuration\n\nTo confirm our configuration file is working, we need to create a file called app.js:\n\nFor simplicity and ease of results, let\u2019s add a console logging statement. Add this to app.js:\n\nWe need to load our script with our html so we can see our results:\n\nOur final step is to configure npm to build with the proper configuration. Edit your package.json:\n\nWe\u2019ve added two options to our script for npm to run. Our first option, We want to setup webpack to build our project. Our second option start uses webpack-dev-server to build our project.\n\nWebpack-dev-server is an express server that serves our webpack bundle. It uses Socket.IO to watch the client\u2019s compilation state and respond to changes.\n\nWe\u2019re configured! Hopefully. Time to build our project:\n\nRunning this command will start our server. Visit http://localhost:8080. Open the console and we should see this.\n\nOur project is now configured and ready for us to begin using react and flux.\n\nComplete code up to this step can be found here:\n\nConfigurationAndHtmlComplete\n\nIn the following section, we will build out our react components. For a to-do list, there is two major components. These are a text-input field and a table that displays the current to-do\u2019s.\n\nFor the scope of this tutorial, I will only be focusing on using flux to create and view the to-do\u2019s.\n\nFor all of our components, we will need to import the react module.\n\nTo create our input field, we need to create a component in our js/components folder. (including .react is not necessary)\n\nAssuming you are familiar with react, there is not significant change from ES5 -> ES6. Here is our code for our textInputField:\n\nThis component takes in placeholder and a callback onSave that is executed when we press Enter.\n\nWe will use our header to pass in our callback to TodoTextInput. Create a file called Header.react.js in our components folder.\n\nWe now need to include this component in our app.js file:\n\nFinally, let\u2019s update our index.html to include a place to put this component.\n\nOpen console and enter a todo item. You should see a console output similar to this image:\n\nWith ES6, we can now extend components to create our classes. Say goodbye to React.createClass({ \u2026.}). Additionally, we no longer use getInitialState() to load our state and prop type variables. We can now use a method called constructor to create our object.\n\nReact.createClass({}) by default bound our methods to our class. In ES6, this is no longer provided. We now need to bind our components to explicitly. This can be done in multiple ways. I will demonstrate two different ways in this tutorial. For this class, we explicitly bound our methods where they are used, see line 41. This can also be done in the constructor which I will demonstrate later.\n\nComplete code up to this point can be found here:\n\nTodoComponent_complete\n\nIn the next part, We\u2019ll cover:\n\nThanks for following along. I hope you found this tutorial useful. Stay tuned for part 2 coming soon!\n\nI\u2019d love to hear your thoughts on this tutorial. Leave a comment, recommend, suggestions, or send me a tweet!\n\nSpecial thanks to the tutorials that I\u2019ve pieced together:\n\nand the others I\u2019m sure I\u2019ve left out."
    }
]