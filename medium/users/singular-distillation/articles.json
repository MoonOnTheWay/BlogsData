[
    {
        "url": "https://medium.com/singular-distillation/learning-exploratory-data-analysis-with-sabermetrics-976484954863?source=---------0",
        "title": "Learning Exploratory Data Analysis with Sabermetrics",
        "text": "No team can win without scoring, yet what makes for a good offensive player?\n\nIt\u2019s easy to accept some of the basic statistics at face value. Batting average is an example of this; the simple hits divided by at-bats seems a good indicator because it describes how often a player get\u2019s to first base for every at-bat. But this isn\u2019t helpful because it doesn\u2019t describe all the ways a player could get to base. It doesn\u2019t account for getting hit by the pitch or if he walks. Both of these cause him to advance a base, so they should be just as valid. Because nobody was valuing these other stats, the Oakland A\u2019s were able to put together a valuable team based on this simple statistic, famously described in Michael Lewis\u2019 book Moneyball.\n\nI\u2019ll be going over some of the offensive statistics used in baseball and how they can be helpful in evaluating players along with how they might be used. I\u2019ve just described batting average, but I\u2019ll also be going over slugging percentage, isolated strength, on-base percentage plus slugging, and so on.\n\nWhy am I writing about this? Because I think sabermetrics serves as a great testbed for doing data exploration, learning SQL, and doing plotting.\n\nBaseball is a unique sport in that every game state can be described by concrete numbers and performance can be isolated, relatively speaking. Hockey stands in contrast to this, as the layout of a team on the ice and the exact shot a player attempts is much more difficult to describe.\n\nThis makes baseball a great game to do data exploration; you can describe the different events relatively well using numbers. The Lahman database that I used here also exists in a SQL form that allows for good basic SQL training, with some people doing all their anlayses with it (as a rule of thumb for readers: if you can do it in excel, you can probably do it in SQL).\n\nNote that some player barely come up to base and thus can be considered outliers. We want players who record at least 100 at-bats. If we assume a player bats at least 40 games a season and has a minimum of 3 plate appearances before game, a reasonable assumption, we can say that 120 at-bats should be sufficient a hurdle.\n\nWe also say that players who batted before 1920 don\u2019t count. Statistics before then were not very accurate and the rules of the game varied wildly. 1920 is a good starting point as that year the spit ball was made illegal, ushering in consistent stats in the game.\n\nLet\u2019s look at batting average first. The equation for batting average is really simple:\n\nFirst, whats \u2018H\u2019? That\u2019s shorthand for a hit, which describes a player who successfully hits a ball into play and advances to first base. \u2018AB\u2019 is short for \u2018At-bat\u2019 which describes a player who comes up to the plate and hits the ball in such a way that no sacrifice blunt, sacrifice fly, walk, or hit by pitch occurs. Basically, he has to earnestly try and put the ball into play and make it to at least first base.\n\nThis is pretty neat, but it looks at year-by-year; let\u2019s look at a given player\u2019s career.\n\nKeep in mind that the below loop will take a little while to run as it loops through every player. It grabs the average for every player.\n\nIt\u2019s important to note that this might not match other lists. This is because we cut off pre-1920 stats. Players like Ty Cobb made the most of their careers before then, so their stats are not included. Players like Bob Hazle only played for a few years; the Lahman database I\u2019m using only records his first year. The worst player on the list was a WWII era batter, likely put into the squad as most of the major leaguers were drafted into the war.\n\nBatting average is a common metric, but it doesn\u2019t tell us much. Players can advance to base for other reasons, such as a walk. It also doesn\u2019t take into account plate appearances that result in a sacrifice fly.\n\nSo we\u2019ll build up that stat. First we need \u2018PA\u2019 or plate appearance:\n\nWhere BB is a walk, HBP is a hit-by-pitch, and SF is a sacrifice fly. From PA we can calculate the slightly more complex stat of onbase percentage.\n\nTo a large degree, this is what we would expect. Roger Hornsby, Ted Williams, Barry Bonds, these are all amongst the top players.\n\nLet\u2019s take a deeper look at the particular years around the book Moneyball. The season they played was in 2002, so we\u2019ll assume they were looking at pre-2001 data.\n\nOne unusual player for the Oakland A\u2019s to acquire was Scott Hatteberg, who went from catcher to first baseman as a result of injury. If we compare him to the 2001 league average, we can see that he\u2019s approximately standard deviation higher. That puts him in the top ~16% of players, astoundingly high especially since they signed him for $925k. This assumes he\u2019d keep a similar on-base-percentage throughout his career, which is a safe assumption and one that will likely get explored in later posts.\n\nSo what was the Oakland A\u2019s particular reason for valuing OBP so highly? It had to do with it\u2019s correlation to runs scored in a season. If we pull up all the data from the pre-2002 season, we can see this in action. We\u2019ll compute a team-wide using the same formula as applied to the players. It\u2019ll be plotted with a line regression to compare against as we can figure out the general correlation via the R\u00b2 value.\n\nAs can be correctly intuited, runs scored by a team contributes to wins by that team. It\u2019s not everything, defense is also important, but this will be covered in a later post.\n\nMore interestingly, it correlated slightly better than simple batting average.\n\nAnother flaw that exists in the batting average is it assumes all hits to be the same. We know intuitively this isn\u2019t so: doubles and singles are clearly worth more. We can use this information to compute a better stat called slugging. The formula is below:\n\nTo get 1B, we\u2019ll need to compute hits minus all types of other bases.\n\nAgain, the values are as we would expect. We can go by player\u2019s career and grab the highest career average slugging as well.\n\nOne weird outlier is Gary Sanchez, who has only played a few years with the New York Yankees. We can quickly pull up his stats to see how many games he\u2019s played.\n\nSo in 53 games played with 229 plate appearances, Gary Sanchez stands out. For curiosity sake, it might be interesting to look at how many standard deviations out he is from the average. This article from Fangraphs talks about his stats to a degree for 2017. The data here only goes to 2016.\n\nNote that this is a so-so method as the distribution follows a pretty large positive skew. Nonetheless, it\u2019s interesting.\n\nThis puts Gary Sanchez (evidently the second one in MLB history) at ~1.5 std. deviations out on the big metrics, which is pretty good. That\u2019s in the top 30th percentile for big league batters in 2016.\n\nOne last metric we\u2019ll talk about is one that Bill James came up with to describe players\u2019 hitting performance on based gained independant of batting average. This is a way of evaluating players who contribute to offense outside of batting average.\n\nThe last metric we\u2019ll go over is that of runs created. The idea is that an offensive player is only as valuable as the runs he contributes to a team\u2019s total. To prove this, we\u2019ll correlate two different variants against team\u2019s runs that year. Then we\u2019ll evaluate it for players and see what turns up.\n\nIf we have the runs created by a team, we can then compute the number of wins a player contributes.\n\nFirst we look at the total runs created by the team. Then, we compute the expected number of wins. The fraction of runs a player contributes becomes the fraction of wins they add.\n\nThe reason for using the predicted wins as opposed to actual wins is because we\u2019re using all theoretical numbers derived from equations. Once we start to cross theoretical numbers and real world numbers, errors can occur. I don\u2019t know those yet, so I\u2019ll hold off on that.\n\nGiven this is difficult to do, I\u2019ll just do an example for the Yankees in 1921 given Babe Ruth\u2019s extraordinarily high record. It\u2019ll be interesting as the Yankees at that time did not have the supporting cast they later would in 1927 with the famed Murderer\u2019s Row, which means they were reliant on Babe Ruth.\n\nSo there you have it! The Yankees, theoretically, would\u2019ve been 23 games fewer without Babe Ruth (assuming he was replaced with a complete non-run producing player). That puts the wins count that season to 75\u201379, which would\u2019ve tied them with the Red Sox for 4th place. They had placed first that season and won the World Series."
    },
    {
        "url": "https://medium.com/singular-distillation/evaluating-players-in-association-football-3f78ae860a77?source=---------1",
        "title": "Evaluating Players in Association Football \u2013 Singular Distillation \u2013",
        "text": "Note: You can check out the python notebook on which this medium article was based on Kaggle. Because it comes straight from that notebook with only some formatting changes, the footnotes and such are a bit off compared to my more usual posts.\n\nGoals are the main thing that matter in a soccer game (and yes, I live in the US so I\u2019ll be using the American term for association football), but there aren\u2019t enough to make statistical analysis. A top ranking team in the English Premier League like Manchester City only scored 80 goals in the entire 2016\u20132017 season. That comes out to ~2.11 goals a game. Given the size of a squad, evaluating players solely on their goal production seems an inefficient way to judge them.\n\nI was on Kaggle recently and spotted this in their public datasets which encouraged me to look at the contributions of a given player. It records events in a given game, including shots on goal.\n\nSo this is a two fold notebook: first is to look at how shots convert into goals (and goals into games).\n\nSecond is to look at what types of shots on goal resulted in goals what percentage of time. From that, I could evaluate players based on their ability to take those types of shot. My intuition was that players who take shots on goal that rarely convert into goals are to be valued less than players who take shots on goal that are likely to convert.\n\nThis is not to say this is a perfect analysis by any means. For instance, defensive players are not evaluated here (the data I have does not record defensive maneuvers so they are more difficult to evaluate). I also took a lot of assumptions in order to make this work.\n\nThus, don\u2019t take it as an end-all-be-all of soccer offensive player analysis, but instead take it as a starting place to look at player contributions in soccer teams.\n\nThis also might serve as an example of how to do proper exploratory data analysis given a set for those learning the ropes.\n\nFirst we need to import all of our data. I\u2019ll assumed you grabbed the data from Kaggle, you\u2019ll need an account to do so.\n\nI like Pandas for data manipulations done locally, so I\u2019ll be using that. They work similar to R dataframes.\n\nNext we need to pull out the shots from the events. All attempts on goal are recorded in the \u2018event_type\u2019 column as the integer 1.\n\nTo start, we\u2019ll first look at how many goals per game are needed on average by the winning team. We\u2019ll assume, and then verify* that this corresponds to an approximate gaussian.\n\n* (at least visually, I know this isn\u2019t as good as verifying for real but this is a fun analysis not an official one!)\n\nAs expected, the distribution skews. Soccer is largely a low-scoring game after all. We can say, with ~48.9% likelihood, that it takes 2.393 goals to win a game. Thus, players who contribute that many goals above the average player in a season are to be valued as 1 \u201cwin above replacement.\u201d This idea of evaluating above or below the average in the league was a method pioneered by Bill James and you can read more about it here.\n\nNext, we\u2019ll look at the average number of shots that conver into goals.\n\nWe could use this to say that each player\u2019s shot on goal contributes 0.1067 goals, but that\u2019s actually not very useful. It makes the big assumption that all shots are to be treated equally, whether long range from the middle of the field or close up for a penalty kick.\n\nLucky for us, our dataset differentiates the different types of goals available. For instance, we can pull up the location on the pitch from which the shot was made. By picking apart these different factors, we can start to get a better sense as to what each shot on goal is worth and evaluate the shots differently.\n\nSo we\u2019ll start by first removing all shots in which the location was not mentioned, replacing the integer values in the CSV with the dictionary provided (that I converted separately), then concatenating them together.\n\nNext, we\u2019ll get all the shots on goal for the unique combinations of words that come up. This unique combination will be it\u2019s own column in the datfarame and will be an amalgamation of the first columns mentioned above (this was technically done in the last slot). As a guess, we\u2019ll say that we want at least 100 occurrences of that particularly unique shot. * We\u2019ll then evaluate all the other low occurence shots as being 0 in goal contribution. **\n\n* This isn\u2019t very rigorous as a way to do this, but as a first approximation I\u2019d venture to say it\u2019s good enough.\n\n** This is a really poor first approximation. If I was to do this a second time around, I\u2019d take all other types of unique shots and evaluate them as an other category.\n\nLastly, we\u2019ll build out the conversion percentages that each unique combo scores on. We\u2019ll look at how many times that unique combo was a goal, how many times it was a shot on goal, and then divide the number of goals by the times it was a shot on goal. That number will be our rough estimate as to the contribution of that \u2018event\u2019 as a fraction of goal with some error. *\n\n* Not sure how to find that error at this moment. Anybody have any ideas?\n\nWe\u2019ll then display the dataframe and grab the most common values:\n\nNote: I am not the world\u2019s biggest soccer fan so I actually don\u2019t know what much of this means. Is it surprising that headers from corner kicks into the bottom right corner score 100% of the time in 214 shots? Keep in mind this set is a collection of 9,074 games across the division 1 European soccer league going back to ~2008. *\n\nSo what does this mean to a soccer player/manager/coach? It means that the types of events they should try and make happen are the ones that more-than-liekly convert into goals. This might be a matter of tactics, whereby the squad tries its damnest to create opportunities like this. It might also be a matter of training drills, whereby teams train on these sorts of conversion events to make it rote memory to attempt when playing.\n\nYou could break these down for opposing teams as well to see what types of goals they are likely to perform (or not perform). Players can be broken down this way too. **\n\n* Minus the Russian Premier League I believe. The information on the Kaggle page was a bit insufficient.\n\n** I actually want to look at outlier teams and players, but that will be for another post.\n\nLet\u2019s now look at players.\n\nPlayers, especially on the offense, contribute goals that contribute to wins. Nothing but goals mattering is a key assumption here and an inaccurate one, but a place to start.\n\nFirst, we\u2019ll manipulate our dataframes to get a dictionary of values. Each conversion percentage counts as a fraction of a goal. We\u2019ll use that to evaluate every shot made in our dataset as a fraction of a goal. From this, we can tally up player\u2019s contributions.\n\nNote again that I assume all contributions that there is insufficient data to judge for are ranked as a big fat Zero (0). This is not the best way to do this, but again as a starting point it\u2019ll be fine.\n\nYou can probably see that this will nix players who do not record a shot on goal. This will eliminate many defensive position players (though not all). As a first proxy for just getting offensive players, I think it\u2019s satisfactory.\n\nInteresting about this data \u2014 the vast majority of players contribute nothing.\n\nAlso of note: almost no players contribute more than 1.0 goals per game.\n\nThis indicates to me, a layman, that soccer is about developing and finding superstars. It confirms a suspicion I have that, although a team sport, it\u2019s a supporting team sport: the team exists to support one or a group of star players. It\u2019s similar to cyclcing and basketball (the latter I can\u2019t quite confirm through numbers, but feel it intuitively).\n\nWe can take this two ways now: either find the average (mean) player or the middle most (median) player. I personally think the median player will be more useful as it will eliminate outliers \u2014 the top players that score a lot will lopsidy the distribution. You can see that to some degree in the plot above as there are outliers that live above ~0.4. This will pull the average up higher than it should be.\n\nNonetheless, I\u2019ll pull all numbers and you can get a base of comparison as you like it.\n\nNow for fun, let\u2019s look at the top players by contribution!\n\nWe\u2019ll filter out people who have played less than 10 games as they likely have insufficient data to properly judge.\n\nAs you can see, the top players are in line with whom we\u2019d expect. Cristiano Ronaldo is considered by many to be the best ever so having him at the top makes a lot of sense.\n\nThe one outlier of Jorginho Frello having played only 11 games in this dataset makes sense as he doesn\u2019t have enough data to properly train. I actually find it surprising he\u2019s the only outlier here.\n\nAlso not surprising is that the majority of players listed are offensive position players. We wouldn\u2019t expect to find defensive players here nor goalie; I don\u2019t believe either is actually in the dataset.\n\nSo what do you make of this?\n\nI think sports is most fun when you pretend to play manager, at least for someone with a more intellectual bent. Evidently a lot of people agree. So one could use this data to evaluate offensive players with a little bit of working to eliminate my poor assumptions.\n\nYou could also use it to evaluate future rookies. If a particular rookie scores more often from some position that is unexpected, that might be a sign he should be played differently than another player.\n\nFormations could possibly be evaluated this way as well. Certain formations will favor certain goal scoring opportunities, so you could play your formation differently for your team or against another team who favors some particular scoring opportunity. Alternatively, your players might be outliers who score better under some circumstances vs others. I\u2019m not sure if there is necessarily enough data in this set to do that, but I\u2019m sure there is enough data somewhere to do that."
    },
    {
        "url": "https://medium.com/singular-distillation/chatbots-are-an-interface-not-your-best-friend-b4c20f490fcf?source=---------2",
        "title": "Chatbots are an Interface, Not Your Best Friend \u2013 Singular Distillation \u2013",
        "text": "Chatbots seem to be the rage these days. Many entrepreneurs are pursuing them in an assumption that they are part of the \u201cnext big wave\u201d of technology companies. First there was the PC, then the smartphone, now the chatbot. But the general approach has disappointed people greatly, with many just being lame to use. It becomes obvious after a few words that I\u2019m not talking to a human being. So unless the chatbot is doing some procedural in processing, it\u2019s doubtful that the interface is useful.\n\nExcept, that\u2019s just the way people are currently building them.\n\nSee, chatbots are terrible as conversational UIs. I dislike talking to them whenever I have to and always prefer a human being. They are generally so terrible that many startups like Fin. and Facebook\u2019s messenger use humans behind the scenes to try and cover up the uncanny valley effect.\n\nBut this is under the assumption that chatbots can be used like regular human beings. There is a saying at Y Combinator that a startup should require only one miracle to work. A company like Airbnb had to prove that people would use the product, not that the underlying technology would work. [1] Chatbots companies rely on achieving this miracle and then other miracles as well.\n\nWhat would be far more useful is to think of chatbots as natural language parsers, not as virtual assistants. The Google search engine is the closest current example I know of as you can ask questions that are naturally parsed language . Thus I can type in \u201caction movie where a german terrorist tries to destroy a building from the 1980s\u201d and get Die Hard as a result. Ok, it\u2019s not the first result, but it\u2019s still really impressive.\n\nIf Google can do it, the technology is therefore proven. That\u2019s not to say it wouldn\u2019t be hard, but it is to say that it\u2019s relatively solved already.\n\nSo what I think startups and technology companies should focus on is not conversational AI, which is much too hard, but instead focus on natural language parsing of commands. That is not only a doable miracle, it\u2019s also one that is extremely useful to solve and thus becomes one startups can possibly seek to do.\n\n[1]: Which is not to say it\u2019s not hard, just that Airbnb\u2019s technology is not what it\u2019s selling. It\u2019s simply part of the machinery required to sell that product. Think of the meat patty machines at McDonald\u2019s: nobody would deny that they are a technical achievement, but McDonald\u2019s uses them to sell a benefit to the consumer."
    },
    {
        "url": "https://medium.com/singular-distillation/some-thoughts-on-modularization-in-machine-learning-fc49f222a647?source=---------3",
        "title": "Some Thoughts on Modularization in Machine Learning",
        "text": "Mindless coding is programming while thoughtful coding is software engineering. The latter involves a certain philosophy of design and approach, with specific and clear guidelines on how to best approach a given problem. However, when it comes to machine learning this idea seems to have fallen out of favor and needs to come back.\n\nI\u2019ve talked somewhat about this in the past. Machine learning is, at this stage, more engineering and less science. Coming up with algorithms is the domain of academia. Companies and startups instead focus on taking these known algorithms and figuring out the best implementation to work in the distributed setting of the firm.\n\nSo what\u2019s the best way to do this?\n\nModularize the machine learning aspect and silo it off from the rest of the application.\n\nMachine learning is an enabler of benefits and not the benefit itself. Ideally, features in an application will, under the covers, call some API to do the machine learning work. The application itself will worry about delivering that result.\n\nWhy silo this way?\n\nThere are a few reasons I can think of that encourage silo\u2019ing. The first is that the data scientist in question is unlikely to know much about UI/UX design. The cognitive biases one needs to do well in that domain don\u2019t have much in common with front facing work. That\u2019s not to say the cross over can\u2019t happen at all, just that it\u2019s rare. Frontend work involves knowing people, having a bit of a visual eye, and knowing cross-platform development. In contrast, data science requires one to be strong in statistics, probability, and distributed computing.\n\nThe second reason is that machine learning is still so complicated that it needs to be worried about on its own. Trying to bring a useful algorithm to life, complete with relevant data and proper scaling, it really hard. Trying to do that as a component within one monolithic application is only going to massively complicate the issue. This is why micro services design works best for machine learning.\n\nThird, the metrics that guide development of both parts may clash. Most companies have KPIs or key performance indicators that guide product development in seeing if their particular product is working or not. There\u2019s a great book this if you\u2019re curious for examples, but the just of the idea is that one metric matters and nothing else does. A social media company might obsess over daily user engagement as that\u2019s a metric that sells ad space, just to cite one example.\n\nIn machine learning, some function of accuracy or speed is desired. Which matters more depends on context. In contrast, user interface design will want to look at user engagement more than anything. This may be a function of the other two, but not always. If you don\u2019t define the most important metric to use, then you won\u2019t optimize for anything. People are too simple to juggle multiple metrics in their head at once and things only get more complicated once you expand beyond one person. By setting down one goal for both teams to spend their efforts optimizing for, then you can make significant progress.\n\nAnyways, those are my thoughts on design in machine learning. The benefit is powered by machine learning but is never the algorithm itself, no customer will ever care. By building silos, you can start to separate and leverage your different team member\u2019s skill sets with data scientists focusing on the numbers and application developers on the experience itself. You can also focus on the specific complexities each face. Lastly, metrics that guide the development of each can be put down without conflict. All are desired when dealing with such a fast moving field."
    },
    {
        "url": "https://medium.com/singular-distillation/standing-on-the-shoulders-of-giants-machine-learning-the-equally-important-progress-of-unrelated-fe82b9d1547d?source=---------4",
        "title": "Standing on the Shoulders of Giants: Machine Learning & the Equally Important Progress of Unrelated\u2026",
        "text": "Often when talking about machine learning, we obsess over the algorithms used. The funny thing is that these algorithms are fairly old. Take neural networks for instance: the basic concept of a perceptron was discovered back in the 1950s. So what took so long for machine learning to go mainstream?\n\nThe former was a breakthrough in how applications could be stored. Machine learning is a computationally intensive process and therefore needs big iron to properly run. Small personal computers could not run these algorithms and, arguably, your cell phone really can\u2019t either. Instead, it\u2019s best to offload that work to a server over the internet. Your personal computer doesn\u2019t run face matching algorithms to determine faces in photos, instead Facebook has its servers do that intensive work.\n\nMicro services design is an equally monumental achievement. At the risk of oversimplifying, software development has fundamentally changed with this transition. Prior, a software development team thought of an application as one large moving part. With microservices, one can think of an application as many parts that move together.\n\nThis seemingly small difference is extraordinarily important. I\u2019ve touched on machine learning being an enabler of benefits (think: features) rather than a core feature itself and this is equally a part of that. Micro services architectures enables teams to think of features as encapsulated mini apps that communicate with other services in the grand application. By doing so, the machine learning enabled services can be put in as their own application.\n\nOf course, this isn\u2019t entirely right. You can approach monolithic development while keeping mind that the machine learning is not the core of the product. But keep in mind that many machine learning companies sell smart APIs. One can now take that API and plug it into the context of the larger application. You can think of this as a benefit enabler, allowing for a \u201csmart\u201d feature to quickly utilize the data. Without micro services, you\u2019d be stuck trying to cram an often complicated algorithm into your whole application. Here, you can not only leverage someone else\u2019s work (to save you effort) but also silo off that often complicated part of the application.\n\nBoth of these improvements, by themselves, see a lot less sexy than the machine learning aspect. It\u2019s much cooler to be the data scientist than the guy maintaining the Hadoop stack these days. But machine learning is a team sport and is the beneficiary of many parallel processes at once, even ones that, at first, didn\u2019t look all that important. So never forget the other technologies at play instead of just the algorithm de jour."
    },
    {
        "url": "https://medium.com/singular-distillation/the-profound-possibilities-of-knowledge-compression-9ad8798ed7af?source=---------5",
        "title": "The Profound Possibilities of Knowledge Compression",
        "text": "Of the many ways to think about machine learning intuitively, I think its use as a way of compressing information is one of its most useful. We often get so lost in the possible world-changing abilities science fiction promises that we lose sight of this simple fact.\n\nThe idea is that machine learning is compressor of knowledge. To take a literal example, we could, potentially, build a table of all possible inputs and all possible outputs and simply lookup that information. Reinforcement learning has actually used this as a premise with simpler toy examples. But as the number of all possible inputs and outputs grows, a lookup table becomes unreasonable to use. So we build functions that compress this knowledge to enable better compute efficiency and smaller models. We can actually see this play out in real examples.\n\nTake one of the most basic models with decision trees. These work like questionnaires, with branching values based on the answers given. A model tallies up the answers and returns a value based on this. They end up being a lot like piecewise functions, a topic American school children learn in the 7th grade.\n\nWe can then look at more complex functions like support vector machines, also called SVMs. Yann LeCunn has childed these as \u201cglorified line fitting\u201d, and in a way he\u2019s right. [1] SVMs work by finding the line that best separates two known shapes. Usually this plays out in high dimensions with the features forming the nth-dimension. New data has its features fall on either side of this line and its then categorized as belonging to that particular group.\n\nMore complex functions still work this way, with ensemble fitting like random forests fitting many different functions and then picking results where the majority agrees. These simpler functions could be decision trees, whereby ten decisions trees are trained and if six of them say new data falls under some label, then the new data is predicted as belonging to that label.\n\nBut then we get the mack daddy of all machine learning algorithms: the neural network.\n\nYou see, they have the truly miraculous ability to recreate any function given enough neurons and data. This is called the Universal Approximation Theorem and its one of the few things we know for sure about how they work.\n\nThis known and provable fact about neural networks forms the backbone of another powerful concept called model compression. The idea behind this is that very complex ensembles of functions, think some 500 different ones, can be modeled by a much smaller and simpler neural network. Having large ensembles can be very accurate but also very slow to run, as the 500 models need to be individually computed. They can also be bulky as each model needs to be stored in memory or disk. By speeding this up with a neural network trained specifically to replicate this model, we can have huge performance increases. [2]\n\nYou can see this idea play out all over the place. I recently wrote up about using neural networks to replicate LIGO wave detection as just one example: the many many templates fitted to a particular wave is slow to compute but can be sped up if we train a network to mimic it.\n\nThis simple idea is so powerful that I\u2019m honestly surprised its not used more often.\n\nThe process is simple: take a lot of inputs, pass them through a known function, and get a lot of outputs. You use the inputs as features and the outputs as target values. You then fit a neural network to this and purposely overtrain it. It will start to replicate the function, plus or minus some noise. This noise is usually so infinitesimally small that it\u2019s not worth worrying about.\n\nWhen one can start to more quickly do functions, a Moore\u2019s Law like runaway effect occurs. The bottleneck moves from the computation of the function to somewhere else on the assembly line of knowledge. This allows for effort to be better spent elsewhere, overall improving efficiency, productivity, etc. for everybody involved.\n\nIn practice, there are some details we don\u2019t know about. We don\u2019t know why some activation functions perform better than others. It\u2019s also difficult to know why multiple layers can help sometimes, though I have seen some people theorize on this. The specifics of network architecture in general is now known as well. But the idea is still a powerful addition to the mental heuristics of thinking about machine learning.\n\nIt\u2019s a tired but honest clich\u00e9 that the hype is overrated, but there\u2019s often a kernel of truth buried at the center. In machine learning, it\u2019s unfortunate that we\u2019ll never have super human assistants or perfect fits-like-a-glove home automation. At least I don\u2019t think so. But we can use some of the core properties, like the ability to mimic any function, to make a dent in the universe and move the human race forward.\n\n[1]: I can\u2019t find the video anymore, but if anybody can point it in my direction, I\u2019d be very grateful to add it!\n\n[2]: It\u2019s also distinctly different than training a neural network on the data itself. This is a result of the overtraining tendency of neural networks, which is complicated enough to get its own post."
    },
    {
        "url": "https://medium.com/singular-distillation/neural-networks-the-grand-adventure-to-model-humanity-a0bae9f35e0c?source=---------6",
        "title": "Neural Networks & the Grand Adventure to Model Humanity",
        "text": "Warning: this might be the spaciest article I\u2019ve ever written.\n\nMachine learning, at its core, learns a function given inputs and outputs. Neural networks specifically learn a function of infinite complexity. This basic premise is sometimes lost in people\u2019s dreams of an AI they can talk to. It may not be quite as magical, but it still offers something profound.\n\nI think a fun topic for engineering types to consider is how society could be perfectly planned if everybody was rational. Unfortunately, we are not rational during the vast majority of time. But this hasn\u2019t stopped people from conjecturing how to fit mathematical equations to market analysis, with the former USSR coming up with cybernetics. [1] At some level it\u2019s how financial institutions work, by allocating capital to firms that are undervalued by the market on a whole. [2]\n\nBut such methods failed because humans are infinitely complex. We really don\u2019t know how complicated we get and neurology\u2019s near constant progress of mind-bending discoveries is evidence of this. You can\u2019t fit simple equations to human being\u2019s erratic behavior. Cybernetics attempts to do this were ill-founded from the start.\n\nThe way financial firms and other organizations deal with this limitation is to attach uncertainty to the equation. There is some unknown we don\u2019t know and will never know and the equations reflect this. Probability was developed to deal with this unknowability. Statistics was developed to allow us to mathematically, and with some precision, describe large groups. Taken together, both can be utilized to make objective measures on an irrational market.\n\nCybernetics, to my knowledge, never got to that point. Instead it failed as no equation or mathematical function could be found to approximate all the minute intricacies of mankind. [3] Another limitation was compute power. We\u2019ve definitely solved the latter and mathematically it might not matter if we never know the exact mathematical function so long as we can bear a black box that gives accurate answers. Neural networks are perfect for precisely that task because they can approximate any function given sufficient inputs and outputs.\n\nThis may seem to contradict a previous piece I wrote in which I argued that general AI is impossible because the problem space is simply much too large, but in this case we can break down the system we\u2019re describing into multiple functions (i.e. neural networks). So long as they agree on a common output (such as price or some other quantity for demand), then they can be used with some compatibility. [4]\n\nAnyways, that\u2019s my thoughts on this. I\u2019ve always been curious about the Utopian ideas of designing the perfect society, whether that be Disney\u2019s Epcot or the crazy attempts by failed socialist states to predict and control their populaces. Machine learning, in many ways, can seem like a first step. This essay is an attempt to reconcile the connections.\n\nIf you like this essay, please leave an applause or a comment! I\u2019m always interested to read what you have to say.\n\n[1]: Also worth reading about is Project Cybersyn in socialist Chile.\n\n[2]: Yes, yes, I\u2019m oversimplifying, but resource allocation does happen in capitalist and capitalist-esque economies. I think this often gets overshadowed in discussions, as if there is no negative feedback to consumption and nobody is controlling how goods get distributed.\n\n[3]: Note that the USSR scientist who came up with cybernetics was actually ostracized to a degree by his own party because they thought it threatened party control was another one. For those curious.\n\n[4]: Just came up with this thought: General AI might be possible if we can build ways for multiple \u201cgenius machine learning models\u201d to communicate through some common signal. Perhaps that\u2019s the great leap forward in Hinton\u2019s CapsNets?"
    },
    {
        "url": "https://medium.com/singular-distillation/little-explanations-information-bottleneck-theory-its-possible-link-to-neural-networks-iii-67a54e918dae?source=---------7",
        "title": "Little Explanations: Information Bottleneck Theory & It\u2019s (Possible) Link to Neural Networks (iii)",
        "text": "The authors of this rebuttal paper (1) also wanted to look into seeing what deeper networks might do. As was talked about in the prior post, the non-linearity of the rectified linear units means that the upside is not bounded. One of the key requirements for Information Bottleneck theory to be applicable to neural networks is the expectation of a compression and fitting phase; if the responses of the neural network are not bounded, then only a fitting phase occurs and no compression. The simple network used thus far, with only one neuron in a hidden layer, is not sufficient to capture all the complexities that may be taking place. Thus, to search for other mechanisms, authors used a tractable and well-studied model: deep linear neural networks.\n\nThese types of networks are prone to overfitting, but they can be stopped early to allow for generalization. Deep linear networks are basically networks without activation functions. The particular one used in this paper had many hidden layers with about 100 neurons per layer. [1] The training mechanism was stochastic gradient descent on the layers. The advantage of having no activation function is that the information plane can be calculated directly. This is because the hidden layer is simply T = WX where X is the input vector for that particular layer, W is the weights, and T is the encoded values.\n\nThe figures above show the mean square error (i.e. accuracy) and the information plane for the deep linear network, respectively. The network was stopped to prevent overtraining and allow for good generalization on the task. The resulting information plane shows no compression.\n\nAlso interesting, if we change the activation function to tanh with modest overfitting, the compression phase occurs despite continued errors. You can see that in the figure below. This identical trajectory in the information plane seems to indicate that the tanh activation function is responsible, not any general properties of neural networks falling under information bottleneck theory.\n\nNext, the authors decided to try using batch gradient descent (BGD) in addition to stochastic gradient descent (SGD). One of the claims of Information Bottleneck theory is that SGD has some randomness with the weight updates. During training, the weights will update with some randomness, behaving like diffusion in weight space. To test this claim, BGD is used. Because BGD involves using the full training dataset, there is no randomness or diffusion-like behavior in its updates. Again, this was tested on both a linear network and network using tanh activation functions. Will there be compression followed by fitting again?\n\nThe answer is no. The figures above show that the tanh network, using either SGD or BGD, demonstrates the compression and fitting phases while the following figure only shows a fitting phase. Again, evidence seems to point to the network information plane exhibiting a fitting phase followed by a compression phase only when there\u2019s a tanh activation function.\n\nAlthough an interesting idea, Information Bottleneck does not seem to explain non tanh networks. The compression in the information plane seems to be a cause of non-linearity and binning (see part ii for more on that). This is proven by looking at different activation functions (linear and rectified linear units) as well as trying deeper networks. Along the way, many of the core arguments behind this idea being applicable are dismantled.\n\nIf you liked this post, please given an applause! It was my hardest one to write yet.\n\nAlso, if you\u2019re curious about the ongoing peer review conversation, then check out the sources below. All figures come from there as well. (1)\n\n[1]: The specific number of layers is not mentioned."
    },
    {
        "url": "https://medium.com/singular-distillation/little-explanations-information-bottleneck-theory-its-possible-link-to-neural-networks-part-8d4322724459?source=---------8",
        "title": "Little Explanations: Information Bottleneck Theory & It\u2019s (Possible) Link to Neural Networks (ii)",
        "text": "This is the second of a multi-part post. You can read the first part here. I suggest reading it before looking here.\n\nI need to go over an important aspect of neural networks: activation functions. When information, in the form of an input vector X, flows from one layer to another layer, it gets transformed by an activation function. There are quite a few of these, I like using Keras as a library and reference for common ones, so you can go there if you are so inclined. How they operate is hugely important to this post and the associated paper, but I\u2019ll be holding your hand through it. Just know that they do exist and how they generally work in neural network architecture.\n\nOne of the key points that Information Bottleneck Theorists make is that mutual information, while being encoded from one layer to another, undergo a phase of fitting, where mutual information between the input layer and the encoded layer increases, and then a phase of compression, where the mutual information decreases.\n\nThat\u2019s a mouthful. So let\u2019s look at a plot from the paper.\n\nThis plot shows that there is an arc, think reverse C, in play here. The mutual information first increases, moving towards the upper right hand corner, which is maximal information, and then decreases back to minimal information. These arcs are the mutual information between the encoded hidden state and the input and the encoded hidden state of the output. The increasing part of the arc is the \u201cfitting\u201d and the decreasing part of the arc is the \u201ccompressing.\u201d\n\nOne of the most important aspects of Tishby et al.\u2019s paper is that he used tanh activation functions for his neural network. That ends up being key for a few reasons.\n\nFor one, it saturates into bins. So, over time, the output value of that neuron will be either 1 or 0. That makes is bounded to binary information and thus behave like bits. [1] Why does this happen? During training, weights tend towards either 0 or infinity. When at 0, the values will be at -1, because they multiply to 0 and tanh is bounded to be -1 at 0. When at infinity, the tanh function prevents them from being any greater than 1. Thus they saturate to these two values.\n\nThat ends up being pretty important for mutual information calculations. Such equations, central to Information Bottleneck theory, use binning to discretize values. Even the naked eye can bin a tanh function to 1 or -1. Therefore, mutual information is simply calculating the probability that some input lands in a particular bin. The binning is central as without it, there would be no loss of information (just a transformation basically) between the encoded representation and the input vector.\n\nWhat happens if we use other activation functions?\n\nWell, it seems to change how the mutual information function works. The authors in particular use rectified linear units, or ReLU. This activation function is bounded to -1 as the weights go towards 0. But its not bounded as the weights go to infinity. There, the output value becomes Gaussian distributed. As I stated prior with respect to binning, this means there is never a loss of information. Thus, compression never takes place. We can see that when we plot the mutual information plots (again, from the paper).\n\nAs one can see, there is no compression (i.e. arcing back) in these mutual information curves. [2,3] This appears to contradict Tishby\u2019s big claim.\n\nIt\u2019s an interesting paper for sure, and this only touches on the first few parts of it. I suggest reading the whole thing or subscribing to \u201cSingular Distillations\u201d so you can be notified of my follow-up posts on this paper.\n\n[1]: Interesting enough, Tishby actually calls them bits in his presentation. Freudian slip?\n\n[2]: Though its interesting that the first curve kind of does this. The authors do not talk about this and this paper is not fully published yet, its still under review.\n\n[3]: Its cut off, but the y axis is the mutual information of T given Y \u2192 I(T;Y)\n\n(1) https://openreview.net/pdf?id=ry_WPG-A- \u2014 The response paper, currently in peer review"
    },
    {
        "url": "https://medium.com/singular-distillation/little-explanations-information-bottleneck-theory-its-possible-link-to-neural-networks-1d4df1badf72?source=---------9",
        "title": "Little Explanations: Information Bottleneck Theory & It\u2019s (Possible) Link to Neural Networks (i)",
        "text": "This is part 1 of a multi-part post. The second part is here.\n\nNeural networks are an extremely powerful tool, but also a difficult one to explain. There is no agreed upon analysis on how they work, leading to the term \u201cblack box\u201d: we know what goes in and what comes out but don\u2019t know what goes on inside the network itself. This is why network architecture is so difficult as there are no ways to mathematically describe exactly what network should be built due to this lack of understanding.\n\nThis black box phenomenon is a must solve problem if neural networks are to be deployed in places like medicine, where being able to explain decision making is legally and ethically required. A group of researchers principally located the University of Jerusalem have decided to engage in this problem by applying Information Bottleneck Theory. They claim that this framework, formulated by Tishby et al. in the late 1990s, explains how neural networks \u201cthink\u201d perfectly. (1,2) However a new paper disputes this claim. (3)\n\nThis post will explore both the Information Bottleneck Theory, shortened to IB here, and the rebuttal paper that is currently under review. Both are cited in the Sources at the end of this post if you want to read the sources. I will gloss over some of the more mathematical parts in an attempt to give a layman\u2019s explanation, so I suggest reading the primary sources if you want to learn more.\n\nIB Theory proposes that, given a feature set X, we want to \u201csqueeze\u201d out Y. We do this by finding the most relevant information from X. That is, what parts of our X vector best describe Y? We do this by looking at encoding X into a representation T and from that finding Y. We want to find X that is the most maximally expressive form of Y.\n\nOf course, the simplest way to describe X is to just make a straight forward vector T where every part of the vector is equivalent. However, that\u2019s a trivial solution. Instead, we want X to not only be maximally expressive of Y but also as compactful as possible. To balance the two, we use a beta value. We\u2019ll call this ideal, encoded representation Z.\n\nSo let\u2019s define a function around this. We\u2019ll call this function I. Our I is defined as encoding some input vector into an output vector. Along with this, it takes some parameter called theta for its underlying function (e.g. the coefficients in a polynomial). We then define a function R that takes some parameters theta and is the function I such that we maximal representation of Y is preserved subject to the constraint of compacting X, with the beta value being used to control that compacting. The figure below describes this in mathematical notation.\n\nTishby et al.\u2019s claim is that neural networks can be thought of as a series of successive, multi-dimensional (think vector) variables passing through functions that encode and decode each variable. Stochastic Gradient Descent (SGD) learns the parameters (network weights) of this function. The network, in effect, \u201csqueezes\u201d out the relevant information to get Y from X.\n\nHe bases this on a few bedrock ideas. One is that we can think of neural networks as markov chains. [1] Two is his concept of the Information plane. This principle (for lack of a better name) says that, given a large enough X (by number of samples), the sample complexity of a deep neural network (i.e. multi layer) is completely determined by the encoder mutual information of the last hidden layer and the accuracy determined by the decoder.\n\nWhat does that mean? Let me break it down.\n\nThe encoder in a neural network is everything up until the last layer. That part of the network, so Tishby et al.\u2019s logic goes, forms the encoding of the information some value T. The last layer then decodes that information to our final value of Y. This intuitively makes sense to me.\n\nWhat makes his analysis more interesting is the idea of mutual information. This is a measure of finding how much information one random variable contains about the other. Briefly stated, if two variables are the same everywhere, then their mutual information is always 1 and if they\u2019re different everywhere, then its 0. Tishby seems to favor cross entropy, which uses the KL distribution deviation, for this measure. (2)\n\nThis is a very bold claim and his plots and videos seem to demonstrate this. (2) [2] If true, it would be an effective way to explain neural networks. But is it really? Stay tuned tomorrow and find out\u2026\n\n[1]: Is this controversial to say? It makes sense to me though I\u2019ve never seen it explained this way. Each value finds itself being either active or not active, effectively, by the weights of the particular function or neuron in the signal path. Because the signal is stochastic, its therefore sensible.\n\n[2]: I actually can\u2019t make sense of what he\u2019s plotting here (2) or in his paper (1). I believe it\u2019s the hidden representation T and its final representation Y, but I can\u2019t be completely sure.\n\n(1) https://arxiv.org/abs/1503.02406 \u2014 \u201cDeep Learning and the Information Bottleneck Principle\u201d by Tishby et. Al.\n\n(3) https://openreview.net/forum?id=ry_WPG-A-\u00aceId=ry_WPG-A- \u2014 \u201cOn the Information Bottleneck Theory of Deep Learning\u201d, rebuttal to Tishby et al."
    },
    {
        "url": "https://medium.com/singular-distillation/finding-functions-a-primer-on-some-implied-ml-theory-adddb722631a",
        "title": "Finding Functions: A primer on (some) implied ML theory",
        "text": "When I first started in machine learning ~4 years ago, there was some notation that always tripped me. I think it was implied and so never got explained in papers or textbooks, at least not in any way I noticed. I\u2019ve written this post in an attempt to explain it in a way that I think makes intuitive sense. It\u2019s about how machine learning models learning functions and how these functions find Y_hat, as opposed to Y, and how this translates into mathematical notation.\n\nMachine learning supposes that you, the researcher/scientist/engineer, have some features and target values, the latter of which you are trying to predict. These features get called X and can be expressed numerically. If these features are categorical or linguistic, you\u2019d have to convert them into numerical values using one hot encoders or other forms of feature engineering. X must be numerical before being used as input into the model.\n\nThe model is trying to learn a function F(x), such that F(X) = Y_hat where Y ~ Y_hat. This function is theoretically supposed to exist such that it can use the numerical values of X and find Y. You can actually generate synthetic data using a real function and get F(X) to be perfect if we have a specific function, like y = x\u00b2. [1] But in the real world, this function is not as concrete as y = x\u00b2. Because of this, we can\u2019t reasonably expect to find Y. Instead we try to find Y_hat, which is close to Y. Put another way, Y_hat is our prediction and Y is the real target value.\n\nLet me re-iterate that F(x) does exist\u2026but it also doesn\u2019t exist. Rather, we assume it exists and assume that our algorithm can find it, but implicitly state that we ourselves can\u2019t find it.\n\nThis, in part, explains why neural networks are so powerful. They innately have the ability to recreate any function given enough neurons. [2] So, if the function exists, then a neural network should find it, provided its sufficiently large to capture the full complexity of the function. [3]\n\nBecause we\u2019re trying to learn this function, we say that the model is \u201clearning\u201d and we call the period in which it is learning \u201ctraining.\u201d\n\nI cannot tell you how long this simple math eluded me. So maybe those post will be useful to those who also struggled with the concept.\n\nP.S. Does anybody know how to insert mathematical symbols into Medium? Would be greatly appreciated to make this more readable!\n\n[1]: This might be a future post, especially with neural networks.\n\n[2]: Ok ok, it\u2019s one part of this and it\u2019s a strong opinion on my part. However, I rarely see this mentioned and think it can\u2019t be understated enough!\n\n[3]: More specific questions like \u201cwhy do convolutional neural networks work so well versus multilayer perceptrons?\u201d are currently unknown to my knowledge. We just know that there are problems in which they do."
    },
    {
        "url": "https://medium.com/singular-distillation/winning-at-the-lottery-with-bayesian-statistics-2d2f15a25543",
        "title": "Winning at the Lottery with Bayesian Statistics \u2013 Singular Distillation \u2013",
        "text": "Bayesian statistics has my vote for least intuitive mathematical idea ever. Given enough problems to study, most people get calculus. Given enough time, they\u2019ll get (frequentist) statistics, even if it makes their head hurt. But some people will struggle to get Bayesian stats. The ideas are difficult to understand because they run counter to not only what we assume but also what we\u2019re taught in introduction to probability and statistics courses. Yet they are a lot more powerful because they don\u2019t assume independent outcomes on events.\n\nTo illustrate, let me give a simple example. Suppose you\u2019re talking to a couple and they tell you they have two children. One of them is a girl. What\u2019s the probability of the gender of the other child?\n\nMost would say its 50/50 boy or girl. That would be wrong. It\u2019s actually 1/3 girl and 2/3 boy. How could that be if the probability of having a kid is 50/50? While that\u2019s true, in this case the other child\u2019s gender is not an independent outcome. What we\u2019re really predicting is the chance of having two children of a particular combination and we already know the gender of one kid. We therefore need to take into consideration all of the possible outcomes.\n\nIt\u2019s 50% * 50% = 25% chance of having a girl-girl two-child combination, a 25% of having a girl-boy, a 25% chance of having a boy-girl, and a 25% chance of having a boy-boy. We\u2019re not given the relative ages, so we can combine the percentages of having split gender children to be 25% + 25% = 50%. We know the combination can\u2019t be boy-boy because one of the kids is a girl, so that choice goes away. That math then becomes (chance of being a girl-girl) / (all possible combinations) = 25% / (50% + 25%) = 1/3 of being a girl.\n\nThis is a remarkably simple example of how Bayesian stats works. So, let\u2019s now apply it to a more real-world example of playing the lottery.\n\nLotteries appear to be played differently depending on the state and game (e.g. NJ pick 3 is different than the NJ pick 5), and the rules for random probabilities can get complex depending on how numbers are drawn, but I\u2019ll be using my limited understanding of the NJ Pick 5 as a base example. In this variant of the lottery, winning numbers are drawn randomly and daily. You pick 5 double-digit (01\u201399) numbers. If the numbers match, you win! If they don\u2019t, you lose and buy another ticket. [1]\n\nTo determine the base value of winning, you can take the frequentist route with combinatorics: 99 numbers across 5 positions gives a 1 in 99\u2075 chance of winning. That comes to 1 over ~9.5 billion.\n\nBut we can get a little better than that. We do have control over what numbers _we_ pick in this game. The frequentist viewpoint says the numbers don\u2019t matter, but in fact they do; we don\u2019t want to pick numbers that have already won. We have a 1 over ~9.5 billion chance of winning with any given set of numbers and have a (1/9.5 billion) * (1/9.5 billion) chance of winning with the same number twice. Alternatively, this can be thought of as the same number combination being drawn twice. [2]\n\nWe can do even better. We can also look at the places that have won the lottery. That chances of any convenience store, and there are about 2700 in the state of New Jersey [3], winning the lottery are very slim. The chances of any convenience store winning the lottery twice is even slimmer. [4] Regardless, the strategy would be to pick a location that had never won the lottery before.\n\nSo there you have it. Probabilities are rarely independent and should never be interpreted as such. This is the fallacy of frequentist statistics. I also realize the controversy of this statement, but I think the work of places like 538 and the more predictive powers of people like Andrew Gelman prove this. Bayesian statistics are more powerful because they take into consideration all of the possible outcomes and not just the narrow one you\u2019re looking at. I hope this post helps you in understanding this with a fun example of trying to win the lottery!\n\nIf you\u2019re more curious about the two schools of thought in statistics, namely Bayesian vs. Frequentist, the Wikipedia pages help a lot. They can be found here and here. Feel free to drop a question or comment as well.\n\nAlso, if anybody has any numbers with credible sources, I\u2019ll update the post with the actual math behind it.\n\n[1] I have never actually played the lottery and apparently the Pick 5 uses randomly drawn playing cards to pick the winning numbers. I\u2019ll just assume they\u2019re perfectly random for the sake of explanation. I can\u2019t actually run calculations with real numbers anyway, see [2] for why. There\u2019s also rules about when and if the ordering matters and you can win partial jackpots if you get the numbers right but in a different order or you get some of the winning numbers, and so forth.\n\n[2] I was unable to find the total number of unique winning numbers so I\u2019m not able to do this math sadly. :/\n\n[3] The source is this \u2192 http://www.nj.com/news/index.ssf/2010/12/shell_wawa_moves_into_morris_c.html\n\n[4] If I had the average number of tickets sold, then you could calculate the odds here."
    },
    {
        "url": "https://medium.com/singular-distillation/little-explanations-finding-gravitational-waves-with-neural-networks-2393f93ccadc",
        "title": "Little Explanations: Finding Gravitational Waves with Neural Networks",
        "text": "As a note, I do not claim to be a physicist. All of my understanding here comes from the principal paper and one of its sources. I wrote this to better understand how they used deep learning and share my findings on Medium. Therefore, my analysis on the Physics aspects is likely lacking in comparison the neural network analysis where I\u2019m more knowledgeable.\n\nThis is the reader\u2019s digest version of a recent paper on neural networks and gravitational waves titled \u201cDeep Learning for Real-time Gravitational Wave Detection and Parameter Estimation: Results with Advanced LIGO Data\u201d. It\u2019s a recent paper put out by the University of Illinois, Urbana. You can find it here on ArXiv.\n\nThe goal of the paper is to build a more efficient method, by measure of computational efficiency and size, of finding gravitational waves from signal data. This would gravitational waves to be detected in more situations. I think it\u2019s a particularly interesting example of building neural networks to compress knowledge, one of the better uses of neural networks in my opinion.\n\nGravitational waves [1] are currently detected from LIGO signals, short for Laser Interferometer Gravitational-wave Observatory. Their detection and the methods used won the Nobel prize in Physics in 2017, making them cutting-edge. Such data is extremely noisy, as can be seen in the figure below. Finding a gravitational wave signal from this proves to be quite computationally difficult.\n\nThe current method consists of matching templates: a computer runs through many templates (in the thousands range seems typical) and looks to see which, if any, match to the current signal. When it finds a match, it can ascertain the value based on its correlation to the template. The paper does not discuss where these templates come from or how they are derived. [2] This technique of template matching is called Matched-filtering.\n\nThis method is somewhat lacking though. For one, it\u2019s computationally heavy. This becomes an issue as current computational efficiency only enables gravitational waves to be identified from a handful of the total situations where they occur. Gravitational waves from binary black hole mergers and the merger of two neutron stars seem to be the only ones that are detectable. In contrast, they may miss gravitational waves generated by compact binary populations formed in stellar environments.\n\nThe authors of this paper built a neural network to distill this information into a deep learning convolutional neural network. They created synthetic training data [3] by injecting signals into sample noise from three events. Two of these events were used for training and validation and the last one was used for testing. They were able to gauge the power of noise and the robustness of the model by playing with the signal-to-noise (SNR) ratio. Labels were created by running these against matched filtering. 2500 templates were acquired from an open source \u201ceffective-one-body\u201d code for the matched filtering.\n\nTwo networks were used for this. One was to classify whether the signal was a gravitational wave at all and the other was to predict the corresponding value. The classifier would run on every step and if it predicted with high probability that a gravitational wave existed, the other network would find the \u201cparameters of the source.\u201d [4] To make the networks more robust, they randomly offset the underlying gravitational wave peaks by 0.2s during training.\n\nThe deep learning network used is a convolutional neural network of four main layers and two fully-connected (i.e. \u201cstandard\u201d MLP) layers. Hyperparameters (number of neurons, types of activation functions, etc.) were chosen based on the results in validation data. The network ended up being a flattening of the last 8192 samples in the 8192hz time series as the input. [5] This went into four convolutional neural networks with filter sizes of 64, 128, 256, and 512, respectively, and kernel sizes of 16,16,16, and 32 with 4 for all max pooling layers. Stride of 1 was also chosen for all convolutional layers and 4 for all pooling layers. These all used the standard ReLU activation function throughout as the non-linearity between layers. [The ADAM algorithm]() was used to train the network. The two full-connected layers were 128 and 64. Both networks were the same for classifier and predictor.\n\nThe final result was a network both smaller in size and more efficient to run, thus accomplishing both of their goals. It was only slightly less powerful than the matched filtering. The sensitivity of Deep Filtering was particularly impressive, achieving a 100% sensitivity at SNR of 10, whose value is on average proportional to 10 +/- 1.5. It was acceptable in performance at rates as low as 4. They actually trained at very high SNR and gradually increased the noise, meaning SNR decreased, in an attempt to train the networks faster, which was evidently successful.\n\nThey then applied Deep Filtering to a real gravitational wave signal using a sliding window of 1s width with offsets of 0.2s through the data. It was powerful in detecting the signal. You can see a demo of this here (its from the paper).\n\nThis paper [6] claims to lay out a new paradigm of real-time gravitational wave analysis with a faster and smaller network for finding them.\n\nWhat I find most interesting on this paper is that it reminds me of model compression as it was discovered by Paul Caruan et al. in 2006. This also fulfills a rule of thumb I have in when neural networks are appropriate. Neural networks, by virtue of being able to mimic any function given enough neurons, can be trained to copy what another function does. In this case, the function was matched filtering, which is a complex ensemble of methods. The Deep Filtering is trained on how matched filtering is labeling the data. Early on in my learning, I was under the mistaken impression that neural networks were learning how to find the gravitational waves on their own when its more accurate to describe them as copying what another function does. [7] I certainly think this paper is illustrative on unintuitive examples where neural networks can be applied.\n\nAlso, I love how simple the neural network is. So many big complex neural networks are used in overkill situations. I find simpler networks, even 1 or 2 layer multilayer-perceptions (MLPs), are sufficient, so its complexity can be flaunted. They seemed to have picked a \u201cjust-right\u201d size network here.\n\n[1]: Referred to as GW in the paper. They used a lot of shorthand to make the writing more compact. I\u2019m going to stick to the full phrase to avoid confusion, provided its not too long.\n\n[2]: I presume that knowledge can be found in dense Physics papers if the reader is curious, my purpose here is to better analyze the neural network.\n\n[3]: They didn\u2019t call it this exactly, but it\u2019s the closest term in machine learning that I think is applicable.\n\n[4]: Their words, not mine, hence the quotes.\n\n[5]: This means 8192 samples per second were collected. The flattening means that the convolutional neural networks were 1 dimension.\n\n[6]: Is this a bold claim? I\u2019m not knowledgeable enough in physics to know. Again, my interest in this paper was in its application of neural networks.\n\n[7]: Functions here are not f(x) = x\u00b2 type of functions. They\u2019re better thought as being the expected value function where we have some function that can be described by some method even if we can\u2019t break it down into mathematical terms. That appears to be the case here, though I\u2019m not versed on the Physics behind this to be fair. Another example of this would be labeling digits, where there is some function being used by the human mind to label the digits and we are having the neural network mimic that function as we can\u2019t describe it any other way.\n\nYou can find it here on ArXiv \u2014 the original paper\n\nhttps://arxiv.org/abs/1602.04531 \u2014 \u201cThe first gravitational-wave source from the isolated evolution of two 40\u2013100 Msun stars\u201d\n\nhttps://arxiv.org/abs/1701.00008 \u2014 The original authors\u2019 paper on using deep learning for finding gravitational wave signals. They frequently cite this."
    },
    {
        "url": "https://medium.com/singular-distillation/neural-networks-when-are-they-called-for-3524a5c2148",
        "title": "Neural Networks: When are they called for? \u2013 Singular Distillation \u2013",
        "text": "The recent explosion in neural networks has become a real phenomenon in the last two years. I can recall starting out ~4 years ago and not finding any serious textbooks from which to learn. [1] Nowadays, everybody and their cousin is using neural networks and has some fancy book on it, with varying rigor in their comprehensiveness.\n\nWhether by marketing hype or by general strength, many companies are now claiming to use them in their products. I\u2019m skeptical as to these claims; neural networks are not a one-size-fits-all solution so this is more likely marketing hype. However, they are really powerful in some specific ways. This article will touch on some rules of thumb that I have come to learn over time.\n\nYou need a lot of data. This can\u2019t be understated, you really need an amount at minimum in the gigabytes range and more realistically in the terabytes range. As a general rule of thumb, you\u2019ll want around ten times as many entries/rows/instances as features in the data itself. Neural networks trained on a small dataset will lose out compared to more traditional methods trained on the same set. The mass amount of data necessary for neural networks is the big reason why they\u2019ve only become practical in the last ten years.\n\nThat leads to a second point: you need a lot of computing resources. This also can\u2019t be understated. Prepare to shell out a lot for either a professional grade GPU or equivalent instance time in AWS. I personally run a GTX 980 and at the time (~2 years ago) it cost a lot, especially for a poor college kid. Many have claimed to run neural networks on raspberry pis, but I find those are often overtrained (an epidemic for getting easy and impressive, though not meaningful, results) or underperform relative to simpler models, which you of course never see.\n\nThe bigger problem with computing resources is that managing them is still a bit of an issue. For a long time in machine learning, training on a distributed system was the big bottleneck. Then came along technologies like Spark and Hadoop to aid in that. Neural networks had the same issue, with technologies like Tensorflow now enabling people to properly distribute the load. But even then, its not perfect by any means and still requires a lot of manual effort to get it running. In the case of Hadoop and Spark, there are whole companies dedicated to building flavors of these open source technologies to aid you. This is to show you that managing massive compute resources is not a small undertaking and may take up most of your time when playing with neural networks. [2]\n\nThat leads to the next point, which is identifying situations in which neural networks excel. A cursory look at the Kaggle Competition Winner\u2019s blog might surprise some caught up in the hype: many winners do not use neural networks. This certainly caused me to rethink what types of situations are reasonable for neural networks. For example, tasks like digit classification can be performed to the 95% accuracy range with just random forests. Depending on the context, this might be accurate enough. That said, there are some good rules of thumb I have found are helpful in considering if neural networks are up to snuff for the job.\n\nThe first situation is when one has lots of unstructured or difficult to structure data. This is why computer vision is being dominated right now by neural networks. Its hard, just looking at a picture, to come up with the specific pixel values that code to some object. You can\u2019t (easily) make a decision tree, e.g. \u201cif-this-then-that\u201d, type of algorithm for such a task. This means the data, whose features are the pixel values for the image in question, are difficult to decipher or label. Often, the columns are just generically labeled something like \u201cX1-X100000\u201d and thus have no explicitly meaning. Neural networks perform supremely in situations like this because they seem to be able to find what\u2019s important and will weight whatever pixels are important based on this, roughly speaking anyway.\n\nThe second situation they perform well on is in the compression of knowledge. As I write this, I\u2019m preparing a write-up of a recent paper on Deep Filtering, a new application of neural networks to replace match filtering, the principal technique used in detecting gravitational waves from noise. The basic just of the paper is that you can take the many many signals one is looking for and have a neural network \u201ccompress\u201d that knowledge. That way, a network that is small and computationally faster runs instead of an algorithm looping through 2500 possible signals. There is a paper from 2006 on this by Caruana et al. in which he showed that big complicated ensembles of models, which take a long time to run, can be compressed by getting one network to \u201cmimic\u201d the full model. This whole idea needs a dedicated post to be properly explored, but the short version is if you have a complex function that you do understand or a series of complex decision trees, you can use a neural network to learn how those functions work and mimic them to a suprising accuracy. This is supported by the fact that neural networks can mimic any function given enough neurons.\n\nComplex probabilities are another arena in which neural networks do well. Because of how the many neurons interact, you can get and pick apart probabilities on events or sub-events in addition to final results when doing prediction. This is an extension of how neural networks can mimic any function given enough data; you can mimic any probability given enough data. Some researchers, such as Yann LeCun, believe that probability theory needs to be thrown out because neural networks can handle so much more sophisticated probability distributions than current theory assumes is possible. [3]\n\nLastly, neural networks dominate in games. I define a game as a situation in which one has infinite tries (i.e. the situation can be played over and over again) and fixed rules that define a limited problem space. Examples of neural networks that do well here include the DQN in atari games and the AlphaGo by Deepmind. [4] By limiting the problem space, an AI can be given a narrow goal, namely to win. I\u2019ve touched on the importance of a limited problem space with respect to reinforcement learning before. Traditional models do not work here as the data is less structured. Its not clear whats important or relevant, nor is it clear how to build a traditional equation to find the solution in some context (i.e. the strategies are harder to define into equations). Neural networks, because they have access to unlimited data in these situations, can find the appropriate function given enough time.\n\nI hope these thoughts might lead you to better think about neural networks. They don\u2019t always work well and time is too important to spend chasing down dead ends. Always use the simplest algorithm first, if not because they are at least easy to check, before moving on to the more complicated ones like neural networks. You may find they perform just as good if not better than neural networks in most situations.\n\n[1]: For those curious, I ended up learning directly from papers, circling every term I didn\u2019t understand on a first read through and drilling down into the cited papers to eventually \u201cget it.\u201d That takes a lot of effort, but I\u2019m pretty boned up on the theory now, textbooks be damned!\n\n[2]: This is a big point many startups are working on actually.\n\n[3]: By the way, I\u2019m not entirely convinced on that point and Prof. LeCun is probably being hyperbolic to make a larger point: we don\u2019t need to assume everything is a gaussian all the time. This, coming out of the central limit theorem, is a key point in traditional probability and statistics.\n\n[4]: These models actually use decision trees in conjunction with neural networks, roughly speaking."
    },
    {
        "url": "https://medium.com/singular-distillation/applying-machine-learning-to-design-by-building-auto-adaptable-interfaces-9b2e1dccef4b",
        "title": "Applying Machine Learning to Design by Building Auto-Adaptable Interfaces",
        "text": "User interface design can be a tricky problem. There are many ways to approach it and increasingly data is a huge part of the process. Heatmaps, customer journeys, and other forms of metrics are collected so you can analyze how users use your website.\n\nThis has the beginnings of an information overload problem. All these metrics are useless if you don\u2019t have the tools to properly analyze them. It can help to know exactly how your site is being used, and for now this seems to be enough, but its less clear how it can be objectively applied to the design. This is where I believe machine learning can help: imagine if the user interface could adapt itself for each individual person without the need for human intervention?\n\nNow I understand this to be a far out idea. This post is written as a sort of quasi-brainstorm. But I don\u2019t think the idea is as far fetched as it sounds and wouldn\u2019t be surprised if many companies are already moving towards it.\n\nFor example, auto-complete is a form of this precise user customization. Companies like Google or Bing know to match your search habits to autocomplete your different queries. They used to not do this and their move to this model is a form of time saver for the user. Users are more likely to get where they need to go if the computer is correctly predicting what needs to happen. Machine learning driven UI that adapts to a particular user\u2019s habits is simply an extension of autocomplete, moving from the query box to the interface itself.\n\nAn adaptable UI, as I envision it, would be as follows: a given interface is generated for some user. That user has a journey to get to some task. In the process, they may waste time as they struggle to accomplish this task. The UI, under supervision of some machine learning model, would change the layout to better suit this task for that particular user and enable her to perform faster, thus improving the experience.\n\nThis idea hinges on finding a precise reward structure. I\u2019ve talked a bit about reinforcement learning in the past, but to restate it simply, reward structures are how reinforcement learning agents know whether they are making progress or not. You assign a positive or negative score to an outcome. The agent then learns to avoid negative outcomes and reach for positive ones by taking the appropriate actions. The actions are what the model learns to choose.\n\nThis is very clear in the case of games, where the reward structure is often as simple as +1 for a win and -1 for a loss. But how are we deciding if the interface improves for a user?\n\nI think you\u2019d start with very simple instances. As an example, you could look at the most frequently changed settings and move those to the top of the heap whenever a user opens the configuration page. More complex ones, such as rearranging your home screen on a social media application, could involve looking at your general activity or clicks. Consumer applications would likely be a better use of adaptable UI as retention is far more paramount and the marginal value is arguably less clear.\n\nBut what about something like Saas products (i.e. B2B)? Those reward structures are trickier to develop. You could likely tie the reward to something that is relative and varies from user to user. For instance, if you were building a CRM tool you could look at average time to respond to a prospect. Then the agent would rearrange the interface to try and decrease that time. In Saas, there is generally a fundamental use you\u2019re trying to optimize for and the interface should be part of that optimization anyway. The algorithm would assist you in this in the early stages before performing it for you in the latter stages.\n\nHowever, there are some problems that would need to be realized in practice. The big one that I can see is how often to change the interface. If you frequently mess with it, you\u2019ll likely frustrate your users.\n\nThis is not a fully-baked idea by any means and these are just my initial thoughts. I\u2019m also certain I\u2019m not the first one to propose this, so please comment on any sources if you\u2019ve heard this before as I\u2019d be super interested! Thanks for reading."
    },
    {
        "url": "https://medium.com/singular-distillation/games-games-games-fighting-overfitting-in-reinforcement-learning-b464ed20cb1d",
        "title": "Games, Games, Games: Fighting Overfitting in Reinforcement Learning",
        "text": "Artificial Intelligence has become quite the hubub recently and one of its testing grounds has been in computer games. While Deepmind\u2019s Go playing AI may have captured the imagination of many, they actually used earlier research on an AI playing atari games to help build it. Now with organizations like open.ai using games as testing grounds for doing general machine learning research, games are increasingly considered the battle ground for comparing AI models. Even Deepmind has moved onto building one to play competitive Starcraft. If it can do that, the belief seems to be, then terminator-level AI is just around the corner. However, issues inside of the Deepmind atari paper lead to some startling insights on how limited these models are to generalizing. I think this is pertinent to machine learning beginners.\n\nIn the case of the Atari model, an AI [1] was trained only on raw pixels and could press any button on a virtual Atari 2600. After many many hours of training, it was then able to play some (but not all) games with above-human ability. The difference in what games it could do this but what games it couldn\u2019t is telling: it performed extremely well on games with a fixed screen but less well on games with many screens.\n\nWhat do I mean by \u201cgames with a fixed screen\u201d? Some video games, especially of that era, were so simple they effective only had one level. Consider Pong for instance, which had a solid color background with two moving paddles, or Space Invaders where the only change is in the speed of the aliens. In these types of games, there is no need to understand anything beyond the simple bounds of this screen. This enables the computer to have less to learn and it can instead focus on memorizing what it sees.\n\nHowever, Atari games like Private Eye are infinitely more complex because they have many different screens. Each level is completely different with regards to the location of objects within the screen (hence the many different screens). The difference between the two is that an AI could over time just memorize the ideal button layouts and react to changes in pixels if playing a fixed screen game. But when playing a multi-screen game, it has to learn to differentiate between different parts of the screen, such as enemies vs power ups, and generalize that information to other levels.\n\nNow I need to state that this is still really impressive and the first group to do this to my knowledge (the paper here if you wanna check it out). I am in no way dismissing Deepmind\u2019s work. In fact, though the performance in games like Private Eye were below human performance, that an AI could play them well at all is marvelous indeed.\n\nThe issue is that people seem to have taken the wrong idea out of this.\n\nI see a lot of different attempts to build AIs for games like Super Mario Brothers. An AI that could fully play this game would be astounding: there are multiple levels in this game and each is sufficiently different that an AI would have to learn many different, complex, and moving parts. But many people are instead training an AI to memorize a given level. By this I mean, they have a model learn to play exactly one level, usually the first one, over and over again until it plays it perfectly. This leads to a non-generalizing AI. This is no different than overtraining a more conventional statistical model.\n\nThese distinctions are important for beginners to know and I\u2019m not hearing it coming from anywhere else. Models that are generalizable are more useful because they can be applied to many different situations and different data. In contrast, overtrained models can only really be used on the data they\u2019ve seen. If the point of machine learning is to build models that can predict, then we want models that generalize so they can act in working with new data. Otherwise, they\u2019ll only be useful when working on the old, past data that we already understand. In the case of video games, that would be levels they\u2019ve already seen and played.\n\nOvertraining is taught in basic statistics classes, let alone in machine learning classes, but for some reason its ignored when doing reinforcement learning. This is to the detriment of the field as it teaches the wrong concepts to look for and leads to people building poor models. Even if progress doesn\u2019t look as fast, learning to prevent overtraining will improve people\u2019s data science skillset astronomically more than overtraining models and being happy with the raw metrics it scores on. So when training a reinforcement learning agent, always test on new levels to make sure it generalizes. [3]\n\nAs always, please let me know if you have any comments, questions, or criticisms!\n\nReferences for this include Deepmind\u2019s original paper, their follow-up in Nature, and this video with Demis Hassabis, Deepmind\u2019s founder.\n\n[1]: More specifically, a reinforcement learning model that used deep recurrent neural networks. I suggest reading the paper for more details if you are inclined.\n\n[2]: Though if you ask me, General AI is highly unlikely to happen regardless.\n\n[3]: And if you build an AI that can train on one Super Mario Brothers level but play the whole game from that level, publish a paper and reference me. :)"
    },
    {
        "url": "https://medium.com/singular-distillation/give-me-different-things-on-my-timeline-ad87ff534add",
        "title": "Give me Different Things on my Timeline! \u2013 Singular Distillation \u2013",
        "text": "One of the prominent uses of machine learning is in guiding human behavior for some tangible benefit. Examples of this include the collaborative filtering systems for recommendations and timeline feeds in which content you\u2019re more likely to click on surfaces. The premise is that users don\u2019t want to waste time on things they clearly don\u2019t like and websites want to keep you for as long as possible by giving you more of what you want. The issue is that neither is all that helpful when you\u2019re building models. Much like the genetic problems associated with inbreeding mice, websites that overly leverage recommendation systems breed models that do poorly in prediction tasks in the long run.\n\nWe don\u2019t need to look at these sites\u2019 specific metrics to see this. [1] Instead, we can take a look at reinforcement learning and its application of randomness in exploring problem spaces. Given its a subdomain of AI, it should be reasonable as a basis of comparison to how these systems can go wrong.\n\nIn reinforcement learning, a situation is defined by an agent trying to pursue some task. The usual example given is a robot trying to move towards a light. The robot is placed in a given area, such as a table, with a lamp shining some distance away. The robot is the agent and its task is to move towards the light until it touches it. You then define a reward and a punishment whereby any action that results in a positive step towards the final goal gives a reward and any action that results in a negative move away from the final goal results in a punishment. This forms the reward structure of the particular agent. In the example mentioned, this might be +1 if the robot touches the light or -1 if it doesn\u2019t. The agent then, over the course of many cycles, will learn the actions that lead to accomplishing the task. A series of algorithms under the covers will be learned as the agent starts to distinguish between actions that get it to accomplishing the final goal and actions that take away from it.\n\nAt the start of a reinforcement learning situation, the agent does not know what actions will be considered. It therefore tries to do some things that are often wacky or nonsensical to us. Once it learns those are not useful actions, it will stop doing them. This presents a problem though, as sometimes those actions are beneficial at one point in time but less beneficial at another. It can also limit the list of possible actions: sometimes the agent will only do actions that\u2019s its previously done before because it has some basis of knowing how they will turn out while never-performed actions are a big unknown and may be weighted very low in the reward structure. [2]\n\nThis problem has been solved primarily be having the agent perform totally random actions from time to time. This allows it to build a diverse collection of experiences with rewards and punishments. The trick is to balance picking moves it knows has some definite reward and actions that its never seen before.\n\nMachine learning, particularly recommendation models, need to go back to having these. By using them, consumer companies can start to delight users with unexpected avenues by which they can explore instead of serving them the same old. I think of the many retail companies, sending emails asking if I\u2019ll buy the same six things I last looked at, having the nerve to call my last viewed items \u201crecommendations\u201d or some similar nonsense. Its not recommending me anything and its not going to bring me back to the site; I know I\u2019ll just see the same old I\u2019ve always seen. I need newness to re-invigorate my love of the product. Its partly why Twitter can be so wonderful (at times): I see genuine randomness on my timeline, which interests me and inspires me to seek beyond my boundaries. I cannot be the only one to think of this as a feature, especially with Twitter\u2019s ill-fated attempt to overhaul its timeline last year that sparking outcries.\n\nBy exposing users to different, random content, the models can also start to see more diverse connections be made. Perhaps a number of long time users might start to do actions that the model did not anticipate. After all, if the users never did it before, how would the model have known? Most machine learning is better thought of as pattern recognition than predictions on the future anyway so its best to increase the number of patterns it can predict.\n\nNow those more knowledgeable in reinforcement learning might mention that these algorithms have a point where random actions stop being introduced. Its true in DeepMind\u2019s AlphaGo for instance. That is true, but remember that the total problem space is limited in games but not in real life, where the total list of actions is virtually limitless. That includes recommendation systems where the total list of viewable items is infinite. There can never be a moment in which some model has seen every possible action. I\u2019ve mentiond this point in past essays before. Thus, I don\u2019t see a machine learning model ever coming to a point where reducing randomness is beneficial.\n\nTo truly improve our algorithms, we need them to see more. If we limit this valuable data too early, the consumer experience will flatline as people get bored of the same old. Variety is the spice of life and data science models, exposing them to a more diverse dataset that will extrapolate better and with better predictive power.\n\nIf you found this post interesting please give me an applause and follow \u201cSingular Distillations\u201d for more machine learning insights!\n\nAlso, if you know of Facebook, Twitter, Instagram, etc. doing this, please let me know! I\u2019d be curious if my general insight is being applied anywhere.\n\n[1]: Which is good because these metrics are never public anyway.\n\n[2]: How it weights goals, rewards, and punishments will also affect this."
    },
    {
        "url": "https://medium.com/singular-distillation/a-little-explanation-on-capsnet-the-newest-innovation-sweeping-machine-learning-bcfe5d96e3e3",
        "title": "A Little Explanation on CapsNet: The Newest Innovation Sweeping Machine Learning",
        "text": "Addendum: There is another blog post I highly recommend that helps to put some of Prof. Geoff Hinton\u2019s video into context. It may help you addition to this article!\n\nGeoff Hinton et al.\u2019s recent paper on Capsule networks has been quite an earth shaking paper in the machine learning field. It proposes a theoretically better alternative to convolutional neural networks, the current state-of-the-art in computer vision. This post is written to better explain the more erudite paper (link is at the end of this post if you\u2019re curious to read the source).\n\nFirst, I want to touch on the sometimes confusing terminology of neural networks. Many ideas in machine learning come from symbolically mathematizing cognitive concepts. To demonstrate, let\u2019s take the example of the neuron. In the physical world, this is a group of cells that takes a signal as input and gives out some signal as output, provided its sufficiently excited. While a naieve explanation, this ends up being sufficient for the machine learning concept of \u201cneural networks\u201d. Here, a neuron is a mathematical unit that takes an input and gives an output of the input using a series of functions. We learn weights to determine which particular inputs might be more important than others using backpropagation during a training phase. We can stack these neurons so that outputs of one layer of neurons becomes the inputs of another. All types of neurons derive from this basic concept, including recurrent neural networks and convolutional neural networks.\n\nNow let\u2019s describe the idea of capsules. Like the basic neuron, they also represent the symbolic mathematizing of a congnitive idea using a somewhat naive assumption: higher up parts of our brains do more interpreting, understanding, and calculating of higher level features, with specific parts of the brain getting specific in what areas or topics they deal with. We don\u2019t take in data to all dimensions equally across the brain but instead we \u201cfeed in\u201d lower level features for processing by higher level parts of the brain to take the cognitive load off the higher level processing. If the lower level feature is not relevant to some higher level part of the brain, it shouldn\u2019t be sent there. At the very least, its signal should be diminished somewhat.\n\nThese capsules were conceived to handle the problem of identifying pose. This is when a model might be trained to identify a dog, but becomes reliant on the orientation of that dog within view. If you turn this dog around and try to take a picture from a different angle, the model might have trouble recognizing it. Capsules attempt to solve this by having a higher level part of the \u201csymbolic mathematical brain\u201d, i.e. the network, handle the identification and post of complicated features while lower level ones handle \u201csub\u201d-features. A higher level capsule might identify a face based on lower level capsules identifying a mouth and nose with agreeing orientation.\n\nConvolutional neural networks currently don\u2019t do this. Instead they rely on a lot of data to include all the possible poses an object can have. They also have other drawbacks.\n\nFor starters there is the problem of context. Information sometimes needs context in order to be valid. Geoff Hinton himself uses the example whereby a tetrahedron cut in half is difficult to put back together, even by MIT professors. Its difficult to know exactly why this is, but it seems to be related to our frame of reference: how we choose to view objects can determine what we make of them and how we identify them. Capsule networks could, potentially, solve this problem by embedding that information into specific capsules that learn the context involved and then feed that information into higher parts of the network.\n\nSecond, convolutional neural networks combine multiple feature detectors by pooling them together. Prior layers of neural networks feed in as features into later layers. It is thought that these earlier networks act as feature detectors, with earlier networks identifying extremely rudimentary features while later ones can identify ears, eyes, etc. By pooling them together, one can solve the variance problem where an ear in the left-hand side of the picture might not mean the same thing as an ear on the right-hand side of the picture as far as the model is concerned.\n\nPooling ends up being pretty invariant though. [1] It causes this information to be spread across many neurons instead of a few. Each neuron therefore has to work harder. It would be better if we could specialize the neurons so that they handle specific identification. We could have a capsule that looks for a nose and a capsule that looks for a mouth. Those capsules could get really good at identifying those very specific objects because they have nothing else to do as far as the whole network is concerned.\n\nRelated is Prof. Geoff Hinton\u2019s ideal goal to have a higher domain of space that an object translates into. Everytime, no matter the orientation, the object is translated into the same rigid shape on this higher domain space. A way to get to there would be to use specializing capsules to help in translating objects into higher domain spaces.\n\nFor building a network of capsules, we can take inspiration from Hough (pronounced \u201choff\u201d) Transforms, a 1980s invention. Applied here, the basic idea is to have a two-part structure I\u2019m calling a speck (for lack of a better name). One half of the speck predicts the probability of a coordinate frame being X and the other half predicts the pose. These child _speck_s feed into a parent speck. If\n\n enough of these child _speck_s agree, then parent speck will give a probability of the item being Y, a more complicated object than X, along with its orientation. For example, child _speck_s could predict mouth, nose, and eyes, with orientations, and feed into a parent speck that predicts the presence of a face and its pose.\n\nNow, let\u2019s replace those neurons with capsules. Lower level capsules make \u201cweak bets\u201d on what an object could be by identifying simpler sub-parts of that object. A higher level capsule then takes these lower-level bets and tries to see if they agree. If enough of them agree, then its likely beyond coincidence that this object is Y. That\u2019s the essence of how these capsule networks work.\n\nThe problem is with routing: how do we route these lower level capsules such that they go to the right higher level capsules?\n\nThis was the innovation that came out six days ago. [2]\n\nSo how does this routing algorithm work? To get there we need to define a few key ideas. To simplify, we\u2019ll assume a two layer capsule network. Raw features feed into layer LA and outputs from layer LA feed into layer LB. Both layers are composed of capsules.\n\nFirst, we weight the matrix of outputs, called u, from layer LA that become inputs into layer LB. These weights will be stored as a vector W. Multiplying the two together will give us u\u2019.\n\nA routing algorithm then determines an additional parameter called the coupling coefficient c. This coefficent will diminish information sent to incorrect capsules by appropriately weighting them less. We also \u201csquash\u201d the total input by using a specific function. This will make sure that low magnitude vectors get shrunk to almost zero and high magnitude vectors will get sized to a length of only slightly below one. [3] This is because the dynamic routing algorithm in this paper is using the magnitude of the vectors to repsent the probability of an object being present in the correct input. Therefore its important these input vectors don\u2019t get too out of hand in magnitude.\n\nI\u2019ll be describing the routing algorithm from the paper in plain english here. You can see the more specific precise form of it in procedure 1 in the paper itself. Keep in mind that they mention this as just one way you could implement a routing algorithm, so expect more to be conjectured as time goes on.\n\nAs a background, b is used to signify the log prior probabilities and the coupling factors c are determined as a softmax function of b. [4]\n\nFor each capsule in layer LA and layer LB, we set the priors b to be 0 across the board. Then, for r iterations, we go through each capsule and set the coupling factor c to be the softmax function of b. We calculate s by multipling c to u\u2019. The resulting value will be called s. The inputs going into layer LB each get \u201csquashed\u201d, using the appropriate function, to get v. Then for each capsule, we adjust b by adding to it the values of u and v.\n\nBelow is the algorithm in a more precise, academia-friendly printing. Its still in my wording.\n\nThere\u2019s a lot more to the paper, which I may get to in future posts, that deal with performance on MNIST datasets and have their particular implementation using convolutional neural networks, called CapsNet, works. But I wanted to touch on the raw overview here, especially given the surplus of \u201chere\u2019s a bunch of code, understand it!\u201d type posts. I don\u2019t believe those every help you actually understand what\u2019s going on. I\u2019ve included the references used in this post below and here\u2019s a code repo with an implementation written in Keras. [5]\n\nFeel free to respond with any comments or questions!\n\nLink to Geoff Hinton\u2019s paper and the prior video here. I think the video is useful for getting the necessary background to understanding the purpose of capsules.\n\n[1]: This is actually insufficient to say. Pooling ends up being invariant, which means that where an object is located within the field of view is not important. But what\u2019s desirable is if its equivariant. This means that changes in viewpoint cause changes in neural activities in which the weights we use encode viewpoint-invariant knowledge instead of encoding for neural activities themselves.\n\n[2]: Note that capsules are an abstract idea. In Hinton et al.\u2019s implementation, neural networks are used (basically anyway, you should read the paper to see exactly what they used) but in theory these capsules could be anything. What CapsNet and the concept of capsules provide more generally is a framework by which to view these concepts. This is why I don\u2019t describe the capsule in all that much mathematical detail.\n\n[3]: They use the term \u201clength\u201d, which really throws me off. I think magnitude is an easier to interpret term here. You can see the function in question as equation 1 within the paper itself.\n\n[4]: Setting up the coupling factors to be the softmax of some prior to the particular algorithm described here and seems to be not necesarily be a strict requirement of all routing algorithms. Then again, there aren\u2019t any others actually proposed yet.\n\n[5]: Can I just say that I\u2019m always impressed with how quickly people come up with code examples on this? Blows my mind everytime!"
    },
    {
        "url": "https://medium.com/singular-distillation/does-machine-learning-really-predict-or-does-it-recognize-529f4fc3f682",
        "title": "Does machine learning really predict? Or does it recognize?",
        "text": "Its a common theme in science fiction that we as human beings will one day merge with computers and become \u201ctranshuman\u201d, elevating ourselves above our mere physical bodies. We imagine sticking small computer attachments to our brains, increasing our general intellectual faculties. It seems so far off because we\u2019re not there yet; I don\u2019t think anybody can imagine sticking a computer implant in their brains anytime soon. But many dream of it happening, a sort of \u201crealistic one-shot-cure\u201d to all your mental deficiencies.\n\nBut less talked about is how we\u2019re morphing into a society that relies on computers to make decisions for us, even those that don\u2019t involve bodily surgery. The most obvious example for me is Tinder. A computer algorithm is, fundamentally, deciding how humans should mate and with whom. At the very least, its guiding the process. That sounds pretty cyborg like to me, even though it doesn\u2019t involve permanently attaching an iPhone in my arm or any other visual changes.\n\nAs we rely on machines to do this sort of decision making, we have to call into question exactly what does machine learning do well and what does it do poor. I\u2019ve increasingly come to the viewpoint that machine learning is poor at prediction but is excellent at pattern recognition.\n\nLet me name a simple example to show you what I mean: computer vision. Attempts to come up with robust, \u201chandmade\u201d algorithms that can identify objects has been a precarious pursuit for decades. One researcher spent some ten years doing work on one, winning a massive academic challenge, only to be outdone the following year by a team doing it part time using a neural network. This anecdote points to machine learning being good at recognizing patterns in the form of objects that we can see. Other successful attempts at machine learning include signal recognition, for things like breakout detection, and categorization of people, for things like advertising. This seems to follow a theme.\n\nBut has it been any good at predicting what advertisements work? No seems to be the answer. It seems like most adverts are no more specific than those I see on TV, perhaps even worse; I remember getting Irish bank ads on YouTube even though I live across the Atlantic. This intuitively makes sense as we can\u2019t (necessarily) conjure the future based on the past so why should computer algorithms based on past data reflect this? Certainly Google can identify the relevant adverts to a particular search query and Facebook can identify people\u2019s traits, but neither is exactly predicting what people will click so much as narrowing down the field of people who might click. That\u2019s why you need to put in a recommended audience when you run advertising across these platforms.\n\nThis might sound semantic, but its a point worth noting. In the case of Target (in)famously predicting that a girl was pregnant, was it \u201cpredicting\u201d her pregnancy or identifying it based on a pattern? We\u2019re not looking to judge human beings so much as aid in our ability to categorize them, at least for the purposes of selling products. [1]Thus its sufficient to simply recognize the patterns, which in this case was whether a girl\u2019s purchasing history matched that of a pregnant woman\u2019s.\n\nWe so want machine learning to help us take a cognitive load off of our minds by telling us what we\u2019ll do in the future, enabling us to focus on other issues. Many companies are built on precisely this premise. But I fear its an ultimately futile endeavor. I\u2019ve yet to see any machine learning startups successfully predict anything of the future. [2] Many hedge funds have not moved into neural networks, but instead rely on Markov processes, Bayesian statistics, and other \u201csimpler\u201d mechanisms to make future predictions on prices. I can\u2019t help but wonder if this is problem these groups recognize.\n\nSo when you hear the hype about machine learning fixing everything, allowing us to become super-humans in the future\u2026 well its simply not so. But nor is it right to be ultimately pessimistic. What instead we need to do is focus our effort in the right direction, taking into account the realistic expectations we can have about machine learning\u2019s possibilities so we can properly exploit them. If you keep the wrong model of this in your head, you, as an entity, organization, or entrepreneur, are doomed to waste valuable time.\n\n[1]: Whether or not its ethical to categorize human beings as such,\n\nthat\u2019s a question the author leaves to the audience to ponder.\n\n[2]: I\u2019m sure some startup has raised a lot of cash to do some\n\nprediction, but I\u2019m talking about startups that have turned profitable\n\nor gone public. If there are any, please leave a comment and let me\n\nknow!"
    },
    {
        "url": "https://medium.com/singular-distillation/hayek-and-ai-dream-1c5be58252bc",
        "title": "Hayek and the A.I. Dream \u2013 Singular Distillation \u2013",
        "text": "As part of the NaNoWriMo, I\u2019m going to write a post every day of November for the rest of the month. Some of these will be technical and others more philosophical. Hope you all enjoy!\n\nIn 1945, there was a proposal to institute the Central Pricing Board. The idea was sound: why not try and smooth out the irregularities involved in commodities pricing by allowing one centralized board to decide prices?\n\nThe problem, as F.A. Hayek noted in his seminal essay \u201cThe Use of Knowledge in a Free Society\u201d, was that this is fundamentally impossible because it assumes that one person, or a small group of people, can know everything about about every aspect of society at all times. The essay has since become quite influential, influencing everything from the modern Libertarian movement to Wikipedia.\n\nA lot has been said about this work that I won\u2019t say here (books have been written about this small essay), but one point that always stood out to me is that people can know a lot about a very small specific topic, but they can never have a high depth of knowledge about all topics. This explains why Ph.D.s specialize; they are out to learn a lot about something very tiny and specific. The essay itself mentions how pricing is best determined by those closest to production because they understand the methods best. I know many who never read the news on the basis of a journalist can never know as much as a physcisist about physics (or whatever other topic is particularly relevant).\n\nThe implications of Hayek\u2019s essay can be extended to the data science/machine learning/artificial intelligence community (note: I use them interchangeably in this post).\n\nThere\u2019s a strong movement amongst AI enthusiasts to bring about General Intelligence. The idea is that we can build a machine learning algorithm or agent, that is as smart as humans or is at least generalizable to any situation (the specifics depend on who you talk to). This idea is popular with figures like Ray Kurzweil of the Singularity University, who believes we will one day upload our conscience to a computer network and live forever, and Nick Bostrom, who thinks we\u2019re doomed to make paperclips forever. Its found strong support from major entrepreneurs like Elon Musk and Larry Page as well.\n\nBut it suffers from the same issues that a central planing board would suffer. We can\u2019t ever know the all of everything. That\u2019s period, point-blank ever. There will always be some unknown, some portion of our reality we just don\u2019t get. To say that we can fully understand everything is to say that we can fit all of reality in our heads. The AI god believers would have you think this is either possible of us or that we can build a machine that does this.\n\nI don\u2019t believe that the former for the reasons laid out in Hayek\u2019s essay and because its self-evident, to me at least. You really should read it as I\u2019m not as smart as Hayek was and can\u2019t explain it as comprehensively as he can. As for being self-evident, I think the whole internet is evidence of this. Arm chair stock analysts who live in their parent\u2019s basements but claim to know more than billionaire hedge fund managers abound in the comments section of most financial publications.\n\nOn the possibility of building a machine that can do this, we as human beings have always succeed in building tools when the problem space as specified very clearly and limited in some way. Let me use an example from the Industrial era to show you what I mean by that. When you wanted to build a machine, say a grain thresher, you had to know how to fully define the problem. In this case, you had a specific purpose for that machine to accomplish, that of separate grain from the stalk. This was a way of limiting your focus, similar to how human beings work with heuristics by ignoring unimportant events. By narrowing down, you could start to think about all the particularities involved, which are often much greater than is seen at first glance.\n\nIn fact, anybody whose been involved with the hard sciences know this to be true. You simplify the problem space by either taking assumptions or by narrowing the field of possible conditions. The former helps when you\u2019re in a new area, but the latter is ultimately necesary as topics grow in their complexity.\n\nBy contrast, General Intelligence does nothing to simplify the problem space. When you do start to limit the problem space, the results are indeed extraordinary. This is one of the reasons why machine learning works so well in games: we can very specifically set down rules to play by and define the space of all possible moves. Its also why computer vision works so well if we have labeled data, as we can specify the exact set of examples to work against. Thus we can build AlphaGo to steam roll the greatest players of our era, but that same AI can\u2019t make me breakfast in the morning or even comprehend how to do that.\n\nThis all has implications in how we approach problems. We try to think of a fits-all solution when fundmentally this is impossible. Each problem, even when using the same dataset, must be approached as if its solution was separate and unique. This cannot be\n\n understated, as sometimes meaningful predictions might only require a linear regresion while other times it might require a complex ensemble. This AI-as-messiah that seems increasingly prevalent misses this problem and leads us down a path of wasted effort. I hope you all avoid this."
    },
    {
        "url": "https://medium.com/singular-distillation/the-virtual-ai-assistant-will-not-look-like-us-9ec27dc2b97b",
        "title": "The Virtual AI Assistant Will Not Look Like Us \u2013 Singular Distillation \u2013",
        "text": "It is often said that technology will change us but never in the ways we anticipate. Take for instance the internet. Many thought it would enable us to send mail very quickly (email). But nobody assumed that it would be used to connect us socially (Twitter, Facebook) or hold the world\u2019s information (Google, Wikipedia). People had a limited imagination about what the internet would be versus what it became.\n\nHowever, that last example isn't entirely true. People did think it would hold the world\u2019s information. But they thought it would be like a library. Google & other search engines show us that the world\u2019s information is rather distilled amongst many people. By accessing Google, we access the world\u2019s combined information. In a way, its a glimpse into our collective unconscious.\n\nThis brings me to Artificial Intelligence as an assistant, which promises to be one of its biggest applications. With Facebook looking to bring a bot marketplace to the messenger platform and companies building bots on Slack growing like gang busters, this seems to be the hot space. After all it fulfils my personal criteria of a consequential innovation: it uses new technology to do some task that wasn't previously possible with older technology. Machine Learning and AI can better know us and communicate with us than computers were previously capable of doing. Throw in powerful pocket computers in the form of smart phones and it looks like a wrap.\n\nBut I have a problem with the input. Its too archaic and feels too inconvenient. We talked to computers in movies like 2001 and Star Trek. Remember that when the original Macintosh came out, nobody had thought of GUIs before (at least as far as story telling is concerned). Looking back at The Next Generation, they seemed more focused on hot keys which were, prior to the Mac, thought of as the next big innovation after typed out commands. AI & Machine Learning seem like they\u2019re on a similar cusp.\n\nI think AI will exist on our phones, but in a messenger or talking format. Its too much effort to take out our phone and talk to it, especially when better alternatives already exist and are already much much cheaper. I\u2019m thinking of the sensors on our phones and smart watches. Your phone being able to pick up your GPS location as you enter into a fast food joint to tell you its not such a good idea or to tell you of the cleanest path to work based on camera shots from similar snowfalls are the better ways to think of AI. As we leave more and more of our life on the Internet, its recorded and query-able. AI should have a better sense of what I want to do prior to doing or even thinking it. This would put it in a league above what humans can do, which would greatly expand people\u2019s willingness to purchase it. Having virtual assistants is a tough sell outside of more business-like situations (Slack / IRC bots) or gimmicks (Facebook Messenger\u2019s Miss Piggy bot). Consumer products will be more demanding and more relevant to the situation I described.\n\nWhich brings about an interesting connection to smart watches. So often we think of smart watches as the next smart phone, yet Pebble, Apple, & Google are instead tethering your watch to your phone. This makes it more of an extension to the phone in my mind. The watch is in a better position for certain activities. [1] It also suits itself to quick information. I love my Pebble smart watch for this reason alone. People always ask me \u201cWhat\u2019s the point when I can check my phone?\u201d but it really is convenient to check my watch quickly for text messages or twitter updates. I almost never check my phone any more. Most messages come from email or Facebook, so I just quickly type out a response on my computer as I\u2019m usually at one anyway.\n\nAs for AI, the smart watch seems really relevant to providing quick information it pulls from the phone. Extend this to AI and it seems fairly clear to me: get the AI serving predictions to me here on my watch. Is it 2pm and I haven\u2019t eaten? Let me know of great places nearby. Have I only been eating red meat recently? Recommend I get some vegetables to balance out my diet. [2] These super-recommendation apps look to be at least the first big AI arena. [3]\n\nThis also sounds really similar to Internet of Things. I\u2019m actually really sceptical of IoT, but in certain settings it makes sense. Smart watches are one of those settings.\n\nThis also really helps with the business model. One could leverage the phone or smart watch, which the customer already owns and presumably purchases every two years, and sell a subscription to one\u2019s AI service. Monthly subscriptions are a tough sell in the consumer market, but better than relying on ads which wouldn\u2019t really fit in here or one-off purchases which wouldn\u2019t generate revenue over time.\n\nThe next big technological step always look unintuitive at first. We expect the world to look and act like Star Trek even when its not practical or useful. Instead, think of simplification, convenience, and first principles. People listening to a computer versus yelling at it seems more sensible. This way of thinking is an extension of that. [4]\n\nIf you\u2019re looking for help on your next big Machine Learning or Data Science project, let me know! I\u2019m always looking for work in interesting projects. Shoot me an email at valexandersaulys@gmail.com\n\n[1] For example, isn\u2019t the accelerometer a great place to track hand motions like a gold swing? (Btw, my Dad is totally looking for this app and will buy an Apple Watch once its made)\n\n[2] How the phone/AI would know this, I don\u2019t quite know. I do feel it can without much hassle to the user though. Also, I just ate so hence the food examples.\n\n[3] And are actually really useful and anti-time suckers. Given how much we spend on social media and mobile games, one can\u2019t expand more from this market of eyeball-attention.\n\n[4] As a closing footnote, people hate interacting with computers. You can tell me how pretty your site looks all you want, but I see people getting aggravated or defeated in the face of technology more often than not. AI that communicates to somebody who doesn\u2019t have to worry about it would be great and a huge improvement. Trying to talk to a chatbot will result in these frustrations more often than not. If I\u2019m wrong, I\u2019d be curious if there\u2019s a working counter example. If there is, please comment below!"
    },
    {
        "url": "https://medium.com/singular-distillation/efficiency-efficiency-efficiency-e56f1485d65c",
        "title": "Efficiency, Efficiency, Efficiency \u2013 Singular Distillation \u2013",
        "text": "Data Science & Machine Learning hold the promise of eventually surpassing humans at some specialized tasks by learning from data, a form of past experience. What makes them different from some of the machines of the past is that they do intelligent work. What makes them the same is that they still do repetitious tasks.\n\nThink about the AlphaGo computer/algorithm developed by Google Deepmind. Sure, it can play Go exceedingly well, but it can\u2019t do much else. AlphaGo can\u2019t do my taxes and it can\u2019t even play other games like Backgammon or Chess. The underlying skeleton of the system can likely be transposed to these other tasks, but not without tweaking from a human.\n\nA lot of the tasks people do for cash money are repetitious. Think about a clerk at a company. He looks through, marks, and interprets papers. To be fair, the job isn\u2019t called a clerk anymore. Sometimes its called an analyst or given some fancy title, but at its core its a paper shuffler.\n\nThere\u2019s nothing inherently wrong with this job, except that computers are getting really close to replacing them entirely. [1] Many people in my University class just want a 9am-5pm where they can do some bit of useful work for some organization, go home, raise kids or enjoy a hobby, go to sleep, and then wake up again. Not everybody has the drive to be an entrepreneur or go-getter, which is fine, but these sorts of jobs are disappearing as machine learning and computer-lead algorithms take over.\n\nBut one interesting aspect from an organizational view, is that we can use these new techniques to improve efficiency in our celebrated bureaucracy we call government.\n\nThe old paradigm of government work was as follows: you had a problem that needed some large oversight or active intervention. Let\u2019s say that problem was the poisoning of a water well. The people banded together, elected somebody who then pushed into place an organization with certain powers and called it the Environmental Protection Agency, whose members had a guiding purpose. [2] This involved hiring and teaching people.\n\nThe issue is that people aren\u2019t cheap, especially in government where failure is generally not punished as severely as in the private sector. This means such organizations are expensive to the budget and, more than that, they don\u2019t go away. Very few departments have been closed since the inception of the US government and those that have generally did so because they split into new departments.\n\nThe last few decades have seen a severe restraint on hiring in these governmental bodies. Nobody wants to put more people into the IRS or any other regulatory body. The Republicans dislike it because they don\u2019t want to increase spending and taxing while the Democrats dislike it because they\u2019d rather put the money into new organizations. This creates a constrained resource with some governmental bodies like the FDA sometimes taking years for approval simple because they lack people.\n\nMachine Learning can be used to solve this.\n\nSo many of the tasks involved are inherently rudimentary and just require a rubber stamping of papers. These clear cut cases can be quickly sorted with borderline cases receiving the attention they need. This would save greatly on manpower. This is also just one example. Machines can help streamline all sorts of processes that previously took many many days and weeks of paid man hours to accomplish. As our society grows larger and lines grow longer, machines are helping to usher in an era of post-scarcity. This includes our most valuable asset: Time.\n\nBack in the 1900\u20131920s, America had an Efficiency Movement, which sought to make government work more efficient. A lot of this era was before the income tax, which severely limited the government\u2019s ability to spend. The movement thought that the ills of society could be solved by making it more efficient, specifically through government. It advocated having hosts of experts look at problems and attempt to limit the waste created from them. Its hard not to fall in love with this idea, though that might be the engineer inside of me speaking. When Toyota engineers are able to cut line times down to 18 minutes from 90 minutes, it becomes even harder.\n\nThe reason I bring up this historical anecdote is because spending is limiting right now in this time and Machine Learning can help overcome this. It can help find the inefficiencies encountered in any bureaucracy and correct for them. It can help do tasks faster than humans could ever do. Being able to live one\u2019s life without having to worry about lines or planning out whole days to be spent at the DMV sounds like a dream, except its attainable with these new machines. Computers have long been promised to help us with these sorts of problems and now they finally can. With it, I think we\u2019ll see a return to a new Efficiency Movement.\n\nUntil then, follow me on Twitter & Medium!\n\n[1] I don\u2019t know (nor care) where you stand politically and this essay is not meant to go down that hole. Its meant o expand one\u2019s conscious on what machine learning can do to help us as a society.\n\n[2] A long history could be written here, but as I understand it, the original purpose of the EPA was to save America\u2019s waterways from ruin."
    },
    {
        "url": "https://medium.com/singular-distillation/little-explanations-2-encoding-text-for-recurrent-neural-networks-fcc244e2ae1e",
        "title": "Little Explanations #2: Encoding Text for Recurrent Neural Networks",
        "text": "I\u2019ve been getting interested in text generation recently. With Microsoft\u2019s new AI on Twitter making headlines (and not always for the right reasons), I figured it would be interesting to explain how a basic recurrent neural network accepts input and generates output. I won\u2019t be explaining Recurrent Neural Networks themselves in much detail, just how text input and parsing works for these sorts of networks.\n\nFirst, understanding and parsing text for machine learning has always been a basic goal. Given the pie-in-the-sky dream to build an AI that people can communicate directly with (think Her ), one can understand why this is so paramount. Thus I think it helps to go over one, older method for doing this.\n\nSuppose I fed a sentence to a machine that read \u201cDad had figs for breakfast.\u201d One way we can break this sentence down is into a list of words. If we had a dictionary for all the words in a given corpus of text, a sentence could be represented by the number of times these words exist. This is called Count Vectorization. Documents, which are defined as a given text of words, can then be defined by the frequency of terms. This then becomes a Term Frequency Dictionary. Scikit Learn provides an example of this here.\n\nNow there are some limits to this. For one, temporal data can\u2019t be accepted. The computer won\u2019t know the ordering of words, which might hide some meaning. However, its surprisingly powerful for tasks such as basic sentiment analysis. It also scales really well, so very large documents and texts can be spread across a Hadoop cluster without much difficulty.\n\nRecurrent Neural Networks can bring this up a notch by providing a way to encode for temporal qualities and word ordering. Instead of accepting a row of features and fitting weights to approximate a list of target values, recurrent neural networks accept a stack of features.[1] This gives them a commonality with convolutional neural networks, which also accept a stack of features, such as three color channels for an RGB image. Normal, mulilayer perceptrons (MLP) do not do this.\n\nRecurrent Neural Networks are used in time series analysis quite often. For example, given the past ten data points, predict the next data point. We can extrapolate this example for text generation and identification.\n\nThe above example, drawn by myself, illustrates this. For a phrase \u201cDad had fig[s]\u201d, the network tries to learn the next letter given the previous letter. So its weight will adjust to predict A the next time it encounters a D. Right now, this isn\u2019t a perfect system. For instance, a space follows the second D, which means the weights can\u2019t be too absolute here. At prediction time, the network will output a set of probabilities for each letter. In essence, the recurrent neural network is learning probabilities of letters.\n\nSo, when preparing texts for feeding into a network, we need to find a way to turn letter form data into number form data so our network can understand it. To do so, we reformat the data like below.\n\nEach letter is given as a feature or column. Though hard to explain, the above graph helps to illustrate the point. This example above is admittedly simplified (and you can see why I picked a nonsensical sentence \u201cdad had fig\u201d so I could fit every letter as columns). In practice, one would not only encode for every letter but also for punctuation, new lines, and spaces.\n\nThe network would also not take just the past letter to help with prediction, but the past N letters. These N letters could be anything from 3 up to 50. The only limit is the computational capability available. Increasing the length of the past letters passed in as input will cause the network to take longer to train. It can also be limiting if one\u2019s text corpus is small.\n\nPeople have played around with these systems to product bots that try to mimic donald trump and the bible. They\u2019re very amusing, but one can see the limits of this very simple system. The grammar rarely makes sense and the punctuation is a bit off without human intervention. My example is based on one given in the Keras github.\n\nI myself have been doing much the same. I\u2019ll write out those soon. I have some interesting texts to share I\u2019ve \u201cco-authored\u201d with computers. Until then, follow me on Twitter and Medium!\n\n[1] The term \u201cstack\u201d is not technical, but I find it helps me to visualize how the data is being fed into the system."
    },
    {
        "url": "https://medium.com/singular-distillation/too-many-toolmakers-7cfe1eec4bee",
        "title": "Too many Toolmakers \u2013 Singular Distillation \u2013",
        "text": "The current dilemma of data science is this: too many people are making tools and not enough are making solutions.\n\nAs an example, imagine if you had people making screwdrivers, but nobody making cars. Or how about everybody making hammers, but nobody making houses. Everybody needs cars & houses, not everybody needs screwdrivers and hammers. [1] So why does the data science community seem to think otherwise?\n\nI love the multitudes of new resources, frameworks, and cloud services. I really really do. You can rent GPUs from Amazon for 60 cents an hour to quickly train complex neural networks. You can pick from dozens of libraries to train neural networks. If I had to write my own networks from scratch, I don\u2019t think I\u2019d have a life outside of Kaggle competitions.\n\nI remember first learning about machine learning two years ago and being mystified by deep learning. It all looked like black magic, if only because there were no easy tutorials for it. I still recall the late nights I spent throwing together matrix multiplications in NumPy that carried out the correct operations to do neural networks.\n\nThe reason I handwrote this all was because of the lack of resources. There were very few libraries around, and those that existed were frankly terrible or abstract to the point of being useless to learn from. There was no Keras, no Tensorflow. Caffe was, and remains, difficult to configure. [2] So, as a reflection of my general stubbornness to learn frameworks, I rolled my own. I wrote my own algorithms and functions in NumPy and SciPy. It didn\u2019t run that well to be honest, but it was enough as a proof-of-concept for myself.\n\nNow there\u2019s tons of libraries and support to use. Many people probably only have a passing knowledge of the underlying algorithms. All you need, as far as coding is concerned, is to call \u201c.fit\u201d and then \u201c.predict\u201d in Scikit-Learn. Frameworks like Keras have wrappers for Scikit-Learn, so even with cutting edge neural networks its easy. [3] This is of course a gross simplication, one should know the guts of whats happening, but most can likely convince themselves they\u2019re pros.\n\nThis isn\u2019t even to mention the various services around. Google Predict, Amazon\u2019s competitor, Microsoft Azure, IBM Watson, the list goes on. Even stranger are how similar they all are. Sure, we can mince details to figure out the differences between them, but they all do basically the same thing. With neural networks, they allow one to specify layers, inputs, and how everything connects. With other types of algorithms, you specify your model with a few parameters. Then you train or fit it to some target values. [4] As cool as this all is, what\u2019s the point? To play Go?\n\nI joke somewhat, because I firmly believe in the deepest of hearts that Machine Learning will be a big deal. But I don\u2019t think these companies are approaching it right. They all want to be platforms, but with nobody building anything useful there\u2019s a limited need for this technology right now. Keep in mind that while DOS was the cash cow for Microsoft early on, Commodore, Amstrad, & Acorn were all competitors trying to be platforms too. It took Visicalc (which I wrote about in a past post) to really push the need of computers to everybody. And Visicalc was on every platform. If you ask me, they were the real winners of the early PC era.\n\nWe need solutions so for the love of God, stop building platforms nobody ever uses.\n\n[1] I guess its debatable whether or not everybody doesn\u2019t need screwdrivers. In my opinion, many people probably feel they don\u2019t.\n\n[2] I don\u2019t like Caffe. If you do, that\u2019s your opinion and I respect it. I know many people who do wondrous things with it. I\u2019m just not one of them.\n\n[3] Setup is really difficult with deep learning I find. Its exceedingly difficult especially for the uninitiated and impossible on Windows.\n\n[4] Unless its unsupervised, which is a bit different. I\u2019m just covering myself here."
    },
    {
        "url": "https://medium.com/singular-distillation/little-explanations-1-neural-networks-model-compression-c054b9ab0fb2",
        "title": "Little Explanations #1 \u2014 Neural Networks & Model Compression",
        "text": "This inaugurates my technical explanations section of Singular Distillations. I will be publishing once a year an explanation of some paper I found. Some will be more complex I\u2019m sure, others will be more hands-wavy. I\u2019ll always post the paper in mind at the bottom for all those who are curious.\n\nVery rarely are straight forward models used these days. Gone are the days where a simple least-squares linear regression would be sufficient. Even a random forest or adaboost model is rarely good enough for the job. Instead, when dealing with difficult problems an ensemble or combination of models will be used.\n\nA problem with these ensembles are that they can be too large. I\u2019ve trained ensembles that take up many hundreds of megabytes to store personally, so I imagine the more complex models that Google or Facebook use can take up many gigabytes. While training, this is often fine. Large ensembles can be trained on very powerful computers after all. But when the jump to usability is made, this becomes more problematic. Users expect near-instantaneous results and large ensembles can take a while to make predictions. It hurts even more when trying to shrink down to a mobile device, which has even smaller constraints with regards to performance.\n\nBucila, Caruana, and Niculescu-Mizil at Cornell tried to solve just that problem. They tried to compress their ensembles into smaller but still capable models. To do so, they decided to take advantage of a unique property. Neural networks have a tendency to overfit to data (why they do this is a discussion for another day). Model compression takes advantage of this by overfitting a much smaller network against the results of a larger model.\n\nTo do so, let\u2019s assume we have a very large and complex ensemble of smaller models. Suppose its too large to be useful on a more daily basis. We can overfit a smaller network to this larger model. That\u2019s fine and simple, but there might not be enough cases from which to sufficiently overfit this model. To compensate, we need to generate synthetic data.\n\nSynthetic data can be made in a few different ways. The paper itself goes into a few examples, which you\u2019re welcome to read about, but the important one they mention is Munge. Munge generates new data by sampling from a distribution with a given probability. This is for use in continuous data. For more categorical data, its sufficient to use the existing training dataset for overfitting & compressing the smaller model.\n\nAfter training this data, the prediction of the model is used. It is key not to use the actual target values, which will simply be training a model from scratch. By training to the predicted values, the smaller model will be overfitting to the larger model\u2019s predictions, thereby compressing it.\n\nThis results in compression ratios of 100,000x. I made a little model compression demonstration below."
    },
    {
        "url": "https://medium.com/singular-distillation/hal-won-t-replace-you-but-it-might-replace-your-nine-to-five-job-1248040f3706",
        "title": "HAL won\u2019t replace you \u2014 but it might replace your Nine-to-Five job",
        "text": "edit: I always thought the \u2018M\u2019 in STEM stood for Management, not Mathematics. My mistake, though I believe the article still holds.\n\nThere\u2019s a strong fear people have with AI & Machine Learning. That fear is that it might replace them. With Alphabet\u2019s Deep Mind defeating Lee Sedol, computers seem to be inching towards replacing us at an alarming rate. It seems like everyday I read some hit piece on progress, that we\u2019re all going to be replaced by really smart AI. These are misinformed at best, malicious at worst.\n\nI am here to tell you that these fears are entirely unfounded. In particular, the University-educated seem scared. Computers will replace some jobs, but they won\u2019t replace all of them either. I\u2019ll start with those they won\u2019t.\n\nComputers will not replace those in (most) STEM fields. These problem-solving focused degrees and majors are simply too complex at the moment for computers to replace outright. A computer or algorithm can be a solution to a problem, but it cannot solve a problem on its own and its highly unlikely that this will change in the future. Machine Learning algorithms infer some prediction from a large mass of data, producing a model to make these guesses. Mathematical models have existed long before computers (see least-squares linear regression), its just that now they\u2019ve simply gotten far more complex with the advent of the silicon revolution.\n\nWhat is at risk are those who major in humanities[1] degrees. Many who major in such areas are preparing themselves professionally for socially outward careers such as lawyers, politicians, spokesmen, and marketers. Those people will be fine, for computers won\u2019t be replacing human social etiquette nor will it be replacing creative work (i.e. marketing and copywriting). But many also major in such degrees to get \u201coffice work.\u201d What constitutes office work is very broad, but to me include doing banking credit checking, secretarial work, HR, and bureaucratic paper shuffling. [2] These sorts of jobs require memorization of some pattern and routine execution of work.\n\nThis is precisely what computers are good at.\n\nFor many kids, going to University feels like a requirement. Many students go to University, pay 30k-50k a year to study whatever their heart desires, then graduate and go into a workplace where they will be very soon obsolete. The jobs they are being prepared for are soon not going to be there.\n\nBut such jobs have been going away for a while. Getting a humanities degree on its own is borderline useless and has become increasingly so, unless you are doing it for networking, personal fulfillment, or for law school. Machine learning and AI aren\u2019t helping the problem, but they aren\u2019t introducing it either. There\u2019s simply too many humanities majors out there relative to the useful work for them to do. Simple economics dictates that too much supply will cause a depreciation of price. Again, if you go to school for personal fulfillment, networking with others, or law school, it might not be a waste, but many also go because they feel its a requirement to enter into a high-paying job and have a good future.\n\nHowever, that\u2019s not to say all STEM degrees are safe. Pharmacist Techs are also going to lose their jobs, or at the least experience mass unemployment. Many biology majors are at risk as well (if they want to work in \u201ctraditional\u201d biology careers). These sorts of jobs require memorization, which computers can outdo humans on. Its not a large jump for computers to begin doing such work as is, let alone in the future where they will be more advanced [4].\n\nI\u2019m not alone in thinking this either. The St. Louis Fed published an essay on exactly this phenomenon.\n\nThis ties into the temp or contractor work force future that companies like Uber seem to be heralding. If you have the creative and intellectual problem solving skills, you\u2019ll be needed in the future and will always have a job. But if you want to work a steady Nine-to-Five job and experience traditional \u201cwork-life balance\u201d those opportunities are drying up. This is the larger trend.\n\nI think a lot of people are scared of the future because they don\u2019t understand it and feel they never can. But the knowledge is there. As human beings, we can all read and reason. We can dig through large stacks of books, large troves of knowledge, and sit down and digest. That\u2019s the true beauty of the world we live in today, one of almost infinite knowledge. To hold everything back because we\u2019re uncomfortable is suicide for our country and our society at large.\n\nOf course, this essay can divert off into a myriad of topics, but for now I\u2019ll end it here. I wanted to provide counterbalance as I seem to read some \u201cwatch out for skynet!\u201d piece everyday, usually written by somebody whose terribly misinformed and engaging in wild speculation. If you like this essay, you\u2019ll probably like what else I\u2019ve got to publish. Subscribe to my publication on Medium to keep with my writings here on Singular Distillations.\n\n[1] I realize that humanities refers to very specific degrees. In context here, I refer to any non-STEM degree. STEM stands for science, technology, engineering, and management. Thus, political science is different from chemistry for the purposes of this essay.\n\n[2] If I come up with a better word, I\u2019ll replace bureaucratic. I don\u2019t mean to give off a negative connotation, but I don\u2019t know a better way to word it at the moment.\n\n[3] Welders, plumbers, handymen, home contractors, etc. These jobs aren\u2019t going away either.\n\n[4] Part of me wonders why they haven\u2019t been replaced already frankly."
    },
    {
        "url": "https://medium.com/singular-distillation/the-next-big-machine-learning-startup-will-look-a-lot-like-visicalc-68c4eb5b6738",
        "title": "The Next Big Machine Learning Startup Will Look a Lot like VisiCalc",
        "text": "My inaugural post for Singular Distillation! I hope you all like it, I will be trying to keep to a twice-weekly schedule, if not thrice-weekly. Connect with my on Twitter < @vasaulys > or here on Medium.\n\nDoes anybody remember VisiCalc? They seem to be a causality of the early personal computing era. Yet they are arguably one of the more important software companies of history.\n\nWay back in the dark ages of 1977, Apple put out the Apple II. Commonly referred to as the first personal computer, it set the standard for home computing by building a (relatively) affordable machine. That part most people know. What most people forget is that the Apple II didn\u2019t sell very well at first. Piecing the numbers together from a source, it seems apple sold at most 10,000 units its first year. [1]\n\nBut in 1979, that all changed. VisiCalc was released. Although its \u201conly a spreadsheet\u201d, it was a real breakthrough because it gave people a strong incentive to buy a computer. Spreadsheets prior to computers were a pain to calculate, even with calculators. The magic with VisiCalc was that cells could be written as formulas that relied on other cells; if those relied-on cells changed, then the formula\u2019s answer changed.\n\nIt was a simple idea, but it a powerful one. By 1982, Apple would sell 750,000 Apple II units and VisiCalc would port its software to other platforms, achieving a million in sales by 1986.\n\nThe power behind it, and why it was so crucial to the Apple II, was because it did something that was not previously possible. Well, technically it was possible, it you were IBM with a small army of MBAs. But VisiCalc, coupled with the Apple II, put this power in the hands of small business owners.\n\nThis is what Machine Learning needs. There are many many platforms and tools but no products or solutions. Companies are racing to build the next platform, but nobody\u2019s racing to build the product. Somebody needs to be building the next VisiCalc. They need to be build the product that wasn\u2019t previously possible. Machine Learning is unlocking potential not previously thought of as realistically possible.\n\nSo forget your new neural network standards and algorithms, show me your cutting edge can-use-in-my-life product. That\u2019s what\u2019s needed right now."
    }
]