[
    {
        "url": "https://towardsdatascience.com/using-word2vec-for-music-recommendations-bb9649ac2484?source=user_profile---------1----------------",
        "title": "Using Word2vec for Music Recommendations \u2013",
        "text": "How we use neural networks to transform billions of streams into better recommendations.\n\nStreaming services have changed the way in which we experience content. While recommendation systems previously focused on presenting you with content you might want to purchase for later consumption, modern streaming platforms have to focus instead on recommending content you can, and will want to, enjoy in the moment. Since any piece of content is immediately accessible, the streaming model enables new methods of discovery in the form of personalized radios or recommendation playlists, in which the focus is now more on generating sequences of similar songs that go well together.\n\nWith now over 700 million songs streamed every month, Anghami is the leading music streaming platform in the MENA region. What this also means, is that the amount of data generated by all those streams proves to be an invaluable training set that we can use to teach machine learning models to better understand user tastes, and improve our music recommendations.\n\nIn this article, I will be presenting a neural network approach that we use to extract song embeddings from the wealth of streaming data that we have, and how we use this model to generate relevant recommendations.\n\nWith enough data, collaborative filtering systems turn out to be effective at recommending relevant items. The basic idea behind collaborative filtering is that if user 1 likes artists A & B, and user 2 likes artists A, B & C, then it is likely that user 1 will also be interested in artist C.\n\nObserving global song preferences across all users and applying classic collaborative filtering approaches such as Matrix Factorization on a user-item rating matrix gives us valuable information about how groups of songs could be related. So if a group of users have a large set of common songs they like, we can infer that those are users who have very similar tastes in music, and the songs they listen to are similar to each other.\n\nThose global co-occurrences across multiple users therefore give us valuable information about how songs are related; however, one thing they do not capture is how songs could be locally co-occurring in time. So they might tell us that users who like song A would also probably like song B, but would they have listened to them in the same playlist or radio? We can therefore see the benefit of looking at not just what songs users play across their lifetimes, but at what context they play those songs in. Or in other words, what other songs do they also play before or after them during the same session?\n\nSo what we\u2019re interested in is a model that can capture not only what songs are similar people generally interested in, but what songs are listened to frequently together in very similar contexts. And this is where Word2vec comes in.\n\nWord2vec is a class of neural network models that were initially introduced for learning word embeddings that are highly useful for Natural Language Processing tasks. In recent years, the technique has also been applied to more general machine learning problems including product recommendations. The neural network takes in a large corpus of text, analyzes it, and for each word in the vocabulary, generates a vector of numbers that represent that word. Those vectors of numbers are what we are after, because as we\u2019ll see, they encode important information about the meaning of the word in relation to the context in which it appears.\n\nThere are two main models defined: the Continuous Bag-of-Words model and the Skip-gram model. For the rest of this discussion, we will restrict ourselves to the Skip-gram model as this is the one that we use.\n\nThe Word2vec Skip-gram model is a shallow neural network with a single hidden layer that takes in a word as input and tries to predict the context of words around it as output. Let\u2019s take the following sentence as an example:\n\nIn the sentence above, the word \u2018back-alleys\u2019 is our current input word, and the words \u2018little\u2019, \u2018dark\u2019, \u2018behind\u2019 and \u2018the\u2019 are the output words that we would want to predict given that input word. Our neural network looks something like this:\n\nW1 and W2 represent weight matrices that control the weights of the successive transformations we apply on our input to get the output. Training the neural network consists of learning the values of those weight matrices that give us an output that is closest to the training data provided.\n\nGiven an input word, we do a first forward propagation pass through the network to get the probability that the output word is the one we expect according to our training data. Since we know with certainty what words we expect at the output, we can measure the error in the prediction, and propagate that error back through the network using backpropagation and adjust the weights through stochastic gradient descent. Through this step, we modify the values of W1 and W2 slightly, so they can more accurately predict the output words we want given that example input word. When we\u2019re done with this step, we move our context window to the next word and repeat the above steps again.\n\nWe repeat the procedure above for all the sentences in the training set. When we\u2019re done, the values of the weight matrices would have converged to the ones that produce the most accurate predictions.\n\nHere\u2019s the interesting part: if two different words largely appear in similar contexts, we expect that, given any one of those two words as input, the neural network will output very similar predictions as output. And we have previously mentioned that the values of the weight matrices control the predictions at the output, so if two words appear in similar contexts, we expect the weight matrix values for those two words to be largely similar.\n\nIn particular, the weight matrix W1 is a matrix that has as many rows as there are words in our vocabulary, with each row holding the weights associated with a particular word. Therefore, since similar words need to output similar predictions, their rows in the matrix W1 should be similar. The weights associated with each word in this matrix are the \u2018embeddings\u2019 that we are going to use to represent that word."
    }
]