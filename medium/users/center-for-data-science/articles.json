[
    {
        "url": "https://medium.com/center-for-data-science/car-talk-will-teaching-vehicles-to-communicate-with-each-other-reduce-accidents-45adb4548e11?source=---------0",
        "title": "Car talk: will teaching vehicles to communicate with each other reduce accidents?",
        "text": "While modern technology has helped us build robust accident-avoidance systems for cars, we\u2019re still witnessing a high volume of highway related fatalities. Human error often plays a role in these accidents, which is why scientists are continually developing new solutions to help cars compensate for our blind spots.\n\nTo improve road safety, CDS\u2019s Kyunghyun Cho, Assistant Professor of Computer Science and Data Science, along with researchers from NYU and FAIR (Facebook Artificial Intelligence Research), have recently been exploring the possibility of teaching vehicles how to communicate with each other.\n\nTheir research is timely since the US Department of Transportation (DOT) has recently proposed a rule that requires all new cars to have vehicle-to-vehicle communication capabilities by 2023.\n\nThe details of the DOT\u2019s proposal, however, remain unclear since we have not yet discovered exactly what type of information cars would need to send each other. Speed? Acceleration? Location? Moreover, while teaching vehicles to talk to each other seems to be a theoretically sound idea, would such a mechanism have a meaningful impact on road safety?\n\nTo begin answering these questions, the researchers used Box2D, a Python gaming framework, to simulate highway scenarios where cars are capable of direct vehicle-to-vehicle communication (V2V), and can transmit three specific types of information to each other: acceleration, steering wheel angle, and brake.\n\nTo gauge how effective V2V actually is, the researchers also used the simulation to construct baseline scenarios (where cars are not capable of communicating with each other), so that they could compare the success rate of cars with V2V and those without.\n\nIn the simulation, all the cars are aiming to drive as fast as possible to a highway exit without crashing into each other.\n\nAs the researchers were also working within the context of reinforcement learning (a machine learning technique that develops models by training them repeatedly on a scenario so that they improve with each iteration), they trained their cars under two different weather conditions \u2014 sunny and foggy.\n\n\u201cOur current results,\u201d the researchers reported, \u201csuggest that the proposed protocol will in fact make the road safer in adverse conditions and enable cars to go faster.\u201d\n\nOf course, the researchers note that a major drawback to their simulation is that it is \u201ca limited interpretation of actual driving scenarios.\u201d Yet, their work confirms is that V2V capabilities can become effective tools for improving road safety. After all, as the researchers suggest, developing V2V capabilities may lead cars to discover better solutions that scientists themselves have not even thought of.\n\n\u201cWe have seen this recently with AlphaGo finding moves in Weiqi that human players had not discovered after many centuries,\u201d the researchers point out. \u201cIn future research, we will compare the model\u2019s learned policies to fixed safety responses.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/financial-stress-testing-powered-by-machine-learning-89fd3e645da5?source=---------1",
        "title": "Financial Stress Testing Powered by Machine Learning",
        "text": "In the wake of the 2008 financial crisis, stress testing became a popular way to measure the strength of financial institutions and instruments. Stress testing analyzes or simulates rare, catastrophic scenarios and determines the response of specific portfolios or institutions. While many approaches to stress testing exist, they usually follow the same two fundamental steps\u2014the generation of the stressed scenario, and then the portfolio/balance sheet projection after the scenario.\n\nCDS\u2019s Bud Mishra, with fellow researchers, has developed a novel stress testing method and algorithm using Suppes Bayes Causal Networks (SBCNs). Mishra\u2019s method differentiates itself from existing methods by discovering the causality structure of financial events. This enables analysts to glean direct and ancillary causes of financial losses from the results of stress tests using SBCNs. Mishra\u2019s SCBNs establish a causality structure by incorporating causality theory which stipulates that the cause of an event must occur before the event (also known as temporal priority).\n\nThis means that the network is arranged according to an intuitive time flow of events. Component branches of the network are also classified as \u201crisky\u201d or \u201cprofitable,\u201d which produces decision trees to reduce computational complexity. Machine learning tools allow the system to identify which scenarios would be most detrimental to specific portfolios and further mitigate the computational complexity of these types of problems.\n\nMishra\u2019s SBCNs are trained on simulated historical data to combine three widely-used stress testing scenario generation methods (historical, hypothetical, and portfolio-specific). Mishra evaluated the performance of his method by embedding causal relationships in the simulated data and determining the capabilities of an SBCN to identify those causes. The model initially returned high numbers of false positives and negatives, but after \u201cbootstrapping\u201d (repeating many times) datasets the SBCNs far outperformed traditional Bayesian Networks.\n\nDespite the advanced machine learning capabilities embedded in their method, however, the role of experts is still important. The researchers note that SBCNs can identify stressed scenarios that would cause adverse financial results, but experts are needed to determine which of those scenarios are actually plausible. Experts can also use SBCNs to test a particular stressed scenario.\n\nMishra and collaborators plan to test their algorithms on real data and compare the results with the research of human experts. Their new approach to stress testing offers financial institutions an efficient approach to fulfill federal and international regulations as well as their own internal evaluations."
    },
    {
        "url": "https://medium.com/center-for-data-science/dr-julia-kempe-appointed-as-director-of-the-nyu-center-for-data-science-9b401bd73ee1?source=---------2",
        "title": "Dr. Julia Kempe appointed as Director of the NYU Center for Data Science",
        "text": "We are pleased to announce that Julia Kempe has been appointed as the Director of the NYU Center for Data Science.\n\nDr. Kempe comes with two decades of in-depth experience in both academia and data-driven industry. She is currently a Senior Researcher at a leading hedge fund that trades in markets around the world, employing complex mathematical models and tools from machine learning and statistics to analyze vast amounts of data.\n\nPrior to working in finance, she held academic appointments as Junior and then Senior Researcher at the CNRS in France since 2001, and as Professor of Computer Science at Tel-Aviv University (2007\u20132011).\n\nHer main scholarly contributions are in quantum computing, a highly interdisciplinary area spanning computer science, physics, and applied mathematics, and she has published in conference proceedings and journals across these fields.\n\nShe has been a Principal Investigator (PI) or co-PI on a number of prestigious grants from the European Research Council, the Israeli Science Foundation, and the French Ministry of Science. In 2010, she became one of the youngest scientists to receive knighthood in the French Order of Merit. She is also the recipient of a French Woman in Gold Prize for Research (\u201cFemme en Or de la Recherche\u201d), the Krill Prize for Excellence in Scientific Research, the Ir\u00e8ne Joliot-Curie Prize of the French government, and the Bronze Medal of the CNRS.\n\nDr. Kempe received a Ph.D. in Mathematics from the University of California, Berkeley and a Ph.D. in Computer Science from \u00c9cole Nationale Sup\u00e9rieure des T\u00e9l\u00e9communications. She also holds an M.S. in Theoretical Physics from the \u00c9cole Normale Sup\u00e9rieure and an M.S. in Algebra from the Universit\u00e9 de Paris 6. She completed her undergraduate work in Mathematics and Physics at the University of Vienna, Austria.\n\nWe also take this opportunity to thank Richard Bonneau, Professor of Biology and Computer Science, who served as Director of CDS since September 2017, for his excellent leadership, energy, and dedication to the Center.\n\nPlease join us in congratulating Julia Kempe on her appointment and welcoming her to NYU."
    },
    {
        "url": "https://medium.com/center-for-data-science/the-state-of-disinformation-on-social-media-397d3c30f56a?source=---------3",
        "title": "The State of Disinformation on Social Media \u2013 Center for Data Science \u2013",
        "text": "CDS affiliated faculty members and researchers from NYU\u2019s SMaPP lab have recently produced a report reviewing current research on how fake news, rumors, deliberately incorrect information\u2014all of which is encompassed by the term \u201cdisinformation\u201d\u2014spreads on social media, and how this online phenomenon affects political life.\n\nThese are central questions that the authors tackle in the review. Doctoral candidates Sergey Sanovich and Denis Stukal from the SMaPP lab, for example, examine the tactics and characteristics behind it. Key tactics include selective censorship, used often by the Chinese government; hacking and leaking sensitive information, such as the email hack of the Democratic National Committee; the manipulation of search algorithms using popular keywords and circuitous links; and the use of bots and trolls to directly share info.\n\nIt\u2019s common knowledge that disinformation proliferated on social media during the 2016 U.S. election, but researchers are only recently beginning to understand the full scope of the impact and the conditions that allowed such disinformation to flow. Sanovich and Stukal\u2019s review includes a report that 400,000 bots posted 3.8 million tweets during the final month of 2016 election. And, during the final three months, users engaged more with the top twenty fake news stories than they did with authenticated ones.\n\nEmerging research demonstrates the startling degree to which bots can deceive users. One study found that 30\u201340% of automated texts on factual topics deceive ordinary users and 15\u201325% deceive experts; for non-factual topics, ordinary users were deceived by 60% of automated information and experts by 30%. The study also showed that information disliked by the crowd has a 10\u201315% higher deception rate for both ordinary users and experts, making politicized disinformation especially potent.\n\nHumans are clearly having a hard time identifying bots\u2014so could automated systems do it?\n\nThe answer is yes\u2014sort of.\n\nBot detection algorithms are typically trained to correctly classify bots within a certain domain; it can be challenging for these systems to identify bots outside their domains. Furthermore, according to a 2017 study, these algorithms degrade in accuracy over time by up to 20% in one year as bots evolve.\n\nSanovich and Stukal identify dependence on ad revenue and optimization algorithms as the two key characteristics that make social media inherently susceptible to disinformation campaigns. The U.S. government does not regulate who can and cannot advertise on social media. Consequently, one report says Twitter offered the Russian state-supported media network RT $3 million for 15% of its U.S. elections advertising; another report says Facebook avoided screening advertisers and allowed ads to be paid for in Russian rubles. Engagement optimization algorithms further exacerbate the problem by incentivizing sensational images and headlines.\n\nSo what can be done? Sanovich and Stukal consider methods for sites to verify stories through automated or manual methods, but they fear that verification could suffer from errors, bias, and have unintended consequences including censorship. The problem of disinformation on social media platforms remains very much an open one \u2014 a fundamental concern of contemporary society whose answer may lie in data science."
    },
    {
        "url": "https://medium.com/center-for-data-science/can-neighborhoods-be-defined-by-attitudes-towards-racism-and-homophobia-on-social-media-c2ad0208782a?source=---------4",
        "title": "Can Neighborhoods be Defined by Attitudes Towards Racism and Homophobia on Social Media?",
        "text": "Our surroundings, especially in dense urban spaces, mold the future conditions of our health, personal economy, and social well-being. From a public health perspective, identifying neighborhoods where discrimination and prejudice are more prevalent can help reduce risk factors for disease and poverty on the local level.\n\nTo accomplish this task, Kunal Relia, Mohammad Akbari, and CDS affiliated faculty members Rumi Chunara and Dustin T. Duncan have developed a new method for assessing how an individual\u2019s exposure to prejudice varies among neighborhoods in New York City based on Twitter. The researchers focus particularly on how men who have sex with other men are affected by homophobia and racism, analyzing mobility data from a cohort of 226 men between January and November 2017.\n\nThe research team\u2019s method, called socio-spatial self-organizing maps (SS-SOMs), uses an algorithm to divide the city into a controlled number of contiguous, non-overlapping clusters based on shared attitudes. (Traditional SOMs do not put constraints on the number of clusters or require clusters to not overlap.) SS-SOMs employ neural embedding to classify text from tweets and neural networks to cluster areas with consistent social attitudes.\n\nTweets were sourced from Twitter\u2019s API for all five boroughs of New York within the same time frame as the mobility data collected from the male cohort, yielding over six million unique tweets, 8,000 of which were used as training data. In a process called neural embedding, neural learning models, trained with keywords and human-labelled examples of racist or homophobic tweets, were then used to semantically determine the probability that a tweet was racist or homophobic.\n\nBased on the results from neural embedding and the researchers\u2019 division of New York by gradations of latitude and longitude to produce 72,484 grids, the SS-SOM algorithm formed 94 grid clusters for racism and 102 for homophobia. Compared to ZIP codes and traditional SOMs, which may overlap and produce too many areas, SS-SOMs produced clusters that contained grids with more similar social attitudes, and both SS-SOMs and traditional SOMs had lower variance between clusters than ZIP codes.\n\nThrough a comparison of racist and homophobic tweets between ZIP codes and SS-SOMs, the researchers found that a third of the cohort of men who have sex with men experienced a difference of more than 50% for racism in ZIP codes versus SS-SOMs. The researchers also found that an increase in racism was associated with an increase in homophobia, but not the opposite.\n\nAn important opportunity for future study will be further linking the online examples of racism and homophobia on Twitter to offline experiences in the same areas. The researchers suggest surveys, more in-depth examinations of how mobility affects experiences of racism and homophobia, and the possibility that social media could be used to define the boundaries of neighborhoods."
    },
    {
        "url": "https://medium.com/center-for-data-science/how-natural-language-inference-models-game-the-task-of-learning-61d2f744955c?source=---------5",
        "title": "How Natural Language Inference Models \u201cGame\u201d the Task of Learning",
        "text": "The goal of natural language inference (NLI), a widely-studied natural language processing task, is to determine if one given statement (a premise) semantically entails another given statement (a hypothesis). To generate data for this task in the past, CDS Faculty Member Sam Bowman orchestrated the crowdsourcing of two large-scale datasets containing human-annotated inferences, known as SNLI and MultiNLI. Bowman directed annotators to respond to a given premise with three hypotheses: one entailment, one neutral, and one contradiction.\n\nAn example of annotations from Bowman\u2019s SNLI dataset:\n\nTypically, a text classification model would be given the premise and asked to classify these crowdsourced hypotheses as entailment, neutral, or contradiction. But in a new study, Bowman, with a team of researchers, explored whether a text classification model can classify human-generated hypotheses without being given the premise by relying on annotation artifacts instead.\n\nAnnotation artifacts are unintentional patterns in the dataset left by human crowd workers that can signal a hypothesis\u2019 type. The researchers, through a statistical analysis, found that generic words (animal, instrument, outdoors) were associated with entailed hypotheses; modifiers (tall, sad, popular) and superlatives (first, favorite, most) with neutral hypotheses; and negative words (no, nobody, never, nothing) with contradictory hypotheses. Sentence length also proved to be an important artifact: entailments often contained fewer words than neutral hypotheses.\n\nBowman and collaborators used a text classifier called fastText to determine the effect of these artifacts on text classification. They found that, without access to the premise, fastText was able to correctly classify 67% of the hypotheses in the SNLI dataset and over 50% of the hypotheses in the MultiNLI dataset, well above a baseline measure. These results demonstrate the high degree to which text classifiers rely on annotation artifacts.\n\nThe researchers note, \u201cThis raises an important question about state-of-the-art NLI models: to what extent are they \u2018gaming\u2019 the task by learning to detect annotation artifacts?\u201d They addressed this question by training three high-end NLI models on the SNLI and MultiSNLI datasets. The three NLI models were then evaluated on the full datasets, hard datasets (the hypotheses which fastText incorrectly identified), and easy datasets (the hypotheses which fastText correctly identified).\n\nThese NLI models performed significantly better on the easy datasets, exposing the extent to which they leveraged annotation artifacts. Based on these results, Bowman and his collaborators concluded that annotation artifacts inflate model performance. To mitigate models\u2019 reliance on annotation artifacts in the future, Bowman suggests artifacts be balanced across classes. His work reveals that, despite recent reported progress, natural language inference is still very much an open problem."
    },
    {
        "url": "https://medium.com/center-for-data-science/mining-wechat-to-understand-the-chinese-diaspora-8d78b2f3a900?source=---------6",
        "title": "Mining WeChat to Understand the Chinese Diaspora \u2013 Center for Data Science \u2013",
        "text": "Diaspora research traditionally relies on data gathered through censuses and sample surveys \u2014 cumbersome methods hindered by bias, limited geographic scope, low response rates, and difficulty obtaining current information. The next wave of diaspora research, however, will rely on data from location-based apps. At the forefront of this wave is Keith Ross, who, with a team of researchers from NYU Shanghai, has imagined a new methodology for collecting and analyzing diaspora data.\n\nRoss\u2019s method looks to country-centric mobile apps (primarily used by residents of a specific country) for precise spatial and temporal data. Ross and collaborators present a case study for their method that examines the ethnic Chinese diaspora through WeChat (a messaging app with over 1.1 billion users), but they argue that the method can be applied to any ethnic diaspora using various country-centric apps.\n\nTo verify users as ethnic Chinese, three human annotators manually examined each user\u2019s language, posts, photos, and bios. Ross and collaborators note that users of WeChat\u2019s People Nearby service are typically 18 to 30 years old, rendering People Nearby a particularly valuable data source for gaining insight into immigration activity of younger generations.\n\nAlong with their analysis of individual user data through WeChat, the researchers utilized Google Places API to identify the number of Chinese business establishments in the same thirty-two cities. Data regarding established businesses reflects immigration flows of older generations and allows useful comparison with recent immigration flows measured through WeChat.\n\nThe case study revealed that of the thirty-two cities sampled, the highest number of ethnic-Chinese WeChat users reside in Prato, Italy (an expected result given that Prato has the largest concentration of ethnic Chinese in Europe), and the lowest number reside in Anchorage, Alaska.\n\nInterestingly, the WeChat sample yielded a relatively low number of ethnic Chinese users in San Francisco, a city where 21% of the population is of Chinese descent according to the 2010 U.S. census. The researchers attribute this aberration to the possibilities that many ethnic-Chinese people in San Francisco may be older than 40, live beyond 2,000 meters of town hall, or not use WeChat.\n\nDespite any limitations, the WeChat case study reveals the value in mining country-centric apps to understand these emerging diasporic populations. As diaspora research continues, country-centric apps will continue to offer data that is more current, comprehensive, and accessible than traditional sample surveys or censuses."
    },
    {
        "url": "https://medium.com/center-for-data-science/capital-one-becomes-founding-partner-of-the-diversity-initiative-at-nyu-cds-b824d4928cf?source=---------7",
        "title": "Capital One becomes founding partner of the Diversity Initiative at NYU CDS",
        "text": "The NYU Center for Data Science is a focal point for New York University\u2019s university-wide initiative in data science and statistics. The Center was established to help advance NYU\u2019s goal of creating the country\u2019s leading data science training and research facilities, and arming researchers and professionals with tools to harness the power of big data. As part of its mission, The Center for Data Science (CDS) has set out to build a culture that embraces inclusion and diversity. These ideals are indispensable for advancing innovation and knowledge within the fields of technology, engineering, mathematics, and sciences. \n\n \n\n Our Diversity Initiative was established to catalyze greater diversity and broader representation in data science; we believe the inclusion of students differing from all backgrounds leads to a higher quality of debate and ideas that may not have been reached otherwise. We are thrilled to announce that Capital One has become one of the founding partners for this initiative. With their support, our objective to increase diversity in data science will be accomplished through the following: providing scholarships and financial support to students, funding research projects, providing travel funds to disseminate research at conferences and seminars, and hosting seminars to bring data scientists to CDS.\n\nAdam Wenchel, Vice President of Machine Learning & Data Innovation and co-lead in Capital One\u2019s Center for Machine Learning, an in-house center of excellence for machine learning, noted, \u201cNYU Center for Data Science\u2019s Diversity Initiative is an important endeavor that we believe will contribute a diverse range of talent, background, perspective, and expertise to the field, and we are proud to support and participate in the program. As the technical applications of data science, statistics, and machine learning become more ubiquitous throughout industry and society, it is imperative to foster a well-informed, inclusive, and multi-faceted populace of practitioners that will create the breakthrough discoveries of tomorrow.\u201d\n\nWhile our partnership with Capital One has just kicked off, we\u2019ve already been hard at work hosting planning meetings and research exchanges, discussing capstone projects, and planning for core areas of research that we look forward to advancing in the year to come. Specific areas of research the team will explore include mathematical statistics, computational statistics and machine learning, optimization and large-scale computation, system design for large-scale data science, data visualization and other subsets of machine learning such as deep reinforcement learning.\n\nWe are looking forward to an exciting future and to the many things we will accomplish together as the program progresses."
    },
    {
        "url": "https://medium.com/center-for-data-science/samsung-and-cds-research-day-94d8c30d72b2?source=---------8",
        "title": "Samsung and CDS Research Day \u2013 Center for Data Science \u2013",
        "text": "Spring is traditionally known as the season of rebirth, rejuvenation, and growth. Trees shrug off the long winter and stretch their arms towards the blue sky, flowers bloom, and birds are all a-flutter with joy \u2014 in every place but New York, it seems! Spring has been slow to grace the Big Apple this year, but as we wait for her to arrive, a spring-like atmosphere of new research activity, collaborations, and partnerships is flourishing at CDS. Just last Friday, our faculty met with employees from the Samsung Research Center to share recent developments in the field and organize future research collaborations between the two institutions.\n\nCDS Director Richard Bonneau kicked off the day with an introduction about CDS, followed by introductions about the Samsung Research and AI Center, which was launched in 2017 after combining the organization\u2019s Software R&D Center and Digital Media & Communications Center. Key areas of focus for Samsung\u2019s new research hub include the Internet of Things (IOT) and artificial intelligence. These research themes are, of course, also near and dear to CDS faculty and students, as was demonstrated through several topic presentations for Samsung employees after the introductions.\n\nFor example, Joan Bruna, Assistant Professor of Mathematics and Data Science (and recent recipient of the prestigious Sloan Foundation Research Fellowship!), presented on his work with graph neural networks, a powerful technique which that has been used to make significant advancements in areas like particle physics, quantum chemistry, and recommendation systems.\n\nBruna was followed by Sam Bowman, Assistant Professor of Data Science and Linguistics, who explained his research on using latent tree learning to teach machines to understand sentences to a meaningful depth \u2014 a crucial feature for those who are hoping to invent personal assistant robots like Siri and Alexa, or to computationally perform translations, summarizations, sentiment analysis. Similarly, Kyunghyun Cho, Assistant Professor of Computer Science and Data Science (and CIFAR Azrieli Global Scholar), demonstrated how he has been using neural networks to improve multilingual machine translations.\n\nAfter a hearty lunch, student researchers who have been working under the guidance of CDS professors provided detailed explanations about how they have applied new approaches and methodologies to specific projects. Bruna\u2019s graduate student researcher, Nicholas Choma, for example, revealed how he has been using graph neural networks for IceCube, a neutrino observatory whose goal is to detect high-energy neutrinos (they are specific subatomic particles) that originate from black holes.\n\nBowman\u2019s graduate student researcher, Nikita Nangia, showed how she created ListOps, a diagnostic dataset for latent tree learning, while Cho\u2019s graduate student researchers Jason Lee and Elman Mansimov highlighted their work on non-autoregressive sequence models for machine translations for texts, as well as audio and image captioning (their model, by the way, boasts a 90% translation rate!)\n\nAs a whole, the day clearly showed that Samsung and CDS have much to gain from each other. New research collaborations between us are hovering on the horizon \u2014 and we here at CDS could not be more excited to see what the future holds."
    },
    {
        "url": "https://medium.com/center-for-data-science/how-machine-learning-can-aid-conservation-of-migratory-birds-6be2e987bf58?source=---------9",
        "title": "How Machine Learning Can Aid Conservation of Migratory Birds",
        "text": "Climate change, habitat loss, and human impact on the environment all pose threats to migratory birds. Consequently, avian conservation efforts depend on understanding the spatial and temporal distributions of bird populations. But since most bird populations migrate at night, avian researchers must typically rely on sound rather than sight. Human observation of nocturnal migratory birds, however, can be cumbersome and impractical.\n\nTo aid conservation efforts, CDS\u2019 Juan P. Bello hopes that machine learning can automate the observation of migratory birds. While some automated methods already exist for recording and detecting nocturnal migratory bird calls, existing datasets are not well suited for machine learning research.\n\nWith human annotations identifying flight calls from the full night recordings as a benchmark, Bello and collaborators trained four systems to detect bird flight calls: two energy-based detectors, a shallow learning pipeline, and a convolutional neural network.\n\nTo evaluate the accuracy of each system, the researchers balanced the dataset with 35,000 additional negative clips. They tested each method\u2019s event detection capabilities on six full night recordings. The convolutional neural network outperformed the other methods with a 95% binary accuracy when augmented with additions of background noise, pitch transpositions, and time stretching.\n\nThe researchers point out that BirdVox-full-night, despite its state of the art capabilities, does have difficulty detecting rare flight calls due to its training. To mitigate this bias, BirdVox-full-night includes a test bed to allow for context-adaptive machine listening.\n\nBello and his team have released the BirdVox-full-night dataset through a Creative Commons license for fellow researchers and conservationists to access. As the first dataset of full night migratory bird recordings annotated by time and frequency, BirdVox-full-night will be a crucial tool for future avian conservationists."
    },
    {
        "url": "https://medium.com/center-for-data-science/informing-e-cigarette-public-health-policy-with-data-science-589f847a93c6",
        "title": "Informing E-cigarette Public Health Policy with Data Science",
        "text": "The U.S. did not officially recognize the health risks of tobacco until the surgeon general\u2019s report in 1964, with the first regulations on cigarettes imposed soon thereafter. Today public health officials have to contend with a new nicotine delivery method: e-cigarettes.\n\nResearchers have struggled to keep up with the rapid expansion of these products. After all, between 2012 and 2014 alone, the number of e-cigarette brands in U.S. markets increased from 250 to 460. To better understand e-cigarette prevalence by state and how prevalence is affected by cigarette regulation, Dustin T. Duncan, an affiliated faculty member at CDS and Assistant Professor of Population Health, has contributed to a new study analyzing 2012\u20132014 data from the National Adult Tobacco Survey (NATS) and the American Lung Association\u2019s State of Tobacco Control (SOTC).\n\nThe NATS collected thousands of responses through a national phone survey to determine cigarette and e-cigarette usage rates in all fifty states and the District of Columbia. The SOTC report evaluated the efficacy of state tobacco control policies based on four measures: smoke free air laws, available resources for people who want to quit smoking, tobacco control spending, and cigarette excise tax.\n\nUsing data from these two sources and accounting for ethnic and socioeconomic demographic factors, Duncan and collaborators performed a statistical analysis to determine e-cigarette awareness, e-cigarette lifetime use, e-cigarette current use, and cigarette current use for all fifty states and Washington D.C. They also created an overall Tobacco Control Index (TCI) based on the SOTC metrics.\n\nOf all U.S. adults surveyed between 2012\u20132014, 85.1% reported ever hearing about e-cigarettes, 16.3% reported ever using e-cigarettes, 5.4% reported currently using e-cigarettes, and 17.4% was the current traditional cigarette use estimate. Each state was also categorized as high cigarette/e-cigarette, high cigarette/low e-cigarette, low cigarette/high e-cigarette, or low cigarette/e-cigarette.\n\nDuncan and his collaborators found that a lower Tobacco Control Index score correlates with lower rates of cigarette and e-cigarette use, meaning that state policies intended to regulate cigarettes also reduce e-cigarette prevalence. However, the researchers caution that their study did not take existing e-cigarette specific control measures into account. The researchers also note that the NATS data relied on self-reporting and that they did not account for dual users of both traditional cigarettes and e-cigarettes."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-laura-nor%C3%A9n-586b7d15967e",
        "title": "5 Minutes with Laura Nor\u00e9n \u2013 Center for Data Science \u2013",
        "text": "Laura Nor\u00e9n is an Adjunct Professor at NYU and a Moore-Sloan postdoctoral fellow at the NYU Center for Data Science. Her research focuses on the impact of design on social behaviors, particularly within the context of technology. Most recently, her focus has turned to the ethics of data science, and her class on the same topic was covered by the New York Times. She is also the Managing Editor for the weekly Data Science Community newsletter.\n\nMany professions like business, journalism, and engineering have offered ethical training and codes of ethics for their members. As data science becomes professionalized, there is increasing attention towards teaching courses and developing codes of ethics that address the specific needs of the discipline while drawing on the wisdom of adjacent fields like computer science as well as fields like moral philosophy that are not always considered adjacent. I\u2019m very excited about the way this type of academic cross-pollination can accelerate data science towards a thoughtful, beneficent role in organizations and society.\n\nOne of the great lessons we\u2019ve drawn from science and technology studies is that science, like everything else, is context dependent. The cultural contexts in which we imagine and define what kinds of information consumers, users, employees are allowed to keep private from governments, companies, and, yes, even scientists, is dramatically dependent on national context.\n\nAmericans tend to be more willing to allow themselves and their information to be tracked, stored, and used to drive decisions about the experiences they have in the world. Continental Europeans are more invested in limiting the reach of institutions into their personal lives and behaviors. The EU\u2019s General Data Privacy Regulation is set to become much stricter in May 2018, and this presents major challenges to the way global companies conduct data science.\n\nStill, the scientific community should not adopt an arrogant attitude towards privacy, and assume that our ability to reproduce results is always more socially beneficial than our research subject\u2019s rights to delete or conceal their data.\n\nWe are working hard to figure out how we can make as much data publicly available to anyone as possible without revealing the user\u2019s identities, or contributing to outcomes that will not advance the public good. There are exciting efforts to share clinical trial data in medicine, for example, or to share government data with scientists and the public\u2014 yet, we still don\u2019t see commercial entities getting into the data sharing movement.\n\nWithin companies, the biggest questions are: How much data should be collected? How long it should be stored? And, what types of insights should be drawn from it. Right now, we have a lot to worry about when it comes to predictive policing, but I think too few people are aware that the minority report scenario (where data predict someone will commit a crime before it happens, thereby allowing people to be detained while innocent) is heating up within predictive HR.\n\nOne other area of research that is quite novel is around how humans are going to share decision making with semi-intelligent machines. As the machines get more advanced, it will really feel like we are sharing tasks with them, somewhat like the way we now share the task of navigating with GPS-enabled navigation devices. Such tools may allow people to have more confidence to explore new places without worrying they\u2019ll get lost, but it may also atrophy our ability to navigate without that kind of assistance.\n\nFiguring out how to get as many benefits as possible from these shared human and algorithm processes without losing elements of life, discovery, and learning that we care about will be a growing area of research. What happens when our cultural products are recommended by Netflix and Spotify? Do we expand our taste horizons or amplify our starting tastes?\n\nI call these questions the cultural economies of data.\n\nI wish more people knew how exciting and positive data science ethics is. The class I\u2019m teaching now is the BEST class I\u2019ve ever taught, close the Hollywood ideal of the Ivy League college course where everyone contributes, students are free to challenge each other productively, and are open to changing their minds. We are working on open questions, searching for ethical frameworks we can adapt to data science, and it feels good to contemplate how we can build the most beneficial, least harmful future."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-judith-goldberg-cd40d9d9cd7d",
        "title": "5 Minutes with Judith Goldberg \u2013 Center for Data Science \u2013",
        "text": "Judith D. Goldberg, a Center for Data Science Affiliated Faculty Member, is a Professor of Biostatistics in NYU Langone Medical Center\u2019s Departments of Population Health and Department of Environmental Medicine. She graduated with a Sc.D. from Harvard in 1972 and has held leadership positions at Bristol Meyers Squibb and Lederle Laboratories, was an Associate Professor at Mount Sinai School of Medicine, and began her career as a statistician at the Health Insurance Plan of Greater New York. She was Founding Director of the Division of Biostatistics at NYU Langone from 1999 to 2013. Her research has focused on statistical methods for the evaluation of screening and diagnostic tests, design and analysis of clinical trials, observational studies, and more. Her collaborative research crosses areas from oncology to cardiovascular disease. She is a Fellow of the AAAS and of the American Statistical Association and received the 2015 Janet L. Norwood Award for Outstanding Achievement by a Woman in the Statistical Sciences and the 2016 Lagakos Outstanding Alumni Award from the Harvard TC Chan School of Public Health.\n\nI include statistics as a key component of data science. The explosion of data from multiple sources, much of which results from new high throughput technologies that allow us to evaluate genetic components of disease incidence and prognosis has expanded what we can incorporate into clinical trials and observational studies. In particular, the computational advances in data science allow the analysis of gene expression data, proteomics, metabolomics, and other technologies, and the integration and analysis of data across multiple databases as well as data mining for exploratory analyses. The integration across all of these types of data allow us to develop models that could enhance the prediction of disease occurrence and outcomes. The potential to develop personalized treatments and prevention strategies for patients is now a possibility.\n\nThis research, in collaboration with radiation oncologists and medical oncologists, led to the identification of improvements in radiotherapy techniques that reduced the long term risks of heart and lung damage from radiotherapy with no loss of efficacy. In addition, in a series of treatment trials in breast cancer, we evaluated immunotherapy for treatment of breast cancer and incorporated the evaluation of immunologic and genetic markers, something that is now possible with the available high throughput technologies and computational platforms. In this research, we also developed some statistical approaches to allow us to to use systematic missing at random (SMAR) study designs that incorporate data from multiple domains using subsamples of the total study population to minimize costs.\n\nI was fortunate in my first position to be responsible for the analysis of the data that resulted from the Health Insurance Plan of Greater New York Breast Cancer Screening Study. We identified risk factors for breast cancer that have held the test of time and estimated the benefits of screening on mortality. This landmark study led to the widespread use of mammography for the early detection of breast cancer.\n\nThe ability to design studies that incorporate data from multiple domains to develop integrated analyses that help us develop personalized treatments and to identify individuals at risk of disease is really exciting to me. To envision combining different types of data at the individual and group and population levels to develop new insights for policy and practice based new approaches to analysis at the interface of statistics and computing is the future."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-sharon-weinberg-33803777b5aa",
        "title": "5 Minutes with Sharon Weinberg \u2013 Center for Data Science \u2013",
        "text": "Sharon L. Weinberg is a Professor of Applied Statistics and Psychology at the NYU Steinhardt School of Culture, Education and Human Development, a core member of PRIISM, and a Center for Data Science Affiliated Faculty Member. She was formerly the Vice Provost for Faculty Affairs at NYU and the former President and Chair of the Board of the Jewish Foundation for Education of Women. She holds three degrees from Cornell University, including an A.B. in mathematics and a Ph.D. in psychometrics and research design methodology. She has co-authored two statistics textbooks published by Cambridge University Press, one now in its third edition and one in its first edition, and numerous articles in the areas of statistics education, faculty diversity, higher education, applied statistics, and evaluation methods.\n\nFrom my own personal experience, as Vice Provost for Faculty Affairs at NYU, there were many issues of equity that I was able to address given my expertise as an applied statistician.\n\nI also proposed a new method for assessing whether salary compression exists at a university. I also investigated the way in which faculty diversity at a university typically is measured, and based on my analysis, called for a new, more granular approach that is not based on data aggregated at the university level. Instead, the new more granular approach assesses faculty diversity at the department level, and as such, offers an assessment that is better aligned with the benefits to be reaped by students in having a diverse faculty.\n\nAdditionally, I studied the impact of the 1993 legislation that uncapped mandatory retirement at institutions of higher education, and showed by applying survival analysis to NYU data as a case study, that since 1993 there has been an upward shift in the probability distribution of the age at which faculty retire. At NYU, the pre-uncapping retirement age, which then was mandatory, was 70. Since 1993 there has been no mandatory retirement age for faculty nationwide. Each of these investigations resulted in a research paper, and each would not have been possible without analyses based on statistical methods applied to a large dataset.\n\nWith the increasing availability of quantitative data, statistical methods become critically more important for uncovering the story behind the numbers and for gaining insight into phenomena of interest, whether it be for descriptive purposes or to impact policy. There are so many well-known examples of research that address issues of equity across the education spectrum, such as class size, school choice, and emotional and cognitive development through publicly available educational opportunities, such as public Pre-K, English as a second language (ESL), gifted and talented programs (G&T), and magnet and charter schools.\n\nOnce again, each study would not have been possible without the combination of reliable data and statistical methods for teasing out the important relationships and effects related to the questions of interest. Although anecdotal information may be revealing in specific situations, as someone once said, the plural of anecdote is not data.\n\nI have been interested in studying equity for as long as I can remember. A particular passion has been gender equity, which explains, in part, my interest in initiating the series of faculty salary equity studies as Vice Provost for Faculty Affairs and being a driving force in creating the Workload Relief Policy at NYU to strengthen NYU\u2019s commitment to work/life balance.\n\nSome of my other research on gender equity has involved the development of a moral orientation scale, based on Carol Gilligan\u2019s care and justice dimensions of moral development; the study of the differential preferences of men and women for intrinsic and extrinsic job characteristics, echoing the call of others for a more flexible work structure to more suitably accommodate work/life balance; and, more recently, a study to understand why college enrollments favor women in the ratio 60:40 using Coleman\u2019s theory of social capital."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-jennifer-jacquet-3c98d647127d",
        "title": "5 Minutes with Jennifer Jacquet \u2013 Center for Data Science \u2013",
        "text": "Jennifer Jacquet is an Assistant Professor of Environmental Studies at NYU and an affiliated faculty member with the Stern School of Business as well as the Center for Data Science. She earned her doctorate at the University of British Columbia (during which she volunteered as a diver in the shark tank at the Vancouver Aquarium!) Her research focuses on large-scale transnational cooperation dilemmas, including climate change, overfishing, and the Internet wildlife trade.\n\nSunandan and I are just finishing up connecting the user interface (which will allow enforcement agencies, NGOs, and some people in the private sector) to search the results from the back-end model, which generates a list of potentially illegal wildlife ads found on the open web each week. We also are just wrapping up work for a model specific to the state of California. We are hoping EGI will be deployed around the world over the next year.\n\nIt\u2019s interesting to see the way shame is already being used to address these issues. You may have heard about \u2018manel shaming\u2019 \u2014 an online collection of conferences and events featuring all male panels from around the world (this recent panel at the International Conference on Complex Systems featured 15/15 males) or the article in the New Yorker about how the jobs site Glassdoor is exposing gender inequity in the workplace. Even more interesting is the way in which algorithms and AI can be deeply prejudiced, an area that people like NYU\u2019s own Kate Crawford work on. It is clear that the ethics of the tech sector as well as the technology itself are both in need of scrutiny.\n\nI wish! Funny to recall, but there was a time that I thought Vancouver was a hyper urban environment (after living in Manhattan, I now see it as bucolic) and I wanted to remain in contact with wild animals. I am conflicted about keeping animals such as sharks in captivity, but did see value in showing children that you can dive in the same very small space with many species of sharks and be absolutely safe. Plus, it was the tank with the warmest water."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-lisa-hellerstein-52beee482698",
        "title": "5 minutes with Lisa Hellerstein \u2013 Center for Data Science \u2013",
        "text": "Lisa Hellerstein is a Professor of Computer Science and Engineering at NYU and an affiliated faculty member at the Center for Data Science. After earning her doctorate at the University of California Berkeley, Hellerstein now specializes in machine learning, algorithms, discrete mathematics, and more.\n\nAlgorithms are a fundamental part of computer science. I first learned about algorithm design in my undergraduate courses. It\u2019s great to have powerful computers, but unless you have good algorithms, the power isn\u2019t useful. In algorithm design, you are solving puzzles. Sometimes, when you begin to look at a computational problem, you have no idea whether it is even possible to solve that problem with an efficient algorithm. You may go back and forth, thinking one day that it\u2019s possible, and another that it is not. It\u2019s especially satisfying when you gain insight into a problem, and through that insight, are able to develop an efficient algorithm.\n\nI have a long-standing interest in algorithms for choosing sequences of questions or tests. Think of the 20-questions game: Which question should you ask first? Which should you ask next? The NSF project on data management was a joint project with a researcher in databases at the University of Maryland, Amol Deshpande. Database design also involves sequences of questions. For example, suppose you ask the database to provide you with the records of all people in your organization who make more than $60K per year and who live in New York City. Given a person\u2019s record, should you first check whether the person lives in New York City? Or whether the person makes above $60K per year? How you make that decision can affect how quickly the database is able to provide the requested records. As a theoretician, I work on abstract versions of question and test ordering problems. I aim to develop efficient algorithms for determining optimal or near-optimal orderings, in worst-case, probabilistic, and game theoretic settings. Although my work is theoretical, the problems I address are connected to practical problems in many different areas, including machine learning, databases, and medical diagnosis."
    },
    {
        "url": "https://medium.com/center-for-data-science/crepe-a-neural-network-with-perfect-pitch-fc67ba99308c",
        "title": "CREPE: A Neural Network with Perfect Pitch \u2013 Center for Data Science \u2013",
        "text": "Audio processing, speech processing, and music information retrieval all depend on pitch tracking, a task for which computational methods have been studied for more than fifty years. Many reliable techniques have been built, but even the current gold standard of pitch tracking \u2014 an algorithm called pYIN \u2014 is a heuristic approach susceptible to inaccuracy from rapid shifts in pitch or uncommon instruments.\n\nTo evaluate CREPE\u2019s performance and compare it with other algorithms, the researchers use two datasets. The first consists of 6.16 hours of audio synthesized from the RWC Music Database (the same database used to evaluate pYIN); the second is a collection of 230 tracks with 25 instruments from MedleyDB, amounting to 15.56 hours of audio. Accuracy is measured in raw pitch accuracy (RPA) and raw chroma accuracy (RCA), which together measure the proportion of frames in the output for which the algorithm detects pitch within a quarter-tone of the ground truth.\n\nCREPE\u2019s accuracy is impressive \u2014 for the RWC dataset it had a nearly 100% accuracy rating, and for the MDB dataset it had over 90% pitch accuracy within 10 cents (a measure that denotes a tiny fraction of a tone). The researchers compared CREPE\u2019s performance to pYIN\u2019s and SWIPE\u2019s (another high-performing pitch tracking algorithm), and found that CREPE could outperform both by over 8% when the evaluation threshold was 10 cents.\n\nBello and collaborators also compared the three methods\u2019 capabilities for tracking the pitch of degraded sound. They simulated pub noise, white noise, pink noise, and brown noise (pink and brown noise have boosted lower frequencies). Except for brown noise, for which the pYIN method is especially attuned, CREPE is generally better at pitch tracking degraded noise than pYIN or SWIPE.\n\nCREPE\u2019s performance is an encouraging development for pitch tracking, but Bello and his group hope to improve the architecture of their model to allow for distortion and reverberation. They also intend to improve CREPE\u2019s pitch estimation capabilities by adding recurrent architecture to increase temporal smoothness."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-cristina-savin-dee2230633cd",
        "title": "5 Minutes with Cristina Savin \u2013 Center for Data Science \u2013",
        "text": "Cristina Savin is an Assistant Professor at the Center for Data Science and the Center for Neural Science. After earning her Ph.D. in Computational Neuroscience at the Frankfurt Institute of Advanced Studies, Savin worked as a researcher at Cambridge University, l\u2019\u00c9cole Normale Sup\u00e9rieure de Paris and IST Austria before joining NYU in September 2017. Her research lies at the intersection between neuroscience and machine learning; it focuses on building conceptual bridges between how learning happens in brains and machines. Her most recent paper explores how new statistical techniques for making sense of neural data.\n\nHaving one student come to me after class and telling me that what he had learned during the lectures helped him succeed in a job interview was a definite highlight. More generally, I have enjoyed making connections with a variety of people across departments and getting a few new projects going in the process.\n\nWithout getting too technical, AI has had several amazing successes by using relatively simple learning procedures on huge amounts of data. Still, it is not clear to me that even more data will suffice.\n\nMy intuition is that we need a principled way to inject domain knowledge into the process, which may take the form of extra structure. If nothing else, biological circuits provide inspiration for how such structure might look like. More generally, I think there is a lot to be learned at the interface between neuroscience, cognitive science and machine learning.\n\nMy main advice goes for everyone: find a puzzle that truly interests you and go for it. For women in particular: get as much advice as possible along the way and be proactive about it. There are a lot of opportunities that get missed just because we never hear about them. So networking is critical for success."
    },
    {
        "url": "https://medium.com/center-for-data-science/could-machine-learning-get-its-own-new-programming-language-c3c166b0a90c",
        "title": "Could machine learning get its own new programming language?",
        "text": "Machine learning research requires a high degree of computational capability and domain specific features from programming languages. Traditional languages like Python scale poorly to these high computational demands, and Python\u2019s semantics complicate some machine learning objectives.\n\nTensorFlow, Google Brain\u2019s open source software library for numerical computation using data flow graphs, is a relatively new programming language that can satisfy domain specific machine learning requirements. But TensorFlow itself involves a type of meta-programming that relies on Python \u2014 which means that the code now has two execution times. (And, debugging becomes much more challenging due to differences in language semantics.)\n\nWhile Karpinski and his collaborators acknowledge that popular numerical languages like TensorFlow can be useful for machine learning, they suggest that a new tailored language would better enable recursion, maintenance, production system integration, abstraction, and program manipulation. The team anticipate that a new language for machine learning would draw inspiration from numerical computing so a user could add data representations in high-level code. They also point out that domain specific language development is not unprecedented \u2014 new languages have proven effective for formal reasoning and verification and cluster computing.\n\nBut there are significant challenges ahead, such as achieving generality alongside performance, and the requirement of a new library ecosystem.\n\nTo meet these challenges, the researchers encourage the machine learning community to draw from the programming language and automatic differentiation communities. They also suggest that the Julia language, a project to which Karpinski contributes, will prove a useful launching pad for future machine learning language research."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-suzanne-mcintosh-43c2636fd3ab",
        "title": "5 minutes with Suzanne McIntosh \u2013 Center for Data Science \u2013",
        "text": "Suzanne McIntosh is an Clinical Associate Professor of Computer Science at NYU, and an affiliated faculty member at the Center for Data Science. Situated in between academia and industry, McIntosh teaches computer science graduate and undergraduate courses while also providing consulting services for major organizations like IBM, where she worked as a technical team leader to develop technologies for virtualization, cloud computing, and big data analytics. She is an expert in the Hadoop and Spark ecosystems, has been the Lead Inventor of multiple patents, acts as an advisor to start-ups and as a STEM mentor at the New York Academy of Sciences (NYAS) and Society of Women Engineers.\n\nI\u2019ve had the honor of developing software solutions alongside engineers working in the defense industry on mission critical communications and space systems, developing base station software for commercial telecom, researching security, virtualization, and analytics solutions for targets ranging from tiny smartcards to green data centers, and now, joining the research community in academia.\n\nDeveloping firmware for mission critical defense systems requires at the very lowest levels of software development and integrating with custom hardware, rather than off-the-shelf hardware. This means that even the hardware is new and untested, making the software development and testing process even more challenging \u2014 and debugging often means working with engineers from a variety of disciplines. It also requires quite a lot of process. While a lot of people tire of all the process involved in tracing requirements to implemented code to prove that all requirements are met and tested, process is what helped ensure the systems deployed were reliable and robust.\n\nContrast that with the interest of commercial telecom, which was to keep the system going even in the face of a serious error \u2014 up-time was the most important metric. The experience of working in commercial telecom taught me to appreciate alternative approaches to fault tolerance. Her, too, we used custom hardware and firmware, but the culture was much looser in terms of process when compared to mission critical defense systems. Here, we used assembly, C, and C++, while in the defense world, we used assembly and later Ada, and then C. For a long time, C compilers were not trusted; the fear was that trap doors and other vulnerabilities could be inserted by the compiler unbeknownst to the programmer.\n\nNext, I joined the research world \u2014 a world I didn\u2019t know existed. It felt like working on a college campus. Here, teams are relieved of the pressure to deliver a fully complete product, hence process overhead was very light. Research teams research innovative ideas by building functional prototypes but they often skip implementation of corner cases. Once completed, the proof-of-concept is zhuzhed up into a product by a business unit \u2014 and this is where every corner case is addressed to build a viable and robust business or product. The goals in this type of job are to build prototypes, publish research papers, and file patents. The idea is to free the researchers to explore advanced ideas that are five to ten years ahead of the current state of technology.\n\nFinally, I\u2019ve had the opportunity to join academia which matches closely to the research environment I worked in, but with the additional chance to provide instruction and mentoring to a new generation of graduate and undergraduate software engineers and data scientists.\n\nSTEM students who are looking for freedom to implement a great new idea they have could consider working in a research environment or academia. They might also enjoy the challenge of establishing their own startup. These are the environments where there is flexibility and freedom to pursue your own ideas. These projects are typically high risk \u2014 there are no guarantees of success and often you are faced with significant obstacles which have never before been encountered, or solved.\n\nNon-research projects are less risky by design \u2014 stable, existing technologies are chosen, especially in defense systems. There is a lot of process overhead to ensure the entire team works in a predictable way to write, test, deploy, and maintain code. There is high likelihood of the project\u2019s success, there is extensive planning by project managers, and schedules are closely watched for progress.\n\nAs a mentor, I hope to be the link between future engineers and the fine professors and engineering colleagues who generously shared their knowledge with me. My goal is to pay it forward : )\n\nThe proudest moment in my career was earning the chance to design and develop a secure embedded realtime operating system for secure radios deployed in the tactical internet. This InfoSec work led to a thesis on secure operating system design, and a first paper published in IEEE MilCom 2000."
    },
    {
        "url": "https://medium.com/center-for-data-science/according-to-the-data-how-social-media-matters-for-protest-and-activism-f88caa05b333",
        "title": "According to the Data: How Social Media Matters for Protest and Activism",
        "text": "If you\u2019ve tweeted \u2014 or retweeted \u2014 about #MeToo, #BlackLivesMatter, #Resist, or #Occupy, just to name a few, you can call yourself an activist. Skeptics like Malcolm Gladwell, however, might call you a slacktivist. But this type of opinion relies on speculation and traditional \u2014 possibly antiquated \u2014 methods of evaluating collective action.\n\nCDS Director Richard Bonneau and affiliated faculty members John Jost, Professor of Psychology and Politics, and Joshua Tucker, Professor of Politics, along with other researchers, have published a new article \u2014 one that relies on data-driven studies rather than speculation \u2014 about how social media impacts political protest.\n\nBonneau, Jost, and Tucker examine a large number of studies linking social media to political protest, and the authors offer three key conclusions.\n\nWhile these conclusions may seem intuitive, they are significant because they are derived from numerous studies that analyzed data from social media activity; these studies often correlate content or metadata (timestamp, location, user account, etc.) with offline events.\n\nFor example, with regard to the exchange of vital information, studies of 2013/14 protests in Turkey and Ukraine showed that Twitter and Facebook were used to facilitate organization, enable transportation, track political development, advise wounded protesters, spread motivational messages, and share international news coverage. Users across the world were able to process events together in real-time as protesters tweeted during demonstrations, but governments were also able to monitor and crackdown on opposition movements.\n\nFrom a social psychological perspective, research shows that an individual\u2019s desire or motivation to protest is affected by moral outrage, social identification, and group efficacy. To examine these motivators, Bonneau, Jost, and Tucker looked to a 2015 study that performed a qualitative analysis of a subset of 80,000 tweets from Occupy Wall Street, tagged #OWS, #occupy, or #mayday. Based on timestamps and other metadata, users who tweeted about motivation were more likely to actually protest. Data analysis methods that can identify a user\u2019s ideology based on who they follow substantiated the fact that pro-OWS individuals were more likely to be liberal. Furthermore, liberals were found to be less likely to express anger or ideological concerns.\n\nIdeological concerns can fundamentally affect the structure of a user\u2019s social media network, producing what is commonly called an echo chamber. According to a statistical model that evaluates retweeting habits, conservatives have a statistically significant higher likelihood of finding themselves in an echo chamber. Three separate studies based on millions of tweets also showed that conservatives were more likely to spread disinformation, be exposed to pro-Russian disinformation, and peddle conspiracy theories; two other studies showed that \u201cfake news\u201d is concentrated on the ideological right.\n\nPatterns of retweeting can materially affect the success of protests because, as two separate studies have shown, retweeting by periphery activists (people who are not actually present at protests) can account for nearly 50% of a protest movement\u2019s reach on social media.\n\nFor future study, Bonneau, Jost, and Tucker suggest data-driven research about how political information shared on social media can be more impactful than traditional political information due to the nature of online friendship."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-juliana-freire-5a7d15591db6",
        "title": "5 minutes with Juliana Freire \u2013 Center for Data Science \u2013",
        "text": "Juliana Freire is a Professor of Computer Science and Data Science. She is the Lead PI and Executive Director of the Moore-Sloan Data Science Environment at NYU CDS. After earning her Ph.D. in Computer Science from SUNY (Stony Brook), she joined the Database Group at Bell Labs. In 2002, she moved to academia, having held faculty positions at the Oregon Health and Science University and at the University of Utah before joining NYU in 2011. She has co-authored over 180 technical papers and received multiple grants from the NSF, DARPA, Gordon and Betty Moore Foundation, Sloan Foundation, Google, Amazon, Microsoft Research, Yahoo!, and IBM for her research projects. She is an ACM Fellow, and in 2017, she was elected as the first female chair of the Association for Computing Machinery\u2019s SIGMOD group in its 42 year history. Her recent research has focused on large-scale data analysis and integration, visualization, provenance management, computational reproducibility, and web information discovery.\n\nIn Brazil, we have to choose a major before we take the entrance exam for the university. Since there are many medical doctors in my family, I was inclined to go to medical school. But I had just gotten my first computer, and since I always loved Math, I thought that Computer Science would also be a good choice. So, I flipped a coin, and Computer Science won.\n\nVisTrails was my first large-scale project and it has inspired many other research problems that I still work on.\n\nOur goal in this project was to develop new methods and systems that enable domain experts to more easily construct the computational pipelines required in scientific exploration. We introduced many new concepts around provenance management, and in addition to authoring patents and publications (which include 2 best-paper awards), we built the VisTrails open-source system. VisTrails systematically captures and maintains detailed provenance (history) of the steps followed and data derived in the course of an exploratory computational task. Besides enabling reproducible results, VisTrails leverages provenance information through a series of operations and intuitive user interfaces that help users to collaboratively analyze data.\n\nWorking on VisTrails was rewarding in many ways \u2014 not only did we find and addressed new computer science problems, but we also had practical impact in many different scientific domains.\n\nYoung researchers have to find their own paths. To be successful at anything, you have to be passionate about what you do. So my advice is find your passion!"
    },
    {
        "url": "https://medium.com/center-for-data-science/understanding-particle-physics-data-science-jets-into-the-future-319e853d338b",
        "title": "Understanding Particle Physics: Data Science Jets into the Future",
        "text": "The Large Hadron Collider, the most powerful particle collider and largest machine in the world, serves as a means to test certain theories of particle physics. Several goals of the project relate to understanding subatomic particles that propel jets. A jet may arise from particles called quarks, gluons, W-bosons, top-quarks, or Higgs bosons. Understanding jets at the subatomic level helps particle physicists understand fundamental forces in the universe.\n\nHere\u2019s where data science comes in: to understand subatomic jet particles, physicists need to be able to classify these particles. Different types of neural networks can address this classification problem. In recent research, recursive neural networks have been used based on an analogy between quantum chromodynamics (QCD) and natural languages \u2014 as a sentence is composed of words following a syntactic structure, a jet is composed of particles following a structure determined by quantum chromodynamics.\n\nBut in new research from Joan Bruna, Kyunghyun Cho, both CDS Faculty and Assistant Professors of Computer Science and Data Science, Kyle Cranmer, CDS Affiliated Faculty and Associate Professor of Physics, Isaac Henrion, and Johann Brehmer, jets are represented as graphs and studied with a Message Passing Neural Network (MPNN) rather than a recursive neural network. The researchers applied their MPNN to a binary classification problem, attempting to distinguish between two classes of jets: QCD jets (arising from a known mixture of quarks and gluons) and W jets (arising from bosons decaying into two quarks).\n\nCompared with the performance of recursive neural networks in achieving this classification, MPNNs did significantly better. Recursive neural networks are restricted to a tree structure while graph-representation for MPNNs allows information between all particles to be exchanged. However, the researchers caution that model configuration must be carefully designed for MPNNs because it influences the final result.\n\nMPNNs offer a new data-driven avenue to explore jets compared to traditional jet clustering algorithms. For future research and experimental design, the researchers suggest improvements for MPNNs to avoid expenses due to high complexities and advances to handle larger scale inputs."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-karen-adolph-e8206a09be34",
        "title": "5 Minutes with Karen Adolph \u2013 Center for Data Science \u2013",
        "text": "Karen Adolph is a Professor of Psychology and Neural Science at New York University, an affiliated faculty member at the Center for Data Science, and director of the Databrary project. Her work focuses on perceptual-motor development, particularly in infants. Adolph earned her doctorate at Emory, and taught at Carnegie Mellon University before joining NYU in 1997.\n\nI\u2019m super excited about our work on infants\u2019 spontaneous locomotor exploration. Before infants are independently mobile, caregivers cart them from place to place. But after infants learn to crawl or walk, they decide when and where to move. We\u2019ve discovered that infants rarely move toward recognizable destinations. Using head-mounted eye trackers, an instrumented floor, and high-tech video, we\u2019ve found that mobile infants rarely get up and go to see something they spied while stationary. Instead, they mostly take steps in place or stop in the middle of the floor beyond arms\u2019 reach of any person, place, or thing. In fact, even with no play partner, infants move just as much in a totally empty room as in a room filled with toys. Infants are \u201cmoving to move\u201d\u2014and they move a lot (4000 steps/hour in an empty room!)\n\nTheir paths are incredibly variable \u2014 in length, direction, speed, and shape \u2014 and cover most ground surfaces and available area. Using simulated robot soccer (Robocup), we showed that this type of variability is a feature, not a bug. Robots trained on natural infant paths win more matches in Robocup compared with robots trained on geometric paths. And, robots trained on more variable infant paths win more matches than those trained on less variable paths.\n\nInfants are fascinating because the changes in their bodies, environments, and behaviors are so dramatic. New behaviors come into being. Existing skills improve and transform. All of this is readily observable and can be recorded directly with video and various types of motion trackers. We can literally watch development in action.\n\nVideo is a uniquely powerful tool for capturing the richness and complexity of behavior. As such, video is ideal for both data and documentation. Databrary is a web-based data library for securely archiving, sharing, and reusing research videos \u2014 including raw research videos and video demonstrations of procedures and displays. Sharing and reusing video will make behavioral and social science more transparent and reproducible, and accelerate progress and magnify public investments in research. Researchers can also find video clips to use for teaching and conference presentations.\n\nDatabrary developed a rigorous policy framework to allow ethical sharing of data that contains personally identifiable information. A unique access agreement allows researchers to both contribute and use data. Databrary provides persistent identifiers (DOIs) to shared datasets, so researchers gain citation credit for their work. Databrary and Datavyu, a desktop video-coding tool, give researchers the resources they need to understand behavior and to accelerate the pace of scientific discovery. The Databrary community currently includes 1000+ researchers from nearly 400 institutions around the globe and it grows weekly."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-soledad-villar-fa0dabf050db",
        "title": "5 Minutes with Soledad Villar \u2013 Center for Data Science \u2013",
        "text": "Soledad Villar is a Moore-Sloan Research Fellow at NYU\u2019s Center for Data Science, and she holds a Collaborative Scientist appointment at the Algorithms and Geometry Simons Collaboration.\n\nShe was formerly a Research Fellow at UC Berkeley\u2019s Simons Institute. She earned a Ph.D. in Mathematics at the University of Texas at Austin, a Master\u2019s in Number Theory and Bachelor\u2019s in Mathematics from Universidad de la Republica in Uruguay, and a Bachelor\u2019s in Computer Science from Universidad Catolica del Uruguay. Her research focuses on optimization, statistics, machine learning, and applied harmonic analysis. She is also interested in number theory.\n\nBeing a Moore-Sloan Research Fellow is a unique opportunity for an applied mathematician. The position provides me with the freedom to develop my own research program and the opportunity to collaborate in multidisciplinary research (not to mention that the NYU Courant Institute is just down the road, which has a top math department). My experience has been great so far. I am happy to be part of the Math and Data group organized by Afonso Bandeira, Joan Bruna and Carlos Fernandez-Granda.\n\nSince I joined NYU, I have started working in different problems in addition to my previous line of work. I have been thinking about deep learning from a theoretical point of view and I am exploring some applications of deep learning to social sciences. I am grateful that my job is awesome.\n\nI am by no means an authority to talk about data science in Uruguay\u2014but from the industry point of view, most of my knowledge about the state of data science in Uruguay is through my sister, Florencia, who works as a data science consultant in the Uruguayan branch of a large multi-national company.\n\nIn her experience, the main differences between data science in Uruguay and US are in terms of scale (the amount of data managed in Uruguay is smaller, given that Uruguay is a country with 3 million people) and broadness (in Uruguay data science is exclusively used for commercial applications, whereas here it has been applied in many fields, including manufacturing and health care).\n\nFrom the academic point of view, based on my experience as a math student in Uruguay, I found that the mathematics curricula was focused in pure mathematics and did not really offer a perspective on applications. Collaborations between mathematicians and researchers from other fields are very rare. I think that trend is slowly changing and now we can see more collaboration between math and engineering.\n\nMy interest in number theory started from the math olympiads (which I started participating in when I was twelve years old). In college, I studied math and computer science, and I was initially interested in the computational side of number theory. For instance, how one can use groups arising from number theory objects (like elliptic curves) to do cryptography.?My undergrad and masters advisor, Gonzalo Tornaria, is a number theorist that sometimes works in very intense computational projects, like verifying a famous number theory conjecture up to 10\u00b9\u00b2 by using sophisticated engineering solutions.\n\nI arrived to optimization from a computational motivation, particularly as an approach to combinatorial problems. I haven\u2019t explored a connection between optimization and number theory yet.\n\nI am currently working on a project with Dustin Mixon (Ohio State) where we analyze some aspects of gerrymandering from a game theoretical point of view and we produce numerical simulations. I am also discussing with Joan Bruna about deep learning for problems in graph and how these techniques can compare to (or can take ideas from) classical optimization approaches.\n\nAnother line of work I have explored with Dustin is the use of generative models for classical signal processing inverse problems like image denoising. We\u2019re trying to explain what activation functions allow denoising by local methods using spherical harmonics.\n\nOn a more theoretical note, I am also co-organizing a seminar with Afonso Bandeira, Shuyang Ling and Tim Kuninsky where we look at a specific problem on random graphs and explore what type of solutions different algorithms from the literature can provide. In particular there is a type of algorithm arising from a Bayesian approach that can be analyzed using techniques from statistical physics. I am especially interested in understanding the techniques and applying them to other problems related to data science (it has been suggested that they may even have applications to deep learning). I am also working with Efe Onaran at NYU Tandon to apply these techniques to graph matching."
    },
    {
        "url": "https://medium.com/center-for-data-science/cds-hosts-women-in-data-driven-careers-in-2018-event-4edd0ad573f5",
        "title": "CDS hosts women in data-driven careers in 2018 event",
        "text": "Last week, CDS hosted the Women in Data group\u2019s most recent event \u2014 the Women in Data-Driven Careers in 2018 panel. Organized and moderated by Dr. Norma A. Padr\u00f3n, the Associate Director of MLHS Center for Population Health Research at LIMR, the event brought together five accomplished women who are working in data science: Cindy Chin, Dr. Noami Derzsy, Dr. Sinziana Eckner, Vivian Zhang, and Anne Warren.\n\nAfter general introductions, the panel began by exploring how family has impacted their careers in the past and present. For Dr. Eckner, who is currently a Data Scientist at JP Morgan Chase (and an NYU alum!), her parental units were remarkably relaxed.\n\nPerhaps Dr. Derzsy\u2019s parents were ahead of the curve, because taking greater involvement with our children\u2019s education has become a trend for mothers. For example, Chin, an entrepreneur, venture strategist, and consultant, explained how she rediscovered her interest in coding when she her twelve year-old daughter was learning how to code at school. \u201cIt\u2019s incredible,\u201d Chin said, \u201cwhen your own children can inspire you like that.\u201d\n\nChin, who works across multiple time zones daily, is also (along with Dr. Derzsy) a NASA Datanaut, a program devoted to promoting data science, coding, and gender diversity. In between her role at NASA, raising her daughter, and consulting, then, like most working mothers Chin\u2019s schedule is tight. When asked what she would learn if she had more time, Chin said that she would like to take a strong Python course \u2014 and, luckily, there\u2019s a school just for that!\n\nThe NYC Data Science Academy, one of the city\u2019s top data science bootcamp centers, is co-run by Zhang, a fellow panelist. Not only is she the academy\u2019s Chief Technology Officer, but Zhang is also of the Founder of the NYC Open Data Meetup group of over 4000 members.\n\nZhang still keeps one foot in academia, however, through her position as an adjunct professor at Stony Brook University, leading moderator Dr. Padr\u00f3n to ask whether one should choose academia or industry after their degree. \u201cThere\u2019s so much pressure in academia to stay,\u201d Dr. Padr\u00f3n said. \u201cIsn\u2019t there some fear when you step outside of it?\u201d\n\nFor almost all the panelists, remembering to remain open-minded was key. While transitioning from academia to industry may seem daunting, Dr. Derzsy explained, programs like Insight can help data scientists adjust to the expectations of industry work. For Warren, a Product Manager who works at Chase Digital with Dr. Eckner\u2019s team, another crucial tip is taking job descriptions with a grain of salt. \u201cEven if you don\u2019t 100% hit on all of the points on the description,\u201d Warren said, \u201capply to the job. Apply, because you may have a skillset that they never knew they needed until you applied.\u201d\n\nThe panel closed with an intriguing question from the audience. \u201cHow,\u201d they asked, \u201ccan one best communicate with others who do not have a statistics background, especially when they have more seniority? Do you have any tips?\u201d As the issues of gender inequality, power, and politics pervade the scientific community, learning how to communicate effectively with one another is vital.\n\n\u201cIt\u2019s good to be as prepared as possible,\u201d advised Zhang thoughtfully. \u201cAnd, remember that men are not always against you. Good communication occurs when you break things down, and prove your point with evidence, logic, and reason.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/a-better-way-to-predict-ambulance-calls-from-two-cds-moore-sloan-data-science-fellows-9a91f9d96394",
        "title": "A Better Way to Predict Ambulance Calls from Two CDS Moore-Sloan Data Science Fellows",
        "text": "In 1854, John Snow made an early stride in the field of spatial epidemiology by mapping London cholera cases to determine the source of the outbreak. Modern spatial epidemiology has, of course, become more effective, especially by using data to predict medical incidents. Most current techniques related to non-infectious disease, however, focus on predicting the incidence of a particular disease without incorporating analysis of demographic or environmental factors.\n\nAnastasios Noulas and Bruno Gon\u00e7alves, both Moore-Sloan Data Science Fellows, avoid this deficiency of spatial epidemiology in new research, which has been peer-reviewed and accepted to the 8th International Digital Health Conference in France.\n\nBroadly, their work aims to \u201cestimate the volume of ambulance calls at the level of individual Lower Super Output Areas (LSOA) in the North West of England,\u201d where LSOA\u2019s are small local areas with a median population of 1,520 people.\n\nAnswering this question required the researchers to examine geographic and demographic data, which they sourced from the UK government, online media sources, and the mobile web. In particular, three key data sources for their analysis were England\u2019s North West Ambulance Service, Foursquare user check-ins, and the UK government\u2019s Index of Multiple Deprivation (IMD) which identifies areas affected by lower quality of life.\n\nThe researchers also improved their predictive capabilities by creating a new variable \u2014 daytime population \u2014 to address daily fluctuation in the number of people in one area. By estimating daytime population (workplace population plus residents younger than 16 and older than 74), the researchers were able to better predict ambulance calls than if they had simply used residential population statistics.\n\nThrough correlation-driven data analysis, the researchers sought to determine whether certain types of ambulance calls are associated with different demographic and socio-economic conditions. They found that daytime population correlates with unconscious/fainting incidents, seizures, and falls; IMD correlates with breathing problems, chest pain, and psychiatric/suicide incidents; and dense urban centers correlate with unconscious/fainting incidents.\n\nWith an understanding of these correlations, the researchers were able to generate a model for predicting both the number and type of ambulance calls in a local area. Though they are encouraged by their results, they recognize the limitations of data from social media due to potential biases. For future research, Noulas and Gon\u00e7alves suggest using \u201creal time digital datasets from location-based services to model medical incident activity not only across geographies, but also over time.\u201d They also hope that this type of research can soon begin to inform relevant public health policies."
    },
    {
        "url": "https://medium.com/center-for-data-science/a-call-to-action-for-empiricism-in-data-quality-research-4b50259bb58",
        "title": "A Call to Action for Empiricism in Data Quality Research",
        "text": "How should the quality of data be measured? How does the quality of data affect the quality of data-driven insights? What is the value of data?\n\nIn a new study, CDS\u2019s Juliana Freire, along with her co-authors, explores these fundamental questions for data scientists. Based on their study, Freire and her collaborators establish a framework for empirical methods of data evaluation throughout the entire data processing pipeline.\n\nThe researchers\u2019 new framework involves two types of metrics (intrinsic and extrinsic) and two scopes of methods (generic and tailored). The framework also includes a data continuum from real data to synthetic data.\n\nIntrinsic metrics focus on the properties of data \u2014 they can be clearly defined and are application-independent such as completeness, lack of duplicates, or format consistency. This set of metrics would apply to information extraction and cleaning; and integration, aggregation, and representation.\n\nExtrinsic metrics, however, are application-dependent, such as examining how well the data supports a specific business objective or determining the value of the data. These metrics apply to the modelling, analysis, and interpretation phases in the data processing pipeline.\n\nGeneric methods can be used in a variety of application contexts, while tailored methods are specific for a particular data type or application domain. This dichotomy, along with the dichotomy of extrinsic and intrinsic methods, classifies four ways of measuring data quality: Generic methods for intrinsic metrics, generic methods for extrinsic metrics, tailored methods for intrinsic metrics, and tailored methods for extrinsic metrics.\n\nAlong with this new classification, the researchers call for three immediate actions that can be taken to promote empiricism in data quality research: the sharing of data, metadata, code, application scenarios, and benchmarks; new guidelines for experimental design to separate between error creation and measurement; and, finally, the expansion of data quality research, including amendments or extensions to their classification."
    },
    {
        "url": "https://medium.com/center-for-data-science/neural-networks-and-toddlers-how-learning-biases-can-improve-word-learning-56e477dc1ee3",
        "title": "Neural Networks and Toddlers: How Learning Biases Can Improve Word Learning",
        "text": "People use prior knowledge to contextualize and make inferences about new concepts, a method of learning that relies on inductive biases. These learning habits begin to develop in childhood as children learn how to learn, and they enable the human ability to learn new concepts from just a few examples. While artificial learning systems like neural networks require hundreds or thousands of examples to learn how to recognize similar objects, they can develop inductive biases much like humans.\n\nIn a new paper, CDS Faculty Member Brenden Lake, with a research partner, investigates whether neural networks develop a particular type of inductive bias called a shape bias. A shape bias is the inclination to relate a new object to a previously seen object of the same name according to shape rather than by texture or color. (While the word bias can carry a negative connotation, shape biases have been shown to be effective facilitators of word learning in early childhood.)\n\nLake modeled his study based on a cognitive study about shape bias conducted with toddlers. The toddler cognitive study involved a 1st-order and 2nd-order generalization test. In the 1st-order test, toddlers categorized unfamiliar objects by shape according to the shape of a familiar object; in the 2nd-order test, toddlers categorized wholly unfamiliar sets of objects by shape.\n\nUsing this framework of 1st and 2nd-order generalization tests, Lake constructed three experiments. In the first experiment, a multilayer perceptron was trained to name synthetic stimuli that represented objects, and the results from the 2nd-order test showed that the network in this experiment passed the threshold for shape bias. In the second experiment, a convolutional neural network (CNN) was trained with synthetic object stimuli encoded as raw images. It was able to learn a shape bias from as few as six examples of eight categories.\n\nFor the future, Lake and his research partner state, \u201cOne implication of this finding is that it may be possible to train largescale image recognition models more efficiently after initializing these models with shape bias training. In future work, we hope to investigate this hypothesis with ImageNet-scale deep neural networks, using an initialization framework designed with the intuitions garnered here.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/love-thy-neighbor-measuring-immigrant-integration-in-world-cities-using-twitter-data-b7b5ea1182c1",
        "title": "Love thy neighbor? Measuring immigrant integration in world cities using Twitter data",
        "text": "Although many cities today are multicultural hotspots, immigrant integration is still an on-going challenge \u2014 primarily because successful integration depends on several aspects like obtaining an education, finding employment, honing the key languages of the new country, and more. How can we measure the current state of immigrant integration?\n\nResearchers typically use metrics like spatial segregation to assess how integrated \u2014 or isolated \u2014 immigrant groups are, relative to the wider community. But the rise of social media means that researchers can also analyze the spatial segregation of languages through data from platforms like Twitter.\n\nNot only does Twitter data \u201chave the particularity of extending beyond national borders,\u201d explained Gon\u00e7alves in his recent co-authored paper in PLOS, but it can also \u201cquantify the spatial integration of immigrant communities\u201d by analyzing the spatio-temporal patterns of different languages in a given geographical location.\n\nWith this in mind, the researchers collected over 350 million tweets posted by 14.5 million users between the years of 2010 to 2015 to examine immigrant integration in 53 cities.\n\nAfter extracting the UserID, geographical coordinates, date, time, and text of every tweet, they used some clever filtering techniques to confirm that each user actually lives in the place where they are tweeting (e.g. they\u2019re not just a visitor). Some of these filtering techniques involved calculating number of consecutive months of activity of each user, and the minimum number of hours spent by each user in the geographical area where their tweets are coming from.\n\nThen, the researchers used CLD2 (Chromium Compact Language Detector) to identify the language of each user\u2019s tweets. In addition to accounting for mutually intelligible languages and dialectical varieties, the researchers also labeled the official language of each city that they were examining as the \u201cLocal\u201d language.\n\n\u201cAfter defining the Local languages in each city,\u201d the researchers said, \u201cwe assign[ed] to each user its most frequent language. In case of bilingual/multilingual users, we set as user\u2019s language the one which differs from English or Local unless there are only two languages in [their] dictionary.\u201d\n\nThe researchers also decided to remove English from their analysis because it\u2019s the world\u2019s lingua franca. \u201cMoreover,\u201d the researchers added, \u201cthe role of English is dominant mainly in the worst links in terms of integration.\u201d\n\nAfter discarding English, their investigation yielded some fascinating results. \u201cArabic rises as the most common spatially segregated community,\u201d the researchers explained, \u201cfollowed by French-speaking communities that are spatially concentrated in other European countries such as Germany and Turkey.\u201d\n\nOn a more positive note, they point out that London is in the lead of hosting diverse communities, followed by San Francisco, Tokyo, Los Angeles, Manchester, and New York.\n\nOf course, however, the researchers caution that Twitter data is only a partially representative sample of the population because the platform itself contains several biases, from the overrepresentation of young people, to the possibility that certain communities \u2014 like Chinese immigrants \u2014 may not use Twitter because it is inaccessible in their country of origin (China). Still, as Gon\u00e7alves and his researchers remind us, \u201cthe important question here is not whether we can find all the [immigrant] communities, but whether we are able to say something meaningful about those detected.\u201d\n\nClick here to learn more about this study."
    },
    {
        "url": "https://medium.com/center-for-data-science/i-got-99-problems-and-the-law-is-one-how-can-we-measure-legislative-efficacy-1d08a903ce4e",
        "title": "I got 99 problems \u2014 and the law is one: how can we measure legislative efficacy?",
        "text": "In 2014, The Washington Post published an article summarizing the career of retired US legislator Robert E. Andrews under a damning headline: \u201cAndrews proposed 646 bills, passed 0: worst record of past 20 years.\u201d\n\nOuch. The statistic appears to suggest that Andrews\u2019s career was a flop \u2014 but as CDS\u2019s Data Science Fellow Andreu Casas explained at a recent Moore Sloan Research Lunch Seminar, new conclusions arise once we consider how the legislative system functions as a whole, and use a more nuanced approach to analyzing the data about successful and unsuccessful legal bills.\n\nPresently, researchers use a metric named the Legal Effectiveness Score (LES) to analyze the efficacy of bills and legislators. The LES scoring system measures how far a particular bill advances within the complex multi-step legislative process.\n\nWith this in mind, analyzing the evolution of these \u2018hitchhiker\u2019 bills, as Casas and co-authors Matthew Denny and John Wilkerson called them, instead of simply counting how many bills passed and failed, would be a more accurate way of measuring legislative efficiency. The question is, how can this be done?\n\nAfter compiling a dataset of 104,005 versions of non-enrolled bills and 4,073 enrolled bills from the 103rd to 113th Congress between the years of 1993 to 2014, Casas and colleagues tracked the insertion of non-enrolled bills into laws using an ensemble of NLP algorithms (that boast a 95% accuracy rate!). Essentially, these algorithms first pre-process the text of bills, reducing them to their core expression, and then evaluate the extent to which the full meaning of each non-enrolled bill has been inserted into a bill that became law in that same Congress.\n\nTheir investigation yielded some revealing conclusions. For example, not only do more senate bills become law as hitchhikers on house laws (1,118) than when enacted on their own (1,037), but that they often become law when included in a bill that concerns a different topic. The key role that hitchhiker bills play in forming larger bills suggest, Casas concluded, that the legislative system is more decentralized and less partisan than we think. When taking these hitchhikers into account,we see that legislation is shaped by more viewpoints, interests, and people.\n\nAn open question, however, is whether we have the right people in Congress. Will data science one day have the power to identify the ill-intentioned from the heroes? Well, it\u2019s not a reality yet \u2014 but one can certainly dream."
    },
    {
        "url": "https://medium.com/center-for-data-science/science-versus-religion-they-may-not-be-as-opposed-as-you-think-61464ee5fca0",
        "title": "Science versus Religion\u2014they may not be as opposed as you think",
        "text": "Science versus Religion\u2014they may not be as opposed as you think\n\nAmerican policy debates about issues like abortion, euthanasia, genetic engineering, and the teaching of evolutionary theory arose from a cultural conflict between science and religion in the 1980\u2019s. While this may suggest that science and religion are inherently opposed systems, a new study by CDS Affiliated Faculty Member Paul DiMaggio, Professor of Sociology, suggests that the reality is more nuanced.\n\nThe new research from DiMaggio and his research team surpasses the typical limits of research about American attitudes toward science and religion in two significant ways. First, it incorporates spiritualism as a third dimension to the debate. Second, it uses two types of data analysis to classify attitudes.\n\nDiMaggio and collaborators applied their two methods of data analysis to the 1988 General Social Survey (GSS) which included 1018 respondents. The first method of data analysis they used, Latent Class Analysis (LCA), identifies subsets of respondents who are in agreement. Members within each subset share positions\u2014for example, members of one subset might agree that science should be trusted more, while members of another subset might put more trust in religion. LCA identified five subsets among the 1988 GSS respondents: Pro-science skeptics (29% of respondents), Anti-clerics (10%), Religious traditionalists (24%), Institutionalists (23%), and Spiritualists (14%).\n\nThe second method of data analysis used by DiMaggio and collaborators, Relational Class Analysis (RCA), identifies subsets of respondents who agree about relationships between items. Members of one subset identified by RCA might, for example, share the attitude that science and religion are inherently opposed, while members of another RCA subset might believe that science and religion do not impinge on each other and can coexist. Members of RCA subsets do not necessarily agree about the issues, but they agree about how items are related. RCA identified three subsets from the 1988 GSS: Science vs. Spiritualism (39%), Religion vs. Science (40%), and Domain Decoupling (a subset that did not construe an opposition between science/religion/spiritualism) (21%).\n\nThe inclusion of spiritualism as a dimension in DiMaggio\u2019s research was critical to exposing this separate dichotomy, but the researchers caution that the survey included responses about spiritual experiences rather than attitudes. They also caution about extending generalizations from their research to the present, given their reliance on data from 1988. Still, their work offers a framework for future research about cultural attitudes, and highlights a potential pathway for alliances between conservative Christians and Americans who have more faith in science."
    },
    {
        "url": "https://medium.com/center-for-data-science/mobile-health-apps-to-download-or-not-to-download-feaf4415f979",
        "title": "Mobile Health Apps: To Download or Not To Download?",
        "text": "For individuals with chronic conditions, adhering to a disease management plan can prolong and enrich life while minimizing healthcare costs. But properly managing a chronic condition can be stressful and demanding. One way chronically-ill people can ease this stress is by using mobile health apps which can facilitate disease management by promoting healthy lifestyle behaviors, tracking exercise, improving nutrition, assisting with weight loss, reminding people to take medication, and more.\n\nOver 3500 health apps are currently available for individuals with chronic conditions, with most designed for users with diabetes or depression. Yet, as mobile health apps have proliferated, little research has tracked how people with chronic conditions actually use them. In a new study, however, CDS\u2019s Dustin T. Duncan, Assistant Professor of Population Health and a team of researchers sought to address this problem and to determine if chronically-ill people believe they can benefit from mobile health apps.\n\nThe researchers sourced their data from a national sample of mobile phone users collected in 2015 for a previous study about mobile phones and health.\n\nAccording to the demographic qualifications for the new study, the sample yielded 1604 respondents to a survey which included 36 questions about health and mobile health app use.\n\nFor their data analysis, the researchers examined health app download and usage by chronic condition, self-reported health, physical activity, and according to demographic factors. 37% of respondents reported very good health and 13% reported excellent health. The most common chronic conditions reported by the remaining participants were hypertension, obesity, diabetes, depression, and high cholesterol.\n\nFor future intervention, the researchers propose improving how the benefits of health apps are communicated and marketed to people with chronic conditions. The researchers also suggest future studies to determine perceptions about the value of health apps among healthcare providers. As new data and analyses continue to emerge about attitudes toward health apps, individuals with chronic conditions stand to benefit from altering perceptions and increasing usage of health apps."
    },
    {
        "url": "https://medium.com/center-for-data-science/memory-lanes-using-neural-circuit-architectures-for-predicting-recognition-behavior-84e7e060bcbb",
        "title": "Memory lanes: using neural circuit architectures for predicting recognition behavior",
        "text": "What sparks recognition? Neuroscientists speculate that it\u2019s usually either \u201cdue to recollection (\u2018here comes my old school buddy\u2019),\u201d as Cristina Savin explains in her most recent co-authored paper, \u201cor due to a vague sense of familiarity (\u2018I\u2019m sure I\u2019ve met this person before, but I have no idea when and why\u2019).\u201d\n\n\u201cDespite over 30 years of memory recognition research,\u201d the researchers explain, \u201cno consensus exists about which class of models provides a more satisfying account of the data.\u201d Savin and her co-author, M\u00e1t\u00e9 Lengyel, however, have developed a new neural circuit architecture that combines both SM and DM models. The neural network provides a more wholesome analysis of neural data, and can \u201cefficiently operate in the face of trace strength-ambiguity.\u201d\n\nRead more about it here."
    },
    {
        "url": "https://medium.com/center-for-data-science/guitar-set-a-new-dataset-for-music-information-retrieval-41b7861a87d7",
        "title": "Guitar-Set, a New Dataset for Music Information Retrieval",
        "text": "Rock-n-roll. Classical. Country. Blues. Punk. Pop. The guitar can do it all. Its substantial range attracts multitudinous musicians and listeners, and it provides engaging material for music research. But for researchers in the music information retrieval (MIR) community, the guitar\u2019s versatility can also complicate analytic processes.\n\nTo facilitate guitar-related music research, CDS affiliated faculty member Juan P. Bello, Associate Professor of Music and Music Education, is developing a new dataset, called Guitar-Set, that consists of recorded guitar performances with detailed annotations. Many existing methods for automated analysis of guitar recordings depend on high-quality labeled data-sets, but labelling these data-sets takes significant time and money.\n\nBased on the hexaphonic recordings, Guitar-Set includes note-level annotations of string and fret position to recorded metadata about tempo, key, style, beat and downbeat, and chords. Guitar-Set\u2019s objective is to create these note-level annotations from individually recorded strings, but the researchers found that a full automation of this process is challenging. They encountered a high rate of false positives on the hexaphonic recording due to picked up vibrations that were not audible on the microphone-recorded track. While this requires a manual comparison between the two types of recordings, Bello and collaborators were able to mitigate some of the false positives by first running the hexaphonic data through a bleed removal algorithm.\n\nDespite the challenges, Guitar-Set will be a useful resource for researchers in the MIR community. Bello and collaborators also anticipate that the new method will aid other tasks such as source separation, understanding sympathetic resources in the acoustic guitar, and understanding guitar right-hand activity."
    },
    {
        "url": "https://medium.com/center-for-data-science/demystifying-deep-learning-f954de6432ef",
        "title": "Demystifying deep learning \u2013 Center for Data Science \u2013",
        "text": "A reason why recognition systems in fields like natural language processing and computer vision have improved so dramatically is due to the increasing application of deep learning networks. The approach has been such a success that deep learning has not only become a buzzword in the tech community but also in mainstream news.\n\nBut as we race towards building bigger, better, and faster machines, it\u2019s important to think about why deep learning is so successful. What are deep learning\u2019s properties? What makes one deep architecture better than another? And, based on the observations that we\u2019ve collected during deep learning\u2019s young lifespan (after all, the powerful tool only emerged in the late-1980s thanks to the likes of Geoff Hinton and Yann LeCun), can we begin to form a rigorous theory about how and why the approach works?\n\nTo get to the heart of the matter, we have to return to the powerful language that underpins all aspects of science and technology: mathematics. Enter Joan Bruna\u2019s most recent co-authored working paper with Rene Vidal (Johns Hopkins), Raja Giryes (Tel-Aviv), and Stefano Soatto (UCLA), which aims to provide some mathematical explanations for some of deep learning\u2019s properties.\n\nTo begin addressing this gap, the researchers collect, consolidate, and explore the mathematics that underpins deep learning\u2019s success \u2014 from aspects like global optimality to geometric stability. Learn more about the work here."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-researchers-invent-new-real-time-data-analysis-system-for-humanitarian-agencies-94d3a0a65850",
        "title": "NYU researchers invent new real-time data analysis system for humanitarian agencies",
        "text": "How can we efficiently help those in need? A difficult reality facing humanitarian agencies is that they cannot immediately address all of the world\u2019s crises, particularly when constrained by limited financial and human resources. This is why they prioritize which emergencies they respond to. But the prioritization process, which involves collecting, organizing, and analyzing vast volumes of secondary data produced by public institutions, NGOs, and news media about each crisis around the globe, is highly time consuming and relies solely on manual human labor.\n\nA new targeted information retrieval system, however, aims to automate these tasks for humanitarian workers so that they can make decisions about aid delivery and disaster response more quickly. Invented by NYU doctoral student Kien Pham and a research team comprising of Juliana Freire, the Executive Director of the Moore-Sloan Data Science Environment at the NYU Center for Data Science, and experts from IBM\u2019s Thomas J. Watson Research Center, the new system contains four main components: a focused web crawler, a metadata extractor, a content classifier, and a feedback mechanism.\n\nCrawlers generally strive to cover as many pages as possible, but \u201ca focused crawler,\u201d as the researchers explain, \u201cis a web crawler that is optimized to seek web pages that are relevant to predefined topics.\u201d And, because emergency situations tend to change rapidly, the researchers also designed a real-time re-crawling strategy in the system. Using a binary classifier, the crawler then categorizes whether particular webpages are relevant or irrelevant to the user\u2019s search topic, and then passes the webpages onto the metadata extractor.\n\nThe extractor concentrates on mining the textual data of those webpages. After singling out the title, content, publication date, and mentioned countries from the relevant webpages that the crawler passed on, a content classifier analyzes and labels the webpages according to what type of crisis they are describing.\n\nBecause the system\u2019s efficiency hinges on the accuracy of the content classifier, the researchers built a vital feedback cycle into the system, which collects user feedback so that the classifier can improve over time. \u201cThis especially increases the robustness of the page classifier,\u201d the researchers explain, \u201cas well as the adaptivity of the crawler.\u201d\n\nThe researchers recently implemented a fully operational prototype of their system for humanitarian experts at the Assessments Capacities Project (ACAPS), an organization that supports crisis responders by providing needs assessments and analysis.\n\nWhile more work still needs to be done to tailor the system for domain-specific needs, the researchers hope that it will not only be widely implemented at humanitarian agencies in the future, but also incorporate social media data into its processes as well."
    },
    {
        "url": "https://medium.com/center-for-data-science/call-me-maybe-a-new-algorithm-detects-call-activity-using-smartphone-sensors-2d354e58eaef",
        "title": "Call me, maybe: a new algorithm detects call activity using smartphone sensors",
        "text": "To make an overseas phone call 50 years ago, you either had to buy a phone card or cough up wads of cash to foot an expensive phone bill.\n\nBut, today, all you have to do is use one of the many internet-based phone apps like Skype, WhatsApp, or WeChat. Not only are these apps free, but they also have strong privacy protocols in place. Unfortunately, however, this is precisely why criminals and other ill-intentioned individuals have begun exploiting such apps to communicate with each other. When suspicious or forbidden calls are placed through these apps, then, security officials cannot effectively identify whom is calling whom, or when such calls are taking place.\n\nBut the new real-time phone call detection algorithm that graduate students Huiyu Sun (lead author) and Bin Li, together with Suzanne McIntosh (professor of Computer Science and CDS affiliated faculty member), have invented is poised to become a powerful tool for assisting the security sector, and more.\n\n\u201cThe proximity sensor,\u201d the researchers explain, \u201cmeasures the closeness between an object and the phone\u2019s screen in centimeters.\u201d The orientation sensor measures \u201cthe phone\u2019s rotation angles in degrees around the x-axis, y-axis, and z-axis.\u201d The orientation sensor can be used to detect whether the caller is sitting/standing, walking, or lying down. A proximity or orientation sensor alone cannot detect a phone call taking place, but the researchers found that combining the sensors leads to successful call detection.\n\nAfter collecting the range of orientation and proximity measurements both when a call is taking place and when it is not, the research team\u2019s algorithm can perform call detection by tracking subtle changes in the prevailing call state classification (e.g. is the person sitting/standing, lying down, or walking) and fusing that with sensed proximity.\n\nTheir system was trained using three classifiers \u2014 Na\u00efve Bayes, SVM, and Logistic Regression \u2014 although the researchers point out that \u201cthere are many more classifiers that could be used such as decision tree, random tree, and neural networks. It is possible that other classifiers could produce better performance.\u201d\n\nPotential applications for their system, the researchers added, include \u201cassisting human activity recognition systems, monitoring health conditions, and many more areas.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/how-do-you-spot-a-russian-bot-answer-goes-beyond-kremlin-watching-new-research-finds-7f231100b428",
        "title": "How Do You Spot a Russian Bot? Answer Goes Beyond Kremlin Watching, New Research Finds",
        "text": "A team of researchers has isolated the characteristics of bots on Twitter through an examination of bot activity related to Russian political discussions.\n\nIts findings, reported in the journal Big Data, provide new insights into how Russian accounts influence online exchanges using bots, or automated social media accounts, and trolls, which aim to provoke or disrupt.\n\n\u201cThere is a great deal of interest in understanding how regimes and political actors use bots in order to influence politics,\u201d explains NYU Professor Joshua Tucker, director of the Jordan Center for the Advanced Study of Russia and one of the paper\u2019s co-authors. \u201cRussia has been at the forefront of trying to shape the online conversation using tools like bots and trolls, so a first step to understanding what Russian bots are doing is to be able to identify them.\u201d\n\nThe findings reveal some notable differences between human and automated posts \u2014 but also several similarities, which may stymie bots\u2019 detection.\n\n\u201cBots are much more likely to use online platforms while humans frequently use mobile devices,\u201d notes co-author Denis Stukal, a doctoral candidate in NYU\u2019s Department of Politics. \u201cHowever, humans and bots are not dramatically different from each other on a number of other features that characterize their tweeting activity \u2014 similarities that reveal a relatively high level of bots\u2019 sophistication.\u201d\n\nThe researchers focused on two specific periods \u2014 February 6, 2014 through October 1, 2014 and January 30, 2015 through December 31, 2015 \u2014 that were notably consequential in Russian politics. They included the Russian annexation of Crimea, conflict in Eastern Ukraine, and the murder of a Russian opposition leader, Boris Nemtsov, in front of the Kremlin. Their analysis included approximately 15 million tweets sent from about 230,000 Russian Twitter accounts \u2014 including 93,000 that were active during both periods.\n\nInterestingly, of those accounts active in both periods, nearly 63,000 (67 percent) were bots. Moreover, among accounts actively tweeting about Russian politics, on the majority of days the proportion of tweets produced by bots exceeded 50 percent \u2014 and this figure increased dramatically around the time of the Russian annexation of Crimea.\n\nOther patterns revealed how bots differ from human posts. In addition to distinctions in platform origination (mobile devices for humans vs. the Web for bots), which is the best predictor of whether or not a tweet is from a bot, the researchers found the following:\n\nHowever, the findings did not suggest bots are exclusively, or even largely, a tool of the Russian government.\n\nThe researchers found that many bots spread pro-regime information, but also that there may be anti-regime bots that either disseminate information about opposition activities or criticize and deride the regime."
    },
    {
        "url": "https://medium.com/center-for-data-science/where-are-you-going-next-a-new-population-movement-predictor-is-on-the-case-ea461c206ad",
        "title": "Where are you going next? A new population movement predictor is on the case",
        "text": "Predictions of human movement patterns can improve transportation management, urban forecasting and development, traffic congestion, and understanding the spread of disease. So far, many data science projects have sought to either predict population movement patterns as a whole or a person\u2019s next location based on a current given location, often using GPS or call data record tracking.\n\nBut Rumi Chunara, a CDS Affiliated Professor, is taking a different approach. Along with two NYU doctoral students, Nabeel Abdur Rehman and Kunal Relia, Chunara has devised a new algorithm for predicting individual mobility patterns that uses publicly available geo-located Twitter data.\n\nFor their dataset, the researchers used data from six months of publicly available geo-located Tweets from the Twitter API. The data ranges between January 1st \u2014 June 30th 2014 and comes from users in New York, Washington, D.C. and San Francisco. This originally yielded over twenty-million Tweets, but this dataset was filtered to exclude accounts with unfeasible location movement, non-personal accounts, accounts using location spoof software, and accounts that tweeted fewer than four times in the six month period. The inclusion criteria limited the dataset to tens of thousands of Tweets which were adjusted for time zone shifts and daylight savings.\n\nThe researchers uniquely address location and time, two fundamental components of predicting mobility. To define location, the researchers divided the area of each city into segments of one square mile. They further subdivided each area into 0.5 square miles and 0.1 square miles, but, intuitively, the algorithm performed best when attuned to larger sections. For time intervals, given the sparse nature of data, the researchers chose one and two hour intervals rather than shorter fifteen or thirty minute intervals which have been used in other research.\n\nWith this set of metrics for location and time, the algorithm can predict individual user location with up to 86% accuracy \u2014 almost 20% better than compared methods. To make accurate predictions, the algorithm incorporates both personal behavior to identify a home and work location (based on next location, previous location, and historical locations) and community behavior, which especially improves accuracy on the weekend when users have a lower likelihood of being at home or work.\n\nChunara and collaborators envision many real-world applications for their research. In particular, they hope that mobility patterns derived using Intermediate Location Computing, coupled with additional information about individuals from social media data, will enhance disease transmission modeling at local levels."
    },
    {
        "url": "https://medium.com/center-for-data-science/delving-into-the-past-present-future-of-dna-sequencing-with-richard-bonneau-8ac4faeab374",
        "title": "Delving into the Past, Present, & Future of DNA Sequencing with Richard Bonneau",
        "text": "If we want know how any living system works \u2014 whether it be a flower or an animal \u2014 we must first discover what its DNA sequence is, because this sequence is the blueprint cells use to create proteins (a cell\u2019s molecular machines) for any living organism, such as hemoglobin (a protein that carries oxygen) or collagen (a protein that forms connective tissues).\n\nLearning more about DNA sequences also means that we can start modifying those sequences. After all, \u201clife,\u201d as CDS Director Richard Bonneau explained at the Courant Holiday Lecture last week, \u201cplays with protein chains\u201d \u2014 and so can scientists. For example, we now have the ability to modify parts of human genome involved in life-threatening diseases, or enhance parts of plant genomes to make them produce proteins that increase their resilience to drought or blight.\n\nThat DNA sequencing and modification can help us improve living systems is one of the many reasons why the task is of major importance to the scientific community. But, until recently, the cost of DNA sequencing was excruciatingly expensive. \u201cWhen I was a graduate student,\u201d Bonneau said, \u201csequencing a few human genomes cost up to $10 billion dollars, and it was usually a multi-institutional effort.\u201d That cost now hovers near $5,000. This reduction in cost has led to an explosion of biologists racing to identify DNA sequences for various organisms. Today, we\u2019re not only sequencing our own DNA; we\u2019re also sequencing the DNA of the bacterial in and on our bodies. (An example of an effort to annotate these microbial genomes is the Microbiome project that IBM and the Simon\u2019s Foundation launched earlier this year.)\n\nThe rapid accumulation of gene sequences has also meant that biologists are turning to data science to help them analyze their datasets. Machine learning, for example, is becoming an increasingly effective approach for predicting what the gaps or \u201cdark matter\u201d might be in particular DNA sequences \u2014 an approach that Bonneau has recently explored in a co-authored working paper.\n\nThe improved future that biotechnology promises, however, also raises some ethical dilemmas that require our attention. For example, \u201cjust because we can extend human lifespans,\u201d Bonneau asked a packed audience, \u201cdoes it mean that we should?\u201d He explained that part of why death is a necessary reality for all living organisms is because it helps to decrease competition for limited resources between parents and offspring. \u201cDeath is a critical part of evolution,\u201d Bonneau said. \u201cThus, for the individual the answer is , yes extend lifespan, but for evolution the answer remains no, don\u2019t extend lifespan much post reproduction and care for offspring.\u201d\n\nMoving swiftly from biology to data science to ethics, the public lecture provided a revealing snapshot of how far the scientific community has come \u2014 and how far we still have to go."
    },
    {
        "url": "https://medium.com/center-for-data-science/machine-learning-for-earth-observation-with-fran%C3%A7ois-petitjean-a0cd1182eb0c",
        "title": "Machine learning for earth observation with Fran\u00e7ois Petitjean",
        "text": "I did my PhD while employed by the French Space Agency (CNES); my role was to prepare the data analysis techniques to support the upcoming Sentinel-2 mission. The Sentinel-2 program, which most recently put a satellite successfully into orbit in March 2017, is a revolution for Earth observation: it provides a full high-resolution picture of our planet every 5 days. This represents a massive amount of data, and machine learning is vital for making sense of it.\n\nData science and machine learning are central to my research on Earth observation; I was fortunate to join the fantastic Machine Learning group at Monash University in Melbourne, Australia in 2012, and to take up a tenured position there in 2016.\n\nWorking in data science is indeed fast-paced, and especially when working with images such as the ones captured by Sentinel-2. Recent years have brought a revolution with deep learning and we all have had to adapt quickly. I think this is, however, not unique to computer science; we\u2019re seeing rapid developments in many fields. Think of CRIPSPR/Cas9 in genomics or the discovery of gravitational waves!\n\nAlthough the way research is done around the world is getting more and more uniform, different cultures certainly bring new perspectives. For example, French researchers tend to think of research in terms of \u201cinteresting problems,\u201d more than \u201cvaluable problems.\u201d Australia is a little less industry-driven than the USA while having some of the smartest people I have met in the world (I\u2019m thinking particularly of my colleagues Professors Geoff Webb and Wray Buntine). In the USA, I have learned how to think of where to go and how to get there efficiently: I learned from Americans how to drive a train at full speed while building the rail for it at the same time.\n\nNYU has been fantastic; I\u2019ve been given all the right conditions for a successful visit, including incredible meetings with CDS academics (I\u2019m thinking particularly of Professors Foster Provost and Yann LeCun) and probably the best seminar series I have had the chance to see (with Professors Jean Ponce, Francis Bach, Yann LeCun again, Kyunhyun Cho). At CDS, you really feel that you are at the center of things."
    },
    {
        "url": "https://medium.com/center-for-data-science/how-can-we-achieve-data-privacy-lets-change-the-privacy-paradigm-6e91642a3e09",
        "title": "How can we achieve data privacy? Let\u2019s change the privacy paradigm",
        "text": "Whether it\u2019s financial data or biometric data, the question remains the same: what can we do to protect our data from hackers, thieves, and the ill-intentioned? Well, as Professor Guillermo Sapiro from Duke University explained at the most recent Math and Data (MaD) research seminar at NYU CDS, answering this question may require us to re-think what privacy actually is.\n\nTo be clear, Sapiro refers not to differential privacy of vast populations but, rather, to privacy as it pertains to specific individuals.\n\nFor example, let us take biometric data. When you travel from one country to another, border patrol officers often use cameras that capture data about your facial structure to confirm your identity.\n\nThe facial recognition software that they use typically aims to measure and record specific details like the distance between your eyes, the width of your nose, and the shape of your cheekbones. Yet, during this process, Sapiro explained, the camera also inevitably captures other details like your gender, ethnicity, height, and weight. Despite our best efforts, then, universal privacy remains impossible in such scenarios.\n\nInstead of focusing our efforts on obtaining universal privacy, then, we need to be asking sharper questions.\n\nTo that end, Sapiro and his research team have been working on building such a framework for closed machine learning systems. Using the above scenario of facial recognition as a case study, Sapiro is exploring ways to construct facial recognition algorithms so that they can confirm someone\u2019s identity without revealing the person\u2019s gender.\n\nA common strategy that many are using to protect their data is to add noise to it \u2014 but what kind of noise would be most effective for this particular scenario?\n\nTaking advantage of new techniques like GANs (Generative Adversarial Networks \u2014 a deep learning algorithm that can create semi-realistic photographs based on the image data that it has been trained on), Sapiro and his team are exploring whether an effective strategy may be to \u201ccheat\u201d the system by training cameras to create a version of yourself in both genders to mask your real gender identity. This way, the camera\u2019s facial recognition technology can still perform the important work of confirming your identity, but the data about your gender remains private.\n\nThe preliminary results on their framework look promising: the accuracy of the camera guessing one\u2019s gender dropped from 96% to 45% \u2014 less than half! \u2014 once their framework was added to the system, while the accuracy of its facial recognition capabilities remained more or less the same.\n\nIf successful, their framework could be implemented across a wide range of institutions \u2014 particularly hospitals. Medical research depends on hospitals sharing data on their patients, but this is often a difficult process as doctors must also protect some aspects of patient data due to confidentiality guidelines. With a framework like this one, however, hospitals and other institutions will be able to share their data publicly while also keeping key data points private."
    },
    {
        "url": "https://medium.com/center-for-data-science/a-new-program-for-art-historians-in-the-digital-age-590edc2b9f74",
        "title": "A New Program for Art Historians in the Digital Age",
        "text": "An art historian\u2019s job typically requires a detailed comparison of two or more works of art. Before the digital age, the conventional method involved examining and arranging physical prints, photographs, or slides on tables or light boxes.\n\nNow art historians rely almost entirely on digital images from their own databases, special collections, or the web. Yet, no integrated software program is attuned to the needs of art historians. Instead, they rely on a hodgepodge of incompatible programs for separate tasks including grouping images, saving and sharing work, making annotations, and performing detailed analysis. Spreading different tasks across incompatible programs complicates an art historian\u2019s job and makes research unnecessarily laborious.\n\nBut a multidisciplinary team of researchers, including CDS faculty members Claudio T. Silva and Juliana Freire, have a solution. They have designed a unique program specifically for art historians called ARIES (ARt Image Exploration Space), funded partly by a seed grant from the Moore-Sloan Data Science Environment at CDS. ARIES is an intuitive web-based interface that supports exploration, manipulation, annotation, grouping, and sharing of art images within a single digital environment.\n\nARIES equips art historians with a virtual toolkit that enables meaningful interactions with digital images. Its main component is a lightbox canvas that allows dynamic overlays, arrangements, juxtapositions, zoom features, filters, annotations, lenses, and selection tools. Art historians can easily save and share their work within ARIES so another expert can seamlessly continue.\n\nBeyond the lightbox canvas, however, ARIES also helps art historians identify differences between artworks by generating a pixel heat map. The pixel heat map can be useful for tracking an artist\u2019s progression over time, evaluating the quality of reproductions, or spotting forgeries.\n\nARIES also incorporates metadata associated with images (i.e. artist, date, geography, and relative size). Based on stored metadata, ARIES users can search from a database to sort images and arrange them by timeline, map them on a Geochart, or display them side-by-side according to relative size. The relative size feature is especially useful for museum curators to visually plan exhibitions.\n\nThe ARIES development team plans to improve the program by adding support for real time collaboration and additional ways to input and leverage metadata. While ARIES was crafted specifically for art historians, the program can help other academics and professionals who interact with a high volume of digital images such as designers, photographers, artists, scientists, and digital humanists."
    },
    {
        "url": "https://medium.com/center-for-data-science/mobility-data-new-ways-to-measure-socio-economic-development-predict-activity-in-urban-spaces-d81cd922dfad",
        "title": "Mobility Data: New Ways to Measure Socio-economic Development & Predict Activity in Urban Spaces",
        "text": "The way people inhabit metropolitan landscapes changes over time, and it often changes rapidly. Traditional methods for tracking urban change \u2014 like censuses and surveys used by governments and special interest groups \u2014 can be costly, cumbersome, and out-paced by the rate of change in a metropolis.\n\nAnastasios Noulas, a Moore-Sloan Data Science Fellow at NYU CDS, addresses this problem by developing new data-driven methods for tracking changes in urban populations and environments. His methods use digital datasets from online social media sources. Two of Noulas\u2019s recent projects involve population activity in London.\n\nIn one project, Noulas and his collaborators used data analytics to determine the effects of cultural investment on socio-economic development. The researchers leveraged existing census data from 2010 and 2015, government spending reports about investment in cultural development, and population mobility data from user check-ins on the location-based social media platform Foursquare.\n\nThrough the application of machine learning algorithms and network metrics, the researchers performed an analysis on the relationship among these three data sets to map how cultural investment impacted specific London neighborhoods over time. They also defined a new set of metrics and proposed a highly accurate model to predict the effects of cultural investment. Their prediction model can help governments more effectively target resources to stimulate culture and commerce in disadvantaged communities.\n\nIn another project, Noulas and collaborators were able to predict the temporal visitation patterns of newly established businesses by using a similar dataset as input for machine learning models. The temporal visitation patterns identified by the machine learning model quantified what we intuitively know: people are more likely to visit certain types of venues (such as bars, gyms, restaurants, or commuter hubs), on certain days of the week, at certain times of day, in certain neighborhoods.\n\nThe prediction model from Noulas\u2019s project, however, is far more useful than simply confirming common intuition. With a high degree of specificity, the algorithm can predict the number of visitors at any venue visited by Foursquare users. This prediction capability is especially useful for business owners who want to target the best resources for marketing, promoting, supplying, or maintaining a venue."
    },
    {
        "url": "https://medium.com/center-for-data-science/still-dont-believe-in-climate-change-take-a-look-at-tree-ring-data-92b5090268e0",
        "title": "Still don\u2019t believe in climate change? Take a look at tree ring data",
        "text": "The weather is fickle \u2014 so much so that climate change skeptics usually argue that global warming is a false phenomenon since the Earth\u2019s weather has always fluctuated between hot and cool periods. Moreover, as these skeptics typically go on to say, how do we know what the weather was like a thousand years ago anyway? We have no temperature records, precipitation data, or climate information from the past to prove that what we\u2019re experiencing in our century is drastically different from what the planet has experienced before.\n\nFortunately, we actually do have precipitation data about the past \u2014 and it\u2019s hidden right in our backyards: trees.\n\nAs Benjamin Cook from the NASA Goddard Institute for Space Studies explained at a recent Moore Sloan Research Lunch Seminar, scientists sometimes use trees to gather data about climatic conditions about the past because of the unique way that they grow.\n\nTrees take approximately one year to grow a new layer of wood, and each new layer is called a tree ring. Not only do the number of rings reveal how old a tree is but, interestingly, the amount of space between each tree ring (i.e. how thick each new layer of wood is) reflects how much moisture the tree received that year. For example, enough moisture and a strong growing period often results in a wide tree ring, but a dry drought season will mean a narrower ring.\n\nThe climate conditions of the past are recorded in the wood \u2014 meaning that we have evidence about past climates that stretch as far back as 800CE. And, what the tree ring data confirms \u2014 along with satellite image data about our planet\u2019s changing geography and global temperature records \u2014 is that our plant is, indeed, warming up at a distressing rate.\n\nAnd, not only are scientists using tree ring data to confirm that global warming exists, but they are also using the data to predict how our climate will change.\n\nCook went on to describe how he, his research team, and other scientists around the world are using our existing data to create models that predict how our planet\u2019s climate will change depending on the different courses of action that we take as a global community.\n\nExamples of these scenarios (or, as scientists say, Representative Concentration Pathways) include speculating about what would happen to our planet if we mitigated our greenhouse gas emissions, compared to what would happen if we took no action at all.\n\nThis predictive work requires an extraordinary amount of computing power and resources since each scenario needs its own model \u2014 which is why this has become a collaborative effort across multiple climate network temperature stations around the world.\n\nSo far, preliminary results from their multi-modal ensemble based on tree ring data and climate models predict that the future in water stressed regions like western North America will become even dryer than the extended dry spells of past centuries. \u201cIf we continue warming at the current pace,\u201d Cook warned, \u201ca megadrought by the end of the twenty-first century is virtually certain.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/phd-candidate-profile-vladimir-kobzar-184a5e211163",
        "title": "PhD Candidate Profile: Vladimir Kobzar \u2013 Center for Data Science \u2013",
        "text": "I am a Ph.D. candidate at the NYU Center for Data Science (CDS), where I am a member of the Math and Data Group. I earned my MS in Mathematics at the Courant Institute under the advisement of Professor Afonso Bandeira, as well as my JD and LLM from the NYU School of Law.\n\nBefore joining the CDS, I worked as a researcher at Argonne National Laboratory operated by he University of Chicago on the development of machine learning models for analysis of time-resolved X-ray scattering data. Before that, I served for several years as Executive Director at Goldman Sachs, where I advised on legal and regulatory aspects of transactions and other initiatives involving data and technology in growth markets.\n\nAt the CDS, I plan to continue working on the development of provably robust and computationally efficient algorithms at the intersection of mathematics and data science. I am also interested in designing a framework and methods for interpreting deep learning and other \u201cblack box\u201d algorithms. These frameworks and methods can then be used to analyze, among other things, the legal and regulatory ramifications of such algorithms."
    },
    {
        "url": "https://medium.com/center-for-data-science/what-kind-of-data-do-location-search-apps-like-foursquare-use-4836eefd572",
        "title": "What kind of data do location search apps like Foursquare use?",
        "text": "Since 2009, FourSquare has had over 11 billion global check-ins. How did this growth occur? At a recent Moore-Sloan Research Lunch Seminar, Foursquare\u2019s Enrique Cruz explained how data science transformed the app into a major location search tool for its users and clients.\n\nLet\u2019s say you\u2019re visiting a new city, and want to find a high quality restaurant, bar, or store. How does Foursquare score and rank each venue so that you can find the location that is best for your needs?\n\nThere are typically three datasets that Foursquare draws from to score various venues. The first is user feedback data, which refers to the explicit rating that users give to a venue after they have attended it.\n\nSecond is user tips data, which refers to comments that users leave behind about how they felt about a particular venue. Unlike aggregating the raw scores from user feedback data, user tips data requires Foursquare\u2019s scientists to perform complex sentiment analysis to parse and categorize user commentaries as positive, negative, or neutral. This linguistic data then needs to be converted numerically so that they can be incorporated into calculating the overall score of the venue. Finally, the last major dataset that Foursquare relies on is user foot traffic data, which refers to how many people are in a given place, as calculated by the app.\n\nTo increase the accuracy of their data, Foursquare also performs predicted check-ins. They run something like this: the app will predict where the user will go, and then ask the user to confirm a week later whether they did, indeed, visit that spot. With so much data, Foursquare is now able to predict the number of daily and weekly visits that a venue will have. These insights will undoubtedly be valuable for business owners who want to increase their client base, or the average Joe who just wants to know the best time of the day to avoid a long queue!"
    },
    {
        "url": "https://medium.com/center-for-data-science/phd-candidate-profile-xintian-han-72d124c94bb7",
        "title": "PhD Candidate Profile: Xintian Han \u2013 Center for Data Science \u2013",
        "text": "Xintian is from China, and obtained his B.S. degree in statistics from Peking University in 2017.\n\nHis current research interests focus on machine learning theory, methodologies and algorithms, especially on high dimensional problems, network models and deep learning. He is also interested in interdisciplinary research. He has done research on network sampling, and high dimensional statistics.\n\nFor network sampling, he develops a new model of respondent driven sampling without replacement and proves the asymptotic bounds of the variance of the estimator in this new model. He develops a simple linear time algorithm to sample a Poisson edge random dot product graph and extends this algorithm to sample simple graph and show that under certain settings this is an approximation to the Bernoulli edge random dot product graph. For high dimensional statistics, he shows the consistency of a new penalized weighted score function method to solve sparse Logistic regression problems.\n\nHe also proves the consistency of a new method to choose the tuning parameter based on the normal approximation of score function by Stein\u2019s method. He introduces a general framework of de-biased estimators for convex penalty functions in high dimensional regression by inverting KKT condition. He proposes an \u201cadd-one-in\u201d method to construct con\ufb01dence intervals for parameters in high dimensional problems, which uses LASSO to select the parameters and adds each one non-selected parameter into the selected set to build con\ufb01dence intervals by linear regression."
    },
    {
        "url": "https://medium.com/center-for-data-science/phd-candidate-profiles-sreyas-mohan-fe208bf0f08",
        "title": "PhD Candidate Profile: Sreyas Mohan \u2013 Center for Data Science \u2013",
        "text": "I am from a small town in the state of Kerala, India.\n\nI just graduated with a Bachelors in Electrical Engineering from Indian Institute of Technology (IIT) Madras. IIT Madras is ranked #1 among all engineering education and research institutes in India by the Ministry of Human Resources and Development, Government of India.\n\nI broadly work in the intersection of Machine Learning and Neuroscience. I use Machine Learning and Data Science techniques on problems in neuroscience to come up with interesting algorithms, and obtain more biological insights on what might be happening in the brain.\n\nI was awarded Subramanian Rajalakshmi Indira award at the 54th Convocation of IIT Madras for the best Interdisciplinary Project across all graduating B. Tech students in IIT Madras, the KVPY(Kishore Vygyanic Protsahan Yojna \u2014 translated as Young Scientist Encouragement Program) Fellowship by the Government of India in 2011, and a DAAD Scholarship by the Government of Germany.\n\nFind out more about Sreyas here."
    },
    {
        "url": "https://medium.com/center-for-data-science/teaching-machines-to-see-implicit-structures-with-jean-ponce-658d22fcc919",
        "title": "Teaching Machines To \u201cSee\u201d Implicit Structures with Jean Ponce",
        "text": "Data scientists are presently preoccupied with training computer vision models to \u201csee\u201d images as precisely as possible. But this is obviously easier said than done: it will take several generations of research before machines match the exacting prowess of the human eye. In the meantime, is there another strategy that we can explore to improve our computer vision models?\n\nAt a recent research seminar, Visiting Researcher from Inria at NYU CDS and former Director of the Department of Computer Science at France\u2019s prestigious \u00c9cole normale sup\u00e9rieure (ENS), Professor Jean Ponce, explained how he is using co-segmentation methodology to explore weakly supervised structure discovery in images and videos.\n\nWhat makes co-segmentation an effective technique is that it doesn\u2019t aim to train a machine to tell us what an image depicts but rather to train a machine so that it can identify images that are similar to each other.\n\nThis is precisely what Ponce and his research team are working on. Instead of trying to train machines to match the human eye, they\u2019re training machines to capture implicit visual structures by looking at an image in terms of several parts and boxes.\n\nSpecifically, their algorithms train machines to pay particular attention to boxes where there is more of the foreground image than the background image, because the foreground is typically where the primary object in an image is located.\n\nOnce the machine has split the image into boxes and identified the object, the next step is to train it to capture the general outline of the depicted object \u2014 box by box \u2014 and then prompt the machine to match those outlines with other images that depict the same object.\n\nPonce and his research team are also applying this technique for training machines to recognize objects in films. Using the popular Bourne trilogy as a case study, they trained their machines to identify all of the cars in the trilogy\u2019s exciting car-chasing scenes, and have since moved on to training their machines to spot animals in films, too.\n\nWhat makes his technique so intriguing is how it does not aim to copy the human eye, but is instead inspired by the way we neurologically process images. \u201cAfter all, it\u2019s not our eyes that see,\u201d Ponce reminds us, \u201cbut our brain.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/videos-ea141403d22c",
        "title": "Videos \u2013 Center for Data Science \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-center-for-data-sciences-phd-student-selected-as-an-adeptmind-scholar-43be5f67bf1e",
        "title": "NYU Center for Data Science\u2019s PhD student selected as an AdeptMind Scholar",
        "text": "New York, NY \u2014 Phu Mon Htut, a Ph.D. student at the NYU Center for Data Science (CDS), has been selected as an AdeptMind Scholar.\n\n\u201cWe\u2019re thrilled that AdeptMind has agreed to fund students CDS to work on natural language processing, machine learning, and information retrieval,\u201d says Sam Bowman, assistant professor of linguistics and data science. \u201cFor me, Phu Mon Htut was an obvious choice for a nominee \u2014 she comes to CDS with an ambitious goal for her research that places her at the border of all three fields, and this fellowship will give her total freedom to pursue it.\u201d\n\nOriginally from Burma, Htut earned her Bachelor\u2019s degree in Computer Science from Nanyang Technological University in Singapore with first class honors, where she worked with Professor Bingsheng He on text mining. She also worked as a Research Engineer at the Institute of High Performance Computing in Singapore for a couple of years before joining CDS.\n\nAt CDS, Htut is a part of the Center\u2019s inaugural cohort of Ph.D. students, and a member of the Machine Learning for Language (ML\u00b2) group where she is co-advised by Bowman and Professor Kyunghyun Cho, assistant professor of computer science and data science.\n\n\u201cThe greatest advances in science are made when scientists are given as much freedom as possible. The AdeptMind Scholar Fellowship provides such an opportunity to the recipients at NYU Center for Data Science, greatly facilitating advances in data science,\u201d Cho added.\n\nHtut is currently working on Open Domain Question Answering Systems and neural Information Retrieval. \u201cI had worked on Question Answering in the past as well, though I mainly used a more \u201ctraditional\u201d approach that involves building knowledge bases and retrieving the answers from knowledge bases using keywords in questions,\u201d she explained. \u201cNow, I\u2019m focusing on how to apply deep learning and reinforcement learning to automate this tedious process end of end.\u201d\n\n\u201cI thank NYU CDS and AdeptMind for offering the AdeptMind scholarship and supporting my research,\u201d she added. \u201cI feel very honored to be offered this scholarship.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/the-moore-sloan-data-science-summit-recap-day-2-c346240aa04e",
        "title": "The Moore Sloan Data Science Summit Recap: Day 2 \u2013 Center for Data Science \u2013",
        "text": "Day two in the Big Easy was jam packed as usual for our researchers at the Moore Sloan Data Science Summit.\n\nBeginning with the Education panel, Richard Bonneau, Director of NYU\u2019s Center for Data Science, Vicky Steeves (NYU), Magdalena Balazinska, Director of UW\u2019s eScience Institute, Rosemary Gillespie (UC Berkeley), Michael Mahoney (UC Berkeley), and Anthony Suen (UC Berkeley) explained how the Moore Sloan grant has helped to develop various data science curricula and programs at the three institutions.\n\nMahoney in particular highlighted how they have been documenting the explosive interest in data science amongst UC Berkeley undergraduates by taking pictures of how many students attended the first day of the Foundations of Data Science course every year since 2015. Sure enough, the photos demonstrated how the crowd grew rapidly from 500 to over 1000 within years.\n\nAs interest in data science grows, however, so will the ways in which the methodology is applied to other fields \u2014 both in the physical sciences like biology, chemistry, and physics, and the social sciences like politics or cultural analysis.\n\nThis question was at the heart of Richard Bonneau (NYU) and Joshua Tucker\u2019s (NYU) roundtable discussion, where they outlined the triumphs and challenges of working with their multi-disciplinary team at the Social Media and Political Participation Lab (SMaPP).\n\nThe rise of data science also means that we must continually think critically about the potential prejudices that influence data-driven results. Jevin West\u2019s (UW) roundtable talk \u2014 colorfully titled \u201cCalling Bullshit in the Age of Data Science Euphoria\u201d \u2014 demonstrated how numerous computer vision and machine learning papers that tout themselves as objective are undercut by fishy methodologies, weak logic, and unconscious biases. West then opened up the floor for suggestions on how to improve the field. Some ideas included Associate Director for UW\u2019s Data Intensive Research in Astrophysics and Cosmology (DIRAC) Institute and former NYU CDS Moore Sloan Fellow Daniela Huppenkothen\u2019s suggestion that we need to instill an understanding that although data science is powerful, it also \u2014 just like every other discipline \u2014 has its limits.\n\nThe day ended with another round of lightning talks and the Career Working Group panel discussion consisting of Michael Laver, NYU Dean for Social Sciences, Ed Lazowska, Founding Director of UW\u2019s eScience Institute, and Henry Brady, UC Berkeley Dean of the Goldman School of Public Policy. Presenting the results from last year\u2019s survey about data science career paths, they found that, as data scientists, we not only have different definitions of what data science actually is, but that we also have different long-term goals. The survey revealed that the priority we all share, however, is maintaining our intellectual freedom.\n\nThe summit closed with a group dinner, drinks, and plenty of fun in New Orleans\u2019s renowned French Quarter.\n\nSeveral thousand thank-you\u2019s are in order for Micaela Parker and Sarah Stone, Co-Executive Directors of UW\u2019s eScience institute, Emily Mathis (NYU), Program Manager, Marsha Fenner (UC Berkeley), Program Manager, and Ali Ferguson (UC Berkeley), Communications Manager, for organizing such an exciting event.\n\nSee you all next year!"
    },
    {
        "url": "https://medium.com/center-for-data-science/using-data-science-to-understand-music-cognition-fcef00020a5f",
        "title": "Using Data Science to Understand Music Cognition \u2013 Center for Data Science \u2013",
        "text": "The buildup before a beat-drop. A long crescendo. Rising pitch. Repetition. Dissonance. These are some familiar examples of musical tension, a broad term that musical theorists associate most closely with changes in loudness, melodic contour, tempo, and harmony.\n\nA new study by two NYU researchers, Morwaread Farbood, Associate Professor of Music Technology, and Marc Scott, Professor of Applied Statistics and CDS Affiliated Faculty, aims to answer this question. Funded by a seed grant from the Moore-Sloan Data Science Environment at CDS, the researchers will investigate the correlation between neural and behavioral responses to musical tension. Their study will be a novel intersection of data science, music cognition, and signal processing.\n\nFarbood and Scott plan to collect neural and behavioral data from twenty music listeners in real time.\n\nAs the listeners hear samples of Western pop, classical music, and non-Western music, they will record their own judgments of musical tension levels with a digital slider. Their recorded judgments of increases and decreases in musical tension will comprise the behavioral data. To collect the neural response data, the researchers will track listeners\u2019 signal processes with electroencephalograms (EEGs).\n\nThe data from listener judgments will be evaluated with a predictive music cognition model used in Farbood\u2019s past research. The model can predict the general tension response of listeners based on quantitative changes in musical features (i.e. loudness, tempo, harmony, pitch). By comparing the model to the actual behavioral response data, the researchers can identify how listeners adjust their ratings to different musical styles and how much the behavioral data varies from the model.\n\nBut this study\u2019s most substantial contribution to data science will involve developing new models to isolate EEG data related to musical response and correlating that data with the listeners\u2019 behavioral data. With mathematical models that link both neural and behavioral data from music listeners, the researchers will shed light on how our minds perceive music in real time."
    },
    {
        "url": "https://medium.com/center-for-data-science/moore-sloan-data-science-summit-recap-day-1-b4516d9e5136",
        "title": "Moore Sloan Data Science Summit Recap: Day 1 \u2013 Center for Data Science \u2013",
        "text": "Every profession has that one annual event where you drop everything to attend. For actors and actresses, it\u2019s the Oscars. For models and designers, it\u2019s New York Fashion Week. And, for data scientists? It\u2019s the Moore Sloan Data Science Summit.\n\nHeld in snazzy New Orleans this year, the annual event brings together data scientists who are supported by the Moore Sloan Data Science Environment, a five-year $37.8 million cross-institutional partnership that aims to advance data-driven scientific discoveries at NYU\u2019s Center for Data Science (CDS), University of Washington\u2019s eScience Institute, and University of California Berkeley\u2019s Institute for Data Science.\n\nThe two-day summit is a chance for researchers to update each other on their progress and share their work. It began with the Software panel discussion consisting of Claudio Silva (NYU), Jacob Vanderplas (UW), and Stefan J. Van Der Walt (UC Berkeley), all of whom outlined major research ventures and new curriculum initiatives supported by the Moore Sloan grant, such as NYU\u2019s Data Science Capstone Projects for its Master\u2019s students, UW\u2019s Data Science for Social Good summer program, and UC Berkeley\u2019s Data Structures for Data Science workshops.\n\nThey were followed by the Data Reproducibility panel consisting of Juliana Freire, NYU CDS\u2019s Executive Director of the Moore Sloan Data Science Environment, and Ariel Rokem (UW), who has had a long engagement with reproducing neuroscience research.\n\nAn example of such a tool is ReproZip, a powerful platform invented by Freire and her team that helps users to preserve and reproduce their data. ReproZip is especially useful for archivists and librarians \u2014 and it\u2019s on the way to becoming a key tool for data journalists, too. In Meredith Broussard\u2019s (NYU) roundtable, Broussard explained how she is collaborating with the investigative news organization ProPublica and the rest of the NYU ReproZip team to find ways of preserving interactive online news applications, such as data-driven demographic maps or interactive databases. After all, several of these applications are currently supported by softwares that are likely to be obsolete soon. (Did you know that Flash, for example, won\u2019t be supported by Adobe after 2020? Yikes!)\n\nAfter a quick lunch, the rest of the day was an opportunity for data scientists to attend short tutorials on new ways to use different computing platforms like Python, Baselayer, and Jupyter.\n\nOf particular intrigue was Jacob Schreiber\u2019s (UW) tutorial, where we discovered that the real die-hard fans of popular television show Gossip Girl should\u2019ve turned to his new Python modeling package, pomegranate, to discover who the show\u2019s absurd blabbermouth was. (It was Dan, for those who still don\u2019t know.) Built as a fast and flexible package for data scientists who need to perform probabilistic modeling tasks, pomegranate is an additional option to using SciKit-Learn or custom packages in R.\n\nThe day closed with a series of quick four-minute lightning talks. Burning through sixteen (!) talks in two hours, some of the most fascinating projects included using data-driven image segmentation for improving brain scans (Anisha Keshavan, UW), applying machine learning to estimate the effect of medical treatments (Soren Kunzel, UC Berkeley), and using agent based modeling to learn more about violence in Chicago (Tom Laetsch, NYU).\n\nMore photos and videos to come. (Follow us on Facebook to get \u2019em once they\u2019re out.)"
    },
    {
        "url": "https://medium.com/center-for-data-science/alumni-spotlight-aditi-nair-e83fdebd6621",
        "title": "Alumni Spotlight: Aditi Nair \u2013 Center for Data Science \u2013",
        "text": "I applied to the M.S. program at CDS because I saw that it was rigorous, challenging, and thorough. This was appealing to me because I hadn\u2019t studied any Data Science before entering the program, and I wanted to build a strong academic foundation in the field. I also liked the center\u2019s strong connection to the Data Science industry in New York.\n\nI\u2019ll be working as a Research Engineer in eBay\u2019s New York office.\n\nMy new role involves working on recommendation algorithms at eBay. I think the tasks required of me will be fairly similar to the projects I completed at CDS \u2014 in fact, for my final project for the Machine Learning course, my project partner and I implemented a classifier that was originally developed at eBay a few years ago. Compared to projects I completed at CDS, however, I will be handling a lot more data, and the data itself will be much more complex Speed and scalability, then, will probably become larger concerns for me.\n\nMy favorite memories at CDS include some of the great classes that I\u2019ve been able to take here. In particular, I really enjoyed Professor Bandeira\u2019s Optimization and Computational Linear Algebra for Data Science course, and Professor Sam Bowman\u2019s Natural Language Understanding with Distributed Representations course."
    },
    {
        "url": "https://medium.com/center-for-data-science/tracking-hackers-with-nlp-and-machine-learning-2560c7fce053",
        "title": "Tracking Hackers with NLP and Machine Learning \u2013 Center for Data Science \u2013",
        "text": "As technology continuously evolves on a massive scale, so does cybercrime. Cybercriminals, especially blackhat hackers and identity thieves, depend on underground online forums to communicate, often for the purpose of initiating transactions. They buy and sell a variety of illicit products and services such as stolen credit cards, online credentials, compromised hosts, hacking tools, and other wares.\n\nCybercrime researchers and law enforcement need to broadly understand the scale and scope of the activity on these underground markets, but it takes a long time for human analysts to peruse entire forums. To expedite this process, a multi-university team of researchers including Damon McCoy, Assistant Professor of Computer Science and Engineering at NYU, has developed new natural language processing tools that can be trained on forum-specific data to categorize posts and determine what products are being bought and sold for what prices. Annotations that would take a human analyst many hours to complete take between five and fifteen minutes for the new NLP tools, depending on the forum.\n\nBut developing NLP tools for underground forums presents a unique set of challenges. Forum users do not adhere to conventional grammar, and sometimes their communication styles are incomprehensible. Furthermore, grammar and styles of communication can vastly vary among forums. McCoy and his fellow researchers overcame this challenge by tuning their NLP tools to complete precise sets of tasks rather than to comprehend the meaning of entire forums.\n\nTheir automated tools can identify post category, product, and price with a minimum accuracy rate of 80%. The accuracy rate can be even higher, sometimes near 100%, but machine learning-based methods degrade when they are applied to different forums from the ones on which they\u2019ve been trained.\n\nThese new NLP tools can be used by future researchers to gain a holistic understanding of the activity on underground forums, and they could be used by law enforcement to respond rapidly to large-scale cybercrime events.\n\nWith an automated approach, investigators can quickly gauge strong upticks in products for sale, such as credit card numbers, that would indicate data breaches, allowing them to respond faster.\n\nTo continue their research, McCoy, along with Sam Bowman, Assistant Professor of Linguistics and Data Science at NYU, and other members of McCoy\u2019s team have received a seed grant from the Moore Sloan Data Science Environment, a cross-institutional initiative at NYU CDS. They plan to explore how private messages (where the actual transactions occur) affect price and ultimately determine the revenue of underground markets."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-center-for-data-science-hosts-nvidia-day-d3e41bf2469c",
        "title": "NYU Center for Data Science Hosts NVIDIA Day \u2013 Center for Data Science \u2013",
        "text": "Last week, we welcomed NVIDIA to spend a whole day with our students. A world leader in visual computing technologies, NVIDIA is a $7 billion hardware and software company with 10,000 employees.\n\nSince the company\u2019s founding in 1993, they\u2019ve been leaders in the gaming market. They invented the graphics processing unit in 1999, and since then, have brought the parallel processing power of GPU computing to areas such as virtual reality, artificial intelligence, and self-driving cars.\n\nJoan Bruna, CDS faculty member and Assistant Professor of Computer Science and Data Science, kicked off NVIDIA Day with a presentation of his research on \u201cdivide and conquer networks.\u201d These are neural networks that control computational complexity by splitting and merging inputs, allowing them to solve problems that are computationally or statistically hard. Applications for these networks include optimizing transportation costs and resource allocation.\n\nNikolai Yakovenko, a Sr. Research Scientist in the Applied Deep Learning Research group at NVIDIA, followed with an overview of the company\u2019s AI research and development. He focused on five examples of applied deep learning research topics:\n\nMariusz Bojarski, a deep-learning R&D engineer at NVIDIA, concluded NVIDIA Day by explaining their autonomous driving system. At their lab in Holmdel, NJ, Bojarski\u2019s team has taught a unique convolutional neural network (CNN) to drive a car by observing human drivers and emulating their behavior. The CNN, called PilotNet, relies on a center-mounted camera on the vehicle and two mirror-mounted cameras on each side.\n\nNVIDIA evaluates its autonomous driving system based on three performance metrics: autonomy (the percentage of the time the car drives without human intervention), precision (the percentage of the time the car deviates from what a human driver would do), and comfort (measured based on the smoothness of steering).\n\nFuture challenges for NVIDIA\u2019s autonomous driving team include transferring neural networks among different cars, enhancing the sensor capabilities with additional cameras, and enabling PilotNet to predict the behavior of other drivers on the road. Bojarski hopes to see PilotNet reach the benchmark of driving one million miles without human intervention.\n\nThe big day ended with NVIDIA raffling off an NVIDIA TITAN Xp \u2014 a powerful GPU for gamers and creatives \u2014 to one of our lucky students."
    },
    {
        "url": "https://medium.com/center-for-data-science/brainpool-ai-a-new-platform-bridging-the-gap-between-science-industry-26123e8c6223",
        "title": "Brainpool AI: a new platform bridging the gap between science + industry",
        "text": "It\u2019s no secret that artificial intelligence is starting to play a key role across all sectors, stretching from the financial industry to healthcare services.\n\nThe demand for scientists who can effectively handle data and apply AI to improve the way businesses, institutions, and organizations serve their clients is growing at an exponential rate.\n\nBut this demand is not being met because of two major reasons. First, the majority of data scientists are currently working in academia. Second, in contrast to more traditional roles, it takes aspiring data scientists several years of studying and training before they can even begin to step foot into their chosen industry.\n\nAs we wait for the tide to turn, how can we democratize access to data scientists in the present?\n\nAt the last Moore Sloan Research Lunch seminar, Paula Parpart from University College London explained how her exciting platform, Brainpool AI, is connecting data scientists with organizations who require their expertise.\n\n150+ data science academics are already members of the Brainpool AI network. The majority hail from top universities in the UK, Europe, and the US, and work on a freelance basis \u2014 meaning that they can maintain their involvement in academic research, but also contribute to industry-specific needs. (If you\u2019d like to apply, the link is here: http://brainpool.ai/apply)\n\nExamples of the projects that Brainpool AI experts are working on include predicting the outcome of the snap election in the UK, developing an automated crib that monitors and (carefully) sways your baby to sleep using different rhythms, and a project that helps hospitals get started with AI to better manage patient pipelines."
    },
    {
        "url": "https://medium.com/center-for-data-science/advancing-neural-data-analysis-with-maximum-entropy-models-3f04b2a3878d",
        "title": "Advancing neural data analysis with maximum entropy models",
        "text": "New recording technologies mean we have more tools than ever to gather data about brain activity. This raises new challenges from the data analysis perspective, as new statistical methods are required to determine how the activity of large neural populations gives rise to brain function.\n\nOne particular challenge is to characterize the statistics of neural activity patterns, sometimes referred to as the \u2018neural dictionary\u2019.\n\nIt is well established that neural responses (described as binary vectors with \u20181\u2019 for active neurons and \u20180\u2019 for silent neurons) are variable but highly structured, with biological activity restricted to a small subset of the exponential number of possible patterns. Figuring out what drives these regularities is critical for understanding the neural code and how neural activity gives rise to behavior, but difficult in practice.\n\nNeuroscientists usually turn to one of two approaches to address the problem, as Cristina Savin explains in her recent co-authored paper with Gaper Tka\u010dik (IST Austria).\n\nBoth approaches are powerful, but they also have their drawbacks. Is there a way to link the two together? For Savin and Tka\u010dik, the answer is yes \u2014 if you use Maximum entropy models (MaxEnt).\u201cMaxEnt models link the two approaches,\u201d the researchers explain, \u201cby being, at the same time, bona fide probabilistic models for neural activity, as well as generalizations of frequentist shuffles.\u201d\n\nMoreover, MaxEnt models can serve \u201cas a baseline comparison to increasingly popular unsupervised models from machine learning.\u201d\n\nTo read more about their work, click here."
    },
    {
        "url": "https://medium.com/center-for-data-science/enabling-evaluating-real-time-collaboration-in-online-learning-platforms-c732a7abedf",
        "title": "Enabling & Evaluating Real-Time Collaboration in Online Learning Platforms",
        "text": "Educators and potential employers frequently emphasize the importance of collaboration in classrooms and workplaces.\n\nEven policy-makers, at both state and federal levels, have acknowledged the significance of collaboration, notably apparent in the Every Student Succeeds Act. Consequently, online courses, flipped classrooms, and other personalized online learning experiences have faced criticism for neglecting the social aspects of learning.\n\nBut with a joint research project focused on developing applicable solutions, Peter F. Halpin, CDS Affiliated Faculty and Assistant Professor of Applied Statistics at NYU Steinhardt, and Yoav Bergner, Assistant Professor of Learning Sciences and Educational Technology at NYU Steinhardt, intend to provide ways for online education and training programs to better incorporate real-time collaboration.\n\nThe researchers plan to integrate open-source real-time collaboration software like TogetherJS with OpenEdx, an online learning management system that will allow instructors and content authors to design and deploy collaborative exercises. They also aim to develop new types of group learning activities, such as curriculum-aligned collaborative physics problems. In these problems, subsets of information required to reach solutions are distributed among students who must strategize how to share the information.\n\nHalpin and Bergner propose that integrating real-time collaboration software with a learning management system will provide opportunities for the collection and analysis of data regarding user behavior during collaborative exercises. Halpin has developed an algorithm to model student engagement in online chat groups. While he has shown how his algorithm can be used to evaluate the intensity of collaborative engagement in past research, this project will involve modeling student engagement across multiple groups and multiple time periods.\n\nThe outcome of Halpin and Bergner\u2019s project will supply an accurate, quantitative method to evaluate collaborative skills based on the intensity of user engagement with group members.\n\nTheir future plans also include developing their proposed prototype for a new collaborative learning management system into a commercially available software as a service."
    },
    {
        "url": "https://medium.com/center-for-data-science/using-nlp-to-ask-the-right-questions-about-questions-a8ac1719f018",
        "title": "Using NLP to ask the right questions\u2026about questions!",
        "text": "We typically assume that people only ask questions to gather information. But, depending on how they\u2019re phrased, questions also have a rhetorical role \u2014 particularly in politics.\n\nAfter all, asking \u201cWill you update us on Afghanistan?\u201d carries a different rhetorical charge than the following, which was asked by an opposition MP in 2013:\n\nThe former question primarily expresses curiosity while the latter sizzles with aggression, and implicitly puts the addressee in a tight spot.\n\nAt present, we usually use human annotators to identify the rhetorical function of different question forms. But CDS\u2019s Director of Graduate Studies, Arthur Spirling, along with Justine Zhang and Cristian Danescu-Niculescu-Mizil from Cornell University have recently published a paper in the Conference on Empirical Methods in Natural Language Processing (EMNLP) about their new framework, which automatically identifies different question types and groups them according to their rhetorical function.\n\nWorking with a dataset of over 200,000+ question-and-answer pairs from parliamentary question periods in the UK, the framework begins by extracting the fundamental syntactical patterns \u2014 motifs \u2014 of all the questions, and erasing any topic-specific nouns.\n\nAfter identifying the motifs of the questions, the framework then groups together questions that get similar answers, and labels them by their rhetorical function.\n\nAfter grouping the questions, the data revealed some fascinating conclusions.\n\nThe researchers found, for example, that newly arrived opposition members in parliament \u201ctend to contribute more condemnatory questions compared to older members\u2026This suggests that parliamentary career effects reflect behavioral changes at the level of individual MPs, whose incentives evolve over their tenure.\u201d\n\nWhat makes their framework especially powerful is that it has the potential to be applied to different domains outside of politics where questions play a key role.\n\nThe dataset and code that they used will be publicly available via the Cornell Conversational Analysis Toolkit.\n\nFor more, see Justine Zhang\u2019s conference talk on this work here, starting at 2:07:00."
    },
    {
        "url": "https://medium.com/center-for-data-science/mysterious-stars-physicists-tackle-anomalies-in-star-data-d93876f072f3",
        "title": "Mysterious stars: physicists tackle anomalies in star data",
        "text": "Did you know that stars \u2014 like humans \u2014 typically couple up, too?Astrophysicists estimate that up to 85% of all the stars in our universe actually orbit each other around a common center of mass.\n\nThese are called binary stars, and astrophysicists study them to answer questions like: What are the physical properties (masses or sizes) of the stars? How far away are the stars from Earth? What are their chemical compositions?\n\nThese are the kinds of questions that David W Hogg, a Professor at NYU\u2019s Department of Physics and the Center for Data Science, is trying to solve. Recently, Hogg and his colleagues from Princeton, Yale, Columbia, and the Flatiron Institute have been investigating scientific data about a curious binary star pair that they called \u201cKronos and Krios\u201d.\n\nBinary stars usually have nearly identical chemical compositions because they formed coevally (meaning that they were born at the same time and in the same place).\n\nBut, when analyzing the scientific data about Kronos and Krios, Hogg and his colleagues discovered that this binary star pair display an unusual difference. Although the data suggests that they formed coevally, their differential analysis of Kronos and Krios\u2019s chemical compositions reveal that they have dramatically different chemical compositions.\n\nThe million-dollar question now is: why?\n\nHogg and his colleagues are considering a number of explanations.\n\nBut the most likely explanation is that the two stars accreted rocky planetary systems that formed after the stars were born, and the amount that each accreted was different. Specifically, the researchers speculate that Kronos has engulfed something like 15 times the mass of the Earth in planetary material.\n\n\u201cIf this is the case,\u201d the researchers continue, \u201cthere may be surviving, highly eccentric giant planets potentially detectable with future data releases.\u201d\n\nWhat planetary systems remain around Kronos and Krios after these violent events? What is the architecture of these unknown systems, and the composition of the planets within them? To find out, we\u2019ll have to wait for Gaia, a spacecraft that has been tasked with creating a three-dimensional map of our Galaxy \u2014 and find the long-period planets within it \u2014 to collect more data."
    },
    {
        "url": "https://medium.com/center-for-data-science/what-is-the-cds-leadership-circle-5-minutes-with-yiran-xu-e7bd9bfa69e",
        "title": "What is the CDS Leadership Circle? 5 Minutes with Yiran Xu",
        "text": "It is a student-run organization at CDS that represents the student body, and helps to build the CDS community. Through hosting events like the Last Summer Panel and Game Night, the Leadership Circle helps first-year and second-year students to get to know each other, have fun, relax, and take a break from our heavy workload. Moreover, the committee members of the Leadership Circle have the opportunity to develop skills in leadership, time management, and communications by organizing the various events and activities for the students throughout the academic year.\n\nAfter having completed an internship over the summer, I\u2019ve realized how important it is to connect with other people in the data science community so that we can all help each other grow. Having fun social events where people can get to know each other in a relaxed setting is usually the best way to build relationships, so this is why I am passionate about working with the rest of the Leadership Circle to organize great events for our students.\n\nDuring my first year here, I have actively participated events held by the previous Leadership Circle, and really enjoyed them. Thus, I would like to contribute this year. I am also really looking forward to developing my leadership and communication skills because I know how important it is to my future career. I see the Leadership Circle as a great opportunity for me to do so.\n\nThe plan for this year is to hold three events per semester. For example, we recently hosted our first event, \u201cLast Summer Panel,\u201d where we invited eight second-year students to share their internship/research experiences with other students.\n\nI really love everything that is happening at CDS! It\u2019s hard to say which is my favorite one. If I had to choose, I would list three workshops that I love the most.\n\nThe first workshop is the Networking and Recruiting workshop where three professionals came and did mock interviews, helped us with our elevator pitches, and taught us how to prepare for interviews. The second workshop was also about preparing for interviews, where I received much information about what data science related interview are usually like. what interviews will test on. The third workshop was a series presentation workshop where a professor came to teach us how to give strong presentations. That workshop in particular really helped me to correct my bad habits and gain more confidence with public speaking."
    },
    {
        "url": "https://medium.com/center-for-data-science/cds-hosts-10th-data-science-showcase-4b176eb078bd",
        "title": "CDS Hosts 10th Data Science Showcase \u2013 Center for Data Science \u2013",
        "text": "This week, CDS hosted the Moore-Sloan Data Science Environment\u2019s 10th Data Science Showcase. Organized around the theme of statistics, the showcase featured short presentations from the following speakers:\n\nProfessor Chen first introduced the IOMS department in Stern and the research area of each professor in the IOMS statistics group. Professor Chen\u2019s research intersects statistics, machine learning, and operations research. In his presentation, \u201cStatistics Meets Big Data,\u201d he explained how his research focuses on making statistical inferences for modern big data applications, such as real-time streaming data, large-scale data stored in a distributed environment, and high dimensional structured data. In addition to statistical learning and inference, he highlighted the importance of decision-making for business analytics. Professor Chen\u2019s research on sequential analysis combines online learning and decision-making in a unified framework. Applications for his research include sequential learning for crowdsourcing and dynamic assortment planning. The ultimate goal of his research is to provide decision makers with tools to help them make wise choices that will save costs.\n\nProfessor Hill provided an overview of the Center for the Promotion of Research Involving Statistical Methodology (PRIISM), of which she is a co-director. She highlighted PRIISM\u2019s collaborative research which applies statistics and data science to the social, educational, behavioral, health, and policy sciences. Professor Hill emphasized the importance of understanding the full lifecycle of data starting from forming research partnerships and thinking carefully about measurement and sampling all the way through to translating results to different communities (stakeholders, academic scholars, and the public). A current joint project of hers is Tracking Hope in Nairobi and Karachi (THINK), which aims to quantify how the attainment of educational goals affects hope, aspirations, and violence in Nairobi and Karachi.\n\nProfessor Liu discussed applications of neural networks in building cancer risk prediction models. She demonstrated that relative risk estimation (which is obtained from sampling cohort data) does not necessarily reflect absolute risk (which depends on population based data). She also clarified that classification outcomes are not necessarily equivalent to prediction outcomes, and that causal interpretations cannot be gleaned from retrospective studies without assumptions or external data. Additional applications included identifying FDNY firefighters with 9/11 lung related injury, classifying patients with or without glaucoma, identifying asthma patients who could benefit from repeated measurements of inflammation biomarkers, and building new breast cancer risk prediction models.\n\nIn Professor Madigan\u2019s presentation, \u201cA Data-Driven World: Opportunities and Challenges,\u201d he explained how he and his colleagues from the Observational Health Data Sciences and Informatics (OHDSI) are using data science to improve medical diagnoses, build predictive models for patient treatment, and perform large-scale population level effect estimation by analyzing over 700 million electronic health records. Madigan also explained how OHDSI\u2019s research teams are creating a new methodology to both improve the reproducibility of the observational medical studies, and reduce human and publication bias."
    },
    {
        "url": "https://medium.com/center-for-data-science/female-islamic-preachers-are-acquiring-online-authority-and-we-can-use-nlp-to-analyze-how-they-do-e8b777cdc65",
        "title": "Female Islamic preachers are acquiring online authority \u2014 and we can use NLP to analyze how they do\u2026",
        "text": "What is authority? For Rich Nielsen from MIT, a working definition of authority is this: \u201cwhen the statement you make influences the way others think.\u201d\n\nSpeaking at a recent Text-as-Data / NLP seminar, Nielsen explained how he is using NLP to explore how Islamic preachers are gaining authority online. Specifically, his work is currently focused on a religious movement within Islam called Salafism. As Nielsen explained, the movement is highly patriarchal: they have little room for women\u2019s authority and avoid gender mixing.\n\nBut, if that\u2019s the case, why are there female Salafi preachers on a popular Salafi website, saaid.net? And, how are these female preachers gaining authority by influencing the website\u2019s user base?\n\nA potential answer that Nielsen is exploring is whether female preachers acquire authority online through their identity as women, which allows them to \u201csupport Salafi ideological positions in ways that men cannot.\u201d\n\nUsing NLP, Nielsen\u2019s case study compares the forum postings of the 172 male and 43 female preachers (in their original Arabic!) to explore his premise.\n\nHe started by performing simple word counts in the text corpus, and found that male preachers have a tendency to use more religious terms like \u201cAllah,\u201d \u201cMuhammad,\u201d and \u201cAlmighty\u201d than female preachers, who favor words like \u201cself,\u201d \u201cwomen,\u201d and \u201chusband.\u201d For Nielsen, these keywords could potentially be classified as \u201cwomen\u2019s issues.\u201d\n\nThen, Nielsen performed topic modelling on the data. A particularly surprising statistic, he notes, is that given the same topic, male preachers cite Islamic hadiths \u2014 reports about the Prophet Muhammad\u2019s actions and behaviors \u2014 to support their points about twice as often as women.\n\nIn both stages of his analyses, the collective de-emphasis on religious terms and religious texts by female preachers appears to indicate that they gain authority through their position as women. After all, as Nielsen argues, it is only a women who would have the identity authority to write lines like \u201cWe are all like mothers, fathers, sisters, and brothers to you.\u201d\n\nFurther investigation is still needed to refine his case study, since identity authority is a difficult metric to track. But his work certainly raises some provocative questions. Even if female preachers are acquiring authority online, does this translate into authority in reality? How legitimate is identity authority? And, how much agency do female Islamic preachers have anyway?"
    },
    {
        "url": "https://medium.com/center-for-data-science/learning-about-cancer-cells-using-image-recognition-823200f63916",
        "title": "Learning about cancer cells using image recognition",
        "text": "How can we stop cancer tumors from growing? One way is to discover conditions that accelerate their growth so that we can find ways to suppress precisely those conditions \u2014 and this is what Carlos Carmona-Fontaine from NYU\u2019s Center for Genomics and Systems Biology is investigating.\n\nAt the most recent Moore-Sloan research lunch seminar, Carmona-Fontaine and his research team at the Carmofon Lab are studying how cells cooperate by placing them in unique 3D printed chambers called MEMICs (Metabolic Microenvironment Chamber).\n\nMEMICs mirror the micro-environments of tumors, and allow biologists to test how cell samples react to specific variables like oxygen and CO2 levels, or varying degrees of proximity to blood vessels that provide key nutrients like glutamine. At present, Carmona-Fontaine\u2019s team are combining MEMICs with high-throughput methods that allow them to incubate cells and follow the over time in roughly 800 different environments.\n\nTheir preliminary results so far suggest that tumor cell populations are especially susceptible to nutrient starvation. To overcome the problem, the lab noticed that these cells begin working together for collective survival.\n\nBut, as Carmona-Fontaine explained, this strategy is only effective when there is high cell density: low density populations die before they can cooperate for survival.\n\nThis is coined as the Allee Effect in biology \u2014 and it\u2019s good news for us. If we can find a way to enhance the Allee Effect (e.g. keep cell populations low), cancer cells will not be able to survive under nutrient starvation even if they engage their cooperative strategies.\n\nIn the meantime, however, Carmona-Fontaine\u2019s researchers still need to collect more data to support their preliminary results. As more cells continue to grow and die within the MEMICs, they are taking high resolution images of the cells so that the researchers can discern live cells from dead ones in each environment.\n\nWith about 50,000 images to analyze, image recognition has made cell-counting easier \u2014 but only if the cells are stained with Green Florescent Protein (GFP) first. Staining cells is a time consuming process, so Carmona-Fontaine\u2019s lab has just begun exploring how machine learning techniques can help them count cells without using GFP."
    },
    {
        "url": "https://medium.com/center-for-data-science/baby-sleep-study-can-sleep-digestive-disruption-in-infants-be-predictive-of-autism-or-other-711091eaee74",
        "title": "Can Sleep & Digestive Disruption in Infants be Predictive of Autism or Other Disorders?",
        "text": "Parents of newborn babies often feel an enormous amount of anxiety. These parents might wonder: Is my baby\u2019s sleep schedule normal? Should my baby be eating more or less often? How frequently should I have to change my baby\u2019s diaper?\n\nUntil now, answers to these questions have been difficult to substantiate.\n\nBut David J. Heeger, Professor of Psychology and Neural Science, Silver Professor, and Carlos Fernandez-Granda, Assistant Professor of Mathematics and Data Science, are using big data to provide a more accurate characterization of normal infant development for parents, pediatricians, and other clinicians.\n\nThe researchers are funded by a seed grant from the Moore Sloan Data Science Environment, a cross-institutional initiative at NYU CDS. They have asked parents and caregivers of newborn babies to be citizen scientists by collecting data with the Baby Connect phone app.\n\nOver 850 babies are already enrolled in the study; the researchers have gathered 1.2 million sleep events, 2.1 million feedings, and 1.3 million diaper changes so far. (Click here if you or someone you know might be interested in enrolling a baby in the study.)\n\nWhen Heeger and Fernandez-Granda have completed the data collection process, they plan to use big data analytics to identify typical and atypical sleep and digestive development patterns in early infancy, and track how these cycles change with age. They have already compiled and analyzed some initial results.\n\nBy using big data analytics and neural networks, Heeger and Fernandez-Granda will also be able to determine if chronic sleep and digestive problems in early infancy can be predictive markers for autism spectrum disorder (ASD) or other developmental disorders.\n\nThe researchers point out, however, that any identified predictive markers would serve as opportunities for early intervention rather than concrete diagnostic evidence. They also anticipate that some deviations in sleeping or eating patterns will be attributable to parenting habits.\n\nParents and clinicians who are already familiar with the well-known height and weight chart may soon be familiar with a sleep and digestion chart driven by data from Heeger and Fernandez-Granda\u2019s study. In the future, the team plans to apply for additional funding from foundations with a particular focus on autism.\n\nFor more about the Baby Sleep Study, click here."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-center-for-data-sciences-kyunghyun-cho-selected-as-2017-cifar-azrieli-global-scholar-4e538f4bea7",
        "title": "NYU Center for Data Science\u2019s Kyunghyun Cho selected as 2017 CIFAR Azrieli Global Scholar",
        "text": "New York, NY \u2014 Kyunghyun Cho, an Assistant Professor at NYU\u2019s Center for Data Science (CDS) and the Courant Institute of Mathematical Sciences, has been selected as a 2017 CIFAR Azrieli Global Scholar.\n\nCIFAR is a global research institute that connects leading scholars through interdisciplinary research programs like child and brain development, cosmology, genetic networks, learning machines and brains, and more.\n\nFounded in 1982, as many as 18 Nobel Laureates have since been associated with CIFAR, and its selected researchers and fellows are continually amongst the most highly cited scholars in their fields.\n\nSupported by the Azrieli Foundation, CIFAR\u2019s two year program offers the opportunity for researchers like Cho be mentored by other experts in his field, as well as exchange ideas with industry leaders outside of academia. He will also receive $100,000 to support his work, which has already made significant contributions to fields like medicine and neural machine translation.\n\nCho\u2019s deep engagement with neural machine translation in particular is poised to increase the internet\u2019s linguistic diversity, and overcome information inequality and digital division.\n\nSince publishing his co-authored paper on applying a novel encoder-decoder approach for multi-way, multi-lingual translation with his colleagues and his mentor, Professor Yoshua Bengio, Cho has continued to collaborate with researchers in linguistics and technology to create better approaches to data-driven translation.\n\n\u201cThe internet has drastically improved the speed at which information spreads, but has not removed language barriers,\u201d Cho explained. \u201cMore than 75% of the content of the internet are in foreign languages to the majority (60%) of internet users.\u201d\n\n\u201cMy current research on character-level, larger-context, multilingual neural machine translation that will reduce the language barrier significantly in the long run. Furthermore, the memory efficiency of neural machine translation will facilitate wider deployment of machine translation systems to places with low bandwidth or unstable internet connectivity.\u201d\n\nOutside of his own research, he co-leads the Computational Intelligence, Learning, Vision, and Robotics (CILVR) research collective with CDS\u2019s Founding Director, Yann LeCun, Rob Fergus, Joan Bruna, Sam Bowman, Brenden Lake, and Rajesh Ranganath."
    },
    {
        "url": "https://medium.com/center-for-data-science/deepmind-fellow-profile-yassine-kadiri-7bfe4a045050",
        "title": "DeepMind Fellow Profile: Yassine Kadiri \u2013 Center for Data Science \u2013",
        "text": "I was born in Morocco and earned my baccalaureate in the French educational system, where I was awarded with highest honors. Then, I enrolled in the preparatory program at Janson De Sailly, which involves two to three years of intense preparation in Mathematics, Physics, and Chemistry to sit the exams for entering French engineering schools. In the end, I chose to sit Ecole Polytechnique\u2019s exam, which is one of the most difficult ones in France. Thankfully, I was admitted. I consider this a key achievement, as it meant that I had gained admission to one of France\u2019s most demanding scientific institutions.\n\nAt Polytechnique, I primarily took courses in Computer Science and Applied Mathematics, although I also studied Physics and Economics. Those courses allowed me to build a strong quantitative skills and technical knowledge.\n\nJoining NYU means joining an already ambitious, demanding, and recognized institution that is currently a major pioneer in the Data Science field.\n\nI aim to specialize in Deep Learning and Machine Learning while expanding my knowledge in Computer Vision, Computer Science, and Data Science. Afterwards, my goal is to join a technological company.\n\nI thank NYU for granting me this opportunity. Most of all, I would like to thank DeepMind for granting me this award, and for supporting my studies at such a prestigious university. This award has a real impact on my academic path. I will do my best to succeed in my studies at NYU and show that I am worthy of DeepMind\u2019s recognition."
    },
    {
        "url": "https://medium.com/center-for-data-science/can-cellphone-data-reveal-mental-health-issues-22b30e95ffa1",
        "title": "Can cellphone data reveal mental health issues? \u2013 Center for Data Science \u2013",
        "text": "In the mid-1990s, Kroenke, Spitzer, and Williams developed the Patient Health Questionnaire (PHQ) through a grant from pharmaceutical giant Pfizer to help doctors diagnose mental health issues like depression, anxiety, alcoholism, or eating disorders.\n\nThe PHQ asks the patient a series of questions, and the patient\u2019s answers are used by doctors to form a preliminary diagnosis.\n\nPart of the problem with PHQs, however, is that they are not only time consuming but they are only administered when a patient sees a doctor, meaning that they can only capture a partial picture of the patient\u2019s mental state in that moment, instead of over a long period of time.\n\nThis is why scientists like Mirco Musolesi from UCL and the Alan Turing Institute are exploring different ways for us to track human behavior using cellphone data. At the most recent Moore-Sloan research lunch seminar, Musolesi explained how the different sensors already built into cellphones can help us diagnose patients who volunteer to have their cellphones monitored.\n\nKey to Musolesi\u2019s system is the way that it can continuously and unobtrusively track a patient\u2019s mental health over time. Take GPS and location data, for example. Knowing that a major indicator of depression is difficulties leaving the house or getting out of bed, one could potentially track the total distance that a patient has walked, or the maximum distance that they have left home, to assess the nature of the patient\u2019s mental state.\n\nMusolesi\u2019s team have been performing small case studies to refine and test these premises. Future directions that this work could take, Musolesi added, is incorporating the ability to analyze the data in real-time, sending SMS messages to the patient at times when the data about their mobility indicates that they may need medical attention, or tracking response time to notifications.\n\nA caveat to using cellphone data to make medical diagnoses, however, is that correlation does not equal causation \u2014 which is why, as Musolesi reminds us, we should not base conclusions solely on the data. Instead, cellphone data should be viewed as a supportive tool that can flag up potential indicators of mental health symptoms that a trained practitioner can follow up on.\n\nAnd, of course, the use of cellphone data to make medical diagnoses in this manner touches on a broader concern that has been dominating the tech community for several years: privacy. What if such data about our daily behaviors and habits were to fall into the wrong hands?\n\nBut, as Musolesi explains, \u201cwe have to consider the cost-benefit trade off.\u201d His methodology has the potential to save lives, and privacy issues can be mitigated so long as proper protocols surrounding data collection, storage, and access are firmly established. Moreover, all the patients are, as Musolesi adds, \u201cperfectly aware of the monitoring. The risks are there, but these are patients that willingly install the app.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/cds-welcomes-plateds-data-science-team-3e09d1819f7c",
        "title": "CDS Welcomes Plated\u2019s Data Science Team \u2013 Center for Data Science \u2013",
        "text": "At the most recent Company Information Session, CDS welcomed Plated, a meal-kit company led by chefs and data scientists.\n\nLike most meal-kit companies, Plated offers a weekly meal subscription plan. Unlike other companies, however, they offer 20 different recipes per week (with dessert options!) and boasts a 100% turnover rate. This means that all 20 recipes change every single week. What\u2019s the secret sauce to their success?\n\nData, of course. For example, let\u2019s take Plated\u2019s recommendation system. The typical data scientist\u2019s gut instinct may be to cluster recipes together by protein, such as chicken. But this would cause the algorithm to continually recommend chicken dishes to the customer. Chicken\u2019s great \u2014 but not every day!\n\nThe trick is not to have a one-size-fits-all recommendation system that is so commonly used in other industries.\n\nWhen it comes to food recommendation systems, one needs to be more imaginative and wide-ranging: you need multiple models that cluster recipes according to different metrics (like calories, cuisine type, prep type, or color) so the algorithm can hit that sweet spot between consistency and variety.\n\nHaving recently been acquired by grocery company giant Albertsons, Plated is looking to expand their data science team to serve their rapidly expanding customers. Can our data students rise to the challenge? (The answer is yes, by the way.)"
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-arthur-spirling-deputy-director-and-director-of-graduate-studies-1bd3afcb8788",
        "title": "5 Minutes with Arthur Spirling, Deputy Director and Director of Graduate Studies",
        "text": "As with every year of the MSDS, we have expanded our numbers \u2014 yet simultaneously become more selective. Our fantastic incoming cohort of 96 were selected from some 1659 applicants, meaning our program is considerably harder to enter than many top law schools or business schools. Every single one of our new students has truly remarkable potential and I know my colleagues are very happy to be working with them.\n\nUltimately, we want to provide a program structure that links domain knowledge and methods via the course offerings. This enables CDS and its students to place themselves at the center of Data Science as the field\u2019s popularity explodes.\n\nPart of our vision at CDS is that Data Science is not just about learning powerful new methods; it\u2019s also about having a deep understanding of the ways those methods can be used in the \u2018real world\u2019 of industry and policy. Our tracks \u2014 and the demand for them from students! \u2014 are proving a great way to make this vision a reality.\n\nHuman beings have been writing things down for around 5000 years, but it\u2019s only very recently in human history that anyone other than the social elite were producing texts. And, even when they did, it often wasn\u2019t preserved for future research purposes.\n\nA major change came with the advent of the Internet, social media, and news sites.\n\nNow, literally billions of people write billions of words every day, whether they be online newspapers, product reviews, government reports, or someone commenting on how cute a friend\u2019s baby looks on Facebook.\n\nFrom a research perspective, we can easily access that information in machine readable form: these huge troves of text data are ready to be analyzed immediately, often in real time.\n\nAt the same time, the technology to make older documents machine readable has also advanced remarkably: one can now take, say, government records from World War II, push them through an optical character recognition system, and have quite high quality documents amenable to statistical work. With the explosion of text data has come methods for dealing with these collections, and that symbiotic relationship seems set to continue.\n\nPersonally, I became involved in text-as-data because I was studying a particular historical puzzle: the democratization of the UK in the 19th Century. It\u2019s an interesting case because in a relatively short period of time (around 80 years) politicians there embarked on very radical reform, going from a narrow franchise where no one could vote, to one where everyone could.\n\nThis is surprising: generally, elites don\u2019t voluntarily give up power to people poorer and less educated than they are. But, more broadly, I noticed that there was actually a lot we didn\u2019t know about politicians and voters back then: how they interacted with each other, how they spoke in parliament, how they organized policy-making and so on. Simultaneously, I also realized that we had millions of records of speeches from which one could make an inference. So I turned to modern technology to understand these events: it\u2019s been a great experience, and I learned a lot both substantively and in terms of methods!\n\nTime flies: it\u2019s my third year! My favorite experience is seeing how MSDS students develop during their time with us, and helping them accomplish their goals professionally.\n\nOur students work very hard: our courses are technically tough, and demand a lot of hours of focus and effort. The students persevere and, more often than not, they land themselves in their dream job at tech firms, banks, in government, or in academic research institutions. They are rightly proud of how far they have come \u2014 and we are proud of them, too."
    },
    {
        "url": "https://medium.com/center-for-data-science/cds-welcomes-benevolent-ai-362902564b04",
        "title": "CDS Welcomes Benevolent AI \u2013 Center for Data Science \u2013",
        "text": "These days, it\u2019s impossible for any human being to process all the available data that we have collected \u2014 especially in the biotech and healthcare industries.\n\nThis is why we are increasingly turning to artificial intelligence companies like Benevolent AI to help us make sense of it all.\n\nAt the most recent Company Information Session, Benevolent AI explained how they\u2019re applying artificial intelligence to a highly specific \u2014 and enormously vital \u2014 area of biotech and healthcare research: the discovery of new drugs.\n\nPart of what they do involves using AI to aggregating previously untapped data sources.\n\nTake research papers, for example. No scientist, as Benevolent AI\u2019s CEO Jerome Pesenti pointed out, could ever read all the scientific papers in their field. So how can they ensure that they\u2019re always updated on the newest developments?\n\nHere is where Benevolent AI comes in. With the power to analyze almost 90 million research papers, not do they help scientists stay abreast of new developments, but they also have a pipeline in place for helping scientists generate hypotheses and experiments for new drug trails. This supports the larger goal of accelerating healthcare discoveries.\n\nAt present, they\u2019re focused on expanding their platform to recruit more data scientists. (And, perhaps some of them will be CDS graduates!)"
    },
    {
        "url": "https://medium.com/center-for-data-science/alumni-spotlight-manoj-kumar-sivaraj-6df5e6d5b0a",
        "title": "Alumni Spotlight: Manoj Kumar Sivaraj \u2013 Center for Data Science \u2013",
        "text": "The program provides a strong foundation of the mathematical concepts of machine learning combined with its practical applications. Additionally, I got the opportunity to continue my work on developing scikit-learn and the related ecosystem as I completed the program. This was a huge plus for me.\n\nI\u2019m joining the one year residency program at Google Brain.\n\nLaunched in 2016, the program invites around 20 to 30 people from varying backgrounds in machine learning to develop state-of-the-art deep learning techniques at Google.\n\nMost of my previous work involved developing and contributing to open-source machine-learning toolkits, so I\u2019m looking forward to this program as a great way to start producing more original research.\n\nThe topics that I\u2019m most excited about are hyperparameter optimisation and music modelling, and some of my work will be similar to the projects that I completed at CDS, such as the reading comprehension system that we tried to build in the Natural Language Understanding class, or the rap generating model for Inference and Representation.\n\nI like the multicultural atmosphere of New York. Similarly, I like that CDS brings together people from different backgrounds both academically and otherwise."
    },
    {
        "url": "https://medium.com/center-for-data-science/deepmind-fellow-profile-ksenia-saenko-e6d0f7574a59",
        "title": "DeepMind Fellow Profile: Ksenia Saenko \u2013 Center for Data Science \u2013",
        "text": "I grew up in Minsk, Belarus and came to the U.S. with my mother when I was 16.\n\nI only spoke a little English at the time, but I enrolled in the Borough of Manhattan Community College, and graduated two years later with a 4.0 GPA. I then transferred to Cornell University and completed a degree in Applied Economics, magna cum laude.\n\nAfter graduating from Cornell, I joined Schuman Cheese as a Risk Management Analyst. In this role, I created new commodity models and currency reports that became standard decision making tools for the executive team. I was soon promoted to the role of Senior Analyst on the strategy team, where I conducted extensive data analysis to drive corporate and business unit strategic plans. I developed and executed multiple presentations for our senior management team to help them make decisions about which products to launch, and which markets to enter.\n\nI am thrilled to be joining NYU this fall, and to learn more about data science from the country\u2019s leading scholars and practitioners. I am incredibly grateful to DeepMind for making this opportunity available to me, and am looking forward to developing my skills in this exciting field."
    },
    {
        "url": "https://medium.com/center-for-data-science/crowdsourcing-its-a-complex-ecosystem-ff9a1939a2fa",
        "title": "Crowdsourcing: it\u2019s a complex ecosystem! \u2013 Center for Data Science \u2013",
        "text": "Initially, the name \u2018crowdsourcing\u2019 conjures up a vague image of a random pool of anonymous people who are simply willing to perform tasks voluntarily, or for monetary rewards.\n\nIn reality, however, crowdsourcing is also a complex ecosystem underpinned by particular patterns \u2014 ones that Djellel Difallah, a CDS Moore-Sloan Data Science Fellow, is trying to understand.\n\nAt last week\u2019s Research Lunch Seminar, Difallah explained how he has been working with Panos Ipeiritos, a professor at CDS and NYU Stern, to use data science for studying underlying dynamics like supply and demand on crowdsourcing platforms.\n\nFor example, after analyzing data about the number of requested human intelligence tasks on Amazon Mechanical Turk (Mturk) from 2009\u20132017, Difallah found that two factors which influenced whether a task would be completed were the number of assignments available within a single task, and how long ago the task batch was uploaded.\n\nThe last factor \u2014 the \u201cfreshness\u201d of a task batch \u2014 is important because, as Difallah explained, by default the platform organizes the tasks according to the time they were uploaded, with the most recent tasks at the top.\n\nDifallah also found that MTurk\u2019s participants as a whole are more driven to a batch when there are more tasks available. As the tasks get completed, the batches get smaller, older, and ultimately suffer from stagnation effects.\n\nUnearthing these patterns could eventually enable us to predict when a task batch will be completed on MTurk, which will be a big help to researchers who rely on crowdsourcing.\n\nPresently, Difallah is also investigating editing patterns on Wikidata, a collaboratively-edited knowledge base, as well as modeling the skillset profiles of those who populate the platform.\n\nMore broadly, Difallah added, analyzing platforms like MTurk and Wikidata as intricate ecosystems will help us understand the behavioral patterns of each platform\u2019s community, improve how they operate, and increase their efficiency.\n\nFor more, see Difallah\u2019s co-authored paper with researchers in the UK and Switzerland."
    },
    {
        "url": "https://medium.com/center-for-data-science/q-a-with-cristina-savin-1f231fd8fe6b",
        "title": "Q&A with Cristina Savin \u2013 Center for Data Science \u2013",
        "text": "I\u2019ve always loved a good puzzle. When I was an an undergraduate studying computer science, I got interested in different aspects of artificial intelligence.\n\nThis brought me in contact with a computational neuroscience lab in Frankfurt, where I remained for graduate school. Over time, the more I learned about brain biology, the more my interests shifted towards neuroscience. Turns out that the mind is possibly the most complex puzzle of all.\n\nWhat makes computational neuroscience intellectually challenging and really fun is the interdisciplinary and the collaborative nature of the work. My research brought me in contact with neuroscientists studying the neural mechanisms of learning in several animals, cognitive scientists studying behavior in humans subjects, and clinicians interested in impairments due to disease.\n\nWhat we \u2014 the theoreticians \u2014 bring to the table is quantitative tools and new conceptual frameworks in which to think about neural computation. Success requires a constant dialogue across disciplines and this means that there is always something new to learn.\n\nArguably, the biggest challenge for modern neuroscience is that we are drowning in data.\n\nRecent years have seen an explosion of experimental tools for recording and disrupting the responses of increasingly large neural populations.\n\nThe resulting data is rich, but challenging for traditional approaches (high-dimensional, non-stationary, with limited recording time).\n\nHence, data science is playing an increasingly important role in making sense of such data. One ongoing project in the lab tackling this problem concerns the characterization of the joint statistics of patterns of neural activity in behaving animals.\n\nThis is challenging for two reasons. First, we need to be able to estimate a complex map between the behavior of the animal and the response of individual neurons. Second, we need ways to model the statistical dependencies across neurons in a way that scales to large populations.\n\nOur solution brings together ideas from Bayesian nonparametrics (Gaussian Processes), statistical physics (a new tractable class of maximum entropy models), and traditional frequentist statistics (hypothesis testing).\n\nMost importantly, when using these novel data analysis tools, we were able to show that neurons in one particular area of the brain called the hippocampus work cooperatively to encode information about the position of the animal within the environment (the so-called \u2018cognitive map\u2019), and that this feature emerges as a result of learning.\n\nI\u2019ve moved a great deal throughout my career, but I see all these moves as critical scientific stepping stones that brought my research to where it is now.\n\nWhile my interest in biological learning developed during my PhD work in Frankfurt, my stay in Cambridge allowed me to hone the machine learning skills that form the core toolset used in the lab, and my stint in Vienna gave me hands-on experience with neural data analysis.\n\nNYU, and in particular the dual appointment in neuroscience and data science, brings together the different strands of my research. It offers an unique opportunity in that it recognizes the inherently interdisciplinary nature of this research, emphasizing both machine learning and neuroscience, both theory and data analysis."
    },
    {
        "url": "https://medium.com/center-for-data-science/say-hello-to-sam-the-new-speaker-affect-model-74f2ed41319f",
        "title": "Say Hello to SAM, the new Speaker Affect Model \u2013 Center for Data Science \u2013",
        "text": "Although we often focus on words when studying political rhetoric, a key component that is often overlooked is sound.\n\nFor example, a person\u2019s vocal tone, rhythm, or timbre can have enormous implications on whether we perceive a sentence to be sarcastic or serious.\n\nThis is why Dean Knox (Microsoft & Princeton) and Christopher Lucas (Harvard) are working on a new project called the Speaker Affect Model (SAM), which they introduced at the most recent Text-as-Data and NLP Research seminar.\n\nSAM analyzes conversations by breaking them down into utterances, or sentence-length audio recordings, then slicing these utterances into even shorter frames that capture the different sonic dimensions of a person\u2019s speech, such as their pitch or volume, at a particular instant in time.\n\nAnalyzing an audio recording at such a granular level, the researchers explained, not only allows us to learn the way that different emotions sound, but also to predict the emotion of an utterance within the context of the conversation around it by using a forward-backward algorithm.\n\nAfter building their model, they conducted an exploratory case study on predicting skepticism in Supreme Court audio recordings.\n\nTheir preliminary results suggest that while skepticism is often difficult to recognize with text alone, it is possible to predict with audio.\n\nFor example, for some Justices, they found that their use of pauses and the word \u201cthink\u201d usually express neutrality, while words like \u201cnothing,\u201d \u201ccan\u2019t,\u201d and \u201ceven\u201d will typically be said with a skeptical tone.\n\nAdditionally, there are also Justices who indicate skepticism by vocal tone alone \u2014 which is precisely why SAM\u2019s attention to sound is so vital.\n\nWhile SAM is not a perfect representation of reality (will anything ever be?), this promising model is poised to uncover the secret sonic ingredients that are responsible for effective political rhetoric. Future presidential hopefuls for 2020 \u2014 take note!"
    },
    {
        "url": "https://medium.com/center-for-data-science/from-micro-to-macro-kyle-cranmer-talks-machine-learning-for-the-natural-sciences-8c4959478f7a",
        "title": "From Micro to Macro: Kyle Cranmer talks Machine Learning for the Natural Sciences",
        "text": "Scientific research often shifts between two different scales: the microscopic and the macroscopic.\n\nOf the two, the latter scale is perhaps the most challenging.\n\nFor example, in epidemiology (the study of how diseases spread), the microscopic narrative is relatively clear.\n\nSomeone coughs on you, or you touch a surface with a virus on it, and then you either get sick or not. You carry the disease; you spread the disease as you go about your day; then you recover, or you don\u2019t.\n\nBut how can we extrapolate this microscopic picture to the macroscopic scale? How can we study the way disease spreads in a city, or across whole populations?\n\nToday, scientists typically turn to simulations to study how disease spreads, but it is a challenge to extrapolate these simulations into a broader theory. The complex interactions at the microscopic scale give rise to emergent behavior at the macroscopic scale. The same is true for many areas of science.\n\nYet, as CDS professor and particle physicist Kyle Cranmer explains in his recent research, new directions in machine learning are starting to change this.\n\nContrast this with language, health care, and the social sciences \u2014 areas that receive a great deal of attention from the machine learning community. These are primarily data-driven as the mechanistic, causal narrative for what is going on is far from clear.\n\nCranmer thinks that machine learning experts might have great impact if they apply their skills fields already have well developed simulations.\n\nWhat we need, Cranmer says, is a parallel effort in machine learning communities to focus on problems that are simulation-driven as well as those that are purely data-driven \u2014 and his work at CERN\u2019s Large Hadron Collider (LHC) already involves doing precisely that.\n\nCombining particle physics simulations with machine learning, his team is working to get the most out of the 50,000,000 gigabytes of data that the LHC produces per year in order to test out theories about the fundamental forces of the universe."
    },
    {
        "url": "https://medium.com/center-for-data-science/5-minutes-with-director-richard-bonneau-dceda547547e",
        "title": "5 Minutes with Director Richard Bonneau \u2013 Center for Data Science \u2013",
        "text": "One of the things that I like most about CDS is its cognitively diverse culture.\n\nI\u2019m inheriting a really multitalented department. For example, I\u2019m a computational biologist, but Arthur Spirling (our Deputy Director) is a political scientist. We also have computer scientists, psychologists, and linguists \u2014 the list is endless.\n\nBut what unites us as a Center is our shared skillset in data science, which we use to solve key problems in our domains. Since data science is our joint \u201clanguage,\u201d so to speak, we have the exciting opportunity to establish more cross-disciplinary collaborations than other departments. Part of my vision, then, is to keep developing this cognitively diverse culture, and this will involve hiring more top-notch faculty and recruiting strong students.\n\nI\u2019m also hoping to expand on CDS\u2019s human diversity. Ensuring that there\u2019s a balanced gender and racial representation in any STEM field is an ongoing challenge, but we\u2019re lucky in that Data Science is a field that naturally attracts applicants and researchers from a range of backgrounds (especially because of the lucrative employment opportunities that await data scientists when they graduate). As a result, our applicant pool is quite diverse.\n\nData science has been a large part of my research since I started working in biology as a doctoral student. Back then, we worked on a smaller scale. In genomics research, for example, we started with annotating small sets of genome sequences, and building detailed models.\n\nBut as new technologies were invented, scientists were able to go from single data points to collecting millions of data points. Suddenly, we could annotate thousands of genomes at a time. We\u2019re typically taught that the life sciences are not data intensive \u2014 but that has certainly changed now. The rate at which we gather data in biology has gone up by a log unit every eighteen months for the last twenty years of my career.\n\nMy grandfather knew how to identify every species of edible plant within a thousand mile radius. He was a naturalist, and taught me how to be in awe of nature.\n\nBut he was also a machinist, and had one of the most incredible machine shops that I had ever seen, where he would fix, craft, and build. So I was taught to be mechanically inclined very early on, but also encouraged to embrace nature.\n\nBy the time I was twenty, I had worked in several biolabs and knew that although my skillset was computational, my questions were primarily biological. Computational biology is really the only field for this particular combination of skills and interests, so that\u2019s where I work now.\n\nThere are all kinds of pioneers, but the people I look up to most are the ones who pursue their own vision or experiment or method without knowing whether it\u2019s actually going to work out or not \u2014 but they go for it anyway.\n\nA good example is Barbara McClintock. She worked on corn, and without the genetics tools that we have today, she scrutinized chromosomes through a microscope and devised numerous theories, that turned out to be true, about genetics and epigenetics (before the current epigenetics craze).\n\nWhile there\u2019s no hard and fast rulebook on how to produce curious thinkers like McClintock, I think one of the best things for encouraging that kind of intellectual growth at CDS is to continue cultivating a stimulating environment where our faculty and our students have the resources to conduct their work, the time to think, and the space for them to collaborate and inspire one another."
    },
    {
        "url": "https://medium.com/center-for-data-science/cds-welcomes-imagen-technologies-2fc70aa95658",
        "title": "CDS Welcomes Imagen Technologies \u2013 Center for Data Science \u2013",
        "text": "Have you ever tried reading an X-ray? It\u2019s a difficult skill to pin down \u2014 which is why even the best radiologists are susceptible to making mistakes. Recent studies suggest that radiologists are hampered by an average error rate of 30%.\n\nFortunately, at last week\u2019s Company Information Session, Imagen Technologies met our students and explained how their machine learning tools are helping radiologists work towards \u201ca world without diagnostic errors.\u201d\n\nPrevious attempts to build AI tools for radiologists have faced two major problems.\n\nFirst is the inaccessibility of medical data. For machine learning tools to become powerful, they need to train on as much data as possible. Yet, hospitals are often reluctant to hand over their X-ray data and patient records because of privacy concerns.\n\nBut having acquired over 12 billion raw images through a major cross-hospital partnership deal, Imagen Technologies can run experiments and improve their models on rich, high-quality data.\n\nSecond, however, is the challenge of labelling that data. As radiology is such a specific science, X-ray labelling tasks cannot be crowdsourced.\n\nThis is why Imagen Technologies has nurtured a close-knit network of world-class physicians who are labelling the raw image datasets together, along with a group of technical advisors \u2014 one of whom is NYU\u2019s very own Rob Fergus, an affiliated faculty member at CDS.\n\nRight now, the team is training their models to detect fractures.\n\nHaving raised over $21 million from venture capitalists and investors, Imagen Technologies is hoping to change the face of healthcare, and our lucky CDS students have an exciting chance to join their expanding data science team."
    },
    {
        "url": "https://medium.com/center-for-data-science/using-neural-networks-to-address-mental-health-crises-64f256e8695f",
        "title": "Using Neural Networks to Address Mental Health Crises",
        "text": "Although the happy faces of friends and family flash up on our social media feeds every day, remember that behind every sepia-toned smile there may lurk a pool of anxiety, fear, or pain.\n\nDespite what our Instagram feeds suggest, our way of being in the world is not filtered by this color or that color, but by our multiple joys and burdens.\n\nMaybe we\u2019re insecure about our weight. Maybe we have been abused in the past, or are being abused right now. Maybe we no longer know how to feel, or why we want to continue living.\n\nBut how can we use these online platforms to quickly identify someone who requires immediate support, like emergency services or hospitalization?\n\nTurning to neural networks might be the answer.\n\nColumbia University graduate student Rohan Kshirsagar collaborated with NYU Center for Data Science professor Sam Bowman and Koko\u2019s co-founder Robert Morris to invent a powerful online crisis detector that outperforms existing solutions.\n\nA typical crisis detector simply reads through the particular problems that users express online, and then determines what support is best for each case.\n\nMany existing detectors, however, suffer from suboptimal accuracy rates primarily because they have been trained on restrictive datasets. For example, several detectors are trained on Twitter datasets, but only on tweets referring to a specific issue like suicide, thereby leaving out other crucial crises like abuse or sexual violence.\n\nSome researchers have even had to filter out millions of tweets down into a tiny set of 2000 to cut costs and increase efficiency.\n\nThis new crisis detector, however, is outstanding for two reasons.\n\nThe detector is split into two components.\n\nThe first component, which determines whether or not to flag a post, reads all of the posted content. \u201cIt also seeds the explanation generation with words from the input that helped to support its prediction,\u201d Kshirsagar adds.\n\nThe second part, however, is responsible for rationalizing the crisis prediction.\n\nWe often blame social networks for increasing our mental health problems. But, with the right changes, they could also become the answer to those problems. After all, their detector\u2019s accuracy and efficiency could make the difference between life and death."
    },
    {
        "url": "https://medium.com/center-for-data-science/openstreetcab-harnessing-mobility-data-for-transparency-in-urban-transport-91393625e0ba",
        "title": "OpenStreetCab: Harnessing Mobility Data for Transparency in Urban Transport",
        "text": "Ride-sharing apps like Uber and Lyft initially promised to save us big bucks in cities with notoriously high cab fares.\n\nBut with the introduction of surge pricing and the lack of strong data regulations in the taxi industry, it\u2019s unclear whether these apps are making a meaningful difference.\n\nHow can you be sure that you\u2019re getting the best deal possible?\n\nEnter Open Street Cab, an app created by CDS fellow Tassos Noulas in collaboration with researchers in the UK and Belgium.\n\nAt last week\u2019s Moore-Sloan Research Lunch Seminar, Noulas explained how their app \u2014 which operates on a Python server \u2014uses Uber and Lyft APIs and yellow taxi data to compare the predicted prices for a given trip, and then advises the user on which option is cheapest.\n\nUsers submit their trip queries into the app, which consults the Uber and Lyft APIs in real time, and then captures the estimated price of the trip from each service.\n\nTo predict the price of yellow cabs, the app draws from previous data sets released by the government as well as real-time traffic information, and then simulates the trip within the context of that data to provide a price estimate.\n\nOperating in New York, Chicago, London, and Manchester, Open Street Cab has been around for roughly three years now, gained over 10,000 active users.\n\nThe app is now exploring the possibility of predicting surge pricing in NYC based on the human mobility data that they have collected, as well as starting a sister app in Cambridge, UK \u2014 but for taxi drivers.\n\nThe new app, Noulas explained, will visualize taxi cab data in real time, and show drivers where there are taxi shortages in the city.\n\nLearn more about Open Street Cab here."
    },
    {
        "url": "https://medium.com/center-for-data-science/deepmind-fellow-profile-etienne-dejoie-9f401c4319fc",
        "title": "DeepMind Fellow Profile: Etienne Dejoie \u2013 Center for Data Science \u2013",
        "text": "I am Etienne Dejoie. I was born and raised in France. I started my higher education in French \u00ab classes pr\u00e9paratoires \u00bb in Paris and Versailles, and eventually enrolled \u00c9cole polytechnique.\n\nThere, I sharpened my knowledge in Applied Mathematics and Computer Science. During these years I also discovered Machine Learning, and began conducting research, most notably during an internship at Montreal University with Yoshua Bengio.\n\nBefore enrolling in the NYU master\u2019s program in Data Science, I took a year off to work for a start-up in the Bay Area, where I developed a lidar system for autonomous vehicles. Being immersed in a high-end innovation ecosystem was a great experience: I discovered that I wanted to be part of a tech startup project sooner rather than later.\n\nI am very honored and grateful to be awarded the DeepMind Scholarship. At NYU, I am really looking forward to honing my skills in Data Science, discovering a learning experience that is totally different than the one I experienced in France and, last but not least, getting involved in research projects with CDS faculty members."
    },
    {
        "url": "https://medium.com/center-for-data-science/juliana-freire-becomes-first-woman-elected-as-chair-of-acm-sigmod-bfbe745ec058",
        "title": "Juliana Freire Becomes First Woman Elected As Chair of ACM SIGMOD",
        "text": "New York, NY \u2014 Juliana Freire has been elected as chair of the Association for Computing Machinery (ACM)\u2019s Special Interest Group on Management of Data (SIGMOD).\n\nFreire is the first woman to be elected as chair of SIGMOD in its 42 year history.\n\nShe is a professor at the NYU Center for Data Science (CDS) and NYU Tandon School of Engineering. Freire is also the executive director of CDS\u2019s Moore-Sloan Data Science Environment, and served as the Director of Graduate Studies at CDS from 2014\u201317.\n\nEstablished in 1975, ACM SIGMOD is focused on database management systems and technology.\n\nIts members include both academics and industrial researchers, and the group has been a pioneer in establishing reproducibility evaluations for its publications.\n\nDuring her tenure, Freire will strive to increase the adoption of transparency and reproducibility best-practices for database research. Freire brings many years of experience to the position through the seminal work that she has done in the area of computational reproducibility.\n\nAddressing the growing need for data scientists to efficiently replicate their own or other scholars\u2019 experiments for validation and review purposes, Freire and her team recently launched ReproZip: The Reproducibility Packer, a vital tool that simplifies the process of creating reproducible experiments.\n\nShe has also played a major role in building data science tools for projects with a social justice slant, such as the Memex Suite from the Defense Advanced Research Projects Agency (DARPA), which has created new methods that streamline the discovery of information on the Web. Memex tools have been successfully used to uncover crimes taking place on both surface and dark web.\n\n\u2014 \n\nAbout NYU\u2019s Center for Data Science\n\nThe Center for Data Science (CDS) is the focal point for NYU\u2019s university-wide initiative in data science and statistics. Established in 2013 by Yann LeCun (who is now the director of artificial intelligence research at Facebook), CDS is a leading data science training and research facility that collaborates with other schools within NYU, and offers both an MS in Data Science and Ph.D. in Data Science. More information can be found at cds.nyu.edu and @NYUDataScience."
    },
    {
        "url": "https://medium.com/center-for-data-science/cds-welcomes-nbc-universal-89c7f8505c36",
        "title": "CDS Welcomes NBC Universal \u2013 Center for Data Science \u2013",
        "text": "At last week\u2019s Company Information Session at the NYU Center for Data Science (CDS), NBC Universal introduced our students to job opportunities available for data scientists in media broadcasting.\n\nAlthough several industries have been quick to adopt data-driven strategies, media broadcasting has been slower on the uptake \u2014 meaning that there\u2019s ample room for data scientists to apply their skills in this industry.\n\nPredictably, the way that broadcasting companies typically distinguish a \u201cgood\u201d television show from a \u201cbad\u201d one is by measuring the average number of viewers of each series.\n\nA similar process is used for measuring whether particular episodes fly or flop \u2014 only instead of tracking the average number of viewers, they observe how many viewers change the channel over the course of an episode.\n\nBut the advent of online streaming and torrenting has raised some exciting new challenges for tracking this data.\n\nFor example, now that televisions are not our only medium for watching shows \u2014 we have our phones, laptops, and other gadgets \u2014 how can we track viewership data more accurately? Should we find ways to feed all the data from each external source back to the broadcasting company, or do we deploy resources outward so that we can capture the data ourselves?\n\nAnd, how can we aggregate all of our content data to build models that will be able to predict whether viewers will enjoy an episode or change the channel \u2014 before they\u2019ve even seen it?\n\nWhomever can solve these questions stands to make major gains in the entertainment industry. (And, perhaps it will be one of our CDS students!)"
    },
    {
        "url": "https://medium.com/center-for-data-science/can-algorithms-hear-musical-structures-introducing-the-l-measure-8c12815506d1",
        "title": "Can Algorithms Hear Musical Structures? Introducing the \u201cL-Measure\u201d",
        "text": "\u201cThe one good thing about music,\u201d Bob Marley once said, is that \u201cwhen it hits you, you feel no pain.\u201d Instead, you\u2019re embraced by a song\u2019s warm tone or jazzy timbre, its thudding beat or classical melody.\n\nAnnotating and categorizing all of the different musical elements in a song is precisely what the field of Music Informatics Research (MIR) does, for a variety of reasons.\n\nAnnotated music can help us discover auditory patterns in songs of the same genre or across different genres. And, it can also be used to train machines that produce music algorithmically, or help DJs craft appealing mash-ups and remixes.\n\nBut a major problem in the field is that there is simply too much disagreement between existing annotators.\n\nAs CDS data science fellow Brian McFee, CDS affiliated faculty and Associate Professor at NYU Steinhardt and NYU Tandon Juan Pablo Bello, Morwaread Farbood from NYU\u2019s Music and Audio Research Lab (MARL), and Oriol Nieto (Pandora) explain in their peer-reviewed paper, part of the issue is that many annotators do not account for the fact that music is hierarchically constructed.\n\nMoreover, many evaluate the musical data according to a single \u201cground truth\u201d rubric, thereby relying on the \u201cunrealistic assumption that there is a single valid interpretation to the structure of a given recording or piece.\u201d\n\n\u201cEven when annotators do account for hierarchy,\u201d McFee adds, \u201cthe methods we have to analyze their annotations cannot handle hierarchies.\u201d\n\nEnter the L-measure, a novel methodology that McFee, Bello, and their team invented to measure the level of agreement between musical annotators.\n\nStepping away from the \u201cground-truth\u201d model, the L-measure embraces the possibility of multiple valid interpretations because it measures the extent to which musical annotators agree and disagree with each other\u2019s evaluations about the data.\n\nMoreover, the L-measure compares hierarchical annotations \u201cholistically across multiple levels.\u201d\n\nTheir promising methodology is poised to become a powerful tool for those in field \u2014 and for those working in music cognition \u2014 for it can help researchers assess the response similarities between different listeners of the same song."
    },
    {
        "url": "https://medium.com/center-for-data-science/q-a-with-brenden-lake-d3be3e53e0bb",
        "title": "Q&A with Brenden Lake \u2013 Center for Data Science \u2013",
        "text": "I first learned about cognitive science in high school, and I have been hooked ever since.\n\nCognitive science is inherently interdisciplinary, sitting at the interface of data science, AI, psychology, neuroscience, linguistics, anthropology, and philosophy.\n\nTo truly understand intelligence, and to build machines that learn and think like people, we will need contributions from each of these disciplines. The more you learn about the mind, the more fascinating (and sometimes mysterious) it becomes. It\u2019s a very exciting area to work in!\n\nI study computational problems that are easier for people than they are for machines. Although there has been exciting recent progress in artificial intelligence, natural intelligence is still by far the best example of intelligence.\n\nWe can accelerate progress by studying problems that people excel at, with the aim of reverse engineering the mind\u2019s solutions.\n\nTypically, I study these questions through a combination of behavioral experiments and computational modeling. My most successful projects have revealed key cognitive ingredients that people use to solve these problems, but are missing from contemporary machine learning and data science. By including these ingredients, we can build smarter and more human-like learning algorithms.\n\nIt\u2019s true that human learning and decision-making aren\u2019t perfect (nobody is!).\n\nBut compared to the best machines, even a four year old is a much more impressive learner in many ways. Children are continually learning new concepts, asking questions, making explanations, and running experiments \u2014 often based on significantly less data than what we provide to machine learning algorithms.\n\nWe still have a lot to learn about intelligence from studying people, especially children, who are the best learners on the planet."
    },
    {
        "url": "https://medium.com/center-for-data-science/cds-alumna-publishes-new-medical-diagnosis-method-in-nature-2c7e49914e22",
        "title": "CDS Alumna Publishes A New Approach To Medical Diagnosis in Nature",
        "text": "It always starts with a mysterious rash, or a sudden onset of fever and chills.\n\nYou type out your symptoms online \u2014 only to find website after website listing a gauntlet of diseases that may be responsible for your condition, each sounding as ghastly as the next. Which one is it? Will you live? And, how accurate are these diagnoses anyway?\n\nAutomated medical diagnosis tools are poised revolutionize healthcare, but only if we are able to improve their accuracy.\n\nThis is why CDS alumna and data scientist Maya Rotmensch, along with former CDS professor David Sontag, Courant graduate student Yoni Halpern, and researchers from Beth Insrael Deaconess Medical Center (BIDMC), are exploring a new approach for inventing better diagnostic tools, and recently published their findings in Nature.\n\nDoctors currently use what are called Health Knowledge Graphs to diagnose their patients, like the one produced by Google.\n\nThese platforms aggregate data from medical textbooks, journals, and reliable online content, use statistical analysis and NLP to find relationships between symptoms and diseases, and then diagnose patients by comparing their symptoms to that data.\n\n\u201cHowever,\u201d the researchers explain, \u201canother potential source of data, currently underutilized, is the electronic medical record (EMR).\u201d\n\nEMRs are physician and nursing freeform notes about each patient\u2019s medical condition, and their highly unstructured nature is why data scientists have, until now, avoided using them.\n\nBut, as the researchers point out, it is precisely their unstructured nature that makes these notes so valuable, for they have \u201cthe advantage of being closer to the actual practice of medicine than the idealized and curated information presented in textbooks and journals.\u201d\n\nThe researchers collected over 200,000 EMRs from BIDMC to create a new health knowledge graph, compared their model against Google\u2019s platform, and found that their graph achieved a strong precision rate, meaning that it is a brilliant candidate for real-life applications.\n\nRead more about their work here."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-center-for-data-science-students-selected-for-forbes-under-30-summit-bb78ae2fee04",
        "title": "NYU Center for Data Science students selected as Forbes Under 30 Scholars",
        "text": "New York, NY \u2014Our first-year M.S. Data Science students, Asena Derin Cengiz, Yanchao Ni, Atakan Okan, and Ksenia Saenko, have been selected as Forbes Under 30 Scholars for the Under 30 summit this October.\n\nThe summit brings together over a global community of CEOs, founders, leaders, and mentors to Boston for four days of discussion panels, networking opportunities, hackathons, and start-up competitions.\n\n\u201cI\u2019m excited to attend the Under 30 Summit because I aspire to learn from leaders who are changing the world using quantitative analysis,\u201d said Ksenia Saenko, who is also a DeepMind fellow at CDS for the 17\u201318 academic year.\n\nWith over 200 world-class speakers ranging from fashion designers to entrepreneurs , the summit is an opportunity to learn, network, collaborate, recruit and exchange ideas.\n\n\u201cI\u2019m so excited to attend the summit,\u201d added Yanchao Ni. \u201cAs I begin my first year at CDS, I\u2019m trying to envision what life would be like post-graduation \u2014 and this summit is a perfect chance for me to find out what the most successful under-30\u2019s are doing.\u201d\n\nThe summit\u2019s speaker line-up includes Yasmin Green, the Director of Research and Development at Jigsaw, a technology incubator for Alphabet Inc., Bozoma Saint John, the Chief Brand Officer for Uber, and Feng Zhang, a core Member of the Broad Institute at MIT and Harvard.\n\n\u201cI\u2019m extremely happy to be chosen as one of this year\u2019s Forbes Under 30 Scholars,\u201d said Asena Derin Cengiz. \u201cI\u2019m looking forward to the summit this October, and to the great opportunities it will offer to me!\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/deepmind-fellow-profile-hollis-nymack-191e15d16423",
        "title": "DeepMind Fellow Profile: Hollis Nymark \u2013 Center for Data Science \u2013",
        "text": "I grew up and currently reside in New York City. My interest in mathematics began when I was a child solving math puzzles with my father.\n\nAfter graduation from the Brearley School in Manhattan, I attended Boston College where I earned my B.A. in Mathematics.\n\nOne of my first positions after graduation was working as a programmatic media analyst at Vivaki. In this role, I began to learn about how data has the power to shape one\u2019s everyday life, and contributed several research papers to Vivaki\u2019s thought leadership program.\n\nCurrently, I am an Associate Director of Analytics at VM1 (a Zenith agency dedicated to Verizon). Some of the projects I have led include conducting an analysis to inform Zenith leadership of the accuracy and authenticity of ad verification vendors.\n\nAdditionally, I designed and coached my team to conduct log-level based time lag to conversion analyses. It has been exciting and gratifying to not only carry out my own analyses, but also to lead and mentor a team on how to do so as well.\n\nAt NYU, I look forward to exploring data outside of the advertising industry to solve new kinds of problems. As a DeepMind Fellow, I hope to not only learn new methods of analysis, but also contribute to projects focused on creating tools and products to improve everyday life."
    },
    {
        "url": "https://medium.com/center-for-data-science/using-data-to-learn-how-to-have-successful-online-interactions-2dac43a9cfbe",
        "title": "Using data to learn about successful online interactions",
        "text": "What makes a successful online interaction? In a working paper that combines sociology with data science, affiliated faculty member at CDS and sociology professor Paul DiMaggio, along with Clark Bernier (Princeton), Charles Heckscher (Rutgers), and David Mimno (Cornell) explore the rules that govern successful online conversations.\n\nTitled \u201cInteraction Ritual Threads: Does IRC Theory Apply Online?\u201d the paper is \u201cthe first empirical application of [sociologist] Randall Collins\u2019s theory of interaction ritual chains (IRC) to internal corporate interactions.\u201d\n\nIntroduced in 1981, IRC theory has been widely used to predict whether human interactions will succeed according to how much emotional energy those interactions produce.\n\nFirst, that people gravitate towards situations where their emotional energy will be enhanced.\n\nSecond, that higher levels of emotional energy are associated with \u201cgreater buoyancy, confidence, attractiveness, and influence, and shared feelings of conviction and moral rectitude.\u201d\n\nWhen applying IRC theory to online interactions, however, some stumbling blocks arise. After all, doesn\u2019t Collins state elsewhere that strong human interactions rely heavily on physical co-presence and temporal synchrony \u2014 the two ingredients that online conversations lack?\n\nOnline forum posting, for example, does not require physical presence or temporal synchrony (since one may post a comment that someone else will not read until ten minutes later).\n\nYet, as DiMaggio and his researchers argue by way of another theorist, Mikhail Bakhtin, online communication is so distinctive (e.g. acronyms, emojis, hashtags) that it can be considered a family of distinctive speech genres.\n\nSo online communication is not merely junk: it comprises many unique language forms.\n\nAnyone who reads or writes on the internet, then, still enters into a meaningful conversation, even without physical presence or temporal synchrony.\n\nWith this in mind, DiMaggio and his team applied IRC theory to analyze online interactions in two internal IBM online forums: \u201cThe Values Jam\u201d and \u201cThe World Jam.\u201d\n\nAfter extracting over 40,000 forum posts, the researchers cleaned the textual data and used topic modelling to categorize each post according to one of 30 topics. They also tracked the use of pronouns, the response time between posts, and average word length of posts.\n\nThe textual data revealed two significant conclusions about what predicted success in these online interactions.\n\n1. Online posts that focus tightly on a specific topic, and sustain that focus over time, are more likely to elicit responses. \u201cCommon focus,\u201d the researchers explained, \u201cmatters in the way that IRC predicts, if not through precisely the same mechanisms.\u201d\n\n2. The quicker the response time to a post, the more likely it is that someone else will respond as well. Again, temporal rhythm matters \u201cin the way that IRC predicts, even without mechanisms that require co-presence.\u201d\n\nAlthough more research is needed to examine whether the same rules apply elsewhere, their fascinating study suggests that IRC is a useful theory for assessing online communications."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-center-for-data-sciences-ph-d-program-begins-with-inaugural-cohort-of-4-students-2f3c13897f57",
        "title": "NYU Center for Data Science\u2019s Ph.D. program begins with inaugural cohort of 4 students",
        "text": "New York, NY \u2014 The inaugural cohort of Ph.D. students at the NYU Center for Data Science (CDS) begin classes today.\n\nThe Center\u2019s Ph.D. program in Data Science is the first of its kind in the country.\n\n\u201cA key aspect of the program is its interdisciplinary foundation,\u201d explained CDS director Richard Bonneau. \u201cWe have a topically diverse and research-active faculty who are experts in business analytics, computational biology, data visualization, reproducibility, political science, high energy physics, to name a few.\u201d\n\n\u201cMentored by our faculty, the students will be engaged in a lot of cross-disciplinary research. The program will help them develop a core set of data science skills, and then apply them to their preferred application domains.\u201d\n\nManaged by CDS Director of Graduate Studies for the Ph.D., Vasant Dhar, the 72-credit hour program is designed to be completed in five years, and all students are guaranteed financial support for that duration.\n\nIt includes a research practicum where students rotate amongst the faculty, and are exposed to the diverse faculty interests from the start. They also have the opportunity to take courses across all the schools at NYU.\n\n\u201cNYU has a great deal of talent and thought leadership in Data Science across the various schools,\u201d said Dhar. \u201cThe new Ph.D. program in Data Science gives students an unprecedented opportunity in learning how to conduct inquiry in this new and exciting field which is transforming society in profound ways, wringing efficiency in every aspect of our lives and enabling new and exciting capabilities that would have been considered science fiction until recently.\u201d\n\nThe program attracted 283 applicants this year, and the acceptance rate was 3.5%. Of the ten offers made, five accepted. \u201cWe expect to increase the size of our incoming Ph.D. class next year by a factor of two to three,\u201d said Bonneau.\n\nThe Ph.D. program is accompanied by a successful and selective Master\u2019s program in Data Science, which has been in place since 2013.\n\nBoth programs have several courses in common. Like the Ph.D. program, the Master\u2019s program is also highly interdisciplinary. Master\u2019s students eventually choose one of six topic \u2018tracks\u2019 during their studies \u2014 data science, big data, mathematics and data, natural language processing (NLP), physics, or biology.\n\n\u201cAlthough our students and faculty work in diverse topic areas, said Bonneau, \u201cour shared skillset in data science is a common thread that weaves us together at the Center. It\u2019s going to be an exciting year ahead for data science at NYU.\u201d\n\nLearn more about how to apply here.\n\nLearn more about our faculty here.\n\n\u201cNew Breed of Super Quants at NYU Prep for Wall Street\u201d (Bloomberg)\n\n\u2014 \n\nAbout NYU\u2019s Center for Data Science\n\nThe Center for Data Science (CDS) is the focal point for NYU\u2019s university-wide initiative in data science and statistics. Established in 2013 by Yann LeCun (who is now the director of artificial intelligence research at Facebook), CDS is a leading data science training and research facility that collaborates with other schools within NYU, and offers both an M.S. in Data Science and Ph.D. in Data Science. More information can be found at cds.nyu.edu and @NYUDataScience."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-center-for-data-science-welcomes-new-director-richard-bonneau-c2a6c7ba73b8",
        "title": "NYU Center for Data Science Welcomes New Director Richard Bonneau",
        "text": "New York, NY \u2014 Today we welcome Richard Bonneau as the new director of the NYU Center for Data Science (CDS). He follows Claudio Silva, who served as director from 2016\u201317.\n\nDr. Bonneau has been a Professor of Biology and Computer Science at NYU since 2005, and an affiliated faculty member of CDS since its establishment in 2013.\n\nHis research uses data science to solve problems in two areas of computational biology: learning biological networks, and predicting and modeling protein structures. He was one of the initial authors of Rosetta, a platform that is now one of the world\u2019s most powerful computational tools for predicting and designing protein structures, and for helping thousands of other biochemists design new life-changing proteins.\n\nHis work also involves using genomics data to learn biological networks for systems biology research.\n\nHe has played core leadership roles across several NYU departments, not only as the head of the Bonneau Lab at the NYU Center for Genomics and Systems Biology, but also as leader of the Social Media and Participation Lab (SMaPP), and as a steering committee member of CDS\u2019s Moore-Sloan Data Science Environment, a $37.8 million cross-institutional grant that harnesses the potential of data scientists and big data for scientific discoveries.\n\nPrior to NYU, he was a senior scientist at the Institute for Systems Biology in Seattle. He received his Ph.D. in Biochemistry working with Dr. David Baker from the University of Washington, and his B.A. in Biochemistry from Florida State University.\n\nOutside of NYU, he is the Group Leader for Systems Biology at the Simons Foundation\u2019s Flatiron Institute, which is devoted to the advancement of scientific research through computational methods and data analysis.\n\nWe are delighted to have him with us, and we look forward to an exciting year ahead.\n\n\u2014 \u2014 \n\nAbout the NYU Center for Data Science\n\nThe Center for Data Science (CDS) is the focal point for NYU\u2019s university-wide initiative in data science and statistics. Established in 2013 by Yann LeCun (who is now the director of artificial intelligence research at Facebook), CDS is a leading data science training and research facility that collaborates with other schools within NYU, and offers both an MS in Data Science and Ph.D. in Data Science. More information can be found at cds.nyu.edu and @NYUDataScience."
    },
    {
        "url": "https://medium.com/center-for-data-science/which-encoding-mechanism-is-best-for-chinese-english-japanese-and-korean-8071e70e0459",
        "title": "Which encoding mechanism is best for Chinese, English, Japanese, and Korean?",
        "text": "As the Silicon Valley quarrels over its diversity policies thanks to the Google Diversity Memo fiasco, CDS\u2019s founding director and director of Facebook AI Research, Yann LeCun, and his doctoral student, Xiang Zhang (NYU Courant), are taking another approach to make the tech community a more inclusive space \u2014 at least, in terms of its languages.\n\nIn \u201cWhich Encoding is the Best for Text Classification in Chinese, English, Japanese, and Korean?\u201d, Zhang and LeCun produce the first systematic study of 37 existing encoding methods on 14 large-scale data sets \u2014 473 models in total \u2014 to find out which one can best handle western and non-western languages.\n\nDrawing from sites like the Chinese online restaurant review website dianping.com, Japanese online shopping website rakuten.co.jp, Korean online shopping website 11st.co.kr, and The New York Times, they collected 14 multilingual datasets and used over 10 million samples for testing and training. \u201cThe model [that] achieved the best consistent performance,\u201d they concluded, \u201cis the character level 5-gram fastText.\u201d\n\nfastText is a popular method developed at Facebook AI research, which is available in open source.\n\nTheir study not only helps computer scientists and data scientists perform multilingual text processing more efficiently, but also encourages the development of research projects with a more global dimension. And, Zhang and LeCun will also be releasing all the code that they used in their study under an open source license for the community.\n\nWell, data scientists? What are you waiting for? It\u2019s time to buck up and embrace the globe more fully. Learn more about their work here."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-center-for-data-sciences-jennifer-hill-wins-miller-prize-d3e3c9bdf828",
        "title": "NYU Center for Data Science\u2019s Jennifer Hill Wins Miller Prize",
        "text": "New York, NY \u2014 Jennifer Hill, a professor at NYU\u2019s Center for Data Science (CDS) and NYU\u2019s Department of Applied Statistics, Social Science, and the Humanities, has won Political Analysis\u2019s Miller Prize for her co-authored paper, \u201cBias Amplification and Bias Unmasking.\u201d\n\nThe Miller Prize is awarded annually for the best work published in Political Analysis, the official journal of the Society for Political Methodology and the Political Methodology Section of the American Political Science Association.\n\nWritten with Joel A. Middleton (UC Berkeley), Marc A. Scott (NYU Steinhardt and also an affiliate of CDS), and Ronli Diakow (New York City\u2019s Department of Education), the winning study investigates unexpected sources of bias in observational studies.\n\n\u201cThey emphasize,\u201d the selection committee explained, \u201cthe potential for group fixed effects, often seen as a benign robustness strategy, to increase bias through both amplification and unmasking.\u201d Moreover, the researchers also \u201cdevelop tools for sensitivity analysis to help applied researchers reason through these problems\u201d in the paper.\n\n\u201cThese methodological contributions are cleverly and lucidly illustrated through constructed observational placebo studies, which demonstrate the striking increases in bias that result from the inclusion of fixed effects.\u201d\n\nThe paper is also central to Hill\u2019s work at the PRIISM Applied Statistics Center, a university-wide initiative devoted to improving statistical practice, where she is co-director with Scott.\n\nLast year\u2019s Miller Prize winner was former CDS Moore-Sloan Fellow, Pablo Barbera, for his paper, \u201cBirds of the same feather tweet together: Bayesian ideal point estimation using Twitter data.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-scientists-invent-new-protein-for-regenerative-medicine-12fb8c32682f",
        "title": "NYU Scientists Invent New Protein for Regenerative Medicine",
        "text": "\u201cHow,\u201d crooned British pop band the Bee Gees to their fans in the 1970s, \u201ccan you mend a broken heart?\u201d Well, those working in the field of regenerative medicine may have an answer. Focused on discovering ways to replace or regenerate organ tissues that are damaged from aging or disease, the field envisions a future where those with spinal cord injuries can walk again, where entire organs can be regenerated for transplants and, of course, where broken hearts can be healed or replaced.\n\nA major challenge facing the field, however, is creating biosynthetic materials that can not only deliver targeted drug therapy or tissue engineering to a damaged area, but also non-invasively visualize the surrounding cells so that doctors can monitor their patient\u2019s treatment progress. Although some advances have been made, a full solution has not yet been discovered \u2014 but it looks like Associate Professor Jin Montclare from NYU Tandon, NYU Center for Data Science\u2019s Director Richard Bonneau, and Associate Professor Youssef Zaim Wadghiri from NYU School of Medicine are on their way to cracking the case.\n\nTheir clever protein is the first of its kind to be engineered on a micrometer scale, and closely mimics the structural and molecular similarities of self-assembling proteins already found in nature. Moreover, their engineered protein microfibers can also bind to other small molecules like curcumin, meaning that they have the potential to store and deliver specific chemical agents to treat affected cells.\n\nWith the support of an NSF grant valued at $1,585,544, they are moving onto the next phase of their project: adding an imaging component. Knowing that processes like MRI scans map cells by coating them with fluorine (a chemical element acting as a tracer) or iron oxide nanoparticles (magnetic contrast agent), they want to tag their protein engineered coiled-coil microfibers with the same element or tracer so that doctors can use biomedical imaging to determine whether the drugs have been successfully delivered, and gather data about treatment progress.\n\nTaking a computational approach, they will further develop their protein engineered coiled-coil using Rosetta, a powerful platform that not only performs protein folding predictions but also helps biologists design new proteins.\n\nCrucially, data collected from fully characterized proteins and initial experiments will be used to refine new computational designs as they are made. Bonneau, a core member behind Rosetta\u2019s development, will provide his expertise during the project\u2019s design and testing stage, while Wadghiri\u2019s radiology experience will guide the construction of appropriate fluorine probes for bioimaging.\n\nTheir innovative project is poised to transform the field of regenerative medicine. And, it also incorporates an excellent mentoring opportunity for young scientists. As part of NYU\u2019s Scientific Outreach and Research program (SOAR) founded by Montclare, undergraduates will be hired to teach the high school students of Urban Assembly Institute for Math and Science for Young Women (UAI) about the biomaterials and biomedical imaging that are a part of this research project."
    },
    {
        "url": "https://medium.com/center-for-data-science/dah-tah-or-day-tah-analyzing-the-global-americanization-of-british-english-49a08c04532e",
        "title": "Dah-tah or Day-tah? Analyzing the global Americanization of British English",
        "text": "When speaking to the British Council in 1995, Prince Charles made an extraordinary claim. \u201cAmerican English,\u201d he exclaimed, \u201cis very corrupting.\u201d He went on to argue that British English must uphold its position as the world\u2019s language. But 22 years on, the results from CDS faculty fellow Bruno Gon\u00e7alves\u2019 data-driven study of the global Americanization of English may ruffle His Royal Highness\u2019 feathers.\n\nIn \u201cThe Fall of the Empire: The Americanization of English,\u201d Gon\u00e7alves, along with professors Luc\u00eda Loureiro-Porto, Jos\u00e9 J. Ramasco, and David S\u00e1nchez ask some powerful questions.* How is English used around the world, both in formal and informal contexts? And, do postcolonial dialects \u2014 such as Nigerian English, India\u2019s Hinglish, or Singapore\u2019s Singlish \u2014 draw from American or British English?\n\nThe team began their investigation by compiling a list of American and British words. Then, they created a dataset comprised of over 200 million geo-tagged tweets globally from 2010 to 2016, and the Google Books repository of books produced by both British and American publishers since 1800. Using this data, they calculated the global presence of British and American English according to two metrics: spelling and vocabulary.\n\nPredictably, they found that British English influence on spelling is strongest in its former colonies, like Australia, India, New Zealand, and South Africa, since their schools typically subscribe to British English. But when it comes to vocabulary, Americans have the global upper hand. The same postcolonial countries which subscribe to British spelling actually use American vocabulary more often, and the same tension between British spelling and American vocabulary also exists in Western Europe. This is likely due to the global dominance of American culture and the fact that several European cities today are major transportation hubs and tourist destinations, like Milan and Paris.\n\nOverall, then, the data suggests that the English language is becoming increasingly Americanized. But the researchers wanted to go even further. Can we pinpoint when this split between American and British English occurred, and visualize how Americanized English evolved over time?\n\nReturning to their data set, they measured the average ratio per year of American and British words in the Google Books data set. Their timelines confirm historical evidence: for example, it shows that the rapid increase of the Americanization of English occurs around 1828, when Noah Webster first published what is more commonly known today as America\u2019s Merriam-Webster Dictionary.\n\nAnother interesting instance is the sudden decline of American vocabulary and spelling ratios from 1960 to 1970. The momentary dip, they explain, hints at the brief Britishization of American English \u2014 a phenomenon that they attribute to the massive influx of European immigrants that fled to the US after WWII. (If British English wants to become the world\u2019s language again, does this data suggest that how migration and border control are key factors that will determine its future?)\n\nPerhaps Prince Charles\u2019s dream, then, was half-realized. Like a seed that blooms into a many-petaled sunflower, English has sprouted into Englishes, where dialects like Chinglish, Singlish, and Hindish combine American vocabulary and British spelling with idioms and words from their first language. English is still, indeed, the lingua franca \u2014 just not the version that he wanted.\n\n*Luc\u00eda Loureiro-Porto is from the Spanish Philology Department at the Universitat de les Illes Balears, while Jos\u00e9 J. Ramasco and David S\u00e1nchez hail from the Interdisciplinary and Complex Systems Physics Institute (IFISC) at the same university."
    },
    {
        "url": "https://medium.com/center-for-data-science/from-newspapers-to-nlp-recording-fatality-data-506e4f848dc",
        "title": "From newspapers to NLP: recording fatality data \u2013 Center for Data Science \u2013",
        "text": "Because approximately 30% of all law enforcement homicides go unreported, we lack reliable data about the frequency of civilian fatalities and police force usage. The alternative is to scour through news reports: crowd-sourced activists like Fatal Encounters have manually read through two million articles, and the Bureau of Justice Statistics hires people for the same task.\n\nBut Brendan O\u2019Connor from University of Massachusetts Amherst may have found a way to make this process easier. Speaking at one of our text-as-data seminars last semester, O\u2019Connor explained how he and his colleagues have trained computational models to obtain fatality records from the news.\n\nTheir new approach uses NLP for social analysis by performing two tasks. The first task concerns database population, where the model computationally infers the names of people killed by police during a particular time frame. The second task is updating the records of an existing database with the new information.\n\nEvery instance of a name in any news report is treated as a \u201cmention.\u201d For each \u201cmention,\u201d the goal is to assess the probability of whether or not it describes whether that person was, indeed, killed by police. The hope is that a reasonable prediction will result from each \u201cmention,\u201d followed by an aggregation of the classification probabilities to determine how probable it is that this person was killed by police. If it\u2019s highly probable, the corresponding data is added to the database.\n\nUsing Google News, the researchers downloaded 1.1 million news reports published between September and December 2016, and ran a news scraper that identifies names and keywords. Compared to Fatal Encounters, the group\u2019s algorithm successfully detected 258 out of the actual 452 positive civilian fatalities caused by police.\n\nFurther work is necessary to achieve the same accuracy as manual techniques before the algorithm can be put to use by practitioners. Ultimately, however, their promising model could become a powerful tool to support groups like Fatal Encounters and the Bureau of Justice Statistics."
    },
    {
        "url": "https://medium.com/center-for-data-science/data-science-medicine-interdisciplinary-learning-14f439e69a79",
        "title": "Data Science + Medicine = Interdisciplinary Learning",
        "text": "While data science often plays a major role in medical research, collaboration between both fields is rare at NYU since our research labs are scattered miles apart in Manhattan and Brooklyn. But this is all set to change thanks to the first workshop on data science for medicine and health that CDS professors Kyunghyun Cho, Rumi Chunara, and Juliana Freire organized last Friday, along with David Fenyo, Narges Razavian, and Daniel Sodickson from NYU\u2019s Langone School of Medicine.\n\nThe workshop, which gathered faculty, researchers, and graduate students from CDS, Langone, the Courant Institute of Mathematical Sciences, and the Tandon School of Engineering, was a chance for everyone to share their work with others outside of their department.\n\nThe talks revealed that neuroscience is an area where data science is becoming increasingly important. From Bijan Pesaran\u2019s presentation on the Brain Initiative, to Fenyo\u2019s biomedical imaging project, and Guido Gerig\u2019s talk about aging and cognition, the brain emerged as a fascinating intersection point for scholars to showcase how data science methodologies have helped to track hormone levels, manipulate protein sequences, and generate clearer MRI scans.\n\nAnother area where data science has become vital is in bioinformatics and population health research. But, as Chunara, Yindalon Aphinyanaphongs , Aristotelis Tsirigos, and Itai Yanai explained during the panel discussion, enhancing these interdisciplinary research opportunities depends on finding a robust way to consolidate all of our departmental datasets, resources, and research projects in one centralized hub.\n\nSome may, of course, fear that fostering links between data science and medical research could cause many to lose their jobs: what if they are replaced by algorithms and artificial intelligence? But the ultimate goal \u2014 as summed up eloquently by Sodickson\u2014 is for data scientists and medical researchers to combine their skill sets to invent more effective technologies to combat complex conditions, and then teaching medical professionals how to use, monitor, and support those technologies when they are used to treat our patients. This successful inaugural workshop is an exciting first step toward that worthy endeavor.\n\nThe event was sponsored by the Moore-Sloan Data Science Environment, the Institute of Computational Medicine (Langone), and the Department of Radiology (Langone), and hosted at Lipton Hall at the NYU Law School. Find out more about the workshop\u2019s presenters here."
    },
    {
        "url": "https://medium.com/center-for-data-science/speaking-robot-nese-3bfefbde4a60",
        "title": "Speaking Robot-nese \u2013 Center for Data Science \u2013",
        "text": "As robots take over our world, they must not only learn how to communicate with us but also with each other. Recent scholarship has so far demonstrated that it\u2019s possible for two robots to communicate in a shared language in the form of binary vectors. These conversations between the sender (Robot A) and receiver (Robot B) are typically mono-directional, and limited to a fixed number of yes/no answers.\n\nBut CDS Master\u2019s student Katrina Evtimova, professor Kyunghyun Cho, Andrew Drozdov (NYU Computer Science), and Douwe Keila (Facebook) all believe that our robots can do better. In \u201cEmergent Language in a Multi-Modal, Multi-Step Referential Game,\u201d they have invented a new conversational game for robots that mimics human communication more closely.\n\nFirst, the robots in their new game are prompted to have bi-directional conversations, meaning that they are allowed to send as many messages to each other as they would like. Second, they must use their shared language to exchange two different modes of information: text and image.\n\nThe game itself, then, unfolds something like this. The sender robot is shown photograph of a mammal, and then instructed to communicate what it saw to the receiving robot. The receiver is prompted to guess what animal the sender saw by reading the textual data, and then asking the sender additional questions about the photograph. This means that the conversation is not merely about shifting information from one robot to another, but exchanging different modes of information (e.g. textual and pictorial) between them. To help the robots, the researchers harnessed the power of neural networks and implemented techniques like visual and textual attention into the game.\n\nThe researchers found that the multi-modal, multi-step game not only improved the robots\u2019 predictive accuracy, but that their conversations also became more human-like. Will these developments bring humans and machines closer together \u2014 or will we become too close for comfort?"
    },
    {
        "url": "https://medium.com/center-for-data-science/using-data-to-detect-discrimination-b77a4f8e5430",
        "title": "Collecting data to detect discrimination \u2013 Center for Data Science \u2013",
        "text": "In \u201cCan the Government Deter Discrimination?\u201d our Moore-Sloan postdoctoral researcher, Andrew Guess, along with Albert Fang (Yale) and Macartan Humphreys (Columbia), conducted a 20 month experiment from April 2012 to December 2013 in partnership with the NYC municipal government to explore how racial discrimination still exists in the city\u2019s rental market.\n\nThe researchers began by scouring daily rental ads on Craigslist (the primary advertising platform for NYC rental listings) and randomly selected the ads where potential tenants were invited to inquire by phone. Then, the experiment employed Black, Hispanic, and White testers to make individual appointments for viewing advertised units, to interact with the associated landlord (or broker), and to record interactions before, during, and after the viewing. When a Black, Hispanic, and White tester of the experiment all made an appointment to view the same housing unit with the same landlord, that landlord was admitted into the field experiment.\n\n652 were admitted in total, and randomly assigned to one of three treatment conditions. The first was the control condition: no governmental messages were sent to landlords and brokers in this group. The second was the monitoring condition. A treatment script informed the landlord or broker that the call was from the NYC Commission on Human Rights, and expressed a friendly reminder of their responsibility to comply with fair housing laws. Lastly, the final condition began with the same treatment script as the monitoring condition \u2014 but with the addition of potential legal ramifications should they not comply.\n\nThe control condition found that Hispanics are 28% less likely than Blacks and Whites to receive a callback or an apartment offer from landlords and brokers, and 49% less likely to receive an offer for an apartment than Whites. There was also no difference between monitoring and punitive messaging relative to the control \u2014 but the data that they collected suggests that punitive messaging has some effect on reducing discrimination towards Hispanics (although the same cannot be said for Blacks).\n\nWhile their data did not produce statistically significant results to all aspects of the study, these preliminary results do challenge the US Department of Housing and Urban Development\u2019s 2012 audit report, which found that there was a lack of racial discrimination in the New York City rental market \u2014 a conclusion that, as Guess and his researchers have shown, may need to be revised."
    },
    {
        "url": "https://medium.com/center-for-data-science/needle-in-a-haystack-improving-crowd-sourced-work-by-finding-the-best-workers-c66140142823",
        "title": "Needle in a haystack: improving crowd-sourced work by finding the best workers",
        "text": "Although computers can surpass humans in both intelligence and speed, there are still some tasks that they cannot perform equally, like language translation, writing product descriptions, or image tagging. Enter crowd sourcing micro-task platforms like Amazon Mechanical Turk (AMT), where humans are paid to complete the work that computers can\u2019t.\n\nNot only do these platforms provide affordable labor for tech giants who require human skills, but they also promise high productivity. Some challenges associated with these platforms, however, are the varying skill sets that the \u201ccrowd\u201d has, which results in equally varied task execution. Inserting a quality verification stage is possible but unlikely as it would raise prices for tasks to be completed. How else, then, can we improve the quality of crowd-sourced tasks while keeping costs low?\n\nPanos Ipeirotis and Foster Provost, two professors affiliated with CDS and working at NYU\u2019s Stern School of Business, have recently been addressing this very question in their paper, Cost-Effective Quality Assurance in Crowd Labeling.\n\nSeeking to strike a balance between the main advantage and challenge of micro-crowdsourcing, Ipeirotis and Provost propose implementing a two-phase framework. Taking labeling as an example of a crowd-sourced task, phase one of their framework is label allocation, where workers label objects as usual. Phase two, however, is the inference phase where an algorithm derives the true labels of the objects.\n\nWhat makes Ipeirotis and Provost\u2019s framework unique is that it allocates crowd sourcing tasks to the worker based on their ability. As labeling tasks often vary in difficulty, when a worker is completing one task, the framework provides \u201calgorithmic estimates of object and worker quality from all the labels obtained so far\u201d so that, next time, it can allocate tasks that match the worker\u2019s abilities more closely. By collecting data on the worker\u2019s performance, this adaptive system could improve the quality of the work that crowd sourcing platforms produce since workers would only be allocated tasks within skill level.\n\nA value metric was also presented in the paper. True to its name, the metric measures the quality of a worker\u2019s labeling, which allows employers to reward workers who meet their quality requirements with bonuses. For interested parties, the software that Ipeirotis and Provost have created is available for use here."
    },
    {
        "url": "https://medium.com/center-for-data-science/serving-the-right-ad-to-the-right-costumer-7f5e1388303e",
        "title": "Serving the right ad to the right customer \u2013 Center for Data Science \u2013",
        "text": "How do webpages regulate the advertisements that you see online? At our Wednesday\u2019s Research Lunch Seminar, Dr. Yana Volkovich, a senior data scientist from App Nexus, discussed how digital advertising relies on data-driven networks.\n\nIn the past, digital advertising operated on a direct buyer-seller model. A company who wanted to place an ad on a webpage would have to contact the webpage\u2019s owner, negotiate a price, send their image files, and then wait for the owner to upload the ad. But, as Volkovich explained, today the game has changed. Although the direct buyer-seller model still exists, much of digital advertising now operates on a data-driven ad network and ad exchange model.\n\nWhen advertisers and websites join an ad network, all available ads are collected in one pool, and all the websites who would like to display ads are collected in another. Then, an algorithm will choose which ad is distributed to a given website within the few milliseconds that a user is waiting for a webpage to load, based on two probability metrics: the highest predicted number of user impressions (how many people will see the ad), or the highest predicted number of user clicks that their ad will generate.\n\nTo make these predictions, multiple calculations must be performed. On one hand, the algorithm must determine what kind of ad they are handling, and identify who the ad\u2019s ideal audience is. On the other hand, the ad network must also analyze massive data sets about the user base of each website within their network. Factors like the average age, gender, location, and occupation of each website\u2019s user base are crucial indicators that determine whether or not an ad will fly or flop.\n\nHowever, App Nexus, where Volkovich works, goes one step further. After aggregating the highest predicted number of user impressions and clicks, they can also calculate click purchase probability, which refers to how many people will buy the item that is advertised. While this is valuable information, the process requires a more detailed look at a website\u2019s user base, like tracking each user\u2019s cookies and recording the history of their clicks.\n\nWhether the strategies of ad networks raise concerns about user privacy remains part of an on-going national debate. But, as you wait for your next webpage to load, it\u2019s hard not to marvel at the computational work that swings ads on and off websites in a matter of milliseconds."
    },
    {
        "url": "https://medium.com/center-for-data-science/women-in-data-gathers-women-from-every-data-driven-field-f4fc10916113",
        "title": "Women in Data gathers women from every data-driven field",
        "text": "Earlier this month, the Center hosted the Women in Data (WID) event, run by Norma A. Padron and Tran Ly. The two women have recently rebooted the non-profit organization, which was founded 5 years ago. Today, WID exists to educate and foster a community of women in data-driven fields like journalism, health care, finance, and technology.\n\nIn this session, both women discussed how to bridge data science techniques and design thinking to improve the healthcare industry. Padron, a current board member of Women in Data and Associate Director at the Main Line Health Center for Population Health Research at the Lankenau Institute for Medical Research, began by explaining the importance of implementing new interdisciplinary approaches like computational design (which uses data to inform how physical spaces are constructed) to improve healthcare.\n\nFor example, noise in hospitals caused by machinery, lights turning on and off, and nurses and doctors making their rounds, all disrupt the much needed sleep that patients need to recover from surgery or other illnesses. By gathering data and interacting with the physical hospital space, however, computational design could measure the times those noises spike and how it affects people\u2019s sleep in order to address the problem.\n\nAnother way data science can help improve healthcare was also highlighted by Ly, a past board member of Women in Data and a data strategist at New York-Presbyterian Hospital. Demographic and medical information, she explained, are not only widely available but also expected to double every 73 days by 2020. With so much data pouring in, those in the healthcare industry need efficient digital tools to organize the data in a meaningful way (such the fantastic City Health Dashboard that NYU Langone is developing!)\n\nThe growing emphasis on exploring the social determinants of health, such as unemployment, insurance, and median income also means that the healthcare industry is increasingly looking beyond the four walls of the hospital to track where, when, why, and how physical ailments and illnesses develop. As doctors prepare to combine medical data with social demographics and statistics, developing an effective interdisciplinary and data-driven approach to healthcare has never been more crucial."
    },
    {
        "url": "https://medium.com/center-for-data-science/true-false-neutral-teaching-machines-to-understand-words-not-just-read-them-4098c7161e47",
        "title": "True, false, neutral: teaching machines to understand words, not just read them",
        "text": "Although natural language processing (NLP) has made major strides in the last few years, to what extent can an NLP algorithm understand human sentences beyond a superficial read? Although they can computationally identify, count, or regurgitate individual words, phrases, and sentences, can they capture the meaning behind the words that they are handling? These questions are at the heart of a fledgling sub-field within NLP called Natural Language Inference (NLI), where Center for Data Science Professor Sam Bowman\u2019s work is currently focused.\n\nA typical NLI test runs something like this: the algorithm is given a sentence like \u201cA man is playing with a dog,\u201d and then a hypothesis about that sentence like \u201cTwo dogs are playing together.\u201d The algorithm would then be asked to compare the sentence with the hypothesis, and infer whether the hypothesis is contradictory (e.g. false), neutral, or an entailment (e.g. true). In this case, the hypothesis is a contradiction, and whether or not the algorithm is able to infer it as such reveals the extent to which the algorithm has understood the sentence.\n\nFor researchers to perform these tests, however, they need a corpus of sentence and hypothesis pairs. Such is the purpose of Bowman\u2019s multi-genre NLI corpus, an exciting project that received a Google Faculty Research Award earlier this year. Working with Adina Williams and Nikita Nangia from NYU, and Angeliki Lazaridou from Google DeepMind, Bowman and his team are creating a multi-genre NLI corpus that builds on his earlier work, the SNLI corpus. The SNLI corpus is a collection of 570,000 human written sentence pairs that were manually labelled. Since its creation, the SNLI corpus has become a vital benchmark for researchers in the field.\n\nBut a drawback to the SNLI corpus is that all of the sentences were extracted from a single genre \u2014 image captions. The multi-genre NLI corpus that Bowman and his researchers are now working on, however, addresses this problem by extracting written sentences from several different areas. In addition to the SNLI image captions, the multi-genre corpus collects sentences from fiction, government documents, news articles, telephone transcriptions, travel guides, the 9/11 report, face-to-face speech, letters, nonfiction books, and magazines.\n\nBy collecting both written and spoken text, this massive multi-genre corpus will help researchers test and identify problems with their current algorithms. It also takes us one step closer to learning how to foster transferable language skills in machine brains. Presently, the project is harnessing the power of crowd-sourcing to manually label the sentence extracts. The multi-genre NLI corpus will also be the basis for the RepEval 2017 shared task later this year \u2014 you can find out more about it here."
    },
    {
        "url": "https://medium.com/center-for-data-science/from-easy-to-hard-can-machines-learn-like-humans-d5023d175d05",
        "title": "From easy to hard: can machines learn like humans? \u2013 Center for Data Science \u2013",
        "text": "At last Thursday\u2019s NLP & Text as Data seminar, professor Hong Yu from the University of Massachusetts Medical School explained the effort that she, along with her fellow colleagues, have made to incorporate human intelligence and cognitive functions to improve deep learning through new models. For instance, the paper \u201cBuilding an Evaluation Scale Using Item Response Theory\u201d discusses Hong Yu, John P. Lalor and Hao Wu\u2019s Item Response Theory model (IRT) that compares Natural Language Processing systems to the performance of human population.\n\nOnce the new deep learning models are built, Yu goes on to evaluate whether or not they learn like the human brain. The human brain has to learn things gradually, beginning with low difficulty content and working its way up to more difficult content. \u201cNo human first understands an Albert Einstein equation before understanding basic physics,\u201d said Yu. A traditional machine learning model can solve difficult problems before it can solve easy ones \u2014 though that is not a sign of machine intelligence, but rather of memorization, or random chance. Yu and her colleagues found that deep-learning models learn like humans, taking less time on easy problems than on difficult ones, and are therefore considered \u201cintelligent\u201d systems.\n\nIRT uses difficulty as a gauge to compare system performance. This method is widely used in psychometrics and psycholinguistics for generating tasks like the much-dreaded SAT, GRE, or other aptitude tests beginning in childhood and onward, and is thoroughly tested in the population. This same methodology is applied to test performance for machine learning models. Using a thousand humans to generate intelligent questions sets to separate NLP performance, the computer system performed with an impressive 97% accuracy, but in fact was in the low 44th percentile when IRT was used as the evaluation metric, meaning there is a 56% chance that humans would outperform the computer.\n\nMemory-augmented neural networks were also discussed. Yu\u2019s memory-augmented network, unlike other deep learning models, accepts noise when up to 35% of the sentence pairs in the training set are randomly replaced with incorrect labels. Intelligent deep learning models are classified as such if they learn problems from easy to hard in difficulty and can tolerate noise, to a degree.\n\nThese intelligent deep learning models would be applied to adverse drug events, or adverse drug reactions, which are a leading cause of death in the U.S. Using Electronic Health Records, Yu\u2019s goal moving forward is to develop a sophisticated competition system that improves drug safety of patients."
    },
    {
        "url": "https://medium.com/center-for-data-science/improving-breast-cancer-screenings-trough-machine-learning-c52919784360",
        "title": "Improving breast cancer screenings through machine learning",
        "text": "Although deep convolutional neural networks can perform image object recognition in natural images, this technology still remains inapplicable for the medical industry. Medical professionals require high resolution images to make fine details visible, and the ability to view the human body from multiple angles to draw up a complete diagnosis for their patients.\n\nEnter the multi-view deep convolutional neural network (MV-DCN), created by the Center\u2019s Professors Kyunghyun Cho and Krzysztof J. Geras, and a research team from NYU\u2019s School of Medicine (Stacey Wolfson, S. Gene Kim, and Linda Moy). They published their findings this past March in \u201cHigh-Resolution Breast Cancer Screening with Multi-View Deep Convolutional Neural Networks.\u201d\n\nBreast cancer screening images served as the focus, the team explained, since breast cancer is the second leading cause of death among women in the United States. Moreover, mammograms, which are the main imaging test to detect breast cancer, lack precision. Between 10\u201315% of women who are screened are called back for further screenings or biopsies, most of which result in false positives, creating anxiety and unnecessary costs.\n\nMV-DCN, however, promises to improve the breast cancer screening process through its innovative architecture, which can handle the four standard views, or angles, without sacrificing a high resolution. As opposed to the commonly used DCN architectures for natural images, which work with images of 224 x 224 pixels, the MV-DCN is also capable of using a resolution of 2600 x 2000 pixels.\n\nAdditionally, MV-DCN produces predictions for medical professionals by assigning different probabilities to three possible outcomes (incomplete, normal, and benign), and also indicates which part of the breast needs to be examined further, if at all. When training MV-DCN, however, the team also faced some stumbling blocks. Training neural networks typically depends on millions of annotated images. Although the researchers collected the largest data set of this kind in the literature, medical images are much harder to acquire in large quantities, making their data set of 103,000 mammogram scans (collected from the NYU School of Medicine) small in comparison to data sets of natural images.\n\nDeep neural networks also require a lot of computation to get from the input, which in this case is the four standard views in a breast cancer screening exam, to the output, the probability distribution over possible decisions. With larger images like the ones being used in this network, the computational issue is amplified. The speed of training the networks is limited by their hardware\u2019s memory as the images being used are so large. In conclusion, the researchers found that performance increases with a larger training set and the original resolution is necessary to achieving the best performance. Currently, they are hoping to improve MV-DCN\u2019s capabilities by collecting an even larger data set."
    },
    {
        "url": "https://medium.com/center-for-data-science/can-someones-vocal-pitch-tell-you-where-they-re-from-b3d4dcec9e8b",
        "title": "Can someone\u2019s vocal pitch tell you where they\u2019re from?",
        "text": "Before written language was invented, different cultures used music to communicate with each other. Investigating how these cultures exchanged musical styles is a major research area in the field of comparative musicology, and its scholars are becoming increasingly reliant on data science and machine learning techniques to help answer their research questions.\n\nFor example, in \u201cTowards The Characterization of Singing Styles in World Music\u201d, the Center\u2019s affiliated faculty member and Professor of Music Technology at NYU Steinhardt, Juan Pablo Bello, Dr. Simon Dixon from Queen Mary University (UK), and their doctoral students Rachel Bittner and Maria Panteli, harnessed the power of machine learning to investigate the similarities and differences between singing styles in folk and traditional music around the world.\n\nFor this investigation, Bello and his team decided to focus on single musical element: vocal pitch, which comprises of smaller markers like vibrato (variation in pitch), melisma (singing one syllable within a range of notes), and slow or fast syllabic singing. They began by training a Random Forest Classifier (a popular machine learning algorithm) to identify pitch contours of the singing voice in different audio recordings, and separate these from non-vocal contours. While they used 62 tracks from MedleyDB containing leading vocals as a training set, the team also worked with a larger dataset of 2808 audio clips from the Smithsonian Folkways Recordings collection, which contains music from 50 different countries and 28 different languages.\n\nThen, they created a dictionary of singing style descriptors. After creating the dictionary, they could then classify the audio clips, and use k-means unsupervised clustering to identify which clips were the most similar in terms of vocal pitch. Their study found that the majority of clusters represent recordings from neighboring countries, or similar cultures or languages. For example, cluster seven contained music from predominantly European cultures, while clusters two and five contained music from African and Caribbean cultures. In the future, the team hopes, with some help from musicology experts, to evaluate the singing style clusters in more detail."
    },
    {
        "url": "https://medium.com/center-for-data-science/constructing-machine-brains-using-neuroanatomy-2f156cb446b5",
        "title": "Constructing machine brains using neuroanatomy \u2013 Center for Data Science \u2013",
        "text": "What can artificial intelligence learn from biological brains? At last Wednesday\u2019s Lunch Seminar series at the Center, Professor Partha Mitra from the Cold Spring Harbor Laboratory explained how he has been mapping biological brain connectivity in his Mouse Brain Architecture Project to discover how we can transfer biological brain architectures to machine brains. AI has made major strides in the last decade but, as Mitra explained, they still have some drawbacks when compared to biological brains. Not only do they use more power (100,000 W compared to 10 W!), but machine brains also require a much larger data set to train on than biological brains, and have non-biological fragilities.\n\nThe largest challenge, however, is that machine brains are still largely \u2018niche adapted\u2019, meaning that they are unable to transfer their skills successfully to tasks outside of their main purpose. While some, like our own Prof. Kyunghyun Cho, have turned to exploring new encoder-decoder approaches in neural networks to overcome this problem, Mitra suggested that analyzing machine brains within the context of biology and neuroanatomy also has a lot to offer.\n\nDrawing on the classic \u2018nature versus nurture\u2019 debate that faces human brains, Mirta pointed out that biological learning takes place on two different scales. First, it takes place at the level of the mesocircuit, a series-specific brain circuit responsible for consciousness (nature). Second, it takes place at the level of adapting to individual environments or experiences over time (nurture).\n\nAn ideal machine brain would comprise of similar components \u2014 on one hand, it would have a specifically engineered architecture but, on the other, its neural networks would also mimic biological brain plasticity that would enable it to adapt, change, and learn in real time. A primary goal behind Mitra\u2019s investigation of mouse brains and their nervous system, then, is to discover how this plasticity works in biological brains, and see if it can be reproduced in machines. Alongside this project, Mitra explained that he is also working on linking the field of statistical physics to machine learning and data science."
    },
    {
        "url": "https://medium.com/center-for-data-science/public-opinion-tracking-the-tones-of-media-coverage-9931099064d2",
        "title": "Public opinion: tracking the tones of media coverage",
        "text": "We already know that the tone of media coverage influences people\u2019s attitudes and opinions. But is that influence conditional? Amber Boydstun, an associate professor of political science at the University of California, Davis, addressed this question at last Thursday\u2019s NLP & Text as Data seminar titled \u201cThe Conditional Effects of Media Tone on Public Opinion: The Case of Immigration.\u201d\n\nThe seminar, based on her Policy Frames Project, which is in collaboration with Dallas Card (Carnegie Mellon University) and Noah Smith (University of Washington), is centered around two hypotheses. First, the effects of media coverage on public attitudes towards immigration is stronger the more coverage there is. And, second, the effects of media coverage on public attitudes towards immigration is weaker the more varied that coverage is.\n\nTo test the two hypotheses, the team initially used keyword searches to identify all relevant articles in LexisNexis for a given issue. For instance, immigration has multiple frames, or dimensions, such as the political or constitutional frames. Within each frame are associated keywords like \u201cfederal\u201d and \u201ccongress\u201d or \u201cillegal\u201d and \u201ccitizenship.\u201d For this particular policy issue, 38,283 archived articles in the database from 1980 to 2012 were collected from 12 major US news organizations such as the New York Times, The Wall Street Journal and The Washington Post.\n\nThe following step was manual coding, or trained-human coding, to identify tone and frames of the articles. Approximately 10% of the 38,283 articles were manually coded, meaning over 4,000 articles were individually read and categorized by human coders. The articles are identified as either implicit, like a hard news piece exploring immigration effects on low-wage job prospects or explicit, like an op-ed that points the finger at immigrants for taking Americans\u2019 jobs, and pro-immigration or anti-immigration, followed by identifying the article\u2019s frame. Finally, the team used NLP techniques to mimic the manual coding for the remaining articles.\n\nTheir findings show that media tone has a significant effect on people\u2019s attitudes, at least in the case of immigration. That effect is stronger when there\u2019s more overall attention but weaker when information is unconcentrated. The group suspects that their hypotheses also apply to other policy issues, and plan to test their collected data on other topics like same-sex marriage, gun control, tobacco/smoking, abortion, and climate change.\n\nMore generally, identifying the \u201cempirical regularities\u201d of how media cues operate is academically and politically beneficial. Further, the Policy Frames Codebook, developed for the project, provides a structure for coding the many frames a policy issue takes, and acts a guiding tool for other researchers seeking to track an issue of their interest and its multiple frames over time."
    },
    {
        "url": "https://medium.com/center-for-data-science/improving-stress-tests-on-financial-portfolios-92cecdc69743",
        "title": "Improving stress tests on financial portfolios \u2013 Center for Data Science \u2013",
        "text": "Although financial agencies and financial instruments vary, they are underpinned by the same risk management methodology: estimate the worst-case hypotheticals to hedge against financial upheavals. Value at Risk (VaR), one quantitative risk management strategy that emerged as a solid method following the 1987 stock market crash, was heavily trusted prior to the 2008 financial crisis when the holes in it became apparent. With the enactment of the 2010 Dodd-Frank Act, stress testing, used previously in the 90\u2019s as a self-assessment, emerged as a regulatory requirement.\n\nA computer-generated simulation technique that tests firms and portfolios reactions to financial scenarios, stress testing, unlike VaR, explores highly unlikely financial scenarios. By testing financial portfolios in even the most implausible of events, institutions are covering all their bases. Current stress testing approaches can be based on historical events, hypothetical events that experts deem damaging and plausible, or portfolios \u2014 but all of these approaches have weaknesses. While it is necessary to learn from historical events, the past is not always applicable in the present. Likewise, expert judgment of hypothetical events is not enough to determine the relationship between financial risk factors and the portfolios. The portfolio-based method utilizes the most widely used stress test, the Monte Carlo Simulation, but still lacks the capacity to deal with many risk factors.\n\nBud Mishra, an affiliated professor at CDS, Gelin Gao (NYU Courant), and Daniele Ramazzotti (Stanford) have therefore suggested a new method for stress testing financial portfolios in their newly released paper, Efficient Simulation of Financial Stress Testing Scenarios with Suppes-Bayes Causal Networks (SBCNs). Their method combines SBCNs probabilistic causation with machine learning classification. SBCNs probabilistic causation confirms causal relationships if the following two conditions are satisfied:\n\nMachine learning classification is a type of artificial intelligence categorization of data that learns from the inputs and does not require explicit programming to do so. This is in line with their method, as it identifies causal relationships between financial factors and portfolios. In turn, this allows for more accurate simulation of stress testing scenarios.\n\nSBCNs alone, however, would not simulate the ideal stress scenarios relevant to stress testing because of their rarity and scarcity in the data. With machine learning classification, the researchers categorize scenarios as either \u2018profitable\u2019 or \u2018lossy,\u2019 cuing the program to focus on the lossy, risky, and uncommon scenarios for computation. Once the framework for stress testing learns SBCNs from the data and classifies entries as either \u2018profitable\u2019 or \u2018lossy,\u2019 stress tests are simulated. Experts can then rule out implausible or flawed scenarios. They may even have a specific stress scenario in mind, which can also be executed.\n\nFollowing the release of their paper, the group is now looking forward to presenting their work at the international Conference on Computational Science and being published in Elsevier Procedia Computer Science. Additionally, with the introduction of their method complete, their focus has shifted to its application through an interface. This is a tool that will be powerful on Wall Street, at FinTech companies, and for artificial intelligence more broadly."
    },
    {
        "url": "https://medium.com/center-for-data-science/creating-synthetic-languages-73a84b0fcfb4",
        "title": "Creating synthetic languages \u2013 Center for Data Science \u2013",
        "text": "Why is \u201cunbreakable\u201d such a special word for linguists? Unbeknownst to the general public, the word is actually composed of three smaller units that linguists call morphemes. Referring to the smallest meaningful grammatical unit of a language, the three morphemes in \u2018unbreakable\u2019 are \u2018un\u2019 , \u2018break\u2019, and \u2018able\u2019.\n\nStudying the underlying morpheme structures within a word has been one focus of Professor Jason Eisner\u2019s research from Johns Hopkins University. When speaking at the Center\u2019s NLP and Text As Data seminar, Eisner explained that they have so far used computational techniques to automatically analyze the pronunciations of a set of words to identify the underlying morphemes that they share. Given that some morphemes are pronounced differently depending on their context, their algorithm also captures the specific sound patterns \u2014 phonemes \u2014 of words.\n\nThis research has major implications for those working in linguistics, cognitive research, and engineering. For linguists, we can now ask how the same sounds are used to construct different \u2014 or similar \u2014 words across languages, enabling us to discover whether our languages are diverse or closely linked. Moreover, tracking which morphemes are typically placed together can help us understand how words are formed. For those interested in cognitive research, word formation can help us better understand how toddlers learn phonological processes. And, for those interested in engineering, we can now begin to combine languages together.\n\nCreating synthetic languages is a large part of what Eisner has been working on. Eisner and his colleague Dingquan Wang have recently released Galactic Dependencies Treebanks, a set of over 50,000 synthetic languages. To put that into perspective, that\u2019s approximately 7 times more languages than all the ones spoken on Earth. Part of creating GDT involved mixing and matching language rules that have already been identified in each existing human language. They take an English sentence, for example, and rearrange its words into another language order, like that of Hindi or Japanese.\n\nOf course, these synthetic languages do not intend to usurp or replace current languages. The point of GDT, Eisner explained, is to provide new data for NLP methods that are aiming to adapt to unfamiliar languages. Training on unfamiliar languages can help these models learn how parsing occurs in languages more generally. Capturing this transferable knowledge means that NLP methods in the future may not have to be taught each language one by one. They will know what clues to look for when analyzing a new language, helping them to unfold its structure. Taking his exciting work on GDT forward, Eisner will be focusing on even more unsupervised and concentrated discovery of syntactic structure."
    },
    {
        "url": "https://medium.com/center-for-data-science/beyond-machine-vision-how-can-ai-generate-realistic-images-8b4155901b12",
        "title": "Beyond machine vision: how can AI generate realistic images?",
        "text": "There are currently two main approaches to generating images using artificial intelligence, Generative Adversarial Networks (GAN) and Variational Autoencoding (VAE).\n\nGAN pits two neural networks against one another in order to improve their generation of photorealistic images. In GAN, there is a generator which produces fake images, and a discriminator, which differentiates the fake images from the real ones. They train together: the discriminator processes the variations between the real and the fake images and informs the generator on how to produce more accurate fake images. Ideally, as the GAN process continues, the generator will eventually produce better and better fake images, so that the discriminator can no longer tell the difference. In terms of efficiency, GANs also benefit researchers because they require only hundreds of training images whereas image-recognition networks require tens of thousands, according to its creator Ian Goodfellow, a computer scientist at Open-AI in San Francisco.\n\nVAE, however, is another approach to image generation whose strong suit lies in the diversity of images it creates. Albeit with lesser quality, some researchers have combined VAE and GAN into a hybrid with the goal of creating an improved generative model.\n\nThese generative models have come a long way. As the Center\u2019s Professor Kyle Cranmer recently explained in an article for Nature, a gap exists between the theory and practical engineering of neural networks. Currently, neural networks produce sound results, but how they do so is still a \u201cblack box.\u201d Neural networks initially involved an input and a prediction. But now, generative networks produce images of dogs, cats, or galaxies that look real, suggesting their understanding of generating real world representations has improved, thereby easing some of the apprehension scientists have had about their \u2018black box\u2019 nature.\n\nThe photorealistic images that generative models create are immensely beneficial for scientists and researchers who need to perform image reconstruction, or to fill in deformities in images through simulation. The applications of this technology, as Cranmer remarks, are \u201cpretty endless.\u201d More broadly, generative networks can also be used to train image-recognition software because the way a neural network eventually recognizes and classifies data alone is through its training process with an original data set that cues its virtual neurons."
    },
    {
        "url": "https://medium.com/center-for-data-science/say-hello-literally-to-nematus-a-new-toolkit-for-neural-machine-translation-325b1d6f810c",
        "title": "Say hello (literally) to Nematus, a new toolkit for neural machine translation",
        "text": "Just last week, Center for Data Science Professor Kyunghyun Cho, along with an international group of colleagues*, released Nematus, an exciting toolkit for Neural Machine Translation (NMT). Funded by the European Union\u2019s Horizon 2020 research and innovation programme, Nematus performs neural machine translation using an encoder-decoder model, an approach that replaces traditional \u2018phrase based\u2019 translation and has become increasingly popular in the field.\n\nThe toolkit \u201chas recently established itself as a new state-of-the-art in machine translation,\u201d explains the team, and is already being used by the World Intellectual Property Organization (WIPO), a special branch of the United Nations, to train WIPO Translate, their machine translation system. WIPO can now perform highly accurate Chinese to English translations on their patent documents thanks to the high accuracy rate of Nematus, and plans to extend their reach into French language patent documents soon.\n\nImplemented in Python, Nematus is open source and available for anyone to use as long as it has been cited. Check it out here.\n\n*Alexandra Birch (University of Edinburgh), Marcin Junczys-Dowmunt (University of Edinburgh) , Orhan Firat (Middle East Technical University), Barry Haddow (University of Edinburgh), Julian Hitschler (Heidelberg University), Samuel L\u00e4ubli (University of Zurich), Jozef Mokry (University of Edinburgh), Maria Nadejde (University of Edinburgh), Rico Sennrich (University of Edinburgh), and Antonio Valerio Miceli Barone (University of Edinburgh)."
    },
    {
        "url": "https://medium.com/center-for-data-science/self-driving-cars-dont-need-driving-lessons-they-need-data-27daebe7e400",
        "title": "Self-driving cars don\u2019t need driving lessons \u2014 they need data",
        "text": "As the data surrounding Automated Vehicles grows, safety improves, but privacy concerns also arise. The majority of human-operated vehicle accidents occur due to human errors like drunk driving or road rage. Presumably, AVs will lead to a safer driving environment by reducing, and one day, removing, the human factor from the road.\n\nAccording to the Center\u2019s Director of Graduate Studies, Prof. Vasant Dhar, in an article for Computer, big data is already being collected from cars with onboard systems. Only recently, MobilEye, an on-board system provider to IBM and Mercedes was acquired by Intel. These on-board systems are well-rounded, constantly recording the vehicle\u2019s speed, the weather and light, measuring the impact in an accident, and even driver awareness and attention.\n\nThe human-driver aspect would be recorded by the onboard system by way of the sensors. Say a vehicle gets too close to another or approaches the edge of the road \u2014 those are signals that a driver is distracted. AVs have yet to learn how to behave like humans, though, before making their debut on the streets seamless. People can read other people\u2019s driving behaviors and react accordingly, and AVs will need to understand that intuition as well.\n\nThe corresponding data collected from on-board systems continually improves at ascertaining fault in accidents. This will, in turn, modify insurance policies that currently leave liability decisions debatable. Eventually, if the on-board systems regularly determine fault accurately, it could automate the liability process. By establishing what or who\u2019s to blame in an accident, the on-board systems also shed light on how future accidents can be avoided. \u201cThe availability of big data obtained from on-board vehicular systems and roadway sensors changes the calculus of liability,\u201d as Dhar sums it up in his Computer article.\n\nBut privacy is still a major concern when data of this magnitude is involved. A driver\u2019s location, which is always recorded, is one example of a potential invasion of privacy. In general, if an insurer has access to the continually recorded data on vehicles, the driver is no longer in control of the sale of their information. Drivers going over the speed limit or rolling a stop sign would be another, less invasive, consequence of the data collection from onboard systems. If the data is legally regulated and privacy concerns are managed, there is much to gain from the insurance policies, accident determination and accident prevention that will improve with the data from onboard systems."
    },
    {
        "url": "https://medium.com/center-for-data-science/united-states-of-data-shaping-governance-through-data-science-9ec4899dc3d7",
        "title": "United States of Data: shaping governance through data science",
        "text": "While we know that data science is rapidly becoming a vital part of the strategy of several academic fields, what role does it play in running our government? Last Wednesday, the Center\u2019s Lunch Research Seminar heard from Dr. Jaime Anne Earnest, a program analyst and data scientist from the US Department of Defense.\n\nUntil former President Barack Obama launched his \u201cDigital Government\u201d directive in 2012, data science played a minor role in constructing governmental policies. It\u2019s not that the data wasn\u2019t there, Dr. Earnest explained, but that it was relatively inaccessible for both governmental staff and the public. Obama\u2019s directive, however, flung open the doors to the government\u2019s big data through the crucial launch of data.gov, the government\u2019s open repository filled with data on crucial topics like climate change.\n\nThe immediate problem now, Dr. Earnest said, is helping people both within the government and outside of it to use the data. Unfortunately, the data cannot be made sense of by those who have understandably been trained in writing up policies, not to perform statistical analysis or comprehend the complexities of casual inference.\n\nThere is an urgent need for more data scientists to work directly within government, particularly because strict policies prevent collaborations between the government and private institutions. But, until more data science experts are recruited, part of Dr. Earnest\u2019s job as one of the few professionally trained data scientists at the DoD is building data science models and visualization frameworks that policy makers can easily navigate to encourage them to create more evidence-based policies, and to encourage the public to use these models, too.\n\nAfter all, it is not only policy-makers that suffer from a superficial understanding of data science, but the general public at large. We only have to remember the 2016 presidential election to understand what consequences can ensue when non-data science experts use data to make predictions without a strong understanding of the theoretical models that underpin their analytical process.\n\nBut Dr. Earnest is optimistic. After all, she stated, trained data scientists have a golden opportunity to make a tangible difference today, since they are the ones with expertise and experience to put our nation back on track where data is concerned. Professors and students of data science alike now face some interesting questions: should they pursue a cool and lucrative project at a start up? Or should they go on to pursue a doctorate, contribute to a specific area in data science research, and teach the students of tomorrow? Or should they step into the White House, and directly apply their skills to legal policies and matters of national importance?"
    },
    {
        "url": "https://medium.com/center-for-data-science/building-better-neural-networks-6346eeeebbb4",
        "title": "Building better neural networks \u2013 Center for Data Science \u2013",
        "text": "A group of professors and researchers at the Technical University of Berlin, the University of Vienna, and ETH Zurich have recently been working on understanding deep neural networks (computer systems that are modelled after the human brain) in \u201ca mathematically sound way\u201d, as Dr. Phillip Petersen refers to it. Although the official paper for this research, \u201cOptimal Approximation with Sparse Deep Neural Networks,\u201d will not be published until next week, Professor Gitta Kutyniok graciously presented a preview of their work for the Center\u2019s Math & Data Seminar group this past Thursday.\n\nNeural networks, or artificial brains, represent functions in mathematics. For these researchers, the main goal is to uncover how well a deep neural network with sparse connectivity can approximate a function. Dr. Petersen likens the network to a tree \u2014 deep neural networks are composed of multiple layers and are connected by edges. Those layers are made of nodes, or neurons where computation occurs, and are sparsely connected if they have few non-zero weights or edges. Thus, the connections between different neurons and the number of connections that there are is the focus at hand.\n\nSome real world applications of neural networks include Siri, Apple Inc.\u2019s computer program, ImageNet, an image database and AlphaGo, an artificial intelligence program developed to play the board game Go. Usually, deep neural networks are trained on particular data like sound, images, text or video. The research in this paper, however, is concerned with the fundamental math. Rather than picking a single network and training it on a function, they established a theorem that is true for all networks. There is no data in the background, only a fundamental theorem on networks.\n\nThus far, their research has presented two new results to the field. First, they have provided new optimality criteria not tied to a specific function class or a universal lower bound \u2014 basically, a connection has been established between the size of the network and the approximation quality. Second, construction of a neural network that\u2019s optimal for a specific function class is given for one specific construction.\n\nDon\u2019t forget to catch the Math & Data seminars again after Spring Break! The full schedule can be found here."
    },
    {
        "url": "https://medium.com/center-for-data-science/when-should-we-let-robots-make-decisions-36237ba8a33b",
        "title": "When should we let robots make decisions? \u2013 Center for Data Science \u2013",
        "text": "Are robots taking over? Vasant Dhar, a data scientist and professor at the Center for Data Science, focuses his research on the balance between automation and humans, and has recently published his work in Harvard Business Review. Given the growing number of industries where computers are able to make decisions, a need has risen for a systematic way to gauge if a machine should be used or not. In response to this need, Dhar has created a \u201crisk-oriented framework\u201d to help guide decisions about what tasks should be performed by humans and what tasks are better performed by robots. This stems from his extensive background applying predictive systems to fields like finance, creating a machine that essentially learns from data and thereby makes investment decisions automatically.\n\n\u201cThe issue of trust is central here. Are you willing to turn over control in terms of strategies and decisions to a machine?\u201d said Dhar when explaining his framework, which categorizes human vs. robot problems by predictability and cost per error. Both classifications express risk. The more predictable a problem, the less risk associated with it and vice versa. Likewise, the higher the cost per error, the more risk associated with the problem.\n\nThe spectrum of predictability ranges from \u201crandom\u201d to \u201csure thing,\u201d with long-term trading, for instance, being completely \u201crandom\u201d and driver-less cars being a \u201csure thing\u201d considering the technology and physics involved. This facet of the framework highlights where automation is worth pursuing and where it needs more work, if any. The second facet of the framework is calculating the cost per error of the same problems.\n\nOnce the two elements of each problem have been analyzed, Dhar plots them on a Decision Automation Map (DA-MAP), plotting predictability against cost per error for each problem like driverless cars and long-term trading. Both axes quantify risk; the x-axis exhibits how frequently things go wrong and the y-axis exhibits how bad is it when they do. The plotted points are not set in stone, however. Technological advancements, societal changes, increased data, and tightening or loosening of regulations can all cause shifts on the map.\n\nViewing automation decisions with these two classifications weighs the costs and benefits. For instance, deciding to fully embrace driver-less cars is tricky because of the serious repercussions of an accident. For early education support, though, the consequences are minimal as it\u2019s an extra tutoring tool for children that simply gauges their academic level and questions them accordingly. Simply put, \u201cA higher cost per error requires a higher level of predictability for automation.\u201d\n\nThe framework, which is less than a year old, and the consequent DA-MAP are useful for decision makers like managers, regulators, investors and others in need of data-driven guidance about automation decisions. Businesses, primarily consulting companies, have so far shown interest in it as they seek data science strategies for their clients. In the long-term, it is a way to for executives to prioritize initiatives from a risk based perspective."
    },
    {
        "url": "https://medium.com/center-for-data-science/saving-climate-change-data-d706b6828a9c",
        "title": "Saving climate change data \u2013 Center for Data Science \u2013",
        "text": "Last month, NYU hosted a #DataRescue event as part of the Environmental Data and Governance Initiative\u2019s (EDGI) effort to preserve environmental evidence. EDGI, which formed directly following the 2016 presidential election, began with a team of 40 researchers and social scientists. In only a few months, the network has grown to include computer programmers and librarians throughout North America.\n\nThe #DataRescue events bring together qualified volunteers to \u2018guerilla archive\u2019 environmental data from relevant agencies, offices, and programs. While the information obtained is secured in EDGI\u2019s Amazon cloud, the main purpose of these \u2018rescue\u2019 events is to make the data publicly accessible. EDGI also provides metadata and documentation to contextualize and describe the resources it is preserving.\n\nOne team at an event, for instance, is assigned to focus on federal government websites and rescue the pages that can be publicly placed on the End of Term Archive, an archive created in the 2008 presidential election to preserve federal government websites, since many of them can be altered or removed during presidential transitions. \u201cEnvironmental policy is driven by evidence,\u201d explained Jerome Whitington, a professor of anthropology at NYU and a member of EDGI\u2019s team and one of the organizers of the NYU #DataRescue event. Without data, environmental experts cannot gather support to pass policies that will execute the positive changes that our nation\u2013and planet\u2013so sorely needs.\n\nThe EDGI\u2019s efforts have seen positive feedback. Congressional offices have reached out to the group, drafts in legislation are at work to protect environmental data, and district attorneys\u2019 have shown support."
    },
    {
        "url": "https://medium.com/center-for-data-science/using-population-models-for-astrophysics-f12e510bfd2a",
        "title": "Using population models for\u2026 astrophysics? \u2013 Center for Data Science \u2013",
        "text": "Galaxies are gorgeous structures containing billions of stars. But they come at a formidable price: the super massive black hole. Invisible, mighty, and mysterious, black holes are space regions where nothing \u2014 not even planets, stars, or light itself \u2014 can escape from inside it once its powerful gravitational pull has swallowed them up.\n\nHow can astrophysicists track these alarming \u2014 and invisible \u2014 regions? This question is at the heart of Yannis Liodakis\u2019 research at the University of Crete. As part of his doctoral work, Liodakis has come to the Center for Data Science as a visiting scholar so that he can learn more about how data science techniques can solve questions about black holes.\n\nAt last Wednesday\u2019s Research Lunch Seminar, Liodakis explained one strategy that astrophysicists have been using to locate black holes. Around every black hole is an accretion disc, a ring of space matter that has been caught by the black hole\u2019s gravitational pull, but not yet consumed by it. Accretion discs contain smaller masses called quasars, which emit strong electromagnetic fields and, most importantly, light. These light jets are called blazars, and their powerful beam can be seen from Earth.\n\nBlazars are important, Liodakis stated, because calculating how bright they are then enables us to calculate its velocity. After discovering a blazar\u2019s velocity, we can then not only calculate its distance and map its location, but also predict where the invisible black hole is, because blazars exist on the accretion discs that surround the formidable mass.\n\nBut a major problem is that the light jets blazars emit to Earth oscillate due to the quasar\u2019s electromagnetic field. In other words, the blazar\u2019s light bends at varying degrees as it travels towards Earth, meaning that the light we would see through a telescope is inaccurate. Astrophysics must therefore calculate how much the light rotates as it travels to Earth to produce an accurate measurement. Additionally, they have to remember that there are competing time scales involved: although the universe is always expanding, the blazar\u2019s light jet is also compressing time while it travels to Earth.\n\nTo account for all the complex factors involved when making these calculations, Liodakis and his team have recently been experimenting with the population models that are often used by data scientists. Population models are powerful because they can show us how highly complex factors of a given topic interact with each other. For example, ecological population models are usually concerned with calculating how population size, age distribution, and physical environment affect a given group of organisms.\n\nApplying population model techniques for astrophysics has allowed Liodakis and his team to produce sharper calculations about how bright blazars are because they can account for all of the elements of the problem \u2014 like oscillation and competing time scales \u2014 in a single model. Liodakis\u2019 innovative work demonstrates how interdisciplinary data science strategies can become. From astrophysics to ecology, who knows where data science will be needed next?"
    },
    {
        "url": "https://medium.com/center-for-data-science/how-can-data-science-improve-national-security-1384b36202d0",
        "title": "How can data science improve national security? \u2013 Center for Data Science \u2013",
        "text": "After our Academy Awards last week, we caught up with some of our student winners to find out more about their work. Scooping up the prize for the project with the greatest social impact was Yiqiu Shen, Zemin Yu, and Xinsheng Zhang\u2019s research on how data science can be used to combat terrorism.\n\nA major challenge facing us today is how to quickly identify the groups responsible behind terrorist attacks so that the relevant individuals can be apprehended. With roughly 3290 unique terrorist groups around the globe, each with their own set of characteristics and motivations, law enforcement agencies require additional tools to help keep our country safe.\n\nTo that end, Shen, Yu, and Zhang have been building a new model that predicts which terrorist group committed a particular act based on specific features like the location of the attack, the nationality of the victims, and the day that the attack occurred. The model\u2019s predictions are based on the wealth of textual data about previous terrorist attacks collected by the Global Terrorist Database (GTD). The database contains meticulous details about each terrorist attack that has occurred since 1970, such as its location or attack type. After extracting the relevant details of each attack from the GTD, Shen, Yu, and Zhang went on to convert the textual data into numerical values using Python. After \u2018learning\u2019 this information, their model can then produce its predictions regarding future events.\n\nThe team speculated that the model could take the form of a web application in the future, where a human expert could quickly input data about an attack immediately after its occurrence through a front-end web user interface. The data would be sent to the back-end where the model is located, and then the model would return its predication of which terrorist group is responsible for the event based on the data received.\n\nYet, as with any new invention, there are also some drawbacks to consider. A key problem, explained the team, is that terrorists could take advantage of the program and change their behavioral patterns to elide law enforcement agencies. Moreover, the model\u2019s inaccuracies could lead to the persecution of innocent individuals, or exacerbate damaging stereotypes based on nationality or race. These ethical considerations remind us that the effects of our technological inventions are always doubled: a significant tool today could also turn against us tomorrow depending on who is in power."
    },
    {
        "url": "https://medium.com/center-for-data-science/sorting-through-the-tags-how-does-tumblrs-graph-based-topic-modeling-work-1d396fb48f54",
        "title": "Sorting through the tags: how does Tumblr\u2019s graph-based topic modeling work?",
        "text": "What makes Tumblr stand apart from other social media platforms lies in the unique way its users communicate with each other. Each user has their own highly customizable blog where they can post and share content \u2014 like articles, images, GIFs, or videos \u2014 or re-post content published by another user. Sharing and re-posting content is not only key to how social connections are formed, but also how trending and popular topics are established, since the user must tag each post that they publish.\n\nBut with over 335 million microblogs, how can Tumblr keep track of which topics are most popular? While handling Tumblr\u2019s massive data set is already a challenge, another part of the problem is interpretability. For example, one user may tag an image of Pikachu as \u2018Pokemon\u2019, while another may tag it as \u2018Pokemon Go!\u2019\n\nCrafting computational solutions to streamline related tags together into a single topic is a major part of Nicola Barbieri\u2019s work at Tumblr as a Senior Data Scientist. At last Wednesday\u2019s Research Lunch Seminar group, Barbieri explained how his team is currently using a graph-based topic modeling process to identify topics on Tumblr\u2019s platform. Although a popular technique for performing topic modeling today is Latent Dirichlet Allocation (LDA), its methodology hits some stumbling blocks when applied to Tumblr. One problem is that LDA typically relies on the data scientist establishing in advance how many topics there are in a data set.\n\nYet, as Barbieri stated, Tumblr is unable to know or predict how many topics there may be, as the platform is not only too large but also prone to particularly dynamic fluctuations as their audience comprises of users as young as thirteen.\n\nBut a graph-based, semi-supervised machine learning approach to topic modeling, Barbieri demonstrated, solves all of these problems. Assuming that tags with the highest number of followers and users are the most popular, this approach allocates each tag with a score that is based on the number of followers and subscribers based on the following formula:\n\nThis approach is scalable and straightforward, and elegantly solves the problem of not knowing how many topics there are in a given dataset. It also allows Tumblr to a more detailed picture of what content their users are sharing, for macro-topics and the micro-topics within those macro-topics can be identified."
    },
    {
        "url": "https://medium.com/center-for-data-science/honoring-the-years-best-projects-b91492921a67",
        "title": "Honoring the year\u2019s best projects \u2013 Center for Data Science \u2013",
        "text": "What do Hollywood celebrities and our faculty and students have in common? Awards! =P\n\nLeaping from one success to the next, this month saw the election of our Founding Director, Yann LeCun, to the National Academy of Engineering (NAE) for his work in computer vision and artificial intelligence. Sam Bowman and Kyunghyun Cho also recently received Google Faculty Research Awards for their work in NLP and neural networks.\n\nOur students were also recently recognized for their research at the very first CDS Academy Awards. Hosted at our Fifth Avenue facilities, the event was complete with gold trophies, beverages, food, and live piano music to celebrate particularly outstanding coursework projects that our students have completed during their degree.\n\nAll the projects were evaluated by the Center for Data Science\u2019s faculty in seven categories. Congratulations to our winners:\n\nA massive thank you to our Leadership Circle, Remi Moss, Kathryn Angeles, and Loraine Nascimento for organizing the event!"
    },
    {
        "url": "https://medium.com/center-for-data-science/can-nlp-reveal-power-imbalances-5d1a6d5ea9a1",
        "title": "Can NLP Reveal Power Imbalances? \u2013 Center for Data Science \u2013",
        "text": "Last Friday, the NLP & Text-As-Data Seminar heard from Vinodkumar Prabhakaran, a postdoctoral fellow at Stanford University who is specializing in computational sociolinguistics. One of his research projects focuses on the workplace \u2014 today, 96% of all office communication, Prabhakaran explained, occurs through mediums like email. But although email may be more convenient, it has also resulted in more people speaking online in ways that they would not during face-to-face communication. For example, one may feel comfortable speaking more sharply over email than they usually would in person, thanks to the detached, quasi-anonymity of a digital screen.\n\nA drawback to this \u2018online disinhibition\u2019 is increased instances of incivility between superiors and subordinates that could then result in reduced productivity. As the first step to tackling the problem is being able to identify such moments of incivility, Prabhakaran\u2019s project involves building an NLP tagger that can predict the direction of power from analyzing a single email between two people.\n\nWorking with an email corpus containing around 36,000 emails from a company called Enron, Prabhakaran and his team started by analyzing the structure of email threads and recording who initiated conversations, who ended conversations, and how long conversations lasted between employees and their superiors.\n\nThen, they performed dialog tagging on the content. Each email was categorized as conversational (\u2018How are you today?\u2019), informational (\u2018The meeting is at 3 o\u2019clock\u2019), an information request (\u2018What is the subject of the meeting?\u2019), or a commitment (\u2018I will submit the report by Friday\u2019).\n\nThe most revealing category, however, is Overt Displays of Power (ODP). A superior writing \u201cDo you think you could send the report by today?\u201d, as Prabhakaran points out, transmits a softer tone than one who writes \u201cI need the report by today.\u201d To Prabhakaran, the former opens up an opportunity for the employee to respond in multiple ways, while the latter appears to restrict the employee\u2019s response options \u2014 a trait that he believes is characteristic of superiors who display ODP. After training the tagger to recognize these categories, Prabhakaran\u2019s team discovered that it had a 61.8% accuracy rate when identifying superiors from subordinates.\n\nMore work is still required to see whether the assumptions and conclusions Prabhakaran\u2019s team has made can be applied to companies outside of the one that they investigated. Additionally, it remains to be seen how accurately email corpuses reflect the actual company culture at hand, given that some people may send many more emails than others and skew the overall results. And, of course, there is always the tricky snag called context: what if one\u2019s superior really does need the report \u2018by today\u2019? These probing questions demand complex answers and deeper reflection, but they are precisely why NLP continues to be an increasingly dynamic field."
    },
    {
        "url": "https://medium.com/center-for-data-science/improving-online-search-with-the-domain-discovery-tool-ecbc8fa9751e",
        "title": "Improving online search with the Domain Discovery Tool",
        "text": "Clear ideas, unclear search results. The rapid expansion of the web has made online search difficult and frustrating, particularly with search engine giants like Google and Bing.\n\nThe primary issues are the unmanageable amount of search results, the unreliable methods of saving the results deemed worthwhile, and the nuances that come with searching just the right keywords. Luckily, the Center for Data Science\u2019s very own Juliana Freire, along with a group of research associates from NYU\u2019s Tandon School of Engineering, have set out to streamline web searching by creating the Domain Discovery Tool (DDT), which allows for interactive domain discovery.\n\nSimply put, the DDT continuously modifies a user\u2019s search by learning and accounting for the user\u2019s interaction with the search engine. As it interacts with a user over time, it becomes an \u201coracle\u201d for their search needs, as one of the research associates, Yamuna Krishnamurthy, put it. Best of all, the team designed the DDT as a ready-to-use platform for any user.\n\nAnother key feature that sets the DDT apart from other search engines is how it harnesses the power of visualization. For instance, with the DDT, page results are organized into total, relevant, irrelevant, neutral, and new pages. These are displayed on a dashboard which also contains a Terms window that updates keywords summarizing previous searches, and suggests ideas for new searches. When a user hovers over a term, the DDT displays corresponding result pages as well, allowing for a contextual understanding of the user\u2019s search.\n\nWith the DDT, the unreliability of bookmarking URL\u2019s that could change or disappear is also no longer an issue because queries and their consequent results are recorded and maintained by the tool. Moreover, DDT is a significant boon for researchers seeking to capture information about sensitive topics that elide typical search engine algorithms.\n\nIn fact, when DDT was compared with Google during an experimental application with a small group of users, the majority of the users found more relevant pages with the DDT than with Google. Could the DDT be on its way to shaking up the search engine industry?"
    },
    {
        "url": "https://medium.com/center-for-data-science/beyond-vectors-efficiency-in-natural-language-processing-c81084abfd52",
        "title": "Beyond vectors: efficiency in Natural Language Processing",
        "text": "When you text a friend saying \u2018I\u2019ll fall you later\u2019, how does your iPhone know to correct \u2018fall\u2019 to \u2018call\u2019? Auto-correct owes its prowess to a field that continues to gain paramount importance among computer scientists, and is an especially lively area of study at our very own Center for Data Science: Natural Language Processing (NLP).\n\nGenerally speaking, part of NLP research involves calculating \u2018the joint probability distribution of words\u2019 in a language. In other words: researchers working in English, for example, use algorithms to analyze large cachets of English documents and texts, and calculate which words most frequently appear beside each other in various contexts, or words that share semantic similarity (synonyms). After identifying dominant word patterns in the English language, researchers can then write programs that will predict what word may come next in a sentence or a paragraph (\u2018probability distribution\u2019).\n\nNLP research not only makes features like auto-correct possible, but also has extraordinary implications for academic research. For example, sophisticated NLP programs could eventually help literary scholars or historians fill in the missing words in aged, damaged, or illegible manuscripts.\n\nToday, there are a number of approaches to capturing and understanding language patterns in NLP. A popular method is word2vec, which transforms words into vectors. Words that often appear close to each other or share semantic similarity end up sharing a similar space on a graph like the one below, which depicts word vectors related to \u2018good\u2019 and \u2018bad\u2019 words.\n\nThe steeper the incline between each word pair (eg. \u2018good\u2019 and \u2018evil\u2019), the more they differ in semantic meaning. And, the word clusters represent associations: \u2018rich\u2019, \u2018important\u2019, \u2018healthy\u2019, and \u2018good\u2019 share a similar space on the graph because these words are most often used together, suggesting that wealth is (unsurprisingly) tied to social importance and physical well-being.\n\nBut a problem with the word2vec approach is that it requires reading a massive text corpus to produce its complex calculations. A more recent approach, \u2018Eigenwords\u2019, is faster and more efficient for NLP work as it uses spectral decomposition to calculate the joint probability of words within a scalable matrix or \u2018context window\u2019. For example, if a researcher defines the context window for a particular word like \u2018cat\u2019 as \u20183\u2019, the Eigenwords algorithm would identify the three words that most frequently appear before and after \u2018cat\u2019 in a corpus, thus capturing the underlying word patterns within a specific context in a shorter amount of time and with less computational work.\n\nBuilding on Eigenwords, MSDS student Raul Delgado Sanchez has been researching how diffusion maps can take NLP research a step further as a part of his Mathematics of Data Science course. Like the Eigenwords approach, a diffusion map analyzes data within a specific context window, and the distance between the data points describes the relationship between those points. Diffusion maps can be applied to NLP research, Sanchez explained, where words become the data points, and the distance between those points \u201crepresent the chances of \u2018walking\u2019 from one word to another\u201d \u2014 in other words, the probability that those words appear close to (or far from) each other.\n\nAfter building a similar matrix that Eigenwords uses, Sanchez conducted a series of short experiments to demonstrate how diffusion maps can calculate the joint probability distribution of words to similar standards achieved by other models, suggesting that diffusion maps may become a promising area for further NLP research."
    },
    {
        "url": "https://medium.com/center-for-data-science/when-text-analysis-turns-into-censorship-f055b68913c0",
        "title": "When text analysis turns into censorship \u2013 Center for Data Science \u2013",
        "text": "A major reason why data science is often touted as a force for social good is because it trades on accessibility. Public data sets and open-source software have allowed both experts and citizens alike to gather knowledge and conduct research at a faster pace. But what happens when data is used to oppress the public rather than solve problems?\n\nA recent study conducted by researchers at the University of Toronto\u2019s Citizen Lab discovered that machine learning may have been used to censor online speech in the People\u2019s Republic of China. When performing a case study on WeChat, China\u2019s most popular messaging app, researchers unearthed a complex censorship system that leaves private conversations mostly untouched, but is able to cleverly scrub messages deemed unacceptable from group chats of three or more people. Significantly, users of the app are not notified when their messages are removed.\n\nOf course, previous censorship systems have always detected and blocked trigger phrases such as \u201cJune 4\u201d (\u516d\u56db), referring to the notorious Tienanmen Massacre. But what makes WeChat\u2019s newest censorship engine stand out is that it can detect and block entire phrases, but allow individual words from censored phrases to pass. For example, while the phrase \u201cJune 4 memorial\u201d (\u516d\u56db\u7eaa\u5ff5\u9986) will be blocked, if key words are separated \u201cToday is June 4, I will go to the memorial\u201d (\u4eca\u5929\u662f\u516d\u56db, \u6211\u8981\u53bb\u7eaa\u5ff5\u9986), then the message is not censored.\n\nAlthough a step forward in a technical aspect, this improved algorithm demonstrates that the censorship model has evolved from the simple \u2018detect-and-block\u2019 strategy to identifying broader topics and tones of a conversation so that it knows only to block \u201cJune 4\u201d if it is written within a particular context. Such an advanced algorithm has significant implications in a society where censorship continues to be a top priority, and demonstrates how social media companies more broadly are increasingly able to shape the realities of users through what is communicated."
    },
    {
        "url": "https://medium.com/center-for-data-science/the-center-for-data-science-welcomes-new-research-seminars-573e0bf8948e",
        "title": "The Center for Data Science welcomes new research seminars",
        "text": "As we learned during our interview with Professors Afonso S. Bandeira, Joan Bruna, and Carlos Fernandez-Granda, CDS will host the new Math & Data Group seminars (MaD) this Spring. We are pleased to announce that we will also host the expanded \u2018NLP and Text-As-Data\u2019 speaker series at our swanky fifth avenue facilities.\n\nInitially focused on text analysis within the social sciences, Professor Arthur Spirling\u2019s Text-As-Data series has not only expanded to formally include Natural Language Processing within its exciting repertoire, but also welcomed a new co-organizer, Professor Sam Bowman.\n\nThe expanded series reflects the research areas of their organizers: Bowman\u2019s hails from a strong background in Computer Science, Linguistics, and NLP, while Spirling\u2019s work largely focuses on applying text analysis to answer questions in political science.\n\n\u201cWe are delighted to be expanding the mission of this Moore-Sloan Data Science Environment series, generously funded by the Methods Working Group. While \u201cText as Data\u201d has built a community around its social science focus, the addition of NLP speakers will encourage new collaborations across disciplinary boundaries \u2014 which is precisely what the Center for Data Science is all about. In tandem with the Math and Data speaker series organized by Profs. Bandeira, Bruna and Fernandez-Granda, our new initiative adds to NYU\u2019s reputation as one of the most exciting places for Data Science in the world,\u201d Spirling explained.\n\nThe MaD seminars meet every Thursday starting 26 January from 2:30pm \u2014 3:30pm in Auditorium 150 at CDS.\n\nThe NLP and Text-as-Data series meet every Thursday starting 2 February from 4pm \u2014 5:30pm on the 7th floor at CDS."
    },
    {
        "url": "https://medium.com/center-for-data-science/algorithmic-accountability-keeping-data-science-in-check-d0d4a0c6960e",
        "title": "Algorithmic accountability: keeping data science in check",
        "text": "As governments increasingly turn to data science to solve social, economic, and legal problems, citizens should be concerned with how ethical such implementations are. When governments use data science and machine learning models to establish policies, who ensures that such models are accurate?\n\nJournalism is one of the few watchdogs over those in power. Today, journalists have learned how to reverse-engineer algorithms used by governments to examine if they are improperly implemented, and take advantage of the affordability of sensors to gather their own data. This new form of journalism, fueled by data science, has been coined as \u2018Algorithmic Accountability\u2019.\n\nWhere data science becomes a problem is when models fail to account for human biases. For example, private companies who use inaccurate algorithms to automate the sorting of resumes might result in a bad pick for a job, or a discriminatory hiring process.\n\nLegal consequences can also ensue from algorithmic bias or inaccuracy. As journalist Julia Angwin discovered this May, when governments use algorithms in justice systems, inaccurate models can result in the release of dangerous criminals, or the incarceration of innocent people. Focusing on the predictive algorithm of future criminality, Angwin\u2019s investigation showed that a risk assessment tool used in several state judicial systems was twice as likely to incorrectly flag black defendants as future criminals than white defendants.\n\nPart of the problem is that governments often outsource the creation of such algorithms to private companies, who guard their algorithms fiercely. This means that citizens or defendants have very little ability to challenge the accuracy of their risk scores. To overcome this stumbling block, Angwin\u2019s team obtained 7,000 risk scores of defendants and independently checked to see how many were charged with new crimes in the next two years. Using data science, Angwin was able to uncover discrepancies that private companies might not be willing to release.\n\nIn a time when the profession of journalism is undergoing radical changes, the tools of data science give many journalists renewed ability to interrogate the powerful."
    },
    {
        "url": "https://medium.com/center-for-data-science/the-masters-experience-msds-winter-graduates-11b54c862210",
        "title": "The Masters experience: MSDS winter graduates \u2013 Center for Data Science \u2013",
        "text": "Congratulations to our Master\u2019s candidates who graduated last semester: Pan Ding, Jacqueline Gutman, Alexandre Sablayrolles, Rama Krishna Raju Samantapudi, and Olivia Yang! We caught up with them over the holidays to find out how their time at NYU has been, and where they\u2019ll be going next.\n\nWhen looking back on their experience with the NYU Center for Data Science, our graduates found that what they loved most about the program was its flexibility. As Gutman explained, the flexibility allowed her to \u201cengage in a wide breadth of problem domains and research questions within the field of data science, and to develop expertise in those areas of specialization that are most in line with my professional goals.\u201d\n\n\u201cI also enjoyed the lectures held by experienced professionals across different industries,\u201d added Yang, referring to the weekly Company Information sessions where numerous firms like PwC, McKinsey & Company, and IBM introduce our students to available job opportunities after graduation.\n\nFor example, Ding recently received an offer from IBM Watson as an entry-level data scientist, while Gutman has already started work as a full-time data scientist at the NYU School of Medicine.\n\nA crucial factor for clinching these positions, Ding explained, was deepening their technical knowledge through the program\u2019s rigorous courses. \u201cI liked the Machine Learning course the most. I felt that it was the one that really led me to the world of data science. The course was very well structured, and the teaching staff were knowledgeable and responsive.\u201d\n\nSimilarly, Gutman cited both the Machine Learning and Inference course as key to her development, since \u201cthese are the two courses I come back to most often in my work.\u201d\n\nThe Inference course was also a favorite of Sablayrolles, who will be pursuing a Ph.D. in Computer Vision. \u201cThe class goes really in depth into advanced algorithms, but the lecturers walk us through the technical details and to help us understand the bigger picture. And the homework assignments have actual real-world applications.\u201d\n\nAlternatively, Samantapudi, who will be working for Factset Research Systems, found that the Natural Language Understanding course was her favorite. \u201cAs part of the course, I worked on Dynamic Memory Networks (DMN) for Natural Language Inference (NLI) task, and I enjoyed that project the most.\u201d\n\n\u201cThe Center is also a space which allows strong collaboration,\u201d Samantapudi added, referring to our brand new facilities on Fifth Avenue, which have rapidly become a popular spot for meeting other students, engaging in collaboration, and exchanging ideas.\n\nAs Ding, Gutman, Sablayrolles, Samantapudi, and Yang go on to pursue their new opportunities, we are gearing up to welcome our students back for the Spring. With more weekly seminar series being held at the Center this semester, such as the popular Text-As-Data series, we hope to become an even livelier hub for data scientists and researchers in New York City."
    },
    {
        "url": "https://medium.com/center-for-data-science/can-data-science-help-tackle-online-harassment-518aeb6812f8",
        "title": "Can data science help tackle online harassment? \u2013 Center for Data Science \u2013",
        "text": "Moderating offensive online comments has become an increasingly difficult (and urgent) challenge. But Kevin Munger, a PhD student at NYU\u2019s Department of Politics, is hoping to change that with data science. In his paper, \u201cTweetment Effects on the Tweeted: Experimentally Reducing Racist Harassment,\u201d Munger demonstrates how his new comment moderation method, which uses \u2018bot\u2019 Twitter accounts, has already reduced offensive behavior up to a month after intervention.\n\nMunger developed his methodology through three major stages. First, he collected a dataset of Twitter users who engaged in racist behavior, by searching for those who had tweeted racial slurs at other users. Previous studies have been limited by restrictive samples since users are unlikely to come forward as openly racist. But, by using data science, Munger could independently discern racial harassment without relying on users to identify themselves.\n\nYet, because \u201ca dictionary method-like researching for ethnic slurs cannot capture any information about the tone of a tweet,\u201d Munger explained, the second stage of his research involved using a program called \u201cstreamR\u201d to assign each user an \u2018offensiveness score\u2019 based on the average number of offensive words per tweet. By assigning an \u2018offensiveness score\u2019, he could discard users who used racial slurs sarcastically. He then narrowed the dataset to 50 white adult males, as they are \u201cthe largest and most politically salient demographic engaging in racist online harassment of blacks.\u201d\n\nFinally, Munger created a suite of Twitter \u2018bots\u2019 that would tweet the users who used offensive racial slurs to remind them that their language was harmful. Interestingly, his bots were classified into four groups: they were either black or white, and had either few followers or many.\n\nHe discovered that those who had been tweeted by his white male bot with a large number of Twitter followers were the most likely to reduce tweeting racial slurs. Remarkably, his bots also influenced users to tweet racial slurs approximately 186 fewer times overall in the month after intervention, suggesting that his method is a strong way forward for tackling online harassment.\n\nHis research also raises a provocative question: what does it say about our online community that is both the white male user who most frequently performs racial harassment, and the white male Twitter \u2018bot\u2019 who has the greatest power to moderate such harassment?"
    },
    {
        "url": "https://medium.com/center-for-data-science/using-the-stars-and-data-as-your-guide-819ff3eb1819",
        "title": "Using the stars \u2014 and data \u2014 as your guide \u2013 Center for Data Science \u2013",
        "text": "While we wait patiently for self-driving cars to become viable, self-sailing boats are already a reality. A Californian company, Saildrone, currently rents self-sailing boats to scientists, environmental groups, commercial fishermen, and other organizations seeking to collect ocean data affordably and efficiently.\n\nCollecting ocean data is typically an expensive and time-consuming practice, and relies on large crews who spend months out on the water, but, as Salidrone\u2019s founders explain, their self-sailing boats can \u201ccost-effectively and autonomously gather data over large ocean areas in any conditions.\u201d\n\nEach Saildrone boat is roughly 23 feet long, with a trimaran hull and carbon fiber sail. Its stabilizer automatically adjusts the wing faster than a human sailor, and its rudder and keel can automatically right the boat if it is knocked over. When encountering potential hazards, the boat can also notify engineers by phone. Every boat is also equipped with 13 sensors to record data on wind, temperature, humidity, salinity, dissolved oxygen and fluorescence. After collecting data, Saildrone boats transmit it back to shore via satellite.\n\nRenting a single boat from Salindrone costs $2,500 per day compared to the traditional research vessel, whose equipment and a crew typically cost up to $80,000 per day. Unsurprisingly, then, Salindrone boats have been hotly sought after in the ocean research community. The National Oceanic and Atmospheric Administration, for example, recently used a Saildrone boat with a traditional Fisheries Survey Vessel to monitor fish stocks in the Bering Sea in 2015. NOAA is also working on an on-going project to use Salindrones for supplementing the Tropical Atmosphere Ocean (TAO) Array in the Pacific, which provides data for an early warning system of El Nino (warming) and La Nina (cooling) events in the eastern equatorial Pacific."
    },
    {
        "url": "https://medium.com/center-for-data-science/phd-in-data-science-application-tips-875750c6ac87",
        "title": "PhD in Data Science Application Tips! \u2013 Center for Data Science \u2013",
        "text": "How can you ensure that you\u2019re putting your best foot forward when applying for our Ph.D. in Data Science program? We caught up with Center for Data Science professors Alfonso S. Bandeira and Carlos Fernandez-Granda to ask them what advice they would give to prospective applicants who are looking to start their doctoral studies at NYU.\n\nAs they explained, there are three major factors that are particularly important in the application:\n\nAlso, if you have an MS degree or have taken similar courses to the PhD course requirements, reach out to datascience-group@nyu.edu to discuss transfer credit, waivers, and other options. Good luck on your application!"
    },
    {
        "url": "https://medium.com/center-for-data-science/photography-as-data-the-power-of-homestyle-photographs-in-political-perception-8b4eda2bbcb8",
        "title": "Photography-As-Data: The power of \u2018Homestyle\u2019 photographs in political perception",
        "text": "Can photography influence politics? At last Wednesday\u2019s Data Science Lunch Seminar Series, Assistant Professor L. Jason Anastasopoulos from the University of Georgia explained how he is using photography-as-data to measure photography\u2019s role in building trust between politicians and their constituents.\n\nTake former president Lyndon B. Johnson, for example. Although many regard Johnson as a civil rights supporter, he actually voted against every civil rights bill before he took office, yet this is rarely discussed. On one hand, Johnson\u2019s previous voting record may have been overlooked simply because of the major civil rights bills he passed during his presidency.\n\nBut, Anastaspolous argues that the rapid transformation of Johnson into a civil rights leader was also facilitated by photography. When Johnson took office in 1964, he hired Yoichi Okamoto, the White House\u2019s first photographer, to snap pictures of him alongside civil rights leaders of the day, thus bolstering his reputation as a civil rights supporter.\n\nPolitical scientists have since developed a theory to define the behaviors that politicians perform to maintain trust between themselves and their constituents: \u2018Homestyle\u2019. This involves three components: (1) resource allocation, (2) Washington activities, and (3) self-presentation (eg. either by themselves or with other individuals).\n\nAnastaspolous\u2019 research focuses on how photography that exhibits the last two \u2018Homestyle\u2019 components can affect political support. His short case study of racial influence in the pictures of US Congressman Lou Barletta has already yielded intriguing results. Anastaspolous selected Barletta in particular because he typically snaps pictures of himself with other individuals in the same room, meaning that contextual bias is reduced as the environment remains constant.\n\nInterestingly, Anastaspolous found that when Barletta was photographed alone, 58% of the participants in the experiment guessed that he was a Republican. But, when photographed next to an African-American, 62% guessed that was a Democrat. Barletta also appeared trustworthier when photographed next to a woman.\n\nThe next stage of Anastaspolous\u2019 project involves using machine learning to build a neural network to identify race so that he can repeat his case study on Barletta, but on a larger scale. After gathering over 50,000 annotated images of African-American, Asian, Hispanic, and Caucasian individuals, Anastaspolous is training his algorithm to learn from the annotated images so that it can independently tag pictures in an un-annotated data-set of 192,000 images of US House and Senate members.\n\nWhile analyzing \u2018photographic homestyle\u2019 is a new approach, Anastasplous\u2019 preliminary research already suggests that it is a valuable lens for those who are studying voter psychology."
    },
    {
        "url": "https://medium.com/center-for-data-science/gathering-the-natural-language-processing-community-9b8b61fc585",
        "title": "Gathering the Natural Language Processing community",
        "text": "On November 30, the Center for Data Science hosted NYU\u2019s first Natural Language Processing (NLP) Reception. Led by esteemed professors Sam Bowman and Kyunghyun Cho, and sponsored by the Moore-Sloan Data Science partnership, researchers working with text-as-data came to network with like-minded scholars, present their projects, and exchange ideas at the reception.\n\nBoth Bowman and Cho have produced groundbreaking NLP research that has helped to shape their field. Presently, Bowman is investigating how unsupervised deep learning can help sentence analysis, while Cho is examining how neural networks can improve computer-generated translations like Google Translate. But alongside their innovative work, the reception featured ten scholars from Engineering, Law, Marketing, and the Social Sciences who are also making fascinating contributions to the field.\n\nAssistant Professor Xiao Liu from Marketing, for example, is interested in using unsupervised deep learning to investigate how online reviews can affect sales on platforms like Amazon. Which reviews are being read, and how much do they influence the customer?\n\nKevin Munger, from Politics, is using NLP to reduce political incivility on platforms like Twitter. By using an algorithm to detect aggression, he intends to create robot Twitter accounts that will intervene in various ways to restore equity online.\n\nLike Munger, Dr. Rumi Chunara, with her research assistant Kunal Relia, from Computer Science and Engineering, are also working on Twitter, but with a different emphasis. After gathering a dataset of almost 18 million tweets that were produced in New York, they used Amazon Mechanical Turk to sort and label the tweets that were homophobic or racist. Sorting these tweets allows them to identify specific neighborhoods where homophobia and racism are most prevalent in the city.\n\nMasatoshi Tsuchiya, a visiting researcher from Japan\u2019s Toyohashi University hopes to use NLP to improve cross-lingual translations and generate automatic summaries. Improving these processes, Tsuchiya believes, will help those who are learning English is a second or third language in Japan and elsewhere.\n\nCollaboration has always been central to the interdisciplinary spirit of the Center for Data Science, especially on a field like NLP, which encompasses such a diverse range of scholars. \u201cI\u2019ve wanted to build an NLP community at NYU since I arrived,\u201d Bowman said at the end of the night. \u201cPart of my job is finding ways to open up a greater dialogue between the social sciences and the data sciences, like hosting this event. These events are also an opportunity for professors to meet graduate students, for collaborating on large research projects. I\u2019m hoping to do smaller, more regular gatherings in the future, and then perhaps another university-wide reception again in a year or two.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/harnessing-new-york-citys-soundscape-data-c4eaeba7b108",
        "title": "Harnessing New York City\u2019s soundscape data \u2013 Center for Data Science \u2013",
        "text": "The noisy din of New York City has caused many sleepless nights, for visitors and longtime residents alike. According to the Environmental Health Perspectives journal, nine out of ten New Yorkers experience noise levels above those considered safe by the Environmental Protection Agency.\n\nFortunately, Center for Data Science Professors Claudio Silva and Juan Bello are harnessing the city\u2019s racket as a vast soundscape of data so that we can monitor the acoustic environments in different neighborhoods. By gathering sound data, Bello and Silva\u2019s \u201cSounds of New York City,\u201d or SONYC, hopes to help the city manage its noise.\n\nThe first phase of SONYC has recently been put into action. Having installed microphones and sensors outside several NYU buildings, they are currently recording audio snippets at random intervals to capture the soundscape of the Washington Square Park.\n\nThe project\u2019s next phase is to analyze the audio snippets with machine-listening software to identify particular sounds, such as sirens or barks. After learning how to distinguish different sounds, the microphones will then be able to recognize the same sounds when they occur elsewhere in the city. An interdisciplinary research team of experts in acoustics, machine listening, digital media, machine learning, data analysis and visualization will also use the sound data to produce aural maps and statistical reports on sound levels, types, patterns, and variations across time.\n\nSONYC\u2019s end goal is to become a \u201cnoise mission control\u201d that can assist government officials and city agencies to combat noise pollution. For example, the system could assist in confirming 311 noise complaints, or identify hotspots of noise pollution that require more urgent intervention.\n\nYou can learn more about the work that our faculty is doing with SONYC in The New York Times and The Economist."
    },
    {
        "url": "https://medium.com/center-for-data-science/big-data-big-questions-can-we-measure-political-sophistication-663c788bf94a",
        "title": "Big Data, Big Questions: Can we measure political sophistication?",
        "text": "Back in 2013, The Guardian proposed that our political speeches are declining in linguistic sophistication. Then, in 2015, Politico\u2019s data analysts concluded that Donald Trump speaks at the level of a third-grader. Today, in 2016, the same man has been elected as our next President.\n\nAre we increasingly favoring less sophisticated political speech? This question is at the heart of a fascinating project led by CDS\u2019 very own Arthur Spirling, along with Kenneth Benoit (LSE) and Kevin Munger (NYU).\n\nLike the case studies in The Guardian and Politico, the team defines linguistic sophistication as the \u2018perceived readability\u2019 of a text according to a particular measure. Today, the standard readability measure is the Flesch Reading Ease (FRE) test. Part of FRE\u2019s process involves calculating the average sentence length of a passage and the average number of syllables per word. The full calculation is as follows:\n\nThe higher the score, the more linguistically complex the passage is. For example, the approximate FRE score for Dr. Seuss\u2019 The Cat in the Hat is 0.7, while George Washington holds a score of 19.3. In contrast, Donald Trump, when speaking in 2015, scored 0.9.\n\nBut although FRE is useful, it hits some stumbling blocks when applied to political texts. For example, FRE overlooks how colloquial diction changes over time. Although \u201cforsooth the cordwainer was afeard\u201d would score the same as \u201cindeed the shoemaker was frightened\u201d since they have the same number of words and syllables, the latter is clearly more \u2018readable\u2019 for today\u2019s audience. FRE calculations also do not account for the persuasive power of rhetorical devices like repetition: repeated sentences simply score the same and do not affect the overall FRE score.\n\nBenoit, Munger, and Spirling are hoping to overcome these challenges by developing a new measure that is not only tailored to handling political texts, but also tempered by human judgment.\n\nTo start, the trio collected the word frequencies of an astounding total of 760,653,149,209 words from the Google Books corpus, and then sorted their frequencies by decades. This data allows their model to measure whether specific words are in or out of fashion based on how often it was used in the English language overall during the time period that the text was written in.\n\nThe team also crowd-sourced some comparison data. After extracting one or two-sentence snippets from several state of the union speeches, they asked users to compare two snippets at a time and decide which one was easier to read. Recording this human judgment data helps their model capture the implicit effects of rhetorical devices like repetition.\n\nAfter fine-tuning their model, the trio hopes to analyze political speeches in other contexts, from congressional floor speeches to debates. Their model will also be released as a part of the quanteda R package so that other researchers can apply their methodology to their own research projects.\n\nBenoit, Munger, and Spirling\u2019s project is built on an innovative approach, interdisciplinary research and collaboration. Their project marks an incredibly exciting time for researchers interested in analyzing text-as-data."
    },
    {
        "url": "https://medium.com/center-for-data-science/does-social-media-actually-impact-how-we-consume-news-943e311e9699",
        "title": "Does social media actually impact how we consume news?",
        "text": "Commentators have made heavy weather of how social media has affected people\u2019s consumption of news. Just recently, journalists have proposed that the circulation of fake news articles on Facebook played a major role in producing the startling election of Donald J. Trump as the country\u2019s next President. The general belief is that social media causes users to exist in an echo chamber where they only consume news that reflects their point of view and that of their friends, who are also likely to share similar viewpoints. But according to Andrew Guess, a Moore-Sloan fellow at CDS, most people\u2019s consumption of online news media is remarkably moderate.\n\nTo test the claim that social media causes users to consume articles that already agree with their own political inclinations, Guess combines data on people\u2019s website visits with their individual-level political leanings in his recently published paper, \u201cMedia Choice and Moderation: Evidence from Online Tracking Data.\u201d\n\nGuess uses data collected by online polling firm YouGov, together with a web tracking service called Wakoopa that was installed by consenting participants in his research. Guess also used YouGov\u2019s individual-level survey responses, which contained demographic information like gender, age, race, and political affiliation.\n\nNext, Guess determined the \u201cideological slant\u201d of the websites visited by assigning each web domain an alignment score of how left or right-leaning it was. This score is based on the average self-reported ideology of Facebook users who shared content from that domain. For example, Fox News might earn a right-leaning alignment score because the main ideology of people sharing Fox News content is Republican.\n\nAfter assigning each web domain with a political alignment score, Guess calculated the number of times a person visited a web domain and how long they spent there, and then determined each respondent\u2019s \u201caverage media slant\u201d. Guess discovered that the majority of Facebook users visited moderate sites such as CNN, MSN, or Yahoo! News, regardless of whether the respondent was a democrat or republican.\n\n\u201cThe results show a remarkable degree of balance in respondents\u2019 overall media diets regardless of partisan affiliation. Whether Democrat, Republican, or independent, the large bulk of these individuals\u2019 media diets cluster around the center of the ideological spectrum,\u201d Guess stated. While there were some respondents who, indeed, visited websites on the extreme ends of the ideological slant spectrum (such as right-leaning Breitbart or the leftist Daily Kos) these were only of a small number.\n\nDuring a time where many feel America is becoming more divided, Guess\u2019 research comes as a surprise. If democrats and republicans both, indeed, consume the same moderate news articles on social media, then Trump\u2019s political rise may be propelled by other factors that have yet to be explored."
    },
    {
        "url": "https://medium.com/center-for-data-science/the-future-is-nigh-nyu-data-future-lab-part-ii-5eefe3952e66",
        "title": "The Future is Nigh: NYU Data Future Lab \u2014 Part II \u2013 Center for Data Science \u2013",
        "text": "What\u2019s next for artificial intelligence? Last Friday, NYU\u2019s Data Future Lab graciously invited our MSDS students to tour their offices and discover the career opportunities that are waiting for them at the five startups currently being incubated. In the second part, we take a look at Carmera and OWAL (read the first part here):\n\nA major problem with Google Maps today is its inaccuracy: it has a tough time keeping up with rapid urban changes, especially in cities like New York where stores and restaurants open, close, or move multiple times over the course of a year.\n\nHow can we use new technologies to produce better maps? Enter Carmera, the world\u2019s only 4D, real-time index of city streets. Key to their technology\u2019s success is partnering with delivery fleets and trucks. Carmera attaches specialized cameras and sensors to the delivery fleets as they run up and down the city streets. As they receive or drop off packages throughout the day, they also help Camera collect real-time, detailed data about the areas.\n\nCarmera\u2019s rich spatial data can also verify, assess, track, and quantify, and predict trends about the areas that it covers. For example, Carmera can assess whether an area\u2019s architecture is becoming more environmentally friendly, or predict whether there will be more SUVs on the streets in the next six months. Carmera\u2019s powerful 4D maps are also an asset for architects and engineers who need to gather accurate details about surrounding buildings and infrastructure\n\nWhile surveillance technologies today are often viewed with suspicion, can we harness their power for social good? A major area where surveillance can help communities is securing the safety of residential buildings and spaces. Using advanced computer vision technologies that can identify animals, cars, people, and objects, OWAL uploads surveillance footage online onto a cloud video platform for residents to view. Aside from protecting the residential space, OWAL\u2019s detailed computer vision technology is so advanced that it can alert residents when there is a parking space available, a package at the door, or if there is a violent altercation occurring.\n\nCrucially, OWAL\u2019s \u2018community camera\u2019 approach raises awareness of residential space while protecting individual privacy. Residents can only view the cameras that are relevant to their own floor or surrounding areas, every resident can see when and how frequently their neighbors have viewed the same footage, and the data gathered is only preserved for three years overall."
    },
    {
        "url": "https://medium.com/center-for-data-science/on-different-sides-of-chinas-internet-firewall-sina-weibo-and-twitter-88ff04d82bdb",
        "title": "On different sides of China\u2019s internet firewall: Sina Weibo and Twitter",
        "text": "Just how much does China\u2019s internet firewall affect what Chinese users inside and outside of the wall discuss on social media? Center for Data Science fellow Bruno Goncalves, along with Northeastern University postdoctoral researcher Qian Zhang, has been investigating this very question by comparing trending Chinese topics on Sina Weibo (China\u2019s most popular microblogging platform) and Twitter. After collecting 216.8 million Weibo messages and 12.3 million tweets in simplified and traditional Chinese sent during 2012, Goncalves and Zhang wrote an algorithm that identifies significant keywords in the messages so that they can be grouped by topic.\n\nBut how exactly does their algorithm work? Key to their research process is combining two methods: term frequency (TF) and inverse document frequency (IDF). TF calculates the most frequent terms in the messages, while IDF calculates the most infrequent terms. Tempering TF\u2019s results with IDF\u2019s calculations is crucial, Zhang explains, because using TF alone would cause words like \u2018a\u2019 or \u2018this\u2019 to be perceived as \u2018topics\u2019 even though they are simply common words that would naturally be found in all messages. Similarly, using IDF without TF would mean that every single infrequent term would become a topic of its own, even though multiple terms could refer to the same topic. \u201cFor example, nowadays \u201c#debate\u201d and \u201c#debatenight\u201d refer to the exactly same topic, and \u201c#nastywomen2016\u201d apparently also refers to the final presidential debate,\u201d says Zhang.\n\nCalculating the most and least frequent words means that the algorithm can cluster messages that refer to the same topic even if they use different hashtags, or contain no hashtags at all. The accuracy and nuance of Goncalves and Zhang\u2019s approach is especially compelling because previous studies have only clustered messages based on hashtags alone.\n\nAfter crafting this powerful algorithm, Goncalves and Zhang clustered the Chinese messages on Sino Weibo and Twitter according to their topics. Unsurprisingly, they found that top Chinese topics on Weibo and Twitter were drastically different. Sina Weibo focused on entertainment, such as singers, actors and games \u2014 the number one topic was a game titled \u201c\u4e09\u56fd\u6765\u4e86,\u201d or \u201cThe Three Kingdoms Have Arrived.\u201d But on Chinese Twitter, the number one topic was \u201cChen Guangcheng,\u201d a Chinese civil rights activist, and the top ten topics were dominated by political topics like \u201cFree Tibet\u201d and \u201cWukan Protest.\u201d\n\nAs Zhang explains, the very fact that Twitter is restricted in China might have attracted people who wished to \u201crebel against the system,\u201d which might have been an influence on the political culture of Chinese Twitter. Interestingly, tweets concerning political topics were \u201cpractically nonexistent\u201d in Weibo. Their results confirm the general belief that users who share the same linguistic background but do not live in the same place will display different interests on social media because they are exposed to different cultural influences."
    },
    {
        "url": "https://medium.com/center-for-data-science/the-future-is-nigh-nyu-data-future-lab-part-i-758b688e9d52",
        "title": "The Future is Nigh: NYU Data Future Lab \u2014 Part I \u2013 Center for Data Science \u2013",
        "text": "What\u2019s next for artificial intelligence? Last Friday, NYU\u2019s Data Future Lab graciously invited our MSDS students to tour their offices and discover the career opportunities that are waiting for them at the five startups currently being incubated. In this first part, we take a look at Wade & Wendy, Paperspace and FindMine:\n\n1. Wade And Wendy\n\nEvery graduate\u2019s headache after tossing their graduation cap in the air is how to clinch a full-time job. Luckily, \u2018Wade and Wendy\u2019 is here to improve the recruiting process using AI. They are named after two artificially intelligent personalities. \u2018Wade\u2019 is a career guide for applicants: he presents available employment options based on the applicant\u2019s wants and needs. Wade\u2019s conversational interactions with the user show his technical strengths in understanding unstructured text, participating in dynamic, real-time conversations, and developing an engaging personality.\n\n\u2018Wendy\u2019 works on the other end, assisting hiring managers with filling open positions. Her technical strengths are sourcing and screening data, clarifying role descriptions, gathering and summarizing feedback, and coordinating interviews. Key to her role is aggregating data from Wade and the company\u2019s hiring manager to build models that can assess which applicants are an ideal fit for the job. However, \u2018Wade and Wendy\u2019 does not replace HR recruiters: in fact, their people-centered philosophy means that they greatly depend on recruiters to train both robots.\n\n2. Paperspace\n\nThe technology space is crowded with new computers and operating systems that are coming out every day. How can we keep track of our data and information as we leap back and forth between desktops, PCs, and tablets? The answer: Paperspace, a new cloud-based computer that can be accessed anywhere on any device. Harnessing the power of GPU (Graphic Processing Unit), Paperspace is a powerful virtual desktop that can run demanding applications quickly making it an ideal fit for personal use, businesses, or educational institutions.\n\n3. FindMine\n\nA major challenge facing online shopping is demonstrating how a product can be integrated in the user\u2019s life, especially when it comes to fashion. A pair of boots may seem appealing on screen, but would they match what you have in your wardrobe? While online stores use algorithms to generate \u2018similar items\u2019 lists for customers, recommendations about other products that complement the selected item are still generated manually.\n\nFindMine automates this process for retailers by using powerful and scalable machine learning techniques. Its complex algorithms analyze the brand\u2019s aesthetics and the user\u2019s tastes to create a personalized shopping experience. FineMine\u2019s three-month pilot for menswear company John Varvatos shows promising results: average orders and the amount of time spent on the site increased by 74% and 107% respectively, and their overall revenue increased by 6.7%."
    },
    {
        "url": "https://medium.com/center-for-data-science/pooling-resources-the-power-of-data-analytics-ce0b03040242",
        "title": "Pooling resources: the power of data analytics \u2013 Center for Data Science \u2013",
        "text": "When working for large corporations, employees rarely step foot in offices other than their own. But last Friday, Nabeel Azar from Verisk Analytics introduced our CDS students to their Data Science Excellence Program, an exciting graduate scheme that allows graduates to travel and work at different Verisk offices across the US for five years.\n\nVerisk is a leading data analytics provider serving clients in insurance, natural resources, financial services, government, and risk management. Key to their position as a top player in the data science industry today is their massive data repository. It operates as a consortium: several companies share their depersonalized data with Verisk to form a large data pool. For example, Argos, their financial services offshoot company, has over 19 billion depersonalized credit and debit card records provided by several major banks across the world.\n\nCompanies generally have data about their own performance, but Verisk\u2019s data assets mean that its data scientists can analyze how their client companies measure up with other competitors. This crucial \u2018Give-to-Get\u2019 philosophy has been in place since the company\u2019s founding in 1971, meaning that they have had over 40 years to collect data.\n\nTheir data centers currently hold around 14 petabytes (that\u2019s 14,000,000 gigabytes!) of data, which is why Verisk\u2019s data scientists often perform large-scale data integration, multispectral data capture, and data visualization. To train future employees how to handle this work, Verisk has rolled out a unique five-year Data Science Excellence Program.\n\nData science graduates like those at Center Data Science undergo three eighteen-month rotations in different Verisk offices across the country to bolster their technical skills, business acumen, and leadership experience while receiving a full-time salary. The rotations are uniquely stretched out so that graduates can contribute, learn, and network meaningfully in each office, building long-term relationships and experiences that will support their careers for years to come.\n\nBy the end of the program, graduates can move into a full-time leadership position. Last year, Verisk recruited two Center for Data Science students into their cohort; undoubtedly, our MSDS 2018 students are also well-positioned to be a part of Verisk\u2019s next intake."
    },
    {
        "url": "https://medium.com/center-for-data-science/a-real-world-babel-fish-using-neural-networks-for-better-translations-3299117adab5",
        "title": "A real-world Babel Fish: using neural networks for better translations",
        "text": "Although machines can outperform humans in almost any skill set today, there is still one process that they have yet to master: translation. Several students learning a second or third language in particular will have undoubtedly encountered some of the more hilarious results produced by Google (mis)Translate.\n\nBut a solution was recently proposed by the Center for Data Science\u2019s very own Kyunghyun Cho. Together with Yoshua Bengio and Orhan Firat, their innovative model \u2014 which is the first to handle multi-way, multilingual translations \u2014 clinched the runners-up position for best paper at the 2016 Annual Conference of the North American Chapter of the Association for Computational Linguistics.\n\nTraditionally, machine translation uses a phrase-based approach. It chops up the source sentence into phrases, which are then directly mapped to a corresponding phrase in the target language. But, as Cho remarks, the drawbacks are that this phrase-based mapping is highly specific to a given language pair, and cannot easily add a third language for translation.\n\nHarnessing the power of neural networks might help us overcome this problem. The inspiration for Cho\u2019s project came from observing how multilingual individuals typically learn new languages more quickly because they capture the underlying structures shared between different languages, and use them to learn a new language. The question, then, was obvious: could machine translations adopt this process by using neural networks?\n\nInstead of the phrase-based model, Cho\u2019s neural network model reads and translates the source sentence into the target language using an encoder-decoder approach. The encoder reads and compresses the sentence into a fixed-size vector. This is then translated into the target language using a decoder. This process increases the translation\u2019s accuracy, which is why Cho\u2019s model outperforms traditional translation software.\n\nThis encoder-decoder approach also allows the machine to capture the underlying linguistic structures shared between languages by observing the patterns in the particular points of the vector where the encoder and decoder operate. Crucially, remembering these patterns means that the machine can also translate a third language based on the lessons learned from the first two.\n\nCho\u2019s approach is also especially compelling because it uses a recurrent neural network. While traditional phrase-based translation only reads the source sentence once before translating it, Cho\u2019s recurrent network repeatedly refers to the source sentence while producing its translation to increase accuracy."
    },
    {
        "url": "https://medium.com/center-for-data-science/big-data-big-questions-what-happens-when-you-refuse-to-guess-46481f0f7c83",
        "title": "Big data, big questions: what happens when you refuse to guess?",
        "text": "Last Wednesday, Mustafa Anil Kocak from NYU\u2019s Tandon School of Engineering came to CDS to explain how we can reduce algorithmic errors by instructing an algorithm not to make predictions under certain circumstances.\n\nMachine learning algorithms are often used to make predictions in finance, medicine, or real estate, but it is still possible for them to miscalculate. This can have severe consequences. For example, if an algorithm misinterprets patient data, a doctor may inadvertently prescribe an ineffective treatment plan. The repercussions would then fall on both parties: the patient might be worse off, and the doctor could face a lawsuit.\n\nGiven the consequences of inaccurate predictions, Kocak is investigating how error rates can be reduced by not guessing. Using a new Conjugate Prediction approach, Kocak has crafted a meta-algorithm can be implemented on top of an algorithm to calculate the error rate of its predictions.\n\nImagine a lamp shade over a lit light bulb. The lit bulb is the algorithm producing predictions, while the lamp shade is the meta-algorithm that is placed over the bulb. The meta-algorithm analyzes the error rate of the algorithm\u2019s predictions as they are generated and, if it determines that the error rate of a certain prediction is high, the meta-algorithm will force the algorithm not to make a prediction in that instance \u2014 forcing the light bulb to \u2018turn off\u2019 instead of continuing to emit light.\n\nAfter testing his model on seven data sets, Kocak found that his meta-algorithm can reduce the error rates of algorithms by up to one quarter, at the cost of asking the algorithm to refuse guessing 11% to 58% of the time.\n\nWhether Kocak\u2019s model can be practically implemented in real-world scenarios still requires further investigation. But the meta-algorithm\u2019s counterintuitive logic of not guessing is an unexpected intervention in a field that prizes predicting outcomes. Kocak\u2019s model also gestures towards a possible paradox in the general predictive process: the most accurate way to guess may sometimes be not to guess at all."
    },
    {
        "url": "https://medium.com/center-for-data-science/protecting-wildlife-from-illegal-trading-with-text-analysis-d46af94fc869",
        "title": "Protecting wildlife from illegal trading \u2014 with text analysis?",
        "text": "Sunandan Chakraborty is a Moore-Sloan Postdoctoral Researcher at CDS. After graduating with a Ph.D. in Computer Science from NYU, he explored the problem of illegal online wildlife trading with complex digital text analyses, in collaboration with Jennifer Jacquet of the Environmental Department at NYU, where he\u2019s a visiting scholar. His research went on to win the United States Agency for International Development (USAID) Wildlife Crime Tech Challenge earlier this year.\n\nCan you give us a bit of background on yourself? How did you get to where you are now?\n\nWhen I was completing my Master\u2019s degree at the Indian Institute of Technology Kharagpur (IIT) and working for Microsoft Research India, I realized that I wanted to develop my interests in Information Extraction, Natural Language Processing, and Machine Learning. I applied to NYU because of its strong research groups in my areas of study.\n\nWhy did you choose to examine the subject of illegal wildlife trading?\n\nIllegal wildlife trade on the web is a major problem that is often overlooked, and the proposals that do address this problem remain inadequate. I felt that a good solution was urgently needed, especially for the law enforcement agencies. The problem\u2019s computational complexity was also an intriguing challenge for me, as issues on such a large scale often require particularly innovative data science solutions. Additionally, I have always been committed to using computational and data-driven methods to conduct research on real world problems in developing regions.\n\nWhat sort of information or trends did you go in looking for?\n\nThe main trends I interrogated were as follows. Firstly, what are the main species and products involved in wildlife trade? Secondly, what are the main sites that are used as a trading platform for these illegal products? Finally, what are the major hot spots for this activity?\n\nAt any point in your research did your objectives change? Were there any assumptions you had going in that had to be adjusted once you started to analyze the data?\n\nOur objective did not change, but we did modify our approach to account for some of our assumptions. This involved changing aspects like the list of keywords, adding more sites to the list which had not seemed to be important before, and adding more common names and code words for certain species.\n\nWith the tool that you designed, are you looking for words, code-words, pictures, something else, or a combination?\n\nIt is a combination of the things mentioned above. There are texts associated with the online advertisements and postings, so they contain specific product information like its price, shipping formation, and item origin. Recording these details were crucial to our problem-solving process. We are presently analyzing the images in these ads by designing an item/species identification tool using the state-of-the-art computer vision techniques.\n\nHow much supervision is required from humans when using your tool?\n\nThe tool is being designed in such a way that the end users will need minimum supervision. During the developmental phase, however, a certain amount of supervision is needed, particularly from industry experts. We rely on their expertise to identify illegal items in these advertisements, and which properties are used to detect illegal objects/species.\n\nWhere are you predominantly looking for illegal wildlife trading? eBay? The Dark Web?\n\nIllegal wildlife trade is flourishing in the open web: we have so far identified many online marketplaces and retail sites across different countries. Interestingly, a recent study found no evidence of illegal wildlife trade in the dark web.\n\nHow has your time with the CDS program been so far?\n\nVery enjoyable. It is a very lively and vibrant community; it is a pleasure to work within this environment. I enjoy being in this interdisciplinary environment. Apart from my main project, I have had the chance to collaborate with other researchers from different backgrounds, and work on several interdisciplinary projects. This gives me the opportunity to learn skills from different communities as well as contribute in research domains other than my own."
    },
    {
        "url": "https://medium.com/center-for-data-science/next-gen-recommendations-using-text-analysis-to-find-your-new-favorite-book-c3986cc339c7",
        "title": "Next-gen recommendations: using text analysis to find your new favorite book",
        "text": "Audible\u2019s Senior Vice President of Data Science, Haftan Eckholdt, visited CDS last week to discuss how data science plays a vital role in maximizing audio book sales. Audible is the world\u2019s leading retailer and producer of digital audio books, and a key to finding, engaging, and retaining Audible\u2019s subscription-based customers is identifying the texts and audio books that are most valuable to their clientele.\n\nSince value is typically marked by what is purchased, generating a best-seller audio book list is relatively straightforward as its based on major trends in Audible\u2019s transactional data. But, as Eckholdt explained, the \u2018best-seller\u2019 group accounts for only half of Audible\u2019s customer base. How can we use data science to identify and understand the diverse and highly particular literary tastes of those outside of the trend line?\n\nWorking with a repository of over 250 thousand English manuscripts and sound files, Audible is using data science to perform topic modelling analyses and explore sound taxonomies. For example, what linguistic markers can best identify writing style so that the platform can recommend audio books that are written most similarly to the customer\u2019s favorite writer? Do customers who share the same age, education level, gender, or marital status prefer the same plot lines? And, which accents and vocal ranges are most appealing to their customers in the US, UK, Germany, and elsewhere?\n\nBuying a book is a highly subjective decision making process (rather than, let\u2019s say, buying a kitchen skillet), and answering these crucial qualitative questions will help Audible match customers to audio books based on individual literary preferences rather than mainstream tastes.\n\nBy using data science to illuminate the secret structures behind words and speech, this data-driven interrogation of literary style also promises to yield valuable suggestions about how we perceive and assign value to literature and sound. For the Center for Data Science students looking to apply their data science skills to questions of literary value, Audible made a strong case."
    },
    {
        "url": "https://medium.com/center-for-data-science/can-data-influence-your-next-purchase-cd9f1d016e3e",
        "title": "Can data influence your next purchase? \u2013 Center for Data Science \u2013",
        "text": "Last Wednesday, eBay\u2019s Director of Software Development and Merchandise, Giri Iyengar, came to the Center for Data Science to discuss data science career options in the e-commerce industry.\n\nInitially founded as an online auction site in 1995, eBay has become of the world\u2019s leading e-commerce platforms today. With over 160 million active members and one billion items listed, eBay\u2019s merchandise team harnesses big data and machine learning tools to boost sales.\n\nAs Iyengar explained, there are four steps to facilitating a successful sale: awareness, interest, desire, and action. Like setting up a store front, awareness involves arranging all of the items for sale in an attractive and user-friendly manner on the platform. After inspiring the undecided customer, the platform must then generate interest and desire by recommending suitable products to the customer based on their previous searches or purchases. These recommendations should eventually lead the customer towards an item that they like enough to click \u2018purchase\u2019. The customer\u2019s purchase completes the final stage: action.\n\neBay NYC is the powerhouse that crafts the powerful algorithms behind this merchandising process. For example, to generate the list of \u2018recommended items\u2019 alongside the initial item (often referred to as the \u2018seed item\u2019) that the customer originally searched for, the algorithm sends the seed item\u2019s data to the Merchandizing Back End (MBE). The MBE compares the seed item\u2019s details with other data sets to analyze the titles, images, purchase rates, and views of other products. Those which bear the most similarity to the seed item then appear as the \u2018recommended items\u2019 for the customer.\n\nTo generate better recommendations, the platform also records its customer\u2019s behavioral data. Which products were clicked on, viewed, and purchased? Which products were clicked on, viewed, but not purchased? Collecting and analyzing these results is crucial in helping algorithms make better recommendations every time customers use their platform.\n\nThis complex data science work is at the heart of eBay NYC, whose office is conveniently located in Manhattan. Being only a stone\u2019s throw away from Center for Data Science, the exciting opportunity to work for Iyengar\u2019s team is not to be missed."
    },
    {
        "url": "https://medium.com/center-for-data-science/what-does-it-mean-for-a-machine-to-think-like-a-human-9761bc726577",
        "title": "What does it mean for a machine to think like a human?",
        "text": "How close is humanity to creating truly artificial intelligence? Machine learning\u2019s remarkable progress in object and speech recognition, video games, and other areas has been broadly covered by news media, and it would seem the AI dream is fast becoming reality. However, are these machines really learning to think like humans?\n\nBrenden Lake, a Moore-Sloan Data Science Fellow at the Center for Data Science, recently published the paper \u201cBuilding Machines That Learn and Think Like People.\u201d He said that while machines have equaled or surpassed humans on many important benchmarks, current systems like deep neural networks differ from human intelligence in crucial ways. Many contemporary deep learning models use a pattern-recognition approach, finding patterns in data to achieve an objective goal. Building machines that utilize a model-based approach to learning, which Lake said is a cornerstone of human thought, is the key to more human-like machines that not only predict and solve problems, but understand and explain their solutions.\n\nLake\u2019s paper put forward several crucial ingredients for human-like thinking that could be integrated into machines. The first set of ingredients was what he termed \u201cdevelopmental start-up software,\u201d like intuitive physics and psychology present in human infants; the second set comprised tools for model-building, such as compositionality and \u201clearning-to-learn.\u201d Lake used two examples to demonstrate the current differences in human and machine learning. The \u201cCharacters Challenge\u201d tests a subject\u2019s recognition of handwritten characters, while the \u201cFrostbite Challenge\u201d tests a subject\u2019s ability to learn and master a game \u2014 in this case, the Atari game \u201cFrostbite.\u201d\n\nThe \u201cCharacters Challenge\u201d demonstrated the vast difference between pattern-recognition and model-building. Given the task of classifying images of numbers into the categories \u201c0\u201d to \u201c9\u201d, humans and neural networks performed equally well, but Lake said two crucial differences were that people learned from fewer examples, and they learned what he termed \u201cricher representations\u201d: humans could recognize a new character from a single example, but beyond pattern-recognition, they learned a model that allowed them to apply their knowledge in new ways, like generate new examples of the character, break it down into its most important parts, and even generate new characters based on a set of related characters, all of which would not be possible for a neural network using a pattern-recognition approach.\n\n\u201cWhen people learn a new letter, such as a letter in a foreign alphabet, they gain many abilities at the same time. People learn a model for a new character that is flexible enough to support all of these tasks,\u201d says Lake.\n\nThe \u201cFrostbite Challenge\u201d also exposed the gulf that still exists between natural intelligence and machine intelligence. Google Deepmind\u2019s \u201cDeep Q-Network\u201d (DQN) was the system trained to play the Frostbite game, where the player\u2019s avatar must construct an igloo by jumping on ice floes in the water. The player earns extra points by gathering fish, while avoiding fatal hazards like polar bears.\n\nWhile the DQN learned to play \u201cFrostbite\u201d at a human-level of performance, it required a much larger amount of experience. The DQN was compared to a professional gamer who received two hours of practice on the game; the DQN was trained on 200 million frames of the game \u2014 approximately 924 hours, and almost 500 times as much as the human gamer. The DQN initially achieved less than 10% of human-level performance; revised variants eventually reached 96% of human performance, but still required a disproportionate amount of experience compared to the human gamer.\n\n\u201cLearning a new video game also speaks to the flexibility of our model-building capabilities. When you learn to play a new video game, you usually aim to get the highest score. But you can also change your goal and still play effectively,\u201d says Lake. He listed examples like collecting as many fish as possible, or teaching one\u2019s friend how to play, as methods of play that differed from the goal of obtaining the highest score. In contrast, the DQN network was inflexible to changes in its inputs and goals; it would not be able to play the game with a different objective without extensive retraining.\n\nLake was intrigued by the differences in performance between humans and machines on the challenges, and that it hinted at fundamental differences in learning between humans and machines. Integrating his recommended developmental start-up software, like intuitive physics and psychology, could produce more powerful learning and thinking abilities in machines. Intuitive physics are physical concepts present in human infants as young as 2 months; for example, infants expect objects to follow principles of persistence (they exist continuously and do not wink in and out of existence) and solidity (objects do not inter-penetrate), among others. According to Lake, these concepts may be used to solve daily physics-related tasks, including games like \u201cFrostbite.\u201d\n\nTherefore, integrating an intuitive physics engine could enable a machine intelligence to adapt to a spectrum of scenarios; a physics-engine reconstruction of a game of \u201cJenga\u201d might be used to predict the possibility and manner of a tower falling, as well as capture human-like hypothetical scenarios such as \u201cwhat would happen if certain blacks are taken away or more blocks are added?\u201d\n\nAccording to Lake\u2019s paper, intuitive psychology is a concept, learned or innate in humans, that leads them to expect that entities will act in a goal-oriented and efficient manner. He used the example of people learning to play \u201cFrostbite\u201d from watching an experienced player play first, and then playing themselves: \u201cintuitive psychology provides a basis for efficient learning from others\u201d, says Lake. \u201cIn the case of watching an expert play \u201cFrostbite,\u201d intuitive psychology lets us infer the beliefs desires and intentions of the experienced player.\u201d Incorporating intuitive psychology into deep learning systems could change the way they learned to play games, reducing the learning curve to one approximating human performance.\n\nDeep learning and other types of machine intelligence still have a long way to go before matching natural intelligence, which according to Lake is still the best example of intelligence: \u201cThis is a higher bar worth reaching for, potentially leading to more powerful algorithms while also helping unlock the mysteries of the human mind.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/big-data-big-questions-how-does-political-conflict-affect-the-economy-f592f4fd3f94",
        "title": "Big Data, Big Questions: how does political conflict affect the economy?",
        "text": "On Wednesday, Center for Data Science\u2019s Moore-Sloan fellow Michael Gill showcased his research on how war and conflict impact the economy.\n\nThe US Department of Defense outsources most of its military equipment. According to the data gathered about the DoD\u2019s business contracts, they purchase tanks, planes, weapons, and other gear from almost 350,000 businesses and multinational corporations. Since these expenditures account for almost 1% of the global GDP annually, it is unsurprising to assume that military events would affect the economy. But is it possible to be more specific about this correlation?\n\nUsing a Bayesian Structural Time Series model (BSTS), Gill is performing the first large-scale data analysis of the DoD\u2019s contracts and corporate welfare to calculate the extent of military events on the economy. He focuses specifically on how revenue for military-related businesses fluctuated after three major \u2014 and unplanned \u2014 conflicts: 9/11, the troops\u2019 surge in Afghanistan, and the death of Osama Bin Laden.\n\nPart of the challenge with studies like these is managing our subjectivity when selecting data sets, defining variables, and singling out particular objects for analysis over others. For Gill, analyzing unplanned military events sharpens our insight on how conflict impacts the economy because the costs of planned military events are accounted for in the budget ahead of time.\n\nCrucially, discovering how military conflicts impact the economy involves using big data to calculate what a company\u2019s revenue would have been had the conflict not occurred, and then comparing the estimated revenue with the company\u2019s actual revenue after the unplanned conflict.\n\nGill\u2019s fascinating research shows that revenue rose after 9/11 and the troops\u2019 surge for most businesses, but either stayed neutral or dipped after the death of Osama Bin Laden \u2014 showing how big data can help us understand how political, economic, and cultural systems interact and influence each other on a national or global scale."
    },
    {
        "url": "https://medium.com/center-for-data-science/nyu-entrepreneurs-challenge-4f75797fa293",
        "title": "NYU Entrepreneurs Challenge \u2013 Center for Data Science \u2013",
        "text": "What do the founders of Pinterest, Seamless, and Twitter all have in common? They all started right here at New York University.\n\nIf you can make it in New York, as Sinatra famously sung, you can make it anywhere. And the boom in successful startup businesses over the past few years has certainly proved this. Maybe you want to transform the code you\u2019ve written into a profitable mobile app. Maybe you want to apply your data analytics algorithm for the financial services industry. Maybe you want to use technology to solve a social problem, like poverty, famine, or access to healthcare.\n\nBut how can you bring your startup ideas to life? Fortunately, NYU\u2019s $300K Entrepreneurs Challenge is here to help you turn your dreams into reality, and is open to NYU students, faculty, and alumni from all departments, including the Center for Data Science!\n\nLast year, over 250 teams competed, and this year, the number is set to grow. The introductory talk that took place on September 14th attracted so much interest that all seats were taken within minutes of the doors opening (if you couldn\u2019t get in, check out the video recording).\n\nGiven the recent explosion of tech incubators and accelerators all over the world, why and how does this competition continue to attract so many aspiring entrepreneurs? One reason, of course, is that there are lucrative prizes up for grabs: $100,000 for the best New Venture or the most cutting edge Technology Venture, and $75,000 for the most innovative Social Venture.\n\nNYU\u2019s Entrepreneurs Challenge, however, also boasts an especially powerful support network of successful CEOs, Angel Investors, Venture Capitalists, and experienced industry professionals in New York City. All teams have an opportunity to work closely with these talented mentors as they progress through the program\u2019s nine-month journey of workshops, networking events, and development boot camps.\n\nAccess to this network is the secret behind NYU\u2019s success in producing entrepreneur after entrepreneur since the competition\u2019s conception twenty years ago. With successful mentors and the unparalleled accounting, legal, and marketing resources provided by NYU\u2019s Leslie eLab, W.R. Berkley Innovation Lab, and Stern School of Business, the time is ripe for aspiring entrepreneurs to step outside the classroom and into the global market.\n\nFor more info, visit the challenge\u2019s page!"
    },
    {
        "url": "https://medium.com/center-for-data-science/ibm-comes-to-the-center-for-data-science-4fefd83e125e",
        "title": "IBM comes to the Center for Data Science \u2013 Center for Data Science \u2013",
        "text": "Last Friday, IBM came to the Center for Data Science to chat with our students about the company\u2019s applications of data science and some of the career opportunities that are available at IBM.\n\nWith a significant presence in 18 industries across 170 countries, IBM is the world\u2019s premier technology consultancy, offering insight into the research, manufacturing, communications, and marketing sectors. Since its conception in 1911, IBM employees have won Nobel Prizes, Turing Awards, and numerous National Medals in Technology and Science.\n\nHistorically, IBM\u2019s success has centered around the use of technology to change and improve human interactions, such as relationships between doctors and patients. And IBM\u2019s approach to data science takes a similar course. For example, by using data analytics to better understand correlations between healthcare treatments and patient outcomes, cancer diagnostics can be rendered and given more quickly.\n\nIn their chat with CDS, IBM stressed that they look to hire applicants that are not only technologically adept, but that are also, \u201cactive listeners first.\u201d Data analytics serve as a window into the human interactions that are becoming increasingly dependent upon technology. And IBM employees are uniquely positioned to study, analyze, and facilitate the human interactions that are at the core of industries such as medicine, communications, and education."
    },
    {
        "url": "https://medium.com/center-for-data-science/how-should-government-agencies-regulate-data-science-2f44f3bc78ce",
        "title": "How should government agencies regulate data science?",
        "text": "In May, the United Kingdom\u2019s Government Digital Service released a paper entitled \u201cData Science Ethical Framework.\u201d Although it is not a legally binding document, the paper was designed as a guide for government employees, and is a template for the appropriate and ethical uses of data science. It gives government employees the confidence to use innovative data science methods, and the tools to avoid ethically questionable projects.\n\nIn the paper\u2019s introduction, the United Kingdom\u2019s Minister of State for Digital and Culture, Matt Hancock, said \u201cdata science carries both huge opportunities and a duty of care.\u201d He continues, \u201cThis document is\u2026 bringing together the relevant law in the context of new technology, and prompting consideration of public reaction so that government data scientists and policymakers can be confident to innovate appropriately with data.\u201d\n\nThe paper outlined six guiding principles for the use of data science in the public sector:\n\nThe paper then followed with instances of where these principles were and were not used. For example, to demonstrate the importance of creating robust data science models, an analysis was shown of Hurricane Sandy\u2019s impact on the states of New York and New Jersey. If one were to simply analyze the data collected via social media, one would come to the conclusion that the epicenter of the damage was in the borough of Manhattan. However, the damage in neighborhoods such as Coney Island, Breezy Point, and Far Rockaway was much more severe, but this was not reflected on social media, because of less social media chatter concerning these areas.\n\nThe United States is facing a similar question as to how data sets can and should be used. There is presently a debate within the criminal justice system concerning whether or not data should be used as a way of determining sentencing guidelines for convicted criminals. And similar concerns about the robustness of data sets are at play in this situation as well.\n\nIn 2014, Attorney General Eric Holder said regarding these systems, \u201cAlthough these measures were crafted with the best of intentions, I am concerned that they inadvertently undermine our efforts to ensure individualized and equal justice. They may exacerbate unwarranted and unjust disparities that are already far too common in our criminal justice system and in our society\u201d\n\nThe \u201cData Science Ethical Framework\u201d was specifically set to address laws already in place in the United Kingdom, and so the paper cannot be used as a universal rule book for data scientists. But it does set a precedent in that government agencies should have some role in mitigating potentially harmful byproducts of advancements in data science. In an interview with Science Friday, DJ Patil, the Chief Data Scientist at the White House\u2019s Office of Science and Technology Policy, called for every data science training program to include data ethics as a core tenet of data science.\n\nPatil continued, \u201cWhen we do work with data, you have incredible opportunities to do great things with it, and you also have the ability to do something that could be very problematic. We\u2019re seeing where people have used data in ways that we think are fundamentally not okay. People have started to talk about this and what we should do about it. I think we have to have much stronger conversation. Privacy components are equally important.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/back-to-basics-the-foundations-of-data-science-17209af9cf5b",
        "title": "Back to basics: the foundations of data science \u2013 Center for Data Science \u2013",
        "text": "Professor Afonso Bandeira is an Assistant Professor of Mathematics at the Courant Institute of Mathematical Sciences.This fall, he will be teaching a course at the Center for Data Science, titled, \u201cOptimization and Computational Linear Algebra for Data Science.\u201d\n\nWhat did you study in school?\n\nI did my Bachelors of Science, and Masters of Science in Mathematics, at the University of Coimbra in Portugal, and I became interested in applied mathematics while studying the connections between harmonic analysis and signal processing. I later completed my PhD in Applied and Computational Mathematics at Princeton University. Since then, I have studied the mathematical processes behind data extraction.\n\nCould you tell us about some of the research projects that you\u2019re working on?\n\nThe cryo-electron microscopy problem is a good example of the type of problem I tend to research. Say you have a projection of an unknown molecule structure, and that projection is given from an unknown direction. If you knew one of the variables \u2014 either the direction of the projection, or the structure \u2014 then you could probably deduce the other unknown variable. But with the cryo-electron microscopy problem, neither variable is known, so determining both at the same time becomes an interesting mathematical problem.\n\nCan you talk about the course that you\u2019re teaching in the fall?\n\nThe course I will be teaching is titled, \u201cOptimization and Computational Linear Algebra for Data Science.\u201d The course aims to provide students with the optimization and linear algebra skills that are commonly needed in the broader field of data science.\n\nHow do optimization and linear algebra fit into a broader data science curriculum?\n\nWhile a lot of data science techniques and methods \u2014 machine learning, neural networks, artificial intelligence \u2014 continue to evolve, the principles of optimization and linear algebra will essentially remain the same, and will continue to be a foundation to data science in the coming decades.\n\nWhat drew you to the Center for Data Science at NYU?\n\nThere\u2019s a level of excitement in being at a place where such a diverse set of research backgrounds are united by an interest in data-driven and data-inspired research. That sort of environment creates potential for collaborations across fields that would, otherwise, potentially not interact.\n\nOn a chalk board at CDS, we have a question: what does it mean to be a data scientist? What does that mean to you?\n\nFor me, being a Data Scientist is not only learning from data itself, but also understanding the process \u2014 both the potential and the limits \u2014 of learning from data, and aiming to develop methods and tools that take advantage of data\u2019s full potential."
    },
    {
        "url": "https://medium.com/center-for-data-science/should-we-be-teaching-computer-science-to-children-part-1-ad18d65be245",
        "title": "Should we be teaching computer science to children? \u2014 Part 1",
        "text": "In the first of our two-part analysis, we interview two professionals in the field of children\u2019s computer science\n\nAs digital technologies become increasingly entrenched into our world, the conversation surrounding when and how to teach computer science is becoming increasingly important. Should computer science courses be requisite the way that math courses are? And if so, at what age would we start educating kids about computer science, and in what capacity?\n\nTo find out more about the current conversation surrounding computer science in childhood education, we spoke with two professionals in the field.\n\nJasmine Ma is an Assistant Professor of Mathematics Education at NYU\u2019s Steinhardt school, and holds a Masters of Technology in Education. She is also one of the faculty members involved in Steinhardt\u2019s Computer Science Education program.\n\nWe also spoke with Jason Briggs, who lives and works in the United Kingdom, and is the author of \u201cPython for Kids,\u201d a programming book designed for elementary school students.\n\nHow did each of you become interested in computer science education at the pre-college level?\n\nJasmine: Most of my research has looked into how educators can connect a students\u2019 out-of-school life with their formal learning environments. When Steinhardt and the Mathematics Education department began conversations about a Computer Science Education program, I was one of the faculty recruited for the effort, given my work in education, pedagogy, and my knowledge of computer science.\n\nJason: My dad bought a computer when I was 8 years old, and I was hooked. Apart from a brief period in my teenage years where motorbikes were far more important, computer science was always going to be in my future.\n\nCan you each talk about the types of computer science opportunities that are currently available to high school and middle school kids? Are there classes? Clubs? Groups?\n\nJason: In the United Kingdom, some time ago, there was a government initiative to improve technology education in school. How well it has worked, I can\u2019t really say. I only have the experience of my daughter to go on, which is anecdotal at best. If her secondary school is typical, there seems to be more focus on how to use programs, as opposed to building programs or programming. The education is more focused on using word processors, PowerPoint, etc, and is less focused on the actual building blocks of computer science: coding and a greater understanding of computers.\n\nJasmine: In New York City, there are several software engineering academies, where software engineering runs through the whole curriculum. After the first one opened, the city of New York also began their Software Engineering Pilot Program, which was a predecessor to Mayor Bill De Blasio\u2019s Computer Science for All initiative.\n\nThe problem is that there is not currently a computer science education certification for teachers in New York State, and there is a lack of computer science education programs to prepare teachers. To my knowledge, Steinhardt is the only school currently offering a program. We have an undergraduate minor, and 2 computer science education courses at the Masters level. Other schools are quickly developing programs, and we will likely see these programs emerge in the fall.\n\nThere are some online programs for schools, where teachers can rely on an online curriculum and platform to deliver aspects of a computer science education, but these online courses are mainly programming focused. I haven\u2019t heard of anything that is as broad as \u201csoftware engineering,\u201d or something that teaches other aspects of computer science.\n\nThere are, of course, a lot of computer science opportunities in after-school and out-of-school spaces all over the city. These include programming classes, but if you pay attention to computer science broadly, you\u2019ll see the teen hackathons, maker spaces and events, and programming that is interest-driven but includes general computer science knowledge.\n\nCoding can seem like such a dry subject. How do you think educators can get more kids involved in computer science and coding?\n\nJasmine: Coding is a dry subject. This is the problem that I mentioned when we equate computer science with programming. Programming is a tool and skill that is part of a computer science education, but it\u2019s a mistake to make coding the end goal.\n\nJason: Giving kids a project to work on, such as coding a simple game, is much more interesting as an educational topic for most kids, and they\u2019re going to learn just as many skills along the way.\n\nA game like Minecraft is another possibility. Using code to build something in-world is going to make programming a lot more approachable. Imagine a class where the students disregard half the lesson after being instructed to build a life-size pyramid, manually, in Minecraft. Then the teacher uses simple code to build the pyramid in a few minutes or seconds. The experience of laboriously doing the same thing over and over again, versus automating the task with a little bit of brainpower, is something that could appeal to students.\n\nHow do you think high schools and middle schools can better promote computer science activities?\n\nJasmine: I think that schools need to think of computer science as a discipline that is much broader that simply programming and coding. A lot of opportunities are lost when there is a hyper focus on coding. Other aspects \u2014 engineering, production, problem-solving, creative uses, and an understanding of hardware \u2014 are ignored.\n\nMaybe the first thing they can do is learn, and teach the kids and parents, what computer science actually is, and then give teachers the space, time, and resources to bring computer science into their curriculum.\n\nThere is also a problem with time as a resource: middle and high schools already feel like there is not enough time to teach the standard curriculum, given the amount of standardized testing that students are subjected to. Many teachers panic at the thought of having something else that needs to be taught, because there is the fear that it will take time from something else during the day. What will they have to give up? More arts? Gym? Lunch? It does not have to go this way, but there need to be more conversations about what incorporating computer science into education would look like in practice.\n\nWe\u2019ve failed to convince people that math is important and valuable for all students. How many adults have said \u201cI\u2019m terrible at math. I hated it in school. I get on fine without it.\u201d This needs to be the start for understanding a computer science education. If we force the curriculum on people, it will not work. We need to ask bigger questions. What are the experiences of the communities that we are supposed to be educating? Why might they care about learning computer science?\n\nSchools need to understand why their students, families, and communities might care about learning computer science. The predominant reasoning for teaching computer science is all about jobs and the economy. But this isn\u2019t the whole story.\n\nBesides career opportunities, why do you think it\u2019s important for kids to get involved with computer science?\n\nJasmine: There is also a civic engagement aspect. As the world becomes increasingly reliant on the digital, it is important to know something about computer science, in order to participate productively in society, to make good decisions, and to protect yourself.\n\nJason: Problem solving skills and logic: both are incredibly useful. Knowing that you can possibly do something, and then, more importantly, having the confidence to figure it out for yourself, is really empowering for kids.\n\nI\u2019ve heard educators talk about the possibility of giving high schoolers the option to take computer science and programming courses instead of high-level math courses such as pre-calculus and calculus. What are your thoughts on this?\n\nJason: Personally I think a foundation in mathematics is extremely important. I can certainly see the attraction. Programming could be viewed as more approachable than algebra, or calculus. But I worry that students would be losing something important.\n\nJasmine: I don\u2019t see any problem with it, but the issue is, a lot of university-level computer science and math classes require those courses. So you might actually be impeding some students who are in the pursuit of a computer science degree. But if a kid is only taking pre-calculus as a requirement, and is likely to struggle through it, then I am all for replacing it with computer science courses.\n\nHowever, if the content of the computer science course is disconnected from the student\u2019s life, and presented as just another requirement, then I don\u2019t see the point."
    },
    {
        "url": "https://medium.com/center-for-data-science/solving-scalability-and-time-issues-in-large-datasets-e100ab6a5b7c",
        "title": "Solving scalability and time issues in large datasets",
        "text": "Last week, the Center for Data Science welcomed Kx Systems \u2014 a software company based in New York \u2014 into our school, where they gave two consecutive day-long workshops on their q programming language, and their kdb+ database.\n\nKx Systems\u2019 kdb+ database was designed for time-series analysis, so it\u2019s a tool that is most useful for either collecting real-time data, or working with huge amounts of data where a time variable is a significant factor. This set of tools has been widely adopted by the finance industry, and is starting to become prevalent in health, communications, and other fields that require time-series analytics.\n\nThis was the second year that Kx Systems led a series of workshops in our office space, and both workshops were open to CDS graduate students and other members of the data science community. We spoke with Simon Garland, a Senior Engineer at Kx Systems:\n\nScalability is frequently talked about on your website. Why is scalability such a challenge, and why is it important?\n\nScalability is important for working with huge amounts of data, and generating complex analytics. You do not want your system to run out of steam just when your results start to get interesting.\n\nLarge systems are often an uncomfortable mix of different technologies, all of which need to be maintained, upgraded, and kept in sync with each other. And the amount of data being collected today has brought a lot systems to their limits.\n\nKdb+ was engineered with extreme data flows, data stores, and parallel hardware in mind, so the program can easily be expanded to accommodate large amounts of data.\n\nCan you talk about the Q programming language, and why it\u2019s better suited for this type of data, as opposed to a language like Python?\n\nToday, a lot of developers are choosing to learn multiple programming languages because of legacy computing resource requirements. However, we think Kx Systems\u2019 technology covers most computing needs, while recognizing that seamless interfaces to more popular programming languages are beneficial to developers.\n\nMost of Kx Systems\u2019 clients are in the finance industry. Why is it that finance has adopted this software?\n\nThe finance industry is much less risk averse than other domains, and historically, has been open to new technologies. For example, IEX \u2014 the latest trading venue gaining approval on the stock exchange \u2014 uses our technology for real-time analytics on a system that sequences orders from broker subscribers, and accumulates quotes and trades from other regulated venues.\n\nData gathered from the \u201cInternet of Things\u201d (Iot) seems to be a big part of Kx Systems\u2019 offering. Can you talk about how kdb+ interacts with Iot?\n\nKx Systems\u2019 technology solves a different problem for IoT analytics than it does for financial services. In finance, there is a constant stream of market data; in IoT applications, there are large bursts of data at lightning fast speeds. Kdb+ is able to keep up with this data flow and analyze it.\n\nOutside of finance, what are some of the other industries that have adopted kdb+?\n\nPharmaceutical companies are using kdb+ for epidemiological studies; sales support dashboards and are exploring IoT applications based on sensor data from manufacturing equipment.\n\nOther IoT use cases are in the oil and gas industry and telecoms. Power utilities are using our technology for combining historical analysis with the real-time monitoring of smart meter sensor data."
    },
    {
        "url": "https://medium.com/center-for-data-science/money-machines-and-markets-7124ad2a71e",
        "title": "Money, Machines, and Markets \u2013 Center for Data Science \u2013",
        "text": "The financial markets \u2014 the stock exchange, bonds, commodities \u2014 are a historically high-risk, high-reward avenue towards making, or losing, money. Besides working as a television weatherman, investment banking is one of the few fields where you only have to be correct sixty percent of the time to be considered a smashing success. Investment decisions are generally gauged against the S&P 500, which is a collection of the largest corporations and their stock; it\u2019s an option that, compared to the rest of the market, promises healthy and steady returns. Of course, humans have always tried to beat this standard, and no secret that most human investors continually underperform against the S&P 500.\n\nAs a field with huge amounts of data, and an almost equal amount of human error, the field of financial services is ripe for advancements through data science. By combining the history of the United States stock exchange with data regarding human investment choices, machine learning techniques are poised to comb through this huge amount of information, and maybe even come up with a stock tip you can actually rely on.\n\nIn a recent paper titled, \u201cShould You Trust Your Money to a Robot?\u201d CDS Faculty member Vasant Dhar investigated the possibility of using machine learning techniques to make financial decisions for humans.\n\nSo should we trust our money to a robot?\n\nIt depends on the investment at hand. Because machines only rely on previously collected data, they can\u2019t spot a \u201cone of a kind\u201d investment opportunity in the same way that humans can \u2014 granted, most humans can\u2019t spot a \u201cone of a kind\u201d investment opportunity either. But since machines rely so heavily on said data, they tend to make much smarter investment decisions. Generally speaking, humans are not very good at investing.\n\nCan you give us a bit of background on why you chose to look into this subject?\n\nOne of my major career objectives was to design a machine that could learn to trade as well, or better, than a human counterpart, given access to the same data.\n\nI became interested in the area of predictive models for financial decisions back in the 1990s, when I saw corporate financial data becoming more readily available. Being located in New York City, the finance industry seemed like the best place to explore all this available data, so I took a few years off from academia and went to work on Wall Street. I began mining the available data, which was mostly market and customer data.\n\nWhich part of the investment process are machines helping with?\n\nMy machine helps pick investments from a defined set of possibilities. But most other machines that are working in the financial sector are performing portfolio optimization, as a way of balancing risk; most other machines aren\u2019t actually helping pick the investments.\n\nIn the introduction to your \u201cShould You Trust Your Money to a Robot?\u201d paper, you talked about several investors (George Soros and Warren Buffet) and the longevity of their careers. Are the decision making processes of these investors used to model your machine learning programs?\n\nNo, they have nothing to do with my programs. I\u2019ve never found a human who always performed better than the market. Buffet and Soros are clearly exceptional. While some argue that their success was pure luck, I don\u2019t believe this. They were special in spotting opportunities. But the opportunities they exploited are extremely rare.\n\nThere\u2019s a wide range of investment opportunities within the financial markets. Which parts of the market have you been conducting your research on?\n\nMachines have already taken over high frequency trading, because that\u2019s an area where humans don\u2019t stand a chance against machines. My research focuses on the mid-range investments \u2014 investments that are made and held for a period of weeks to months.\n\nWhat is the biggest weakness that machines face in making financial decisions?\n\nThe noise and chatter surrounding the financial markets make markets incredibly hard to predict. There are too many random shocks and unpredictable events that impact investments; both humans and machines have a hard time being calibrated for these sorts of unpredictable events."
    },
    {
        "url": "https://medium.com/center-for-data-science/tell-me-what-to-do-but-dont-tell-me-what-to-do-2cf2619638c3",
        "title": "Tell me what to do \u2014 but don\u2019t tell me what to do \u2013 Center for Data Science \u2013",
        "text": "One of the most important conversations in the field of machine learning is the debate surrounding the use of predictive methods to influence or inform human decisions. Broadly speaking, the field of machine learning is the practice of programming computers to be more self-sufficient, and to create systems that can operate on their own, with minimal human guidance. This can extend itself to anything from data collection, to data analytics, to creating decision trees. Machine learning processes and predictive methods can, hypothetically, make decisions for humans, but should they? And if we allow machine learning techniques to begin informing our decisions, or making decisions for us, where should we draw the line? In a recent lecture cohosted by the Center for Data Science \u2014 as a part of the ongoing NYC Data Science Seminar Series (DS3) \u2014 Jon Kleinberg, a Computer Science Professor at Cornell University, gave a talk titled, \u201cHuman Decisions and Machine Predictions.\u201d He spoke about the possibilities and limits of allowing machines to facilitate decision making processes in a number of fields \u2014 from neural networks, to chess, to criminal justice.\n\nMachines making decisions for humans can sound like the plot of a science fiction film, and Kleinberg opened his talk dispelling the frequently cited idea that \u201cthe machines are taking over.\u201d Like many machine learning practitioners, Kleinberg stressed that machine learning is most effective when conjoined with human intelligence. Intelligence is not a single variable, and one of the foundational points for machine learning is the idea that computers and humans have differing strengths in the wide field of intelligence: computers are much more adept at arithmetic and counting, while humans are remarkably well trained at logic and reasoning. Kleinberg believes that these differing forms of intelligence are compatible, not diametric opposites.\n\n\u201cAlgorithms are a lens into human decision making,\u201d Kleinberg said. With its plethora of complex choices and pieces, the game of chess has historically been the parameter for using data analytics to track human decisions. With all the possible decisions in a single chess move, algorithms can create decision trees, which allow humans \u2014 or a computer program, like IBM\u2019s Deep Blue \u2014 to see all of the resulting consequences from one move. Kleinberg then asked the question, \u201cHow can machines and algorithms help us catch bad decisions?\u201d\n\nThe talk then moved to a field with more gravitas than the game of chess: criminal justice. Kleinberg has chosen to research how machine learning techniques might apply to decisions made in a courthouse. A quote from the former Attorney General, Eric Holder, was displayed on the screen. Holder once said that, within the context of the criminal justice system, the superficial application of algorithms has the potential to: \u201cexacerbate unwarranted and unjust disparities that are already far too common in our society.\u201d And there is certainly truth to this. Machine learning systems only use readily available data, and so if there is already a discrepancy in the criminal justice system \u2014 based on either race, class, or gender \u2014 a program calibrated against our already established norms might only exacerbate these disparities.\n\nBut Kleinberg doesn\u2019t believe that machine learning should replace a judge or jury, rather, he believes that machine learning techniques can be used as a way of catching biases in judicial situations. Kleinberg used the example of a defendant seeking bail while awaiting trial. When determining whether or not a defendant should be granted bail, a judge is supposed to look at a myriad of factors to inform their decision: prior arrests, flight risk, and any previous convictions, among other variables. The judge is not supposed to take into account extraneous factors \u2014 race, how the defendant is dressed, or how the defendant acts in court \u2014 but the judge may do so anyways. Kleinberg believes that, instead of having machines make decisions for judges, machine learning systems have the potential to look at whether or not a judge is taking extraneous factors into account when making judicial decisions.\n\nMachine learning systems can easily comb through huge amounts of data, and Kleinberg posed the possibility of a machine learning system going through a judge\u2019s entire judicial history. A machine learning system can analyze all the cases in which, say, a given defendant has one prior arrest, and no prior convictions, and look into instances when a given judge does and does not grant bail. A defendant\u2019s record will not reflect how the defendant acted in court, but it will reflect the ethnicity of the defendant, and the defendant\u2019s place of residence. If a judge is using extraneous factors to inform their decisions in otherwise similar scenarios, a machine learning system will notice.\n\nThe idea of machine learning keeping the criminal justice system in check, as opposed to replacing the criminal justice system, circled back to one of Kleinberg\u2019s first points: machine learning processes should never replace human decision making processes, but machine learning processes can help humans catch their own mistakes."
    },
    {
        "url": "https://medium.com/center-for-data-science/the-unknown-unknowns-of-machine-learning-adf8db4586d1",
        "title": "The \u201cUnknown Unknowns\u201d of Machine Learning \u2013 Center for Data Science \u2013",
        "text": "One of the struggles in the field of data science is striking the necessary balance between human decision making, and automated computer processing. The field of machine learning \u2014 which looks to create computing systems that can solve problems on their own \u2014 is a perfect example of where human intelligence and mechanical computing power must go hand in hand. Machine learning systems can quickly and effectively assemble a data set, but there can be blindspots in what a machine learning system looks for when collecting data. Humans, on the other hand, are much more competent in determining where there might be holes in a data set.\n\nIn his research, Panos Ipeirotis \u2014 an Associate Professor at the Center for Data Science and the Stern School of Business \u2014 combines the computing power of machines with the problem solving capabilities of humans, as a way of generating more effective machine learning techniques. He recently published a paper titled, \u201cBeat the Machine: Challenging Humans to Find a Predictive Model\u2019s \u2018Unknown Unknowns,\u201d which examined the effectiveness of having humans look for the blind spots in machine learning techniques\n\nHow did the project of \u201cunknown unknowns\u201d come to be?\n\nWe were thinking of better ways to improve our machine learning systems, and we realized that humans are quite competent at figuring out how to create and look for errors in automatic processes.\n\nCan you give a specific example of one of these \u201cunknown unknowns?\u201d\n\nRight now, we\u2019re building a system for detecting misconduct within financial institutions. We have teams of independent financial analysts \u2014 who have significant domain expertise \u2014 to come up with \u201cunknown unknown\u201d cases of financial misconduct. In this situation, the data set already includes past cases of misconduct, and so we ask analysts to come up with cases that haven\u2019t been caught yet, or might not be anticipated.\n\nHow does the difference between machine intelligence and human intelligence factor into your research?\n\nMachines implicitly operate with the assumption of the \u201cclosed world\u201d, i.e., the world of the data at hand. However, the data at hand is not necessarily representative of the world as a whole. Humans are much more capable in coming up with examples that could be missing from a data set. In the case of AlphaGo \u2014 Google\u2019s machine learning program that was taught how to play the game Go \u2014 the only game that it lost was due to an \u201cunexpected\u201d move from its opponent, which indicates that the machine was not able to cope with \u201csurprises.\u201d\n\nHow do you measure a system\u2019s performance when the system might not even know all the variables?\n\nThere are multiple metrics that can be used to measure performance in machine learning, and most of these metrics will suffer from that same \u201cclosed world\u201d assumptions. So we optimize our models with this \u201cunseen\u201d test data, so that we\u2019re testing against the known world, not just the world of the data set.\n\nWhat were your findings with this project?\n\nHumans should always be part of machine learning solutions, as they can guide machine learning systems to learn about things that the systems doesn\u2019t yet known \u2014 the \u201cunknown unknowns.\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/crowdsourcing-data-to-understand-speech-disorders-4307f762a019",
        "title": "Crowdsourcing data to understand speech disorders \u2013 Center for Data Science \u2013",
        "text": "Daniel Fern\u00e1ndez is a new Post-Doctoral Fellow at the Center for Data Science; his research focuses on the ways in which data science can be used to help individuals with speech impediments.\n\nCan you talk about the research projects you\u2019re currently working on at CDS?\n\nI am mainly working on a project that focuses on communication disorders, such as speech impediments. These disorders can affect a person\u2019s ability to participate in occupational, social, and academic settings, and affect up to 10% of the total population.\n\nCan you talk about why you wanted to pursue this project at CDS specifically?\n\nCDS gathers researchers from a wide range of fields, which creates an enriching network, and a productive environment. I was particularly interested in the interaction between statisticians, computer scientists and professionals from other fields, because I believe this is the sort of interaction necessary for research in the field of data science.\n\nWhat drew you to work on a project focusing on speech impediments?\n\nI have been always interested in research topics concerning the social integration of underrepresented minorities. I completed my doctoral work at Victoria University of Wellington in New Zealand, and while I was there, I worked at the University\u2019s Disability Service\u2019s Department, which helped students with social communication disorders or language-based learning disabilities. My work there certainly reinforced my interest in solving these sorts of problems.\n\nHow are you going about collecting data regarding speech disorders?\n\nWe are using crowdsourced experiments to obtain data in the study of speech-rate tasks. An example of a speech-rate task would be a child saying a word which contains an \u201cr\u201d sound. This word is uploaded into the crowsourced experiment, and a group of non-expert listeners rate whether this particular child has produced the sound of the \u201cr\u201d correctly or incorrectly. Crowdsourced studies help us collect data that rate the degree of a person\u2019s speech disability, so that we can better understand how widespread certain speech disorders are, and how noticeable the disorder is.\n\nWhat are some of the advantages of crowdsourcing data, as opposed to other collection methods?\n\nIn the context of measuring speech production, traditional methods require either highly trained personnel or a large numbers of human listeners in a laboratory setting, making the process slow, and expensive. This poses a major rate-limiting factor in the study of speech production. Crowdsourcing represents a valid way to measure speech quality and intelligibility, where data can be collected quickly, cheaply, and efficiently.\n\nHave you run into any difficulties with crowdsourcing?\n\nCrowdsourcing provides a way to collect data quickly, but it is harder to control the quality of the data. Because of this, we are developing statistical methods to evaluate rater and speech-task quality in these experiments. We are also investigating the optimal number of raters and speech tasks required to obtain reliable and robust estimates.\n\nI am also working on a somewhat related project to determine which factors impact how speech pathology experts are hearing sounds in people \u2014 mainly children \u2014 with speech disabilities. The purpose of this experiment is to explain the variations in how experts rate those speech sounds."
    },
    {
        "url": "https://medium.com/center-for-data-science/analyzing-data-is-all-the-rave-but-collecting-it-is-the-real-challenge-1da403dfb227",
        "title": "Analyzing data is all the rave \u2014 but collecting it is the real challenge",
        "text": "With all of the advancements that are taking place in the field of data science, one aspect that can often be overlooked is the actual collection of data. While machine learning, neural networks, and artificial intelligence have done wonders for data analysis, the problem of data collection still troubles data scientists. In the social sciences, data collection has often relied upon expert academics to gauge and contribute data. While this method works well, the problem is feasibility, as experts in any field are expensive to hire. And tied to the issue of feasibility, is the issue of reproducibility: the ability to replicate data collection and analysis. Often times, data scientists have difficulty presenting their exact protocols in a way that allows other researchers to replicate the process with another problem.\n\nCrowd-sourced data has the potential to help data scientists solve the issue of feasible data collection, and can help standardize reproducibility. Certain fields within the social sciences don\u2019t require experts, and this provides opportunities for data scientists to develop new and innovative data collection methods. Take the field of political science. The average person might not be an expert in the field, but they could probably tell you the policy differences between Bernie Sanders and Donald Trump. This is where crowd-sourced data collection comes into play: instead of hiring experts, data scientists can use the internet to draw from a wide pool of paid workers, who can gauge a political ideology along an axis. The effect is a cheaper and more diverse data set, that can be more effectively replicated, as the protocol for data collection can be more easily presented.\n\nMichael Laver \u2014 a Center for Data Science Faculty member, and a Professor of Politics at NYU \u2014 recently co-authored a paper titled, \u201cCrowd-Sourced Text Analysis: Reproducible and Agile Production of Political Data.\u201d In this paper, he demonstrated the use of crowdsourcing as a tool for data collection, specifically in the field of political science, although the results of this paper have an impact on almost all of the social sciences.\n\nCan you talk about how this project came to be?\n\nOriginally, we were looking at how political discourse can be analyzed through language processing. Our baseline interest was in scaling political documents \u2014 locating them on left-right, or liberal-conservative, scale. But over time, we became increasingly aware of the importance of having our own well-tested system for quality control, and the emphasis shifted more and more towards replicability.\n\nCan you talk about the importance of replicability in the field of data science?\n\nMany of the major datasets in the social sciences \u2014 however professionally collected \u2014 cannot be easily replicated. In other words, it would be difficult for a third-party researcher to use an identical protocol, and collect the same data.\n\nWhy is data replication so difficult?\n\nData replication is difficult for two reason: the initial costs, and nonspecific protocols. Some of these big datasets cost a fortune to assemble, and nobody is going to pay to reassemble them over and over to demonstrate their replicability. Crowd-sourced data sets address both of these issues.\n\nThe cheapness of crowdsourcing makes it much more feasible, in term of resources, to collect \u201cthe same\u201d data again. To replicate a crowd-sourced dataset, you just need the crowdsourcing code and a small amount of funding.\n\nAre there any disadvantages for using crowd-sourced data collection methods?\n\nThe data collection protocols must be very explicit, since they are written to understood by crowd workers all over the world. Every crowd-sourced data collection task has to be broken down into very simple and easy-to-understand instructions. Some data collection tasks may be too complex for this.\n\nIs there a difference in the way that political data is collected as opposed to other types of data?\n\nIn politics, a politician or political actor is represented as a data point along an axis that gauges their relative political standing (left to right, liberal to conservative). So in the field of political science, all of the data points are relative to each-other, as opposed to being gauged against a pre-set variable.\n\nWas there anything that you had to take for granted before proceeding? Did any of your assumptions change as your research went on?\n\nSome of our early advisors recommended that we only work in the English language, as a way of ensuring consistency. But eventually, we decided to challenge this by replicating our methods in five languages: German, Spanish, Italian, Greek and Polish. Since the crowd workers are responding on numbered scales, we didn\u2019t have to translate any answers, we only had to make sure that our questions were properly translated. This expanded the pool we could draw from, and worked much better than we expected.\n\nGenerally speaking, what were your findings with this project?\n\nWe found that labeling political texts with ideological positions can effectively be achieved through crowd-sourced data, with results that closely match those achieved by highly trained experts, and at a much lower price point."
    },
    {
        "url": "https://medium.com/center-for-data-science/data-science-students-tackle-patient-readmission-rates-at-the-iowa-business-analytics-competition-4d9a4cb60b6c",
        "title": "Data science students tackle patient readmission rates at the Iowa Business Analytics Competition",
        "text": "Last April, two CDS students, Zewei Liu and Olivia Yang, along with two Stern Business School Students, Andrew Hamlet and Troy Manos, won the University of Iowa\u2019s MBA Business Analytics Case Competition. We asked Zewei and Olivia a few questions about their work.\n\nCan you give us a bit of background about the competition?\n\nOlivia: It was a business analytics case competition hosted by the University of Iowa. 14 teams participated and represented universities across the nation. At least two participants from each team had to be MBA students. The competition expected participants to have a strong sense of business and data analytics. The case was provided by a hospital called UnityPoint, who sponsored the competition.\n\nZewei: We had to give two rounds of presentations to a panel of 14 individuals. The panels were comprised of analytics professionals and faculty from the University of Iowa.\n\nWhat was the case that your team had to solve?\n\nZewei: We were given a healthcare case about readmissions. A readmission is when a patient is readmitted to the hospital within 30 days, for the same reason as the initial stay. Readmissions are generally used as a measure for how well a hospital is doing in treating its patients.\n\nThe case was provided a week ahead of time, giving us time to do research and data analysis. The deliverables were:\n\n1) to come up with a readmissions risk score for each patient\n\n2) build a guided user interface (GUI) for clinicians to use\n\nOlivia: The challenge was to reduce the readmission rate for the hospital, and provide the optimal post-discharge care plan for the patient, accounting for price and quality of care.\n\nHow did you go about solving it?\n\nOlivia: We applied machine learning techniques to predict the readmission rate and see which elements impact the readmission rate for patients. By assigning scores to different post-discharge care plans, we determined which plans have the lowest rate of readmission. Then we built a GUI for patients and physicians, to simplify the process in the future.\n\nYou both were on a team with two Stern Business School MBA candidates. Can you talk about working with students from another area of NYU, and what each group of students brought to the team?\n\nOlivia: It was a great experience working with the Stern MBA students. MBA students tend to think differently than most data scientists. Troy and Andrew\u2019s strong business sense enriched the project and made this project run smoothly.\n\nZewei: Everyone on our team was technically savvy and knew how to code, so working with data and building the interface came somewhat naturally. The hardest part was putting all of the elements together in one coherent story that made sense and actually solved the problem for the hospital. I think that working with a different group of NYU students motivated us to think about the problem from different angles, and forced us to have a comprehensive understanding of the case.\n\nDid any of the other teams have members from data science programs?\n\nOlivia: Yes, most of the teams had at least one student with mathematics/computer science background to balance their skill set.\n\nZewei: Most of the teams were all MBA students, and some teams had members from analytics programs, and some teams had MBA students in business analytics track programs.\n\nHow does the problem you guys had to solve compare to a problem that data scientists in the real world deal with every day?\n\nZewei: Our case and data was from UnityPoint, and they also have data scientists working to solve this same problem.\n\nCan you talk about how the CDS program prepared you both for the Iowa Business Case Analytics competition?\n\nZewei: David Rosenberg\u2019s Machine Learning class, Carlos Fernandez-Granda\u2019s Statistical and Mathematical Method\u2019s class, and Foster Provost\u2019s Intro to Data Science class were foundational for me. All the methods that we used in this competition were from the CDS program. But what I found most useful about the CDS experience was how students were encouraged to not just solve a problem, but gain insight into the data. Being able to gain insights from data greatly helped us in this case competition.\n\nOlivia: Machine learning is the key method we applied and the NYU Master of Science in Data Science program does an excellent job of not only teaching us the theory behind data science methods, but also encourages us to gain a sense of real world problems."
    },
    {
        "url": "https://medium.com/center-for-data-science/can-your-neighborhood-affect-your-health-data-science-says-yes-3600594491b3",
        "title": "Can your neighborhood affect your health? Data science says yes",
        "text": "Dustin Duncan is an Affiliated Faculty member at the Center for Data Science, an Assistant Professor at NYU\u2019s Department of Population Health, and the Principal Leader at the Spatial Epidemiology Lab. His work focuses on the intersection of of public space and personal health, backed by a fundamental understanding of data science.\n\nWhat did you study in school? How did you get to what you study now?\n\nAs an undergraduate student at Morehouse College, I majored in Psychology and minored in Public Health Sciences. I completed my master\u2019s work at the Harvard T.H. Chan School of Public Health (HSPH), and went on to complete my doctorate in Social Epidemiology at HSPH, in addition to being an Alonzo Smythe Yerby Postdoctoral Fellow at HSPH from 2011 to 2013.\n\nHow did you become interested in social epidemiology?\n\nI became interested in studying the social epidemiology of neighborhoods during my master\u2019s program, when I was a research assistant at Dana-Farber Cancer Institute\u2019s Center for Community-Based Research (DFCI). At DFCI, I co-authored a paper demonstrating that perceiving one\u2019s neighborhood as unsafe is associated with reduced walking among urban predominantly racial/ethnic minority low-income adults. This was my first published peer-reviewed paper, and was published in PLoS Medicine.\n\nIn spatial epidemiology, are you using data science to look at the ways in which health outcomes are affected on a micro-level (such as disease mapping and clustering), or is it more of a macro view (which areas are most likely to be affected)?\n\nMy research embraces a social ecological perspective, interrogating the impact of different \u201clevels\u201d on people\u2019s health. However, the majority of our work is at the community level, where we investigate how neighborhood characteristics can influence population health and health disparities in vulnerable populations predominantly in urban environments.\n\nCan you talk about the history of the field of spatial epidemiology and how it has evolved over time with advancements in statistics and data collection?\n\nSocial scientists have long recognized the salience of context in health. For instance, Louis Ren\u00e9 Villerm\u00e9, a noted French physician and statistician, studied neighborhood effects on health in Paris. In 1830, he published a paper examining mortality patterns in different Parisian neighborhoods, and found that observed differences in death rates were highly correlated with the degree of poverty in a given neighborhood. Sadly though, over the years, research has been primarily focused on biomedical individualism (i.e. individual-level factors). But there has been a resurgence of interest in context as it relates to health, which stems from an increasing appreciation for recognizing that a myriad macro-social factors are important to health.\n\nRecently, the field of spatial epidemiology has seen data collection shifts stemming from technological advancements and the use of newer statistical methods. My colleagues and I are using GPS technology to define more realistic views of neighborhood contexts called \u201cactivity space neighborhoods.\u201d My work has argued that, compared to static administrative boundaries \u2014 such as ZIP codes and census tracts \u2014 egocentric and GPS-defined neighborhoods are the best methods in defining neighborhood contexts.\n\nWhat sorts of public health problems are you trying to study?\n\nMy lab, the Spatial Epidemiology Lab (www.spatialepilab.org), employs a geospatial lens in studying health behaviors and outcomes, especially obesity, hypertension, type 2 diabetes, drug abuse, and HIV/AIDS.\n\nCan you give a couple of examples of specific projects?\n\nWith funding from NYU\u2019s Center for Drug Use and HIV Research, and My Brother\u2019s Keeper, my colleague Dr. DeMarc Hickson \u2014 from the Jackson State University School of Public Health \u2014 and I are currently conducting a study to examine the feasibility of obtaining GPS spatial behavior data among a sample of approximately 100 black men who have sex with men in metropolitan centers in the Southern United States.\n\nAlso, I recently received a National Institute for the Humanities award for a project that uses advanced GPS methods to understand how certain neighborhoods influence HIV outcomes in New York City. The study will use real-time geospatial methods to investigate mobility across neighborhoods and how this affects HIV risk among young men who have sex with men.\n\nWhen did you start to incorporate data science into your research?\n\nData science has been an increasingly important element to my research for some time now. In my studies of neighborhoods and health, for example, we have utilized novel data sources such as Walk Score, Grindr and electronic health records.\n\nHow are you using these data sources?\n\nWalk Score\u2019s web-based algorithm calculates a score of walkability based on distance to various categories of amenities (e.g. schools, stores, parks, and libraries). We\u2019ve documented associations between Walk Score and cardiometabolic outcomes. That is to say, living in a more walkable neighborhoods is associated with increased walking in neighborhoods, less body mass index, less waist circumference, less systolic blood pressure, less diastolic blood pressure and a lower resting heart rate.\n\nElectronic health records can be utilized to collect objectively measured clinical health data, as a way of correcting errors and biases associated with self-reported survey measures. Because electronic health records have address data, researchers can geocode that information to estimate neighborhood-level factors to link to clinical health outcomes. In a recent study, we examined the association of walkable built environment characteristics with body mass index (BMI) among a large sample of children and adolescents. We found that built environment characteristics that increase walkability were associated with a lower BMI.\n\nAdditionally, my work uses GPS devices and smartphones to examine social networks in neighborhoods. Geosocial-networking applications, such as the dating application Grindr, utilize GPS technologies to allow users to browse user profiles and facilitate connections between users based on physical proximity. Grindr is a commonly and widely used geosocial-networking smartphone application for sexual minority men to meet anonymous sexual partners, creating a new digital environment worthy of further investigation in studies of sexual risk behavior and substance use in sexual minority men. My recent work has been some of the first of its kind to utilize broadcast advertisements on Grindr to recruit participants and deliver surveys to assess risk behaviors of application users.\n\nWhat drew you to the CDS program at NYU?\n\nI was drawn to the CDS program at NYU in part because of its interdisciplinary commitment and perspective. The key to making advances in the world of public health really depends on different sectors and disciplines working together. For example, in my own research, I engage with colleagues trained in a wide range of disciplines \u2014 economics, engineering, statistics, medicine, geography, sociology and psychology \u2014 because each can bring unique knowledge and perspectives to the study, making the work stronger and more relevant."
    },
    {
        "url": "https://medium.com/center-for-data-science/data-scientists-wanted-844951623528",
        "title": "Data scientists wanted! \u2013 Center for Data Science \u2013",
        "text": "One of the most compelling reasons to enter into a Master of Data Science program is the breadth of networking opportunities that help students begin their professional or academic careers after graduation. Here at the Center for Data Science, we continually cultivate professional relationships with human resource representatives from companies looking to hire the next generation of data scientists.\n\nFor our final biweekly Roundtable Session of the semester, we invited: Pymetrics, Two Sigma Investments, Major League Baseball, First Derivatives, Verificient Technologies, MullenLowe Profero, and Spheryx. All of these companies are actively looking to hire Master of Data Science students upon graduation, and even have Summer internship opportunities available for students currently completing their degrees:\n\nPymetrics combines neuroscience and data science to help narrow your job search. By using online computer games to determine how a player thinks, and what their personality type is, Pymetrics uses an extensive backlog of data regarding how different personality types line up with different types of jobs. They even help customers get started on their career search once they\u2019ve determined the type of career that suits a customer\u2019s temperament and personality.\n\nTwo Sigma Investments is a hedge fund trying to take the guess work out of the financial industry by using artificial intelligence and machine learning to make crucial financial decisions. In addition to using a wide variety of data science techniques in their vetting and investment processes, Two Sigma is also one of the leading hedge funds to invest in the next wave of data science companies.\n\nMajor League Baseball (MLB) is the face of America\u2019s pastime, and now they\u2019re using data science to help expand their reach in America, and overseas. Whereas data science in sports used to be limited to tracking scores and statistics, MLB is now using data science to see which foreign markets have a demand for American baseball, and which cities in the United States could use an extra dose of America\u2019s favorite sport.\n\nFirst Derivatives designs global capital markets software that allows for high-frequency trading on international stock exchanges. They use data science to offer their clients peace of mind in the ever-changing securities market.\n\nVerificient Technologies is a company that partners with online educational institutions, and provides services ensuring that when a student logs on to take a test, their answers are not being influenced by any outside forces.\n\nMullenLowe Profero is a fully integrated digital marketing agency, that offers everything from creative services to technology strategy. They use data science to segment any market that they are working in, and ensure that brands are marketing to the right consumer.\n\nSpheryx offers a range of data science-based products that can monitor food, medicine, and almost any other consumer product on a microscopic level, which ensures safety, quality, and affordability."
    },
    {
        "url": "https://medium.com/center-for-data-science/teaching-computers-to-see-with-data-36245f34fecb",
        "title": "Teaching computers to \u201csee\u201d with data \u2013 Center for Data Science \u2013",
        "text": "Kyunghyun Cho is an Assistant Professor at NYU\u2019s Center for Data Science, and conducts research in the field of natural language processing. His paper \u201cShow, Attend and Tell: Neural Image Caption Generation with Visual Attention\u201d introduces a machine learning based model that can describe the contents of images.\n\nCan you give us a bit of background on why you choose to look into the subject of image description?\n\nOne big question in the field of machine learning and artificial intelligence research is whether there exists a single, generic learning mechanism that can work with any type of data and task. Can we build an artificial neural network that works both on text and images? Can the deep convolutional neural network \u2014 which is widely used in object recognition \u2014 also work well with natural language text? These questions motivate much of my research.\n\nBefore I came to NYU, I was conducting research at the University of Montreal, and my colleagues and I had developed a machine system that could translate a sentence into a target language. So then, I wondered: are the properties of neural machine translation generic enough to work on other types of problems? We noticed that, to an artificial neural network, an image is not too different from a natural language sentence, once it\u2019s transformed into a real-valued continuous representation: they are both a bunch of numbers! This naive but important realization motivated me to work on translating an image to its description.\n\nWas the task to be able to identify a single object, or was the task to figure out how objects stand in relation to each-other?\n\nThe goal was to generate a natural language description that would describe how parts of an image stand in relation to each other, as opposed to tagging which objects are in the image. Imagine an image where a dog is chasing a cat. Object detection will return a set of objects \u2014 the dog and the cat \u2014 but image caption generation will tell you that the dog is chasing the cat.\n\nWhat are some of the practical applications for having computers being able to describe an image?\n\nThis sort of technology could be a new era for blind people. If you had this technology embedded into something like Google Glass, the image description system could tell a blind user what is going on in front of them.\n\nAt any point in your research did your objectives change? Were there any assumptions you had going in that had to be adjusted once you started to analyze the data?\n\nWhen working on this kind of short-term research project, the first thing I do is to set my expectations for the outcome. What do I expect to learn from this project? How will this let me make another step toward the ultimate goal of understanding intelligence? In this case, my expectation was that a neural network can automatically figure out a sequence in which different parts of a given image are described. With this expectation, my colleagues and I began our experiments.\n\nAs with any scientific discipline, we began by designing a model based on our observations of the phenomenon; in this case, we were looking at how humans describe images. After the first model was built, we evaluated it to see how well it met our expectations. That greater understanding led to an improved model, and that cycle continued.\n\nWhat were the findings of your paper?\n\nWe found that an artificial neural network is able to extract complicated, underlying structures across multiple modalities (in our case, an image and natural language text.) Now we\u2019re wondering how far we can push this: what is the limit on the number of modalities one network can handle? What kind of structures can a network find, other than simple alignment between a word and an object?\n\nI would imagine that the next step for this sort of technology would be to analyze videos. What sort of leaps in technology are needed to get to that point?\n\nThere are a lot of leaps in technology needed until we can generate a full-on video description. The first problem is a computational issue. Even a high resolution image is is manageable, but this is not true for videos, as a video can consist of hundreds of thousands if not millions of images.\n\nThe other problem is that image description relies on supervised learning, where annotations are available for each image. To use the dog and cat example, at this point we have to tell the model where the dog is and where the cat is, before the model can tell us that the dog is chasing the cat. We have plenty of images that have been previously annotated, but previously annotated videos are harder to come by.\n\nTo solve this problem of creating annotations, I think there is a lot of potential in the fields of semi-supervised and unsupervised learning. In unsupervised learning, we aim at building a machine learning model that can learn without strong supervision, and would be able to create those annotations."
    },
    {
        "url": "https://medium.com/center-for-data-science/on-learning-and-loving-data-science-95469fb6ed53",
        "title": "On learning \u2014 and loving \u2014 data science \u2013 Center for Data Science \u2013",
        "text": "Junbo (Jake) Zhao is a MS in Data Science student from the class of \u201816. Prior to studying with us, Jake received a Bachelor\u2019s degree in Electronic Engineering. He also has experience in Computer Vision and Music Information Retrieval. Jake is primarily interested in Large Scale Machine Learning and Big Data.\n\nI was born in Beijing, China, and graduated from Wuhan University in electrical engineering. On my sophomore year, I got the chance to work as a research assistant at the signal processing lab \u2014 my main project was pedestrian detection, where I first learned about, and fell in love with, machine learning . Then, as a senior student, I got to work in the top university in China, Tsinghua University, on a face recognition project. I also did my first internship at Douban Inc., working with music information retrieval.\n\nWith those experiences under my belt, I got into the Center for Data Science at NYU. Soon after I landed at New York, I started seeking out research opportunities. I reached out to Ross Goroshin, a student of Professor Yann LeCun who graduated last year (2015). He was very nice, and spent several hours talking to me about his project. Ross has a research interest in auto-encoders \u2014 which is one particular family of models in deep learning I also happen to be interested on, since I implemented and experimented on such models back on the internship at Douban Inc.\n\nNot long after I got a desk at the CIVLR lab open space, I worked on a project, \u201cStacked What-Where Auto-encoders\u201d, a model that tried to provide a generic architecture able to unify different learning modalities. Joint with Michael Mathieu, Ross Goroshin and Professor LeCun, we got the paper accepted into an ICLR 2016 workshop presentation.\n\nI also had the opportunity to collaborate with another Professor LeCun\u2019s students, Xiang Zhang, on a NLP project. Xiang had the idea of using convolutional neural network on document classification based upon a character-only representation of the corpus. I tried to compare his scheme to the traditional NLP approaches. We got the paper into NIPS 2015.\n\nNIPS is one of the top-tier conferences in the realm of machine learning. Luckily, I got the opportunity to participate, with the poster presentation of our paper with Xiang Zhang and Prof. LeCun. It was an amazing opportunity to discuss ideas and brainstorm with lots of well-known researchers and professors. It also helped us find people who have common research interests and could initiate some collaborations. The diversity of thinking on similar topics in machine learning was one of things that impressed me the most \u2014 it really gave me new perspectives.\n\nAll in all, I would say my experience at NYU\u2019s Center for Data Science has proved to be a solid next step in my career. Not only is the course structure outstanding from a learning point, but the opportunities for research, collaboration and industry exposure make this school the best experience I could have as a data scientist right now."
    },
    {
        "url": "https://medium.com/center-for-data-science/understanding-policy-making-through-wikileaks-with-data-science-514458d843f5",
        "title": "Understanding policy making through Wikileaks \u2014 with data science",
        "text": "Communications between embassies, government entities, and diplomats take the form of classified diplomatic cables. In 2010, over 150,000 of these cables were released by Wikileaks, a nonprofit organization that publishes classified government documents. The effect of the leak was twofold; not only did previously secret information become readily available, but now, the general population could glimpse into the inter-workings of diplomacy.\n\nLast year, Arthur Spirling, an Associate Professor of Politics and Data Science at New York University, co-authored a paper titled, \u201cDimensions of Diplomacy,\u201d regarding his research on these Wikileaks cables. We got the chance to ask him a few questions about his research, his findings, and the nature of governmental secrecy.\n\nCan you give us a bit of background on why you choose to look into the Wikileaks cables? What sort of information or trends did you go in looking for?\n\nMy coauthor, Michael Gill, and I were interested in the idea of private information in the realm of international relations. Scholars believe that presidents, prime ministers and other policy makers have data about the world that they don\u2019t make public, and so this information is, by definition, hard to obtain and study.\n\nDocuments pertaining to international affairs are declassified from time to time, and you can get a sense of what policy makers were thinking in a given crisis, but that\u2019s not true of all cables they send and read (the most secret ones stay secret!). Plus, we don\u2019t generally have such information for the most recent periods of history \u2014 e.g. US involvement in Iraq.\n\nThe Wikileaks cables contained recent communications, some of which were at a relatively high level of confidentiality. As a result, they represent an unusual opportunity to get a sense of how policy makers think about their world.\n\nIn your paper, you talked about how diplomatic cables are so hard to theorize, largely because we, the general public, do not even know how secrecy works in diplomacy. Could you talk about your findings regarding the mechanics of secrecy?\n\nWhen we looked at the nature of secrecy, we saw that, in these cables at least, there are two types. First, and perhaps most naturally, diplomats keep capabilities secret. That is, information about military matters is reserved for those with high security clearances.\n\nBut they also keep \u2018procedural\u2019 matters secret too. That is, information pertaining to the everyday nature of diplomatic efforts, which involves meeting ministers and other political contacts, hearing their demands, and explaining the US position. This may be partly to protect their sources, but it may also be a general international diplomatic norm to allow reputations for honesty and integrity to be built up: you are more likely to reveal what you want and know in the long term if you believe it won\u2019t be shared beyond the ambassador you are talking to. So, even quite banal things may be kept secret (because you expect bigger secrets to be revealed down the line).\n\nCould you talk about your findings in terms of how governments go about obtaining information or how governments go about keeping it classified?\n\nWe don\u2019t really have much to say about how the US government runs its classification regime. We know, from our work, what it believes is important to keep secret. We don\u2019t know, however, how it actually enforces this \u201con the ground\u201d\n\nHow did you go about grouping and categorizing the cables? I would imagine that the documents didn\u2019t have categorizations when they were leaked, is that correct?\n\nActually, the documents are pre-categorized by the US government in terms of subject matter. There are a large number of \u2018TAGS\u2019 which diplomatic staff apply to a cable so that receivers know what it is about substantively.\n\nCan you talk about how you went about looking through this huge number of documents? What were the data science methods that you used, and what variables or pieces of data were you looking for?\n\nWe used some python scripts to \u201cclean\u201d the cables, and we used text analysis methods \u2014 such as topic models \u2014 to get a sense of what was in them. One of our important steps was to obtain a \u201cbalanced\u201d sample \u2014 meaning a sample in which the cables generally dealt with similar topics, but differed by security level \u2014 early on. That meant our inferences about what makes for a more secret cable could be sharper, and it also meant the problem was \u2018smaller\u2019 and much easier to handle than the original 250,000 documents.\n\nAt any point in your research did your objectives change? Were there any assumptions you had going in that had to be adjusted once you started to analyze the data?\n\nFrom seeing the media reaction, we initially assumed that pretty much \u2018everything\u2019 that could have been accessed by Chelsea Manning was released. We now know this is false. Unless some embassies just didn\u2019t send cables in certain months, there was just no way that the leak was \u201ccomplete\u201d in terms of coverage. This observation lead to our first paper on the leak. We also noticed that the cables were pretty unbalanced in terms of location, subject matter and date. That is, the leak contains many more cables about certain issues in certain places than others.\n\nWikileaks was obviously an illegal leak. Did that pose any sort of problem in your research?\n\nNot directly. We, of course, took legal advice from Harvard General Counsel about publishing our work. Oxford University Press \u2014 the publisher of the relevant journal \u2014 also looked into the matter. In all cases, it was felt that our research would not cause legal problems for us as authors, the university or the journal. To be clear, neither my coauthor nor me wanted to \u201ccause trouble\u201d. We made efforts to use the cables responsibly by, for example, never looking for \u2018real names\u2019 or attempting to uncover the specific content of sensitive conversations. Ultimately, we want to understand how policy-makers think about secrecy and international relations \u2014 nothing more."
    },
    {
        "url": "https://medium.com/center-for-data-science/exploring-child-health-with-data-science-d1894e3d7718",
        "title": "Exploring child health with data science \u2013 Center for Data Science \u2013",
        "text": "Jan Blustein is a Professor of Health Policy and Medicine at NYU\u2019s Wagner School of Public Service, and an Affiliated faculty member at the Center for Data Science.\n\nHer work takes a data-driven approach to studying the intersection of social science & policy with early childhood development.\n\nWhat did you study in school? How did you get to what you study now?\n\nI studied medicine, but I\u2019ve always been interested in social science and policy. After I finished medical school, I worked in government, and then eventually made it to NYU, where I got a Ph.D. at the Wagner School.\n\nWhen did you start to incorporate data science into your research?\n\nI\u2019ve always been interested empirical work rather than theory \u2014 and that kind of work requires data.\n\nCould you tell me about some of the projects you\u2019re working on?\n\nI have a couple of projects right now. One is on cesarean delivery, and its consequences for child health. With colleagues at NYU medical school and Peking University, I\u2019ve just completed a study of cesarean delivery in China, and we found that the number of cesareans was considerably lower than previously thought, however, the geographic variations were remarkable. That study is based on over 1 million births!\n\nHow has the way in which you use data science in the field of public health changed over the years?\n\nThe size of datasets used to be a big issue, but it no longer is.\n\nCould you talk about how you\u2019re using data science to look at correlations between perinatal exposures and child chronic disease?\n\nMuch of my epidemiologic work draws on surveys that follow subjects over time. For instance, along with colleagues at NYU medical school (Leo Trasande, Martin Blaser and Teresa Attina), I looked at correlations between exposure to antibiotics early in life, and later life obesity. We analyzed a British study that followed ca. 12,000 children from birth to age 21 years. We found that early life (0\u20136 months) exposure to antibiotics was associated with a higher risk of obesity later. We did not find similar associations for exposure to antibiotics later in early childhood (6\u201312 months, 12\u201318 months).\n\nWhile the epidemiological work was correlational, it is consistent with controlled evidence from Dr. Blaser\u2019s laboratory, where mice exposed to antibiotics in the days after birth are at greater risk for adiposity in later life. Mice exposed later in early childhood do not have the same elevated risk.\n\nThis work (along with other evidence) suggests that in very early life the bacterial ecology is especially fragile. Perhaps we should be especially careful in using antibiotics with very young children. This is an area of active research among clinicians, bench scientists, and public health researchers.\n\nAre there any areas of your field that aren\u2019t being impacted by data science that should be?\n\nI\u2019m sure there are. There\u2019s no shortage of data out there. The real challenge I think is not so much the technical challenge, as the imperative to analyze thoughtfully and knowledgeably."
    },
    {
        "url": "https://medium.com/center-for-data-science/participating-on-the-moore-sloan-data-science-environment-summit-f4b95d88ab59",
        "title": "Participating on the Moore-Sloan Data Science Environment Summit",
        "text": "The Moore-Sloan Data Science Environment Summit is an annual meeting between the data science centers at New York University, the University of California at Berkley and the University of Washington. We spoke with Daniela Huppenkothen, a Data Science Fellow at NYU\u2019s Center for Data Science about her experience at the 2015 summit.\n\nCan you tell us about your work and experience as a Data Science Fellow at NYU?\n\nI spend my time working at the Center for Data Science and the Center for Cosmology and Particle Physics, where I\u2019ve been studying statistical and machine learning methods in Astronomy. At NYU, I found a combination of experts in data science methodologies, and domain experts in Astronomy and Physics.\n\nWhat other activities or events are you involved with in the data science community?\n\nA while back, we organized a workshop called Astro Hack Week, where we had tutorials from a wide range of domain specialists. One of those presentations was actually from another CDS Fellow, Andreas Mueller, who gave a presentation on Scikit Learn. These lectures were focused on teaching the essential skills for working on large astronomical data sets.\n\nCan you talk about your involvement in the Moore-Sloan Data Science Environment Summit?\n\nThe summit is an annual gathering of data scientists from the three universities within the Moore-Sloan initiative. There were talks about people\u2019s research, discussion sessions, tutorials, and mixer events where people discussed each other\u2019s work.\n\nDid you present something at the summit last year?\n\nI gave a presentation on a tool Brian McFee and I came up with for Astro Hack Week. We had to select the participants from a huge number of applications, so Brian and I came up with a tool that would select applicants based on a certain criteria. This was a complex optimization problem to solve because of the varied criteria we had to include. This seemed to be an interesting project to present and it also gave us an opportunity to get some feedback on our work.\n\nWhat were some of the highlights from the summit?\n\nThere was a big data tutorial, which helped me clarify my perspective on the subject. Also, it was great to meet and talk personally with people who were doing such interesting work and research. There were quite a few astronomers, in fact some of them were those who came to the Astro Hack Week.\n\nWas the summit mostly attended by data scientists? Or were some of the attendees domain experts?\n\nActually, most of the people were domain experts from different areas of research, and all of the researchers have some domain specific questions in mind. So basically, everyone there was a Data Scientist, while many of them being experts in their domain."
    },
    {
        "url": "https://medium.com/center-for-data-science/finding-the-best-data-science-job-a5500e3c7408",
        "title": "Finding the best data science job \u2013 Center for Data Science \u2013",
        "text": "One of the most exciting aspects of pursuing an advanced degree in the study of data science is the breadth of career opportunities available upon graduation. From nonprofits to private banking, NYU\u2019s Master of Data Science program not only gives its students the knowledge and tools to begin a career in the world of data science, our program also gives students the connections and access to top companies looking to hire the next generation of data scientists. Our career information sessions with HR representatives from companies such as Apple, Intel, American Express and Capital One allow students to see how their personal interests can be combined with their knowledge of data science to find a post-graduation job to fit their personal and career goals.\n\nThe United States Government is one of the top employers of graduates in the field of data science, and this past fall, we welcomed The New York Fire Department and the Central Intelligence Agency to come and talk with out students. From national security initiatives to ensuring safety in our local communities, data scientists are helping the United States government serve its citizens through cost-saving measures, increased efficiency, and more accurate outcomes. The public sector is one of the fastest growing opportunities for young data scientists, as an increasing amount of public funding becomes available to make way for data science opportunities in both local and national governmental organizations.\n\nFinance has often been characterized as a field dominated by luck and insider knowledge. Innovations in data science are finally beginning to take the guesswork out of the financial markets, and the result is an increased demand for business savvy data science students. Like any industry, finance has a wide breadth of specializations and work environments, from Wall Street banking firms, to cutting edge venture capital groups. This past fall, CDS welcomed several financial sector leaders, from traditional banking organizations like JP Morgan/Chase, and American Express, to cutting edge venture capital firms such as Blumberg Capital, and the world\u2019s largest asset manager, BlackRock. These firms all specialize in different aspects of the financial market, but are united in their need for data scientists to help them pioneer new data collection methods and modes of analysis.\n\nAnd what would data science be without traditional technology firms? At the Center for Data Science, we continue to use our partnerships within the larger tech community to welcome industry leaders to discuss career opportunities with our students. From Apple to Intel, traditional technology companies have been innovators and proponents within the field of Data Science, and are one of the biggest opportunities for CDS students upon graduation.\n\nYou can find a full list of our career information sessions from the Fall 2015 semester below:"
    },
    {
        "url": "https://medium.com/center-for-data-science/what-do-the-brain-data-science-and-an-emmy-have-in-common-2fc6ac4bc6b0",
        "title": "What do the brain, data science and an Emmy have in common?",
        "text": "Eero Simoncelli is a faculty member at the Center for Data Science, NYU\u2019s Center for Neural Science, and a fellow at the Institute of Electrical and Electronics Engineers. His research spans several fields, including medicine, neural sciences, mathematics, & psychology. This past fall, he received an Engineering Emmy award for his work in computational vision.\n\nWhat did you study in school? How did you get to what you study now?\n\nI had this notion in 8th grade that I wanted to understand how the brain worked, and by the time I finished high school, I was pretty set on going in this direction. As a Physics undergraduate, I worked in a \u201cpsycho-biology\u201d lab that was looking for reward pathways in the rat brain, and then in a bio-engineering lab that studied locomotion and neural response dynamics. And as a graduate student, while most of my courses were Electrical Engineering, I was also learning about visual perception and physiology. When I started my faculty job at the University of Pennsylvania, I was working on computer and biological vision problems in parallel, looking for common principles that would serve as a framework for both.\n\nWhen did you start to incorporate data science into your research?\n\nMy research has always involved data, whether from humans, animals, real-world signals (like images, or sounds) or computer simulations of models, so I guess this means it\u2019s always been a form of data science, in a way.\n\nCould you tell me about some of the projects you\u2019re working on?\n\nI\u2019m pretty excited about some recent work aimed at hierarchical processing of visual inputs. This is a bit like the deep networks that have become so popular in recent years, but we\u2019re not trying to learn them from labelled data. Rather, they are engineered using constraints from the statistical properties of natural images, as well as our knowledge of response properties of neurons in the human visual system.\n\nYou\u2019re associated with a number of centers in different fields, including medicine, neural sciences, mathematics, and psychology. Is there a thread or similarity that runs throughout your work in all these different fields?\n\nYes \u2014 the labels of traditional fields don\u2019t really capture what we do in my group. The \u201cglue\u201d that holds it all together is that we are interested in common principles underlying the representation of visual information, whether in biological or machine systems.\n\nHow has the way in which you use data science in neural science changed over the years?\n\nIn my early work, I tested principles and predictions of theories by running simulations, and comparing these to existing data. But more and more, our work involves flipping this around: We make a prediction based on a particular theory or model, we run experiments (either ourselves, or in collaboration with experimental colleagues) to gather data that will constrain such a model, and then develop methods to fit models to those data and interpret the outcome. So there\u2019s more emphasis on development of proper constraints and parameterization that will allow optimization.\n\nWhat do you think is the single biggest affect that data science has had on neural science, or how we approach the field?\n\nThe importance of data science in neural science is growing rapidly, largely in response to huge changes in the technologies by which we measure brain activity. Fifty years ago, this was mostly limited to recordings of single cells, using single electrodes, and EEG measured through the scalp. Over the past 25 years, functional MRI, high-density multi-electrodes, and various kinds of specialized imaging and microscopy have all provided dramatic new measurement capabilities, rapidly outstripping our expertise and abilities to analyze and interpret the data. This is partly a computational issue (there\u2019s too much data, and it is very high-dimensional), but it\u2019s also conceptual: we need more sophisticated hypotheses and models for large scale neural activity. I think this theme can be found in many data-rich fields.\n\nAre there any areas of neural science that aren\u2019t being impacted by data science that should be?\n\nI\u2019d say that it\u2019s crucial that a curriculum for training researchers in Data Science include not just methods/algorithms for handling and analyzing large amounts of data, but also tools for developing models, testing and comparing them, and interpreting and reasoning about the hypotheses from which they arise.\n\nCan you talk about the Primetime Emmy Engineering award? How does your work in neural sciences fit into the work you did for the award?\n\nAbout 12 years ago, I had an extremely talented postdoctoral fellow, Zhou Wang, in my group who was working on a method of assessing the quality of a photographic image. The goal was to do this in a way that matched what a human would report, which is quite different than the \u201cmean squared error\u201d that underlies most engineering. This problem of photographic image quality had been studied for more than a decade, but comparative tests showed that various methods that had been proposed did not offer significant improvements. Zhou had started working on this problem as a graduate student at U.T. Austin with his advisor Al Bovik. When Zhou arrived at NYU, we set about improving it, incorporating some ideas about how the human visual system analyzes and represents visual images, as well as some new methods of experimentally testing how well our quality measure captured human perception. We wrote up a thorough account in a paper published in 2004, documenting a substantial improvement over mean squared error, and the method took off \u2014 more than 10 years later, it\u2019s still the most widely used method for assessing image quality (although we and others have developed solutions that are better!). A few years ago, the television industry started using it to test the qualify of broadcast video, and turned out to be quite useful\u2026 I had no idea it was even being used in this context, and so was quite surprised to hear we had won the award!\n\nWhat drew you to the CDS program at NYU?\n\nI\u2019ve always felt the scientific world needs a better infrastructure for analyzing, testing, reasoning about data. This comes traditionally from the field of statistics \u2014 for example, many science departments require their students to take a course or two in statistics for analysis and presentation of data. But I think it\u2019s abundantly clear that this traditional curriculum needs to be updated to prepare students to handle larger and more complex data sets, using more sophisticated mathematical computational methods. I\u2019ve incorporated some of these things into my courses in Neural Science, but I think the issue cuts across many departments. I never knew what to call this new unified collection of topics, but I suppose \u201cdata science\u201d is a good a name as any, and I\u2019m happy to be participating in building it at NYU!"
    },
    {
        "url": "https://medium.com/center-for-data-science/from-patterns-and-predictions-to-data-science-87d813be69d9",
        "title": "From patterns and predictions to data science \u2013 Center for Data Science \u2013",
        "text": "Vasant Dhar is a professor at the Stern School of Business and the Center for Data Science at the NYU. He is the one who defined the field of Data Science himself \u2014 you can find his definition here, and a few questions we asked him about his work below.\n\nWhat did you study in school? How did you get to what you study now?\n\nI studied Chemical Engineering at the Indian Institute of Technology, Delhi before doing my graduate work at the University of Pittsburgh, where I did my thesis work in Artificial Intelligence back in the 1980\u2019s, when the field was mostly concerned with knowledge representation, search, and \u201creasoning.\u201d\n\nHow did you wind up getting into machine learning?\n\nI got into machine learning in 1990 thanks to a project with Nielsen. I was exploring genetic algorithms for rule discovery and one of my first discoveries was that \u201colder women in the northeast did a lot of shopping on Thursdays!\u201d This turned out to be the coupon day back then so there was a simple explanation for what I had found, but I was fascinated that my algorithm had managed to ask an interesting question during its exploration, namely, \u201cwhat possible reason could there be for this surge in spending on Thursdays?\u201d All my subsequent research had this flavor to it, which I described as \u201cpatterns emerge before reasons for them become apparent\u201d.\n\nWhen did you start to incorporate data science into your research?\n\nI started getting into \u201cbig data\u201d in 1991, after my Nielsen project. At that time, banks and telecomm companies were the ones with all the data, so I took a few years off and went to work on Wall Street, where I developed machine learning based approaches to market prediction, customer behavior, and risk management.\n\nCould you tell me about some of the projects you\u2019re currently working on?\n\nMy longest standing project has been building a robot that uses data to develop a trading strategy, and then executes. Financial markets are about as close to random as you can get, so designing good predictive systems is a real challenge. When you\u2019re looking at profits and losses every day, it forces you to keep the science rigorous, simple, and honest. In finance, good science doesn\u2019t assure success, but bad science does assure failure.\n\nWhat do you think is the single biggest affect that data science has had on the field of predictive analytics?\n\nI think it\u2019s more that prediction has had a bigger affect on data science, not the other way around. Data Science and Prediction go hand in hand, as prediction is a key epistemic criterion that should be used in data science to assess whether something should count as knowledge.\n\nHow has the way in which you use data science changed over the years?\n\nMy questions have broadened as I integrate my findings from across the above areas, namely, finance, healthcare, sports, and education. Working in multiple domains over the last 20 years has given me a good appreciation of the noise levels in these respective areas and how it impacts the properties of the predictive system that one is trying to build. This comparative view has enabled me to ask a very interesting question that is central to data science, namely, \u201cwhen should we trust robots with decisions?\u201d"
    },
    {
        "url": "https://medium.com/center-for-data-science/falling-in-love-with-data-496367bdddef",
        "title": "Falling in love with data \u2013 Center for Data Science \u2013",
        "text": "Alexandra Simonoff joined the Master of Science in Data Science program in Fall 2015. Prior to this, Alex received her Bachelors degree from NYU in Mathematics, graduating with a University Honors Scholar Award. After graduating in 2013 she began working at AIG, first in Investment Risk Management for a year and currently as a Data Scientist since August 2014. She is interested in data mining, predictive modeling and sports analytics, specially their application in professional hockey.\n\nI was raised on Long Island, New York, and I\u2019ve lived in the NYC metropolitan area my entire life. My mother worked in banking before my brother and I were born, and my father is a statistician and professor, so I was raised to love numbers and data. I majored in Mathematics and minored in Computer Programming and Web Applications at NYU, graduating with my degree in May 2013. After interning at AIG, a property casualty insurance company, the summer before graduating, I took a full time offer to work in their Investment Risk Management group (specifically working in Market Risk Analytics) beginning July 2013.\n\nAfter speaking with peers in AIG\u2019s Science department and taking statistics and data mining courses in school, I found myself enamored with data science and using data to uncover business trends. After a year in risk management I transferred internally to AIG\u2019s Science department to work as a data scientist. I found I really enjoyed using data to help AIG and it\u2019s various business units succeed. People welcomed innovation and change, and being behind several transformational efforts made me feel like I was changing people\u2019s lives (at least their work life, and that\u2019s a pretty significant thing).\n\nIn order to become better at my job and learn more about the world of data science, as well as stay on top of the latest developments, I decided to pursue a Master\u2019s in Data Science. I\u2019m very interested in developing predictive models, especially by ways of machine learning. Another field I am fascinated with is sports analytics; I competed on NYU\u2019s Varsity Track and Field team through college and after graduating began playing ice hockey.\n\nMy (albeit short) experience at the NYU\u2019s Center for Data Science has been everything I hoped it would; I have met brilliant people from all over the world with dozens of different backgrounds, all who share the same passion of statistics and data. The professors I got a chance to learn from are leaders in their industries and give us tools to help us succeed wherever we ultimately take our knowledge. I am happy to work every day alongside passionate classmates with very diverse backgrounds!"
    },
    {
        "url": "https://medium.com/center-for-data-science/how-education-and-data-can-fight-inequality-dbbd075965f9",
        "title": "How education \u2014 and data \u2014 can fight inequality \u2013 Center for Data Science \u2013",
        "text": "Benjamin Jakubowski is a first year student at the NYU\u2019S Data Science MS program. After receiving his BA in mathematics and chemistry from Oberlin College in 2011, he served as an AmeriCorps*VISTA volunteer in Baltimore and taught 8th grade science in District of Columbia Public Schools. He is interested in helping government and non-profit organizations use data-driven problem solving to promote social good.\n\nThe path that led me to NYU\u2019s MS in Data Science program is somewhat circuitous. In 2007, I graduated from Oberlin College with a degree in mathematics and chemistry. While in college, I was interested in epidemiology and public health, and during my senior year I strongly considered immediately pursuing graduate study in biostatistics. Instead, I decided to defer graduate study in favor of spending a year in service as an AmeriCorps*VISTA volunteer. Thus, after graduation I packed up and moved to Baltimore to begin managing a residential energy efficiency program through the Neighborhood Design Center.\n\nMy time in Baltimore unexpectedly led me even further from graduate study. During my VISTA year, I found myself searching for opportunities to deepen my impact on problems of inequality; while my VISTA project offered real benefits to participating households, I wanted work closer to the root causes of poverty and inequality than high energy bills. At the time I was also volunteering as an after school mentor with Higher Achievement, spending one evening a week working on math and English lessons with three fifth-grade students. I began feeling my tutoring was having greater impact than my VISTA project, so I started exploring pathways to teaching. Ultimately this led me to Washington D.C., where I spent three years teaching 8thgrade science.\n\nThe time I spent teaching was extremely rewarding, and definitely allowed me have a real impact on the lives of my students and on my community. However, after three years, I decided to head back to school. Through my time as a VISTA and as a teacher, I\u2019d retained a real passion for data \u2014 I spent most of one summer working through John\u2019s Hopkins\u2019 Data Science Coursera offerings \u2014 but I hadn\u2019t had many opportunities to apply this passion professionally. Additionally, throughout my time teaching, I had seen data be used in both highly informative and (in my opinion) ill-considered ways across our education system. I decided I wanted to contribute to the project of using data to promote public wellness and address inequality, both in the education system and more broadly.\n\nThus, I began exploring options for graduate study. I primarily looked at traditional statistics programs, but I was intrigued by the NYU\u2019s Data Science MS program. Ultimately, I chose this program because I felt it was unique (relative to more traditional/less interdisciplinary programs) in offering training covering the entire data pipeline. Between the core and elective coursework, I believe I\u2019ll be able to build a deep toolkit for getting and cleaning data, conducting exploratory analyses, training and validating models, and building powerful visualizations.\n\nMy experience to date has definitely validated this choice. The program has given me the opportunity to simultaneously explore the theoretical underpinnings of statistical learning techniques, and gain hands-on experience getting, cleaning, and applying these techniques on real datasets. I\u2019ve also been able to pursue several independent and group projects that have made me more passionate about using open data to increase government transparency and promote social good. I\u2019ve enjoyed my experience in the MSDS program, and I\u2019m excited to spend three more semesters expanding my data science toolkit.\n\nOn a final note, if you\u2019re reading because you\u2019re potentially interested in applying to the MSDS program, here are two suggestions. First, if you\u2019re still in school, build a solid mathematical foundation \u2014 as an MSDS student I\u2019ve felt grateful to my undergrad math professors for solidly preparing me for this type of graduate study. Second, think about the types of problems you\u2019d like to work on \u2014 entering the program with a strong interest in open civic data has really helped me define the objective and scope of my independent projects."
    },
    {
        "url": "https://medium.com/center-for-data-science/faculty-profile-juan-pablo-bello-d4d3cb483306",
        "title": "Music and lyrics \u2014 and data \u2013 Center for Data Science \u2013",
        "text": "Juan Bello is an Associate Professor of Music Technology in New York University\u2019s Steinhardt Program, and an Affiliated Faculty member at the Center for Data Science. He is the co-founder of the Music and Audio Research Lab (MARL) and his research focuses on digital signal processing, machine listening and music information retrieval.\n\nWhat did you study in school?\n\nI played music from a young age, but always had an interest in mathematics and technology. My dream career was to be an audio engineer, even a producer, but those types of college courses were not offered in my home country of Venezuela, so I enrolled in the closest thing I could find: electronic engineering. I discovered an affinity for signal processing and programming, which led me to Undergraduate thesis work in sound synthesis.\n\nHow did you get to what you study now?\n\nI did my graduate studies in audio signal processing at Queen Mary, University of London. The lab I joined specialized in audio coding and digital effects, but was becoming more interested in music pattern analysis. I was fortunate enough to be around for the genesis of Music Information Retrieval as a field, and as a first-year PhD student, I attended the first conference ever on the subject. I haven\u2019t looked back since.\n\nWhat drew you to Music information retrieval?\n\nThe combination of engineering, computer science and music was hard to resist. And at the time, it was a totally wide open field. People had worked on speech processing, and there have been decades of work in computer music, but using computers for music analysis was rare. There was ample space for young doctoral students to cut their teeth on and explore.\n\nCould you tell me about some of the projects you\u2019re currently working on?\n\nMy lab is focused on extracting high-level information from music audio. Recent efforts include techniques for automatic melody extraction, downbeat tracking, chord estimation, instrument classification and music structure analysis. Some of this work is focused on machine learning, such as how to design a framework for data augmentation is specific to audio and music data. All of this work is informed by domain expertise in music and sound.\n\nWhat are some of the real world implications of the research you\u2019re doing?\n\nWe\u2019re starting to pursue work in environmental sound analysis. One project, SONYC, seeks to combine sensor networks, machine listening and data science to monitor and identify patterns of noise pollution in New York City. Noise pollution has proven affects on health, education, and the economy, and is obviously a huge issue in most urban areas.\n\nHow has SONYC been received?\n\nWhat is surprising is the extent to which cities are limited in their ability to monitor compliance and enforce such regulatory frameworks. Our project offers an alternative, scalable solution based on smart sensing technologies that can empower the work of city agencies and other stakeholders. In fact, we are already building partnerships with various departments in the NYC government. This is joint work with researchers across NYU, including in Steinhardt, CDS, Tandon and CUSP, as well as in Ohio State University.\n\nCan you talk about the differences between working with environmental and musical sounds?\n\nMusic is often recorded in very controlled, clean conditions, with little interfering noise. Speech signals in most commercial applications rely on a high signal-to-noise radio. But environmental recordings are often full of background noise and the relative weakness of sources of interest.\n\nHow does your analysis of the two types of audio differ?\n\nEnvironmental sound analysis is a window into a real-world phenomena: birds migrating, traffic, construction, etc, that we aim to understand in order to gain scientific knowledge or solve a problem. Music on the other hand is a heavily designed sonic phenomenon, the result of a carefully concocted composition of sounds resulting in structures nowhere to be found in the natural acoustic world. Sound is a manifestation but not necessarily the object of study per se, but music is the end result, the object of study.\n\nWhat aspects of audio analysis have the largest untapped potential for work in the world of data science and machine learning?\n\nWe\u2019ve only scratched the surface thus far, but I think environmental sound analysis is the next big frontier. The space of possibilities is huge: sport analytics, bioacoustics, intelligent homes, security & surveillance, failure detection in machinery, all the way to understanding food ingestion habits. Sound is a rich source of information about the world that can be sensed cheaply, constantly and without \u201cfield-of-vision\u201d limitations, and the widespread availability of mobile and remote sensing devices will only increase the amount of data available.\n\nWhat drew you to the CDS program at NYU?\n\nThe key motivation was becoming part of a heterogeneous community of scholars with shared interests and complementary expertise. This in turn creates unique opportunities to learn from both the state of the art in core areas of data science such as data visualization, machine learning, and citizen science, as well as from new and exciting applications across disciplines as varied as finance, politics, cosmology and medicine. This process is already helping me build new collaborations, understand better what is common and can be transferred across disciplines, what is unique about my field, and how we can contribute back to the core and beyond. In the end, I find that diversity is perhaps the most precious asset of the CDS community. Plus, I want musicians to be involved.\n\nOn a chalk-board at CDS we have the question: \u201cWhat does it mean to be a data scientist?\u201d Could you answer this question?\n\nTo me, a data scientist is defined by pursuit of unlocking meaning from vast quantities of data, as opposed to the specific tools or path one uses to find an end result."
    },
    {
        "url": "https://medium.com/center-for-data-science/can-computational-text-analysis-lead-to-more-persuasive-politicians-f415f1f238ff",
        "title": "Can computational text analysis lead to more persuasive politicians?",
        "text": "Nick Beauchamp is an Assistant Professor in Northeastern University\u2019s department of Political Science. He recent spoke at the NYU Center for Data Science\u2019s Text as Data conference.\n\nWhat did you study in school? How did you get to what you study now?\n\nLike many data scientists, I took a somewhat winding path to arrive where I am now. Although I started college studying physics and math, I wound up double-majoring in philosophy and literature. I then went to grad school in English at Johns Hopkins, worked on a thesis that combined literature, philosophy of mind, and politics.\n\nHow did you get from English studies and politics to data science?\n\nI think because of my background in literature, I was immediately interested in computational text analysis: inferring ideology from speech and the dynamics of opinion change. After doing my work in English at Johns Hopkins, I went to the Carter Center to work on election observation and electoral fraud analysis. That led me back to grad school in politics with a focus on quantitative methods at NYU.\n\nHow do the methodologies in data science differ from the social or hard sciences?\n\nThe natural tendency in the sciences is towards specialization. We are in a rare \u2014 and perhaps brief \u2014 moment where one can work in a number of fields \u2014 computer science, statistics, a substantive field such as politics \u2014 and produce novel and important work. With the social or hard sciences, interdisciplinary synthesis is the exception more than the norm, and I hope the interdisciplinary aspect of data science continues.\n\nWhen did you start to incorporate data science into your research?\n\nDuring my graduate work at NYU, one of my first projects was analyzing congressional speech using computational methods to determine speaker ideology. But I already had some programming experience at that point.\n\nCould you tell me about some of the projects you\u2019re working on?\n\nI\u2019m currently working on a project that algorithmically generates more persuasive text by looking at how people shape their opinions through political rhetoric surrounding Obamacare. By analyzing participants\u2019 reactions to three sentence paragraphs, we can figure out which topics are the most persuasive, and look at ways in which the structuring of arguments affects how people respond. It not only gives us insight into what makes a convincing argument, but also how ideas shape political opinions.\n\nI\u2019ve also been modeling online arguments within political groups, which tend to be much more deliberate and persuasive than arguments between differing political ideologies. This allows us to discern mental connections between topics, how these mental connections take shape, and how one gradually shapes their ideology.\n\nThere are so many ways you can use data science to look at a problem in politics. Why does text analysis appeal to you?\n\nI\u2019m fundamentally interested in how people think about and argue politics. Ideology in politics has for decades been modeled as a simple left-right spectrum (or perhaps two dimensions), which seemed to me to deeply miss out on the full complexity of political thought \u2014 the ways ideas and beliefs interrelate and affect each other, and the ways we learn and change our beliefs. Words and text are really the only window into the full complexity of all of this, and after all, it is through words that we learn and change all of our beliefs. Text analysis lets us see into this incredibly complex psychological and social world, if we develop the right tools to model and understand it.\n\nHow has the way in which you incorporate data science into political science changed over the years?\n\nIn the \u201cold days\u201d I would develop all my tools from scratch. I still do a lot of that, but text analysis has become a booming industry in the last 5 years, which means there are now a lot of sophisticated models to build upon.\n\nWhat do you think is the single biggest affect that data science has had on political science, or how we approach political science?\n\nData science now allows us to build more complex models of social behavior, with richer psychological and social dimensions that speak to personality, ideas, and social movements beyond the left/right dichotomy.\n\nAre there any ways in which text analysis is being used differently in political science than in other fields?\n\nPolitical science has more sharply defined subject outcomes than other social sciences. But text analysis is also a bit hampered in political science by our emphasis on the unidimensional left-right dimension, which can sometimes limit the analysis and prevent models from capturing the richer complexity of speech.\n\nAre there any areas of your field that aren\u2019t being impacted by data science that should be?\n\nSo far the modeling side of our field, as in economics, tends to be dominated by analytic, game-theoretic models rather than computational or agent-based models. The real world of data science is extremely messy and it\u2019s important to have models that are analytically tractable and theoretically understandable. But it will also be interesting to see how more complex computational models will allow more interaction between the messy world of data and the theoretical world of modeling.\n\nFor you, what was the biggest takeaway from the Text as Data Conference?\n\nI thought there were a number of great papers focusing on long-standing fundamental problems in text analysis, such as n-grams and sentiment. But I want to see broader social theories incorporated into the computer-science work you see in natural language processing. There\u2019s lot of room for growth on the theory side \u2014 not modeling per se, but social theories that have ramifications beyond small-scale political domains.\n\nOn a chalk-board at CDS we have the question: \u201cWhat does it mean to be a data scientist?\u201d Could you answer this question?\n\nA data scientist is someone who finds theoretical and methodological connections across disciplines, and is ready to make new discoveries wherever she finds them. The surge in computation means that many of these discoveries are entirely novel, and are new insights into complex systems that could not have been made with previous methods."
    },
    {
        "url": "https://medium.com/center-for-data-science/using-statistical-physics-to-understand-society-afa822760e24",
        "title": "Using statistical physics to understand society \u2013 Center for Data Science \u2013",
        "text": "Bruno Goncalves is a Fellow at NYU\u2019s Center for Data Science, and a recent recipient of the CSS Junior Scientific Award. His interdisciplinary approach to social science combines data science with traditional research methods for a greater understanding of human behavior.\n\nWhat did you study in school?\n\nOriginally, I was a physicist with a dash of computer science. I spent my undergraduate days studying statistical physics, a branch of physics that focuses on understanding how microscopic behaviors result in macroscopic phenomena.\n\nHow does your background in physics apply to your work in the social sciences?\n\nFor me, the common denominator between physics and the social sciences has been connecting macro and micro elements to solve problems.\n\nWhen did you start to incorporate data science into your research?\n\nFrom my early days as a physicist, I would write simulations to generate data that I would analyze separately. However, the real transition to data science was during my doctoral studies. I started working on complex networks using the \u201craw\u201d data generated by web-servers to look at how users navigate through web pages.\n\nHow does data science connect with the projects you are currently working on?\n\nRight now I\u2019m using geolocated online data sets from sites like Wikipedia and Twitter to study how human mobility and social behavior are interrelated. And with some of the same data sets, I\u2019m using natural language processing tools to look at how the use of language changes regionally, and how language evolves over time.\n\nCan you tell me about your new book, Social Phenomena?\n\nI think the subtitle, \u201cFrom Data Analysis to Models\u201d not only expresses the idea behind the book, but also the guiding principles of my research. The main question being: how can we use the ever increasing availability of large scale online data sets, coupled with recent developments in data science, to model and understand individual and societal human behavior?\n\nWhat has been the biggest affect data science has had on the social sciences?\n\nData science allows researchers to harness large data sets on how we behave and interact; this gives us a holistic birds eye view of society. We\u2019re complimenting our work in data science with more traditional social science studies that rely on in depth interviews and surveys of smaller groups. Combining both perspectives (the macro and the micro) is paramount for the future development of social science as a whole.\n\nWhat drew you to the CDS program at NYU?\n\nThe richness and diversity of the program. Being part of an institution that hosts leading researchers from economics, politics, neuroscience, psychology, computer science, statistics, music, business, and beyond is a very exciting opportunity.\n\nThe Masters in Data Science program also has the advantage of attracting a lot bright students. I\u2019m extremely proud to be a part of CDS and hope to be able to contribute, in some small measure, to its success during my tenure.\n\nOn a chalk-board at CDS we have the question: \u201cWhat does it mean to be a data scientist?\u201d Could you answer this question?\n\nOver the past few years we\u2019ve seen a lot of hype surrounding \u201cBig Data\u201d and \u201cData Science\u201d so there are many ways to interpret this question. I prefer using unexplored and diverse data sets to answer larger societal and scientific questions, as opposed to only looking at data fitting a certain model."
    },
    {
        "url": "https://medium.com/center-for-data-science/tackling-problems-in-neuroscience-computer-vision-and-medical-imaging-809b4c87e993",
        "title": "Using data science to tackle problems in neuroscience, computer vision and medical imaging",
        "text": "Assistant Professor of Mathematics Carlos Fernandez-Granda has recently joined the faculty of the Courant Institute of Mathematical Science and the Center for Data Science and is currently teaching \u201cStatistical and Mathematical Methods,\u201d a required course for the MS in Data Science. A native of Madrid, Fernandez-Granda earned his Ph.D. in Electrical Engineering from Stanford University, his M.S. in Applied Mathematics (Machine Learning and Computer Vision) from \u00c9cole Normale Sup\u00e9rieure de Cachan (Paris) and engineering degrees from Universidad Polit\u00e9cnica de Madrid and \u00c9cole des Mines (Paris).\n\nThis fall, you\u2019re teaching an introductory course for the statistical and mathematical methods needed for data science. Are you enjoying it?\n\nYes, I\u2019m enjoying teaching very much. The point of the course is to give students a background in probability, statistics, linear algebra and optimization so they can understand more advanced algorithms in machine learning and data science. Most of the students are going for their master\u2019s so even though in the beginning the material is not that complicated, I try to treat it at a more rigorous level than what they have probably seen in undergrad. I give them a practical viewpoint of the subjects but also tell them about the more theoretical aspects so they will know both.\n\nI got really lucky when I was assigned this course because it combines some of the subjects that I like the most \u2014 specifically, probability and linear algebra. Also, a lot of my research is in optimization so it\u2019s a very good fit. I\u2019m writing my own notes because there isn\u2019t really a book that comprises all of the basic math you need for data science. I\u2019m really enjoying teaching this subject which is basic but can be quite tricky. I think it\u2019s important for students to get it right in order to understand more advanced algorithms.\n\nWhat interests you about probability, statistics, linear algebra and optimization?\n\nThey are all crucial to data science. Probability, I find, is a very useful way of dealing with uncertainty and partial knowledge in a principled way. Linear algebra allows us to build linear models with intuitive geometric interpretations. Statistics enables us to extract conclusions from data and examine the accuracy of those conclusions. Regarding optimization, maybe ten to twenty years ago, not a lot of people doing a master\u2019s in statistics would learn very much optimization but now it\u2019s becoming increasingly important, as it allows us to use more flexible and complex models. Students need to have strong foundations in each of these four areas in order to understand the different approaches to data science which they will be exposed to in the program.\n\nHow did your background prepare you for your current work?\n\nI grew up in Madrid; my father was an electrical engineer and my mother was a doctor. In high school, I was mainly interested in literature, but I ended up doing my undergrad in electrical engineering. In Spain, it\u2019s common for good students to go into electrical engineering because it\u2019s prestigious academically, like physics in Germany. Fortunately, I really liked the math and programming courses so it turned out alright. It also allowed me to do the last year and a half in Paris at l\u2019\u00c9cole des Mines, which was a great opportunity.\n\n\u00c9cole des Mines was mainly geared toward management or finance. Since I wanted to learn about machine learning, artificial intelligence and computer vision, I did a masters at \u00c9cole Normale at the same time. That\u2019s when I discovered that I really like these areas. Then I did my master\u2019s thesis at Philips in Germany on magnetic resonance imaging and really enjoyed that very much. I liked the application but realized I didn\u2019t know as much as I wanted to about the theoretical framework that underlies some of the algorithms, which is why I decided to go for a Ph.D.\n\nHow much do math and data science methodology vary from country to country?\n\nIn France, students tend to have a very mathematical background. Spain is less theoretical, probably due to a lack of tradition. I can\u2019t comment very much on Germany because when I was there, I was at a company doing research.\n\nRegarding American universities, I have met many researchers who are very applied but still have an interest in theory. At the same time, more theoretical researchers often have a good knowledge of applications. I find this very appealing.\n\nWhat was the subject of your Ph.D. at Stanford?\n\nBefore getting my Ph.D., I developed algorithms for magnetic resonance imaging based on optimization. I was very interested in this methods and as I mentioned earlier, I wanted to understand the underlying theory better. At Stanford, my Ph.D. was under Emmanuel Cand\u00e8s who has been very influential in optimization-based methods for inverse problems. I studied how these algorithms can be applied to problems such as super-resolution in fluorescence microscopy and derived theoretical guarantees that show under what conditions we can expect them to work.\n\nWhat is the focus of your current research?\n\nOn the one hand, I am still interested in the theoretical analysis of data-analysis methods based on optimization. On the other, I have become interested in more practical applications, such as spike sorting in neuroscience. This problem entails processing data from sensors that pick up signals from different neurons. You want to un-mix these signals in order to decide which one is coming out of each neuron. There are many interesting challenges, such as the sheer amount of data. Because there are so many measurements, it is crucial to make sure you\u2019re applying the algorithms to the parts of the data where there is activity; otherwise things become too computationally expensive.\n\nIn data science, where do you see the next big discoveries coming from?\n\nLately, there have been some amazing advances in machine learning applied to problems in computer vision and speech recognition. These advances have been made possible by the availability of huge datasets with annotated examples. I think a crucial challenge in the coming years is to be able to apply these techniques to other areas where we don\u2019t have so much curated data. Some of the ideas should definitely transfer but you\u2019re going to have to use data in a subtler way because it\u2019s not going to be as well categorized. For example, medical diagnostics, the social sciences, economics, policy \u2014 you have a huge amount of data in these areas but it\u2019s usually very heterogeneous, so you cannot form problems that are as clean as \u201cLet\u2019s identify some person\u2019s face.\u201d\n\nRight now, we have exciting big data techniques that are working very well for things like computer vision but we still have to work out how to apply them to many other areas where they could have an impact. It\u2019s not obvious how to pull the data together, identify the parts that are interesting and then combine them in sophisticated ways \u2014 which is what we are able to when we have huge curated data sets, as in computer vision. This is what I think is fascinating.\n\nDo living in New York and working at Courant and CDS suit you?\n\nI was very excited to come to New York. I think Courant is a great place to be. I\u2019m more mathematically oriented so I\u2019m happier here than in an engineering department. But at the same time, I feel there is a strong connection to applications. I\u2019m now becoming interested in neuroscience and Courant/CDS is an ideal place to do those kinds of things."
    },
    {
        "url": "https://medium.com/center-for-data-science/charting-the-great-unknown-with-data-d2d3a6ec1477",
        "title": "Charting the great unknown \u2014 with data \u2013 Center for Data Science \u2013",
        "text": "Michael Blanton is an astrophysicist using data science to study galaxy evolution and map the Universe. He is the director of the Sloan Digital Sky Survey, an Associate Professor in NYU\u2019s physics department, and an Affiliated Professor at the Center for Data Science.\n\nWhat initially drew you to the field of astrophysics?\n\nI studied engineering physics in college, and became interested in astrophysics through a planetary astronomy course I took as a sophomore. What drew me to the field was how a modicum of physical knowledge, or even a small amount of data, could be used to infer the conditions in distant and otherwise unknowable places.\n\nHow did you start to incorporate data science into your research?\n\nEven before I knew about the term \u201cdata science\u201d my research required considerable work in data management, provenance tracking, and the use of statistical inference. Astronomy involves large complex data sets, and to understand them you need to account for the particular environmental conditions during the observations.\n\nCan you talk about your goals for the Sloan Digital Sky Survey, and how data science plays into this?\n\nThe three main goals are:\n\nWith galaxies like the Milky Way, we\u2019re using data science to study the formation of its stars, and their elemental abundances. Whereas to study the expansion of the Universe, we\u2019re mapping quasars to trace density fields.\n\nAll of this data is public information you can find at: skyserver.sdss.org.\n\nHow are improvements in data visualization impacting the way we visualize galaxies and groups of stars?\n\nMost of the innovations aren\u2019t in visualizing the actual galaxies, but in visualizing the data we collect about galaxies in relation to each-other. We have an enormous amount of information, which can be translated into characteristics such as: stellar density, formation rate, chemical make-up, photoionization states, and planetary movements. Visualizations of these characteristics allow us to understand the correlations and connections in all this data. The challenge is giving astronomers tools to make useful visualizations that reveal meaningful insights.\n\nHow has data science\u2019s role in the field of astrophysics changed over the years?\n\nWe\u2019ve moved away from bespoke products that were made for specific purposes, and now rely on standard computer science and data management tools. Just like hardware, the existence of a common tool base makes things far easier to develop and to maintain.\n\nAre there any ways data science could be used more effectively in the field of astronomy?\n\nOne of my soap boxes is that a lot of science research is based on data sets that have already been calibrated. I think the major opportunity is in creating these calibrations by improving the survey operations and analyzing raw data.\n\nIn your opinion, what is the single biggest affect data science has had on the field of astrophysics, or how we approach the field of physics in general?\n\nI think the biggest effect has been sociological. When I started working in the field of astronomy, there was a sense that computing and data management skills would have some sort of practical application in the future, but nobody was really sure how it would happen.\n\nThe existence of data science as a field gives a sense of value in developing a particular set of data skills. Technical expertise is never the same as scientific expertise, and there is an increased sense of the importance in understanding data\u2019s technical aspects.\n\nWhat drew you to the CDS program at NYU?\n\nCDS gathers scientists from a wide range of fields and allows us to start conversations from a shared technical perspective. Also, I can finally learn about the huge range of science research being conducted at NYU!\n\nOn a chalk-board at CDS we have the question: \u201cWhat does it mean to be a data scientist?\u201d Could you answer this question?\n\nA data scientist is somebody who implements solutions in data management, distribution, or analysis that are applicable to their own domain, but also extensible to other fields."
    },
    {
        "url": "https://medium.com/center-for-data-science/using-data-science-to-improve-new-york-5496ce5a5e73",
        "title": "Using data science to improve New York \u2013 Center for Data Science \u2013",
        "text": "Shivendra Panwar is a Professor in the Electrical and Computer Engineering Department at NYU Polytechnic School of Engineering, and the director for the Center for Advanced Technology in Telecommunications (CATT).\n\nWhat did you study in school? How did you get to what you study now?\n\nI studied Electrical and Computer Engineering at the University of Massachusetts, Amherst. In the early 80\u2019s when I was doing my undergraduate research, I started getting interested in what is now called the internet and wireless communications. From there I went on to study networks in the fields of computer and data networks.\n\nWithin the fields of computer and data networks, what sort of projects are you working on?\n\nIn the field of mobile networks, I\u2019ve worked on a video streaming technology called Streamloading, as well as protocols and architectures that support 5G networks. I\u2019ve also been working on full duplex communications, a technology that allows radios to transmit and receive information on the same spectrum, at the same time.\n\nCould you talk about the CATT grant and your involvement?\n\nCATT is a New York state sponsored program that works to improve existing technology and infrastructure through university and industry partnerships in the fields of economic development, technology transfer, workforce training, entrepreneurial support, and research & development. Data science is one of the three ares we are focusing on, in addition to wireless networking and cyber security. I got involved in the mid-1980s, and I\u2019ve been the center\u2019s director since 1996.\n\nHow have you gone about ensuring funding for the CATT program? What advice would you give to somebody looking to secure a CATT grant or a similar grant?\n\nIt\u2019s important to contribute towards the state of New York\u2019s goals for technologically based economic development. You need to have a grasp on what the major economic issues facing the state are, and find companies with emerging technologies that can help combat or understand these problems in a meaningful way. Start with a couple of companies and find out how they can fit into your proposal. Then build up a business case for your project.\n\nWhat are some of the tangible impacts that the CATT grant has made in the past?\n\nWe have made a positive impact of over $330 million to the New York state economy, and we have created several hundred jobs.\n\nWhat projects has the CATT grant worked on in the past?\n\nMedical imaging has been one of the most exciting projects we\u2019ve helped develop, as well as several projects in the field of forensics.\n\nAs for the field of data science, we\u2019ve developed a model for traffic planning and capacity engineering with several other organizations. The prototype includes a generic model for the simulation and analysis of different network services and architectures. It can be used for traffic planning for frame relay and asynchronous transfer mode based services.\n\nWhat do you hope to accomplish with CATT in the future?\n\nWe are continually reinventing ourselves to handle the needs of the New York state economy, and various evolving industries."
    },
    {
        "url": "https://medium.com/center-for-data-science/when-political-science-meets-data-science-ccf8b398f4d8",
        "title": "When political science meets data science \u2013 Center for Data Science \u2013",
        "text": "Nathaniel Beck is a Professor at New York University\u2019s Wilf Family Department of Politics, and an Affiliated Faculty member at the Center for Data Science.\n\nWhat did you study in school? How did you get to what you study now?\n\nI went to the University of Rochester almost half a century ago. I started off as a math major, but I soon realized I would never be a really good mathematician. Around that time I was excited by Isaac Asimov\u2019s science fiction series, Foundation, and I became interested in his psycho-historian characters who could forecast mass movements in society, so I turned to political science. It was sheer luck I was at the University of Rochester, as they had just hired Bill Riker who was one of the key founders of modern political science (both in terms of game theory and top empirical work). Rochester was also the kind of place where an undergraduate could just walk into a Ph.D. course and easily chat with someone like Riker. I loved the research and have never looked back.\n\nHow did you start to incorporate data science into your political science studies?\n\nI developed an interest in non-linear models very early on, so moving to data science was not a hard sell. And I had been doing supervised machine learning for a long time before I even knew that term existed\n\nWithin the cross-section of political science and data science, what are some of your research interests?\n\nOne of my main interests is figuring out the usefulness of machine learning methods for political science, where we want to describe what the so called, \u201cblack box\u201d looks like. I am also actively trying to see if there are ways to combine machine learning methods with structural assumptions we have about political systems.\n\nHow does machine learning factor into your work?\n\nThe use of regression models is just a very simple form of machine learning. 50 years ago regression was state of the art. Now more complicated models (trees) are in that state. We still need to figure out how to get trees to tell us about social phenomena (as opposed to simply classifying things, our task is not to read zip codes) and we need to continue to put them on a firm statistical basis. Fortunately Bayesian models can be very useful here.\n\nWhat data science methods have you used in the past for your work in political science? Natural language processing seems to be an obvious one, but I\u2019m curious if there are other disciplines in data science you\u2019re using to study politics.\n\nWhile I\u2019m interested in text analysis, my own interests have always been in non-linear methods. My first presentation in a statistics class at Yale was on an early tree based method (Automatic Interaction Detector), and though at the time it was laughed at, with more modern thinking about cross-validation and such, this has become a rather useful tool (essentially the move from stepwise regression to LASSO).\n\nAs a person interested in time series, I have always thought about the possibilities with out of sample validation, and cross validation has proved increasingly useful. I\u2019ve worked with Jonathan Katz on time-series \u2014 cross-sectional models, and I also did some work with Simon Jackman on generalized additive models. Gary King, Langche Zeng, and I have looked into neural nets in to study conflict under the hypothesis that one needed high order complex interactions to model conflict.\n\nThese days I expect all of our graduate students to become conversant with these methods. There has also been a revolution in computations, with Markov chain Monte Carlo making things possible. We\u2019ve gone from this being cutting edge to a standard tool in a bit over a decade.\n\nWhat aspects of politics and political methodology have the largest untapped potential for work in the world of data science?\n\nWhile we are doing a lot, the area is just taking off. There is a lot of potential in new sources of data, such as the work coming from NYU\u2019s Social Media and Political Participation lab. There\u2019s obviously lots of potential in text (where I have some undergraduate honors students studying thinks like rhetoric in the Federal Reserve), geo-spatial analysis (colleagues using satellite imaging to study fires as a means of ethnic cleansing in Kenya), novel data collection (colleagues tracking refuges with cell phones), etc. etc.\n\nYour faculty bio says your \u201cundergraduate teaching is primarily related to a new Public Policy major which should be announced soon.\u201d Can you tell me about this new major, and data science will enter into the equation?\n\nIt is easier to fit data science into public policy than political science. We have all these great new applications, whether it be using tax returns to measure inequality, looking at how people are moving around, on a daily or permanent basis, or something like NYU\u2019s Center for Urban Science and Progress\u2019 metered city project. Public policy is also taking advantage of new ideas in research design, whether in the use of experiments or quasi-experiments. So many of the courses stress what we can learn from data, but also, how we learn from data.\n\nI\u2019m teaching the first capstone seminar next year, and expect that most of the projects to be data oriented. NYU\u2019s College of Arts and Science is particularly interested in training students who can live and work and understand things in the 21st Century, and our major fits very nicely into that. The Politics Department is also considering revamping its methods sequence to give undergraduates more skills and appreciation for what modern data science can do (our Ph.D. students are already great at this).\n\nWhat drew you to the CDS program at NYU?\n\nWhen CDS started, Yann LeCun reached out to me because he had seen my work on neural nets. I started going to CDS events, and I met people with very different substantive interests, but using similar methodological processes. Who would have guessed that David Hogg and I are using similar methods, even though he is studying distant solar systems?\n\nOn a chalk-board at CDS we have the question: \u201cWhat does it mean to be a data scientist?\u201d Could you answer this question?\n\nI think what distinguishes data science from statistics is a real appreciation for exciting new sources of data and a willingness to deal with the very messy problems of such data. The trick is to not lose all the good done by statisticians (understanding causality and uncertainty) while devising methods to deal with this very messy data.\n\nTo know more about our faculty and programs, visit cds.nyu.edu"
    },
    {
        "url": "https://medium.com/center-for-data-science/women-in-data-science-ca0df7262458",
        "title": "Women in Data Science: \u2013 Center for Data Science \u2013",
        "text": "Nuclear power is a highly controversial topic within the debate on clean energy. 20% of the United States total energy usage is derived from nuclear reactions, and in one year, a nuclear reactor can generate the equivalent energy of 13.7 million barrels of oil, or 3.4 million tons of coal. It is a highly efficient, and promising, energy source, albeit one with several disastrous incidents. In the past 50 years, several catastrophic instances have tarnished the reputation of nuclear power.\n\nTo tackle these pressing issues in the field of nuclear power, Kathryn Huff is using computer modeling and simulation to eliminate human error in nuclear reactions and find safer ways to continue the practice of nuclear energy. Kathryn was drawn to the field of nuclear energy out of a concern for the environment, and she sees nuclear fission as an essential part of a sustainable energy future for the world. As a Moore/Sloan fellow, Kathryn\u2019s work focuses on the intersection of nuclear engineering and data science. Specifically, she works on Python\u2019s application in the field of nuclear science, and uses Python\u2019s open-sourced nature to collaborate and share her work with other nuclear scientists.\n\nOne of Kathryn\u2019s main projects, PyRK, is a python modeling package that maximizes the amount of energy harnessed from nuclear reactions, while ensuring safety. Python allows a high number of variables to be factored into an equation using a relatively small amount of code, and with nuclear reactor equations, the number of variables increases as each reaction progresses. The power created from neutrons during the initial nuclear fission go back and contribute more power to the original kinetic reaction, creating a feedback equation. PyRK models this equation, and monitors possibly disruptive variables in the process. PyRK also allows scientists to monitor the start ups and shut downs of nuclear reactors, the procedures prone to the highest number of accidents. Similarly, nuclear reactors are in constant need of coolant introduced to the system to prevent overheating, and PyRK allows scientists to accurately calibrate how much coolant is needed.\n\nThe same concepts in PyRK can be used for other feedback equations, in both the sciences, and other fields. The underlying theme of Kathryn\u2019s work is to one day eliminate nuclear disasters thought improved modeling solutions, and use Python to create a safer and more reliable energy solution for the world. There is a huge demand for a reliable clean energy sources that is both safe, and efficient, and PyRK is one step closer towards nuclear energy becoming a leading clear energy solution."
    },
    {
        "url": "https://medium.com/center-for-data-science/can-data-science-save-the-new-york-times-4d9d9932111c",
        "title": "Can Data Science Save The New York Times? \u2013 Center for Data Science \u2013",
        "text": "Chris Wiggins describes the practice of data science as the pain that you feel when you find yourself awash in data. For most of his career, he\u2019s worked in academia, spending the last decade studying molecular biology, most recently at Columbia University. Data Science has been integral in both his research, and in his work outside of the university setting. Wiggins is highly active in New York\u2019s start-up community, and in 2010, he co-founded Hack NY, a non-profit group that sponsors student \u201chackathons\u201d as a way of fostering growth within the city\u2019s innovative technology community. Despite his wide breadth of expertise in the field of data science, Wiggins sought a new challenge when he took a semester sabbatical in the fall of 2013. As he puts it, one of his friends advised him, \u201cGo to the New York Times, it will be weird.\u201d He took the advice, and as a result, Chris Wiggins found himself as The New York Times\u2019 first ever Chief Data Scientist.\n\nTechnology and journalism have a bit of a tenuous relationship, but it has become clear in the past few years that traditional long-standing media conglomerates have to embrace the digital revolution. It\u2019s an understatement to say that the internet has drastically changed journalism. Daily coverage has morphed into a 24-hour digital echo-chamber. With the emergence of branded content, viral media, and the supremacy of the almighty ad-click rate, it is impossible to draw the line between real news, and shareable content. The challenge being:\n\nWiggins\u2019 entrance to the New York Times came at a particularly rocky period for the publication. A few months after he was brought on, a leaked internal report showed how the Times\u2019 business model had failed to reflect the changing tides of print media. Their social media efforts were scattered at best, and the back end of the Times website was revealed to be incredibly inefficient. Many of the news editors were described as being \u201cunfamiliar with the web.\u201d Possibly the most troubling aspect of the report, was an undertone suggesting content consumption trends could render the New York Times\u2019 hallowed journalistic practices financially unfeasible.\n\nMost media companies heavily relying on data science collect information from consumers to decide which types of content to publish. When data scientists at a viral content producer recognize postings of cat pictures are generating more views, and as a result more ad-dollars, than other posts, the editorial teams are informed to produce more media that follows. The result is a media landscape where share-ability trumps integrity, and catchy headlines become more profitable than careful reporting.\n\nChris Wiggins has a different approach to how the New York Times should use data science. Media websites are no longer solely a company\u2019s e-presence, they are a microscope into a reader base, and Wiggins believes this is the key to securing journalism\u2019s future. The New York Times data science team knows the highest performing articles, but they have made a conscious decision to not let this affect editorial planning. Whereas the Buzzfeeds of the world use data science to calculate the types of content generated, Wiggins\u2019 mentality is that data science should be applied from a financial standpoint to support business operations, letting editorial teams function independently. Wiggins is adamant there is a need for a new sustainable financial model to support the traditional craft of journalism, and although its feasibility is unclear, he believes data science can be the basis of this new model.\n\nIn the past year, a major overhaul has changed how the New York Times sees its digital business model. With all the data the Times collects via its website, Wiggins and his team can take the guess work out of financial decisions that have plagued print media. Need to know which website layout keeps readers around longer? The Times is now using A/B testing to determine which features readers prefer. How many newspapers need to go on each newsstand around the world? The Times now taps into previously recorded sales and other measurable statistics instead of using industry rules of thumb. What are the indicators leading to subscription cancellation? Wiggins\u2019 team can now spot larger trends earlier to keep a robust subscription base.\n\nAt the core of Wiggins\u2019 work is the idea that traditional journalistic practices are essential to our country\u2019s cultural fabric. He believes data science can be used to support and sustain this tradition, as opposed to fundamentally changing the way it operates. Wiggins argues the internet\u2019s affect on journalism has forced news outlets to think like startups, as opposed to traditional media conglomerates. The main indicators a startup will be successful is whether its business model is scalable and repeatable. Is this the case for The New York Times? \u201cWe don\u2019t know yet.\u201d Wiggins responded. It remains to be seen. But Wiggins seems confident that applied data science will modernize the New York Times\u2019 business model, while keeping traditional journalistic practices alive and well. And judging by the drastic turnaround the New York Times has seen on its digital side in the past year, his plan might just be working."
    }
]