[
    {
        "url": "https://towardsdatascience.com/what-is-wrong-with-convolutional-neural-networks-75c2ba8fbd6f?source=user_profile---------1----------------",
        "title": "What is wrong with Convolutional neural networks ? \u2013",
        "text": "Of course convolutional neural networks (CNNs) are fascinating and strong tool, maybe it\u2019s one of the reasons Deep learning is so popular these days, since Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton published \u201cImageNet Classification with Deep Convolutional Networks\u201d in 2012, CNN's has been the winning card in computer vision achieving superhuman performance in many tasks, but are CNN\u2019s flawless? is that the best that we can do? i guess from the title you figured that the answer is NO.\n\nDecember 4, 2014, Geoffrey Hinton gave a speech in MIT about a project of his called capsule networks, and he discussed the problems with CNN\u2019s and why pooling is very bad and the fact that it\u2019s working so well is a disaster\n\nA Convolutional layer have a set of matrices that get multiplied by the previous layer output in a process called the convolution to detect some features this features could be basic features (e.g. edge, color grade or pattern) or complex one (e.g. shape, nose, or a mouth) so, those matrices are called filters or kernels\n\nThere is more than one type of pooling layer (Max pooling, avg pooling \u2026), the most common -this days- is Max pooling because it gives transational variance \u2014 poor but good enough for some tasks \u2014 and it reduces the dimensionality of the network so cheaply (with no parameters)\n\nmax pooling layers is actually very simple, you predefine a filter (a window) and swap this window across the input taking the max of the values contained in the window to be the output\n\nbackprob is a method to find the contribution of every weight in the error after a batch of data is prepossessed and most of good optimization algorithms (SGD, ADAM \u2026 ) uses Backpropagation to find the gradients\n\nbackpropagation has been doing so good in the last years but is not an efficient way of learning, because it needs huge dataset\n\ni believe that we can do better\n\nwhen we say translational invariance we mean that the same object with slightly change of orientation or position might not fire up the neuron that is supposed to recognize that object\n\nAs in the image above if we assumed that there is a neuron that is supposed to detect cats it\u2019s value will change with the change of the position and rotation of the cat, data augmentation partially solves the problem but it does not get rid of it totally\n\nPooling layers is a big mistake because it loses a lot of valuable information and it ignores the relation between the part and the whole if we are talking about a face detector so we have to combine some features (mouth, 2 eyes, face oval and a nose) to say that is a face \n\nCNN would say if those 5 features present with high probability this would be a face\n\nso the output of the two images might be similar which is not good\n\nCNN\u2019s are awesome but it have 2 very dangerous flaws Translation invariance and pooling layers, luckily we can reduce the danger with data augmentation but something is coming up (capsule networks) we have to be ready and open to the change"
    }
]