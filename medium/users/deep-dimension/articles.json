[
    {
        "url": "https://medium.com/deep-dimension/deep-learning-papers-review-universal-adversarial-patch-a5ad222a62d2?source=---------0",
        "title": "Deep Learning Papers review \u2014 Universal Adversarial Patch",
        "text": "In this article, I\u2019ll initially be discussing about generating adversarial images and then I\u2019ll slowly steer the discussion towards an interesting paper published by researchers at Google Brain about an Adversarial Image Patch (https://arxiv.org/pdf/1712.09665.pdf). This paper presents a generic image patch, which when added to images would cause any Neural Network to misclassify them. The authors of the paper themselves have demonstrated this through a youtube video :\n\nLet\u2019s first find out why such adversaries can be formed in the first place.\n\nDeep Neural Networks have certainly been producing \u201chigh accuracy\u201d results for object recognition lately. Yet, one can make a Neural Net misclassify an image with minimal perturbations. Let\u2019s take a look at the possible reasons :\n\n[There was recently some work on Capsule Networks by Geoff Hinton, which are invariant to image transformations. Yet, capsules are vulnerable to other type of adversarial attacks. And even Convnets are more or less scale and transformation invariant ]\n\nThese weaknesses of Neural Nets gave rise to an entire field called \u201cAdversarial Deep Learning\u201d (in general \u201cAdversarial Machine Learning\u201d for any type of input signal)\n\nGenerating adversarial images to fool a Neural Network classifier isn\u2019t a new problem. There have been a lot of proposed methods in the past to generate adversarial examples. The simplest way to do this would be to change the value of individual pixels of the image until the probability of a new class is maximized. Mathematically,\n\nThere are also gradient based iterative methods like, Fast gradient sign method (FGSM), Iterative gradient sign method and Iterative Least-likely Class Method to produce adversarial examples. These methods primarily use the gradient of the cost (J) of the output class with respect to the input image, to iteratively change the input image based on the gradient. Let\u2019s take a look at the mathematical equation of FGSM :\n\nIn a nutshell, FGSM iteratively increases the input signal by a small amount in the direction of the gradient of the cost with respect to the input.\n\nApart from the above techniques, of course there are the popular GANs (Generative adversarial networks) to generate adversarial images.\n\nWhile the above methods generate satisfactory adversarial examples, they aren\u2019t robust enough to work on equivalently transformed images. This paper titled \u201cFoveation-based Mechanisms Alleviate Adversarial Examples\u201d by Luo et. al, shows that the above adversarial examples fail when they are cropped along the object of interest (Foveated). This is because, Convnets are robust towards scaling and translation. But, such a transformation rule doesn\u2019t apply to the noise or perturbation added to the image, i.e., the perturbations aren\u2019t robust enough to fool the Convnet even after the image getting transformed. Another paper titled \u201cNO Need to Worry about Adversarial Examples in Object Detection in Autonomous Vehicles\u201d, has almost the same intent.\n\nSo, is it even possible to produce a robust set of adversarial images ? Well, there have been some interesting papers lately that discuss about producing robust adversarial examples. We\u2019ll take a look at some of them :\n\nWe\u2019ll mainly be looking into the first 2 papers.\n\nThe work from the first paper (i.e., Synthesizing robust adversarial examples), produces adversarial examples that are robust enough to \u201cfool\u201d a Neural Network classifier under most image transformations. Essentially, what happens here is that, the Expected probability of a class is maximized, over all possible transformation functions (t ~ T), with a constraint on the Expected effective distance between the transformed original and transformed perturbed image. Let\u2019s try to understand what that means.\n\nIn EOT, the given image is first made adversarial using one of the above mentioned methods. Now, we define a transformation space \u2018T\u2019, which houses transformations like rotation, scaling, translation and so on. Then, we calculate the expectation of the log probability of our desired class label. This is what it looks like mathematically :\n\nWe then try to maximize this expected probability under the constraint that the Expected effective distance between the transformed original and the transformed perturbed image is less than some value \u2018\u03b5\u2019. So, by considering the expected probability (or log probability), we\u2019re accounting for all the transformations present in the Transformation space. And the constraint is to ensure that the generated images are as close as possible to the original transformation. This is what the final equation looks like :\n\nFrom the video above, it\u2019s clear that we are hunting for a \u201cUniversal\u201d image patch, which when added to any image will make a Neural Network misclassify the image. For this, an operator is first defined. The operator takes in a patch, an image, co-ordinates in the image (to place the patch) and transformations like translation, rotation and scaling to be applied on the patch.\n\nTo find the optimal patch, Expectation over Transformation is used for a given label to maximize the probability of misclassification. Mathematically, it looks like this :\n\nThe original paper used \u201cToaster\u201d as the adversarial class and the final patch looked like this :\n\nOne limitation about this adversarial patch is that, you can\u2019t fool object detection models (models that recognize various object in an image). For eg., I recently tried to upload an image with this patch onto Facebook (:P). Since Facebook lists all the predictions about the image in the attribute of the tag that houses it, you can check its predictions as soon as you upload the image. Here\u2019s what I tried :\n\n[The 3rd paper in the list above, i.e., \u201cTowards Imperceptible and Robust Adversarial Example Attacks against Neural Networks\u201d came out just about a week back. In that paper, they\u2019ve taken into account the human perceptual system while generating adversarial examples]"
    },
    {
        "url": "https://medium.com/deep-dimension/deep-learning-papers-review-i-ntm-d55d2a1a0096?source=---------1",
        "title": "Deep Learning papers review \u2014 NTM \u2013 Shravan\u2019s Blog \u2013",
        "text": "Deep Learning has been an exponentially growing field in the past decade or so. And to show my enthusiasm for the field, I\u2019m starting this series in which I\u2019ll pick up a published paper related to deep learning (or at least something close enough) every one or two weeks, and do some sort of an analysis on the paper and give more insights into it. This way I felt I could make the community aware of some of the recent advances in Deep Learning and also arouse the curiosity of fellow Deep Learning enthusiasts.\n\nBefore I get started on today\u2019s paper, I\u2019d like to share this Github repo link that has a lot of Deep Learning papers listed nicely : Deep Learning Roadmap\n\nAnd also, just like other programmers, I love bringing in memes all the time :P\n\nIn this article, I\u2019ll be discussing about Neural Turing Machine (NTM), which was first released by DeepMind in the year 2014. Here\u2019s the relevant paper : https://arxiv.org/pdf/1410.5401.pdf\n\nIn fact, NTM has been further improved by DeepMind after their initial publication and is now called the Differentiable Neural Computer (DNC), which is much more sophisticated and has really cool applications. Here\u2019s its paper : https://www.nature.com/articles/nature20101.epdf\n\nIn this article, I\u2019ll mostly be basing my discussions on NTM (the first one). I\u2019ll also mention other similar work done in this field\n\nThe Turing Machine is the most fundamental concept in computer science and the functioning of the modern day computers is derived from this. A Turing machine is an infinite tape containing slots, with a bit (either a 0 or a 1) at each slot. The bits are accessed or modified using a read or write head. The bits can be randomly accessed, erased or modified. Yet, the Turing machine is an ideal device and no \u201ccomputer\u201d is completely a Turing Machine and can perform all the tasks that a Turing Machine can perform.\n\nA NTM is basically a neural computer. You can think of this as the neural version of a computer as described in the Von Neumann diagram. A NTM can read and write to a memory bank just as a regular computer, but all of those operations in a NTM are differentiable. This system can be trained using backpropagation. So, just as RAM in a computer, a NTM consists of memory. And elements are read from or written to the memory using read/write heads (Inspired by Turing Machine). These read/write heads are controlled by a \u201cController\u201d. The controller is essentially just a neural network (either feed-forward or recurrent). Let\u2019s analyze the NTM piece by piece.\n\nThe controller in a NTM is analogous to CPU in a modern day computer. This controls the read and write heads by emitting necessary vectors that lead to the formation of the weight vector. The weight vector represents the attention distribution towards the memory. It shows what parts of memory the heads focus on and by how much. As mentioned earlier, the controller is just a neural network. It could be a feed-forward net or a recurrent net. Just as any type of neural net, the controller takes in some input, gives out predictions and gets trained on the target values.\n\nSo, where do we use the data from the memory in here then ? The answer is that, the read vector from the previous time step is passed along with the data (from the dataset) as input to the controller. At every time step, the controller produces the necessary vectors to compute the weight vectors (read and write vectors have separate weights). These vectors go through a series of computations like convolution and sharpening, which we\u2019ll see in the \u201cAddressing mechanisms\u201d sections below. These weight vectors are then used to read from and write to memory. Here\u2019s a flow diagram of the controller.\n\nMemory in a NTM is analogous to RAM in a modern day computer. In a NTM, memory is essentially just a matrix (a vector of vectors) from which elements are being read or to which elements are being written to. Let\u2019s denote this matrix by M. Also let\u2019s add a suffix \u2018t\u2019 to \u2018M\u2019, which represents the state of the matrix.\n\nUnlike a modern day computer, read operation in a NTM is differentiable. Mathematically, the read operation is just the sum of scalar multiplication of the weights and the row vectors in the memory matrix.\n\nTo get better intuition, let\u2019s assume that each row in the memory matrix just has one element. So, the read operation now becomes equivalent to the dot product of the weight vector and elements in the memory matrix. Naturally, the elements in the memory which are being multiplied with a higher value of weight would stand out in the final product as compared to others. This means that, the attention given to various parts of the memory are different. So, some parts of the memory are more important than others. This attentional mechanism is something that the controller learns as per the input data distribution.\n\nWrite operation is merely just erase + add. This is actually inspired from LSTMs as they consist of input and forget gates. The erase operation is a weighted subtraction of the erase vector from the memory. Similarly, the add operation is the weighted addition of the add vector on the memory matrix. The erase and add vectors are produced by the controller.\n\nLet\u2019s looking into this from a math perspective\n\n[for simplicity of implementation, you can assume the read and write weights are same ]\n\nThis is the process that converts the outputs of the controller into the weight vector. There are 2 types of addressing mechanisms: Content based addressing and location based addressing. And usually in practice, we use both the types of addressing mechanisms to produce the weights.\n\nIn this type of addressing, similarity measures are produced for the elements of the memory and the Key Vector (this is one of the outputs of the controller). Let\u2019s look into its mathematical equation :\n\nThe main intuition behind the content-based weights in that, they are in favor of the row in the memory matrix, which is most similar to the controller output.\n\nThis addressing mechanism involves 3 processes : interpolation, convolution and sharpening. Interpolation is the weighted sum of the content-based weights and the weights from the previous time step. In fact there is just one \u2018weight\u2019 used in that sum and it\u2019s called the interpolation gate (g). Here\u2019s the equation for interpolation :\n\nConvolution is the process of rotational shifting. This is done using a convolutional or a shift filter \u2018s\u2019. This shifting is analogous to head shifting in an actual Turing Machine. Let\u2019s look at it\u2019s math equation :\n\nThe process of convolution here, is often known to cause leakage or improper dispersion of the weights. So, to mitigate this, sharpening is used. Sharpening is collectively the process of amplifying the weights by raising them to a power and normalizing them between 0 and 1. Here\u2019s the math equation :\n\nThe use of location based addressing is that, it facilitates random access and iteration through the memory.\n\nAnd in practice, we perform both the types of addressing mechanisms, as the heads should be able to use the similarity between the memory and the controller output as well as the location of the memory it focuses the most.\n\nThe copy task as the name says simply imitates the input at the final output of the controller. The final output scores of the controller here are simply compared against the input vector for optimization. Optimization is a regular neural network optimization like stochastic gradient descent. And some latent information is written to the memory while training\n\n[That implementation isn\u2019t complete as yet. It\u2019s on process]\n\nFeel free to suggest a good paper for the next post ;)"
    },
    {
        "url": "https://medium.com/deep-dimension/gans-a-modern-perspective-83ed64b42f5c?source=---------2",
        "title": "GANs, a modern perspective \u2013 Shravan\u2019s Blog \u2013",
        "text": "In today\u2019s world, GAN (Generative Adversarial Networks) is an insanely active topic of research and it has already attracted a lot of creative applications like this one\n\nIt all started in the year 2014 when Christian Szegedy and a couple of others at Google, noticed that neural nets can be fooled easily by adding just a small amount of noise. They used gradient ascent to make their deep neural network classify a given image as something else other than its ground truth class. Surprisingly, they only required a small amount of distortion to convert images of one class to another.\n\nMathematically, this change of class can be implemented using Fast Gradient Sign Method (FGSM just iteratively adds a small amount noise in the direction of the gradient of the objective function with respect to the input values)\n\nThe most interesting part is that, the model after adding noise has much more confidence in the wrong prediction than that when it predicted right !\n\nThese are just some of the reasons in layman\u2019s terms. For a clearer understanding about such adversaries, I\u2019d highly recommend this tutorial by Ian Goodfellow.\n\nWell, there\u2019s still a lot of research happening on this and there isn\u2019t a clear cut answer yet. One of the solutions proposed for this was to train the net on adversarial examples as well. And these adversarial examples could be generated using Deep Generative models. There were multiple generative models proposed as well, some of the notable ones being PixelCNN, Variational Auto-encoders and Generative Adversarial Networks (or GANs). In this article, we are particularly gonna explore GANs.\n\nGenerative Adversarial Networks consist of a generator and a discriminator neural network. The purpose of the generator is to take in noise vectors and produce images that resembles the input data distribution closely and try to fool the discriminator into classifying a fake image as a real image. The function of the discriminator is to classify a generated image as real or fake. What\u2019s going on between the generator and the discriminator here is a 2 player zero sum game. In other words, in every move, the generator is trying to maximize the chance of the discriminator misclassifying the image and the discriminator is in turn trying to maximize its chances of correctly classifying the incoming image.\n\nFor more information on adversarial examples, do watch this talk by Ian Goodfellow\n\nObscure isn\u2019t that ? No worries ! it\u2019s easy to understand !\n\nLet\u2019s analyze both the terms in the objective function.\n\n[ Note : \u03b8g and \u03b8d are just weights of the generator and discriminator networks. You can ignore them while trying to interpret the equations ]\n\nThis term represents the Log probability of the discriminator output with input data from real data distribution. Now, look at this term from the Discriminator\u2019s perspective. According to the discriminator, it should maximize its probability of classifying an image correctly as real or fake. Here, the images are sampled from the original data distribution, which is the real data itself. Also, remember that represents the probability that the input image is real. Hence, the discriminator will have to maximize (i.e., it has to be close to \u20181.0\u2019) and . And hence, Term I has to be maximized.\n\nThe explanation for this term is quite similar. But you should view this equation from the Generator\u2019s perspective. Here, images from the generator\u2019s output are passed in to the discriminator. So, according to the generator, it has to maximize the chances of the discriminator getting fooled by the generated images. Which means, the generator should want to maximize . Which means, it should look to minimize and hence .\n\nGANs are one of the hottest research topics today and there are a good number of proposals for GAN implementations in the past couple of years. Here, I\u2019ll discuss only a few of them, although I\u2019ll make sure to list all of the types.\n\nThis is the simplest type of GAN and in this case the generator and the discriminator are just simple multi-layer perceptrons. Vanilla GANs simply just seek to optimize the mathematical equation using stochastic gradient descent. Let\u2019s take a look at the algorithm.\n\nIn layman\u2019s terms, the generator here takes in a noise vector (\u2018z\u2019, usually 100-dimensional) and produce an image (G(z), which is just a flattened vector of all the pixels in the image). This image is used in the equations we saw previously to simply update the weights of the generator and the discriminator by computing gradients through backpropagation.\n\nThis is one of the most popular types of GANs today. In this case, ConvNets are used in place of the multi-layer perceptrons. The objective function remains the same here. Let\u2019s now take a look at the architecture.\n\nThe architecture of the discriminator is mostly just the opposite of that of the generator, i.e., it takes in an image and produces 2 numbers (which are just the probabilities of the image being fake or not). One more thing to note here is that, in the discriminator, the forward process consists of the Conv Transpose or Deconv operation at every layer. Let\u2019s take an example. Let\u2019s say that you wanna map a \u2018 4 X 4 X 1024 \u2019 layer to a \u2018 8 X 8 X 512 \u2019 layer and let\u2019s say that we\u2019re using 512 filters of size \u20183 X 3\u2019, then you\u2019ll just have to pad the existing layer\u2019s cross-section with zeroes and add zeroes between each element in the cross-section and do the regular convolution operation with strides. Take a look at the below gif for a clearer understanding.\n\nOk, let\u2019s now take a look at the results that DCGANs have produced.\n\nThis type of GAN conditions the output data distribution based on a Condition layer. So, in the objective function, and will be replaced by and . Rest of it would be the would be taken care of by the respective networks, i.e., creating latent representations and managing weights. You can think of this as more like an additional input of \u2018y\u2019 always exists in the networks. And of course, the objective remains same here.\n\nThis type of GAN is known to produce very high quality image samples. This uses multiple Generator-Discriminator networks at various levels of a Laplacian Pyramid. Precisely, the image is first downsampled to half its size at each layer of the pyramid. Then, in a backward pass through the pyramid, at each layer, the image acquires a noise generated by a Conditional GAN at that layer and then upsampled to twice its size. This way the image is reconstructed back to its own size.\n\nYou can find most types of GANs here :\n\nThis paper here has a lot of information on improving GANs : https://arxiv.org/pdf/1606.03498.pdf"
    },
    {
        "url": "https://medium.com/deep-dimension/an-analysis-on-computer-vision-problems-6c68d56030c3?source=---------3",
        "title": "An analysis on computer vision problems \u2013 Shravan\u2019s Blog \u2013",
        "text": "At least for about a decade now, there have been drastic improvements in the techniques used for solving problems in the domain of computer vision, some of the notable problems being, Image classification, object detection, image segmentation, image generation, image captioning and so on. In this blog post, I\u2019ll briefly explain some of these problems and also I\u2019ll try to compare and contrast these techniques from how humans interpret images. I\u2019ll also steer the article towards AGI (Artificial General Intelligence) and pitch in some of my thoughts on that.\n\nBefore we dive deeper, let\u2019s get some motivation from how some companies have creatively used computer vision. One of the coolest startups according to me is clarifai.com . Clarifai was started up by Matthew Zeiler who, with his team went on to win the imageNet challenge during the year 2013. His model reduced the error rate in image classfication by almost 4% from the best accuracy of the previous year. Clarifai is basically an A.I. company that provides APIs for visual recognition tasks like image and video labelling. Clarifai has a demo here. This company is very promising and it\u2019s image and video recognition technology are insanely accurate. Let\u2019s now move on to Facebook\u2019s automatic image tagging. The next time you login to your Facebook account, right click on any image and click on \u201cinspect element\u201d (this is for chrome; there are equivalent stuff on other browsers). Check out the alt attribute in the img tag ( should look something like this ). You\u2019d find that the alt attribute has text prefixed like this \u201c Image may contain : \u2026.. \u201c. This technology is quite accurate too. This technology recognizes people, text, mountains, sky, trees, plants, outdoor and nature and many more. Another cool technology is that of Google\u2019s. Go on to photos.google.com and type something in the search bar. Let\u2019s say you\u2019ve typed in \u201cmountains\u201d, then you\u2019ll accurately get all of your photos in the search result that contains mountains. The same is true for Google Image Search too. The best part about image search is that, the reverse also works, i.e., you can upload an image and get the best possible description of the image and also get images that are similar to the uploaded image. This technology quite spot on too.\n\nOk then, I hope you would\u2019ve gotten enough motivation by now. There definitely are a ton of other technology that I\u2019m missing. In fact there are a lot more similar technology that, one blog post isn\u2019t enough for me to fit in all of that.\n\nLet\u2019s now check out some of those computer vision problems !\n\nImage classification basically just involves labelling an image based on the content of the image. There would generally be a fixed set of labels and your model will have to predict the label that best fits the image. This problem is definitely hard for a machine as, all it sees is just a stream of numbers in an image.\n\n[The above picture was taken from Google Images]\n\nAnd, there usually are lots of image classification contests happening around the world. Kaggle is a very nice place to find such hosted competitions. One of the most famous such contests is the ImageNet Challenge. ImageNet is basically a humungous repository of images (about 14 million of them at the time when this article was written) with over 20000 image tags. This is being maintained by the computer vision lab at Stanford University. The ImageNet challenge or the Large Scale Visual Recognition Challenge (LSVRC) is an annual contest that has various sub challenges such as object classification, object detection and object localization. The LSVRC, especially the object classification challenge, started gaining a lot of attention from the year 2012 when Alex Krizhevsky implemented the famous AlexNet, which stole the show by reducing the error rate on images to 15.7% (which at that time was never achieved). And, looking at the latest results, Microsoft\u2019s ResNet has achieved an error rate of 3.57% and Google\u2019s Inception-v3 has achieved 3.46% and Inception-v4 has gone even further.\n\n[ That image originated in this paper written during 2017 by Alfredo Canziani, Adam Paszke and Eugenio Culurciello]\n\nObject detection in an image involves recognizing various sub images and drawing a bounding box around each recognized sub image. Here\u2019s an example :\n\n[The above image was taken from Google Images]\n\nThis is slightly more complicated to solve as compared to classification. You\u2019ll have to play around with the image co-ordinates a lot more here. The best known way to do detection right now is called Faster-RCNN. RCNN is Region Convolutional Neural Network. It uses a technique called Region Proposal Network, which is basically responsible for localizing on the regions in the image that need to be processed and classified. This RCNN was later tweaked and made more efficient and is now called Faster-RCNN. A Convolutional Neural Network is generally used as a part of the region proposal method to generate the regions. The most recent image-net challenge (LSVRC 2017) had an object detection challenge and was bagged by a teamed named \u201cBDAT\u201d which consisted of folks from Nanjing University of Information Science & Technology and Imperial College London.\n\nImage segmentation involves partitioning an image based on the objects present, with accurate boundaries.\n\nImage segmentation is of 2 types, Semantic segmentation and Instance segmentation. In Semantic segmentation, you\u2019ll have to label each pixel by a class object. Essentially, in such a case, every object that belongs to the same class (say every cat), will be colored the same. Whereas in Instance segmentation, every object is classified differently. This means that every cat in a picture would be colored differently.\n\n[The above image was taken from Google Images]\n\nIt can also be seen that Semantic segmentation is a subset of Instance segmentation. So, we\u2019ll see how to solve Instance segmentation.\n\nThe latest known technique to solve this is called Mask R-CNN, which is basically a couple of convolutional layers on top of the R-CNN technique we saw earlier. Microsoft, Facebook and Mighty Ai have jointly released this dataset named COCO. It is similar to ImageNet, but is mainly for segmentation and detection.\n\nThis is one of the coolest computer vision problems with a tinge of natural language processing I\u2019d say. This involves generating a caption that is most appropriate for your image.\n\n[The above image was taken from Google Images]\n\nImage captioning is basically Image Detection + Captioning. Image detection is done by the same Faster R-CNN method that we saw earlier. Captioning is done using a RNN (Recurrent Neural Net). To be very precise, LSTM (Long Short Term Memory), which is an advanced version of RNN is used. These RNNs are quite similar to our regular Deep Neural Networks except that, these RNNs depend on the previous state of the network. You can think of it more like a neural net with neurons building over time alongside space. Structurally, RNNs look like this :\n\nUsually, these RNNs are used for problems in which your data is time dependent. For example, if you wanna predict the next word in a sentence, then the new word depends on all the words that showed up in the previous time steps.\n\nLet\u2019s now switch gears a little bit and look at human visual understanding.\n\nBefore really going into the details about the majestic human brain, I\u2019d like to discuss a downside of these deep neural nets.\n\nAlthough Deep Neural Nets seem wonderful and magical, they can unfortunately be fooled easily. Take a look at this :\n\n[The above image was taken from Andrej Karpathy\u2019s Blog]\n\nAs the image shows, every image is super imposed with a noise image, which visually doesn\u2019t change the original image at all and yet got misclassified as ostrich !\n\nSuch attacks are called the Adversarial Attacks on Deep Neural Nets. They were initially brought up by Szegedy et al. in the year 2013 and was then further investigated by Goodfellow et al. in the year 2014. It was basically found that, we can find a minimal noise signal by optimizing the pixel intensities in the image to give priority to a different class in the deep neural network instead of the current one. This engendered the growth of Generative Models. There are 3 well known generative models as of today, namely, Pixel RNN / Pixel CNN, Variational Auto-encoders and Generative Adversarial Networks.\n\nAlthough we\u2019ve come a long way in developing cool technology pertaining to computer vision, in the long term, humans are much better at image understanding than any technology. This is because, machines are quite narrow sighted in the sense that they just learn stuff by going through a fixed category of images. Although they might have learned from a massive quantity of images (typically about a million for image-net challenges), it isn\u2019t anywhere close to what humans can do. I\u2019d mainly attribute this to the human brain, precisely to the neocortex in the human brain. Neocortex is the part of the brain responsible for recognizing patterns, cognition and other higher order functions such as perception. Our brain is so intricately designed that, it helps us remembering stuff without just directly dumping the required data into memory as in the case of a hard disk. The brain rather stores patterns of the stuff that we witness and later retrieves them if necessary.\n\nBesides, humans are continuously collecting data (for eg., collecting images through vision) at every point in their life unlike machines. Let\u2019s take an example. Most of us see dogs almost everyday. And this also means that we would\u2019ve seen dogs in different postures and from different angles. This means that, given an image which has dogs, there is a very very high probability that we\u2019d be able to recognize a dog in the picture. This isn\u2019t true for machines. Machines might\u2019ve been trained only for a certain amount of dog images and hence can be fooled easily. If you feed in the image of the same dog with a slightly different posture, it might get misclassified.\n\nWell, this been a very contentious topic in the past. Let\u2019s analyze it !\n\nIn a talk by Jeff Dean, he mentioned the number of parameters that make up a Deep neural network for most of the published ones since 2011. And if you\u2019d noticed, for humans, he mentioned \u201c100 trillion\u201d. Although he seemed to have considered that some what like a joke, that seems quite true given the amount of complex stuff the human brain can handle. Assuming that our brain is that complex, is it even practical to design a system with so many parameters ?\n\nWell, it definitely is true that there were some major breakthroughs in the field of Artificial Intelligence like AlphaGo beating a world champion in the game of Go, OpenAI\u2019s Dota2 bot beat experts in the game and many more. Yet, these sort of stuff seem very niche in the sense that, a Dota2 bot is very specific to Dota2 and not to anything else additionally. On the contrary, human brain is very generic. You\u2019d use your brain for almost all of your day to day activities. From this what I\u2019d like to infer is that, in order to compete with the mammalian brain, we\u2019ll need a General Artificial Intelligence !\n\nI\u2019d say using Reinforcement Learning (RL) (specifically Deep Reinforcement Learning:DRL) takes us a step closer to solving general intelligence. In RL, an agent itself discovers optimal ways to take actions by fiddling around with the environment. This also seems analogical to how humans learn stuff. Humans learn to do things by getting to know if their action was correct or not. In the same way, in reinforcement learning, the agent performs random actions and each action has with it an associated reward. The agent learns from the reward that it gets for the actions, i.e., the agent picks an action in such a way that the total future reward it gets is maximized.\n\nThis is currently an active area of research and involves giants like DeepMind and OpenAI. In fact, DeepMind\u2019s main motto is to \u201cSolve General Artificial Intelligence\u201d !"
    },
    {
        "url": "https://medium.com/deep-dimension/deferred-execution-models-33af978dd54a?source=---------4",
        "title": "Deferred Execution Models \u2013 Shravan\u2019s Blog \u2013",
        "text": "As a Google Intern at Hyderabad, I got introduced to a couple of really cool libraries (TensorFlow and FlumeJava) that use the so called Deferred Execution Model. I found some very interesting similarities between the two libraries and in this blog post, I\u2019ll seek to discuss on how deferred execution works by taking instances from the two libraries. I\u2019ll also discuss the pros and cons of Deferred Execution.\n\nThe term Defer simply means to postpone. This is exactly what happens in Deferred Execution too. Every variable or operation is just deferred on a dependency graph. The dependency graph is a directed acyclic graph. Each node in the graph represents either a variable or an operation. Each edge in the graph represents the dependency of a variable or an operation on another. This dependency graph can be executed starting from any node that you specify. Execution happens in such a way that, all dependency nodes (the nodes that the selected node would depend on) execute first and this follows up until finally your chosen node is executed. On the dependency graph, it\u2019s more like, the children nodes are recursively executed first in a bottom-up fashion. Algorithmically, this is just a Depth First Search starting from the node that you specify on the dependency graph, in which the nodes execute on post visit. For example, the binary add operation depends on 2 numbers (variables). So, in this case, there would be 3 nodes, one for a variable representing the first number, one for a variable representing the second number and then the add operation. There would be 2 edges, one between the first variable and the add operation, the other between the second variable and the add operation. Here, the edges simply mean that the add operation depends on each of the variables.\n\nLet\u2019s dive deeper by taking examples from Tensorflow and FlumeJava.\n\nIn Tensorflow, the dependency graph is formed by Operations and Tensors . A Tensor simply encapsulates a variable that is either explicitly specified or is returned by an operation. An Operation in Tensorflow simply takes input tensors and returns output tensors after some computation. These elements are essentially just deferred on a dependency graph and would be executed as per developer\u2019s requirement.\n\nI\u2019ll start with something as trivial as a binary add operation. I\u2019ll juxtapose immediate execution (more like an antonym for deferred execution) and deferred execution, so that you can get some clarity. In Python3, a simple binary add operation boils down to just 2 or 3 lines of code. So, let\u2019s directly jump into the execution diagram for immediate execution.\n\nThen let\u2019s see the case for deferred execution.\n\nAs you can see, that Python3 code snippet when executed would just give an output of \u201c11\u201d. Here\u2019s the dependency graph for the above code :\n\nThe explanation for the above graph is same as what we saw earlier. The binary add operation depends on 2 variables and they\u2019re represented here as 2 different Tensors.\n\nThe dependency graph for that example is quite simple. While working on large scale machine learning projects, it\u2019d be better if there\u2019s a tool to help us visualize the dependency graph. Tensorflow actually has this built-in tool named Tensorboard, which helps you visualize the graphs. Tensorboard comes as a command-line tool along with the pip installation of Tensorflow. In order to visualize the graphs, all you need to do is use to log the graph of the current session. In the above code, the 2 lines involving the writer identifier would log the graph of the current session into the logs as specified in the path. Here is the graph simulation from Tensorboard :\n\nHere\u2019s a more complex example in which a 2 hidden layered neural network is used to train a bunch of images from the MNIST training set and then one test image is retrieved from the dataset to check the accuracy.\n\nThe \u201cgradients\u201d node in the graph just computes the gradients and exchanges the required Tensor with the operation after evaluation. This node is implicitly created by Tensorflow.\n\nIf you notice in the above code, for running the neural network on test data, I\u2019ve used just . This means that, it\u2019d skip the part of the graph above fully_connected_2 in the diagram. This is a prime advantage of deferred execution. If not deferred execution, bringing in such abstractions could certainly be painstaking.\n\nThe line(s) with in Tensorflow would execute the dependency graph up until the specified nodes, which we saw earlier. Now, this would execute only the nodes that are required to be executed and skip the rest of those that wouldn\u2019t be a part of the graph path constructed using the specified nodes and their children. In a normal single CPU/GPU machine this would certainly run sequentially. In order to incur parallelism, some infrastructure set up is certainly required. Tensorflow wouldn\u2019t implicitly build strategies to optimize the graph execution for parallelism. The developer will have to explicitly allocate workers or GPUs in order to incur batch processing. Here are some useful links to run Tensorflow using multiple devices :\n\nFlumeJava (let\u2019s call it \u201cFlume\u201d from now on) runs on Google\u2019s infrastructure and was developed by Google researchers in the year 2010. Flume (not to be confused with Apache Flume) was primarily built for developing efficient data processing pipelines. The pipeline can consist of several MapReduces and Flume gives you very high level abstractions for managing such pipelines. You can think of Flume as a wrapper over Google\u2019s MapReduce tool and with additional functionality. Flume has a comprehensive documentation written in cloud.google.com.\n\nSimilar to Tensorflow, elements are deferred here as well. Individual deferred elements in Flume are called PObjects. For example, if you want a Long value to be deferred, you should represent it as in Java (PObject in general where, T is type of the deferred object). Most use cases of Flume involve deferring a Java collection and the deferred collections here are called PCollections. They\u2019re represented as where T is the type of each element in the collection. The PCollections aren\u2019t in memory as the regular Java collections. They\u2019re rather distributed. PCollections are usually read from sources like files or databases.\n\nThe simplest way to batch process the data in Flume is by using a DoFn. DoFn is simply a class which has an overridable void method named process. This is the method where you should write your computation code for one single element in the collection. In a DoFn, Flume creates multiple workers and assigns all of them to do the computation simultaneously. The DoFns are usually invoked inside a method called , which built into every PCollection. The DoFns can also be instantiated anonymously inside the parallelDo() function.\n\nFlume also provides better abstractions on top of DoFn like MapFn and FilterFn, which are just special cases of DoFn.\n\nHere\u2019s some sample Java code that uses Flume :\n\nHere\u2019s the dependency graph for the above code (only for the part inside function as that\u2019s the only thing that composes the graph nodes ) :\n\n[I don\u2019t have a lot of code to showcase here as I haven\u2019t used this technology outside Google]\n\nFor more information about the API, please refer to the following links :\n\nIn Flume, the analogy to is :\n\nUnlike Tensorflow, the value of the collection or object wouldn\u2019t be explicitly returned by . Instead, Flume would just materialize them. This means that, the computation is done and they can brought down to an in-memory variable or collection if required. Execution in Flume happens in a much more sophisticated way as compared to Tensorflow. This is mainly because, Flume uses Google\u2019s infrastructure for computation. In Flume, when you execute a PObject or PCollection using , it\u2019d automagically figure out the best possible strategy to run all the nodes in the dependency graph. Flume\u2019s strategies are so versatile that it even combines multiple operations (DoFns) into one for optimality.\n\nIf we take a deeper look at Flume\u2019s strategizing, we\u2019ll be able to find out that by strategizing, Flume is trying to solve the problem of DAG Scheduling. Now, for a variable number of processors, this is an NP-Complete problem. Hence, Flume technically wouldn\u2019t give you the most optimal strategy to execute a given graph. Yet, this is much much more optimal and efficient than the regular sequential execution.\n\nBy now, you\u2019d certainly have a decent idea on how deferred execution works. Let\u2019s discuss some merits and demerits of deferred execution."
    }
]