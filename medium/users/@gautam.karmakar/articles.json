[
    {
        "url": "https://medium.com/@gautam.karmakar/enterprise-data-science-how-to-do-it-right-4f94f3ee622a?source=user_profile---------1----------------",
        "title": "Enterprise Data Science \u2014 How to do it right. \u2013 Gautam Karmakar \u2013",
        "text": "Data science is critical for business success today but how to get it right is still not a complete science. Data science process often tends to be complex, interconnected with many different systems and requires collaboration across multiple teams. Moreover, most of the time key people, process and technology resources for data science spread into business and IT which adds to the complexity. On the other hand, methodology and technology behind data science process are rapidly evolving. To fully realize the potential of data science and establish a strong data Science practice requires well thought out strategy, standardization and an operating model for agile execution.\n\nThis article attempts to provide life cycle of data science process, the key requirement at each stage of the cyclic process and some technology examples of data scientist\u2019s most popular toolset.\n\nThe life cycle of data science process revolves around four major processes. It starts with a business objective, defined scope and clearly articulated expected outcome, KPIs from data science project. Data science process analyzes data and understands business functions to recommend the best action to be taken given a situation and environment which is described by new data.\n\nThese business understanding also includes understanding current processes by exploring current data sets. Data scientist performs several iterations of exploratory and interactive data analysis which requires to ingest, blend and process data from multiple data source systems. A typical example would be data from enterprise analytical system of Hadoop and spark systems or any relational database systems. Data could be internal to a company or could be external. A marketing team of data science wants to analyze Twitter data or Facebook posts to understand the sentiment of their customers or wants to track click on their recent promotional ad campaign. These require a standard process to ingest and integrate external data.\n\nData scientist iterate through multiple stages of exploratory data analysis, cleansing, normalizing and applying relevant metadata. This provides insights into data and exposes the functional impact of business operations for a data scientist to decide how data can be modelled for a specific problem. This is by far most complex, impactful and also time-consuming process in data science life cycle.\n\nModel development and evaluation process include model selection, training, evaluation and reporting. A typical data science model development process involves experimenting with multiple modelling techniques. For example, for classification problem data scientist may try decision tree, support vector machine or more advanced ensemble method to evaluate results in terms of different accuracy metrics, computational efficiency and generalizability.\n\nFeature Engineering: ML models requires human expertise and domain knowledge to generate usable features from raw data. Often times it requires significant processing and normalizing data, apply data manipulations to manufacture features.\n\nModel Selection: In this stage, the data scientist is selecting an appropriate model. Depending on the problem, it could be between 5\u201320 different algorithms to be explored to select optimum model.\n\nModel Training: Training each model using fast compute such as GPU or CPU HPC cluster, In memory cluster (spark) is required for speedy data fitting.\n\nEvaluation: In this stage, models are tested for generalization and tackle overfitting problem by tuning hyperparameters.\n\nModel Visualization & Reports: In this stage proper documentation using visualization of model performance, reasoning and feature importance required to be done.\n\nOnce a model is tested and evaluated and ready to move to production, a data scientist needs to collaborate with data engineers. Typically data engineers work with data scientist to convert data scientist output to a production scale implementation. He or she transfers the code and other artefacts into enterprise IT system, transform to more scalable, robust and efficient applications that also aligns with a current production system. For example, a model developed by data scientist in python or R could be transformed into more performant C++ application and made compatible to work as a distributed parallel application that can run on GPU cluster machines.\n\nAlso, data engineer create service module and interface for publishing model results by building APIs and web services.\n\nEnterprise needs to standardize processes, technology and toolsets used by different business and IT group to improve quality, reduce overhead and increase ROI. A matured organization will focus building a collaborative, reusable framework and operating model for successful execution and increase the value of data science for business.\n\nA unified Data Science and Machine Learning development and collaboration platform for a data analyst, data scientist and data engineers, bring data, explore and engineer,"
    },
    {
        "url": "https://medium.com/@gautam.karmakar/manhattan-lstm-model-for-text-similarity-2351f80d72f1?source=user_profile---------2----------------",
        "title": "Manhattan LSTM model for text similarity \u2013 Gautam Karmakar \u2013",
        "text": "A Brief Summary of Siamese Recurrent Architectures for Learning Sentence Similarity:\n\nOne of the important tasks for language understanding and information retrieval is to modeling underlying semantic similarity between words, phrases or sentences. However, problem remains hard because as having labeled data is scarce and understanding variable length complex data structure is difficult. Traditionally TF-IDF models ruled over several years in natural language processing but limited to understanding context by their inherent term-specificity.\n\nIn 2013, Mikolov et al had shown effectiveness in understanding semantic meaning of words using context they are used following the famous quote, \u201cYou shall know a word by the company it keeps.\u201d (Firth, J. R. 1957:11). Mikolov word2vec model (skip-gram & CBOW) proved effectiveness of distributional representation of words in contexts using neural network and revolutionized ability to understand expressiveness of natural language. Recently, it has been progressed from that individual word level understanding to sentence or text level understanding representing each sentence as fixed length vectors (Socher & Manning 2015).\n\nLong Short Term Memory model (Hochreiter & Schmidhuber, 1997) have been particularly successful in language translation and text classification tasks. LSTM model is built upon basic RNN model but avoiding one of the key limitation of RNN to work with long sequences due to vanishing gradients. LSTM develops a memory cell and use gates to decide how much information needs to be forgotten or need to flow through the time steps. In this way useful information can be kept and unnecessary information can be dropped. LSTM uses backpropagation through time (BPTT).\n\nLSTM model and its variants such as Gated Recurrent Unit (GRU) of Cho et al.(2014) have shown that if effectively trained can encode meaning of sentences into a fixed length vector representations.\n\nSiamese Neural Network and one shot image recognition proposed by Koch et al. is a different way of classifying image where instead of training one model to learn classify image inputs it trains two neural network that learns simultaneously to find similarity between images. The similar model is extended to text data using this model Siamese LSTM.\n\nIn original paper described the model as supervised learning where input is two sentence pairs having different sequence length and a label for the pair which describe underlying similarity between sentence pairs. It has shown that this algorithm produces a mapping from a general space f variable length sequences into interpretable representation with fixed dimensionality vector space.\n\nOne of the motivating task that can be performed by these representation is scoring similarity between representations of different sentences based on their semantic similarity in meaning.\n\nMaLSTM Model: Jump to code and save time if you wish here https://github.com/GKarmakar\n\nIn the model there are two identical LSTM network. LSTM is passed vector representations of sentences and output a hidden state encoding semantic meaning of the sentences. Subsequently these hidden states are compared using some similarity mechanism to output a similarity score.\n\nLSTM network learns jointly a mapping from the space of variable length sequences to encode as a fixed dimensional hidden state representation. Similarities in the representation space are subsequently used to infer the sentences underlying semantic similarity. In this model the similarity technique used is: g(h(a) Ta , h(b) Tb ) = exp(\u2212||h(a) Ta \u2212 h(b) Tb ||1) \u2208 [0, 1].\n\nA little girl is looking at a woman in costume.\n\nA young girl is looking at a woman in costume. 4.7 4.5 4.8\n\nThe performer is tricking a person on a motorcycle. 2.6 4.4 2.9\n\nSomeone is pouring ingredients into a pot.\n\nNobody is pouring ingredients into a pot.\n\nSomeone is pouring ingredients into a pot. 3.5 4.2 3.7\n\nLet\u2019s get to the implementation of MaLSTM model using Keras to find distance between two texts data:\n\n\u2018\u2019\u2019 Pre process and convert texts to a list of words \u2018\u2019\u2019\n\n#Prepare embedding of the data \u2014 I am using quora question pairs\n\n# Iterate through the text of both questions of the row\n\nif word in stops and word not in word2vec.vocab:\n\nif word not in vocabulary:\n\n# Replace questions as word to question as number representation\n\nI am using 300 dimension for my embedding i.e. there will 300 vectors for each word in the corpora represented for neural network model.\n\nembeddings[0] = 0 #padding will be ignored\n\nKeras doesn\u2019t come with Manhattan distance calculation, hence we need to write a routine to do that for us.\n\ndef exponent_neg_manhattan_distance(left, right):\n\n \u2018\u2019\u2019 Helper function for the similarity estimate of the LSTMs outputs\u2019\u2019\u2019\n\n return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))\n\n# Since this is a siamese network, both sides share the same LSTM\n\n# Calculates the distance as defined by the MaLSTM model\n\n# Pack it all up into a model\n\nWe need set an optimizer, I am using adadelta but any other popular optimizer such as RMSProp, Adam and even SGD could be tested to see if it increases accuracy, decreases training time by finding better local minima (yes, global minima is an elusive goal still).\n\nNow we will compile and train the model.\n\nThat\u2019s all for now, I will share the code in my github here: https://github.com/GKarmakar"
    },
    {
        "url": "https://medium.com/@gautam.karmakar/whats-next-after-deep-learning-specialization-from-deeplearning-ai-f615d6062e5d?source=user_profile---------3----------------",
        "title": "What\u2019s \u201cnext\u201d after Deep Learning specialization from deeplearning.ai?",
        "text": "I thought it may be a good idea that I summarize the \u201cnext thing\u201d after this deep learning specialization as there is n number of discussion around this by folks that completed or about to complete 5-course specialization.\n\nI think this might be the next several paths that can be taken to continue:\n\n1. Go to CS231n and CS244(d) if you haven\u2019t completed already. Watch videos, take notes, go to slides and reading suggestions and if possible try to complete assignments.\n\n2. fast.ai is a great place to go after. Preferably start with their part-1 and then move on to part-2. Watch Jerome\u2019s video, spend time in the forum, read wikis and if possible experiment with given notebooks with different datasets. Search or ask answers in the forum if you are stuck one something. There are plenty of great materials in fast.ai that will supplement your understanding and grip on many complex ML concepts, take full advantage of those besides notebooks. Look at their github, pickup one and reproduce that for another use case. Don\u2019t copy and paste code type every line (pain I know) and ask yourself can you explain this to others now?\n\n3. More advanced learners can go deep and go fundamentals such as the theory of deep learning https://stats385.github.io/ and understand how masters of the master researchers and scientist who spent their lifetime understanding intelligence and now explaining deep learning which sometime may seem like a miracle. There is a video series of statistical learning from MIT in youtube and http://www.mit.edu/~9.520/fall17/ where you will find Thomas Poggio and team (CBMM) created object detector using SVM for autonomous vehicle 20 years back and how it progressed today with millions time improvement. Read textbooks such as deeplearning book, ISLR, ESLR and PRML. I heard Yashua Bengio said more people bought this book than people can understand. There is only handful of people in this world who can understand deeplearning book. \n\n4. Practical hands-on experience can only get you a job, so folks who aren\u2019t working in AI already go for solving some Kaggle competitions and make sure you write about it, blog and MUST create a nice GitHub repo for showing off. Note that those are your resume.\n\nOkay, enough said. Feel free to add here. Happy learning."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/attention-for-neural-connectionist-machine-translation-b833d1e085a3?source=user_profile---------4----------------",
        "title": "Attention for Neural (connectionist) Machine Translation",
        "text": "A quick recap of encoder-decoder (seq2seq) model: In part 1 of this series https://medium.com/@gautam.karmakar/seq2seq-model-for-language-a099387ce837 I tried to explain RNN encoder decoder model first proposed by Cho et al in 2014. It uses two recurrent neural network. One encodes the input sequence into a fixed length vector representation and another decodes that representations into another sequence of symbols. They are jointly trained to increase conditional probability of the target sequence given the input sequence. In addition to standard log loss of recurrent neural network using conditional probabilities of phrase pairs computed by RNN encoder-decoder found to improve empirical performance.\n\nIn part 2 of the series https://medium.com/@gautam.karmakar/learning-phrase-representation-using-rnn-encoder-decoder-for-machine-translation-9171cd6a6574 we saw how vanilla encoder-decoder model is used for Neural Machine Translation\n\nProblem with encoder-decoder model: The problem with this model is that decoder has to learn to predict target sequence just from a fixed length vector created at the end of encoder steps. To encode all the relevant information of the input sequence especially for longer sequence to a single vector is problematic. Cho et al, 2014a showed that this problem of encoding information into a single vector increases or in other words performance of decoder decreases almost linearly as length of input sequence increases. Also seeing how human perform translation tasks it doesn\u2019t make sense that encoder needs to encode all the information once and decoder has to predict everything from that vector, because human translators does pay attention in parts of source sentence and then go back to source sentence as many times as needed to translate complete sentence. Hence intuitively it make sense for model not to depend on soley one vector of encoder to predict the complete translation.Attention model simulates this pattern of learning by looking back at the source when decoding. This provides shorter distance for decoder to source removing information bottleneck and we are no longer constrained to one single hidden state to learn decode.\n\nAttention Model: Attention model unlike basic encoder-decoder model pays attention to a subset of hidden state from encoder step for each word in the sentence. It adaptively learns to choose this subset at every decoding step. It also continue to use previous prediction like basic encoder-decoder model. Originally developed by Sutsekever et al, 2014 and Cho et al, 2014a called this alignment but later on attention caught on.\n\nIn this model at each hidden state h_i a attention score is calculated alpha<i> such that alpha_ti adds up to 1. Then it produces a context vector c_i which is weighted hidden state from time step i.\n\nIn encoder-decoder model encoder produces a fixed context vector C from input sequence {x1, x2, \u2026.x_Tx} where hiddent state at time step t,\n\nNormally a LSTM model is used for f and q. Decoder predicts next word in output sequence based on context vector C and previous predictions {y1, y2, \u2026..y_Ty}\n\nThere are also cases in papers where encoder is created using CNN architecture instead of standard LSTM.\n\nAs shown above in attentional mechanism of encoder-decoder architecture in practice a bidirectional stacked LSTM is used as encoder and similar decoder using stacked LSTM is designed.\n\nThe context vector is calculated as weighted sum of all input hidden states h_i. Note that each h_i encodes input upto that step and hence store local context and learned weights alpha_ti decides how important that hidden state is in order for decoder to predict next sequence as output.\n\nThe weights are calculated as none other than a softmax as shown below.\n\nThis is called alignment model in the paper. This scores how well inputs around position j and position i match. The score is based on the RNN hidden state S_i -1. In this model alignment is learned using feedforward network and not considered as latent variable.\n\nThe probability alpha_ij or its associated energy e_ij reflects to previous hidden state s_i-1 in deciding the next state s_i and generating y_i. Intuitively it acts as a attention in the decoder. Decoder learns to pay attention to subset of encoder hidden state at each time step and relieves encoder from the burden of storing all information of input sequence into a fixed length vector.\n\nInformation can be stored in sequence of annotations of hidden states that can be adaptively selected or rejected while decoding."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/learning-phrase-representation-using-rnn-encoder-decoder-for-machine-translation-9171cd6a6574?source=user_profile---------5----------------",
        "title": "Learning phrase representation using RNN Encoder-Decoder for Machine Translation",
        "text": "RNN encoder decoder model first proposed by Cho et al in 2014. It uses two recurrent neural network. One encodes the input sequence into a fixed length vector representation and another decodes that representations into another sequence of symbols. They are jointly trained to increase conditional probability of the target sequence given the input sequence. In addition to standard log loss of recurrent neural network using conditional probabilities of phrase pairs computed by RNN encoder-decoder found to improve empirical performance. The paper has shown that Encoder-Decoder model learns a semantically and syntactically meaningful representation of linguistic phrases.\n\nDeep neural network has shown great success in object recognition tasks. Furthermore it has been successful in NLP tasks such as language modeling (Bengio et al), word embeddings extraction (Mikolov et al) and paraphrase detection (Socher et al).\n\nEncoder-Decoder model maps the vector representation back to a variable length target sequence. Additionally, it proposes a sophisticated hidden unit in order to improve both the memory capacity and the ease of training.\n\nThe paper described that it uses a novel hidden unit is empirically evaluated on the task of translating from English to French. We train the model to learn the translation probabilities of an English phrase to a corresponding French phrase. The model is then used to part of a standard phrase based SMT system by scoring phrase pairs in the phrase table. Encoder-Decoder model is better in capturing linguistic regularities in the phrase table, indirectly explaining the quantitative improvement in the overall translation performance. It also learns continuous space representation of the phrase that preserves both the semantic and syntactic structure of the phrase.\n\nRNN is a neural network that consists of a hidden state h and an optional output y which operates on a variable length sequence x = (x1, \u2026..,xT). At each time step t, the hidden state h(t) of the RNN is updated by\n\nWhere f is a non-linear activation function. F may be as simple as an element wise logistic sigmoid function and as complex as a long short-term memory (LSTM) unit (Horchreiter and Schmidhuber, 1997).\n\nThe output at each time t is the conditional distribution p(xt|xt-1, \u2026.x1) For example, a multinomial distribution (1-of-K coding) can be output using a softmax activation function.\n\nFor all possible symbols j = 1, \u2026.,K where w(j) are the weight of the rows of a weight matrix W. by combining these for all symbols we compute probability of sequence x as\n\nFor this distribution we can sample output sequence conditioned on input sequences.\n\nSo, an encoder RNN (or its other variants such as LSTM, GRU) learns a fixed length vector representation from variable length sequence input (its vectorized equivalent such as embeddings). Decoder RNN (similar to encoder architecture) converts (or decodes) a variable length sequence output from the fixed length vector representation as explained in the picture bellow.\n\nHere we can make a note that sequence length of encoder input and decoder output (T, T\u2019) could be different as in the case of machine translation happens more frequently where length of a sentence of source language (or word) may not necessary have same length when translated into target language.\n\nAt each time step, encoder reads the input at that time step and also add hidden state from previous step. At the end of input sequence it outputs a hidden state that are meant to encode entire input sequence of both semantic and structural regularities in the input sequence. Note that here in this type of process there is no additional feature specific grammars syntax are applied and encoder RNN pretty much encodes the necessary features from the sequence itself. This is quite amazing in comparison with traditional statistical machine translation technique.\n\nDecoder on the other hand generates output y(t) at time step t given the final hidden state from encoder C and also depends on previous hidden state of decoder and previous output y(t-1) unlike RNN shown above (Eq. 1)\n\nh(t) = f(h(t-1), y(t-1), c) and so conditional distribution changes to:\n\nBoth encoder and decoder RNN are jointly trained with objective function:\n\nWhere theta is the set of model parameters and (xn, yn) represents input and output sequence. Typically as standard RNN this is differentiable and optimized using Gradient Descent algorithm.\n\nEncoder-decoder can be used for predicting a next sequence given a sequence as output and also generating a score for a given input-output sequence combination.\n\nCho also introduced GRU \u2014 Gated Recurrent Unit which is a special case of LSTM architecture and much simpler. The paper of encoder-decoder model proposed GRU to be used as RNN unit for both encoder and decoder architecture.\n\nThey proposed a reset gate and an update gate. Reset gate intuitively helping to forget what is not important and ensure to pay attention to what is current input.\n\nAs we see from the equation of the reset gate if reset gate vector is zero then Wr . h(t-1) get to become zero and in that case it ignores past sequence and only focus on current input Xt. This allows to ignore information that are irrelevant from past sequence. For example, in a movie review starts with describing the plot and lastly comment that it was a boring movie then all the past information that talks about plot of the movie wasn\u2019t informative to classify this review as negative, only information that is sufficient is where it says movie is boring.\n\nUpdate gate whereas allows to pay attention to all past sequences and balances reset gate\u2019s effect. It ensure to pay attention no matter how useful past sequences are. For example, it will take care a situation where movie review starts with saying \u201cI love this movie\u201d and then starts to get into movie details. The first sentence was good enough and needed to be memorized. Update gate just does that and as Z gets to be close to 1 network can just copy hidden vectors from past and also reduces possibility of vanishing gradient remarkably. Units with short term dependencies has reset gate very active and units with long term dependencies makes update gate very active. Also these effect of reset gate and update gate are learned jointly using back propagation and no need any special processing.\n\nEncoder-decoder model has been proved more robust and scalable compared to statistical machine translation model such as CSLM. They have compared RNN and also RNN + CSLM model and compared result. Paper has found that best phrase translation was found using both CSLM and phrase scores form RNN encoder-decoder model. RNN encoder-decoder model able to learn linguistic regularities whereas standard STM model learns statistics of word co-occurrences in the corpus and relied on the statistics.\n\nRNN encoder-decoder model able to learn from arbitrary length sequence and generate target sequence of arbitrary length. The model is very successful either predicting a target sequence based on a input sequence or output a probability score given a input and output pair. When tried to score for machine translation task out of phrase mapping table model demonstrated capability to understand linguistic regularities and able to propose well-formed target phrases. Current state of the art language model and many NLP tasks are based on the premise of this encoder-decoder model including machine translation and sentiment analysis."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/seq2seq-model-for-language-a099387ce837?source=user_profile---------6----------------",
        "title": "Seq2Seq Model for Language \u2013 Gautam Karmakar \u2013",
        "text": "Deep neural networks that are mainly feedforward fully connected neural network are powerful but not really appropriate for sequential data such as time series data or language. They are very good to map input data to discrete output or continuous variable but not sequence to sequence mapping. Seq2seq model learns from variable sequence input fixed length sequence output. It uses two LSTM model, one learns vector representation from input sequence of fixed dimensionality and another LSTM learns to decode from this input vector to target sequence. LSTM is a variant of recurrent neural network that solves problem of handling long sequences using different gates.\n\nSeq2seq model solves a specific limitation of deep neural network. DNN requires fixed length vector representation of input and output. But machine translation where input language is converted to target language input and output sentence length can vary. In question answering problem any length of input question needs to mapped to any length of answer sequence.\n\nLSTM models ability to learn from map between variable dimensional vector representations and ability to learn from long sequence makes it very useful for these type of problem where DNN aren\u2019t good fit.\n\nApplications: This type of encoder-decoder model are used in language translation, speech recognition and question-answering task. From the paper seq2seq model on WMT\u201914 dataset it reach BLEU score of 34.81 on english to french translation. BLEU, Bilingual Evaluation Understanding score is measuring technique used for neural machine translation. Human BLEU score is considered 1. One interesting finding on the paper is that reversing the order of the input sentence (not the target) remarkably improve the performance of LSTM. According to the paper this introduced many short term dependencies between source and target sentence which made the optimization problem easier.\n\nRecurrent Neural network (Hopfield, 1982) are connectionist models that capture the dynamics of the sequences by cycles in the network of nodes. Unlike feedforward neural network RNN can maintain a memory with a state that can represent the sequence of vectors of arbitrary length. In a way, RNN is generalization of neural network for sequence data.\n\nRecurrent neural network maps input sequence (x1, x2,\u2026..xT) to output sequence (y1, y2, \u2026.yT\u2019) and T and T\u2019 are the lengths of input and output sequences which may differ by iterating this equation:\n\nThe goal of LSTM is to learn conditional probability as per the equation:\n\nFirst a LSTM called encoder LSTM computes the conditional probability by first obtaining a fixed dimension V from the input sequence (x1, x2, \u2026.xT). V represents a hidden state for the second LSTM often called a decoder LSTM which learns conditional probability distribution for (y1, y2, \u2026.yT\u2019 as shown in the equation.\n\nThe distribution p(yt|v, y1, y2\u2026..yt-1) is represented with a softmax over all the words in the vocabulary. Each input and output sentences ends with a symbol <EOS> which enables network to learn distribution over all possible lengths e.g. the network learns from A, B, C, <EOS> to X, y, Z, <EOS> mapping.\n\nSeq2seq model uses two different LSTMs for input and output sequences. Doing so it increases number of parameters which improve learning at the minimal cost of increased computational cost. It helps network to learn multiple languages simultaneously. Also seq2seq model uses deel LSTM instead of shallow one, in fact the original paper says they used 4 layers LSTM. This helped to improve performance remarkably. Layers of LSTM with enough parameters get to learn different features from word vectors. Lastly, original author of the paper described that reversing the sequence of input vectors (not the output sequence) helped to improve accuracy. This means instead of mapping A,B,C, <EOS> they mapped C,B,A, <EOS> to X,Y,Z, <EOS>.\n\nThe inversing the sequence put A close to its mapping X, B to Y and C to Z. In this way SGD established closer communication between word vector pairs."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/linear-regression-using-neural-network-d8815324017f?source=user_profile---------7----------------",
        "title": "Regression using Neural Network \u2013 Gautam Karmakar \u2013",
        "text": "Keras a warpper API that runs on top of Tensorflow or theano is very popular and easy to use. Scikit learn also very popular libraries for machine learning.In this post I will show how to use keras and scikit learn to build neural network architecture in python and develop a regression linear model.\n\nDefine a base model to be used to build a model for regression using scikitlearn API KerasRegressor.\n\nNow we write a method for training the model we created above:\n\nDefine a method to visualize loss \u2014 we are using MSE loss for regression.\n\nMain method to perform data preprocessing such as replace null values, standardize data and split into train and test.\n\nCreate predictions and submission file for kaggle like submission.\n\nThe previous example showed how easy it is to wrap your deep learning model from Keras and use it in functions from the scikit-learn library.\n\nIn this example, we go a step further. The function that we specify to the build_fn argument when creating the KerasRegressor wrapper can take arguments. We can use these arguments to further customize the construction of the model. In addition, we know we can provide arguments to the fit() function.\n\nIn this example, we use a grid search to evaluate different configurations for our neural network model and report on the combination that provides the best-estimated performance.\n\nThe create_model() function is defined to take two arguments optimizer and init, both of which must have default values. This will allow us to evaluate the effect of using different optimization algorithms and weight initialization schemes for our network.\n\nAfter creating our model, we define arrays of values for the parameter we wish to search, specifically:\n\nOptimizers for searching different weight values. Initializers for preparing the network weights using different schemes. Epochs for training the model for a different number of exposures to the training dataset. Batches for varying the number of samples before a weight update. The options are specified into a dictionary and passed to the configuration of the GridSearchCV scikit-learn class. This class will evaluate a version of our neural network model for each combination of parameters (2 x 3 x 3 x 3 for the combinations of optimizers, initializations, epochs and batches). Each combination is then evaluated using the default of 3-fold stratified cross validation.\n\nThat is a lot of models and a lot of computation. This is not a scheme that you want to use lightly because of the time it will take. It may be useful for you to design small experiments with a smaller subset of your data that will complete in a reasonable time. This is reasonable in this case because of the small network and the small dataset (less than 1000 instances and 9 attributes).\n\nFinally, the performance and combination of configurations for the best model are displayed, followed by the performance of all combinations of parameters.\n\nThis might take about 5 minutes to complete on your workstation executed on the CPU (rather than CPU). running the example shows the results below.\n\nWe can see that the grid search discovered that using a uniform initialization scheme, rmsprop optimizer, 150 epochs and a batch size of 5 achieved the best cross-validation score of approximately 75% on this problem.\n\ndef gridSearch_neural_network(df_train, ytrain):\n\n # fix random seed for reproducibility\n\n seed = 7\n\n numpy.random.seed(seed)\n\n X_train, X_val, y_train, y_val = train_test_split(df_train, ytrain, test_size=0.1, random_state=42)\n\n \n\n print(\u201cTrain Data:\u201d, X_train.shape)\n\n print(\u201cTrain label:\u201d, y_train.shape)\n\n # evaluate model with standardized dataset\n\n estimator = KerasRegressor(build_fn=baseline_model, nb_epoch=100, batch_size=5, verbose=0)\n\n \n\n # grid search epochs, batch size and optimizer\n\n optimizers = [\u2018rmsprop\u2019, \u2018adam\u2019]\n\n dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n\n init = [\u2018glorot_uniform\u2019, \u2018normal\u2019, \u2018uniform\u2019]\n\n epochs = [50, 100, 150]\n\n batches = [5, 10, 20]\n\n weight_constraint = [1, 2, 3, 4, 5]\n\n param_grid = dict(optimizer=optimizers, \n\n dropout_rate=dropout_rate, \n\n epochs=epochs, \n\n batch_size=batches, \n\n weight_constraint=weight_constraint, \n\n init=init)\n\n \n\n grid = GridSearchCV(estimator=estimator, param_grid=param_grid)\n\n grid_result = grid.fit(X_train.values, y_train.values)\n\n # summarize results\n\n print(\u201cBest: %f using %s\u201d % (grid_result.best_score_, grid_result.best_params_))\n\n means = grid_result.cv_results_[\u2018mean_test_score\u2019]\n\n stds = grid_result.cv_results_[\u2018std_test_score\u2019]\n\n params = grid_result.cv_results_[\u2018params\u2019]\n\n for mean, stdev, param in zip(means, stds, params):\n\n print(\u201c%f (%f) with: %r\u201d % (mean, stdev, param))\n\nIn this post, you discovered how you can wrap your Keras deep learning models and use them in the scikit-learn general machine learning library.\n\nYou can see that using scikit-learn for standard machine learning operations such as model evaluation and model hyperparameter optimization can save a lot of time over implementing these schemes yourself.\n\nWrapping your model allowed you to leverage powerful tools from scikit-learn to fit your deep learning models into your general machine learning process"
    },
    {
        "url": "https://medium.com/@gautam.karmakar/ml-skills-for-beginners-only-b7fd6f5e1da8?source=user_profile---------8----------------",
        "title": "ML skills for beginners (only): \u2013 Gautam Karmakar \u2013",
        "text": "I have posted a blog on useful resource to gain knowledge in machine learning and deep learning https://www.facebook.com/groups/839604392844670/permalink/872030412935401/\n\nThis new posts is going to be about basics about approaching from without any background in machine learning.\n\nOne high level but very important point is that there are many ways of learning \u2014 one could be very theory oriented where someone is skillful to understand concepts and explain them to others with ease. Another expertise is to solve problem by writing applications using any programming language suitable for machine learning. These two skills can be combined and can become very powerful. So point that I am trying to make is learning to code and solve real life machine learning problem is very important or at least that is what I believe.\n\nBasically two word really. Learn python. There are resources that are free and online would take years to really get everything covered. But here are some useful way to think learn python as a programming language but for data science and machine learning tasks it is often beneficial to to data collection, cleaning and wrangling and generating features for ML models using python programming skills.\n\nMake sure you can code with python\u2019s data processing libraries like Numpy and Pandas. It is very important that you can communicate your audience and present your data and results using visualization so also learn matplotlib a great python data visualization library.\n\nHere are some resources that I used and found useful:\n\nI found matplotlib documentation and examples online are inadequate but there are several examples in stackoverflow that comes handy all the time.\n\nDatacamp (paid) is a great place to build these skills, introductory and intermediate level of python, pandas, numpy is very hands on way to gain expertise. Also there are tons of materials within various (free & paid) courses on edx.org, coursera.org that can be great place to learn these.\n\nBooks: for beginners I would suggest two great books. These books will take no assumption of any previous skills, Ok, I might be going out of the way here, some of the above notes will prepare you better with python data handling skills using python data types (list, dict, tuple), numpy and dataframes that will be required for these two books. Okay the books are\n\nIntroduction to machine learning using python by Andreas Muller and the very similar book and famous too which is Python Machine Learning by Sebastian Raschka.\n\nFor next series please check out:"
    },
    {
        "url": "https://medium.com/@gautam.karmakar/xgboost-model-to-win-kaggle-e12b35cd1aad?source=user_profile---------9----------------",
        "title": "XGBoost \u2014 Model to win Kaggle \u2013 Gautam Karmakar \u2013",
        "text": "I have recently used xgboost in one of my experiment of solving a linear regression problem predicting ranks of different funds relative to peer funds.\n\nXGBoost is a very popular modeling technique that is continuously wins kaggle competitions.I have never used it before this experiment so thought about writing my experience.\n\nInstall XGBoost: easy all I did is pip install xgboost but here is the official documents for further information XGBoost documentation website.\n\nNow there is really lot of great materials and tutorials, code examples of xgboost and hence I will just provide some of the links that I referred when I wanted to know about xgboost and learn how to use it.\n\nThe best source of information on XGBoost is the official GitHub repository for the project.\n\nOne of the great article that I learned most from was this an article in KDNuggets. XGBoost-Top ML methods for Kaggle Explained & Intro to XGBoost\n\nThis gives some overview about the model and I learnt that Tianqi Chen created this model.\n\nAlso this seems to be the official page for the model (my guess) has some basic information about the model XGBoost\n\nXGBoost supports three main form of Gradient Boosting such as:\n\nWhat it is good for:\n\nXGBoost implements Gradient Boosted Decision Tree Algorithm. Model boosting is a technique to use layers of models to correct the error made by the previous model until there is no further improvement can be done or a stopping criteria such as model performance metrics is used as threshold. There are various type of boosting algorithms and there are implementations in scikit learn like Gradient Boosted Regression and Classifier, Ada-boost algorithm.\n\nOne thing I want to highlight here is to understand most important parameters of the xgboost model like max_depth, min_child_weight, gamma, reg_alpha, subsample, colsmaple_bytree, lambda, learning_rate, objective.\n\nExperiment: As I said above I was working on a linear regression problem to predict rank of a fund relative to other funds:\n\nI have read train and test data and split them after shuffling them together to avoid any order in the data and induce required randomness.\n\nI also did mean imputing of the data to handle missing value but median or most frequent techniques also can be applied.\n\nSklearn has a great API that cam handy do handle data imputing http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n\nIn actual experiment there are additional feature engineering step that may not be relevant for any other problem because it is specific to this data and problem I was trying to solve.\n\nAfter that I split the data into train and validation set using again scikit learn train_test_split api.\n\nThen I have created a loop that will loop through three ensemble tree model to and choose best model depending on the lowest rmse score. Now as I was solving linear regression problem which will be tested using rmse error I used root mean squared error as my loss function to minimize.\n\nThis is a dictionary of all the model I wanted to try:\n\nAlso for each model I searched for best parameters using GridSearchCV of scikit learn as follows:\n\nBased on the winner model having lowest rmse on validation set I then predicted using test data and stored test prediction.\n\nNow at this time we are ready to submit our first model result using the following code to create submission file.\n\nBut I also tried to use xgboost after base model prediction is done. In this case instead of choosing best model and then its prediction, I captured prediction from all three models that were giving comparable performance and they were RandomForest, ExtraTreesRegressor and GradientBoostingRegressor.\n\nAfter that I applied xgboost model on top of the predicted value keeping each predictions as features and rank as target variable.\n\nParameter search using GridSearchCV for XgBoost using scikit learn XGBoostRegreesor API:\n\nparams = {\u2018min_child_weight\u2019:[4,5], \u2018gamma\u2019:[i/10.0 for i in range(3,6)], \u2018subsample\u2019:[i/10.0 for i in range(6,11)],\n\nNow here is the most interesting thing that I had to do is to try several different parameters to tune the model to its best. Most of the parameters that I tuned are max_depth, minchild_weight, learning_rate, lambda, gamm and alpha_reg. I was trying to reduce overfitting as much as possible as my training error was less than my test error tells me I was overfitting.\n\nHere are few notes on overfitting xgboost model:\n\nmax_dealth: I started with max_depth = 6 and then end up reducing it to 1 Now in general think 3\u20135 are good values.\n\nMin_child_weight: when overfitting try increase this value, I started with 1 but ended up with 10 but I think any value between 1\u20135 is good. Start with 1 and then if overfit try to increase it.\n\nreg_alpha, gamma and lambda are all to restrict large weight and thus reduce overfit. Normally they are good with very low value and even as 0.0 but try to increase little if we are overfitting.\n\nThere is also a important parameter that is num_boosting_rounds and that is difficult to tune. I tried many values and ended up using 1000. This parameter is similar to n_estimators (# of trees of ensemble tree models) hence very critical for model overfitting.\n\nHere is one great article I found really helpful to understand impact of different parameters and how to set their value to tune the model.\n\nFinal words: XGBoost is very powerful and no wonder why so many kaggle competition are won using this method. But it is very easy to overfit it very fast, hence to make model more general always use validation set to tune its parameters. Use GridSearchCV or cross_val_score from scikit learn to search parameter and for KFold cross validation. Start to solve underfitting problem first that means error on test set should be acceptable before you start handling overfitting and last word make note of all the observations of each tuning iterations so that you don\u2019t lose track or miss a pattern."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/my-experience-with-new-deep-learning-course-from-deeplearning-ai-coursera-585c6997401f?source=user_profile---------10----------------",
        "title": "My experience with new deep learning course from deeplearning.ai @coursera",
        "text": "My experience with new deep learning course from deeplearning.ai @coursera\n\nI am deeply intrigued by advancement of AI that is happening in recent years fueled by deep learning techniques. As of millions of others around the globe I also started learning about this technique and got more and more convinced maybe we are lucky to be around this time when AI is really taking off after many winters before. In fact when Richard Socher famous for his work in NLP and CS224D in stanford (A Deep Learning for NLP course) said that this may be AI spring, it bolstered my belief and I decided to invest more time into it.\n\nBeing a full time software professional mainly working in enterprise for developing analytics and data driven insights also motivated me to learn this techniques and look for opportunity to apply to understand data and help business applying predictive power of machine learning and deep learning.\n\nWith this motivation I have done some online MOOCs starting with Andrew Ng\u2019s Intro to ML at Coursera and I deeply fall in love. Initially it was hard for me because of unfamiliarity with Matlab but I worked through my way by searching and going through documentation. It took about two months for me to finish but It boosted my confidence that followed by further study this techniques from places like edx.org, coursera.org, udacity.com, udemy.com and online materials.\n\nRecently I encountered a course from Andrew Ng at coursera from his company deeplearning.ai. I immediately understood that when it is Andrew Ng definitely it will be an exciting learning opportunity. In the meantime I have gained some understanding of deep learning and already coded some solution using mostly python and tensor flow. But while learning mostly from online resources I knew that I am creating some knowledge debt for me because while I am applying this technique and have some understanding how they are impacting my model\u2019s performance sometime it was not very clear how they really work and mostly why they help deep learning models. Hence I had in my mind was waiting for a course like this where important concepts of deep learning will be explained in depth and step by step so that I can grasp this and apply in practice efficiently. I am lucky to have taken this course and it was exactly what I desired.\n\nDeep learning specialization is a 5 course program and the first one is about introduction to neural network and deep learning. I will break down my experience and some notes about the first course of the specialization and will post in future blogs about rest of the courses, because I am planning to\n\nBefore getting into the course details I would like mention that there are very few pre requisite of this course that I believe would help and this may be different from what course actually has listed.\n\nProgramming: Basic python programming skills is required. This means ability to write simple code which are not more than few lines, knowledge of numpy libraries is helpful.\n\nMath: Knowledge about linear algebra like matrix, vectors, addition, division and dot product of matrices will be helpful. If you are familiar of these operation using numpy library that would be really great. Also some familiarity of calculus like taking first order derivative of simple functions will be helpful.\n\nAll these are nice to have not really mandatory to pass the course as course will have enough to prepare you with required knowledge.\n\nFor math you can get help from khanacademy.org whenever needed to revisit some concepts.\n\nComparison with similar courses: There are many great courses for beginning a journey in neural network and deep learning. Few great of these courses are Deep Learning NanoDegree (DLND) @Udacity and fast.ai mooc part1 and part2.\n\nThe distinction of the course at coursera is that it really get into skins of things and take step by step approach from very simple to more complex bottom up. It will teach you to do forward propagation, defining and optimizing loss using backpropagation by gradient descent algorithm. But then it builds from that to get into building deep network, practical understanding of all hyper parameters and how to tune them, describing dropout, adam, rmsprop and all other most important concepts to build a practical efficient deep learning model.\n\nUdacity DLND is fast forward into quickly understand the concepts of neural network intuitively and without getting into math it directly takes off into coding part and let\u2019s you build sophisticated practical deep learning models. It stresses on programming network and not too much on understanding nuts and bolts.\n\nFast.ai believed to be taken top down approach, they use mostly keras APIs to build model and leverage transfer learning techniques. Then jeremy piece together different parts of the working model and explain and sometime provide resources for self learning. It is a fast track (as names says it all) for really developing very powerful models and utilize most advanced techniques for all its practical purpose.\n\nLet\u2019s get into the first course see what is in there:\n\nWeek-1: First week is really very introductory where Andrew familiarize us (students) with basics of the neural network without any math or getting into a lot of details which I believe he will in later weeks.\n\nHe explains what is a simple neural network and from a bird\u2019s eye view how it works. He describes supervised learning with neural network and informs us the most of the success of deep learning is still in the supervised learning space.\n\nHe reminds us of the fact that many of the research on neural network is decades old but why it is currently taking off. Fundamentally lots of data in the era of the smartphone connected the device, social media and exponential growth of data collection has made it possible to experiment with the large neural network.\n\nMassive compute power (invent of GPU in mainstream computing \u2014 thanks to game technology) has made it even easier to develop and train large models at a reasonable time and scientist finally continued to research as there is a lot of commercial and academic interest never seen before.\n\nHe also has a great interview >45 min with Godfather of Deep Learning Jeff Hinton was really enjoyable and informative. I came to know that Hinton is continuing to experiment a lot of his early ideas on mapping works of the brain into developing models like his circulation algorithm.\n\nThere is a quiz of some basic 10 questions mostly on the topic he discussed on the lectures.\n\nWith this, I conclude my update on week-1. See you next time for week-2.\n\nAs expected Andrew takes a bottom up approach, unlike many other similar deep learning MOOCs.\n\nHe lets you learn bit by bit of internal details and sew them together to bring the bigger picture.\n\nThis week is about learning logistic Regression and implement one from scratch. All you will be using Python, numpy, matplotlib (for visualization) and create a classifier.\n\nThe video lectures help you to understand most critical building blocks of logistic regression for binary classification.\n\nFirst, it explains how each neuron can be considered as logistic regression operation and logistic regression can be thought of as a shallow neural network.\n\nHow it works by explaining P(y=1|x) = sigmoid(W.T+X, b )\n\nIt goes step by step into details and (remember from the intro to ML) write all the derivation maths. He explains loss function used in Logistic Regression and why SSE come short. How it is a convex problem and how to use Gradient Descent to find global minima. How to calculate average loss over all training example (not SGD yet) and optimize loss by iterating update of weights and biases. Here Andrew explains some calculus for the driving derivative of loss function over model parameters and he explains knowledge of calculus is great to have but he will provide sufficient explanation of key areas needed to understand neural network through this course. He does an excellent job in explaining backpropagation algorithm using the chain rule. He explains backpropagation as a computational graph and I would say it is as good as CS231n (Karpathy class).\n\nThe most valuable part of this week for me was how he helps to implement vectorization of these operations and leverage numpy library as a tool. This is great as it saves a lot of computational cost for a large network and only practical. So I would stress the fact try avoiding \u201cfor loop\u201d and implement vector operation, it really reduces the number of operation as well as the number of lines of code.\n\nAt the end, there is a quiz and one practice program to go over few basic Python and numpy operation taught and needed for logistic regression. There is a one graded programming assignment to build a binary classifier to classify cat images.\n\nThis means you get to write your few lines of code to create activation function (sigmoid), calculate the cost, implement Gradient Descent to optimize your model and predict class using your model.\n\nAlso strongly recommend playing with num iterations, learning rate to see their effect on test accuracy, plot learning curve to see performance over them and gain intuition,\n\nWith this, I will conclude and wish everyone happy [Deep] learning!!!\n\nWeek-3:Week-3 is really interesting where we get to gain understanding of actual neural network from logistic regression from week-2. Here Andrew explains what is hidden layer of neural network and explains the reason to have that which wasn\u2019t there in logistic regression model. In fact he explains logistic regression could be thought of as simplest form of neural network without any hidden layer. Also he go over on the non linearity requirement of neural network and what it is needed. He explains how without nonlinearity neural network becomes a series of linear combinations of input data and whole network could be thought of as a single logistic regression and final weight (W) are the combinations of weights in the previous layers. He then justifies usage of nonlinearity in neural network to learn features that are more complex and also relations between input data and not just learn input observation in isolations.\n\nImportant aspects to note here that how neural network is built upon logistic regression with activation by using nonlinear function like sigmoid, tanh or ReLU.\n\nHe goes over various activation functions (most popular) and explains their strength and limitations. Like sigmoid has problem of saturation when input is very high or low which may arise depending on the data and can make gradient so small that learning becomes very very slow. But sigmoid squashes output to [0,1] and provides probability like interpretation. He explains how hyperbolic tangent solves that problem but it has zero gradient for negative input. He mentions that ReLU or Leaky ReLU probably mostly used nonlinear activation till date. Beside binary classification for which sigmoid could be better choice (two class output) pretty much ReLU is a good idea.\n\nThere is a quiz on basics of shallow neural network concepts and also a programming assignments on planar data to classify using a single layer neural network with ReLU in hidden layer and sigmoid in the output layer. As a tips, all I want to say that take notes and walk along with Andrew sir\u2019s maths and you should be fine. Pay close attention with vector operations of the implementation that later you will do it using numpy library.\n\nOk, with this I will conclude week-3 summary, best of luck everyone.\n\nThis is the last week of the first course of 5 course specialization. In this week we take the neural network and make it large in terms of number of layers. So far we had seen logistic regression which is neural network without hidden layer and nonlinear activation and one hidden layer neural network. In practical implementation we have see that we were achieving ~80% training accuracy, but now it is time to reduce bias in our model by creating bigger network.\n\nNote that making large model will enable learn more complex features from data and training accuracy will improve but if model size or number of neurons at each layer increases disproportionately with number of training observation then model will overfit and generalize poorly on test data.\n\nIn this week Andrew will explain how step by step same methods that we have learned in previous week \u2014 calculate logit Z = WX+b and activation A = g(Z) will be done layer after layer with correct dimension of W and b. Note that W will be always (#number of neuron in current layer, # of row of input) for first hidden layer and (#number of neuron in current node, # neuron in previous layer) for rest of the hidden layers before output layer. Bias b would be a vector (# of neuron in current layer, 1).\n\nThere is a programming exercise where you get to build a deeper network and test your model on training and test accuracy. You will try different number of layers and plot learning curve to see where you are getting better accuracy.\n\nWith this I will conclude my summary of the first course. I would like stress few pointers to get the most from this course:\n\n1> Pay attention at all times and rewind forward video to capture important points discussed in the video and explained in details.\n\n2> Take notes, write formulas, steps and work with Andrew as he writing all the formulas and steps.\n\n3> be sure to understand vector implementation of math operations, no company will pay you for writing for loop where vectorization was possible.\n\n4> Brush up your python skills a bit and more specifically numpy library for linear algebra operation like np.add, np.divide, np.dot, np.multiply, np.random.randn, np.linalg.norm etc. , learn about numpy broadcasting.\n\n5> play around with the code once your submission are graded and passed to see impact of different hyper parameters like learning rate, number of layers in the network etc.\n\n6> Google any specific concept you need to dig down little more.\n\nI have completed first three courses of this specialization that are available and planning to finish rest two \u2014 convolutional neural network and recurrent neural network and will post about them too."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/word-embedding-and-convolutional-neural-network-for-sentence-classification-89e3f300ce2f?source=user_profile---------11----------------",
        "title": "Word embedding and Convolutional Neural Network for Sentence Classification",
        "text": "Deep learning model architecture can be used to classify sentence based on semantic meaning of sentences. Deep learning has been reaching heights in computer vision and speech recognition tasks for a while now. But lately there are major success of deep learning techniques in natural language processing tasks like sentiment analysis, machine translation, text summarization etc. Much of deep learning work on natural language text is based on vector representation of words through language model. Deep learning pioneer researcher Yashua Bengio on 2003, Mikolov et al. in 2013 and Colbert et al. showed use of vector representation of word and use of classification on top of learned features from vector representation of words.\n\nWord embedding maps word to 1-V representation (V is vocabulary size) and then use hidden layer (Neural network) to learn semantic representation from word vectors and then use classifier at the end of the architecture. Hidden layers of neural network essentially act as a feature extractor representing word vectors into lower dimensional vector space and encode semantic meaning of the words.\n\nConvolutional Neural Network (Yann Lecun, 1998) has been very successful in computer vision tasks and also showed excellent results in natural language processing tasks like semantic parsing (Yih et al 2014), Search Query Retrieval (Shen et al 2014), Sentence Modeling and other traditional NLP tasks.\n\nIn this sentence classification tasks we will be using CNN model to classify sentences based on their meaning. Each sentences could be rules associated with fund disclosures. At first these sentences and words associated with it will be converted to its vector representation using pre trained GloVe or word2vec model. These models are trained with billions of words from google news (word2vec) or millions of wikipedia pages (GloVe).\n\nWe will keep these vectors as static vectors and show it as a universal feature extractor that can produce very good result. But as a next stage we can use fine tuned word vectors for task specific training. This will show that pre trained vectors can be utilized for various tasks different from original task where these vectors are trained for.\n\nIn our architecture we will use simple convolution layer on top of learned vector embeddings of words. First words/sentences will be passed to the embedding layer to generate vector. We will add convolutional layer on top of it with filters of multiple sizes. This will extract features and encode semantic meaning of words and sentences. After that we will add softmax layer to classify sentences.\n\nHere is some snapshot of codes that will be used to build out model architecture:\n\nWe will use dropout as regularizer for our model for fair amount of generalization. We will see batchnormalization adds any value of not for NLP tasks. We will try both ADAM or softmax_cross_entropy_with_logits loss function to see the accuracy of prediction comparison.\n\nWe will visualize result with tensorboard if time permits we will also use t-sne presentation."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/some-useful-resource-for-machine-learning-deep-learning-9cba82f1bf92?source=user_profile---------12----------------",
        "title": "Some useful resource for Machine Learning & Deep Learning",
        "text": "If you are here that means you are already aware of Deep Learning. AI by now has a long history and it went through several waves of ups and downs. I would refer this video presentation by Frank Chen\u2019s Artificial Intelligence & Deep Learning where he beautifully and informatively walked through the history of AI and why it is making headlines now.\n\nArtificial Intelligence and mainly Machine Learning (A Data Driven technique to solve AI problems) is every part of our digital life now. It is everywhere, in our Smart Phones, Television, News Broadcast, Social Media. It is also the most actively researched field of computer science.\n\nArtificial Intelligence using Machine Learning is solving many of these problems that were thought of impossible to solve before starting from handwritten digit recognition, object detection in images, Speech Recognition, Machine Translation to Medical Diagnosis and Self Driving cars.\n\nBut what is that makes it so much successful now? There are many aspects to the answer of this question but I would highlight some of the most important breakthroughs that really catapulted these capabilities.\n\nDeep Learning is based on something called Artificial Neural Network which has its own long history of development over decades.\n\nThere is definitely growing interest to learn Machine Learning and more specifically Deep Learning to understand and potentially contribute to this great journey of human to understand intelligence by creating it in machines. But how one can start and ride this journey?\n\nI will try in this blog is what have been my journey so far and what I have done to learn Machine Learning and Deep Learning.\n\nAlso, would like to highlight here that there are really two aspects of learning \u2014 one about the understanding theory behind machine learning and the other is the able to implement this learning as solutions to real life problems. I will try to cover both aspects from my personal experience.\n\nThere are also many ways to learn e.g. Books, Tutorials, Courses (online) and I will provide these as they are easily available.\n\nProbability & Statistics: Probability and Statistics are immensely important for machine learning because machine learning could be thought of a subfield of statistical learning.\n\nBook: I recommend this book Probability and Statistics for Machine Learning (Fundamental and Advanced Topics) By Dasgupta.\n\nCourse: Intro to Statistics @Udacity as a short course on a quick walkthrough on important topics.\n\nAlso, check this more intense course on the same Edx Course on Probability & Statistics\n\nLinear Algebra: This is very important to learn for Deep Learning. Knowing matrix operations, linear transformation will help to clear some of the computation in Deep Learning.\n\nI would refer MIT Professor Gilbert Strang\u2019s class which is freely available in Youtube Linear Algebra by MIT \u2014 Gilbert Strang\n\nThis is very comprehensive and takes a lot of time to complete, hence as an alternative topic by topic learning from khanacademy.org will also help.\n\nAlso, need to have a good understanding of calculus like differential and integral calculus, partial derivatives, chain rule are important to understanding.\n\nI also recommend reading these books for gaining a solid understanding of the theoretical and practical foundation of machine learning.\n\nAn Introduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani\n\nElements of Statistical Learning by Trevor Hastie and Robert Tibshirani\n\nCoding skills: Programming skill is very important for learning machine learning, it is an absolute must.There are really many programming languages that could be used for developing machine learning solutions.But as a starter Python and R are two really popular programs. I personally prefer Python over R but R is very powerful for statistical data analysis and visualization.\n\nHere are some resources that I used for python.\n\nMachine Learning: There is ocean of resources for getting started in machine learning. But I would only mention that I used so far I liked.\n\nPython Machine Learning by Sebastian Raschka & Machine Learning with R by Brett Lantz\n\nCourse on Edx: Programming with Python for Data Science\n\nAnd of course the most popular and very good course by Andrew Ng, Stanford Professor\n\nAlso, I have referred this book often to brush my understanding Machine Learning, An Algorithmic Perspective by Chapman & Hall\n\nFor theoretical in-depth understanding for most of the popular models like Regression, Clustering, and Classification another great course I would suggest machine learning course from the University of Columbia is Machine Learning on Edx.\n\nLast but not the least is this 4 parts courses from the University of Washington in Coursera which is by name is a Machine Learning Specialization using hands-on practical (using Graphlab software package) Machine Learning Specialization in Coursera.\n\nI also highly recommend this theory heavy course from caltech which is very popular to my knowledge to gain in-depth understanding of machine learning principles and theory behind.https://work.caltech.edu/telecourse.html\n\nI am not a big fan of R and don\u2019t like it as much as Python. For R I use https://www.amazon.com/Machine-Learning-R-Brett-Lantz/dp/1782162143 and also there is a great course in edx.org by MIT https://www.edx.org/course/analytics-edge-mitx-15-071x-3\n\nAlso, here is a very good list of free resources that can be very helpful.\n\nDeep learning uses neural network technique but in much larger in size and complexity to understand hidden structure from raw data to produce representation and widely used for advanced machine learning applications. There is also growing the number of very useful resources for learning deep learning techniques in the world wide web. It is most complex but also most powerful effective ML model out there so learning deep learning worth it.\n\nI would like to highlight for deep learning is really understood some more math concepts to really get to know how different sections of deep learning work.\n\nCourses: Intro level deep learning course at Udacity is really great and very practical will teach you a basic intuition of ANN, CNN, RNN and its LSTM variant.\n\nAlso, udemy.com has this good short course on deep learning basics with hands-on exercises Deep Learning A-Z on Udemy.com\n\nLectures & Tutorials: I have followed lectures of Prof Ali Ghodsi https://www.youtube.com/watch?v=fyAZszlPphs&list=PLehuLRPyt1Hyi78UOkMPWCGRxGcA9NVOE in youtube for various ML and Deep Learning topics.\n\nThere is also excellent lectures (math heavy) by Hugo in youtube that I have worked through to understand much important math and theory behind deep learning algorithms https://www.youtube.com/watch?v=rxKrCa4bg1I\n\nI refer machine learning mastery for short excellent blog posts. Also, there is wildml.com & Nervana.com has some great blogs. Following some machine learning experts, researchers in Reddit and Twitter expose a lot of useful information.\n\nThe latest book by Ian Goodfellow et al.http://www.deeplearningbook.org/ is an excellent book to understand probably all of the theory behind deep learning. This book starts with basic and then build upon deep learning like CNN, RNN, LSTM, Representation Learning, Energy Based Methods, Generative Models.\n\nFor computer vision using deep learning Stanford\u2019s Andrej Karpathy\u2019s CS231N course (Videos are available in Youtube.com) is an excellent resource. This course will also teach you fundamentals of deep learning like perception, backpropagation and also deep dive into a convolutional neural network which is state of the art for the image.\n\nFor natural language processing, the best course is again from Stanford\u2019s course taught by Chris Manning and Richard Socher CS224D.\n\nThese course videos are free and excellent resource for learning state of the art techniques for Computer Vision & Natural Language.\n\nAlso, https://nlp.stanford.edu/fsnlp/promo/ book is an awesome resource for understanding Natural Language Processing.\n\nThere are much deep learning frameworks and my personal favorite is Tensoflow. Tensorflow is a good balance in creating an abstraction of complex computation yet give you enough flexibility to optimize your application and model. You can learn tensoflow from https://www.tensorflow.org/ and also there are many online resources (codes are written by others) to learn from.\n\nAnother of my favorite is keras https://keras.io/ which can run on Tenorflow, Theano (another framework like tensorflow) and CNTK (Deep learning framework from Microsoft). Keras works as an API layer on top this framework and further simplify deep learning implementations.\n\nApart from that, there are caffe, torch and pytorch which are also good to be familiar with.\n\nI want to highlight that there are a plethora of free and paid resources on the internet on machine learning and deep learning. Choosing the right resource and using them in the proper sequence is very important. Theory and practice are both equally important. Hence I would recommend starting coding as you are reading books, taking a course or learning from tutorials, blog posts etc. Get a good grip on Python and it will be worthwhile.\n\nI recommend to learn from multiple sources for a concept or implementation and build upon it as you learn through beginner, intermediate to advanced level. While taking any course through MOOC make sure to complete all assignments, participate in discussion forum even though you may not need any help. Get certified as much as you can to ensure you keep motivated yourself. Also use Twitter, Quora, Facebook groups, Reddit groups to get updated with the latest information on Machine learning. Read papers when you can even though you may not understand all the concepts and complexities. https://arxiv.org/ from CMU contains all the paper released on Machine learning, deep learning and any AI related topics is very good place to keep looking for good papers.\n\nI believe in self-learning and a methodical approach to learning from these and other resources are definitely going to build your skills at a very nice level that you can be proud of.\n\nPlease let me know what are other useful resources that are out there."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/https-medium-com-gautam-karmakar-logistic-regression-using-tensorflow-f7220e067477-c2dde5c6c73b?source=user_profile---------13----------------",
        "title": "Single Layer Neural Network using Tensorflow. \u2013 Gautam Karmakar \u2013",
        "text": "In the first post https://medium.com/@gautam.karmakar/logistic-regression-using-tensorflow-f7220e067477 I have created a simple logistic regression to classify character data. In this post we will use same dataset but instead of logistic regression we will use single layer neural network. Note that this neural network will be based on previous logistic regression and will also create a classifier but use a intermediate hidden layer. Let\u2019s get started.\n\nOne note that we will be using sigmoid function as activation function. Sigmoid function squashes very small to very large number within [0,1] range and provides a nice representation as probabilistic outcome.\n\nLets get to to code:\n\nHere I am pasting most of the same code to load packages and data as before.\n\n#Lets look at some data of letter A\n\nNow lets convert categorical label to it\u2019s one hot representation.\n\ndef to_onehot(labels, nclasses = 5):\n\n \u2018\u2019\u2019\n\n Convert labels to \u201cone hot\u201d format\n\n \u2018\u2019\u2019\n\n outputlabels = np.zeros((len(labels), nclasses))\n\n for i, l in enumerate(labels):\n\n outputlabels[i, l] = 1\n\n return outputlabels\n\nSplit the data into train and test set:\n\n# These will be inputs\n\n## Input pixels, flattened\n\nx = tf.placeholder(\u201cfloat\u201d, [None, 1296])\n\n## Known labels\n\ny_ = tf.placeholder(\u201cfloat\u201d, [None,5])\n\nWe will use single hidden layer with 128 neurons.\n\n# Actually train\n\nepochs = 10000\n\ntrain_acc = np.zeros(epochs//10)\n\ntest_acc = np.zeros(epochs//10)\n\nfor i in tqdm(range(epochs), ascii=True):\n\n if i % 10 == 0: # Record summary data, and the accuracy\n\n # Check accuracy on train set\n\n train_accuracy = accuracy.eval(feed_dict={x: train.reshape([-1,1296]), y_: onehot_train})\n\n train_acc[i//10] = train-acc\n\n# And now the validation set\n\n test_accuracy= accuracy.eval(feed_dict={x: test.reshape([-1,1296]), y_: onehot_test})\n\n test_acc[i//10] = test_accuracy\n\n train_step.run(feed_dict={x: train.reshape([-1,1296]), y_: onehot_train})\n\nAfter model is trained and train and test accuracy are computed let\u2019s plot accuracy and look at confusion matrix."
    },
    {
        "url": "https://medium.com/@gautam.karmakar/logistic-regression-using-tensorflow-f7220e067477?source=user_profile---------14----------------",
        "title": "Logistic Regression using Tensorflow \u2013 Gautam Karmakar \u2013",
        "text": "Google has open sourced Tensorflow, a computational graph based language to develop production scale machine learning applications. There is many valuable tutorial on their official website at https://www.tensorflow.org/tutorials/. They have installation instruction and tutorials on image recognition using Convolution Neural Network, word2vec model implementation and seq2seq model for machine translation. I am going to share a simple logistic regression.\n\nWe will be using to predict labels of hand written digit as a supervised machine learning technique. Model will learn from training data with labels for each digits and will be able to predict for new digits.\n\nFirst we will import required libraries required.\n\nAt the beginning we will look into some basic tensor operations using tensorflow to get familiraty.\n\nA common operation in logistic regression is to use sigmoid function that can squash output between [0,1] which can be interpreted as probability.\n\nWe can also define constants\n\nOkay, now we can get into simple logistic regression using tensorflow:\n\nFirst we will use digits with lables numpy file\n\nLook at some data\n\nWe need to convert labels to one hot vectors to work with tensorflow\n\nWe will split data into train and test\n\nWe need to define loss in order to optimize parameters against minimizing loss which will reduce difference between predicted and true labels.\n\nDefine a train step: Great that tensorflow provides a inbuilt function to minimize loss using Gradient Descent optmizer.\n\nWe have done model building now we can start training this model for epochs number as iteration.\n\nWe can plot accuracy curve and see training and test accuracy for possible overfit.\n\nAlso we could look into confusion matrix which will show us true and false positive for classification.\n\nWe can also see some weights"
    }
]