[
    {
        "url": "https://medium.com/@thushv89/stock-market-predictions-with-lstm-in-python-ec97b0e5bd92?source=user_profile---------1----------------",
        "title": "Stock Market Predictions with LSTM in TensorFlow \u2013 Thushan Ganegedara \u2013",
        "text": "This is a tutorial on how to use LSTMs for stock price movement prediction. I have seen quite a few tutorials on using LSTMs for stock price predictions and sadly most of them perform quite poorly. And the ones that actually work are sometimes poorly documented, so one can easily get lost in the wilderness of unforgiving-thorny-unexplained details.\n\nThe tutorial starts by looking at some basic averaging methods that can be used to model stock price movements. However these averaging methods rapidly become impotent when we need to predict multiple steps ahead with existing data.\n\nOn the other hand, LSTMs are much powerful in learning patterns in time-series data and highly regarded for their ability to persist long term memory sometimes for hundreds of time steps. Here we investigate how we can use LSTMs to decently predict stock price movements. This is what the results look like at the end of learning. Pretty impressive huh?\n\nNote: If you enjoy the tutorial, make sure to upvote the post on DataCamp website. :) I will be soon sharing the interactive Jupyter notebook as well."
    },
    {
        "url": "https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-understanding-kl-divergence-2b382ca2b2a8?source=user_profile---------2----------------",
        "title": "Light on Math Machine Learning: Intuitive Guide to Understanding KL Divergence",
        "text": "I\u2019m starting a new series of blog articles following a beginner friendly approach to understanding some of the challenging concepts in machine learning. To start with, we will start with KL divergence.\n\nFirst of all let us build some ground rules. We will define few things we need to know like the back of our hands to understand KL divergence.\n\nBy distribution we refer to different things such as data distributions or probability distributions. Here we are interested in probability distributions. Imagine you draw two axis (that is, X and Y) on a paper, I like to imagine a distribution as a thread dropped between the two axis; X and Y. X represents different values you are interested in obtaining probabilities for. Y represents the probability of observing some value on the X axis (that is, y=p(x)). I visualize this below.\n\nThis is a continuous probability distribution. For example think of axis X as the height or a human and Y as the probability of finding a person with that height.\n\nIf you want to make this probability distribution discrete, you cut this thread to fixed length pieces and turn the pieces in such a way that they are horizontal. And then create rectangles connecting the edges of each piece of thread and the x-axis. That is a discrete probability distribution.\n\nFor a discrete probability distribution, an event is you observing X taking some value (e.g. X=1). Let us call P(X=1) probability of the event X=1. In continuous space you can think of this as a range of values (e.g. 0.95< X<1.05). Note that the definition of an event is not restricted to the values it takes on the x-axis. However we can move forward considering only that.\n\nTo continue from this point onwards, I will be humbly using the example found in this blog post [1]. It is a great post explaining the KL divergence, but felt some of the intricacies in the explanation can be explained in more detail. All right let\u2019s get into it.\n\nSo the gist of the problem that is being solved in [1] is that, we\u2019re a group of scientists visiting the vast outer-space and we discovered some space worms. These space worms have varying number of teeth. Now we need to send this information back to earth. But sending information from space to earth is expensive. So we need need to represent this information with a minimum amount of information. A great way to do this is, instead of recording individual numbers, we draw a plot where X axis is different numbers of teeth that has been observed (0,1,2,\u2026, etc.) and make Y axis the probability of seeing a worm with x many teeth (that is, number of worms with x teeth / total number of worms). We have converted our observations to a distribution.\n\nThis distribution is efficient than sending information about individual worms. But we can do better. We can represent this distribution with a known distribution (e.g. uniform, binomial, normal, etc.). For example, if we represent the true distribution with a uniform distribution, we only need to send two pieces of information to recover true data; the uniform probability and the number of worms. But how do we know which distribution explains the true distribution better? Well that\u2019s where the KL divergence comes in.\n\nSo we could use the KL divergence to make sure that we matched the true distribution with some simple-to-explain and well-known distribution well.\n\nTo be able to check numerical correctness, let us change probability values to more human friendly values (compared to the values used in [1]). We will assume the following. Say we have 100 worms. And we have following types of worms in following amounts.\n\nQuick sanity check! Let\u2019s ensure that the values add up to 100 and probability add up to 1.0.\n\nHere\u2019s what it looks visually.\n\nNow that out of the way, let us first try to model this distribution with a uniform distribution. A uniform distribution has only a single parameter; the uniform probability; the probability of a given event happening.\n\nThis is what the uniform distribution and the true distribution side-by-side looks like.\n\nLet us keep this result aside and we will model the true distribution with another type of distributions.\n\nYou are probably familiar with the binomial probability through calculating probabilities of a coin landing on it\u2019s head. We can extend the same concept to our problem. For a coin you have two possible outputs and assuming the probability of the coin landing on its head is p and you run this experiment for n trials, the probability getting k successes is given by,\n\nLet\u2019s take a side trip and understand each term in the binomial distribution and see if they make sense. The first term is p^k. We want to get k successes, where the probability of a single success is p. Then the probability of getting k successes is p^k. Remember that we\u2019re running the experiment for n trials. Therefore, there\u2019s going to be n-k failed trials, with a failure probability of (1-p). So the probability of getting k successes is the joint probability of p^k (1-p)^{n-k}. Our work doesn\u2019t end here. There are different permutations the k trials can take place within the n trials. The number of different permutations k elements to be arranged within n spaces is given by,\n\nMultiplying all these together gives us the binomial probability of k successes.\n\nWe can also define a mean and a variance for a binomial distribution. These are given by,\n\nWhat does the mean reflect? Mean is the expected (average) number of successes you get if you run n trials. If each trial has a success probability of p it make sense to say you will get np trials if you run n trials. Next what does the variance represent. It represents how much the true number of success trials to deviate from the mean value. To understand the variance, let us assume n=1. Then the equation is, variance=p(1-p). You have the highest variance when p=0.5 (when it is equally likely to get heads and tail) and lowest when p=1 or p=0 (when for sure you\u2019re getting head/tail).\n\nNow with a solid understanding about the binomial distribution, let us spiral back to the problem at our hands. Let us first calculate the expected number of teeth for the worms. It would be,\n\nWith mean known, we can calculate p where,\n\nmean = np\n\n 5.44 = 10p\n\n p = 0.544\n\nNote than n is the maximum number of teeth observed from the population of worms. You might ask why we did not choose n to be the total number of worms (that is 100) or total number of events (that is 11). We will soon see the reason. With that, we can define probabilities of any number of teeth as follows.\n\nFrom the perspective of the coin flip, this is like asking,\n\nFormally, we calculate the probability pk^{bi} for all different values of k. Here k becomes the number of teeth we would like to observe. And pk^{bi} is the binomial probabilities for the k th bin of teeth (that is, 0 teeth, 1 tooth, etc.). So when we calculate them as follows,\n\nThis is what a comparison between the true distribution and the binomial distribution looks like.\n\nOkey, turn back and reflect on what we did so far. First we understood the problem we want to solve. The problem is to send statistics of teeth of a certain type of space worms across the space with minimal effort. For that we thought of representing the true statistics of worms with some known distribution, so we can just send the parameter of that distribution instead of true statistics. We looked at two types of distributions and came up with the following statistics.\n\n\n\n* Uniform distribution \u2014 with probability of 0.0909\n\n* Binomial distribution \u2014 with n=10, p=0.544 and k taking different values between 0 to 10\n\nNow let\u2019s visualize everything in one place\n\nNow with all these fancy calculations, we need a way to measure the matching between each approximated distribution and the true distribution. This is important, so that, when we send the information across, we can have a peace of mind without worrying about the question \u201cdid I choose correctly?\u201d for the rest of our lives.\n\nThis is where the KL divergence comes in. KL divergence is formally defined as follows.\n\nHere q(x) is the approximation and p(x) is the true distribution we\u2019re interested in matching q(x) to. Intuitively this measures the how much a given arbitrary distribution is away from the true distribution. If two distributions perfectly match, D_{KL} (p||q) = 0 otherwise it can take values between 0 and \u221e. Lower the KL divergence value, the better we have matched the true distribution with our approximation.\n\nLet\u2019s look at the KL divergence piece by piece. First take the log(p(x_i)/q(x_i)) component. What happens if q(x_i) is higher than p(x_i)? Then this component will produce a negative value (because log of less than 1 values are negative). On the other hand if q(x_i) is always smaller than p(x_i) this component will produce positive values. This will be zero only if p(x_i)=q(x_i). Then to make this an expected value, you weight the log component with p(x_i). This means that, matching areas where p(x_i) has higher probability is more important than matching areas with low p(x_i) probability.\n\nIntuitively it makes sense to give priority to correctly match the truly highly probable events in the approximation. Mathematically, this allows you to automatically ignore the areas of the distribution that falls outside of the support (support is the full length on the x axis used by a distribution) of the true distribution. Additionally this avoid calculating log(0) that will come up if you try to compute the log component for any area that falls outside of the support of the true distribution.\n\nLet us now compute the KL divergence for each of the approximate distributions we came up with. First let\u2019s take the uniform distribution.\n\nNow for the binomial distribution we get,\n\nLet\u2019s just play around with the KL divergence now. First we will see how the KL divergence changes when the success probability of the binomial distribution changes. Unfortunately we cannot do the same with the uniform distribution because we cannot change the probability as n is fixed.\n\nYou can see that as we are moving away from our choice (red dot), the KL divergence rapidly increases. In fact, if you print some of the KL divergence values small \u0394 amount away from our choice, you will see that our choice of the success probability gives the minimum KL divergence.\n\nNow we arrive to the end of our discussion about KL divergence.\n\nNow we have some solid results, though the uniform distribution appears to be simple and very uninformative where the binomial distribution carries more subtlety, the uniform distribution matches the true distribution better than the binomial distribution. To be honest, this result actually took me by surprise. Because I expected the binomial to model the true distribution better. Therefore, this teaches us the important less of why we should not trust our instincts alone!\n\nYou can have more fun around playing with the KL divergence to understand KL divergence better. You can read more about this in my blog post.\n\nNote: Please go and checkout my website as I post more machine learning stuff there as well."
    },
    {
        "url": "https://towardsdatascience.com/bringing-computer-vision-datasets-to-a-single-format-step-towards-consistency-870b8323bcf0?source=user_profile---------3----------------",
        "title": "Bringing Computer Vision Datasets to a Single Format: Step towards Consistency",
        "text": "When you have a good working algorithm and you want to test your masterpiece on some dataset, almost always have to spend quite a lot of time on actual loading and preprocessing of data. It would be quite nice if we could have all the data in one single format and consistent way of accessing data (e.g. always store training images under the key \u201ctrain/image\u201d).\n\nHere I\u2019ll be sharing a github repo written by me that converts several popular datasets into HDF5 format. Currently supports following datasets.\n\nSo this repository does quite a few things. First let me tell you the organization. Code base is pretty simple. It has a single file for each dataset to preprocess data and save as HDF5 (e.g. for Imagenet we have , CIFAR-10 and CIFAR-100 we have and for SVHN we have ). Essentially each file does the following:\n\nBelow I\u2019m gonna tell what the ImageNet file does. This is the most complicated file and the others are quite straight-forward.\n\nHere I discuss what file does. This basically saves a subset of ImageNet data as a HDF5 file. This subset is a data belonging to number of natural classes (e.g. plant, cat) and artificial classes (e.g. chair, desk). Furthermore you can normalize data while saving data.\n\nThe function takes over as soon as you run the script. This function first create a mapping between the valid dataset filenames and labels (i.e. . Next it isolates the classes related to the classification problem of the ImageNet dataset (1000 classes) with or . Then we write the selected artificial and natural class information to an xml file using method.\n\nNext we sweep through all the subdirectories in the training data and get all the related data points into the memory. Next we create HDF files to save data. this is done with the function. The data will be saved under the following keys:\n\nYou can access this saved data later as:\n\nThe code is available here and you can view a full description about what the code does and how to run in my blog post.\n\nNote: If you see any issues or errors you get while running the code, please let me know as a comment or by opening an issue on Github page. That\u2019ll help me to improve the code getting rid of any pesky bugs."
    },
    {
        "url": "https://towardsdatascience.com/unraveling-bayesian-dark-magic-non-bayesianist-implementing-bayesian-regression-e4336ef32e61?source=user_profile---------4----------------",
        "title": "Unravelling Bayesian Dark Magic: Non-Bayesianist Implementing Bayesian Regression",
        "text": "To start with, I\u2019m a deep learning guy and I\u2019ve been quite happy with that until recently. But I was recently the victim of a vicious small-sample size high-dimensional problem! So I had to turn my head towards algorithms that are good at \u201clearning from little\u201d and tell me \u201chow confident I can be about that learning\u201d.\n\nThis was quite a sudden change for me. Because I\u2019ve been away from Bayesian \u201cstuff\u201d for so long. But time has come for things to change. So I had to learn (almost) from the scratch. I was bombarded with terms like priors, posteriors likelihoods, KL-Divergence, Evidence Lower BOund (ELBO) (Hopefully will write another post about the last two some other day!). Then I realized \u201chey, it\u2019s not that different from frequentist type algorithms\u201d. So time to untangle this big ball of mess to structured knowledge hopefully not falling down any rabbit holes.\n\nLet me give a quick tour about what\u2019s going to happen. So we\u2019re going to start with a frequentist type regression example. Then we will see why we might want to move onto solving our example with a little more promising technique like Bayesian linear regression (obviously the uncertainty information). There after we will state the Bayes rule, followed by a quick note on how we can adopt the Bayes rule to find a good model for the given data. With this we can define prior, likelihood and posterior terms (I mean define. Not understand). Then we will drive our discussion on a long path (it\u2019s long, but quite enlightening) discussing each of these terms. While we discuss each of these terms we will see the results we get in our example to build some context. And finally we will do a quick evaluation by comparing what we started with (prior) to what we found to be the best (posterior).\n\nThe first assumption when it comes to linear regression is that, we assume data has the following form.\n\nAnd data generated from such a model would look like below.\n\nSay we got this dataset and we need to fit a line. We do this by formulating the problem as follows.\n\nOur goal is to find \u03b21 and \u03b20 such that we have the minimum RMSE (root mean squared error) of the data. Which is mathematically,\n\nIt doesn\u2019t look bad. And in fact is almost the correct one. But can I really rely on the answer given by the linear regression for the limited data area? I don\u2019t think so. I would like a measure that basically says,\n\nSomething like this.\n\nYou can see how the confidence bounds increase (thus uncertainty of the answer increases) where there is no data. But linear regression can\u2019t give you this. And this is why we need Bayesian linear regression. So how can we use the superior \u201cBayesian\u201d healing to fix this problem?\n\nFirst things first! The holy grail of all the Bayesian \u201cstuff\u201d\n\nThe probability of an event A happening given event B happened (posterior \u2014 thing you\u2019re interested about but don\u2019t know) is given by, the probability of event B happening given A happened (likelihood) multiplied by the probability of B happening (prior).\n\nThat\u2019s nice and we\u2019ve heard this a million times! What I\u2019d like to know is how does this related to machine learning. For that, let A be the parameters of the learning model (i.e. \u03b20 and \u03b21) denoted by \u03b8, and B be your data, D. Now let\u2019s interpret each entity in the Bayes rule.\n\nBy solving this we will get a joint distribution for all parameters in \u03b8 (\u03b20 and \u03b21) given the Data. Which is what we need. In other words P(\u03b8|D) will tell us given data \u03b20=0.5 and \u03b21=2.0 with 0.5 probability and \u03b20=1.5 and \u03b21=3.0 with 0.1 probability. So we know that \u03b20=1.5 \u03b21=3.0 is not completely out of this world (which we actually assume in the frequentist method)! This is called the Posterior distribution.\n\nAnd we calculate this by calculating\n\nWe need to start with some parameter values right? And in Bayesian settings you don\u2019t specify things with one value, instead you say in terms of distributions (e.g. Gaussian/normal distribution). In our example we might say,\n\nHey, I believe the parameters \u03b20 can be represented by a normal with mean 0 and standard deviation 3. That is,\n\nSimilarly you say for \u03b21,\n\nIf we sample many values of beta, we get closer to the true normal distribution. As you can see for \u03b20 and \u03b21, lot of values have been sampled close to 0, but \u03b21 is more squashed compared to \u03b20 (less concentrated near 0 compared to \u03b21)\n\nWhy do we use normal distribution you ask? The normal distribution has very nice analytical properties. Thus we have an analytical solution for the mean and variance of the posterior distribution. More details about analytical posterior solution.\n\nBecause your posterior depends on that. That is, close your prior to the posterior, quicker you\u2019ll reach the true posterior. To see why, say your prior and posterior are identical, then when you sample from prior, you\u2019ll actually be sampling from posterior (which is what we need).\n\nNow off to the next important bit; the likelihood of seeing data.\n\nHow can we calculate the likelihood of our data for a given set of parameters. In other words we wan to calculate,\n\nBelow, we will do a step-by-step exploration to find an equation that we can compute a value for this entity.\n\nNow let\u2019s try to develop an equation for the above term.\n\nThis is true because of the assumption,\n\nNow let\u2019s recall what our data looks like,\n\nWe can fit in this equation in the likelihood equation to get,\n\nWe have all x, \u03b21, \u03b20 given so we can remove them from the equation to have,\n\nThink about this transformation this way, If I need P(A+B|B) this is same as P(A|B) because we know that B is \u201cgiven\u201d. In other words P(B|B) = 1 so we can remove B from top of P(A+B|B) and have P(A|B) unharmed.\n\nNow we are off to the next important assumption,\n\nWhich gives us,\n\nRemember that our noise is normally distributed and we can get the probability density function (PDF) of a normal as,\n\nIn linear regression, we make two important assumptions about the noise component of this. That is, it is a normal distribution and,\n\nWith these two assumptions, and plugging in the values of noise to the equation we get,\n\nAnd we have that,\n\nSo let\u2019s go and plug that in,\n\nYup, all done! It\u2019s a simple series of transformation coupled with assumptions to reach some computable term for the likelihood function.\n\nTo make things simpler from this point onwards we say,\n\nWe have two parameters in our example. So to make things simple, forget about \u03b20 for the moment. Say we have y and x being generated according to\n\nNow we try to approximate by trying a bunch of different \u03b21 values, which produce a new y\n\nfor a range of \u03b21 values. The likelihood P(y|x,\u03b21) would look as follows.\n\nYou can see that close to 4 we have a high likelihood of seeing data. Which is what we need.\n\nAnd you can generalize this to any number of \u03b2 values (\u03b21 and \u03b20 values in our example). This is the graph we get in the example.\n\nThe above graph says that when \u03b20 is close to -2.5 and \u03b21 is close to 1.5, we get the best chances of seeing the data X,Y. Also note how the likelihood for \u03b21 is a big spike where for \u03b20 it\u2019s more spread. Now as an exercise think about what this mean in terms of the impact each variable has on the accuracy of the model.\n\nHopefully, this clarified things enough. Let\u2019s move on to the posterior\n\nPosterior is the probability density function of parameters (i.e. \u03b20 and \u03b21) given that you observed your data. Quickly recall that this is the final result we want (left side of the Bayes rule). To calculate this we need to calculate P(Y|X,\u03b21,\u03b20), P(\u03b21,\u03b20) and P(X,Y) which is put together to produce,\n\nNow the entity P(X,Y) will be constant. Because whatever the data we have will be there whatever we do with our model. So it won\u2019t change. So I\u2019m going to replacy P(X,Y) with a constant Z. And we\u2019ll discuss how to calculate this Z later.\n\nSo there are two ways of solving this.\n\nWe are interested in how the posterior behaves with the likelihood and the prior. So getting the answer with the analytical solution will be cheating for us. So let us calculate this by sampling.\n\nSo we will formulate the above equation to fit sampling.\n\nThen we approximate the full posterior by taking 15000 different values for \u03b20 and \u03b21, drawn from their respective priors and calculating the likelihood for each of those \u03b20 and \u03b21 combinations. More concretely, for our example,\n\nwe have the posterior. Since data is i.i.d and \u03b21 and \u03b20 are independent we can write the above entity as,\n\nAnd by assigning the notation simplification we discussed we get,\n\nP(D) \u2014 This is sometimes called the evidence and denotes the probability of seeing data. Given by,\n\nHow can we approximate this? We just get the total of P(D|\u03b21,\u03b20)P(\u03b21,\u03b20) (i.e. top part of the Bayes rule) for all different combinations of \u03b21 and \u03b20 sample. More we sample, truer the probability of observing data, P(D) will be. More specifically we can calculate Z by,\n\nThis is nothing to be scared of, this is just adding the top part of P(\u03b21(i),\u03b20(i)|xj,yj) over and over for the total number of times we sample different \u03b21 and \u03b20. And Z is there so that, the area under the posterior adds up to 1 (you know, because it is a probability distribution)\n\nBelow we show the prior, likelihood and posterior all in one place to show how the look.\n\nAnd finally we do a comparison to see the difference of lines sampled from the prior distribution (only the first 500 samples) and the posterior distribution. I\u2019ll leave it up for you to judge which is better.\n\nThis is quite straight-forward now that we have a posterior distribution of what \u03b21 and \u03b20 looks like. You just sample different \u03b21 and \u03b20 from the posterior and get the value of y (i.e. y= \u03b21 x + \u03b20), for given x, for each of those \u03b21 and \u03b20. With this set of potential candidates of y for x, you can calculate the mean value of y and the standard deviation of y.\n\nJupyter notebook is available here:\n\nSo this was a bit long, but hopefully should demystify some of the \u201cdark secrets\u201d the Bayesian methods hold. We understood how the Bayesian linear regression works, through an example task; Y= \u03b21 X + \u03b20 + \u03b5.\n\nWe first saw that the frequentist approach solve the problem, but ignores some critical information; uncertainty of an answer. However we saw that the Bayesian method will not only give the most likely answer, but also tells us how uncertain it is about that answer."
    },
    {
        "url": "https://towardsdatascience.com/troubleshooting-gcp-cuda-nvidia-docker-and-keeping-it-running-d5c8b34b6a4c?source=user_profile---------5----------------",
        "title": "Troubleshooting GCP + CUDA/NVIDIA + Docker and Keeping it Running!",
        "text": "I had a Google Cloud Platform (GCP) instance which was all setup well and running fine a day ago, which was set up following my previous tutorial.\n\nLet me tell you a bit about the machine I had,\n\nBut, something really weird happened to me when I started my GCP instance and tried running the docker container yesterday, using,\n\nNope, it didn\u2019t work as I wanted it to and took me inside the container. Instead, it gave me the following error\n\nThis is likely to have happened because my GCP instance bravely decided that it would be totally fine to go ahead and update everything itself and things will magically work out so much better! Well, I got news! doesn\u2019t happen that way. So it would be much appreciated if GCP gives us a way to turn off automatic updates during initial setup. In fact there has been many reports about NVIDIA drivers going bananas (or missing) with other updates (evidence 1, 2, 3).\n\nAlright, before getting into the details of troubleshooting, let me summarize what I\u2019m going to do step by step.\n\nFirst and foremost, before checking if libraries are properly installed, see of your machine physically sees the GPU by typing in,\n\nThis should give something like,\n\nIf not, this could be due to GPU being unplugged or just nudging out from the socket because you moved the machine or something.\n\nThe first thing to do is, NOT jump to conclusions and rigorously start typing hoping for the best! In fact the best thing in such a situation (and mostly ignored by many) is to identify what is the issue. First let us see what we\u2019ve got. Go ahead and type,\n\nYou should ideally get something like,\n\nIf you get something like,\n\nThis could be due to two reasons,\n\nSo for NVIDIA-SMI to work properly, you need few things properly setup. They are,\n\nLet\u2019s check if these are installed, try typing\n\nYou should get\n\nNote that, actual list is much longer. But if get quite a few hits for CUDA, things should be alright. But don\u2019t get too comfy! I had encounters things didn\u2019t work even with these installed.\n\nLet\u2019s do another check to see if NVIDIA kernel modules are properly loaded by,\n\nIdeally you should see,\n\nIf you don\u2019t see anything, that is problematic! This means that NVIDIA drivers haven\u2019t been loaded properly.\n\nIf you get both these outputs as they are shown, but not the correct message, you have the things you need all in the machine. So it could be due to some simple mis-configuration of PATH variables. So open up the file and append the following two lines to it.\n\nThen exit the text editor and run,\n\nReboot the machine and see if things are working fine by trying out again. (PS: Remember to try with privileges)\n\nIf you are reading this part, you weren\u2019t lucky as some of the folks out there. Alright, let\u2019s go ahead and toil the soil! In my opinion, it is not worthwhile anymore to dig deeper and try to find the single grain of mistake in a bowl full of libraries. In fact, it would be much easier, if we remove the current corrupted libraries and install things correctly from the scratch.\n\nFirst we need to figure out which goes along with which. By this I mean we need to make sure we download specific (and correct) versions of CUDA and NVIDIA drivers correct for the Graphic card you have. So let\u2019s go ahead and find out what is the latest.\n\nAnd type in the details about,\n\nNow we\u2019ll make sure we stick to these particular versions when doing the installation, to avoid any discrepancies.\n\nFirst let us remove any existing CUDA/NVIDIA libraries with,\n\nFirst get the CUDA toolkit with,\n\nReboot the system and try,\n\nand you should see the correct output as shown in figure 2 and figure 3.\n\nI just realized the NVIDIA driver 387.xx which is shipped with CUDA 9.1 didn\u2019t work for my NVIDIA Tesla P100. So I had to first uninstall that and install NVIDIA 384.xx. This could be different for you depending on the GPU card you have in your instance.\n\nYou know what is weird! I was totally fine working with CUDA 9.1 and NVIDIA 387.xx as seen in figure 1. But now, NVIDIA 387.xx wasn\u2019t compatible with CUDA 9.1 any longer. I\u2019m not sure why but hopefully can get to the bottom of that!\n\nLet\u2019s do this by,\n\nNow, let us install NVIDIA 384.xx driver manually by,\n\nYou will need for . Now do a quick check on path variables to see if they are properly set\n\nNow try and you should see something similar to figure 1, meaning things are back to normal (hopefully!).\n\nOne more thing, don\u2019t forget what started this whole ordeal in the first place. It was the automatic updates. Updates are important to keep your machine secure from outside threats and everything, but if it is going to break my machine every 5 seconds I update, NO THANK YOU! I\u2019ll manually update it myself. So to do this, open the following file in a text editor,\n\nThis should stop those \u201cpesky\u201d (however, important) automatic updates off. But remember to update your OS consistently because, you don\u2019t want someone to hack into your machine.\n\nSo in this post, we discussed how to troubleshoot the GCP instance if you ever get into trouble of broken configuration, missing drivers, etc. And the process I recommend is,"
    },
    {
        "url": "https://towardsdatascience.com/whats-after-setting-up-a-gcp-computing-instance-running-a-custom-docker-container-with-tensorflow-eb0f077983c6?source=user_profile---------6----------------",
        "title": "What\u2019s After Setting up a GCP Computing Instance? Running a Custom Docker Container with Tensorflow",
        "text": "Cloud based computing platforms such as Google Cloud Platform (GCP) has gained quite a lot of attention with the emergence of deep learning and realization of \u201cdeeper the model, better the performance is\u201d (I\u2019m not here to argue about the truth of this concept, but merely stating a widely-accepted belief). And \u201cdeeper the model, better the computing resource should be\u201d.\n\nAnd it is not an economically viable strategy to buy and install the latest GPU cards whenever they are released (especially for small and medium scale technical institutes). If one is adopting this strategy, they should be prepared for the following challenges and many more.\n\nHowever, GCP provides this for a small fee (of course will add up in the long run). But importantly you don\u2019t have to worry about maintenance or initial setting up (e.g. installing an operating system (OS)), and offers a wide variety of customization to choose from (e.g. OS, disk storage, GPU type, number of GPUs, etc.). For example you might choose an instance with Ubuntu 16.04, 50GB storage, 2 Tesla P100 GPUs, etc.\n\nIn this post, I\u2019ll be discussing how to setup a custom docker image, create a container with the image and and get your python + Tensorflow scripts running in that.\n\nI\u2019m not going to go through the setting up, and there is quite a few nice resources out there explaining how to create and setup a computing instance on GCP. One of my favorites is,\n\nI\u2019ll summarize what is in there in plain english,\n\nHere we are going to discuss where to go from here. First let me state the libraries and versions I\u2019ll be using.\n\nI\u2019ll now briefly summarize what we\u2019re going to do. We will first download an image that supports python 3.x Tensorflow. Then we will create a container with the downloaded image. Finally we will run some commands to make sure it works.\n\nPS: Which python version you want to use depends on the problem you\u2019re trying to solve. There are differences between python 3.x and python 2.x. However, python 2.x is to retire in the near future and python 3.x will take it\u2019s place. So it\u2019s better to migrate to python 3.x from python 2.x\n\nTo ssh into the GCP instance, you can either use the\n\nUnfortunately, gcr.io does not provide us with a python 3.x compatible Tensorflow images, but only python 2.x compatible Tensorflow images. So we will have to download a python 3.x compatible Tensorflow image from DockerHub. And you can see all the Tensorflow Images in DockerHub.\n\nI\u2019m going to go with the 1.3.0-gpu-py3 image (You will see this if you scroll down on Tensorflow Images in DockerHub )\n\nTo download first ssh into your instance using either gcloud shell or a terminal. Once you have access type,\n\nWhen you type this, Docker will automatically look for the image with the tag name specified after the double colon (in DockerHub of course). Then you should see in the shell, the image being downloaded and extracted. To make sure the image was downloaded try\n\nand you should see the image listed\n\nAfter this operation is finished, we can create a docker container with the image. But before that, create a folder with the name docker_tensorflow_example. This will be saving the data we create while running things within the container, and we will discuss how to do that mapping later. Then change permission of this folder to the following,\n\nso the owner has full access. Then we create our docker container with,\n\nThat was a mouthful wasn\u2019t it. Let\u2019s break this baby into pieces then,\n\nIf you see the error,\n\nThen you probably will have to login as the root first with,\n\nand try the command again.\n\nIf you need to run a Jupyter Notebook Server use the above command without bash.\n\nNow you should have the docker container running and your shell should be within the container, the Terminal should have the prompt as,\n\nwhere 8301xxxxxxxx is the container ID. Now try,\n\nThis should work properly, and you should get 1.3.0 as the output. If that happens, Congratulations! You have your libraries installed. If not, make sure you followed steps currently for both initial setting up and setting up the docker container."
    },
    {
        "url": "https://towardsdatascience.com/neural-machine-translator-with-less-than-50-lines-of-code-guide-1fe4fdfe6292?source=user_profile---------7----------------",
        "title": "Neural Machine Translator with Less than 50 lines of Code + Guide",
        "text": "Around 15 lines for input, output definitions. The input words are sent through an embedding layer to obtain embeddings for each word in both source and target sentences. The final inputs fed in to the encoder and decoder will be for following sizes.\n\nThese inputs are \u201ctime_major\u201d meaning that the axis 0 represents time. Next we define a label mask for the decoder which we use when defining the loss function.\n\nEncoder, just 3 lines. Essentially we\u2019re saying, use a single LSTM cell with zero-initialized states as the encoder. is a wonderful function introduced by seq2seq library that can handle arbitrary length inputs. And the length of each input is provided to it via keyword.\n\nDecoder, just 5 lines. There\u2019s bit of work than the encoder, because the Decoder uses a softmax layer to produce predictions belonging to the target language. Concisely the above snippet is saying. Define a that uses a LSTM cell, encoder state as its initial state and a softmax layer as the final output layer. Then we use function to decode the predictions over time.\n\nThat\u2019s it just around 15 lines. We define a cross-entropy loss with the masks defined above and use Adam and standard SGD optimizer with gradient clipping. We need two optimizers because using Adam long-term seem to give weird curve for the loss over time. If you need more info about how various optimizers work see: Evaluation of Optimizers for Neural Machine Translation\n\nWe test our model on a German to English translation task. You can download the Jupyter notebook below.\n\nHowever, you will need to download the following to make it work.\n\nNot doing so well yet\u2026\n\nNot bad for 30 minutes of training (on a NVIDIA 1080 GTX).\n\nAgain, if you need a thorough walkthrough about the model. Refer to my original post."
    },
    {
        "url": "https://medium.com/@thushv89/a-practical-guide-to-understanding-stochastic-gradient-descent-methods-workhorse-of-machine-50c9d94cb34?source=user_profile---------8----------------",
        "title": "A Practical Guide to Understanding Stochastic Gradient Descent Methods: Workhorse of Machine\u2026",
        "text": "A good optimization is an essential part of machine learning as significant performance boost often comes from better optimization techniques. This is especially the case for stochastic algorithms. Because, in stochastic settings we only observe a subset of the data at a given time and better optimization techniques help to exploit data efficiently. One such trick would be, maintaining a running mean of gradients over time and adding that to the current gradient. This is in fact the idea behind momentum. In this post, we will be talking about Stochastic Gradient Decent (SGD) methods.\n\nHere\u2019s what SGD looks like in a simple optimization problem\n\nIn SGD with Momentum, a \u201cmomentum\u201d entity is introduced, which is a running mean of the accumulated gradients in the past.\n\nBut, if not careful with the momentum and train longer, you can overshoot and go to undesired areas easily.\n\nHere, instead of blindly taking a step in the opposite direction of the gradient as we did in momentum, we \u201clook ahead\u201d to see what the surface in front of us looks like. This helps to avoid overshooting that might happen if we move forward without looking ahead.\n\nNext we talk about Adagrad, RMSProp and Adam.\n\nIf you are interested about learning these methods in detail, please take a look in my blog post (link below), where I proceed from the basic SGD algorithms to more advance variants, by exposing limitations of each and how other methods solve them.\n\nIf you are already familiar with the intuitions, and want to fiddle around with the shown examples above (and more!), the Jupyter notebook tutorial is available at: https://github.com/thushv89/nlp_examples_thushv_dot_com/blob/master/intuitive_guide_optimization_algorithms.ipynb"
    }
]