[
    {
        "url": "https://towardsdatascience.com/new-york-seeks-haikus-generating-haikus-from-nyc-government-job-descriptions-c27496a376fd?source=user_profile---------1----------------",
        "title": "New York Seeks Haikus: Generating Haikus from NYC Government Job Descriptions",
        "text": "The goal was to use NYC job descriptions to create haikus. Originally a Japanese poetic form, haikus are poems which contain three lines with five syllables in the first line, seven in the second and five in the third.\n\nTo produce a haiku, I used a custom markov chain method. A markov chain is a technique to generate a sequence given the current value and the probabilities of what values would follow the current one. In this case, given a word, what words are likely to follow?\n\nThe first step is to determine these probabilities. I divided the data up by civil service title (Computer Systems Manager, Painter, Civil Engineer, etc.) and for each, built a separate corpus of data from the text in the job description and preferred skills fields. Then I split the corpus into sentences and divided the sentence into words and counted the number of times A followed B.\n\nThe following example shows the most common words to follow \u201cdata\u201d for a Computer Systems Manager. Given a table like this, the markov chain will pick a random next word weighted by the probabilities. It will then take that resulting word and repeat the process again and again.\n\nSince I was generating haikus, I had a strict syllable constraint. I only considered the next word if it fit within the syllable limit. For example, if I was on the first line (5 syllables) and my current word was \u201cdata\u201d, I wouldn\u2019t choose \u201canalysis\u201d or \u201cintegration\u201d as the next word because it would put the line over 5 syllables.\n\nDuring this process the generator would at times write itself into an impossible state, when there were no valid choices within the syllable limit. In this case, it would go back and try a new word to see if it would lead to a valid haiku.\n\nThe haikus came out of the process in a raw state \u2014 all lower case, no punctuation and sometimes they just weren\u2019t very good. The biggest problem I found was that, because haikus are so short, the results were often incomplete thoughts.\n\nThe markov chain would be in the middle of a sentence when it hit the syllable count and stopped short. I tried to correct for that by only ending with words that could be logical ending words, but it didn\u2019t work for every situation.\n\nFor example, both of the following end with \u201cdesign and construction process\u201d but only the first is a complete sentence:\n\nSome of these results were actually amusing:\n\nThrough a semi-manual, semi-automated editing process, I cleaned up the haikus to get presentable results. The final piece had 750+ haikus."
    },
    {
        "url": "https://medium.com/@docmarionum1/how-accurate-are-groundhog-day-predictions-2018-update-fadb5d856591?source=user_profile---------2----------------",
        "title": "How Accurate are Groundhog Day Predictions? (2018 Update)",
        "text": "Based on my criteria of warmer than average temperatures in Februrary, March and April, 2017 had an \u201cearly spring\u201d with USA-wide temperatures in Februrary 5.3 degrees warmer than average!\n\nHow did the groundhogs fare?\n\nEveryone\u2019s favorite groundhog, Punxsutawney Phil, predicted 6 more weeks of winter, continuing to prove that he has no predictive power.\n\nBut there were two groundhogs that I told you to keep an eye on: Stormy Marmot and Poor Richard.\n\nThey were both right! I should never have doubted them. In addition, my updated analysis shows that Dover Doug is a better bet as well. So keep an eye on those three come this Friday!\n\nThat\u2019s all! Tune back in a year from now for more hot groundhog analysis."
    },
    {
        "url": "https://towardsdatascience.com/how-to-create-a-codenames-bot-part-1-word2vec-62701de38e66?source=user_profile---------3----------------",
        "title": "How to Create a Codenames Bot Part 1: Word2vec \u2013",
        "text": "Players are split into two teams, red and blue. A board consisting of 25 randomly selected word tiles arranged in a five-by-five grid, like so:\n\nEach team has one player as the spymaster and only the spymasters know which cards belong to their teams. Each spymaster is tasked with giving clues to her team to make them guess their cards while avoiding the other team\u2019s cards.\n\nFor the above board, the red spymaster might give a clue such as \u201cdish 2.\u201d \u201cDish\u201d is the associated word and \u201c2\u201d is the number of cards associated with it, in this case \u201cplate\u201d and \u201cwasher.\u201d\n\n\u201cWeight 3\u201d might be a bad clue because, while it could refer to \u201cpress,\u201d \u201cforce\u201d or \u201cplate\u201d it could also refer to blue\u2019s card \u201cscale.\u201d If a team accidentally guesses their oppontent\u2019s card, their turn is over.\n\nSpymasters take turn giving clues until one team has guessed all of their words.\n\nIf you have 45 minutes to kill and want a better sense of the game, TableTop made an episode about it."
    },
    {
        "url": "https://medium.com/@docmarionum1/how-accurate-are-groundhog-day-predictions-577141dbe961?source=user_profile---------4----------------",
        "title": "How Accurate are Groundhog Day Predictions? \u2013 Jeremy Neiman \u2013",
        "text": "Now that we\u2019ve defined our terms and sorted our data, let\u2019s try and answer our initial question: Can Punxsutawney Phil actually predict the weather? What we\u2019re looking to do is test whether there is any correlation between Phil\u2019s prediction and the weather for any given year. If Phil predicts an early spring, does it tend to be warmer, and vice versa? But finding a correlation isn\u2019t enough. We want to ensure that any correlation is statistically significant. In imprecise terms, a statistically significant correlation is one that we can say, with a certain degree of confidence, exists not just by pure chance. Making Sure Our Results Are Statistically Significant A good way to illustrate this concept may be to consider two sets of data, one with only three data points, and another with 10 (shown below). The points come from the same line, but are randomly perturbed up or down. If you wanted to guess the line, you\u2019d probably draw something like the red line in the graphs. But with only three data points, it\u2019s hard to be certain you\u2019ve got the answer right. A small shift in only one of those points could have a big impact on the line you draw. With 10 points, small changes to any one point wouldn\u2019t impact your guess much, because you have all the other points still falling along the line.\n\nWhen we test for statistical significance, we need to choose a significance level to use before we start the analysis. This is the probability that we will get a false positive, or rather, that we will see a correlation just by coincidence. A common significance level statisticians use is .05, meaning there is a 5% chance we find a relationship in the data that doesn\u2019t exist in reality. When we do the analysis, we will obtain something called a p-value. A p-value is the probability that we found a correlation between two things that aren\u2019t actually correlated. We can say that the observed correlation is statistically significant if the p-value is less than the significance level we\u2019re using. When this is true it means that it\u2019s more likely for the correlation to exist in reality than simply being a fluke in the data. In the example graph above with three points of data, a p-value of .217 means there is a 21.7% chance that these plotted points are random, with no real relationship between them. This 21.7% chance outweighs the .05 significance level, so the correlation is not statistically significant. On the other hand, with 10 points, we have (almost) a 0% chance that we\u2019re wrong about a correlation between X and Y, and thus it is statistically significant. To test for a correlation, we can use an ordinary least squares linear regression (OLS) model. OLS regression can tell us the best relationship between a set of predictor variables and a target variable. In the previous example, our predictor was X and the target was Y. For our Groundhog Day analysis, our predictor is Phil\u2019s prediction, and the target is the temperature offset. Running the six different models, one for each of the time periods, one does, in fact, prove to be statistically significant (at a significance level of .05): When Phil predicts an early spring, April is on average about 1 \u00b0F colder. Whoops. The below graph illustrates the correlation. The temperature offset for April is plotted against the year. Points are colored by Phil\u2019s prediction for that year \u2014 blue when he predicted 6 more weeks of winter, and red when he predicted an early spring. As you can see, many more of the red points are concentrated below 0. This means that if you were trying to bet whether April would be hotter or colder than average, based only on Phil\u2019s prediction, you would be better off betting colder.\n\nAre All Those Other Groundhogs Also Good-For-Nothings? Seeing Phil\u2019s (unearned) fame and fortune over the years, dozens of other groundhogs have gotten into the meteorological business. Wikipedia is the best source of historical predictions for all the other groundhogs. Unfortunately, it only goes back to 2008, so there\u2019s not much to work with, but we\u2019ll do our best. Repeating the process we did for Phil for all of these other groundhogs, two stand out: Stormy Marmot in Aurora, Colorado and York, Pennsylvania's stuffed groundhog, Poor Richard. When Stormy Marmot predicts an early spring, we can expect March to be on average 6 \u00b0F warmer, and April to be 2.5 \u00b0F warmer. When Poor Richard predicts an early spring, we can expect February to be 4 \u00b0F warmer and March to be 8 \u00b0F warmer. Poor Richard, the stuffed groundhog meteorologist, showing Phil you don\u2019t need to be alive to predict the weather. (Fox43) We ran almost 300 different models \u2014 48 different groundhogs, and 6 time periods for each. So we should expect, just by random chance, for some of them to be \u201cstatistically significant.\u201d In fact, based on our significance level of .05, and 288 models, we would expect about 15 of them to exhibit a correlation even if no relationship exists. This is known as data dredging. Tyler Vigen\u2019s Spurious Correlations illustrates this problem beautifully. If you take enough random data sets and mash them together, there\u2019s bound to be strong correlations between things that have no causal relationship. Dammit, Nicolas Cage, would you stop drowning people already? (Spurious Correlations) Sorry Stormy Marmot and Poor Richard, but you\u2019re probably not as good at meteorology as it would appear. What If They All Worked Together? Let\u2019s try one last thing \u2014 what if we use the predictions from all of the groundhogs? Could we get a better result than looking at them individually? This can be seen similar to the Delphi Method or Good Judgment Project, where forecasting is done by a group of experts and the individual forecasts are blended into one. Of course, I\u2019m not sure I\u2019d classify these rodents as experts. This composite model will be a little different than what we did before. Instead of considering the temperature versus a single predictor, it will be versus 48, one for each groundhog. The model will try to learn which combination of the 48 predictions best predicts the weather. Individually they may not have predictive power, but our hope is that together they do better than any of the groundhogs individually. In the end, one model stood out as being slightly better than pure chance. 75\u201380% of the time our super-groundhog correctly predicted what March was going to be like. That sounds pretty good, but for 2008\u20132016, always picking \u201csix more weeks of winter\u201d would be correct two thirds of the time, so it\u2019s only 10\u201315% better than always picking \u201csix more weeks of winter.\u201d None of the models for the other months performed any better than pure chance."
    },
    {
        "url": "https://medium.com/@docmarionum1/what-wikipedias-network-structure-can-tell-us-about-culture-38f8caabf69d?source=user_profile---------5----------------",
        "title": "What Wikipedia\u2019s Network Structure Can Tell Us About Culture",
        "text": "Those already familiar with graph theory can skip this section.\n\nGraphs, in the mathematical sense, are structures that connect objects together with links. In Graph theory jargon, the objects are nodes and the links connecting them are edges. For example, in the graph below there are three nodes: A, B and C. A is connected to both B and C, but B is only connected to C through A.\n\nEdges can also be directional, signified by an arrow. For example, in the graph below you can travel from B to A, and C to A, but you can\u2019t go anywhere from A itself.\n\nThat\u2019s really all there is to understanding basic graphs, but starting with just those fundamentals a lot of interesting work can be done. When modeling Wikipedia each article is a node and each link between articles is an edge. For example, the article on Graph Theory links to the article on Mathematics which links back to Graph Theory, represented by the two-headed arrow below.\n\nThe first step was collecting the network structure data for each language. Wikipedia is big \u2014 Almost 40 million pages today and when last calculated in 2009, 275 million links. Instead of working with the whole of Wikipedia, we wanted to get an \u201cequal\u201d sample of pages from each language. We did this by picking five pairs of city+language where the city is a major global city and the primary language spoken in that city is one of the largest Wikipedias. For each of these languages, we built a network using Wikipedia\u2019s API by starting at all five cities, visiting all the pages linked to from that page and then visiting all the pages linked to from those pages.\n\nThe table below shows the five languages and cities as well as the number of nodes and edges in our sampled network for each of those languages. For comparison I\u2019ve also included the total number of articles and the ratio to the total in our sample.\n\nFor each of the networks we did the following:"
    }
]