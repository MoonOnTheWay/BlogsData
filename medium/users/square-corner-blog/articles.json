[
    {
        "url": "https://medium.com/square-corner-blog/new-class-naming-rules-in-ruby-bb3b45150c37?source=---------0",
        "title": "New Class Naming Rules in Ruby \u2013 Square Corner Blog \u2013",
        "text": "It\u2019s been a longstanding rule in Ruby that you must use a capital ASCII letter as the first character of a Class or Module name. This limited you to just these 26 characters:\n\nIn Ruby 2.6, non-ASCII upper case characters are allowed. By my count, that makes a total of 1,853 options! Here are the 1,827 new characters that can start a Class or Module name in Ruby 2.6:\n\nThis change supports upper case characters in other languages but doesn\u2019t go so far as to allow emoji as a Class or Module name. These examples are now valid Ruby:\n\nIt\u2019s worth noting that some constants in Ruby could already begin with these characters \u2014just not modules or classes. Now they\u2019re also valid class and module names.\n\nWhy support these additional characters? Sergei Borodanov started an issue ticket asking about support for Cyrillic characters. Matz decided, \u201cmaybe it\u2019s time to relax the limitation for Non-ASCII capital letters to start constant names.\u201d Nobuyoshi (\u201cnobu\u201d) Nakada (a.k.a. \u201cpatch monster\u201d) wrote and committed the patch to support this new feature.\n\nWith the addition of this feature, Rubyists in various languages can use their own alphabet for the first character of a Class or Module. For example, a Greek Rubyist can now have an \u03a9\u03bc\u03ad\u03b3\u03b1 class, instead of an O\u03bc\u03ad\u03b3\u03b1 class \u2014 where the first letter is transliterated. Thanks to the Ruby core team for making this change! It will be shipped on December 25, 2018 with Ruby 2.6.\n\nWe use Ruby for lots of things here at Square \u2014 including our Square Connect Ruby SDKs and open source Ruby projects. We\u2019re eagerly awaiting the release of Ruby 2.6!\n\nWant more? Sign up for your monthly developer newsletter or drop by the Square dev Slack channel and say \u201chi!\u201d"
    },
    {
        "url": "https://medium.com/square-corner-blog/migrating-to-warmer-times-ahead-a5f6404fb09f?source=---------1",
        "title": "Migrating to Warmer Times Ahead \u2013 Square Corner Blog \u2013",
        "text": "Caviar\u2019s consumer web frontend architecture has changed a lot over the years. It was built as one large Rails application with Slim used as the templating engine, and jQuery and CoffeeScript added to handle client-side logic. They were the tools most familiar to the people who used them at that time.\n\nAs modern web frameworks and libraries progress incredibly fast, so has Caviar\u2019s codebase and the engineers\u2019 skillsets. We\u2019ve moved off Sprockets to bundle our web assets to the amazing Webpack and Yarn, and we\u2019ve moved off using CoffeeScript (thanks to the life-saving decaffeinate tool written by a fellow Square!) into modern ES6+ and Babel \u2014 all while maintaining functionality, which by no means is an easy task for a large application with a small engineering team.\n\nMost notably was following the trend of moving more logic to the client frontend to take advantage of increasing browser processing power. Back in early 2015, we introduced React into the codebase \u2014 at that point still a relatively new tech; nowadays, the popular open-source UI library and one of the core facets of modern day web development.\n\nOne of the best and core parts of React is its flexibility. As the docs say right on its homepage:\n\nThis was awesome for us \u2014 we could incrementally write the new stuff in React, take advantage of a highly active community, and integrate our new and shiny components within our Slim templates with the handy package react-rails. The choices we made at that time couldn\u2019t go wrong, right?\n\nThe thing that hasn\u2019t changed for the past couple of years was our use of Fluxxor. It\u2019s a neat little library implementation of the Flux application architecture used to handle the flow of data through the rest of the application. To quote the official Flux docs on how the pattern works:\n\nIt worked great for the time, but as our application became more and more massive, we had more and more state in the client that became hard to track and hard to keep right \u2014 mutations in state make it super hard to debug when something goes wrong, and dealing with asynchronous calls adds a whole other level of complexity. Fluxxor in particular was not ideal for our situation as time passed:\n\nWe decided as a team to move on to another popular option to manage data flow, the tried-and-true Redux. I won\u2019t dive into why one might choose to use Redux over Flux, but an excellent argument can be found here. Redux has a number of obvious benefits over Fluxxor, including:"
    },
    {
        "url": "https://medium.com/square-corner-blog/rubys-new-infinite-range-syntax-0-97777cf06270?source=---------2",
        "title": "Ruby\u2019s New Infinite Range Syntax: (0..) \u2013 Square Corner Blog \u2013",
        "text": "On Christmas Day, 2018, Ruby 2.6 will be released with support for a new syntax denoting an endless range:\n\nSo why do we need this new syntax? Up to this point, it has been a bit clunky in Ruby to create an infinite range:\n\nNobody likes typing or over and over. It doesn\u2019t seem like a nice solution to put forward as the idiomatic way in Ruby. However, some Rubyists argued that we didn\u2019t need an infinite range because already lets you iterate from to infinity:\n\nIn the end, Matz (the creator of Ruby) made the decision to accept the endless range syntax proposal, saying, \u201cI honestly feel there should be a fluent way to do .\u201d\n\nWhat can we do with this new syntax? One useful thing we can do is access elements until the last element in the series:\n\nIn previous version of Ruby, we would have used to denote that we want all elements up to the last element:\n\nRanges also respond to threequals, so we can now do neat things with case statements:\n\nAnother use for the new ranges is to use them with to approximate a combination of , and :\n\nOr two infinite ranges can even be zipped together lazily:\n\nFinally, one of the most useful things to do with an infinite range is to iterate infinitely:\n\nThe new infinite range syntax is due to be released with Ruby 2.6 on December 25, 2018. If you\u2019d like to play with it before then, try the nightly Ruby snapshot. Or it will be included in the upcoming release of ruby-2.6.0-preview2. We have Yusuke Endoh to thank for this new syntax, as he both proposed and implemented the feature.\n\nWe use Ruby for lots of things here at Square \u2014 including in our Square Connect Ruby SDKs and our open source Ruby projects. We\u2019re eagerly awaiting the release of Ruby 2.6!\n\nWant more? Sign up for your monthly developer newsletter or drop by our dev Slack channel and say \u201chi!\u201d"
    },
    {
        "url": "https://medium.com/square-corner-blog/super-simple-serverless-ecommerce-68d2792e8285?source=---------3",
        "title": "Super Simple Serverless eCommerce \u2013 Square Corner Blog \u2013",
        "text": "Things you need to build this:\n\nThe basic structure of our little store will be a static HTML page hosted on S3 and spiffed up with CSS and JavaScript. We\u2019ll then process our checkout through Lambda functions that allow you to just purchase the one item you clicked on. This is intended to be super basic to illustrate how to get things up and running on AWS \u2014 we can explore more advanced ways of doing this all in future posts. We\u2019ll start with the HTML and getting that into S3 to create our static website.\n\nTake a look at the meat of our by just looking at the product div elements. We\u2019re hard coding our item\u2019s variant id to each div and stuffing that into a hidden input field as well. This will let us easily pass this to our Lambda function with a POST request. My main reason for this approach is entirely the simplicity of it. We don\u2019t need to write any JavaScript to handle passing the information \u2014 we can just use the native behavior of a form element to POST our item info to our function.\n\nIf we put in our 12 products, we get something similar to what we have below:\n\nWe\u2019ll skip over the stylings and the jQuery used to make modals, since that\u2019s not really the focus here. All we have to do is upload all of our files to S3 and enable \u201cStatic website hosting\u201d in Properties to have a publicly accessible site.\n\nNow we should have a nice page for everyone to see our products, but we need to have a way to allow everyone to buy those products. Now we finally start digging into creating our AWS Lambda functions (serverless functions). This is what we\u2019ll create to place in the form \u201caction\u201d attribute so that our form POSTs to our function.\n\nYou should be able to access your static site by visiting the URL provided by AWS and you can see all your wonderful items on display."
    },
    {
        "url": "https://medium.com/square-corner-blog/lightning-talks-a-tour-of-duty-at-the-u-s-digital-service-a8d6fa6fa3ca?source=---------4",
        "title": "Lightning Talks: A Tour of Duty at the U.S. Digital Service",
        "text": "Every month, Square Engineers give lightning talks internally. We\u2019re excited to share some of our favorites!\n\nZach talks about working with the USDS, some of their core values, and his work on login.gov. In learning about the values of the USDS, we can see how the government aims to better serve people in the future as we become more reliant upon the internet and digital services."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-massively-multi-user-datastore-synced-with-mobile-clients-a9ce0b5e5260?source=---------5",
        "title": "A Massively Multi-user Datastore, Synced with Mobile Clients",
        "text": "At Square, we manage large amounts of information for our merchants. This includes the data surrounding what a merchant sells \u2014 their products, prices, taxes, and the configurations associated with those entities. We refer to this dataset as a merchant\u2019s catalog.\n\nManaging this data can be challenging. Merchants\u2019 catalogs can be quite large. They must be synced with mobile devices which may be offline for extended periods of time, allowing the two versions of the catalog to diverge. Catalogs need a sophisticated query interface to support a responsive web UI, but they also need to support large bulk operations via API, including re-writing the entire data set. Additionally, the data needs structure to allow it to display and function properly across a variety of services and mobile clients, but it also needs enough flexibility to allow rapid development of new features, and to enable merchants and third parties to create custom data specific to their business or integration.\n\nWe recently re-architected the system that we use for storing catalog data, and are writing this article to share some of our learnings. We needed a number of the features offered by traditional databases. For example, we needed to write data without impacting reads until the entire operation is complete (even across multiple API calls). We wanted to be able to page through data consistently even if other clients are writing to the catalog. These features needed to be per-user, so that activity by one user would not impact another. At the same time, we needed much more structure than a traditional database would provide, while still allowing new elements to be added without the need for a schema change, re-deploy or migration.\n\nWe accomplished this by using an entity-attribute-value data model, where entities have types which may be system defined and attributes that follow specific attribute definitions, which may be system or user defined. By using an append-only data model, we were able to achieve the transactionality properties that we needed without relying on transactions in the underlying sharded MySql database, which we chose as a storage infrastructure due to strong institutional support.\n\nWe needed an object model that was structured enough to allow clients to handle data in predictable formats, but which had enough flexibility to allow clients, integrators, and even end-users to extend the data model for their own use cases. We settled on an entity-attribute-value store with a few characteristic features. The model could be represented as follows:\n\nEach object has a unique token and a predefined type. It has a set of attributes, which must use a predefined definition. An attribute definition specifies the type of the attribute and is used by clients to validate and interpret the attribute data. It is namespaced in a way that identifies its owner. Definitions and (eventually) types can be created via API. The same definition can appear in multiple attributes, allowing for sets of values to be represented. Attributes can reference other objects by their tokens, allowing for more complex data to be represented through related objects. We will discuss location overrides below.\n\nOne of the key values of this model is that it allows clients to operate with structured data which they understand, while allowing indefinite extensibility. New object types can be added with ease, including allowing clients to create new object types via API. Likewise, new attribute definitions can be added via API. Having attributes definitions namespaced (according to the Java package convention) makes it easy to distinguish the owner of the attribute definition, and can be used to prevent non-owners from overwriting definitions. For efficiency and consistency, standard types and definitions are shared across all users, while user created types are only visible to the individual user.\n\nOne domain-specific behavior that we wanted to support in a first-class fashion is the concept of the location. Within our user model, a merchant may have multiple physical or virtual locations where they conduct business. Some clients are location-aware, which means that they only need to have the data particular to a specific location, while others are merchant-aware, meaning that they need visibility into all locations. Values on the same object can also vary by location, for example, the same product might have different prices in different locations. We support this by giving each attribute a location value, which is global by default. When a location-aware client requests data, we send only the global attributes and attributes for the client\u2019s location, allowing the client to operate on a smaller and simpler data set. Merchant aware clients must handle the complexity of values which may have different values at different locations. Objects can also be entirely removed from individual locations through a special \u201cenabled\u201d attribute, which can toggle availability.\n\nOne of the more interesting features of our datastore is its use of synchronizable constraints. Having data with a constant structure is key to building clients that can understand and use it properly. Constraints ensure that clients can expect data to have a given format, and protects them from other clients creating or modifying data in an unexpected way.\n\nInstead of relying on database level constraints which require schema changes to alter, we wanted constraints to be easy to introduce while maintaining consistent behavior across the core datastore and mobile clients. For this reason, we modeled constraints as catalog objects themselves, which allows them to be created via API and synced with mobile clients. In this way, a client creating a new object type or attribute definition could create new constraints ensuring that it has the specified structure.\n\nConstraints are built on special global attributes that trigger software validations that must be built on mobile clients as well as the core service. They provide for validations ranging from requiring attributes, to specifying valid integer ranges, to regular expression matches, to requiring that references not be broken, to cascading deletions if a referenced object is deleted. Put together, they create a large palette of options that enables object structure to be tightly constrained.\n\nAllowing these constraints to be synced to clients enables clients to enforce constraints as soon as a violation takes place, rather than waiting for the invalid object to be sent to the server to receive an error message. This speeds debugging, and, in the unfortunate case where code that creates invalid objects is released, limits the potential impact of that bug on customers.\n\nAt the core of the merchant catalog is an append-only data model. When a request is received to delete or modify an object, entities are marked as having been deleted by the request in question, and are not returned in future requests. This is made relatively efficient by handling modifications by creating deletions at the attribute level. Thus, a request that modifies a single attribute only creates one new attribute entry, rather than an entire new object.\n\nThis data model allows a number of useful features. For example, a response to a query to page through the merchants data includes a paging token that encodes the current catalog version. Requests for additional pages with the same token will ignore deletions and creations that took place after that version, allowing for consistent paging.\n\nThis tooling is easily extended to allow a historic lookback of a catalog as of a specific request. Supplying a specific catalog version (which is incremented with each write request) makes it possible to view the entire catalog as it existed at that point in time. It also makes it possible to query the set of changes after that point in time, which makes it trivial to revert those changes, and restore a catalog to a previous version. Because rollbacks are themselves append-only changes to the datastore, they can be reverted in turn.\n\nThis same functionality makes it possible to expose user level transactions. As an optional parameter, a put request can request to open a transaction. If a transaction is opened, the merchant\u2019s catalog is locked with a token that is returned to the client. Other attempts to write the catalog are blocked while the catalog is locked, and requests to read receive that catalog as of the lock version without any incremental changes. Additional writes with the token update the lock version, allowing changes to continue inside the transaction for as long as necessary without impacting other users. When the transaction is completed, the lock is removed, and all operations access the catalog with the modifications from the transaction.\n\nIf a request is received to roll back the transaction, all changes made after the lock version are deleted, and there are no side effects from the aborted transaction. Likewise, if a client that opens a transaction does not make any write calls within a timeout, the transaction is automatically rolled back. This allows long running operations, such as the import of new catalog data, to execute atomically, and allows clients to always read the catalog as of a clean version.\n\nFinally, this model provides auditability. Because every write is tagged with information about the caller, it becomes possible not only to view the catalog as of a certain period, but to attribute specific changes to specific callers. This is a great help in debugging clients that make unexpected changes to a catalog, and also makes it possible for users to know the individual responsible for specific modifications.\n\nWhile the merchant catalog datastore was designed for our specific needs, it has a number of behaviors which may be of use for other applications. Specifically, using an append-only datastore enables a number of useful behaviors which greatly increase the flexibility of the platform. APIs for creating attribute definitions and syncable constraints allow data to be structured and validated while enabling multiple parties to independently iterate on their own parts of the model. We hope that our learnings may be useful to others facing similar problems."
    },
    {
        "url": "https://medium.com/square-corner-blog/documentation-details-matter-38e5e4c7b606?source=---------6",
        "title": "Documentation Details Matter \u2013 Square Corner Blog \u2013",
        "text": "Not too long ago I joined the Developer Platform team at Square as a product designer. One of the first things I was tasked with was addressing some quick-hit, tactical improvements we could make to our existing site to improve the developer experience. A designer on a developer team? Working on documentation? \u201cOil and water,\u201d you might say, but quite the opposite. I\u2019ve waded through my fair share of documentation, Stack Overflow posts, and random blogs, teaching myself various languages over the years. Needless to say, I assuredly empathize with our developers. Documentation that is poorly structured, has poor wayfinding, or doesn\u2019t include enough context is\u2026 frustrating, to say the least. The following is a rundown of the frustrations our developers have encountered and the steps we\u2019ve taken to address them. Square\u2019s developer resources have been growing for some time. Currently we have our main documentation, a few separate full API references, and our Developer Dashboard. Until now, there was no consistent way to navigate between all of these properties. We recently added a layer of navigation above these separate destinations to allow visitors to quickly get to the information they need without getting lost.\n\nOur documentation contains a lot of information on how to get going and be successful with our APIs. Similar to other documentation you may be familiar with, this information may consist of one-off pages or multipage guides. The latter were particularly problematic because there were no clear \u201cwhat next\u201d guiding calls to action at the bottom of pages. Furthermore, what if a reader started with the second or third article in a series that had crucial information in the first? How would they know what they\u2019d missed? To address this issue, we added the ability for our technical writers to specify explicit \u201cPrevious\u201d and/or \u201cNext\u201d navigation with the related articles\u2019 titles at the bottom of the page to help guide readers to the subsequent article in the sequence.\n\nArticles with lots of information can be hard to parse if the typography isn\u2019t up to snuff. Poor line-height and text contrast (weight, size, color) choices can easily impair reading comprehension, which is definitely not something you want in your docs. One of our least obvious tweaks to our documentation was to clean up some of our headers and make sure they had enough white space around it to clearly divide sections without disconnecting related text. We also tweaked the header font-weight to medium from regular to make sure content was easier to quickly scan.\n\nIt\u2019s pretty common in documentation to have supplemental, aside-type information to give more context to the concepts being explained, be it general information, tips, or even warnings. The treatment of these asides may seem a bit trivial at first: just stick \u2019em in a box, and maybe give the container a color fill to differentiate it from the article content and call it good. It\u2019s actually pretty easy to get acclimated to seeing and skipping over them, which may be fine for some general notes but could be potentially disastrous for warning content. Unfortunately, this was exactly our predicament. (Before) Different types of messages that all carry the same importance. We addressed this issue by adding more visual indicators to our asides via color and iconography, while keeping a consistent base structure to continue to help set them off from the normal article content. Now we\u2019re able to differentiate between warnings, tips, and general informational notes. (After) Asides that clearly highlight the type of information they contain. In the development process, it\u2019s pretty common to copy and paste blocks of code of varying sizes from online guides into a file you\u2019re working on. It can be tricky to select and copy whole blocks, especially if they have horizontal scrolling and require you to scroll vertically to see it all. We added a copy-to-clipboard feature to some of our code blocks to make it easy for our readers to copy entire examples with the click of a button."
    },
    {
        "url": "https://medium.com/square-corner-blog/making-ai-interpretable-with-generative-adversarial-networks-766abc953edf?source=---------7",
        "title": "Making AI Interpretable with Generative Adversarial Networks",
        "text": "AI has made tremendous advances in technology, business, and science in the past decades, and this progress continues to accelerate today. Many of our experiences in daily life are influenced by AI and machine learning. For example, music is recommended to us by artificially intelligent systems. Our eligibility for financial services is driven by credit-scoring machine learning models. Automobiles are speedily moving toward full autonomy, and many production vehicles are now equipped with AI-based driving assistance. Even medical diagnosis relies on complex statistical algorithms for identifying medical conditions in patients. Criminal courts also use statistical models to estimate the risk of recidivism when they are determining sentences.\n\nSome of the best performing models, however, are very hard to explain. This usually has to do with the complexity of the algorithm. While simpler algorithms are easier to interpret, their performance is lacking compared to more opaque and complex algorithms. Since we, as data scientists, want to use these better-performing, complex models, it is up to us to make these model decisions more interpretable so we can explain model predictions to partners and consumers, diagnose what went wrong in cases where we get false predictions, and keep consumers informed about the rationale behind automated decisions.\n\nAt Square, we are proud of our fraud prevention system that relies heavily on machine learning models to detect high risk and potentially fraudulent behavior to help us stop it before it happens. In this post, we share a framework we use for expanding the interpretability of our complex machine learning models.\n\nAs stated above, relatively simple models tend to have comprehensible explanations. There are standard practices to inspect model decisions and to generate \u201creason codes\u201d, i.e. statements that describe the reason for a model\u2019s decision. For example, the value of coefficients of a linear model tell us about the relative importance of each factor in the decision. Among the flaws of this approach is that it assumes a simple model architecture and assumes the ability to isolate the importance of a factor from that model architecture. However, these assumptions are rarely satisfied when attempting to generate reason codes for more complex models, such as a Random Forest, where we can\u2019t isolate the importance of a feature in an individual decision.\n\nFortunately, there are some approaches that have been designed to generate reasons for individual predictions from more complex models. One such approach includes fitting local simple models to approximate regions of the model decision boundary and then applying standard interpretation techniques to those simple models. However, fitting local models around individual decisions is a relatively expensive approach, especially when an organization is generating millions or more decisions per day.\n\nIf reason codes are thought of as \u201cthings that would need to change to produce an alternative decision,\u201d then another way to approach reason codes is to find minimum changes to an input that would produce these alternative decisions. For example, if a seller\u2019s account is suspended after a model determines that their activity looks very suspicious, then the question becomes, \u201cWhat would that same seller have to look like (with regard to model input signals) in order not to have been suspended?\u201d If we could do this, then we could generate model decisions that are clear and could even provide proactive recommendations.\n\nA simplistic approach would be to permute the input values until the model produces an alternative decision. However, randomly permuting signals independently of one another could produce unrealistic, impossible, or even contradictory results. For example, if two signals are correlated in reality, it would make no sense to permute one independently and form reason codes on that basis. Simply put, we want to be able to generate realistic perturbations.\n\nWhy does it matter for the perturbations on the input data to be realistic? That is outlined in detail here. Essentially, it is surprisingly easy for an attacker or malicious user to create \u201cadversarial examples\u201d to fool a machine learning model. For example, here is a random perturbation of pixel values on an image of a panda. In the first case, the model correctly identifies the image as that of a panda. In the second case, noise has been added to the pixels. We humans can tell that it is still a picture of a panda, but the model is now convinced that it is a gibbon.\n\nSo how can a model be robust against these kinds of errors? Especially if we want to use a model for reason codes based on feature permutations, how would our model be able to evaluate the \u201creasonableness\u201d of synthetic nearest neighbors? An existing framework we thought to try was the Generative Adversarial Network (GAN).\n\nWith GANs, we should be able to generate synthetic sellers that appear to be from the real distribution of Square sellers. This framework operates as follows: A generator model creates fake data from random noise. The discriminator is trained to determine whether the example was generated or real. And a feedback cycle allows the generator weights to be updated by the training of the discriminator. In other words, the discriminator becomes more and more robust against fake data because the generator is updated to produce more and more realistic examples. In the end, you have well-trained models to distinguish real data from fake data and to generate realistic new data.\n\nGAN Framework. The Generator tries to create data from random noise. The Discriminator tries to distinguish generated from real data. The weights of both networks are updated through the process, so that the generator gets better at deceiving the discriminator and the discriminator gets better at identifying fake data.\n\nWhen training our model, we found that an Actor-Critic Framework worked best to generate synthetic data from our training set. At first, we trained using a binary Generator-Discriminator approach but found that our GAN suffered from \u201cmode collapse\u201d, a phenomenon in which the generator learns to generate data within a small range of possible values \u2014 specifically, in a range of values where the discriminator does poorly to accurately classify the data as real or synthetic. The Actor-Critic framework solved this problem by evaluating the Wassertstein distance between the real and synthetic data rather than evaluating binary cross-entropy.\n\nWe applied our framework to a modeling population related to fraud risk. Specifically, we wanted to see if we could use GANs to provide reasons for model decisions to review accounts that are flagged as potentially fraudulent.\n\nOne of the advantages of this method is the relative ease of implementation. Since the GAN model training and generating of the synthetic sellers all takes place offline, the synthetic sellers can be stored in a database that is accessible from the production environment.\n\nDifferent supervised models typically have clear evaluation metrics. For a GAN, it\u2019s a bit tricker to evaluate based on mere classifier performance, since we are training two coupled models with competing goals. One way we chose to evaluate the model was to compare the correlation matrix of the synthetic data to the correlation matrix of the real data. For reference, here\u2019s the correlation matrix of the real data.\n\nSince the generator starts by generating essentially random data at first, that randomness is exhibited by the correlation matrix. Here\u2019s the correlation of some generated data before training has taken place.\n\nAs the model trains, the correlation matrix starts to take a non-random form.\n\nAt one point, the model overshoots and starts producing too-strongly correlated output. If we compare this chart to the correlation matrix of the original data, we see that the generated data has more and stronger correlations than the real data does.\n\nAnd eventually, we get a correlation matrix very similar to that of the real data, which the reader can see by, again, comparing to the original correlation matrix of the real data.\n\nAfter about 2000 epochs, we found that our model performance stabilized, meaning that the correlation matrix of the synthetic data didn\u2019t change much from one epoch to the next.\n\nWhen comparing univariate distributions of the real and generated data side by side, we see that the output values of the generator consistently fall within reasonable ranges. This is shown by plotting the distribution (using kernel density estimation) for the real and synthetic sellers side by side. The reader can see that the synthetic data doesn\u2019t map precisely to the real data, but it does well. The synthetic data unsurprisingly follows a smoother, more Gaussian-looking distribution but with more outliers. One interesting thing to note was that we reduced the magnitude of mode collapse, a phenomenon that results in generating synthetic data within narrow concentrated bands. This was due to our use of the Actor-Critic framework.\n\nOnce the model is trained, we use it to generate an arbitrarily large database of synthetic sellers. Given these synthetic sellers, we can now compare a \u201cbad\u201d real seller (one that was suspended by the model, for example) to \u201cgood\u201d synthetic sellers (ones that the model would have cleared). In order to make the comparison, we specifically compare them to the K most similar \u201cgood\u201d synthetic sellers. The reason for using K-nearest neighbors is to reduce possible noise in the comparison by averaging the value of those K neighbors and comparing the real seller to the average of those K values.\n\nThere are many approaches for computing pairwise similarity. We chose to use cosine similarity of normalized signals, but other metrics could be substituted depending on the type of data.\n\nOnce a seller\u2019s K neighbors are found, computing reason codes is straightforward: We determine which signals are similar to and which signals differ from the seller\u2019s neighbors. The ways in which the seller most differs from its neighbors constitute the most likely reasons for the decision.\n\nWe found this to be the case when reviewing sellers that were suspected of fraudulent activity by the model. For example, for one of our high-risk sellers, the top contributing signals were related to their transaction behavior and association with other known bad actors. For other sellers, we were able to generate similarly intuitive reason codes. It is important to note that the reason codes created by this method did not just tell us how a seller differed from the whole population, but these reason codes were able to tell us specifically what stands out about a seller, i.e. what makes them look different from otherwise similar sellers.\n\nIn our example, we applied this technique to a single domain, i.e. fraud, to explain individual model decisions and predictions in terms of input features. In this example, we were able to generate codes that could be used to explain to a seller what exactly it is about them that looks suspicious. This could be extremely useful in applications of automation that result in adverse decisions for a customer in other domains.\n\nSquare\u2019s purpose is economic empowerment. We think that there is an opportunity for AI to help create a fair and sustainable environment for seller and customers to connect. Taking this big step toward interpretability of machine learning models can help achieve greater fairness and transparency."
    },
    {
        "url": "https://medium.com/square-corner-blog/ember-and-yarn-workspaces-fca69dc5d44a?source=---------8",
        "title": "Ember and Yarn Workspaces \u2013 Square Corner Blog \u2013",
        "text": "Square has several Ember web applications, and Square Dashboard is the oldest and the biggest. The original commit was lost to the dustbin of history, but I\u2019ve been told it started in 2011 as a simple Sales Reports app on Sproutcore 2.0 (the predecessor to Ember). Today, it\u2019s an Ember 3.0 app with Reports, Customer Management, Invoicing, Employee Timecards, and much, much more. Almost 100 Square engineers touch the codebase each month, and we deploy to production daily.\n\nBuilding so much functionality into a single application allowed us to move quickly with new products and bug fixes. Until late last year, we didn\u2019t have another option\u2014we were using a bespoke Rails-based build pipeline that didn\u2019t understand Ember applications. But the size of the application is a heavy burden: build and CI times are very slow, and the codebase is intimidating to new and seasoned developers alike.\n\nWe finished migrating to Ember CLI on December 1st, and we\u2019re starting to break up the monolith into Ember-specific NPM packages called addons and engines for two reasons:\n\nHowever, breaking up the application is not a panacea. If we simply moved code from the application into separate NPM packages like this:\n\n\u2026 and install the addon and engine into Dashboard like this:\n\n\u2026 developing code simultaneously across packages would be quite painful. By default, NPM and Yarn would copy files between packages. If you change a file in and want to see it reflected in , you\u2019d have to completely reinstall the engine package.\n\nWhat we wanted was a way to break up the app without hampering the development experience for everyone\u2014especially as we encourage engineers to spend most of their time developing addons and engines, instead of adding to the main application.\n\nOne solution would be to use npm link, a tool to symlink packages together. ( does exactly the same thing.) Using it is fairly straightforward:\n\nAfter running those commands, you\u2019d see a symlink to the engine folder in the package\u2019s folder:\n\nThis structure allows Dashboard to access files directly from your working copy of during development.\n\nHowever, this is insufficient if you want to see changes reflected across all three packages at once. What you really want is:\n\nThis arrangement would take numerous commands to construct, especially once we have more than three packages. And personally, I could never reliably get to work\u2014the tool works correctly, but I kept swapping the commands or forgetting whether I had run them already.\n\nSupport for workspaces arrived in Yarn in August, 2017, and we\u2019ve found the feature to be stable and easy to use since version 1.3.2. And as of Ember CLI 3.1 (now in beta), workspaces and Ember are best friends!\n\nTo set everything up, you\u2019ll need to move your packages into a \u201cworkspace root\u201d with its own file:\n\nThe contents of the workspace root are very simple:\n\nNow when you run anywhere inside the workspace root, Yarn will discover the dependencies between packages and hoist symlinks up to a top-level folder:\n\nThere is no step #2! Most engineers working on Dashboard probably don\u2019t realize this is even happening.\n\nIf you\u2019d like to know more about how the packages find their dependencies in the top-level folder, the Node docs explain the lookup algorithm.\n\nEmber CLI\u2019s original solution for simultaneously developing addons is in-repo addons, which have never required any shenanigans. We have a few reasons for not using them:\n\nNB: Naming is hard\u2014our \u201creal\u201d addons are inside the same git repository, so aren\u2019t they \u201cin-repo\u201d addons? \ud83e\udd2f To disambiguate the two patterns, we use the term \u201cmonorepo addon\u201d for addons in our Yarn workspace.\n\nNot really! You must be on Ember CLI \u2265 2.18 for Yarn workspaces and Ember to work at all (Edward Faulkner\u2019s commit has a great explanation for why that is).\n\nIf you\u2019re on Ember CLI \u2264 3.1, commands like will default to using NPM, but this is remedied by adding the flag:\n\nYou\u2019ll have the best luck if you follow this three-step process:\n\nIf you don\u2019t move the original lockfile, Yarn will create a new lockfile and upgrade all your transitive dependencies. This could lead to unexpected breakage."
    },
    {
        "url": "https://medium.com/square-corner-blog/oauth-wherefore-art-thou-b7034098a0fd?source=---------9",
        "title": "OAuth, wherefore art thou? \u2013 Square Corner Blog \u2013",
        "text": "Shakespeare\u2019s plays are wonderful expressions of beauty and romance. His plays are also complicated, tragic, and hard to understand. Working with OAuth can feel a lot like Shakespeare. It sounds overly complicated, but the more time you spend with it, the more you see the underlying beauty and begin to truly understand it.\n\nLet\u2019s demystify some aspects of OAuth so you, too, can see the beauty and benefits to using it. If you\u2019re already familiar with OAuth, skip ahead to take a look at the basic OAuth setup with Square.\n\nBefore we go into explaining OAuth , it\u2019s worth talking about when you should be implementing it. A lot of API\u2019s require utilizing OAuth, but sometimes you might not really need to use it. If you\u2019re building an integration solely for your own use or your own application, you\u2019re probably fine using your personal access token. If you\u2019re wanting to build an application that needs access to multiple accounts or needs granular access, you\u2019ll want to build out an OAuth integration. Even for your own infrastructure, if you have an ETL service, you might want to create an access token specifically for just that service that has read-only access.\n\nTo explain OAuth, we\u2019ll walk through it like acts in a play. The first act is the authorization, the second act is the redirect, and the third act is acquisition. There\u2019s also a prologue \u2014 getting things set up with your authorizer (in this case, Square) to register your application and get all your special credentials to use in the process.\n\nIn the first act, we\u2019re sending the user off to the authorization server to ask for some permissions \u2014 like a kid who wants to go on a field trip. In order to attend, they\u2019re sent to their parent with a list of what the field trip entails, and the parent has to sign off on it. For OAuth, you\u2019re sending a user to the authorizer with your scope (permissions) that you would like to have. Then the user can see everything being requested and confirm that they would like to grant your application access for those permissions.\n\nThe second act is the redirect, which is where Square sends back a code to your application that can be used to acquire an access token. This should seem familiar if you\u2019ve ever used SMS to log in to something. You need to provide both your password and the code that was sent via SMS in order to log in. The code sent back to your application from Square is your authorization code and is used by your application in the final act of getting the access token.\n\nThe third act is where we finally receive our access token from Square so we can process API requests for our user. Here, we need to provide the authorization code we received in the redirect to our callback url and our own application secret (which is acquired when registering with Square) in order to get our user\u2019s access token. The access token can then be used to make API calls on behalf of our user.\n\nWe\u2019ve gone through a few analogies and it\u2019s clear that this process is a lot more complicated than simply asking a user for their password or making your user go get a special access token for you. The complexity ensures the process is secure \u2014 but it also gifts us more flexibility.\n\nIf we simply ask a user for their password, and they forget their Square password, then we\u2019ll have to update our system with this new password and our integration would likely break in the interim (also, don\u2019t ever ask for someone\u2019s password). We also give the end users the ability to gracefully revoke access without having to change their password.\n\nIn addition, since we specified the scope up front, we only have to give access to whatever was specified. This means the end user can make an informed decision on what\u2019s being granted. When providing a password, you can\u2019t really control what\u2019s being accessed. It\u2019s all or nothing."
    },
    {
        "url": "https://medium.com/square-corner-blog/product-analytics-at-square-8796766a14e3",
        "title": "Product Analytics at Square \u2013 Square Corner Blog \u2013",
        "text": "Product Analysts at Square leverage engineering, analytics, and machine learning to empower data-driven decision making in the full lifecycle of product development. We lead experimentation and growth initiatives, develop automation solutions to personalize product experience, provide insights to our sellers about their business, and much more. Square\u2019s purpose is economic empowerment, and our team\u2019s mission is to use data to understand and empathize with our customers, which enables us to build a high quality product experience.\n\nToday, Square has a whole suite of business-focused products and services, such as Marketing, Appointments, and Capital. But four years ago, we were primarily known for our credit card readers and payment processing. At that time, Square had one central analytics function that encompassed business intelligence (BI) developers, data scientists, and a newly emerging team of product analysts. I was the second member to join the Product Analytics team.\n\nThe BI team was experienced in building data warehousing components and creating tightly curated reports. The team of data scientists was hard at work solving problems where automation had the highest leverage, such as in risk detection and underwriting for our newly formed Capital loan team. Product Analytics, however, was tasked with solving a different problem.\n\nWith Square growing and its products expanding, developing, and launching, an analytics support gap emerged for our SaaS product teams. In their infancy, these new product teams needed to learn, iterate, and grow their reach. The SaaS products were not at the same scale as our payments business, which leveraged existing data infrastructure to automate decisions. Instead, product teams needed low-friction data access to make quick decisions. More importantly, they needed insightful analysis of seller behavior to drive product and marketing strategy. Addressing this gap was the catalyst for growing our Product Analytics discipline. We started assembling a small team to help our business partners move fast and make well-informed decisions.\n\nWhat we learned in the beginning\n\nIn the earliest iteration of the team, we were not incorporating analytics into product decisions in a high-impact way. We found ourselves in SQL Monkey mode: stakeholders would give us precise requests for data points without context on how it would be used, so we would throw the resulting data over the fence and forget about it. Not surprisingly, this working model turned out to be ineffective. We learned that having product partners be too prescriptive about how to approach analytics problems ran the risk of requesting inefficient and tangential solutions, such as asking for a time-consuming, precise calculation to inform a decision where quick directional data would suffice. Similarly, product teams needed guidance on how to think about statistical principles, such as correlation vs. causation.\n\nIt made the most sense for analytics to define the approach for tackling business questions, with our knowledge of data and statistics. We also found that we were in the best position to provide recommendations to our product partners on what types of analysis would be most impactful. With this shift we were no longer throwing data over the fence without regard to how it would be used, we were invested in and accountable for making the ultimate decision. The added context and accountability helped ensure that we always went the extra mile to make sure decisions were as data-driven as possible. It also made us equal stakeholders in the business, motivating us to be proactive and engaged with our products.\n\nThroughout the years, Product Analytics has optimized our approach of end-to-end problem solving, now a core competency of our team. Product analysts are responsible for taking a high-level question, developing the approach, conducting the analysis, and ultimately delivering an actionable recommendation.\n\nA great example of end-to-end problem-solving was our early work on Square Appointments. Square Appointments is a fully standalone SaaS product that helped service-based businesses, such as a hair salon, take and manage bookings, particularly online. The launch went smoothly, but we discovered that our free-trial-to-paid conversion rate was lower than we expected. To investigate why, I segmented free trial merchants into three different categories: 1) existing Square sellers, 2) sellers who were new to Square and signed up for both payment processing and Appointments in tandem, and 3) sellers who were new to Square but signed up for Appointments but not payment processing. My hypothesis was that these different groups had drastically different intent and knowledge of Square that translated into different behavior. Some merchants predominantly wanted to use Square to take payments and saw business tools as a nice bonus. On the other hand, we saw merchants who were already using another company to take payments and were coming to Square specifically for our business tools. In fact, many of these sellers had very little knowledge of Square\u2019s payments product and how they could leverage it. These differences translated into different behaviors in the subscription funnel.\n\nMy theory proved to be correct. Sellers who had high intent and knowledge of Square\u2019s payments business, even if they were new, had a conversion rate three times as high as sellers who used Appointments as a standalone product.\n\nThis discovery was significant, but we weren\u2019t quite done. In addition to the insight, I needed to provide the team with a call to action to solve the problem. What should my product team do differently, knowing this insight?\n\nTo figure out the correct call to action, I had to noodle over the above finding a bit. Why would a merchant be far more likely to convert if they also intended to use Square to take payments? For one, this smelled a lot like self-selection bias: merchants who rely on two Square services are probably more likely to subscribe than merchants who rely on just one. But could something else be contributing to this large gap? I found my answer when I went through the onboarding flow as a standalone Appointments business. It turned out that as soon as I entered my credit card information for the free trial, I was taken from the Appointments product and dropped into Square\u2019s subscription management dashboard in our payment processing ecosystem. Not only was the branding completely different, but all of a sudden I was being asked to activate my profile, link a bank account, and take a payment, while being notified of a 2.75% payments processing fee! As a standalone Appointments user, I would have very little context on these fees for other Square services and why I would use them. This experience was very confusing and intimidating. We had to fix it!\n\nAs soon as we identified the problem, we were hard at work building a more delightful experience that made sense to an Appointments-only customer. The design and product teams created an onboarding flow that was localized within the Appointments product and shared the same branding. This flow was both more pleasing visually and more effective in practice.\n\nSquare Appointments has come a long way since then. Over it\u2019s lifetime, we\u2019ve equipped the team with insights and recommendations on a range of issues spanning onboarding, billing, usage, and engagement. In a recent launch they completely integrated the Appointments and payments experiences within the product.\n\nThis was an early example of product analytics work at Square, and a great example of end-to-end problem solving. Over the last four years, the frequency and quality of our insights and recommendations have drastically improved. We\u2019ve added many tools to our toolbelt, including merchant-facing applications of statistics (UX analytics) and data science (Clustering).\n\nOur discipline is still evolving. Product Analysts have both tremendous technical abilities and an understanding of the bigger picture of Square\u2019s business. Over the last two years we have started to leverage these skills to create data products that we can give back to our sellers.\n\nEarlier this year, David Feng published a blog post to describe how we used our own ROI data to form a \u2018closed loop\u2019 for our marketing and CRM products. I would encourage everybody to read his post, but in short, we used a seller\u2019s return on investment from a punch-card-like tool called Square Loyalty to drive the pricing conversation in a way that felt fair and highlighted the value of the product.\n\nJust another cool example: Corin Qi used ROI data for gift cards to show our sellers how much additional spend they might typically see by introducing a gift cards program:\n\nAs we take our SaaS products from newly launched, growth focused tools to mature services that are foundational tools in our sellers\u2019 business, we are constantly looking for ways to bring more value to our sellers, either by making the product more delightful or more insightful.\n\nIf this sounds interesting to you, please reach out to us!"
    },
    {
        "url": "https://medium.com/square-corner-blog/how-to-be-a-more-influential-engineer-d1fcbcdd3e7a",
        "title": "How To Be a More Influential Engineer \u2013 Square Corner Blog \u2013",
        "text": "One of the most significant transitions for an engineer\u2019s career is when they break dependence on senior team members and start to actively contribute to the team\u2019s decision-making process. Once this transition is complete, the engineer is contributing to the team at levels far more significant than the code they can sling. They become a more productive team member and help unlock team potential by freeing up others\u2019 time and effort. How do you get there? By developing certain leadership skills, and practicing them smartly through a framework we\u2019ll discuss.\n\nAs every senior engineer will tell you, the longer your career, the more you understand that technology is rarely the \u201chard part.\u201d People, processes, working within teams, prioritizing and sequencing work, are what will trip you up.\n\nEvery engineer will go through this learning process at least once. This point of your career is when it becomes necessary to internalize that your job is mostly about people and only partially about technology. The skills presented here are broadly applicable, but are best applied if you find yourself in the following situations:\n\nWhen you find yourself in this situation, it\u2019s not because you\u2019re not trying hard enough to make an impact, or that you lack the willpower to be an effective member of the team. The problem here isn\u2019t effort or desire, but knowledge. You need a skill set and a plan to grow said skills \u2014 with that, you can accelerate growth into the next level of effectiveness. Let\u2019s break the skill set down:\n\nThis is probably the most underrated skill for technical leaders. Quality of communication impacts the speed at which information is absorbed, the number of errors that happen in translation, the cost of context switching, and more. Investing in this skill means writing better design documents, disseminating experience reports, communicating via email, writing better tests and code documentation, and generally expressing your point of view in the most direct, clear manner possible.\n\nTo become more influential on your team, you\u2019ll need to bring people together to make decisions, spread information, solve hard problems, and get on board with your ideas. Meetings get a bad rap \u2014 because so many meetings are poorly executed. Work to create only engaging, productive meetings.\n\nAs you take on more responsibility, you\u2019ll have increasing influence over what work you do. You\u2019ll need to differentiate the most important work from the \u201cnice-to-have\u201d work and the work to defer for now. The difference between a product that users successfully use and warrants continued investment, and a never-done black hole of time and effort, is prioritization.\n\nAs you establish yourself as someone who can take more responsibility, you\u2019ll be asked to tackle bigger, more undefined projects. These can be assigned in as little as one sentence (e.g. \u201cAdd a calendar for business to manage their clients.\u201d) that translate to multi-week projects. Your task as the engineer is to take that short description and splay it out into a well-defined set of tasks.\n\nEngineers early in their career often build the muscles of estimating how long a particular task will take. They provide feedback, and this feedback is usually taken into account at the sprint level, fitting stories into the week or weeks of scheduled planning. Agile and similar methodologies discourage doing detailed planning for periods of time that are on the order of months. There are good reasons for this, but as an engineer helping to make decisions, it is important to give these high-level estimates for long-term planning purposes.\n\nWhen you work on larger projects that touch many things, dependencies become a significant factor in the success of a project. Learning how to identify and measure risk for each of these becomes a key skill. Dependencies on outside teams, analysis, design, or engineering work can really slow a project down and put it at risk.\n\nGiven a bunch of tasks for a single feature, we can often separate out the work for individuals or for team members to do the work in parallel. Understanding work that can be done in a different work stream that doesn\u2019t step on each other\u2019s toes can accelerate the delivery of a feature. For big projects, this is essential so the project doesn\u2019t take months to ship.\n\nThe list of skills needed to make this transition can be daunting\u2014it\u2019s a lot to learn. But each of these skills are additive: you don\u2019t need every single one of them to improve yourself as an engineer, and it isn\u2019t necessary to be an expert in all of them at once to overcome the plateau and start making a larger impact.\n\nSkills, though, aren\u2019t the only thing that you need to be effective. You need to build trust and influence within your team; people should be looking to you to participate in high-value team decisions and that doesn\u2019t happen just because you have the skills needed. If you haven\u2019t established yourself as a trustworthy source for this information on the team, you may not be included in the decision making process.\n\nAnd, finally, even with skills and trust, you have to make the decision to consciously step into the role of greater responsibility and impact. The rest of your team should know you want to and you should ask for their support, mentorship, and feedback as a part of the process.\n\nSo, here is a framework (not the only framework, just a framework) for achieving your goals, and then we\u2019ll break these down:\n\nWork with your manager, tech lead, product manager, or all three to set intentions that you\u2019d like to be involved in these team decisions. If you are new to this, state your recognition that you are learning and are approaching this with a beginner\u2019s mindset. If you believe you have the requisite skill set, explain you think you can contribute but that you\u2019re open to learning and feedback (and mean it!).\n\nTry to focus on one skill at a time. For example, you could pick meeting facilitation and ask if you could run the next roadmap discussion or planning session. Focus on that one thing and see what you learn from the process.\n\nFind mentors outside of your team. This can be people on adjacent teams, or people outside of your company entirely. Lean on them for ideas, approaches, and learnings from their experiences.\n\nThis can be in 1:1 settings, feedback discussions, or during coffee breaks. Actively seek out feedback and ask for specifics on the tools you are working on and the level of trust you\u2019re trying to build.\n\nBuild trust through humility, servant leadership, and by calling your shots. Find the ways you can be most helpful. For example, if you have specific knowledge of a particular technology that isn\u2019t widely known, talk about your experience and what you think may affect estimates. Call your shots. You\u2019ll be wrong, but you\u2019ll also be right as long as you hold yourself accountable.\n\nThe combination of these skills and this process in your repertoire can increase your influence\u2014and thus your contributions to your team, the product, and others\u2019 careers. You can drive product and engineering changes that you feel passionate about. As you build trust, you\u2019ll be given more opportunities to make decisions, which will then build more trust. It is a virtuous cycle.\n\nAs these skills are honed, more opportunities will arise. It isn\u2019t just about how to be influential on the team, but instead where you want to be influential. Do you want to be a technical leader? Do you want to help design and build the architecture? Do you want to be a product focused engineer? Do you want to dig deep into tough challenges that no one else has the patience for? All of these are futures opened up by laying the foundation of influence and trust within your team."
    },
    {
        "url": "https://medium.com/square-corner-blog/understanding-composition-browser-events-f402a8ed5643",
        "title": "Understanding Composition Browser Events \u2013 Square Corner Blog \u2013",
        "text": "Okay, great! Now we know how an IME works. So what was the bug?\n\nNotice how there was a lot of keyboard input happening for interacting with the IME \u2014 we can use the arrow keys, spacebar, enter, and more. At first, I naively believed that, since the IME is an OS program, any typing or editing would exist outside the browser, and thus the browser would be oblivious to the fact that I\u2019m using an IME. But this isn\u2019t the case! Each keyup/keydown in an IME is responded to both by the OS and the browser! So, when the user would hit the enter key to confirm one of their IME selections, our code would immediately process a keydown event, opening a new tab even though the user may not have been done with their search query. Not good! With the code in this state, we would be making it very cumbersome for many Square sellers to use our search box in their native language.\n\nSo how can we fix it? It took me a bit of research, but I eventually learned about two browser events that I\u2019d never encountered before: and . When a user starts typing with an IME, modern browsers fire the event, and when the text is finally confirmed, will fire (with one exception \u2014 see below). This is exactly what we need! Now we can set some state in the application about whether or not the user is currently composing some text via an IME, and if they are, we won\u2019t use any of our own keyboard event logic. The code looks like this:"
    },
    {
        "url": "https://medium.com/square-corner-blog/how-to-create-and-deliver-a-successful-tech-talk-823b1f43fa02",
        "title": "Lightning Talks: How to Create and Deliver a Successful Tech Talk",
        "text": "Every month, Square Engineers give lightning talks internally. We just started sharing some of our favorites.\n\nOver the last two years, I\u2019ve provided speakers with early feedback on their technical presentations. Eventually, it made sense to compile these tips into their own lightning talk. Here are some pointers on how to create a successful tech talk that will grab your audience\u2019s attention.\n\nI also delivered a more in-depth version of that talk at Droidcon NYC:\n\nWhat other tricks do you use?"
    },
    {
        "url": "https://medium.com/square-corner-blog/deep-neural-networks-for-survival-analysis-based-on-a-multi-task-framework-ded8697be85c",
        "title": "Deep Neural Networks for Survival Analysis Based on a Multi-Task Framework",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/square-corner-blog/you-are-your-own-best-hype-person-cf1e3a83c0c2",
        "title": "You Are Your Own Best Hype Person \u2013 Square Corner Blog \u2013",
        "text": "At some point in your career, you\u2019re going to have to prove to someone that you\u2019re worth it: worth hiring, worth promoting, worth the raise, worth taking a risk on. If you\u2019re lucky, you might have someone who has seen your work first hand, who believes in you and can vouch for you.\n\nBut even if you have the most supportive and engaged manager in the world, they still won\u2019t know everything you do and all the valuable contributions you make \u2014 unless you tell them.\n\nJust because you do the work doesn\u2019t mean people will magically know about it (even if it\u2019s open source). Someone has to tell them why what you do matters, and it might as well be you.\n\nHyping yourself isn\u2019t about pride. It\u2019s not about puffing yourself up, or making yourself look better than you actually are.\n\nHyping yourself is about accurately explaining the value that you contribute. It\u2019s about saying, \u201cHere are all my accomplishments; here is what I\u2019ve been up to; here are the ways that I\u2019ve been growing and will continue to grow.\u201d You demonstrate to others why you\u2019re worth it and you show up with the receipts to back every claim you make.\n\nA Hype Doc is a running list of all your accomplishments and successes. It\u2019s a place where you keep track of your growth, and regularly jot down the things you\u2019re proud of doing.\n\nA Hype Doc is not a resume or a promotion packet or a formal self assessment \u2014 but it\u2019s certainly helpful when filling those out! While you can (and should) share your doc with others, the primary audience of your Hype Doc is always yourself \u2014 it\u2019s for you to know what you\u2019re doing and why it matters, so that you can present yourself well.\n\nAnything and everything you\u2019re proud of doing or that shows your growth! This can include:\n\nUltimately, a Hype Doc is only useful if it works for you, so follow whatever process and format suits you best. That said, a few things to consider:\n\nWhat are your best tips and practices for keeping and updating a Hype Doc? Share them with us below in the responses!"
    },
    {
        "url": "https://medium.com/square-corner-blog/uncovering-inconsistent-keychain-behavior-while-fixing-a-valet-ios-bug-7c9beece0aa1",
        "title": "Uncovering Inconsistent Keychain Behavior While Fixing a Valet iOS Bug",
        "text": "A few months ago, a developer sent the following error stack trace to Square from their iOS app:\n\nThis led to a patch of Valet, Square\u2019s popular open source library for managing iOS and macOS keychains. Along the way, we also uncovered a subtle but interesting inconsistency in keychain behavior on iOS and macOS.\n\nThis post will cover the following:\n\nThe error message above indicates that isEqualToString (an NSString method) is being called on an __NSCFData instance (a private subclass of NSData that does not implement isEqualToString).\n\nLooking further down the stack trace, the first call that is not part of Apple\u2019s CoreFoundation framework is:\n\nVALValet is a class in Valet, and the migrateObjectsMatchingQuery method calls isEqualToString once here:\n\nImmediately before the code calls isEqualToString and the crash occurs, the kSecAttrAccount value retrieved from the keychain entry is stored as an NSString:\n\nThe exception, however, indicates that key is actually an NSData object.\n\nWhy does Valet assume that the kSecAttrAccount value is a String?\n\nBefore answering this, having answers to these prerequisite questions is helpful:\n\nIf you\u2019re already familiar with the iOS keychain, feel free to skip this section.\n\nOne way of persisting data between app launches is via UserDefaults, which is a key-value store with a simple API:\n\nThe iOS keychain is another persistence option, allowing apps to store sensitive data, such as passwords, in an encrypted SQLite database file on the device. Keychain entries can be retrieved and stored through the use of predefined attribute fields (e.g., kSecAttrAccount, kSecClass, kSecAttrService).\n\nThe iOS keychain is secure storage (and has a hard-to-use API), while UserDefaults is entirely insecure (but has an easy-to-use API).\n\nUserDefaults stores data in plain text in a .plist (property list) file within the application\u2019s sandbox. Utilities like iBrowse and iExplorer can easily inspect files on a device, making UserDefaults only suited for storing non-sensitive data like user preferences and settings.\n\nThe following illustrates the minimum amount of code necessary to create and fetch a username/password entry in the the iOS keychain:\n\nNot only is keychain usage verbose, but every function that interacts with the keychain (e.g., SecItem*) returns a bevy of error codes (see OSStatus in SecBase.h) that need to be checked and handled.\n\nDealing with the keychain is complex, prone to error, and usually includes a large amount of boilerplate code. Unfortunately, it\u2019s also unavoidable for any app that needs to store account information or sensitive data.\n\nTo make keychain management easier, Valet was developed by Square as an open source library for both iOS and macOS that hides the complexities of the keychain under a simple key-value interface.\n\nStoring and retrieving a username/password in the keychain via Valet is similar to UserDefaults:\n\nNow that we have more context, let\u2019s return to our investigation of the error received by the developer.\n\noccurs in VALValet\u2019s migrateObjectsMatchingQuery method. What does this method do?\n\nTypical usage of the migrateObjectsMatchingQuery method looks like this:\n\nmigrateObjectsMatchingQuery:removeOnCompletion: allows an application to shift management of existing keychain entries to Valet. If the application was utilizing the keychain to persist data prior to using Valet, this method restructures existing keychain entries to a format that allows these entries to be updated or fetched via Valet\u2019s simpler API.\n\nThe query parameter is used to select the keychain entries to be migrated to Valet, and uses the standard keychain attribute field names as filter criteria.\n\nMigration allows apps to transition to Valet without forcing users to re-enter account passwords or other information stored in the keychain.\n\nRevisiting the code block in migrateObjectsMatchingQuery where the exception is thrown:\n\nPrior to migration, this section of code is responsible for sanity checking the existing keychain entries that are candidates for migration. For example, to be eligible, existing entries must have kSecAttrAccount and kSecValueData values.\n\nqueryResultWithData is an Array containing data for the entries returned after searching the keychain using the query supplied to the migrateObjectsMatchingQuery method.\n\nEach entry\u2019s kSecAttrAccount value is stored in the key variable. Note the assumption here that the value is a String. The compiler allows this due to the fact that NSDictionary values are id pointers, which allow implicit casts.\n\nBefore each entry is sanity checked, key is compared against VALCanAccessKeychainCanaryKey and all checks are skipped if a match is found.\n\nVALCanAccessKeychainCanaryKey is a string constant stored in the kSecAttrAccount field of an entry by Valet when performing exploratory (canary) keychain entry inserts. These inserts verify that the proper keychain accessibility was specified when initializing a Valet instance.\n\nIf the query results contain a canary entry, we want to skip any sanity checks for that entry. Canary entries are a special case that should not cause any migration sanity checks to fail.\n\nEverything works fine when key (the value in an entry\u2019s kSecAttrAccount field) is an NSString. The exception, however, proves that it\u2019s possible for key to be an NSData object.\n\nThe problem is the assumption Valet makes that an entry\u2019s kSecAttrAccount value will be an NSString object:\n\nApple\u2019s documentation for the keychain value associated with the SecAttrAccount attribute provides the following description:\n\nFrom this, it seems reasonable to assume that kSecAttrAccount values will always be strings.\n\nAlso, from this example of Valet usage:\n\nValet only accepts String arguments for keys (myUsername in the example above), which are then stored as kSecAttrAccount values in the keychain.\n\nAny entries created by Valet abide by Apple\u2019s type restriction on kSecAttrAccount values. However, for existing keychain entries, Valet makes the flawed assumption that either:\n\nThe exception received by the developer, however, indicates that it is possible to store an NSData object in a keychain entry\u2019s kSecAttrAccount field, which contradicts Apple\u2019s documentation.\n\nThe following code snippet proves that this is indeed the case, and the same exception is thrown when attempting to migrate keychain entries.\n\nSince it can no longer be assumed that kSecAttrAccount is always a string, we need to add a type check prior to the string comparison to avoid sending the isEqualToString message to a non-NSString object.\n\nSince Valet always uses NSString values in the kSecAttrAccount field for entries it creates, a Valet canary entry\u2019s key will always be an NSString. Therefore, adding this type check is a valid requirement.\n\nNext, we added the test below to verify our change (Valet is written in Objective C):\n\nThe test deliberately stores dataBlob, which is an NSData object in the kSecAttrAccount field of keychainData, which is inserted in to the keychain. As expected, the fix prevents an exception from being thrown.\n\nInterestingly, the last assertion passes on an iOS target but fails in a macOS environment (Valet can manage iOS and macOS keychains):\n\nRunning the test on a macOS target resulted in a VALMigrationErrorKeyInQueryResultInvalid error, indicating that the keychain entry\u2019s kSecAttrAccount value was nil. Recall that any keys that are candidates for migration must have a value in the kSecAttrAccount field.\n\nInserting an entry containing a kSecAttrAccount Data object on macOS results in the kSecAttrAccount value not being persisted if it is not the correct type. iOS, on the other hand, persists the kSecAttrAccount value regardless of whether it is the correct type or not.\n\nInspecting the keychain entry after inserting keychainData in both operating systems confirms that the kSecAttrAccount data value is stored successfully on iOS but not on macOS:\n\nA developer at Apple posted this reply to a thread inquiring about this very issue:\n\nIn short, these two factors contribute to this difference in behavior:\n\nSQLite, which is the data store backing the iOS keychain, uses type affinity rather than rigid typing, meaning a column\u2019s datatype in SQLite is more a recommendation rather than a requirement.\n\nThis explains the observed behavior in iOS, which allowed an NSData object to be stored in the kSecAttrAccount column, even though the recommended data type is a string.\n\niOS and macOS both use the SecItemUpdate function to update keychain entries. Its implementation can be found in SecItem.cpp, which is open sourced as part of Apple\u2019s Security framework:\n\nIt\u2019s apparent from the can_target_(ios|osx) boolean values and SecItemUpdate_(ios|osx) functions that the codepaths are different for each operating system.\n\nFor interested readers, SecItemUpdate_ios is aliased via a preprocessor macro to the SecItemUpdate function in SecItem.c. SecItemUpdate_osx is defined in SecItem.cpp. The entire Security framework can be downloaded here.\n\nTo summarize what we\u2019ve learned about keychains as a result of this investigation:\n\nReturning back to the failing Valet test, in order to account for the differing keychain behavior across platforms, the following:\n\nneeds to be replaced with:\n\nWith this change the test now passes, and a link to the final Valet fix can be found here on Github.\n\nAfter completing our investigation, we relayed to the developer that the crash was due to an NSData object being stored against a keychain entry\u2019s kSecAttrAccount attribute.\n\nThey responded that their app had always stored kSecAttrAccount string values, but were using a third-party library that was also storing credentials containing kSecAttrAccount values in the keychain. It was highly probable that this library was inserting the offending keychain entry. The library, however, was critical to the app\u2019s functionality and could not be removed.\n\nIt\u2019s common to use the app\u2019s main bundle identifier as the kSecAttrService attribute value, which acts as a namespace within the keychain. The developer\u2019s code and the library were most likely storing entries under the same namespace.\n\nThe code used in the developer\u2019s app to migrate their keychain entries to Valet:\n\nThe query above returns all generic passwords stored under the app\u2019s bundle identifier, and in this case, it included an entry stored by a third party library with a kSecAttrService Data object.\n\nUpdating the developer\u2019s app to a version of Valet containing the fix resolved the issue for them.\n\nI\u2019d like to thank Eric Muller and Dan Federman, the creators of Valet, for helping to debug and fix this issue."
    },
    {
        "url": "https://medium.com/square-corner-blog/rubys-new-jit-91a5c864dd10",
        "title": "Ruby\u2019s New JIT \u2013 Square Corner Blog \u2013",
        "text": "There have been many attempts at implementing a JIT for CRuby, the reference implementation of Ruby. None have been merged, until now.\n\nTL;DR: Ruby 2.6 will have an optional flag that will increase startup time and take more memory in exchange for blazing speed once warmed up.\n\nSome previous attempts at a JIT for Ruby, like rujit, have been successful at speeding up Ruby at the cost of excessive memory usage. Other attempts, like OMR + Ruby, use an existing JIT library such as Eclipse OMR. Another example, llrb, uses the JIT library that ships with LLVM. The biggest perceived problem with these implementations is that JIT libraries are moving targets, and tie Ruby\u2019s survival to the unknown future of these underlying JIT projects.\n\nVladimir Makarov is no stranger to Ruby performance. His reimplementation of hash tables in Ruby 2.4 sped up hashes considerably.\n\nIn 2017, Makarov undertook a major new project, called RTL MJIT, rewriting the way Ruby\u2019s intermediate representation (IR) works and adding a JIT at the same time. In this incredibly ambitious project, the existing YARV instructions were replaced entirely with a brand-new instruction set in Register Transfer Language (RTL). Makarov also created a JIT called MJIT, which produces C code from the RTL instructions, and then compiles that C code onto native code with existing compilers.\n\nThe problem with Makarov\u2019s implementation is that the brand new RTL instructions meant a major rewrite of Ruby\u2019s internals. It could take years to polish this work to a point where it\u2019s stable and ready to be merged into Ruby. Ruby 3 may even ship with the new RTL instructions, but hasn\u2019t yet been decided (and would be years away).\n\nTakashi Kokubun is no stranger to JITs and Ruby performance either. He was the author of the llrb JIT, and sped up Ruby\u2019s ERB and RDoc generation by several times in Ruby 2.5.\n\nKokubun took Makarov\u2019s work on RTL MJIT and extracted the JIT portion while maintaining Ruby\u2019s existing YARV bytecode. He also pared down MJIT to a minimal form, without advanced optimizations, so it could be introduced without major disruption to other parts of Ruby.\n\nKokubun\u2019s work was merged into Ruby and will be released with Ruby 2.6 on Christmas 2018. If you\u2019d like to try out the JIT now, you can do so with Ruby\u2019s nightly builds. The performance improvements are fairly modest at the moment, but there\u2019s plenty of time to add optimizations before Ruby 2.6 is released. Kokubun\u2019s strategy for safe, gradual improvement worked. Ruby has a JIT!\n\nRuby has to go through a few steps in order to run your code. First, the code is tokenized, parsed and compiled into YARV instructions. This part of the process comprises about 30% of the total time a Ruby program takes.\n\nWe can see each of these steps using RubyVM::InstructionSequence and Ripper from the stdlib.\n\nProjects like yomikomu and bootsnap show how you can speed up Ruby by caching the YARV instructions to disk. In doing so, the instructions don\u2019t have to be parsed and compiled to YARV instructions after the first time the Ruby script is run, unless there are changes to the code. This doesn\u2019t speed up Ruby on the first run, but subsequent executions without code changes will be about 30% faster\u2014because they can skip parsing and compiling to YARV instructions.\n\nThis strategy of caching the compiled YARV instructions actually has nothing to do with the JIT, but it\u2019s a strategy that\u2019s going to be used in Rails 5.2 (via bootsnap) and likely in future versions of Ruby, too. The JIT only comes into play after YARV instructions exist.\n\nOnce the YARV instructions exist, it\u2019s the RubyVM\u2019s responsibility at runtime to turn those instructions into native code appropriate for the operating system and CPU that you\u2019re using. This process consumes about 70% of the time it takes to run a Ruby program, so it\u2019s the bulk of the runtime.\n\nThis is where the JIT comes into play. Instead of evaluating the YARV instructions each time they\u2019re encountered, certain calls can be turned into native code, so when the RubyVM sees it a second or subsequent time, the native code can be used directly.\n\nWith MJIT, certain Ruby YARV instructions are converted to C code and put into a file, which is compiled by GCC or Clang into a dynamic library file. The RubyVM can then use that cached, precompiled native code from the dynamic library the next time the RubyVM sees that same YARV instruction.\n\nHowever, Ruby is a dynamic language, and even core class methods can be redefined at runtime. There needs to be some mechanism to check if the call that is cached in native code has been redefined. If the call has been redefined, the cache needs to be flushed and the instruction needs to be interpreted as if there were no JIT. This process of falling back to evaluating the instruction when something changes is called deoptimization.\n\nNote, in the example above, that the C code MJIT produces will deoptimize and re-evaluate the instruction if the call is redefined. This takes advantage of the fact that most of the time we\u2019re not redefining addition, so we get to use the compiled, native code with the JIT. Each time the C code is evaluated, it will make sure that the operation it\u2019s optimizing hasn\u2019t changed. If there\u2019s a change, it\u2019s deoptimized and the instruction is re-evaluated by the RubyVM.\n\nYou can use the JIT by adding the flag.\n\nThere are also a number of experimental JIT-related flag options:\n\nYou can try the JIT interactively in IRB.\n\nIt seems to be a bit buggy in these early days, but the JIT also works with Pry.\n\nStartup time is one thing to take into consideration when considering using the new JIT. Starting up Ruby with the JIT takes about six times (6x) longer.\n\nWhether you use GCC or Clang will also affect the startup time. At the moment, GCC is considerably faster than Clang, but it\u2019s still more than three times (3x) slower than Ruby without the JIT.\n\nAs such, you probably won\u2019t want to use the JIT with very short-lived programs. Not only does the JIT need to start up, but to be effective, it also needs time to warm up. Long-running programs are where the JIT will shine \u2014 where it can warm up and get a chance to use its cached native code.\n\nIn 2015, Matz announced the 3x3 initiate to make Ruby 3.0 three times (3x) faster than Ruby 2.0. The official benchmark for Ruby 3x3 is optcarrot, a working Nintendo emulator written in pure Ruby.\n\nThe actual Nintendo runs at 60 FPS. Kokubun\u2019s benchmarks of optcarrot on a 4.0GHz i7\u20134790K with 8 cores show Ruby 2.0 at 35 FPS. Ruby 2.5 is a 30% improvement at 46 FPS. With the JIT enabled, Ruby 2.6 is 80% faster that Ruby 2.0 at 63 FPS. This is the carrot part of optcarrot (optimization carrot), a playable NES game at 60+ FPS.\n\nThat\u2019s a big improvement! Adding a JIT to Ruby has already moved Ruby 2.6 well towards the 3x3 initiative goal. The initial JIT improvements are fairly modest, since MJIT was introduced without many of the optimizations found in the original RTL MJIT. Even without these optimizations, there is notable performance improvement. As additional optimizations are added, these improvements will become more profound.\n\nThe benchmark below shows optcarrot performance for the first 180 video frames across a variety of Ruby implementations. Ruby 2.0 and 2.5 show a fairly flat performance. TruffleRuby, JRuby and Topaz are Ruby implementations that already have a JIT. You can see that the implementations with a JIT (the green, red and purple lines below) have a slow startup and then take a few frames to start warming up.\n\nAfter warming up, TruffleRuby pulls well ahead of the pack with its highly optimized GraalVM JIT. (See also TruffleRuby with SubstrateVM, which significantly improves startup time with only slightly less speed once warmed up.)\n\nThe official optcarrot benchmarks don\u2019t yet include Ruby 2.6-dev with the JIT enabled, but it won\u2019t be able to compete with TruffleRuby at this point. TruffleRuby is well ahead of the rest of the pack, but not yet ready for production.\n\nModifying optcarrot benchmarks to show Ruby 2.6-dev with the GCC-based JIT enabled, we can see that it take a few frames to warm up. After warming up the JIT, even with most optimizations current not enabled, it pulls ahead. Note the green line starting off slow but then ramping up and staying ahead thereafter.\n\nIf we zoom in, we can see that Ruby 2.6-dev with the GCC JIT hits the point of inflection where it pulls ahead of Ruby 2.5 after about 80 frames \u2014 just a couple seconds into the benchmark.\n\nIf your Ruby program is short-lived, and exits after only a few seconds, then you probably won\u2019t want to enable the new JIT. If your program is longer-running, and you have a bit of memory to spare, the JIT may bring substantial performance benefits.\n\nHere at Square we use Ruby broadly internally, and we maintain dozens of open source Ruby projects, including the Square Connect Ruby SDKs. So for us, CRuby\u2019s new JIT is exciting. There are still lots of kinks to work out, and many low-hanging optimizations to take advantage of before its release this Christmas. Please try out the JIT now on Ruby trunk or the nightly snapshot, and report any issues you encounter.\n\nVladimir Makarov and Takashi Kokubun deserve a tremendous amount of credit for moving Ruby forward with the addition of a JIT that will continue to bring new speed improvements for years to come!\n\nWant more? Signup for our monthly developer newsletter."
    },
    {
        "url": "https://medium.com/square-corner-blog/stop-using-servers-to-handle-webhooks-675d5dc926c0",
        "title": "Stop Using Servers to Handle Webhooks \u2013 Square Corner Blog \u2013",
        "text": "Webhooks are increasingly becoming the main method to get real time data from different services. GitHub, Slack, SendGrid, and even Square use webhooks to let you see data or be notified of events happening on your account. Webhooks are awesome, since they are fairly easy to deal with and prevent developers from having to build some archaic polling system that ends up being fairly wasteful in terms of network requests made vs. actual useful data retrieved.\n\nWhen creating a service to process webhooks, you have a few choices available: you can extend our application to handle incoming data from a defined URL, you can create a microservice, or you can create a Function as a Service (FaaS) function for processing our webhooks. We\u2019ll briefly go through each of those options and the possible tradeoffs, and then we\u2019ll wrap up with an example implementation of a FaaS webhook handler for Square.\n\nExtending your application gives you the benefit of leveraging any helpers or other libraries you already have in your application. Your helpers (or other application tools) can assist with processing this incoming data and might make it easier to manage. Your application is likely continually running anyways, so there isn\u2019t a problem with having it also handle listening for incoming data for your webhooks. However, this approach can be a drawback, since you might be extending your application to handle something that\u2019s not a core functionality or shouldn\u2019t really be coupled with it. How the extension works can really depend on how your own application is structured, but it might be best to separate how your webhooks are handled to something outside of your application.\n\nMeanwhile, a microservice approach might help you move a step away from your application and allow it to simply communicate or process this new data to be consumed by the application later. Unfortunately, we still have the drawback of scalability and provisioning, since we would still need to be continually listening for the new data being sent to the webhook handler. Although it\u2019s entirely possible to estimate how much data might be coming into our webhook handler and provision accordingly, it\u2019s still pretty likely to be a lot of downtime where it\u2019s simply just waiting to service a request.\n\nAt this point I know it\u2019s pretty obvious that I am going to advocate for all the wonderful benefits of using FaaS for processing webhooks, though I do acknowledge there are some pretty annoying tradeoffs. First the benefits. One advantage of using FaaS for processing webhook data is that it allows for nearly unlimited scalability, so you don\u2019t have to worry about being over or under provisioned. Your function only runs when a new event occurs, so you could be saving infrastructure costs by not having to run a server continuously just for processing webhook data. On the other hand, the drawbacks around using FaaS are usually around maintainability, testing, and cold starts. There are some tools that help with maintaining versions of your functions, deploying functions, and keeping the functions warm. Since webhooks are not directly servicing users and most webhook providers are fairly forgiving about required response times, FaaS is really well suited for processing webhooks despite the issues around cold starts."
    },
    {
        "url": "https://medium.com/square-corner-blog/getting-started-exploring-sdks-with-repl-driven-development-in-node-js-49e6316dc6a0",
        "title": "Getting Started Exploring SDKs with REPL-Driven Development in Node.js,",
        "text": "The faster it is to get up and running with an API the better. SDKs help speed up that on-boarding process by providing an already-written, pre-made API client that just works in the language you\u2019re already using. The Square Connect API, for example, has SDKs in half a dozen languages.\n\nSo how do we explore an SDK? One way is to take examples from the documentation, save them to file, then run them. That process works, but you can decrease the time to first API call by removing the save-to-file step and using something called a REPL.\n\nMany, if not most, languages these days ship with a REPL. Every Square Connect SDK language actually ships with a REPL! Below we\u2019ll take a quick look at examples of getting started with REPL-driven development in a variety of languages. If you already know what a REPL is and want to jump straight into examples, here\u2019re links for Node.js, Ruby, Python, PHP, CSharp and Java.\n\n\u201cREPL\u201d is an acronym that stands for \u201cRead,\u201d \u201cEvaluate,\u201d \u201cPrint,\u201d and \u201cLoop.\u201d You usually enter a REPL from the command line and then interact via a text interface. The REPL reads what you type up until you hit \u201cEnter.\u201d The text you entered is then evaluated by the programming language and the result is printed to screen. Then comes the loop part, starting over to read input again \u2014 so you get an interactive experience.\n\nA REPL lets you try a snippet of code and immediately see the result. It lets you run your code directly in the terminal and see the result right away. That\u2019s often a more convenient way to try out code compared to saving it to a file and then manually executing that file with the appropriate language.\n\nFirst off, let\u2019s establish that REPL-Driven Development (RDD) is not a replacement for Test-Driven Development (TDD). RDD is something that you can use to augment your TDD. The REPL is a place to explore and then dispose of the mess you\u2019ve made. Your tests solidify what you\u2019ve learned in code that is kept for posterity. The REPL is actually really handy for exploring your tests too, especially when tracking down what went wrong. The tight, interactive feedback loop that a REPL provides speeds up debugging your code and tests alike.\n\nThere\u2019s something wonderful about throwing away code. It takes the pressure off when you know you\u2019re not going to keep it, so you can just explore. You can always do this with a file that you change and run repeatedly, but it takes time to shift back and forth between text editor and terminal. And then at the end you\u2019ll have to clean up what you\u2019ve done or leave behind cruft. A REPL lets you explore freely and interact with your code!\n\nExploring an SDK that you\u2019re looking into is a great use for a REPL. It not only lets you try out various endpoints while navigating the API, but it also lets you introspect and ask responses what all they can do. We\u2019ll use the Square Connect SDKs as an example below, which supports Ruby, PHP, Python, Node.js, Java, and CSharp. Let\u2019s look at what it takes to get a development environment REPL installed for each of those languages using macOS with the popular Homebrew package manager.\n\nWhile Ruby is already installed on macOS, it\u2019s nice to grab the latest version with brew:\n\nWe don\u2019t need to install a package manager for Ruby, since the command ships with Ruby. Ruby also ships with a REPL called .\n\nIt\u2019s worth mentioning a popular alternative called . Conrad Irwin gave a great talk that shows off advanced features called REPL driven development with Pry. It\u2019s well worth a watch.\n\nSince RubyGems ships with Ruby we can just install gem:\n\nNow we\u2019re ready to leave the shell and enter the Ruby REPL by just typing :\n\nThis new command line is an interactive Ruby interpreter. From here we can start using the Square SDK:\n\nHere\u2019s what all the steps above look like in realtime from the terminal:\n\nWith that we\u2019re ready to explore the Square Connect SDK from Ruby interactively! Let\u2019s take a look at getting up to this point in other languages next.\n\nPHP also ships with macOS. We\u2019ll go ahead and use the macOS system version of PHP, but we\u2019ll need to install a PHP package manager called Composer:\n\nFrom there it\u2019s simple to install with :\n\nAnd finally we can enter the simple REPL that ships with PHP and require the Square Connect SDK:\n\nPHP\u2019s built-in REPL doesn\u2019t have many features so it\u2019s worth considering an alternative such as Boris.\n\nWe\u2019ll use Mono to run CSharp on macOS. This package takes a while to install compared to the others:\n\nThen we can use to install the Square Connect SDK for CSharp:\n\nMono ships with a CSharp REPL, so we\u2019ll use that. We\u2019ll just have to point the REPL at the Square Connect SDK that was just installed by :\n\nPython 2 ships with macOS, but let\u2019s go ahead and grab Python 3:\n\nThen we can use the Python package manager to install the Square Connect SDK:\n\nPython also ships with a built-in REPL, so we\u2019ll use that:\n\nNode.js doesn\u2019t ship with macOS, so lets grab it from Homebrew:\n\nAnd then let\u2019s use Node\u2019s NPM package manager to install the Square Connect SDK:\n\nNow we\u2019re ready to use Node\u2019s built-in REPL:\n\nUnlike the other languages we\u2019re looking at today, Java installation requires a download from Oracle\u2019s webpage and click through agreement to install. Once Java is installed we\u2019ll need a package manager, so we\u2019ll go with Maven:\n\nThen we can install the latest version of the Square Connect SDK:\n\nSince JDK 9, Java has shipped with a REPL called . It\u2019s a bit more verbose, but you can load the Square Connect SDK just like the other REPLs:\n\nOkay, that was a lot of installing! Now that our REPLs and SDK packages are installed we\u2019re ready to explore the Square Connect SDK in any of the above languages. Each of the REPLs above have different feature and capabilities. Some languages ship with a great REPL, but it\u2019s often worth exploring alternatives. Some REPLs are fairly simple, while others let you dive deep into the code and can be used for serious debugging. When you\u2019re trying out a language or an SDK, consider exploring with a REPL. Happy coding!"
    },
    {
        "url": "https://medium.com/square-corner-blog/lets-build-a-markdown-parser-7b02e91444c5",
        "title": "Lightning Talks: Let\u2019s build a Markdown parser! \u2013 Square Corner Blog \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/square-corner-blog/caviars-word2vec-tagging-for-menu-item-recommendations-13f63d7f09d8",
        "title": "Caviar\u2019s Word2Vec Tagging For Menu Item Recommendations",
        "text": "As mentioned above, our approach was to recast what would typically be a classification problem as a similarity search problem. The basic steps followed were:\n\nWord2Vec is neural network word embedding technique that learns a vector space model from a corpus of text such that related words are closer in the space than non-related words. This allows for interesting operations like similarity comparisons and vector algebra on concepts. For our purposes, we\u2019d like to see a vector space model similar to the following:\n\nWe used the Gensim package to train a Word2Vec model on a corpus of Caviar restaurant menus. While there are a number of good pre-trained Word2Vec models based on large corpuses such as Wikipedia and Google News that we tried first, we found they did not perform as well as our custom model trained on Caviar\u2019s restaurant menus alone. It appears that food language in menus is qualitatively different than food language in general sources like encyclopedias and news. This is something we plan to explore more in the future.\n\nOne limitation of Word2Vec for our purposes is that it only deals with individual words. For our formulation to work, we need to be able to create fixed-length vectors for the many multi-word tags and menu items that we have (e.g., the tag \u201cIndian Curry\u201d and the menu item \u201cBig Bob\u2019s Chili Sombrero Burger\u201d). There are advanced techniques for this such as Doc2Vec, but we found that simply averaging the vectors for each word in a phrase worked well in practice.\n\nA key step in our approach is selecting the candidate tags. Our primary method is to compile sets of related tags such as cuisine types(e.g., \u201cPizza\u201d, \u201cBurger\u201d, and \u201cThai Curry\u201d) and dietary restriction (e.g., \u201cVegetarian\u201d, \u201cVegan\u201d, and \u201cGluten Free\u201d) where we expect most menu items to be best classified by only one of the tags in the set. We used cluster visualization to demonstrate this approach with cuisine types. An additional promising method is crafting individual multi-word tags that capture a broader concept (e.g., \u201cCake Cookies Pie\u201d) and match menu items beyond just those listed in the tag (e.g., match cupcake and donut items for \u201cCake Cookies Pie\u201d). Crafting these types of tags is more of an iterative process akin to coming up with a good search engine query. We explored this approach with a couple of in-app collections as shown in a later section.\n\nAs mentioned in the introduction, we wanted to avoid the cost of supervised methods, specifically the creation of a ground truth set for training and validation. However, we still needed a way to validate our tags, so as a compromise, we leveraged interactive cluster visualization to do ad hoc manual validation instead. We adapted the Tensorflow Embedding Projector for this purpose:\n\nBy following the outlined steps for every menu item with cuisine types as the candidate tag set, we obtained a cosine similarity score for each of the tags. We classified each menu item as the cuisine type with highest similarity score. The following sequence of figures highlights the explorations and validations we performed on the resulting data.\n\nIn the following figure, clusters are colored by most similar tag with no minimum similarity threshold set. This is our \u201chigh recall\u201d scenario. It\u2019s quite noisy, and we see a number of misclassified menu items. In some cases, this is because the menu item is just hard to classify. In many cases, though, it is because the menu item\u2019s true cuisine type is not present in our cuisine type candidate tag set (e.g., we didn\u2019t include tags for \u201cKombucha\u201d, \u201cCheesecake\u201d, or \u201cCoconut Water\u201d), and with no minimum similarity threshold set, an incorrect naive classification is made.\n\nThe following figure demonstrates one of the many correct classifications:\n\nThe following figure demonstrates one of our misclassification scenarios. In this case, Tom Yum Noodle Soup is incorrectly classified as \u201cThai Curry\u201d. This is harder to classify correctly due to Tom Yum being more closely related to Thai cuisine than to typical soups like chicken noodle and minestrone:\n\nThe following figure demonstrates our primary misclassification scenario. In this case, Gyros Plate is incorrectly classified as \u201cFries\u201d due to the menu item\u2019s cuisine type (e.g., \u201cMediterranean\u201d or \u201cGyros\u201d) not being present in our cuisine type candidate tag set:\n\nIn the following figure, clusters are colored by most similar tag with a minimum similarity threshold set (i.e., only classifications of 0.7 cosine similarity or greater are kept). This is our \u201chigh precision\u201d scenario and it is much better! We see good visual separation via inspection, and we find almost no misclassified menu items beyond the ambiguous cases we discuss next:\n\nIn the next figure, we wonder, \u201cIs it pizza or is it fries?\u201d The answer is \u201cBoth!\u201d The Pizza Fries menu item has high scores for both the \u201cPizza\u201d and \u201cFries\u201d tags, with \u201cPizza\u201d edging out \u201cFries\u201d. That the menu item is located equidistantly from the two distinct clusters demonstrates one of the intuitive strengths of this method. There is no reason we couldn\u2019t classify items like this with multiple best tags:\n\nIn the next two figures, clusters are labeled with the classified tag rather than menu item name. The first figure is colored by the classified tag and the second figure is colored by similarity to the \u201cPizza\u201d tag. In the first figure, we see \u201cPasta\u201d and \u201cPie\u201d items are closer to \u201cPizza\u201d than other less similar items like \u201cSushi\u201d or \u201cDumplings\u201d. In the second figure, thanks to the similarity gradient, we can easily see a range from \u201cPizza\u201d to \u201cNot Pizza\u201d. This is another demonstration of the intuitive mapping between the spatial arrangements and the Word2Vec-based similarities that allowed us to perform ad hoc validations on our results:\n\nOur ultimate goal with this work was to automate menu item recommendation collections from the Word2Vec-based taggings, and we\u2019ve already implemented a few examples. The following figures demonstrate collections for both simpler cuisine type tags and more advanced multi-word concept tags.\n\nIn the following figure, we show recommendation collections for the \u201cPizza\u201d and \u201cThai Curry\u201d tags. These are very promising, showing a range of items from the standard (e.g., Cheese Pizza and Panang Curry) to the exciting (e.g., Calabria Pizza and Chicken Pumpkin Curry):\n\nIn the following figure, we show recommendation collections based on the interesting approach of crafting multi-word concept tags. We used \u201cCake Cookies Pie\u201d as the tag for the \u201cFor Your Sweet Tooth\u201d concept collection and \u201cTikka Tandoori Biryani\u201d as the tag for the \u201cNorth Indian Fare\u201d concept collection. These too are very promising. In the \u201cFor Your Sweet Tooth\u201d collection, we see items beyond the \u201cCake Cookies Pie\u201d tag such cupcakes, donuts, ice cream, and even an ice cream scoop. In the \u201cNorth Indian Fare\u201d concept collection, we see items beyond the \u201cTikka Tandoori Biryani\u201d tag such as Saag Paneer and Itsy Bitsy Naan Bites:"
    },
    {
        "url": "https://medium.com/square-corner-blog/free-square-plugin-now-available-for-woocommerce-31490d989671",
        "title": "Free Square plugin now available for WooCommerce \u2013 Square Corner Blog \u2013",
        "text": "WooCommerce integrated Square\u2019s Transactions, Catalog, and Inventory APIs in one easy-to-install extension to make selling online and in-person simpler.\n\nFor developers who manage WordPress or WooCommerce sites for clients: we wanted to make sure you saw that the Square extension for WooCommerce is now available to install for free. This has been a top request from sellers and developers alike, and we\u2019re excited it\u2019s now a reality.\n\nThe extension, which was previously available for $79/year, has been instrumental for businesses that sell both online and in-person, like Maryland-based Candles Off Main. \u201cUsing Square in-store and online has made running my business simpler: one provider, one set of reports, and automatic inventory syncing with my WooCommerce store,\u201d owner Michael Waldon shared with our Product team.\n\nWooCommerce achieved this by integrating with Square\u2019s Transactions API and payment form, Catalog API, and Inventory API. Since everything is pre-built, the implementation process is quick \u2014 just download and configure the extension per the documentation.\n\nThe extension is available in the WooCommerce Extension Library here."
    },
    {
        "url": "https://medium.com/square-corner-blog/soft-skills-reading-list-a8af5a6296e5",
        "title": "Soft-skills Reading List \u2013 Square Corner Blog \u2013",
        "text": "As an engineering manager here at Square, I am often asked by engineers and other managers for advice on all sorts of different topics. If I\u2019m lucky, they enjoy reading as much as I do, and I get the opportunity to recommend a book! Over the years, I\u2019ve found myself returning to a core set of books that cover topics from communication and strategy to biases and leadership. Even if you\u2019re not a manager yourself, understanding many of these topics can make you better at your job and improve your relationship with your own manager. The titles that stand out include:\n\nThere are also several articles that I find myself referencing and returning to often:"
    },
    {
        "url": "https://medium.com/square-corner-blog/implementing-sqpaymentform-in-reaction-commerce-31cfdb737ee2",
        "title": "Implementing SqPaymentForm in Reaction Commerce \u2013 Square Corner Blog \u2013",
        "text": "Under the Payment Card Industry Data Security Standard (PCI-DSS), to process payments securely, you must minimize the amount of sensitive cardholder data that you handle. If you don\u2019t, you could be vulnerable to a breach, lose your ability to process payments, or incur fines from your processor.\n\nAs soon as you accept, process, store, or transmit sensitive cardholder data, you become responsible for safeguarding that data. So how can you be sure that your customer\u2019s data is secure? We know hackers are constantly innovating new ways to steal sensitive information. At Square, we provide PCI compliant products and services so you do not need to individually validate your state of compliance.\n\nSquare helps you avoid handling sensitive payment information altogether by providing the SqPaymentForm JavaScript library to allow safe handling of customer payment information. is a hosted JavaScript library that you load on your page and it helps generate an iframe form that safely captures a customer\u2019s payment information. When the form is processed, it will generate a nonce which replaces sensitive cardholder details with a stand-in token. You can reference this token to process your customer\u2019s payment and to perform other actions using Square\u2019s APIs.\n\nTo give a demonstration around how to implement a secure payment form, I went ahead and implemented one using Reaction Commerce. For those unfamiliar, Reaction Commerce is an open source eCommerce platform that is built using Meteor. This lets us show ways of loading the payment form in a trickier environment that forces us to use a less straightforward implementation.\n\nSo with all of these descriptions out of the way, let\u2019s actually dig into examples and walk through one way to implement inside of Reaction Commerce.\n\nNormally, you need to include on the page where you want to process a customer\u2019s payment using a script tag (usually this is on the checkout page).\n\nThe above will work really well whenever you can simply use HTML tags to include your scripts. Unfortunately, we cannot use this approach for our implementation since Meteor has a unique templating system (Blaze) that makes including external scripts difficult. In order to guarantee that the object is accessible at checkout, we could add the script tag to the headers of the main HTML template. The problem with that is it just seems clunky to be including this script in the main HTML template and loading it for every page of the application when we only really need it for checkout.\n\nThe main problem with trying to include the script in the template for the checkout is that we cannot guarantee that Square\u2019s payment form script has been loaded by the time we\u2019re trying to instantiate the object. We can circumvent this using jQuery\u2019s method since Reaction Commerce provides this for us, and we can guarantee we have loaded the script when we\u2019re trying to use the object in our client-side script.\n\nYou can see that we\u2019re now gaining access to the object within the promise chain of the method. In my fork of Reaction Commerce, I had actually wrapped this all in a function so that it could be called again in case we needed to destroy the payment form and recreate it again (see more on this in the troubleshooting section of our tutorials here).\n\nThis should get us far enough that we can get the payment form to load and have the stylings that we want, but we\u2019re still missing functionality around generating our for processing the payment. The first step to this is triggering to request a card for us.\n\nSo here we\u2019ve just added a listener on our submit button to intercept the submission so that we can trigger the method first. Once we have the , we can then trigger the form to actually submit with the returned placed in a hidden input field. provides us a callback to use for knowing when we\u2019ve received the card response, called .\n\nWe have very basic error logging here in case there was an issue generating or receiving the , but then we resume or usual form submission process after injecting the into our hidden nonce input field.\n\nSince Reaction Commerce uses for handling all of their form processing, I figured it would be better to try utilizing this to be more inline with how the platform is configured. has a few lifecycle methods for handling the form submission, which is where we\u2019ll be processing our and passing it back to the server for processing. Within , there is a hook that we can use for processing everything.\n\nThe form values are passed into the function through the doc variable, and we can use this for pulling out our to send to the server. We also add in the after a successful processing, so that there aren\u2019t any issues if they decide to continue shopping right after making their order.\n\nNow we have a successfully generated and processing payment form using ! All that would be left to do is implement the back-end processes for processing the payment. This was just to be a quick run through of implementing the front-end of payment processing in a non-normal environment so that you can get the card generated and sent to your back-end server.\n\nYou can check out my fork over at https://github.com/mootrichard/reaction to see all of it (it still only has the front-end forms setup to be working and still has some unnecessary logging). Please try it out for yourself, leave me a comment and let me know what you think about it. You can sign-up for your own Square developer account at https://connect.squareup.com/apps to get started and if you\u2019re interested in learning more about Square\u2019s developer platform then signup for our newsletter."
    },
    {
        "url": "https://medium.com/square-corner-blog/accessible-colors-for-data-visualization-2ad64ac4ee7e",
        "title": "Accessible Colors for Data Visualization \u2013 Square Corner Blog \u2013",
        "text": "A good graph is easy to read. A goal when creating data visualizations is to convey information in a clear and concise way. One of the most prominent features of most data visualizations is color, but what happens to a graph for someone who has difficulty differentiating colors?\n\nFor example, an orange and green color that may appear separate to some could appear nearly identical to individuals with protanopia vision, a type of colorblindness.\n\nAt Square, we\u2019re building a guide for visualizing data so that data is accessible to as many people as possible. Today, we\u2019re open-sourcing the design file to continue evolving this resource. Additionally, later this year, we\u2019re open-sourcing a Python package for you to easily use and adapt this design file in your visualization work.\n\nFirst, let\u2019s look at what data visualizations and color vision deficiencies are:\n\nData visualization (visualisation), or the visual communication of data, is the study or creation of data represented visually. Michael Friendly describes it as, \u201cinformation that has been abstracted in some schematic form, including attributes or variables for the units of information.\u201d Nicholas Felton describes his work as, \u201ctranslating quotidian data into meaningful objects and experiences.\u201d\n\nData visualization comes in many forms. Some of my favorite places to look at and think about different types of visualization:\n\nThe Data Viz Project by Ferdio, a Copenhagen-based infographic agency.\n\nGiorgia Lupi and Stefanie Posavec\u2019s project Dear Data \u2014 now a part of the Museum of Modern Art collection \u2014 is a collaboration of personal data visualizations that Giorgia and Stefanie mailed to each other on postcards.\n\nMike Bostock, one of the key developers of D3.js, creates interactive data visualizations.\n\nAlso fun to check out the popularities of different types data visualizations and resources at The Visualization Universe, a collaboration between Adioma and Google News Lab.\n\nSources: Ferdio, Michael Friendly, Nicholas Felton, Dear Data by Giorgia Lupi and Stefanie Posavec, Buy or Rent Calculator, The New York Times by Mike Bostock.\n\nColor vision deficiency is a condition where a person\u2019s eyes are unable to see colors in normal light situations. People with color vision deficiency have a hard time telling colors apart. Most people who suffer from color vision deficiency are not blind to color, but have a reduced ability to see them. However, the most severe forms of these deficiencies are referred to as color blindness.\n\nColor blindness affects millions of people worldwide: it is estimated that 260\u2013320 million total individuals are affected genetically \u2014 1 in 12 men and 1 in 200 women. However, color deficiencies \u2014 of all kinds \u2014 vary between individuals. The World Health Organization (WHO) identifies 4% of the global population as being visually impaired, 4% as having low vision, and 0.6% as being blind.\n\nThere are a variety of classes of color vision deficiency; the most common is red-green color blindness or protanopia. Other color vision deficiencies include: protanomaly, deuteranopia, deuteranomaly, tritanopia, tritanomaly, achromatopsia, achromatomaly, and low-contrast.\n\nColor vision deficiencies can be acquired, but most are inherited genetically. The genes that influence the colors inside the eyes, called \u2018photopigments,\u2019 are carried on the X chromosome. If these genes are abnormal or damaged, color blindness occurs. Sometimes color blindness can be caused by physical or chemical damage to the eye, the optic nerve, or parts of the brain that process color information. Color vision can also decline with age, most often because of cataract \u2014 a clouding and yellowing of the eye\u2019s lens.\n\nThe use of color in data visualization is primarily to differentiate data categories to illustrate the information described as quickly and easily as possible.\n\nThe color sets in this guide are created for contrast across multiple types of reduced vision and color vision deficiencies, containing colors that are differentiated across hue, saturation, and brightness.\n\nWe also intended to use colors that we find aesthetically pleasing. Of course this is subjective and contextual\u2014a red color might be preferred to conform to a brand\u2019s color set, but have negative implications when used for visualizing financial information.\n\nAn element of contrast that isn\u2019t included in this guide yet\u2014as it\u2019s harder to systematize\u2014is texture. If a bar chart incorporated diagonal lines and a grid of dots, there would be another type of differentiation that would assist in clearly communicating two separate data sets.\n\nTo contribute to the design file: Open the Figma file, login to Figma (or sign up for free), click to leave a comment, enter your feedback, and click \u201cPost.\u201d"
    },
    {
        "url": "https://medium.com/square-corner-blog/topic-modeling-optimizing-for-human-interpretability-48a81f6ce0ed",
        "title": "Topic Modeling \u2013 Square Corner Blog \u2013",
        "text": "Have you ever skimmed through a large amount of text and wanted to quickly understand the general trends, topics, or themes that it contains? This ranges from that book you never finished for book club to parsing through user feedback and comments at work. Well, rest-assured you\u2019re not alone and there are tools out there to help you accomplish this feat, such as topic modeling.\n\nTopic modeling is a type of unsupervised machine learning that makes use of clustering to find latent variables or hidden structures in your data. In other words, it\u2019s an approach for finding topics in large amounts of text. Topic modeling is great for document clustering, information retrieval from unstructured text, and feature selection.\n\nHere at Square, we use topic modeling to parse through feedback provided by sellers in free-form text fields. One example is pictured below \u2014 a comment section for sellers to leave feedback about why they\u2019ve decided to leave the product and how we can better serve them and other customers like them in the future.\n\nAlthough topic modeling is a powerful statistical method, it can be difficult to evaluate the effectiveness of a topic model or interpret the results. Two major questions data practitioners ask when using topic modeling are:\n\n1.) What is the best way to determine the number of topics (\ud835\udf05) in a topic model?\n\n2.) How do you evaluate and improve the interpretability of a model\u2019s results?\n\nIn this article, I\u2019ll walk through a few techniques that can help you answer the above questions.\n\nWhen implementing a topic model, it\u2019s important to clean your text data to ensure you get the most precise and meaningful results. This includes common data pre-processing techniques such as tokenization, removing stopwords, and stemming/lemmatization. These are ways to segment documents into their atomic elements of words, remove words that provide little to no meaning, and ensure that root words similar in meaning get their proper weightage of importance in the model.\n\nThe next step after adequately cleaning and preparing your corpus is to construct a document-term matrix and train your model on it. This is where the first question of number of topics (\ud835\udf05) to choose arises since \ud835\udf05 is a common model parameter for topic models.\n\nWhen determining how many topics to use, it\u2019s important to consider both qualitative and quantitative factors. Qualitatively, you should have domain knowledge of the data you\u2019re analyzing and be able to gauge a general ballpark of clusters your data will separate into. There should be enough topics to be able to distinguish between overarching themes in the text but not so many topics that they lose their interpretability. In the case of evaluating our sellers\u2019 text responses, from a qualitative perspective, 10 topics seemed like a reasonable place to start because it gives enough room to capture actionable topics such as functionality and cost without getting to granular.\n\nFrom a quantitative perspective, some data practitioners use perplexity or predictive likelihood to help determine the optimal number of topics and then evaluate the model fit. Perplexity is calculated by taking the log likelihood of unseen text documents given the topics defined by a topic model. A good model will have a high likelihood and resultantly low perplexity. Usually you would plot these measures over a spectrum of topics and choose the topic that best optimizes for your measure of choice. But sometimes these metrics are not correlated with human interpretability of the model, which can be impractical in a business setting.\n\nAnother quantitative solution you can use to evaluate a topic model that has better human interpretability is called topic coherence. Topic coherence looks at a set of words in generated topics and rates the interpretability of the topics. There are a number of measures that calculate coherence in various ways, but Cv proves to be the measure most aligned with human interpretability (see Figure 2 below).\n\nBecause Cv is generally the most interpretable, I used this coherence measure to refine my qualitative selection of 10 topics for our sellers\u2019 text responses using Latent Dirichlet Allocation (LDA).\n\nLDA is a generative statistical topic model used to find accurate sets of topics within a given document set. The model assumes that text documents are comprised of a mix of topics and each topic is comprised of a mix of words. From there, using probability distributions the model can determine which topics are in a given document and which words are in a given topic based on word prevalence across topics and topic prevalence across document. Looking at the topic coherence across a number of topics ranging from 0 to 15, seven was the optimal number of topics to use for this model to maximize topic coherence.\n\nOnce you choose the optimal number of topics, the next question to tackle is how to best evaluate and improve the interpretability of those topics. One approach is to visualize the results of your topic model to ensure that they make sense for your use case. You can use the pyLDAvis tool to visualize the fit of your LDA model across topics and their top words. If the top words for each topic are not coherent enough to pass a word intrusion test ( i.e., if you cannot identify a spurious or fake word inserted into a topic), then your topic model can still benefit from interpretability improvements.\n\nAnother approach is to look at the topic coherence across different models. For my analysis, I looked at how LDA performed in comparison to other topic models such as Latent Semantic Indexing (LSI), which gives you ranking order of topics using singular value decomposition, and Hierarchical Dirichlet Process (HDP), which uses posterior inference to determine the number of topics for you. Overall LDA performed better than LSI but lower than HDP on topic coherence scores. However, upon further inspection of the 20 topics the HDP model selected, some of the topics, while coherent, were too granular to derive generalizable meaning from for the use case at hand.\n\nOverall, when choosing the number of topics and evaluating the interpretability of your topic model, it is important to look at both qualitative and quantitative factors. It\u2019s also important to balance interpretability with other quantitative metrics, such as accuracy and perplexity, that help you gauge the model\u2019s performance.\n\nThis also holds true for how you evaluate the results of the model. In our use case of evaluating our sellers\u2019 text responses, it\u2019s quantitatively useful to have topic categories to understand general trends in how our sellers\u2019 needs are growing and how we as a company can evolve to meet those needs. It\u2019s also qualitatively useful to drill into those topics and understand the nuances of each of our sellers\u2019 individual requests.\n\nNo matter how you choose to use topic modeling, keeping these tips in mind will help you to best optimize your model and get results that are valid, useful and applicable for whatever your business needs may be."
    },
    {
        "url": "https://medium.com/square-corner-blog/how-we-use-stackoverflow-to-support-our-developers-d800fc37f650",
        "title": "How we use StackOverflow to support our developers \u2013 Square Corner Blog \u2013",
        "text": "StackOverflow is an invaluable resource for developers of all skill levels, and is a part of millions of developers\u2019 regular workflow. All of that knowledge is only useful if people are answering questions. We use a combination of tools, design, and analytics to make sure we can efficiently answer the questions we know best.\n\nQuestion tags are one of the most basic pieces of meta-data that StackOverflow has. We do our best to answer questions our developers have, but it's hard to find them if they aren\u2019t properly tagged. To get developers to that point, we try to make our tag as obvious as possible:\n\nHaving a question tagged doesn\u2019t get it answered, so we use a couple of integrations to help keep us up-to-date with questions under the tag.\n\nWith any product or service, it\u2019s important to measure how effective it is in meeting your customers\u2019 needs. For us and StackOverflow, we want to make sure developers get unblocked quickly and can easily access help when needed. Luckily, the Stack Exchange Data Explorer provides a handy way to access any information we might want. There are tons of helpful metrics; one that\u2019s particularly interesting is the median response time for questions asked with the tag. You can see the query here, and a graph of the data below:\n\nLooking at one of the most popular tags, you can see that there are definitely some trends at play when you have tens of thousands of questions being asked. I\u2019ll let you decide what the cause is.\n\nI hope you enjoyed this behind-the-scenes peek at some of the things we do to support developers! If you want to see more posts like this, or you think we could be doing things better, let us know in a comment below. \ud83d\ude01"
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-the-square-connect-node-sdk-4f2a30f46b6e",
        "title": "Introducing the Square Connect Node SDK \u2013 Square Corner Blog \u2013",
        "text": "As part of our goal of empowering our developers to innovate, build, and create, the developer platform team is happy to announce a new addition to the Square SDK family: the Square Connect Node SDK. With the release of the Square Connect Node SDK, Square now offers eight different SDKs ranging from iOS to C# to help developers effortlessly and seamlessly integrate with Square on any platform.\n\nEnabling Square for a Node.js application can be done in three easy steps.\n\nThe Node.js SDK is catered for versions 6.0 or later. If your app meets this requirement, adding the SDK is as easy as adding the square-connect package to your application.\n\nNow that square-connect is added to your application, you need to add authorization. Your access token and other credentials can be found on your Square Application Dashboard located in the Square Developer Portal.\n\nOnce the square-connect package is installed and Oauth2 is configured, you\u2019re ready to make your first API call.\n\nWe can hit an endpoint simply by initializing a new API client and then calling a function named after your desired endpoint.\n\nThe Node.js SDK, like all our other SDKs, was created to enhance your developer experience. By adding a Node.js SDK, we hope to lighten the load for our developers and make it easier to leverage the full power of the Square platform."
    },
    {
        "url": "https://medium.com/square-corner-blog/storing-customer-card-information-securely-using-point-of-sale-api-9ee028b63190",
        "title": "Storing customer card information securely using Point of Sale API.",
        "text": "You can enable Card on File right from the Square Point of Sale app:\n\nUsing the Point of Sale APIs with your app uses the same payment flows that the native Square Point of Sale app uses. That means that the flow for saving a Card on File for a customer after a transaction is the same as if you were using the Point of Sale app by itself. Since you have the Point of Sale app installed on the device, you are able to save customer\u2019s cards directly through the Point of Sale app as well.\n\nYou can learn all about saving a Card on File in this article, but one of the easiest ways to add a Card on File for is after a transaction. If you pass in a when initiating a transaction then you\u2019ll see a button for adding the Card on File on the confirmation screen in the top left. To pass the in with your transaction, you could add the following field to your iOS code:\n\nor with your Android app:\n\nThen your confirmation screen would have the additional \u201cAdd Card\u201d button.\n\nNow that you have that Card on File for a customer, you can use the Charge endpoint to bill the customer for follow-up services or any other type of subscription. There are existing guides that highlight the process with code, but at a high level, you\u2019ll need to:\n\nSee a complete example of charging a card on file in the Recurring Payments documentation and the Recurring Payments with PHP blog post. If you have any questions/comments on this post, or others, let us know by commenting or reaching out to @SquareDev on Twitter."
    },
    {
        "url": "https://medium.com/square-corner-blog/oauth-with-php-part-two-refreshing-revoking-tokens-9ae065537c41",
        "title": "OAuth with PHP Part Two: refreshing & revoking tokens",
        "text": "This is a follow up to part 1 that talks about creating access tokens from authorization codes.\n\nAt the end of the first part of our PHP journey with OAuth, we had two scripts: an that would begin the OAuth process and a that would accept the result from Square and use the provided authorization code to create an access token that your application can use to use Square\u2019s APIs on a merchant\u2019s behalf.\n\nYour newly minted access token will work great for about a month, but then your token will expire and your app won\u2019t be able to do anything with Square\u2019s APIs until the merchant reauthorizes the application. That isn\u2019t very convenient for your users, so luckily there is a way to programatically refresh your OAuth credentials without requiring your end users to authorize your application. Refreshing your tokens will only work if your permissions do not change your application\u2019s permissions. Any time you want to change the scopes available to your application, you must reauthenticate with the merchant.\n\nWhen you exchange an authorization code for an access token, you\u2019ll get back a json response like this:\n\nYou\u2019ll want to keep track of that time, so that you will be able to refresh the token in time. You can even refresh a token after it is expired, as long as you do it within 15 days of the expiration date.\n\nThe endpoint to refresh your access tokens is a little different than most of our endpoints because you need to authorize with your instead of the in the header. Here is what it looks like with PHP using code:\n\nIf everything goes well, you should get a response similar to when you first got your access token, with the refresh date now a month after the date you refreshed:\n\nThat is all you need to do to refresh your access tokens! Now your application will be able to continue making requests on other merchants\u2019 behalf.\n\nRevoking an access token removes any access it has to Square merchant accounts. This endpoint is accessed similarly to the renew endpoint, with the in the header:\n\nIf your request is successful, then the API will return the following response:\n\nNow the access token that you had created is invalidated, and cannot be used, or refreshed. Your end users will need to authorize your application again in order for your application to be able to use Square\u2019s APIs on their behalf.\n\nTo simplify everything, I\u2019ve included all of the code (including the authorization from part 1) into a single page that will redirect you to Square, create an access token, renew the token and then revoke it all in one go.\n\nNow you should have a complete example in PHP to do everything you can do with OAuth. If you have any questions, leave a comment on this post, and of course take a look at the official Square OAuth documentation."
    },
    {
        "url": "https://medium.com/square-corner-blog/oauth-with-php-part-one-getting-access-tokens-5a18b0b70099",
        "title": "OAuth with PHP, Part One: getting access tokens. \u2013 Square Corner Blog \u2013",
        "text": "OAuth is an important part in safely controlling access between developers\u2019 applications and Square merchant accounts. If you are building your application for multiple Square accounts to use, you\u2019ll need to implement it.\n\nIn addition to this post, you should look at Square\u2019s OAuth documentation while building your integration\u2014it\u2019ll always be the source of truth, and constantly updated should anything change.\n\nAt its core, OAuth requires you to redirect users to a special URL on Square\u2019s site that includes your . The merchant then decides whether or not to allow your application access, and which permissions your app will have. You application then gets an authorization code, which it will exchange for an access token with an authenticated request.\n\nLet\u2019s see what that would look like in code:\n\nSince this is an example, we will use PHP\u2019s built-in web server to host the files, and PHP\u2019s built in URL request functionality to make the requests to Square\u2019s servers. You will also need to create an application in the Square development portal, if you haven\u2019t done so already, to get an that you will need in the next step. You\u2019ll also need to add the following as your callback URL under the OAuth section of your application: .\n\nYou can start by creating a new folder on your filesystem and an that we will put our code into, and starting up the development web server.\n\nThe first step in the flow involves an end wanting to connect your application to their Square account, usually by clicking a button. You can just add a link to with your appended to the end.\n\nNow if you go to localhost:8080 you should see something like:\n\nIf you click on the link, you will be prompted to accept the application (if you have already accepted it, or created the application yourself, then you won\u2019t see that screen until you remove the application permissions in your Square dashboard) and then be redirected to the callback URL that you set in the developer portal for the application ( ). Let\u2019s fill in that now:\n\nIf the merchant accepted the permissions for your application, then you will get an authorization code sent to your callback URL as a GET parameter. In your production code, you should check to see if the end user rejected your application (in that case you\u2019ll get a different response without an authorization code) as described in the OAuth documentation, but you can skip that step in for this demo.\n\nTo start, will need some of the values that you have set from your application portal, then it will bundle those into a json body for a POST request and send them to Square\u2019s token endpoint, printing the results on the screen.\n\nIf everything goes well, then you should get the json body from the API with your access token printed onto the screen.\n\nSo at the end of this, the complete example looks like:\n\nRemember that you\u2019ll need to fill in the options for your , , and of course setting the ( ) in the developer portal for the application.\n\nI hope this was helpful to those of you struggling with OAuth! In part two, we\u2019ll look at how to refresh and revoke the access tokens. Let me know if you have any questions or comments, and of course, follow us on Twitter at @SquareDev."
    },
    {
        "url": "https://medium.com/square-corner-blog/creating-a-personalized-professional-ticketing-experience-for-any-business-powered-by-square-apis-15108a2bf318",
        "title": "Creating a personalized, professional ticketing experience for any business, powered by Square APIs",
        "text": "SimpleTix founder Aron Kansal first started tinkering with an event ticketing solution in college, over ten years ago. What began as a ticketing page he built in Adobe Dreamweaver for a small theater in Canada has now evolved into a flexible, full-service ticketing platform for thousands of venues.\n\n\u201cI learned through my work as a freelance web designer that many venues needed an affordable way to create a professional event ticketing experience,\u201d Kansal said of founding SimpleTix. \u201cAs SimpleTix has evolved, we have differentiated with a product that can be personalized to each customer.\u201d\n\nCompared to competitors with a standardized approach, SimpleTix puts the client\u2019s brand front-and-center, making the ticketing page look and feel like it was built in-house. They can create a single-event page for individual events and festivals, or a complete events listing page for a venue with a full slate of events. The latter option is built to look exactly like any other page on the client\u2019s website, creating a consistent experience for the end-customer. They can also create custom seating charts \u2014 key for venues selling tickets to events with assigned seats.\n\nSimpleTix\u2019s customers were the main drivers for Kansal to integrate his product with Square. \u201cOne theater in New York City really pushed us to offer Square as a payments partner,\u201d he told us. He found our ECommerce API and quickly had a solution up and running.\n\nKansal pointed to two features in particular that sold him on the Square integration (beyond fulfilling an increasingly popular customer request!).\n\nFirst, he loves the oauth permissioning process, which makes for a seamless onboarding experience. His clients do not need to copy and paste any keys or configure anything. Oauth is simple, secure, and fast. Plus, anyone without a Square account can set one up in minutes during SimpleTix\u2019s onboarding \u2014 much less onerous than a typical payments signup process.\n\nSecond, Square is unique in that it enables his venues to sell both online (with SimpleTix) and in-person (with Square Point of Sale), creating an integrated multi-channel sales solution. Many of his customers use Square POS to sell tickets at the door, and SimpleTix is able to send the tickets directly to the customer\u2019s phone instantly.\n\nIntegrating with Square has also brought in new business for SimpleTix: \u201cWe have gotten much better leads from Square than we have from any other partner,\u201d Kansal said. \u201cMost of our customers are really happy with our product and the Square integration, so they refer us to other theaters managers, conference organizers, and festival organizers.\u201d In just a few months, he had a wave of customers signing up and using Square for their SimpleTix account.\n\nWith the launch of Multi-Party transaction capabilities through Square\u2019s E-commerce API, SimpleTix added a way to charge their clients a flat, per-ticket fee as part of the payment flow. This provides a revenue stream for SimpleTix that is aligned with their clients\u2019 growth. When an event performs well, SimpleTix also benefits from the service they provided in helping their client succeed.\n\nSimpleTix is hoping to take their platform in-person next, by building a SimpleTix Point-of-Sale app that processes through Square. This will enable them to create a more cohesive multi-channel experience for their customers, leveraging Square\u2019s payment processing and hardware."
    },
    {
        "url": "https://medium.com/square-corner-blog/version-2-5-1-of-our-client-sdks-a1560b0f9b66",
        "title": "Version 2.5.1 of our Client SDKs \u2013 Square Corner Blog \u2013",
        "text": "We\u2019re always working to expand our APIs and client libraries to give developers the tools they need to build their integrations with Square businesses. Here\u2019s what\u2019s new in our most recent version (2.5.1):\n\nAll of the most recent SDKs are already available in GitHub, and you can update through your favorite package manager.\n\nIf you have any questions about our most recent SDK release, let us know in the comments, or reach out to @SquareDev on twitter. You can also see the official release notes below \u2b07\ufe0f."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-deep-dive-into-authorization-and-delayed-capture-24ce2b9fd5e",
        "title": "A deep dive into authorization and delayed capture.",
        "text": "Placing a hold on a card, or \u201cpre-auth\u201d-ing a transaction is very similar in practice as charging a card directly with the APIs.\n\nChoosing to delay capture on a transaction instead of directly charging is usually decided by the type of business you have. You could place a hold on someone\u2019s card while they borrow equipment or rent property. Other common use cases involve needing to confirm that customers have enough funds to purchase before they \u201ctest drive\u201d a product, or implementing room for tipping or optional gratuity in a purchase flow.\n\nThe only difference when making a charge is the parameter included with the request, like this:\n\nThen the status from the transaction will show instead of the usual , like this:\n\nIf you customer doesn\u2019t have enough funds or credit in their account, this transaction will fail.\n\nYou now have a charge that is Authorized, and you have six days to either capture the payment, or void it out.\n\nIf you decide that you want to capture the payment for your authorization, you\u2019ll need to use the CaptureTransaction endpoint:\n\nFrom the example transaction above we would use:\n\nThis endpoint doesn\u2019t provide a response so an empty json object means there was a successful capture. You can always retrieve the transaction later to get the transaction details.\n\nAn authorized transaction will automatically be voided after 6 days, but it is a best practice to void the payment as soon as you know you won\u2019t be capturing the funds so your customer can regain access to their money or credit.\n\nVoiding is very similar to capturing, with only a slight difference in the endpoint\u2019s URL.\n\nFrom the example transaction above we would use:\n\nThis endpoint also doesn\u2019t provide a response, so an empty json object means there was a successful voiding of the authorized transaction.\n\nYou\u2019re now set to authorize transactions for later capture with Square\u2019s eCommerce APIs. If you have any questions, let us know in the comments. Until next time!"
    },
    {
        "url": "https://medium.com/square-corner-blog/whats-new-in-version-2-5-0-of-our-client-sdks-2673076b99e8",
        "title": "What\u2019s new in version 2.5.0 of our client SDKs. \u2013 Square Corner Blog \u2013",
        "text": "Here\u2019s a rundown of the changes:\n\nYou can see the full diff of all the changes here:\n\nHopefully you enjoy the new features we\u2019ve added, and as always, if you find any mistakes or room for improvement leave us a comment here or submit a pull request!"
    },
    {
        "url": "https://medium.com/square-corner-blog/updating-from-factorygirl-to-factorybot-22e5a8c61a65",
        "title": "Updating from FactoryGirl to FactoryBot \u2013 Square Corner Blog \u2013",
        "text": "I first encountered FactoryGirl when I started at Square in June. The name felt somewhat odd to me, odd enough that I looked up why the authors picked this name. I wasn\u2019t alone in my quest: the project developers wrote this explanation in 2016 to clarify the name\u2019s origins. I shrugged it off as being lost in translation and moved on. Then again, I\u2019m a cis-het guy, and don\u2019t really need to think much about these things.\n\nIt bothered other people. Github user tastycode submitted an issue asking about the name, saying they think about renaming it to simply \u2018Factory\u2019. After a brief back-and-forth, during which tastycode suggested the new name, the project leaders agreed that it should be changed, and a few months later ThoughtBot developer Eraldius renamed the library to FactoryBot.\n\nUser maxkwallace provided this anecdote to help people understand why the name was problematic:\n\nThis episode speaks to the micro-aggression that women face all too often, sometimes daily: an insinuation that they are different, unusual, perhaps even unwelcome, and at the very least, guests in the man\u2019s world of software engineering. Although individual instances may appear benign, they add up over a lifetime and, over time, marginalize women. This also happens with people of color, LGBTQ people, and, indeed, nearly any group that isn\u2019t a \u201cstandard\u201d straight, white male. Perhaps it even impacts these very same white males."
    },
    {
        "url": "https://medium.com/square-corner-blog/createcheckout-options-explained-278a3093c28c",
        "title": "CreateCheckout options explained. \u2013 Square Corner Blog \u2013",
        "text": "We released Square Checkout as a way for developers to accept payments without having to deal with a card nonce or building a payment form user interface. In this post, we\u2019ll dive deep on the different options available to you when generating a checkout form.\n\nIf you want to see more tips like these, be sure to follow The Corner on Medium!"
    },
    {
        "url": "https://medium.com/square-corner-blog/so-you-have-some-clusters-now-what-abfd297a575b",
        "title": "So You Have Some Clusters, Now What? \u2013 Square Corner Blog \u2013",
        "text": "How to Add Value to Your Clusters\n\nOne of the most common ways to apply unsupervised learning to a dataset is clustering, specifically centroid-based clustering. Clustering takes a mass of observations and separates them into distinct groups based on similarities.\n\nFigure 1: Taking a 2 dimensional dataset and separating it into 3 distinct clusters\n\nFor those who\u2019ve written a clustering algorithm before, the concept of K-means and finding the optimal number of clusters using the Elbow method is likely familiar. The harder question to answer is \u2014 What does each cluster represent? We ran into precisely this problem when using clustering to understand our sellers\u2019 behaviors.\n\nWe started out with certain assumptions about how the data would cluster without specific predictions of how many distinct groups our sellers split into. The first instinct was to manually generate a set of signals that we knew to be interesting. For example, payment volume, payment amount, and business type are some of the most common dimensions that set our sellers apart, so we attempted to use them in our clustering analysis. The resulting clusters were most optimally separated along dimensions we already knew to be important and supported our initial understanding. Retail sellers are different than Food & Drink sellers, large businesses with multiple locations are different than small single-location businesses, etc. While valuable in confirming previous understanding and analyses \u2014 this didn\u2019t tell us anything new!\n\nWe didn\u2019t gain any new insights about how our sellers differ because we embedded our biases of which signals are important into our dataset. So how do we get around this?\n\nTo try to unlock more interesting insights, we proceeded to rebuild our clusters with a dataset containing less bias. Square\u2019s risk systems generate thousands of signals for each seller based on a variety of behavioral signals. This is essentially the kitchen-sink of data and we used this dataset instead of hand-picking signals, which introduced the aforementioned bias. There are some reservations with this method, of course, including the possibility that we swapped known biases for unknown ones. We hope to visit this specific topic in a future blog post.\n\nAt this point, we have our data, our set of features, defined a number of K clusters, and clustered sellers into our K clusters. Next, we try a few different approaches to assign \u2018profiles\u2019 to each of our clusters:\n\nOnce we construct the clusters, we can produce a list of all sellers and which cluster they belong to. We can then take a specific cluster and study the seller characteristics along known dimensions. Alternatively, we can look at which groupings over- or under-index on specific dimensions compared to other clusters. For both approaches, we come up with a list of dimensions that differ across clusters, then make guesses on what the clusters will represent.\n\nHere\u2019s an example of the second approach. In this example, we joined our primary key and cluster label to the features we are interested in profiling and plotting each cluster\u2019s average scaled value. The feature of interest is on the x-axis with the scaled average value of that feature on the y-axis. Visualizing the differences allows us to see that cluster zero is categorized by the first and last features. Meanwhile cluster three has the highest values for almost every feature profiled. From this we can start to translate nondescript cluster labels into meaningful ones explicable to an audience.\n\nFigure 2: Average \u2018score\u2019 of each cluster along 4 different dimensions\n\nDepending on the purpose of clustering the data, the feature set to profile clusters will change. When using clustering to understand how different sellers use a Square product, the dimensions are specific to usage data. When wanting to determine those sellers\u2019 value to Square the feature set was comprised of broad variables related to revenue, seller size, LTV, etc.\n\nWe can figure out which seller from each cluster is the closest to the centroid (center point) and mark that seller as the most \u2018representative\u2019 seller for that cluster. We can then study the sellers along known dimensions or signals to come up with our best guess of the cluster profile as with the empirical approach. An alternative approach is to study the hypothetical centroid instead of the most representative element. For both approaches, we are making assumptions about how representation works within our groups, so which is the better approach of the two is strictly situational. Of course, if the variance within clusters are large, then neither the hypothetical nor empirical centroid would feel very representative of the group.\n\nHere\u2019s an example of this approach. In the graph below, the star in the middle of each cluster indicates the most representative member of the cluster.\n\nFigure 3: Stars identify the centroid of each cluster\n\nOnce the clusters are created, we can isolate the most influential features that vary the most across the different clusters, and make an educated guess about the cluster\u2019s profile. This is a more sophisticated version of determining how clusters are over or under-indexing.\n\nOne way to tease apart the influential features this is with a Random Forest Classifier. For example, if we have 4 clusters, we can use our existing signals to predict the probability of everyone belonging to the first cluster, then the second, etc. The result would be K models to match K clusters, one model per cluster, and predicting whether an instance is likely to belong to each cluster. Most standard implementations of the Random Forest algorithm will show you which features are important for each particular prediction, which can inspire more intuitions on what these important features says about the cluster itself.\n\nAnother way to arrive at feature influence is to use a single multiclass classification model (sklearn.multiclass) with your response as the column \u2018Cluster\u2019 with values of 1,\u2026,K. This provides the benefit of a singular model that can be used to predict an instance\u2019s cluster using additional signals outside the K-means dataset. For example, in the application of product usage, you could profile users into clusters without them having used the product yet to segment your addressable market more specifically. We hope to expand on this technique in a future blog post.\n\nOnce we determine which dimensions mattered in separating our clusters, we are likely to find one of two things: either the top signals are not aligned with what we previously believed to be important, or that they are.\n\nIf our result is the former, then great! You can gain insights about what new behaviors your observations diverge on and how they\u2019re different than your existing assumptions. Ideally, these divergences are significant and actionable.\n\nMore often than not, however, the top signals that the clusters split on are already known and expected. This at the very least validates that existing segmentations are meaningful, but we can learn more.\n\nGiven that your data is most optimally separated by your top signal, what if you cluster again within these clusters? This would then reveal a second set of signals that are important, controlling for the first set. We did exactly that when we discovered that our first clusters were separated into our existing segmentation of business size. We added an additional list of signals that weren\u2019t present in our first dataset and a list of insights that clued us into how our sellers differ at various business sizes emerged. This set of insights informed strategy across marketing and product teams, and provided a personalized approach to product development and seller interactions.\n\nWhen it comes to clustering, you are rarely done after you pick the optimal number of clusters and run the algorithm. There is a lot of value in the interpretability of your clusters and the additional understanding you gain of your data."
    },
    {
        "url": "https://medium.com/square-corner-blog/adding-card-on-file-details-with-python-4bc827dbf62",
        "title": "Adding card on file details with Python \u2013 Square Corner Blog \u2013",
        "text": "In this post we\u2019ll be using the endpoints to add a card on file to a customer that you could then use for subscription billing online or in-person. But remember, you should not link a card on file to a customer without the customer\u2019s express permission. For example, you can include a checkbox in your purchase flow (unchecked by default) that the customer can check to specify that they wish to save their card information for future purchases.\n\nTo get started you\u2019ll need a Square account, an application in the Square developer portal, and of course, at least one customer to attach the card to. You can create your Square account at https://squareup.com/developers, your developer application at https://connect.squareup.com/apps and to create your customer you can either use the Square Point of Sale app or use the APIs to create a customer in your customer directory.\n\nAdding a card on file is very similar to charging a card for an e-commerce transaction. You\u2019ll have to generate a card nonce using the and then pass it to the CreateCustomerCard endpoint. Since the payment form will have to be embedded in a different client-facing page, and the REST request to attach the customer card will be hidden away on the server\u2019s side, this example will just use a hardcoded testing nonce. Let\u2019s look at the code:\n\nHere you can see the code first initializes the Square Python SDK and imports a couple of useful tidbits, such as the CustomersAPI and the model for creating the customer card request. Then you\u2019ll need to set the access token. I recommend using a sandbox access token so that you can use the fake card nonce also hard coded in this example.\n\nWhen it comes to actually creating the , the code uses the model that was imported above, and attaches the card nonce, as well as made up . You also have the option to add a but it is not required.\n\nYou can run this code snippet directly from your command line with a command link (or whatever you decided to name the file). If everything goes well, you should see something like the below as your customer card details are printed on the screen.\n\nNow you can use the card to make card-on-file purchases in person or online to meet your billing needs!\n\nThanks for reading. If you have any questions, comments, concerns, or excitements, feel free to comment on this post below, or reach out to us on Twitter."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-new-way-for-developers-to-monetize-on-squares-platform-5186a8109b9e",
        "title": "A new way for developers to monetize on Square\u2019s platform",
        "text": "Following our announcement in September that we\u2019re working with Eventbrite to expand our offering for marketplaces, we\u2019re excited to share that now marketplaces and platforms using our eCommerce API or Square Checkout can pay out their merchants and get paid themselves.\n\nIn the last few months we\u2019ve worked closely with several developers and partners including Weebly and Schedulicity to test, refine, and provide feedback about the product. Using Square Multi-Party transactions, Weebly was able to expand Square\u2019s offering to all of its plans:\n\nOn the other hand, Schedulicity uses the product to solve complex payments problems:\n\nSimilarly, developers building solutions on these APIs now have a way to monetize by taking a portion of the transactions they process. Square supports a few methods for developers to do this:\n\nFlat fee: A product or service costs $100, plus a $10 fee. In this case the platform will charge the buyer $110 while using the additional recipient object to direct $10 to themselves:\n\nPercent-based fee: A $100 product or service with a 10% fee. You must calculate the amount of the percent explicitly, and send it in the request. We do this to prevent any ambiguity around rounding if we calculated the % for you. In this case the platform will charge the buyer $111.11 while using the additional recipient object to direct $11.11 (10%) to themselves:\n\nTransaction-based fee: A $100 product or service with a 10% fee. In this case the platform will charge the buyer $100 while using the additional recipient object to direct $10 to themselves:\n\nIn all of these cases Square will continue to take its standard processing fees (Square\u2019s fees vary according to geographic locations) before the payment is split between parties.\n\nAt Square, we\u2019re always working to empower more sellers and developers with access to the economy and financial tools they need. We\u2019re excited to take steps to help developers succeed financially on our platform.\n\nThis feature is available for developers building solutions for merchants based in Canada, the United Kingdom, and the United States.\n\nWe are also introducing a set of endpoints to enable platforms to report on the funds they received. These endpoints are:\n\nSquare was founded on simple and transparent pricing. We clearly present the additional recipient funds taken from a given merchant on the Square seller dashboard.\n\nAs you can see the transaction window will include a new \u201cPlatform \u2014 transaction-based fee\u201d line item indicating the funds the additional recipient took (10% in this case). As a developer you can customize the message a merchant will see on dashboard by using the description field in the additional recipient object.\n\nTo get started, go the Square\u2019s Developer Portal to create an account (if you don\u2019t already have one) and get your API credentials, ask your sellers to approve an additional oAuth permission PAYMENTS_WRITE_ADDITIONAL_RECIPIENTS, choose which above scenario is most relevant for you, and add the additional recipient object to your Charge request.\n\nThis is one step to better support our developer community with the means to monetize the solutions they build on top of the Square\u2019s ecosystem."
    },
    {
        "url": "https://medium.com/square-corner-blog/how-to-add-masterpass-support-to-your-site-through-squares-ecommerce-api-284ffd28b44e",
        "title": "How to add Masterpass support to your site through Square\u2019s E-commerce API",
        "text": "As part of our continuous work to make it easier for our developers to support the growing range of online payment methods, we are happy to announce that now Square\u2019s E-commerce API will support Masterpass for our U.S. sellers. By doing this we guarantee that our sellers will be able to close their sales with every method buyers are interested in paying with.\n\nMasterpass is Mastercard\u2019s digital wallet, which lets buyers checkout easily, quickly, and securely from your site using all major card brands.\n\nTo enable Masterpass, simply follow the three steps below:\n\nStep 1 - Add your Location ID to the SqPaymentForm. (Note: you can find your Location ID by logging into your developer portal):\n\nStep 2 - Similar to the other wallets we support, add the following in the callback section:\n\nBy adding Masterpass support for our developers and the sellers they serve, buyers can pay using the payment method they prefer. To learn more about what you can do with Square\u2019s E-commerce API, see our documentation on embedding a payment form."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-fresh-update-to-our-platform-docs-c2ea2dc12623",
        "title": "A Fresh Update to our Platform Docs \u2013 Square Corner Blog \u2013",
        "text": "Thoroughness, accuracy, and consistency are important tenets for any platform\u2019s documentation. At Square we take our technical writing seriously and strive to ensure our documentation meets these tenets for our developers. But we also challenge ourselves to go further and ensure developers can easily understand how the different components of our API can fit together.\n\nWe recently launched an update to our developer platform documentation. As we\u2019ve talked with developers building apps and integrations with Square, we\u2019ve learned that we could better help them quickly understand and identify what APIs and technologies will best fulfill their development goals.\n\nThe new design clarifies how our APIs work together to provide a complete commerce platform for developers and the sellers they serve. We\u2019ve built overview pages for core API categories, including taking payments, managing products, driving growth, and running business to make it easier for developers to create purpose-built solutions for specific tasks. These category pages provide a quick summary of which APIs are most relevant for specific use-cases, along with at-a-glance tables comparing API functionality.\n\nWe hope this update will help our developers find the right APIs for their needs and help them solve problems more easily. We can\u2019t wait to see new apps and integrations built with Square and we\u2019d love to hear what you think! Please send any feedback to developerfeedback@squareup.com."
    },
    {
        "url": "https://medium.com/square-corner-blog/creating-an-employee-with-python-e810b4a2eec1",
        "title": "Creating an employee with Python \u2013 Square Corner Blog \u2013",
        "text": "In this post we\u2019ll go through the steps to create a new employee record attached to your Square business. If only actually hiring employees was as easy!\n\nYou\u2019ll likely use this employee creation script as a part of some import or syncing script, but for this example we\u2019ll make a standalone snippet that creates an employee.\n\nLet\u2019s take a look at each part of the process:\n\nFirst we the squareconnect package; if you don\u2019t have it installed already, you can use or install it directly from github This package gives you access to some helper methods and models that we import in the next couple lines that make calling the APIs (and the associated HTTP requests) a little easier.\n\nNext up, you need to specify the access token that you are using to access Square\u2019s APIs. This not only tells Square which account to do the operations on, but also makes sure that you have the appropriate permissions. You can get this token from the Square Developer Portal, and it should always be kept secret.\n\nNow that our setup is done we can create an instance of our API and a new instance of our employee model. Then you can just assign values to the different properties to fill in the information about the employee. Here I\u2019ve added a first name, last name, and an email to the employee.\n\nFinally we can call the API and print out the result. If everything went well, you should see something like this:\n\nThat is it! If you try this out and have any problems, or have any comments, let us know on Twitter or just by commenting on this post!"
    },
    {
        "url": "https://medium.com/square-corner-blog/whats-new-in-version-2-4-1-of-our-sdks-96140a97b466",
        "title": "What\u2019s new in version 2.4.1 of our SDKs \u2013 Square Corner Blog \u2013",
        "text": "We\u2019ve just released the latest iteration of our client libraries. Here\u2019s what\u2019s new from the 2.3.0 release:\n\nIf you are interested in diving into the code changes for all of the SDKs, take a look at the full diff on GitHub. If you notice any problems, or have any comments, let us know on Twitter!"
    },
    {
        "url": "https://medium.com/square-corner-blog/creating-a-customer-in-php-ac2eb35afe3a",
        "title": "Creating a Customer in PHP \u2013 Square Corner Blog \u2013",
        "text": "You can\u2019t have a business if you don\u2019t have any customers, but importing customers from an existing system can be a pain, and sometimes you need to be able to do so programmatically. In PHP, our PHP SDK makes it fairly easy to create these customers using one simple script:\n\nThe code starts out by loading the PHP SDK that was installed with Composer and then setting the access token for the account. Then you can create a new instance of the and a new object that will hold all of the customer information that you are trying to upload. Finally, you call the method with your customer object and voil\u00e0, you have a brand new customer in your account.\n\nI hope you liked this post, let me know what you\u2019d like the next one to be about by reaching out to @SquareDev on twitter."
    },
    {
        "url": "https://medium.com/square-corner-blog/product-planning-for-machine-learning-3714e875d967",
        "title": "Product planning for machine learning \u2013 Square Corner Blog \u2013",
        "text": "When it comes to applied machine learning, the algorithm is often what captures the popular imagination. Most machine learning articles highlight the method to derive the model rather than the operating conditions needed to use the model successfully. However, the model is just one piece of a deeper system. In a company context, an algorithm\u2019s leverage comes from being a part of large-scale running processes that ultimately serve a customer need. Moving an ML-based solution from a personal development environment into an production system introduces additional factors, ones that organizations often overlook in the rush to capitalize on ML.\n\nWe recognize that ML is a versatile tool. But, like any tool, its effectiveness is tied to the operational environment and problem context. This blog post covers the factors that product and process managers should know in order to plan for ML-based solutions well.\n\nAn org can use ML in different parts of product strategy. Some applications of ML are more strategic and involve applying statistical methods to uncover insights and make product- and corporate-level business recommendations. Examples include identifying bottlenecks in the customer acquisition funnel, or finding statistical clusters that may lead to better segmentation and targeting.\n\nOther applications of ML are tactical in nature and involve using ML to automate or optimize some process. A few examples of this second category of ML applications are fraud detection, marketing campaign optimization, product recommendation, document translation, driving a car, and many others.\n\nWhat these examples have in common is that they tend to involve automated decision-making. Automation allow you to do the same amount of work with fewer people, or free up people and other expensive assets to do more valuable projects. A supporter can answer the same issue again and again, but it\u2019s even better if their attention is freed up so they have the capacity to notice emerging inquiry patterns, and share those customer insights with product managers to guide innovation and improve customer experience.\n\nThis remainder of the blog post focuses on the set of applications of ML within product strategy that correspond to optimization and automation.\n\nIt\u2019s important to distinguish between automation and ML. The two are not strictly synonymous, though ML is supportive of automation. Automation can be achieved through identifying optimal business rules, but nowadays, ML is often a more accurate and more efficient way of doing intelligent automation. ML allows an organization make predictions directly from data and use those predictions to automate some sort of behavior.\n\nHow is an ML-based approach different from traditional automation?\n\nTraditional automation: Business process management and rule-based systems. Rules are determined by business experts and analysts. Rules are static but can be revised.\n\nAutomation with machine learning: \u2018Rules\u2019 use optimized functions learned from data. \u2018Rules\u2019 are flexible and can adapt to new behavioral patterns.\n\nAutomation with ML still requires embedding the ML engine in some way into the decision engine. This can be thought of in two ways: ML acting within the rules engine, or ML as an input to the rules engine. Either way, building a sophisticated rules engine that uses ML to route decisions requires coordination among data scientists and engineers.\n\nWhat needs to be built for production. In its mature state, this scoring/decision system will be able to automatically make a decision given a new observation by (1) taking in that observation\u2019s metadata, (2) identifying the appropriate model and attributes of that observation to evaluate, and (3) applying decision thresholds against predefined evaluation criteria.\n\nThis usually takes the form of some call-out to a scoring API from the decision engine in production. Of course, this assumes the existence of and communication among several working systems, such as the decision engine, model scoring application, live data feed or queryable data store.\n\nSometimes, when managers talk about planning for ML, they focus on the steady-state, what the ML environment will look like in production. Since assumptions about the existence of data that\u2019s necessary to train models are not always satisfied, the discussion about the steady-state often comes prematurely in product planning.\n\nThis isn\u2019t to say that we shouldn\u2019t think about these things, but laying out the roadmap to getting there is crucial. The first stage of planning for an ML-based solution is to ensure that the model development process is feasible. Here are some distinctions in the kind of language and topics related to production- vs. development-specific planning.\n\nThese distinctions between the way we tend to talk about production vs. development provides a segue into data acquisition and feedback loop. Most PMs know already that ML depends on the availability of data, but it\u2019s also important to understand that how ML models are trained influences how we plan for future-state solutions. In order to train a model, you not only need an ability to ingest data live, but you also need a large set of observations called training data. This data used to calibrate and optimize the model to perform the function it is intended to do.\n\nML is a way to generate predictions by using data (aka. observations, examples) to learn the optimal function that relates inputs to outputs. In operational applications, the learned function is used to produce a score that can then be used to automate decisions and/or processes. Here are some examples related to things we work on at Square.\n\nTask: You want to automate a process by creating a function that yields some output for new input data.\n\n- Example: A function that calculates a risk score for a seller, customer, &c.\n\n- Example: A function that predicts if a seller will accept a marketing offer.\n\n- Example: A function that predicts if a seller will process a fraudulent chargeback.\n\n- Example: A function that predicts if a seller is high installment payment risk.\n\nSolution: Use machine learning to generate that function!\n\nHowever, just assuming that ML can be \u201cthrown\u201d at a problem to solve it is a misunderstanding of how ML works. There are some key assumptions that need to be satisfied before a problem can be approached using ML methods.\n\nIn simpler terms, the data contains the inputs and the outputs of the function you\u2019re trying to approximate.\n\nThe assumption that you have labeled data is a big one. This is not always going to be the case. When planning for the incorporation of ML into a product, you need to know what the cost and time of data acquisition for the development work is going to be. Typically, there are tradeoffs between the cost/time of data acquisition and the quality/certainty of the data.\n\nIf you are starting to use ML from scratch, it\u2019s very rare for there to be data that\u2019s an exact match from the problem at hand. Some data assets are more directly relevant than others. However, an organization typically stores historical data for some purpose other than ML, and has a queryable backlog of events of different types with relevant associated metadata. This may be the ingredients for the relevant data If you are lucky.\n\nIn some cases, there\u2019s no such internal data (at least yet). Here, it pays to know the the relative value and cost of internal and external data acquisition strategies. You need to estimate costs and benefits of integrating various internal & external data sets into your ML solution.\n\nUsually, data storage and access systems are built with some of the lower sophistication use cases in mind, such as describing and diagnosing what has happened. When designing these systems, it\u2019s important to plan for applications of ML down the road, which is concerned with predicting what will happen and what to do about it. This involves the design of what is collected (via the product) as much as it involves how to store it (design of the database). If you are not logging and storing potentially useful data now, it\u2019s going to be a pain to implement ML later.\n\nTherefore, data collection and storage is important. It is also important to start considering how that data will be stored efficiently and how it can be retrieved by model-hosting systems that require access to it when scoring. Related to these are concerns about the latency, fidelity, consistency, and immutability of the data.\n\nDepending of the maturity of your product/organization in using ML you may or may not have the necessary data for training a model. There are 3 data basic states you can be in with regard to supervised learning models.\n\nLet\u2019s consider the most tricky case, when you have little or no data.\n\nOne way of generating data on the fly is actually to launch a product relatively blindly.\n\nSince you need to generate positive and negative outcomes to create training data for future modeling, and no such data already exist, one way to gather information and start progress is to simply launch the product and observe outcomes. By doing this, you are in effect buying the information you need using trial and error. For example, lenders seeking to serve a new customer base needs to recalibrated their risk models for the new population. They may start by offering loans using common sense heuristics, and incur short term losses. They do this knowing that they are buying valuable information, and these losses will eventually be offset by the longer term gains of being the first to build up the relevant data to successfully serve this unknown market.\n\nIf you are in a small or no data situation, ask yourself the following:\n\nOne high value application of ML is the automation of routine processes. ML can enable automation by generating scores, which feed into rule-based decision management systems. However, the success of ML-based solution depends on having the right operational environment. It is crucial for decision makers to understand and plan for the production and development environments needed for your ML solution to function. It is hard to overstate the importance of the development environment, which begins with collecting, storing, and labeling data. Being a mature company with good data does not guarantee that you have the data needed to solve the problems or automate the processes at hand. The planning roadmap for data acquisition proposed here applies just as much to companies getting off the ground as it does to mature companies developing new products and entering new markets."
    },
    {
        "url": "https://medium.com/square-corner-blog/check-out-the-new-example-in-the-sandbox-section-of-squares-developer-docs-1644a5d11e07",
        "title": "Check out the new example in the sandbox section of Square\u2019s developer docs",
        "text": "From one point of view, applications built on top of Square\u2019s APIs are just a series of API requests and code to display, store, or manipulate the data that gets returned. Wouldn\u2019t it be interesting to see app in its simplest form \u2014 as an interactive webpage that you could use to try out your API requests? Here it is :)\n\nLike our APIs, to use the new tool you will need to create a Square Account and to create an application in the Square Developer Portal. Once that\u2019s done, you can copy your sandbox access token and paste it in the appropriate place in the tool. From there you can select from a dropdown which API call you\u2019d like to make and see an example JSON body (if the request requires one). This makes it super simple to see what goes on with the API without having to touch the code yourself.\n\nTry it out in the link below \u2b07\ufe0f\n\nIf you have any questions, feedback, or notice something not right, feel free to leave a comment on this post, or reach out to us on Twitter."
    },
    {
        "url": "https://medium.com/square-corner-blog/listing-your-locations-with-python-fe1a74dcf303",
        "title": "Listing your locations with Python \u2013 Square Corner Blog \u2013",
        "text": "Listing your locations programmatically can be important for managing applications for large companies or even multiple companies. Here is how you can get a list of your locations using our Python SDK.\n\nThe first thing you\u2019ll need to do is make sure that you have python working and install Square\u2019s Python SDK. You can find the installation instructions here: https://github.com/square/connect-python-sdk#installation--usage\n\nYou\u2019ll have to replace the with your access token from your Square Developer Portal. but then you should be able to save the above code to a file and then run the file with:\n\nThen, your locations should print on on this screen:\n\nHope you find this post on listing locations useful! Remember that you can now also get your location ID\u2019s from the Developer Portal."
    },
    {
        "url": "https://medium.com/square-corner-blog/see-your-location-id-without-the-api-call-90f49a661ab7",
        "title": "See your location ID without the API call \u2013 Square Corner Blog \u2013",
        "text": "When merchants sign up for Square they create what we called a \u201clocation.\u201d This location represents a distinct place that they do business. It could be a physical store, an e-commerce site, or even a mobile location like a food truck. These locations can change over time, as the business grows and expands or moves into new domains.\n\nFor developers, locations are particularly important, since they determine where the transactions you are charging get attributed to, or which location\u2019s items are are being updated in your inventory. Often times you\u2019d use the List Locations endpoint to find the location ID that you want to perform the operations on, and then spend the rest of your time with a single location hard coded into your software.\n\nNow instead of having to make a one-off List Locations call, you can just pull the location ID from your Developer Portal!\n\nThis is a change that is available for all developers in the Square Developer Portal. You can log in today and select an application to see for yourself. Learn more about a Square business in the developer document Structure of a Square Business."
    },
    {
        "url": "https://medium.com/square-corner-blog/keeping-the-daggers-sharp-%EF%B8%8F-230b3191c3f",
        "title": "Keeping the Daggers Sharp \u2694\ufe0f \u2013 Square Corner Blog \u2013",
        "text": "Dagger 2 is a great dependency injection library, but its sharp edges can be tricky to handle. Let\u2019s go over a few best practices that Square follows to keep mobile engineers from hurting themselves!\n\nSeveral Square teams are using it to detect common Dagger mistakes, check it out.\n\nThese guiding principles work well for our context: small heterogeneous teams working on a large shared Android codebase. Since your context is likely different, you should apply what makes the most sense for your team."
    },
    {
        "url": "https://medium.com/square-corner-blog/creating-a-customer-with-ruby-48d15570d98f",
        "title": "Creating a customer with Ruby \u2013 Square Corner Blog \u2013",
        "text": "Customer management via API can be incredibly useful when syncing with other systems, automating marketing tasks, and more. If your Customer Directory looks empty and you want to add to it, take a look at the following ruby script:\n\nThis script is fairly simple because Square\u2019s Ruby SDK does most of the heavy lifting through authentication and parsing the data returned from the API. You can even run it easily from the command line with . With a few modifications to the script you could loop through customer creation to import your customers from a different database or even a backup file that you have created in the past.\n\nThat\u2019s it! If you have any questions or comments, feel free to reach out to us at @SquareDev on twitter or look at the official documentation."
    },
    {
        "url": "https://medium.com/square-corner-blog/using-neural-networks-to-predict-customers-needs-63e5f2715e40",
        "title": "Using neural networks to predict customers\u2019 needs \u2013 Square Corner Blog \u2013",
        "text": "Square provides a powerful analytics tool called Seller Dashboard that gives our sellers insights into, and tracking of, their businesses. Using this tool, Square sellers can view their daily sales summary, manage inventory, confirm deposits, and do much more to untangle complicated tasks they face every day.\n\nThe primary goal of the Seller Dashboard team here at Square is to promptly address our sellers\u2019 problems when they arise, while also continuing to enhance and build new features for this product. In order to do this well, we first have to understand whether Seller Dashboard is being used effectively by our sellers.\n\nBecause every seller uses their dashboard differently, providing support for all sellers\u2019 specific problems can be time-consuming and resource-intensive for both sellers and Square. So we looked into automating custom, individualized solutions for each seller.\n\nAs sellers click through and view Square\u2019s web pages, Square analyzes this behavior. This data includes important cues that allow us to understand how sellers use their dashboard. Given the vast amount of data involved, the biggest challenge was compressing and aggregating this data in a way that could provide important insights for the business.\n\nWe looked into various supervised and unsupervised machine learning techniques in order to analyze the web page flow data. Traditional analytic techniques, including n-gram analysis and building Markov chains, provided valuable insights about which pages are most heavily used and which are closely linked together. What turned out to be most fruitful in learning the context of this complicated customer behavior, however, was a neural network model that could detect usage patterns indicative of sellers experiencing a problem.\n\nArtificial neural networks, mimicking a behavior of biological neurons, have been a powerful tool for pattern recognition, and have been utilized in recent years with improved performance over traditional statistical learning tools. Feed-forward neural networks perform well for computer vision and speech recognition by picking up various spatial features of data, while recurrent neural networks are powerful tools that allow learning dependencies in sequential data, such as text and music.\n\nI was inspired by Fran\u00e7ois Chollet, a primary author of one of the most popular open-source neural network library called Keras, using a subset of recurrent neural networks, LSTM (Long-Short Term Memory) to classify IMDb movie reviews*. In this research example, the sentiment of a review could be extracted with high precision from the user-inputted free form text.\n\nThe problem at hand was similar in that we wanted a binary classifier that could detect signals from a series. Think of each page in the website as a token of words, and whether someone needs help as positive or negative outcome, and we have the exact same problem the aforementioned research strives to solve. There were some final hurdles to jump through, including the fact that we had an imbalanced dataset, as only a small fraction of the population requiring help while using our product. This was mitigated by exploring traditional resampling options, such as undersampling the majority class or oversampling the minority class, to create a more balanced sample before training our model.\n\nPredicting human behavior is a rather difficult problem to solve. This is because everyone reacts differently, even if they look like they are performing the same set of behaviors. For example, some people actively seek help through support documents; others reach out to colleagues, or find other ways to resolve their problems. Given this fact, we were able to detect signals for support with a high recall rate of 66%, while keeping our fall-out rates at around 24%. This is a huge improvement on 5% recall we would otherwise get with random guesses. It also significantly outperforms Markov chain models and heuristic approaches.\n\nThe ability to correctly identify signals from the page flow is highly valuable outside the scope of this analysis. Other than promptly helping other sellers with better support, this methodology could be used to collect additional signals and strengthen Square\u2019s focus on automation to enhance customer experience."
    },
    {
        "url": "https://medium.com/square-corner-blog/building-a-successful-business-by-helping-merchants-easily-integrate-online-forms-into-their-e-d7f56a21e168",
        "title": "Helping merchants easily integrate online forms into their e-commerce site",
        "text": "A drag and drop form builder that was designed to let anyone \u2014 skilled developers to technophobes \u2014 create online forms in a few clicks.\n\nJotform started as a way for anyone to publish online forms that could take contact info and online applications. After building a strong base, JotForm realized that Square was a popular payment platform for their existing customers.\n\n\u201cMany JotForm users had an easy way of collecting payments in person with Square, but needed a way to process Square payments online, too,\u201d says Neil Acero, Integration Developer.\n\nAdding an integration with Square has made JotForm a super compelling option for merchants of all sizes. It enables them to collect customer information and payments simultaneously, turning JotForm into a powerful tool for processing orders, event registrations, and donations.\n\nAs with many of JotForm\u2019s important decisions, the decision to integrate with Square started with a request from a JotForm user \u2014 a request they heard quite a bit. Since it\u2019s really simple to create a payment form using JotForm, it made a ton of sense to integrate with Square where the online payment collection part would be just as seamless as collecting in-person payments.\n\n\u201cOur customers have been asking for us to integrate with Square even before their e-Commerce API was released,\u201d says Neil Acero, JotForm\u2019s Integration Developer.\n\nMotivated by how popular Square is as a way for business owners to get paid, JotForm started working on the integration as soon as they heard that it was possible. Jotform wanted to be the first form builder to integrate with Square because they felt it would add a lot of value for anyone using Jotform, and it has.\n\nTo make the integration possible, JotForm uses Square Payment Form for the front-end, utilizing both Square Connect v1 and v2. The integration connects to the 4 endpoints: Transactions, Locations, Payments (v1), OAuth2.\n\nTheir implementation is quite straightforward:\n\nThey also run a scheduled task that finds expired tokens and attempts to renew them to minimize the need for customers to renew their tokens mid-transaction.\n\nJotForm\u2019s experience with Square\u2019s APIs has been pretty seamless so far. They were able to test the API and make mock payments very quickly. The \u2018Getting Started\u2019 pages and the API references made writing the implementation a breeze. Square\u2019s developer support was also very responsive and was able to answer questions accurately and promptly.\n\n\u201cIt has been amazing thus far. Documentation is top-notch!\u201d says Neil Acero.\n\nHow Square has impacted this developer\u2019s business and what is in store for the future\n\nThanks to the integration with Square, JotForm is able to tap into a huge market of small businesses who want to use the same payment processor for online and in-person payments. The integration with Square has brought many new businesses their way and is a strong selling point for JotForm. JotForm was so successful using the Square platform that our team worked with them to get published on the Square App Marketplace, which now offers them a new powerful channel to share their solution with the millions of businesses using Square.\n\nJotForm is a very dynamic company eager to innovate and that constantly brings new features to its users. In the spring of 2017, they launched JotForm 4.0, which was a reimagined version of their traditional form builder, allowing users to create forms using any device. It also allows users to work on forms collaboratively with members of their team in real time.\n\nWhat they will unveil next is a new style of forms that really focuses on the experience of the person filling out the form, not just the form creator. This could mean higher conversion rates, which means more revenue for their users selling products using a JotForm/Square-powered order form.\n\n\u201cThe growth we have seen with Square has been pretty steady. Square has been one of the fastest growing integrations we have and the feedback we have received for offering Square have been great so far,\u201d says Neil Acero, Integration Developer.\n\nFor more information about Square\u2019s APIs and what they enable you to build, check out the documentation on our developer portal."
    },
    {
        "url": "https://medium.com/square-corner-blog/lets-encrypt-app-engine-pt-2-832583ef6ce4",
        "title": "Let\u2019s Encrypt & App Engine, pt. 2 \u2013 Square Corner Blog \u2013",
        "text": "For more background on using Let\u2019s Encrypt with Google\u2019s App Engine, read the original post Setting up https for your website\n\nWhen it came time to renew my Let\u2019s Encrypt certificates for Google\u2019s App Engine, I wanted to see whether I could improve the process. For my first attempt, I tried out the DNS verification method. I updated the appropriate records for my domain, but either wasn\u2019t able to do it correctly, or wasn\u2019t patient enough for the caches to refresh fully, so I went back to good ol\u2019 https verification instead. This ended up being a good choice when I decided to add more automation to the process, and now you can benefit my from my work \ud83d\ude42.\n\nIf you have several different projects on the same machine, the first thing you might want to do is get a handle on creating a separation of concerns for each project. An easy start is adding a folder to your project to help maintain all the relevant items for each.\n\nEach folder can be used as a way to check in different aspects of the Let\u2019s Encrypt process into my repository. We also want to start associating our projects with a configuration file, instead of remembering all of the different flags and commands needed. Initially my configuration file looked like this:\n\nNow we can run and go straight to the step to upload the challenge responses to my server. Unfortunately, all that work doesn\u2019t quite make the process of renewing certificates any easier just makes the command a little shorter. Luckily certbot has some new tricks up its sleeves for that.\n\nCertbot has a couple new features that help automate the certificate request process on a machine different than the one serving your website. Those features are the and flags. They allow you to specify shell scripts that will be run with pertinent details included as environment variables. You can read all about the Certbot hooks in the official user guide.\n\nBecause we\u2019re set on using a config file for everything let\u2019s add a couple lines for my new pre & post validation hooks.\n\nEach of those scripts is pretty simple. The auth script will add the appropriate file to serve to meet the url challenge (& deploy my application), and the cleanup script will delete the files that get made. Let\u2019s look at first:\n\nThe first line just creates a little output that tells me where the file is being created, and the second line places the content of the environment variable into a file named after the environment variable. Then deploy the App Engine application specifying the project. The flag prevents the need for any confirmations during the process for maximum automation.\n\nIt is easy to meet the URL challenges if you have the right file handler set in your for the project:\n\nFor any request that gets sent under yourdomain/.well-known/acme-challenge, App Engine will look for an appropriately named text file and return the contents.\n\nAfter the validation, this script just cleans up the files only so that they don\u2019t end up crufting up your repo.\n\nOnce successful, you\u2019ll get a certificate in a file. You\u2019ll need to add that to your certificates page for App Engine For the you\u2019ll first have to modify the private key file. You likely have into an RSA private key which is pretty simple with openssl:\n\nNow you can upload your new and patently wait another three months until you need to renew again. \ud83d\ude09\n\nI hope you enjoyed reading the second part of my App Engine SSL adventures with Let\u2019s Encrypt! Let me know if you have any thoughts, or suggestions for improvements in the comments."
    },
    {
        "url": "https://medium.com/square-corner-blog/tips-from-a-ghc-vet-ef6b3d26cc9b",
        "title": "Tips From a GHC Vet \u2013 Square Corner Blog \u2013",
        "text": "This will be my sixth year attending Grace Hopper. I have seen it grow from a small conference to over 10,000 employees. Square recently hosted a kick-off event where I talked through what I learned over the past couple of years. This post is meant to provide some tips and tricks for getting the most out of your experience.\n\nUpload your resume to the database.\n\nThe database is already open, but if you plan on talking to companies at the career expo, I highly suggest uploading your resume to the database. A lot of large companies will have iPads to sign in at their booth before talking with someone and they will use that information to follow up with you. Recruiters will also sort through the database before and after the conference to see if there is anyone that they are interested in contacting.\n\nHave your elevator pitch rehearsed and ready.\n\nThis is good for all networking, but really comes in handy at GHC. It takes practice to nail down, but it will eventually become second nature. A good elevator pitch should convey to the other person: who you are, what you do, and where you work. It\u2019s not just your quick introduction \u2014 it also showcases your confidence in what you do.\n\nPick up your packet on Tuesday before the conference or extremely early on Wednesday.\n\nWednesday is when the madness starts; lines will be long and people will be rushing to get to the keynote at 9am. Lines on Tuesday, however, are nonexistent. Make sure you get your packet early!\n\nHave a game plan for each day.\n\nGo through the GHC session schedule and figure out which presentations, panels, or workshops you are interested in, and plot them out. The schedule is already posted on the GHC website so why not start now? Also, be sure to look into where the sessions are held. If the sessions are close together, then the commute between the two rooms will be easier since you\u2019re not fighting through large crowds in the hallways and on escalators. A shorter commute means you will most likely arrive early and get a seat in the session.\n\nComfy shoes that you can stand wearing all day are a must. Other than that, most people are casual, wearing a t-shirt representing their company. A pair of sunglasses for your walk or shuttle ride might help if it\u2019s sunny. Most importantly, bring a sweatshirt to stay warm! Even though the conference is in Orlando this year, the AC in the conference center will be on full blast. I forgot a sweatshirt one year and ended up cuddling with my laptop for warmth.\n\nLaptop/charger: The conference can be a whirlwind, and taking notes during sessions can help you track the key takeaways that resonate with you. If you\u2019re a faster typer on your laptop, bring your computer and charger. Keeping notes on your computer also helps you reference notes from previous years and events.\n\nPortable battery pack: Ever been in a venue with 15,000 people where every person has a phone? You can\u2019t get service \u2014 at least, not good service. When your phone is struggling, the battery will drain faster than normal. A portable charger won\u2019t keep you tied to a wall all afternoon.\n\nYour contact information: You need a way to hand out your info. If that\u2019s business cards, bring them; just be aware they may get lost. A QR code pointing to your LinkedIn page: have that set up. You\u2019ll meet a lot of people and you\u2019ll need a fast, easy way to get each other\u2019s information. Students, if you don\u2019t have a LinkedIn profile, make one now!!!! I can\u2019t stress this enough. LinkedIn is your online resume. You\u2019ll need one after graduation, anyway, so just get on it now!\n\nDon\u2019t be afraid to take a break.\n\nMy first GHC was 2009 in Tucson. The conference was at a beautiful golf resort in the desert that had an amazing pool with a great view. At Friday\u2019s keynote, the speaker asked how many women took a break from the conference and laid by the pool. Maybe five hands went up. The woman responded, \u201cYou know, if this was a conference full of men, you better believe they would have taken a break from the conference to get in a game of golf.\u201d I will never forget that because she was right.\n\nThe conference is nonstop from Wednesday morning to Friday night. The keynotes are in the morning, sessions throughout the day, a career fair going on, afternoon sessions, then dinner and evening events/parties. By the end of the day, I\u2019m running on pure adrenaline; by the end of the week, I\u2019m flat-out exhausted.\n\nGHC can be overwhelming. Leave time for a break. You don\u2019t have to be in a session at all times. If a session isn\u2019t relevant \u2014 or interesting \u2014 to you, don\u2019t go. If you are not getting anything useful out of a session, leave! If your brain is mush and can\u2019t comprehend anything that is going on around you, then leave the conference. Get a manicure, have a glass of wine, take a nap \u2014 whatever you personally need to recharge. Don\u2019t feel guilty about it.\n\nDon\u2019t be afraid to ask questions, even seemingly silly ones.\n\nEach session will have a Q&A portion. If you have a question, it\u2019s likely other people in the crowd have the same one.\n\nA few years ago, I attended a financial panel where one attendee stood up and said, \u201cI majored in math and minored in computer science; am I even qualified for a job in CS?\u201d Mouths dropped across the panel and she was swarmed after the event \u2014 but for her, it was a legitimate question, and one to which she didn\u2019t know the answer.\n\nIf you are too nervous to stand up in the session, use the time after to talk to the presenters; if there\u2019s no time, take down their names and emails and follow up. There is so much knowledge at GHC; it would be a shame to have questions left unanswered.\n\nGo to the career fair.\n\nThe career expo is like no other. Companies go all out with large budgets and over the top setups to attract people to their booths. It\u2019s a great place to talk to a variety of companies and see if there are any opportunities that you are excited about. There\u2019s so much diversity to what kind of groups you can talk to, and just because you haven\u2019t heard of a company, doesn\u2019t mean they aren\u2019t doing amazing things. You never know who or what will excite you. And if you are considering graduate school or pursuing a PhD, schools will also be at the career fair ready to talk to you about their programs. Even if you are not looking for a job, it\u2019s still a sight to be seen, a great place to pick up swag, and a great place to make connections.\n\nStrike up conversations with people you don\u2019t know (yet).\n\nTry to avoid always hanging out with the same familiar faces. At such a large conference, it\u2019s easy to stick to your safe zone with people you know, but start by taking one lunch and just sitting down at a table where you recognize no one. You never know what the conversation will lead to.\n\nAt a 15,000-person event, there are constantly lines: for coffee, for lunch, for elevators, for entering sessions, etc. It\u2019s a great time to meet and network. My second time at GHC, I met someone who worked at the company I was interviewing with the next day. We ended up talking for two hours about the company and its interview process \u2014 by the time I had my interview, I was calm, prepared, and knew exactly what to expect.\n\nAlso, be willing to talk to male allies that are attending the conference to get their perspective. This may be their first time at a tech event where their gender is the minority.\n\nMost of all, do not forget to have fun! GHC is a great experience with plenty of amazing women doing amazing things in technology. I hope these tips help prepare you for wonderful 3 days. See you there!"
    },
    {
        "url": "https://medium.com/square-corner-blog/a-new-partnership-with-eventbrite-and-the-future-of-squares-commerce-platform-3397c7ea2b3d",
        "title": "A new partnership with Eventbrite, and the future of Square\u2019s commerce platform",
        "text": "Today, we\u2019re excited to share that we\u2019ll be working closely with Eventbrite to ultimately provide their event creators with a single platform to accept online, mobile, and in-person payments using Square. These omni-channel payment capabilities will provide Eventbrite with one fully integrated solution for its event creators to take payments anywhere, and enable a seamless purchasing experience for event attendees.\n\nAs the lines of online and offline commerce continue to fade, it becomes more important for businesses to be able to create and manage a seamless omni-channel experience for their customers. We first launched our Build with Square APIs so any seller or developer could integrate their apps or websites with Square\u2019s managed payments and ecosystem. Since then, we\u2019ve seen developers build connected apps for everything from boba tea shops and clothing stores, to taxis in Washington, D.C. And with Caviar (Square\u2019s food ordering platform), we\u2019ve learned what a marketplace needs not only for payments, but also for advanced functionality like catalog support and order management. Working closely with the Caviar product and engineering teams, we\u2019ve added support for all platforms (and developers) to take a percentage of a payment for the services they provide. Caviar now processes all online payments via Square\u2019s public APIs.\n\nThe functionality we built for Caviar and other marketplaces is part of the foundational platform we will leverage for Eventbrite \u2014 our e-commerce and Point of Sale APIs will drive a single omni-channel payments platform that better services Eventbrite\u2019s event creators, and ultimately event attendees with a seamless purchasing experience.\n\nAs we expand our tools and platform at Square, we always keep external developers top of mind so that our APIs and SDKs operate at the highest functionality for everyone. Developers of all sizes will have access to the same tools that Eventbrite uses to meet the needs of any marketplace. With Eventbrite, we have a like-minded, customer-first partner focused on simplifying commerce, and building the best technology platform for live experiences. We\u2019ve already started, and we\u2019re excited to continue the hard work!"
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-apple-pay-on-the-web-for-squares-apis-1f85cb6fe433",
        "title": "Introducing Apple Pay on the Web for Square\u2019s APIs \u2013 Square Corner Blog \u2013",
        "text": "As many of you know, Square\u2019s latest reader supports Apple Pay for payments made in-store. Today, we\u2019re excited to announce that we are expanding our APIs to enable buyers to pay with Apple Pay on the web as well.\n\nIn the same way Apple Pay reduces lines and expedites checkout at a physical business, Apple Pay for web improves checkout speed and conversion online since buyers don\u2019t have to enter credit card details and can authenticate the payment with Touch ID. With consumers increasingly making purchases on their mobile devices, we want to make it as easy and seamless as possible to provide that optimized checkout experience.\n\nStarting today, all Square Checkout sellers have Apple Pay on the Web support with no changes needed to your code. Now your customers can pay with Apple Pay through Square Checkout and complete purchases with Touch ID or passcode on iPhone, iPad, Apple Watch, and Mac, so payments are faster and more secure than ever.\n\nWe are also excited to announce that starting today Apple Pay on the web is supported on our e-commerce API. To start using Apple Pay on the web:\n\nOnce the domain is registered, and if you are already using our e-commerce API all you need to do to enable Apple Pay on the web is adjust your code and add the locationId in the SqPayment form:\n\nAdd the following in the call back section:\n\nTo read more about what you can do with Square\u2019s e-commerce API see our documentation on embedding a payment form.\n\nWe are excited about the release of Apple Pay on the web in Square Checkout and our e-commerce API as it has already enabled many sellers to double their checkout conversion rates and reduce checkout time and drop off rates."
    },
    {
        "url": "https://medium.com/square-corner-blog/announcing-version-2-3-0-of-our-client-libraries-f98c7ab0e5fb",
        "title": "Announcing Version 2.3.0 of our Client Libraries \u2013 Square Corner Blog \u2013",
        "text": "If you are familiar with How Square makes its SDKs then you know that one of the best ways to see what\u2019s new in our latest SDK version is looking at the diff in our API specification on github (link).\n\nThe largest update in this release is support for Orders endpoints. The new Orders endpoints allow you to create itemized transactions online, and are a big improvement in reporting for online sales. You can read more in the official announcement or jump right into the documentation.\n\nWith any new release, there are always some earlier mistakes that get fixed. This one includes things like:\n\nTake a look at the latest & greatest version of whichever SDK you like to use, and spend a moment upgrading as well. If you notice anything amiss, file an issue in the appropriate GitHub repository and we\u2019ll get to it ASAP."
    },
    {
        "url": "https://medium.com/square-corner-blog/building-for-an-omni-channel-business-with-squares-apis-has-never-been-easier-3b5e0977741a",
        "title": "Building for an omni-channel business with Square\u2019s APIs has never been easier",
        "text": "Starting with the launch of our E-Commerce API, we\u2019ve worked to create a powerful commerce platform for both online-only and multi-channel businesses. In the last few months we\u2019ve introduced our Catalog API, a tool to help businesses manage their products more efficiently, as well as Square Checkout, an out-of-the-box, hosted checkout for Square developers. And today, we are tying it all together to make sure developers can provide all the necessary tools for every omni-channel business eCommerce and in person.\n\nWith the new Orders API, the tools developers build for Square sellers connect their Catalog to their online sales through our APIs for online payments. This full itemization translates to more meaningful reporting for online businesses, and especially for those who also sell in-person and share a catalog between channels.\n\nTo implement full itemization, all a developer needs to do is create an Order and reference the order_id in our Charge endpoint or directly pass the order as part of the Checkout POST request.\n\nLike the earlier release of Checkout we didn\u2019t limit this to just merchants who have catalogs stored on Square. You can still use the feature even if you just want to create, one-off, ad hoc items.\n\nNow, when creating an Order and charging it, the full itemization will be available via Square dashboard with full discount, taxes and item breakdown. Custom amounts are still available if you don\u2019t use itemization. We also allow you to consume that information directly via our API. All you need to do to see your order information is: ListTransactions to get the order_id and then BatchRetrieveOrders.\n\nWhen creating custom items, Square\u2019s internal systems will take care to aggregate your custom item sales data to show you the full picture of your business."
    },
    {
        "url": "https://medium.com/square-corner-blog/autoscaling-based-on-request-queuing-c4c0f57f860f",
        "title": "Autoscaling based on request queuing \u2013 Square Corner Blog \u2013",
        "text": "Modern web development often includes traffic-heavy web applications and APIs. These services are tasked with a double-whammy: they need to be able to do a ton of work and to be able to do it for a significant number of requests. For example, a single application can be responsible for user authentication, request routing, view rendering (both HTML and JSON), business logic, and accessing data from a permanent data store [1]. Being able to do that and respond to high levels of load is a big ask. Applications attempting to do both do not scale well on their own and, as an industry, we\u2019ve found some ways to solve this challenge.\n\nThe tool that we reach for first is to solve them problem through hardware. At relatively small scale, adding more resources to the server that is doing the work (i.e. adding more CPU, RAM, or disk space) is cheaper than scaling horizontally (which we will talk about in a moment). This can be called \u201cscaling vertically\u201d or going \u201cBig Iron\u201d. This was the way the Caviar first solved our scaling problem (and we still do for certain applications, actually).\n\nScaling this way is a two-phase attack on the problem by 1) adding more resources or 2) using those resources more effectively. Using the resources more effectively means to not actually change the amount of resources on the machine, but to use techniques such as threading and parallelization to get the most of them. This can also mean switching to other models of use.. for example using event-based programs instead of having e.g. I/O block use of threads and, ultimately, the underlying resources. Eventually, though, you run out of room to optimize and you need to add hardware. Adding better, faster, more spacious hardware can only take you so far, though. At some point, hardware becomes more and more expensive as the demands required go beyond the capabilities of commodity hardware.\n\nAn alternative is to use horizontal scaling. Instead of adding CPU, RAM, and disk space to a single server, we add more servers. These servers are typically equipped with mid-range commodity hardware and are extremely cost effective. With this fleet of servers, you place your same service on two or more of them and distribute requests across each server. There are a few implications here. First is that all state must be stored in a centralized data store, like a database [2]. Second is that you can effectively scale to infinite size on this setup, adding as many commodity hardware servers as you need.\n\nOne of the great upsides of the rise of cloud computing is that they are very good at horizontal scaling. Almost by default, modern web applications use centralized data stores provided by these cloud services. Cloud service providers like Amazon and Google have created load balancing appliances that distribute requests. Those load balancers handle some of the difficult, non-obvious hurdles that come up like service registration, health checks, and connection draining. Cloud computing providers can add or remove resources in minutes. This allows us to scale up to our customer demands very quickly and it allows us to stop using resources as we need less and less of them.\n\nPutting horizontal scaling into practice comes in many flavors and one of the easiest approaches to start is to over provision. We achieve this by finding the peak usage of our services and, at the very least, scale our number of servers up to the size that would enable them to handle that amount of load. This can have some challenges. Caviar, for example, experiences asymmetric load over the course of the day. As you can imagine, for a food ordering service, lunch and dinner are our peak times. Here is the load profile from one of our services, which has stayed remarkably similar over the past 3 years.\n\nAs you can see by just counting the horizontal grid lines, our peak load is about 5x our minimum load for this particular service (seen near the edges). This means that for all the time that is not peak load, there are resources just sitting idle. This is both wasteful and brittle. If you have a special promotion, your provisioning may not be adequate and you\u2019ll have to increase your resources. If you don\u2019t anticipate this perfectly, you run the risk of overloading your systems and having the site go down due to load.\n\nThe alternative is to scale with your load. This solves both problems experienced in the over provision scenario. First, your system is dynamic and responding to change all the time, so an unexpected increase in load should be handled no problem. Second, you can match your resource provisioning profile to your load profile, and save time and money on those resources. But, you need to make a decision regarding how to autoscale your services. There is not a bunch of information out there on different approaches to this, but I\u2019ll talk about two examples and Caviar\u2019s experience with each.\n\nThe first approach we used at Caviar and the one I had used previously was to use the response time of the service as the indicator of the load profile of the service. We used this while we had a single monolithic Rails service that had hundreds of different endpoints each with different expected response times. This method gave us low visibility into the true health of our service. If we were backing up on our fast API endpoints that were calling us all the time, it would often be masked by higher response time endpoints that were called less frequently. It made scaling unpredictable and did not achieve our ultimate goal of having enough resources to serve each user request in a timely manner.\n\nWe switched to another method using request queuing. There is a bit of nuance here and we evolved our approach over time, so I\u2019ll explain the differences between each of the ways we approached them and where we are at today. We\u2019ve tried to different methods of measuring request queuing and have landed on a system that we think functions pretty well.\n\nWe started by using a rough approximation of request queuing by calculating the amount of time that it took to get from the load balancer (in this case, it was Amazon\u2019s ELB) to the first part of the Rack stack. Since ELB does not provide a timestamp header added to the request (which you can do in e.g. nginx), we needed to calculate it via reported metrics from the ELB and from Rack for each individual request. This was better than use response time because it gave us insight into how much time each request was waiting for the server to become available to service a new request. This would happen on each endpoint and we could measure the approximate request queuing time across all endpoints independent of their ultimate response time. The downside of this was that it was both a little unreliable (difficult to perfectly match everything so we were potentially missing data or incorrectly reporting data) and it was dependent on other variables like the network stack. The network stack added a relatively stable, but not always predictable, overhead cost to our calculations. So, our request queuing metric would never fall to a flat 0 because it always takes some time for the ELB to process a request, offload SSL if it needed to, and route the request to the server waiting for the request. Generally, we could work around that, but we found another way to solve the problem.\n\nWe realized that what we were really wanting to know is if there was anything queuing up at our Rails application. We use puma as our web application server and it has a bunch of advantages that we can go into another time, but one of the things is that is allows us to inspect the queue length at any given time. At startup, we start a new thread that will report statistics on a looping basis up to our metric collector (CloudWatch in our case). Every 30 seconds, we query the puma master process to fetch the stats for each working, including the number of working threads and the current backlog (queue) count. We report those metrics to our metric collector. We then aggregate the queue across all of the running services to get an idea of how backed up we are. If we are backed up for a sustained period of time, we add services to clear that queue. If we are clear for a longer period of time, we remove services from the stack (down to some minimum, we always have at least 3 for redundancy). This system allows us to respond both quicker and with a great deal of clarity to our production load. In addition, the same scaling system works exactly the same for background workers. Instead of using puma queues, we use Sidekiq queues and scale on similar parameters.\n\nThere are still improvements to be made to this system, but today we\u2019re pretty happy with the overall setup. The main issue that we have with this setup is that with queuing being the only metric we use to respond to autoscaling up and down, there is no \u201csteady state\u201d and we\u2019re constantly under/over provisioning and changing the number of services that we are running. We will continue to tweak the system in order to find the right balance of tradeoffs for us going into the future.\n\nOverall, using request queuing as the input to our service scaling has gotten us to a place where we feel confident in our ability to respond to change. We\u2019ve been running with some form of autoscaling based on request queuing for over a year the effort was far and away worth it. On a final note, I wanted to drop a sample GitHub repository that had a puma web server that reported metrics up to AWS CloudWatch as a starting point. Once you get there, it is all about creating alarms and autoscaling, which I\u2019ve including some example CloudFormation templates for how to do that part. In the end, if you have a large scale [3] web system that needs to respond to variable traffic, autoscaling based on request queues is what I would recommend.\n\n[1] There are other interesting architectural methods of reducing the load instead of leaning on the server to do so much work. This includes single page applications (SPAs) and statically pre-generated websites. I\u2019m interested in exploring those topics later, but for this post we\u2019re going to focus on more classic web applications.\n\n[2] This actually isn\u2019t usually a problem. Most modern web stacks start off with a database backing them as part of the bootstrapping step. Other things to consider are caching, uploaded files, etc. Caching can be offloaded to a caching store like memcached and files can be stored on a file repository like S3.\n\n[3] I cannot/will not speak to \u201cglobal scale.\u201d In theory, these techniques work, but I\u2019ve never tested it in those environments."
    },
    {
        "url": "https://medium.com/square-corner-blog/transitioning-from-v1-items-to-catalog-apis-57d4badb012",
        "title": "Transitioning from v1 Items to Catalog APIs \u2013 Square Corner Blog \u2013",
        "text": "Earlier this year, Square released new catalog endpoints to help manage your items in a more streamlined and scalable way. You might be familiar with the v1 endpoints , , and (CRUD) operations with items and their associated variations, modifier lists, modifier options, discounts, fees, etc. Catalog endpoints streamline all of these different entities into a single type with new endpoints for doing batch operations for multiple items at once. As a result, managing items across multiple locations is much easier. For the rest of this post I\u2019ll go over what these differences actually look like for a couple of operations.\n\nPreviously you only had available endpoints for creating new items, variations, modifiers, etc. Frequently you had to use all of them to create a new item, with complex chaining of requests to make sure that everything was associated correctly. Multiplying that workflow for all of the items in your account can quickly turn \u201cimporting your items\u201d into waiting for a gazillion requests to finish as you create new items, variations, discounts, and modifiers, all in serial.\n\nWith the catalog endpoints, that workflow has been slimmed down into one endpoint, or upsert catalog object. While this might seem complex at first, catalog objects are very powerful and simplify the number of other calls that you need to make by adding multiple object types to the same request. You can now create your items in parallel to their variations, options, and modifiers, all while operating at great scale due to the endpoint options.\n\nLet\u2019s take a look at the code required to create a new item and a couple of variations.\n\nThis is another area where the Catalog endpoints can make a big difference, especially if you manage items for multiple locations.\n\nAt first glance, the two snippets look similar, but the little differences can make a big impact in how easy it is to manage a real world example. First of all, you don\u2019t need a location to retrieve your items, since items are no longer necessarily scoped to individual locations. Further, you have the option of requesting all of the related objects with the parameter, which includes all the related category, taxes, and modifier lists that apply to the item. The same operation with the v1 item\u2019s endpoints would require you to call specific endpoints for each of the different properties you want to retrieve.\n\nThis is just a taste of what\u2019s better about using the new Catalog APIs over the v1 items endpoints, so if you haven\u2019t made the switch yet, give it a try and see how it might it simplify some of your code.\n\nSquare is also hiring now for our developer platform and more! Take a look at Square.com/careers to see if there is an opportunity you might be interested in. Come work with us :)."
    },
    {
        "url": "https://medium.com/square-corner-blog/the-ember-run-loop-and-asynchronous-testing-c03326181623",
        "title": "The Ember Run Loop and Asynchronous Testing \u2013 Square Corner Blog \u2013",
        "text": "This summer, I interned on the Square Seller Dashboard team, which works on the web application that Square merchants use for everything from checking sales, to managing their payroll, to signing up for Square Capital. For some context, I\u2019m a rising senior at the University of Chicago and before this summer I\u2019d never worked on a web application.\n\nDashboard is built with Ember \u2014 an open-source web framework for single-page web applications. Working on a team that focused on such a large app meant that I could dive deeply into Ember \u2014 and one thing I kept noticing throughout the summer was the special, powerful, and at times, confusing way that Ember handles asynchronous work.\n\nOne project I worked on was displaying a warning before merchants were logged out due to inactivity.\n\nThis required monitoring different state throughout the life of the application, and rendering components accordingly. I used an Ember Service, which allowed me to set global state on the app based on code that was executed every 250 milliseconds. Removing the API calls and some of the complexity of a service, the code looked something like this:\n\nIn development, it worked great. But when I ran acceptance tests, they would hang and then timeout, throwing a bunch of unpredictable errors. You can see that by running the tests in this EmberTwiddle.\n\nEven though this was supposed to be the simple part of the project, I couldn\u2019t get a single acceptance test to finish running, much less pass. Turns out, the reason my tests were breaking was because of the way I used which is supposed just run the callback function after the given milliseconds. Finding the solution to my problem revealed a lot about how (awesome) the Ember Run Loop is, and the way it interacts with asynchronous testing.\n\nIt\u2019s nearly impossible to work in the Ember World without running into some mention of the Run Loop, though the docs indicate that many developers won\u2019t deal with it directly\u2014\n\nWhether or not you work with the Ember Run API directly, the Run Loop is fundamental to Ember applications, and is often at the heart of figuring out strange errors and unexpected side effects.\n\nIn general terms, the Run Loop efficiently organizes and schedules work associated with events. The work triggered by user events (i.e. , etc) is often asynchronous. Ember can do other work before all the side effects are executed. The Ember Run Loop schedules these side effects and organizes any asynchronous work. Which sounds awesome, but needs some more context.\n\nA bit of a misnomer, the Ember run loop isn\u2019t really a loop \u2014 and there doesn\u2019t have to be just one of them. Rather, a run \u201cloop\u201d has six different queues that organize work triggered by events. So, the Run Loop (capitalized) is more of a concept, and an app will have multiple run loops running at once.\n\nThe docs list the priority order of the queues as:\n\nThe docs also give a short explanation of each queue\n\nMy short interpretation of the job scheduling \u201calgorithm\u201d is that the Run Loop executes jobs with the highest priority first, based on queue. These jobs may add work to other queues, and the Run Loop will loop back to the highest priority jobs until all jobs are completed.\n\nI really love Katherine Tornwall\u2019s explanation of the Ember Run Loop! It has a wonderful in-depth explanation of each queue, and I used some of her descriptions and examples to help with this illustration of a run loop.\n\nThe inner details of each queue are interesting, but the most important detail is that events trigger a run loop and may place various asynchronous work into different queues. For example, a event could start a run loop, and other work associated with it would be placed in the appropriate queues. Ember ensures work related to data synchronization happens before any rendering. If data synchronization happened after rendering, it might change the rendering of templates that are bound to that data. Then, more expensive re-rendering would be required!\n\nFor an interactive demo, check out Machty\u2019s Run Loop visualization or the simple example in the Ember docs.\n\nThe way my service took advantage of the run loop was one of the many Ember.run API\u2019s.\n\ndoes has an advantage over or other alternative timers\u2014 it lives in the \u201cEmber World,\u201d so it can take advantage of all the run loop has to offer (efficiency, organization). also respects Ember\u2019s internal queue of timers (which can\u2019t be said of normal javascript timers).\n\nIf we look back at my component, we see that I call recursively in one of the methods:\n\nUltimately, as I\u2019d discover later, it was these recursive calls to that caused the tests to timeout.\n\nI had a hunch that was causing my problem, since replacing it with stopped the tests from hanging (more on the drawbacks of that later). As I googled around, I realized that my tests failed for the same reason that makes Ember.run.later awesome \u2014 because there was always work scheduled for a future run loop.\n\nHow did I figure that out? I wasn\u2019t sure what the problem was at first, so after some searching around, I checked in the source. In the source code, we can see that the test helper checks to make sure all run loops are completed and there are no queued timers.\n\nBut why? Initially, it seems inconvenient. As it turns out, Ember tests want to make sure all asynchronous work to complete before they finish. Internally, this means that Ember helper checks to make sure that all run loops are completed and there are no more queued timers \u2014 if this check fails, clearly there\u2019s still work to be done.\n\nSince I was calling recursively, there was always either a job in the run loop or a scheduled timer, so the tests would never finish.\n\nWaiting for all work to finish is fundamental to the way that Ember tests asynchronous code. Of course, it\u2019s not the only way to test asynchronous code. For example, capybara uses a maximum wait time for asynchronous calls and automatically ends tests after that time has passed.\n\nThat can lead to problems of its own. For example, suppose we click to close a modal, and want to assert that the modal is closed. In capybara, when the time runs out, if the asynchronous work of closing the modal happens after the time limit, the test will end and fail. On the other hand, Ember can ensure that the asynchronous work associated with closing the modal finishes before calling assert.\n\nNow that I understood the bug, I needed to figure out a solution. I bounced around a few ideas, especially at the beginning. Even though I had wanted to use , my initial reaction was to try to go back to using\n\nRunning the acceptance tests again, I got this infamous error:\n\nAssertion failed: You have turned on testing mode, which disabled the run-loop\u2019s autorun. You will need to wrap any code with asynchronous side-effects in an Ember.run\n\nWhy does that happen? Well, I like to think there are two worlds in your Ember application: the Ember World, and the World Outside Ember. The Ember World uses run loops, and asynchronous work in the World Outside Ember, like websockets, ajax calls, should also be wrapped in an so that the run loop can handle side effects correctly.\n\nIn fact, there\u2019s actually a lot of things that can have asynchronous side effects, even calling \u201cEmber.object.set\u201d on a property bound to a template. Ember saves us by smartly by wrapping your code in an \u201cAutorun\u201d, basically an , which runs in development and production, so that the side effects occur in a run loop.\n\nHowever, Autoruns are disabled in testing. The Ember docs list several reasons for this:\n\nThis means if you try to use , make ajax calls, or even set properties which have asynchronous side effects, you\u2019ll get an error in testing. That way, you can catch potentially unpredictable side effects before they happen in production.\n\nOne commenter calls the problem with recursive a \u201churtful issue,\u201d and I can see why. There is no perfect elegant, concise solution to this problem. The right solution requires thinking deeply about your own application and testing.\n\nOne suggestion that appears frequently on the Ember forums is to wrap the callback in an Ember.run:\n\nThe advantage of this solution is that in testing, the timer functions the same way it would in production.\n\nThe disadvantage of this solution is that still lives in a world outside of Ember, and as mentioned previously, won\u2019t respect an internal timer queue. In my project, I found this to be more unpredictable than I would have liked. Sometimes, due to run loop execution, the next would execute after the element that was supposed to stop it was destroyed. If you\u2019re doing something like , you\u2019re probably encountering an error like this.\n\nSince my code dealt with a lot of application-wide state, I really wanted to take advantage of the Ember Run Loop.\n\nI turned to the Square #ember Slack channel, and immediately someone suggested I check out rwjblue\u2019s Ember Lifeline. Thank you, slack channel hero. Ember Lifeline had an innovative approach in the function \u2014 rather than hackily incorporating , handles work differently in testing versus development/production. In development/production, is called recursively. But in testing, the next asynchronous is saved, and you\u2019re able to manually forward the poller.\n\nSince I was working in an app that doesn\u2019t yet support using Ember add-ons, and I had no need for the whole Ember Lifeline library, I just incorporated a few key ideas into my own app. I can manually tick the timer and control exactly what work I\u2019m doing. My tests finally didn\u2019t hang, nothing broke, and I\u2019d found a satisfying solution to my all my errors.\n\nFor me, it didn\u2019t matter that the timer wasn\u2019t \u201cactually\u201d running in my tests. I cared more that the functions executed on each tick were monitoring correctly and making API calls at the correct frequency. This had the additional advantage that my timer wouldn\u2019t end up running in the background of anyone else\u2019s tests. There\u2019s understandable hesitation in running different code in testing than in production. But the thing about projects that rely on timing is that it\u2019s nearly impossible for tests to be the same as production. Even if you\u2019re using you\u2019ll often end up using Sinon\u2019s fake timers\u2014so your code already won\u2019t execute exactly as it would in production. Writing concise, testable code that doesn\u2019t actually rely on any of the timing means higher test coverage and makes time less of an issue.\n\nIt took awhile for me to find the right solution for my project, and while it worked for me, it might not be the right choice for everyone. I\u2019m interested to see what sort of different solutions the Ember community comes up with in the future as they deal with this problem!\n\nI learned a lot this summer about Ember, but also how to solve problems in general. I can\u2019t capture everything I learned in a few bullet points, but here goes:\n\nThis summer was amazing. A huge shout-out to my team for encouraging me and supporting me, in my work and in this blogpost. And special special thanks to George, for always reminding me how important this project is. Marie Chatfield, a mentor, inspiration, and Ember queen. And Lenny, whose incredible knowledge is only outshined by his ability to share it, and who sat with me for countless hours debugging, reading through backburner.js, and discussing my midnight slack rants. Overall, thank you to Square for being such a wonderful, inclusive, smart place."
    },
    {
        "url": "https://medium.com/square-corner-blog/tips-and-tricks-for-api-pagination-5cacc6f017da",
        "title": "Tips and tricks for API pagination \u2013 Square Corner Blog \u2013",
        "text": "Welcome to the world of pagination \u2014 spreading out results over a number of pages or sections. I\u2019ll provide an overview into some basic pagination concepts, and dive deep into how we do things at Square.\n\nThe concept of pagination originally comes from books, but I think it\u2019s easier to describe in terms of lists. You see pagination all the time on blogs, news sites, image sharing sites, etc. Whenever there isn\u2019t enough room on the page to show all of the posts at once, the big list of posts is broken up into different \u201cpages\u201d. You are probably familiar with the UI components similar to the one below that help you navigate between pages.\n\nPagination with APIs works a little bit differently but has the same underlying concept. When you try to retrieve a list of objects that is too large, either for the computational costs associated with retrieval (or consumption) or any of the plethora of networking concerns (waiting for a few gigs of data from an API call can have unexpected effects on your application), the service will generally respond with information about how to access the next \u201cpage\u201d, or section, of results.\n\nI\u2019m going to refer to this tidbit of information as a \u201ccursor\u201d, but it could have many names and forms including:\n\nTo help explain the concept a little more I made an animation of what a cursor added to a response looks like for our v2 List Transactions endpoint. Your code would make a request to the List Transactions endpoint, and if you have more than 50 transactions, the API would return the first 50 and attach another field to the json response called . This is a pagination token, and attaching that to your next request (in the form of a URL parameter) tells the API which transactions you have already seen, and where to start when returning the next 50 results. That response includes a different pagination cursor for you to get the next 50 results. That continues until you have \u201cpaged through\u201d all the results that you requested, with the last page usually having fewer results than the expected page size. You can see a conceptual example of what that looks like in the animation below.\n\nSquare\u2019s payment APIs use the token based approach, but in two different ways: The endpoints return a header with a link to where to send the next request, while the endpoints add a cursor token to the json response, and accept it in subsequent requests as a URL parameter.\n\nWith the release of our SDKs, we\u2019ve added additional functions to access the pagination tokens from the headers, like in PHP. Use those to get the pagination tokens with the client libraries more easily.\n\nIn most cases, when you experience pagination, you likely want all of the items in the list, and aren\u2019t very happy about the extra work required to page through the results. Here are some best practices to make it a little less painful to get all of your results:\n\nAlthough a little cheeky, conceptually this is how you will be able to get the most performance. Many APIs allow you to query for items based on specific criteria, or only return certain subsets of the data. Square\u2019s List Transactions endpoint for example allows you to query based on time ranges. If you know that the transactions you are looking for only occur in a certain time range, you can narrow down the result set from the beginning, in many cases eliminating pagination and giving you a performance boost in comparison to over fetching and then filtering in your application.\n\nDepending on your programming language of choice, pagination can be a wonderful use case for . The basic workflow is that while you are getting a pagination token in your response, keep making subsequent requests. In pseudo code that might look like this:\n\nAs soon as your API stops responding with a pagination cursor, then you can stop looping and continue execution with the complete set of data. There are a couple important points to remember with an approach like this:\n\nHopefully this is a helpful resource for the next time you need to paginate (or avoid paginating!) your results from an API. If you have more questions about pagination with Square\u2019s APIs, take a look at the resources for v1 & v2."
    },
    {
        "url": "https://medium.com/square-corner-blog/using-word2vec-to-power-a-recommendation-engine-1c18b93fbb30",
        "title": "Using Word2Vec to Power a Recommendation Engine \u2013 Square Corner Blog \u2013",
        "text": "Nicole is a rising junior at Rice University and Joy is a rising senior at University of Pennsylvania, and we are both software engineering interns at Square this summer. Square hosts an Intern Hack Week every summer, allowing interns the opportunity to implement our ideas for new features or tools. Below is a description of our Intern Hack Week project.\n\nAt Square, we\u2019re always looking for ways to leverage our large data set to automate customer-facing experiences. As Intern Hack Week approached, we wanted to pick a project that could help inspire everyone to think through more areas where we could automate the work that Square sellers usually have to do when setting up their accounts.\n\nThe purpose of this hack week project was idea generation, rather than implementation as a product feature.\n\nWe decided to address the perennial issue for sellers of deciding what products to offer and figuring out good prices to drive sales. Market research can be extremely time-consuming, costly, and resource-intensive. Business owners may spend days trying to understand their neighborhood trends and business environment or how they can improve their offerings and prices to maximize their revenue.\n\nAfter discussing this problem with product managers, analysts and the sales team, we decided to build a tool to give sellers actionable insights into industry and neighborhood-specific trends. This inspired us to develop a tool that would automatically recommend item pricing and additional menu items to sellers based on information from local businesses in their neighborhoods.\n\nWe had one week for this project, so we decided to scope it to the food and beverage industry. Nearly 20% of Square\u2019s GPV (gross payments volume) comes from the food and drink industry alone. This massive amount of transaction data provides a bird\u2019s-eye view into neighborhood and industry markets.\n\nFor demo purposes, we built a Python Flask app with D3 visualization that included features for both existing Square sellers and new sellers.\n\nWe created a tool where current sellers can enter their user ID and view a customized report. In the first section, sellers can select a product from their own product offerings and see a pricing distribution of all similar items sold by other sellers in their neighborhood. We also included a second section that lists three items the seller is not currently offering but should consider selling, based on what nearby similar Square sellers have on their menus. (See the next part of the article for how we came up with the suggestions!)\n\nPrice Suggestions to Help New Sellers Onboard\n\nFor new Square sellers, we used local item pricing data to build a tool that helps them price their menu items. Given the new seller\u2019s zip code, he or she can enter an item to the search bar and get a distribution of prices for that item in their neighborhood, as well as an averaged suggested price-point based on local sellers\u2019 transactions.\n\nThe Data Science Behind It\n\nText data is notoriously difficult to work with! The challenge with free form text data is that sellers enter their own item names for the products they sell, leaving a lot of room for variations in naming conventions. Without rules governing the text entry, analyzing item-level data is quite tricky. For example, a coffee shop owner can create a \u201clatte\u201d or a \u201cCafe Latte\u201d or a \u201cFrapp LATTE\u201d or a \u201cpumpkin spice latte\u201d and the list goes on. Without a canonical list of item names, the data is extremely noisy and hard to group.\n\nTo deal with this lack of structure in item names, we filtered the items in our transaction data using a food database to get only food and drink items.\n\nOur next step was to group these food and drink items, so that we could compare pricing of similar items sold at nearby businesses and suggest new items for Square sellers to offer. The difficulty we encountered when analyzing this text data was accounting for the underlying subtext. Because it is challenging to compare words like \u201csoda\u201d and \u201cpop\u201d and understand their similarity purely by looking at the text, we used a pre-trained word2vec model to perform a lexical analysis. This gensim word2vec model, trained on Google News, represents words as vectors.\n\nWith vectorized words generated from running the model on our food and drink data, we were able to do text classification since we could perform numerical comparisons of items. The more closely words are related, the shorter the distance between their corresponding vectors. Because an item name can be composed of multiple words, we summed the vectors of each word and normalized it to get a vector for that item. Therefore, the result of running this model on our food items data set was the grouping of similar items in vector space.\n\nIn order to determine these groupings, we calculated the cosine distance between each pair of vectorized items. Taking this n x n matrix, we chose a threshold to classify nearest neighbors and then tuned it until it gave us reasonable results. To describe this more visually, in effect we drew a sphere around each item in a vector space and considered all items whose vector was within that sphere to be very similar. Below is an example of our model output. As you can see, the similar items aren\u2019t perfect, but are fairly reasonable. If we analyze words related to burger, we capture chicken sandwich and falafel sandwich, which are obviously not the same, but still related.\n\nFor each menu item from a given seller, we were able to generate a price distribution using the prices of other items in its nearby space. We estimated an underlying probability density of the distribution through Kernel density estimation, and plotted it to represent the item price distribution among nearby sellers. Looking into the similar items within each item \u201csphere\u201d for a given seller, we were able to generate item suggestions by returning the top three most frequently occurring similar items that were not on the seller\u2019s menu.\n\nThis project has helped our product teams think through ways we could automate onboarding for food and drink sellers. Pre-populating prices as new sellers set up their menu items in our point of sale system would streamline much of the work for our sellers and make pricing a much easier decision. In addition, this work could be applied to the seller dashboard reporting page, adding to existing analytical insights powered by Square\u2019s data."
    },
    {
        "url": "https://medium.com/square-corner-blog/boosting-conversion-rates-with-analytics-design-principles-and-ux-3ee133545610",
        "title": "Improving Conversion Rates with Analytics, Design Principles, and UX",
        "text": "We have big plans for Square Invoices, and we are constantly building new features to help our sellers easily send invoices and get paid fast.\n\nA few months ago, we reached an inflection point with the product. Square Invoices had come a long way from where we started and our sellers love the simplicity of our solution. However, as we ramped up thousands of new sellers and developed more and more features, the product began to feel cluttered and confusing. We literally couldn\u2019t add another feature to our creation form without the layout becoming overwhelming. We needed to increase functionality without sacrificing the ease-of-use that Square sellers value and for which they came to us in the first place.\n\nThat\u2019s when our team used analytics and design thinking to solve the problem. Our original form gave high visual prominence to fields that were seldom used and lacked a clear visual hierarchy, contributing to a sense of clutter that was compounded by the addition of any new features.\n\nAnd the data proved it.\n\nWe performed UX analysis of field usage data to determine how existing sellers were interacting with fields and features in the Invoices form. We found the most frequently-used fields on the creation form forced a customer to dart from the top left side of the screen, to the middle right, and then back to the bottom left. Finally, they had to move their attention back to the top of the form to send the invoice to their buyer.\n\nObviously, this zig-zag pattern was less than ideal. In order to improve our product, we had to make some pretty significant changes to our design.\n\nWe built a new experience that drastically improved and streamlined the invoicing process. A single-column layout was used to create a clear page hierarchy, with left-side labels for improved scannability and comprehension and a strong visual foundation for future features. It even follows the recommended F-Pattern layout pretty well too.\n\nWe tested the usability of this new form with a few select Square Invoices sellers who actively use this product. Once we had initial qualitative feedback, we prepared a good old-fashioned A/B test to determine which form performed better.\n\nBefore starting this experiment, we discussed and outlined our primary success metrics. The most important of those was the rate of invoices being sent between the control group and our test group. Our hypothesis was that Square sellers would be able to glide through the new invoice creation flow more efficiently with the more organized layout, which would lead to an increase in the number of invoices being created and sent in the test group. We also tracked a few secondary metrics including: creation rates for new sellers who had never used the product before, bottom of the funnel product usage, and the number of support tickets that were created.\n\nAnd the results? Drum-roll please\u2026 The new form won! We saw a statistically significant 1 percentage point increase in the number of invoices being sent with the group of sellers that saw our new creation form. Not only that, but there was an increase in the number of sellers sending a second invoice with the new form. In other words, this re-design helped drive engagement at the top and bottom of our product usage funnel.\n\nThis result was great validation that the new form was a dramatic improvement over the old experience. We didn\u2019t change any of the core functions within the form, we simply made it easier to use by designing an improved organized experience \u2014 and that drove growth and conversions.\n\nBut as all product teams know, \u201cnothing lasts, nothing is finished and nothing is perfect\u201d (Richard Powell). We\u2019re working on even more updates and features for Square Invoices to help our sellers send their invoices easily and get paid quickly!"
    },
    {
        "url": "https://medium.com/square-corner-blog/version-2-2-1-of-our-sdks-are-now-live-972d7683b70c",
        "title": "Version 2.2.1 of our SDKs Are Now Live \u2013 Square Corner Blog \u2013",
        "text": "One of the easiest ways to describe the changes in the latest version is to look at the square/connect-api-specification using GitHub\u2019s compare view. You\u2018ll notice between versions 2.2.0 and 2.2.1 we had commits from different team members, and even a bug fix from one of our developers.\n\nIf you want to better understand how we make our SDKs, and how our API specification works with templates to create a complete SDK, check out How Square makes its SDKs . Below we\u2019ll get into what\u2019s new and improved in version 2.2.1 of our SDKs.\n\nThe commit messages alone only show some of the detail. You can get additional details in the full diff. This version fixes some of the v1 models that, in some cases, prevented developers from getting all of the information back from the API when using the SDK.\n\nOne of the most anticipated SDK features is support for v1 pagination. All client libraries now have methods like for PHP to process the returned headers and get the appropriate token that you can then use to request the next page of results.\n\nThe last change in this release is improved documentation for SDK model methods that get and set properties. Viewing the generated documentation for a model (for example, the transaction model for PHP) now tells you explicitly which methods you will need to use to access the underlying data.\n\nYou can download or upgrade the latest version of our SDKs from your favorite package manager.\n\nIf you have any suggestions, fixes, or issues with our current SDKs, feel free to raise an issue or draft a PR in the relevant GitHub repository. You can also discuss the SDKs on our developer Slack \u2014 head to squ.re/slack for an invite."
    },
    {
        "url": "https://medium.com/square-corner-blog/building-web-based-points-of-sale-for-android-ios-9dfbc0f261e4",
        "title": "Building web-based points of sale for Android & iOS",
        "text": "With Square, you can build a web app that switches to the Square Point of Sale app to accept in-person payments. The best part is that you can do this with both Android and iOS, so it is possible to build a web-based point of sale that can utilize Square\u2019s hardware to accept in person payments on both iOS and Android. In this post we\u2019ll walk through the necessary steps to build your own apps.\n\nIn order to make a cross platform web app, we need logic to detect what platform the target device is running. Doing this accurately can be really hard, and there are whole libraries dedicated to parsing out user agents. To keep this example simple we\u2019ll use a javascript solution roughly outlined in this Stack Overflow answer that uses a regular expression on the user agent.\n\nIf the user is on an Android or iOS device, then we will display a button that will switch over to the native Square Point of Sale app when clicked. For desktop and other platforms we\u2019ll show a dummy e-commerce form for users to input their card information.\n\nIf you haven\u2019t created a Square account or registered an application to get API credentials, follow steps 1 & 2 in this Getting Started guide for the Square APIs. You\u2019ll need your and to fill out your s.\n\nYou initiate the switch to the native Square app with a link, just like navigating to any other page on the web. Keep in mind the link is going to look a little different than what you normally put in an .\n\niOS: \n\nThe basic structure is to have a link like this where the is an encoded JSON object with information about how much the sale is for, what tenders the Square app should allow for payment, etc. Your might look something like this:\n\nBe sure to remember to encode them with a function like so that your final URL looks something like this:\n\nYou can learn more about the the options at the Using the Point of Sale API for iOS guide.\n\nAndroid:\n\nAndroid links look very different, but should be somewhat familiar if you used Android before. All you have to do is put your intent directly in the link:\n\nAll the information for the transaction is added into different parameters. See the reference at Using the Web API for Android.\n\nNow we have the logic for roughly detecting the operating system, as well as handling payments for iOS, Android, and the web. The rest of our code is made up of HTML and javascript gluing everything together. Don\u2019t forget, an easy way to test out the code is to use ngrok to expose your development environment to a URL.\n\nIf you have any questions about this code, or using the web API for Point of Sale, please read more in the documentation, or leave a comment, or reach out to us on Twitter (@SquareDev). And check out the final code in the example we created below!"
    },
    {
        "url": "https://medium.com/square-corner-blog/easing-your-development-with-ngrok-5389d6bbbc68",
        "title": "Easing your development with ngrok \u2013 Square Corner Blog \u2013",
        "text": "ngrok let\u2019s you easily test your local code, webhooks & more with one a simple command.\n\nngrok is a tool that allows you to create a \u201ctunnel\u201d from your localhost to the internet. It is best explained in an example, so I\u2019ll demonstrate how to get up and running if you wanted to test a web app on multiple devices, or if you are developing for webhooks.\n\nGetting ngrok on your computer couldn\u2019t be much simpler. will likely have the right executable for your platform. Downloading and unzipping will get you an executable that you could run with a command like , but instead of keeping track where you stuck that download, move it into your to get access to everywhere.\n\nLet's say that you are working on a application, perhaps something as simple as:\n\nAn you are running a local development server with something like\n\n . That is great for all your local testing, but in order to test how that code might work on a different device (especially if you are using the Point of Sale APIs to build a cross-platform web-based POS) you will need to expose something on the web.\n\nThe simplest invocation for ngrok is . This takes the service that you are exposing on and gives it an externally accessible URL. The URLs that it generates are random, but you can sign up for an account to get custom domains that you specify, as well as other advanced features.\n\nNow all you have to do is navigate to your new URL on a different device to continue your testing and development!\n\nWebhooks are an area where ngrok really shines. You can develop your pages on your local machine, and then use your ngrok url to receive webhooks against it. Here is an example:\n\nI have my local site exposed on , so I\u2019m going to add that as my webhook url in the Square Developer Portal.\n\nWhen I get a webhook request, not only can I see it in the ngrok terminal window, I can also use web interface to see exactly what the request I received was, as well as what my app\u2019s response was. Knowing how your application responds can be SO USEFUL when testing something like webhooks where you don\u2019t see the requests directly.\n\nYou can also replay the request right from this dashboard if your response wasn\u2019t quite right and you want to tweak your code.\n\nngrok is a great tool for developing web Point of Sale API integrations (or any kind of multi-device website testing) and is especially useful for developing integrations with webhooks. I hope that you find it as useful as I do!"
    },
    {
        "url": "https://medium.com/square-corner-blog/debugging-a-series-of-miscalculations-in-osquery-14cce2cfe39c",
        "title": "Debugging a series of miscalculations in osquery \u2013 Square Corner Blog \u2013",
        "text": "osquery is an an open source tool by Facebook that provides a SQL interface for system information.\n\nOver the course of the summer I\u2019ve been working on integrating osquery results into our internal asset tracker for automation/validation purposes. It\u2019s been incredibly useful for monitoring various stats, and the SQL interface makes it easy to use.\n\nI\u2019d been collecting data for about 2 weeks, when I changed the UI to showing human-readable numbers, and noticed the total disk size for a host was 0.58 GB (585871964 bytes). It seemed illogical to have a disk that small, so I looked up the actual disk size using on that host, and it was 300 GB (299966445568 bytes.)\n\nIt turns out yields the correct size. But what is 512, anyway?\n\nThe basic data primitive in computing storage is the byte\u2013 however, many storage devices perform I/O operations in larger units (the block size) for efficiency reasons. It\u2019s historically 512 bytes (but it can differ) because of the physical notion of a sector on a disk.\n\nTurns out, despite the documentation stating this was returning in bytes, it was actually returning by block size; in this case 512. \n\nI hard coded it into my code and decided to submit a patch to osquery to fix the issue and also add a column to the table for device .\n\nWhen I went to fix the Linux implementation and add the column, I saw that there was also a Darwin implementation. Curious to see if the issue was persistent, I ran it on my mac, and found that it was also inaccurate. I have a 256GB model.\n\nMy SSD is 1892089856 bytes? Or ? (I wish I had almost a terabyte). Psych, it\u2019s gotta be something else! \n\nTook a look at the Darwin implementation, and found that it was using a bunch of IOKit stuff I had no idea about. I asked around for help, and we eventually found the Apple open source header for the method we needed to dissect. (I learned in C you can have opaque structs with hidden definitions)\n\nIt was difficult to figure out what it was doing at the system level, but the documentation pointed me to a dictionary that contained something for the key . Found some code on StackOverflow to print this dictionary, and turns out it was indeed returning the correct number. That\u2019s when I became suspicious of a truncation error.\n\nThe first dictionary printed, which would be . The line with size:\n\nConverted it to a 32 bit int in ruby, and it confirmed that was the issue.\n\nI saw that there were checks for what type of it was, but for some reason it wasn\u2019t getting caught. Apparently it was hard coded it as a somewhere in a series of nested method calls. I changed it to actually use the of the number.\n\nAfter fixing the Darwin implementation, I decided to return that in block sizes as well, to maintain consistency with the Linux implementation. Made this choice because Linux was already providing in block size, while the Darwin implementation was meaningless. Added the column to both tables.\n\nSee the PR / Issue on GitHub"
    },
    {
        "url": "https://medium.com/square-corner-blog/live-from-atlanta-code-camp-2017-e2b6408ff279",
        "title": "Live from Atlanta: Square Code Camp 2017 \u2013 Square Corner Blog \u2013",
        "text": "We launched Code Camp in 2012 to inspire, educate, and empower the next generation of women in technology. Today we\u2019re kicking off the inaugural Code Camp Atlanta, and we\u2019re excited to show off a Square office that\u2019s integral to our robust engineering team.\n\nOur 13 stellar participants are joining us from universities near and far: from USC to Harvard to Spelman to UT Austin. They\u2019re in for a jam-packed week full of coding workshops, networking opportunities, and leadership sessions, including with our CEO, Jack Dorsey. I\u2019m incredibly excited to head down there this week and spend time with this cohort, and am greatly looking forward to our lunch and hackathon on Friday.\n\nIn addition to working closely with Square engineers to learn, build, and develop coding skills, Code Campers will have the opportunity to visit three other Atlanta tech companies: Google Fiber, CallRail, and FullStory.\n\nWe are committed to preparing these women to forge their paths in engineering, wherever their careers take them. Over the years, Code Camp has fostered this incredible network of women who learn and grow together far beyond the five day program. They leave with skills and relationships that last a lifetime.\n\nStay tuned for a Code Camp wrap-up next week, including insights from attendees and a behind-the-scenes look at this year\u2019s group Hackathon!"
    },
    {
        "url": "https://medium.com/square-corner-blog/sandbox-dashboard-part-3-customers-d8ea3371e03f",
        "title": "Sandbox Dashboard part 3: Customers \u2013 Square Corner Blog \u2013",
        "text": "This is the third post in our series illustrating how to make a dashboard for your sandbox developer account. For more context, take a peek at Part 1 and Part 2.\n\nIn previous posts, we added the ability to view location information as well as a full record of all transactions. For the third installment of our Sandbox Dashboard series, we\u2019ll add functionality to display and interact with our sandbox customers. Let\u2019s take a look at where we ended up last time:\n\nFirst things first, we\u2019re going to add a new item in the navigation bar so that we can access the customers page, as well as create a new php file for all the code. Now our navbar looks like this:\n\nTo display the customers, we can take a similar approach to the transaction page by using a table of all customers and their associated information. To do this, we can call the endpoint and iterate though the array of customers that gets returned.\n\nNow we have a new page listing all sandbox customers as well as any other contact information customers choose to provide: an email address or phone number, as well as a link to delete the customer. One of the most important uses of a dashboard or any GUI is to give you access to run commands in a way that is usually a little easier than running them from the command line.\n\nWhenever you hit the \u201cdelete\u201d button, it calls a javascript function that initiates an request to a PHP script that does the deleting and sends the of the customer to be deleted as a variable. It also creates a dialog to confirm the deletion, as well as some logging to the developer console if something went wrong.\n\nThe PHP script that actually deletes a customer is very simple. It does a couple error checks, and then uses Square\u2019s PHP SDK to call the method on the supplied .\n\nThat sums it up for our sandbox dashboard customer\u2019s tab! You can see the work in progress for this example on GitHub, and feel free to add any comments if you have feedback or suggestions.\n\nIf you think you might be interested in working on projects like these, take a look at our open positions on Square.com/Careers. Our developer\u2019s team is hiring!"
    },
    {
        "url": "https://medium.com/square-corner-blog/deep-dive-on-ember-events-cf684fd3b808",
        "title": "Deep Dive on Ember Events \u2013 Square Corner Blog \u2013",
        "text": "A few days ago, I was working on a really exciting new feature. As part of rolling out these changes, I implemented an onboarding tour \u2014 a sequence of tooltips that teach users how to interact with the different parts.\n\nI made one small change to a template in one of our Ember apps, and everything broke. Can you guess what happened?\n\nBefore: Clicking on \u201cNext Step of the Tour\u201d opened The Thing.\n\nAfter: Clicking on \u201cNext Step of the Tour\u201d opened The Thing and immediately closed it \u2014 so fast that it looked like nothing happened.\n\nThe Only Change: became\n\nSo off I went to the debugging races, and with a little bit of work came to the conclusion that despite their visual similarity, and represent completely different ways of listening for clicks.\n\nUsing listens for DOM events that the browser sends directly, whereas listens for actions fired by Ember in response to browser events. I could tell these two types of event listeners had subtly different behavior, but couldn\u2019t yet articulate what or why.\n\nI realized that I was just scratching the surface of how events are handled in Ember and there was a lot more to learn.\n\nI spent the next three days staring at my computer screen, muttering on my walks home from work, and dancing in my kitchen when I finally managed to get a comprehensive understanding of how DOM events and Ember actions fit together.\n\nThis is what I\u2019ve learned!"
    },
    {
        "url": "https://medium.com/square-corner-blog/a-peek-into-machine-learning-at-square-bdd75b272e23",
        "title": "A Peek into Machine Learning at Square \u2013 Square Corner Blog \u2013",
        "text": "We\u2019ll be posting more about specific projects, methods, and how we use machine learning in our day-to-day work \u2014 stay tuned! For now, here\u2019s a glimpse into machine learning at Square.\n\nSquare has a large, unique data set that covers businesses across the U.S., Canada, Australia, Japan, and the UK. Machine learning is a key way we are taking a data-driven approach to automating both our internal tools and customer-facing experiences.\n\nSquare takes a new approach to supporting small business owners by starting from a position of trust. This is possible because of the strength of our machine learning models, which are at the core of our business and enable us to onboard individuals who might otherwise have been excluded from the economy. More than 90% of sellers self-onboard and are accepted \u2014 more than double what\u2019s typical in the industry.\n\nHow do we decide when to use machine learning?\n\nOur teams typically go through a development workflow similar to the following:\n\nWe have implemented machine learning in most areas of our business including risk management, marketing, sales, analytics, and customer support; and we are continuing to find more ways to integrate it. Below are some examples of where we are using machine learning to scale our business and improve customer experience.\n\nBack when Square was founded, as a brand-new company, we had very little data to work with so we started with simple heuristics, similar to a basic SQL query, plus manual review. Over time, with more data and experience, we were able to build out machine learning models that accurately detect risk and fraud automatically. This has allowed Square to be more precise, efficient, and scalable as we push for best-in-class risk management without negatively impacting customer experience. We\u2019ve invested heavily in using machine learning to minimize the amount of manual work that is required to assess risk. This is something we track very closely and over the past six months, we have been able to reduce this manual work by 40%.\n\nOur real-time data-driven understanding of our sellers helps us reduce the risks associated with business lending and credit lines. Because we can look into a seller\u2019s payment history, inventory movements, and hiring practices, we can quickly and easily determine who qualifies for a small business loan based on their ability to repay. We\u2019ve been able to grow Square Capital loan volume 68% year-over-year, with an average loan size of $6,000. We\u2019re not competing with financial institutions, but rather creating a new market where previously the only other option for a seller was asking for a loan from friends or family.\n\nPredicting best product fit for sellers\u2019 needs\n\nWe can also utilize Square data and machine learning to help our sellers find the best Square products for their business. We have a data science team devoted to using domain knowledge and feature engineering to predict a seller\u2019s likelihood to use any given suite of Square product offerings, such as Invoices, Appointments, Gift Cards, Instant Deposit, Customer Engagement, Point of Sale, Employee Management, Loyalty, and Payroll. This prediction allows us to surface Square products to the right sellers at the right time \u2014 streamlining a seller\u2019s search and potentially bringing new products to their attention that could help them grow their business.\n\nSquare\u2019s customer support team is constantly innovating on support tactics to provide fast, efficient answers to our sellers\u2019 questions. Using machine learning models, we are able to draw connections and make recommendations dynamically and in a personalized way. We predict likely issues a customer is having, allowing us to proactively resolve their issues. This method intelligently balances both technical and human solutions to customer success \u2014 and gives them time back so they can focus on running their businesses.\n\nWork on these machine learning challenges and more with us! square.com/careers/data"
    },
    {
        "url": "https://medium.com/square-corner-blog/the-coffee-shop-kiosk-68454b20ec53",
        "title": "The Coffee Shop Kiosk \u2013 Square Corner Blog \u2013",
        "text": "When I joined the Developers team at Square, the first thing I did was build an application that uses our APIs. Every new engineer on the team does this (you can read about my colleague\u2019s example Android app here) so that we can walk in our customers\u2019 shoes. It\u2019s become a great tool to reinforce feedback from developers when building real world applications. For my project, I chose to build a customer-facing kiosk application as we\u2019re seeing more and more demand for this type of custom application. For the application I built I used our Point of Sale API, which leverages the iOS app-switching API to enable apps to accept in-person payments using the Square Point of Sale app. We\u2019ve also provided a Point of Sale SDK, which is a thin wrapper around our API that makes it incredibly fast and easy to get up and running.\n\nThe Coffee Shop is a customer-facing kiosk app that allows customers to order and pay for their drinks at a fictional coffee shop.\n\nThe advantage of a kiosk is that customers can place their own orders, leaving baristas free to focus on making drinks and engaging with their customers instead of running the register. Because this is a customer-facing app, the interface needs to be both simple and fast. Customers should be able to easily choose a drink, customize it, and pay.\n\nThe Drink model defines some basic properties like name and description, as well as an array of DrinkOptionGroups.\n\nWhen you buy a drink at a coffee shop you usually have a few choices to make:\n\nDrinkOptionGroup is a simple struct that can be used to model each of these use cases:\n\nIt defines a group of related options which can be applied to a drink. As an example, here\u2019s a DrinkOptionGroup which defines the sizes that are available for a drink:\n\nI\u2019ve added a few different sizes at various price points (in cents) and specified that the customer must choose one and only one option. You can also define a DrinkOptionGroup where multiple options can be selected:\n\nTo bring it all together, here\u2019s the drink definition for coffee:\n\nIn this sample app several drinks are defined statically in a configuration file, but you could also imagine them being pulled from a server.\n\nThe app has two screens which are managed by the AppFlowController class. AppFlowController is a UIViewController subclass which is responsible for displaying and transitioning between screens in the app. It\u2019s instantiated with an array of Drinks and displays them by showing the SelectDrinkViewController:\n\nOnce the customer selects a drink, the CustomizeDrinkViewController is presented with the various options that can be used to customize it:\n\nFinally, when the customer taps the checkout button, AppFlowController kicks off a Point of Sale API request which transitions the customer into the Square Point of Sale app so they can pay for their drink. Once the customer completes their payment, the Square Point of Sale app automatically switches back to The Coffee Shop to take the next customer\u2019s order.\n\nProduction apps are often much more complex than this simple example. By using Square\u2019s APIs you don\u2019t need to understand or worry about the complexity of accepting payments. Instead, you can focus on building a great experience for your customers.\n\nAll of the code for this app is available on Github and documentation is available on our developer portal. Feel free to clone the repo and try building a kiosk like The Coffee Shop for yourself!\n\nWe believe that commerce is changing, and that change comes with big, new problems to solve. Come join us at Square as we build a commerce platform that helps developers empower sellers to start, run and grow their businesses."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-womeng-hear-now-tech-talks-july-2017-4a16ad672cc7",
        "title": "Square WomEng Hear + Now Tech Talks: July 2017 \u2013 Square Corner Blog \u2013",
        "text": "Lightning talks on the theme \u201cNerd Out Loud.\u201d\n\nWe just wrapped up another edition of Hear + Now, Square\u2019s quarterly series of lightning tech talks that we offer to marginalized groups as a non-intimidating space to present.\n\nAfter one of our WomEng members attended AlterConf, we decided to open up the speakers to anyone who identifies as belonging to a marginalized group (and wrote a blog post explaining these changes).\n\nOur latest event featured six speakers: three technical, two on engineering practices, and one diversity talk.\n\nSarah Harvey, who works on Square\u2019s security team, started the event off with her talk, \u201cMaking Use of Visualization to Understand Traffic/Data Flow.\u201d\n\nSukhada Gholba, from keereo, spoke about what she learned from starting her own company.\n\nNatalie Judd joined us from Oregon-based Connective DX to discuss hosting productive post mortems.\n\nIn \u201cAll the Things I Didn\u2019t Know About Big Data,\u201d Alivia Blount, who\u2019s currently studying computer science at UC Davis, talked about solving a big data problem at her previous internship.\n\nTo close out the night, Matthew Jacobs from Square shared his experiences transitioning while working in the tech industry.\n\nIt was a successful and educational night for all in attendance \u2014 one of our speakers even brought her daughter, who we were told had a blast attending. It was great showcasing smart and accomplished voices in tech to all ages.\n\nTo speak at, or attend, our next Hear + Now event, be sure to follow SquareEng on Twitter."
    },
    {
        "url": "https://medium.com/square-corner-blog/how-square-makes-its-sdks-6a0fd7ea4b2d",
        "title": "How Square makes its SDKs \u2013 Square Corner Blog \u2013",
        "text": "The developer platform team at Square is a little different than most. We don\u2019t focus solely on creating the APIs that our developers use, we focus on exposing the internal APIs that our first-party products use to create the best experience for developers. That way, our external developers get to use the same source of truth for all of our shared customer\u2019s information.\n\nAn important factor in why we make our SDKs this way is the structure of Square\u2019s product teams internally. Like many companies, Square operates a variety of different services to run different parts of the customer experience, each with a dedicated team to make sure everything is working properly while adding new features. Our developer platform team works to expose these internal services to the outside world, while handling things that are important to our third-party developers like authorization, rate limiting, and more. Our team works hard to enable other teams within Square to quickly and easily expose their APIs to the world, and having to write an SDK for a new feature release shouldn\u2019t act as a bottleneck.\n\nInstead of writing each of our SDKs by hand (which would not only be time consuming, but slow down the release of new features into the SDKs) we use a process that relies heavily on SDK generation. It all starts with our specification.\n\nWe use the OpenAPI (formerly known as Swagger) standard to define our APIs. This is just a (or ) file that defines the urls of our API endpoints, what kind of HTTP request to make, as well as what kind of information to provide, or expect to get back. Our specification is made up of 3 main parts: the general info/metadata, the paths, and the models.\n\nThis contains some of the descriptive information for the API overall, like where you can find licensing information, or who to contact for help.\n\nThese describe the individual endpoints (or URL paths) for the API. It describes what kind of request to make, how it should be authorized, and what kind of information you should add to the request, and what you should expect to get back. In the example below, you can see that it is a request, There are a couple required parameters in the URL, another one in the body, and you get back a .\n\nThe models describe the different objects that the API interacts with. They are used primarily for serializing the JSON response from the API into native objects for each language. In this one, , you can see it has a couple other models that it is comprised of, as well as a description and even an example of what the response looks like.\n\nYou can see the most recent version of our specification to date version in the Connect-API-Specification repo on GitHub.\n\nThe specification is an important part of our generation process, as it is the source of truth about how our APIs work. When other teams want to expand their APIs, release new APIs, or just increase the clarity of a model description, they can make a edit to this single file and have their changes propagate to all of the client SDKs. We actually generate most of our specification from the files that describe the internal service to service communication for even more process automation.\n\nNow that we have the specification for our APIs ready to go, how do we turn it into a client facing SDK? The answer is swagger-codegen. Swagger-codgen is an open source project that applies your Open API specification to a series of templates for SDKs in different languages. Because the swagger-codegen project is so active, we actually check in a copy of our template files for each of our supported SDKS, and pin to specific swagger-codegen versions to make sure that we don\u2019t accidentally push breaking changes to our users as a result of all the automation. You can see the templates that power the {Java, PHP, C#, Python, Ruby} SDKs in the same repository as our specification file: Connect-API-Specification. Generating SDKs for all of these languages, however was still a long manual process.\n\nThat isn\u2019t good enough for when multiple internal teams are releasing updates and making changes concurrently. Luckily, Travis CI can help us out. In addition the the Connect-API-Specification repository, we also have separate repositories for each of our supported SDK languages (like https://github.com/square/connect-php-sdk for PHP). We\u2019ve set up Travis CI for the Connect-API-Specification repository to automatically build the SDKs using the specification file and templates that we have checked in. We first install swagger-codegen to the travis machine, generate the SDKs, and then use an encrypted set of keys to push those new SDKs to the appropriate GitHub repository. The best part of using Travis CI is that all of it is public \u2014 you can see exactly what goes on in our travis.yml file and associated folder of scripts. Don\u2019t hesitate to submit a PR if you think there is something we could be doing better!\n\nHope your enjoyed the look into our SDK generation process. You can also see a recorded talk I gave at DevRelCon about the subject here. If you want to learn more about our SDKs, or other technical aspects of Square, be sure to follow on this blog and our Twitter account!"
    },
    {
        "url": "https://medium.com/square-corner-blog/recurring-charges-with-php-and-card-on-file-60f2bcb9aeac",
        "title": "Recurring charges with PHP and Card on File \u2013 Square Corner Blog \u2013",
        "text": "Whether you have a subscription service, or regular clients that you want to charge repeatedly, you can use Square\u2019s e-commerce APIs and Card on File.\n\nKeeping a customer\u2019s card on file is almost identical to charging it with Square\u2019s e-commerce APIs. The key difference is that instead of submitting a card nonce to a the endpoint, you attach it to a customer and then submit that customer\u2019s attached card to be charged.\n\nNow let\u2019s dive into what this looks like from a code perspective. First, end users will need to input their card information into the SqPaymentForm. This is required the first time you store a user\u2019s credit card; you cannot send us a previously recorded credit card number (or PAN) and attach it to a customer. For the most part we\u2019ll copy & paste the sample form from the documentation site, with one important addition. We also must add a checkbox notifying the buyer that their card will be stored for future purchases. As noted in our documentation, linking cards without obtaining permission from the buyer may result in your app being disabled.\n\nNext, we\u2019ll add inputs to specify information about the customer that we want to attach the card to. In a production situation where you would be charging recurring payments, or subscription billing, you would likely want your customers to log in to your app or website and use the information from their logged in state to identify them as a specific customer. Since this is an example, I added a first and last name when the form is filled out, and create a new customer to attach the card on file for each time.\n\nWith the modified card form up and running, we can start writing the code to process the payment. The form on our webpage will generate a card nonce, and then post it along with the customer information to a different php script . This page will use Square\u2019s PHP SDK to create a new customer, use the card nonce to attach the card to the customer for future subscription billing charges, and then charge the card. Lets dive in:\n\nSince we are using the PHP SDK, most of the hard work for creating a customer is hidden inside some useful functions. All we need to do is to pass the details of the customer (in our case a first and last name) to the method and voila \u2014 we\u2019ve created a customer.\n\nWith our customer in place, the next step is to add the card (represented by the generated nonce) to the customer. Again, the PHP SDK will be doing much of the heavy lifting:\n\nNow that we have attached the card to the customer, we can use that information to make the charge. This is just like making the charge with directly with a card nonce, but instead of the card nonce we will be providing the endpoint with the and that we want to charge:\n\nta-da! That\u2019s all there is to charging a card on file. At the beginning of the month, or next time you want to charge a recurring fee, you only need to do the last step and charge the customer (since the customer, and their card, already exist).\n\nIf you want to learn more about recurring payments with Square, check out the official documentation, as well as the API reference for the relevant endpoints. As always, follow this blog and @SquareDev on twitter to keep up to date with new product releases and more tips & tricks."
    },
    {
        "url": "https://medium.com/square-corner-blog/surfacing-hidden-change-to-pull-requests-6a371266e479",
        "title": "Surfacing Hidden Change to Pull Requests \u2013 Square Corner Blog \u2013",
        "text": "In code review we focus on the visible because that is what is presented to us in the diff.\n\nProgramming frequently deals in visible changes: the logic in your code, the dependencies you declare, the API you expose. There is, however, quite a bit of associated hidden change: transitive dependencies, generated code, and manifest files.\n\nIt\u2019s equally as important to pay attention to the hidden changes during code review. Transitive dependency changes or generated code changes might bloat the resulting binary or reduce performance. Manifest changes might cause an incompatibility downstream. These problems are often found much later in the release process where an investigation is required to find the cause.\n\nAfter running into a few problems with hidden changes on the Cash Android team, we decided to promote them to be more visible at the point where they change: the pull request.\n\nIn every CI build, we calculate interesting pieces of data like the binary size and method count (an important metric for Android apps) and write them to shared storage. When a CI build runs for a pull request, we figure out the difference of these values from the ancestor commit on master. The numbers are sent back to the pull request in the form of a comment to ensure the author and any reviewers are notified.\n\nThese important metrics are now surfaced explicitly to ensure that the result is what you intended. Does that new dependency only add 10 methods or do we incur 10,000? Does that new hero image add only 20KB or occupy 2MB?"
    },
    {
        "url": "https://medium.com/square-corner-blog/always-be-closing-3d5fda0e00da",
        "title": "Always Be Closing \u2013 Square Corner Blog \u2013",
        "text": "This is the story of a resource leak in one of Square\u2019s Go services and the process of uncovering the root cause. Facing spiky and mysterious system metrics, we used a variety of standard and homemade tools to uncover a subtle bug in one of our core internal frameworks.\n\nEarlier this year, one of our Go services began using significantly more memory, breaking out of a steady state of flat memory use and predictable garbage collection patterns. At the same time, CPU use rose as well. This particular service has generally used around 200 MB of memory, but this figure soon grew to be several GB.\n\nAfter much investigation, it turned out that for a substantial number of requests, a combination of bugs was causing the service to retain per-request metadata for almost 17 hours instead of a few milliseconds. Ultimately, there were two root causes, both of which were tickled by a seemingly benign configuration bug in another service. While only one service was affected, many of our other Go services were potentially susceptible to this bug.\n\nThis post first explains the bug we encountered then recounts the various steps we took to investigate, some of which were more fruitful than others.\n\nUltimately, the root cause was due to incorrect uses of Go\u2019s type. At Square, we have a custom framework for handling incoming requests and dispatching to handler functions. This framework code was creating s with deadlines but, after the request was completed, not canceling the s to release related resources. This resulted in the lifetime of each object on the heap exceeding that of its related request.\n\nThis root cause was tickled by a configuration change in another service, which inadvertently set its client request timeout to 60,000 seconds instead of the intended 60,000 milliseconds. This client misconfiguration led to each of its requests having their s retained by the buggy server for 60,000 seconds (16.7 hours) instead of a few milliseconds.\n\nIn Go, is an interface used as a general way of passing around immutable metadata for a single request or operation.\n\nA is created with a key-value pair and can be wrapped by another to add or override data. The outer/wrapping/child has access to its wrapped/parent 's data but not vice versa.\n\nHere\u2019s a quick example of how 's are created:\n\nThe relationships between the s in the example look like this:\n\nIn addition to storing key-value data, 's may also be created with associated wall-clock deadlines. When a with a deadline is created, it registers a callback function that marks the context as canceled after the deadline passes. This callback function is scheduled by Go runtime internals.\n\nCritically, the timer\u2019s callback function keeps a reference to the to be canceled. If application code does not explicitly cancel the , the runtime timer remains set and the remains in memory until the timer fires.\n\nThis was our core issue: some of our framework code was not explicitly canceling the 's it was creating and passing to handler code.\n\nTo illustrate, framework code was essentially doing this:\n\nThe correct way to use 's is to always cancel them when the the related operation \u2014 the request handler, in our case \u2014 completes. As a rule of thumb, any code that creates a new should ensure that it is canceled.\n\nGiven the buggy snippet above, the actual code fix is simple:\n\nAlways rollback when possible. This was less a lesson learned than a maxim ignored. When we started investigating this issue, one of the first things we did was look for other changes that were happening around the same time that the performance degraded. While a few interesting changes had occurred within a few days, the change to the client timeout configuration had a very good correlation in terms of timing. Despite several engineers reviewing the configuration changes as part of its initial review and again during this investigation, everyone dismissed them as irrelevant (\u201cwe\u2019re just bumping the timeout a little bit \u2014 no big deal!\u201d).\n\nGiven the close timing between the change and the incident, as well as the very low cost of rolling back the client configuration change, we should have immediately rolled back the timeout change. This would not have fixed the root cause but would have helped pinpoint where to investigate further.\n\nGaps in Go tooling made investigating difficult. Debugging this issue was hampered by a notable gap in Go tooling that we would not have faced were our code running on the JVM. All signs pointed to this issue being a fairly classic memory leak but Go does not really have tooling to dump and analyze heaps. While the pprof tool can be useful to find some memory issues, and did give some useful hints in this case, it is limited: it gives statistics on where objects are allocated but not what is retaining them.\n\nThe remainder of this post goes through the investigation in roughly chronological order, with some commentary around observations that were, in hindsight, wrong or misleading.\n\nThe general progression went as follows.\n\nThe fourth and fifth steps required the most hacking but were ultimately the most fruitful.\n\nFirst, we made a list of observed changes:\n\nWe also made a list of what didn\u2019t change:\n\nSince we saw alarming GC patterns, we decided start by looking through GC logs to find any odd behavior. The main takeaway from reading these logs was that despite our heap continually growing, garbage collection was not taking an outsize amount of time; each GC run was short and released only small amounts of memory.\n\nAside from that, this step was a useful exercise in better understanding Go\u2019s GC log format, so let\u2019s take a look at a snippet of what we encountered:\n\nHere\u2019s how to interpret the components of the first line:\n\nThere were also some additional logs from the garbage collector\u2019s scavenger:\n\nThe scavenger releases unused memory back to the system, truly freeing memory.\n\nWith evidence of a memory leak, we decided to use pprof to narrow down which code was leaking memory. Pprof is a standard Go tool for dumping, exploring, and visualizing statistics about memory and CPU use of Go apps. With pprof, you can either analyze a single dump or the diff between two different dumps. That latter is useful for finding changes after an application is running in a steady state.\n\nOur Go applications have internal HTTP endpoints to retrieve data for pprof, so we used that to get a dump of a running process:\n\nOnce we had a dump, we were able to use pprof to show which methods were allocating the most data. Here\u2019s one of the commands we ran, which displays the functions allocating the most unreleased memory.\n\nEach of these lines indicates how much memory was used by objects created inside a specific function. It\u2019s important to note, though, that this only indicates where objects were created, not what is still retaining references to them.\n\nLooking closer, the method that handles all incoming requests is responsible for lots of retained objects, at least indirectly:\n\nThis line indicates that created 128 MB (6.82% of the heap) of objects that are still reachable. Furthermore, plus methods reachable from it are holding on to 1.4 GB of objects. In a normal state, both of these numbers would be a few dozen MB at most.\n\nWhen we first saw this data, we correctly assumed that request handling code was leaking objects. We also noticed that despite lots of objects being retained, the request messages were not being retained. However, we still did not understand the root cause at this point. In hindsight, we could also have noted that 94 MB of memory allocated inside is unusual and explored that further.\n\nAt this point, we decided to go off the beaten path.\n\nWhile pprof gave us hints that request data was leaking, it didn\u2019t pinpoint the cause of the leak and what was holding on to them. Many languages have tools to dump an application\u2019s heap and analyze and traverse the object graph \u2014 if this were a Java app, we would have happily fired up the Eclipse Memory Analyzer, for example. Unfortunately, Go is not one of those languages.\n\nRich heap dump support is incredibly useful. It makes it much easier to find why an object is not being garbage collected by traversing the path from a retained object to a root object. In this case, we wanted to find a data structure holding on to tons of request-related data and I suspect having a real heap analyzer tool would have drastically cut down the time it took us to debug.\n\nWhile Go doesn\u2019t have support for exploring heaps, it does have an unofficial method in the package for generating something approximately like a heap dump: . This function outputs some low-level heap state of a running process in binary format. The Golang wiki has incomplete documentation of the format so we ended up having to read the source code in order to understand it.\n\nThere is an enormous downside to the output of : it essentially dumps the raw bytes of each object along with a bit set indicating which words within those bytes represent pointers to other objects. It omits any data about the type, including the type\u2019s name.\n\nTraversing an object graph without types was a tedious at best and impossible in most cases. We found that finding obvious strings and working backwards through referring objects was the only way to infer object types. Even this required knowing how Go represents core data structures, like slices and maps, in memory.\n\nHere\u2019s what a dumped object looks like. Each field line below represents a word of memory, and some link to other objects when a word represents a pointer.\n\nThe image above is from an open source project, heapdump14, which was written circa Go 1.4. Older Go runtimes included more detailed type information in the heap but the amount of available metadata has been reduced as the runtime has evolved in newer versions of Go. We decided to try and use this tool to better understand a heap object graph; getting it running required applying this patch to the tool\u2019s source and then writing an additional patch of our own.\n\nAfter traversing the graph as viewed through heapdump14, we determined that the object in the image above was an instance of a type that looks like this:\n\n\u201c64_PPSSP\u201d in the image is heapdump14\u2019s name for the type. The number is randomly chosen by the tool but is unique to the type. indicates the types of each word in the object: [pointer, pointer, scalar, scalar, pointer]. Per the image above, the object is 7 words, where each field row represents a word. This is not the 5 suggested by ; it\u2019s unclear how this name was formed or why the dumped type has 7 words instead of the expected 3 (one for the pointer and two for the slices).\n\nWhile traversing the heap was ultimately a failure, the effort was not a total bust. We noticed that strings in the heap were useful hints of some types being retained \u2014 especially useful in our case, since requests to the server contain lots of strings.\n\nWith this in mind, we wrote a simple tool to extract counts of common strings in the heap. It also counts strings in the distinct format that Square\u2019s RPC framework uses to represent request IDs.\n\nThe following sample of the output indicates that much of the retained data was related to requests from a single service \u2014 the one with the misconfigured timeout.\n\nThe first line of output show the count of request IDs in the heap \u2014 indicating how many requests have some related data still in memory.\n\nThe remaining lines first output a count, then if the count is for a string encoded in a protocol buffer message, and finally the string itself, surrounded by backticks. For example, , means that the protocol buffer string appears in the heap 995,090 times.\n\nIn hindsight, we really should have rolled back the offending calling service at this point, but unfortunately we did not.\n\nAt this point, the mystery we were intent on solving was why only requests from a specific service were causing leaks. We used some hacky Go runtime tricks to help and finally figured it all out.\n\nTo determine characteristics of specific objects being retained, we turned to Go\u2019s function. This function instructs the Go runtime to execute a callback immediately prior to garbage collecting an object. Setting up the callback is easy, although I would not recommend using it in real production code:\n\nWith in mind, we set finalizers for a small set of types and had our server occasionally log counts of each types created and garbage collected. The code to track s looked roughly like this:\n\nThe output of this logging unambiguously showed that some s were not being GCed but that request objects were. The counts of outstanding s kept rising. In the snippet below, of the 162,573 s allocated during the life of the process, 4,920 remained in memory.\n\nBy this point, we knew that s were leaking and where those s were being created. We reread the documentation and scanned through its source code to get a better understanding of how it is implemented. This led us to seeing its use of and how that interacts with the runtime. We then formed a hypothesis that these timer callbacks were leaking. This was soon confirmed by a more thorough reading of the source code of our server framework.\n\nIn the end, the fix was an incredibly simple change to our request handler framework code:\n\nThis fixed the issue we witnessed in our misbehaving service. Since the framework is shared by other internal Go apps, the fix guards them against misbehaving clients as well.\n\nLooking back on the whole ordeal reiterated the importance of a few key engineering practices.\n\nOne is that Go\u2019s conventions around not ignoring returned errors and cleanup functions exist for good reason and should not be casually ignored. The original framework code should not have ignored the associated with s given that the documentation clearly states that it releases resources. I would argue that any code swallowing cleanup and error return values is exceptional and should have a clearly documented rationale.\n\nAnother practice is to not hesitate to roll back code when it closely correlates to an issue in a related system. The cost to do so is very low but may yield the benefit of helping narrow down triggers of a bug. In this particular case, we would have saved significant time debugging.\n\nFinally, debugging travails aside, we enjoyed learning more about various parts of Go\u2019s internals and the tooling ecosystem around them."
    },
    {
        "url": "https://medium.com/square-corner-blog/pricing-subscription-products-with-a-data-driven-conscience-f85ae11613dc",
        "title": "Pricing Subscription Products with a Data-Driven Conscience",
        "text": "At Square, Product Analytics is embedded within full-stack product teams, which allows each of us to become a data expert for our respective products. Our high-visibility position(s) within each product allows us to drive the decision-making process with the entire analytics toolkit: consulting, plumbing, analysis, science and evangelism. Here follows a canonical example: pricing products judiciously and optimally.\n\nThe first incarnation of Square Loyalty, which enabled Square sellers to offer digital rewards programs (e.g. digitized punch cards) to their customers, was released in 2013. This post details the data-driven steps we took to provide our sellers a fairly-priced, high-value product that can help them run and grow their businesses.\n\nWhen launching a product, our main goal at the outset is to drive adoption. With this in mind, we launched Square Loyalty in 2013 with very basic features (a.k.a. minimum viable product or MVP) at no cost to our sellers to encourage everyone to try it. We tracked Square Loyalty\u2019s metrics around how many sellers were using it (acquisition), how often (engagement) and for how many months/years (retention), and how much value they got from it (product-market fit).\n\nAs we iterated on and improved Square Loyalty, we saw our sellers gaining more value in their own businesses from using it, which increased seller engagement and retention. We knew we had a solid product \u2014 one that would benefit from having more resources (e.g., engineering and product management time) behind it to drive significant improvements for our sellers.\n\nFor example, by studying the data we logged and listening to customer feedback, we found, to our surprise, that Square Loyalty was too automated and thus easy to miss during the checkout experience. It was built into the bottom of receipts, but perhaps so smoothly that buyers were unaware that they had rewards to work toward and redeem. We owed it to our sellers to place a smarter, more useful tool into their point of sale. To that end, we doubled down and reimagined the product experience.\n\nThe result was the relaunch of Square Loyalty in June 2016. The rewards program was decoupled from receipts, increasing buyer awareness. Further, switching to a pure SMS-based system permitted a more seamless experience (provide your phone number instead of looking up your coupon code). Following up the SMS notifications with email-collection tools ensured existing sellers could continue to de-anonymize, identify, engage and reward their top customers.\n\nAt this point, we also knew that charging for this product could help us generate the revenue needed to support the work of building product features and addressing seller feedback. Why? Charging for a product increases overall happiness with the product if it means increased resources for marketing, product improvements, and most importantly, prioritizing and addressing customer feedback. But what was the best price for this product to ensure our sellers got the highest possible return on their investment?\n\nAt Square, our suite of seller tools gives us unique access to what we call the \u201cclosed loop\u201d of data on payments attribution: every transaction at a Square seller passes through our credit card readers or payment APIs, which enables us to determine the effectiveness of Square Loyalty digital rewards programs (and other marketing campaigns run by our Square sellers) in a very precise fashion. We can determine how many buyers return to a business because of the loyalty program versus those who would have come back anyway. With our closed loop, there are fewer proxies and less guessing, more knowing.\n\nBefore determining how much to charge, we first had to assess and measure the incremental value that sellers get from a digital rewards program like Square Loyalty. The usual answer to everything \u2014 run an A/B test \u2014 was a non-starter because it would interfere with our sellers\u2019 business and potentially create confusion. Instead, we got creative and defined a natural experiment to compare buyers who were aware of and redeemed their Square Loyalty rewards against buyers who were not aware of and did not redeem their rewards.\n\nWith that distinction made, it was simply a measurement of metrics for both buyer groups. Buyers who were \u2018aware\u2019 and enrolled in Square Loyalty returned at least twice as often as those who were \u2018unaware.\u2019 While there was a drop in payment amount due to the reward-redemption discount, the former effect of buyers coming back twice as often was much larger. Even before overhauling the product with major improvements, we could see this was already a seller tool worth paying for.\n\nAnother key assumption was deciding whether the incremental lift between the two buyer groups could be generalized to all buyers, all buyers enrolled in Square Loyalty, or all buyers who had earned a reward and returned. Since we know the answer lies somewhere in between, we simply tried all of them with best- and worst-case scenarios in mind. But putting that aside for now, we had used our closed loop and opportunistic operational intuition to prove the incremental value that Square Loyalty brings to our sellers.\n\nNow that we knew for sure that our sellers were getting a lot out of Square Loyalty, the next question was how much to charge. We used our closed loop to generate a \u2018many-worlds\u2019 interpretation of our potential price that mimics the retail context. Since we know the value our sellers derive from our products, we can assume that this is their marginal willingness to pay, minus assumptions about gross margin and perceptual differences. We could then construct a price-elasticity curve for our sellers that shows how increases in price result in fewer subscribers. This allowed us to calibrate the price against what we expected to receive in fees.\n\nThe revenue-maximizing price for Square was not the right answer because this is derived from the price elasticity for retention, which assumes that a seller can feel the value the same way we do and actually tries the product long enough to do so. Charging a high price, even if the product is worth that much, is a bad idea if it scares everybody from signing up in the first place. Nevertheless, it was a useful exercise in illustrating the ceiling and trade-offs available to us in making a decision as important and sensitive as pricing.\n\nPricing tests commenced in April 2016, and we were ready with our final decision on pricing by the time Square Loyalty was relaunched in June. The result has been overwhelmingly positive: Square Loyalty has one of the lowest churn rates we\u2019ve seen, and our monitoring of the value sellers get out of the product (once again, through our closed loop) shows that value has increased monotonically since launch and our Square Loyalty subscribers get great ROI.\n\nAre we done yet? Never. Pricing should never be considered a resolved issue. Our aim is always to give every seller a fair price that is equally commensurate with value, yet is still easily interpretable and transparent to all. Simultaneously, we stand behind the proven value of our product, and continue to ship improvements at a furious cadence. We have only tapped the surface of what is possible with Square Loyalty, and we hope you\u2019ll join us for the ride."
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-paralayout-d5ac09e93fb0",
        "title": "Introducing Paralayout \u2013 Square Corner Blog \u2013",
        "text": "You know the feeling: the design spec comes in, polished and perfect, and you know at a glance that you\u2019ve got two ways to go:\n\nI\u2019ll spare you a lengthy exposition of Autolayout, Apple\u2019s constraint-based system for specifying UI via rules rather than code. While technically impressive, in practice we have found that it makes UI take longer to implement, and is much more difficult to review, reuse, debug, modify, animate, and performance-tune. On the Cash iOS team, we have embraced a different philosophy that prioritizes development speed, correctness, readability, maintainability, and performance. We call it Paralayout, it\u2019s 100% Swift, and we\u2019re excited to announce it\u2019s open source!\n\nThe premise is simple: rather than taking over the task of layout, we instead make the task easier. While the Greek root auto means \u201cself,\u201d the root para means \u201chelp\u201d (think paralegal or paramedic). Paralayout is not an abstraction layer; it\u2019s a collection of \u00e0 la carte convenience methods.\n\nConsider this screen in the Cash app, as it appears on an iPhone 7 and iPhone 4:\n\nThe title and two buttons are vertically distributed such that the empty space above and between them is proportionally expanded or compressed on a larger or smaller screen. Our toolset gets this done with one easy-to-read line of code, regardless of screen size:\n\nBut there\u2019s more to this design! The title text wouldn\u2019t look correct with a naive layout: the text has a soft line break in it to look more visually balanced:\n\nParalayout extends UILabel to provide custom \u201ccompact\u201d line wrapping that eliminates the need for manual typesetting. It accomplishes this by performing a binary search to find the narrowest width for the text that does not wrap to a third line. It also provides conveniences to specify attributes like kerning and line spacing without the need for messy code to compose an NSAttributedString and NSParagraphStyle. The result is text that\u2019s much more easily localized, reused, and modified without need for extraneous testing and customization.\n\nWe\u2019re not done! Within the buttons, the margins are fixed constants. The designer chose them based on the text itself, i.e., the baseline and the \u201ccap height\u201d:\n\nAn engineer typically takes those margins and reinterprets them relative to the bounds of the UILabels themselves. If and when the font sizes change, keeping them correct is a huge pain. Paralayout interprets margins relative to the font metrics within each label, so your code exactly matches the numbers in the designer\u2019s head:\n\nLayouts in the Cash app can get even more nuanced. This is a header in your payment activity, which gracefully collapses into a navigation bar as you scroll:\n\nThere are three distinct phases to this gesture-driven transition: the avatar hits a minimum margin, shrinks to half its height as it fades out, and then the name eases into a vertically aligned position while the other elements fade out. Getting all the math right requires calculating numerous boundary dimensions, and interpolating different values cleanly between them. We use a simple value type (a number between 0 and 1 under the hood) to get all the math right, so the code can concern itself only with the parameters:\n\nParalayout also takes care of pixel-level rounding, i.e., snapping views to \u00bd-point boundaries on the iPhone 7 and \u2153-point boundaries on the 7 Plus, as well as highly adaptable and customizable tools for sizing views before laying them out. It also provides an AspectRatio value type to support aspect-fit and aspect-fill calculations (to pixel-level precision, natch).\n\nIt\u2019s been enormously useful to us, so we\u2019ve decided to package it all up and make it open source:\n\nWe hope you like it, and would love to incorporate your ideas and other challenging layout use cases into Paralayout to make it even more\u2026 helpful. Cheers!"
    },
    {
        "url": "https://medium.com/square-corner-blog/no-billing-postal-code-no-problem-4067c3e31840",
        "title": "No Billing Postal Code? No Problem! \u2013 Square Corner Blog \u2013",
        "text": "As Square\u2019s eCommerce platform expands outside the United States and Canada, we\u2019ve heard feedback from developers in other countries (e.g. Japan & Australia), where requesting billing postal codes is uncommon, that they\u2019d like to remove the postal code field in their eCommerce Payment Form.\n\nGood news! Effective today, to remove the postal code field, all you need to do is change one line of code.\n\nWhen you load the eCommerce Payment Form \u2014 replace:\n\nWith the following:\n\nOnce this is done, the payment form will load without the postal code field!\n\nWarning: the postal code field is still required to process payment online for merchants based in the United States, Canada, and United Kingdom. Removing the field will result in these transactions being declined. With the following error message:\n\nIf you are interested in learning more about how to style the payment form, you can read about it here."
    },
    {
        "url": "https://medium.com/square-corner-blog/leadership-management-and-tribes-dd778b778a25",
        "title": "Leadership, Management, and Tribes \u2013 Square Corner Blog \u2013",
        "text": "There are a lot of great books about leadership and management. One is Tribes by Seth Godin, which is a great read for any aspiring leader. Tribes talks a lot about leaders versus managers and, as an Engineering Manager, it got me thinking about my own views on leadership and management. Every organization needs great leaders as well as great managers, but are they the same people? And what role do they play?\n\nGiven that entire books can be written on the characteristics that make leaders or managers great, I\u2019m going to focus on a few key principles that are paramount to successful leaders, as well as key attributes of the best managers.\n\nLeaders are the force that propel their organizations and their industries forward. They constantly come up with new ideas to solve problems, to drive change, to make the team more efficient, to improve products and to move into entirely new problem spaces. While leaders are the catalysts of change, they invariably face opposition by the status quo. Change is hard. The larger the magnitude of a change, the greater the support the leader will need to drive its adoption. How great leaders build that support is worthy of its own post, but it typically involves trust, respect and strong influence strategies.\n\nLeaders are willing to make big bets and hard choices. They are willing to be bold to challenge the status quo. Leaders are willing to disrupt their own businesses before someone else does. They are willing to fail fast and move onwards.\n\nLeadership opportunities come in all shapes and sizes. Even change within a small team can have a dramatic impact on productivity and success. As such, leaders can exist in all levels of an organization, from the board of directors to new grads. In fact, it\u2019s possible to have a team entirely composed of leaders. There are so many different aspects of work, that it\u2019s easy to imagine a world where one person takes the lead on making a task more efficient, and someone else leads an interaction with a partner team, and a third person leads the integration of a new technology. It\u2019s quite natural to be leading one initiative while taking the lead from someone else on a different initiative. A team composed of many leaders, that fosters the best ideas that they collectively generate, will almost always outperform a team headed by a single strong leader who has a tribe of obedient followers.\n\nSo where do managers fit in? Managers often have a broad view of the team\u2019s past and current endeavors. And with that perspective, managers can find synergies across the initiatives of multiple leaders, which can lead to even bigger wins. But great managers can have even more impact than the gains from a broad perspective.\n\nA great manager knows the skills, expertise and ambitions of people on her team and can fit people with projects that can leverage those strengths. A great manager can amplify the leaders that work for her, inspiring them to achieve greater successes.\n\nThe biggest difference, however, is that great managers are thinking about developing their people. Leaders grow ideas; managers grow people. In the past, a manager might have said that her team works for her. In today\u2019s workplace, the relationship is flipped: a great manager works hard for her team. A great manager is there to support, encourage, challenge and find new opportunities for her team in ways that will stretch and grow their capabilities. And the best managers will do it all while advancing the overall goals of their organization.\n\nWhat are more important \u2014 great leaders or great managers? The truth is, both are fundamental to an organization. Leadership is key for driving the changes that are crucial for future successes. Management is key to building a team where each person is empowered, challenged, happy, and developing into a more effective leader.\n\nAre an organization\u2019s managers also leaders? In my experience, usually. But every thriving organization will also have leaders who are not managers, each who brings a unique perspective and set of innovative ideas.\n\nDoes your organization have both great leaders and great managers?"
    },
    {
        "url": "https://medium.com/square-corner-blog/activemerchantsquare-for-squares-e-commerce-api-111ea63e530a",
        "title": "ActiveMerchantSquare for Square\u2019s e-commerce API \u2013 Square Corner Blog \u2013",
        "text": "We released ActiveMerchantSquare for developers who want to use Square\u2019s e-commerce API with the ActiveMerchant rubygem. If you are a ruby developer you may use the popular ActiveMerchant gem as a common abstraction layer (in production use since 2006) to work with many payments gateways.\n\nActiveMerchantSquare depends on and enhances the ActiveMerchant gem so that you can use it in conjunction with the existing use of ActiveMerchant. Call the standard ActiveMerchant API methods using a card_nonce from the payment form instead of raw card numbers. Square\u2019s ecommerce API removes you from PCI scope as your servers do not ever see raw payment card numbers.\n\nOther features (in our V1 and V2 APIs) such as the newly released Catalog API is only available in our connect-ruby-sdk. You can use both gems side-by-side if you want to. As with all our client libraries, we hope to help you decrease the development time required to integrate with Square.\n\nYou can use the ActiveMerchantSquare by adding it to your Gemfile:\n\nThis simple example demonstrates how a purchase can be made after getting a card nonce.\n\nIn addition to calling purchase, you can also call these methods, exactly the same as you can in ActiveMerchant. (Details in the rubydoc).\n\nYou can take a look at our working example application here. And let us know other ways you use ActiveMerchantSquare to save development time when integrating your apps with Square."
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-square-checkout-2991fc713c5c",
        "title": "Introducing Square Checkout \u2013 Square Corner Blog \u2013",
        "text": "Over a year ago we introduced Build with Square, our online and offline payment platform alongside our reporting APIs. Since then, we\u2019ve worked closely with developers and our sellers to identify how we can make it easier to start selling online while harnessing the power of our APIs. One of the pieces of feedback we keep hearing from sellers is: Both buyers and sellers recognize, trust, and love the Square brand; can Square bring that trust online?\n\nWith Square Checkout, we are enabling sellers to leverage the trust consumers have put into Square at a crucial point of the payment flow \u2014 entering their credit card details online. By clearly stating that the checkout experience is powered by Square, we are looking to bring the same trust buyers experience in stores to their online purchases.\n\nAnd now with our hosted checkout solution, developers can focus on building a great buyer experience online, and not worry about the payment processing and complex security and regulation involved since we take care of that (for example, Square Checkout doesn\u2019t require your site to have SSL certification).\n\nSquare Checkout beta customer 123ContactForm helps small businesses deal with large amounts of data via online forms. Its customers include companies selling products or services online and nonprofits collecting donations. When 123ContactForm wanted to expand its available payment processing options, it listened to what its customers wanted.\n\nSquare Checkout requires just one integration that gives access to a number of Square capabilities.\n\nSquare Checkout lets developers pass a full transaction itemization (items, taxes, and discounts) as part of the POST request. To pass an itemized transaction you just need to pass us the order object with the line items you want:\n\nEvery time a purchase is made through your hosted checkout page, we record the customer and items sold, track the data over time, and provide access to reports through Square Dashboard. This way sellers can keep track of their customers and the items they\u2019re selling.\n\nAs part of the Square Checkout integration, developers benefit from the Customers API with no extra development. This allows sellers to have buyer data associated with all their sales, providing a full holistic view of sales reporting and shipping information.\n\nIn our efforts to make the experience easier for developers, we also provide the following benefits when using Square Checkout:\n\nLike our other products, all this information can be exported to your own systems by using our listCustomers and listTransaction APIs at any time.\n\nPlease feel free to reach out with any feedback you might have on the product."
    },
    {
        "url": "https://medium.com/square-corner-blog/faster-app-recovery-with-bounded-queues-d546820a0abc",
        "title": "Faster App Recovery With Bounded Queues \u2013 Square Corner Blog \u2013",
        "text": "A problem we\u2019ve encountered in the past is when a Ruby app becomes unresponsive due to downstream issues. After the issue is resolved, the app continues to be unresponsive or very slow to process requests. While a restarts clear it up immediately, but the less manual steps we have to take to recover an app, the better.\n\nBecause testing on production is bad, we need a simple way of replicating this locally.\n\nAt Square, our Ruby apps use NGINX and Puma to serve requests. We write out a simple server side test case, which we can run by using and . Then we set up the client side test case:\n\nNow we have a test endpoint, which sleeps for 5 seconds before responding, and the benchmark which makes 10 requests. With our initial configuration, the benchmark looks like:\n\nRequests #0 through #5 returned and #6 through #9 returned . This is expected since our test case for NGINX has set, and Puma is configured to process one request at a time, it takes about 30 seconds to process the first 6 requests, and the remaining 4 timeout in NGINX.\n\nComparing the Puma logs, we see that Puma saw 10 requests, all of which returned HTTP 200s, NGINX saw the same as the client, 6 requests with HTTP 200 and 4 with HTTP 504.\n\nThis is one of those fun gotchas. The client spent 30 seconds waiting for responses, but the server took 50 seconds to process it. Here\u2019s what\u2019s going on:\n\nEven better is if the client has automated retries. If a client has a timeout of 500ms on the endpoint , and that endpoint now takes 1,000ms, the server has to spend 1,000ms of processing time for every 500ms the client spends. If the client retries 3 times, your server now spends 3,000ms for every 1,500ms the client spends.\n\nEspecially in a microservice architecture, that one endpoint could call another service, which calls another and so on. We\u2019d quickly end up DoSing ourselves, and making an outage worse than it was originally due to requests being backlogged.\n\nIdeally, our app is configured to reject requests when it\u2019s overloaded. It prevents the above scenario of cascading failures, and means the app can recover without having to restart and flush out all the long running requests.\n\nIn this case, we have a few levers we can tweak. We\u2019ll start with the obvious one, which is Puma.\n\nDigging through the Puma code, we found that by default, Puma has an unbounded queue, and accepts a connection from the socket then queues it up for processing in server.rb:\n\nDouble-checking the thread_pool.rb in Puma, we find:\n\nThe variable is an unbounded array which stores the HTTP requests in memory until a thread is available to process. If our app is unable to keep up, we could have a queue with hundreds/thousands of requests that timed out a long time ago. The app may eventually catch up, but we don\u2019t want to wait 10 minutes for it to recover if 95% of the requests it is processing timed out already.\n\nLooking at the code though for server.rb, the option looked promising. Turns out, it\u2019s an option that does exactly what we want! We can make Puma only accept connections when it has a thread available to do so.\n\nAs a note, Puma 3.9.0, which was released at the start of June 2017, now only accepts a connection if it can immediately process it. The timing was unrelated to our digging into it, and was a happy coincidence.\n\nAfter all that, we add to our Puma configuration and rerun the test\u2026 and get exactly the same results. Puma processed 10 requests, and the benchmark saw 6 successes and 4 failures.\n\nFrom prior experience in working with sockets, we know that eventually a TCP socket ends up using the syscall listen (in Linux). Looking at the documentation, we see the backlog argument which \u201cdefines the maximum length to which the queue of pending connections for sockfd may grow\u201d.\n\nOne of the core parts of TCP is buffering. When clients send HTTP requests, they don\u2019t first connect, wait for the server to accept, and send the HTTP request. They attempt to send off everything at once and parts of the request will sit in buffers between the client and the server until the server accepts the connection or times out.\n\nGood for performance, not so when we want predictable request queuing. At this point, we pinged Evan Miller, a fellow Square engineer, on the semantics of backlog and queues, which led to learning that Linux has a backlog for post-handshake sockets (the backlog we\u2019re looking at) and then also a pre-handshake socket. This gets into semantics of how TCP works, which I won\u2019t cover here but Wikipedia has an article explaining the handshake process.\n\nBecause we\u2019re putting NGINX in front of Puma, we actually have four additional queues: one pre and post handshake in Puma, and another in NGINX. Looking at our server configuration and the documentation, we found the max backlog is 128 for the post-handshake queue.\n\nWe configured Puma to use a backlog of 2x what a single instance could process. For example, if one of our smaller apps can handle 10 requests simultaneously, we set the backlog to 20. Given there are multiple TCP queues, this isn\u2019t perfect, but it helps narrow the funnel of where requests can queue up.\n\nWe\u2019ve done what we can for the Puma side, but we still haven\u2019t looked at NGINX.\n\nAfter chatting with other engineers, they mentioned max_conns, a NGINX option that became available in 1.11.5 (the paid version had it earlier).\n\nThe documentation looked promising, \u201climits the maximum number of simultaneous active connections to the proxied server\u201d, though it has some caveats further down noting it can exceed the limit due to keep alive connections.\n\nWe used the same defaults we settled on for the backlog: each instance can queue up to 2x the requests it can handle at once. For our test, the server handles 1 request at a time, and we set . Rerun the test, and we see:\n\nPuma logs only show 3 requests, and nginx rejected the other 7 because we exceeded the maximum number of connections.\n\nQueues are fun, and trying to ensure everything between the client and the server is complicated. The NGINX option is good enough for most cases, and has the added advantage of rejecting requests before it hits the Ruby apps. The TCP backlog and nginx queuing changes are an extra layer of protection, and reduce the chance of long-lived requests hiding out in a queue between NGINX and Puma.\n\nWith these changes, we\u2019re expecting to see more deterministic failure modes when apps slow down due to downstream issues. Instead of queuing up more requests than the app can realistically process, they\u2019ll start responding with errors more quickly.\n\nThese aren\u2019t perfect, but the faster an individual host goes unhealthy, the faster it can be taken out of rotation and given a chance of catching up and becoming healthy again."
    },
    {
        "url": "https://medium.com/square-corner-blog/building-a-sandbox-dashboard-pt-2-adding-a-transactions-list-49fd754e659b",
        "title": "Building a Sandbox Dashboard Pt. 2: Adding a Transactions List",
        "text": "At the end of the last post, we created a basic copy of your Square Dashboard that was able to retrieve your location information. In this post we\u2019ll use Square\u2019s transaction reporting APIs to list out completed transactions within the sandbox account. Let\u2019s take a look at the work we\u2019ve done so far:\n\nWe have a page with a layout similar to Dashboard with only one tab: Locations. We also have a couple input fields to add in your and chosen . Keep in mind that these are for tutorial purposes only! You don\u2019t want to expose access tokens on the client side in a production environment.\n\nThe first step in building the transaction display page is adding some navigation to get to it. Let\u2019s add some code to the nav bar:\n\nAnd add a new file. We\u2019ll use the same structure for the code we wrote to list locations in part one, such as using the shared and having error messages displayed for missing parameters. Instead of making large blocks with the transactions like we did for locations, we\u2019ll print the transaction information into rows, similar to the Square Dashboard.\n\nFirst, configure your access token from the one that is stored in your session and create a new object to interact with the API.\n\nThen we\u2019ll call the List Transactions endpoint, and iterate though the result, displaying a new row for each transaction with its , , and as well as an indication whether the payment has been refunded.\n\nNow we have a table of all the transactions for the sandbox account credentials provided:\n\nHere I chose to only display a snapshot of the information provided, but it should be a good starting point to exposing the transaction information for your sandbox account. Remember that you can use tools like Postman to easily see your sandbox account details as well. Follow this blog, and @SquareDev on Twitter to find out about Part 3 in the series! And, revisit Part 1 here."
    },
    {
        "url": "https://medium.com/square-corner-blog/rrule-expansion-in-ruby-1f7e9694d79b",
        "title": "RRULE expansion in Ruby \u2013 Square Corner Blog \u2013",
        "text": "On the Square Appointments team, we often need to deal with events that repeat \u2014 from simple cases (like a weekly lunch meeting) to more complex ones (like a haircut every other month on the second-to-last Tuesday, except for September 19th). Fortunately, the iCalendar specification provides a compact way to represent all of these cases. The challenge for us is to turn a text RRULE into a set of actual date occurrences that we can use in our application. Given Ruby\u2019s vast gem ecosystem, we expected to find something right away to perform this task for us, but couldn\u2019t find anything that was exactly what we needed.\n\nThe Ruby ecosystem already contained great gems that deal with recurrences \u2014 ice_cube is probably the most popular. allows you to create recurrences via a fluent Ruby interface, and can convert a recurrence created in Ruby into its RRULE text equivalent. Unfortunately for us, our problem is a little different. We\u2019re primarily processing RRULEs embedding in recurring events imported from Google Calendar and turning them into a set of Ruby objects \u2014 essentially the reverse of the problem is designed to solve. does provide some ability to parse RRULEs but doesn\u2019t support some options (like BYSETPOS, the ability to specify things like \u201cevery second and fourth Wednesday in a month\u201d), and we couldn\u2019t see a way to incorporate the features we needed without significant rework of the gem. Since we deal with external data from Google that isn\u2019t under our control and could potentially contain any valid RRULE, we needed to support as close to the full set of the iCalendar spec as possible.\n\nThere were libraries that did exactly what we needed \u2014 but unfortunately, they were in Python and JS. And if you\u2019ve ever written date/time code, you know that it\u2019s very easy to get it wrong \u2014 off-by-one errors abound (is your list of months 0-indexed or 1-indexed?). Rewriting one of these libraries into Ruby without introducing subtle errors seemed like a daunting task.\n\nLuckily, we were trying to solve a well-bounded problem that lent itself quite well to a data-driven test suite \u2014 and we had an already-written set of tests that our mobile team had used months before to test yet another RRULE parsing implementation in Objective-C. This test suite covered just about every combination of RRULE options we could think of.\n\nThis test suite gave us the confidence to do a line-by-line rewrite of Python\u2019s dateutil library (adding in some Ruby idioms when they were obviously applicable). I\u2019ll admit that when writing this first version of our new library, I didn\u2019t understand most of what I was transcribing \u2014 but it didn\u2019t matter. As long as the tests passed, I knew that it worked. Once we\u2019d gotten to green, we were then able to refactor into something much more understandable and idiomatically Ruby, confident that our extensive tests would keep us safe.\n\nWe\u2019ve just released our internal library as \u2014 a small gem focused entirely on the specific task of generating instances from a given RRULE and recurrence start date. It allows you to do things like:\n\nWe\u2019ve been using it in production for about a year at Square, so we\u2019re confident in its accuracy. Right now it supports all of the cases that we\u2019ve encountered in Google data, but we\u2019d like to eventually go further and support the entire iCalendar spec (which can specify repeats down to the level of seconds). If you\u2019re writing a Ruby app and are dealing with RRULEs, we hope that might be helpful to you. You can find the source code at https://github.com/square/ruby-rrule."
    },
    {
        "url": "https://medium.com/square-corner-blog/text-for-your-sales-using-twilio-functions-and-square-e30d6537b720",
        "title": "Text for your sales reports using Twilio Functions and Square",
        "text": "When building an application for your business (or many businesses) notifications are an important layer to that experience. Twilio has a number of products that allow you to add communications and messaging to your app, but we tried out out the newly released Twilio Functions to build an experience that adds extra value without having to complicate your existing application.\n\nThe basic idea is to send a simple text message and receive an update with your day\u2019s current sales. You could use easily modify this to check your inventory via text, or even let customers text for information about your business.\n\nFor this demo, in addition to my Square account, I also created a Twilio account \u2014 the free trial worked perfectly for my needs. I also had to set up a phone number in my Twilio account and of course create an app in the Square Developer Portal to access my personal access token.\n\nTwilio Functions allow you to put your code in a serverless environment to run automatically for incoming messages, phone calls, etc. That way you don\u2019t have to worry about servers, and instead can focus on writing your code.\n\nTo create our new serverless function, I visited to the Functions page of the Twilio Console. From there, I clicked on the \u201cCreate a Function\u201d button to start making a new Function.\n\nI selected the \u201cHello SMS\u201d template for my application. Next, I went back to the phone numbers section of my Twilio console to assign my phone number to respond with incoming messages with my function.\n\nWith that in place, I decided to give my Function a test message:\n\nThat was easy! Then I added some logic to connect to Square\u2019s APIs from my Function, and after navigating back to my function, wrote the code to query the List Transactions endpoint, sum the results, and return the message back to me. You can see my total code below (be sure to replace in the URL with your location, and with your personal access token from the Square developer portal.\n\nNow when I text my Twilio number, my code will execute calling Square\u2019s APIs and getting the total of all my sales:\n\nFor further reading, check out Twilio\u2019s blog post announcing Twilio Functions, or Square\u2019s documentation. Let us know how else you modify what we did and use Twilio Functions with your Square apps."
    },
    {
        "url": "https://medium.com/square-corner-blog/five-ways-to-make-the-most-of-your-internship-bb2a8b1a192",
        "title": "Five Ways to Make the Most of Your Internship \u2013 Square Corner Blog \u2013",
        "text": "Each summer we see an infusion of fresh energy in the office, which means that intern season has officially kicked off! The Bay Area increases by a few thousand students looking to work on some of the most challenging but intellectually satisfying problems in the world.\n\nSquare is fortunate to welcome interns this summer from various schools around the US and Canada who will work in functions across the company, including software, mechanical and electrical engineering, product management, design, hardware, and operations, to name a few. A few of our former interns have written about their experiences at Square \u2014 take a look here and here.\n\nRegardless of what field or function you\u2019re working in, below are some tips to make the most of your internship:\n\nCongratulations! You\u2019ve secured a spot at a company with a mission you feel passionate about.\n\nWhile you are there to do a specific job and contribute to the best of your ability, it is equally important that you come to the internship with your own goals and expectations.\n\nSchedule a 1:1 meeting with your manager in your first few weeks and come prepared so you can fully own that meeting. Talk about the reasons why you decided to join the company, what you\u2019ve learned in school, and ideas you have on your given project. You can also discuss any stretch goals you may have outside of your core responsibilities, such as helping out on another team, presenting work on a specific topic, or gaining more insight into a project. You may not be able to delve into every area that you want by the end of the summer, but at least you\u2019ve expressed interest and initiative. Your manager will help point you in the right direction \u2014 they, too, want you to be engaged and leave the internship with a positive experience.\n\n2. Understand how your work is contributing to the larger team or company\n\nIt\u2019s likely that your internship work is one piece of a larger puzzle, but that doesn\u2019t make it any less important. From the outset, understand how your specific contribution fits within the larger scope of a company project or roadmap, and how it ladders up to the overall initiatives. Having that \u201czoom in / zoom out\u201d quality is an important trait that many executives possess, and it\u2019s beneficial to work on this frame of mind early in your career. Quantify that contribution as much as possible: did you make a system ___% efficient? Did you launch a product ahead of time by __ number of days or week? Did you save $___ on an initiative? This will also help you when updating your resume at the completion of your internship.\n\nBeing inside a company is a unique experience wherein you have access to all of your fellow employees. While your work is always the priority, take advantage of this amazing opportunity to meet with as many people as you can! Before you begin outreach, ask your manager what typical protocol is. Each company will have its own culture and levels of transparency. For example, some CEOs are open to interns reaching out to them directly to schedule a coffee chat, while others are more formal and meet with people based on titles. Outside of executives, take the time to meet with other interns (many companies have social programs in place for interns to meet), managers, and teams outside of your own \u2014 you never know what you\u2019ll discover by meeting with someone who works on a team you\u2019d never even considered. The phrase \u201cI\u2019m new\u201d is an amazing conversation tool!\n\nOnce you\u2019ve left your internship, keep in touch with the people you\u2019ve made the closest connections to (including your manager). LinkedIn is a wonderful tool to manage contacts, and you will be amazed at how small the world is: six degrees of separation really do apply.\n\n4. Get to know your recruiting team\n\nRecruiting teams work very hard each year to bring the best and brightest talent to their company. They are likely the first people you\u2019ll speak with, and do the backend work to ensure you have a smooth onboard and experience.\n\nYour recruiter should stay connected with you through the summer, but if that\u2019s not the case, proactively reach out to them. Talk about your options and timeframe in case you\u2019d like to return for another internship or even full-time after graduation. The more you are on their mind, the stronger they will advocate for you. Recruiters are also goldmines of advice \u2014 it can be helpful to hear their perspective on what drew them to the company and any advice they have for you to be successful.\n\n5. Treat this as an exploratory period and have fun!\n\nThis may be one of the only times in your life where you can work at a company for a few months without any obligation to stay. Choosing an internship can be similar to evaluating colleges: big vs. small, focus area, student body culture, and location, amongst other factors.\n\nUse this time at your internship to first determine if this is the right company and industry for you. Then take a close look at the culture: do they value the same things you do? Do you feel included and part of the community? If the answer is yes, then hopefully you\u2019ve found where you want to start your career. If the answer is no, you can move on to another internship without any repercussions. You\u2019ll spend a great portion of your adult life at work, so it\u2019s important to ensure you are making the right decisions for yourself. What is right for you will not be the same as anyone else, so embrace your uniqueness \u2014 it\u2019s what got you to where you are now!\n\nSaqi Mehta leads the University Recruiting team at Square, visiting schools around the country to bring the next generation of talent to the company. She is passionate about all things education and is co-founder of ReigningIt which brings awareness to diversity in STEM by profiling other womens\u2019 stories. Saqi also writes for the Huffington Post, Daily Muse and Blavity amongst other contributions. In any non-typing free time she is an art, interior design, and travel enthusiast. Say hi to her on Twitter!"
    },
    {
        "url": "https://medium.com/square-corner-blog/cards-saved-with-squares-card-on-file-now-automatically-update-when-they-expire-77649c5a3f9d",
        "title": "Cards Saved with Square\u2019s Card on File Now Automatically Update When They Expire",
        "text": "Many of our e-commerce developers and merchants have quickly adopted our Card on File capability. It\u2019s a great way to build stronger relationships with buyers and tremendously simplifies the buying process for recurring customers. However, there is nothing more frustrating for an e-commerce merchant (or a buyer) than declining a recurring payment because of an expired card, and then missing out on a sale or purchase. When it happens, merchants need to manually send a notification to their buyers to update their card information and it can quickly become a maintenance nightmare while racking up significant costs for a business.\n\nStarting today, cards saved with Square\u2019s Card on File through our e-commerce API will automatically update when they expire. This new feature, called Card Updater, makes the card refresh process completely seamless for both our merchants and their buyers at no additional cost. When a card on file expires, we automagically work with the card networks to refresh the account. All of this happens in the back end and is aimed at making merchants\u2019 lives easier while reducing the costs associated with declined, expired cards."
    },
    {
        "url": "https://medium.com/square-corner-blog/squares-register-api-is-now-point-of-sale-api-a9956032c32a",
        "title": "Square\u2019s Register API is now Point of Sale API \u2013 Square Corner Blog \u2013",
        "text": "We recently announced a new name for the Square Register app to better reflect everything it can do to help run your business: Square Point of Sale. You can use the app to take payments, track sales, manage customers and employees, and more \u2014 everything you need from a complete point of sale.\n\nAs part of this broader change, we are also renaming Register API to Point of Sale API. The Square Point of Sale API uses inter-app communication to let your Android or iOS app open the Square Point of Sale app to process in-person payments with Square hardware (including the Square contactless and chip reader).\n\nYou will see this name change reflected across our documentation and other developer tools:\n\nTo ensure that you always have the latest version of our open source SDKs, just update your build dependencies and update your code as outlined below.\n\nOn Android, we have updated the framework name and Github repository and have also created a new Maven artifact. The API itself has been updated to version 2.0, which is supported in Square Point of Sale 4.64 or later (simply update the app to make sure you are on the latest version).\n\nIf you want to upgrade your project to take advantage of future releases (there are no new features in the 2.0 version itself, other than the name change), follow the instructions in the sections below. Once you upgrade, you should update your code as follows to reflect the name changes:\n\nIf you are using the Web API to initiate a Square Point of Sale transaction from your website, you can change your web intent keys to replace with (note that this is not compulsory, since the Point of Sale app will remain backwards compatible).\n\nTo update the SDK, replace with as a dependency in your build.gradle:\n\nTo update the SDK, replace version 1.2 with version 2.0 as a dependency in your pom.xml:\n\nOn iOS, we have updated the framework name and Github repository. We have also created a new CocoaPod and deprecated the original pod.\n\nIf you\u2019re using our SDK in your native app, you\u2019ll need to update your references to the SDK. Note that method signatures have not changed, so your code requires no change other than updating import declarations. If you built a web app using the Point of Sale API, or if your native app builds its own requests using the URL scheme, no changes are required to support the rename.\n\nIn order to upgrade your project, follow the instructions below.\n\nThese instructions apply only to projects that have previously added the SquareRegisterSDK-iOS repository as a Git submodule. Otherwise, follow these instructions to add the SDK as a submodule.\n\nNote: Github forwards all SSL and HTTPS requests from the old URL to the new URL. Remote repository URLs do not need to be updated.\n\nYou can always learn more about the Square APIs in the documentation. Don\u2019t hesitate to reach out if you have any questions!"
    },
    {
        "url": "https://medium.com/square-corner-blog/code-camp-takes-atlanta-31f5497c75c5",
        "title": "Code Camp Takes Atlanta \u2013 Square Corner Blog \u2013",
        "text": "Code Camp \u2014 Square\u2019s immersion program for college women interested in pursuing careers in tech \u2014 is coming to Square\u2019s Atlanta office for the first time!\n\nWe developed Code Camp five years ago in San Francisco, then took it to New York City, and are excited to expand to another city that\u2019s integral to our robust engineering team. In August, participants will join us in our Atlanta office for five life-changing (so we\u2019ve heard!) days of coding workshops, leadership sessions, and networking opportunities.\n\nHere at Square, we\u2019re committed to improving the ratio of women in technical positions. We launched College Code Camp to inspire, educate, and empower the next generation of women in technology. Over the years, 140 women have participated in Code Camp, creating an incredible network of women spanning more than 50 universities across the US and Canada.\n\nAtlanta Code Camp will run from August 14\u201319, 2017 and applications are open through June 1, 2017. Thirteen women from across the U.S., U.K., and Canada will be selected to participate. Learn more here, or email us at codecamp@squareup.com."
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-the-new-square-catalog-api-3e2ebf254967",
        "title": "Introducing the new Square Catalog API \u2013 Square Corner Blog \u2013",
        "text": "Exposing APIs which allow our partners to build meaningful and delightful experiences for Square sellers is fundamental to meeting the needs of our diverse community of sellers. For most sellers their item library is critical to their success, so when we first launched our API platform one of the introductory endpoints we exposed was Items.\n\nSince that launch, Square has continued to invest in our Items platform (aka Catalog), supporting much larger item libraries with improved scale and decreased latency, and also introducing first class multi-location support. Our continual investment in the platform is an investment in our sellers, so as they grow our platform will continue to support all of their item library needs.\n\nThis launch moves our API endpoint used to access items to Square\u2019s newest API platform, Connect V2. This ensures that the Catalog API performance and reliability are up to the standards our sellers and developer partners expect from Square.\n\nOur Catalog API transitions our item management API from a per location solution to a per seller solution. If you have one location or many, our API allows for you to define and manage a single Item Library. Within the single Item Library items can be shared across locations or active at only a single location. This is not only more scalable and efficient, but also aligned with how businesses conceptualize and manage their products.\n\nTo ensure that solutions built with the Catalog API remain efficient for sellers with many items and locations, we\u2019ve provided bulk API endpoints needed to update things like prices, discounts, or taxes for item variations across one or many locations."
    },
    {
        "url": "https://medium.com/square-corner-blog/building-jukebox-a-viral-way-to-share-music-when-a-simple-text-meme-or-gif-isnt-enough-6a47fb063d03",
        "title": "Building Jukebox: A viral way to share music when a simple text, meme, or gif isn\u2019t enough",
        "text": "Eric King and Damola Omotosho are two old friends who met at Virginia Tech while earning their engineering degrees. After graduation, they both ended up pursuing a master\u2019s degree in engineering before working full-time in tech. While they were off to a strong start in their early careers, they had a desire to shake things up and find an outlet for their urge to build innovative solutions to problems they experience in their everyday lives. In order to follow their passion for building software and creating new products, Eric and Damola decided to start Swyft Studios.\n\nSwyft Studios\u2019 mission is to build innovative and fun products that sit at the intersection of technology and culture. A core focus area is to target underserved markets where they see an unmet need for a new service.\n\nThe Square team had a chance to meet Damola and Eric at the SXSW Hackathon in March 2017. We loved meeting them because their level of energy and passion is addictive and uplifting.\n\nDuring the Hackathon, they shared their positive experience with the Square\u2019s developer team. \u201cIt is obvious that Square cares about developers and the participants of the hackathon. It is rare to see a company send actual developers on-site instead of marketing folks\u201d.\n\nAfter reviewing the APIs available for the Hackathon, Eric and Damola started to work on Jukebox, an app that allows users to easily search, find and share high quality audio and video micro-content from some of the biggest stars in music and entertainment. According to Damola, \u201cthere is no viral way to share music today,\u201d and as a result he wanted to address that unmet need.\n\nJukebox allows users to purchase and send their favorite line from a Beyonce or Bruno Mars song to a friend or significant other. This model also allows content creators to earn revenue from their songs or videos when it is shared digitally from their fans.\n\nThis awesome concept helped them join the select group of winners at the 2017 SXSW Hackathon!\n\nJukebox was built using APIs from MediaNet to access digital music data and rights, Cloudinary to distribute feature rich, high performance image and video content and the Square\u2019s e-commerce API to enable users to buy tokens and pay for music directly within the app. Check out the high level cloud-based architecture below:\n\nJukebox functions by first leveraging the MediaNet API to secure the rights and original content. MediaNet provides a helpful set of APIs that allow for discovery and download of high-fidelity audio content and artist information.\n\nUpon download from MediaNet the song is then indexed and \u201csnipped\u201d by aligning key parts, catchy hooks and the information necessary to play the snippet. This information is then stored in the Jukebox database.\n\nFrom here Cloudinary\u2019s image and video management cloud API is utilized to store, retrieve as well as play snippets that are purchased and shared within the Jukebox App.\n\nThe combination of the Jukebox API combined with the crowd-sourced purchase and sharing data enables visualization of virality as well as some functionality within the Jukebox app to enable users to see what snippets are trending on the platform.\n\nUsing Square\u2019s e-commerce API, Damola and Eric were able to integrate the payment features allowing users to purchase e-coins to use on content snippets to share with friends.\n\nThe .NET library provided by Square made integration a breeze. Squares library includes calls that will help Jukebox improve future user experience by keeping user\u2019s payment information secured in the Square cloud while maintaining PCI compliance.\n\nEric and Damola plan to add functionality within the app to share content natively via text message or email so that it can be easily shared without users being required to download the Jukebox app to listen to a clip they received. Looking ahead, they plan to ship the app to the app store this summer and gauge adoption.\n\nAfter discussing Jukebox \u2014 Eric and Damola also told us about one of Swyft Studios\u2019 first products called CutUp, a digital platform that aims to take the frustration out of finding hair care services for people of color, anywhere in the world. The idea came from their personal experiences of dealing with the frustration of finding a good barber when relocating to a new area. They realized that many barbers and hair stylists that service their community\u2019s specific needs lack the technology to be easily found by potential customers.\n\nInspired by their discovery of Square\u2019s APIs during the SXSW hackathon, Swyft Studios is now looking to enhance CutUp by offering the ability for barbers and hair stylists to take payment directly within their mobile application. They love the idea of empowering small business owners to accept any form of payment using their app and Square\u2019s hardware can help them expand from cash only transactions.\n\nWe love working with creative and passionate developers around the world like Eric and Damola and we can\u2019t wait to see the creative apps they build in the future.\n\nFor more information about our APIs and what they enable you to build, check out the documentation on our developer portal."
    },
    {
        "url": "https://medium.com/square-corner-blog/using-squares-rest-apis-to-build-a-sandbox-dashboard-part-1-authentication-locations-3e5b5a551cc7",
        "title": "Using Square\u2019s REST APIs to Build a Sandbox Dashboard Part 1: Authentication & Locations.",
        "text": "One of the things we hear most often from our developers is the lack of a dashboard for their sandbox accounts. This will be the first post in a series, each talking about a different part of our APIs. By the the end of the series, we\u2019ll have a dashboard for your sandbox account. Let\u2019s get started!\n\nAt Square, there is a whole team dedicated to our Dashboard, and I\u2019m not going to get close to replicating all the work they have done. I will however start with some boilerplate HTML that sets up the general structure of the page, as well as some JavaScript and CSS to make the page more usable. Plus, the dashboard for sandbox accounts will closely match certain visual elements of the real dashboard. My goal isn\u2019t to create an exact replica of the Square Dashboard, but instead to make one that is functional and can be used with the sandbox credentials. I was able to make a fairly simple reproduction of the left side navigation with a couple of s and some liberal use of the CSS property. I broke out the navigation into its own file since it will be shared between all the pages in my app. Right now, my file structure looks like:\n\nAnd my page looks like:\n\nYour developer sandbox works the same as your production or \u201creal\u201d account, with the exception of one key difference: it\u2019s a playground! The e-commerce transactions you take with your sandbox credentials won\u2019t get charged, and any customer operations you take won\u2019t affect your actual customers. At the time of writing, you aren\u2019t able to use the API sandbox with any of the v1 endpoints, including item and employee management.\n\nUsing the API sandbox is easy, all you have to do is use your sandbox access token instead of your personal access token, or the access token you would get from OAuth. First I\u2019m going to add an input element to my left side navigation for you to put in the access token you want to use. Keep in mind, this is only a tutorial for you developers, you would never want to expose your secrets like this. I decided to use PHP\u2019s sessions to keep track of your credential, and to make it easy to switch between access tokens. As soon as you paste in your sandbox (or other) access token, or do anything else to trigger a JavaScript event, some in-page JavaScript will send the value to simple PHP page ( ) that will store the value in the session.\n\nNearly all of Square\u2019s REST endpoints require you to supply a location parameter in the URL of the request. Square Dashboard lets you select a locations with an intricate series of modals and drop downs. I\u2019m only going to list the locations and let you specify the ID of the location you want to use, but to do that I\u2019ll need to add the code to list the locations. I\u2019ll use Square\u2019s PHP SDK to call the List Locations endpoint, and display them on the screen. First I\u2019ll add a file and add the appropriate version of the SDK to it.\n\nA quick gets all the dependencies installed, and all I need to include in my is to have access to the SDK\u2019s functions (like ).\n\nI added a new file, and added some logic to list the locations for the provided access token, and simply dump them out on the screen.\n\nPutting it all together gives you a quick view of your locations, and the underlying json:"
    },
    {
        "url": "https://medium.com/square-corner-blog/washington-dc-taxis-are-moving-to-new-digital-meter-apps-that-connect-with-square-44ef7191a048",
        "title": "Washington, DC Taxis Are Moving to New Digital Meter Apps That Connect with Square",
        "text": "Most taxis are still running outdated, legacy metering hardware that doesn\u2019t integrate with the point of sale or credit card reader. As technology changes the way we pay, taxis have to be able to innovate quickly and keep up with technology shifts.\n\nToday, Washington, DC\u2019s Department of For-Hire Vehicles (DFHV) announced that all the city\u2019s taxis will shift to new apps to replace antiquated payment terminals and legacy metering hardware. We\u2019re proud that Square will be powering the first digital meter apps in DC through our point-of-sale API, enabling any taxi to use Square hardware and give their riders an updated payment experience.\n\nAn integrated digital and payment experience in taxis makes fare calculation easier for drivers and provides the rider with a transparent and simple experience. Riders can calculate tips quickly with our Smart Tipping screen, pay with any form of payment they have in their pocket\u2013like phones, chip cards, or cash\u2013and receive digital receipts and deposits as soon as the next business day.\n\nThe approximate 8,500 drivers and more than 90 cab companies in DC will make the switch by August 31, 2017. So if you find yourself hailing a cab in Washington in the coming months, look out for new apps and Square readers."
    },
    {
        "url": "https://medium.com/square-corner-blog/generating-kotlin-code-with-kotlinpoet-119dc20f74d4",
        "title": "Generating Kotlin code with KotlinPoet \u2013 Square Corner Blog \u2013",
        "text": "Java code generation has become a popular solution to simplifying library code. Dagger generates interface implementations, Butter Knife generates Android UI boilerplate, and Wire generates implementations of value classes for binary encoding of data.\n\nDespite Kotlin\u2019s strong interop with Java, the generated Java code for these libraries can feel foreign and convention-violating as they\u2019re targeted at Java consumers and lack Kotlin features.\n\nSquare\u2019s history of code generation started with JavaWriter\u2013a linear code generator that required emitting from top to bottom. We collaborated with Google\u2019s Dagger team on its successor, JavaPoet, which took the concept further in providing builders and an immutable model of the generated code. KotlinPoet builds on the success of JavaPoet by providing similar models for creating Kotlin:\n\nThe above code produces the following Kotlin:\n\nGenerating Kotlin also comes with an advantage over generating Java: JavaScript and native are available as first-party compilation targets. This allows you to use the same tool and the same generated code for multiple platforms.\n\nKotlinPoet is currently an early-access release. Not every language feature and syntax is covered yet, but it\u2019s enough to get started and make public. We\u2019re looking forward to seeing what you build with this library!"
    },
    {
        "url": "https://medium.com/square-corner-blog/kotlins-a-great-language-for-json-fcd6ef99256b",
        "title": "Kotlin\u2019s a great language for JSON \u2013 Square Corner Blog \u2013",
        "text": "Though it has its wrinkles, I really like JSON. It\u2019s easy to read, pretty fast to parse, and refreshingly simple. Here\u2019s a sample message from GitHub\u2019s exemplar API:\n\nKotlin\u2019s concise immutable data classes make it easy to build a basic model for this JSON.\n\nThat\u2019s it. No , , or boilerplate. We don\u2019t even need a builder! Let\u2019s extend the model to take advantage of Kotlin\u2019s default values and explicit nulls:\n\nDefault values fill in the gaps when decoding JSON from the network. I like that I can leave them out when creating sample data in my test cases. Explicit nullable types prevent data problems.\n\nToday we\u2019re releasing Moshi 1.5 with powerful Kotlin support via the module. Moshi\u2019s type adapters and annotations bind JSON to an idiomatic data model.\n\nThis class uses proper types instead of strings for the issue\u2019s state and timestamps. The annotation maps names in JSON to property names in Kotlin.\n\nTo set this up I need a and a . I can use Kotlin\u2019s raw strings to embed a sample message right in the code.\n\nIf you\u2019re using JSON, Moshi and Kotlin help you to build better models with less code. Note that uses for property binding. That dependency is large by Android standards (1.7 MiB / 11,500 methods). We\u2019re thinking of creative ways to address that!\n\nThis post is part of Square\u2019s \u201cSquare Open Source \u2665s Kotlin\u201d series."
    },
    {
        "url": "https://medium.com/square-corner-blog/an-optionals-place-in-kotlin-17d7b271eefe",
        "title": "An Optional\u2019s place in Kotlin \u2013 Square Corner Blog \u2013",
        "text": "With nullability being a first-class citizen in Kotlin\u2019s type system, the need for an type seems all but diminished. Just because you can explicitly express nullability, however, does not mean that null is always allowed.\n\nFor example, Retrofit provides adapters for RxJava 1.x and 2.x which allow modeling your requests as a single-element stream.\n\nRxJava 2 differs from RxJava 1 in that it does not allow null in its streams. If we\u2019re using RxJava 2 and the converter for returns null an exception will occur. Thus, in order to represent the absence of a value for the response body inside this stream we need an abstraction like .\n\nRetrofit 2.3.0 introduces two new delegating converters for the types in Guava and Java 8. Unlike the other converters for libraries like Moshi, Gson, and Wire, these new ones are different in that they don\u2019t actually convert bytes to objects. Instead, they delegate to other converters for processing the bytes and then wrap the potentially-nullable result into an .\n\nWith one of the converters added alongside a serialization converter, requests whose bodies may deserialize to null can be changed to instead return an .\n\nToday\u2019s Retrofit 2.3.0 release contains the same JSR 305 annotations for explicit nullability in Java and Kotlin as our recent Okio and OkHttp releases. As we\u2019ve seen above, though, just having these annotations\u200b or a type system that can model nullability sometimes is not enough. For these cases, in Java and Kotlin alike, the use of has its place.\n\nThis post is part of Square\u2019s \u201cSquare Open Source \u2665s Kotlin\u201d series."
    },
    {
        "url": "https://medium.com/square-corner-blog/non-null-is-the-default-58ffc0bb9111",
        "title": "Non-null is the Default \u2013 Square Corner Blog \u2013",
        "text": "Yesterday I wrote about how we\u2019re rolling out throughout Square\u2019s open source Java libraries. It helps our tools to alert us of possible .\n\nThe imposes an obligation on the consumers of the value: they must prepare for it to be null. But what happens when I\u2019m the one producing the value? Suppose that I\u2019m calling the method above.\n\nWe\u2019ve got a problem. If the file doesn\u2019t exist this method passes a null source to and that will trigger a crash. We need the opposite of : an annotation that alerts us if we put null somewhere it doesn\u2019t belong.\n\nThe JSR-305 annotations package has this and it\u2019s called . We just need to put it everywhere that we don\u2019t already have .\n\nBeing explicit about which values mustn\u2019t be null lets our tools warn us if we make a mistake.\n\nUnfortunately, everywhere is boilerplate everywhere. This extra code distracts from the real problems we\u2019re solving. Thankfully, there\u2019s a solution: JSR 305\u2019s indicates that everything in an entire package is non-null unless otherwise specified. Apply it by creating a file in each package.\n\nBy coupling with we\u2019re able to use null safely.\n\nIf you\u2019re defining Kotlin APIs you don\u2019t need non-null annotations. The entire language is non-null by default and only types suffixed with can be null.\n\nIf you\u2019re calling Java APIs from Kotlin, you need to be a bit more careful. The language doesn\u2019t yet honor when it\u2019s used to call Java APIs. JetBrains is tracking this and hopefully it\u2019ll be supported in a future release.\n\nToday we\u2019re releasing OkHttp 3.8, a library where everything is non-null by default. The update is more strict about incoming nulls in three methods. Please read the change log to avoid any surprises!\n\nThis post is part of Square\u2019s \u201cSquare Open Source \u2665s Kotlin\u201d series."
    },
    {
        "url": "https://medium.com/square-corner-blog/rolling-out-nullable-42dd823fbd89",
        "title": "Rolling out @Nullable \u2013 Square Corner Blog \u2013",
        "text": "Lots of us have strong opinions on null. I don\u2019t think it\u2019s evil, or that using null is sloppy. I just want to be deliberate: wherever a parameter or return value can be null, the interpretation of that should be explicit.\n\nKotlin\u2019s great because it requires us to be explicit when we use null. Declaring the same method in Kotlin puts null into the type system.\n\nBut what happens when we want to call Java APIs from Kotlin? We need an annotation to signal that our return value can be null. But \u2014 frustratingly\u2014 there isn\u2019t a annotation built-in to Java. Instead, we need to pick a third-party library to supply it:\n\nWe\u2019re following Guava\u2019s lead and will adopt in our open source projects. Java users benefit by getting additional support from their IDEs and static analysis tools. Kotlin users get even more: compiler-enforced null safety. If your current Kotlin code is not null-safe you\u2019ll need to fix it when you integrate these updates.\n\nThe dependency is compile-time only, so your build scripts won\u2019t change and your binaries won\u2019t get any bigger.\n\nToday we\u2019re releasing Okio 1.13 which adds to help you to avoid problems with null. Over the next few days we\u2019ll follow up with releases of our other open source libraries. Enjoy!\n\nThis post is part of Square\u2019s \u201cSquare Open Source \u2665s Kotlin\u201d series."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-open-source-loves-kotlin-c57c21710a17",
        "title": "Square Open Source \u2665s Kotlin \u2013 Square Corner Blog \u2013",
        "text": "Consumers of Square\u2019s open source libraries may remember our \u201cSeven Days of Open Source\u201d prior to Google I/O 2013. We released major versions of some of our projects in the days leading up to the event. This culminated in us also having a booth inside the developer space at I/O and getting to meet and chat with you all about those projects and others.\n\nWhile we don\u2019t have seven projects to release this year, we do have a few days of blog posts and releases centered around a common theme: Kotlin!\n\nIn January 2015, Kotlin was approved internally at Square for use in our Android applications. Since then we have published the document that made the case for investment and our engineers have spoken at conferences around the world evangelizing the language. In addition to Android apps, new Gradle and IntelliJ plugins are being written in Kotlin. Later this year we\u2019re hoping that Kotlin will also be deployed on the server.\n\nAs the language\u2019s usage has grown internally and externally, a few rough edges have emerged with our open source libraries. The blog posts over the next few days will outline some of these problems and they\u2019re paired with corresponding library releases to remedy them.\n\nWe\u2019re committed to Kotlin at Square. These changes should help improve the developer experience of our libraries for all of you who have also embraced the language on your platform of choice.\n\nBe sure to check back each day for updates!"
    },
    {
        "url": "https://medium.com/square-corner-blog/setting-up-https-for-your-e-commerce-website-with-lets-encrypt-and-google-app-engine-465a866790b5",
        "title": "Setting up HTTPS for your e-commerce website with Let\u2019s Encrypt and Google App Engine.",
        "text": "Security on the web is hugely important, especially when someone is trusting you with their payment card information. Even though Square handles the complex parts of PCI compliance for you, we do require your e-commerce forms to be served from an HTTPS domain. There is plenty of literature online about why you should be using HTTPS for your websites (including this great post by Google), so let\u2019s focus on how to actually get a certificate for your website. I\u2019ll demonstrate with my personal website.\n\nThis walkthrough will assume you have the following:\n\nI\u2019ll enable HTTPS for my personal website: www.tristansokol.com. I use Google App Engine to serve my site, and will be using Let\u2019s Encrypt\u2014a free, automated, and open Certificate Authority.\n\nIf you use a different web host, it\u2019s a good idea to see if your web hosting supports Let\u2019s Encrypt directly. If so, it should be fairly easy to enable it for your website. You can see a list of supported web hosting providers here: https://community.letsencrypt.org/t/web-hosting-who-support-lets-encrypt/6920.\n\nThere are also a wide variety of other Certificate Authorities that can offer you similar or more advanced services for a price.\n\nFirst, use OpenSSL to generate a new private key and Certificate Signing Request.\n\nThe command is pretty long, so let\u2019s break it down:\n\nYou can read more about the options for on the offical man page: https://www.openssl.org/docs/man1.1.0/apps/req.html\n\nYou should now have two new files in your working directory (and again, yours will have your domain name instead of mine):\n\nCertbot (formerly the Let\u2019s Encrypt Client) is a tool to obtain certificates from Let\u2019s Encrypt. I\u2019m on a Mac, so I\u2019ll be using for installation. https://certbot.eff.org should have instructions for your system.\n\nCertbot will now take you through a guided walkthrough to set up your certificate. Eventually, I hit the following step:\n\nThis is a challenge (the url) and response pair that will be used to prove that you own this domain. That means I\u2019ll need to add an additional request handler to this site to respond to these challenges. Luckily, it is fairly simple with App Engine.\n\nIn order to serve the challenge response from my website at the challenge URLs, I need to do a couple things. First, I created a new text file with the the response I was supposed to serve, and saved it as ACME-response.txt. Then I updated my app.yaml to serve that page at the specified directory.\n\nThis tells App Engine to respond to requests at with the content of my response. Then I used the local development server with to double-check that everything was working before deploying with .\n\nAfter repeating those steps for the www. version of my domain, I can continue.\n\nWith my challenge responses good to go, Certbot generated a certifcate and two cert chains.\n\nNow that I\u2019ve verified and generated everything, I can go to the certificate management page of my app engine project and hit upload new certificate. For the PEM encoded X.509 public key certificate I\u2019ll upload my file that I just generated. For the Unencrypted PEM encoded RSA private key I uploaded my file from Step 1.\n\nThen I enabled it for both my naked and www. domains. I also edited my app.yaml file to include for the request handlers of my pages. Now when I navigate to my personal website, users will be automatically redirected to the https version, inspiring trust for visitors to my site, and keeping my e-commerce customers safe.\n\nTo learn more about Square\u2019s e-commerce platform, check out square.com/developer."
    },
    {
        "url": "https://medium.com/square-corner-blog/how-to-install-the-beta-sdks-b746503515d9",
        "title": "How to install Square\u2019s beta SDKs \u2013 Square Corner Blog \u2013",
        "text": "[Update: the beta has completed, download the latest version of our SDKs here] As a reminder, if you want help with installation, to give us feedback on the new design and features, or simply to learn more about the APIs, request an invite to our new Slack community here.\n\nThe differences in version 2.1.0 of our new SDKs are the same for each language (of course, these changes look a little different in each language). The main differences are:\n\nand with version 2.1.0 and forward, that same request would look like:\n\nYou can install the beta 2.1.0 version of our SDK with composer with the command line:\n\nor by adding the following to your file for your project.\n\nYou can also install from GitHub with:\n\nand use the familiar within your application.\n\nYou\u2019ll need to get the source from GitHub and:\n\nYou should now have a new file to include with your project.\n\nWith Ruby, you can specify the 2.1.0.beta version of the square-connect gem to install by running:\n\nYou can install the Python package straight from GitHub with:\n\nYou\u2019ll need to download the files from GitHub and switch to the 2.1.0 version:\n\nYou can then build the by running for Unix-based systems or for Windows. You will likely need to download the dependencies, but you should be prompted to do so to build.\n\nTo learn more about how our SDKs are made, as well as the release of our Java SDK, check out our earlier post announcing new versions of our client SDKs."
    },
    {
        "url": "https://medium.com/square-corner-blog/announcing-our-new-versions-of-our-client-sdks-1336d26e8099",
        "title": "Announcing new versions of our client SDKs \u2013 Square Corner Blog \u2013",
        "text": "[Update: the beta is now over; you can download the most up to date versions of our client SDKs here.]\n\nAt Square, we want to make building integrations with our platform as fast and easy as possible. To help with that, we are working on a big update to our SDKs to expand their functionality and make them even easier to use. We are also launching a Slack community to gather feedback about the new SDKs we\u2019re releasing. If you are interested in talking about our new SDKs, or just meeting other Square developers, click the link below to put your name on the invite list.\n\nWe are constantly working on developing new features for our APIs, so we make liberal use of the OpenAPI Specification in our SDK creation process to avoid hand-crafting each one. We publish our specification file in the square/connect-api-specification repository on GitHub. We then use the Swagger Codegen project to turn our specification into client libraries for PHP, Python, Ruby, C#, and Java. We\u2019ve put in long hours investing in the continuous integration for this process with Travis CI, to give you a full look into the whole process.\n\nWe are very excited to announce our first-ever Java SDK. Java is one of the most popular languages for developers using our APIs, and they have gone underserved for too long. This release is a direct result of our new continuous integration pipeline, which should allow for other languages that we don\u2019t currently have to follow quickly. You can grab the source for our Java SDK on Github, or install with Maven or Gradle.\n\nToday, you will see that each of our SDK repositories have a branch called . This branch (and our SDKs going forward) has a couple distinct differences:\n\nThese are some BIG changes, and if you try to upgrade your SDK without changing any of your logic, it will break your app. That is why we are releasing these SDKs in beta, which you can find today on github. You can use the right now to start getting your application ready for the upgrade, or checkout the installation guides for each language.\n\nWe will be doing other proactive outreach to developers to make sure you know about these changes, our new community, and how to update applications. Over the next few months we will be incorporating feedback, and putting the final touches on our SDKs. Keep an eye on this blog, @SquareDev on twitter, and our new Slack community for more info!"
    },
    {
        "url": "https://medium.com/square-corner-blog/improving-the-square-register-api-payment-experience-2ae13453da94",
        "title": "Improving the Square Register API Payment Experience",
        "text": "We recently introduced new functionality in the Square Point of Sale iOS and Android apps that greatly improves the payment experience when using Register API.\n\nOn iOS, we\u2019ve added functionality that keeps your Square contactless & chip card reader connected in the background (note: this is already available on Android). And, on Android, we\u2019ve enabled a transparent background for Register API customers.\n\nSimply update your version of the Square Point of Sale App to take advantage of these improvements.\n\nOnce you or your customer pairs and connects the contactless & chip card reader to the Square Point of Sale app, the reader will maintain its connection any time you app switch and while your app is foregrounded. This means that whenever you initiate a transaction and the API app switches into the Square app, the reader no longer has to reconnect in order to begin processing the payment. Once the app switch takes place, you\u2019ll see the reader\u2019s green light come on, indicating that it\u2019s ready to take a payment almost immediately. In addition to speeding up transaction times, this update also has the added benefit of improving your reader\u2019s battery life.\n\nWhen your app initiates a transaction using the Square Register API, the payment flow now has a transparent background layered over your app. This creates an even more seamless experience for merchants using apps that integrate with the Register API, as it no longer feels like they are leaving your app and entering a new environment every time they process a payment.\n\nThis is what it looks like using our Bikeshop sample app:\n\nThe only thing you and your customers need to do to take advantage of these improvements is update to the latest version of the Square Point of Sale app from Apple\u2019s App Store or Google Play.\n\nWe\u2019d love to hear your feedback on the API and the new and improved app switch experience. Lets us know your thoughts at @SquareDev.\n\nTo learn more about our APIs visit our developer documentation site."
    },
    {
        "url": "https://medium.com/square-corner-blog/how-square-writes-commit-messages-8e92fcbf77c9",
        "title": "How Square writes commit messages \u2013 Square Corner Blog \u2013",
        "text": "Commit messages are an important form of communication for us at Square, so writing them well is a great way for us to give time back to each other. Often we wish we\u2019d done a better job earlier. The commits from those days are still in our history, and many of them are still very important.\n\nWe\u2019ve collected our thoughts on how we invest in great commit messages that save us time not just immediately, but down the road as well. We\u2019re sharing them here in the hope they\u2019ll also be helpful to your team.\n\nHere at Square, we write a lot of code. At its best our code is fairly self-explanatory. Where our programming language is insufficiently expressive, there are comments in the code explaining what the code does, or even better, why it does what it does. If a developer checks out a SHA and looks at the code, she should be able to understand what it does and why it works the way that it does.\n\nCommit messages aren\u2019t about the code, they\u2019re about the change itself. This is a crucial distinction that is often lost.\n\nWhen we make a commit, we\u2019re changing some ideally very clear, self-describing code to some other very clear, self-describing code. No matter how clear the before-code and the after-code, there\u2019s a contextual gap between them. The commit message bridges that gap.\n\nConsider the readers of a commit message. They basically fall into two camps: reviewers and archaeologists.\n\nThe immediate audience is likely code reviewers, who must decide whether they approve of the change, and whether the actual change in code correctly reflects its author\u2019s intent. Reviewers need context on the change itself in order to evaluate it, and it\u2019s much easier for them if the change comes with a message providing that context than if they have to deduce it from the before and after.\n\nLonger term, the audience is likely developers attempting to fix a problem in the software, remove code that\u2019s no longer needed, build atop existing code, just make similar changes, or some combination of these. Sometimes, it\u2019s a developer just trying to understand a past decision (often the author!). This is archaeology, and it\u2019s dramatically easier if each commit comes with a helpful note explaining the change. Otherwise the archaeologist is left trying to reconstruct a commit author\u2019s thought process from nothing more than the code before and the code after\u2013 often an impossible task.\n\nOur great commit messages also help us to promote our best engineers. They provide context and detail that help us evaluate the technical acuity in an engineer\u2019s body of work. Equally important, our best commit messages show our skill and empathy as communicators. Clearly-written, insightful, helpful commit messages are a clear sign that an engineer is building with foresight and consideration for other people. Evidence of technical ability, empathy for others, and effective communication are all key to earning a promotion at Square.\n\nAre we fixing a bug? We\u2019ll briefly explain the expected and actual behavior, and how this change is related. Are we introducing a new feature? We\u2019ll summarize it.\n\nIt\u2019s helpful to assume that the reader has no idea why we\u2019re making a change. For many readers, this will be true \u2014 for even the smallest, most \u201cobvious\u201d change. Writing down the motivation is especially crucial.\n\nThis would seem to go without saying, but in the course of responding to reviews or otherwise iterating on a change, it\u2019s easy to overlook updating the commit message. To guard against inaccuracy, we authors and reviewers double-check our messages before merging.\n\nEspecially when we\u2019re touching infrastructure code or doing something a little unusual, it\u2019s worthwhile to record why it\u2019s ok to do what we\u2019re doing. It reassures readers and puts our considerations and assumptions on the record in case there are problems later.\n\nJira for sure. Design docs, google groups threads, Slack threads as appropriate. The more context we can put around a change, the easier it\u2019ll be for archaeologists to understand it.\n\nOf course, simply linking to a Jira ticket or discussion thread isn\u2019t very helpful. Why make readers (who may not even be reading in a browser) go somewhere else to get any understanding at all of the change? They might not bother, and might miss crucial detail. They also might not be able to! For example, once upon a time we used Github Enterprise. Now all of those PRs on GitHub are lost. Tools change over time, but the commit log is forever.\n\nJira and Stash are helpful for capturing the conversation context, to understand the back and forth that happened while we decided on a change. Our commit messages capture the end result of that back and forth, and should stand on their own if links go dead.\n\nOur messages will be read in a variety of tools and contexts \u2014 the pull request, IDEs, `git log` command lines that may show abbreviated or full messages, etc. There are some important things to know about writing in this medium.\n\nWe\u2019re careful to choose the right medium for our messages.\n\nNote: Cross-referencing among the above is encouraged!\n\nAs you might be able to tell from above, a lot of thought goes into a good commit message, and like most things it takes practice and help from each other to build the habit. Here are some common ways we go wrong.\n\nWe sometimes write messages like \u201cFixes <bug>\u201d or \u201cAdds <functionality>\u201d. Sometimes those messages don\u2019t even describe the bug or functionality. These are just hints and clues, really, exercises left to the reader to complete.\n\nDescribing new code is different from explaining the change. Sometimes we\u2019ll try to be helpful by describing the mechanics of the new code, how it\u2019s supposed to operate. Code comments are better suited for that, though. Sometimes we\u2019re tempted to explain high-level design or architecture, which better lives in a design doc that we could just link to. At any level, explanations of code have better homes than the commit message.\n\nThis one\u2019s really common, unfortunately. In our haste, we\u2019ll write a message that just describes the change tersely and literally. \u201cDeleted foo\u201d, or \u201cRenamed bar to baz\u201d. This is at least the right idea, but messages like this are too literal. They lack the context, motivation, and assurance of safety that readers are looking for.\n\nHere\u2019s an example of a really unhelpful commit message I wrote. Sorry, team!\n\nYou can see that it provides no context, provides no motivation for the change, sort of just describes the code. I have no idea why I wanted to change that request from a POST to a GET. Was there a bug? Probably, but what was it? Was it with the request itself? Was I working around a bug in our REST library, which may now be fixed? Should this change be reverted at some point? What exactly was wrong with POST, and why did I think GET would be better? So many questions! Not even I know the answers now.\n\nI think I did a much better job here:\n\nThis time, we have a lot more answers. There\u2019s some history, including a link to where the problem was introduced for further reading. I explained the bug that I was fixing in enough detail for the reader to understand it (at least I do, eight months later). Finally, with that supporting context, I explained briefly what I was changing, and why it would fix the problem. This is much more helpful to me today, and I hope to others."
    },
    {
        "url": "https://medium.com/square-corner-blog/see-square-in-the-north-east-and-learn-about-our-apis-f49ea1d25095",
        "title": "See Square in the Northeast and learn about our APIs",
        "text": "In April, we are going to show some love to Boston, Baltimore, and New York, and we would love to see you at some of these events!\n\nFirst off, we will be at Droidcon Boston, one of the best community-driven Android conferences, on April 10th & 11th. It is being hosted in Boston this year for the first time ever, and we\u2019ll be there showing off our Android point of Sale API and our new Java SDK for our REST APIs. To learn more (and register!), visit http://www.droidcon-boston.com/.\n\nWhile we are in Boston, we\u2019re also hosting our own event, just for Square developers. On April 12th, we will be hosting a developer workshop in Back Bay. We\u2019ll be giving an overview presentation about the Square developer platform, and leaving most of the time open for Q&A. If you are interested in learning about some of the new features that Square\u2019s APIs have to offer, or meeting with some of the people who make the APIs that you use, we\u2019d love to see you. You can find out all of the details and register here: Square Boston Developer Workshop.\n\nNew York is the city that never sleeps \u2014 and neither do we! After our work is done in Boston, we\u2019re headed straight to New York to host our own developer event April 13th. It will be similar to Boston\u2019s, with an overview of Squares offerings to developers, and time dedicated to answering your questions. If you are interested in using Square for e-commerce or creating custom apps for in-person payments, sign up here: Square NY Developer Workshop.\n\nApril 24th-28th we will be at DrupalCon Baltimore. It is an event not to be missed for Drupal developers, and if you\u2019ll be there, please stop by the exhibit hall and say hello!\n\nWe\u2019ll have more events like this planned all year long; give us a shout at @SquareDev where we should go next!"
    },
    {
        "url": "https://medium.com/square-corner-blog/build-with-square-is-creating-waves-in-australia-6a7135c6cf7f",
        "title": "Build with Square is creating waves in Australia! \u2013 Square Corner Blog \u2013",
        "text": "Since the U.S. launch of Build with Square in March 2016, Square has been committed to bringing our tools to more developers and sellers of all sizes. Our collection of APIs enable developers to build custom solutions for sellers to empower them to run every part of their business in one place. In January 2017, we made our APIs available to Australian developers, giving them the ability to integrate with Square hardware and payments with just a few lines of code. Integrating with Square\u2019s APIs is simple, so developers don\u2019t have to piece together hardware, software and payment services from multiple vendors \u2014 they get it all in one place.\n\nSince the launch, our e-commerce API has quickly taken off, and merchants are already using our e-commerce integrations to process payment on their e-commerce site. Other developers have embraced our platform and are building creative apps with the point-of-sale API to help their customers accept payments easily, wherever they are.\n\nSwiftHero, a Melbourne startup, uses Square\u2019s point-of-sale API to facilitate a seamless billing process. By integrating Square\u2019s newly released API, tradespeople can now accept card payments right from within the SwiftHero Pro app.\n\nIn an industry where cash remains king, SwiftHero aims to bring small businesses up to speed with technology. Square\u2019s payments APIs have allowed the company to solve another major issue facing the industry \u2014 and equip tradespeople with the tools to survive in a fast-moving economy.\n\n\u201cSquare opens up an entirely new world for small businesses.\u201d says Michael Foster, co-founder at SwiftHero. \u201cWith SwiftHero and Square, tradespeople can run their whole business from their pocket \u2014 paperless and cashless.\u201d\n\nAnd starting today, two of our largest partners are bringing their Square integration to their point-of-sale apps in Australia. Australian retailers and restaurant owners will now be able to deploy a Vend or TouchBistro point of sale while leveraging Square\u2019s hardware and payment processing services.\n\nWe\u2019re proud to empower developers to build tools that can help merchant grow their business \u2014 online and off. No matter what you are trying to build, Square has you covered. Learn more about Build with Square."
    },
    {
        "url": "https://medium.com/square-corner-blog/moshi-another-json-processor-624f8741f703",
        "title": "Moshi, another JSON Processor \u2013 Square Corner Blog \u2013",
        "text": "It\u2019s rare for a programmer to get to work on the same problem twice. Either the first is good enough and you\u2019re done, or it wasn\u2019t and you\u2019re foolish to try again. Circumstance and hubris have given me two opportunities to make another attempt.\n\nWhen I started at Square in 2012, the Android team was using Google Guice for dependency injection. I\u2019d contributed to Guice and so it was nice to use familiar code at my new job. But there was a problem: Guice was too slow for mobile. Even the newest Gingerbread devices weren\u2019t fast enough.\n\nI proposed fixing the problem by switching from the reflection-based Guice to a new codegen-based dependency injector that we\u2019d create. I started the prototype that would become Dagger. It was a lot easier to build something the second time! I knew which features were important and which could be left out. Dagger 1.0 borrowed Guice\u2019s core features and made them simpler and faster.\n\nGoogle has since taken over Dagger development. They\u2019ve expanded its API and improved its performance.\n\nI\u2019d also contributed to Google Gson, which Square uses for JSON encoding and databinding. As with Guice, it\u2019s nice to use something I\u2019ve worked on and am proud of. Gson is fast, secure, and easy to get started with. It has a flexible databinding model that scales up to sophisticated use cases.\n\nBut the more I use Gson, the more a few old mistakes annoy me. For example, Gson\u2019s default date format is broken:\n\nThe encoded date doesn\u2019t have a time zone so data are corrupted when the encoder and decoder don\u2019t share a time zone. This frustrating part of this bug is that it\u2019s also dangerous to fix: we don\u2019t know what applications are expecting the current format and we don\u2019t know what will break if we change it. (Gson will read RFC 3339 dates like by default).\n\nAnother wrinkle is how the library responds to bad input. When you use to convert a string to JSON it\u2019s always lenient. It considers to be equivalent to . It also returns normally for invalid inputs like the empty string and null. This hides bugs but fixing it could break existing applications.\n\n18 months ago Jake, Scott, and I created Moshi in an attempt to borrow some of Gson\u2019s core ideas and make them simpler and faster. We wanted a library that was small, efficient, and safe by default.\n\nMoshi tries to prevent errors when encoding and can detect errors when decoding. For example, if you add a field to one of your model objects Moshi will complain unless you explicitly register an adapter for that type. That way you won\u2019t inadvertently leak JDK implementation details into your JSON data.\n\nSimilarly, Moshi has a strict mode that fails on unused fields. This is a great option to enable for integration testing!\n\nWe\u2019ve been quietly completing Moshi\u2019s feature set and improving its performance. With the recent 1.4 release we added support for JSON values which convert your objects into either a byte stream or a memory model.\n\nIf you\u2019re building a new application and need JSON, please consider Moshi. It\u2019s ready."
    },
    {
        "url": "https://medium.com/square-corner-blog/anatomy-of-an-haproxy-java-tls-bug-5c0b3b8fb085",
        "title": "Anatomy of an HAProxy <-> Java TLS bug \u2013 Square Corner Blog \u2013",
        "text": "TL;DR: Java interprets the TLS specification strictly and does not allow session resumption when the connection is closed uncleanly.\n\nWhile deploying an internal load balancing project based on HAProxy to our staging environment with a configuration that included 100s of services and backends, HAProxy immediately pegged CPU usage at 100%. Since there was no traffic flowing through HAProxy yet, the only possible culprit was the health checking done for each service backend.\n\nSquare uses TLS for all service-to-service communication including health checks, so HAProxy was configured to use SSL without certificate verification. Assuming that the CPU usage spike was due to the cost of continuously creating new connections to the backend servers, a patch was created to add support for health check persistent connections to HAProxy. This resolved the problem, but since HAProxy is open source, having upstream integrate the patch would be the best solution.\n\nOnce submitted to the mailing list, Willy Tarreau \u2014 HAProxy\u2019s creator \u2014 pointed out that new TLS sessions, which involve CPU-intensive key generation, should only be created on the initial connection and that subsequent connections should use session resumption. Lightbulb, head-desk, duh.\n\nThankfully, HAProxy exposes a stats socket with number of SSL keys exchanged per second for backends.\n\nwas continuously over 300, meaning that HAProxy was establishing new (expensive) SSL sessions 300 times a second. Since HAProxy is a single-threaded, event driven server, this was saturating CPU and it was having trouble even returning the stats information.\n\nBy slowly paring down the configuration file to a bare minimum and monitoring the , the problem was eventually discovered to only be affecting Java services.\n\nUsing tcpdump and Wireshark, a snapshot of the SSL traffic between HAProxy and the backend was taken:\n\nIn the packet dump, it appeared that HAProxy would receive a Session ID from the backend and reuse it in the next connection, but the backend would still insist on a full key exchange for that new connection.\n\nThe OpenSSL client was able to confirm that session resumption still worked properly on the backend:\n\nFinally, the smoking gun was found once a small reproduction case was created.\n\nBy enabling Java\u2019s SSL debugging ( ), the logs showed that sessions were being invalidated when the connection was closed.\n\nLooking through HAProxy\u2019s health check code, there are a few places that the connection is shutdown, but most notable were these lines:\n\nin turn calls a shutw function on the SSL session with an unclean flag set:\n\nSSL_set_quiet_shutdown sets a flag that, when the SSL session is shutdown, will not send a \u201cclose notify\u201d packet to the server.\n\nThis behavior became valid in the TLS 1.1 specification:\n\nIn the log messages from Java, it\u2019s indicated that the SSL session is invalidated because of an attack against TLS found in 2013 referred to as a \u201ctruncation attack.\u201d Java mitigates this by requiring a complete TLS close sequence to allow session resumption.\n\nIn the end, the patch was essentially a five character change:\n\nBy at least attempting to cleanly shutdown the SSL session, a \u201cclose notify\u201d will (almost) always be sent and the session can be resumed cleanly. CPU usage decreased to normal. The patch was accepted and released in version 1.7.4.\n\nYou can find the entire thread here:"
    },
    {
        "url": "https://medium.com/square-corner-blog/getting-started-with-postman-and-squares-apis-e6bd0f2a8a75",
        "title": "Getting started with Postman and Square\u2019s APIs \u2013 Square Corner Blog \u2013",
        "text": "Being able to try out an API as quickly as possible is important when learning about its features. One of the tools to help you get up and running quickly is Postman, which allows you to organize and run groups of REST API calls. We have added a new collection of our API requests to make using Postman even easier\u2014great for both first-time developers and veteran Postman users.\n\nBefore you can use Postman, you need to install it. Visit https://www.getpostman.com/ and download the preferred version for your system.\n\nThis step is easy. Just click on the big Run in Postman button on this page and you\u2019ll be prompted to import the collection inside of Postman. You can also find this button in the Square API documentation and the Square API reference.\n\nOur collection makes use of Postman\u2019s environment variables to easily manage your API credentials. Clicking the Gear icon -> Manage Environments -> Add should open up a dialog to create a new environment. You can call the environment or whatever your want. You will want to add a key of with a value of your access token to handle the authentication for each endpoint. Whenever you see a variable in double brackets like that indicates a variable that can be pulled from your environment.\n\nNow it\u2019s time to try out some API calls in Postman!\n\nOur collection has a few folders in it, each with different API calls to fulfil a different task, along with a V1 & V2 reference that contains all of the endpoints. Let\u2019s look at the First Transaction folder. It has all of the calls for a standard e-commerce scenario of creating a customer, charging them, and viewing their resulting transaction. You should be able to run each request in order without any trouble. Relevant IDs in the responses will be automatically added to your environment for subsequent requests. You\u2019ll need to use your API Sandbox credentials to use the testing card nonces.\n\nYou can always learn more about the Square APIs in the documentation. Don\u2019t hesitate to reach out if you have any questions!"
    },
    {
        "url": "https://medium.com/square-corner-blog/incident-summary-2017-03-16-2f65be39297",
        "title": "Incident Summary: 2017\u201303\u201316 \u2013 Square Corner Blog \u2013",
        "text": "On March 16, beginning at 10:02 a.m. PDT, Square\u2019s serving infrastructure experienced a service disruption. Most of Square\u2019s products and services were affected in this outage, including payment processing, Point of Sale, Dashboard, Appointments, and Payroll. At 11:55 a.m. PDT, service was restored for all services except SMS delivery of two-factor authentication codes. SMS recovered at 1:12 p.m. PDT.\n\nThis postmortem document aims to communicate the root cause of this outage, document the steps that we took to diagnose and resolve the outage, and share actions that we are taking to ensure that we are properly defending our customers from service interruptions like this in the future.\n\nThis is our engineering-focused timeline. Please see https://www.issquareup.com for a high-level overview.\n\nRoot Cause: Merchant authentication service unrecoverably overloaded due to bug in adjacent service\n\nImpact: Service outage for all merchants between 10:02AM-11:55AM; 2FA outage until 1:12PM\n\n9:55 We began rolling out changes to \u201cRoster,\u201d the service that handles customer identity, in a single datacenter.\n\nRoster (and our other critical systems) are active-active in multiple datacenters. It is normal procedure for us to deploy continuously throughout the day, and overall we deploy our various services over 250 times per day to production.\n\n10:02 \u201cMultipass\u201d, our customer authentication service, began reporting timeouts to oncall engineers. This backend service is critical to most of Square\u2019s core product services.\n\n10:08 Our external monitoring notified us that api.squareup.com was down.\n\n10:10 Engineers from across Square began reporting that services depending on merchant identity were experiencing failures. It was clear to us at this point that this was a major outage. We executed a few standard operating procedures:\n\n10:23 Roster completed rollback. We believed that this would have a high probability of resolving the outage, as Multipass has a dependency on Roster and the timing of the Roster deployment lined up with the beginning of issues in Multipass. Unfortunately, this did not resolve the outage. Looking at the data in retrospect, Roster continued to be available for the duration, serving out of other datacenters, as designed.\n\n10:25 We shut Multipass off and turned it back on. Our theory was that Roster put Multipass into a bad state and that restarting it would restore service. This theory was proven false.\n\n[Support] 11:10 We noticed our status page was loading slowly, and immediately contacted our status page vendor. They updated their status page to explain that their databases were under heavy load, and were actively working on a resolution.\n\n[Engineering] 10:30\u201311:30 We split into a few workstreams of diagnosis, testing theories, and tweaking tunables in our systems. Our engineers discovered that Multipass\u2019s Redis database had peaked out on capacity \u2014 suddenly and unexpectedly:\n\nWe used both Redis SLOWLOG and MONITOR to determine what operations were hitting Redis hardest. We then traced them back to a specific code path in Multipass which retries a Redis optimistic transaction. We determined the retries had a high upper bound (500) and no backoff causing a harmful feedback loop, and started working on a fix.\n\nThough this was promising, our engineering processes dictate that we do not stop exploring solutions until we have a confirmed fix. We began identifying key services and gave them full access to the machines that they were running on. Square hosts multiple services on each of its servers and uses Linux cgroups to isolate them from each other. This limits the amount of CPU and memory that a given service is able to use on a machine. This had the effect of giving services critical to our payment processing flow the maximum possible priority on our compute resources.\n\nBy 11:45, our fix was written, reviewed, built, and tested.\n\n11:48 West coast datacenter Multipass deploy providing the ability to reduce retries on Redis transactions.\n\n11:48 multipass-redis immediately recovered as the improvement rolled out. Payments began to flow normally!\n\n[Support] 11:56 Our status page vendor updated us that a fix for their load issues had been released. However, our status page was functioning normally for a large percentage of visitors during the timeframe that they were experiencing load problems.\n\nAt this point, we began exercising all of our systems to validate that service had truly restored. We discovered that one issue remained \u2014 over the course of testing our signout/sign-in flow, we realized that two-factor authentication codes were not being sent to customers who had enabled 2FA. This was the result of a rush of SMS messages caused by the sudden rush of logins to our system. We contacted our SMS vendor and worked to remove the bottleneck in sending SMS. With them, we determined that a rebalancing of our outbound SMS phone numbers would increase capacity here.\n\n12:32 We began adding new SMS numbers to the pool.\n\n12:42 We cleared old numbers from the pool.\n\n12:46 During testing, an engineer received new 2FA code in ~10s. Recovery is underway.\n\nWe gathered information until 4:30PM, at which point we held a post-mortem meeting and discussed what went well and poorly in our response. Overall, our processes for triaging the issue worked as intended, but the thorny nature of this problem meant that they did not drive the quick resolution that we are used to. We feel that improvement in in-app messaging would have benefited our customers. We experienced some small delays in triage due to the current setup in our workroom. We took several action items, listed below, that we believe will address these issues. Bolded items directly remediate the technical root cause of the outage. Items not in bold are opportunities that we identified to improve the resilience of our systems and efficacy of our processes."
    },
    {
        "url": "https://medium.com/square-corner-blog/ok-google-charge-2-dollars-for-coffee-4d7fdbacd6ef",
        "title": "Ok Google, Charge $2 for Coffee \u2013 Square Corner Blog \u2013",
        "text": "When I received my Google Home, I immediately felt the urge to build something with it. After a good night\u2019s sleep, I had an idea: Baristas and business owners usually have their hands full behind the counter. What if they could complete transactions without tapping buttons at their point of sale? What if a merchant could accept Apple Pay transactions using only their voice?\n\nIn the video, we\u2019re using Google Home to activate the Square Contactless Reader and take a real Apple Pay transaction backed by a Square Cash virtual card. This was built in a few hours using only public APIs. Let\u2019s look at how it works! Here\u2019s what seems to be happening:\n\nWhile baristas make it look really simple, making espresso actually depends on many moving parts. Similarly, the above diagram is really a large oversimplification. Many more components were involved in enabling this seemingly simple transaction. Let\u2019s start with the connection to the Square Contactless Reader. Square does not currently offer APIs to connect directly to it. However, our Point-Of-Sale app exposes an API that lets other apps move the Point-of-Sale app to the foreground to process in-person payments. That means we can build a custom merchant app that calls the Point-of-Sale API. Once in the foreground, the Point-of-Sale app activates the NFC chip in the reader over bluetooth:\n\nLet\u2019s create a new Android app, Home Charge, that triggers the Point-of-Sale API! According to the documentation, we just need to register our new app, then add a dependency and a few lines of code: Calling in Home Charge moves the Point-of-Sale app to the foreground and activates the Contactless reader. We want to trigger this without any user touch input. That means we need a server to send a message to our app, which will then call . Let\u2019s use Firebase Cloud Messaging to push server messages to our app. The setup is straightforward, and we just need to add a few lines of custom code: We can retrieve the registration token with and manually test the app works with : Google Home provides APIs to have a two-way dialog with users. There\u2019s also a much simpler solution: IFTTT (if this, then that) supports Google Assistant triggers. It can match any text pattern with a number and a text parameter. This is perfect for us, as we want to parse a simple sentence: . We also need to define an Action in response to that Trigger. IFTTT has a Maker channel that provides a Web Request Action. Unfortunately, we cannot use it to directly call the Firebase Cloud Messaging servers, because it does not support authorization headers. Let\u2019s create an endpoint that will receive IFTTT HTTP requests and proxy those to Firebase Cloud Messaging. One quick way to do this is to set up an App Engine project and write a good old Java servlet: Then we just need to define the corresponding IFTTT Web Request Action: Now that we know the steps needed to turn voice into coffee, we can update our initial sequence diagram to include all the interactions taking place:"
    },
    {
        "url": "https://medium.com/square-corner-blog/taking-our-show-on-the-road-ee5ecc77be68",
        "title": "Taking Our Show on the Road \u2013 Square Corner Blog \u2013",
        "text": "Come meet members of our team and learn more about our APIs at some upcoming events across the country!\n\nOur first stop was at the local ForwardJS/ForwardSwift conferences here in San Francisco on March 1st & 2nd. We had a few Squares in attendance to learn about the upcoming technologies in Javascript and Swift, as well as a booth where we discussed some of the new features of our APIs and even some of our open front-end and iOS positions, (if you missed the event and are interested in our listed positions, check out Square Careers). It was great to see all the local developers interested in the latest and greatest development technologies.\n\nNext up you\u2019ll find our team in Austin, TX for the SXSW Hackathon on March 14th \u2014 15th. This year it is focused on three verticals (music, VR/AR, film) with challenges in things like creation, distribution, and most importantly for us, commerce and monetization. We are super excited to see what the hackers come up with for musicians and artists of all types to turn their passion into a profession. If you are going to be in town (even if you don\u2019t participate in the hackathon) we\u2019d love to see you at the afterparty at Green Light social! Sign up here: https://squareshackathonafterparty.splashthat.com/\n\nOur last stop in March is Devoxx US in San Jose, CA from March 21st \u2014 23rd. Py \u2694 will be giving a talk on our open source tool LeakCanary and how you can use it to automatically find memory leaks, as well as how to fix them. Other members of our team will be there to show off some of the new offerings we\u2019ll have for Java developers. If you are reading this, and will be at Devoxx, let us know! We\u2019d love to see you there. And stay tuned to hear where we\u2019ll be next."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-at-wecode-harvard-97485545c5ed",
        "title": "Square at WeCode Harvard \u2013 Square Corner Blog \u2013",
        "text": "Earlier this month, I had the privilege to again attend WeCode Harvard at Harvard University. Each year, the conference \u2014 which stands for Women Engineers Code \u2014 brings together 500 women from around the world who are passionate about technology and entrepreneurship. The weekend was filled with incredible activities, all of which underscored the fact that the community of women engineers is extraordinarily strong, supportive, engaged, and growing \u2014 quickly.\n\nConsidered the \u201cGrace Hopper of the Northeast,\u201d WeCode is a combination of panels, fireside chats, dev sessions, keynote speakers, networking events, and a career fair. What\u2019s most impressive is that the conference is entirely student run. As someone who plans and runs events for a living, this is quite an undertaking.\n\nAs soon as you walked into WeCode, you could feel the energy. It became clear over the course of WeCode that while women in tech are an incredibly diverse group, everyone shares one goal: solving core problems in the world to make it a better place.\n\nOn Saturday, Square engineer Marie Chatfield, a member of our WomEng community, hosted a sold-out session on Building & Deploying an App & Server. We were also fortunate enough to hear the keynote from renowned technologist and Professor Latanya Sweeney, the first black woman to earn a PhD in computer science from MIT. We were also able to spread the word about our upcoming Square Code Camp, Square\u2019s immersion program for young women considering or pursuing engineering careers, which will be held in Atlanta for the first time this summer.\n\nLater that day I spoke on a panel on promoting inclusivity in the classroom & workplace. This is a panel I very much wanted to be a part of, as recruiting more women in tech is simply half of the equation: We need to ensure that companies are providing the tools and resources to creating a positive, inclusive environment and retain diverse talent. At Square, we\u2019re focusing on growing a strong community by diversifying our efforts and employing a wide range of tactics. For example, we have both Women at Square and Women Engineering communities with leadership teams, meetings and sponsored events. Beyond that, we have numerous additional employee resource groups like LatinX, Black Squares Association and Squeers. And support starts at the top, with participation and backing from our CEO and executives for events like GraceHopper.\n\nThe highlight of the conference was Square hosting a screening of She Started It, a documentary about empowering the next generation of women founders. We were honored to host the film\u2019s director, Nora Poggi, and facilitate an interactive Q&A where she shared her intentions in making the film, what she learned, and how she navigated the VC world.\n\nOn Sunday, I participated in a fireside chat on Making the Most of Your Internship. I spoke about how to choose a company, what factors to consider when making a decision, how to utilize your time wisely, and making an impact. The conference closed with a career fair, and as you can see, our table was swamped with women who want to work at Square!\n\nWhile tech and computer science have many forms (a term called CS+x), every industry/vertical needs more women to fill technology roles. It was wonderful to take part in a weekend full of empowerment and enthusiasm from women who are at early stages in their careers and inspired to stay in CS \u2014 so inspired, in fact, that #WeCode2017 was trending on Twitter! There are numerous women mentors and male allies who are more than willing to help with school and career advice, overcoming imposter syndrome, and being cheerleaders for one another \u2014 it\u2019s simply a matter of reaching out.\n\nDuring the conference I asked some of the amazing women present why they chose to attend and why they thought WeCode was so important:\n\n\u201cThe WeCode Conference is probably one of my favorite things about spring semester because I love coming and seeing all the diverse women (and occasional men) there. It\u2019s always really fun to reconnect with old friends, meet new people, and have the opportunity to get face-to-face with companies. I think this supports women in tech 100% because it provides the opportunity for all of us to help one another feel empowered, loved, and supported.\u201d \u2014 Tarlon Khoubyari, UNC Greensboro\n\n\u201cA lot of what I learned was not limited to how to gain a valuable educational experience or how to push boundaries in the workplace, but rather, how to lead a really fulfilling life. We should work to create every educational setting, from the classroom to the conference room, a psychological safe space to experiment, to fail and still call it a success. Secondly, combining tech with another field is an incredibly eye-opening experience. To find a way to insert technology while respecting another field of study is to tackle a new frontier with a whole host of new challenges.\u201d \u2014 Sharon Roy, The University of Georgia\n\nI am positive that WeCode will continue to be one of the premier conferences in the Northeast and am glad to be part of such a supportive community, one that companies like Square continue to invest time and resources in fostering.\n\nSaqi Mehta is a professional career counselor who leads the University Recruiting team at Square. She is passionate about all things education, specifically women in tech and is the cofounder of ReigningIt. Saqi serves on the board of National Tech Diversity Magazine and 500 Miles. Say hi to her on Twitter!"
    },
    {
        "url": "https://medium.com/square-corner-blog/product-managing-an-integrated-hardware-product-at-square-989294765cad",
        "title": "Product managing an integrated hardware product at Square",
        "text": "I switched to hardware because I wanted to learn about something exciting and completely new. I should be careful what I wish for \u2014 the first few months felt like information overload!\n\nOf course, there are plenty of complicated software systems, and just as many simple hardware products \u2014 but the fact that there are so many Engineering teams involved in building hardware makes the PM\u2019s scope quite sizeable.\n\nA typical software PM might work with two or three different Eng teams. For hardware at Square, I can count at least nine different teams \u2014 Client (Android, iOS), Server, Embedded Software (Firmware), Hardware Security, Platform Engineering, Electrical Engineering, Mechanical Engineering, Manufacturing Test Engineering, QA Engineering \u2014 and that\u2019s just for Eng. Even for design, I work with both industrial design and software product design.\n\nIterating on a software feature or product is mostly within your team\u2019s control, but hardware manufacturing, of course, involves numerous external parties in addition to careful coordination among many teams within Square. Hardware development also brings a natural timeline that means once you cross certain milestones, you hit points of no return, where changing course can mean major delays in your roadmap \u2014 not to mention major additional costs!\n\nFor example, Square\u2019s contactless and chip reader has red and green LED lights. What if, in the middle of development, the PM had decided to add blue LEDs? Between the board design team redesigning and prototyping; the manufacturer ensuring it can produce the new design; actual production; sourcing new components (that will be delivered on time and at cost); additional testing; and updates to firmware \u2014 this process can take weeks.\n\nCompare that to a UI color change on a software product that usually takes minutes.\n\nIt\u2019s normal for a brand-new hardware product to take more than a year to ship. Even iterations on software updates for already-launched hardware products can require a combination of server, client, firmware, and OS changes, all of which make for a much more time-intensive process.\n\nHardware is, by nature, a tangible product that ships to customers, meaning hardware PMs have another separate issue on their plates: delivery and returns!\n\nCustomers can\u2019t just download their hardware, or sign up by clicking a button. Whether it\u2019s delivered to a business address or purchased through a major retailer, product purchasing requires another suite of functions: supply & demand, fulfillment, logistics, packaging, and retail partnership.\n\nPackaging plays a crucial role: it\u2019s the first way users experience the product. Good packaging is attractive, functional, and durable, not to mention includes clear instructions on getting started. And returns are much more than simply reversing delivery logistics \u2014 what about warranties, or diagnosing issues and failures, or refurbishing?\n\nYou\u2019ve probably seen labels on your phone or appliances proving your electronics conform to standards and certification specs. These regulatory bodies ensure that all electronic products used in a certain region are manufactured to meet a certain emission and safety standards. And guess what? If you\u2019re creating a payment-processing device, you\u2019re looking at an entirely separate world of certifications.\n\nThese standards are great, but sometimes they have specific set of requirements that can dictate or limit product behaviors. It provides the product team with extra challenges in coming up with ways to develop a product that meets all the requirements while not degrading the user experience."
    },
    {
        "url": "https://medium.com/square-corner-blog/how-a-square-developer-is-helping-small-business-owners-wear-their-favorite-hat-991108c9d99e",
        "title": "How a Square developer is helping small business owners wear their favorite hat",
        "text": "It all started when Donovan Janus\u2019 wife, Allison, decided to pursue her true passion of photography. When she started a photography business in 2011, business took off immediately. The problem was that, while an exceptional photographer, she didn\u2019t have experience managing the operational aspects of a business. By necessity, she started to wear many hats\u2014managing accounting, clients, invoices, and more. Operations quickly became a burden that took her away from her original purpose, and what she loved doing the most: taking beautiful photographs of people\u2019s lives.\n\nAs Donovan watched his wife spend hours dealing with paperwork, invoices, bookkeeping, and many other headaches, all while using a different app for each task, he realized that there was an untapped business need waiting to be addressed. What if he could build an application that would help small business owners manage the operational aspects of their businesses in one central place, so they could spend their time focusing on what they love and do best?\n\nThat\u2019s now 17hats\u2019 mission: Create a world where anyone with passion, ideas, and ambition will never be held back by the barriers of running their own business.\n\nIt took Donovan a couple of years to build the first iteration of the product, which launched 3 years ago. Since then, 17hats has seen impressive growth, adding thousands of new clients each month.\n\n17hats is successful because of the benefits they provide that are of value to any small business owner: saving countless hours of time, adding clients easily, and getting paid quickly.\n\nOne key element of 17hats\u2019 application is that it helps clients get paid faster \u2014 on average, in under three days. It may sound trivial, but sometimes business owners can be hesitant when asking for timely payment, and cash flow is essential when operating a small business. 17hats handles the logistics so they can see their hard-earned cash instantly, without having to manually send multiple reminder emails to clients asking for payment.\n\nA turning point for 17hats to enhance its payment feature was when Square released access to its APIs, enabling independent developers to integrate Square\u2019s payment solution within their own applications.\n\nWhen Donovan and his team found out about the Square\u2019s payment APIs through Hacker News, he says their immediate reaction was: \u201cThank God! They are finally opening their APIs.\u201d\n\nDonovan added: \u201cSquare has a fantastic brand and a lot of our customers wanted to use Square for online payments.\u201d Thus, the integration enabled them to unlock a new set of clients who were already using Square in their store and didn\u2019t want to use another payment provider through 17hats.\n\nFor Donovan\u2019s customers, having to enter their bank information into an online system can be scary \u2014 it can be hard for users to develop trust in a new payment system. But because Square has a trusted, nationwide reputation as a safe, secure, and reliable brand, it is making 17hats\u2019 job that much easier and keeping their customers satisfied.\n\nWhen the Square team discussed the integration with 17hats, Donovan told us it took a week to implement it and another week to bring it to a live customer. \u201cSquare\u2019s platform was very straightforward to use. The platform is well done technically, easy to integrate, and super reliable; we have not seen any issues; and on top of everything, the response time of the API calls is fast,\u201d said Donovan.\n\nThe feedback from their merchants is positive, and they\u2019re thrilled with the implementation. Connecting to Square is easy: merchants just open the 17hats app, select Square as their payment provider, set up the Square account, and complete the OAuth connection. It\u2019s that simple \u2014 and they can start accepting credit card payments in minutes.\n\n17hats has built an incredibly useful product for small business owners, and Square is excited to help them grow by simplifying a key aspect of their offering. They offer a growing number of services they to their customers, and the future looks bright. [Seventeen] Hats off to a very successful integration!\n\nLearn more about 17hats here and about Square\u2019s Platform here."
    },
    {
        "url": "https://medium.com/square-corner-blog/traits-that-are-important-for-product-managers-that-you-wouldnt-find-in-job-descriptions-39c17130da75",
        "title": "The traits you need as a Product Manager that you won\u2019t find in job descriptions",
        "text": "You\u2019ve read the job descriptions, familiarized yourself with the responsibilities, and understand where PMs fit in at a company. But what really makes a successful PM?\n\nAlmost all job descriptions for tech company Product Managers (PM) have something similar to the following:\n\nYou also may have heard a lot of tech companies in Silicon Valley describe the PM as a CEO of product.\n\nThose are somewhat obvious representations of what PMs do\u2014but any experienced PM knows there are many other traits that are important in being a successful PM.\n\nFor context, I am a Product Manager at Square, where I\u2019ve worked alongside around 100 engineers during my three years at the company. I\u2019ve worked on various app features, Invoices, and most recently on Square\u2019s industry-leading contactless and chip reader.\n\nI understand that every PM has a way of describing his/her role, and depending on the nature/size/stage of the company or product, PMs might have to adapt and make use of a certain set of traits more than others. But one particular view, or attitude rather, has kept me grounded and helped to develop close-knit, collaborative, successful teams.\n\nI compare my role as PM to fair wind. As defined by Google: \u201cNoun (nautical) A wind blowing in the direction the sailor wants to go, i.e. favorably\u201d. So imagine that you have a destination you want to get to (success metric, goal, product launch, etc.). You have people on a boat who want to sail to that destination (your crew, including designers, engineers, and your many cross-functional partners). There are so many different routes your boat could take to the destination\u2014and, inevitably, obstacles along the way.\n\nThe PM\u2019s job is to be the fair wind that guides the boat to its destination in the most effective way. While there\u2019s no denying that PM is a leadership role, I deliberately didn\u2019t use the analogy of the boat\u2019s captain. Fair wind is in the background \u2014 people might not notice when fair winds are blowing and it\u2019s smooth sailing, but they will definitely notice when fair winds are not there. To be a PM is largely a behind-the-scenes job.\n\nNow, back to the main point. What are some of the lesser-known traits that are important in being the fairest wind?\n\n1. Be a champion for your team\n\nThis is not necessarily unique to PMs, but is good practice for all leaders. A PM often is the \u201cface\u201d of the team as the main contact person or DRI (Directly Responsible Individual, borrowing Apple\u2019s term). This means when things are going up and to the right, the PM tends to get credit or visibility on behalf of the team from other teams or execs \u2014 and when things are going south, the PM also needs to take responsibility on behalf of the team.\n\nGood PMs will own both sides. You should be taking as many opportunities as possible to give credit to the team and individuals going above and beyond, and celebrating small wins often \u2014 and conversely, shielding the team from unnecessary distractions and blame, and helping them focus, particularly when things are not going well.\n\nLet\u2019s start with being a good note taker. :) While taking notes, be curious (identify open questions, ask for more details, question the rationale), and be clear (note the decisions made with rationale, options considered, specify next steps and action items with clear owners).\n\nIf your team is small and communicates often, you may not think this is important. But this practice serves multiple purposes beyond intra-team communications. First, you have something to go back to if your memory fails you. I don\u2019t know about you, but my senility is catching up with me. When new team members of the team or other partners ask you what you have considered, tried, and decided\u2014and why\u2014you don\u2019t want to depend on your memories. Second, as the team or company grows, there might be broader implications on decisions. It helps to communicate widely in a succinct form to keep people in the loop. Third, specifying clear next steps, action items, and owners helps give the team focus and accountability.\n\nKeep in mind that good communication doesn\u2019t just apply to within your team. For a bigger company with many product teams, the PM will also have to be the biggest evangelist for the team\u2019s work and make sure it\u2019s visible to a broader audience. Consider monthly/quarterly summary emails with key accomplishments, Lunch & Learn-type presentations to the company, a roadshow in various all-hands meetings, etc.\n\nThis may seem counterintuitive, but PMs shouldn\u2019t be the decision maker on every single thing. I see a lot of junior PMs fall into this pitfall. You do not need to have a strong say or opinion on everything. You work with smart engineers and designers, and you all share the same goal. Trust them. If an engineer needs to work with a designer, have them talk to each other directly. The PM doesn\u2019t need to babysit or facilitate. Have them own it. Verify their decision and intervene only if you have a strong opinion. Also, trust me: if they reach an impasse, they will come to you. If a decision needs to be made on a technical implementation detail, leave it to the Tech Lead or Eng Manager. If there are product implications, make sure you work with your team to weigh pros and cons. Providing autonomy on problem-solving and room for making decisions will empower the team \u2014 and free up the PM\u2019s time to do more high leverage work!\n\nWhat are other important PM traits you have learned over time?"
    },
    {
        "url": "https://medium.com/square-corner-blog/looking-back-on-the-grace-hopper-celebration-db15f4c6b125",
        "title": "Looking Back on the Grace Hopper Celebration \u2013 Square Corner Blog \u2013",
        "text": "This fall, 25 Squares attended The Grace Hopper Celebration of Women in Computing (GHC). The event may have concluded in October, but the learnings from GHC continue to impact Squares. Gloria Kimbawala and Rodion Steshenko took time to reflect on their personal experiences at GHC. Read on for more details.\n\nI was extremely excited to be part of the team that represented Square during GHC. The first time I heard about GHC was from the other women at Code Camp (Square\u2019s immersion program aimed at inspiring high school and college women to pursue careers in computer science) a couple of years ago, when they mentioned there was a conference with 8,000 women interested in computer science. Fast forward a couple of years, and the Grace Hopper Celebration now represents the gathering of nearly 17,000 women.\n\nThis year, 20 people from Square\u2019s WomEng community attended, as well as five members of the Talent team. I spent a lot of time working in Square\u2019s booth, and boy, was it busy \u2014 we ended up passing out 5,000 pieces of swag! It was great to see people all around the convention center walking around toting a Square logo.\n\nBut the event is far more than passing out swag, and several memories stand out in particular. We received so much positive feedback about our WomEng program. Many people would come up to tell me that someone they knew had a terrific experience attending our Hear + Now Events in New York and San Francisco. Others would talk about how helpful they found Square\u2019s open-sourced WomEng Handbook, and how the Handbook led them to consider creating a WomEng community in their place of business. It quickly became very apparent that the events happening in Square\u2019s WomEng community have a ripple effect on the larger Women in Tech community as a whole.\n\nI was also blown away by the number of women who were already familiar with Square\u2019s campus programs. The was a constant flow of women visiting the booth because they had applied for Code Camp, attended a campus event, or knew a friend that was an intern and had a positive experience. It was great to hear and see firsthand the positive image Square has garnered within the collegiate engineering scene.\n\nThe last piece, which was most representative of the impact that we can have on women in tech community, was listening to the feedback from interviewees. Many candidates shared that this was the first time they\u2019d interviewed with a woman engineer, and were impressed and appreciative that Square took the time to ensure each candidate would be able to do so. While I\u2019m so proud of Square for leading the charge here, it\u2019s something that needs to change \u2014 quickly \u2014 across the industry.\n\nIt\u2019s nice to know that while swag is always appreciated, what\u2019s most meaningful to GHC attendees\u2013and the broader world of women in engineering \u2014 is Square\u2019s strong community of engineers; our programs; and our impact on and ongoing work to empower the greater tech community. I\u2019m already looking forward to next year, where we hope to have even more WomEng members participate on panels, train as interviewers, engage with candidates, and most importantly, learn something new. In the end, celebrating women in computing, both within Square walls and beyond, is what\u2019s most important, and I\u2019m happy to have played a part.\n\nEarlier this year, I was asked if I\u2019d like to attend GHC. I knew why; it would be nice to have someone attend who had embedded software experience, and I had shown interest the year before. I don\u2019t recall, however, why I originally expressed interest in the first place. I wouldn\u2019t say that one-and-a-half years ago, improving diversity in tech (or any workforce for that matter) was very high on my list of concerns as an employee or as a person. Don\u2019t get me wrong, it\u2019s not that I didn\u2019t see that there was a problem, but knowing that a problem exists and actually wanting to do something about it are two very different things.\n\nAt the time, I thought that GHC stood for Grace Hopper Convention. Conventions I knew; as the proud owner of many baseball cards and comic books from the 90s, I\u2019d attended a lot of conventions. I was a convention-attending pro, I said to myself. I woke up early on the first day, got a coffee and my badge, and went straight for the opening keynote. I wasn\u2019t a fool; I\u2019d arrive early and get a nice seat in front where I wouldn\u2019t have to look at the back of someone\u2019s head and could stretch my feet out. I was ready for the convention.\n\nThe first speaker was Latanya Sweeney, who discussed the ethics of computer science and how she discovered that different Google ads were being served to those based on the perceived-ethnicity of the name. She also talked about the Journal of Technology Science and how she was able to put researchers in front of regulators to help educate decision-makers about the rapidly changing world of technology so that our government could keep up.\n\nAnother was Alyssia Jovellanos, winner of the bigly named 2016 Women Of Vision ABIE Award Winner for Student of Vision. She talked about her journey into the field of computer science, which she thanked her brother\u2019s girlfriend for introducing her to. She has co-founded a hackathon and instructed middle schoolers on the field of Computer Science, amongst many other achievements.\n\nSo here I was, sitting in the Toyota Center, where attendance had since swelled to near capacity. My father was a programmer when it was a lot less glamorous, and despite living in a one-bedroom apartment in the Bronx with four children, he bought us an Atari ST on which I toyed with animation software, played games, and wrote Transformers fan-fiction. I had it hard, right? I wasn\u2019t from wealth, right? If I could become the \u201caccomplished\u201d engineer that I am today then anyone could, right?\n\nBut I looked around the arena at the thousands in attendance, listened to Latanya Sweeney and Alyssia Jovellanos speak, and remembered the stringent criteria by which we sorted resumes leading up to GHC. I was blown away.\n\n\u201cWhat had I accomplished?\u201d I asked myself figuratively. I was overcome with two emotions: a potent mix of dread and admiration. These speakers had incredible achievements, overcoming incredible obstacles, and were actively working to improve the state of the industry \u2014 and what had I done? I was never discouraged from pursuing engineering either implicitly or explicitly, and I had my father as a role model for someone in the industry. My eyes watered a bit, as they are wont to do during moments of introspection, and I concluded: I could be doing so much more. But first, I had to focus on working the booth and interviewing candidates.\n\nThe hour-long interviews were different than any other interview I\u2019ve ever taken part in. Typically, I\u2019m interviewing someone who has been in the workforce for some time and is looking for their 2nd, 3rd, or 10th job. I know what that\u2019s like, so I talk about my progression over three-and-a-half years at Square. I\u2019m proud of my time here, and people respond well to the variety of work and the fact that my career hasn\u2019t been stagnant since joining. When I was interviewing students, I didn\u2019t think that would hook them like it would an industry vet, so I asked if they\u2019d rather hear about how I got into computer science in the first place and how I found myself at Square. No one passed up the opportunity.\n\nThere isn\u2019t necessarily anything special about my experience\u2014 but it didn\u2019t need to be remarkable. I shared my career path in the hopes they could connect with part of it \u2014 perhaps there was something about my experience that matched closely with theirs. If they could connect with my story, maybe they could feel a little less nervous, knowing that I wasn\u2019t so different from them. Maybe they could see themselves following a similar path, and finding success in this field. Maybe they could see themselves wanting an internship at Square.\n\nI also spent time at Square\u2019s booth, largely talking to prospective Squares and feeding the frenzied rush for Square tank-tops like we were in a Yeezy pop-up shop. When I\u2019d ask if anyone wanted to discuss internship opportunities at Square, someone would look my way, approach with zeal, and hand me her resume. The conversations were varied: some launched into a brief bio, some asked me what I do for Square, and some asked me what Square does. You might think after three days, it would get boring, repetitive, or annoying, but you\u2019d be wrong. Each conversation was unique. Sure, both she and I had scripts in our heads that we would begin with, but each time, it quickly evolved into a real conversation between two people, both interested in finding an opportunity. For one of us, it was an opportunity to get a first internship or job, which can be harrowing and scary; for me, it was an opportunity to find another highly talented person to join Square.\n\nI talked a lot about myself and my experiences in the field, and a lot of people had similar experiences, such as entering college with one major in mind but changing directions after their first CS class. It was nice to establish these commonalities, because, to be honest, when I first got there, I didn\u2019t think that I belonged \u2014not only was I never discouraged from pursuing my career path, I grew up the son of an engineer. But when I was there, talking to these women, I realized that we had more in common than I previously thought. We wanted to know how things worked, we wanted to make new things, and we wanted to solve puzzles.\n\nI had a list of salient conclusions that I came to because of this trip, but if there were only one worth sharing and beating the reader over the head with, it\u2019s: Don\u2019t look for others to draw conclusions for you; expand your experiences and draw your own.\n\nThere are so many other things that I could talk about, but this has to end somewhere. Still, this was a fantastic experience, and as much as I want to go again next year, I know that spots are limited, and I think that different people should go and have the same opportunity I had to take time and really think about something that affects all of us in very obvious ways.\n\nThank you to everyone who helped make this trip possible. Thank you to everyone who attended from Square. Thank you to everyone who took the time to attend our booth, come to our happy hours and lunches, and get interviewed by us.\n\nOh, and I think I owe you a tl;dr, so here it is:\n\ntl;dr: The \u2018C\u2019 in GHC stands for \u201cCelebration\u201d and that\u2019s what it was. A celebration of women in technology, and I\u2019m really happy to have taken part in it."
    },
    {
        "url": "https://medium.com/square-corner-blog/anonymous-sandbox-789459c92380",
        "title": "Anonymous Sandbox \u2013 Square Corner Blog \u2013",
        "text": "You might have noticed that we have a new section in the sample code section of our API reference. Say hello to your new Anonymous Sandbox!\n\nWe created what we call Anonymous Sandbox to make it easy to try out and explore the functionality of our APIs. You can find your credentials at the top of the page in our API Reference, which allow you to interact with a sandboxed environment without logging in or creating a Square account. We made these credentials persistent, so you should be able to use the exact same credentials the next time you visit the documentation.\n\nYou can use the anonymous sandbox credentials just like you would use the production and sandbox credentials for your Square application. Download the client library for your favorite language to add your credentials, or paste them into one of our code samples.\n\nThis sandbox is great for trying out API requests just to get the feel of the responses, or to make some API calls and see what you get back, without worrying about breaking anything in one of your \u201creal\u201d environments.\n\nThe Anonymous Sandbox is still new, and for now, you can only use it with our V2 APIs. We are working on further integrating it into our sample code and documentation so that developers can get up and running with their integrations as quickly as possible.\n\nWe\u2019d love to hear your feedback on the Anonymous Sandbox, and how it\u2019s working for you to test our APIs functionality. Lets us know your thoughts at @SquareDev."
    },
    {
        "url": "https://medium.com/square-corner-blog/developer-night-recap-544bb900d0e1",
        "title": "Developer Night Recap \u2013 Square Corner Blog \u2013",
        "text": "A few weeks ago, we invited a small group of local developers to come visit the Square offices and take a figurative peek behind the curtains to see how our APIs are made, and what we are working on in the near future.\n\nWe\u2019re so excited about the things we are working on that we wanted to share them publicly\u2014so everyone who is building with Square can share in our excitement!\n\nYou can currently connect your custom-built POS app to Square payments, but sometimes you want to start accepting payments as quickly and as easily as possible\u2014without writing a custom integration. Hosted checkout allows you to use a pre-made payment form hosted by Square to finalize purchases people make on your site. Since Square hosts all of the payments forms, you won\u2019t need to use https on your website (though it is always a good idea!).\n\nRight now, if you use Square\u2019s e-Commerce APIs, all of your sales show up as a \u201cCustom Amount\u201d in your dashboard. We know we can provide more detail, and that doesn\u2019t let you leverage any of the inventory capabilities that non-API sales utilize. As e-Commerce itemizations get released, you\u2019ll be able to pass an itemized cart with the transaction creation, or create new items on the fly that will be associated with the transaction.\n\nItems are at the core of many types of businesses, but keeping track of all of the different variations, discounts, and taxes can be complex, and isn\u2019t done very well in the current items V1 API. In the new version, we\u2019re creating a broader concept of an item catalog that will help you manage items (and all their variations & discounts) across locations. The new catalog endpoints will also allow batch uploads and changes, something that has been a top ask from developers. We\u2019re listening!\n\nOur current Webhooks implementation is limited. You are notified of changes, but then it\u2019s more work to figure out what has changed and in what way. Shifting more APIs to the v2 framework means that Webhooks is getting an upgrade, too. Webhooks v2 will add relevant information into the notification payloads, as well as offer notifications for all of our APIs\u2014not just the existing transactions, items and customers.\n\nWe\u2019ll announce the beta tests and releases of these features in the coming weeks. To make sure you hear about them, be sure to follow us on twitter (@SquareDev) and keep watching The Corner."
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-enhancements-to-squares-register-api-5ba1edd0b4b8",
        "title": "Introducing enhancements to Square\u2019s Register API \u2013 Square Corner Blog \u2013",
        "text": "Square has released a set of new features and enhancements to its developer platform. These enhancements are part of the Register API and include:\n\nNote: This is a new feature for iOS that was already available for Android.\n\nOn iOS, it is now possible to pass a customer ID into a Register API initiated transaction request. The customer ID can be retrieved using our ListCustomers Endpoint.\n\nWhen a customer ID is passed to Register API, two things happen:\n\nFor more details, visit our developer portal here.\n\nNote: This enhancement is available on both iOS and Android.\n\nPreviously, if you wanted merchants to use your app, you were required to build out the OAuth flow. This flow redirects a merchant using a third-party app to a web browser, where the merchant signs in using their Square credentials and approves certain permissions, thus enabling the third-party app to process payments on the merchant\u2019s behalf.\n\nHowever, the merchant was also required to download and sign into the Square Register App on their device, which wasn\u2019t an optimal flow.\n\nBy removing the OAuth requirement, merchants will only have to sign in once within in the Register App, and no longer via the OAuth flow. This allows you to get up and running much faster, removing the need to stand up a server to support the OAuth flow.\n\nNote: The iOS Web API has been available on Square\u2019s developer platform since March 2016.\n\nIf you are building an Android web-based POS solution that requires processing in-person payments, you can now use the Register API to do that.\n\nThe flow is as follows:\n\nA merchant uses the web-based POS on a mobile device from within a mobile browser. When it\u2019s time to charge, the merchant taps a button; the device app switches into Register; the merchant proceeds through the payment flow within Register; and when the transaction finishes, the Register app switches back into the mobile browser.\n\nFor those who don\u2019t have native mobile expertise, this new feature makes it extremely easy to integrate EMV payment processing for any business. With the Square iOS web API and this new Android version, we are now excited to offer a cross-platform web point-of-sale solution."
    },
    {
        "url": "https://medium.com/square-corner-blog/web-sockets-now-shipping-in-okhttp-3-5-463a9eec82d1",
        "title": "Web Sockets now shipping in OkHttp 3.5! \u2013 Square Corner Blog \u2013",
        "text": "Unlike the traditional request/response model of HTTP, web sockets provide fully bi-directional streaming of messages. This means that both the client and the server can send any number of messages to the other peer at any time. With today\u2019s version 3.5 release, OkHttp now offers native support for web sockets!\n\nConnect a web socket by passing a request to the method along with a listener for server-sent messages.\n\nEnqueue text or binary messages by calling or , respectively. Because OkHttp uses its own thread for sending messages, you can call from any thread (even Android\u2019s main thread).\n\nMessages received from the server will be delivered to the listener\u2019s callback as a or . The listener also has callbacks which notify you of the connection lifecycle.\n\nWe\u2019re excited to finally be able to share a stable web socket API in OkHttp. Use version 3.5 in your app by adding the following to your :\n\nThe full change log for this version is available here."
    },
    {
        "url": "https://medium.com/square-corner-blog/new-york-tech-talk-videos-a16c85d1d0cf",
        "title": "New York Tech Talk Videos \u2013 Square Corner Blog \u2013",
        "text": "Recently, Ron Shapiro and David P. Baker from Google joined me to present a pair of tech talks in Square\u2019s New York City office. (Thanks to everyone who attended!) These talks were recorded and are now available on our Square Engineering YouTube channel. Enjoy!\n\nDon\u2019t forget to subscribe to the YouTube channel for more great content in the future!"
    },
    {
        "url": "https://medium.com/square-corner-blog/inclusion-up-and-to-the-right-how-were-improving-hear-now-tech-talks-4e653c8056b1",
        "title": "Inclusion up and to the right: how we\u2019re improving Hear + Now tech talks",
        "text": "tl;dr Square WomEng cares about inclusion, which is why we\u2019re acknowledging how our tech talk series Hear + Now has failed to be as inclusive as we\u2019d like, and committing to improvements.\n\nHear + Now is a quarterly tech talk series hosted by Square WomEng. When we first started Hear + Now a year ago, we were focused on creating a space for engineers who identified primarily as women to have their voices heard. We\u2019ve had some really amazing women speak about their experiences and their areas of expertise \u2014 from first-time speakers to women seasoned on the conference circuit!\n\nAt every Hear + Now, we leave plenty of time for mingling and discussion. We know that the women engineers and allies who come to our events are all inspiring in their own right, and we love creating a space for them to meet each other and share their stories.\n\nI have really enjoyed both hosting and attending Hear + Now events this past year, but after speaking at AlterConf Portland earlier this month, I began to think about Hear + Now a bit differently.\n\nFirst of all, I was incredibly inspired by the way that AlterConf approaches inclusion. A few of the things that really stuck out to me:\n\nNeedless to say, I learned so much from all of the speakers at the conference, as well as the incredibly thoughtful way the event itself was run.\n\nReflecting on what I had heard and seen after the conference, I realized with a sinking feeling that the way that we had been running Hear + Now was unintentionally excluding a lot of people who are marginalized and underrepresented in tech.\n\nIn particular, I considered the way we marketed the event for \u201cwomen engineers and our allies,\u201d and realized that this excluded people who identify as non-binary or trans. If we didn\u2019t intentionally indicate that trans women and non-binary people were welcome, would they feel safe and comfortable joining us? With our focus on one marginalized identity (being a woman in tech), were we asking attendees with multiple, intersecting marginalized identities to minimize parts of their identities? Did queer women of color feel that they could bring their whole selves to Hear + Now?\n\nI learned at AlterConf that trans men can lose access to women-in-tech communities and resources as they transition. By reserving our event for women, we were denying trans men access to this space, or asking them to attend as our allies.\n\nWhat about other men with marginalized identities, like men of color or men with disabilities? They could also use a space for their voices to be heard \u2014 and we weren\u2019t providing that kind of platform for them with Hear + Now.\n\nI returned to Square determined to make some changes and transform Hear + Now into a more inclusive place.\n\nWhen I first proposed these changes to the rest of our WomEng leadership group, I was worried I might have to defend or explain them. Instead, each proposal was met with enthusiasm and support from our leadership team, the rest of WomEng members, and our allies. Every individual\u2019s response was, \u201cYes, this is the right thing to do, and I am so excited that we\u2019re making these changes.\u201d\n\nI shouldn\u2019t have been surprised \u2014 Squares are a pretty amazing group of people. So I am excited to announce that we are committing to the following updates to Hear + Now.\n\nOne question you may have at this point: what counts as a marginalized identity in tech? The short answer is any part of your identity that results in systematic discrimination or oppression. AlterConf has a pretty great list of marginalized groups in their speaker application. Since we won\u2019t be policing people\u2019s identities, we aren\u2019t going to require you to disclose why you identify as marginalized \u2014 we don\u2019t want to negate anyone\u2019s experiences, and we trust our community.\n\nWhether you\u2019re a repeat attendee of Hear + Now, or this is the first time you\u2019ve heard of us, we would love for you to join our next event!\n\nWe\u2019ll resume our regular tech talk series in 2017, but in the meantime we\u2019re holding a discussion night on Wednesday, November 30. If you are a person with a marginalized identity and/or you identify as an ally, we would love to see you there!\n\nCan\u2019t make it to the next event? Join our newsletter for updates from Square WomEng about Hear + Now and calls for speakers.\n\nInterested in previous talks? Check out a few videos here.\n\nWe\u2019re excited about the ways that we\u2019re making Hear + Now a more inclusive space, but we recognize there is always room for improvement. If you have any recommendations or concerns, I would love to hear from you. Find me on Twitter (@mariechatfield) or send me an email (marie@squareup.com)."
    },
    {
        "url": "https://medium.com/square-corner-blog/advice-from-the-insider-landing-your-dream-engineering-job-f90ba6475c89",
        "title": "Advice from the Insider: Landing Your Dream Engineering Job",
        "text": "When it comes to scoring a full-time position as an engineer, it\u2019s no secret that competition is fierce. That said, if joining an innovative, cutting-edge community of sharp minds strikes your fancy, a bit of prep and determination will serve you well on the road to snagging your dream role.\n\nAt Square, we believe in the art of simplicity. This guide will break down the seemingly daunting job hunt process in just four simple steps. Intrigued? Let\u2019s get started.\n\nDepending on the role you\u2019re seeking, application timelines will vary, so first things first: familiarize yourself with the key dates and plan ahead. Give yourself plenty of lead time to prep all pertinent materials in a timely manner.\n\nBelow, you\u2019ll find a sample Engineering application timeline. The specific year isn\u2019t important; what matters is that for Engineering positions slated to begin in the summer (e.g. summer 2017), your application process should begin one year full prior (e.g. summer of 2016).\n\nOnce you\u2019ve identified the timeline that best fits your job search, you\u2019re ready to get down to business and curate your professional presence. It\u2019s time to tailor your resume, cover letter and online presence to fit the kind of roles and companies you\u2019re seeking.\n\nNext up: explore and discover companies that you\u2019d like to work for.\n\nArmed with the knowledge gained during step three, it\u2019s time to make an informed decision by weighing the various options at hand and ranking what matters most.\n\nThere are multiple important factors worth considering. Take time to prioritize and assess where you\u2019re willing to compromise on things like:\n\nIf possible, contact someone at the company, whether it is a friend, acquaintance or collegiate alum, to let them know you\u2019re applying for a role there. Most companies have internal submission processes where existing employees can recommend candidates for a role. Reach out via email, Twitter, or LinkedIn, tell them you\u2019re on the job hunt, and ask if you can keep in touch if/as the interview process progresses.\n\nNow, shake off that application anxiety and dive in headfirst. Statistics show that on average, men apply for a job when they meet only 60% of the qualifications, while women wait to apply for a job where they meet 100% of the qualifications. There\u2019s absolutely nothing to lose by trying.\n\nYou\u2019ve perused, planned, polished, and prepped. Go forth and APPLY.\n\nAnd did we mention we\u2019re hiring?"
    },
    {
        "url": "https://medium.com/square-corner-blog/no-cause-for-concern-rxjava-and-retrofit-throwing-a-tantrum-96c9e4ba8a6c",
        "title": "No Cause for Concern \u2014 RxJava and Retrofit Throwing a Tantrum",
        "text": "Last week, we found an interesting API design issue in the class of the JDK that led to bugs in RxJava and Retrofit. This is a write-up of how we found those bugs.\n\nMonday morning, Nelson Osacky opens a pull request to enable RxJava Assembly Tracking in the debug build of Square Register Android.\n\nAccording to the Javadoc:\n\nHere\u2019s an example (thanks David):\n\nAn exception is raised when we call subscribe, so the stacktrace shows that something went wrong when subscribing. We\u2019re left trying to understand the error message and figuring out what all these Rx internal classes are about.\n\nWe get an extra cause at the end:\n\nIt\u2019s now clear that the failure was called by the creation of the observable. expects the source observable to emit only a single item.\n\nUh-oh, this one-line pull request is breaking a UI test my team wrote. I look at the failing code:\n\nWeird, a Retrofit (1.x) call should only ever raise a , and we\u2019re getting something else. Here\u2019s the log:\n\nThe actual stacktrace of the is missing.\n\nUgh. What is this ?\n\nInteresting. What does do?\n\nWhenever an is created, captures a stacktrace and holds on to it. Then, when an exception is thrown in the corresponding Rx chain, an is added as a cause at the bottom of the exception chain, with a message string that contains the stacktrace of where the was created. Neat!\n\nWe saw the result of this in our initial example:\n\nNow that I understand how assembly tracking works under the hood, I look at the logs again:\n\nI still don\u2019t know where this is coming from, since I don\u2019t have a stacktrace for it. The message says Cause already initialized, and we just saw a piece of code trying to do just that:\n\nTada! I just found our mystery exception. What exactly is going on with and ? Let\u2019s look at how a is constructed:\n\nA can be constructed with no cause, in which case the field is set to to mark that the cause hasn\u2019t been initialized. You can then call later\u2026 but only if the cause has not yet been initialized.\n\nWe\u2019re crashing because we\u2019re calling but the cause has already been set. Let\u2019s look at the code again:\n\nWe\u2019re going down the exception chain until we find an exception that has a cause. Then we call on it. When is returning null?\n\nWait a minute. The cause can be nonexistent or it can be unknown. Those are two different things!\n\nIn terms of Java code, here\u2019s the difference:\n\nIn both cases, returns . However, if the cause is unknown (initialized to ), then will throw an .\n\nCan we fix RxJava so that it doesn\u2019t call when the cause is unknown? Well, it turns out, there is no way to check for this.\n\nThe only option is to try and report a failure. I open a pull request in RxJava.\n\nAfter reading the Javadoc, I realize that exists for backward compatibility issues where a legacy exception class lacks a constructor that takes a cause.\n\nWhile assembly tracking seems to be built on a hack, the benefits clearly outweigh the downsides of this unknown cause edge case.\n\nNow that RxJava has proper error reporting when assembly tracking fails, I can figure out what happened in the UI test failure that triggered this investigation:\n\nWe are testing an HTTP error scenario, so Retrofit creates the raised exception with :\n\nBingo! creates an exception with an unknown cause, which is why assembly tracking is failing.\n\nonly exists in Retrofit 1.x, so I submit a pull request to fix it on the 1.x branch. There won\u2019t be a new public release, since Retrofix 2.x has been out for two years now.\n\nSquare Register is our last app not migrated to Retrofit 2. We will soon complete the migration work. In the meantime, we\u2019ll use an internal release of Retrofit 1.x.\n\nLooking into a UI test failure led us on an interesting journey! Today, we learned that:\n\nFeel free to provide more insights or ask questions!"
    },
    {
        "url": "https://medium.com/square-corner-blog/the-bikeshop-da7d52fc6317",
        "title": "The Bikeshop \u2013 Square Corner Blog \u2013",
        "text": "When I joined the Developers team at Square a few weeks ago, I was told that, as part of onboarding, every new team member makes a sample application using our APIs. Rich sample applications are great because they not only show developers how the API works, but also spark ideas about the variety of complex things developers can do with our tools. Making these sample apps gives us an opportunity to gain firsthand experience with the APIs, as well as feel the same pain points as our developers and provide the team with valuable feedback about the product.\n\nI developed a sample application called the Bikeshop. Our promotional materials for the API show a fictional point-of-sale app for a custom-built bike shop named Owen\u2019s Bikes\u2014we decided to bring these two-dimensional promo images to life and build a real application.\n\nThe main screen features a bike that the user can customize with a number of hardware parts and accessories. The checkout button triggers the Register Android SDK, which takes the customer into Square Register to complete the transaction. After the transaction completes, the result is sent back to the Bikeshop by the API and is used to display the \u201cOrder Complete\u201d page.\n\nCreating the Bikeshop was a valuable experience for a number of reasons, including developing very quickly and then iterating until the code was open-source ready. This iteration was a really fun process because it involved cleaning up my code\u2014and who doesn\u2019t love clean code? I learned (or reinforced) a few things through the process of designing and improving this application with my knowledgeable new coworkers, and while they\u2019re pretty basic concepts, I\u2019m hopeful they\u2019ll help someone on their quest for an Android application full of squeaky-clean code.\n\nOne tricky aspect of our designs was the changing layout of the two displayed lists, labeled \u201cBike\u201d and \u201cAccessories\u201d (labeled \u201cmodifiers\u201d and \u201caccessories\u201d in the application). As demonstrated in the screenshots, there are two lists side-by-side in portrait, while in landscape they\u2019re stacked.\n\nMy first thought was to create two separate lists and just stack them in landscape \u2014 but something that looks like one list but scrolls in two places would create a bad user experience. With that out of the question, I was stuck with creating two lists in portrait and one in landscape.\n\nHow to set this up? Well, fragments were out because Py \u2694 advocates against them. So I decided to use several RecyclerViews for the list(s). They would all be populated with a single adapter class, which wouldn\u2019t know what kind of list it was talking to. In the main activity, I kept three RecyclerAdapters and used one or two of them depending on the orientation.\n\nSounds simple enough, right? Right. Here\u2019s what my initial code looked like to set up the views and adapters.\n\nThis clogged up the main activity with a lot of confusing boilerplate\u2014not a great example for our developers.\n\nWhat\u2019s more, any time the activity talked to the adapter, the orientation had to be checked again to make sure the right adapter was used. More annoying if/else branches like this:\n\nThe first change to start fixing this was to move the code that talked to the adapters into a new class called the ItemManager. The goal here was to leave the main activity with as few responsibilities as possible, because that\u2019s just good practice: see the Single Responsibility Principle.\n\nNext step: The ItemManager shouldn\u2019t need to know whether the device is in landscape or portrait to call a method on the adapter. This begged for an interface, so we made one:\n\nWe implemented this interface with two classes, AdapterController.Landscape and AdapterController.Portrait, to get the desired behavior. Now when the ItemManager wants to get the note, total, or adapter it can just call the appropriate interface method. The orientation is only checked once, when the main activity creates the static AdapterController to pass to the ItemManager.\n\nNo more need for that isLandscape() method we were calling all the time, and all the confusing if/else statements could be deleted.\n\nIn the app, we load the modifiers and accessories from a static catalog. Each item has a list of options. For example, the item \u201cFrame Size\u201d contains options for the size of the frame, from \u201c42 cm\u201d to \u201c62 cm\u201d. The set of options never changes, but the user can change which option they\u2019ve selected. Originally this was reflected in my item model class:\n\n\u2026and on and on with getter methods. After a bit of review, however, we noticed a few things that could be improved about this model object. First, the getters and setters were not necessary. What I thought was best practice was leading to unnecessary method calls elsewhere in the app. Second, it looked a lot like it should be an immutable object. The same items were read from the same catalog every time, and nothing about them ever changed except the currently selected option. The warning sign here was that all but one of the fields were final. So, we decided to save the selected options in the ItemManager and let the BikeItem class be an immutable object representing an item in the catalog, which was its original purpose.\n\nIt ended up looking like this:\n\nHere\u2019s another fun cleanup. When the bikeshop receives a ChargeResult from the Register API, it takes the ORDER_NUMBER String from the Intent and passes it in a new Intent to start the TransactionSuccessActivity. The TransactionSuccessActivity then extracts the ORDER_NUMBER String from the Intent to display it to the user. This pattern resulted in the ORDER_NUMBER constant being defined in two places. Unsightly.\n\nSince the TransactionSuccessActivity was the one displaying the order number, it should be responsible for the string constant. So we removed the String from MainActivity and put it there. MainActivity still needed to pass the String along, so we created a static method in TransactionSuccessActivity.\n\nThen MainActivity just calls TransactionSuccessActivity.start() with the context and order number value and doesn\u2019t need to know about the String ORDER_NUMBER. This is a really simple change that removes duplication, prevents future bugs, and makes the MainActivity look even cleaner.\n\nThere were countless improvements to the quality of this code as we iterated. They\u2019re not all interesting, but they certainly helped me get my feet wet with Android while learning about the quality of code that\u2019s expected of a Square engineer. Of course, there\u2019s still plenty of additional clean-up potential\u2014not to mention countless features to add! Allowing users to customize the colors for each piece of the bike, or obtaining images of different bike parts so that we could swap them out when a user makes a selection, would enhance the app experience. But as much as I would love to keep improving Bikeshop, there\u2019s work to be done on our APIs!\n\nYou can find the Bikeshop sample app on GitHub with the Register Android SDK, and there are instructions for building the app so you can start playing around with it right away. Feedback and contributions are welcome. Also available on GitHub is an APK for download, so you can use the application without needing a Square developer account.\n\nWe hope that this is just the first of many full-fledged sample applications for our developers. We\u2019re optimistic that someone can use Bikeshop as a jump-off point for their own awesome point of sale. Show us what you can do!"
    },
    {
        "url": "https://medium.com/square-corner-blog/low-power-mode-managing-your-emotional-and-creative-energy-3ce6faad74a8",
        "title": "Low Power Mode: Managing Your Emotional and Creative Energy",
        "text": "Good afternoon! My name is Marie Chatfield, and I am so honored to be here with you today. I\u2019m originally from Houston, and now live in San Francisco, where I work on a front-end infrastructure team at Square.\n\nI\u2019ve been a full-time software engineer for about a year and a half now, and I fully intend to stay in this industry for years to come \u2014 so I have a vested interest in making sure I don\u2019t burn out. To that end, I\u2019ve been thinking a lot about managing energy at work, and I\u2019d like to share some of my thoughts with you in the next fifteen minutes.\n\nBefore we go any further, I\u2019d like to give a quick shoutout to the artist Gwenn Seemel, whose beautiful art is used throughout this talk. She has actually added all of her work to the public domain. You can find her on Twitter @gwennpaints or at gwennseemel.com if you\u2019d like to see more of her work or support her!\n\nWhen I first started working at Square, I joined a really amazing team of engineers working on interesting and challenging problems. Fresh out of university, I felt that there was so much I needed and wanted to learn, and I was in an environment where my questions were encouraged. I found a few communities at Square that I really cared about, including our Women in Engineering group, an a cappella group, and a boardgames night. I volunteered to meet with a middle school student every Tuesday during the fall semester through Square\u2019s partnership with local non-profit Spark. I\u2019d had some really interesting conversations about using my privilege to be a better ally, and I started an internally crowdsourced list of tangible ways to be an ally \u2014 and then worked on a Chrome extension side project that would remind you to be an ally.\n\nI was firing on all cylinders, and I loved it \u2014 I was shipping code, organizing events, and giving back to causes I was passionate about.\n\nIt wasn\u2019t long before I realized that, although all my efforts were worthwhile, I was giving too much of myself away. I was leaving work feeling exhausted and worn. I was working at a pace that wouldn\u2019t be sustainable for the life-long career I\u2019d envisioned for myself.\n\nI quickly began to learn that work-life balance is more than just the breakdown of work versus personal time on your calendar, but about the quantity and quantity of energy you exert in different areas of your life.\n\nIf my story resonates with you and your experience at all, I would love to share some of the strategies that I\u2019ve learned for managing my energy at work. I\u2019ve organized the strategies into three general categories.\n\nAs an occasional poet, I love a good extended metaphor. So today, we\u2019re going to consider the ways that we are like smartphones \u2014 we are incredibly powerful, but we have a limited amount of energy which takes time to replenish.\n\nWe\u2019ll talk first about what it means to enable low power mode, then how to charge your batteries, and finally the importance of powering down every now and then.\n\nAs a disclaimer, not all of the techniques that work for me will work for you, especially if you do not have a manager or team which respects the boundaries you set on your energy and time, even more so if you are already marginalized in tech.\n\nIf you are a manager or leader, please do all that you can to create a culture where your employees feel empowered to set and enforce boundaries. You can do this by leading by example and by not penalizing employees who do introduce limits.\n\nSo what do I mean by low power mode? It\u2019s this great setting on phones that recognizes that energy is valuable and limited. When low power mode is on, the phone stops using energy for background tasks or other non-essential work.\n\nYou can likewise enable low power mode in your own life by focusing your energy on the essential tasks, and dropping everything else. It\u2019s especially helpful to enable low power mode when you\u2019re running low on energy, but you can also hang out there forever, even if you have energy to spare.\n\nThe first step in enabling low power mode is ruthlessly prioritizing your responsibilities. There\u2019s a lot of work that we want to get done in a day, work that we think is important and should be done, plus some other stuff that we really should do. The work will never stop, but we will.\n\nSo we should constantly communicate and negotiate with our managers and our teams or other stakeholders to determine what absolutely has to happen this week, or this sprint, or this quarter. When something new comes up that just has to be done, I have found it useful to ask \u201cHow should I prioritize this new work with regards to my existing work? What can I drop in order to get this done?\u201d If the answer is that nothing can be dropped, that tells me that this new piece of work doesn\u2019t have enough priority to make it onto my todo list.\n\nRemove work notifications from your personal devices. This one isn\u2019t always practical, especially if you have an on call rotation or other times when you are expected to be highly available. So when you\u2019re not in a highly-available period, I challenge you to remove all the work stuff from your phone or laptop.\n\nI\u2019ve personally found that I can handle having work email on my phone during the week, but it\u2019s too distracting on the weekends. So I take turn off my work email on Friday afternoons, and turn it back on Monday morning.\n\nThis was a step that I had to take when I looked around one day and realized that I was on the leadership teams for both our general Women at Square group and our Women in Engineering at Square group. It was a lot for me to handle all at once, and it also meant there was less opportunity for other people to grow in those leadership roles.\n\nI was more invested and involved in the Women in Engineering group, so I decided to completely step down from the general Women group and stop going to those events for the time being. I wrapped up my leadership commitments to the Women in Engineering group, helped transfer my knowledge to the next round of the leadership team, and then enjoyed being a regular member who just shows up. Next quarter, I plan to help lead a smaller event for our Women in Engineering group that I\u2019m passionate about. I feel like I have a bit more energy to spare now, and that\u2019s where I\u2019d like to focus my time.\n\nThere may be seasons in your life when you have extra energy and you can spend it on being involved in communities. But there may also be seasons when that\u2019s not the best use of your energy.\n\nThis is important to know: if you are a member of an underrepresented group in tech, you are not obligated to show up, much less organize or lead, any event or community that is targeted for you. The only exception is if someone is literally paying you to be there.\n\nIf you have the energy to give back, that\u2019s great \u2014 but if you don\u2019t right now (or ever), that\u2019s great too. You are here, and that is enough.\n\nOn a related note: just say no to unpaid diversity work. Note that there is a difference between deciding to volunteer your time for an organization or event that you have decided is meaningful to you, and being asked to contribute your work or time for free \u2014 particularly when the beneficiary or audience of your work is a company or a non-marginalized group.\n\nYour time and your energy are worth more than that, and it is not your responsibility to be the face or voice of diversity for anyone.\n\nThe next category of strategies is to charge your batteries. The same way that we charge our phones every night, we need to find ways to replenish our own energy during our daily lives. This means identifying what gives you energy and joy, and then making time to do on a regular basis.\n\nWalk outside the office for lunch or coffee. Stroll around the block, find a nearby park, or visit a local shop or restaurant. Just get outside and enjoy the fresh air!\n\nWhen I\u2019m feeling particularly tired during an afternoon and realize that I need a quick break, I love to walk to a coffee shop down the block from my office. I\u2019ll usually invite my teammates to join me \u2014 at least one or two of them will tag along, even if they don\u2019t want to buy anything. When we get back to the office, I always feel refreshed and ready to tackle the next piece of work.\n\nTake regular screen breaks. Far too often, I\u2019ve gotten up from my desk to go refill my water or grab a snack and have immediately switched from staring at the big screen of my monitor to the small screen of my phone. I would walk around the hallways of our office like a screen zombie for a few minutes, then go back to staring at my monitor for another hour. It\u2019s no wonder my eyes and my brain would be exhausted by the end of the day!\n\nDon\u2019t be a screen zombie \u2014 try giving your eyes and your mind a break by leaving your phone at your desk and just resting for a few minutes as you walk around the office.\n\nPut time for creative outlets on your calendar. This is one of my favorite strategies \u2014 I love writing poetry, but left to my own devices, I hardly ever find the time to sit down and write. When I actually schedule an hour to go and write, and then stick to it, I always finish feeling energized after engaging my creative muscles.\n\nI can\u2019t encourage you enough to also pursue your own creative outlets. It can be so refreshing to create something completely unrelated to your day job, even if you never share the end result with anyone other than yourself.\n\nMake time for movement that gives you joy. This may look different for everyone, depending on your ability, energy, strength, and interests. Maybe you swim or wander aimlessly around the neighborhood or dance Zumba or play on a wheelchair basketball team or do breathing exercises or restorative yoga, or maybe you do all of those things.\n\nIf there is some way that you move your body that fills you with joy and makes you feel better and gives you energy, do that thing \u2014 even if it means leaving work a little bit earlier or getting in a little bit later.\n\nThe last category of strategies for managing energy is to power down. In the words of the time-honored IT advice: \u201cturn it off and back on again.\u201d\n\nEven more so than our phones, we need times to completely rest and reset. We can do this by disconnecting completely and giving yourself space to recover.\n\nLeave work at work, at a reasonable hour. It\u2019s important to not arrive too early, or stay too late, too regularly. But it\u2019s also important to not bring work home when it\u2019s possible.\n\nMake a commitment to yourself to not take work home just one day a week. If you feel comfortable doing that already, try two days or more!\n\nLearn ways to disrupt unhealthy thinking habits. When you realize that you\u2019re thinking or stressing out about work, you\u2019ll need a way to stop that train of thought. You may find that meditation, or visualization, or prayer, or singing silly songs at the top of your lungs works for you.\n\nThis is honestly something that I\u2019m still working on. On rare occasions, I\u2019ve actually been so completely entangled in trying to solve a tricky problem that I\u2019ve found myself literally dreaming about different approaches. Once, I even came up with a solution while asleep \u2014 not necessarily a good solution, mind you, just a solution \u2014 then promptly woke up in the middle of the night because I was so excited about it. But how do you manage to go back to sleep after that? I tossed and turned and finally just turned on the light and read a book until my brain was able to calm down.\n\nIf you find that you\u2019re constantly stressed or anxious about work, you may want to see a therapist or other mental health professional \u2014 which is something that I would honestly recommend for everyone at least once in your life.\n\nEnjoy your sleep and get enough of it. This can be tough, especially if you have insomnia or small children or other circumstances that might disrupt your sleep. But to the extent that it\u2019s reasonable for you, try to get some solid hours of uninterrupted sleep. If you realize that there are patterns in your life that are preventing you from resting, change them!\n\nFor example, I have realized that I sleep much better, and that the quality of the time directly before and after I sleep is much improved, when I leave my phone on the other side of the room from my bed at night. If my phone is within reach, I end up scrolling through Twitter before I sleep, and again when I first wake up, even before I\u2019m fully functional.\n\nWhen I\u2019m oncall, I prefer to sleep with my phone on full volume next to my head so that I don\u2019t miss a page or any other important notification in the night. The thing is, I rarely get paged at night \u2014 but I do have friends and familiar who live in earlier time zones than I do. Which means that the last time I was oncall, I was woken up at 6am every single morning for a straight week by texts from one of my best friends as she started going about her day. I eventually figured out a way to change my phone settings so that PagerDuty and my immediate family can contact me, but everyone else is silenced when I\u2019m asleep and oncall.\n\nFinally, take vacation and refuse to be reachable. Don\u2019t check email or messages or answer your phone while you\u2019re out of the office, and don\u2019t make promises to do so \u2014 that only perpetuates a culture of never being offline.\n\nCut the cord, and enjoy your time off. You work hard, and you need time to recover. This may be difficult especially if you work for a smaller company or run your own business, but you can start with taking a single day (or maybe just half a day) for self-care and see if you can work your way up from there.\n\nSo now we\u2019ve covered strategies for managing your energy that cover three main groups: enabling low power mode by focusing on essential tasks, charging your batteries by making time to do what you love, and powering down by disconnecting and giving yourself space to recover.\n\nI would love to hear if any of these strategies help you, or if you have tactics of managing your own energy that I\u2019ve missed. Please come find me after, or tweet me @mariechatfield.\n\nI wish you all great rest and good energy for the rest of this weekend and beyond. Thank you!"
    },
    {
        "url": "https://medium.com/square-corner-blog/android-leak-pattern-subscriptions-in-views-18f0860aa74c",
        "title": "Android leak pattern: subscriptions in views \u2013 Square Corner Blog \u2013",
        "text": "In Square Register Android, we rely on custom views to structure our app. Sometimes a view listens to changes from an object that lives longer than that view.\n\nFor instance, a HeaderView might want to listen to username changes coming from an Authenticator singleton:\n\nonFinishInflate() is a good place for an inflated custom view to find its child views, so we do that and then we subscribe to username changes.\n\nThe above code has a major bug: We never unsubscribe. When the view goes away, the Action1 stays subscribed. Because the Action1 is an anonymous class, it keeps a reference to the outer class, HeaderView. The entire view hierarchy is now leaking, and can\u2019t be garbage collected.\n\nTo fix this bug, let\u2019s unsubscribe when the view is detached from the window:\n\nProblem fixed? Not exactly. I was recently looking at a LeakCanary report, which was caused by a very similar piece of code:\n\nLet\u2019s look at the code again:\n\nSomehow View.onDetachedFromWindow() was not being called, which created the leak.\n\nWhile debugging, I realized that View.onAttachedToWindow() wasn\u2019t called, either. If a view is never attached, obviously it won\u2019t be detached. So, View.onFinishInflate() is called, but not View.onAttachedToWindow().\n\nWe\u2019re inflating the view hierarchy the typical Android way:\n\nAt that point, every view in the view hierarchy has received the View.onFinishInflate() callback, but not the View.onAttachedToWindow() callback. Here\u2019s why:\n\nViewRootImpl is where the onAttachedToWindow() call is dispatched:\n\nCool, so we don\u2019t get attached in onCreate(), what about after onStart() though? Isn\u2019t that always called after onCreate()?\n\nNot always! The Activity.onCreate() javadoc gives us the answer:\n\nWe were validating the activity intent in onCreate(), and immediately calling finish() with an error result if the content of that intent was invalid:\n\nThe view hierarchy was inflated, but never attached to the window and therefore never detached.\n\nHere\u2019s an updated version of the good old activity lifecycle diagram:\n\nWith that knowledge, we can now move the subscription code to onAttachedToWindow():\n\nThis is for the better anyway: Symmetry is good, and unlike the original implementation we can add and remove that view any number of times."
    },
    {
        "url": "https://medium.com/square-corner-blog/announcing-the-vendor-security-alliance-cb247e3f12a2",
        "title": "Announcing the Vendor Security Alliance \u2013 Square Corner Blog \u2013",
        "text": "At Square, we protect data like our business depends on it \u2014 because it does. We adhere to industry-leading standards to manage our network, secure our web and client applications, and set policies across our organization.\n\nWe believe our external partners should follow the same standards to which we ourselves adhere. We dedicate significant time and resources to vetting vendors: reviewing their policies, providing feedback, and pushing for stricter and stronger standards.\n\nThat\u2019s why we\u2019re excited to be a founding member of the Vendor Security Alliance, a coalition of companies that will seek to both streamline and strengthen the cybersecurity vetting process for vendors. We\u2019re partnering with industry leaders, including Atlassian, Uber, Airbnb, Docker, Dropbox, GoDaddy, Palantir, and Twitter, to establish a single organization that can further these vendor security efforts and represent all of us in doing so. Creating and enforcing higher cybersecurity standards will benefit companies, customers, and vendors alike, as we unite to push for a stronger, more secure ecosystem.\n\nExperts from each of the VSA\u2019s member companies will collaborate to build a robust and comprehensive questionnaire for use in evaluating and qualifying potential vendors\u2019 cybersecurity practices. This questionnaire will also help benchmark current practices across industries, as well as develop and spread standardized, widely-accepted cybersecurity standards.\n\nThe first questionnaire will be available to the public on October 1st, 2016. The VSA will develop a new questionnaire each year to ensure practices are evolving alongside advances in technology.\n\nHigher cybersecurity standards make the internet a safer place for everyone. We\u2019re excited to be a part of this important work. For more information, visit https://www.vendorsecurityalliance.org/."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-brief-guide-to-squares-product-manager-intern-program-2992753c12c8",
        "title": "A Brief Guide to Square\u2019s Product Manager Intern Program",
        "text": "Previously, Square focused on software engineering internships. But this year, for the first time, they offered Product Manager internships. Four of us were hired in total, three of us at Square\u2019s Headquarters in downtown San Francisco and the fourth in New York. One came from MIT Sloan, another from Dartmouth Tuck, the third from Wharton, and myself from Kellogg. While some of us had past experience either in tech or finance, three out of four of us had not previously been Product Managers. so don\u2019t let that prevent you from applying.\n\nThe interviewing process was straightforward and similar to that of other high tech companies. It included a phone screen followed by conversations with a current PM and Engineering Manager. At that point, you\u2019re considered for on-site interviews which consist of two to three in-person interviews. I read through Lewis Lin\u2019s book Decode & Conquer which was helpful, but in general expect questions based on analytical thinking, product strategy, and technical aptitude questions. Square places a premium on culture fit, so don\u2019t be afraid to come as you are and share what you\u2019re passionate about. Practice*: Currently, Square offers its magstripe reader for free. What are the implications of offering a free reader vs. charging for it?\n\nIn my first week, my manager shared with me an on-boarding guide that outlined a few different projects I might consider working on. There were projects of all different size and scope and each with varying degrees of business impact. My main recommendation here is to find at least one meaty project you can sink your teeth into and manage. Not only will this give you a better idea of what is required by PMs, but it will be more satisfying to start and finish something substantial. Scope it out so you can complete it by your end date, and don\u2019t forget to save time at the end to analyze and share out the results.\n\nAs the inaugural class of PM interns, my peers and I weren\u2019t quite sure what to expect (beyond the delicious free food), but we were greeted with open arms by everyone. Early on, a senior PM set up a mentorship program that gave us the chance to chat with PMs across the company. As you can imagine, they were super valuable. We also enjoyed the monthly Product All-Hands hosted by the effervescent Head of Caviar Gokul Rajaram with guest speakers across the company. We also had special Q&A sessions with our CEO Jack Dorsey and our equally fantastic CFO Sarah Friar. And every other Friday afternoon was Town Square, a company-wide All-Hands where different teams would show off what they\u2019ve been working on and Squares could ask Jack live questions about any number of company issues. Tip: As an intern (or full-time hire!), plug-in to your company and take part. You\u2019ll meet great people and feel closer to the company and its mission.\n\nFast forward to the end of your summer. By this point, you\u2019ll have analyzed product usage, written a few Product Requirements Docs (PRDs), encountered bugs on the day of your launch that you hadn\u2019t seen while testing, and through your sheer persistence (and the help of countless others), you\u2019ll have shipped your new feature. Now is the time to document the results and follow-up with people.\n\nAt Square, we pride ourselves on transparency, and we have a company-wide email alias that employees can message for any new product announcement or feature change. Did your new feature automate some existing process, improve the merchant experience, or perhaps, speed up the on-boarding process? Write about it and describe the improvements quantitatively.\n\nFinally, since no feature ever ships without the help of many, thank the people you\u2019ve worked with \u2014 whether it was just someone who gave you advice over a coffee chat or it was your primary engineering partner who\u2019s the real reason your product shipped, be sure to thank them for the time they\u2019ve spent to help you. Beyond the fact that it\u2019s just the right thing to do, Silicon Valley is a remarkably small place and your reputation matters.\n\nAnyone exploring Product Management has likely heard the phrase: \u201cPMs are the mini-CEOs of their product.\u201d It sounds great in theory, and I too, used to believe it. Now, having actually been a PM, I think that metaphor can attract people for the wrong reasons. The title CEO (mini or otherwise) connotes power, authority, and vision. The reality is you have very little power as a PM, limited authority, and while product vision is important, it doesn\u2019t mean much unless you can relentlessly execute. So before you apply, make sure you\u2019re doing it for the right reasons.\n\nThat being said, there are few things more satisfying than seeing the feature your team built go live in the product. I\u2019m thrilled to have had this opportunity and grateful to Square for launching what hopes to be a robust PM intern program. It truly was an exciting, immersive, and hands-on experience getting to build new products at Square, and I look forward to seeing what next year\u2019s group of PM interns will accomplish!"
    },
    {
        "url": "https://medium.com/square-corner-blog/empowering-businesses-what-its-like-to-intern-at-square-6ea8624a7bee",
        "title": "Empowering Businesses: What It\u2019s Like to Intern at Square",
        "text": "Square is focused on economic empowerment and making commerce easy. As an engineering intern at Square, I had the opportunity to build impactful features that actually shape sellers\u2019 lives. The company is small enough to provide hands-on problem solving and project ownership, but large enough to support strong mentorship and discussion, in my case with brilliant engineers with impressive backgrounds. I had many opportunities to not only improve my engineering skills, but also influence product decisions, voice my opinions on other projects, learn about the industry, and more.\n\nI worked on the web app for Square Appointments, an online scheduling tool that helps sellers manage their appointments. Imagine a hair salon with multiple locations, numerous employees, varying hours of business, and hundreds of appointments daily. Appointments solves the everyday issues sellers face in managing those logistics, from allowing customers to book reservations online to providing a richly featured calendar app, sending reminder texts to both sellers and customers, and more.\n\nSquare gives its interns great responsibility and provides even stronger mentorship. At Square, I was treated as an \u201cengineer\u201d first and foremost, and then as an \u201cintern.\u201d This meant my mentor had me build product improvements and take ownership of many projects over the course of the summer, just like the rest of my team. From others at larger companies, I\u2019ve heard of weeklong onboarding. Our orientation program wrapped up in one and a half days, after which my mentor helped me hit the ground running: I was able to ship several enhancements to Square Appointments in my first week.\n\nOne feature I developed was a double-booking warning that sellers now see if they attempt to schedule an appointment that is outside of an employee\u2019s available hours or in conflict with a personal obligation. What was great about building this seemingly simple idea was the collaboration, advice, and feedback loop that comes with iterative development: I iterated closely with my team\u2019s designers and product manager to account for internationalization, and made tough tradeoffs between clarity and performance. From my teammates, I learned to value simple, maintainable code over marginally faster and more complicated solutions.\n\nSomething central across the company was the concept of working efficiently. Square\u2019s cultural message was clear to me from week one: the company prioritizes fostering a positive office environment and work-life balance for Squares. Interns enjoyed several excursions outside of the office and, in mid-July, partook in Square\u2019s annual Intern Hack Week \u2014 five days to take a step back, collaborate with other interns, and build anything we wanted. Winning teams pitched to Jack and other executives. After work, I could relax in the ninth-floor \u201cTreehouse,\u201d a space for employees to take a breather and play a game or music.\n\nNear the end of my summer, I took on a larger project, a frequently requested feature for Appointments: enabling sellers to customize an employee\u2019s availability to work from week to week. My project\u2019s data model could then be refactored in the future to support other Square products outside of Appointments, such as Employee Management.\n\nThere are many steps from inception to a GA release. I needed to design new data models touching all parts of the product, build a responsive and easy-to-use UI, and implement heavy changes across the stack visible to both sellers and their customers. For the UI, my team\u2019s product lead and product manager worked with me to create a seamless experience for sellers to customize staff hours. On the engineering side, I led a design session with other engineers on my team to discuss the data model and architecture for my project. We debated various solutions focusing on simplicity, long-term implications, and compatibility with other related data models in Square\u2019s codebase. My next step was to write an engineering design document and forward it to other teams for feedback across the company. Getting grilled by more experienced engineers sounded intimidating at the outset, but it was a fantastic experience and I considered myself fortunate to get their opinions. This is something I came to love about the company: Squares love to get involved and collaborate with others, and they\u2019re accessible at every level, from individual contributor to executive team member.\n\nFollowing the design process, there were two weeks to implement the project from scratch. Far from easy, these two weeks of crunch-time were some of the best memories I had at Square. Although the path was filled with obstacles (I won\u2019t look at time zones and Daylight Saving Time the same way ever again), being pushed allowed me to grow significantly as an engineer. Pair programming proved to be a quick, fun way to break past development roadblocks, and my mentor was always there to bounce ideas off. Owning this project from inception and delivering it end to end was a rewarding challenge.\n\nOn my second-to-last day at Square, a prospective merchant came in, and I had the opportunity to demo the feature and help pitch Appointments to him. That was the highlight of my summer \u2014 as an engineer, I could think of no greater pleasure than seeing people use the feature I helped build.\n\nSpecial thanks to the Appointments team and every single Square employee for making my summer as incredible as it was!"
    },
    {
        "url": "https://medium.com/square-corner-blog/becoming-a-well-rounded-square-e1f57e3e40e1",
        "title": "Becoming a Well-Rounded Square \u2013 Square Corner Blog \u2013",
        "text": "Scattered throughout the week were technical workshops on Ruby, Rails, design, and iOS and web development. Each workshop was an introduction to its subject, but went in-depth enough for us to have learned something new that we could use in our passion project later in the week.\n\nLaura taught us about \u201cmetaprogramming\u201d with Ruby\u2019s method called define_method; Alyssa showed us MVC structures and how Rails apps work like Shake Shack \u2014 that is, with clients and servers; Nick taught us about the extensive research and constant iterations involved in the processes of making great designs; Ghassan helped many of us create our very first iPhone apps; and Marie taught us about the magic that happens behind the scenes on websites, otherwise known as back-end web development.\n\nTo me, mentorship happens constantly; it happens every time a person tells me about themselves, inspires me, or motivates me. When I asked Nick, a Square designer, about how to justify my design choices for our passion project, he spoke to me about how good design communicates ideas. His passion for and comfortability with the subject encouraged me to be more confident about my own ideas. In those few minutes that we talked, he was my mentor.\n\nAlyssa, a software engineer at Square, was assigned to be my mentor over lunch one day, and she told me about how she started Square\u2019s WomEng group \u2014 a community of women engineers at Square that began as a small group but has since expanded to become a network outside of Square. Alyssa wasn\u2019t my mentor because Gloria paired her up with me as part of the program; Alyssa was my mentor because she reminded me of my aspirations to create a similar group at my school, which previously felt a little like a pipe dream, and inspired me to act on the changes I want to see in the world.\n\nI feel so lucky to have spoken with so many talented people at Square that week, as I got more mentorship than a girl could have wished for.\n\nThe goal of our hackathon was to work with the Global Women\u2019s Institute \u2014 a nonprofit dedicated to ending violence against women and girls \u2014 to propose new features for their website\u2019s media library. When it was time to start working on our passion projects, my teammates and I gathered around a table and talked about our skills to figure out what we could build. Most of us already had experience with HTML and CSS, so we decided to recreate the website with increased functionality, using the knowledge on design principles and Ruby on Rails we just acquired.\n\nShortly after making this decision, my teammate, Aida, pitched an idea about using Bootstrap in our web app, except it wasn\u2019t just a pitch, she said, \u201chey, we could also incorporate Bootstrap \u2014 or, I mean, I couldn\u2019t, but one of you probably could.\u201d I distinctly remember being in the middle of sketching a wireframe that included all the new features for the webpage we just discussed, when I heard this statement and was immediately taken aback. I was in a room full of brilliant, talented women and one of them had just inadvertently insulted herself.\n\nI felt awful, hearing such a talented individual put herself down so blatantly, so I had to confront her on it. Before Aida could continue, I gently interrupted and said, \u201chold on, don\u2019t say that, you could totally do it, too!\u201d What followed felt like a flood of good vibes, as my other teammates enthusiastically agreed and chimed in to reassure their teammate: \u201cyou can do it!\u201d and \u201cyeah, we can help you!\u201d and more. In that moment I felt so fortunate to be part of such a wonderfully supportive community of women."
    },
    {
        "url": "https://medium.com/square-corner-blog/optimizing-ember-templates-c479d26fe58e",
        "title": "Optimizing Ember Templates \u2013 Square Corner Blog \u2013",
        "text": "When source code goes through a compiler or transpiler, it\u2019s easy to create a disconnect between the perceived weight of a piece of code and its actual size in the shipping product. This is especially true with Ember templates, where small changes can have a profound impact on the amount of JavaScript that is generated. \u201cEvery byte matters.\u201d \n\n \u2014 Every Front-end Web Developer\n\n (after exhausting the other ways of optimizing app startup) Consider a component template that renders two paragraphs, the second is conditionally rendered using an #if helper: The total size of the generated code for this component is 3.8 KiB, surprisingly large! Where does all the code come from? Each template is converted into a JavaScript module at build time. Each module contains the logic, code and data needed to create the DOM and populate its children and values (sample below). The surprise here is in the way the conditional IF block is compiled. It turns into a full template child, almost as large as the parent template, complete with metadata and rendering functions of its own (shown in red).\n\nThe {{#if}} helper is very easy to use, and mentally it\u2019s a lightweight solution to a conditional situation. However, behind the scenes each conditional block is actually represented by the same infrastructure as an entire new template. If instead we conditionally hide the paragraph element using a class name it removes half the generated code and saves 1.2 KiB off the total app size! This may seem like a controversial way to optimize a template, but it illustrates the general concept that we will continue to explore: small changes can save a lot of code, sometimes at the expense of maintainability \n\nand readability.\n\nA more complex template involving a series of If/Else statements shows how generated code can become even larger. Every conditional block is converted into a child template the same size as the red text in the example above. So while this looks like one template, it\u2019s actually eight.\n\nWe can refactor this component using a #each loop and move the creation of the instrumentList array into javascript: Even when accounting for the additional JavaScript needed to build the instrumentList, the new implementation saves 12 KiB of code. Another technique for avoiding conditional blocks in templates is to move the work into computed properties. In the example below you save more than 50% of the component size by calculating the displayed value in JavaScript.\n\nSVG also increases in size when used in templates. This SVG file renders a checkmark inside a circle; the source is 298 bytes but compiles to 2.2 KiB of JavaScript! Remember to run the SVG through a minifier like SVGO before adding it to your template. Manually inspect the result to cleanup any additional unused attributes. You can safely omit the xmlns and version attributes on inline SVG. Better yet, try moving the SVG into an external file and referencing it as a background image or as the source for an <img> tag. It will save the cost of the entire template, a wise choice if it\u2019s rarely used. Sometimes you can even create the same image using CSS! Every Ember component has an implicit wrapping tag that can be used for styling and layout. Even though it\u2019s not defined in the template, you can customize that tag using the component\u2019s JavaScript file. In the Loop example above we can optimize out the wrapping tag \n\n<section class=\u2019instruments-container\u2019> by modifying our component: Any template with a single top level element in the template is a candidate for using the implicit component tag instead. This technique also helps reduce the number of nodes in the DOM. When adding new functionality to an existing component it can be tempting to wrap the base component in a new template and relay all the attributes. This has the unfortunate side effect of creating a new template file, and can often be avoided entirely by extending the existing component: Understanding the Ember template compiler helps make these techniques more intuitive. A quick way to go deeper is to implement the same template in multiple ways and compare the results. You can use a diff tool to see how the final output compares and which method works best for your scenario. Happy templating!"
    },
    {
        "url": "https://medium.com/square-corner-blog/grpc-reaches-1-0-85728518393b",
        "title": "gRPC reaches 1.0 \u2013 Square Corner Blog \u2013",
        "text": "Last year, in collaboration with Google, we introduced gRPC to the world. Internally, the Traffic Engineering team at Square has been working on integrating gRPC with Square\u2019s infrastructure \u2014 I recently presented some of our work:\n\nOver the course of a year of active development, we\u2019ve seen a fast-growing community, a repository of contributions, and even get-togethers in meatspace.\n\nNow we\u2019re pleased to announce that gRPC has reached 1.0.\n\nWe have a powerful, feature-rich, cross-platform RPC framework backed by a vibrant, active open-source community that is capable of Google-scale performance in a production-ready release. Here\u2019s to the software engineering community never having to implement another bespoke, custom RPC layer for the next decade!\n\nGoogle has published more details on this release too.\n\nDownload, learn about, and contribute to gRPC on grpc.io. We\u2019d love to collaborate with you!"
    },
    {
        "url": "https://medium.com/square-corner-blog/about-square-engineering-5ca2a189fc15",
        "title": "About Square Engineering \u2013 Square Corner Blog \u2013",
        "text": "Square began with a simple idea: everyone should be able to accept credit cards. We\u2019ve been rethinking buying and selling ever since.\n\nFor sellers, we\u2019re creating one cohesive service to run your entire business, from a register in your pocket and analytics on your laptop, to small business financing and marketing tools that drive new sales. For buyers, we\u2019re making it faster to order from your favorite restaurants and more fun to pay your friends back.\n\nBuying and selling sound like simple things \u2014 and they should be. Somewhere along the way, they got complicated. We\u2019re working hard to make commerce easy for everyone.\n\nSquare\u2019s engineers regularly contribute to the open source community and enjoy sharing our thoughts on algorithms, security, design, and other topics that will ultimately help sellers grow their business.\n\nWe let developers build an ecosystem for businesses to seamlessly accept payments online, in store, or on the go, and we\u2019ll also share posts about our APIs and developer platform for developers to create customizable solutions that work for any size business."
    },
    {
        "url": "https://medium.com/square-corner-blog/stay-connected-21ae6a9efdd2",
        "title": "Stay Connected \u2013 Square Corner Blog \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/square-corner-blog/careers-81f0eee26898",
        "title": "Careers \u2013 Square Corner Blog \u2013",
        "text": "Square creates products and services that push boundaries and innovate business. We\u2019re looking for curious, hard-working leaders to join our team and help empower our global community of sellers. Have an idea? Pitch it \u2014 entrepreneurial minds thrive at Square.\n\nSee where you fit in. Careers at Square."
    },
    {
        "url": "https://medium.com/square-corner-blog/open-source-code-of-conduct-e5c4dc56229e",
        "title": "Open Source Code of Conduct \u2013 Square Corner Blog \u2013",
        "text": "At Square, we are committed to contributing to the open source community and simplifying the process of releasing and managing open source software. We\u2019ve seen incredible support and enthusiasm from thousands of people who have already contributed to our projects \u2014 and we want to ensure our community continues to be truly open for everyone.\n\nThis code of conduct outlines our expectations for participants, as well as steps to reporting unacceptable behavior. We are committed to providing a welcoming and inspiring community for all and expect our code of conduct to be honored.\n\nThis code is not exhaustive or complete. It serves to distill our common understanding of a collaborative, shared environment, and goals. We expect it to be followed in spirit as much as in the letter.\n\nWe encourage everyone to participate and are committed to building a community for all. Although we may not be able to satisfy everyone, we all agree that everyone is equal.\n\nWhenever a participant has made a mistake, we expect them to take responsibility for it. If someone has been harmed or offended, it is our responsibility to listen carefully and respectfully, and do our best to right the wrong.\n\nAlthough this list cannot be exhaustive, we explicitly honor diversity in age, culture, ethnicity, gender identity or expression, language, national origin, political beliefs, profession, race, religion, sexual orientation, socioeconomic status, and technical ability. We will not tolerate discrimination based on any of the protected characteristics above, including participants with disabilities.\n\nIf you experience or witness unacceptable behavior \u2014 or have any other concerns \u2014 please report it by emailing codeofconduct@squareup.com. For more details, please see our Reporting Guidelines below.\n\nSome of the ideas and wording for the statements and guidelines above were based on work by the Twitter, Ubuntu, GDC, and Django communities. We are thankful for their work."
    },
    {
        "url": "https://medium.com/square-corner-blog/upgrading-a-reverse-proxy-from-netty-3-to-4-878ec407665a",
        "title": "Upgrading a Reverse Proxy from Netty 3 to 4 \u2013 Square Corner Blog \u2013",
        "text": "Tracon is our reverse HTTP proxy powered by Netty. Several years ago, as we started to move to a microservice architecture, we realized that we needed a reverse proxy to coordinate the migration of APIs from our legacy monolith to our rapidly expanding set of microservices.\n\nWe chose to build Tracon on top of Netty in order to get efficient performance coupled with the ability to make safe and sophisticated customizations. We are also able to leverage a lot of shared Java code with the rest of our stack in order to provide rock-solid service discovery, configuration and lifecycle management, and much more!\n\nTracon was written using Netty 3 and has been in production for three years. Over its lifetime, the codebase has grown to 20,000 lines of code and tests. Thanks in large part to the Netty library, the core of this proxy application has proven so reliable that we\u2019ve expanded its use into other applications. The same library powers our internal authenticating corporate proxy. Tracon\u2019s integration with our internal dynamic service discovery system will soon power all service-to-service communication at Square. In addition to routing logic, we can capture a myriad of statistics about the traffic flowing into our datacenters.\n\nNetty 4 was released three years ago. Compared to Netty 3, the threading and memory models have been completely revamped for improved performance. Perhaps more importantly, it also provides first class support for HTTP/2. Although we\u2019ve been interested in migrating to this library for quite a while, we\u2019ve delayed upgrading because it is a major upgrade that introduces some significant breaking changes.\n\nNow that Netty 4 has been around for a while and Netty 3 has reached the end of its life, we felt that the time was ripe for an overhaul of this mission-critical piece of infrastructure. We want to allow our mobile clients to use HTTP/2 and are retooling our RPC infrastructure to use gRPC which will require our infrastructure to proxy HTTP/2. We knew this would be a multi-month effort and there would be bumps along the way. Now that the upgrade is complete, we wanted to share some of the issues we encountered and how we solved them.\n\nUnlike Netty 3, in Netty 4, outbound events happen on the same single thread as inbound events. This allowed us to simplify some of our outbound handlers by removing code that ensured thread safety. However, we also ran into an unexpected race condition because of this change.\n\nMany of our tests run with an echo server, and we assert that the client receives exactly what it sent. In one of our tests involving chunked messages, we found that we would occasionally receive all but one chunk back. The missing chunk was never at the beginning of the message, but it varied from the middle to the end.\n\nIn Netty 3, all interactions with a pipeline were thread-safe. However, in Netty 4, all pipeline events must occur on the event loop. As a result, events that originate outside of the event loop are scheduled asynchronously by Netty.\n\nIn Tracon, we proxy traffic from an inbound server channel to a separate outbound channel. Since we pool our outbound connections, the outbound channels aren\u2019t tied to the inbound event loop. Events from each event loop caused this proxy to try to write concurrently. This code was safe in Netty 3 since each write call would complete before returning. In Netty 4, we had to more carefully control what event loop could call write to prevent out of order writes.\n\nWhen upgrading an application from Netty 3, carefully audit any code for events that might fire from outside the event loop: these events will now be scheduled asynchronously.\n\nIn Netty 3, the SslHandler \u201credefines\u201d a channelConnected event to be gated on the completion of the TLS handshake instead of the TCP handshake on the socket. In Netty 4, the handler does not block the channelConnected event and instead fires a finer-grained user event:SslHandshakeCompletionEvent. Note that Netty 4 replaces channelConnected with channelActive.\n\nFor most applications, this would be an innocuous change, but Tracon uses mutually authenticated TLS to verify the identity of the services it is speaking to. When we first upgraded, we found that we lacked the expected SSLSession in the mutual authentication channelActive handler. The fix is simple: listen for the handshake completion event instead of assuming the TLS setup is complete on channelActive\n\nIn addition to our normal JVM monitoring, we added monitoring of the size and amount of NIO allocations by exporting the JMX bean java.nio:type=BufferPool,name=direct since we want to be able to understand and alert on the direct memory usage by the new pooled allocator.\n\nIn one cluster, we were able to observe an NIO memory leak using this data. Netty provides a leak detection framework to help catch errors in managing the buffer reference counts. We didn\u2019t get any leak detection errors because this leak was not actually a reference count bug!\n\nNetty 4 introduces a thread-local Recycler that serves as a general purpose object pool. By default, the recycler is eligible to retain up to 262k objects. ByteBufs are pooled by default if they are less than 64kb: that translates to a maximum of 17GB of NIO memory per buffer recycler.\n\nUnder normal conditions, it\u2019s rare to allocate enough NIO buffers to matter. However, without adequate back-pressure, a single slow reader can balloon memory usage. Even after the buffered data for the slow reader is written, the recycler does not expire old objects: the NIO memory belonging to that thread will never be freed for use by another thread. We found the recyclers completely exhausted our NIO memory space.\n\nWe\u2019ve notified the Netty project of these issues, and there are several upcoming fixes to provide saner defaults and limit the growth of objects:\n\nWe encourage all users of Netty to configure their recycler settings based on the available memory and number of threads and profiling of the application. The number of objects per recycler can be configured by setting -Dio.netty.recycler.maxCapacity and the maximum buffer size to pool is configured by -Dio.netty.threadLocalDirectBufferSize. It\u2019s safe to completely disable the recycler by setting the -Dio.netty.recycler.maxCapacity to 0, and for our applications, we have not observed any performance advantage in using the recycler.\n\nWe made another small but very important change in response to this issue: we modified our global UncaughtExceptionHandler to terminate the process if it encounters an error since we can\u2019t reasonably recover once we hit an OutOfMemoryError. This will help mitigate the effects of any potential leaks in the future.\n\nLimiting the recycler fixed the leak, but this also revealed how much memory a single slow reader could consume. This isn\u2019t new to Netty 4, but we were able to easily add backpressure using the channelWritabilityChanged event. We simply add this handler whenever we bind two channels together and remove it when the channels are unlinked.\n\nThe writability of a channel will go to not writable after the send buffer fills up to the high water mark, and it won\u2019t be marked as writable again until it falls below the low water mark. By default, the high water mark is 64kb and the low water mark is 32kb. Depending on your traffic patterns, you may need to tune these values.\n\nWhile debugging some test failures, we realized that some writes were failing silently. Outbound operations notify their futures of any failures, but if each write failure has shared failure handling, you can instead wire up a handler to cover all writes. We added a simple handler to log any failed writes:\n\nNetty 4 has an improved HTTP codec with a better API for managing chunked message content. We were able to remove some of our custom chunk handling code, but we also found a few surprises along the way!\n\nIn Netty 4, every HTTP message is converted into a chunked message. This holds true even for zero-length messages. While it\u2019s technically valid to have a 0 length chunked message, it\u2019s definitely a bit silly! We installed object aggregators to convert these messages to non-chunked encoding. Netty only provides an aggregator for inbound pipelines: we added a custom aggregator for our outbound pipelines and will be looking to contribute this upstream for other Netty users.\n\nThere are a few nuances with the new codec model. Of note, LastHttpContent is also a HttpContent. This sounds obvious, but if you aren\u2019t careful you can end up handling a message twice! Additionally, a FullHttpResponse is also an HttpResponse, an HttpContent, and a LastHttpContent. We found that we generally wanted to handle this as both an HttpResponse and a LastHttpContent, but we had to be careful to ensure that we didn\u2019t forward the message through the pipeline twice.\n\nAnother nuance we discovered in some test code: LastHttpContent may fire after the receiving side has already received the complete response if there is no body. In this case, the last content is serving as a sentinel, but the last bytes have already gone out on the wire!\n\nIn total, our change to migrate to Netty 4 touched 100+ files and 8k+ lines of code. Such a large change coupled with a new threading and memory model is bound to encounter some issues. Since 100% of our external traffic flows through this system, we needed a process to validate the safety of these changes.\n\nOur large suite of unit and integration tests was invaluable in validating the initial implementation.\n\nOnce we established confidence in the tests, we began with a \u201cdark deploy\u201d where we rolled out the proxy in a disabled state. While it didn\u2019t take any traffic, we were able to exercise a large amount of the new code by running health checks through the Netty pipeline to check the status of downstream services. We highly recommend this technique for safely rolling out any large change.\n\nAs we slowly rolled out the new code to production, we also relied on a wealth of metrics in order to compare the performance of the new code. Once we addressed all of the issues, we found that Netty 4 performance using the UnpooledByteBufAllocator is effectively identical to Netty 3. We\u2019re looking forward to using the pooled allocator in the near future for even better performance.\n\nWe\u2019d like to thank everyone involved in the Netty project. We\u2019d especially like to thank Norman Maurer / @normanmaurer for being so helpful and responsive!"
    },
    {
        "url": "https://medium.com/square-corner-blog/squares-first-ever-hack-night-71687398707e",
        "title": "Square\u2019s First-Ever Hack Night \u2013 Square Corner Blog \u2013",
        "text": "On July 19th, Square held its \u201cHack Night 0\u201d, where teams competed to build on Square\u2019s growing commerce platform using its existing and newly-released APIs.\n\nWinner Ethan Fan\u2019s creation, \u201cChatbot Ordering\u201d, integrated with Square\u2019s Items, Inventory and Register APIs to enable users to order ahead from a favorite food truck or restaurant by messaging a chat bot on Slack, then head straight to the register for quick pickup.\n\nWhen a hungry user sends a Slack message asking, \u201cWhat\u2019s for lunch?\u201d, the bot responds with available menu items using the Items and Inventory APIs, and is able to then take the user\u2019s order. When it\u2019s time to grab the food, checkout is smooth and speedy with Ethan\u2019s custom point of sale using the Register API. The crowd\u2019s favorite part? A visual tipping feature tied to emojis:\n\nJudges awarded Chatbot Ordering top prize for its simplicity, business potential, and of course, integration with Square technology.\n\nRunners-up included Eric Cochran, who integrated the Register API for Android with IFTTT to intercept successful or failed transactions as a trigger for notification-based actions, and Nimitha Ramesh, whose proof-of-concept enabled eventgoers to pay for an event upon arrival at the door using the Register API for Android.\n\nWant to hear about future Hack Nights? Follow @SquareDev and sign up to be notified."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-womeng-hear-now-tech-talks-june-2016-e174e0529834",
        "title": "Square WomEng Hear + Now Tech Talks: June 2016 \u2013 Square Corner Blog \u2013",
        "text": "Square WomEng hosts a quarterly lightning talk series, Hear + Now, that gives a voice to accomplished engineers across the tech industry. Women, genderqueer, trans, and nonbinary engineers and allies are welcome and encouraged to attend.\n\nThis quarter\u2019s theme was: How to ____? 10 Minute Guides to Get You Started. Each of our speakers presented on a practical and helpful skill related to their work in the tech industry.\n\nIf you\u2019re interested in attending or speaking at a future Hear + Now event, please join our newsletteror follow @SquareEng.\n\nWatch all the videos on our WomEng Hear + Now 2016 Q2 playlist on YouTube."
    },
    {
        "url": "https://medium.com/square-corner-blog/valet-beats-the-os-x-keychain-access-control-list-zero-day-vulnerability-3c45840c5b7f",
        "title": "Valet Beats the OS X Keychain Access Control List Zero-Day Vulnerability",
        "text": "On June 17th, The Register reported that there was a zero-day vulnerability in the iOS and OS X Keychain that compromised secure data stored in the Keychain. The article claimed that data written to the keychain could be read by a malicious application. At Square, we write iOS code that moves money. Security is always our first priority, and we took these claims very seriously. If the article\u2019s assertions were correct, the world was on fire.\n\nWhen we became aware of the vulnerability a little after 8am PDT, we immediately opened up the paper describing the attack. We found that the attack worked as follows:\n\nAt this point, we could breathe a sigh of relief: Access Control Lists only exist on Mac OS X \u2014 not iOS \u2014 so despite the headlines, our applications were not vulnerable to an attack. However we had just open sourced Valet, a cross-platform keychain wrapper, two weeks prior, and its OS X component was vulnerable. This wasn\u2019t acceptable to us, so we hardened Valet against this attack.\n\nApple\u2019s Keychain offers only three tools for updating the keychain: add, update, and delete. But we now know that on OS X, updating is inherently insecure. We cannot trust that an existing key in Keychain does not have a compromised Access Control List. So, we\u2019re left with just adding and deleting.\n\nThe solution seems obvious. Instead of updating an existing keychain entry when changing the value of an item in the keychain, delete the existing item and then add a new one.\n\nBut Apple\u2019s docs are very clear: Do not delete and then add \u2014 always update. Why? Because \u201c[w]hen you delete a keychain item, you lose any access controls and trust settings added by the user or by other applications.\u201d While that sounds ominous, it\u2019s just the effect we want. Better yet, this warning does not apply to Valet, which shares keychain values on OS X and iOS using secure Shared Access Groups rather than ACLs.\n\nOur first step was to write a unit test to examine our hypothesis. We wanted the test to mimic the attack as closely as possible; so we started by inserting a key into the keychain that had multiple applications in the ACL. This compromised key is added using the same base query as our test VALValet, so that our valet is able to read and write to the compromised key.\n\nOnce we ran this code, we saw the compromised value in the keychain.\n\nNow that we had a compromised value, we changed Valet\u2019s code on OS X to call SecItemDelete followed by SecItemAdd on setObject:forKey:. We then added the following lines to the test:\n\nAfter running that code, we saw that the value was no longer compromised.\n\nWe then added a few lines to programmatically test the compromised keychain entry was indeed deleted on setString:forKey. We ran the test locally on our Mac OS X Yosemite machine \u2014 it passed, and we pushed!\n\nThen something surprising happened. Our test failed in CI. Specifically, the line that programmatically tested that the compromised keychain entry was deleted was failing. The biggest difference between our development environment and the CI environment was that Travis CI \u2014 our CI solution for our public GitHub projects \u2014 was running Mac OS X Mavericks (10.9) rather than Yosemite (10.10). So we found a machine in the office we hadn\u2019t yet upgraded to Yosemite and ran the test locally, and voil\u00e0! Our test failed in the same spot.\n\nAfter some experimentation and a little Googling, we found that prior to Yosemite, SecItemDelete did not in fact delete keychain items that has ACLs associated with them, despite a return code suggesting that it had succeeded. This means that our patch wouldn\u2019t work on 10.9, since the vulnerability occurs when someone has maliciously added an ACL to your keychain item.\n\nWe briefly experimented with using SecKeychainItemDelete (which does manage to delete items with ACLs on them) on 10.9 machines, but then found that SecKeychainItem* and SecItem* don\u2019t play nicely together. So instead of rewriting all of Valet for 10.9 using SecKeychainItem*, we bumped Valet\u2019s minimum version to 10.10.\n\nOnly 8 hours after being made aware of the vulnerability, we had successfully patchedValet. Developers utilizing Valet on Mac OS X are immune to the hack.\n\nThink we missed something? Let us know by filing an issue or opening a PR against Valet. If you\u2019re interested in creating a SecKeychainItem* solution for 10.9, we\u2019d love to hear from you!"
    },
    {
        "url": "https://medium.com/square-corner-blog/superdelegate-the-better-app-delegate-93a5b704b231",
        "title": "SuperDelegate: The Better App Delegate \u2013 Square Corner Blog \u2013",
        "text": "A Swift framework that provides a consistent and bug-free App Delegate API across all iOS SDKs.\n\nIf you\u2019ve written an iOS application, you\u2019ve implemented a UIApplicationDelegate. UIApplicationDelegate is central to how our apps interact with iOS, but the API is incredibly idiosyncratic. When an app is launched due to a Quick Action via 3D Touch, iOS will notify the app in both the application(_:*FinishLaunching:) methods and application(_:performActionForShortcutItem:completionHandler:), unless the app signals to iOS that it handled the shortcut in application(_:willFinishLaunching:) by returning false. Handling URLs (and push notifications, and user activity items) is even worse. When an app is launched with an NSURL it\u2019ll be notified in the application(_:*FinishLaunching:) methods as well as the application(_:openURL:options:) method, and there\u2019s no way to prevent application(_:openURL:options:) from being called even if the URL is handled during application(_:*FinishLaunching:).\n\nAlso, key questions are left unanswered by the UIApplicationDelegate protocol. Want to know if a customer tapped on a push notification or if the notification was delivered due to a content-available flag? Or if calling registerUserNotificationSettings(_:) will silently work without prompting a customer for access? Each app needs to engineer their own solutions to these problems.\n\nTo make matters worse, each iOS release and SDK can introduce undocumented breaking changes into the application lifecycle. On iOS 8.4, application(_:handleWatchKitExtensionRequest:reply:) is called before application(_:didFinishLaunching:), but this isn\u2019t true on iOS 8.0\u20138.3! iOS 8.0\u20138.2 will crash if an app tries to load UI while the app is in the background, unless the app forces the creation of a sharedCIContext within application(_:didFinishLaunching:). The list goes on.\n\nEach time Apple introduces a new quirk in the application delegate contract, every app in the app store has to find the quirk and work around it to ensure their customers are not negatively impacted.\n\nInstead of making each developer learn about these quirks the hard way, let\u2019s create a central repository of UIApplicationDelegate tribal knowledge. Enter SuperDelegate. SuperDelegate takes everything we at Square learned about the UIApplicationDelegate and turns it into a clean application delegate API. SuperDelegate is stable across all versions of iOS 8 and 9 and handles boilerplate app delegate code for you. It\u2019s written entirely in Swift, and we\u2019re open sourcing it today.\n\nSuperDelegate ensures that unique notifications, URLs, user activity items, and shortcuts are be delivered to an app only once. After an app registers for user notifications the first time, SuperDelegate automatically registers for user notifications on application(_:willEnterForeground:) so that an app always knows the current state of push notification permissions. When an app receives a push notification, SuperDelegate tells the app whether the customer tapped on it. SuperDelegate guarantees that it will invoke setupApplication() and loadInterface(launchItem:) prior to forwarding any other app delegate API calls. Adopting this clean, surprise-free API makes implementing new iOS features significantly easier and faster, and will fix bugs in your existing implementation you didn\u2019t even know you had.\n\nWe\u2019re actively working to adopt Swift 3.0 and the iOS 10 SDK, and we\u2019re doing it in the open. As we find new eccentricities in the iOS 10 API, we\u2019ll file issues and update our develop/v0.9 branch. We welcome you to follow along, and encourage you to let us know what you find in the API!"
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-cleanse-a-lightweight-dependency-injection-framework-for-swift-c1b0ddefd9c9",
        "title": "Introducing Cleanse: A Lightweight Dependency Injection Framework For Swift",
        "text": "Several years ago, I was introduced to dependency injection(DI) working on a Java service here. Our Java \u201cService Container\u201d is built on top of Guice. After a small learning curve, it clearly became one of those technologies I couldn\u2019t live without. DI enables software to be loosely coupled, more testable while requiring less annoying boilerplate code.\n\nAfter working on a couple Java services, it was time for me to go back to iOS to build our lovely Square Appointments App in Objective-C. Moving to Objective-C meant giving up the power DI frameworks such as Guice and Dagger. Yes, there were and still are a couple DI implementations for Objective-C, but we felt they lacked safety, excessively used the Objective-C runtime, or were just too verbose to configure. Well, that didn\u2019t stop us. We came up with a Frankensteinian solution that used LibClang and generated code based on \u201cAnnotations\u201d. It was modeled after Dagger 1 and gave compile-time checking and several other benefits.\n\nAbout a year ago, we started adopting Swift. Having a DI library based on Objective-C started to show its weaknesses, even after we added support for creating modules with swift. We couldn\u2019t inject things that didn\u2019t bridge to Objective-C such as structs or Observables. The code generation solution required a bit of Xcode trickery such as maintaining custom build rules and having to manually touch files to trigger recompilation. We didn\u2019t want to give up DI though since we also hate writing boilerplate code!\n\nIn an ideal world, we\u2019d have implemented something like Dagger 2 for Swift. Unfortunately, Swift is lacking tooling such as annotation processors, annotations, Java-like reflection library, etc. Swift does, however, have an incredibly powerful type system.\n\nWe leverage this type system to bring you Cleanse, our dependency injection framework for Swift. Configuring Cleanse modules may look very similar to configuring Guice modules. However, a lot of inspiration has also come from Dagger 2, such as components and lack of ObjectGraph/Injectortypes which allows for unsafe practices. This lets us have a modern DI framework with a robustfeature set that we can use today!\n\nWe made a small example playground that demonstrates wiring up an HTTP client to make requests to GitHub\u2019s API.\n\nUnlike the two de facto Java DI frameworks, Dagger and Guice, which support several types ofbindings and injections, Cleanse operates primarily on Factory injection. In this context, factory is just a function type that takes 0..N arguments and returns a new instance. Conveniently, if one has a GithubListMembersServiceImpl type, GithubListMembersServiceImpl.init, is a factory for GithubListMembersServiceImpl, which makes it almost equivalent to Constructor Injection.\n\nLet\u2019s say we have a protocol defined which should list the members of a GitHub organization.\n\nAnd we implement this protocol as GithubListMembersServiceImpl\n\nWe want this implementation to be provided whenever a GithubListMembersService is requested. To do this, we configure it in a Module. Modules are the building blocks for configuring Cleanse.\n\nYou may have noticed that GithubListMembersServiceImpl requires an NSURLSession. To satisfy that requirement, we\u2019ll need to configure that as well. Let\u2019s make another module:\n\nWe can assemble these two modules in a Component. A Component is essentially a Module which also declares a Root type for an object graph. In this case, we want our root to be GithubListMembersService.\n\nNow its time to build the component and get our GithubListMembersService! We call build() on an instance of our component. This returns the Root type, GithubListMembersService.\n\nIf there are validation errors constructing our object graph, they would be thrown from the build() method.\n\nNow, let\u2019s see who the members of Square\u2019s GitHub org are:\n\nA more detailed getting started guide can be found in the README or by taking a look at ourexample app.\n\nOne can check out Cleanse on GitHub.\n\nCleanse is a work in progress, but we feel it has the building blocks for a very powerful and developer friendly DI framework. We\u2019d like to encourage community involvement for developing more advanced features (e.g. Subcomponents like in Dagger 2). Its current implementation supports both Swift 2.2 and the open source version of Swift 3.\n\nWe\u2019re in the process of completely migrating Square Appointments App to Cleanse for all our DI needs in the near future. Expect to see exciting new features, improvements, more documentation, examples, and maybe even some more articles over the coming weeks and months."
    },
    {
        "url": "https://medium.com/square-corner-blog/caviar-ios-migrating-from-advancedcollectionview-to-pjfdatasource-81f8c2e4fcdf",
        "title": "Caviar iOS: Migrating from AdvancedCollectionView to PJFDataSource",
        "text": "In December 2014, we released the first version of our iOS app for Caviar, our high quality restaurant delivery service. The view controller architecture of the app was largely based on the framework laid out in Apple\u2019s 2014 WWDC Talk: Advanced User Interfaces with Collection Views and accompanying sample code. This was a good starting point for our app, but we eventually became frustrated with it for reasons I\u2019ll detail below.\n\nAfter considering multiple approaches, we decided to build a small library reimplementing the functionality of AdvancedCollectionView that we valued the most, while leaving out any complexity we didn\u2019t need. To push us towards building a generally useful library that\u2019s not tied too closely to the Caviar app, we built this from the beginning as an external library. We\u2019ve been successfully using it for over 6 months now and have decided to to open-source it to the public as PJFDataSource.\n\nIn this post, we\u2019ll look at the creation of PJFDataSource as it evolved: as a replacement for the AdvancedCollectionView sample code in our early app architecture. If you\u2019d rather just dive in, head over to our GitHub page.\n\nBasing Caviar\u2019s iOS app architecture off of Apple\u2019s AdvancedCollectionView sample code was a great starting point, despite the issues we eventually encountered. It provided a consistent approach for loading data and display content throughout the app, and pushed us towards many best practices.\n\nMost importantly, AdvancedCollectionView helped us avoid creating Massive View Controllers, a common iOS app architecture pitfall (for which there are many goodsolutions). It did so primarily by requiring each piece of content to have a data source object, separate from the view controller itself. These data source objects are the model for your content view. In this implementation these objects are also responsible for loading their content \u2014 another responsibility that might end up in the view controller with a less disciplined approach.\n\nThese data sources can even be composed together like building blocks into an \u201caggregate data source\u201d, encouraging the creation of smaller component data sources. This allowed for code reuse for identical components in different parts of the app. For example, the list of food items you\u2019ve added to your cart and the list of food items on a historical receipt could share one of these components data sources, as they\u2019re representing the same content, just in different contexts.\n\nAdvancedCollectionView also provided a consistent approach to displaying loading, no-content, and error states. It did so by tightly coupling the data source object with the UICollectionView displaying your content, adding supplementary placeholder views when appropriate. While non-obvious at first, this became one of our favorite \u201cfeatures\u201d of AdvancedCollectionView and played a large part in the shaping of PJFDataSource.\n\nAs we moved past the initial release of Caviar for iOS and began to settle into a regular feature-building and bug-fixing release cycle, we began to recognize some drawbacks to using the AdvancedCollectionView as heavily as we did. These are all interrelated, but I\u2019ll discuss them around the themes of flexibility, stability, and community.\n\nAs you may have guessed from the name, AdvancedCollectionView requires the use of a UICollectionView. You\u2019ve probably heard the adage \u201cif all you have is a hammer, everything looks like a nail\u201d. For us, UICollectionView was our hammer, even though it wasn\u2019t always the tool best suited for the job. This lack of flexibility hit us in several ways.\n\nA simple, concrete example comes from the Home screen, where Caviar lists all of the restaurants you can order from. In our larger markets, this can be hundreds of restaurants, each with a variable-height cell depending on the associated metadata. Traditionally, collection views and table views need to determine the size of every cell in order to calculate its own content size. With a lot of content, this can be slow and negatively impact the user experience. For UITableViews, Apple added tableView:estimatedHeightForRowAtIndexPath:, an elegant solution for this issue. Without access to something similar in our collection view layout at the time, we were forced to implement our own fast view sizing and caching \u2014 complexity we would have preferred to avoid.\n\nMore generally, using UICollectionViews nearly everywhere in the Caviar app was simply overkill. Almost all of the screens could be implemented more simply using a UITableView. Some could even be implemented with a UIStackView-based layout (with OAStackView for iOS 8 compatibility), or even with manual UIView-based layout.\n\nThis experience was the main reason we decided that PJFDataSource should be view agnostic, leaving it up to the app to provide its own content view. Rather than being tightly coupled with the collection view, PJFDataSource provides a \u201ccontent wrapper view\u201d, which will display either the content view you\u2019ve provided, or one of the various placeholders for handling loading, no-content, and error states.\n\nIn the Caviar app today, we try to use the tool that fits best. The Home screen is now a UITableView, taking advantage of automatic cell sizing and using estimated heights for speed. The Menu view, where we show mouth-watering photos of all the food available at a restaurant in a waterfall layout uses a UICollectionVIew. The Account view, with its limited content, uses a UIStackView \u2014 one of my current favorite tools.\n\nOnce the Caviar app was publicly released and being used by orders of magnitude more customers than our internal test group, we found that a few of those mysterious \u201cone off\u201d crashes weren\u2019t \u201cone off\u201d at all. Instead, they were early warnings signs of crashing bugs somewhere in our data source stack.\n\nOnce we realized this, we dug in deep to find and fix the underlying issues. We set a target of 99.9% crash free users, as measured by Crashlytics. We added Crashlytics logging to support better forensic investigation after a crash in the wild. We wrote automated tests to stress-test problematic areas. We learned a lot about common bugs and/or misuses of UICollectionView. We fixed several issues, making a dent in our crash rate. But we couldn\u2019t figure out all the issues, and we couldn\u2019t quite get the crash rate down to our target.\n\nThese mysterious crashers were a major driver in our decision to ultimately abandon AdvancedCollectionView and create PJFDataSource. We made hitting our crash rate target the primary goal of the migration. We staged our engineering and rollout incrementally, prioritizing the pieces of the app most affected by these crashes.\n\nLet\u2019s look at some real numbers from the last half of 2015 for context:\n\nAs you can see, we started with crash-free users rate of about 97.5%. This means that a customer using our app had about a 2.5% of seeing a crash on any given day. Ouch.\n\nWe made some incremental improvements (and regressions) throughout the first couple months. In our mid-September update, we shipped the first version with a partially rolled-out PJFDataSource. We see a great improvement right off the bat, and then continue to improve as we update more portions of the app and our customers upgrade to the latest version. In the last ~6 weeks of the year, we manage to finally hit our 99.9% crash free target, which we continue to maintain today.\n\nI strongly recommend setting an aggressive target like this for your crash-rate, and quickly fixing any regressions that cause you to dip below. This ensures a strong signal-to-noise ratio in your crash reporting, making regressions easy to spot and fix. It also helps you notice the crashes faster, while the recently-changed code is still fresh in your mind.\n\nIn retrospect, one issue with AdvancedCollectionView that we initially underestimated was the lack of community support for it. There wasn\u2019t a canonical home for it on GitHub where you could interact with the authors or other users. There wasn\u2019t anyone planning to fix bugs. There wasn\u2019t anyone making compatibility fixes when a new version of iOS or Xcode came out. There weren\u2019t any experts for us to lean on when we hit a wall.\n\nAll of this was clear from the beginning, and is inherent in taking a 6,000 line sample project from WWDC and building on it \u2014 we just underestimated the long-term cost. This experience made me better appreciate the many vibrant communities that exist throughout the iOS ecosystem.\n\nIt\u2019s worth reiterating how incremental this change was, as I think it\u2019s a good example of how to consider and execute major architectural changes in a shipping app:\n\n2. Look for smaller changes to address\n\n3. Come up with a plan and goals\n\nHopefully this backstory on the creation and evolution of PJFDataSource will be of some use to you next time you\u2019re evaluating a potential dependency or considering a significant refactoring.\n\nIf it looks like something you might want to use yourself, head over to our GitHub page."
    },
    {
        "url": "https://medium.com/square-corner-blog/xcknife-faster-distributed-tests-for-ios-e1eb6c23a6d5",
        "title": "XCKnife: faster distributed tests for iOS \u2013 Square Corner Blog \u2013",
        "text": "At Square, we highly value the efficiency of our engineers. As our codebases grow and our test suite reaches tens of thousands of tests, our engineers wait longer and longer for their builds to be green in our Continuous Integration system.\n\nWe recently optimized our internal testing infrastructure to allow our iOS test suite to be easily and efficiently distributed across dozens of machines. In order to provide our engineers with the fastest results possible for their pull request tests, we also had to properly balance the test shards, as your test suite will always be as fast as its slowest component.\n\nA perfectly balanced test suite would look like this:\n\nGenerating a perfectly balanced sharded test is equivalent to a known hard problem of scheduling jobs and machines. Approximating it is fast enough, and we empirically observed it to be about 10% off the optimal partition.\n\nIn order to compute the approximated balanced shards we created a project called XCKnife, which we are open sourcing today. XCKnife partitions XCTestCase tests in a way that minimizes total execution time (aka makespan). After employing it we saw 30% faster CI test builds on our largest project.\n\nXCKnife works by leveraging Xctool\u2019s json-stream reporter output for both historical timing information and for current test suite definition. Xctool is an open source tool created by Facebook, that serves as a replacement for Apple\u2019s xcodebuild and makes it easier to test iOS and Mac products.\n\nXCKnife generates a list of only arguments meant to be passed to Xctool\u2019s -only test arguments, but alternatively could be used to generate multiple xcschemes with the proper test partitions.\n\nXCKnife also automatically handles deleted tests and test targets, and extrapolates the expected time for new ones.\n\nIt is common for our test suites to contain multiple partition sets, or multiple lists of test targets that are run in different configurations. For example, given the following test structure (from this sample project):\n\nThe three test targets (_CommonTestTarget_, iPadTestTarget and iPhoneTestTarget) each have a list of test classes, each of which take different timing data. We can group these test targets into three partition sets, for example:\n\nUsing XCKnife to split the iPad and the Basic partition sets within 3 machines, we get a json representing following partition:\n\nXCKnife will partition at most classes rather than methods, as otherwise it would the call the class\u2019 initialization more than once, which is not always safe if, for instance, you use network connections that mutate state (like making Rest calls to an external stateful web service).\n\nXCKnife is available both as an executable and as a Ruby gem. You can install it with:\n\nWhich yields the partitioning information as json:\n\nFor more detailed examples on using XCKnife, refer to the docs.\n\nThe 2-approximation for balancing a single partition set is a greedy algorithm that is O(N) after a sorting step O(N logN), where all the test classes are sorted by their computed time and put in a queue with the longest time classes in the front. Each iteration step takes the a test class from the queue and assigns it to the machine that is the least busy.\n\nThe 2-approximation means the final makespan for each partition set looks like this:\n\nOf course, test time variance and extrapolations play a key role here as well.\n\nXCKnife will report the test imbalances for both partition sets amongst themselves and for its internal partitions. These are always reported against an ideal scenario (average time), and cannot be necessarily attainable. For example, if you have two partitions of two test classes, each with one test method, with expected times of 1 second and 11 seconds, the average time is 6 seconds, but since we can\u2019t break the test method in two, the ideal scenario is not attainable.\n\nThe chart below exemplifies a situation where the ideal time would be 1 second, but the total computed times for the partitions are 0.5, 1.1, 0.8 and 1.3:\n\nThanks to Dimitris Koutsogiorgas and Justin Martin for design and code reviews throughout the project.\n\nTo send contributions, go to XCKnife\u2019s GitHub page."
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-squares-register-api-for-android-85e4cee5aa3d",
        "title": "Introducing Square\u2019s Register API for Android \u2013 Square Corner Blog \u2013",
        "text": "Following our launch of Register API for iOS in March, beginning today, developers can now build custom Android point-of-sale applications that take swipe, dip, or tap payments through Square hardware, and integrate with Square\u2019s software and services. This builds on our existing API offerings for Square Register and eCommerce.\n\nThe Square Register API lets you focus on what you do best: creating an amazing point-of-sale experience for your merchants while Square takes care of moving the money. You can build a custom point of sale with specific features for your business\u2019 needs, or start a technology company for a new point of sale and sell it to businesses. Get started today by creating your Android application!\n\nMaybe you will build a custom point of sale for lawn care service companies, an in-store on-floor retail business checkout, an optimized cart and membership funnel for wineries, a self checkout for doggie daycares \u2014 your imagination is the limit! Your app doesn\u2019t have to handle any payments information, which makes PCI compliance a non-event. And you won\u2019t need to think for a second about integrating with hardware card readers. Build your custom app and distribute it like normal on the Play Store. When it is time for your app to initiate a payment, call our SDK with an amount to start the Square Register app on the payment screen. The buyer completes the payment in Register (by swiping, tapping, dipping, or keying in the card) and then focus and control automatically returns back to your app with the result of the charge. Thanks to Square Register, all the money movement heavy lifting is taken care of. Android Register API supports all our hardware, including the new Square Contactless + Chip Reader.\n\nTo start taking payments, it\u2019s as simple as three lines of code:\n\nSquare Register will come to the foreground and complete the payment on your behalf. Once that\u2019s done, we\u2019ll return the payment result to your app.\n\nPricing is the same as other payments completed using Square Register. The Register API for Android is currently only available in the US and Canada, with other markets to quickly follow.\n\nWe are busy building out Square\u2019s commerce platform, to give merchants solutions to help them easily run their business. We\u2019re eager to hear your feedback! You can reach us at developers@squareup.com and follow @SquareDev on Twitter for more updates about Square\u2019s developer platform and community. Get started today by creating your Android application!"
    },
    {
        "url": "https://medium.com/square-corner-blog/content-security-policy-for-single-page-web-apps-78f2b2cf1757",
        "title": "Content Security Policy for Single Page Web Apps \u2013 Square Corner Blog \u2013",
        "text": "This week we fully enabled CSP on cash.me. It\u2019s been months in the making and the result is a safer experience for all our customers. This is how we did it.\n\nImplementing Content Security Policy (CSP) on an established website is like flipping over a rock while you\u2019re on a hike, exposing a world of creepy crawlers that you know are there but don\u2019t really expect. It surfaces all the connections, sources, redirects, iframes, and unexpected bad things too!\n\nCSP is a set of rules, provided by the server, that instructs the browser which sources, destinations, and protocols are permitted for each type of resource. It allows you to restrict which domains images can be loaded from and JavaScript can connect to. You can also completely disallow inline scripts.\n\nWhen used correctly CSP provides a second line of defense against cross site scripting vulnerabilities that happen despite our best efforts and practices. It can also protect customers from some types of malicious browser plugins, malware, and compromised JavaScript dependencies. And finally, it provides a way to report when bad things are happening.\n\nIn particular, CSP helps against certain types of browser malware which can compromise customer browsing without exploiting a flaw in the website itself. In a 24 hour period the Square Cash CSP policy blocks hundreds of script injections and many outgoing connection attempts that are caused by trackers and malware in infected browsers.\n\nThe purpose of CSP is to block bad things from happening. It\u2019s best to take an iterative approach, deploying in stages, to avoid accidentally disabling part of your web app.\n\nThis is process we followed:\n\nStart by listing source domains (with protocols) that you expect your application to load resources from or connect to. Doing this first allows you to start with a strict policy that doesn\u2019t have many exceptions. Later you can add things that can\u2019t be fixed.\n\nBelow is a sample policy that has been annotated. It is based on the Square Cash policy:\n\nA strong default-src is important because it represents any directives that are not explicitly defined. In the example above default-src will be allied to the missing font-src, object-src, media-src and any other directives not explicitly listed. The policy contributes to total response size on every HTML response. Smaller is faster, and fewer definitions are easier to understand.\n\nTip: Avoid using wildcard domains unless you are confident that all subdomains are secured. If an attacker can create a new subdomain and host malicious content there, they can bypass CSP protections.\n\nDeploying the initial policy with the HTTP header Content-Security-Policy-Report-Only instructs the browser to report the violations but also them to execute. Violations are reported to the URL defined in the report-uri section. This lets us collect information about violations and fix issues incrementally.\n\nDepending on your analytics framework, you may want to listen for securitypolicyviolation events with JavaScript and collect more information about the client before reporting.\n\nFixing violations in the application can be tricky. Resist the urge to loosen the policy. Sometimes this means changing the way a feature works, or choosing a new library. Additionally, many popular dependencies present their own interesting policy requirements:\n\nAngular and CSP\n\nYou need to opt-in when using CSP with AngularJS. Add the attribute directive ng-csp to your top level angular application tag and include the angular-csp.css style sheet.\n\nBrowser Extensions\n\nTo allow browser extensions to interact with the web page content include safari-extension:// and chrome-extension:// for the directives you\u2019d like to allow.\n\nGoogle Conversion Tracking\n\nThe Google Conversion Tracking JavaScript generates a series of redirects that end in the Google Top Level Domain of the local user (.ca .com .au, etc). It\u2019s unfeasible to list all the google domains, and we can\u2019t use a wildcard in that position. A compromise is to list countries you are most interested in.\n\niOS Native App Deep Linking URLs\n\nNative app URLs have their own protocol. Square Cash uses squarecash://square.com/\u2026 to deep link into the app. We use the iframe deep linking technique and it requires adding the protocol squarecash: to both script-src and frame-src.\n\nThere are two ways to deploy a policy: HTTP headers, and HTML <meta> tags. In order to get comprehensive coverage on target browsers Cash.me required both.\n\nHeaders are more effective than <meta> tags because they are applied immediately before any content can be modified. If both are present, the meta tag can be used to strengthen the headers policy, but it cannot add new domains as valid sources. We use this fact later to make Safari more secure. (More on multiple policies).\n\nWhen you\u2019re confident that the remaining Violation Reports are for things you want to block, change the header name to Content-Security-Policy. Violations will continue to be reported to the same report-uri, but will now be blocked from executing.\n\nWhen building a single page web app that bootstraps data in the template (like the sample below), we need at least one inline script to deliver the initial data payload. This is in direct competition with our desire to avoid specifying script-src unsafe-inline (because that is what stops most XSS attacks).\n\nFortunately there\u2019s a way to bless individual script tags with a nonce. Nonces are server generated tokens applied to individual script tags, like this: <script nonce=\u201d{{scriptNonce}}\u201d>. Nonce values must be included in the script-src section of the CSP policy and can only be delivered in headers (they don\u2019t work in meta tags).\n\nTip: Script nonces must be unique random values generated for each request. If they are guessable an attacker could predict the nonce and bypass your policy.\n\nTo work around Safari\u2019s lack of support for script nonces in CSP Level 2, we serve a Content-Security-Policy header with the script-src directive that includes both a nonce and unsafe-inline. At first look this seems like an error, but luckily browsers that support nonces will see the nonce and ignore the unsafe-inline.\n\nWe then serve a more strict version of the directive (omitting the \u2018unsafe-inline\u2019) into the HTML template where it can be immediately turned into a <meta> tag after the first inline script has been executed. This provides XSS protection against script injections from user-supplied content that gets generated by your application logic later.\n\nThe technique above completes our support across the evergreen versions of all major browsers, and degrades gracefully on older browsers.\n\nThis is exactly the setup we hope for when building progressive web apps. The CSP deployment silently added a new layer of security that works today, without dropping support for older browsers.\n\nThanks to Daniele Perito and Sean Slinsky for their assistance!"
    },
    {
        "url": "https://medium.com/square-corner-blog/tailoring-pants-for-square-6c47ea2d729d",
        "title": "Tailoring Pants for Square \u2013 Square Corner Blog \u2013",
        "text": "This week, the Pants project announced a 1.0 release of the open source Pants Build System. The 1.0 release of Pants indicates that the tool is ready for more widespread adoption.\n\nSquare is a proud contributor to Pants. Developers at Square have been using and contributing to Pants since 2014 to develop our Java services. When we first joined the project, we found a tool that required lots of customization and insider knowledge to install and operate. Today the tool has a streamlined installation process, extensive documentation, a clean extensible design, a vibrant community and a history of stable weekly releases.\n\nWith Pants we get:\n\nTo understand why Square uses a tool like Pants, it helps to understand our software lifecycle. We use a monolithic codebase (monorepo) for many of the same reasonsthat Google does.\n\nWe build and release services from HEAD of master. Our Java codebase is housed almost entirely in a single repo consisting of over 900 projects and 45,000 source files. In this style of development, we prefer keeping all code at HEAD consistent using global refactoring and rigorous automated testing instead of maintaining strict API backwards compatibility and long deprecation cycles within the codebase.\n\nWe also have a strong commitment to using open source libraries. We extensively rely on artifacts published through the Maven Central Repository. When we upgrade a library, we can do so in one place and update the code dependencies for all services.\n\nWith such a large codebase, it becomes impractical to load the entire repo into the IDE. At Square, we primarily use IntelliJ IDEA to develop code. We use Pants to configure and launch it. Probably the most valuable feature for day to day development is simply having the ability to quickly bring up any portion of the repo in IntelliJ. With a single command, Pants configures IntelliJ to edit a module and all of its dependencies defined in the repo.\n\nMaking it easy to configure any project in the IDE means that developers can easily contribute to any project in the codebase. Being able to easily switch between branches encourages developers to collaborate. Now it is convenient to check out each other\u2019s work locally when performing code reviews. It is easier to confine change requests to small, manageable chunks and switch between them while waiting on code reviews to complete.\n\nWe came to the Pants project looking for a tool to help solve problems in our build environment. Previously, we used Apache Maven to compile and package binaries. Maven is a powerful and popular tool with a modular design that makes it easy to extend with excellent support from third party tools and libraries. We had a significant investment in Maven, including many custom plugins for running code generation and supporting a distributed cache for artifacts in our continuous integration (CI) build system.\n\nUsing Maven with our \u201cbuild everything from HEAD\u201d policy strains the Maven model. Maven is designed to support editing a few modules at a time while relying on binary artifacts for most dependencies. To support building the entire repo from HEAD, we set every Maven module in the repo to a SNAPSHOT version.\n\nUsing Maven in this way works, but has drawbacks. Running a recursive compile of all dependent modules incurs a lot of overhead. We had wrapper scripts to help us try to be productive in this environment, say to run just run code generation or only run a subset of tests. Still, developers would get into trouble in some situations, often having to deal with inconsistencies between stale binary artifacts and the source on disk. For example, after using mvn install, pulling in new changes from the repo or switching back to an older branch could leave them compiling against stale code. When developers routinely question the integrity of their development environment, they waste a lot of time cleaning and rebuilding the codebase.\n\nOur first priority was to allow developers to quickly configure their workspace in the IDE. Next, we migrated to using Pants as the tool to test and deploy artifacts in our CI builder. As of this writing, we have replaced all of our use of Maven in this repo using Pants, including:\n\nReplacing all of our uses of Maven was not easy. We were able to do this by generating the Pants configuration using the Maven pom.xml files as a source of truth. During an interim phase we supported both tools. Through collaboration with the Pants open source community, we were able to modify Pants through hundreds of open source contributions.\n\nPants comes out of the box ready to edit, compile, test, and package code. Beyond that, we were able to leverage Pants\u2019 extensible module based system. A current favorite is a small custom task to deploy directly to staging environments over our internal deployment system. Along the way, other custom modules run custom code generators, gather meta information about our build process for our security infrastructure, and package the output of annotation processors into yaml files for our deployment system. Today we have about two dozen internal Pants plugins that do all those things, plus additional tools to audit our codebase, integrate with our CI system, and customize our IDE support.\n\nAt Square, Pants has helped us realize the promised benefits of monorepo style development at scale. Making sure the development process is consistent and reliable increases our developer velocity. Being able to quickly and reliably edit, compile, and test allows developers to concentrate on reviewing and writing code, not struggling with configuring tools. We believe that Pants is ready for more widespread adoption and encourage you to give it a try."
    },
    {
        "url": "https://medium.com/square-corner-blog/ios-9-upgrade-why-did-my-unit-tests-grind-to-a-halt-7460ae8285bb",
        "title": "iOS 9 upgrade: Why did my unit tests grind to a halt?",
        "text": "While upgrading Square Register from iOS 8 to iOS 9, we found that our unit test behavior changed in a way that was difficult to diagnose. We investigated the issue and retraced our steps to arrive at the root cause. Ultimately we found that the underlying problem was not just specific to Square, but an issue that could affect any iOS developer.\n\nAt nearly 7 years old, Square Register for iOS is a hefty app that contains well over a million lines of code. While moving the base SDK from iOS 8 or iOS 9 may seem straightforward, the sheer scale of such a change presented many challenges.\n\nChief among these challenges involved the app\u2019s test suite. Shortly after implementing the change, we noticed that our unit test suite \u2014 with its thousands of tests run for each build \u2014 had significantly slowed and would eventually fail. These test failures never occurred when run individually, but often failed as part of a larger suite of tests, and not always in the same way. Since no single test was faulty, a plausible explanation indicated pollution of the shared test environment.\n\nThe next step was to reduce the size of our haystack. We began commenting out tests to narrow the scope. Commenting out file after file grew tedious \u2014 we\u2019ve included a category trick that makes life easier (below). Unfortunately, a better way to run just a subset of tests matching a naming pattern in iOS doesn\u2019t exist:\n\nAbove: a useful category to only run a particular set of tests that match some string pattern.\n\nUsing this filtering, we narrowed it down to a handful of test classes to consistently reproduce a failure. Then we noticed a curious thing: it didn\u2019t matter exactly which tests were running per se; it was the quantity of tests which determined the outcome.\n\nWe saw that the set of tests needed to run for around three minutes; any less and they\u2019d all pass. One test in particular would fail before all the others, and it became the focus of the rest of the investigation:\n\nAbove: the primary test failure that we tried to track down. Note the use of spinning the runloop (waiting for the expectation) and locking.\n\nThe failure happened after spinning the runloop, i.e. the expectation was never fulfilled. We were waiting for work to be performed on a queue, but the test would time out instead. So, we decided to look at all the queues\u2026\n\nAh ha! The main queue was already clogged with an excessive number of operations by the time we reached the problem test in our larger test run:\n\nWe had found the culprit: the tests were slow because the mainQueue was overburdened, and our first test failure was due to waiting for an unlock operation that was stuck on this queue.\n\nTo demonstrate that the queue was the source of problems, we added a call to [[NSOpertaionQueue mainQueue] cancelAllOperations] in the test\u2019s tearDown method; the tests now passed!\n\nBy stepping through line by line and watching for changes in [[NSOpertaionQueue mainQueue] operationCount], we tracked it down to a specific module that was invoked when setting up test fixtures. It would add 4 scheduling operations to the mainQueue for each test fixture that was created (one for every setUp). These operations would accumulate because they were not given an opportunity to drain off of the mainQueue. That explains why a large set of tests exhibited the issue, but a smaller set did not.\n\nIt looked like the case was closed. We were ready to make a few adjustments to the module in question and \u201ccall it a day\u201d, but this particular \u201cday\u201d would not be \u201ccalled\u201d so readily and the case remains open!\n\nWe had fixed the scheduling issue with the test fixture, but found that there was still a problem.\n\nWe first attempted to verify our fix by swizzling NSOperationQueue such that it refused all other work added on the main operation queue, except for the work done by the failing test.\n\nBad news: the failure still happened, but now, it took a much larger test suite to reproduce! The failure would occur even when the mainQueue had just one operation to perform, and it was puzzling why this queue\u2013with only a single operation\u2013would not drain. It clearly was not a case of the queue merely being \u201cslammed\u201d as previously thought.\n\nIn other words, the same test still waited forever for an operation on the mainQueue, even when the mainQueue looked like this:\n\nThe unlock operation was still stuck on the queue, even when nothing else was on the queue!\n\nSince our test failure manifested as a deadlock (a lock that\u2019s never released), we dug into the synchronization code (excerpt below) and searched for race conditions next. Because tests ran in parallel, it seemed plausible, but this was grasping at straws. Though we frequently thought we were on to something, we came up empty handed when it came to finding a race condition.\n\nEven though the main queue and locks are involved, and no matter how badly we wanted to find a race condition, this was not the issue.\n\nOut of ideas, we start ripping apart the problem test to see what surfaced. We commented out or replaced as much as we could to reduce its scope, and even replaced the XCTestExpectation with hand-rolled runloop spinning.\n\nI made a mistake with time unit conversion while I was replacing waitForExpectationsWithTimeout, and now the test would spin for 1000x the intended duration. I paused the debugger as it spun, and noticed something curious. More often than not, pausing the debugger would land on processPendingChanges, a method that NSManagedObjectContext uses to save CoreData updates.\n\nAlthough pausing the debugger while the problem test spun was one of the first things I tried (and gave up on), I didn\u2019t land on anything interesting until this moment (Instruments could have also provided a clue if I had thought to use the Time Profiler). A couple of missed chances, but now we were on to something.\n\nA recurrence of processPendingChanges while waiting seemed interesting, but we needed more information to determine what it meant.\n\nTo see how many NSManagedObjectContexts were calling processPendingChanges, we added instance logging to our NSManagedObjectContext class using a static NSHashTable (similar to the below example):\n\nWe subclassed NSManagedObjectContext and added instance tracking similar to the example above.\n\nUsing this instance tracking, we found that hundreds, and potentially thousands of these contexts were persisting between test runs. Even worse, they would perform work on the main runloop, and it seemed plausible that these leaked contexts were starving the NSOperationQueues out of the runloop.\n\nIn other words, there was a (mixed metaphor warning) stampeding herd of NSManagedObjectContext processPendingChanges calls during the runloop which starved our NSOperationQueue. Our mainQueue was not able to drain because this avalanche of scheduled calls ate up all of the runloop\u2019s available bandwidth each cycle.\n\nOk, so NSManagedObjectContext was being retained (leaked) between test runs. That reduced down to finding where the object\u2019s retain count was increased, where it was decreased, and why the former was greater than the latter.\n\nLuckily, Xcode comes with a tool called Instruments which has a Leaks Diagnostic tool. So we just right-clicked on the test name and selected Profile SQOTAdditionsTest and voila, Instruments made it all clear. WRONG! Sadly, none of that happened, starting with Instruments attaching itself through this menu:\n\nFrustratingly, this Xcode feature merely launches an inert instance of Instruments instead of attaching to a process.\n\nInstead, to profile a test run, we had to:\n\nThe Leaks tool did not consider the NSManagedObjectContext (an SDManagedObjectContext in the screenshots below) to be a leak because it was eventually torn down. But it did report a list of retain/release pairs. So, we looked at a list of over 6000 retains/releases for a single object. At the time, we didn\u2019t see the culprit, so it was back to debugging via lldb.\n\nAfter a lot of debugging and printing out the [context retainCount] we found that the the context\u2019s save: method adds a retain, and, if we commented this line out, the context was cleaned up as expected(!), and the leak was plugged.\n\nI returned to Instruments to cross-check its report with this new information, and sure enough, there it was:\n\nA detail view of the retain which leads to the leak:\n\nDetail view from Instruments: the stack trace includes save:, CFRetain, and _registerAyncReferenceCallback (sic)\n\nTo debug, I overrode and also set a symbolic breakpoint for [NSManagedObjectContext(_NSCoreDataSPI) performWithOptions:andBlock](based on Instruments output above). I\u2019m not fluent in assembly, but I was able to follow along by reading the comments:\n\nOn line 91, we increase our retain count. On line 132, we add asynchronous work to a queue.\n\nWe deduced that when save: is called, the context is queueing some work and retaining itself in the meantime (stepping through and calling p (int)[$esi retainCount] confirmed this). This work also decrements the retain count, but since it stays on the queue, the self-retained context sticks around forever. We also stepped through while using sqlite3 and .dump to verify that no data was lost if the enqueued work never ran.\n\nThinking that we finally understand what\u2019s happening, we stepped away to create a small test project to isolate our suspicions and confirm that the issue is present in a simpler example. It\u2019s confirmed! The sample project exhibited the same issue (try it out!).\n\nUsing symbolic breakpoints to determine the behavior differences in Xcode 6 v. 7\n\nWe compared iOS 8 vs iOS 9 using this test project. It was clear: save: isn\u2019t async in iOS8!**\n\n** Let me qualify this: (as far as I can tell from the assembly) iOS 8 save: was synchronous in its \u201cstandard codepath\u201d and did not enqueue work with dispatch_async. As of iOS 9, it now is asynchronous and immediately invokes dispatch_async as part of its \u201cstandard codepath.\u201d I encourage checking out the sample project and comparing it in Xcode 6 vs. Xcode 7 (compare the masterbranch with xcode-6).\n\nUnfortunately, the only way to get the context\u2019s async work off of the mainQueue is to spin the runloop. If left alone, operations will build up in the background, and when the runloop finally does run, the contexts will starve the operation queues.\n\nSo, our fix is to add code that looks like this to tearDown:\n\nSome contexts were still leaked after this, but these were legitimate memory leaks from services that weren\u2019t fully torn down.\n\nEnsuring all of our t\u2019s are dotted, I add conditional breakpointing to the context\u2019s dealloc breakpoint, and have it play a sound and halt when there are more than 2 dozen instances hanging around:\n\nPlaying a sound and using a condition means we can let the tests run and get alerted when too many instances piled up.\n\nAlthough it takes a while to work through the entire test suite, thanks to our instance tracking, it\u2019s a mechanical process to track down the remaining leaks and verify that none remain.\n\nAfter tracking down the remaining leaks and adding some runloop spinning at tearDown, the runloop is clear and the tests are 28% faster. Although it took a considerable amount of time to debug, hopefully this time lost is repaid in the future via shorter, stabler test runs\u2013not to mention we got a blog post out of it.\n\nThanks to Kyle Van Essen, Eric Muller, Justin Martin for their assistance!"
    },
    {
        "url": "https://medium.com/square-corner-blog/shift-safe-and-easy-database-migrations-42108ee216f",
        "title": "Shift \u2014 Safe and Easy Database Migrations \u2013 Square Corner Blog \u2013",
        "text": "Shift is an application that makes it easy to run online schema migrations for MySQL databases. We use it at Square to run thousands of databases migrations every month. Today we are happy to be open sourcing the project, which can be found on GitHub at https://github.com/square/shift.\n\nTwo years ago almost every database at Square lived on shared hosts, meaning a single server with a single MySQL instance could have anywhere from 2 to 60 schemata on it. For the most part this actually worked pretty well for us, particularly since we didn\u2019t yet have the tooling in place to manage thousands of database hosts. The main drawback to this setup, though, is the obvious lack of isolation \u2014 any one service behaving badly could disrupt \u2014 and even take down \u2014 tens of other services. Combine that with a general lack of visibility into which service was causing performance problems for a database host and you start to get an idea of the issues we were dealing with back then. Suffice it to say, shared databases served us well for a while but it was time to develop a more mature infrastructure.\n\nWe decided to use containerization (via LXC) to provide isolation between databases, but there was an important thing we had to keep in mind when doing this \u2014 with the number of services in our ecosystem growing every week (well into the hundreds by that point) it was imperative that we made sure our small operations team could support the move away from shared hosts. Containerizing all of our databases would turn tens of shared hosts into thousands of containers, and we needed to be able to handle the transition seamlessly. So, we set out with a secondary \u2014 and actually more important \u2014 goal of making databases at Square easy for developers to manage on their own. Shift is a service that evolved from that goal.\n\nThe process for migrating database schema at Square used to be rather cumbersome. It looked something like this:\n\n1. Developer files a ticket explaining the change they want to make\n\n 2. Operations team writes up a procedure to do the alter with pt-online-schema-change\n\n 3. Someone approves the procedure\n\n 4. Operations team manually runs the procedure\n\nOr, worse yet, a developer might accidentally run a migration live during a deploy, potentially causing downtime for their service. Needless to say this wasn\u2019t a refined process. Now, all database migrations go through Shift.\n\nWithin Shift, developers can submit migrations that they need to have run on their databases (altering, dropping, or creating a table). After submission, a dry-run is executed to make sure that the migration is valid, and some basic stats about the table being altered are collected and presented in the UI. With these stats in mind, another user must decide whether or not to approve the migration, optionally leaving feedback in the form of a native comment. After receiving approval, the migration can then be run \u2014 all with just the click of a button. Shift has simple hooks you can plug into that provide granular control over which users have access to perform each action for a given database.\n\nSome additional features that we built into the service are:\n\nShift removes the error-prone and manual work from altering database schema, and it gives invaluable time back to operations teams. As of writing this, Shift has had no problem running hundreds of migrations a day or running migrations that take weeks to complete. You can read more about it\u2019s features and components, as well as watch a demo video at https://github.com/square/shift.\n\nShift works pretty well with the current workload we have at Square. There are some features \n\nwe plan to build into it in the near future to make it more robust, one example being to make it easier to integrate with developer workflows (ex: direct integration with ActiveRecord). On the long term roadmap we may implement our own version of pt-osc to support things like \u201cWHERE\u201d clauses in an alter (for pruning data), as well as explore the possibilities of running migrations on slaves and then promoting them to master.\n\nPlease follow the installation guide here."
    },
    {
        "url": "https://medium.com/square-corner-blog/pair-a-git-author-tool-b9685ba4e313",
        "title": "pair, a git author tool \u2013 Square Corner Blog \u2013",
        "text": "Software engineers at Square sometimes write code by themselves, and sometimes with others. When we write code with others at the same computer we call that \u201cpair programming\u201d or \u201cpairing\u201d. We use this technique in our interviews, and in our work when appropriate. When we\u2019re ready to commit our work in our version control system, git, we want to ensure that all the authors are listed for posterity.\n\nWhen I started we had a few different ways of solving this problem. Some people used a ruby gem, some had written their own tool for the job, and some simply edited their git author configuration by hand every time. I wanted a way to quickly expand names and create a joint email alias for all the authors of a commit, and I didn\u2019t want it to work only at the whim of my current ruby and rvm configuration.\n\nI decided to write pair in Go as a way to teach myself a little bit of the language and to make sure it was fast and reliable. It\u2019s installed by default on all the machines at Square, including our interview machines. It\u2019s great for any group that has a reasonably stable list of people, like the engineering group in a company or open source projects that do pair programming. Here\u2019s how you use it:\n\nUse it with an arbitrary number of authors:\n\nAnd you can use it to restore your git author configuration to normal:\n\nThis simple tool is used frequently at Square, and I hope it\u2019s useful for you too. For more information or to install it on your machine, check it out on GitHub."
    },
    {
        "url": "https://medium.com/square-corner-blog/android-spring-cleaning-tech-talks-ae2a2196e111",
        "title": "Android Spring Cleaning Tech Talks \u2013 Square Corner Blog \u2013",
        "text": "Recently, John Rodriguez, Lisa Neigut, Jake Wharton, and Lisa Wray gave lightning talks and participated in a panel discussion at our \u201cAndroid Spring Cleaning\u201d event in Square\u2019s New York City office. (Thanks to everyone who attended!) These talks were recorded and are now available on our Square Engineering YouTube channel. Enjoy!"
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-squares-register-api-and-e-commerce-api-555701db8f1b",
        "title": "Introducing Square\u2019s Register API and E-commerce API",
        "text": "Have you ever looked at Square\u2019s hardware offerings (maybe our newest EMV and NFC card reader) and thought, \u201cI could build something amazing using that\u201d? Historically, payments and hardware development were often associated with archaic and difficult to use technology. Today we are excited to launch Square\u2019s Register and E-commerce APIs. Any developer can build an iOS app that takes payments from a swipe, dip, or tap. Developers can now also enable a website with online payments through Square.\n\nThe Square Register API lets you focus on what you do best: crafting an amazing point of sale experience for your sellers. We\u2019ll take care of moving the money. With just two API calls, you tell us how much you\u2019d like us to charge a customer and let Square Register do all the heavy lifting. Customers pay just like they\u2019re used to at any other Square seller, and we\u2019ll even text, email, or print a receipt on the seller\u2019s behalf. Best of all, it supports all our hardware, including the Square Stand and the new Square Contactless + Chip Reader.\n\nIn your app make an API request:\n\nSquare Register switches to the foreground and completes the payment on your behalf. Once that\u2019s done, we\u2019ll call your app back with the result.\n\nNow any developer can build solutions for their customers to take payments on a website using Square. Whether you\u2019re getting started selling online with Square\u2019s Online Store; you\u2019re growing through Square\u2019s integrations with E-commerce platforms like Bigcommerce, Weebly, and Ecwid; or now, you\u2019re building a custom website, any business can scale online with Square payments. What\u2019s more, with our E-commerce API, Square takes the hassle out of PCI compliance without interfering with the design or look and feel of your website or re-directing customers off-site.\n\nSquare\u2019s JavaScript library uses a set of transparent, dynamically styled iframes to accept the sensitive cardholder information. This way cardholder data never touches your web site or servers. In exchange, it returns a single-use card nonce:\n\n(See the docs for a full working example).\n\nYour server uses the card nonce and charges the buyer\u2019s card.\n\nOr you can store the card with a customer record to charge on an ongoing basis.\n\nWith today\u2019s launch of the Register and E-commerce APIs developers can build solutions that provide any seller with online and in-person payments, all powered by Square. When you use Square\u2019s commerce platform, you are giving the seller a solution that results in a single, cohesive view of their entire business. Sellers can identify when customers are purchasing online, in-store, or across both. Using Square\u2019s APIs and tools make it simple for sellers to better understand their customers\u2019 buying trends and habits. Finally, because developers are leveraging Square\u2019s platform, the seller can also benefit from everything that Square payments offers: transparent pricing, no monthly fees, support for challenging chargebacks, chargeback protection, great analytics, open APIs, as well as access to Square Capital and Square\u2019s SaaS offerings.\n\nGet started today by signing up for Square, reading the documentation, and creating an app."
    },
    {
        "url": "https://medium.com/square-corner-blog/framed-data-team-joins-square-6f12d1fcef27",
        "title": "Framed Data Team Joins Square \u2013 Square Corner Blog \u2013",
        "text": "From the early days of Square, we built a discipline in data science that allowed us to take a differentiated approach to risk management, and thus reduce the barriers to entry for our sellers to accept credit cards. Our data scientists are constantly building new models, and then analyzing and acting on the data we receive \u2014 not only to manage risk, but to build new products for our sellers. This has allowed us to offer innovative financial services on top of payment processing, from Square Capital, to Instant Deposit, to Chargeback Protection, that help our sellers grow.\n\nSo we are thrilled that the Framed Data team has joined Square. Framed Data built an incredible product rooted in machine learning to help businesses predict customer engagement and prescribe marketing actions. Like Square, the Framed Data team believes that using data insights and machine learning to help customers improve their businesses creates immense value and empowers business owners to make better decisions.\n\nThe Framed Data team will work on Square Capital, building models that allow us to further extend financing to businesses, including those that would otherwise find it difficult or impossible through traditional banking. This approach helps accelerate growth for underserved businesses and empowers them to increase participation in their local economies. Welcome to the team!"
    },
    {
        "url": "https://medium.com/square-corner-blog/vulnerability-in-okhttps-certificate-pinner-2a7326ad073b",
        "title": "Vulnerability in OkHttp\u2019s Certificate Pinner \u2013 Square Corner Blog \u2013",
        "text": "Security researcher John Kozyrakis from Cigital recently discovered a vulnerability in OkHttp\u2019s CertificatePinner. He responsibly disclosed the issue to us via Square\u2019s open source bug bounty program at HackerOne.\n\nAfter feeling just a little bit embarrassed, I implemented a fix and released it as OkHttp 3.2.0. We also backported the fix to OkHttp 2.7.5. If you\u2019re using OkHttp in your application, please upgrade to the latest release.\n\nFor a complete explanation of the problem, its origins, and consequences, see John\u2019s post. Security is a difficult problem, and we Squares take it very seriously. We\u2019ll continue to work hard to keep our code secure!"
    },
    {
        "url": "https://medium.com/square-corner-blog/the-journey-of-android-engineers-tech-talks-dd61b426b507",
        "title": "The Journey of Android Engineers: Tech Talks \u2013 Square Corner Blog \u2013",
        "text": "Recently, John Rodriguez, Effie Barak, Eric Burke, and Christina Lee gave lightning talks and participated in a panel discussion at our \u201cJourney of Android Engineers\u201d event in Square\u2019s San Francisco office. (Thanks to everyone who attended!) These talks were recorded and are now available on our Square Engineering YouTube channel. Enjoy!\n\nFor the past three years, Square Register for Android has leveraged Dagger to wire up Java objects. However, Register\u2019s scope hierarchy and complexity are increasing and pushing the limits on Dagger. In this lightning talk, John highlights how and why the Register team incrementally migrated the Android app to Dagger 2.\n\nAfter 10 years at Microsoft, Effie made the transition from Windows Phone to Android development. In this talk, she shares her experience: What was easy to pick up, what was hard, what was baffling, and what was thrilling. In a world where Android developers are in high demand and developers change stacks all the time, what does it take to make the switch and how can we help more people have an easier transition?\n\nWe all want to hire amazing software engineers and build the best teams possible. With this in mind, it\u2019s important to recognize that our own biases can lead us to flawed interviewing and hiring decisions. \u201cUnconscious bias\u201d refers to behaviors that are triggered without our awareness, leading to quick judgments and assessments of people and situations based on our background, cultural environment, and personal experiences. In this lightning talk, Eric shares practical tips that all of us can use to interrupt bias before, during, and after interviews.\n\nBetween React, Cycle.js, Flux, and Redux, many interesting UI related architectures, frameworks, and libraries are gaining traction on the web. While React Native, a port of one such concept, is gaining popularity, it is not a feasible choice for every company. In this lightning talk, Christina covers how the engineers at Highlight built a Redux-like framework in Kotlin to simplify state management and reduce bugs in challenging UI components, all without having to reinvent their Android app or sacrifice hard won expertise.\n\nDon\u2019t forget to subscribe to the Square Engineering YouTube channel for more great content!"
    },
    {
        "url": "https://medium.com/square-corner-blog/open-sourcing-squares-womeng-handbook-829e9e320490",
        "title": "Open sourcing Square\u2019s WomEng Handbook \u2013 Square Corner Blog \u2013",
        "text": "At Square, we have a strong, supportive, and empowering community of women engineers and allies. We come together as a group: WomEng. Our group grew from a mailing list to regular, in-person gatherings. Today, WomEng has expanded outside of Square and includes a large network of women engineers.\n\nWe had many conversations and meetings to establish our group and come up with our recurring events. So, to help others start or expand a WomEng group in their community, we\u2019re open sourcing a handbook with everything someone needs to get started.\n\nThe handbook can be found on GitHub at https://github.com/square/womeng_handbook.\n\nPull requests are welcome. We encourage you to share your own resources and event ideas. Together, we can continue to grow the WomEng community."
    },
    {
        "url": "https://medium.com/square-corner-blog/query-sniper-b4be1912c1f6",
        "title": "Query Sniper \u2013 Square Corner Blog \u2013",
        "text": "Picture this: your service receives a request and kicks off a database query; the request times out and leaves the query running \u2014 leading to an unnecessary database load and possibly starving the database of resources. When services automatically retry requests, the problem is only exacerbated and can bring the most powerful database servers to their knees.\n\nSo, how does one guard against this?\n\nWhile this is a fairly common problem with many platforms, we\u2019re solely covering Java and JDBC running on a MySQL back-end. However, ideas discussed throughout should be applicable to other platforms and databases.\n\nVery recent versions of MySQL allow the setting of global timeouts, such that no query can take more than a predefined amount of time to run. This is a fairly coarse-grained approach, since limits have to be set for the longest-running query possible. So, this approach was ruled out. (Plus, we don\u2019t run bleeding-edge versions of MySQL in production.)\n\nYou can set a timeout on MySQL\u2019s JDBC driver. We use HikariCP for connection pooling; it also allows you to configure the pool accordingly, delegating to the driver to set timeouts. This sounded promising.\n\nFor anyone who\u2019s read the source code of Connector/J, MySQL\u2019s JDBC driver, you\u2019re probably shuddering at this point. The driver was written when Java 1.3 was all the rage, and received only a few minor updates. The code is spaghetti; full of coarse synchronization and generally inflexible and not very extensible.\n\nPoking into the driver, I saw that a JDK Timer is started for each connection instance and a cleanup task is scheduled to run after the predefined time has passed. In com.mysql.jdbc.StatementImpl we see:\n\nThe cleanup task scheduled by the driver is simple (also in com.mysql.jdbc.StatementImpl):\n\nAside from cleaning up the state held on the Statement instance (inside some truly awful synchronization blocks), the task performs two critical actions:\n\nThis works well except for two issues:\n\nJDK Timers are old, clunky and moreover, expensive. Each one creates a new thread. (Even Javadocs on Timers recommend using ScheduledExecutorServices instead.)\n\nCloning the JDBC connection each time can lead to problems with the number of available connections a database server may have available \u2014 particularly if it is processing runaway queries that are hogging resources. Such a timeout task may not be able to connect to the database to kill runaway queries.\n\nBecause of these shortcomings, we decided to write our own query sniper rather than simply set JDBC timeouts. The query sniper effectively does exactly the same thing as setting a timeout on the JDBC connection, except it\u2019s implemented as a JOOQ ExecuteListener and submits a task to a single ScheduledExecutorService. This is configured with just one maintenance thread for the entire system every time a query is executed. Here is a simplified version of the idea:\n\nQuery Snipers are added to all JOOQ DSL sessions using JOOQ\u2019s Configuration.set() API:\n\nFor added benefit, the query sniper inspects the request context and determines how much time it has remaining to run the query before the request times out; thereby, setting fine-grained timeouts on a per-query basis.\n\nWe wanted to maintain a separate, pre-established connection pool consisting of a single connection, solely for the query sniper to kill long running queries. That way, we\u2019d overcome the issue of not being able to establish a new database connection. But, we hit a snag. To maintain our own connection pool for this purpose, we\u2019d have to hand-craft our own KILL QUERY <connection_id> statement. MySQL\u2019s JDBC driver doesn\u2019t expose the transaction ID to allow us to do this.\n\nSadly, the query sniper just ends up calling Statement.cancel() on a running statement it wants to kill. This still causes the connection to be cloned, etc. just as before.\n\nWith our query sniper in place and running in production on a number of systems, we saved ourselves a fair few outages \u2014 outages we\u2019d seen prior when testing a new system we built which had a few rogue queries. These outages would continue to happen (judging by the query sniper logging its activities) had it not shot down these abandoned queries.\n\nAs much as we\u2019d like to open source this piece of code, it is far too closely tied to Square\u2019s infrastructure to be extracted into a separate library. (If you want to see more, you could always join our team.) However, I hope the pattern described above and the code snippet included will be useful to others.\n\nI\u2019d really like to be able to maintain a persistent connection to the database. To this end, I hope to patch MySQL\u2019s driver to expose its transaction ID, to allow for hand-crafted KILL QUERY \u2026 statements.\n\nOr maybe what I really need to do, is rewrite the MySQL JDBC driver using modern ideas of concurrency and thread safety, resource management, configurability and extensibility? ;)"
    },
    {
        "url": "https://medium.com/square-corner-blog/square-presents-the-journey-of-android-engineers-14ab271cd4f",
        "title": "Square Presents: The Journey of Android Engineers \u2013 Square Corner Blog \u2013",
        "text": "Join us on Thursday, February 18th for the third edition of our Square Presents Series. It\u2019ll be a night of discussion and lightning talks from Square and our friends at Udemy and Highlight. The event starts with lightning talks from John Rodriguez, Effie Barak, Eric Burke, and Christina Lee. The overall focus is on technical growth and covers transitioning to Android, becoming a better interviewer, and exploring new technologies.\n\nWe\u2019ll finish the night with a panel discussion moderated by our very own Pierre-Yves Ricau.\n\nYou can see full event details and register on our registration page. Space is limited and this is sure to fill up quickly! If you\u2019re not in the Bay Area, you won\u2019t miss out; we\u2019ll record the event and share the videos here soon.\n\nFor the past three years, Square Register for Android has leveraged Dagger to wire up Java objects. However, Register\u2019s scope hierarchy and complexity are increasing and pushing the limits on Dagger. In this lightning talk, John highlights how and why the Register team incrementally migrated the Android app to Dagger 2.\n\nJohn Rodriguez is an Android engineer at Square working on Square\u2019s free point-of-sale app. Prior to Square, John worked at Amplify on their award-winning Android tablet platform for K-12 education and at Major League Baseball on iOS and Java server-side applications.\n\nAfter 10 years at Microsoft, Effie made the transition from Windows Phone to Android development. In this talk, she shares her experience: What was easy to pick up, what was hard, what was baffling, and what was thrilling. In a world where Android developers are in high demand and developers change stacks all the time, what does it take to make the switch and how can we help more people have an easier transition.\n\nEffie Barak is an Android developer on the mobile team at Udemy. She began working as a C# developer 10 years ago, but most recently became a mobile developer 4 years ago. After moving to San Francisco in 2013, she worked on the Slack for Windows Phone app.\n\nWe all want to hire amazing software engineers and build the best teams possible. With this in mind, it\u2019s important to recognize that our own biases can lead us to flawed interviewing and hiring decisions. \u201cUnconscious bias\u201d refers to behaviors that are triggered without our awareness, leading to quick judgments and assessments of people and situations based on our background, cultural environment, and personal experiences. In this lightning talk, Eric shares practical tips that all of us can use to interrupt bias before, during, and after interviews.\n\nEric Burke is an Engineering Manager at Square with more than 20 years experience. He co-created Square Register for Android, leads Square\u2019s QA and test engineering teams, and has interviewed over 600 software engineers. He developed and teaches a class on unconscious bias as well as a class on interview techniques. He\u2019s also an avid woodworker and you can find him on Twitter at@burke_eric.\n\nBetween React, Cycle.js, Flux, and Redux, many interesting UI related architectures, frameworks, and libraries are gaining traction on the web. While React Native, a port of one such concept, is gaining popularity, it is not a feasible choice for every company. In this lightning talk, Christina covers how the engineers at Highlight built a Redux-like framework in Kotlin to simplify state management and reduce bugs in challenging UI components, all without having to reinvent their Android app or sacrifice hard won expertise.\n\nChristina Lee is currently the Android lead at Highlight. In addition to trying to convince the world to use Kotlin on a daily basis, she also enjoys building beautiful UIs, extolling the virtues of Rx, and reading well documented APIs."
    },
    {
        "url": "https://medium.com/square-corner-blog/shared-app-functionality-via-javascript-bbba4f0cb3ea",
        "title": "Shared App Functionality via JavaScript \u2013 Square Corner Blog \u2013",
        "text": "When building version 2.9 of the Square Cash app, we ran across an interesting challenge. The app was fetching payment objects from our server that needed to be displayed to the user. This basically involved choosing a template for each payment and formatting the data. For simplicity, let\u2019s say that the data included a date, an amount, and recipient name, and it needed to produce a displayable string like: \u201cYou sent $20 to Erin Hills at 3:00 pm.\u201d\n\nWe first considered baking the logic and templates into the app, but that would have resulted in an inflexible system where we\u2019d need to launch a new version of the app to pick up a change. Old versions would be stuck presenting payments incorrectly, and the logic would need to be duplicated in each supported platform (iOS and Android). Our engineering velocity would diminish tremendously.\n\nFor a long time, we solved this issue by having the server return fully presented data rather than raw data. This achieved flexibility but at the price of some big drawbacks. The most significant issue was that the app needed a network connection and needed to constantly ask for the presented view of a payment (since the presented data is time sensitive). This resulted in excessive data usage and the lag could introduce staleness. As we worked towards the ability to view payments offline, we knew this approach was no longer viable.\n\nThe solution that we landed on is to use a JavaScript engine in the app to produce the presented strings from the raw objects. This works remarkably well. A JavaScript library that runs in the JavaScript engine effectively contains a series of templates and a translation function that consumes the raw objects, then applies the appropriate template, and finally outputs the presented data.\n\nHere\u2019s a simplified example of a raw object fetched from the server that is passed from the app into the JavaScript library:\n\nThe JavaScript library produces the following object and passes it back to the app:\n\nThe presented data is sensitive to the current date and timezone. If the user opens the app a day later, without a network connection, the same JavaScript might produce:\n\nUpdating the strings or adding support for new types of payments can be done by changing the JavaScript. Here\u2019s a simplified example of when we added support for donations to nonprofits. Raw data:\n\nThe JavaScript would convert it into:\n\niOS 7 introduced support for a browserless JavaScript engine with its JavaScriptCore framework. This made for a simple, out-of-the box solution that took very little time to get up and running.\n\nIn Android, there was no built-in framework included in the standard APIs. So, we built an open source Java binding to Duktape, an embeddable JavaScript engine. We built in support for timezones, converting JavaScript stack traces to Java stack traces (when the JavaScript throws), and other features.\n\nThe JavaScript functions that the apps call into (effectively, the interface between native code and JavaScript) need to use primitive objects. In practice, the functions exposed to the apps by our library all have one or more input string parameters and all return back a string. The JavaScript library is responsible for parsing the inputs into JSON objects. The apps are responsible for serializing the output from stringified JSON to native objects.\n\nThe server hosts the JavaScript library as a single file and the apps periodically check for an updated version. Our implementation of this mechanism uses ETags to redownload the JavaScript file only if it changed.\n\nOne implementation detail that we found useful was passing the raw object to the app as an opaque string rather than as a strong typed object. This lets the server have complete control of the raw object and the JavaScript that uses it. New fields can be added to the raw objects without the apps knowing or being affected.\n\nOne of the nice benefits of JavaScript is that it\u2019s not limited to native apps. The same library can also be used in the web. This leads to perhaps one of the best aspects of this approach: the logic lives in a single location (the server), rather than duplicated across iOS, Android, and the web. This helps to ensure a consistent experience, reduces the amount of testing required, and increases our engineering velocity.\n\nThe use of a shared JavaScript library has allowed Square Cash payments to be viewable offline while allowing the presentation to be updated independent of a new app release. We\u2019re excited to expand the use of this approach into other areas of the app. Thanks to Michael Druker, Dan Federman, Matt Precious, Alec Strong, Shawn Welch, Jesse Wilson, and Shawn Zurbrigg for their contributions to the effort."
    },
    {
        "url": "https://medium.com/square-corner-blog/breaking-up-with-your-monorail-780927542097",
        "title": "Breaking up with your MonoRail \u2013 Square Corner Blog \u2013",
        "text": "At Square, I work on our Payments Scalability team. In 2015, our task was to modernize our payments infrastructure by extracting it from Square/Web, our monolithic Rails app backed by a single MySQL instance. Square/Web has been a core component in processing transactions since the company was founded in 2009.\n\nPlanning in earnest started at the end of 2014, when we already were starting to hit limits on what we were comfortable with managing operationally.\n\nWe were faced with three options:\n\nThis requires spending money on hugely expensive enterprise flash cards. Scaling vertically would have been a short term fix, and over time it would have cost more and more to keep up with growth. Operationally, it\u2019s much harder to rebuild and backup big databases. We discarded this as a long term option.\n\nInstead of managing a single MySQL instance, we could also shard it to help scale horizontally. We evaluated a few gems, such as Octopus and db_charmer, and built a proof of concept sharded system. We decided retrofitting sharding would be more work and doesn\u2019t get us away from simplifying the monolith; in fact, it made things significantly more complex.\n\nThis ended up being our preferred approach. We decided to extract payments functionality and data from Square/Web into a separate service. This was also in line with Square\u2019s overall architecture philosophy of avoiding monoliths.\n\nWhile Square/Web hasn\u2019t handled charging payments for years, other parts of the Register app rely on it for transactional data. To reduce the chance of breaking anything, we needed to break the work into small chunks. Below, I\u2019ll be covering how we built an intermediary API on top of ActiveRecord, Rail\u2019s ORM, then gradually built up the functionality to move away from direct SQL calls.\n\nBefore building the intermediary API, we needed to figure out how payments were being accessed. Unfortunately, Rails doesn\u2019t make it very easy to audit queries, and our monolith contained around 100,000 lines of Ruby code, excluding 200,000 lines of tests. So, we created a new gem,active_record-sql_analyzer, which can de-dupe SQL queries and tag call sites from Ruby to gain an idea of access patterns.\n\nOur initial run gave us a list of 500 distinct SQL queries, which we used to come up with seven protobuf APIs. Those ended up being the basis for Spot, our new payments searching service. We chose to back it with MySQL, sharded by merchant key, as we knew the majority of our access patterns were scoped to a single merchant.\n\nActiveRecord\u2019s flexibility is nice when we were writing SQL queries; not so much when we were trying to convert it to a protobuf API. We also needed one place to funnel calls through to gradually roll out the new service.\n\nWe came up with a simple abstraction layer, like so:\n\nWe could then start porting ActiveRecord calls to the PaymentApi class, and split up the work of migrating call sites among multiple engineers.\n\nSpot returns protobufs through an RPC API, and we needed a way of switching between data backed by protobufs and backed by MySQL, without having to rewrite every call site.\n\nWe settled on writing a converter to turn ActiveRecord models into the same protobuf that Spot uses, and then wrote a class that converted ActiveRecord models to protobufs. This wasn\u2019t the most efficient, but it gave us consistency between Spot and ActiveRecord.\n\nExpanding on our PaymentApi class, we added a PaymentApi::Converter and PaymentApi::Wrapper:\n\nNow that we had a converter, we could expand our lookup call:\n\nWhile Spot wasn\u2019t ready yet, we had finalized our proto API and could start migrating code without having to worry about constant changes to the underlying API. Finalizing the proto API also helped the rollout go more smoothly, as we could flip the flag off if we found a bug in production.\n\nAt this point, we also started double running our entire test suite with the proto wrapper flags on and off. This prevented any other engineering teams from shipping code that broke our extraction efforts and made sure we knew the old code paths still worked.\n\nSquare/Web\u2019s DB schema date\u2019s back to 2010, while Spot\u2019s is from 2013 and based on protobufs. Due to the significant divergence, we confirmed that our wrappers returned data in a manner Square/Web expected:\n\nWe manually ran this until we had fixed all the discrepancies it found, this actually caught a lot of subtle bugs.\n\nNow, it was time to tie it all together! We had enough confidence in our wrapper that we could start sending requests through Spot. This just required another flag and a few additional lines of code.\n\nWe\u2019re done! We started rolling out traffic to Spot, and re-running the SQL analyzer and watching the number of queries go down.\n\nFor the sake of simplicity, I\u2019ve omitted how we ported searches into Spot. What I outlined above is the same process we followed for searches. The main difference was it required a bit more validation on search arguments.\n\nBefore migrating anything, we did an audit of payments related code to find anything we could delete. We ripped out approximately 56,000 lines of code total in the extraction process.\n\nTo provide the best experience to our merchants, Square avoids deprecating Register clients as long as possible. We have active versions going back two years on both Android and iOS. Any time we removed fields that were returned to the Register app, we ended up having to audit six different code bases (three versions on both iOS and Android).\n\nWe have some APIs that use the ActiveResource pattern, which makes auditing and migrating hard. Since it provides a HTTP -> SQL layer, it\u2019s easy for new calls to be introduced, making migrating it a moving target.\n\nLack of solid API contracts between services made it hard to determine what data was used and what Square/Web specific idiosyncrasies they were relying on.\n\nOur existing APIs to other services in Square/Web primarily use JSON. Defining a clear contract with something like protobufs would have made it easier to understand exactly what columns and data were expected.\n\nWhile we can\u2019t do anything about existing Register clients, using protobufs from the start would have provided a clearer audit trail of what data was needed for which version. JSON fields can\u2019t be audited when building a complicated hierarchy that branches based on various payment states.\n\nFinally, we made a mistake that was obvious in hindsight by using randomized rollouts instead of stable to the merchant. We ended up with a regression bug that only occurred when using a six-month-old version and an up-to-date version of Register together. It wasn\u2019t noticeable with randomized rollouts, because at lower percentages, merchants are more likely to just refresh a page and then see it work.\n\nNeither are good user experiences, but a stable rollout mitigates the Register app to consistently breaking for a smaller number of merchants, rather than inconsistently breaking it for a larger subset of them.\n\nThe final result was a successful extraction in January 2016, with all reads going to Spot for around six weeks prior to turning off local writes. If it had been necessary, we could have turned writes off sooner, but we saw no need to make a major infrastructure change right before holiday break.\n\nI\u2019d like to call out my fellow coworkers: Alyssa Pohahau, Andrew Lazarus, Gabriel Gilder, John Pongsajapan, Kathy Spradlin, and Manik Surtani, who all contributed to the planning and extraction effort. There\u2019s too many people to call out who also contributed early analysis on the stability of the databases and other parts of Square/Web. This ended up being a company-wide effort directly involving most product engineering teams and non-engineering teams."
    },
    {
        "url": "https://medium.com/square-corner-blog/ziggurat-ios-app-architecture-b54b3f7132f0",
        "title": "Ziggurat iOS App Architecture \u2013 Square Corner Blog \u2013",
        "text": "Back in June, I gave a talk on Preventing Massive View Controllers and described an app architecture with a one-way data flow in Swift. At the time, the architecture didn\u2019t have an accompanying blog post or even a name. Now, it has both. I\u2019d like to introduce Ziggurat: a layered, testable architecture pattern incorporating immutable view models and one-way data flow.\n\nThis architecture is dubbed \u201cZiggurat\u201d after the stepped pyramid. Like the steps of this pyramid, data complexity scales down as data flows in a single direction through the layers of the app. This one-way, immutable flow of data reduces cognitive load and results in smaller classes. Compared with Ziggurat, a typical Model-View-Controller architecture provides less guidance, and data and state may be mutated from a variety of places, including the view controllers.\n\nZiggurat remixes concepts from a variety of talks and articles, which I\u2019ll list in the Further Reading / Watching section. (Spoiler: it\u2019s inspired directly by early talks on Facebook\u2019s React Native and is similar to Flux). Of the alternative architectures referenced herein, any would be a favorable choice versus Model-View-Controller.\n\nI\u2019ll present Ziggurat, describe its background, contrast its tradeoffs, define its components, and show it in action with a sample app.\n\nZiggurat is an alternative pattern to Model-View-Controller (MVC). MVC is a basic way to divide responsibility in an app, but it inadequately separates roles and responsibilities. These are some common problems experienced in MVC apps:\n\nHard to Test Code with Too Much Responsibility\n\nIn practice, MVC\u2019s lack of separation leads to apps without design clarity that become hard to test and debug. Engineers should not need to reverse engineer an app\u2019s design and data model in order to work effectively in a codebase.\n\nI\u2019ll go over the basics of the Ziggurat pattern, but for a more detailed description see the resources listed below:\n\nZiggurat\u2019s one-way data flow is visualized above (and demonstrated in action here):\n\nZiggurat made life better in several ways, and enabled our small team of developers with a mix of iOS experience (ranging from none to several years worth) to hit an ambitious deadline.\n\nFor one thing, it clearly defines layers and roles in the app by providing guidelines and guardrails for engineers who are new to the codebase. Ramp-up time is short for new engineers, since the role of each component is well-defined and fits a well-defined mental model. Secondly, the Ziggurat pattern makes it easy to add tests. For example, we use the view model layer to compare structs as expected output. This wouldn\u2019t be feasible in MVC.\n\nOne-way data flow prevented MVC spaghetti and shrank view controllers drastically. Immutable types, smaller objects (preferably structs), and more layers reduce mental overhead. It\u2019s also easier now to run apps in a \u201cheadless\u201d mode since the view layer is devoid of business logic; all business logic is testable without a view layer; view state can be recreated with point-in-time view model snapshots. And finally, we found that the app was portable. Thanks to layers and dependency injection, it was straightforward to transition from an app to being an applet contained in another app.\n\nZiggurat made a few things more difficult, like animations. We recommend using Ziggurat in apps which use animation sparingly. View Controller animations could be disrupted by an incomingupdate() call, which could cause flicker unless carefully managed. With Flux, this issue is solved by using it with React Native.\n\nAdditionally, some boilerplate code resulted from pushing data through additional layers, although it provided additional compile-time checks. There\u2019s also a potential bottleneck with single data update pipeline. Unlike React, properties are not key-value observed; instead, updates come in piecemeal as view model structs. Some optimizations may be needed, such as discarding extraneous render calls or pruning the view model with diffing.\n\nDependency injection requires careful design. Our initial approach led to a large object graph. We had circular dependencies, which led to refactoring.\n\nNew projects offer an opportunity to try out new ideas. MVC has a number of clear weaknesses and has gained ignominy. Meanwhile, a number of talks on subjects such as Flux, React Native, and using value types more extensively with Swift have gained momentum.\n\nWe built out a complete Swift app with an architecture that remixes these ideas. We\u2019re pleased with the results of using one-way data flow, testability, dependency injection, lightweight view controllers, and value-typed view models.\n\nThere are tradeoffs of Ziggurat, but it\u2019s served us well so far. I hope you\u2019ll consider adopting some (or maybe all!) of the ideas here in your next project, especially before settling on an MVC architecture.\n\nI watched early videos related to React, and they were a direct inspiration for Ziggurat. Flux and React Native use similar concepts and are implemented in JavaScript.\n\nFlux is a robust, action-centric pattern that pairs well with React Native. Ziggurat is easily used in an app with minimal animations, but does not provide a solution for complex animations that could be interrupted by unpredictable re-renders. (Flux has this issue too, which React Native addresses.)\n\nWe hope you find the Ziggurat pattern helpful, and/or discover one of many better alternatives to MVC covered in the articles below:\n\nThanks to Ruby Chen for the illustration."
    },
    {
        "url": "https://medium.com/square-corner-blog/ace-the-square-pairing-interview-1a886fec98be",
        "title": "Ace the Square pairing interview \u2013 Square Corner Blog \u2013",
        "text": "An engineer friend recently decided to apply to Square and asked me for interviewing tips. Luckily, I do a lot of interviews! I put together some notes, then shared them with other interviewers at Square and asked for comments. This is our collective advice for acing the Square pairing interview.\n\nThe Corner has posts that explain the philosophy behind the Square Engineering pairing interviews. Don\u2019t let the dates put you off. Our pairing interview has barely changed, and we\u2019re still guided by the same philosophy. So, these posts are still totally relevant.\n\nWith less than an hour for each pairing, we have to do fabricated problems. We\u2019ve tried to design problems that help us learn as much as possible about how you think, how you write code, and how you collaborate.\n\nTogether, we\u2019ll write and execute code during the interview, and try to get a working solution within the time allotted. Practice solving toy problems in a time limit prior to the interview.\n\nMost problems are algorithmically focused. Problem sets like Project Euler are useful for getting into that mindset. Many of us recommend books like Cracking the Coding Interview. You shouldn\u2019t need to memorize specific algorithms, just get your brain into that mode.\n\nWrite code like you normally would if you were going to submit real production code for review.\n\nUse tools and languages you\u2019re most comfortable in, so that you can demonstrate your expertise through working code. Don\u2019t worry about what your interviewer likes, just make yourself at home and set yourself up for success.\n\nUse whatever resources and references you normally would. Using resources like Google and StackOverflow effectively is not only acceptable, it\u2019s encouraged. Knowing how to find information is a key skill!\n\nCare about the things you\u2019d normally care about \u2014 good style, reasonable performance characteristics, etc. \u2014 but don\u2019t let them prevent you from getting to working, running code!\n\nDon\u2019t go overboard with \u201cbest practices\u201d in an attempt to impress. It\u2019s tempting to try to score points by laying it on thick with tons of testing, verbose comments, shiny libraries, and optimization deep-dives. Remember, we\u2019re trying to get to know you as an engineer.\n\nJust do your thing, and when in doubt, ask what the interviewer is looking for.\n\nWe\u2019re big on curiosity at Square, and our favorite interviews usually involve someone learning something.\n\nOur interview problems are designed to have many solutions that we know about, and hopefully many more that we don\u2019t. We love it when we see new variations on a theme or when a good solution comes entirely out of left field. We\u2019re always excited to learn tools and techniques that are comfortable to you but new to us, and happy to return the favor.\n\nAt the end of the interview, we try to keep some time to chat. This is a great opportunity to ask questions, and great questions show us your capacity for critical thinking and learning with your team. Feel free to ask challenging questions, and we\u2019ll do our best to answer anything we can.\n\nShow your understanding of the problem. Identify requirements and constraints clearly. There\u2019s no need to expound at length on every nook and cranny of the problem; we just want to see that you get it.\n\nTest your solutions to demonstrate thoroughly that they work. Using a formal framework is not necessary. We just want to see that you\u2019re thoughtful about what can go wrong, and that you know how to attain confidence.\n\nAbove all, we want to arrive at a working solution in the time we have together. Temper all of the advice above with that knowledge. Manage your time.\n\nMake necessary and appropriate tradeoffs, and call them out.\n\nDon\u2019t be afraid to iterate; get to a working solution first and improve performance afterward.\n\nCompile early, compile often. It\u2019s better to get the first half of the problem working than to run out of time and have nothing that works. This will help you pace your work, as well.\n\nWe want to hire you, not keep you away! Our ideal outcome is to have you come work with us.\n\nYour interviewers are looking for things you do well. Doing well outweighs mistakes.\n\nYou can expect your interviewers to be reasonably helpful and completely honest. If you\u2019re offered a hint by an interviewer, don\u2019t worry, it\u2019s not a trick!\n\nWe know interviews are stressful. The problems aren\u2019t perfect, and nobody fires on all cylinders all the time. So, don\u2019t worry if you stumble a bit; we\u2019ll help you get back on track.\n\nIf you\u2019re interested in interviewing at Square, take a look at squareup.com/careers for career opportunities, and feel free to leave any questions in the comments below."
    },
    {
        "url": "https://medium.com/square-corner-blog/welcome-to-the-color-matrix-64d112e3f43d",
        "title": "Welcome to the (Color) Matrix \u2013 Square Corner Blog \u2013",
        "text": "This post started as an internal email thread to share a cool trick. I have little formal knowledge on the matter described below and took an empiric approach. Feedback is welcome in the comments!\n\nI was working on a new screen that showed a downloaded image with a title on top and these constraints:\n\nA designer sent me a few example mockups:\n\nHow could we dynamically tint those images?\n\nMy first instinct was to add a layer of the target color on top of the image with alpha transparency.\n\nLoading and transforming an image is easy with Picasso:\n\nHere is how to draw a color on top of the bitmap:\n\nIt\u2019s ok but not great. The colors from the original image are bleeding in, and for instance, you can see the hints of red in the woman\u2019s face.\n\nI sat down with the designer to understand how the mockup was assembled. He was using blending modes to merge the image with a colored layer \u2014 with some degree of opacity. He was tweaking each example to find the perfect mode and values that made the result look nice.\n\nWhile it\u2019s a great empiric approach, it\u2019s hard to translate to code. We tried more examples but could never come up with generic mode and values that would look great for all cases.\n\nLet\u2019s take a step back: we want to tint an image. We could first build a grayscale version of that image, and then tint it (thanks Romain Guy for the idea!).\n\nIn grayscale, the RGB channels of a pixel all share the same value. One common way is to compute the luminance as a weighted average that favors green, because human eyes are more sensitive to green.\n\nWe could loop on every pixel, as shown in a previous post, but that wouldn\u2019t be very efficient. Instead, we\u2019ll use a ColorMatrix to draw on a canvas with a color filter.\n\n4x5 matrix for transforming the color and alpha components of a bitmap. The matrix can be passed as single array and is treated as follows:\n\nWhen applied to a color [R, G, B, A], the resulting color is computed as:\n\nLet\u2019s apply our grayscale formula. Here\u2019s what we want:\n\nSo our color matrix should be:\n\nWe can apply the color matrix by drawing the bitmap onto a canvas using a paint with a color matrix color filter:\n\nTo apply a tint, we just need to multiply each channel by the corresponding channel in the tint.\n\nIn other words, we need to concatenate our grayscale matrix with the following tint matrix:\n\nHere\u2019s how it translates to code:\n\nThe image was tinted as expected, however the result was still not great. We wanted the text on top to be readable and didn\u2019t want the image below it to steal the user attention.\n\nRoman Nurik wrote a great article on how he came up with blurred wallpaper images for Muzei, and I thought we could try blurring. However, our design team wanted the image details to be visible.\n\nWhat\u2019s going on? There\u2019s simply too much black and overall too much contrast.\n\nThe luminance of each pixel is between 0 (black) and 255 (white), and we multiply that by our channel color (say red). So, we\u2019re effectively scaling it to [0, channel value], which means that blacks will stay black, and whites will change into our target color.\n\nThinking about it more, what we actually want is for each pixel to be of the custom color, plus / minus some variation that\u2019s going to let the image details surface in the background through contrast.\n\nIn other terms, we want to rescale the grayscale image to have a lower contrast, and shift its tint to be centered on the target color.\n\nFor example, if my custom color has red = 100, and I want a contrast amplitude of only 20%, we want to transform the the [0, 255] red channel to only 0.2 * 255 = 51 values centered around 100, (i.e. we should transform the red channel from [0, 255] to [75, 126]).\n\nLet\u2019s apply that scale and translate to our color matrix:\n\nJed Parson pointed out that the contrast felt softer for dark target colors, and he suggested that the contrast amplitude should vary according to the relative luminance of the target color.\n\nI changed the scale factor to be 1 minus the luminance of the target color:\n\nAll we needed to do now was add a TextView with the right text color on top of our image. Lucky for us, the support library provides ColorUtils to do just that.\n\nBy combining Picasso with color matrices, we were able to quickly iterate on our image transformation to get to an output that suited our needs.\n\nFeel free to provide feedback and additional insight in the comments!"
    },
    {
        "url": "https://medium.com/square-corner-blog/enabling-android-teams-tech-talks-fe531e69bee",
        "title": "Enabling Android Teams Tech Talks \u2013 Square Corner Blog \u2013",
        "text": "Recently, Haley Smith, Ray Ryan, Ty Smith, and Jesse Wilson gave lightning talks and participated in a panel discussion at our \u201cEnabling Android Teams\u201d event in Square\u2019s San Francisco office. (Thanks to everyone who attended!) These talks were recorded and are now available on our Square Engineering YouTube channel. Enjoy!"
    },
    {
        "url": "https://medium.com/square-corner-blog/optimizing-ruby-protobuf-deserialization-by-50-8aa14ca330d9",
        "title": "Optimizing ruby-protobuf deserialization by 50% \u2013 Square Corner Blog \u2013",
        "text": "I\u2019m on a team colloquially called \u201cWebScale\u201d at Square, which deals with extracting legacy payments data from our monorail. One of our recent tasks has been the rollout of a new internal service to search for payments on Square. We mostly use protobuf as our data transport, and a single payment proto is about 1,500\u20132,000 bytes when serialized and returns up to 500 records per request.\n\nOnce rollout of the new service started, a few requests from a Ruby MRI 2.2.3 client went from 250ms to 1,600ms on a few endpoints. We already have metrics around time spent in RPC requests, so we took a guess that protobuf deserialization was the culprit.\n\nTo verify our guess, we used ruby-prof to profile code in Ruby (in this case, the protobuf gem). RubyProf is a helpful tool for profiling MRI code, and here\u2019s how we used it to find hotspots.\n\nRubyProf supports various ways of printing a report. I picked FlatPrinter by using the scientific method of it appearing \u201cfirst on the list\u201d. I\u2019m only showing code that took more than 1% of the time for the sake of brevity.\n\nContinuing with our scientific method of \u201cfirst on the list\u201d, we\u2019ll dig into Protobuf::Enum. There are a few uses of to_i in that class, but the one that stands out is:\n\nThe code has great documentation on how the method works, which makes things easy for us. We use a lot of enums in our protos, and looking around the Protobuf::Enum class it\u2019s clear enums_for_tag is called frequently (in enum_for_tag and fetch). Looking at the all_tags and values methods, we know it\u2019s safe to cache enum data. In this case, it\u2019s a straightforward optimization of O(n) -> O(1). We can only map int -> enum because you can alias enums. That\u2019s fine for this case though.\n\nThe full changes are at the end, but the gist is:\n\nAnd then rerun it in RubyProf\n\nMuch better! The total time listed in RubyProf is accurate in percentage saved (~25%), but the total time is higher because RubyProf hooks into Ruby to profile everything. Without RubyProf, we saw requests from the Ruby MRI 2.2.3 client drop from 1,000ms to around 750ms.\n\nSince 750ms is still too long, we\u2019ll take a look at the next slowest callsite Protobuf::Decoder and the read_varint method.\n\nThe code is simple enough that any optimizations will be marginal at best. Ruby is just too slow for the amount of calls we\u2019re making, which makes this a candidate for rewriting the method in C. It turns out, the authors of the ruby-protocol-buffers gem (a different ruby protobuf gem from the one we use), released released a gem called varint which does exactly what we need.\n\nNormally, I don\u2019t like replacing Ruby with C, especially from a library. It\u2019s not guaranteed that the C and Ruby code will function the same way, and both debugging and maintenance are harder. Since it\u2019s already used by another gem, and the code is self contained though, I gave it a shot.\n\nWe add a simple monkeypatch to our test case:\n\nWe got a 18% optimization by blatantly cheating and having C do it instead. Even with all these improvements, our decoding time is around 600ms still.\n\nMoving on, we take another look at Protobuf::Decoder. Unfortunately the read_field and read_key methods are too simple for us to get any real optimization. Kernel#respond_to? is used in too many places for safety, such as returning nil if to_i doesn\u2019t exist rather than erroring.\n\nThat really just leaves us with Protobuf::Message::Fields::ClassMethods, unless we want to optimize 50 callsites for a few milliseconds each. Looking at get_field, we see:\n\nLooking at the RubyProf trace above, we already know that respond_to? is called 94,000 times total. After adding some debugging to count calls, it turns out get_field accounts for 72,000 (75%) of the calls to respond_to?, and it only calls to_sym 2,000 times. Essentially, only 2.7% of respond_to? calls result in a true.\n\nInstead of calling respond_to? and to_sym, we can cache a map of string field -> field and make it two O(1) lookups. The get_field method then looks like:\n\nOne last rerun of RubyProf\n\nThis gives us another 12% improvement, and Kernel#respond_to? was dropped to 22,000 calls (1.29% of self%). Overall, we optimized deserialization by 50%, and it only took a few hours of investigating with RubyProf.\n\nRubyProf is a great tool, and you can get even more details using a GraphPrinter or CallTreePrinter which can be used with KCachegrind. For our case, FlatPrinter was simple, and the code was easy enough to trace through to narrow down what to optimize. Examples of various printers in RubyProf can be found on the repo.\n\nWe shipped all of the changes upstream into the protobuf gem. You can see the exact changes that were made in the varint, get_field/get_extension_field, and enumoptimization PRs. If you\u2019re using the protobuf gem, v3.5.5 and higher with the varintgem (MRI only) installed will give you the full set of optimizations.\n\nIn the future, we\u2019re going to look at moving to protobuf3. The Ruby gem is in alpha, but it\u2019s built off Google\u2019s first party C++ library. In benchmarks against the optimizations above, Google\u2019s implementation is about 7x faster than ruby-protobuf."
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-git-fastclone-576ee734c509",
        "title": "Introducing git fastclone \u2013 Square Corner Blog \u2013",
        "text": "When your application and organization are growing, module boundaries get redefined. This can take the form of splitting large components, or combining and rewriting several related smaller components. When pursuing these efforts, you may find yourself having many git submodules and a monolithic build. Ever-increasing build times can make you more frustrated and unproductive, and the cost of smaller tasks can be magnified if repeated on many machines.\n\nFastclone alleviates these problems by creating a reference repo for every repository and their submodules, all in a simple ruby gem that can be run by typing git fastclone. It updates mirrors from origin and clones them into the target directory, working quickly with a recursive and multithreaded approach.\n\nThe first clone will take some time, but Fastclone shines when there are repeated clones on build slaves or multiple checkouts of the same repo:\n\nIn order to get to a clean checkout, we keep our .git directories around and do a quick check for incremental updates with the git server. After that, we perform a parallelized assembly of a proper working copy at the revision we asked for.\n\nWe checkout without a working tree using git clone \u2014 mirror. Since there is no working tree to update git fetch \u2014 all will always overwrite the local state with the remote state. These reference repositories have full history. Both git clone and git submodule init can be configured to use a reference repository over the remote by using \u2014 reference. By putting these all together, we get a recursive checkout with full history.\n\nThe rest of handling submodules is thread management and caching the contents of .gitmodules for each repository. Caching the submodule dependency tree allows us to spin all the update threads early in the clone operation. Since submodule links don\u2019t change that often, prefetching submodules we\u2019ll need from the git server gives a significant speedup over fetching lazily.\n\nThe submodule and git checkout caches are kept in /var/tmp/git-fastclone. This is configurable via an environment variable.\n\nA note about submodules: getting a clean checkout for CI or development purposes is an absolute bear. You can git pull, but you don\u2019t get submodule changes \u2014 and maybe some of your submodules have submodules. It gets more interesting when a teammate deletes a file in a submodule and git submodule update \u2014 init \u2014 recursive won\u2019t succeed either. These sorts of edge cases become worse as you have more developers and more submodules. As your repository gets bigger, it also takes more time to clone. Many simple timing optimizations find themselves at odds with the edge cases and impact the overall reliability of your checkout.\n\nSquare uses git-fastclone as part of our iOS and hardware CI systems. Being able to quickly clone into an empty directory, saves us time and ensures we always know the starting state for our builds \u2014 no matter what has happened in previous builds. This in turn increases the reliability of the system overall and benefits our engineers.\n\nFastclone uses git interfaces that are mostly stable. Future changes to git will most likely not impact fastclone.\n\nWe will be publishing improvements and bug fixes to git-fastclone as we write them. Pull requests for additional functionality are most welcome, and we\u2019d love to hear if you find our little tool useful.\n\nGet it here.\n\nAcknowledgments to Michael Tauraso for the earliest versions of fastclone and this post."
    },
    {
        "url": "https://medium.com/square-corner-blog/even-better-protocol-buffers-with-wire-2-1812fad303f2",
        "title": "Even Better Protocol Buffers With Wire 2 \u2013 Square Corner Blog \u2013",
        "text": "We\u2019ve been making steady progress on Wire, our implementation of compact, immutable protocol buffers for Android and Java. Today I\u2019m happy to announce Wire 2.0, which further refines our take on what protocol buffers should be.\n\nOne of my favorite features of Wire 2 is our sophisticated schema pruning. Suppose you have an analytics API defined by event.proto:\n\nThe browser field on the event is neither useful nor necessary to the Android app. But since the Event type depends on it, the Browser class and Feature enum are both dragged into the generated code. Unused code slows down the build, enlarges the APK, and distracts from the useful code around it.\n\nWire 2 can prune unwanted garbage with includes and excludes compiler options. We can prune the Browser type everywhere it appears:\n\nOr only this occurrence:\n\nIn either case, we\u2019ll get an Event message that\u2019s better suited to the client that consumes it. Provide whitelisted and blacklisted identifiers, and Wire 2 will retain the matched items and their transitive dependencies.\n\nUse pruning to hide old, deprecated stuff from newer clients while services retain everything for backwards compatibility. Or perhaps hide the new, unstable fields from your fast-moving client teams!\n\nWire 2\u2019s changelog lists all that\u2019s new (it\u2019s substantial)! Get Wire 2.0.0 on GitHub."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-short-guide-to-kernel-debugging-e6fdbe7bfcdf",
        "title": "A Short Guide to Kernel Debugging \u2013 Square Corner Blog \u2013",
        "text": "This article began as an internal email describing a problem that was found on Square\u2019s production hosts. The story is written to serve as an introduction to the tools and practice of debugging the Linux kernel, though it assumes a quite a bit of familiarity with how C programs are constructed.\n\nWhile I was on-call for our deployment infrastructure, a problem came my way: a service\u2019s latest release wasn\u2019t deploying properly. Log files pointed me to one host that seemed stuck. After logging into the machine and looking around, something seemed very wrong:\n\nKilling a management script unstuck the deployment process but left behind an orphaned, weeks-old, unkillable rm. My immediate problem was mitigated, yet there was clearly a deeper problem. What was going on?\n\nI knew the process wasn\u2019t spinning in user-space \u2014 the process didn\u2019t show up in top\u2019s output as a heavy CPU user before I tried to kill it, and it didn\u2019t die afterwards \u2014 so it must have been blocked inside a system call to the kernel. Often, a blocked process can be quickly diagnosed just by knowing which function it blocked in. Hoping for a quick solution, I looked at the process\u2019s kernel call stack:\n\nOf course rm was blocked unlinking a file; it doesn\u2019t do much else. What part of unlinking was it blocked on? Which file was it removing? What was so special about that file? Those answers depended on the data in this process\u2019s kernel call stack, not just a list of its functions. I needed to peek into the kernel\u2019s memory.\n\nAt this point, it was clear that simple diagnostics wouldn\u2019t reveal the problem. A reboot would \u201cfix\u201d it quickly but wouldn\u2019t prevent this from happening again. I sighed and set aside the rest of my day to find the root cause.\n\nDebugging the Linux kernel is more involved than debugging a normal process. With an interesting problem to solve as an example, let\u2019s take a look at some of the tools, techniques, and data structures that will be useful for understanding the internals of the kernel. Hopefully, this discussion can be a starting place the next time your own problem wanders out of user-space.\n\nA typical way to examine a process\u2019s state is to attach a debugger, like gdb, to it. The kernel is no different. Just like user-space processes, the binary must be built to support debugging. The machine we\u2019re debugging runs RedHat 6.5, and thankfully RedHat builds its kernels with some debug options set. To save space, the debug symbols are separate from the main kernel binary, so we\u2019ll need to install those.\n\nAnd if we want to understand what we\u2019re seeing, we\u2019ll have to install the source code for the kernel. The debug packages already install source code inside /usr/src/debug/\u2026. If you want the source code without the debug package (perhaps on another machine), it can be generated from a Source RPM. (1)\n\nNow, how do we attach gdb? There are many methods you could use depending on how stable the machine is, whether you can pause/reboot it, and whether it\u2019s running in a virtual machine. This is a bare-metal machine that I don\u2019t want to take down, so we\u2019ll used RedHat\u2019s crash tool. Crash is a wrapper around gdb that augments it with common functions for understanding kernel data structures. It\u2019s also capable of debugging the live kernel it\u2019s running on without disturbing it: just what we need.\n\nLet\u2019s start with an easy task: we\u2019ll verify that the program\u2019s stack is still the same as before:\n\nWhen a process makes a system call, all its registers are saved at the bottom of its stack; that\u2019s what we see here. We could use this to reconstruct the syscall parameters using the amd64 syscall calling convention. We also get to see struct task_struct and struct thread_info pointers for the process. These two structs are what the kernel uses to track every process/thread in the system. (In Linux, a \u201cthread\u201d is just another process that happens to share the same address space, exception handlers, file table, etc..)\n\nLooking at the full stack, it\u2019s clear that the process is waiting to acquire a mutex (the mutex_lock() call). With line numbers now:\n\nIf we look into \u201c/fs/namei.c\u201d to figure out which mutex_lock() call it\u2019s blocking on:\n\nLine 2810 is blocking? No, the backtrace shows that the return address goes to line 2810. The mutex call is before that, in line 2809.\n\nA struct dentry is the data structure that Linux uses to track and cache file system names. It associates a name in some directory with the actual file, a struct inode, that holds the data. Usually, files have only one name (and one corresponding dentry) in a file system. Hard links create additional names for a single file; removing an open file allows the file to have zero names (it won\u2019t be truly deleted until the file handle is closed).\n\nSo our process blocks acquiring an inode\u2019s mutex. Which inode? We could extract the file\u2019s name from the arguments to sys_unlinkat(), but that wouldn\u2019t lead us to the inode data structure. Let\u2019s see if we can access some of the function parameters from the call stack.\n\nCrash doesn\u2019t support gdb\u2019s standard \u201cup\u201d command for traversing a call stack. Instead, we can print a \u201cfull\u201d backtrace that includes the stack\u2019s memory.\n\nNormally, gdb would interpret that memory for us. The binary\u2019s debugging info tells gdb how to find the location of any saved variables or registers from any instruction. However, crash doesn\u2019t expose this functionality, forcing us to unwind the call stack manually while looking for useful context.\n\nAccording to the amd64 calling conventions, most function arguments are passed via registers without being explicitly saved anywhere else. We have access to the task\u2019s stack, but we don\u2019t get intermediate register states. Our best hope for recovering context is to look for a function in the call stack that happened to save an important variable to the stack.\n\nAfter looking through a few disassembled functions, we find do_unlinkat() where it calls into vfs_unlink():\n\nThis assembly block corresponds to the following code segment.\n\nThe second argument to vfs_unlink(), a dentry pointer, can tell us which file is being removed. We know from the calling conventions that the second argument is passed in the %rsi register. Its values comes from %rdx, which is saved to the stack before the function call, where we can recover it. (2)\n\nNow, we can look at a full memory dump of the task\u2019s stack, extract a frame pointer (FP), and retrieve the dentry:\n\nNow we can dump the relevant file system data structures:\n\nSo the file is \u201cfire.js\u201d in the \u201cjs\u201d directory. That\u2019s good to know. Now, let\u2019s see which process owns the mutex:\n\nSo, we now know which process has the mutex. What is it doing now?\n\nIt\u2019s waiting to acquire a write lock on a read-write semaphore. Which one? Using the same approach from above, we can recover frame #4\u2019s first argument:\n\nSo that one kernel can handle many different filesystems, there is a generic struct inode that the main virtual file system layer (VFS) can understand, but each different file system also has its own specific inode data structure. XFS uses a struct xfs_inode, and one if its fields is the embedded generic inode. For low-level synchronization, it includes a binary semaphore i_iolock, and this process is blocked trying to acquire that semaphore.\n\nAs it turns out, we\u2019ve seen this inode before:\n\nIt\u2019s the \u201cfire.js\u201d inode, whose i_mutex lock is already held by this process. It\u2019s now acquiring an XFS-specific semaphore on the same inode.\n\nUnfortunately, semaphores don\u2019t carry an \u201cowner\u201d field (in this configuration) like a mutex. So how do we find its owner? In this case, we can get lucky by assuming first that we\u2019re investigating a deadlock (though we haven\u2019t quite found that yet) and second that this process is a critical part of that deadlock. If we\u2019re right, another interesting process might be waiting on this process\u2019s mutex.\n\nWe started our session with process 5109, but what\u2019s this other process, 4724?\n\nHmm, I see XFS-specific code calling back into generic VFS code. Indeed, looking at xfs_file_splice_write(), you can see where it grabs the XFS i_iolock then calls generic_file_splice_write(), which then grabs the generic i_mutex.\n\nNow there is enough data to put together a complete picture of what happened. Most XFS write operations acquire a file\u2019s i_mutex first, then acquire i_iolock. The sys_open() call is an example of this ordering. However, the splice write operation is coded differently. It acquires i_iolock, then acquires i_mutex. We see that in the sys_sendfile64() call. So, whenever two processes concurrently write and splice to the same inode, each process can acquire one lock and then block on the other, leaving them deadlocked in uninterruptible sleeps and poisoning the inode.\n\nSearching for \u201cxfs splice deadlock\u201d turns up an email thread from 2011 that describes this problem. However, bisecting the kernel source repository shows that the bug wasn\u2019t really addressed until April, 2014 (8d02076) for release in Linux 3.16.\n\nSo why did we just spend all this time debugging a longstanding known and fixed problem? RedHat doesn\u2019t use the latest version of the kernel in their Enterprise Linux product. Using older, battle-hardened versions is a good thing when your goal is long-term stability. However, bugs will still be found, and they must be backported into the stable versions. RedHat just hasn\u2019t included this one particular fix yet. Square is working with them to reach a solution. Until then, all current versions to-date of RHEL/CentOS 6 and 7 are susceptible to this bug. Update as of 9/2/16: RedHat has now shipped updated kernels that contain a fix for this deadlock: version 2.6.32\u2013642 for RHEL 6 and version 3.10.0\u2013327.22.2 for RHEL 7.\n\nAs for the process that initiated this deadlock, it\u2019s a regular MRI Ruby 2.2.2 interpreter. Its core I/O functions have been optimized to use zero-copy operations when possible. Since early 2014 on Linux, a call to FileUtils.copy_file() from the standard library will use sendfile64(). Hence, there is a fairly compact trigger:\n\nI suspect that Square has seen Java processes deadlock in this way too. These programs are trying to do the right thing, and I would expect that as more libraries and runtimes mature, they will also try to optimize their I/O behavior by adopting zero-copy operations.\n\nCorrections: (1): Added explanation that you get the kernel source code along with the kernel-debuginfo-common package. (2): The second argument is passed in %rsi, not %rdx."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-presents-enabling-android-teams-ccb0cac7a7f5",
        "title": "Square Presents: Enabling Android Teams \u2013 Square Corner Blog \u2013",
        "text": "On Tuesday, November 10, we\u2019re hosting an Android event at Square\u2019s San Francisco office with our friends from Slack and Twitter. If you\u2019re in the Bay Area, join us for a night of technical discussion and lightning talks with expert engineers Haley Smith, Ray Ryan, Ty Smith, and Jesse Wilson. They\u2019ll fill us in on why cross-team collaboration is crucial to success, how future you will benefit from Android test automation, how to scale up architecture with reactive programming, and even discuss a bit of .dex internals.\n\nWe\u2019ll then jump immediately into a panel discussion moderated by Eric Burke that\u2019s guaranteed to spark a lively debate.\n\nYou can see full event details and register on our Eventbrite page. Space is limited and this is sure to fill up quickly! If you\u2019re not in the Bay Area, you won\u2019t miss out; we\u2019ll record the event and will share the videos here soon."
    },
    {
        "url": "https://medium.com/square-corner-blog/applications-now-open-college-code-camp-6-fcc819c45755",
        "title": "Applications now open: College Code Camp 6 \u2013 Square Corner Blog \u2013",
        "text": "This January marks two years since I attended Square\u2019s College Code Camp. Looking back, the most valuable aspect of Code Camp for me was the community we built. My fellow campers were intelligent, passionate, and opinionated, and their interests spanned from computer science to being a mom to interning at NASA. It was refreshing to be around people that reminded me of myself; to be around people who I aspired to be like; and to be around people I could inspire as well.\n\nCode Camp created a safe space where we could talk about anything on our mind, like imposter syndrome which many Code Campers wrote about. (Our lunches were far from small talk.) One of my favorite conversations centered around everyone\u2019s thoughts on wearing dresses to a conference. After a long discussion, we decided you should wear one if you want to, even if you feel that you\u2019ll stand out. To celebrate this decision, we all wore a dress on the final day of camp.\n\nTo this day, I keep in touch with the fellow participants from my cohort. I run into them at meetups, hackathons, and conferences, and I make an effort to reach out when I\u2019m in someone\u2019s hometown. We\u2019ve created a network of amazing women engineers, and I\u2019m grateful to be a part of it. I hope many more can join as well. With that, I\u2019m happy to announce that Code Camp applications are opening today."
    },
    {
        "url": "https://medium.com/square-corner-blog/fieldkit-a-simple-library-for-complex-input-formatting-ae9ad5f67169",
        "title": "FieldKit: A Simple Library for Complex Input Formatting",
        "text": "To create the best user experience, we strive to build a beautiful and simple interface for our products. So when we set out to build a checkout flow for our online store, we wanted customers to have the same great experience they\u2019d have on native apps when entering credit card and contact information. This would require a lot of work, but we knew we needed to banish the \u201cno dashes or spaces\u201d message often seen on websites. We did this with FieldKit.\n\nFieldKit provides real-time, text field formatting as users type. It simplifies input formatting and creates a more polished experience for users, while outputting standardized data. Fortunately, this is now easy for anyone to implement, because we\u2019ve open sourced FieldKit.\n\nYou can download the latest version of FieldKit and include the .js file. You can use also use npm to install FieldKit for use in your project\u2019s npm-based build system.\n\nBy default, FieldKit provides FieldKit.TextField as a simple base class that wraps a text field without changing its behavior.\n\nFieldKit also enables you to listen on some basic events over the lifetime of an input. Throughout the life of an input, there might be some actions taken on it, like textFieldDidBeginEditing or textDidChange. FieldKit notifies you when this happens.\n\nFieldKit is most useful when it\u2019s used with formatters. We have formatters for credit card numbers, expiry dates, phone numbers, localized currency, and Social Security numbers. FieldKit also provides the building blocks to create custom formatters.\n\nWith formatters, you can:\n\n* Control the data that\u2019s allowed to be entered into an input;\n\n* Insert extra characters \u2014 that were not typed by the user \u2014 into the input to standardize the format;\n\n* And, use custom formatting logic that changes based on the situation.\n\nAlongside formatters, we have a few different field types that add additional functionality to specific use cases. For example we have a CardTextField. This field automatically uses the AdaptiveCardFormatter, exposes a Luhn Check method, and blurs the full card number to only show the last 4 digits.\n\nThe other field we\u2019re including is the expiry date field. For more information into any of these areas, please head over to the source code and wiki.\n\nFieldKit is a tool that can be used to create a custom input experience for your application. Being able to customize the data input, including how it\u2019s formatted and parsed, will allow for a safer and more consistent user experience.\n\nFieldKit has been used for years at Square in just about every web project. Now, we\u2019re excited to see where you take it. Please help us improve FieldKit and make web inputs a better place."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-atlanta-is-growing-ab6ae00a4bef",
        "title": "Square Atlanta is growing \u2013 Square Corner Blog \u2013",
        "text": "It\u2019s hard to believe that Square\u2019s Atlanta office just celebrated its three year anniversary! Our engineering presence has nearly quadrupled since we planted our roots in July 2012, and it continues to thrive as we hire more engineers to take on additional projects. Today, the enginering team works on infrastructure and helps build tools that sellers use to manage their business and employees. We\u2019re also big fans of open source development, and projects such as Wire originated from the Atlanta office. We also actively develop on Pants.\n\nWe\u2019re committed to growing in Atlanta and plan to double the number of engineers in the city. We\u2019re looking for incredible server, iOS, and Android developers to help us build tools for every part of running a business. All career opportunities can be found on our careers page. We\u2019re also starting our search for next summer\u2019s interns.\n\nIf you\u2019re interested in learning more about development at Square, join us for Square Atlanta\u2019s first tech talk on September 10 at 6:00pm. David Carlson will present on service discovery at Square, and dive into how we graduated from static config to a platform on Zookeeper."
    },
    {
        "url": "https://medium.com/square-corner-blog/streamlining-android-apps-tech-talks-654ecc8ef645",
        "title": "Streamlining Android Apps Tech Talks \u2013 Square Corner Blog \u2013",
        "text": "Recently, PY and I gave tech talks at our \u201cStreamlining Android Apps\u201d event in Square\u2019s San Francisco office. (Thanks to everyone who attended!) These talks were recorded and are now available on our Square Engineering YouTube channel. Enjoy!"
    },
    {
        "url": "https://medium.com/square-corner-blog/a-small-leak-will-sink-a-great-ship-efbae00f9a0f",
        "title": "A small leak will sink a great ship \u2013 Square Corner Blog \u2013",
        "text": "This post started as an internal email thread when I was building LeakCanary. I found a strange memory leak and started digging in order to figure out what was happening.\n\nTL;DR: Prior to Android Lollipop, alert dialogs may cause memory leaks in your Android apps.\n\nI was getting memory leak reports from LeakCanary:\n\nIn plain words: a Picasso thread was holding on to a Message instance as a local variable on the stack. That Message had a reference to a DialogInterface.OnClickListener, which itself referenced a destroyed Activity.\n\nLocal variables are usually short lived since they only exist on the stack. When a method is called on a thread, a stack frame is allocated. When the method returns, that stack frame is cleared and all of its local variables are garbage collected. If a local variable is causing a leak then it normally means that a thread is either looping or blocking and keeping a reference to a Message instance while doing so.\n\nDimitris and I looked at the Picasso source code.\n\nThis thread receives messages through a Handler implemented in a very standard way:\n\nThis was a dead end. There was no obvious bug in Dispatcher.DispatcherHandler.handleMessage() that would somehow keep a reference to a Message through a local variable.\n\nEventually, more memory leak reports came in. It wasn\u2019t just Picasso. We would get local variable leaks from various types of thread, and there was always a dialog click listener involved. The leaking threads shared one common characteristic: they were worker threads and received work to do through some kind of blocking queue.\n\nLet\u2019s look at how HandlerThread works:\n\nThere\u2019s definitely a local variable referencing a Message. However it should be very short lived and cleared as soon as the loop iterates.\n\nWe tried to reproduce by writing a bare bones worker thread with a blocking queue and sending it only one message:\n\nOnce the message was printed in the log, we expected the MyMessage instance to be garbage collected.\n\nAs soon as we sent a new message to the queue, the previous message was garbage collected, and that new message was now leaking.\n\nIn the VM, each stack frame has a set of local variables. The garbage collector is conservative: if there is a reference that might be alive, it won\u2019t collect it.\n\nAfter an iteration, the local variable is no longer reachable, however it still holds a reference to the message. The interpreter/JIT could manually null\u00ad out the reference as soon as it is no longer reachable, but instead it just keeps the reference alive and assumes no damage will be done.\n\nTo confirm this theory, we manually set the reference to null and printed it again so that the null wouldn\u2019t be optimized away:\n\nWhen testing the above change, we saw that the MyMessage instance was garbage collected immediately after message was set to null. Our theory about the VM overlooking the local message variable seemed to be borne out.\n\nSince this leak could be reproduced on various thread and queue implementations, we were now sure that it was a VM bug. On top of that, we could only reproduce it on a Dalvik VM, not on an ART VM or a JVM.\n\nWe found a bug, but can it create huge memory leaks? Let\u2019s look at our original leak again:\n\nIn the messages we send to Picasso dispatcher thread, we never set Message.objto a DialogInterface.OnClickListener. How did it end up there?\n\nFurthermore, after the message is handled, it is immediately recycled and Message.obj is set to null. Only then does HandlerThread wait for the next message and temporarily leak that previous message:\n\nAs that point, we know that the leaking message has been recycled, and therefore doesn\u2019t hold on to its prior content.\n\nOnce recycled, the message goes back in a static pool:\n\nWe have a leaking empty Message that might be reused and filled with different content. A Message is always used the same way: detached from the pool, filled with content, put on a MessageQueue, then handled, finally recycled and put back in the pool.\n\nIt should therefore never keep its content for long. Why do we always end up leaking DialogInterface.OnClickListener instances?\n\nNotice that the click listener has a reference to the activity. The anonymous class gets translated to the following code:\n\nSo the OnClickListener is wrapped in a Message and set to AlertController.mButtonPositiveMessage. Let\u2019s look at when that Message is used:\n\nThe message is cloned, and its copy is sent. This means that the original Message is never sent, and therefore never recycled. So it keeps its content forever, until garbage collected.\n\nNow let\u2019s assume that this message was already leaking prior to being obtained from the recycled pool, due to a HandlerThread local reference. The Dialog is eventually garbage collected and releases the reference to the message held by mButtonPositiveMessage.\n\nHowever, since the message is already leaking, it won\u2019t be garbage collected. Same goes for its content, the OnClickListener, and therefore the Activity.\n\nCan we prove our theory?\n\nWe need to send a message to a HandlerThread, let it be consumed and recycled, and not send any other message to that thread so that it leaks the last message. Then, we need to show a dialog with a button and hope that this dialog will get the same message from the pool. It\u2019s quite likely to happen, because once recycled, a message becomes first in the pool.\n\nIf we run the above code and then rotate the screen to destroy the activity, there are good chances that this activity will leak.\n\nNow that we\u2019ve properly reproduced it, let\u2019s see what we can do to fix it.\n\nSupport only devices that use the ART VM, ie Android 5+. No more bugs! Also, no more users.\n\nYou could also assume that these leaks will have a limited impact and that you have better things to do, or maybe simpler leaks to fix. LeakCanary ignores all Messageleaks by default. Beware though, an activity holds its entire view hierarchy in memory and can retain several megabytes.\n\nMake sure your DialogInterface.OnClickListener instances do not hold strong references to activity instances, for example by clearing the reference to the listener when the dialog window is detached. Here\u2019s a wrapper class to make it easy:\n\nThen you can just wrap all OnClickListener instances:\n\nFlush your worker threads on a regular basis: send an empty message when a HandlerThread becomes idle to make sure no message leaks for long.\n\nThis is useful for libraries, because you can\u2019t control what developers are going to do with dialogs. We used it in Picasso, with a similar fix for other types of worker threads.\n\nAs we saw, a subtle and unexpected VM behavior can create a small leak that ends up holding on to huge chunks of the memory, eventually crashing your app with an OutOfMemoryError. A small leak will sink a great ship.\n\nMany thanks to Josh Humphries, Jesse Wilson, Manik Surtani, and Wouter Coekaerts for their help in our internal email thread."
    },
    {
        "url": "https://medium.com/square-corner-blog/ios-build-infrastructure-8f69f2d7adec",
        "title": "iOS Build Infrastructure \u2013 Square Corner Blog \u2013",
        "text": "Square has dozens of iOS engineers working on Square Register. Additionally, Square publishes multiple iOS apps to the App Store and distributes several iOS apps internally. To support the work of our application programmers, we\u2019ve invested in a continuous integration and test cluster.\n\nWe\u2019ve recently replaced the backend of this cluster so that it could scale beyond 8 machines. As a direct result of this commitment, over the last year we\u2019ve increased the reliability of builds and have nearly quadrupled the quantity of tests we\u2019re able to run for every engineer\u2019s contribution to the Register repository. This had a noticeable effect on the velocity of our feature teams and has informed us of many show-stopper regressions before we ship code.\n\niOS applications are a bit of a special snowflake when it comes to continuous integration. This post is about the backend infrastructure choices we made to support our iOS team.\n\nWe use Jenkins, along with a custom plugin to Stash, to form the backbone of our CI system. This plugin is similar to Palantir\u2019s Stashbot; it acts as a notifier to Jenkins when various sorts of git pushes occur. The build slaves in our build cluster connect to the Jenkins master using the swarm plugin. All builds run on the slave machines. The master Jenkins instance is responsible for serving the Jenkins web interface internally and moving artifacts and logs to and from the build slaves.\n\niOS applications must be built on OSX. We briefly considered using virtualized build infrastructure due to its many advantages; however in practice, we found that it was lacking in performance for us. Builds usually require many reads and writes of small files. This pushes the scheduler and the filesystem driver rather hard. With a virtualization system, there are two schedulers and two disk drivers. On Linux these systems work together well, but we found this not to be the case on OSX.\n\nAfter some testing we settled on the following bare-metal configuration, which is currently ~$1200 per build slave:\n\nWe also purchase fake hdmi monitors so that tests using the iOS simulator will use the graphics card rather than software rendering. This optimization provides a 10% speedup to our tests.\n\nWe wish it were still possible to purchase the 2012 quad-core configuration from the Apple store. Currently only the dual-core configuration available must be purchased as new. Our workload is highly parallelizable and having more compute units in the mini form-factor is desirable.\n\nIn order to get reliable and repeatable build results out of the machines, we wanted to make sure they are configured identically. We use DeployStudio to image new Mac minis over the network, and then do final setup with Ansible. Ansible allows us to have a checked-in record of our machine\u2019s configuration, as well as what machines we have in service at any given time. This is invaluable when we\u2019re testing new configuration or trying to understand problems that occur on the build slaves.\n\nChanges to the ansible repo also undergo code review and testing. Even with complex playbooks, we insist on writing them in idempotent pieces so we can quickly run the whole playbook against the entire cluster and cause no changes.\n\nWe\u2019ve found 10.10.4 Yosemite to be the most stable version of OSX with Xcode 6 and its simulators. At time of writing, we\u2019re using Xcode 6.1.1 on our cluster and are upgrading Xcode 6.3.2 piecemeal \u2014 testing reliability as we go.\n\nBeyond installing Xcode, OSX, and the Xcode command line tools, there are a number of system setup tweaks that make Xcode and the iOS simulators more reliable on the command line than they are out of the box. We want to be able to get reliable behavior out of OSX and Xcode, and these configuration tweaks have helped us do so.\n\nWe have all of our build slaves configured to automatically log-in the build user. This provides us with a GUI context for running the iOS simulator and means that we can start the Jenkins agent as a user Launch Agent. This allows any processes started by Jenkins access to that gui context. Many issues running the iOS simulator remotely stem from lack of API facilities found in a usual OSX login.\n\nWe augment this by running caffeinate on the Jenkins slave agent, so that the mini never goes to sleep. There are many sleep settings on OSX, and new settings are added in each release. We\u2019ve found setting a power assert using caffeinate to be the most reliable way to keep a build slave from going to sleep.\n\nThe build user being automatically logged means the build user\u2019s password is easily discoverable on the filesystem. For build slaves that create release binaries, we disable passworded SSH access and turn off VNC entirely. A trusted group at the company can access the code-signing machines to deploy our code-signing certificates, but no other remote access is permitted. The build user has sudo access to the slave itself, and we turn on DevToolsSecurity so that Xcode can function normally.\n\nXcodebuild uses the accessibility access hooks in OSX to control the iOS simulator. The manner that OSX gives access to these hooks has gained application-level granularity with Mavericks (10.9). Previously it was a system-level parameter that allowed all processes to access one another. If access is not granted, it can result in maddening issues where xcodebuild will run flawlessly when ran from Terminal.app, but will fail inexplicably when launched from Jenkins or SSH.\n\nMavericks (10.9) and Yosemite (10.10) determine if a process can access accessibility hooks via the parentage of the accessing process. By putting launchd in the list of allowed processes, processes launched via SSH or Jenkins have access to the accessibility hooks across the system. To do this you can modify the TCC database, per this gist. A reboot is required to make the change take effect.\n\nAs mentioned before, we use the Jenkins Swarm plugin to configure our slave machines. This is opposite of the normal Jenkins usage, in that the slaves each connect to the master node rather than being managed from the Jenkins configuration. This keeps us from having to maintain configuration on the master when adding or removing slave nodes, or when changing their IPs or DNS names.\n\nThe main thing to watch out for in the swarm plugin is a mismatch in the Jenkins remoting component. This often occurs during Jenkins upgrades and requires a push of a new swarm jar to the slaves. Because slaves that fail to connect to Jenkins simply don\u2019t appear in the list, it\u2019s advisable to monitor the machines externally to make sure they remain connected.\n\nWe maintain a high bar of build stability for the cluster, and we\u2019re currently operating 24 Mac minis. There are some aspects of maintaining the cluster that aren\u2019t quite smooth yet.\n\nXcode upgrades are still a bit of work. It\u2019s difficult to predict the level of stability we can expect from new releases. Most often the higher the point-release number the more reliable Xcode is in a larger number of situations. We often find ourselves filing radars and figuring workarounds for the behavior of new upgrades. When Xcode 6.1 was new, we decided to only have a single configuration of Xcode on a given machine (as there were some bugs with the beta interoperating with old versions of Xcode). This support has gotten better, and we\u2019re changing our config to allow multiple versions of Xcode on a single build slave. The support for multiple iOS simulated devices on a single host has also gotten easier over time, and we\u2019re working to make that airtight in our installation as well.\n\nWe hope this information is useful to folks building continuous integration systems for iOS. As it continues to change, we\u2019ll be updating The Corner with our current configuration practices. We\u2019re also interested in your experiences with Apple\u2019s software construction platform. Stackoverflow questions and blog posts have been some of our best teachers through this setup process, and we\u2019re grateful to everyone who\u2019s taken the time to educate us. Also, if you\u2019re interested in build and release tools and passionate about Apple\u2019s platform, drop us a line, we\u2019re hiring."
    },
    {
        "url": "https://medium.com/square-corner-blog/tech-talks-a-peek-into-ios-engineering-at-square-58231eb99c90",
        "title": "Tech Talks \u2014 A Peek Into iOS Engineering at Square \u2013 Square Corner Blog \u2013",
        "text": "During the week of WWDC, we hosted a tech talk to share some of the things we\u2019re doing at Square in iOS engineering. We enjoyed getting to chat with so many of you. For those who weren\u2019t able to attend (or would like to relive the tech talks), we recorded the event and uploaded the videos to YouTube.\n\nWe have a playlist of all the talks. We\u2019ve also broken out the individual talks:\n\nThe History of Square (Jack Dorsey)\n\nIntroduction and Welcome (Shuvo Chatterjee)\n\nScaling Square Register (Kyle Van Essen)\n\nCocoaPods at Square (Brian Partridge)\n\nPreventing Massive View Controllers (Alan Fineberg)\n\niOS Build & Test Infrastructure (Michael Tauraso)\n\nDependency Injection in iOS (Eric Firestone)\n\nCustom Morph Transitions (Shawn Welch)\n\nQ&A\n\nWe\u2019re excited to share these videos with you, and as always, we welcome your feedback. Find us at @SquareEng."
    },
    {
        "url": "https://medium.com/square-corner-blog/dude-wheres-my-char-f1eb223f8800",
        "title": "Dude, Where\u2019s My char[]? \u2013 Square Corner Blog \u2013",
        "text": "This article started as a thread on an internal mailing list and I thought it would also be of interest to people outside of Square.\n\nWhen Android M preview 2 was released, I started receiving reports of LeakCanarycrashing when parsing heap dumps. LeakCanary reached into the char array of a String object to read a thread name, but in Android M that char array wasn\u2019t there anymore.\n\nHere\u2019s the structure of String.java prior to Android M:\n\nWhere did that char[] go? To learn more, let\u2019s see what happens when we concatenate two strings:\n\nThe actual concatenation is done in mirror::String::AllocFromStrings in mirror::String.cc:\n\nFirst, it allocates a new string of the right size using Alloc in string-inl.h:\n\nSo this means that the char array is inlined in the String object.\n\nAlso notice the zero length array: uint16_t value_[0].\n\nWhere GetValue() is defined in mirror::String.h and returns the address of the uint16_t value_[0] that we noticed above:\n\nThis is a quite straightforward copy from the memory address of one array to another.\n\nYou probably noticed that the offset field is now entirely gone. Java strings are immutable, so older versions of the JDK allowed substrings to share the char array of their parent, with a different offset and count. This meant holding onto a small substring could hold onto a larger string in memory and prevent it from being garbage collected.\n\nThe char array is now inlined in the String object, so substrings can\u2019t share their parent char array, which is why offset isn\u2019t needed anymore.\n\nLet\u2019s speculate on why those changes are interesting:\n\nString is one of the most used types of the VM, so these micro optimizations will add up to huge improvements.\n\nBecause the char[] value field was removed from String.java, it could not be parsed in heap dumps. However in Android M Preview 2 the char buffer is still serialized in the heap dump, 16 bytes after the String address (because the String structure isn\u2019t longer than 16 bytes). This means we can get LeakCanary to work again with Android M:\n\nThis hack will eventually be fixed in Android M by inserting a virtual char[] valuefield in all String objects when dumping the heap.\n\nHuge thanks to Chester Hsieh, Romain Guy, Jesse Wilson, and Jake Wharton for their help figuring this out."
    },
    {
        "url": "https://medium.com/square-corner-blog/application-visibility-84133ffd95af",
        "title": "Application visibility \u2013 Square Corner Blog \u2013",
        "text": "Square has experienced tremendous growth over the past five years. Our technology stack evolved from a handful of monolithic rails applications to a microservices architecture. The change and growth in our services brought new challenges for application visibility. In today\u2019s blog post, we will review a few guiding principles and provide a sneak peak into various technologies we are using to monitor and visualize our diverse service ecosystem. We will be open sourcing various parts of our service monitoring and visualization stack, starting today!\n\nFew of our guiding principles are:\n\nSome of the applications we use at Square to follow these principles include:\n\nToday, we are open sourcing one of our smaller \u2014 but important \u2014 projects in our ecosystem: inspect. inspect is a collection of libraries that we use to collect Linux, MySQL, and PostgreSQL metrics. The project also includes a command line utility for Linux that can perform basic problem detection.\n\nWe hope you will find inspect helpful and that this blog post provided a sneak peek into various monitoring and alerting systems used at Square. We will be covering each of these systems in detail in subsequent blog posts. As always, please tune into The Corner for further updates!"
    },
    {
        "url": "https://medium.com/square-corner-blog/stricter-tests-for-expectations-set-on-nil-in-rspec-1b33244ca75d",
        "title": "Stricter Tests for Expectations Set on Nil in RSpec",
        "text": "When I started on one of my first pieces of code here at Square, I was tasked with making sure a Ruby program from the Data Platform team didn\u2019t continuously retry queries on data when the program hit a non-transient error. Our logs were getting filled up with info about multiple queries even after an error \u2014 such as table not found. No matter how many times we retried, the table was not going to be found! So as I set out to prevent some of these excessive queries, I started with our tests.\n\nAs my teammates reviewed my code, they spotted a bug pretty quickly: I had been refactoring some of the variables, and in the middle of doing so, I forgot to ensure that my instance variables were properly instantiated. I had over eagerly changed our @logger to logger in our initialize method of the class in question. It\u2019s an easy mistake to make, and luckily an easy one to spot. We quickly caught the bug in code review and were left wondering why our tests still passed. We ran the tests locally again. I skimmed the output of the build tied to the PR. Everything was green.\n\nAfter a few \u201chuh, that\u2019s weird\u201ds, we decided to run the tests one more time just to see what would happen. This time we watched the tests. Sure enough, they were still green, but this time around we saw there was some output that blew by the screen that we hadn\u2019t seen before. They were warnings from RSpec!\n\nThis brought to light two common problems with testing: \n\n1. Developers are lazy: if you have to scroll up to see the output, you probably won\u2019t.\n\n2. If everything\u2019s green, problems in your tests won\u2019t be seen.\n\nTo solve the above problems, which admittedly are not really the fault of testing itself, we thought it would be beneficial to force a test to fail when an expectation is set on nil. In RSpec-mocks, there\u2019s already the option to set a flag in your test to suppress the warning messages. We wanted something more strict for our code; we wanted the test to fail when we put an expectation on nil. Let\u2019s catch those false positives!\n\nWe started out by simply mimicking the behavior of what was already in place for suppressing the warning messages. There\u2019s a method you can call in your test, allow_message_expectations_on_nil, to do this. We went about implementing a disallow_message_expectations_on_nil method in the exact same manner. When discussing how to implement this, we immediately thought about how great it would be to have this as a config flag that you could simply set in your RSpec configuration but decided against doing so immediately. Since it was a small but new feature for the library, we decided it would be best to do the minimal viable implementation and open a PR so that we could get feedback from the maintainers on whether it was even something they would want to integrate first.\n\nOf course, one of the first comments from the maintainers was that yes, they would like to include the feature, but wouldn\u2019t it be better as a global config? It was suggested that we implement this stricter testing configuration like so:\n\nBy refactoring the allow_message_expectations_on_nil flag, we were not only able to move it into a place where you\u2019d no longer have to call that API for each test in which you wanted that setting, but we also expanded the options for how the mocks should behave. Previously, the only options were to allow with a warning or to allow and suppress the warning.\n\nNow, we have the following options that can be set inside the RSpec mocks configuration:\n\nOne of the benefits to this change was that we were able to refactor some of the code already in place for the warning. While previously it was just a method inside of the ProxyForNil class that called Kernel.warn(\u2018message here\u2019) with a hardcoded string, we were able to clean it up by using the ErrorGenerator class that had to be created at some later point. The ProxyForNil class in fact already had an instance of the ErrorGenerator instantiated, but it wasn\u2019t being used. Now when either the default behavior of warning or the newer implementation that raises an error is called , both methods use the ErrorGenerator class. This not only creates a more consistent implementation of errors and warnings, but it also allows for more precision in the testing of the framework itself!\n\nAnother other major challenge in contributing to a testing framework is wrapping your head around using the framework to test itself. It\u2019s much harder to read tests where the wording you use to test it is the same wording you are testing. Eventually, we got to the point where our test included this line that appears a bit mind-boggling at first:\n\nAfter getting to the point where we could expect all our expectations to behave the way we wanted, another challenge was figuring out how to test the configuration of the framework without changing the configuration for the whole suite from within your test. This stumped me for quite some time. We could get my test passing when we used the new feature to disallow expectations on nil, but then the stricter configuration would fail other tests in the suite!\n\nThankfully after poking around in the other tests that we suspected would encounter a similar problem, we found a handy shared context in the library\u2019s spec_helper file:\n\nBy including the shared context of the isolated configuration, testing the testing framework became easy. It also helps when the maintainers of the library are as helpful, responsive, and welcoming as the Rspec team \u2014 special thanks to Jon Rowe, Xavier Shay, and Myron Marston and the rest of the team!\n\nThe PR was successfully merged and put in place for you to use. Next time you run your test suite, consider forcing stricter tests by disallowing expectations on nil and see how many previously passing tests were false positives."
    },
    {
        "url": "https://medium.com/square-corner-blog/building-portable-binaries-50ca4f3d75cd",
        "title": "Building Portable Binaries \u2013 Square Corner Blog \u2013",
        "text": "Deploying your code is the last major hurdle in getting shiny new features or important bug fixes out to users. However, making sure your application has everything it needs can be a chore. Three typical approaches for preparing your application for deployment are:\n\nThis post will talk about approach number three. But first, I\u2019ll point out some of the problems we had with approach one and two, and how both options drove us to the third option.\n\nWith options one and two, your app is relying on the system to provide something. Normally that is perfectly fine, but imagine if you happen to upgrade your OS and it includes an update to your runtime. For example, Python is present on many Linux boxes. You may have just broken your app and caused downtime. With option one you may have also lost all system dependencies. Even if you try to reinstall them, they may not work with the newer runtime.\n\nWe\u2019ve seen this happen before when an OS upgrade broke a handful of services relying on a system binary. These services were using an older MySQL shared object that disappeared after the OS upgrade. So, we looked for a better solution: option 3.\n\nWhen looking for solutions, we didn\u2019t find anything that completely isolated both the runtime and dependencies in the way that we needed. Instead, we turned to two, well-known \u2014 if somewhat obscure \u2014 pieces of the Linux stack: shebangs and rpaths.\n\nShebangs are the first line of scripts \u2014 like /bin/bash or /usr/bin/env ruby \u2014 that begin with #!. The kernel reads #! and will execute the script with this interpreter.\n\nThe rpath is the runtime search path for shared libraries and is hard coded into the header of the compiled binary. This allows the binary to search for other parts of itself or core things like libc, which provides the core C standard library.\n\nAll this work started with a simple need to fix an application that was relying on the system to provide all of its dependencies and broke when deploying to new machines. While Virtualenv came up as a solution, it doesn\u2019t help with the Python installation problem. As a result, once I had a relocatable Python, I installed the dependencies like normal \u2014 thus Virtualenv wouldn\u2019t provide anything extra.\n\nThe biggest drawback with shebangs is that they don\u2019t support relative paths with respect to the binary location. You need to either hard code a system path or use a path that is relative to your current working directory when executed. The first option doesn\u2019t work when relocating binaries, and the second option doesn\u2019t work because it requires you to cd into the correct directory.\n\nHere\u2019s an example pip shim from a version of Python that I compiled:\n\nYou\u2019ll notice the #!/home/vagrant/python/bin/python shebang, which fails if I move the Python binary elsewhere.\n\nCertain languages have supported tricks that make this problem easier to solve. For example, a long time ago kernels didn\u2019t support shebangs; so, you\u2019d have to make sure you were running in your interpreter. Inspired by this work around (and after much trial and error), I came up with the following:\n\nWhen executed in Python, strings next to each other are simply concatenated and act as a no-op. However in Bash, it\u2019s executed as code with the whole quoted piece treated as one argument to eval \u2014 which is exactly what we needed.\n\nI had to use the eval and exit 0 commands paired with the -e flag instead of exec due to the way it expects arguments, because otherwise you\u2019d end up with a command like:\n\nThe tricky part is that the space is treated as a literal space in the argument name, but when printed it is no different than the correct set of arguments:\n\nThe single quotes were the final piece that took me a while to get correct so that the $@ (i.e. the arguments to the script), were passed along correctly.\n\nAn rpath for an executable is a header in a program that helps the linker find the needed shared objects at runtime. A shared object is a set of compiled code that is intended to be shared amongst a bunch of different binaries. Libc is the best example of a shared object as it is required by just about everything. Here are some examples of the headers for /bin/bash:\n\nYou\u2019ll notice bash requires libc, libdl, and libtinfo. Running ldd on that binary gives you the exact locations of the dependency resolution the linker has done:\n\nldd will report the locations of the shared objects it finds, but it ignores the rpath (if present). bash doesn\u2019t use an rpath, but here\u2019s an example of the Python we\u2019ll be compiling soon to help reach option three (i.e. shipping both the runtime and dependencies with the app):\n\nNotice the RPATH line? That\u2019s the magic that allows us to relocate binaries. It\u2019s specifically important for Python, because the binary requires a shared object. That $ORIGIN variable is also super important as it allows you to make paths relative to the binary. In this case, Python is expected to look in /example/lib for shared objects, along with the default linker locations. The man page for ld.so has more details.\n\nWhile that magic is awesome, actually going from source code to an actual binary is somewhat difficult. Due to the fact that variables in shell are of the form $var, nested calls to a shell can cause the variable to be interpolated or malformed. (Note: For this rpath to work, the literal string $ORIGIN needs to be present.)\n\nI know, this is a cliffhanger. Why did I choose option three? Hold tight. You first need background on why I was working with rpaths. I was using Python and planned to compile mod_wsgi as my application server, but it requires Python to be a shared object. As a result, when compiling Python you can give it the \u2014 enable-shared flag to have it build the shared object. The only downside is that the Python interpreter now needs to be able to find that shared object. After my first attempt compiling a default Python build, I ended up this error:\n\nWell, that wasn\u2019t what I wanted. I needed to do one of the following:\n\nThe biggest downside with LD_LIBRARY_PATH is it needs to be set as an environment variable, which is a bit of a pain to remember everytime you want to use Python. Instead, I decided to go with option number two (RPATH), which led to me to this first attempt at compiling with an rpath:\n\nThe configure runs fine, so make should be a piece of cake:\n\nNotice the RIGIN/../lib missing the $O portion? The variable needs to be passed all the way to the linker as $ORIGIN/../lib for this trick to work. After a fair amount of trial and error, the final command to get the variable passed to the linker correctly was:\n\nI was able to verify using the readelf I posted above, and the final confirmation came by running:\n\nIf you\u2019d like to triple check that it\u2019s loading the correct library (i.e. your compiled one and not the system one if it happens to exists), you can use strace:\n\nYou can see that it eventually ends up looking in the /example/python/libdirectory.\n\nObviously, choosing option three wasn\u2019t the easiest or most obvious approach \u2014 compiling relocatable binaries can be hard (and sometimes impossible). However, using relative shebangs and rpaths help make it easier and allow more flexibility with your application deployment, while still being reliable."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-presents-streamlining-android-apps-885839e1774c",
        "title": "Square Presents: Streamlining Android Apps \u2013 Square Corner Blog \u2013",
        "text": "On June 25 we\u2019re hosting an Android-themed tech talk at our San Francisco office featuring Jake Wharton and Pierre-Yves Ricau. There will also be time to meet one another over snacks and drinks.\n\nJake kicks off the event with a talk on \u201cEliminating Code Overhead,\u201d demonstrating techniques that both libraries and applications can implement to minimize CPU and memory usage.\n\nThen, Pierre presents LeakCanary, our new memory leak detection library for Android and Java. Thanks to LeakCanary, we reduced OutOfMemoryError crashes in Square Register for Android by 94% in just a few weeks.\n\nIf you\u2019re an Android developer, we\u2019d be thrilled to give you a peek into engineering at Square. You can RSVP to the event here."
    },
    {
        "url": "https://medium.com/square-corner-blog/dependency-injection-give-your-ios-code-a-shot-in-the-arm-ba98594f5002",
        "title": "Dependency Injection: Give Your iOS Code a Shot in the Arm",
        "text": "Dependency injection (DI) is a popular design pattern in many languages, such as Java and C# , but it hasn\u2019t seen wide adoption in Objective-C. This article aims to give a brief introduction to dependency injection using Objective-C examples, as well as practical methods for using dependency injection in Objective-C code. Although the article focuses on Objective-C, all of the concepts apply to Swift as well.\n\nThe concept of dependency injection is very simple: an object should require you to pass in any dependencies rather than creating them itself. Martin Fowler\u2019s excellent discussion on the subject is highly suggested background reading.\n\nDependencies can be passed to an object via the initializer (or \u201cconstructor\u201d), or via properties (or \u201csetters\u201d). These are commonly referred to as \u201cconstructor injection\u201d and \u201csetter injection.\u201d\n\nAs Fowler describes, constructor injection is preferred, and as a general rule you should only fall back to setter injection if constructor injection is not possible. With constructor injection, you\u2019ll likely still have @property definitions for these dependencies, but you can make them read only to simplify your object\u2019s API.\n\nDependency injection offers a number of benefits, but some of the more important ones are:\n\nYour codebase may not already be designed using the dependency injection design pattern, but it\u2019s easy to get started. One nice aspect of dependency injection is that you don\u2019t need to adopt \u201call or nothing.\u201d Instead, you can apply it to particular areas of the codebase and expand from there.\n\nFirst, let\u2019s classify classes into two buckets: basic classes and complex classes. Basic classes are ones which have no dependencies, or which depend only on other basic classes. Basic classes are highly unlikely to be subclassed, because their functionality is clear, invariable, and doesn\u2019t reference external resources. Many examples of basic classes come from Cocoa itself, such as NSString, NSArray, NSDictionary, and NSNumber.\n\nComplex classes are the opposite. They have other complex dependencies, include application-level logic (which may need to change), or access external resources such as the disk, the network, or a global in-memory service. Most of the classes in your application will be complex, including almost any controller object and most model objects. Many Cocoa classes are complex as well, such as NSURLConnection or UIViewController.\n\nGiven these classifications, the easiest way to get started is to pick a complex class in your application and look for places where you initialize other complex objects within that class (search for \u201calloc] init\u201d or \u201cnew]\u201d). To introduce dependency injection into the class, change this instantiated object to be passed in as an initializer parameter on the class rather than the class instantiating the object itself.\n\nLet\u2019s look at an example where the child object (the dependency) is being initialized as part of the parent\u2019s initialization. The original code might look something like this:\n\nThe dependency injection version of this is a simple change:\n\nSome objects aren\u2019t needed until a later time, after initialization, or may never be needed at all. An example (before using dependency injection) might look like this:\n\nIn this case the race car hopefully never crashes, and we never need to use our fire extinguisher. Because the chance of needing this object is low, we don\u2019t want to slow down the creation of every race car by creating it immediately in initialization. Alternatively, if our race car needs to recover from multiple crashes, it will need to create multiple extinguishers. For these situations, we can use a factory.\n\nFactories are standard Objective-C blocks that require no arguments and return a concrete instance of an object. An object can control when its dependencies are created using these blocks without needing to know the details of how to create them.\n\nHere\u2019s an example using dependency injection that uses a factory to create our fire extinguisher:\n\nFactories are also useful in cases where we need to create an unknown number of a dependency, even if that creation is done during initialization. For example:\n\nIf objects shouldn\u2019t be allocated within other objects, where are they allocated? And aren\u2019t all these dependencies hard to configure? Aren\u2019t most of them the same every time? The solution to these problems lies in class convenience initializers (think +[NSDictionary dictionary]). We\u2019ll pull the configuration of our object graph out of our normal objects, leaving them with pure, testable, business logic.\n\nBefore adding a class convenience initializer, make sure that it\u2019s necessary. If an object has only a few arguments to its init method, and those arguments don\u2019t have reasonable defaults, then a class convenience initializer isn\u2019t necessary and the caller should use the standard init method directly.\n\nTo configure our object we\u2019ll gather our dependencies from four places:\n\nA class convenience initializer for our race car would look like this:\n\nYour class convenience initializer should live where it feels most appropriate. A commonly used (and reusable) configuration will live in the same .m file as the object, while a configuration that is used specifically by a Foo object should live in a @interface RaceCar (FooConfiguration) category and be named something like fooRaceCar.\n\nFor many objects in Cocoa, only one instance will ever exist. Examples include [UIApplication sharedApplication], [NSFileManager defaultManager], [NSUserDefaults standardUserDefaults], and [UIDevice currentDevice]. If an object has a dependency on one of these objects, then it should be included as an initializer argument. Even though there may only be one instance in your production code, your tests may want to mock that instance out or create one instance per-test to avoid test interdependence.\n\nIt\u2019s recommended that you avoid creating globally-referenced singletons in your own code, and instead create a single instance of an object when it is first needed and inject it into all the objects that depend on it.\n\nOccasionally there are cases where the initializer/constructor for a class can\u2019t be changed or can\u2019t be called directly. In these cases you should use setter injection. For example:\n\nSetter injection allows you to configure the object, but it introduces additional mutability that must be tested and handled in the object\u2019s design. Luckily, there are two primary scenarios that cause initializers to be inaccessible or unmodifiable, and both can be avoided.\n\nThe use of the \u201cclass registration\u201d factory pattern means that objects cannot modify their initializers.\n\nAn easy replacement for this is to use factory blocks rather than classes to declare your list.\n\nStoryboards offer a convenient way to lay out your user interface, but they also introduce problems when it comes to dependency injection. In particular, instantiating the initial view controller in a storyboard does not allow you to choose which initializer is called. Similarly, when following a segue that is defined in a storyboard, the destination view controller is instantiated for you without letting you specify the initializer.\n\nThe solution here is to avoid using storyboards. This seems like an extreme solution, but we\u2019ve found that storyboards have other issues scaling with large teams. Additionally, there\u2019s no need to lose most of the benefit of storyboards. XIBs offer all of the same benefits that storyboards provide, minus segues, but still let you customize the initializer.\n\nDependency injection encourages you to expose more objects in your public interface. As mentioned, this has numerous benefits. But when building frameworks, it can significantly bloat your public API. Prior to applying dependency injection, public Object A may have used private Object B (which in turn uses private Object C), but Objects B and C were never exposed outside of the framework. With dependency injection Object A has Object B in its public initializer, and Object B in turn makes Object C public in its initializer.\n\nObject B and Object C are implementation details, and you don\u2019t want the framework consumer to have to worry about them. We can solve this by using protocols.\n\nDependency injection is a natural fit for Objective-C (and Swift by extension). Applied properly it makes your codebase easy to read, easy to test, and easy to maintain."
    },
    {
        "url": "https://medium.com/square-corner-blog/valet-a-better-place-to-put-your-keys-dda59b986f31",
        "title": "Valet: A Better Place to Put Your Keys \u2013 Square Corner Blog \u2013",
        "text": "If you\u2019ve ever had to pore over SecItem.h or interact with the Keychain, you\u2019re well aware that it\u2019s a terrifying API. You pass in a dictionary and then you get an error back that lets you know that at least one of your parameters was wrong. Want to know which one? No dice. Once you finally validate those dictionary parameters, there are still no set semantics. The only arrows you\u2019ve got in the quiver are add and update, but adding a key/value pair will fail if the key already exists in Keychain. Worse yet, there are subtle and undocumented differences between the key/value pairs you should pass into SecItem* on iOS and Mac.\n\nThe pain doesn\u2019t end there. Keychain is a loose cannon in multithreaded environments, since there\u2019s nothing in place to guarantee atomicity of Keychain operations. Even if you do everything right, you might be bitten by a SecItemUpdate call that erroneously tells you it has succeeded updating that metadata you passed in \u2014 when, in fact, it only updated a subset of the elements it should have updated.\n\nIf you\u2019re storing your secrets in a storage mechanism other than Keychain, you\u2019re doing it wrong. There is no other secure place to store your data. There are NSUserDefaults, plists, etc., but these are risibly poor places to store anything requiring any level of security \u2014 just ask anyone who has used iExplorer. And if you\u2019re thinking that you\u2019re fine because you\u2019ve rolled your own encryption mechanism, you\u2019re breaking a fundamental rule of crypto: don\u2019t write your own crypto.\n\nValet has an API inspired by NSMapTable that uses Keychain under the hood. Every Cocoa API you are used to has set/get semantics, and so does Valet. Valet ensures atomic reads and writes into the keychain; you\u2019ll never get a failure, because you\u2019re interacting with Valet on multiple threads. If you haven\u2019t touched Keychain before, we promise you\u2019ll never have to read SecItem.h, and you\u2019ll have those tokens migrated out of your plists and into a Valet instance in no time.\n\nIf you\u2019ve spent time learning the ins and outs of the Keychain, we\u2019re sorry. But on the bright side, Valet\u2019s API is already in your lexicon. Oh, and we\u2019ve got a one-line method to import your old keychain items into Valet. Once you migrate to Valet, you\u2019ll never need to write another switch for errSec* handling again. Valet calls will never fail as long as -canAccessKeychain returns YES.\n\nEven better, you never have to worry about stomping or deleting a secret you didn\u2019t mean to \u2014 every Valet lives in a sandbox. If two Valets can read or write to the same space, they\u2019ll pass both -isEqual: and == checks. Each Valet is sandboxed based on four core attributes: initializer, identifier, accessibility, and class. This sandbox means that migrating from one Valet that is accessible only when the phone is unlocked to another Valet that is accessible after first unlock is guaranteed to work. (You\u2019re not going to get bitten by a bad SecItemUpdate call.) When you migrate these items between your Valet\u2019s, the items are literally copied from one Valet to the other rather than updating them in place (which we estimate has a silent failure rate of about 0.5%).\n\nBest of all, Valet makes it easy to harness the full power of Keychain. Want to share secrets across multiple applications you are writing? Just pass your shared access group identifier into the shared access group initializer. Want to sync your secrets to other devices via iCloud? We\u2019ve got a Valet for that. Want to store your secrets on the Secure Enclave, requiring Touch ID or the device\u2019s passcode to retrieve each secret? We\u2019ve got a Valet for that too.\n\nSo what are you waiting for? Hand over the keys and get back to doing the work you actually want to do.\n\nThis post is part of Square\u2019s \u201cWeek of iOS\u201d series."
    },
    {
        "url": "https://medium.com/square-corner-blog/space-commader-take-command-of-objective-c-whitespace-d1b69213097d",
        "title": "[ Space Commader]: Take Command of Objective-C Whitespace",
        "text": "Square is releasing [ Space Commander] on GitHub. [ Space Commander] is a set of tools we use to setup, customize, and enforce Objective-C formatting across our Register iOS team, and reclaim time that would be spent manually formatting code.\n\nManually fixing code formatting issues can burn a lot of development cycles that are better spent elsewhere \u2014 but the alternative is a \u201cbroken windows\u201d situation, as inconsistencies mount and readability decreases. Enter automated code formatting!\n\nTo date, we\u2019ve automated the formatting of over 3500 distinct Objective-C files and removed the need to comment on formatting from over 1200 pull requests and counting, an estimated time savings of (at least) 200 developer hours.\n\nclang-format, provided by LLVM, is a tool that we\u2019re excited about at Square, since it can automatically fix code formatting. It does a lot, yet we needed additional tools to fit our needs of formatting Objective-C code to our particular specification, and coordinating a team of developers easily. That\u2019s where [ Space Commander] comes in.\n\n[ Space Commander] has brought our iOS team into the future of automatically formatted code files. We\u2019re happy to hand over the \u201ccommand\u201d of where spaces, tabs, braces, etc. go in our Objective-C files, so we can ship pristine code faster.\n\nYou can clone [ Space Commander] in order to use Square\u2019s code style, or fork it to customize the formatting rules to your own liking.\n\nWe hope [ Space Commander] brings you pristine code and saves you time, and of course we welcome contributions of improvements!\n\nThis post is part of Square\u2019s \u201cWeek of iOS\u201d series."
    },
    {
        "url": "https://medium.com/square-corner-blog/aardvark-your-logs-slurped-your-bugs-licked-b57f972e129a",
        "title": "Aardvark: Your Logs Slurped, Your Bugs Licked \u2013 Square Corner Blog \u2013",
        "text": "We\u2019ve all experienced the frustration of getting a bug report and not having enough information to track it down. The email hits your inbox: \u201cHey, I tried to do X and it didn\u2019t work.\u201d There\u2019s no build number, no screenshot, and no explanation of what \u201ctried\u201d or \u201cdidn\u2019t work\u201d actually mean. You know you have a bug, but you don\u2019t have what you need to track it down. You desperately email the reporter seeking the information you need. Maybe you even send them instructions for how to import console logs from their device. By the time they get back to you, they\u2019ve already updated their build, their logs have already rolled over, and they have no idea how to reproduce what they saw.\n\nIt\u2019s hard to file a good bug. Nobody\u2019s memory is perfect\u2013and a person who takes the time to report a bug is only going to take so much time out of their day to let you know how you screwed up. So let\u2019s take the burden off the tester.\n\nAardvark is a comprehensive logging system that makes it extremely easy to log every event in your app and get those logs off of a device at the moment you need them: when the bug is filed. When someone sees something go wrong, all they need to do is press and hold with two fingers. Aardvark takes a screenshot, asks the customer what went wrong, and then generates an email bug report with your logs and the screenshot attached. Now you have an actionable bug report.\n\nThere\u2019s also a log viewer built into Aardvark, so you can view the logs directly on the device without having to compose an email. When you\u2019re working in the simulator, you can export logs to a file or print to console. When you\u2019re working on device, you\u2019ve got Airdrop at your disposal.\n\nBetter yet, Aardvark is a cinch to hook into your app. Start logging to Aardvark by replacing all of your NSLog calls with ARKLog. Then set up bug reporting with a single line of code. That\u2019s it!\n\nAardvark is more than just a bug filing tool\u2013it\u2019s capable of managing all your logging needs. If you use logging not only to help track down bugs, but also to track how customers interact with your apps (and to learn what a customer was doing right before a crash happened), you can use Aardvark to send these logs out to other services.\n\nHow? When you call ARKLog, Aardvark shoots that log off to a Log Distributor, which then notifies each of its Log Observers about the message. Each Log Observer then consumes the log and makes sure it goes where it needs to. Want to forward your logs to Crashlytics (the awesome crash reporting tool that Square uses)? With Aardvark, you can do so with just 10 lines of code. Just add a log observer to the default log distributor that calls CLSLog when it receives a message.\n\nRemember that time you put too many NSLog statements into your app, and it slowed to a crawl? That won\u2019t happen with ARKLog. There\u2019s only one piece of work that Aardvark will do on the queue that calls it: construct the format strings you send it. Once the string is constructed, Aardvark asynchronously dispatches the string to a serial background queue for processing. Worried about all those logs eating up memory? Don\u2019t be. Aardvark\u2019s ARKLogStore utilizes a background queue to serialize the logs directly to disk. It even manages keeping the log file size to a minimum for you. It pulls logs into memory only when a bug is filed.\n\nWe hope Aardvark is as useful to you as it is to us, and we hope you contribute back any improvements you make!\n\nThis post is part of Square\u2019s \u201cWeek of iOS\u201d series."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-week-of-ios-9685c2019d73",
        "title": "A Week of iOS \u2013 Square Corner Blog \u2013",
        "text": "At Square, we\u2019re big fans of WWDC. Our iOS developers come together to learn about the latest trends and technologies in iOS, and share best practices with the community. Today, we\u2019re kicking off something new.\n\nThis week leading up to WWDC, we\u2019re sharing some of our own best practices and open sourcing several of our iOS projects. And to wrap up the week on June 9, we\u2019re hosting a tech talk. We\u2019ll give demos and present lightning talks (including a continuation of Scaling Square Register). After, there\u2019ll be time to meet one another over snacks and drinks.\n\nIf you\u2019re an iOS developer, we\u2019d be thrilled to give you a peek into iOS engineering at Square. You can RSVP to the event here. If you won\u2019t be in the Bay Area, you won\u2019t miss out. We\u2019ll be updating The Corner with new posts throughout this week."
    },
    {
        "url": "https://medium.com/square-corner-blog/build-stability-82dabdf00c3a",
        "title": "Build Stability \u2013 Square Corner Blog \u2013",
        "text": "Every developer at a company that practices continuous integration (CI) has the following conversation at some point about their builds:\n\nDev 1: \u201cMy build failed, but I didn\u2019t change anything related to testSwissCheeseController\u201d\n\nDev 2: \u201cOh just re-run it. That test has been failing for everyone. It\u2019s not your fault\u201d\n\nThis is a frustrating exchange, especially when it occurs randomly and blocks important work. Automatic checking of code is only useful so far as the signal from it can be trusted. Unfortunately the sort of failure that these hypothetical developers describe is difficult to reproduce, diagnose, fix, and verify for exactly the same reason it\u2019s infuriating: it doesn\u2019t happen all the time.\n\nThere are many best practices you can use to reduce shared state in your tests, write better tests, and prevent these sorts of failures proactively. But when those techniques fail, what do you do?\n\nCI builds attempt to measure whether application code is good code or not. Good code builds and passes tests. We can\u2019t measure whether code is good code or not; we can only measure whether code passes tests once on a particular machine on a particular day.\n\nWe have two main types of measurement error: false negatives, where the build is red on good code, and false positives, where the build is green on bad code. Typically developers and build systems treat a green build as the \u201csafe-to-merge\u201d signal. Therefore it is possible for low-probability, false-negatives to accumulate on master and never get fixed. False negatives can also be measured by running tests on code that has at least one green build.\n\nFalse positives cannot be measured as easily. Consider a test that always passes no matter the application code. It would be difficult to find this sort of test automatically. At Square, we use proactive techniques in our build scripts to avoid false positive errors from the CI system itself. Most notably, we use execution constructs in ruby and bash that ensure any process exiting abnormally is an immediate build failure unless handled explicitly.\n\nEvery night on our CI cluster, we re-run a successful build from the previous day to measure false positives. Using all of our build machines in parallel, we can get several hundred builds done while most developers are not working. Every morning, we count and triage the failures. This is a similar task to crash triage of a consumer facing application. Some of the failures are filed as bugs and assigned to the appropriate team depending on their nature and magnitude.\n\nWhen we take data from a night of builds, we only have the number of times the build has run at a particular revision and the number of times it has failed. This failure percentage alone is occasionally misleading as to the nature of a bug.\n\nThe build team has a responsibility to maintain a 99% reliability SLA. We\u2019ve noticed that when a build has <1% chance of failing, developer trust in the builds increases. Failures above the 1% level are very visible and frustrating \u2014 even in a group of a couple dozen developers.\n\nSometimes a 1% error won\u2019t recur in 200 runs. Sometimes a 0.1% error will happen twice in the same number of builds. We have limited build machine resources and limited people available for triage and investigation. How do we use the failure percentage in order to allocate those resources well? We can set this up as a statistics problem.\n\nGiven the total number of builds and failures, we\u2019re looking for a way to guess the range of the actual probability of failure. The binomial distribution describes the behavior of an unfair coin with probability P of getting heads. Stability check builds are an extremely unfair coin, as they typically have a success probability very close to 1.\n\nWe calculate a confidence interval about our measured probability using the Wilson score interval. The Wilson score interval is an extremal approximation of the Binomial confidence interval based off the normal distribution.\n\nA good way to think about this is to consider a build that fails 1% of the time. We run it 200 times and get one failure. We might conclude therefore that the failure rate is 0.5%. This is a good guess, but we only have a single data point. If it fails again on the 201st trial, our guess will change dramatically. Likewise, if we see the same failure in the first 10 builds, we would guess that it happens 10% of the time. This is a reasonable guess, but another 10 trials with no additional failures would change our expectation significantly.\n\nIn the first example above, the 80% confidence interval from the Wilson calculation is 0.27%\u20131.54%. In the second example it is 5.5%\u201326.4%. These ranges show what we have actually found out given the limited number of trials better than the lone 0.5% or 10% number. The ranges quantify our current understanding of the failure as well as its magnitude.\n\nThis spreadsheet provides a visual representation of the probability model. The first sheet lets you input the number of builds and failures. The second shows the binomial distribution, as well as 90%, 80%, and 50% confidence intervals. The spreadsheet does a numerical integration, so the results are not exact and do not match the Wilson score calculation.\n\nAt Square, we use the width of the confidence interval to decide whether to file a bug or not. If the confidence interval width is narrower than the observed error percentage for that category, we file a bug. If we observe a greater error percentage, we set the bug\u2019s priority higher. We have calibrated thresholds on this process internally to make sure errors that affect the overall SLA are prioritized appropriately.\n\nThis technique is ideal to draw attention to errors of the build and test system that haven\u2019t received attention, because they happen too infrequently. For things that happen often or failure modes that are well known, a proactive approach is more effective.\n\nMaking decisions based off the confidence interval was instrumental in squeezing the last bits of error out of the iOS CI system at Square. We found ways to configure our build slaves to avoid Xcode and simulator bugs. We found and fixed a rare memory mishandling bug affecting hundreds of our tests. We were also able to find several race conditions in new features before deploying Register to our manual testers.\n\nAt our most stable, we\u2019ve measured one failure out of 1,500 builds in a week. Approaching 99.9% stability has massively increased the trust in the iOS CI system. We\u2019re working on applying this approach and analysis to other areas where CI is practiced at Square.\n\nThis post is part of Square\u2019s \u201cWeek of iOS\u201d series."
    },
    {
        "url": "https://medium.com/square-corner-blog/okhttps-new-url-class-515460eea661",
        "title": "OkHttp\u2019s New URL Class \u2013 Square Corner Blog \u2013",
        "text": "Android and Java developers have several options when it comes to manipulating URLs:\n\nWithin OkHttp, we\u2019ve always used java.net.URL as our preferred model. It works, but it\u2019s a hassle; each method is almost right but not quite. So, we lean on helper methods and workarounds to get the behavior we want. For example, this is our code to get a URL\u2019s port:\n\nWe are tired of the workarounds. So in OkHttp 2.4, we created our own URL model, HttpUrl. It improves upon its predecessors in four important ways.\n\nWhat\u2019s the worst thing about being a Java programmer? Catching exceptions that cannot possibly be thrown:\n\nOur new HttpUrl class doesn\u2019t force you to deal with a MalformedURLExceptionor URISyntaxException. Instead, parse() just returns null when it doesn\u2019t understand what you passed it. The lack of an exception means we can finally declare URL constants:\n\nThe parser is strict enough to produce only well-formed URLs, but lenient enough for raw user input. It\u2019s suitable for use in a browser\u2019s address bar and consistent with URL parsers in major web browsers.\n\nLet\u2019s parse some URLs, add them to a set, and print the result. This is an easy way to see how equals() is implemented.\n\nThe first two URLs in the example are semantically equal: They have the same hostname (hostnames are case-insensitive), and the same port (since 80 is the default for HTTP). The third URL is different.\n\nBut java.net.URL treats all three URLs as equal, because square.github.io and google.github.io are hosted at the same IP address! The above program prints this:\n\nEw! Calling URL.equals() also does a DNS lookup, which is bad for both performance and correctness. This is a longstanding bug, and it\u2019s bizarre that it remains unfixed two decades later.\n\nNeither URI and Uri canonicalize their input. So although the first two URLs are semantically equivalent, they aren\u2018t equal. For those two models, the program prints:\n\nWith HttpUrl, we do light canonicalization of input URLs. It prints two values:\n\nA good equals() method means that HttpUrl is suitable for use as a key in a LinkedHashMap or even a Guava Cache.\n\nJava\u2019s built-in URL classes lack any ability to extract the query parameters from a URL. Suppose you have a URL like this one for a Twitter search:\n\nCalling getQuery() or getRawQuery() returns a single string with all parameters glued together:\n\nThis is awkward. Fortunately HttpUrl can decompose the query without fuss:\n\nThe queryParameter() method extracts the requested value and decodes it. The above code prints this:\n\nThere\u2019s also a method to retrieve the query parameter without decoding it (if you\u2019re into that sort of thing).\n\nJust as the HttpUrl class lets you decompose a URL into its scheme, host, path, query, and fragment, its builder can compose a URL from raw materials.\n\nThe builder accepts each component in either its semantic or encoded form. It won\u2019t double encode the percent character, or misinterpret a plus as a space. The builder also makes it easy to build upon an existing URL:\n\nGet OkHttp 2.4.0 on GitHub. It has just what you\u2019ll need to make HTTP requests in Java and Android applications."
    },
    {
        "url": "https://medium.com/square-corner-blog/open-source-security-bug-bounty-9905b3f423fc",
        "title": "Open Source Security Bug Bounty \u2013 Square Corner Blog \u2013",
        "text": "Today, we\u2019re excited to announce our security bug bounty program for our open source software. We recognize the important contributions the security research community can make when it comes to finding bugs, and we\u2019re asking for your help to report security bugs in our open source code.\n\nWe\u2019ve released more than 50 open source projects \u2014 many of which are critical components of our infrastructure. With so many sellers relying on Square to run and grow their business, it\u2019s our number one priority to ensure our code is secure. We welcome you to report problems for any project that has a BUG-BOUNTY.md file, including Keywhiz, KeywhizFs, js-JOSE, Go-JOSE, OkHttp, Squalor, Retrofit, Okio, Wire, and pam_krb5_ccache. We\u2019ll continue to add projects to the bounty.\n\nIf you discover a security flaw, head to our HackerOne page (created specifically for our open source software), and read about our program. While it\u2019s not required that you attach a fix to bug reports, patches are greatly appreciated. To preserve confidentiality of potential security issues, please do not open a pull request against the project to fix issues you report; instead, create a patch and attach it to the HackerOne report."
    },
    {
        "url": "https://medium.com/square-corner-blog/leakcanary-detect-all-memory-leaks-875ff8360745",
        "title": "LeakCanary: Detect all memory leaks! \u2013 Square Corner Blog \u2013",
        "text": "In Square Register, we draw the customer\u2019s signature on a bitmap cache. This bitmap is the size of the device\u2019s screen, and we had a significant number of out of memory (OOM) crashes when creating it.\n\nWe tried a few approaches, none of which solved the issue:\n\nThe bitmap size was not a problem. When the memory is almost full, an OOM can happen anywhere. It tends to happen more often in places where you create big objects, like bitmaps. The OOM is a symptom of a deeper problem: memory leaks.\n\nSome objects have a limited lifetime. When their job is done, they are expected to be garbage collected. If a chain of references holds an object in memory after the end of its expected lifetime, this creates a memory leak. When these leaks accumulate, the app runs out of memory.\n\nFor instance, after Activity.onDestroy() is called, the activity, its view hierarchy and their associated bitmaps should all be garbage collectable. If a thread running in the background holds a reference to the activity, then the corresponding memory cannot be reclaimed. This eventually leads to an OutOfMemoryError crash.\n\nHunting memory leaks is a manual process, well described in Raizlabs\u2019 Wrangling Dalvik series.\n\nHere are the key steps:\n\nWhat if a library could do all this before you even get to an OOM, and let you focus on fixing the memory leak?\n\nLeakCanary is an Open Source Java library to detect memory leaks in your debug builds.\n\nLet\u2019s look at a cat example:\n\nYou create a RefWatcher instance and give it an object to watch:\n\nWhen the leak is detected, you automatically get a nice leak trace:\n\nWe know you\u2019re busy writing features, so we made it very easy to setup. With just one line of code, LeakCanary will automatically detect activity leaks:\n\nYou get a notification and a nice display out of the box:\n\nAfter enabling LeakCanary, we discovered and fixed many memory leaks in our app. We even found a few leaks in the Android SDK.\n\nThe results are amazing. We now have 94% fewer crashes from OOM errors.\n\nIf you want to eliminate OOM crashes, install LeakCanary now!"
    },
    {
        "url": "https://medium.com/square-corner-blog/protecting-infrastructure-secrets-with-keywhiz-af674410832f",
        "title": "Protecting infrastructure secrets with Keywhiz \u2013 Square Corner Blog \u2013",
        "text": "At Square, our number one priority is security. We needed something to protect secrets, especially as their number increased with our adoption of a service-orientedmicroservice architecture. Although protecting infrastructure secrets is a common need, we weren\u2019t able to find an adequate secret management system. (More on this under \u201cExisting Practices.\u201d) So, we built Keywhiz.\n\nKeywhiz is a secret management and distribution service that is now available for everyone. Keywhiz helps us with infrastructure secrets, including TLS certificates and keys, GPG keyrings, symmetric keys, database credentials, API tokens, and SSH keys for external services \u2014 and even some non-secrets like TLS trust stores. Automation with Keywhiz allows us to seamlessly distribute and generate the necessary secrets for our services, which provides a consistent and secure environment, and ultimately helps us ship faster.\n\nTo better understand our secret management solution, it\u2019s helpful to expound on what we require from a system.\n\nWe\u2019ve found there are a few common patterns for storing infrastructure secrets, including storing secrets in source code, manually deploying to servers, and using configuration management.\n\nStoring secrets in source code is a prevalent anti-pattern in security. Source code must be accessible on development, revision control, testing, and continuous integration systems \u2014 none of which are designed to securely store or distribute secret information. Additionally, updating secret content shouldn\u2019t be tied to a revision of code; merely rotating keys shouldn\u2019t cause system changes.\n\nOn a small scale, manually deploying secrets to servers is a reasonable approach. However, without a secret management system, this approach quickly becomes unwieldy as more secrets are inevitably created, replaced, and replicated across more systems. This approach leaves secrets prone to be left in home directories, temporary folders, and backup copies. Some secrets are inevitably not updated or stored with incorrect permissions. Auditing access to secrets or reasoning about them comprehensively, like determining upcoming certificate expirations, becomes difficult. If deploying from an encrypted store, there may be one master key to decrypt everything or a complex mapping of what should be deployed where. An old, misplaced, or improperly erased disk can lead to secrets being leaked.\n\nSecret management schemes based on configuration management systems have the same disadvantages as storing secrets in source code and on server disks. Although they have the advantage of being able to decouple secret changes from code changes, configuration management systems are meant to be widely visible and replicated, and to retain change history \u2014 all of which are antithetical to secret management. Many projects have been made to encrypt secrets before placing them in a configuration management system, typically using GPG or a home-grown use of AES. Then, a trusted individual enters the key at deployment time and plaintext secrets are deployed onto server disks. Conclusively deleting a secret is hard (to nearly impossible). Key rotation in a configuration management system must have autonomous authority to make changes to configuration, and must sometimes have access to decryption keys. Also, coupling secret management and configuration management makes it difficult to migrate to another system in the future.\n\nThe Keywhiz system is primarily composed of Keywhiz servers and a FUSE filesystem client called KeywhizFs. FUSE enables a program to expose a virtual filesystem without actually storing anything on disk. Administration of Keywhiz servers is done through a web app, CLI, or an automation REST API. Communication between servers, KeywhizFs, and automation clients is protected using mutual authentication with TLS.\n\nWithin Keywhiz, access control is defined in terms of clients, groups, and secrets. Each certificate that authenticates for secrets is called a client. Clients are assigned membership to an arbitrary number of groups. To allow a client to access a secret, the secret must be granted to at least one of the groups the client is in. In practice, we create a group for each service on a specific server, a group for each service, and a group that everyone is included in. These three groups cover most use-cases.\n\nTo protect secrets stored on the server side, every secret is AES-GCM encrypted with a unique key before being stored in a database. This unique key is generated using HKDF. Square uses hardware security modules to contain derivation keys.\n\nServices get access to secrets through KeywhizFs. At Square, each service on every host has a directory where a KeywhizFs filesystem is mounted. Services merely have to open a read-only \u201cfile\u201d in that directory to access a secret. Performing a directory listing shows which secrets are accessible. Local access control is straightforward; traditional Unix file permissions are used for the secret \u201cfiles.\u201d The advantage of a file-based representation is that nearly all software is compatible with reading secrets from files.\n\nKeywhizFs uses UNIX permissions to provide local access control and separation. KeywhizFs client certificates, processes, and virtual directories are owned by a special KeywhizFs user, distinct from the user a service uses. Assuming service users are non-privileged, the KeywhizFs mount point (owned by the KeywhizFs user) is the sole interface. To make secrets accessible to a service, the KeywhizFs mount point is assigned to the service user\u2019s group and all secrets are group-readable. This works in the majority of cases, but the occasional software package has strict requirements on file ownership or permission. In such cases, extra metadata is stored with the secret on Keywhiz servers and instructs KeywhizFs to present special ownership or permissions.\n\nRather than copying files to a remote server, KeywhizFs actually queries Keywhiz servers for information and caches data in KeywhizFs process memory. In the event of a network disruption or Keywhiz server failure, KeywhizFs will continue to serve authorized secrets that were previously accessed. Secrets are never written to disk but cached in memory. This is an additional safety mechanism on top of clustering Keywhiz servers to ensure there\u2019s not a foundational outage. If a server is powered off, no data is persisted to disk.\n\nKeywhizFs has additional benefits over actual files. For example, every secret access is logged, including the user behind it. Other ideas \u2014 such as client-side cryptography and exposing older versions of secrets \u2014 are under consideration.\n\nKeywhiz uses many TLS certificates, one for each server and each KeywhizFs mount point. That\u2019s a certificate for every service on every server that it\u2019s deployed to. This presumes a PKI system for creating trusted certificates and a deployment system that determines where software should be running.\n\nCompanies devise various PKI systems \u2014 from internal Certificate Authorities on special hardware to using the portal of a public Certificate Authority. Keywhiz only requires TLS certificates with particular Common Name fields, so it is compatible with most PKI systems. If you don\u2019t have an existing PKI, certstrap is a simple starting point.\n\nSquare\u2019s deployment system is authoritative for what software runs where. When a service is first being deployed to a server, the deployment system will insert secrets and authorize a new client via Keywhiz\u2019s automation APIs. A new certificate for KeywhizFs is generated by our certificate authority, an fstab entry written, and KeywhizFs is mounted in a standard directory for the service to read from. On subsequent deploys, some secrets are automatically renewed, including the certificate used by KeywhizFs. When the service is decommissioned, the relevant access is removed and secrets deleted.\n\nKeywhiz has been extremely useful to Square. It\u2019s supported both widespread internal use of cryptography and a dynamic microservice architecture. Initially, Keywhiz use decoupled many amalgamations of configuration from secret content, which made secrets more secure and configuration more accessible. Over time, improvements have led to engineers not even realizing Keywhiz is there. It just works. Please check it out."
    },
    {
        "url": "https://medium.com/square-corner-blog/our-fifth-college-code-camp-bfeae8efc8db",
        "title": "Our Fifth College Code Camp \u2013 Square Corner Blog \u2013",
        "text": "College Code Camp is our five-day, in-house immersion program for women studying computer science. Spelman student Brianna Fugate shares her experience.\n\nIn January, I had the privilege of participating in Square\u2019s College Code Camp with 22 other women engineering students from all over North America. We were a diverse group with a variety of backgrounds, but we all shared a passion and intellectual curiosity for programming and careers in engineering. When we first arrived in San Francisco, we bonded over icebreakers and art \u2014 creating t-shirts and bags at a silk-screening printshop. This was the beginning of a sisterhood that would only get stronger.\n\nOver the next few days, we focused on our personal and professional development. Square engineers led technical sessions (aka: tech treks) on various areas of engineering. These tech treks were one of my favorite aspects of Code Camp because of the conversations I had with engineers who are creating revolutionary technologies. They shared insight into how we could utilize the same technologies for our hackathon project, which we\u2019d present at the end of Code Camp.\n\nWe also engaged in meaningful conversation with Square\u2019s CFO Sarah Friar about her experience in leadership positions. She is an inspiration. Her leadership gave me the confidence to continue to persevere as a woman in tech. I also gained confidence to pursue my passion for programming because of my mentor\u2019s support and advice. (All participants were given a personal mentor, who would show them a day in the life of a Square engineer.) My mentor encouraged me to continue to work hard and pursue what I love.\n\nFor our final group project in the hackathon, my team built a mobile app for Children International, a nonprofit that provides assistance to children and families struggling in poverty. This was my first time using Swift and building an app. I love that Code Camp focused on both growing a foundation for a sisterhood and helping us learn new skills. I left San Francisco with so many new friends and new technical skills, like machine learning and Swift.\n\nCollege Code Camp showed me the importance of diversity in the tech world. Ideas can come from anywhere \u2014 people just need opportunities to grow both personally and professionally. I now have the confidence and information to help those around me through computer science. I encourage any woman who is pursuing a career in tech to apply for College Code Camp. Through the program, you\u2019ll make friends who\u2019ll last a lifetime, gain a great network and support system of Square employees (who are rooting for your success), and have an incredible time in San Francisco."
    },
    {
        "url": "https://medium.com/square-corner-blog/scaling-square-register-c7f53df3dd2",
        "title": "Scaling Square Register \u2013 Square Corner Blog \u2013",
        "text": "Recently I had the opportunity to write about Register iOS development for objc.io. My article focused on the tools and processes we chose at Square to help scale and streamline the engineering process. If this sounds interesting to you, you can check out the article here: http://www.objc.io/issue-22/square.html"
    },
    {
        "url": "https://medium.com/square-corner-blog/grpc-cross-platform-open-source-rpc-over-http-2-56c03b5a0173",
        "title": "gRPC \u2014 cross-platform open source RPC over HTTP/2 \u2013 Square Corner Blog \u2013",
        "text": "Several months ago, we evaluated open sourcing our proprietary, internal RPC library (which is based on protocol buffers). Along the way, we also decided to move this framework to HTTP/2. Luckily before putting forth too much effort, we learned Google was attempting to open source something similar \u2014 and was much further along. We paused our efforts and reached out to Google to see how we could help.\n\nGoogle looped us in, and we got involved with gRPC, an open source (BSD-licensed) cross-platform library for making remote procedure calls. (This is useful for both client-server communications as well as server-server communications in a microservice architecture.) It\u2019s been a fun process discussing designs, playing with an early implementation, and collaborating to open source gRPC.\n\nYou may be thinking, \u201cAnother RPC library? What\u2019s wrong with JSON-over-REST?\u201d Well, nothing \u2014 except that binary protocols have inherent benefits in terms of bandwidth and CPU efficiency, type safety, and memory footprint. Also, building on top of the recently finalized HTTP/2 specification gives us bidirectional streaming, multiplexed connections, flow control, and header compression.\n\ngRPC is cross-platform too with support for C, C++, Java, Go, Node.js, Python, and Ruby, with libraries for Objective-C, PHP, and C# being planned.\n\nThe project is run completely in the open with upstream master repositories on GitHub. Communications (IRC, Google Group) and documentation is all open and public. Please tag StackOverflow questions with \u201cgrpc\u201d.\n\nGoogle blogged about this release too.\n\nWe\u2019d love to hear your thoughts on gRPC. Contribute by forking and submitting pull requests, chat with us, and follow @grpcio."
    },
    {
        "url": "https://medium.com/square-corner-blog/sqlbrite-a-reactive-database-foundation-40e3a617866d",
        "title": "SQLBrite: A reactive Database Foundation \u2013 Square Corner Blog \u2013",
        "text": "Storing, accessing, and modifying persisted data on Android has been an ever-changing landscape since the platform\u2019s inception. Last year, after evaluating 16 libraries on a spectrum of strict requirements, we determined that nothing existed which met all our goals and instead chose to embark on writing our own solution.\n\nSQLite is the obvious choice for persisting and querying complex data. Over the last year we designed, debated, and prototyped a complete solution for simplifying an application\u2019s SQLite interaction. This included features such as automatic table creation and migration, object-mapping for rows, type-safe querying, and notifications for when data changes.\n\nWhen it came time to integrate with Square Cash, we knew we had a lot of great concepts, but the implementation suffered from being heavily churned upon. So a few weeks ago, we decided to incrementally rewrite the library from scratch in order to retain the ideas and concepts \u2014 but have proper architecture.\n\nSQLBrite is the first release of this effort and it will serve as the foundation of future additions. Instead of single executions, you can subscribe to queries using RxJava observables:\n\nNo attempt is made to hide SQL, Cursor, or the semantics of SQLiteOpenHelper(Android\u2019s SQLite wrapper). Instead, those three concepts are given a superpower: data change notifications.\n\nWhenever data in a table is updated from insert, update, or delete operations (whether in a transaction or as a one-off), subscribers to that data are updated.\n\nWhen multiple queries are constantly refreshed with data, the UI updates in real-time instead of staying as a simple, static page.\n\nSQLBrite is open source on GitHub at github.com/square/sqlbrite as a first step. Over time, the feature set will become more comprehensive with the help and support of the community."
    },
    {
        "url": "https://medium.com/square-corner-blog/our-new-high-school-code-camp-9d84fa1a8b6f",
        "title": "Our New High School Code Camp \u2013 Square Corner Blog \u2013",
        "text": "Two years ago, we learned only 1% of high school girls believed coding would be part of their educational or professional future. We wanted to encourage more young women to pursue STEM and launched High School Code Camp, an eight-month, after-school program designed to prepare young women in San Francisco for the AP Computer Science exam. Square engineers built the computer science curriculum, and over the school year helped the young women create through code \u2014 all while breaking any preconceived notions they had about programming.\n\nSince then, many of our engineers began teaching local students in the classroom through programs like TEALS and Girls Who Code Clubs. So we\u2019re evolving our in-house program into a week-long camp that focuses on real-world exposure into careers in technology and encourages students to stay curious about the field of engineering.\n\nWe\u2019re opening our second High School Code Camp to young men and women in San Francisco, and will select a diverse group of students that represents what the future of tech should look like. The camp will take place March 30 \u2014 April 3, and participants will attend hands-on coding workshops (taught by Square engineers), mentorship sessions, career panels, and tour tech companies. They\u2019ll also meet with Square\u2019s inspiring executives, including CFO Sarah Friar, Engineering Lead Alyssa Henry, and CEO Jack Dorsey.\n\nHigh school students can submit short essays online about why they\u2019d like to be part of the program at squareup.com/code-camp/high-school-code-camp until February 20. Twenty participants will be selected."
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-javapoet-f9a41435e396",
        "title": "Introducing JavaPoet \u2013 Square Corner Blog \u2013",
        "text": "I used to hate generated code. Early implementations of JSPs, EJBs, and protocol buffers spewed out huge, misformatted files littered with warnings and fully-qualified type names:\n\nBut things get better. Integrating generated code into a project is easier thanks to Java\u2019s powerful annotation processing facilities. Testing code generators has been tamed by Google\u2019s compile-testing library. Today Square moves faster thanks to the generated code in AutoValue, Dagger, Butter Knife, and Wire.\n\nUnfortunately the code that generates code in each of these libraries is still awkward. For example, Dagger writes each generated file top to bottom which means it needs to prepare imports as a clumsy separate step. It models the .java file as a string and doesn\u2019t understand its structure.\n\nToday I\u2019m eager to announce JavaPoet, a successor to JavaWriter. JavaPoet models a .java file with types, methods, and fields and does imports automatically. Generate \u201cHello World\u201d like this:"
    },
    {
        "url": "https://medium.com/square-corner-blog/open-source-code-of-conduct-1c726ad3ab3c",
        "title": "Open Source Code of Conduct \u2013 Square Corner Blog \u2013",
        "text": "At Square, we\u2019re committed to contributing to the open source community and simplifying the process of releasing and managing open source software. We\u2019ve seen incredible support and enthusiasm from thousands of people who have already contributed to our projects \u2014 and we want to ensure our community continues to be truly open for everyone.\n\nToday, we\u2019ve published our open source code of conduct, which outlines our expectations for participants, as well as steps to reporting unacceptable behavior. We are committed to providing a welcoming and inspiring environment for all and expect our code of conduct to be honored.\n\nYou can review our code of conduct on the homepage of The Corner.\n\nThank you for your support, and thanks to the Twitter, Ubuntu, GDC, and Django communities, who provided guidance for our code of conduct."
    },
    {
        "url": "https://medium.com/square-corner-blog/everything-i-wish-i-didnt-know-about-concurrency-53e9a34b449a",
        "title": "Everything I wish I didn\u2019t know about concurrency \u2013 Square Corner Blog \u2013",
        "text": "Everything we do as engineers involves concurrency. Thanks to limitations in heat dissipation technology, Moore\u2019s Law hit practical limits over a decade ago and the only way computing has progressed has been to cram more cores onto a die, and/or use more CPUs. And Amdahl\u2019s Law has ruled ever since.\n\nConcurrency, though, is everywhere, and there exists a large class of systems that we don\u2019t traditionally think of as \u201cconcurrent\u201d \u2014 such as the interactions between a client and a server, single-threaded applications running on a multi-user system, OS virtualization, shared datastores, and many more. Thinking about concurrency is vital to writing correct, scalable, and performant software. However, the successful application of concurrency is a unique blend of theory and practice.\n\nAt Square, we recently organized a seminar titled \u201cEverything I wish I didn\u2019t know about concurrency\u201d, where we started with a short refresher on some concurrency concepts and idioms. We then presented a series of questions to a panel of Square engineers, who shared war stories, anecdotes, and puzzlers, and talked about interesting applications of concurrency they\u2019ve encountered.\n\nYou\u2019ll get a glimpse of the mental heuristics applied when designing, building, and debugging concurrent systems. Whether you triggered your first race condition on an Apple II in 1977 or on a mission-critical production system earlier this month, there should be content that is relevant, fun, and interesting for you!"
    },
    {
        "url": "https://medium.com/square-corner-blog/webhooks-in-the-square-connect-api-d4d38c4b4d9f",
        "title": "Webhooks in the Square Connect API \u2013 Square Corner Blog \u2013",
        "text": "Square\u2019s partners have contributed an expansive collection of app integrations to our platform over the past year. From accounting solutions like Intuit Quickbooks and Xero, to the Fresh KDS kitchen display system, these integrations give merchants even more tools to help them run their business \u2014 and they are all built on top of the Square Connect API.\n\nThe Square Connect API is available to everyone. If you\u2019re a developer (who also happens to run a business on Square), you can build applications to download your business\u2019s transaction history and help manage your items catalog.\n\nRecently we launched a new API feature: Webhooks. Now, your application can get real-time notifications when you process a payment or refund. Let\u2019s try it out!\n\n1. Create a Square account. If you don\u2019t already have a free Square account for your business, download the Square Register app to your Android or iOS device and create one.\n\n2. Set up a server to listen for notifications. Webhook notifications come in as HTTP POST requests, so you\u2019ll need a server running to listen for them. For this example, head to requestcatcher.com and create a subdomain (such as example.requestcatcher.com). Run the sample curl command it provides to confirm that the server is listening.\n\n3. Register an application with Square. Sign in to connect.squareup.com/appsand create a new app. After you click Save, copy the personal access tokenthat\u2019s generated for your application. This token lets you access your own merchant data with the Connect API.\n\n4. Enable Webhooks support. While you\u2019re on the Apps page, scroll down to the Webhooks section and click Enable Webhooks if it isn\u2019t already enabled. Also specify the URL that your server is listening on (e.g., http://example.requestcatcher.com/test) and click Save again.\n\n5. Subscribe to webhook notifications. Your application is ready to receive webhook notifications, but it still needs to specify which merchants it wants to receive notifications for. Subscribe to notifications for your own merchant account with the following curl request, substituting your personal access token where indicated:\n\n6. Create a payment. Open up Square Register on your Android or iOS device and simulate a cash payment. Within about thirty seconds, your requestcatcher.com page should receive a POST request with a JSON body similar to the following:\n\nA full-fledged application could then pass the value of entity_id along to the Connect API\u2019s Retrieve Payment endpoint to get the full details of the updated payment.\n\nAnd that\u2019s Webhooks! We\u2019re really excited to see what developers can build with them. For more in-depth info on all things Connect API, check out the documentation. This Github sample also demonstrates a basic web server that listens for webhook notifications.\n\nAs always, if you hit a snag working with the Connect API, don\u2019t hesitate to post a question with the square-connect label to Stack Overflow."
    },
    {
        "url": "https://medium.com/square-corner-blog/anders-hejlsberg-tech-talk-on-typescript-e77a438eaac5",
        "title": "Anders Hejlsberg Tech Talk on Typescript \u2013 Square Corner Blog \u2013",
        "text": "Anders Hejlsberg, author of Turbo Pascal and lead architect of C# at Microsoft, recently gave a tech talk on Typescript at Square. Typescript is a strict superset of Javascript that adds optional static typing and object-oriented constructs.\n\nBecause Typescript aligns with ECMAScript 6, it offers a partial \u201cpreview\u201d of the future of Javascript development. Best of all, Typescript is open sourced and available on Github under the Apache license.\n\nWe are making Anders\u2019 talk available so everyone can learn more about this elegant, yet practical, language. Enjoy!"
    },
    {
        "url": "https://medium.com/square-corner-blog/human-botnet-part-1-bae75c8ef0e4",
        "title": "Human Botnet Part 1 \u2013 Square Corner Blog \u2013",
        "text": "This is the first in a series of posts from the Square Information Security Team. Over the course of these posts, we\u2019ll detail some of the initiatives we\u2019ve undertaken in order to scale security as the organization grows while still managing our risk effectively.\n\nOver the last few years, Square has grown dramatically both in the number of services we offer as well as the size of the engineering organization. As these numbers grew, it became obvious that manually testing and monitoring every release of every service would be impossible. Scaling security to support our growing engineering team has therefore been a primary goal of the Application Security Team since February. In that time, we\u2019ve focused mainly on two things:\n\nWe can\u2019t effectively manage risk that we don\u2019t know about. Therefore our first goal was to aggregate relevant metrics so we can integrate and contextualize disparate data sets. We collect both hard and soft metrics and pool them by project, allowing us to assess the risk of a project as the sum of its parts. A few examples include:\n\nHaving all of these metrics in one central location gives us insight into how much risk a particular project presents and puts us in a position to detect problems early on. For example, a project with a recent increase in code churn as well as pull requests that were merged without approval (potentially across multiple repositories), would automatically alert us based on the combined increase in these factors. This allows us to effectively prioritize where we spend our time and prevent vulnerabilities in our software before it gets deployed to production.\n\nHowever, in order to minimize our day-to-day involvement, it is important to evangelize ownership of vulnerabilities to the team leads that oversee projects. Our metrics aggregation assists with this by tracking vulnerability remediation SLAs. The SLA is based on the priority of a given issue, which is driven by the vulnerability class and security zone where is it located. The engineering teams can then be evaluated against their remediation time lines across all of their projects. Similarly, the security team uses the aggregated metrics to identify systemic issues across the organization. By classifying issues using a single bug taxonomy, we can see trends across projects and teams that we then use to drive our engineering efforts. This allows us to stop spot fixing vulnerabilities and instead address the class of problem only once for the entire engineering team as a whole.\n\nOnce we prioritized our efforts, we freed ourselves up to build automation. With the number of applications we support growing every day, we needed a way to ensure that all projects \u2014 including those that don\u2019t get a lot of attention from our team \u2014 still undergo some baseline of security testing prior to release. So, we built a dynamic analysis framework that allows us to manage all of our automated security scans from a central location. Using the framework we can easily plug in off the shelf scanners, run customized fuzzers, and quickly write one-off detection scripts.\n\nRunning all of our scans in one place using this framework lets us easily coordinate scheduling and gives us the added benefit of tying the vulnerability reporting mechanism back into our metrics feedback loop. We can automatically file tickets for vulnerabilities with high signal-to-noise ratios and disregard known false-postives. This minimizes the number of potential issues that require triage by our team before escalating them to the project\u2019s owner. Again, this encourages the engineering team to be responsible for the security of their applications. This is obviously not a sufficient means to find certain classes of issues (e.g., business logic vulnerabilities), but does increase our overall coverage and effectively mitigates regression. This combination let\u2019s us focus design review and manual testing efforts such that we don\u2019t have to spend lots of time on low hanging fruit.\n\nScaling security with the engineering organization does not happen naturally, but distributing load to engineers and responsibility to team leads helps tremendously. Automating as much as we can so that we can prioritize from a reasonably sized list of concerns allows us to stop carrying stones and start building wheelbarrows. Additionally, automatically assigning issues to the appropriate teams and escalating issues out of SLA for remediation encourages ownership of product security. Finally, tracking metrics and reporting on how well we are performing helps us focus on priorities by quickly identifying where we should be spending our time.\n\nStay tuned for the next post in this series out next month. We\u2019ll cover how we\u2019re scaling access controls with our single sign-on system, Doorman."
    },
    {
        "url": "https://medium.com/square-corner-blog/learning-ruby-632785a477f0",
        "title": "Learning Ruby \u2013 Square Corner Blog \u2013",
        "text": "When I was an undergrad studying computer science, I thought I\u2019d eventually settle on a single programming language, gain mastery, and use that language for the rest of my career. I couldn\u2019t have been more wrong. It seems like I learn a new language every couple of years. When I joined Square two years ago, I learned Ruby \u2014 and I\u2019m still learning more about it everyday.\n\nRecently, a new employee reached out to our Rubyists mailing list and asked for advice on how to start learning Ruby. After writing my reply, I realized it was general enough advice that it would be useful for any beginning Rubyist. So, we\u2019re sharing it here on The Corner.\n\nAs I see it, there are different aspects to Ruby mastery:\n\nFor #1, I used David Black\u2019s Well Grounded Rubyist. I often find intro books are too vague, but I enjoyed the level of education the book provided. It went into enough detail to satisfy my curiosity, while sometimes telling me, \u201cFor your own good, ignore this until later.\u201d\n\nFor #2, I just had to read/write a bunch of code. I read Well Grounded Rubyist prior to joining Square, so I had a handle on the basic syntax \u2014 but that didn\u2019t get me very far. My first few months were spent learning Ruby conventions, the standard library, and the Rails API. I learned an incredible amount from my colleagues through our frequent code reviews and by reading Ruby\u2019s docs when I had a specific question. The Enumerable Module doc is worth reading in its entirety. I use those methods daily.\n\nFor #3, I highly recommend Sandi Metz\u2019s Practical Object Oriented Ruby. I\u2019m a huge fan of Sandi. In the book and in her talks, she uses examples that are complicated enough to demonstrate the principle in a non-trivial way, yet general enough that I can apply the lesson to my own code. Don\u2019t think that you have to have mastered #1 and #2 before trying this book. In fact, you might try starting with this book, and then use other books to understand why the code is behaving as it does.\n\nIf you\u2019re looking for a quick intro into Sandi\u2019s genius, check out this 40-minute video where she refactors a huge, ugly if statement into small testable objects: All the Little Things. She specifically highlights that initial refactoring efforts can actually increase complexity in the short term and junior developers have trouble seeing the light at the end of the refactoring tunnel.\n\nRails and Rspec use Ruby\u2019s meta-programming features to construct domain-specific languages. I recommend that you do not try to learn those DSLs from top to bottom. You\u2019ll pick them up quickly enough while you\u2019re doing your job. I would however recommend trying to figure out whether a given bit of magic is provided by Ruby, Rails, or Rspec (or some other gem). Then, if you want to use the #present? method in a Sinatra app some day, you\u2019ll know how to import it.\n\nOf course one of the best ways to learn is to ask a friendly coworker!"
    },
    {
        "url": "https://medium.com/square-corner-blog/better-parameterized-tests-with-burst-6f17560013a",
        "title": "Better Parameterized Tests with Burst \u2013 Square Corner Blog \u2013",
        "text": "At Square, we invest a lot of effort in testing to ensure that our software is reliable. Not only do we unit test our business logic, we also use automated UI tests to detect bugs and prevent regressions in our applications. We also use manual testing and phased rollouts as additional safeguards.\n\nSometimes we want to repeat a test several times with different parameters. If we\u2019re testing a web server for example, we might want to repeat a request using different versions of the HTTP protocol.\n\nTest authors often use foreach loops to enumerate test parameters. When such a test fails, the investigation usually starts with determining the parameter or parameters which triggered the failure. In the interest of making tests easier to maintain, it is helpful to have a framework which can immediately show you the parameters with which a test failed.\n\nJUnit 4 ships with a test runner called Parameterized for this purpose. We could write a web server test like this:\n\nParameterized will generate the following hierarchy of tests, making it easy to see where the failure occurred:\n\nParameterized is useful, but declaring each combination of test parameters can be cumbersome. They are declared using object arrays, which are awkward to use. Additionally, there is no way to parameterize a single test method (apart from moving it into a separate class).\n\nFinally, since Parameterized is part of the JUnit 4 framework, it is not available in JUnit 3. While most modern projects have adopted JUnit 4, Android\u2019s test framework still requires JUnit 3.\n\nBurst uses enums to provide test variations in a clean and type-safe manner. If you add one or more enum parameters in a test\u2019s constructor, Burst will automatically generate a test for each combination of parameters; there\u2019s no need to list them out. Here\u2019s an example:\n\nRunning this will produce a hierarchy of tests similar to the one produced by Parameterized:\n\nIn addition to class-level parameters, Burst also supports parameters at the method level. This can be used standalone or in tandem with class parameters. Here\u2019s an example with both class and method parameters:\n\nIf you declare more than one enum parameter in a constructor or method, Burst will generate a test for each unique combination of parameters.\n\nSometimes your test parameter isn\u2019t a single value that can trivially be represented by an enum. But keep in mind that enums are flexible \u2014 you can embed whatever data you need in them. If you\u2019re testing some logic that involves credit cards and want to repeat a test using several cards, you could declare:\n\nAndroid\u2019s test framework is built on the older JUnit 3 API, which has no built-in support for test parameterization. We provide a test runner called BurstAndroid for use in Android tests.\n\nLike our JUnit 4 runner, BurstAndroid lets you add enum parameters to test constructors. Method-level parameters are unfortunately not supported, as Android\u2019s test framework only includes zero-parameter methods when constructing a test suite.\n\nJUnit 4 has a concept of assumptions, which allow a test to indicate that it isn\u2019t applicable to the current test environment and should be skipped. JUnit 3 lacks this feature, but our BurstAndroid runner provides similar functionality. By overriding isClassApplicable or isMethodApplicable, you can filter out tests based on the current environment.\n\nOur apps have certain features which only exist on phones or tablets, so we annotate associated tests with @PhoneOnly or @TabletOnly. BurstAndroid lets us skip these tests based on the device being tested.\n\nBurst is available in Maven Central; see our GitHub project for details. Please give the library a try and if you find any issues, let us know in the comments or on GitHub.\n\nFor more details on how we approach UI testing at Square, check out Dimitris\u2019 Droidcon slides."
    },
    {
        "url": "https://medium.com/square-corner-blog/welcome-to-square-lets-learn-d3-df268f5616aa",
        "title": "Welcome to Square! Let\u2019s Learn D3! \u2013 Square Corner Blog \u2013",
        "text": "Recently, I started helping out with Square\u2019s new engineer onboarding program, which we call NEO. NEO covers an engineer\u2019s first 3 weeks at Square. We give these new engineers, or NEObies, a hands-on introduction to the code behind many of Square\u2019s products; they work on tickets, and we offer a series of talks on the various technologies that we use.\n\nWhile our resident visualization expert Tom Carden was out of office, I filled in and taught our introduction to D3.js class to our NEObies.\n\nD3.js is a great tool. (We\u2019ve posted about D3 on the Corner before; we use for merchant-facing analytics, and it powers part of Cube which we use internally on occasion.) However, D3 can have a steep learning curve, it took me a little while to really get it. So before teaching this class, I decided to write down my thoughts. The notes ended up taking a tutorial-like format with examples, so I ended up turning those notes into an actual tutorial.\n\nWe published the tutorial internally and figured people outside of Square might find it useful \u2014 I like to imagine it\u2019s like dropping in on a NEO class at Square. Please check it out our Intro to D3.js!\n\nWe\u2019re happy to accept contributions. Please check out the source in our square/intro-to-d3 GitHub project and feel free to send us pull requests!"
    },
    {
        "url": "https://medium.com/square-corner-blog/our-fourth-college-code-camp-7c68dcfef288",
        "title": "Our Fourth College Code Camp \u2013 Square Corner Blog \u2013",
        "text": "In 12 hours, women engineering students programmed a web page that will help AIDS organization (RED) spread awareness of its mission. After one conversation with Vivienne Harr, the 10-year-old founder of Make a Stand, women engineering students were inspired to program the newly-launched Make a Stand app for global crowdfunding. Both accomplishments were outcomes of College Code Camp, our five-day, in-house immersion program that uses leadership sessions, coding workshops, and a hackathon, to bring together women engineering students and build a stronger community of women in technology.\n\nAfter every Code Camp, we are always inspired by the confidence the young women exude and the projects they created during their time at Square, which show the endless possibilities of creating through code. We\u2019ve fostered a community of nearly 70 young women who have participated in Code Camp, spanning more than 30 universities in the U.S. and Canada, and six San Francisco high schools.\n\nCode Camp not only inspires, educates, and empowers the next generation of women in technology, but it also creates a strong, supportive community. Our previous Code Camps have been incredible:\n\nToday, we once again invite college students to apply to our five-day College Code Camp at square.com/code-camp. The program will take place January 6\u201310 in San Francisco and will include leadership sessions, coding workshops, and a hackathon to build a stronger community of women in technology.\n\nWe look forward to meeting the most intelligent, inspiring, dynamic women, and growing our Code Camp community to nearly 100 young women."
    },
    {
        "url": "https://medium.com/square-corner-blog/advocating-against-android-fragments-81fd0b462c97",
        "title": "Advocating Against Android Fragments \u2013 Square Corner Blog \u2013",
        "text": "Recently I gave a tech talk (fr) at Droidcon Paris, where I explained the problems Square had with Android fragments and how others could avoid using any fragments.\n\nIn 2011, we decided to use fragments for the following reasons:\n\nSince 2011, we\u2019ve discovered better options for Square.\n\nOn Android, Context is a god object, and Activity is a context with extra lifecycle. A god with lifecycle? Kind of ironic. Fragments aren\u2019t gods, but they make up for it by having extremely complex lifecycle.\n\nSteve Pomeroy made a diagram of the complete lifecycle, and it\u2019s not pretty:\n\nThe lifecycle makes it difficult to figure out what you should do with each callback. Are they called synchronously or in a post? In what order?\n\nWhen a bug occurs in your app, you take your debugger and execute the code step by step to understand what is happening exactly. It usually works great\u2026 until you hit FragmentManagerImpl: Landmine!\n\nThis code is hard to follow and debug, which makes it hard to correctly fix bugs in your app.\n\nIf you ever found yourself with a stale unattached fragment recreated on rotation, you know what I\u2019m talking about. (And don\u2019t get me started on nested fragments.)\n\nAs Coding Horror puts it, I am now required by law to link to this cartoon.\n\nAfter years of in-depth analysis, I came to the conclusion that WTFs/min = 2^fragment count.\n\nBecause fragments create, bind, and configure views, they contain a lot of view-related code. This effectively means that business logic isn\u2019t decoupled from view code \u2014 making it hard to write unit tests against fragments.\n\nFragment transactions allow you to perform a set of fragment operations. Unfortunately, committing a transaction is async and posted at the end of the main thread handler queue. This can leave your app in an unknown state when receiving multiple click events or during configuration changes.\n\nFragment instances can be created by you or by the fragment manager. This code seems fairly reasonable:\n\nHowever, when restoring the activity instance state, the fragment manager may try to recreate an instance of that fragment class using reflection. Since it\u2019s an anonymous class, it has a hidden constructor argument to reference the outer class.\n\nDespite their drawbacks, fragments taught us invaluable lessons which we can now reapply when writing apps:\n\nLet\u2019s look at the fragment basic example, a list / detail UI.\n\nNow this is interesting: ListFragmentActivity has to handle whether the detail is on the same screen or not.\n\nLet\u2019s reimplement a similar version of that code using only views.\n\nFirst, we\u2019ll have the notion of a Container, which can show an item and also handle back presses.\n\nThe activity assumes there\u2019s always a container and merely delegates the work to it.\n\nThe list is also quite trivial.\n\nNow, the meat of the work: loading different XML layouts based on resource qualifiers.\n\nHere is a very simple implementation for those containers:\n\nIt\u2019s not hard to imagine abstracting these containers and building an app this way \u2014 not only do we not need fragments, but we also have code that is easier to understand.\n\nUsing custom views works great, but we\u2019d like to isolate business logic into dedicated controllers. We call those presenters. This makes the code much more readable and facilitates testing. MyDetailView from the previous example could look something like that:\n\nLet\u2019s look at code from Square Register, the screen for editing discounts.\n\nThe presenter manipulates the view at a high level:\n\nWriting tests for this presenter is a breeze:\n\nManaging a backstack does not require async transactions. We released a tiny library that does just that: Flow. Ray Ryan already wrote a great blog post about Flow.\n\nMake your fragments shells of themselves. Pull view code up into custom view classes, and push business logic down into a presenter that knows how to interact with the custom views. Then, your fragment is nearly empty, just inflating custom views that connect themselves with presenters:\n\nAt that point you can eliminate the fragment.\n\nMigrating away from fragments wasn\u2019t easy, but we went through it \u2014 thanks to the awesome work of Dimitris Koutsogiorgas and Ray Ryan.\n\nDagger & Mortar are orthogonal to fragments; they can be used with or without fragments.\n\nDagger helps you modularize your app into a graph of decoupled components. It takes care of all the wiring and therefore makes it easy to extract dependencies and write single concern objects.\n\nMortar works on top of Dagger and has two main advantages:\n\nWe used fragments intensively and eventually changed our minds:"
    },
    {
        "url": "https://medium.com/square-corner-blog/a-pair-of-android-tech-talks-f53072cd8003",
        "title": "A Pair of Android Tech Talks \u2013 Square Corner Blog \u2013",
        "text": "On Tuesday, Jesse Wilson and I gave Android-themed tech talks in our new Kitchener-Waterloo office. (Thanks to everyone who attended!) We know not everyone can make a trip to Canada on a Tuesday, so we are making the talks available online. Enjoy!"
    },
    {
        "url": "https://medium.com/square-corner-blog/logging-can-be-tricky-9575d190c045",
        "title": "Logging can be tricky \u2013 Square Corner Blog \u2013",
        "text": "At Square, we have a mailing list that we use to discuss extremely technical issues, usually requiring in-depth knowledge of underlying system architecture. We dedicated a group to discuss internals with a focus on learning precisely how and why systems operate. When answering questions about systems, the group must also share how they arrived at the answer. We meet weekly to share techniques and raise interesting problems. This post is an example of one such issue raised in our weekly get-together.\n\nA Go application showed around 200ms latency in the 99th percentile but only on one of three machines in an otherwise uniform cluster. No significant amount of resources appeared to be in use \u2014 disk I/O, network I/O, CPU, and memory all looked fairly tame. Furthermore, the bulk of queries being served were all performing as expected. Here\u2019s how we found the problem. First, we started with a graph:\n\nWe already looked at the usual suspects: top, vmstat 1, iostat -x 1, lsof, and all the various metrics we graph. Everything looked mostly similar between the systems \u2014 same load, same descriptors, same memory use patterns.\n\nSo what was up with that problematic blue line only on one host? After verifying it was safe to strace, we poked around a bit in the process, comparing the problematic host with the non-problematic machine from elsewhere in the cluster.\n\nStrace -c showed us a summary table of time spent in each system call. Notable: We were spending more than twice as long in fsync() on our problem host. Why was that? Let\u2019s look at fsync in isolation. We\u2019ll trace only the fsync syscall and capture the time spent in each call:\n\nWow, smoking gun! Time spent in fsync accounts for almost exactly the delay we were seeing. We thought this was likely our problem.\n\nWe then had to figure out why fsync was only slow on one of these systems. Here\u2019s the catch: fsync() latency is VERY sensitive to writes elsewhere on the filesystem. It is a synchronization point for activity. Neither system has a particularly huge disk I/O workload, but the problematic system\u2019s disk workload is slightly higher (around 20% util) than the non-problematic system (around 5% util). This is enough churn to cause large and occasional latencies when fsync() must block on actually putting bytes on the drive.\n\nWell, our next question: Why is the application calling fsync()? We saw it was syncing descriptor 3, so we had a look:\n\nIt\u2019s a logfile. From the filename we could tell it was the glog framework, which must be calling fsync(). The glog framework buffers logs and serializes access to the buffer. The lock is held while logs are flushed, which includes holding the lock during the time spent in fsync(). We had a simple fix: Patch glog and remove fsync() entirely. This allowed logs to hang around in the page cache \u2014 and while this didn\u2019t ensure the write() itself won\u2019t block it, the solution does make it much less likely to occur.\n\nMaking small blocking writes to the disk in the critical path of serving logic \u2014 while assuming they are non-blocking \u2014 is a fairly common pattern, (and a fairly common source of occasional, hard-to-locate errors in the nth percentile). It\u2019s always surprising when I/O slows under a surprisingly small amount of otherwise unrelated disk activity."
    },
    {
        "url": "https://medium.com/square-corner-blog/trying-on-pants-fb82dbdbae9b",
        "title": "Trying on Pants \u2013 Square Corner Blog \u2013",
        "text": "For the last few months, we\u2019ve been working to migrate some of our build processes to the up-and-coming Pants build tool. We are working to replace some of our uses of Apache Maven.\n\nPants is compelling to us for a few reasons. First of all, the overall design was inspired by Google\u2019s internal build system, which we know scales well to large repos containing many projects. Beyond that, its feature set meshes with our needs, including:\n\nBut the overwhelming selling point is that Pants has a very open development community and has multiple companies actively contributing.\n\nPants is still a somewhat nascent project and didn\u2019t work out of the box for all the the features we are using in our repo. Currently, Pants is configured to run side-by-side with Maven in our repo. In the future, we plan to:\n\nRight now we\u2019re still experimenting with Pants, starting to use it with some projects internally, and we are actively contributing back. We are hoping that other organizations with similar needs will also take a look at Pants. Learn more at http://pantsbuild.github.io/."
    },
    {
        "url": "https://medium.com/square-corner-blog/todo-talk-openly-develop-openly-2582a1ee054f",
        "title": "//TODO: Talk Openly, Develop Openly \u2013 Square Corner Blog \u2013",
        "text": "On behalf of Square, I am proud to announce that we are one of the founding companies of //TODO: Talk Openly, Develop Openly, a group dedicated to utilizing and releasing open source software. We all believe we can better improve our open source programs \u2014 and our contributions to the open source movement as a whole \u2014 by working together.\n\nAt Square, we are committed to contributing to the open source community. We always have the mindset to open source our code when we build; and over the last four years, we have written countless blog posts and given multiple presentations on open source and engineering, and released many projects that we developed internally.\n\nWe are proud of our engineers and work to simplify the process of releasing and managing open source software. And now, we are proud to stand with other companies who strive to do the same.\n\nThe TODO group will share experiences, develop best practices, and collaborate on common tooling regarding open sourcing software. Companies with similar programs, or companies that are looking to begin open sourcing software, are also invited to join at todogroup.org.\n\nTogether, we will work to improve the experience of open sourcing so we can focus on what\u2019s most important \u2014 the quality of the projects themselves."
    },
    {
        "url": "https://medium.com/square-corner-blog/sqrome-a-chrome-extension-e2e90c1c5d2",
        "title": "Sqrome, a Chrome extension \u2013 Square Corner Blog \u2013",
        "text": "When I joined Square a few months ago, I often ran into email threads where I had no idea who anybody was \u2014 thanks to our impressive growth rate and my not-so-impressive memory. Every quarter, Square sets aside a week (we call it hack week), so that everyone has the opportunity to reach outside of their day-to-day projects, and build something that speaks to them personally. I decided this was a perfect time to solve my email problem, and in the process learn how to make my first Chrome extension.\n\nWe use Google Apps for our email. Gmail works great for us. Unfortunately, the Google+ widget shown on the right side of the email doesn\u2019t provide work-related information on my colleagues, like the team they work on and desk location. Sqrome, or Square Chrome extension, was born during our hack week. It is a quick reference to relevant information on my colleagues. Sqrome shows a simple widget with the person\u2019s name, photo, title, team, desk location, Twitter handle, and bio for every person included on the email thread. In other words, it is our internal Rapportiveextension.\n\nWhile Sqrome talks to our internal directory to get its information, it can be easily adapted to work with any company\u2019s directory.\n\nIt turns out that it\u2019s pretty easy to develop a Chrome extension. Every extension is just a zipped bundle of files: images, HTML, CSS, and Javascript. I\u2019ll go briefly into how to build Sqrome.\n\nmanifest.json\n\nThis file defines the name, version, and permissions of the extension.\n\nThe extension has a background script that reads from the company\u2019s internal directory and caches to local storage. This allows the extension to work even when you\u2019re outside the company VPN, or using Gmail offline.\n\nThe extension also has a content script, which checks whether the page is an opened email. To detect whether the current tab is an opened email, we just have to poll for changes to document.location.href, and then wait for the appropriate email DOM elements to appear. We then query an internal endpoint to get more up-to-date info for each participant on the email thread.\n\nInside addCustomSidebarElement method, you can now insert anything you like (e.g. a profile widget).\n\nYou can also add some CSS animation to celebrate your colleagues\u2019 work anniversaries.\n\nThe last step is to publish the extension. Google Web Store lets you publish Chrome extensions internally, so you won\u2019t have to host the crx file manually.\n\nAnd there you have it! Now you can recognize all of your colleagues, right from your inbox."
    },
    {
        "url": "https://medium.com/square-corner-blog/college-code-camp-iii-85b2b8844340",
        "title": "College Code Camp III \u2013 Square Corner Blog \u2013",
        "text": "Two years ago, we launched our first College Code Camp, a four-day immersion program for women studying computer science. We held our second College Code Camp in January. Inspired by the program, a Square engineer had the idea for High School Code Camp, an eight-month program designed to teach young women programming and prepare them for the AP Computer Science exam.\n\nWe\u2019ve created a community of nearly 50 young women who have participated in Code Camp \u2014 spanning more than 20 universities in the U.S. and Canada, and six San Francisco high schools. We\u2019ve been so inspired by what we\u2019ve seen these young women accomplish that we decided to host our second College Code Camp of the year.\n\nThe next Code Camp will take place August 13\u201316 in San Francisco. Students will attend coding workshops (developed and taught by Square engineers), mentorship sessions, and tour Silicon Valley. They will also meet with Square\u2019s inspiring women executives, including CFO Sarah Friar, Business Lead Fran\u00e7oise Brougher, and Engineering Lead Alyssa Henry.\n\nCollege students in the U.S. and Canada can apply to the program at squareup.com/code-camp. Twenty winners will be selected to participate.\n\nOnly 20% of software developers are women. We want to make a change and empower a new generation of entrepreneurs."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-summer-of-kochiku-5df812cea04a",
        "title": "A Summer of Kochiku \u2013 Square Corner Blog \u2013",
        "text": "Kochiku is an open-source tool which automatically partitions test suites into distributable chunks, or shards. Most of Square\u2019s projects use Kochiku to quickly get test results by sharding the work between a few hundred Kochiku-workers running in Amazon EC2. Since the number of developers pushing code at Square is rapidly increasing, a team was formed to focus on the improvement and administration of Kochiku. The team has already made a number of quality-of-life changes for projects on Kochiku and written entire features that make the tool even more useful. We decided to open source Kochiku about a year ago, so our team works to keep the public version of Kochiku up-to-date with Square\u2019s internal version \u2014 often building out feature branches on GitHub and pulling them into the Square version later. This means the work we\u2019re putting into Kochiku benefits everyone.\n\nHere\u2019s a quick rundown of the growth of Kochiku, roughly in the order that the new features were developed:\n\nKochiku\u2019s ability to run tests is entirely language agnostic, as it simply invokes a given shell script, but the real strength of the tool is in its partitioning. By using the often overlooked time_manifest feature, Kochiku can split files between workers for optimal test completion time. However while this is awesome for ruby apps, it isn\u2019t the best way to think about all projects.\n\nMuch of Square\u2019s Java code base lives in a single, monolithic repo, separated into many maven modules; a practice made popular by companies like Google and Facebook that allow changes across many parts of the company\u2019s infrastructure to be pulled in atomically. Running tests for commits in such a large repo is very time consuming if not done intelligently, and the Maven Partitioner was built as an extension to Kochiku to shard our monolithic Java repo.\n\nMaven projects are organized according to a fairly strict convention. By using knowledge of these conventions and maven\u2019s dependency annotations, the Maven Partitioner builds and tests only modules that are impacted by the changes in a given commit. This cuts the number of modules to be tested from well over a hundred to about a dozen on average, with no loss of fault detection despite skipping entire test suites.\n\nThe changes to the partitioning logic \u2014 which made way for the Maven Partitioner \u2014 also allow for additional partitioners to be built with knowledge of other languages, build tools, and frameworks. This allows Kochiku to grow in both strength and simplicity as a tool for building varieties of projects. As Square continues to support Kochiku in the future, we\u2019d like to add intelligence for using other technologies, such as the Go language or Twitter\u2019s Pants Build Tool."
    },
    {
        "url": "https://medium.com/square-corner-blog/even-more-advanced-interactive-menus-6502066e90e1",
        "title": "Even More Advanced Interactive Menus \u2013 Square Corner Blog \u2013",
        "text": "Three weeks ago, we introduced Menu Embeds to our merchants. Since then, thousands of merchants have embedded their store fronts (aka. \u201cmenus\u201d) on their sites to drive sales. We received a lot of great feedback and heard a common theme: make the embed experience even more streamlined and efficient. Today, we are launching the Menu Embed v2, which adds Shopping Basket support.\n\nIn the initial release, when a customer clicked on an item in a seller\u2019s menu, they would be taken to the item\u2019s page on Square Market. This flow nudged customers away from seller\u2019s sites and also made buying multiple items unnecessarily tedious. We\u2019ve greatly refined the experience with today\u2019s release. When a customer clicks on an item, a responsive item modal displays so they can customize their order right on the seller\u2019s site.\n\nWe\u2019ve included a basket tab on the right side of the site too. Clicking on it slides out a shopping cart so customers can easily edit and review their purchase before checking out.\n\nWhen they\u2019re ready, customers can simply checkout through a white label flow in Market.\n\nWhen we set out to create embeddable menus, we came across technical challenges that gated design flexibility. As an example, some merchants have tiny menus, which limits the space we have to work with. This presents a challenge: how do we show ancillary information about each item? Imagine a sandwich shop that only sells a single item \u2014 let\u2019s call it the \u201cSandwich\u201d \u2014 but it comes in many different sizes (small, medium, large) and has several different modifiers (add mayonnaise, remove pickles, extra cheese). Our designers experimented with a number of approaches, but providing a great experience for all variations of menu content proved challenging. Eventually, we agreed that opening a modal with the extra information was the right approach.\n\nYou may be wondering, \u201copening a modal isn\u2019t hard, so why didn\u2019t you just do that from the start?\u201d Solving this issue in a robust way was not as trivial as we initially thought it was going to be! The embed iframe occupies a small area that is displayed inline with the rest of the seller\u2019s site. Opening up a modal requires gaining access to the entire viewport of the browser, not just the portion inside the iframe. If we just change the size of the iframe, not only would the viewer\u2019s experience be jarring, but the integrity of the seller\u2019s site design could be compromised. We considered opening up two iframes to make the experience feel more seamless, but this solution had several major pitfalls.\n\nUsing multiple iframes is a viable solution in many cases as long as the embed isn\u2019t too complex. Once a certain threshold is crossed things get complicated quickly. Compounding the issue, using multiple iframes can require initializing full-fledged web applications multiple times. For our use case, not only would this approach greatly increase the load time for both server and browser, it would also become increasingly problematic to synchronize data across the iframes. Keeping data consistent across data storage is a very difficult computer science problem even though the scale could be kept relatively smaller on the client-side.\n\nSquare strives for elegance in both engineering and design implementation. Since using multiple iframes wouldn\u2019t provide us with an optimal experience, we came up with a solution that would: iframe explosion. Taking a script we\u2019ve embedded on the page, we fixed-position the iframe so that it fills up the viewport, then hide its contents. In doing so, we can show a modal that feels tightly integrated into the site on which it\u2019s embedded.\n\nStill, this presented a new challenge: if we position the iframe outside the document flow, the page\u2019s contents will collapse, destroying the illusion. Even if we could hold the iframe\u2019s position, we had yet to find a way to keep the menu in place and responsive so it would hold up to interaction convincingly. How we did this is detailed below and fairly straightforward, but it took countless hours of rendering and performance optimization to make it feel right.\n\nWhen a buyer clicks on an item in the menu, a script on the page creates a temporary placeholder with the same dimensions as the iframe. The iframe invisibly expands to the size of the viewport and opens a modal. The placeholder\u2019s styles are then passed back to the iframe and applied to the original menu, so it looks as if the menu is still fixed in place underneath the modal. Since the exploded iframe is positioned on top of the site, we can intercept the click when the buyer wants to close the modal. The result is a beautiful solution that preserves our standards for both engineering and design.\n\nIf you want to test-drive this experience with your own menu, get started by logging into your dashboard.\n\nWe\u2019ll be continuing to build a world-class embedded experience for your business. If you have any feedback on how we can improve and fine-tune menu embeds, please let us know!"
    },
    {
        "url": "https://medium.com/square-corner-blog/squares-security-bug-bounty-f00d7c5b3e8e",
        "title": "Square\u2019s Security Bug Bounty \u2013 Square Corner Blog \u2013",
        "text": "With so many sellers relying on Square to run and grow their business, we\u2019ve made protecting them a priority. We monitor every transaction from swipe to payment, innovate in fraud prevention, and adhere to industry-leading standards to manage our network and secure our web and client applications. We protect our sellers like our own business depends on it \u2014 because it does.\n\nToday, we\u2019re very excited to announce our security bug bounty with HackerOne. We recognize the important contributions the security research community can make when it comes to finding bugs, and we\u2019re asking for your help. If you\u2019re interested in getting paid to find security bugs, head to our HackerOne page to read about our program."
    },
    {
        "url": "https://medium.com/square-corner-blog/advanced-embedding-with-square-market-42881b77ef4a",
        "title": "Advanced Embedding with Square Market \u2013 Square Corner Blog \u2013",
        "text": "We built Square Market to give businesses a way to participate in and navigate the world of online commerce. Particularly those businesses traditionally not part of online commerce, like caf\u00e9s, restaurants, and bars.\n\nFirst, we built a way for these types of businesses to take more orders with a comprehensive self-service offering so they could sell food and beverages online for pickup.\n\nNext, we took a look at the menus of these businesses. While many of our favorite restaurants, caf\u00e9s, and bars have beautifully designed sites with detailed information about the venue and location, almost none of them had a menu listed. And if they did, it was often a clunky PDF. We knew we could give them a way to share their menu with the same quality and care as the rest of their sites.\n\nThe concept of \u201cembedded commerce\u201d is the idea of an atomic unit of commerce embedded into a website that transforms that site into a place of commerce (A similar idea to a YouTube video widget or Facebook social widget). On Square Market we started small with item embeds with the plan to build embedded shopping carts, checkout flows, and even menus next.\n\nLooking at these static PDF menus, it became clear it was time to get to work on embedded menus. The first step was to determine how to do it. Should we use Javascript to inject HTML into the host page, create an interactive menu web component, or should we go with the tried and true approach of embedding the content using an iFrame?\n\nInjecting straight HTML was dangerous: who knows what world your HTML is walking into. With HTML, we would lose control of the styling and interactivity of the menu \u2014 it\u2019s just too easy to override the styles and event handlers. While there were ways to scope and control the styling, it wasn\u2019t reliable enough across browsers.\n\nWeb components are incredibly promising. With a web component, we can define a new HTML element with custom markup and well-defined interaction and lifecycle event behavior. The Shadow DOM allows us to apply our own styles and event handlers independent of the host page. Unfortunately, web components are still a bit too immature. Browser support is pretty abysmal; Chrome and Opera have decent support, Firefox is clearly trying, but Safari and IE are still terribly behind.\n\nIn the end, we decided on an iFrame, an approach we knew would work. It\u2019s supported in every browser, provides a good isolated sandbox for styles and scripts, and is easy to update after it\u2019s been deployed on a host site.\n\nOnce we decided on an iFrame, we spun up a simple NodeJS app that served an Ember app to render the interactive menu. We chose Node to keep the entire stack in Javascript and to take advantage of the broccoli asset pipeline. We\u2019re planning to extend the embedding functionality significantly and chose Ember because we\u2019re really comfortable with Ember at Square. Ember is fantastic for removing boilerplate and enforcing consistent code style, especially with large complex frontends. We extended our existing embed script to handle the new menu embed type and we now have a product we\u2019re excited to share.\n\nWe recognize it is essential to allow businesses to customize their menu to fit their existing web site. So, we\u2019ve built a simple, extensible API for customizing the menu contents.\n\nThe basic embed code looks like this:\n\nWith a few data attributes anyone can customize the contents.\n\nIt\u2019s easy to show item images and descriptions in a column layout with an orange accent and no surrounding border, for example, by doing this:\n\nWe\u2019ve built a nice little UI as a starting place for building up these embeds.\n\nWe\u2019re very excited to see how our sellers use our new menu embeds. We\u2019re also planning to continue building out the embedded shopping experience beyond the basic menu, so please, stay tuned!"
    },
    {
        "url": "https://medium.com/square-corner-blog/okhttp-2-0-6da3fe12c879",
        "title": "OkHttp 2.0 \u2013 Square Corner Blog \u2013",
        "text": "OkHttp started as a fork of Android\u2019s HttpURLConnection. That gave us a solid foundation to build upon. Working behind a widely-used API meant that upgrading from AOSP\u2019s HttpURLConnection to ours was easy: just configure URLs stream handler factory.\n\nBut we\u2019ve outgrown the HttpURLConnection API. Its fully-synchronous API means that application code needs to manage how requests are dispatched. Requests and responses are together in one class which makes certain operations awkward or inefficient. Most painfully, we\u2019re limited in what we can expose: Was my request redirected? Did the response come from a cache? OkHttp knows the answers to these questions but the HttpURLConnection API offers no way for you to ask them.\n\nWith 2.0, OkHttp introduces three fundamental types: Request, Response, and Call. A call executes a request to produce a response. You can do that synchronously with a blocking call:\n\nAbove we complained that HttpURLConnection lacks APIs to ask how a response was reached. With redirects and caching, that journey can be interesting! One handy example is what happens when OkHttp downloads its own jar file from Maven Central.\n\nFirst we\u2019re redirected from the HTTP to the HTTPS site:\n\nNext we\u2019re redirected from the LATEST tag to the specific file we want.\n\nFinally we download the file. The response includes headers to make caching possible.\n\nOkHttp exposes these redirects with Response.priorResponse(). If you\u2019re redirected multiple times, you will have a chain of prior responses. These will be returned in order from last-to-first; reverse them for chronological order:\n\nIf we re-download the same URL, OkHttp\u2019s response cache may kick in. Responseoffers two methods to check where a response came from: cacheResponse() and networkResponse(). Note that for conditional gets, both will be present!\n\nWhen downloading a jar file from Maven Central, Response.cacheResponse()returns the cached response from above:\n\nThis cached response required validation from the server. OkHttp needed to ask, \u201cHas the cached value changed?\u201d Checking Response.networkResponse() confirms that it had not:\n\nPrior responses from redirects also support cacheResponse() and networkResponse(). Use this to interrogate whether any part of a response used the network.\n\nOkHttp 2.0 is not backwards-compatible. The changelog describes what\u2019s changed and what\u2019s gone. For example, to continue using the HttpURLConnection API, you will need the optional okhttp-urlconnection dependency. To make upgrading easier, we\u2019ve also released OkHttp 1.6 which is the 1.5 code plus some new 2.0 APIs. Use 1.6 to transition to 2.x APIs.\n\nGet OkHttp from Maven or download it from the project website. Read more code examples in the project\u2019s Recipes doc."
    },
    {
        "url": "https://medium.com/square-corner-blog/generating-thumbor-urls-on-ios-28c6e789275a",
        "title": "Generating Thumbor URLs on iOS \u2013 Square Corner Blog \u2013",
        "text": "A little while back, we talked about dynamic images with Thumbor.\n\nTo go with our Pollexor library on Android, today we\u2019ve open-sourced ThumborURL to do the same on iOS.\n\nIt\u2019s really slick and very easy to use. In five lines of code, you can request a resized version of an image:\n\nTry it out! Let us know what you think."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-and-girls-who-code-da1f1919e58d",
        "title": "Square and Girls Who Code \u2013 Square Corner Blog \u2013",
        "text": "From June to August, we are hosting 20 local high school students through Girls Who Code\u2019s Summer Immersion Program. Girls Who Code is a national non-profit organization that aims to inspire, educate, and equip young women for futures in the computing-related fields.\n\nWe are thrilled to partner with Girls Who Code, as we share the same vision of closing the gender gap in technology. Over the past two years, nearly 50 young women have participated in our own initiative, Code Camp. Code Camp uses leadership sessions and coding workshops to bring together top women engineering students and build a strong community around women in technology.\n\nOur third College Code Camp begins on August 13, and by also participating in the Summer Immersion Program, we can empower more women to pursue computer science. Throughout the 7 week program, 20 local high school students will come to Square for 250+ hours of instruction in multiple areas of engineering. The curriculum pairs intensive instruction in robotics, web design, and mobile development, with mentorship from developers and entrepreneurs. The young women will also meet with our Engineering Lead Alyssa Henry, who will speak with the students about her engineering journey.\n\nWomen make up less than 25% of the computer workforce. After participating in Girls Who Code, 99% of the young women consider pursuing a career in tech. By taking action, we can improve the ratio of women in technical positions.\n\nRead more on Girls Who Code\u2019s Summer Immersion Program: girlswhocode.com/2014launch"
    },
    {
        "url": "https://medium.com/square-corner-blog/ios-lightning-talk-open-source-tour-8aec759875ff",
        "title": "iOS Lightning Talk: Open Source Tour \u2013 Square Corner Blog \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/square-corner-blog/ios-lightning-talk-register-architecture-5eaddeb95f0d",
        "title": "iOS Lightning Talk: Register Architecture \u2013 Square Corner Blog \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/square-corner-blog/one-year-later-822c03becf2",
        "title": "One Year Later \u2013 Square Corner Blog \u2013",
        "text": "In the year since I joined, many aspects of my life at Square have changed. Here are a few of my observations.\n\nThe most obvious change is our scale. When I started last April, Square was roughly 450 people. Now we are more than 800; and with our growth, previous structures and systems reached their limits. Physically, the number of our offices grew dramatically \u2014 now with a footprint in 6 cities globally. And in San Francisco, we moved to a new office with more than 150,000 sq ft and our own restaurant. Abstractly, many teams reorganized to accommodate new people and keep teams small. Our goal is to have no more than 8 people per team, which helps us move quickly and ensures everyone has a major impact.\n\nScaling our software to accommodate massive amounts of traffic, data, and engineers has precipitated change as well. From a development perspective, we had around 50 services when I started. We now have well over 200! Similar to the Unix philosophy, each service should do only one thing and do it well. From a development perspective, this means I can ignore a vast majority of the services and code at Square to narrow in on the task at hand \u2014 saving time and bugs.\n\nBefore Square, I worked at a startup with fewer than 10 people. There was little sense of code ownership, as much of the engineering was outsourced. Coming to Square was a breath of fresh air. My first few months were a whirlwind of learning new languages, frameworks, and patterns: Ruby, Rails, Ember + coffeescript, and proper TDD. All of these enabled me to help update Square Dashboard for our launch in Japan.\n\nWith each new product comes a new set of skills to master. Recently, I\u2019ve been learning email templating, distributed systems, protos, multithreading, and some Java. These skills were necessary to build a service dedicated to sending digital receipts and launch Square Feedback. I love that not only do I get to ship exciting new products all the time, but that in order to do so, I am constantly stepping up my game and learning new skills.\n\nThere are some things that shouldn\u2019t change, though. At Square, company culture has remained constant. We continue to ask tough questions to the team during our Friday all-hands (Town Square); we continue to spawn amazing products during Hack Week; and we continue to share notes from every meeting (even board meetings), with everyone to ensure transparency and ownership.\n\nIn one word, the culture at Square is thoughtful. I know people give each decision careful consideration. Every decision feels thoughtful chosen \u2014 from the furniture in our office, to company-wide priorities. That doesn\u2019t mean I always agree or we get it right the first time, but it does mean I have enormous confidence in my co-workers to make decisions.\n\nChange is a good thing. In the year since I started, Square substantially matured, onboarding thousands of new sellers, shipping tons of new products, and launching in a new country. But what keeps me coming back into work every day is not what changes, it\u2019s what stays the same. It\u2019s the thoughtfulness of (now) more than 800 people all working toward the same goal. I can\u2019t wait to see what the next year holds!"
    },
    {
        "url": "https://medium.com/square-corner-blog/hello-picasso-2-3-a13e23348bef",
        "title": "Hello Picasso 2.3 \u2013 Square Corner Blog \u2013",
        "text": "Today we are publishing a new version of Picasso, our solution for image downloading and caching on Android. One of the highlights of this release is what we refer to as \u201crequest replaying.\u201d Before we showcase it, let\u2019s dig into how Picasso is architected and why that helps us provide cool new features.\n\nAnyone involved with Android development \u2014 and for that matter, any app developer \u2014 knows not to mess with the main thread. In the first version of Picasso (which was accomplished during a hackweek at Square!) things were simple. The main thread would make a new request for an image and would also manage canceling requests. But as we went on and imagined what features we really wanted to bring into Picasso, we quickly saw that this simple pipeline wouldn\u2019t allow us to achieve our goals. We wanted a a foundation to build upon, while ensuring the main thread does as little as possible.\n\nWe literally went back to the drawing board, and with Picasso 2.0 we introduced a new layer in the pipeline: The Dispatcher. The Dispatcher is a dedicated thread responsible for coordinating all incoming requests from the main thread. There was a shift of responsibilities from the main thread to the dispatcher. A lot of the work was offloaded and the two things the main thread now cares for are \u201cI want this image\u201d or \u201cI dont want this image.\u201d High maintenance indeed.\n\nThe dispatcher now has all the burden of fetching images, often by delegating to one of its \u201chunters\u201d (because \u201cworkers\u201d is so 2000late). The intention here was for the dispatcher to grow into something bigger, rather than being just a pass-through stage in the pipeline. For example, one of the neat tricks of dispatcher is the ability to batch completed requests and send them off to the main thread \u2014 effectively reducing the amount of messages the handler queue has to process.\n\nApps that heavily depend on images to display useful information to the user, such as a static maps image showing a location, would often require the user to go back and reload the page when their network was available to load the image.\n\nThis user experience is subpar. To improve this, the dispatcher now has the ability to keep track of failed requests. When the network is back up, the dispatcher will automatically replay requests that previously failed.\n\nYou don\u2019t need to do anything to enable this feature; it\u2019s the responsibility of the dispatcher to get the image for you, while keeping the main thread work at a bare minimum.\n\nToday we are also slightly expanding our fluent API to support widgets and notifications (RemoteViews). As you\u2019d expect, invoking Picasso is very simple:\n\nRequest replaying is supported everywhere, including notification and widget requests.\n\nWe\u2019re also bringing logging into Picasso. Going through each stage of the pipeline can be rather difficult to follow, especially when dealing with adapters that create/cancel requests as fast as the user can scroll.\n\nWe came up with a simple yet effective scheme to log every stage, as well as an easy way to track a single request. Additional metrics are also presented so you can further optimize \u2014 such as the time needed to transform a bitmap. Finally, each request has an ID associated with it. You can easily track a request just by grepping the output on the ID of the request.\n\nThe scheme is as follows:\n\nDo NOT enable logging automatically (even for development builds). Displaying such detailed information will slow down the overall performance your app \u2014 something you wouldn\u2019t even want your alpha or beta users to go through.\n\nA lot of cool tricks and features are waiting to be added into Picasso. We hear you loud and clear. We tailor our roadmap towards the needs of the community.\n\nAt the moment we are considering a cache control system for each request (for controlling fetching and expiration) as well as a new mechanism for handling large UIs with a ton of image views.\n\nWe\u2019d like to thank everyone who is supporting Picasso including Pinterest, Spotify, Pandora, Lyft, Uber, New York Times, Locish, Pinnatta and many others."
    },
    {
        "url": "https://medium.com/square-corner-blog/ruby-serialization-and-enumeration-68962286f305",
        "title": "Ruby, Serialization, and Enumeration \u2013 Square Corner Blog \u2013",
        "text": "We came across an interesting Ruby puzzler the other day. Code that had been working fine had started failing during serialization with a scary looking backtrace from within activesupport:\n\nThe issue was isolated to a particular change to a class that was being serialized inside a larger data structure (which I\u2019ve further simplified):\n\nThe addition of the @logger variable was causing the problem, even though it was never used! Commenting out that line made the problem go away. This is certainly non-intuitive. What is going on? Let\u2019s find out.\n\nOne of Ruby\u2019s best features is the transparency of its libraries. We have the full path to the source of the exception right there in the backtrace. Opening up /gems/activesupport-4.1.1/lib/active_support/core_ext/object/json.rbin your favourite editor (obviously your path will be different) gives a major hint: this file is monkey-patching a number of core classes to support serialization.\n\nLooking at the Metric code above, how did it know how to serialize the object to JSON in the first place? Sure enough, elsewhere in json.rb (which also appears in our stack trace):\n\nMetric does not implement to_hash, so all of its instance variables are being included in the JSON representation. That explains why the unused @logger variable was the difference between success and failure. An easy fix then is to implement to_hash. Even if @logger was serializing correctly, it is unwanted in the JSON output.\n\nHowever that still doesn\u2019t explain the opaque error: not opened for reading (IOError). The quickest way to track that down, since we already have json.rbopen, is to add some quick debugging output before the line that failed:\n\nThis is why library transparency in Ruby is particularly amazing: you can also modify them in-place and see the results immediately!\n\nIn this instance a single line is output:\n\nThat explains it. For better or worse, IO objects in Ruby are enumerable. The semantics here are questionable: to_a is indeed part of the Enumerable interface, but making an IO into an array doesn\u2019t really make sense. And having each chunk on line breaks is somewhat arbitrary anyway.\n\nThis mismatch has historically dogged Enumerable and uses of that interface. Take this gem from rspec-expectations:\n\nThe comment says it all really! Funnily enough, the caller of this method has to special case IO as well:\n\nString is no longer Enumerable since Ruby 1.9. Should IO follow in its footsteps?"
    },
    {
        "url": "https://medium.com/square-corner-blog/a-comparison-of-go-web-frameworks-f47804cf86f6",
        "title": "A Comparison of Go Web Frameworks \u2013 Square Corner Blog \u2013",
        "text": "A few months ago we introduced Go to our system at Square and it\u2019s quickly become one of our sharpest tools. We recently evaluated Go web frameworks looking for one that fits us best.\n\nTL;DR: We recommend just using the net/http package in the standard library to start. And if you want help with request routing we recommend looking at Gorilla and Gocraft/web. Both Revel and Martini have too much dependency injection and other magic to make us feel comfortable. Gorilla is the most minimal.\n\nAll of the frameworks we looked at are built on top of the net/http package.\n\nRouting is the mechanism by which requests get mapped to a handler function.\n\nRouting is the base functionality for all of these frameworks. Gorilla seems to\n\nhave the most flexibility, but they are all roughly equivalent. One important\n\nnote is that the implementation of this functionality is very straightforward.\n\nData binding is the mechanism by which request parameters are extracted for use by handlers.\n\nControllers or Context are used to maintain per-request state.\n\nMiddleware is the terminology used for providing common functionality across a set of handlers. A common example of middleware is a logging module. Note that there is nothing magical about the infrastructure to support middleware. Instead of calling a single handler method, the framework is calling a series of methods.\n\nDoing it yourself\n\nIn our analysis we didn\u2019t gravitate toward any full-featured framework. The strength of the standard library lets us build moderately complex apps without any of the above, and of these options, Gorilla is our favorite because it is minimally intrusive.\n\nRather than using a package that gives us routing, middleware and controllers bundled together, we prefer small libraries encapsulating useful behavior. It\u2019s not bad to forgo the use of middleware if it means you get to keep control over the behavior of your app and you can keep things simple. For example, if you want to authenticate a user you could have a common bit of code at the beginning of every handler to invoke some user authentication method:\n\nThere\u2019s redundancy there, but it\u2019s explicit and fits nicely with the Go style of extremely readable code at the cost of slightly more verbosity."
    },
    {
        "url": "https://medium.com/square-corner-blog/connection-goes-down-business-stays-up-93856c8a6903",
        "title": "Connection goes down, business stays up \u2013 Square Corner Blog \u2013",
        "text": "Like a great employee or business partner, sellers should always be able to count on Square. And there\u2019s nothing more important than being able to accept a payment, no matter where you\u2019re selling. That\u2019s why last week we introduced Offline Mode for Square sellers.\n\nNow when a mobile seller temporarily loses a cell connection or the Wi-Fi goes down at a brick-and-mortar business, Offline Mode will save them the headache of lost sales.\n\nOffline Mode allows sellers to continue ringing up sales and accepting credit card payments while temporarily without an Internet connection. Square simply stores payment information when the card is swiped and saves it until the connection comes back, keeping a seamless experience for both buyer and seller.\n\nWhen we built Offline Mode, we wanted the experience to feel the same as regular payments, regardless of the technical steps necessary to make the process as secure and reliable as all of our products. Here\u2019s how we did it:\n\nWe designed security and privacy into Offline Mode from the beginning. To ensure that payment data is protected while it\u2019s stored on a seller\u2019s device, we encrypt it with a unique public key that rotates daily. The associated private key never leaves Square\u2019s servers, so the payment data can only be decrypted after it\u2019s uploaded to Square for processing and cleared from the device.\n\nThe most important job of Offline Mode is to process offline payments (and get them into sellers\u2019 accounts) as quickly as possible.\n\nWhenever unprocessed payments are on the device, we try to upload them in batches to reduce the number of network requests made. When the server acknowledges receipt, the payment\u2019s encrypted data is cleared from the client. Meanwhile, the server takes care of all the payment-processing mechanics, including authorizing, capturing, sending a receipt, and recording the sale in our reporting infrastructure. After that\u2019s done, the payment appears in payment history like any other payment.\n\nOnce a seller goes into Offline Mode, selling should feel like business as usual. After taking payments in Offline Mode, sellers have 72 hours to reconnect to the Internet so that payments can be processed. Sellers get reminders on their device that they have payments awaiting processing. And for sellers using iOS 7, we use its background fetch API to attempt to upload payments accepted offline even when the app isn\u2019t running.\n\nAnd that\u2019s it. We\u2019re in the business of making selling simple. Offline Mode is just the next step in giving sellers the tools they need to make sales and grow their business.\n\nHappy selling, wherever it is you may be."
    },
    {
        "url": "https://medium.com/square-corner-blog/open-sourcing-viewfinder-261518a98c20",
        "title": "Open Sourcing Viewfinder \u2013 Square Corner Blog \u2013",
        "text": "Five months ago, our small team at Viewfinder joined Square. We were inspired by Square\u2019s mission and eager to join a world-class engineering and design team, but we were also excited by the company\u2019s commitment to open source.\n\nToday, we add our efforts to that commitment by releasing the complete source for the Viewfinder server and iOS and Android apps. We\u2019re releasing this code in the hopes that it can be of utility to others, either as an archaeological resource or as the basis for other exciting efforts.\n\nOur days are now filled with other priorities, so we won\u2019t be able to provide support or bug fixes for this code. And while the code is not as clean as it could be, we would rather share it with you than hold it hostage.\n\nAll of our code \u2014 250,000 lines worth \u2014 can be found at: https://github.com/viewfinderco."
    },
    {
        "url": "https://medium.com/square-corner-blog/a-square-deal-for-women-403e970fea63",
        "title": "A Square Deal for Women \u2013 Square Corner Blog \u2013",
        "text": "College Code Camp is our four-day immersion program for women studying computer science. We\u2019ve been so inspired by what we\u2019ve seen these young women accomplish that we decided to host our second College Code Camp of the year.\n\nWe are now accepting applications for College Code Camp, which takes place August 13\u201316. Wellesley student Tali Marcus shares her experience participating in Code Camp and being a women in STEM.\n\nThis past January, I had the chance to spend a week in San Francisco, where I visited well-known startups, learned new development languages, and met with leaders at Square. Perhaps surprisingly, I did all this with 20 other women. Our trip was sponsored by Square as part of their second annual College Code Camp, an initiative to encourage women to enter technical fields.\n\nAs a student at Wellesley College, a women\u2019s school, I have taken for granted that technology is \u2014 and should be \u2014 a viable career option for women. Since my first computer science course at Wellesley (which I took on a whim, I might add), I have had the luxury of studying this subject in an exclusively female environment. In this setting, I have never been embarrassed to ask a question, hesitant to answer a question, or worried that I might be perceived as less intelligent because of my gender. This is a privilege, though one I may not have fully comprehended until participating in Code Camp. For me, it was a revelation to see how unusual and exciting it was for my fellow Code Campers, brilliant and inspiring women, to learn, discuss, brainstorm, and write code exclusively with other women.\n\nAs the week-long program progressed, I began to think about why so much interest and advocacy has been generated around encouraging women to pursue STEM subjects. What was the trigger? Why is Square so invested in the issue that it flew 21 students out to San Francisco and rearranged its executives\u2019 schedules to talk with us? Why has the company gone so far as to create High School Code Camp, a program for female high school students to come to Square twice a week and learn computer science in preparation for the AP exam?\n\nOne important reason is statistics. When a significant portion of half the population (i.e., women) opts out of an industry\u2019s workforce, the industry has squandered valuable resources. Any smart company realizes that encouraging women towards STEM fields, and eventually recruiting from that pool, is not just good business, but a potential competitive advantage.\n\nBut it\u2019s not just a question of attracting talented workers. Adding diversity to the workforce can make the whole greater than the sum of its parts. Just as people of different ages and races bring unique perspectives to the table, so too can the inclusion of both genders provide a critical dimension of diversity.\n\nThe benefits of diversity extend beyond simply including different perspectives. Imagine a young woman new to the workforce who starts a job as an entry-level programmer. When gender diversity doesn\u2019t extend through leadership at the highest levels, how can she be expected to picture herself as a CEO, a CFO, a COO, or any other C-[insert letter here]-O if she has never seen a woman succeed at the top? While there is much to learn from male leaders, female leaders have their own unique lessons to teach and examples to set. We all benefit when we can learn from a variety of leadership models.\n\nLet me be clear: I am not suggesting that a woman receive a C-level position because of her gender. In fact, I think that would actually be detrimental to herself, the company\u2019s success, and the reputation of female leaders. As important as diversity in leadership may be, it is also essential that leaders are qualified to do their jobs. In the STEM fields, this requires years of practice. It\u2019s not so much that women consciously opt-out of STEM subjects, but that at an early age, they choose not to opt-in. Females constituted less than 20% of the high school students who took the AP Computer Science exam in 2013. If these patterns persist, we will face slim pickings when it comes to diverse and qualified leadership.\n\nDespite these disheartening numbers, I have a lot of hope for the future of women in computer science. I have not been deluded into thinking that the make-up of the Computer Science department at Wellesley represents the make-up of computer programmers in the real world \u2014 though sometimes I do forget that male programmers exist. Rather, what gives me hope are the kindergarteners I work with at a local Boston area school. I help teach basic computer science concepts to these kids using ScratchJr, an educational programming language developed at the MIT Media Lab and Tufts University. I am often astonished at the children\u2019s ability to grasp new ideas, explain concepts to each other, and implement the programs they envision. Watching them makes me optimistic about the future of girls in STEM subjects \u2014 at this young age, there is no obvious gender division in skill level, and more importantly, in interest level. I see girls help boys, and I see boys help girls, but more importantly, I do not see any girl choose not to participate because computer science is \u201cfor boys,\u201d or because she feels \u201dless qualified\u201d than the 5-year-old boy across the table.\n\nThis program is valuable not only because of the head start these students have in learning to program, but also in the confidence they gain at a young age \u2014 confidence that will influence whether they decide to continue their computer science studies; their comfort level in what will likely be male-dominated classrooms; and ultimately, their career choices. It can be intimidating to hear of fellow computer scientists (often males) who began programming their first games when they were in middle school, and had already released multiple mobile apps by the time they were in college. The chance to start learning basic computer science skills on a relatively level playing field, before any gender divisions have developed, is a huge advantage, and one which can provide the confidence boost needed to assure oneself that \u201cI can succeed in an engineering classroom/position/team/etc. as well as anyone else.\u201d\n\nMy week at Square was a revelation on many levels. Being at Square allowed us all to go beyond our own stereotypes \u2014 we saw that there are all kinds of women who have found a comfortable place for themselves in the STEM fields. And beyond the incredible women, it was encouraging to meet all of the men who were just as passionate about creating a supportive environment for everyone. We could not have felt more welcomed by the people there, and it was clear that our excitement about the program was reciprocated by all the \u201cSquares\u201d we met. We could easily envision making a career at Square, or places like it.\n\nNot only is Square doing the right thing with its Code Camp initiative, it is doing the smart thing \u2014 a classic case of doing well by doing good."
    },
    {
        "url": "https://medium.com/square-corner-blog/buffering-data-with-okio-f83823d9ba25",
        "title": "Buffering data with Okio \u2013 Square Corner Blog \u2013",
        "text": "One of the most challenging projects I\u2019ve worked on is OkHttp, Square\u2019s HTTP client for Java. It offers interesting problems for API design (how does the application configure caching?), performance (how long should we hold idle connections?) and code structure (how eagerly do we parse headers?). These decisions interact like Rock-Paper-Scissors:\n\nThe most satisfying improvements are the ones that simultaneously speed up the code, simplify the implementation, and improve the API. In this post I\u2019ll describe an ugly situation that led to one of these solutions.\n\nIn recent months we\u2019ve been implementing drafts of http/2, which multiplexes all HTTP messages over a single socket. One thorny challenge is concurrency: we have many application threads but they can\u2019t all do I/O on the same socket simultaneously. Our strategy is to restrict network access to a dedicated producer thread that delivers data to consumer threads.\n\nOur first implementation used a fixed-size, circular byte array as a place for network data to land while waiting for an application thread to consume it. Here\u2019s a (slightly simplified) snapshot of that code, including my original ASCII art:\n\nI was not proud of this code. The fixed size buffer meant that every http/2 connection allocated a 64 KiB buffer, whether it was reading 1 byte or 1 gigabyte. The buffer also imposed that all network data be copied twice, from the network to the buffer, and from the buffer to the application.\n\nImagine our network thread as a mustached mailman. Each delivery is in an InputStream containing a long paper scroll with printed data. Its recipient is a mailbox containing a blank scroll that\u2019s ready to receive that data. The mailman transcribes data from his scroll to the mailbox scroll character-by-character, being careful to wrap around when he reaches the bottom.\n\nIt\u2019s a lot of work to copy data from one scroll to another! Our obvious-in-hindsight solution to this problem is to replace scrolls (that were tied to their owners), with pages (that can be exchanged). The mailman removes pages from his bag, adds them to the mailbox, and continues on his route.\n\nWe created a new datatype, Buffer. Like ArrayList, you don\u2019t need to size your buffer in advance. Each buffer works like a queue: write data to the end and read it from the front.\n\nInternally, Buffer is implemented as a linked list of segments. When the producer writes to the consumer, it reassigns ownership of the segments rather than copying the data across.\n\nBy using a Buffer instead of byte arrays, we write less code that runs faster and has a more natural API.\n\nOkio is Square\u2019s new open source library that complements java.io and java.nioto make it easier to access, store, and process your data. It includes Buffer and a few other simple-yet-powerful classes for common problems like gzip. The name Okio obeys the trend in Java I/O library names: IO, NIO, and now OKIO. Though it was started in OkHttp, we\u2019re eager to use everywhere it fits.\n\nSee the Okio GitHub page for a full walkthrough of the API, code examples, and downloads."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-cash-introduces-activity-view-64b44ab4d4e",
        "title": "Square Cash introduces Activity View \u2013 Square Corner Blog \u2013",
        "text": "At Square we view commerce as a form of communication \u2014 a way for individuals to exchange value with one another, whether it be goods, services, ideas or experiences.\n\nWhen we built Square Cash, we had this idea of conversation in mind. Our goal was to make it as easy to send money as it is to send an e-mail. We want everyone \u2014 no matter where they are or what devices they use to be able to exchange value with their friends and family effortlessly. Why shouldn\u2019t exchanging money be as simple as sending a quick message or a hello?\n\nIn October, we enabled anyone to send money by simply composing an e-mail, ccing: cash@square.com and putting the dollar amount in the subject line. We eliminated the need for complicated sign-up processes and secondary holding accounts, we eradicated confusing fee structures and long wait times. We made it fast, free and easy to send money to anyone.\n\nOver the last six months, Square Cash has grown considerably and is processing hundreds of millions of dollars annualized. We\u2019re seeing people use the service to collect money for birthdays, baby showers and concert tickets. Friends are sharing the cost of bachelor parties and college students are requesting money from their parents.\n\nGiven the frequency with which people are sending Square Cash, we want our customers to have an easy and familiar way to keep track of all of their past and current Square Cash transactions. Today we are introducing the Activity view.Activity enables any Square Cash app customer to see all of their past payments and view their current requests and receipts.\n\nThe Activity view showcases your Square Cash payments as a conversation; a dialogue between friends; a reminder of who owes whom; and an organized place for you to stay on top of outstanding requests.\n\nIt is even easier to reconcile requests for payments, by enabling anyone to respond directly within the app. So now you can simply tap a button and pay the money you owe.\n\nWe\u2019ve built a simple and beautiful service that we hope will continue to delight you and we look forward to bringing you more great features soon."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-market-accepts-bitcoin-b1a2abce5eb",
        "title": "Square Market Accepts Bitcoin \u2013 Square Corner Blog \u2013",
        "text": "Sellers should never miss a sale. We\u2019re building tools so sellers can accept any form of payment their customers want to use. Making commerce easy means creating easy ways to exchange value for everything from a massage to a DODOcase for your iPad.\n\nIn that spirit, starting today, buyers can purchase goods and services on Square Market with Bitcoin.\n\nFor sellers, the experience won\u2019t feel any different. Whether selling services or goods, sellers don\u2019t have to change a thing, except potentially expecting new trailblazing customers and more sales.\n\nWhether the buyer is new to paying with Bitcoin or not, it should feel like a VIP experience. We focused on making the experience smooth and simple.\n\nLet\u2019s dive into the technical details of checking out. When a buyer opts to \u201cPay with Bitcoin\u201d we first generate a new Bitcoin address and attach it to the order. We will continually monitor this address throughout the checkout process so we know when it has received payment.\n\nNext, the buyer submits their payment. Buyers with a mobile Bitcoin wallet, simply open their wallet and scan the QR code to load the transaction details. Those with a hosted Bitcoin wallet receive instructions for entering the required information.\n\nOnce the payment details are loaded into the buyer\u2019s wallet, they submit their payment to the network. Next, we detect that our receiving address was successfully funded and automatically advance the buyer to the order confirmation page. It\u2019s pretty magical to witness first-hand!\n\nKeeping it simple for the seller, the seller receives the amount of the purchased goods or services in USD and in the amount of USD advertised to the sellers\u2019 customer at the time of transaction, so the seller takes no risk on Bitcoin value fluctuations. The seller then fulfills their customer\u2019s order. Seamless!\n\nIt\u2019s exciting to take another step towards helping sellers reach more customers and making commerce easy for everyone."
    },
    {
        "url": "https://medium.com/square-corner-blog/bookfresh-joins-square-2bb6c3050e0a",
        "title": "BookFresh joins Square \u2013 Square Corner Blog \u2013",
        "text": "Sellers use Square for far more than accepting payments: They manage their point of sale with Square Register, run loyalty programs and promotions for customers, and access important analytics that help them make better business decisions. Sellers of all sizes want to grow their business, which is why we\u2019re constantly working to provide them with simple, accessible, affordable tools that help them do so.\n\nToday we are excited to announce that we have acquired BookFresh. BookFresh\u2019s software helps local sellers create a seamless, self-service appointment booking experience that connects them instantly with new and existing customers. Just like Square, BookFresh is designed to give time back to sellers so they can focus on their business.\n\nTo BookFresh customers: It\u2019s business as usual. The team remains dedicated to maintaining and improving the product and supporting you.\n\nTo Square sellers: We\u2019ll continue to make commerce easy for you and build even better experiences for you and your customers.\n\nWe\u2019re thrilled to join forces and work together."
    },
    {
        "url": "https://medium.com/square-corner-blog/eliminating-stringly-typed-code-in-objective-c-3c5fd65fefa7",
        "title": "Eliminating stringly-typed code in Objective-C \u2013 Square Corner Blog \u2013",
        "text": "As iOS developers, we can use any development environment we want, as long as it\u2019s Xcode. Beyond the Objective-C language, Xcode gives us a bunch of really awesome features for app development, including storyboards and asset catalogs. But all of the great convenience comes with some potential headaches.\n\nTo reference an image set you\u2019ve defined in an asset catalog, you use the imageNamed: method on the UIImage class, as such:\n\nTo reference a table cell you\u2019ve defined in a storyboard, you tell the table view to dequeue one of its cells:\n\nIn a master-detail relationship set up in a storyboard, you populate the detail controller with content when your master controller gets its prepareForSegue:sender:method called:\n\nWhat happens when your designer switches out your green button for a blue one? What happens when your design changes from a cell with a switch to a cell with a button? What happens when you rename your detail view controller?\n\nYou can probably fix the problem with a find-and-replace, but if you forget, you\u2019ll have no indication that anything is wrong until your app has a blank spot or worse, a crash. If your test coverage is lacking, it could be your customers who find the bug first.\n\nYou can mitigate the problem with comprehensive testing, but even then, you\u2019ll only find an issue at runtime \u2014 and I can guarantee that compiling your code is faster than running your tests.\n\nIt\u2019s not so hard to write a script that puts your strings in compiled code and lets you avoid a whole class of bugs. After all, asset catalogs are just nested folder structures with JSON metadata, and storyboards are a pretty readable dialect of XML.\n\nobjc-assetgen takes your asset catalogs and objc-identifierconstants takes your storyboards and outputs a couple of small source code files that turn this code:\n\nA small change, to be sure, from super-minimal extra code, so you won\u2019t be tempted to override it or edit it. But now if you rename that table cell identifier or that button image, you\u2019ll immediately get a build error and stop yourself from having that bug.\n\nAs a special bonus, you can use the resizable cap inset features of asset catalogs on an iOS 6 deployment target, since objc-assetgen will generate code that appropriately applies them as defined.\n\nFun fact: you can make custom color palettes for Mac OS X\u2019s standard color picker, and then you can consistently use them throughout your storyboards.\n\nBut if you want to reference those same colors elsewhere, you\u2019re stuck redefining them and hoping you\u2019ve done it the same way.\n\nAgain, though, we can write some code to solve our problem. objc-colordump will extract the colors defined in a custom color palette into a small source code file. It turns this:\n\nYou can find these three tools \u2014 along with a README that more fully explains their use \u2014 on the objc-codegenutils GitHub repository. They have no external dependencies, and you can easily add them as dependencies of your app in Xcode and run them as a build phase.\n\nThis blog post is adapted from a talk I gave at a recruiting event at Carnegie Mellon University on February 4th and at Princeton University on February 6th. If you\u2019re interested in joining us here at Square, we\u2019re hiring! Check out the link on the right side of the page for more information."
    },
    {
        "url": "https://medium.com/square-corner-blog/faster-rsa-in-java-with-gmp-8b13c51c6ec4",
        "title": "Faster RSA in Java with GMP \u2013 Square Corner Blog \u2013",
        "text": "At Square, we care a lot about seller experience, and part of that is making sure our services run as fast as possible. One service I\u2019ve had the chance to help build is our seller session service. The session service essentially does two things:\n\nHere\u2019s where session validation gets a little complicated. As a payments processing company, we take the security and integrity of our data very seriously, and we care about things like security proofs. It was important to us to design a strong user authentication model that could go deep into our software stack.\n\nOur solution was to have the session service produce a user certificate with every request. This certificate is a small piece of data cryptographically signed by the session service using RSA. As the request makes its way from service to service, each recipient uses a well-known public key to verify the authenticity of the certificate, which proves that the request is on behalf of a particular logged-in user. The verification is fast and doesn\u2019t require calling back to the session service.\n\nUnfortunately, generating the certificate was slower than I expected. RSA signing is computationally expensive; performing a 2048-bit private key operation takes several solid milliseconds on a modern CPU. The operation can\u2019t be parallelized either, so while adding CPU power enables us to sign hundreds of requests per second, additional CPUs can\u2019t make an individual operation run any faster. I needed a faster implementation.\n\nThe critical operation in RSA is modular exponentiation, that is, (base ^ exponent % modulus). The Java implementation of java.math.BigInteger.modPow() is pretty fast, but it turns out that the one in libgmp (the GNU Multiple Precision Arithmetic Library) is a lot faster\u2026\n\nI\u2019m happy to announce that today we\u2019re open sourcing a small Java library that wraps libgmp. The only feature we\u2019re launching with is modPow, the key operation in RSA. It\u2019s significantly faster than the native Java version. Here\u2019s a benchmark on my MacBook Pro:\n\nAs you can see, the native version is more than 4x faster than the Java version. We found similar performance gains on our production boxes, where we were able to chop the latency of our session service in half.\n\nUsing the library is simple, just add maven dependency on com.squareup.jnagmp:jnagmp and then:\n\nwhere the arguments are all BigInteger. (The \u201csecure\u201d version is hardened against timing attacks. There\u2019s also an insecure version, which is faster, but vulnerable to timing attacks. For private key cryptography, you\u2019d definitely want to use the secure version.)\n\nThe code we\u2019re releasing today uses the excellent JNA library to talk to libgmp. We originally used plain JNI with some C++ glue to talk to libgmp. The code itself wasn\u2019t bad, but the extra native compile step was really annoying. So we rewrote it using JNA, and the result is much nicer. We\u2019ll be able to add additional features and expose more libgmp methods just by writing more Java.\n\nThe maven artifacts contain pre-built libgmp binaries for Mac and 64-bit Linux, but your can build your own binary, or use one installed on your system.\n\nTo demonstrate one way to use jnagmp, we\u2019re also releasing a second module, com.squareup.jnagmp:bouncycastle-rsa, a reimplementation of Bouncy Castle\u2019s org.bouncycastle.crypto.engines.RSAEngine using native modPow. You can use it as a drop-in replacement for RSAEngine.\n\nSo far we\u2019ve only implemented modPow, but there are many other functions in libgmp we could expose. (We welcome contributions.) We only have a few pre-built libgmp binaries, so it would be nice to get binaries for a broader variety of platforms, including Windows.\n\nOn the crypto side, it would be awesome if someone knowledgeable about java.security wants to figure out the magic we\u2019d need to register native RSA as a security provider.\n\nHuge thanks to libgmp, and to the JNA project. They did all the hard work, we just put it in an easy-to-use package."
    },
    {
        "url": "https://medium.com/square-corner-blog/an-announcement-of-almost-incalculable-value-8776feb757a9",
        "title": "An announcement of almost incalculable value \u2013 Square Corner Blog \u2013",
        "text": "It has come to our attention that certain other companies are running around making misguided claims as to the primacy of certain dinosaurs. We wish to set the record straight.\n\nThe Shuvosaurus is the greatest dinosaur of all time. Bar none, without exception and basically by definition. Consider that all other dinosaurs were painstakingly unearthed, grain of dirt by grain of dirt. Armies of khaki safari vested graduate students spent thousands of hours carefully extracting the precious dinosaur bones from the ground where they\u2019d laid dormant for millions of years. Shuvosaurus on the other hand dug its own way out of the earth and presented its magnificent skeleton to an 8 year old boy. Imagine the level of greatness you must embody for you to spend the 223 million years after you die digging your mortal remains out from the bowels of the earth to the surface. And then presenting yourself to a person from a species that you\u2019ve never even heard of. And then naming yourself after that person. That is truly great.\n\nAlso unlike all other dinosaurs named after where they were found or the old dead paleontologist who found them, Shuvosaurus is named after an awesome software engineer. And everyone knows that software engineers are 1000 times as cool as old dead paleontologists. This makes Shuvosaurus 100,000 times cooler than other dinosaurs. That\u2019s just math. You can\u2019t argue with math.\n\nTo prove this point, we surveyed a representative sample of humanity*. After adjusting for sampling error with some simple statistical techniques (of the kind widely deployed during presidential elections; note the lack of error bars in the chart) we found that Shuvosaurus is widely acknowledged as the best dinosaur. Followed, of course, by Larry King, Iggy Pop and the Velociraptor.\n\nWe hope that we have set the record straight on this matter once and for all on the greatest dinosaur to ever walk the earth.\n\n*Re: People at Square who responded within an hour of sending an email."
    },
    {
        "url": "https://medium.com/square-corner-blog/campfire-camaraderie-a365b0a05909",
        "title": "Campfire Camaraderie \u2013 Square Corner Blog \u2013",
        "text": "College Code Camp is our four-day immersion program for women studying computer science. Berkeley student Shana Hu shares her experience.\n\nLast week, I went camping for the first time. There were no tents or sleeping bags, but there was a fire on the first night. And in its dim glow, 21 women from colleges across the U.S. and Canada milled about a chilly backyard in San Francisco, slowly getting to know each other. We were essentially a group of strangers, but the one thing we had in common was an invitation from Square to participate in Code Camp 2014, so there we sat, laughing around a campfire at the start of a four-day program which would amaze and inspire us in ways we could have never predicted.\n\nOur days were packed with activities, including three Tech Treks spread over two days, which were 2.5 hour development sessions led by experienced Square employees. I got to clean up CSS code (which doesn\u2019t sound like much fun but trust me \u2014 it felt so rewarding at the end), learn some iOS development by creating a custom lock screen, and hack a web app by exposing its security flaws.\n\nThe Tech Treks also helped introduce us to technologies we could use for our Dev Day project, which began through a gathering (complete with hot chocolate, comfy floor pillows, and an 8-bit fire animation on a TV screen) of current High School Code Campers and College Code Camp alumnae. When asked if we had any project ideas, one camper suggested the idea of building a sort of memory game \u2014 an app which could dispel stereotypes, inspire girls interested in engineering, and remind us all of the amazing women we had met. Campers excitedly began contributing ideas and potential features, and I realized that there are few things more amazing than being surrounded by a group of passionate people. When Dev Day rolled around, we tackled Android, iOS, and Ruby on Rails with the help of Square engineers, and after a lot of learning, helped build something out of nothing but excitement and code.\n\nWe met several employees throughout the week, including our own personal mentor who we got to know over meals. Others helped us through Tech Treks and Dev Day, or even just stopped by to say hi. Some campers and I met Sarah Friar, Square\u2019s CFO, in line at the smoothie bar one day and then ate breakfast with her the next, listening to her talk about how she developed a love of engineering early on, pursued a degree in a heavily-male industry, worked in both goldmines and Goldman, and now \u2014 still bright-eyed and passionate about technology \u2014 helps lead Square.\n\nAs a woman in a prime leadership role, she was already an inspiration to us all, but hearing her say that she never had a doubt in her mind that she wanted to have children made her relatable in a way that male executives weren\u2019t. In an industry where we almost never hear leaders discuss the difficulties of balancing having a career and having a child, it was raw and real to know that, as young women who have yet to create a career or a family, we could have both.\n\nWhereas Sarah showed us a glimpse at our potential futures, Vivienne Harr taught us that it is never too early to start making a difference. At the age of ten, she\u2019s already raised over $1 million to help end child slavery through Make A Stand Lemon-Aid, a company she founded after she saw a photo of slaves her age, and decided she had to do something about it. But she didn\u2019t start with the B Corp she has today; she started with a simple lemonade stand, and with that stand she stood for 365 days, selling her homemade lemonade cup by cup, all the while tweeting her journey and raising money with Square. As we sat before her, each sipping a glass of the bottled lemonade her company now sells in stores, she spoke of the importance of compassion with action, the ability of each and every person to make a difference no matter how small, and the power of technology to change the world. At the end, she posed a single question to get us thinking: \u201cHow do we code empowerment?\u201d\n\nAnd of course, we were able to spend some time with Jack Dorsey. When asked what he thought about the recent push to teach programming in elementary and middle schools, he agreed that kids should learn to code, but not necessarily because they should become programmers. They should learn to code because coding teaches a different way of thinking, the same way painting does. His answer fit perfectly into the philosophy behind Square which we saw threaded throughout the office \u2014 that eye-opening combination of development and design in a company sparked by the idea that technology could aid not only a single artist trying to sell a piece of glasswork, but also a local coffee shop, a mom-and-pop corner store, or even a little lemonade stand with a big mission.\n\nThe learning didn\u2019t stop with our schedule. We were given chances to make our own opportunities, and now that camping is over, I find myself realizing that many of my favorite moments were unplanned.\n\nAn hour of free time at the office turned into a fortuitous meeting just because I took the chance to ask an employee about her role on the Brand team. She jumped at the chance to introduce me to one of Square\u2019s designers, who then took the time out of his day to sit with me and a fellow camper, speaking for over an hour about Square\u2019s creative process. The last-minute meeting, which concluded after most employees had left for home, ended up one of my favorite moments of the camp, and left me amazed at the detail and passion that goes into making Square the company it is, not only in development but across the board.\n\nA lunch with women in engineering at Square transformed from a Q&A into a full-blown discussion with both employees and campers contributing experiences. We were building a community, rather than a hierarchy, and bonding over a common background. One camper expressed her frustration at the fact that people took her less seriously or wouldn\u2019t talk to her at all when she wore a dress at a conference, and some of the most spirited advice came from another camper who encouraged us all to be confident in ability, to be comfortable in appearance, and \u201cto not let anyone squash you.\u201d\n\nAn exploration of San Francisco became a testament to how much of an impact technology can make in someone\u2019s everyday life when three other campers and I visited the farmers\u2019 market by the Ferry Building. The merchants we talked to all loved the impact Square had on their businesses, but the highlight of our outing was when we approached a young girl, no older than nine or ten, who sat on the street by a crate of bracelets she had woven. We asked if she had heard of credit card readers, and she immediately nodded, saying just one word \u2014 \u201cSquare.\u201d\n\n\u201cMy mom got me a reader, but we lost it,\u201d she told us, at which point I reached into my bag for the newest iteration, which all campers had received in a goody-bag, and placed it in her palm.\n\nAfter three days of learning about Square from the inside, hearing about it from executives, engineers, mentors, and more \u2014 this was the first instance of being able to see the impact of a single square out in the wild. And it was amazing to see that the energy and passion which permeated the office didn\u2019t only belong to Square employees, it belonged to Square merchants, of all ages, as well. I was helping a young business bloom just by handing over a piece of plastic, and it was inspirational to realize through that single gift that technology can indeed change the world one step at a time.\n\nCamping went beyond the office walls. We had evenings free to get to know each other better, and these moments were what made the trip for me. My first night began with a game of pool on a tilted table, and then after laughing about the rigged game, five of us sat and discussed perception, planetary revolutions, and programming. Other evenings featured conversations in a local caf\u00e9 and an impromptu dance party in one of our hotel rooms, and it was wonderful knowing that I wasn\u2019t just making connections \u2014 I was making friends.\n\nThere is something magical about being surrounded by 20 women with shared interests and diverse backgrounds, who are both humbly down to earth and unbelievably out of this world. We had campers from all sorts of hometowns, with educational experiences ranging from small private women\u2019s colleges to large public schools with majority-male CS courses. As someone from the latter, Code Camp was more than just refreshing \u2014 it was one of the only times in my life so far that I\u2019ve been able to sit with a group of women and discuss everything and anything, ranging from the rationality of love to Ruby on Rails.\n\nWhat I learned from Square Code Camp is that there are some absolutely incredible women out there. And when brought together, we will not only be inspired by designers and developers, new-grad employees and technological leaders, ten-year-old merchants and ten-year-old philanthropists \u2014 but we will inspire each other. By helping us forge friendships that span not only the continent but the globe and by helping us build communities of women passionate about technology, we are helping both spread encouragement to others and empower ourselves. We can be our own heroes, but sometimes we just need a bit of support.\n\nAnd at the end of the day, supporting women in technology is not only about gender \u2014 it\u2019s about diversity. It\u2019s about the power in harnessing the passion of capable individuals from various backgrounds. It\u2019s about crafting a community which welcomes all who wish to explore, create, and contribute to products which can change our world. It\u2019s about learning to think in a different way, whether it be by code or paint, and about realizing that whether someone chooses to dress in slacks or skirts has no impact on their capability as a coder.\n\nThank you to everyone @Square who made Code Camp an unforgettable experience. And here\u2019s to the 20 other women who shared my first campground \u2014 we will not be squashed!"
    },
    {
        "url": "https://medium.com/square-corner-blog/meet-jake-9a9d93b568c",
        "title": "Meet Jake \u2013 Square Corner Blog \u2013",
        "text": "While Jake Wharton cannot divulge what he is currently working on at Square \u2014 it\u2019s a \u201ctop-secret new project\u201d \u2014 he did agree to answer a few questions.\n\nWhat are you currently working on?\n\nI am currently working on the Android client for a brand new product. Previously, I worked on the Android clients for Square Wallet, Square Register, and to a much lesser degree, Square Cash.\n\nHow would you describe Square\u2019s engineering culture?\n\nThe engineering culture is a blend of specialists, who posses an extreme depth of knowledge on a subject, and generalists, whose breadth allows them to work with many different platforms and technologies. These two types of people are distributed on small teams, each of which are responsible for a significant chunk of all the machinery that makes Square move.\n\nWhat\u2019s the most challenging engineering project you\u2019ve worked on in the past year?\n\nLast summer a unique opportunity presented itself; we were in a place to significantly improve some of the libraries on which all of our apps are built. A few of us spent four weeks in heated API debates, deep dives into intricacies of the Java language, and re-writing massive sections of code for better speed, efficiency, and testability. It was a month of late nights and long weekends that ended up being both very challenging and fun. These libraries are the foundation on which all of our Android apps are built and the investment we made during that time has paid off in both developer productivity and app features.\n\nWhy is open source important to you?\n\nOpen source has been the foundation of almost everything I have ever worked on. Beyond just providing tools with which I can build applications more rapidly, it has been and remains a fantastic tool for expanding your knowledge and skill set.\n\nWhat\u2019s your most memorable moment at Square?\n\nCoordinating and participating in the work for our Seven Days of Open Source, which culminated in manning a booth in the Developer Sandbox at Google I/O with a few of my fellow engineers.\n\nJava for the real things. It has unmatched tooling and IDE integration. Python for scripting everything else.\n\nIf I\u2019m excluding Square\u2019s libraries that I work on\u2026\n\n* Serious \u2014 Google Guava: The missing standard library for Java offering anything and everything for simplifying common operations. It will be interesting to see how it evolves with the now imminent Java 8 release.\n\n* Useful \u2014 Google compile-testing: Allows running and interacting with the java compiler inside your unit tests. For libraries which need to predictably fail your build this provides an extremely elegant way to test that behavior.\n\n* Playful \u2014 lolcommits: Capture a webcam image every time you make a commit. Predictable hilarity ensues when enabled."
    },
    {
        "url": "https://medium.com/square-corner-blog/simpler-android-apps-with-flow-and-mortar-5beafcd83761",
        "title": "Simpler Android apps with Flow and Mortar \u2013 Square Corner Blog \u2013",
        "text": "When Fragments were introduced to Android the Square Register team jumped on board. We were already chopping activities into subscreens in a clumsy way, and we did ourselves a lot of good moving to the new hotness. But we also bought ourselves a lot of headaches: default constructors only, offscreen fragments mysteriously being brought back to life at odd moments, no direct control over animation, an opaque back stack manager \u2014 to use Fragments well required mastering an increasingly arcane folklore.\n\nSo now we do something else, and we\u2019d like to share a couple of new libraries that help us do it. We use Flow to keep track of what View to show when. Yes, we just use View instances directly as the building blocks of our apps. It\u2019s okay because the views stay simple (and testable) by injecting all their smarts from tidy little scopes defined with Mortar.\n\nThe first new toy, Flow, is a backstack model that knows you likely have to deal both with the device\u2019s back button as well as an ActionBar\u2019s up button. The things that Flow manages are referred to as \u201cscreens,\u201d with a lowercase \u201cS\u201d \u2014 they\u2019re not instances of any particular class. To Flow a screen is a POJO that describes a particular location in your app. Think of it as a URL, or an informal Intent.\n\nFor example, imagine a music player app, with Album and Track screens:\n\nThe two screen types provide just enough information to rummage around for the model classes that have the actual album and track info. The optional HasParentinterface implemented by the TrackScreen here declares what should happen when the up button is tapped. Moving to the right track screen from a list on the album screen is as easy as:\n\nGoing back or up are just as simple, flow.goBack() or flow.goUp().\n\nSo what happens on-screen when you call these methods? You decide. While Flow provides the @Screen annotation as a convenience for instantiating the view to show for a particular screen, actually displaying the thing is up to you. A really simple Activity might implement the Flow.Listener interface this way:\n\nIt shouldn\u2019t take a lot of imagination to see how to embellish this with animation based on the given Direction (FORWARD or BACK).\n\nIf Flow tells you where to go, Mortar tells you what to build when you get there.\n\nMajor views in our apps use Dagger to inject all their interesting parts. One of the best tricks we\u2019ve found is to create @Singleton controllers for them. Configuration change? No problem. The landscape version of your view will inject the same controller instance that the portrait version was just using, making continuity a breeze. But all those controllers for all those views live forever, occupying precious memory long after the views they manage have been torn down. And just how can they get their hands on the activity\u2019s persistence bundle to survive process death?\n\nMortar solves both of these problems. Each section of a Mortar app (each screen if you\u2019re also using Flow) is defined by a Blueprint with its very own module. And the thing most commonly provided is a singleton Presenter, a view controller with a simple lifecycle and its own persistence bundle.\n\nGoing back to our music player example, using Mortar the AlbumScreen might look something like this:\n\nWe\u2019re imagining here an app-wide JukeBox service that provides access to Albummodel objects. See that @Provides Album method at the bottom? That\u2019s a factory method that will let the AlbumView inflated from R.layout.album_view simply @Inject Album album directly, without messing with int ids and the like.\n\nTo take the example further, suppose the AlbumView is starting to get more complicated. We want it to edit metadata like the album name, and of course we don\u2019t want to lose unsaved edits when our app goes to the background. It\u2019s time to move the increasing smarts out of the view and over to a Presenter. Let\u2019s keep the Android view concerned with Android-specific tasks like layout and event handling, and keep our app logic cleanly separated (and testable!).\n\nThe view will now look something like this (in part). Notice how the AlbumView lets the presenter know when it\u2019s actually in play, through overrides of onAttachedToWindowand onDetachedFromWindow.\n\nBecause AlbumScreen.Presenter is scoped to just this screen, we have confidence that it will be gc\u2019d when we go elsewhere. The AlbumScreen class itself serves as a self-contained, readable definition about this section of the app. And, do you see those onLoad and onSave methods on the Presenter? Those are the entire Mortar lifecycle. We just haven\u2019t found a need for anything more.\n\nSo that\u2019s how we\u2019re doing it these days, and life is pretty good. Flow and Mortar are both still taking shape, though \u2014 hopefully with help from you."
    },
    {
        "url": "https://medium.com/square-corner-blog/android-string-formatting-with-phrase-33b7c8707e18",
        "title": "Android String Formatting with Phrase \u2013 Square Corner Blog \u2013",
        "text": "Phrase is a micro-library for text token replacement on Android. Phrase solves a few simple problems we noticed after translating Square Register to French and Japanese.\n\nFormatting the greeting is straightforward. Android\u2019s Context even provides an overloaded getString(\u2026) method that retrieves the string and formats in a single step.\n\nFormat specifiers like %1$s are not obvious to linguists, who are rarely programmers. With each translation, we\u2019d find a few typos and mistakes in these cryptic specifiers. The problem was worse when specifiers were adjacent to punctuation and other characters.\n\nAnother problem is that specifiers like %2$d are not descriptive. We must study surrounding text to discern the meaning and their order must match the Java code to avoid bugs.\n\nFinally, Context.getString(\u2026) cannot handle styled text such as bold and italic. If your strings.xml contains simple HTML tags alongside format specifiers, the HTML tags are silently ignored.\n\nWith Phrase, the greeting changes to the following. As you can see, cryptic specifiers are replaced with readable keys like {name} and {yield}.\n\nReducing translation errors is our primary goal and we achieve this by keeping the rules simple.\n\nMore flexibility adds complexity so we don\u2019t allow uppercase letters, numbers, or any symbols other than underscores.\n\nFormatting follows a fluent style. Using named keys makes life easier for programmers because key/value substitution is not order-dependent.\n\nNotice that Phrase returns a CharSequence rather than String. This is because Phrase takes care to preserve spans, enabling a single string to include both HTML tags and keys.\n\nPhrase adheres to a fail-fast philosopy. Phrase throws an exception if you make any of the following mistakes.\n\nThrowing exceptions and crashing makes it easy to spot mistakes during development and minimizes odds of shipping embarrassing translation mistakes or displaying unformatted string templates.\n\nBecause keys are straightforward, one could easily write a post-translation validation script that ensures strings.xml files in each language contain the exact same Phrase keys.\n\nThat\u2019s it! Phrase consists of a single Java class and a suite of unit tests. It reduces translation mistakes by simplifying the patterns a linguist must know, relying on named keys rather than positional arguments, and failing fast with helpful error messages."
    },
    {
        "url": "https://medium.com/square-corner-blog/square-from-the-trenches-month-one-b1a19462a656",
        "title": "Square From the Trenches, Month One \u2013 Square Corner Blog \u2013",
        "text": "I joined Square\u2019s New York engineering team a little over a month ago. Prior to joining Square, I had been aware of the company and was impressed by their products, but had no clear picture of the culture and the inner workings of teams. This post is intended to shed a little light on my first month experience.\n\nTo ease the transition into a new company, Square has a new employee orientation program called Square One. Square One is a week long program that provides an introduction to the company culture, the various systems and groups within Square, an alphabet soup of acronyms (ACH, AML, MCC, SMB, \u2026), and regular access to the amazing on-site food and coffee. The information content of Square One is high and the presenters are uniformly passionate about the topics they are presenting. For an engineer, the most fundamentally important part of Square One is the setup of the development environment and introduction to the Square development processes. But there are also plenty of presentations that focus on the business at large to make sure we\u2019re all well rounded.\n\nAfter Square One I was ready to get down to business. Almost. I had the great luck, with a dash of foresight, to arrange for my first week at Square to be capped by the company holiday party. All I can say is that Squares like to have a good time. Work hard, play hard. Did I mention there is a weekly happy hour? Often two, actually.\n\nOpenness is a key tenet at Square. Many companies pay lip service to openness, but I have never seen it more clearly and succinctly expressed than the notes@ mailing list we utilize. At Square, notes are taken for every meeting and then sent to the notes@ mailing list, which every Square is encouraged to follow. This holds true for board meetings and executive meetings, all the way to daily stand-up meetings amongst members of a product team. This transparency is reflected in how on point the average Square employee is with the company message and direction.\n\nTeams at Square are fairly small. My team has 9 individuals: 6 engineers, 2 designers, and 1 product manager. I only recently learned of Jeff Bezos\u2019 2 pizza rule, but I\u2019ve been a believer of that philosophy for a while. We\u2019re a \u201cfull stack\u201d team, with work spanning Java and Ruby on the server, JavaScript and CoffeeScript in the web frontend, and Objective-C and C++ in the iOS client. This is the perfect setup for me; I thrive when my contributions can go exactly where they\u2019re needed most.\n\nThe importance of communication is emphasized by the daily stand-up meetings amongst team members. As a new employee these stand-up meetings provide a critical forum for airing issues that are blocking your work. From the banal \u2014 how to do a git pull request when coming from the worlds of Perforce and Mercurial \u2014 to the explicitly situational \u2014 the exact input data to reproduce a problem at scale. During the day, our team hangs out in a Campfire room which provides a forum for both quick questions (\u201cIs XYZ unit test failing for anybody else?\u201d), and more involved discussions (\u201cCheck out this profile showing the CPU usage of MakeNewFrozBoz during account sign in.\u201d).\n\nThe most important aspect of the team experience is the quality of the other team members. I can report that Square\u2019s interview process is working well. Fellow engineers have consistently been top notch and willing to lend a helping hand; from helping a git newbie, to providing guidance when introducing a performance optimization to a complicated system. Square practices good development hygiene and engages in code reviews, unit testing and continuous integration.\n\nSquare\u2019s mission is to make commerce easy and the opportunities are exciting both in their impact and the challenges that need to be met. With the first month\u2019s foundation laid, I\u2019m excited to get started with the work ahead."
    },
    {
        "url": "https://medium.com/square-corner-blog/dynamic-images-with-thumbor-a430a1cfcd87",
        "title": "Dynamic Images with Thumbor \u2013 Square Corner Blog \u2013",
        "text": "Square uses a lot of user-generated images which are displayed on many mediums. An image for a single item can appear in the Dashboard, on a Market page, in the Register client (on an iPad, iPhone, or Android), in an email receipt, or in Wallet (on iPhone or Android). In order to do this in an efficient, consistent, and cross-platform way we leverage an application named Thumbor.\n\nThumbor is an open-source, on-demand image service which allows for server-side cropping, resizing, and compositing of images. This is particularly useful for applications where the density and resolution of the target screen can vary wildly. Rather than downloading a large image and scaling it to fit the display, the image is requested at the exact desired size and the server does the work on-demand to resize it before delivering.\n\nThumbor runs as a web application which uses specially constructed URLs to define how it behaves. The simple case of resizing the Google logo at 100px*100px we would use this URL:\n\nTwo security options that prevent others from abusing your service are opt-in: domain whitelisting and authentication. Whitelisting domains ensures Thumbor will only transform images which originate from your servers. Authentication uses a private key to add a checksum to the URL which is verified before performing the requested transformations. These two options restrict use to only allowed images and ensure that only the clients of your choosing can use the service. The secure version of the above URL becomes:\n\nThumbor has many options for transforming and compositing images. Combining multiple resize operations and arranging them into a single image further compounds all of the aforementioned positive effects. With all of these configuration options the URL can become very complex to construct. Square has developed two libraries to simplify this task.\n\nPollexor is a Thumbor URL builder for Java suitable for use both in server-side applications and Android clients.\n\nConstructing the URL for the previous example using the Google logo becomes more declarative:\n\nComplex configurations can also be constructed in a clear, descriptive manner:\n\nOur forthcoming ThumborURL library allows for easy URL creation for iOS-based clients.\n\nUsing Thumbor has alleviated the burden of dealing with images of all different sizes in our clients while also allowing them to operate much more efficiently."
    },
    {
        "url": "https://medium.com/square-corner-blog/capture-the-flag-8d47daed00a5",
        "title": "Capture The Flag \u2013 Square Corner Blog \u2013",
        "text": "I\u2019m very happy to announce the launch of the Matasano/Square Capture the Flag Challenge.\n\nThe challenge is based around finding vulnerabilities in some firmware running on an MSP430 chip. The CTF gives you a debugging interface to the system, and each level has a new vulnerability that you\u2019ll have to find and exploit in order to get to the next stage. You\u2019ll learn assembly, overflow buffers, and crack DRM as you advance through the ranks.\n\nIf all that sounds like fun to you, you\u2019re right: it is. But if you\u2019ve never heard of the MSP430 before, or if you don\u2019t know the first thing about finding and exploiting vulnerabilities, don\u2019t let that stop you. We hope that the challenge is straightforward enough for anyone with a programming background to get started. Thanks to our friends at Matasano, the CTF interface is easy to use and there are links and hints at each stage that will point you in the right direction. The only thing you\u2019ll need is curiosity and time \u2014 it\u2019s a ton of fun.\n\nTo give it a shot, just visit microcorruption.com. And if you\u2019re interested in working on this kind of thing for real, we\u2019d love to hear from you!"
    },
    {
        "url": "https://medium.com/square-corner-blog/a-journey-on-the-android-main-thread-lifecycle-bits-d916bc1ee6b2",
        "title": "A journey on the Android Main Thread \u2014 Lifecycle bits",
        "text": "In the previous part we took a dive into loopers and handlers and how they relate to the Android main thread.\n\nToday, we will take a closer look at how the main thread interacts with the Android components lifecycle.\n\nLet\u2019s start with the activity lifecycle and the magic behind the handling of configuration changes.\n\nThis article was inspired by a real crash that occurred in Square Register.\n\nA simplified version of the code is:\n\nAs we will see, doSomething() can be called after the activity onDestroy()method has been called due to a configuration change. At that point, you should not use the activity instance anymore.\n\nThe device orientation can change at any time. We will simulate an orientation change while the activity is being created using Activity#setRequestedOrientation(int).\n\nCan you predict the log output when starting this activity in portrait?\n\nIf you know the Android lifecycle, you probably predicted this:\n\nThe Android Lifecycle goes on normally, the activity is created, resumed, and then the orientation change is taken into account and the activity is paused, destroyed, and a new activity is created and resumed.\n\nHere is an important detail to remember: an orientation change leads to recreating the activity via a simple post of a message to the main thread looper queue.\n\nLet\u2019s look at that by writing a spy that will read the content of the looper queue via reflection:\n\nAs you can see, the message queue is merely a linked list where each message has a reference to the next message.\n\nWe log the content of the queue right after the orientation change:\n\nHere is the output:\n\nA quick look at the ActivityThread class tells us what those 118 and 126 messages are:\n\nRequesting an orientation change added CONFIGURATION_CHANGED and a RELAUNCH_ACTIVITY message to the main thread looper queue.\n\nLet\u2019s take a step back and think about what\u2019s going on:\n\nWhen the activity starts for the first time, the queue is empty. The message currently being executed is LAUNCH_ACTIVITY, which creates the activity instance, calls onCreate() and then onResume() in a row. Then only the main looper processes the next message in the queue.\n\nWhen a device orientation change is detected, a RELAUNCH_ACTIVITY is posted to the queue.\n\nWhen that message is processed, it:\n\nAll that in one message handling. Any message you post in the meantime will be handled after onResume() has been called.\n\nWhat could happen if you post to a handler in onCreate() during an orientation change? Let\u2019s look at the two cases, right before and right after the orientation change:\n\nHere is the output:\n\nTo sum things up: at the end on onCreate(), the queue contained four messages. The first was the post before the orientation change, then the two messages related to the orientation change, and then only the post after the orientation change. The logs show that these were executed in order.\n\nTherefore, any message posted before the orientation change will be handled before onPause() of the leaving activity, and any message posted after the orientation change will be handled after onResume() of the incoming activity.\n\nThe practical implication is that when you post a message, you have no guarantee that the activity instance that existed at the time it was sent will still be running when the message is handled (even if you post from onCreate() or onResume()). If your message holds a reference to a view or an activity, the activity won\u2019t be garbage collected until the message is handled.\n\nStop calling handler.post() when you are already on the main thread. In most cases, handler.post() is used as a quick fix to ordering problems. Fix your architecture instead of messing it up with random handler.post() calls.\n\nMake sure your message does not hold a reference to an activity, as you would do for a background operation.\n\nRemove the message from the queue with handler.removeCallbacks() in the activity onPause().\n\nUse handler.postAtFrontOfQueue() to make sure a message posted before onPause() is always handled before onPause(). Your code will become really hard to read and understand. Seriously, don\u2019t.\n\nDid you notice that we created a handler and used handler.post() instead of directly calling Activity.runOnUiThread()?\n\nHere is why:\n\nUnlike handler.post(), runOnUiThread() does not post the runnable if the current thread is already the main thread. Instead, it calls run() synchronously.\n\nThere is a common misconception that needs to die: a service does not run on a background thread.\n\nAll service lifecycle methods (onCreate(), onStartCommand(), etc) run on the main thread (the very same thread that\u2019s used to play funky animations in your activities).\n\nWhether you are in a service or an activity, long tasks must be executed in a dedicated background thread. This background thread can live as long as the process of your app lives, even when your activities are long gone.\n\nHowever, at any time the Android system can decide to kill the app process. A service is a way to ask the system to let us live if possible and be polite by letting the service know before killing the process.\n\nSide note: When an IBinder returned from onBind() receives a call from another process, the method will be executed in a background thread.\n\nTake the time to read the Service documentation \u2014 it\u2019s pretty good.\n\nIntentService provides a simple way to serially process a queue of intents on a background thread.\n\nInternally, it uses a Looper to handle the intents on a dedicated HandlerThread. When the service is destroyed, the looper lets you finish handling the current intent, and then the background thread terminates.\n\nMost Android lifecycle methods are called on the main thread. Think of these callbacks as simple messages sent to a looper queue.\n\nThis article wouldn\u2019t be complete without the reminder that goes into almost every Android dev article: Do not block the main thread.\n\nHave you ever wondered what blocking the main thread actually means? That\u2019s the subject of the next part!"
    },
    {
        "url": "https://medium.com/square-corner-blog/let-it-snow-ebfcc27cdd18",
        "title": "Let it snow! \u2013 Square Corner Blog \u2013",
        "text": "If you sent or received Square Cash recently, you might have noticed slight precipitation backdropping our email headers. It all started last week when we decided to do something special for the holidays. I reckon my teammates thought I was joking when I shouted out with glee, \u201cfalling snowflakes!\u201d But then I followed up with a working prototype. What says \u201cHappy Holidays!\u201d better than parallax-scrolling, alpha-composited, bokeh snowflakes, amirite?\n\nWhile the effect is simple, generating it was anything but. Hacking together a proof-of-concept took moments. Profiling and optimizing the rendering speed and file size required orders of magnitude more effort. We didn\u2019t draw this animation in Photoshop. I wrote a couple hundred lines of Java code instead. The challenges were many, but we Square engineers pride ourselves on our ability to push the envelope at every layer of the stack, up to and including email animations.\n\nMost email clients don\u2019t support HTML5. We\u2019re stuck with 90s era technologies, or in this case, animated GIFs. Obviously the animations should look crisp and beautiful, especially on today\u2019s high resolution displays. We render them at 2X resolution \u2014 more than 300k pixels per frame. The animations include custom text. This requires us to render them on the fly and poses some fun challenges. Once rendered, the animation needs to download and play quickly, even over slow mobile networks.\n\nTo start, I pored over the 25-year-old GIF spec, reading it backwards and forwards, looking for opportunities to cut down the file size without compromising the design. I used pngquant\u2019s best-of-breed Median Cut quantization algorithm to precompute an optimal 16-color palette. Restricting the animation to 16 colors enabled us to encode the images using 4 bits-per-pixel[^1]. It also resulted in slight posterization, promoting color repetition and further improving compression, particularly in the gradient at the bottom of the image. I even tried plotting only the pixels that changed between each frame, but to my surprise, the diffs resulted in more complexity, less repetition, and therefore worse compression than the full frames.\n\nThe biggest win came from an optical illusion aimed at reducing the total number of frames. The animation looks like it necessitates 240 frames, but it really only requires 60 (a 75% savings). The animation is composed of four layers of snowflakes, each meant to appear a different distance from the observer. As the layers get further away, the snowflakes get smaller, lighter, slower, and denser, resulting in the illusion of depth. Here are the separate layers ordered closest to furthest:\n\nThe trick is only the closest layer travels the full height of the frame. The lower layers travel only a fraction of the height, but they tile and repeat, giving the illusion of continuity. For example, during the course of the animation, the second layer travels only half way down the height of the image, but it repeats twice and looks like it scrolls continuously. The third layer travels one third of the way and repeats three times, and the fourth layer travels one fourth of the way and repeats four times. As you can see, this repetition becomes less evident when you layer the snowflakes on top of each other and animate them at different speeds.\n\nThe final animation clocks in at less than 650KB \u2014 **0.25 bits/pixel!** To see it in action, send some Cash to a loved one. And check out the final code below. Consider it my gift to you! Happy holidays.\n\n[^1]: It may sound counterintuitive, but using 4 bit-per-pixel can result in a larger file size than using 8 bits-per-pixel. Which encoding is better depends on which bit width produces more repetition in the bytes, and that depends on the nature of the image. The easiest strategy is to try both and see which results in a smaller file."
    },
    {
        "url": "https://medium.com/square-corner-blog/evenly-joins-square-bf0448365bb",
        "title": "Evenly Joins Square \u2013 Square Corner Blog \u2013",
        "text": "At Square, we are committed to creating a better experience for our merchants by helping them focus on their customers, rather than on the payment itself. From paying with your name with Square Wallet, to designing Square Stand for conversational ordering, we constantly optimize for customer experience.\n\nSo we\u2019re pleased to announce that the Evenly team is joining Square. Evenly\u2019s app made it easy for anyone to send or collect payments from friends, anywhere, anytime. But more importantly, the team showcased the importance of prioritizing experience over the technical aspects of the product itself. In their own words, \u201cLife is about sharing experiences, not splitting transactions.\u201d\n\nThe Evenly team will work on seller initiatives, bringing to Square the same focus on simplicity and design that they brought to their own app. We are thrilled to welcome them aboard."
    },
    {
        "url": "https://medium.com/square-corner-blog/securing-rubygems-with-tuf-part-3-44383319ac53",
        "title": "Securing RubyGems with TUF, Part 3 \u2013 Square Corner Blog \u2013",
        "text": "This is the third and final part of the Securing RubyGems with TUF series. Start with part 1 and part 2 if you haven\u2019t read it yet.\n\nIn the last post, we covered how The Update Framework (TUF) enables developers to securely sign for their code, protecting clients from installing maliciously modified gems. In this post we cover a grab bag of other features provided by TUF. It defends against many more attacks than I would have thought of by myself!\n\nIf an attacker gains access to the RubyGems server, it is trivial for them to prevent further updates from reaching clients by simply shutting down or otherwise crippling the server. This attack is highly visible though, since clients will start receiving errors.\n\nA more subtle attack is to serve a version of a targets file (such as or ) that contains references to gems with known security vulnerabilities. Note that the attacker cannot serve up arbitrary content since the targets file is signed by an offline key, but they can reuse an old one. There is no way for a client to know it is receiving stale information.\n\nTUF defends against this attack by requiring that all metadata files contain an expires header, after which the client must not trust that file.\n\nFiles like that are signed by the offline key need to have expiration dates rather far in the future. To enable updates to these files before they expire, TUF also provides a metadata file that expires every minute, and points to the latest versions of files. If clients always start with , they know they are getting fresh information. Note that is still vulnerable to spoofing if an attacker gains access to the server, but it protects against malicious mirrors and enables legitimate updates to long-expiring files.\n\nUpdating content on a file system, especially one distributed geographically by CDNs, is not atomic. This is a problem: if a client downloads a new version of a with an old version of a file it refers to, it will fail signature verification and look like an attack.\n\nTUF solves this by storing all mutable files with their digest in their file name. A new metadata file contains the mapping of filenames (without digest) to the current version.\n\nFor RubyGems, most files (such as the gem files themselves) are immutable so we do not need to include a digest in their file name. This is handy because it means everything is backwards compatible. The gem files are still available in the same location, so a non-TUF aware client can simply grab the file directly without going via the metadata. For the mutable index files, we write two copies of the file: one with a digest in the filename to support TUF, one without to support legacy clients.\n\nWith this in place, only needs to point to the most recent . The final system looks like:\n\nThis series has only scratched the surface of TUF. It also contains features for updating the offline key (in case of compromise), threshold signing (require that two developers out of three sign for a gem), further timeliness guarantees, denial of service defense, and optimizations for large numbers of files.\n\nOur implementation of TUF is heavily inspired (if not outright copied) from PEP 458: Surviving a Compromise of PyPI. That and the TUF specification are the best places to learn more. There is also a proof-of-concept TUF implementation for RubyGems where you can actually experiment with the concepts in this series.\n\nThere is still a lot of work remaining before we can roll this change out to the community, but the RubyGems and Bundler maintainers are onboard. Get in touch if you would like to help out."
    },
    {
        "url": "https://medium.com/square-corner-blog/securing-rubygems-with-tuf-part-2-7b4e709784f5",
        "title": "Securing RubyGems with TUF, Part 2 \u2013 Square Corner Blog \u2013",
        "text": "This is the second part of the Securing RubyGems with TUF series. Start with part 1 if you haven\u2019t read it yet.\n\nIn the last post, we covered how The Update Framework (TUF) protects clients from installing maliciously modified gems. In this post, we extend that system to allow developers to update their own gems. As a refresher, our current setup looks like this:\n\nIn our simplified example, RubyGems maintainers need to sign any changes to with offline keys. This clearly does not scale up to thousands of developers pushing thousands of gems. I don\u2019t want to require my gem to be re-promoted to by hand every time I push a new version! That would introduce an unacceptable window of opportunity for an attacker to compromise new versions of gems.\n\nTo support secure updating of verified gems, we need to allow developers to sign with their own offline keys. Let\u2019s work through the process for the first and second uploads of the cane gem.\n\nAfter building the gem, a developer also creates a metadata file for that gem. This file contains a digest of the gem file, and then is signed by the developer\u2019s private key.\n\nThey then push their gem, the metadata file, and their public key up to RubyGems. Since RubyGems has not seen this public key before, it is added to which is then re-signed with the online key. cannot be changed by the server, since it does not have access to the developer\u2019s private key, so it serves that up as-is.\n\nAt this point, these metadata files are still vulnerable to an attacker with access to the server since is signed by the online key. Note however that the server did not need to directly sign any of the files provided by the developer.\n\nDuring the regular manual update cycle by RubyGems maintainers, the delegation for cane will be moved from to . Recall that the latter is signed by the offline key.\n\nTo upload a new version of the gem, a developer adds a file entry for the new gem into the metadata, re-signs it, and uploads that alongside the new file.\n\nThe server can then replace its copy of with the new one. The contents of does not change, so it does not have to be re-signed. Viola! At this point, an attacker cannot provide a malicious version of cane without access to either the RubyGems offline key, or the developer offline key.\n\nLet me emphasize just how big deal this is: gem authors can publish gems, developers can verify that those gems are the same as when the author published them, no external key distribution mechanism is needed\u00b9, and the system is robust even if the main RubyGems server is hacked. All with no extra effort required by gem users and very little extra effort required by gem publishers.\n\nLet\u2019s walk through a client request for to see exactly how it is secured. (It will help to follow along on the diagram above.)\n\n2. Since is not listed in , the client fetches the first delegation: . This file is signed using the offline key, the public part of which we already have but is also provided in targets.txt for convenience.\n\n3. is not listed in either, so the process is repeated by fetching the first delegation to . is verified using the developer\u2019s public key ( ) provided in .\n\n4. is listed in , so we download it and verify its digest against the one listed in .\n\nAt every point along the way, we only ever trusted files that were secured using an offline key. Not too shabby!\n\nIn the next installment, I\u2019ll cover some more esoteric attacks on packaging systems and how TUF defends against them.\n\n\u00b9 The current RubyGems signing system requires gem authors to publish their keys somewhere, and for other developers to manually import those after locating them. This is too much work, so virtually nobody does it."
    },
    {
        "url": "https://medium.com/square-corner-blog/securing-rubygems-with-tuf-part-1-d374fdd05d85",
        "title": "Securing RubyGems with TUF, Part 1 \u2013 Square Corner Blog \u2013",
        "text": "Over the last week, a small team\u00b9 here at Square has been working on integrating The Update Framework (TUF) with RubyGems to help protect the community from compromises like the one that happened earlier this year.\n\nIn this series of blog posts, I aim to explain the fundamental concepts of TUF and how they apply to RubyGems. This is not a rigourous explanation or justification of the security properties of TUF (refer to the specification for that). Instead, it will equip you with the mental models needed to go exploring and learn yourself. I have simplified many of the details for sake of explanation.\n\nFundamentally, RubyGems is a file server. To download a gem, I first fetch the index ( ) to determine the latest version. From there, I construct a URL to download the actual gem ( ). If a malicious user gains access to the server, they can change the contents of these files to serve malicious code. TUF aims to defend against this attack and more.\n\nI\u2019ll start with a high-level overview of the system. The details are explained further down, so I recommend not spending too much time on this the first time around, and then coming back and reading it in detail after you\u2019ve read the rest of the post.\n\nTUF adds metadata files in parallel to the existing rubygems files. This metadata stores digests of the files which can be used to verify the files\u2019 contents. The trick is to ensure that this metadata can be securely delivered to the client.\n\nTo do this, TUF divides the metadata into different files called roles, and each role is asymmetrically signed by one of three types of keys: an offline key, an online key, or a developer key. Each key has a private and a public component: the private part is used to sign a role, and the public part is used to verify that signature.\n\nThe private part of the offline key is stored off the internet (\u201cair-gapped\u201d) by the RubyGems maintainers\u00b2. If the RubyGems server is compromised, the offline key is still safe. The private online key is stored on the RubyGems server, and so is not safe in the event of a compromise. (The need for this unsafe online key is covered below.) The private developer keys are stored securely by developers, and I\u2019ll cover how they fit into the picture in part 2.\n\nThe public parts of all keys are published in the metadata files. In addition, the public offline key is distributed with RubyGems. This is required to bootstrap the process; we need a known good key that is not told to us by the server. Otherwise a compromised server could just serve up a bad key and we wouldn\u2019t know the difference!\n\nLet\u2019s try serving up just a single file with a simplified version of TUF. The file tree looks like this:\n\nFirst, the client requests /metadata/targets.txt, which contains the following:\n\nThis document was created by a RubyGems maintainer, signed using the offline key.\n\nUsing the public offline key we have locally (distributed with RubyGems), we can verify that the signature is valid for the signed part of this document. This gives us confidence that someone with access to the offline key created this document. Now we can request and validate its digest against the one provided in this file. Even if an attacker gains access to the RubyGems server they cannot access the offline key since it is not stored on the server. While they can shut down the server, they cannot serve up malicious content.\n\nThat system is really secure, but consider what has to happen to add a new file. A maintainer needs to manually sign the new metadata file on their non-internet connected computer, then transfer that to the rubygems server. What a chore! Clearly this does not work for a system like RubyGems, where new files are being added all the time by many different people.\n\nTo get around this we could use the online key instead of the offline one, which is available on the RubyGems server and so can be used to automatically sign new metadata files whenever a new gem is pushed. The problem is that if the server is compromised, the attacker can replace the online key with their own version and use that to sign malicious versions of gems. We are not totally back to square one though; using an online key in this way protects us from any man-in-the-middle attacks, such as the compromise of a mirror.\n\nWe seem to be at an impasse. An offline key is the most secure, but requires human intervention to apply. An online key can be used by robots, but is more easily compromised. Can we get the benefits of both?\n\nWe can! TUF supports a concept called delegation. Rather than providing a list of files, a target role can instead provide a list of other targets to go and reference for files.\n\nNow to find , a client fetches and verifies , then starts iterating through its list of delegations. First it tries , which it verifies with the offline key, but it does not contain the file it is looking for. is next on the list, which is verified with the online key, and does contain the digest for .\n\nOn the server, any new files can automatically be placed in the recent role. At regular intervals, a person promotes all files in recent to verified by moving all the entries into and re-signing it with the offline key.\n\nIf the server is compromised, all files in are vulnerable to tampering; files that have been promoted to are safe.\n\nUsing this mixed strategy we get the benefit of offline-level security for old files, while still enabling new files to be handled automatically. Pretty neat!\n\nIn the next installment, I\u2019ll cover how TUF enables developers to securely update their own gems to help get RubyGems maintainers out of the loop.\n\n\u00b9 Myself, Tony, Neal, Justine, Charlie and Jim. Shoutout also to Justin Cappos and his students at NYU-Poly for providing examples for us to work off, answering our questions, and reviewing our code!\n\n\u00b2 Actually there are multiple offline keys. Each maintainer can have their own. A threshold number (usually three, but configurable) is required to adequately \u201csign\u201d anything. The same arrangement is also true for developer keys. I ignore this for simplicity in the main text, but it is an important feature of TUF."
    },
    {
        "url": "https://medium.com/square-corner-blog/pair-programming-interviews-part-ii-283489e4fb7f",
        "title": "Pair Programming Interviews, Part II \u2013 Square Corner Blog \u2013",
        "text": "You may have heard that Square conducts several pair programming interviews when evaluating full-time and intern software engineers. You may also kinda-sorta know what pair programming is, but maybe you\u2019ve never done it before. Or maybe you have but you\u2019re not sure if it\u2019s the same process at Square.\n\nWhat now? You really want to work at Square and you\u2019re pretty sure that you have the chops, but you\u2019re a little (maybe more than a little) nervous about the prospect of pair programming. Sitting at a computer, furiously writing code to solve a complicated problem, watching the clock\u2013all while being watched by a random engineer who has your Square career in his/her hands. What\u2019s not to love?\n\nFirst of all, take a deep breath. The Square engineers you will be programming with aren\u2019t malicious\u2013they are as eager to hire great engineers for Square as you are to be hired. They aren\u2019t going to make your interview easy, but they\u2019re not trying to trip you up with trick questions.\n\nEvery Square engineer has a favorite interview challenge (or two) that they\u2019ll ask you to solve, usually in smallish, incremental steps. The problem is interesting (and maybe even fun), or they wouldn\u2019t have chosen it! While your interviewer won\u2019t be as helpful as a true pair, they will help you get past trivial gotchas as you solve their problem. They\u2019re not interested in silly little things that you might have trouble with\u2013syntax errors, API details, etc.\u2013but in figuring out just what your skills are and how well they line up with our needs.\n\nYou\u2019ll get to pick your programming language and a development environment or editor that you\u2019re comfortable with. Your pairing machine will be a modern iMac\u2013if you aren\u2019t familiar with OS X or a flavor of *nix, you should probably practice up before your interview. Any mainstream programming language you\u2019re proficient with is likely installed\u2013C/C++, Java, Python, Ruby, even C# (using MonoDevelop). There are a number of IDEs and editors installed on your pairing machine, too\u2013Eclipse, IntelliJ IDEA and RubyMine, Xcode, vim, Sublime, TextMate, and probably a few more.\n\nIf you don\u2019t have much on-the-job programming experience (e.g., you\u2019re a recent college graduate or applying for an internship), you should make sure you are reasonably comfortable sitting down at your computer and writing a command line application (at least) that solves a problem or performs a task. If you aren\u2019t, all is not lost\u2013you can practice on your own and get there! If the vast majority of your programming consists of homework assignments and group projects, you may not get through your Square pre-interview screening, let alone your pair programming interviews!\n\nEven experienced programmers might benefit from looking up a modest coding challenge or two on the Internet and solving it. Project Euler, CodeKata, and Ruby Quiz are worth a look\u2013there are lots of others\u2013and the problems are interesting to solve in any programming language. The challenge that you\u2019ll work on in a Square interview will be bounded\u2013you\u2019ll have less than an hour to work on it!\n\nYou don\u2019t need to memorize every nuance of the standard library of your language. Just like in a normal workday, if you need to use an API that you\u2019re not familiar with, you can look it up or ask your interviewer. You may want to find and remember a site that documents your language in a way that you like, but even that isn\u2019t necessary.\n\nIf you want to work at Square, chances are you like programming and enjoy solving problems. Approach your interview challenge as you would any other problem and have fun! Your enthusiasm and passion\u2013almost as much as your skills\u2013will leave a positive impression on your interviewers."
    },
    {
        "url": "https://medium.com/square-corner-blog/viewfinder-joins-square-62b1570ee85b",
        "title": "Viewfinder Joins Square \u2013 Square Corner Blog \u2013",
        "text": "We are excited to announce that the Viewfinder team is joining Square. The team is incredibly talented, having built an app that blends beautiful design and highly technical engineering to create personal, human experiences. This too is our mantra at Square, and the Viewfinder team\u2019s expertise in building simple, elegant mobile applications will help us in our mission to make commerce easy for everyone.\n\nThe Viewfinder team will work on seller initiatives out of our New York office. As senior members of our East Coast team, they\u2019ll help expand our NYC presence and achieve our goal of tripling the size of our engineering team in the city within the next year.\n\nOn a personal note, I couldn\u2019t be more excited to work again with Spencer, Peter, Ben, et al. Spencer and Peter first crossed my radar in the 90s when they created GIMP, the popular open source image editor. We crossed paths again at Google where they kindled the Google NYC engineering office and spearheaded key infrastructure like Google Servlet Engine and Colossus, Google\u2019s exascale distributed \ufb01le system.\n\nThe team\u2019s passion, experience, and dedication to pushing the envelope makes them a perfect fit for Square, and we\u2019re thrilled to work with them."
    },
    {
        "url": "https://medium.com/square-corner-blog/culture-fit-7142f50ed79",
        "title": "Culture Fit \u2013 Square Corner Blog \u2013",
        "text": "At Square we talk a lot about our \u201chiring bar.\u201d If you\u2019ve ever interviewed here you know that we put a lot of work into each interview. You\u2019ll have a recruiting coordinator ensuring the schedule for your interview day (yes, it\u2019s a full day) is clear and reasonable, and your interviews stay on-time. At the beginning of the day you\u2019ll have time to talk to your future manager and relax (we know interviews are stressful). You\u2019ll then have the hardest (and most fun) part of the interview: three sessions of pair programming with a veteran engineer. When you\u2019re done, you\u2019ll have lunch with an engineer whose sole task is to help you relax from the interviews and ensure you get good food in your belly. We make sure to schedule down time because the interviewing itself can feel intense.\n\nInterviewing is very hard to do well. It\u2019s the job of the interviewer to collect enough data to make an informed decision and to do so in a dignified way that doesn\u2019t waste the interviewee\u2019s time. Failing to collect this data means the decision will be based on \u201cculture fit\u201d[1] because the brain likes jumping to conclusions based on limited data, as if other data didn\u2019t exist.[2]\n\nI\u2019m very proud to work at a company that has rejected brilliant assholes and hired a few junior \u2014 but enthusiastic \u2014 people. We want to bring in engineers who are enjoyable to work with, who level-up our teams, and who can contribute on day one. And we\u2019ve got about 5 hours in which to determine all this.\n\nSince the very early days of the company, our CTO Bob Lee has honed an approach to gathering hiring data using two methods:\n\nSquare does not believe in trick questions. How many gas stations are in the U.S.? Who cares? What\u2019s the ideal JVM garbage collector for an application with these characteristics? Oh, that\u2019s not your specialty? Sorry then, I should have asked a much better question.\n\nWe do, however, need to know if you\u2019ll be a strong contributor to the growth and success of Square so we need to measure your skill, but we need to do it in a non-bullshit way. The best way that we\u2019ve found is to pair-program with you for an hour. This means you and I will sit down at an iMac, we\u2019ll chat for a couple minutes so you can relax, then I\u2019ll describe a fun programming idea to you and we\u2019ll build it together. In other words: It\u2019s just like a work day.\n\nUnfortunately, coding sessions are not a good indicator for any skill other than the actual coding part of your job. So we have a second kind of interview where two Square engineers and yourself get together for 40 minutes and the Square engineers work to find your strengths. Rather than pushing you to prove that you don\u2019t make mistakes under pressure (spoiler: Squares make mistakes constantly) we look vigilantly for the kindness, creativity, integrity, and willingness to try difficult things that might have been missed during other parts of the interview. This will involve asking technical questions but that\u2019s just grist for the mill \u2014 we don\u2019t care if your answers are right as much as we care that the interactions between us are healthy and that we\u2019re getting a sense of your skill level.\n\nSquare has many diversity initiatives and several of us work full-time on ensuring we represent and attract a healthy mix of people. It would be so easy to undercut all of this if we took a less methodical approach to interviewing. We\u2019re naturally attracted to people just like us so the default is to hire everyone who reminds us of ourselves. But we believe that our best products are yet to be built and we need new, diverse perspectives and skills to help us create what Square will become.\n\nIf you enjoy building beautiful systems used by millions of people please join us! https://squareup.com/careers"
    },
    {
        "url": "https://medium.com/square-corner-blog/a-journey-on-the-android-main-thread-psvm-55b2238ace2b",
        "title": "A journey on the Android Main Thread \u2014 PSVM \u2013 Square Corner Blog \u2013",
        "text": "There is an article on coding horror about why we should learn to read the source. One of the great aspects of Android is its open source nature.\n\nWhen facing bugs that were related to how we interact with the main thread, I decided to get a closer look at what the main thread really is. This article describes the first part of my journey.\n\nAll Java programs start with a call to a public static void main() method. This is true for Java Desktop programs, JEE servlet containers, and Android applications.\n\nWhen the Android system boots, it starts a Linux process called ZygoteInit. This process is a Dalvik VM that loads the most common classes of the Android SDK on a thread, and then waits.\n\nWhen starting a new Android application, the Android system forks the ZygoteInit process. The thread in the child fork stops waiting, and calls ActivityThread.main().\n\nAccording to Wikipedia, a zygote is a fertilized biological cell.\n\nBefore going any further, we need to look at the Looper class.\n\nUsing a looper is a good way to dedicate one thread to process messages serially.\n\nEach looper has a queue of Message objects (a MessageQueue).\n\nA looper has a loop() method that will process each message in the queue, and block when the queue is empty.\n\nThe Looper.loop() method code is similar to this:\n\nEach looper is associated with one thread. To create a new looper and associate it to the current thread, you must call Looper.prepare(). The loopers are stored in a static ThreadLocal in the Looper class. You can retrieve the Looper associated to the current thread by calling Looper.myLooper().\n\nThe HandlerThread class does everything for you:\n\nIts code is similar to this:\n\nA handler is the natural companion to a looper.\n\nYou can associate multiple handlers to one looper. The looper delivers the message to message.target.\n\nA popular and simpler way to use a handler is to post a Runnable:\n\nA handler can also be created without providing any looper:\n\nThe handler no argument constructor calls Looper.myLooper() and retrieves the looper associated with the current thread. This may or may not be the thread you actually want the handler to be associated with.\n\nMost of the time, you just want to create a handler to post on the main thread:\n\nLet\u2019s look at ActivityThread.main() again. Here is what it is essentially doing:\n\nNow you know why this thread is called the main thread :) .\n\nNote: As you would expect, one of the first things that the main thread will do is create the Application and call Application.onCreate().\n\nIn the next part, we will look at the relation between the Android lifecycle and the main thread, and how this can lead to subtle bugs."
    },
    {
        "url": "https://medium.com/square-corner-blog/shuttle-powerful-cross-platform-localization-46abeda54bb0",
        "title": "Shuttle: Powerful cross-platform localization \u2013 Square Corner Blog \u2013",
        "text": "Internationalization is hard. It\u2019s a monumental effort to take a large codebase originally written for the United States and make it global. Square first faced this challenge with our launch in Canada, and an appreciable portion of the work was in localization.\n\nFollowing the flurry of furious coding and growing pains that defined our first foray into international waters, we set out to replace the hodgepodge of scripts and manual workflows left standing after the dust settled. These ad-hoc tools shipped our content between our engineers and translators, but it quickly became apparent that we would not be able to preserve our fast development cadence without major changes.\n\nEnter Shuttle. My team was given the tall order of building a tool that would cure frustrations shared by both developers and translators, a single platform where developers could easily import strings from their projects, and where translators could do their jobs as efficiently and accurately as possible. It was a formidable challenge, but we felt empowered by the lessons learned from our first tentative steps towards becoming an international company. We would get it right the second time.\n\nShuttle is a website for developers and translators. After importing their projects, developers submit revisions (commits) for translation. Shuttle imports these commits, scanning each source file for localizable strings. Shuttle recognizes localizable content in many platforms, including Ruby on Rails, Cocoa/iOS, Android, Ember.js, and Java. These strings are parsed and imported into Shuttle\u2019s database of translatable content.\n\nEach string is associated with its unique, platform-specific key. Shuttle searches for prior translations of these keys, and any remaining untranslated strings are made available for translation.\n\nTranslators are presented with a user interface designed to help them be as fast and accurate as possible. They choose a commit to translate, based on its priority, due date, and the amount of work required. The translator workbench displays strings requiring translation, and exposes a suggestion engine powered by Shuttle\u2019s glossary and translation memory.\n\nReviewers who are experts in their respective languages then review these translations for accuracy, and approve them when they are ready. Reviewers also populate the glossary, ensuring that multiple translators share a consistent voice.\n\nOnce all pending translations are completed and approved, the commit \u201cgoes green\u201d and is ready for deployment. Shuttle provides API endpoints that ship archived manifests of translated strings to the client. Developers write build scripts that download these archives and integrate them into the deploy artifact. The artifact is then shipped off for deployment or release.\n\nDevelopers can submit as many commits for translation as they wish at any point, involving the translators early in the development process. A merge commit that lands a feature onto the release branch should \u201cgo green\u201d immediately, as it adds no additional strings, and therefore is ready for deployment/release.\n\nShuttle is built around the philosophy of software development we employ here at Square: It doesn\u2019t get released until it\u2019s finished. Here it is not acceptable that a new feature would be released without being fully translated and approved in every supported language; we vowed to give our international customers the same polished experience that our U.S. users enjoy.\n\nShuttle is intended for projects with in-house translation, or those that contract individual translators or translation houses. Shuttle has no support for computerized translation, as it is not yet something we utilize at Square.\n\nMore information about Shuttle and a tutorial video are at useshuttle.com. The Shuttle source code is available on GitHub. The README and the developer guide have all the information you need to install Shuttle and integrate it into your workflow.\n\nShuttle is production-ready but it\u2019s far from complete. We are constantly striving to improve the quality of our translations by improving our translator workbench. In the future we will be adding an advanced suggestion engine, allowing translators to view similar past translations to help improve the speed and consistency of their translations. We will also be adding support for \u201cexplosions\u201d \u2014 bifurcating strings depending on a language\u2019s pluralization and gender requirements.\n\nCurrently at Square we use different internationalization libraries for each programming language we develop in, and each of these supports different features, such as translator context or language-specific pluralization rules. In the future we hope to normalize this by providing our own cross-platform library with support for a wide range of grammar rules.\n\nAt Square, Shuttle has vastly improved developer happiness, boosted translation accuracy and throughput, and significantly reduced the pain involved in launching in a new country. If you have similar internationalization goals, then I encourage you to give Shuttle a try."
    },
    {
        "url": "https://medium.com/square-corner-blog/parliament-a-threshold-secret-sharing-service-db86e4a29bd3",
        "title": "Parliament: A Threshold Secret Sharing Service \u2013 Square Corner Blog \u2013",
        "text": "High-value secrets need proper protection. A well-established scheme to achieve this is secret sharing, which splits a secret into many parts and reveals nothing about the secret unless a quorum of shares is provided. This technique is used during key ceremonies. To secure a private key, the key is split and shares are distributed to several individuals. When the key is needed, all the individuals need to come together and provide their share. Depending on how often the secret needs to be accessed, the process of getting shareholders together can be costly and time consuming. It is also necessary to make sure the person who originally generates the shares, (and the person who combines the shares), does not retain a copy.\n\nAt Square, we protect signing keys, encryption keys, and other secrets with a secret sharing scheme. When access is requested, it is critical that shareholders can grant access to their share without them needing to be physically present \u2014 all the while maintaining a reasonable level of access control around the shares. For this reason we developed Parliament, a service that manages shares on the shareholder\u2019s behalf.\n\nInternally, Parliament uses Shamir\u2019s threshold secret sharing scheme and encrypts every user\u2019s shares under their public key. When shareholders decide to grant access to a secret, they submit their private key to Parliament and allow the service to temporarily decrypt their share and recompute the secret.\n\nIn the case of an attacker gaining temporary access to Parliament, only the secrets that are \u201cunlocked\u201d at the time of the attack are compromised. All other secrets remain confidential. In the following, we describe the cryptographic details of Parliament. Thank you to our advisor, Dan Boneh, for the useful insights and review of the protocol.\n\nParliament uses a threshold-based system where at least \\( k \\) out of \\( n \\) parts are required to reconstruct the original secret. Cryptographically, fewer than \\( k \\) parts combined reveal nothing about the secret; whereas the secret can be constructed deterministically from at least \\( k \\) parts. Parliament distributes shares so that at least \\( k \\) users need to give their approval before anyone can gain access to a secret. The threshold \\( k \\) can be set as a parameter when splitting a secret depending on the security requirements.\n\nParliament enables secret sharing of secrets with arbitrary lengths. Because computations for secret sharing require a prime modulus that is larger than the secret, (and it is preferable to use a fixed value for the prime modulus), Parliament implements arbitrary-length secret sharing on top of fixed-length secret sharing. In detail, Parliament first encrypts the secret \\( s \\) with a randomly generated symmetric key \\( k_s \\) (using AES256/GCM, which has a constant length). The result of this step is the encrypted secret \\( t=enc(s, k_s) \\) and the secret decryption key \\( k_s \\).\n\nThe secret decryption key \\( k_s \\) is then split into \\( n \\) shares using Shamir\u2019s method and discarded afterwards. Shamir\u2019s secret sharing involves generating a random polynomial \\( f(x)=a_0+a_1x+a_2x^2+\u2026+a_{k-1}x^{k-1} \\) with \\( a_0=k_s \\), \\( a_i \\in [0; p-1] \\) chosen at random for \\( i=1, \u2026, k-1 \\), and \\( p \\) is prime. All computations involving the polynomial are made modulo \\( p \\) and the first coefficient corresponds to the secret decryption key. Because the length of \\( k_s \\) is 256 bits, the prime \\( p \\) is chosen to have an effective bit length of 257 bits so that \\( 0 \\le k_s < p \\).\n\nEach share \\( u_j= (x_j, y_j=f(x_j)) \\), \\( j=1, \u2026, n \\) corresponds to a point on the polynomial curve, where any \\( x_j \\) can be public, but \\( y_j \\) should be kept secret by the owner of the share. There are multiple strategies to pick \\( x_j \\). We chose to draw \\( x_j \\in [0; p-1 ] \\) at random in order to simplify the generation of additional shares at a later point in time. Sequence numbers or the hash of a user identifier are other conceivable values for \\( x_j \\), but such strategies would require bookkeeping of the highest assigned sequence number, or would not allow more than one share per user.\n\nAfter the previous step, the initial secret sharing is complete with the encrypted secret \\( t \\) and the \\( n \\) shares \\( u_j \\) as the output. These shares can now be distributed to users, whose responsibility is to keep them secret. Of course, when access to the secret \\( s \\) is needed, users could choose to designate a trusted party that collects the shares and recombines them in order to gain access to the decryption key \\( k_s \\) and, eventually, decrypt the secret \\( s \\). However, this approach has several drawbacks, including (1) requiring users to manage their shares themselves; every time a new secret is added to the system or a secret sharing instance needs to be replaced, users would need to update their list of shares. And (2), every time a user is designated as the trusted party, that user is gaining access to the plaintext secret and could secretly keep a copy of it for unauthorised subsequent use.\n\nTo address these issues, we implemented Parliament to manage the shares on the shareholder\u2019s behalf and store them in an encrypted way. We use public key cryptography so that Parliament can add shares by encrypting them with a user\u2019s public key without the need for any user interaction. When a user, (in our case one of our security engineers), wishes to unlock a share to grant access to a secret, they provide Parliament with the corresponding private key. In principle, Parliament could have sent each share to its owner so that they decrypt it on their own, but this is impractical. As a security/usability trade-off, we decided that instead the user would send the secret key to Parliament and trust Parliament not to record the private key, decrypted share, or secret. That is, our design provides a reasonable protection against attackers with sporadic access to Parliament\u2019s memory or database, but not against malicious code modification or administrators with permanent access to the machine running Parliament. This is not unlike the trust placed on the machine that combines the shares in any secret sharing algorithm.\n\nInternally, the shares are encrypted using a hybrid encryption scheme: The secret share value \\( y_j \\) is encrypted with a randomly generated symmetric key \\( k_{y_j} \\) (using AES256/GCM): \\( v_j=enc(y_j, k_{y_j}) \\); the public value \\( x_j \\) remains unencrypted. The key \\( k_{y_j} \\) is then asymmetrically encrypted with the user\u2019s public RSA key \\( k_j^{pub} \\) (using RSA2048/OAEP): \\( w_j=enc(k_{y_j}, k_j^{pub}) \\). In its database, Parliament stores the encrypted secret \\( t \\) along with a list of the users \\( j \\) holding a share of the corresponding decryption key, and for each user \\( j \\), Parliament stores the encrypted share \\( v_j \\) along with \\( x_j \\), the public part of the share, and the asymmetrically encrypted share decryption key \\( w_j \\).\n\nWhen a user or an application requests access to a secret, Parliament will search its database to determine which users own a share and ask them to review the request. If a user approves, the user submits their private key to Parliament. Parliament decrypts the user\u2019s share, caches it locally in volatile memory, and erases the user\u2019s private key from memory. As soon as a threshold of \\( k \\) decrypted shares is reached, the authorised application can request the secret and Parliament will derive it from the decrypted shares. For security reasons, Parliament never caches the decrypted secret, but instead reconstructs it from the cached unencrypted shares. Also, requests for secrets time out after a configurable time interval, which will cause the corresponding unencrypted shares to be purged from memory.\n\nIn detail, when a user submits their private key \\( k_j^{priv} \\) to grant access to their share, Parliament uses it to decrypt the encrypted share decryption key \\( w_j \\) to obtain \\( k_{y_j}=dec(w_j, k_j^{priv}) \\) and then decrypts the secret share data \\( y_j=dec(v_j, k_{y_j}) \\). Once a threshold of at least \\( k \\) decrypted shares \\( u_j=(x_j, y_j) \\) has been reached, Parliament uses Lagrange interpolation to derive the secret sharing polynomial \\( f(x) \\) from the shares. The secret decryption key \\( k_s \\) corresponds to the value \\( f(0) \\), thus Parliament computes \\( f(0)=\\sum_{l=0}^{k-1} y_l \\cdot \\prod_{l \n\ne m} \\frac{x_m}{x_m-x_l} = k_s \\). With \\( k_s \\), Parliament decrypts the secret \\( s=dec(t, k_s) \\) and makes it available to the requesting application or user.\n\nWhen a new user gains the right to grant access to a secret, an additional share needs to be generated for that user. It is possible to generate additional shares for an existing secret sharing instance by re-deriving the polynomial from the existing shares and evaluating the polynomial at a random point \\( x\u2019 \\), as long as the threshold \\( k \\) remains the same. Changes to \\( k \\) require a new polynomial of higher or lower degree, thus new shares for all authorising users.\n\nUnder certain circumstances, shares need to be revoked. This can happen when a share has been disclosed, or when an employee is no longer authorised to approve requests for a given secret. In such a case, we generate a new secret sharing instance for the same secret and replace the old shares. Because we use a random symmetric key ( \\( k_s \\) ) to encrypt the secret to be shared, two successive invocations of the secret sharing procedure on the same secret will produce two sets of shares that are incompatible with each other. The encrypted secret \\( t \\) is replaced, too, which means that the old shares cannot be used any more to gain access to the secret \\( s \\), unless there exists a backup of \\( t \\). For this reason, high-value secrets may justify the additional effort of changing \\( s \\) altogether instead of changing only the shares.\n\nThere are a few attacks that Parliament does not defend against because we consider them out of scope. An attacker could replace each user\u2019s public key with a value that is under the attacker\u2019s control, and for which the corresponding private key is known to the attacker. When Parliament encrypts new shares with the attacker\u2019s public key, the attacker can unlock all shares and gain access to the secret. Similarly, an attacker could replace all shares of a secret sharing instance with shares of another, possibly older and revoked instance, which would cause legitimate users of Parliament to use an outdated or incorrect secret. While the former attack could be devastating and leak secrets, the latter attack would most likely result in denial of service only. Both attacks require write access to the database. They highlight that in addition to a secure execution environment for the Parliament service, its database must be protected against unauthorised modifications, too. This assumption is common to many centralised services (such as LDAP) and we believe that it provides much better security than a decentralised approach with its usability issues and potentially lower adoption. Note that because we are using the Galois Counter Mode (GCM) when we symmetrically encrypt secrets, its built-in integrity check allows us to detect shares that do not belong to the same set, or shares that have been tampered with.\n\nWe implemented Parliament as a Java library using Bouncy Castle and BigInteger arithmetic, and on top of it a stand-alone command-line interface and a service interface. We are in the process of making the library and command-line interface available as open-source software."
    },
    {
        "url": "https://medium.com/square-corner-blog/hardware-at-square-b92cc63c70c4",
        "title": "Hardware at Square \u2013 Square Corner Blog \u2013",
        "text": "At Square, we are developing hardware in an industry that has seen very little innovation in more than 50 years.\n\nOur first Square Reader was soldered together in our original office, Jack Dorsey\u2019s apartment. Five years and three major iterations of Square Reader later, we continue to lead all electrical, mechanical, and firmware design and engineering in-house, under one roof. (We even have a dedicated hardware lab in our new headquarters.) For each new version of Square Reader, we focus on creating a beautifully designed product with better swipe accuracy, while keeping costs low, and the reader small and power efficient.\n\nWhile our design philosophy has stayed constant, the growth of our company has presented new opportunities and technical challenges for our hardware team. This past July we launched Square Stand, a beautiful piece of hardware for brick and mortar businesses that turns an iPad into a complete point of sale. Building Square Stand required close collaboration between teams to integrate Square Register software and Square Stand firmware. For months before launch, software and hardware engineers sat side-by-side refining and testing daily. Like Square Reader, Square Stand is modern and minimal. Nothing is there that doesn\u2019t have to be there.\n\nYou can learn more about our hardware team \u2014 from our vision, to design and engineering challenges \u2014 in the video below:\n\nFor more information about hardware careers at Square, check out squareup.com/careers/engineering."
    },
    {
        "url": "https://medium.com/square-corner-blog/how-to-win-at-career-fairs-7b60a338483",
        "title": "How to win at career fairs \u2013 Square Corner Blog \u2013",
        "text": "College students! Especially those of you in the programming-type vocations!\n\nIt\u2019s recruiting season again \u2014 and by \u201crecruiting season\u201d I mean \u201cthe school year.\u201d If you\u2019re smart, you\u2019re looking for work, either for next summer or for when you graduate. (Funny story: when I was in college, my advisor famously addressed a room full of first year \u2014 make that first week \u2014 undergrads and told us in no uncertain terms to go to the upcoming job fair and talk to every company possible.)\n\nI\u2019ve just returned from three career fairs at three awesome universities on the recruiting side of the equation. After meeting and collecting a r\u00e9sum\u00e9 from several hundred bright-eyed engineering students, some differentiating factors have stood out.\n\nHere\u2019s some unsolicited advice on how you can stand out in a field of a hundred people who are all really talented.\n\nWe\u2019re looking for people who live and breathe code. People who, when faced with a challenge they couldn\u2019t solve by plugging together libraries, buckled in and wrote something new and then released it on GitHub.\n\nYou probably had great success at your last internship learning how to use the query analyzer to speed up the performance of a website by 8 percent, but it\u2019s a lot more impressive if you went and built the query analyzer from scratch because it didn\u2019t already exist.\n\nNo joke, having your GitHub profile URL on your r\u00e9sum\u00e9 is an instant bonus. Especially if there\u2019s something there beyond just the blank profile.\n\nOne page is enough for your r\u00e9sum\u00e9. Really. If there\u2019s something that doesn\u2019t fit on the first page, it\u2019s probably not important enough to still be on there.\n\nIf you hand me a stack of four pages, not only am I going to think you\u2019re not very organized, I\u2019m also going to get distracted reading through the whole thing while you\u2019re trying to talk to me.\n\nOf the 200 people we talk to at your career fair, somewhere between 10 and 30 of you will get an interview. You have, depending on how long the lines are, anywhere from 45 seconds to four minutes to convince me that you should be the number one lead on our list of people to call and interview.\n\nContrary to popular controversy, your accent doesn\u2019t matter. It really doesn\u2019t matter if English is your native language or not.\n\nHere\u2019s what does matter: speaking clearly. (Don\u2019t mumble.) Telling me about why you\u2019re awesome instead of just reading your r\u00e9sum\u00e9 to me. (I\u2019ve noticed that the people who do this are usually the ones with four pages to get through.) Highlighting extracurriculars instead of your classwork. Telling me what you built last summer instead of where you worked.\n\nWhen I ask you if you have any questions for me or what I can tell you about Square, don\u2019t tell me what Square does. I dare say I know what Square does. Ask me for the information you don\u2019t already have.\n\nDon\u2019t walk up to an employer and ask if they\u2019re hiring. Would they be attending your career fair if they weren\u2019t hiring?\n\nAt least for today, the supply of programmers is steadily increasing. But the demand \u2014 the number of available jobs for them \u2014 is increasing quite a bit faster still.\n\nAny tech company you talk to will be hiring as many people for all positions as they possibly can. And if the company isn\u2019t in the tech industry but usually hires programmers, it\u2019s simple enough to do some background research and find out for yourself. Even if the company doesn\u2019t have an open job requisition for somebody of your background, there\u2019s no harm to be done with walking up to a recruiter, introducing yourself, and making it known you want to work at his or her company.\n\nOn a related note, don\u2019t go out there calling yourself a \u201cweb developer\u201d or a \u201cJavaScript programmer\u201d or a \u201cfront-end guy\u201d or a \u201cgame developer.\u201d You\u2019re going to school for a CS degree, for crying out loud. You\u2019re paying a bunch of money for a piece of paper that tells the world you\u2019re a programmer who can figure anything out. Talk the talk. (Walking the walk comes later on, during the interview.)\n\nLots of standard guides tell you to dress up. This may or may not be a good idea: if you and everybody else is wearing a suit, you\u2019ll still be just a face in the crowd. And if you don\u2019t know how to wear a suit \u2014 rocking an ill-fitting shirt or a mismatched tie or an overly-long pair of pants, for example \u2014 you\u2019ll come off as a bit of a slob even though you aren\u2019t.\n\nOn the other hand, if you have that perfectly tailored number in your closet, it will give you a boost of self-confidence and you\u2019ll feel like a million bucks.\n\nWear what you\u2019re comfortable in and what best represents you, unless it\u2019s going to offend or scare the people you\u2019re meeting. That plaid button-down and nice pair of jeans is going to make a much better impression than your one dress shirt that\u2019s three sizes too big.\n\nBeyond the clothes, look at what your r\u00e9sum\u00e9 says about you, too. That default Word template tells me you\u2019re boring. (You don\u2019t even have to be a designer! Just pick a Word template other than the standard one!) Spelling errors will make a bad situation worse. I promise, you\u2019re not really a \u201cdesginer.\u201d\n\n(On the other hand, if your r\u00e9sum\u00e9 lists\n\nas one of your skills with the correct orthography, it tells me that you\u2019re a nerd. And there\u2019s absolutely nothing wrong with that.)\n\nAt the end of the day, it\u2019s still inevitable that you\u2019re up against all of your peers in your program. You\u2019re competing with people who are likely your friends; you have the same educational background as they do and every one of you is incredibly smart.\n\nWith a little bit of thinking about who it is you\u2019re presenting to the recruiter at that career fair, you can be the one with your email box overflowing and the one in the tough spot with a dozen offers to choose from.\n\nHopefully, one of those offers is from Square. Needless to say, we\u2019re hiring a whole lot of people to tackle ambitious technical challenges and help us invent the future of commerce. Join us today at squareup.com/campus."
    },
    {
        "url": "https://medium.com/square-corner-blog/squares-open-approach-to-code-c94c04a9feae",
        "title": "Square\u2019s Open Approach to Code \u2013 Square Corner Blog \u2013",
        "text": "Nearly 3 years ago we open sourced our first project at Square: Retrofit. Since then we\u2019ve released more than a quarter of a million lines of code in more than 60 open source projects \u2014 a project, on average, every 2.5 weeks.\n\nAs a member of the open source community \u2014 and a company that\u2019s benefited from many open source libraries \u2014 we have a responsibility to pay it forward. Our open source efforts also benefit our customers: making internal work suitable for public consumption requires an extra layer of polish that helps to improve our products across the board.\n\nSo thank you to everyone who has submitted patches to our open source projects. We have much more to contribute.\n\nIf you\u2019re interested in learning more about open source at Square, you can read about our efforts here. For more information about the broad technical challenges we face at Square, visit squareup.com/careers."
    },
    {
        "url": "https://medium.com/square-corner-blog/the-square-tweetwriter-36df94de438",
        "title": "The @Square Tweetwriter \u2013 Square Corner Blog \u2013",
        "text": "Each quarter, Square sets aside a full week for everyone in the company to build something they believe should exist in the world and transform ideas into working prototypes. Hack Week is an opportunity to create and innovate in the exact same way Square itself was formed: 3 people in a tiny room going after a big problem. Sometimes these ideas turn into new products; other projects serve as learning exercises. The @Square Tweetwriter project was somewhere in between.\n\nSquare\u2019s office is littered with monitors that display various dashboards and metrics. We call them inforads (\u201cinformation radiators\u201d), and they display everything from transaction visualizations, to system health, to our @Square Twitter feed. We like inforads because they provide us with useful information and give us an opportunity to experiment with visualization technologies.\n\nDuring a Hack Week earlier this year, I worked on an inforad called the Odominator, a mechanical counter designed to track our Gross Payment Volume. Since then our Twitter inforad started acting ornery, and the display was reappropriated for introducing new employees to the company. As our next Hack Week approached, I stumbled across a typewriter in my garage that I\u2019d modified to work as a teletype, and a new Twitter inforad was born.\n\nCreating a program to read Twitter data using the search API is straightforward with the help of Python and rauth, but I didn\u2019t want to commit a machine to power a typewriter. Using a Raspberry Pi emerged as the best solution given both cost and availability. Combined with a USB wireless adapter, the Pi can do its job from inside the typewriter.\n\nAll that was left was connecting the two. The typewriter exports a USB interface (providing a serial port) which understands a certain character set that corresponds to the keyboard. This is more complicated than just transmitting ASCII because typewriters (being optimized for writing and not for computing) have keys for characters like \u00a7, \u00b6, \u00bd and \u00bc while lacking the ability to print curly braces or a caret. The Python script replaces all unrecognized characters with question marks, and rate-limits its serial traffic to 0.1s per character and between 0.2s and 2.2s for carriage returns, based on how far the head has to move to get back to the beginning of the page.\n\nThis is all coordinated by an init script, so the typewriter resumes automatically after power strip mishaps.\n\nI expected there would be a lot more action than usual on Square\u2019s Twitter feed once news of this thing spread, and naturally the first tweets afterward were intended to break it. Here at Square, we test our code exhaustively. Here\u2019s Kyle testing the typewriter\u2019s emoji support:\n\nNaturally it wasn\u2019t long before other people started abusing it as well, for the benefit of everyone watching.\n\nFor the curious, the code powering the tweetwriter is available. In true Hack Week tradition, it is not without its bugs \u2014 the typewriter sometimes doesn\u2019t notice a key was repeated and prints out a single character instead of two \u2014 but it works well enough that the typewriter been stationed by our pool table. Unlike our other inforads, it does require some regular maintenance, such as recentering the paper in the feed (every few days), replacing the ink ribbon (every few weeks), and replacing the continuous-feed paper (every few months)."
    },
    {
        "url": "https://medium.com/square-corner-blog/pair-programming-interviews-7a34168e43eb",
        "title": "Pair Programming Interviews \u2013 Square Corner Blog \u2013",
        "text": "My name is Parth Upadhyay and I\u2019m a rising senior at the University of Texas at Austin, majoring in Computer Science. This summer, I interned at Square on the Register team.\n\nWhen I first started exploring internship opportunities at Square, I didn\u2019t know a lot about the engineering culture at Square \u2014 that is until I came in for pair programming interviews. Previously I had only experienced a more standard interview process. Someone would sit across the table from me and go through the motions of grilling me on my resume and asking me pre-baked data structures questions whose answers had become rote review. These interviews felt like tests, and more than that, they felt orthogonal to the work that I would actually end up doing.\n\nSquare\u2019s interview, however, was really different. The first thing we did during the interview was spend time setting up a development environment on the pairing machines. My interviewer asked me to take my time and set up the machine to my liking.\n\nI was confused. I thought I was going to sit across the table from someone and return rehearsed, stock answers to their standard interview questions. Instead I sat next to someone on a real computer and wrote real code that ran. On top of that they actually wanted me to spend time setting up my own development environment.\n\nSensing my confusion towards this foreign interview style, he explained that he wanted me to be comfortable coding on the machine so we could actually focus on the problem we would solve, and not have to worry about the cruft surrounding it. He was completely right! Coding in an unfamiliar environment is kind of like running a race with your shoes untied; you\u2019re spending so much time tripping over yourself that it\u2019s hard to even show off what you\u2019re capable of.\n\nAfter setting up the environment, I did 3 pairing interviews with 3 different Square engineers. They would usually start off the problem by providing some of the boilerplate code, (sometimes even write tests), and then we would work through it together.\n\nThese interviews stood out because I was coding just as I would in the real world. I got to sit at a computer and just program \u2014 working through the problem, bouncing ideas off and asking question of the engineer who was interviewing me, googling syntax and APIs that I had forgotten, and actually running, testing, and debugging the code until it worked. I didn\u2019t even have time to be stressed during the interview process because I was having fun doing what I loved: coding.\n\nThis unique interview experience gave me insight into the engineering culture at Square, where collaboration is core. Working at Square has been everything, and more, than I thought it might be based on my interview experience. It\u2019s been an incredible summer working side-by-side with people who are pushing the boundaries, both in tech and in commerce, and I\u2019ve learned so much from my team \u2014 not only about programming but about workflow, time management, product design, everything. And I got my first taste of all of it throughout the interview process."
    },
    {
        "url": "https://medium.com/square-corner-blog/kochiku-ci-for-long-test-suites-15203601a1e6",
        "title": "Kochiku: CI for long test suites \u2013 Square Corner Blog \u2013",
        "text": "Back in the summer of 2011, every request to Square\u2019s servers went through a single monolithic Rails application we lovingly call SquareWeb. At the time, SquareWeb had hundreds of models and controllers, all with tests to cover their behavior. In addition to the many unit tests written with RSpec, we had hundreds of acceptance tests written with Cucumber \u2014 some of which used Selenium to open a web browser and step through long flows.\n\nDue to its size, the SquareWeb test suite took between two and three hours to run on CI. If a single flaky test failed, we retried the entire build, waiting hours more. Our engineers stopped running through the entire test suite before submitting code. Not good.\n\nWe know Rails tests don\u2019t have to be slow (at least not the unit tests); Corey Haineshas done a great job of publicizing methods for removing the Rails framework from the flow of a test so that only the business logic runs. Unfortunately, we were learning about these techniques after two years\u2019 worth of code had already been written. We had a problem that we needed to address right away, or we were going to have to abandon our test suite.\n\nIn July of 2011, we started working on a test system that would automatically partition our build into distributable chunks. We named it Kochiku, after a Japanese word for \u201cconstruction.\u201d It works like this:\n\nWe started with a cluster of 10 Mac minis to serve as our build workers. Toward the end of last year, we realized we needed more capacity, so we expanded into EC2. Today we have 225 spot instances on EC2 that serve as build workers. This may sound like a lot, but Kochiku now runs the test suites for over 65 different repositories developed by over 150 engineers who believe strongly in automated test suites. It\u2019s used at Square for Java, Javascript, Ruby and Rails codebases.\n\nKochiku has been a big boost to productivity for engineers at Square. In addition to acting as the CI server that automatically kicks off a new build whenever someone pushes to the master branch, Kochiku starts a new build for every pull request that\u2019s created and reports the status back to Github using their status API. Engineers can also initiate a build on their topic branch at any time, since it\u2019s important to know whether tests pass before issuing a pull request.\n\nIn 2011, there were a few tools around for running a test suite in parallel. Some that we looked at were hydra, parallel_tests, and a Jenkins plugin or two. We evaluated each of them but didn\u2019t get the outcome we needed. Both parallel_tests and hydra were designed to parallelize test suites by using multiple cores of a single machine, or by splitting the tests across just a handful of machines. We needed something that would scale to dozens of machines and support a variety of platforms.\n\nAt the time, Travis CI was growing in popularity. We think Travis is a great project, and we realized what we really needed was something like Travis, except built specifically to partition a test suite into many pieces. We considered building the functionality that we wanted into or on top of Travis, but we decided instead to tailor an app to meet Square\u2019s needs without extra cruft.\n\nHad Travis Pro existed back in 2011, we probably could have made that work, but it wasn\u2019t until late 2012 they announced the ability to parallelize test suites with folder partitioning. Folder partitioning is somewhat restrictive, however, because it assumes that you don\u2019t have a single folder with a lot of slow tests, and it limits your options for organizing your code.\n\nKochiku has some powerful features that differentiate it from other CI software packages out there. Going forward, we want to develop these to their full potential:\n\nWe\u2019ve been quietly incubating Kochiku inside of Square for the last two years, and we\u2019re excited to release it as an open-source project today. We\u2019re looking forward to incorporating new ideas and contributions from the community.\n\nThe source code is available on Github at square/kochiku and square/kochiku-worker."
    },
    {
        "url": "https://medium.com/square-corner-blog/squares-code-camp-for-high-school-and-college-women-engineering-students-4c04d24d55cc",
        "title": "Square\u2019s Code Camp for High School and College Women Engineering Students",
        "text": "Last year I participated in Square\u2019s inaugural College Code Camp, a four-day immersion program for women engineering students from all over the United States and Canada. The program was built around four (we like the number four a lot at Square) foundational goals: leadership, community, engineering ability, and career preparation. These goals were made concrete through a variety of activities ranging from screenprinting T-shirts at a local Square merchant, to meetings with Square executives, to crash courses in Ruby, iOS, and design.\n\nThe activities were fantastic, but the best part was (without question) the people involved in the program. The other young women who participated were some of the most impressive people I\u2019ve ever met, and we all still keep in touch. The engineers I met during Code Camp continued to serve as mentors after I joined Square full time.\n\nCode Camp taught me to speak up, to not be afraid to ask questions, and to be more confident in my abilities. As a woman in computer science who came to the field late, I always felt like I was somehow \u201cbehind.\u201d Life\u2019s not a race, and the most valuable asset that anyone can have is an eagerness to keep on improving. My advice to anyone, not just women in engineering, is to always search for a place that will help you keep growing professionally and as a person. Code Camp made it clear to me that Square was that place for me.\n\nToday we announced our second annual College Code Camp, and our first annual High School Code Camp. High School Code Camp will be an eight-month program designed to prepare young women in San Francisco for the AP Computer Science exam. Participants will follow a curriculum created and led by Square engineers, which includes lectures, pair programming sessions, and hands-on workshops building mobile applications for Android.\n\nWe\u2019re now accepting applications for College Code Camp, and applications for High School Code Camp. Both programs will be held at Square\u2019s San Francisco headquarters.\n\nI\u2019m looking forward to meeting and mentoring this year\u2019s participants!"
    },
    {
        "url": "https://medium.com/square-corner-blog/fly-vim-first-class-610f1a39b572",
        "title": "Fly Vim, First-Class \u2013 Square Corner Blog \u2013",
        "text": "Engineers at Square use a wide variety of code editors: Sublime, IntelliJ, Xcode, and Vim are among the most popular. Over time, the Square Vim enthusiasts have compiled settings, shortcuts, and plugins into a single repo we lovingly call Maximum Awesome, which we are open-sourcing today! We want anyone running OS X to be able to get up and running with Vim in minutes.\n\nWe pair program often (but not always) at Square. It\u2019s a great way to tackle complex problems, onboard new teammates, and try out new ideas. By using the same base config files, there\u2019s no need to relearn shortcuts when using someone else\u2019s computer \u2014 it\u2019s all standardized! This reduces a lot of friction and allows us to focus on the code.\n\nMaximum Awesome comes with many of the features you expect from a full IDE: syntax highlighting, code completion, error highlighting, etc. But it doesn\u2019t stop there! To get things started, here are just a few of my favorite plugins and shortcuts:\n\nThere are also some components included beyond Vim. Maximum Awesome comes with iTerm 2, a replacement for Terminal, a tmux config, and the Solarized color scheme. This just scratches the surface, though. Hop over to the README for a more exhaustive list.\n\nOn your Mac, Maximum Awesome will set everything up automatically for you. Just run the command below in your terminal:\n\nSymlinks are created in your home directory that point to the repo so updating is as easy as git pull && rake. If you already had Vim or tmux config files, they\u2019ve been backed up. For example, your old .vim directory is now .vim.bak. Keep reading about \u201cCustomizing\u201d if you want to incorporate your existing settings.\n\nHaving problems installing? Open an issue on GitHub and we\u2019ll take a look right away.\n\nIn your home directory, Maximum Awesome creates a .vimrc.local file where you can customize Vim to your heart\u2019s content. However, we\u2019d love to incorporate your changes and improve Vim for everyone, so feel free to fork Maximum Awesome and open some pull requests!\n\nWhether you\u2019re the type of person with an hjkl shirt, or just trying out Vim for the first time, we hope Maximum Awesome makes writing code easier. Enjoy!"
    },
    {
        "url": "https://medium.com/square-corner-blog/lgtm-a-validation-library-for-javascript-e94e05af079b",
        "title": "LGTM: A Validation Library for JavaScript \u2013 Square Corner Blog \u2013",
        "text": "Validating user input and displaying error messages is a core part of nearly any application, especially when that application is a web page. There are many existing solutions to this problem, but too often they make assumptions that don\u2019t work for your application. For example, jQuery Validate has a lot of features but depends on jQuery and is tied to the DOM. For another, joi is promising but it is completely synchronous and has no way to declare validations that apply conditionally or depend on multiple values.\n\nAt Square, we use a few different JavaScript frameworks for creating dynamic web applications, the most common being Ember.js and Backbone. These frameworks have significantly different philosophies and any DOM code would be specific to one or the other. Because of this we felt that having a shared library for the core validation functionality would be the preferred way to go. That library should be asynchronous, validate at the object level, allow conditional & multi-property validation, and be completely independent of the DOM.\n\nThat library is LGTM*. It can be used in either node.js or the browser. It\u2019s built to support async all the way through. It\u2019s built to validate an object at a time and understands dependencies between properties. It leaves decisions about how to validate, when to validate, and how to display errors in your hands. Here\u2019s a simple example of building a validator:\n\nOnce you have a validator you can validate objects with a callback (validate also returns a promise):\n\nFor more information on custom validations, multi-property validations, promisesupport, and more please see the wiki. We hope you enjoy using LGTM in your apps, and we hope you contribute back any improvements you make!\n\n*An initialism standing for \u201clooks good to me\u201d."
    },
    {
        "url": "https://medium.com/square-corner-blog/introducing-wire-protocol-buffers-bb46f410e041",
        "title": "Introducing Wire Protocol Buffers \u2013 Square Corner Blog \u2013",
        "text": "Wire is a new, open-source implementation of Google\u2019s Protocol Buffers. It\u2019s meant for Android devices but can be used on anything that runs Java language code.\n\nAt Square, we use Protocol Buffers extensively. Protocol Buffers are a language- and platform-neutral schema for describing and transmitting data. Developers can use the same schemas across diverse environments, including the environments we care about at Square, such as Java and Ruby servers and Android and iOS devices.\n\nProtocol Buffer .proto source files are human-readable and can contain comments, so it\u2019s easy to document your schema right where it is defined. Protocol Buffers define a compact binary wire format that allows schemas to evolve without breaking existing clients. Typically, a code generator is used that reads .proto files, and emits source code in the language of your choice. This approach helps to speed development since the resulting code is expressed in the same language as the rest of your application, allowing tools such as IDE autocompletion to do their job fully.\n\nAs we began to run into limitations of the standard Protocol Buffer implementation in our Android apps, we made a wish list of the features we wanted for a future implementation:\n\nBefore we decided to build a new library we looked at several alternatives, including the recent Nano Android Protocol Buffer library. While Nano Protocol Buffers generate very few methods, they didn\u2019t meet our other goals. Ultimately, we decided to create our own library from scratch, built on Square\u2019s ProtoParser and JavaWriter libraries.\n\nIt\u2019s handy to be able to use generated Protocol Buffer classes as full-fledged data objects in your app. By including equals, hashCode, and toString methods, messages can participate in Java collections. Since the generated code is clean and compact, stepping into it in the debugger is not a problem. And because comments in your .proto files are copied into the generated Java source code, the documentation for your messages and fields is right there in your IDE.\n\nIn the past, Android developers attempting to use Protocol Buffers have paid a steep price. The standard Protocol Buffer implementation (protoc) generates at least nine methods for each optional or required field in your schema (variants of get, set, has, and clear), and at least eighteen methods for repeated fields!\n\nHaving all this flexibility is great in non-constrained environments \u2014 whatever method you need is probably just a few auto-completed keystrokes away. But in Android environments the Dalvik bytecode format imposes a hard limit of 64K methods in a single application. For the Square Register app, generated Protocol Buffer code was taking up a large chunk of our method space. By switching to Wire, we removed almost 10,000 methods from our application and gained a lot of breathing room to add new features.\n\nThe Person class Wire generates is below (the complete generated code is\n\nhere):\n\nAn instance of a message class can only be created by a corresponding nested Builder class. Wire generates a single method per field in each builder in order to support chaining:\n\nWire reduces the number of generated methods by using a public final field for each message field. Arrays are wrapped, so message instances are deeply immutable. Each field is annotated with a @ProtoField annotation providing metadata that Wire needs to perform serialization and deserialization:\n\nUse these fields directly to access your data:\n\nThe code to serialize and deserialize the Person we created above looks like this:\n\nSome features, such as serialization, deserialization, and the toString method are implemented using reflection. Wire caches reflection information about each message class the first time it is encountered for better performance.\n\nIn standard Protocol Buffers, you would call person.hasEmail() to see if an email address has been set. In Wire, you simply check if person.email == null. For repeated fields such as phone, Wire also requires your app to get or set a List of PhoneNumber instances all at once, which saves a lot of methods.\n\nWire supports additional features such as extensions and unknown fields. At present, it lacks support for some advanced features including custom options, services, and runtime introspection of schemas. These can be added in the future as use cases for them on constrained devices emerge.\n\nWire deliberately does not support the obsolete \u2018groups\u2019 feature.\n\nWe encourage you to try Wire, contribute to the code, and let us know how it works in your apps!"
    },
    {
        "url": "https://medium.com/square-corner-blog/bluetooth-printing-support-for-ipad-46176cfe7f47",
        "title": "Bluetooth Printing Support for iPad \u2013 Square Corner Blog \u2013",
        "text": "I\u2019m a software engineering intern at Square and have been working on Bluetooth printer support for iPad. This feature was released in our latest Square Register build and brings physical receipt printing using Star printers to a mobile customer base. Before this version, Register for iPad supported printed receipts through Star printers connected by WiFi and USB. Adding support for a new type of printer was a great choice for an intern project as it was a combination of building off of an existing hardware integration layer and extending it to support new concurrency requirements. Integrating this new feature into Square\u2019s codebase was a great lesson of how a well-designed code base can make writing new code easier.\n\nImplementing Bluetooth printing introduced new limitations in parallel processing and communication that required me to rethink how we manage concurrency in our printing system. If you\u2019re familiar with multi-threaded code, you probably already realize that expensive operations, such as peripheral communication, downloading and uploading, and heavy computation, are best run asynchronously off the main UI thread. In our code for WiFi printing, this is done using the most basic functionality of Grand Central Dispatch (GCD), one of Apple\u2019s built-in mechanisms for managing concurrent tasks. For more detailed info about GCD, read Apple\u2019s documentation . Each print operation and printer discovery operation is added to a global concurrent queue that runs the code on a private thread, abstracting away the mechanics of threading. The result of this is that the different operations are allowed to run in parallel with no restraints and no control.\n\nI started off the implementation of Bluetooth printer support by using the same concurrency model used by our WiFi printer code and running Bluetooth printing and printer discovery blocks on a global concurrent queue. I quickly found what appeared to be a non-deterministic bug \u2014 sometimes I could print a receipt with no problem, and other times it would fail part way through. What I was in fact encountering was a timer firing and searching for connected printers while the app was in the middle of sending data to a printer. While this behavior was perfectly acceptable in communication for WiFi printers (TCP supports multiple simultaneous connections), it totally broke down in the new communication channel. It became clear that some kind of mutual exclusion was necessary to prevent this kind of collision.\n\nMy first inclination, having just taken a class in C++ using pthreads, was to simply use locks. I created a shared NSLock through a class method and locked before listening and printing, as described in Apple\u2019s Threading Programming Guide. I was excited when it accomplished the mutual exclusion I wanted, but my coworkers were less thrilled about using such a low-level solution given the higher level abstractions available in iOS. In search of a higher-level solution, I replaced the global concurrent queue on which I was running the printer code with a serial dispatch queue, allowing only one block to be run at once. I was getting warmer, but my new solution still raised some concerns.\n\nI kept the static method I\u2019d written for having a single, unique lock and changed it to create a single, unique dispatch queue. The static method, however, introduced a new issue I\u2019d overlooked: upon logging out of the app, the queue would persist while the code blocks it ran tried to access deallocated data. This new issue finally pushed me to use an even higher level concurrency abstraction, NSOperationQueue (see Apple\u2019s documenation for more details). NSOperationQueues are built on top of GCD but provide several extra features, including the ability to cancel operations. Moving printer communication code to an NSOperation subclass was a small amount of extra work that paid big dividends. By storing the operation queue as an instance variable on the print controller, I was able to cancel all waiting printing and printer discovery operations and deallocate the queue itself in just a few lines of code.\n\nI\u2019d like to take a step back from discussing integration challenges to mention a few examples from the Square Register code base that made my life easier for this project. The code separates operations looking for connected peripherals and interacting with them and uses abstract superclasses to provide a consistent API to the rest of the app. Because the code was so robust, my new printer code fit well into the rest of the app, and existing code used by other printers fit well into my own new code. One good example is our receipt image renderer. The renderer works by putting data into an HTML document styled by CSS, rendering that HTML into an unseen web view and then rendering the view into PNG image data that can be sent to the printer. The HTML and CSS scale to fit different-sized windows and thus are a good generalizable solution for multiple print widths. All of the printers that we had previously supported (WiFi and USB printers) printed three-inch wide receipts. However most of the Bluetooth printers I worked with use two-inch wide paper. Thanks to the flexibility of our renderer, all I had to do was set the correct print width of a printer and voil\u00e0 \u2014 the receipts were seamlessly resized and looked great.\n\nThe rest of the peripheral library code was also reusable. Admittedly, our deep class hierarchy was rather intimidating when I first came to Square, but it ended up being possibly my greatest resource in writing this feature. At the highest level, Register has a Peripheral class to model every hardware device we support. It includes common properties like manufacturer name, model name, and its connection state. At the next level down is the Printer class, which encapsulates printer-specific information like the kind of data it can print and its print width, as well as abstract methods for printing images and text. Having this infrastructure in place made the design of the Bluetooth printer class clearer, and provided a clean, generic API for the rest of the app to use to print images and text to any printer, leaving the low-level details of the printer communication to the subclasses.\n\nBluetooth printing was a great project \u2014 it brought many unique engineering challenges and taught me a lot about how Square\u2019s code base works. I\u2019ve always been taught to use inheritance when possible to organize and share code, but working with the peripheral class hierarchy was a strong reminder of just how valuable a good, well-organized, object-oriented architecture can be. There was a lot of shared code between WiFi and Bluetooth printers that I could have copied and pasted, but I\u2019m glad that I took the time to not just make it work, but to do it right. I\u2019m thankful that my co-workers pushed me to take the time to experiment with different abstractions of concurrency. Not only is my code better, but I now know many ways to approach the problem. In software, there\u2019s rarely just one solution, and this is a good reminder to explore every possibility before choosing one \u2014 because it was the first idea that seemed to work.\n\nLast but not least, though I\u2019m enough of a geek that the code itself would have been interesting and fun for me, it was really special getting to see first-hand how the product truly helped people. Meeting a merchant who had previously only been able to give out handwritten receipts, handing her a Bluetooth printer, and seeing how our new feature would make her job easier was the highlight of my summer."
    },
    {
        "url": "https://medium.com/square-corner-blog/responsive-images-with-apropos-40afab95f306",
        "title": "Responsive Images with Apropos \u2013 Square Corner Blog \u2013",
        "text": "When Square\u2019s front-end engineering team was building our Japanese website, we decided early on that we needed to support a wide variety of devices, and that responsive design was the right way to do that. (Who wants to support a completely separate \u201cmobile site\u201d?) However, we weren\u2019t sure how we would serve every visitor the appropriate version of the gorgeous images shot by our talented photographers.\n\nWe took a look around at existing solutions for serving responsive images, but none of them quite met our criteria. Using Javascript meant extra downloads of unused images. Using cookies meant losing cacheability and made load time worse. We quickly decided that we should rely on CSS, taking advantage of the browser\u2019s ability to parse media queries in stylesheets and only download the resources it needs on a particular device.\n\nHowever, as we started prototyping early versions of pages, we noticed that the amount of CSS we were writing was really adding up, despite the built-in conciseness of Sass. We had settled on three breakpoints, and wanted to support both standard and high-resolution \u201cretina\u201d displays, so we had six different media queries to write for each image. We also planned to support more languages with the same design, which would add yet another dimension of variation.\n\nTo solve these problems, we built a Compass extension called Apropos. The basic principle is simple: you set up your variants, which could represent different sizes, resolutions, or localizations of an image, and you define extra file extensions which map to those. You end up with a simple file naming system that looks like this:\n\nWith all the images in place, it takes just one line of Sass to load them all:\n\nWhen the stylesheet is compiled, it generates a whopping 28 lines of CSS for you:\n\nWe also added features such as generating a CSS height property at each breakpoint, calculated from the image file, so that you have more flexibility to adapt your layout at each breakpoint. And we built Apropos so that it\u2019s easy to extend to handle localization and other variations. For more information, check out the documentation.\n\nToday we\u2019re releasing Apropos as an open-source project. Check it out, give it a try on your site, and let us know what you think!"
    },
    {
        "url": "https://medium.com/square-corner-blog/small-grunts-40cb582f8cb6",
        "title": "Small Grunts \u2013 Square Corner Blog \u2013",
        "text": "At Square, we encourage people to not limit their ideas to things they already know how to do. There are plenty of opportunities for people to expand their skills, like programming classes for engineers and non-engineers. Here\u2019s a post I shared on my personal blog for people who want to learn the basics of Grunt.\n\nLet\u2019s say you\u2019re a front-end dev who wants to leverage the power of Grunt. You\u2019re making a Wordpress theme, and you want to concatenate and minify your JavaScript files so your site will load faster. We\u2019ll assume that the site is using jQuery, a few jQuery plugins that rely on jQuery, and a site-wide script that relies on both the plugins and jQuery.\n\nHow difficult is that for someone who\u2019s proficient in jQuery, maybe has even written a couple plugins, but has never used JS outside the browser?\n\nWhen you go to the Getting Started page on the Grunt site, the first thing you are told to do is run:\n\nThis is a sign of things to come. The main thing I learned while writing this article is that in order to be comfortable with Grunt, you need to be comfortable with node.js \u2014 which means also being comfortable with:\n\nIt\u2019s not mind-bendingly difficult stuff once you get your feet wet, but still new knowledge you\u2019re going to have to take on before you can be slinging Grunt tasks like a pro.\n\nThe upside is once you know these things, you gain amazing Grunt powers on your development machine \u2014 and you get to do it all with that programming language you know and love: JavaScript. You might even become interested in using node.js on the server.\n\nBest to start from the beginning.\n\nThe node package manager lets you install node.js packages in a local directory so they can be found by node.js programs when they use the require() method.\n\n\u201cBut I\u2019m not using node.js,\u201d you say? Oh yes, you are. Grunt runs on node, and you\u2019ll be using npm to install any Grunt plugins you want to use. So it\u2019s time to learn a bit more about npm.\n\nBy including a package.json file in your project, you can specify exactly which modules (and which versions of those modules) to install via npm. This file has to be true JSON, and it must include the required keys of \u201cname\u201d and \u201cversion\u201d. You can run npm init and answer a bunch of questions to get a package.json with more than you really need in it. Or, you can just copy this JSON right here:\n\nPlop that in a package.json file in the root of your project, and then run npm install. Voila! You have installed Grunt in your project, along with some helpful plugins.\n\nWhy is \u201cgrunt\u201d in package.json if we already installed it with the npm command? What we installed before was grunt-cli, (some very simple code to make the grunt command available everywhere). The local Grunt code installed via package.json and npm install is what actually runs your project\u2019s tasks, defined in your project\u2019s Gruntfile.\n\nWe include both the Gruntfile and package.json in our project to be sure we are using a version of Grunt that is compatible with our Gruntfile.\n\nIf npm install is called without any specific package name, it will go through your package.json file and install all of your dependencies \u2014 and unless you tell it otherwise, your devDependencies as well.\n\nTo install new Grunt plugins, you have two options. First, you can add a new line to your package.json file that mentions the new plugin and then run npm install. Or (and this way is my favorite), you can run:\n\nThis command will download the specific node package, (in our case, it\u2019s grunt-contrib-requirejs, a node package which happens to be a Grunt plugin), and also adds that package\u2019s information to the devDependencies section of our package.json file.\n\nIf you used \u2014 save instead of \u2014 save-dev, the packages get added to the dependencies section instead of devDependencies. This would be important if you were making your own node package. For this example, it really doesn\u2019t make a difference, but devDependencies is more semantically correct.\n\nWhen you run a grunt command, Grunt looks in your current directory for a Gruntfile that defines tasks it knows how to run. A Gruntfile is a file named either Gruntfile.js (written in JavaScript) or Gruntfile.coffee (written in CoffeeScript).\n\nIf there isn\u2019t a Gruntfile in your current directory, Grunt will look in each directory above it. So, if you put your Gruntfile in the root of your project, you\u2019ll be able to use the tasks defined in that Gruntfile anywhere within your project.\n\nIf there isn\u2019t a Gruntfile anywhere above your current directory, you\u2019ll get an error: A valid Gruntfile could not be found.\n\nIn fact, this Gruntfile is so simple it contains no tasks. Any attempt to use it will result in a warning: Warning: Task \u201cfoo\u201d not found.\n\nBut we can still see the structure of a Gruntfile. A Gruntfile is actually a node module. Node modules signal what they want to export by assigning it to module.exports. In this case, we\u2019re exporting a function that receives a grunt object when it\u2019s called. This grunt object provides methods that allow you to configure Grunt. It also gives you access to things like grunt.log. You can find out everything you can do with the grunt object in the API docs.\n\nHere\u2019s a Gruntfile with a task you can actually run. The task is still useless, but whatevs\u2026\n\nWe call the registerTask method on the grunt object and pass a task name, description (option), and a function that defines the actual task.\n\nSave that into your Gruntfile.js and then run grunt pig and you\u2019ll see your output. Tada!\n\nNow we know how to register our own tasks with Grunt. This is amazingly powerful \u2014 not only do you have the entire Grunt API to work with, you also have all of node! The sky is the limit when writing your own tasks.\n\nBut why reinvent the wheel? There are existing Grunt plugins that handle most common tasks. One of those is the concat plugin, and that\u2019s what we\u2019ll use to solve our original problem of concatenating our JavaScript into a single file.\n\nHere\u2019s the structure of our JavaScript files.\n\nTo use the concat plugin, the first thing we have to do is to make sure we actually have the code available. If you used the sample package.json file above, you already have it installed. If not, install it and add it to your package.json with:\n\nNow that the plugin is available, we can load that plugin by calling the loadNpmTasksmethod on the grunt object. Here\u2019s our Gruntfile now:\n\nIf you run grunt -h, Grunt will show you some help text, along with a list of available tasks. You\u2019ll see that concat is now available. However, if you run grunt concat, you\u2019ll get an error: No \u201cconcat\u201d targets found.\n\nWe need to add some configuration, and we do that via the grunt object\u2019s initConfig method. You pass it a hash, and if you want to configure a specific task you use the task\u2019s name as a key in that hash.\n\nFiguring out how to format the parameters to concat is a little hit or miss. While it\u2019s not the easiest to read through, I recommend checking out the examples in the concat documentation \u2014 and thankfully, you have this tutorial!\n\nHere we\u2019re creating a subkey called js, which is a target of the concat task. Some tasks (called \u201cmulti-tasks\u201d) allow you define a range of targets. Many plugins are multi-tasks, including concat. If you run grunt concat:js it\u2019ll concat using that target\u2019s config. If you just run grunt concat it will run all targets.\n\nFor our js target, we define a list of source files in the order we want to concat them, and a destination file:\n\nTry out grunt concat and you\u2019ll find that a build.js file has been created.\n\nNote that we\u2019re able to use a wildcard *.js to grab all the JS files in the plugins directory. You can also use some/path/**/*.js to grab all the JS files in some/path and its subdirectories, no matter how deep!\n\nSince a lot of Grunt tasks deal with sets of input files and sets of output files, Grunt has some standard ways of representing source-destination file mappings. If you find yourself using Grunt a lot, you\u2019ll want to read through the documentation at some point.\n\nIn our src definition above, we put the jQuery file first, then the plugins and our site file so that they concat in the right order. This assumes that none of the plugins depend on each other. If you have to, you can name some files explicitly. Grunt concat is smart enough to not duplicate files grabbed by wildcards. In the following example, jquery.timeago.js would only appear once in the built file.\n\nThe concat plugin also has a few options you can set in order to add banners to the built file, strip banners from source files, set separator characters, and even process source files as if they\u2019re Lo-Dash templates. You can check those out at your leisure.\n\nWe can now type grunt concat and build our files. This is great, but y\u2019know what\u2019s even better? Less typing!\n\nLet\u2019s set a default task in our Gruntfile by adding this line:\n\nNow we can just run grunt and Grunt will run our concat task.\n\nWe\u2019ve concatenated our JavaScript together, but let\u2019s go one step further and minify them using the uglify plugin.\n\nFirst, we make sure we\u2019ve installed the plugin locally:\n\nThen we add the tasks to our Gruntfile:\n\nThen we add the configuration for uglify, and also make sure we use a ; for a separator when we concat our JS files (just to be safe).\n\nNow we can run grunt uglify:js or grunt uglify.\n\nI\u2019ve chosen the name \u201cjs\u201d for the uglify target. This is purely coincidence. It doesn\u2019t have to be the same target name as the one we used for concat.\n\nWe can now run grunt anywhere in our project, and Grunt will concatenate our JS files and minify them.\n\nWe\u2019re now doing a lot of work by just typing a single five letter command, but what if we could type even less!? Using the watch plugin, we can run Grunt tasks automatically whenever your project\u2019s files change.\n\nBy now this should be old hat. Install the plugin:\n\nAdd its tasks to the Gruntfile:\n\nNow we can run the grunt watch task, and Grunt will sit there watching our files. If any of the matched files change, Grunt will run the associated task(s).\n\nGrunt was created specifically for JavaScript developers. If you\u2019re at the level where you\u2019re writing unit tests for your JavaScript, you\u2019ll definitely want to start working with Grunt on a daily basis. Combining the watch plugin with the jshint and qunit plugins gives you constant feedback about the quality of your code.\n\nIf you\u2019re not yet at that level with your JavaScript, Grunt may still be for you. It\u2019ll depend on whether you\u2019re willing to put in the time to learn how to set up a Gruntfile, or find a suitable template. Grunt gives you an incredibly powerful set of tools, but it\u2019s up to you to put them to use.\n\nThe ability to create templates with grunt-init is cool, too. I strongly recommend the grunt-init-jquery template if you\u2019re ever making a jQuery plugin. It includes some great code examples.\n\nContact me on Twitter or Github if you have questions!"
    },
    {
        "url": "https://medium.com/square-corner-blog/reversing-bits-in-c-48a772dc02d7",
        "title": "Reversing Bits in C \u2013 Square Corner Blog \u2013",
        "text": "Thanks to readers rjs, meteorfox in the comments, and reader lnxturtle over on the Reddit thread (!) for correcting my poor choice of words. I incorrectly used the phrase Cache Coherence when the correct phrase was (Spatial / Temporal) Cache Locality. Your attention to detail is appreciated!\n\nInteresting lessons can come from unexpected places! I was pleasantly surprised at how something as \u201csimple\u201d as reversing bits in a byte could lead me on an unexpectedly deep exploration: operation vs instruction count, memory access patterns and cache behavior, and low-level CPU instructions. It\u2019s often very easy to make assumptions about the performance of code that we write, and I hope that this article serves as a reminder that the map is never the territory, and that the only way to understand what\u2019s happening inside your code is by observing and measuring it.\n\nWhile doing some investigations into one of our core iOS libraries, some code jumped out at me:\n\nSince I\u2019m not a cyborg wizard ninja with the ability to do 64-bit multiplication and bit-twiddling in my head, I decided to try and figure out what this code was doing. A simple search on one of the constant numbers led me immediately to the famous Stanford Bit Hacks page. This, in retrospect, should have been obvious.\n\nThe description of the algorithm is: \u201c**Reverse the bits in a byte with 3 operations (64-bit multiply and modulus division)**\u201d. This confused me a little more, since all current iOS devices have 32-bit ARM CPUs. Was this code actually efficient? It has the fewest \u201coperations\u201d, which is probably why it was originally chosen.\n\nBeing curious, I decided to take a few minutes and look at all of the approaches and learn how they actually performed on real-world hardware. This code, like all code, runs on an actual CPU. How do the different approaches perform on real hardware here in the real world?\n\nHere are all of the algorithms listed on the Bit Hacks page, with their advertised \u201coperation counts\u201d:\n\nLet\u2019s take a step back here. We\u2019re running this code on a specific family of CPUs, the ARMv6 and greater (available in all iPhones). ARMv6 and greater CPUs have an instruction called RBIT. This single instruction does exactly what we need: it reverses the bits in a 32-bit register. Unfortunately, while a lot of CMSIS implementations provide an RBIT intrinsic, it doesn\u2019t look like one is provided with Xcode. It\u2019s easy enough to drop down into inline assembly, though, and call the instruction ourselves:\n\nLet\u2019s add this approach to our collection and see how it performs.\n\nNote: Intel x86/x64 processors don\u2019t have this instruction, so this is definitely not a portable solution. (That\u2019s why I call it cheating!)\n\nHere are the timings for reversing the bits in a byte, performed 50 million times each.\n\nObservations of interest:\n\n* ReverseBits3ops64bit was 50x slower than the fastest 2 algorithms, despite taking O(1) time!\n\n* ReverseBitsObvious (O(N), remember) was only 3\u20135x slower than the O(1) solutions.\n\n* ReverseBits5logNOps (O(logN)) was slightly faster than ReverseBits4ops64bit O(1).\n\n* ReverseBitsLookupTable is ever-so-slightly faster than ReverseBitsRBIT? Why?\n\nSo what\u2019s the takeaway here? What have we learned from this experiment?\n\nChoosing an algorithm based on the number of \u201coperations\u201d is a nonsensical approach. ReverseBits3ops64bit was the clear loser because not only did it have to do some large 64-bit math, it also needed to do a 64-bit modulo with a non-power-of-2 divisor. That\u2019s one mathematical operation, but a large number of CPU instructions. CPU instructions are what matter here, though, as we see, not as much as cache coherency.\n\nAsymptotic Big-O analysis doesn\u2019t tell the whole story. It\u2019s very easy to simplify and select algorithms based on their asymptotic behavior. Unfortunately, many algorithms aren\u2019t actually dominated by their asymptotic behavior until N gets very large. In many cases, and certainly in this one we\u2019re studying, the algorithms are dominated largely by instruction count and cache access patterns. ReverseBits5logNOps runs in O(logN) time, and yet is faster than the O(1) ReverseBits4ops64bit implementation, which doesn\u2019t even do any expensive (non-power-of-2 modulo) math! Asymptotic behavior analysis is crucial when dealing with huge data sets, but is misleading and incorrect here.\n\nCache locality is crucial. ReverseBitsLookupTable was tied for first place because the 256-byte lookup table fits entirely in D-cache. If we were to evict the table from the cache between each iteration, we would pay the time penalty for the cache miss and subsequent reload.\n\nTake advantage of your specific CPU! RBIT will win in the general case here because it\u2019s a single instruction with relatively low latency and throughput. It does no memory access, so we don\u2019t have to worry about cache misses. It requires no extra storage, like the ReverseBitsLookupTable solution does. If you ever need to reverse a full 32-bit word, RBIT will take the same amount of time. In fact, our ReverseBitsRBIT function would be even faster when reversing a full register-sized 32-bit word, since the final right shift could be eliminated.\n\nUnfortunately, I wasn\u2019t able to figure out a way to get Apple\u2019s LLVM compiler to optimize a C loop down into a single RBIT command. This isn\u2019t surprising, as it\u2019s a fairly complex algorithm for an optimizing compiler to match and apply a strength reduction against. Compilers are getting smarter every day, but in the meantime, it\u2019s always good to know how to do this kind of manual work when necessary.\n\nIf you\u2019re curious about this and want to dive in and run your own experiments, I\u2019ve posted the test code I used to gather my results (as well as the cleaned-up assembly output emitted by the various functions), all on my public GitHub space. Fork away, and let me know what you discover!"
    }
]