[
    {
        "url": "https://towardsdatascience.com/using-word2vec-for-better-embeddings-of-categorical-features-de75020e1233?source=user_profile---------1----------------",
        "title": "Using Word2Vec for Better Embeddings of Categorical Features",
        "text": "Back in 2012, when neural networks regained popularity, people were excited about the possibility of training models without having to worry about feature engineering. Indeed, most of the earliest breakthroughs were in computer vision, in which raw pixels were used as input for networks.\n\nSoon enough it turned out that if you wanted to use textual data, clickstream data, or pretty much any data with categorical features, at some point you\u2019d have to ask yourself \u2014 how do I represent my categorical features as vectors that my network can work with?\n\nThe most popular approach is embedding layers \u2014 you add an extra layer to your network, which assigns a vector to each value of the categorical feature. During training the network learns the weights for the different layers, including those embeddings.\n\nIn this post I will show examples of when this approach will fail, introduce category2vec, an alternative method for learning embedding using a second network and will present different ways of using those embeddings in your primary network.\n\nEmbedding layers are trained to fit a specific task \u2014 the one the network was trained on. Sometimes that\u2019s exactly what you want. But in other cases you might want your embeddings to capture some intuition about the domain of the problem, thus reducing the risk of overfitting. You can think of it as adding prior knowledge to your model, which helps it to generalize.\n\nMoreover, if you have different tasks on similar data, you can use the embeddings from one task in order to improve your results on another. This is one of the major tricks in the Deep Learning toolbox. It\u2019s called transfer learning, pretraining or multi-task learning, depending on the context. The underlying assumption is that many of the unobserved random variables explaining the data are shared across tasks. Since the embeddings try to isolate these variables, they can be reused.\n\nMost importantly, learning the embeddings as part of the network increases the model\u2019s complexity by adding many weights to the model, which means you\u2019ll need much more labeled data in order to learn.\n\nSo it\u2019s not that embedding layers are bad, but we can do better. Let\u2019s see an example.\n\nTaboola\u2019s research group develops algorithms that suggest content to users, based on what they\u2019re currently reading. We can think about it as a Click Prediction problem: given your reading history, what is the probability that you will click on each article?\n\nTo solve this problem, we train deep learning models. Naturally, we started with learning the embeddings as part of our network.\n\nBut a lot of the embeddings we got didn\u2019t make sense.\n\nThe simplest way is to take embeddings of several items and look at their neighbors. Are they similar in your domain? That can be pretty exhausting, and doesn\u2019t give you the big picture. So, in addition you can reduce the dimensionality of your vectors using PCA or t-SNE, and color them by specific characteristic.\n\nThe advertiser of an item is a strong feature, and we want similar advertisers to have similar embeddings. But this is what our embeddings for different advertisers looked like, colored by the language of that advertiser:\n\nOuch. Something is clearly not right. I mean, unless we assume that our users are multilingual geniuses that read one article in Spanish and then effortlessly go and read another one in Japanese, we would probably want similar advertisers in our embedding space to have content in the same language.\n\nThis made our models harder to interpret. People were upset. Some even lost sleep.\n\nCan we do better?\n\nIf you\u2019ve ever heard about embeddings you\u2019ve probably heard about word2vec. This method represents words as high dimensional vectors, so that words that are semantically similar will have similar vectors. It comes in two flavors: Continuous Bag of Words (CBOW) and Skip-Gram. CBOW trains a network to predict a word from its context, while Skip-Gram does the exact opposite, predicting the context based on a specific target word.\n\nCan we use the same idea to improve our advertisers\u2019 embedding? YES WE.. well, you get the idea.\n\nThe idea is simple: we train word2vec on users\u2019 click history. Each \u201csentence\u201d is now a set of advertisers that a user clicked on, and we try to predict a specific advertiser (\u201cword\u201d) based on other advertisers the user liked (\u201ccontext\u201d). The only difference is that, unlike sentences, the order is not necessarily important. We can ignore this fact, or enhance the data set with permutations of each history. You can apply this method to any kinds of categorical features with high modality, e.g, countries, cities, user ids, etc.. See more details here and here.\n\nSo simple, yet so effective. Remember that messy visualization of embeddings we had earlier? This is what it looks like now:\n\nMuch better! All advertisers with the same language are clustered together.\n\nNow that we have better embeddings, what can we do with them?\n\nFirst, note that category2vec is just one example of a general practice: take the embeddings learned in task A and use them for task B. We could replace it with different architectures, different tasks, and in some cases, even different datasets. That\u2019s one of the great strengths of this approach.\n\nThere are three different ways to use the new embeddings in our model:\n\nSometimes we can get great results simply by taking some existing method, but applying it to something new. We used word2vec to create embeddings for advertisers and our results were much more meaningful than the ones obtained with embedding layers. As we say in Taboola \u2014 meaningful embeddings = meaningful life.\n\nDid you use word2vec in an unusual manner? Have some pro tips on categorical features? Read an interesting related paper? I\u2019d love to learn about it in the comments :)"
    }
]