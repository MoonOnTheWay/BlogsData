[
    {
        "url": "https://medium.com/@erikhallstrm/backpropagation-from-the-beginning-77356edf427d?source=user_profile---------1----------------",
        "title": "Backpropagation from the beginning \u2013 Erik Hallstr\u00f6m \u2013",
        "text": "The whole network is shown below, from the input vector x , to the output activation vector a\u1d38 . The connections leading in to a specific neuron is shown in colors in two layers:\n\nThe matrix multiplications in this formula is visualized in the figure below, where we have introduced a new vector z \u02e1. which is the activation without the application of a component-wise transfer function, so that a \u02e1 = \u03c3( z \u02e1 ). I will call this value the \u201cinput sum\u201d of a neuron.\n\nWe have a fully-connected feed-forward neural network. It has L layers (could be any number) and any number of neurons in each layer. The activations of the neurons in layer l is stored in an activations column-vector a \u02e1, where the superscript index denote the layer. The connections from the neurons in layer l-1 to the layer l are stored in a weight matrix W \u02e1, and the biases for each neuron is stored in a bias column-vector b \u02e1.\n\nI have tried to understand backpropagation by reading some explanations, but I\u2019ve always felt that the derivations lack some details. In this article I will try to explain it from the beginning hopefully not leaving anything out (theory wise at least). Let\u2019s get started!\n\nOne thing we can do, just to get a feeling of it is to write the network computation into one mathematical expression. Let\u2019s write down the formula for calculating the n:th element of the output vector in the final layer:\n\nHere we have introduced new notation where w\u02e1\u1d64\u1d65 is denoting the connection from the v:th neuron in layer l-1 to the u:th neuron in layer l, and b\u02e1\u1d64 is the bias of the u:th neuron in layer l. The expression can be a bit confusing, particularly because of all the new indices. But the biggest takeaway from this is that the neural network is just a mathematical function. And this function can be derived with respect to any variable. We will use our newly introduced notation, and define an error function, or \u201ccost\u201d function C using a sample of our training data, and then see how the error changes as we change our weights.\n\nThree adjacent layers anywhere in the network are shown in the figure below, the index letter for the neurons in the layers are j, k and m respectively, and the index letter for the layer is l.\n\nFirst calculate the input sum of a neuron k in layer l:\n\nThen take the transfer function, it could be any function with a first derivative:\n\nNow finally calculate the input sum of a neuron m in layer l+1.\n\nHere we have gone forward one step in the layers, from the activations in layer l-1 to the input sums of neurons in layer l+1. An error function C is defined using one example from our training data, and its derivative is calculated with respect to a single weight in layer l.\n\nYou may notice the sum in the expression above, it is due to the chain rule, all contributions from the neurons in layer l+1 have to be accounted for since their value is affecting the end error (their value is depending on the weight that we are taking the derivative with respect to). This is visualized in the figure shown above.\n\nOne important thing to remember is that we have fixed k and j, thus we only see how the error changes when one single weight is manipulated in the calculation. All the other weights are held constant, and the derivative of a constant is simply zero. But the m index in layer l+1 is not fixed, the activation of all neurons in that layer are changed as we change our specified weight.\n\nNow let\u2019s make a definition, the \u201cerror signal\u201d of a neuron k in layer l as how much the total error changes when the input sum of the neuron is changed:\n\nSo every neuron in the whole network now has an error signal defined. But if you look at the equation above, you will see that we have already expanded this expression:\n\nSo we have a recursive formula for the error signals, using our definitions:\n\nYou may wonder what happened with the biases, they are also \u201cweights\u201d and the error function should be derived with respect to them as well.\n\nSo the gradient of the cost function with respect to the bias for each neuron is simply its error signal!\n\nIn order to use this recursive formula we need to obtain the first error signal in the series, i.e. the error signal of the neurons in the final layer L. This is the starting value, after having this we can \u201cpropagate\u201d backwards calculating all the error signals.\n\nThe only derivative that has to be calculated here is the derivative of the cost function with respect to the activations of the last layer, which is the output of the network. So this will depend on the error-function we choose.\n\nSince we now can recursively calculate all the error signals for all neurons in the network, it is possible to obtain the derivative of the cost function with respect to all weights. Training is done by subtracting from each weight a small fraction of its corresponding derivative, called the delta rule. Since everything is known, the network is trainable! But we would like a more efficient notation, since we previously only worked with scalars. Vector and matrix notation is to the rescue! Remember that we stored all weights for layer l in a matrix W\u02e1, where the weights connecting from all neurons in l-1 to a neuron in l are stored as as rows? Take a look at the first figure in the article to be reminded about this. Now use the weight matrix, take its transpose. That will mean that the connections to the neurons in the layer are stored as columns. Put all the error signals for the layer in a column vector, and multiply it with with the transposed weight matrix. Sorry about this, but new notation again, there are a total of M neurons in layer l+1 and K neurons in layer l.\n\nThe similarity sign is because we are lacking the multiplication of the derivative of the input sum in layer l. Make sure you understand this matrix multiplication, use pen an paper to sketch the calculations if possible.\n\nFinally using all we derived previously, we can state formulas for calculating the gradient of the cost function with respect to the weights as following:\n\nThe nabla symbol is the derivative with respect to the output variables of the final layer, and the dot with a circle denotes component-wise multiplication.\n\nUsing these formulas we can effectively write an algorithm train the network, using single training sample at a time.\n\n7. Update the weights according to the delta rule.\n\nThis explanation is how to train a network using only one training sample at a time, but how to do it using batch learning? What we can do is just putting the inputs of the training samples as columns in a matrix, doing the forward propagation the input sums and activations will also be in matrices, where column index is the sample index, and the row index is the neuron index.\n\nSimilarly the error signals for different layers and samples will be in matrices. However, the derivatives of the cost function with respect to the weights will be in a three-dimensional tensor using the dimensions . Reducing a sum over the samples in this tensor will give the gradient matrix for the weights in the actual layer, and the weights can be updated using the delta rule.\n\nIn the next article we will be building a simple neural network from scratch, that can be trained using this theory (coming soon)."
    },
    {
        "url": "https://medium.com/@erikhallstrm/using-the-tensorflow-multilayered-lstm-api-f6e7da7bbe40?source=user_profile---------2----------------",
        "title": "Using the Multilayered LSTM API in TensorFlow (4/7)",
        "text": "In the previous article we learned how to use the TensorFlow API to create a Recurrent neural network with Long short-term memory. In this post we will make that architecture deep, introducing a LSTM with multiple layers.\n\nOne thing to notice is that for every layer of the network we will need a hidden state and a cell state. Typically the input to the next LSTM-layer will be the previous state for that particular layer as well as the hidden activations of the \u201clower\u201d or previous layer. There is a good diagram in this article.\n\nWe could continue to store the states for each layer in many , but that would require a lot of overhead. You can only input data to the placeholders trough the as Python lists or Numpy arrays anyways (not as ) so we still would have to convert between the datatypes. Why not save the whole state for the network in a big tensor? In order to do this the first thing we want to do is to replace and on line 81\u201382 with the more generic:\n\nYou also have to declare the new setting in the beginning of the file, but you may choose any number of layers. The \u201c2\u201d refers to the two states, cell- and hidden-state. So for each layer and each sample in a batch, we have both a cell state and a hidden state vector with the size .\n\nNow modify lines 93 to 103 (the run function and the separation of the state tuple) back to the original statement, since the state is now stored in a single tensor.\n\nYou can change these lines 28 to 30 in the previous post:\n\nTo a single placeholder containing the whole state.\n\nSince the TensorFlow Multilayer-LSTM-API accepts the state as a tuple of LSTMTuples, we need to unpack the state state into this structure. For each layer in the state we then create a stated, and put these in a tuple, as shown below. Add this just after the placeholder.\n\nThe forward pass on lines 40 and 41 should be changed to this:\n\nThe multi-layered LSTM is created by first making a single , and then duplicating this cell in an array, supplying it to the API call. The forward pass uses the usual , let\u2019s print the output of this function, the and variables.\n\nTake a look at the tensor names between single quotes, we see that the RNN is unrolled 15 times. In the all outputs have the name \u201cCell2\u201d, it means that we get the output of the last LSTM layer\u2019s hidden state in the list. Furthermore the LSTMStateTuple in the gives the whole state of all layers in the network. \u201cCell0\u201d refers to the first layer, \u201cCell1\u201d to the second and \u201cCell2\u201d to the third and final layer, \u201ch\u201d and \u201cc\u201d refers to hidden- and cell state.\n\nThis is the whole self-contained script, just copy and run.\n\nIn the next article we will speed up the graph creation by not splitting up our inputs and labels into a Python list."
    },
    {
        "url": "https://medium.com/@erikhallstrm/using-the-dynamicrnn-api-in-tensorflow-7237aba7f7ea?source=user_profile---------3----------------",
        "title": "Using the DynamicRNN API in TensorFlow (5/7) \u2013 Erik Hallstr\u00f6m \u2013",
        "text": "In the previous guide we built a multi-layered LSTM RNN. In this post we will speed it up by not splitting up our inputs and labels into a list, as done on line 41\u201342 in our code. You may remove these rows where and are declared. Next change the call on line 47 to the following:\n\nThe function takes the batch inputs of shape , thus the addition of a single dimension on the end. Output will be the last state of every layer in the network as an LSTMStateTuple stored in as well as a tensor with the shape containing the hidden state of the last layer across all time-steps.\n\nThe tensor is reshaped on the second row in the code sample above to shape , we will see the reason for this shortly. You may read more about in the documentation.\n\nNow input this two lines below the reshaping of the .\n\nNotice that we are now only working with tensors, Python lists were a thing of the past. The calculation of the and the are visualized below, notice the variable that was reshaped earlier. In TensorFlow reshaping is done in C-like index order. It means that we read from the source tensor and \u201cwrite\u201d to the destination tensor with the last axis index changing fastest, and the first axis index changing slowest. The result of the reshaping will be as visualized in the figure below, where similar colors denote the same time-step, and the vertical grouped spacing of elements denote different batches.\n\nLet\u2019s go trough all the tensors in the figure above, first let\u2019s start with the sizes. We have that , , and . The tensor have shape , have shape , have shape , have shape and have shape . It can be a bit tricky to keep track of all the tensors, but drawing and visualizing with colors definitely helps.\n\nNext calculate the predictions for the visualization:\n\nHere we actually split the tensors into lists again. This is perhaps not the best way to do it, but it\u2019s quick and dirty, and the plot function is already expecting a list.\n\nThe can take the shape of our tensors! Modify the calculation to this.\n\nAs we can read in the API the must have the shape and must have the shape . But now we are treating all time-steps as elements in our batch, so it will work out as we want.\n\nThis is the whole self-contained script, just copy and run.\n\nIn the next part we will regularize the network to use dropout, making it less prone to overfitting."
    },
    {
        "url": "https://medium.com/@erikhallstrm/using-the-dropout-api-in-tensorflow-2b2e6561dfeb?source=user_profile---------4----------------",
        "title": "Using the Dropout API in TensorFlow (6/7) \u2013 Erik Hallstr\u00f6m \u2013",
        "text": "In the previous part we built a multi-layered LSTM RNN. In this post we will make it less prone to overfitting (called regularizing) by adding a something called dropout. It\u2019s a weird trick to randomly turn off activations of neurons during training, and was pioneered by Geoffrey Hinton among others, you can read their initial article here.\n\nFortunately this is very simple to do in TensorFlow, between the lines 41\u201342 you simply add a with the probability to not drop out, called . Change lines 41\u201342 to the code below.\n\nDon\u2019t drop out too much or you will need a large state to be sure to keep some of the information (in our toy example at least). As you can read in this article dropout is implemented between RNN layers in TensorFlow, not on recurrent connections.\n\nThis is the whole self-contained script, just copy and run.\n\nIn the next part we will further regularize it by using something called batch normalization. Stay tuned, it will be coming soon :)"
    },
    {
        "url": "https://medium.com/@erikhallstrm/using-the-tensorflow-lstm-api-3-7-5f2b97ca6b73?source=user_profile---------5----------------",
        "title": "Using the LSTM API in TensorFlow (3/7) \u2013 Erik Hallstr\u00f6m \u2013",
        "text": "In the previous post we modified our to code to use the TensorFlow native RNN API. Now we will go about to build a modification of a RNN that called a \u201cRecurrent Neural Network with Long short-term memory\u201d or RNN-LSTM. This architecture was pioneered by J\u00fcrgen Schmidhuber among others. One problem with the RNN when using long time-dependencies ( is large) is the \u201cvanishing gradient problem\u201d. One way to counter this is using a state that is \u201cprotected\u201d and \u201cselective\u201d. The RNN-LSTM remembers, forgets and chooses what to pass on and output depending on the current state and input.\n\nSince this primarily is a practical tutorial I won\u2019t go into more detail about the theory, I recommend reading this article again, continue with the \u201cModern RNN architectures\u201d. After you have done that read and look at the figures on this page. Notice that the last mentioned resource are using vector concatenation in their calculations.\n\nIn the previous article we didn\u2019t have to allocate the internal weight matrix and bias, that was done by TensorFlow automatically \u201cunder the hood\u201d. A LSTM RNN has many more \u201cmoving parts\u201d, but by using the native API it will also be very simple.\n\nA LSTM have a \u201ccell state\u201d and a \u201chidden state\u201d, to account for this you need to remove on line 79 in the previous script and replace it with this:\n\nTensorFlow uses a data structure called internally for its LSTM:s, where the first element in the tuple is the cell state, and the second is the hidden state. So you need to change line 28 where the is placeholders are declared to these lines:\n\nChanging the forward pass is now straight forward, you just change the function call to create a LSTM and supply the initial state-tuple on line 38\u201339.\n\nThe will be a list of hidden states as tensors, and will be a LSTMStateTuple which shows both the hidden- and the cell state on the last time-step as shown below:\n\nSo the returns the cell- and hidden state in a tuple. They should be separated after calculation and supplied to the placeholders in the run-function on line 90.\n\nThis is the full code for creating a RNN with Long short-term memory.\n\nIn the next article we will create a multi-layered or \u201cdeep\u201d recurrent neural network, also with long short-term memory."
    },
    {
        "url": "https://medium.com/@erikhallstrm/tensorflow-rnn-api-2bb31821b185?source=user_profile---------6----------------",
        "title": "Using the RNN API in TensorFlow (2/7) \u2013 Erik Hallstr\u00f6m \u2013",
        "text": "This post is the follow up of the article \u201cHow to build a Recurrent Neural Network in TensorFlow\u201d, where we built a RNN from scratch, building up the computational graph manually. Now we will utilize the native TensorFlow API to simplify our script.\n\nRemember where we made the unpacking and forward passes in the vanilla RNN?\n\nReplace the piece of code above with this:\n\nYou may also remove the weight- and bias matrices and declared earlier. The inner workings of the RNN are now hidden \u201cunder the hood\u201d. Notice the usage of instead of when assigning the variable. The accepts a list of inputs of shape , and the is simply one in our case (input is just a series of scalars). Split doesn\u2019t remove the singular dimension, but unpack does, you can read more about it here. It doesn\u2019t really matter anyways, since we still had to reshape the inputs in our previous example before the matrix multiplication. The unrolls the RNN and creates the graph automatically, so we can remove the for-loop. The function returns a series of previous states as well as the last state in the same shape as we did before manually, here is the printed output of these variables.\n\nHere is the full code:\n\nIn the next post we will improve the RNN by using another architecture called \u201cLong short-term memory\u201d or LSTM. Actually this is not necessary since our network already can solve our toy problem. But remember that our goal is to learn to use TensorFlow properly, not to solve the actual problem which is trivial :)"
    },
    {
        "url": "https://medium.com/@erikhallstrm/work-remotely-with-pycharm-tensorflow-and-ssh-c60564be862d?source=user_profile---------7----------------",
        "title": "Work remotely with PyCharm, TensorFlow and SSH \u2013 Erik Hallstr\u00f6m \u2013",
        "text": "Wouldn\u2019t it be awesome to sit at a caf\u00e9 with your laptop, creating large neural networks in TensorFlow, crunching data with speeds of several terraFLOPS, without even hearing your fan spinning up? This is possible using a remote interpreter in PyCharm, and you get almost the same experience working remotely as working locally.\n\nHowever, this is currently only possible in PyCharm Professional (Community Edition will not do). If you are a student your University should have an arrangement so you can download it for free, otherwise you\u2019ll have to buy it. Here is how I set it up from scratch (you may want to skip some of the steps):\n\nThis is your stationary remote machine, perhaps fitted with one or several state-of-the-art GPU:s from Nvidia! (I don\u2019t like the current deep learning monopoly, but TensorFlow can only use Nvidia GPUs). First let\u2019s install the latest Ubuntu, I recommend the desktop version, you can always kill the GUI-service later to free up graphics memory. Connect it to Internet and check you LAN IP-address by opening up a terminal typing . I will assume it is in the instructions later.\n\nIn order to be able to communicate with your crunching-machine, you need to install SSH on it. Open up a terminal on your stationary computer and get it:\n\nEnable SSH X11-forwarding so that you can plot things, open the configuration file like this.\n\nThen locate the row that says\n\nSimply remove the hash-sign to uncomment the line, save and close the file.\n\nNext install the graphics drivers, they are usually proprietary, so you need to add a new repository to your package manager. What package you\u2019ll need depend on your graphics card and Ubuntu version. As of writing nvidia-367 is the latest one, see more on this page.\n\nNow it\u2019s time to install Cuda toolkit and and cuDNN, which are required to run TensorFlow. They are available from Nvidia\u2019s webpage, and to download cuDNN you are required to register. As of writing Cuda 8.0 and cuDNN 5.1 are the latest versions. For Cuda I prefer using the built in package manager, it makes it easier to keep track of what you have installed:\n\nMake sure that the symlink is set up correctly:\n\nThis is how to extract the cuDNN headers and copy them into the Cuda folder, and make them readable in the terminal (some of the filenames may be different for you):\n\nFinally add the environment variables you will need, append them to your file and then source it:\n\nAnd then install GPU enabled Tensorflow, check the version you need on this page ( is different for different systems):\n\nVerify that the installation is working by typing the following in your terminal:\n\nYou should get output similar to this if you have installed it on a GPU enabled system:\n\nDid it work? Great! Let\u2019s move on to your laptop\n\nOpen up your laptop and connect it to the same local network as your stationary machine.\n\nSo I\u2019m using a Macbook and it allows me to install programs with a very nice package manager called Homebrew. Even desktop apps can easily be downloaded with Homebrew Cask.\n\nGet what you need, including the PyCharm IDE.\n\nGenerate a SSH key-pair by executing the command below and then walk trough the guide (if you haven\u2019t done this already):\n\nNow copy the key to your remote machine so you can connect to it without typing a password every time. On the first time doing this you need to authenticate yourself with the password of your remote machine:\n\nEnable compression and X11-forwarding (useful for plotting data) by appending this to your file on your local machine.\n\nVerify that everything is working by connecting to your remote machine from your laptop.\n\nWhile still logged in, you should disable password login on your remote machine for security reasons. Open the configuration file with your favorite command-line editor.\n\nAnd uncomment the following line by removing the hash-sign:\n\nRestart your SSH server while still logged in on your remote (you have to authenticate yourself again).\n\nThe final thing you should do while still logged in with SSH on your remote is to find your display environment variable. This will be used later for plotting, I usually get .\n\nRemember the output of this command, we will use it later.\n\nThis is the funny part, how we can set up the remote interpreter so you execute the scripts on your remote machine. Let\u2019s get started, start up PyCharm and create a new Python project.\n\nOpen \u201cPreferences > Project > Project Interpreter\u201d. Click on the \u201cDotted button\u201d in the top-right corner and then \u201cAdd remote\u201d.\n\nClick on the \u201cSSH Credentials\u201d radio-button and input your information. Select \u201cKey pair\u201d on the \u201cAuth type\u201d, and select the \u201cPrivate Key file\u201d. It should be located in .\n\nClick on \u201cOK > Apply\u201d. Notice the \u201cR\u201d for remote on the Project Interpreter.\n\nThe remote interpreter can not execute a local file, PyCharm have to copy your source files (your project) to a destination folder on your remote server, but this will be done automatically and you don\u2019t need to think about it! While still in the \u201cPreferences\u201d pane, open \u201cBuild, Execution, Deployment > Deployment > Options\u201d. Make sure that \u201cCreate empty directories\u201d is checked. This way PyCharm will automatically synchronize when you create folders:\n\nNow go back to \u201cBuild, Execution, Deployment > Deployment\u201d and click on the \u201cPlus button\u201d, select \u201cSFTP\u201d and give a name to your remote. Click on \u201cOK\u201d:\n\nSet up the connection by first typing the IP of your remote in \u201cSFTP host\u201d, then selecting \u201cKey pair\u201d on the \u201cAuth type\u201d, and finally selecting the \u201cPrivate Key file\u201d. It should be located in , as shown in the screenshot below. You may then click on \u201cTest SFTP connection\u201d. Given that you can successfully connect you should set up mappings. If you\u2019d like you can click on \u201cAutodetect\u201d beside the \u201cRooth path\u201d, it will then find the place of your home directory on the remote. All paths you specify after this will be relative to this home path. Then go to the \u201cMappings\u201d tab.\n\nAs soon as you save or create a file in your local path, it will be copied to the \u201cDeployment path\u201d on your remote server. Perhaps you want to deploy it in a folder as shown below. This will be relative to your \u201cRooth path\u201d specified earlier, so the absolute deployment path will in our case be be :\n\nNow we are finished with the preferences, click on \u201cApply\u201d > \u201cOK\u201d, and then click \u201cTools > Deployment > Automatic Upload\u201d and confirm that it is checked:\n\nTo do the initial upload, right-click on you project folder in the project explorer and click on \u201cUpload to remote\u201d:\n\nYou should get a \u201cFile transfer\u201d tab on your bottom pane where you can see all the progress:\n\nThen click on \u201cTools > Deployment > Browse Remote Host\u201d. Drag and drop the window just beside the Project tab to the left. That way it will be really simple to switch between your local and remote project.\n\nThese deployment settings will work seamlessly as soon as you save and run a file, it is done so quickly you won\u2019t even notice it.\n\nOpen \u201cPreferences > Build, Execution, Deployment > Console > Python console\u201d and select the \u201cPython interpreter\u201d to be your remote one. Next click on the \u201cDotted button\u201d and input the required environment variables that we added before to when we set up the server. Notice that we also added a value to the \u201cDISPLAY\u201d variable we found out earlier when connecting to the server with SSH:\n\nThen go back to \u201cBuild, Execution, Deployment >Deployment > Console\u201d and select \u201cAlways show the debug console\u201d. It will be very handy when we\u2019re debugging:\n\nCreate a simple test-file called in your project, just containing this.\n\nNow go to \u201cRun > Edit Configurations\u2026\u201d Click on the \u201cPlus button\u201d and create a new Python configuration. Name it and select the script to run:\n\nNow enter the required environment variables as before. Tips: You can copy them all from the console settings we specified earlier, by using Ctrl+A and then the copy/paste buttons in the lower left corner. You access them by clicking the \u201cDotted button\u201d just to the right of the \u201cEnvironment variables\u201d line.\n\nNow we should be all done, it\u2019s time to test our setup. First open a terminal and make sure that you have at least one SHH channel with X-forwarding connected your server. If you have had a connections open for a while, you may have to exit and restart them:\n\nThen open the \u201cPython Console\u201d in the lower bar in PyCharm and type . Then you may type to verify that you are actually executing the commands on your server! This is what the output should be:\n\nNow go over to your script and select \u201cRun > Run\u2026\u201d from the top toolbar. Select your newly create run configuration \u201cTest\u201d. It should output something like this:\n\nLet\u2019s do some plotting, change your file to this:\n\nAnd then run it again with your run configuration \u201cTest\u201d, you should get this plot.\n\nThe plot is actually done on your remote server, but the window data is forwarded to your local machine. Notice that we changed the backed with , because it\u2019s a X11-supported display backend. You can read more about Matplotlib backends here. You can also change the default behavior in your -file. Remember that you need to have at least one open SSH-connection in a separate terminal to get this to work, with the correct value of the environment variable. If it didn\u2019t work try to restart your SSH connection.\n\nFinally do some debugging, click on the left bar to put a breakpoint, then go \u201cRun > Debug\u2026\u201d and select the \u201cTest\u201d configuration. You will see that the execution has halted and you are debugging your script remotely.\n\nIn order to access your machine over the internet you have to forward ports on you home router, that is different for different vendors. I recommend forwarding a different port than 22 on your router. There are plenty bots out there trying to hack in, and they will check that port by default, and might slow your connection (although you are pretty secure since you have turned of password authentication). So you could perhaps forward port 4343 on your router to port on IP (the default IP of our remote in this tutorial). Also to speed up the plotting you may change to a faster encryption.\n\nNext, let\u2019s do some more TensorFlow, perhaps experimenting with matrix multiplication on the CPU and GPU? (coming soon)"
    },
    {
        "url": "https://medium.com/@erikhallstrm/hello-world-tensorflow-649b15aed18c?source=user_profile---------8----------------",
        "title": "Introduction to TensorFlow \u2014 CPU vs GPU \u2013 Erik Hallstr\u00f6m \u2013",
        "text": "In this tutorial we will do simple simple matrix multiplication in TensorFlow and compare the speed of the GPU to the CPU, the basis for why Deep Learning has become state-of-the art in recent years.\n\nIt\u2019s a framework to perform computation very efficiently, and it can tap into the GPU (Graphics Processor Unit) in order too speed it up even further. This will make a huge effect as we shall see shortly. TensorFlow can be controlled by a simple Python API, which we will be using in this tutorial.\n\nWhen a native computation is done in many programming languages, it is usually executed directly. If you type in a Python console, you will immediately have the result. Running a number of mathematical computations like this in an IDE also allows you to set breakpoints, stop the execution and see intermediate results. This is not possible in TensorFlow, what you actually do is specifying the computations that will be done. This is accomplished by creating a computational graph, which takes multidimensional matrices called \u201cTensors\u201d and does computations on them. Each node in the graph denotes an operation. When creating the graph, you have the possibility to explicitly specify where the computations should be done, on the GPU or CPU. By default it will check if a GPU is available, and use that.\n\nThe Graph is run in a Session, where you specify what operations to execute in the -function. Data from outside may also be supplied to placeholders in the graph, so you can run it multiple times with different input. Furthermore, intermediate result (such as model weights) can be incrementally updated in variables, which will retain their values between runs.\n\nThis code example creates pairs of random matrices, clocks the multiplication of them depending on size and device placement.\n\nYou see that the GPU (a GTX 1080 in my case) is much faster than the CPU (Intel i7). Back-propagation is almost exclusively used today when training neural networks, and it can be stated as a number of matrix multiplications (backward and forward pass). That\u2019s why using GPU:s are so important for quickly training deep-learning models.\n\nIn the next post we will use TensorFlow to create a recurrent neural network."
    },
    {
        "url": "https://medium.com/@erikhallstrm/hello-world-rnn-83cd7105b767?source=user_profile---------9----------------",
        "title": "How to build a Recurrent Neural Network in TensorFlow (1/7)",
        "text": "In this tutorial I\u2019ll explain how to build a simple working Recurrent Neural Network in TensorFlow. This is the first in a series of seven parts where various aspects and techniques of building Recurrent Neural Networks in TensorFlow are covered. A short introduction to TensorFlow is available here. For now, let\u2019s get started with the RNN!\n\nIt is short for \u201cRecurrent Neural Network\u201d, and is basically a neural network that can be used when your data is treated as a sequence, where the particular order of the data-points matter. More importantly, this sequence can be of arbitrary length.\n\nThe most straight-forward example is perhaps a time-series of numbers, where the task is to predict the next value given previous values. The input to the RNN at every time-step is the current value as well as a state vector which represent what the network has \u201cseen\u201d at time-steps before. This state-vector is the encoded memory of the RNN, initially set to zero.\n\nThe best and most comprehensive article explaining RNN:s I\u2019ve found so far is this article by researchers at UCSD, highly recommended. For now you only need to understand the basics, read it until the \u201cModern RNN architectures\u201d-section. That will be covered later.\n\nAlthough this article contains some explanations, it is mostly focused on the practical part, how to build it. You are encouraged to look up more theory on the Internet, there are plenty of good explanations.\n\nWe will build a simple Echo-RNN that remembers the input data and then echoes it after a few time-steps. First let\u2019s set some constants we\u2019ll need, what they mean will become clear in a moment.\n\nNow generate the training data, the input is basically a random binary vector. The output will be the \u201cecho\u201d of the input, shifted steps to the right.\n\nNotice the reshaping of the data into a matrix with rows. Neural networks are trained by approximating the gradient of loss function with respect to the neuron-weights, by looking at only a small subset of the data, also known as a mini-batch. The theoretical reason for doing this is further elaborated in this question. The reshaping takes the whole dataset and puts it into a matrix, that later will be sliced up into these mini-batches.\n\nTensorFlow works by first building up a computational graph, that specifies what operations will be done. The input and output of this graph is typically multidimensional arrays, also known as tensors. The graph, or parts of it can then be executed iteratively in a session, this can either be done on the CPU, GPU or even a resource on a remote server.\n\nThe two basic TensorFlow data-structures that will be used in this example are placeholders and variables. On each run the batch data is fed to the placeholders, which are \u201cstarting nodes\u201d of the computational graph. Also the RNN-state is supplied in a placeholder, which is saved from the output of the previous run.\n\nThe weights and biases of the network are declared as TensorFlow variables, which makes them persistent across runs and enables them to be updated incrementally for each batch.\n\nThe figure below shows the input data-matrix, and the current batch is in the dashed rectangle. As we will see later, this \u201cbatch window\u201d is slided steps to the right at each run, hence the arrow. In our example below , , and . Note that these numbers are just for visualization purposes, the values are different in the code. The series order index is shown as numbers in a few of the data-points.\n\nNow it\u2019s time to build the part of the graph that resembles the actual RNN computation, first we want to split the batch data into adjacent time-steps.\n\nAs you can see in the picture below that is done by unpacking the columns ( ) of the batch into a Python list. The RNN will simultaneously be training on different parts in the time-series; steps 4 to 6, 16 to 18 and 28 to 30 in the current batch-example. The reason for using the variable names is to emphasize that the variable is a list that represent a time-series with multiple entries at each step.\n\nThe fact that the training is done on three places simultaneously in our time-series, requires us to save three instances of states when propagating forward. That has already been accounted for, as you see that the placeholder has rows.\n\nNext let\u2019s build the part of the graph that does the actual RNN computation.\n\nNotice the concatenation on line 6, what we actually want to do is calculate the sum of two affine transforms in the figure below. By concatenating those two tensors you will only use one matrix multiplication. The addition of the bias is broadcasted on all samples in the batch.\n\nYou may wonder the variable name is supposed to mean. When a RNN is trained, it is actually treated as a deep neural network with reoccurring weights in every layer. These layers will not be unrolled to the beginning of time, that would be too computationally expensive, and are therefore truncated at a limited number of time-steps. In our sample schematics above, the error is backpropagated three steps in our batch.\n\nThis is the final part of the graph, a fully connected softmax layer from the state to the output that will make the classes one-hot encoded, and then calculating the loss of the batch.\n\nThe last line is adding the training functionality, TensorFlow will perform back-propagation for us automatically \u2014 the computation graph is executed once for each mini-batch and the network-weights are updated incrementally.\n\nNotice the API call to , it automatically calculates the softmax internally and then computes the cross-entropy. In our example the classes are mutually exclusive (they are either zero or one), which is the reason for using the \u201cSparse-softmax\u201d, you can read more about it in the API. The usage is to have is of shape and of shape .\n\nThere is a visualization function so we can se what\u2019s going on in the network as we train. It will plot the loss over the time, show training input, training output and the current predictions by the network on different sample series in a training batch.\n\nIt\u2019s time to wrap up and train the network, in TensorFlow the graph is executed in a session. New data is generated on each epoch (not the usual way to do it, but it works in this case since everything is predictable).\n\nYou can see that we are moving steps forward on each iteration (line 15\u201319), but it is possible have different strides. This subject is further elaborated in this article. The downside with doing this is that need to be significantly larger than the time dependencies (three steps in our case) in order to encapsulate the relevant training data. Otherwise there might a lot of \u201cmisses\u201d, as you can see on the figure below.\n\nAlso realize that this is just simple example to explain how a RNN works, this functionality could easily be programmed in just a few lines of code. The network will be able to exactly learn the echo behavior so there is no need for testing data.\n\nThe program will update the plot as training progresses, shown in the picture below. Blue bars denote a training input signal (binary one), red bars show echos in the training output and green bars are the echos the net is generating. The different bar plots show different sample series in the current batch.\n\nOur algorithm will fairly quickly learn the task. The graph in the top-left corner shows the output of the loss function, but why are there spikes in the curve? Think of it for a moment, answer is below.\n\nThe reason for the spikes is that we are starting on a new epoch, and generating new data. Since the matrix is reshaped, the first element on each row is adjacent to the last element in the previous row. The first few elements on all rows (except the first) have dependencies that will not be included in the state, so the net will always perform badly on the first batch.\n\nThis is the whole runnable program, just copy-paste and run. After each part in the article series the whole runnable program will be presented. If a line is referenced by number, these are the line numbers that we mean.\n\nIn the next post in this series we will be simplify the computational graph creation by using the native TensorFlow RNN API."
    }
]