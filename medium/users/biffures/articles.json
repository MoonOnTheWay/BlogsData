[
    {
        "url": "https://medium.com/biffures/rl-course-by-david-silver-lectures-8-9-328220e08b2f?source=---------0",
        "title": "RL Course by David Silver (Lectures 8 & 9) \u2013 Biffures \u2013",
        "text": "I won\u2019t cover lecture 10 here, although interesting. The lecture video is here and the associated slides (the slides are not readable in the video) are here."
    },
    {
        "url": "https://medium.com/biffures/rl-course-by-david-silver-lectures-5-to-7-576188d3b033?source=---------1",
        "title": "RL Course by David Silver (Lectures 5 to 7) \u2013 Biffures \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/biffures/rl-course-by-david-silver-lectures-1-to-4-7667608bf7d3?source=---------2",
        "title": "RL Course by David Silver (Lectures 1 to 4) \u2013 Biffures \u2013",
        "text": "I continue my quest to learn something about reinforcement learning in 60 days (this is day 20), with a 15 hour investment in Deepmind\u2019s David Silver\u2019s course on Reinforcement learning, which itself draws from Sutton and Barto\u2019s 1988 book, Reinforcement Learning: an Introduction."
    },
    {
        "url": "https://medium.com/biffures/network-sparsity-and-rectifiers-619e4908fab9?source=---------3",
        "title": "Network sparsity and rectifiers \u2013 Biffures \u2013",
        "text": "Activation functions in networks: the case for rectifiers In the rest of this article, we examine why rectifiers stand out as a choice of activation function. We start by ruling out linear neurons as candidates for neural networks, then explore the advantages of rectifiers over comparable neurons, like the sigmoid logistic ones. Linear activation neurons do not stack. N layers of linear neurons are equivalent to a single layer of linear neurons, given how information propagates in neural networks. This defeats the purpose of building neural networks, and a mental note that any good activation function candidate for a multi-layer network will need to be non-linear. Sigmoid activation neurons, which we know are able to perform logistic regressions on their own, work fine in deep neural networks too. We simply note that all neurons in a network of such neurons are always active and firing signal. Rectifying neurons are neurons using x \u2192 max(0, x) as their activation function. They have no clear use as single neurons but actually do well in deep neural networks. In Glorot et al., we learn that rectifying neurons have three interesting properties: \u201cRectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent [and sigmoid] networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data.\u201d (Glorot et al., 2011) What do those properties mean? Biological plausibility is the quality of a model being close to biological reality, with the hope that learnings from the model could inform our understanding of biology, or that conversely, that the model could inherit some of properties of the reality it drew inspiration from. Here, biological plausibility is about two things: (a) plausibility of the neuron\u2019s activation function, and (b) plausibility of the neural net\u2019s overall behavior. (a) Biological neurons are units that receive an electrical current as input, and fire output electrical signals characterized by their frequency if the input current is strong enough. In the absence of input current, biological neurons are not activated, and with increasing current, their fire rate increases.\n\nThe biological model is the leaky integrate-and-fire model to the left, and in contrast, we have 4 activations functions used in machine learning to the right. Tanh is considered worst from a biological plausibility perspective, because of the antisymmetry around 0. Sigmoid and softplus are better but continue firing across the real space. Rectifier is considered best because it is inactive for input values under 0. (b) Because rectifiers output 0 for negative inputs, they cause networks to have a large number of inactive neurons during forward passes(all neurons with a negative weighted input end up producing no signal). This behavior matches observations from \u201creal\u201d neural networks: \u201cStudies on brain energy expense suggest that neurons encode information in a sparse and distributed way (Attwell and Laughlin, 2001), estimating the percentage of neurons active at the same time to be between 1 and 4% (Lennie, 2003). This corresponds to a trade-off between richness of representation and small action potential energy expenditure.\u201d (Glorot et al., 2011) Only rectifiers produce that behavior out of the 4 listed activation functions, winning it the title of most plausible function. We note however that rectifiers are not perfect, and scale to infinity for example. A question that I ask myself is, if biological plausibility is so important, why not use the LIF model straight away, or x \u2192 max(0, tanh(x)), or a step function? First off, step functions are not friendly to the back-propagation algorithm, which requires non-zero derivatives: LIF neurons have actually been used successfully in artificial neural networks, as described in Spiking Deep Networks with LIF Neurons (Hunsberger and Eliasmith, 2015)\n\nFinally, generally, a trend seems to be happening Towards Biologically Plausible Deep Learning (Bengio et al., 2016), which seems to invite more plausibility in the field of artificial neural networks, challenging even one of the main algorithm in deep learning, the back-propagation algorithm \u201cWhereas back-propagation offers a machine learning answer, it is not biologically plausible, as discussed in the next paragraph. Finding a biologically plausible machine learning approach for credit assignment in deep networks is the main long-term question to which this paper contributes.\u201d (Bengio et al., 2016) In the context of this article however, we stick to a back-propagation based approach to neural networks, a context in which rectifiers offer a good compromise of performance, cost, and biological plausibility. Rectifiers offer performance comparable to or better than that of tanh and softplus neurons:\n\nFinally, we read that rectifier networks create \u201csparse representations\u201d. In the case of the MNIST above, this translated as networks with as high as 85% of its neurons producing true zeros, without impact on the network\u2019s test error. \u201cAfter uniform initialization of the weights, around 50% of hidden units continuous output values are real zeros, and this fraction can easily increase with sparsity-inducing regularization.\u201d (Glorot et al., 2011) Glorot et al. claim that sparsity has mathematical advantages on top of being biologically plausible. I reproduce below and without demonstration some of the key arguments advanced by the paper, in favor of sparsity: Information disentangling: sparsity reduces the complexity of connections between layers of neurons, isolating key non-zero neurons from irrelevant variations, therefore creating robust information paths. Variable-size representation: each input activates an information path of its own, of a dimensionality that can ultimately be commensurate with complexity of the information received. Computational efficiency: Sparse representations\u2019 efficiency is greater than that of dense ones, even though dense representations are the richest ones."
    },
    {
        "url": "https://medium.com/biffures/all-the-single-neurons-14de29a40f47?source=---------4",
        "title": "All the single neurons \u2013 Biffures \u2013",
        "text": "A neuron in machine learning looks like this if you Google it:\n\nHere is my own representation of a single neuron \u2014 a bit more wavy, unpractical when it comes to drawing networks, hopefully not as bad for education purposes.\n\nA neuron has a body, dendrons to the left connecting to that body, synapses that are receptors for input coming to the neuron; to its right, the neuron has an axon, and an axon terminal (or multiple ones) that transfer the neuron\u2019s information to the next one. The axon terminals connect to synapses.\n\nMessages at the synapses are numbers, that are amplified or reduced as they pass through the dendrons, in proportion to the dendrons weights, noted w, or b in the case of the bias dendron (the bias dendron is always plugged to an incoming signal equal to 1). If a weight w is equal to 0, then the dendron is as good as dead \u2014 it passes no information at all. Signals from each dendron are summed in the neuron\u2019s body, as the weighted input z.\n\nThe weighted input z then passes to the axon where it is optionally touched by an activation function \u03c3. The activation function can be the Identity function (~the activation function does nothing to the weighted input), the Sigmoid function, the Rectified Linear function, but really, can be any function needed for a given application. What we get after the activation function is a\u00b9, the neuron\u2019s activation state.\n\nFrom a place where I can actually use MathJax, here is what I have to say about the forward pass for our single neuron:\n\nSo far our neuron takes input and produces an output, but this is it. If the weights and bias are set in to certain values, the output can be useful \u2014 otherwise we just have a random number generator.\n\nIn order to make the neuron useful we need to train it. So we need a trainer for that neuron, that will (i) be able to evaluate the neuron\u2019s performance, and (ii) create a learning procedure so the neuron performs better. We use a cost function to define (i), and the back-propagation algorithm to address (ii).\n\nSo what now? Well, let\u2019s replace C and \u03c3 in the equations above with actual functions and we will start to see how those formulas are just generalizations of classic regression problems.\n\nThe goal of linear regressions is to find, given a dependent variable and n explanatory variables, a line, expressed as a function of the explanatory variables such that the distance between the dependent variable and that line is minimal. A simple example of a linear regression is:\n\nHere is exactly the same program, but written using the conventions we have used to describe our neuron so far:\n\nin which we recognize a single neuron net problem, where the axon activation function \u03c3 is the identity function, and C is the squared l2 (or Euclidian) norm.\n\nSimilarly, logistic regressions can be expressed as single neuron network problems. Consider the stated goal of a logistic regression:\n\nwhich is equivalent to the single neuron net program:\n\nIn summary, using a single neuron, logistic vs linear regressions are simply a matter of flipping the activation and cost functions of the neuron:\n\nHere is how the single neurons fared on four different regression exercises:"
    },
    {
        "url": "https://medium.com/biffures/reinforcement-learning-from-0-to-something-in-60-days-e67e2a66c762?source=---------5",
        "title": "Reinforcement learning, from 0 to something in 60 days",
        "text": "In this series, I will attempt to learn something about reinforcement learning in a limited period of time, after work hours, with the intent to have a somewhat performant system built by the end of that period. A system that plays Atari games maybe, or a trading system \u2014 we will see.\n\nThere is no intended audience for this series, though if you wish to follow this, you should know that I know some fundamental of machine learnings but am no expert. I will try to learn as much as possible, with 50% planning, 50% improvisation as I will run into yet unknown unknowns. If you are a pro, you might find this boring and inaccurate, if you know nothing about maths and stats, I cannot guarantee you will like this either.\n\nBelow is my work plan, which will also be the summary of my findings, when I produce the associated articles. I will come back to this work plan and update it as we go. Today is Feb 28, 2018, and here is what I think we will explore, until end of April 2018:"
    },
    {
        "url": "https://medium.com/biffures/part-5-hashing-with-sha-256-4c2afc191c40?source=---------6",
        "title": "Part 5: Hashing with SHA-256 \u2013 Biffures \u2013",
        "text": "This continues a series on bitwise operations and their applications, written by a non-expert, for non-experts. Follow Biffures for future updates.\n\nHash functions transform arbitrary large bit strings called messages, into small, fixed-length bit strings called message digests, such that digests identify the messages that produced them with a very high probability. Digests are in that sense fingerprints: a function of the message, simple, yet complex enough that they allow identification of their message, with a very low probability that different messages will share the same digests.\n\nIn SHA-256, messages up to 2\u2076\u2074 bit (2.3 exabytes, or 2.3 billion gigabytes) are transformed into digests of size 256 bits (32 bytes). For perspective, this means that an object 7 times the size of Facebook\u2019s data warehouse in 2014 passed to SHA-256 would produce a chunk of data the size of a 32-letter string of ASCII characters, and that string would the object\u2019s very special fingerprint.\n\nA prominent use case of hashing is data integrity verification of large files, which relies on the comparison of actual and expected message digests, or checksums. Another is hashing as part of the encryption/decryption journey. Before a message can be encrypted with an algorithm like RSA, it needs to be hashed. In the rest of this article, we explore what hashing does to a message, with a view to later develop a better understanding of RSA.\n\nThis article now draws heavily from FIPS 180\u20134 all the while trying to offer some simplifications, but for the full details you may want to refer to the source material instead. With this in mind:\n\n1. Padding. If we note M the message to be hashed, and l its length in bits where l < 2\u2076\u2074, then as a first step we create the padded message M\u2019, which is message M plus a right padding, such that M\u2019 is of length l\u2019, a multiple of 512. Specifically, we use a padding P such that M\u2019 is:\n\n2. Blocks. M\u2019 is parsed into N blocks of size 512 bits, M\u00b9 to M\u1d3a, and each block is expressed as 16 input blocks of size 32 bits, M\u2080 to M\u2081\u2085."
    },
    {
        "url": "https://medium.com/biffures/part-4-bitwise-patterns-7b17dae3eee0?source=---------7",
        "title": "Part 4: Bitwise Patterns \u2013 Biffures \u2013",
        "text": "In the following examples, we work with bit words of fixed length n, and we note a, b two n-bit words. For the examples and illustrations, we set n=6, so the 6-bit words a, b represent positive integers between 0 and 63.\n\nWe explained in Part 1 what bit strings are, and in Part 2 and 3 , that interesting patterns can be found in bitwise operations such as AND and OR. In Part 4, we accelerate these findings as we present key bitwise operations and their patterns with minimal commentary. Usual caveats: I use the notations found in FIPS 180\u20134 ; assume bit strings are representations of positive-only integers, most significant bit to the left.\n\nLooking at the patterns, it seems clear that AND and OR must be connected by some sort of relationship. In fact, at least two are worth mentioning:\n\nThe observant ones will have noted that the addition is the only operation here that I defined numerically only, without explaining what it did on a per-bit basis. In fact, the addition is the only operation on this page that does not qualify as a bitwise operation according to Wikipedia, for it does not \u201coperate on one or more bit patterns or binary numerals at the level of their individual bits\u201d.\n\nIt is possible however to define the addition as a bitwise operation, i.e. as an operation defined first and foremost by what it does to the bits of two n-bit strings. Let\u2019s do it.\n\nWe note a, b the two n-bit strings, and s their sum modulo 2\u207f: s = a + b; and we note a\u1d62, b\u1d62, s\u1d62, the i-th bits in those bit strings, so that for instance, a can be written as: a\u2099\u208b\u2081a\u2099\u208b\u2082\u2026a\u2082a\u2081a\u2080.\n\nThis method is very similar to the addition algorithm we\u2019ve all learnt in elementary school. First step, compute the sum of the rightmost digits. If they add up to more than 1 (i.e., a\u2080 \u2227 b\u2080 = 1), carry 1, and jolt down a zero (a\u2080 \u2295 b\u2080 = 0). If not, do not carry (a\u2080 \u2227 b\u2080 = 0), and jolt down a one if either bit is a one, or a zero if both bits are zeros (a\u2080 \u2295 b\u2080). Repeat the steps for all numbers to the left, but taking into account this time the carry if any, etc., etc.\n\nAnd\u2026 We\u2019ve done it! We now have a bitwise definition of the addition. Real question though, is do we need one and if so, why? Well, as far as understanding what the addition does, we probably don\u2019t need one. However, these instructions actually enable us to create a basic electronic adder using only logic gates. In turn, those adders and logic gates help create arithmetic logic units, which are building blocks in CPUs, GPUs, FPUs. And so \u2014 who knew \u2014 by examining simple bitwise operations, we got our first glance at the world of computing using electronic circuits."
    },
    {
        "url": "https://medium.com/biffures/part-3-or-and-20ccc9938f05?source=---------8",
        "title": "Part 3: Bitwise OR, AND\u2019s sibling \u2013 Biffures \u2013",
        "text": "A review of the bitwise OR function (\u2228 or |) and of its relation to AND.\n\nThe result of a bitwise OR operation between two bit words b1 and b2 is a bit word containing 1s in slots where either b1 or b2 contains 1s. In the example above, b1 has 1s in positions 2, 3, 5 (from right to left), and b2 has 1s in positions 2, 3, 4, 7, so b1 \u2228 b2 is a bit string with 1s in positions 2, 3, 4, 5, 7, that is, 01011110. Again, simple.\n\nJust like we did in The beauty of bitwise AND, we will ignore the numeric function, and try to find the pattern of OR, by examining simple properties as well as some of the first values of a \u2228 b. Let\u2019s go!\n\nIf we note f : (a, b) \u2192 a \u2228 b, where \u2228 is the OR operation, f has the following properties:\n\nWith these observations and a few manual calculations, we get the following graph:\n\nThe pattern is familiar. It shares with that of AND, that it is organized in tiers, with 3, 7, 15 playing key roles in separating those tiers; each tier possesses three square blocks, and all blocks are tied by simple mathematical relationships. (Note: tiers and blocks are defined in the previous post)\n\nThis is enough information to define an iterative way to draw an OR graph of arbitrary size. Here are the results:"
    },
    {
        "url": "https://medium.com/biffures/part-2-the-beauty-of-bitwise-and-or-cdf1d8d87891?source=---------9",
        "title": "Part 2: The beauty of bitwise AND (\u2227 or &) \u2013 Biffures \u2013",
        "text": "Part 2: The beauty of bitwise AND (\u2227 or &) Exploring and visualizing the many facets of the bitwise AND operation\n\nThe result of a bitwise AND operation between two bit words b1 and b2 is a bit word containing 1s in slots where both b1 and b2 contain 1s. In the example above, b1 and b2 both have 1s in positions 2 and 3 (from right to left), hence b1 \u2227 b2 is 00000110. Simple.\n\nWhile the result is easily computed and the process easily understood in base 2 (i.e., using the \u2018bit strings as language\u2019 lens), the numerical AND function looks in fact non-trivial:\n\nYou can test by yourself that this functions indeed gives 22 \u2227 78 = 6.\n\nNow, if you find that function dreadful \u2014 I am with you. When I first wrote it down, I found it both complicated and unhelpful; after formulating it, my mind was no closer to understanding the kind of pattern the AND function followed, if any.\n\nIgnoring the complex math formula above, we can still find a number of interesting properties regarding the AND function. Considering the function f : (a, b) \u2192 a \u2227 b, where \u2227 is the AND operation, f has the following (easy to demonstrate) properties:\n\nThis little information is enough to get us started; with some additional calculations, we find the following graph:\n\nThe horizontal axis is for values of the first operand a; the vertical axis is for values of the second one b; values in cells is the result of f(a,b) = a \u2227 b.\n\nThis chart has striking features that we did not foresee in our preliminary analysis. Observations:\n\nAnd very naturally, through simple observations and intuitions, we have found a generalized way to draw any arbitrary-sized graph for the AND function:\n\nI get that my hand-drawn charts are far from perfect. If you have survived till this section, you deserve to see the better graphs \u2014 made this time with d3js.\n\nAs before, cells represent f(a,b) = a \u2227 b for values of a determined by the horizontal axis; and values of b on the vertical axis; colors are a function of the value (darker is low, brighter is high).\n\nWe easily recognize the features determined above: the linear growth along the bisector, the symmetry, the tiers and blocks, and the mathematical relation between all those blocks.\n\nWhile this says a lot about the AND function and could be interpreted for many more minutes, I also find this graph simply nice to contemplate. There is a beauty to this AND graph similar to the one I find in Pascal\u2019s triangle. And I hope that beauty has become slightly more visible to you today.\n\nEdits based on the HN discussion: Thanks for the great response! Truly humbled to see your interest for that topic and article. Based on feedback: (i) corrected, f(a, b) \u2264 min(a, b) (vs max initially). (ii) The assumption is indeed that we work here with natural integers only; though bit strings for negative integers behave the same way as bit strings for natural ones in bitwise operations, the transcription from base 2 to base 10 for those two groups does not work the same (see 2\u2019s complement). For the sake of simplicity, we assume bit strings as numbers to always be positive integers using the convention that any bit in position i is just worth 2^i."
    },
    {
        "url": "https://medium.com/biffures/bits-101-120f75aeb75a",
        "title": "Part 1: Bits 101 \u2013 Biffures \u2013",
        "text": "Secondly, a few properties which I find useful to remember at all times when dealing with bits:\n\nBits strings as binary numbers. In this interpretation, bit strings are simply numbers; \u2018truth-y\u2019 bits in position i are worth 2\u2071, and bit strings are equal to the sum of their bits (more about the general concept in this Wikipedia article).\n\nHere, the binary representation is a contingency, useful to understand and define specific functions that will run fast on computers given their binary architectures. However, because the notation does not convey meaning in itself, binary numbers and functions are as well described in any arbitrary base, for example base 10 for humans, or base 16 (hexadecimal) for conciseness.\n\nBits strings as language. In this view, bit words are more similar to human words: each bit, as a letter, helps form a bit word; letters do not increase the \u201cvalue\u201d of words but do change their meanings; converting words to numbers is possible but counterproductive.\n\nAs an example, think of \u201cpear\u201d and \u201cpeer\u201d: the difference in meaning between these words is useful, but the fact that their number value would be different \u2014 if you were to convert \u2014 is irrelevant. Likewise, in this interpretation of bit strings, we consider that 10001 and 11001 differ in an interesting way, not because their base-10 value is different, but because their meaning, according to a defined grammar, is different. We find such grammars (though they may not be called as such) in UTF-8 encoding, in bit flags, or in padding steps for encryption and hashing algorithms."
    },
    {
        "url": "https://medium.com/biffures/launching-bitwise-bits-from-the-ground-up-9565303caf13",
        "title": "Launching Bitwise: bits from the ground up! \u2013 Biffures \u2013",
        "text": "TL,DR: this series will cover bits (as in, 01110110101), the base signals that run today\u2019s computers; their behavior and their roles in some of the most common and fundamental computer applications: encoding, hashing, encryption.\n\nIt is written by a learning enthusiast with a decent scientific background and no experience writing on such topics (hi! :3), so please feel free to let me know if you find outrageous mistakes in this series!\n\nWhile this may simply remain a private repo for my learnings, any fellow curious minds that would stumble upon this is warmly welcomed. Happy reading!"
    }
]