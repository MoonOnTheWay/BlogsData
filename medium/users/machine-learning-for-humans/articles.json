[
    {
        "url": "https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12?source=---------0",
        "title": "A Beginner\u2019s Guide to AI/ML \ud83e\udd16\ud83d\udc76 \u2013 Machine Learning for Humans \u2013",
        "text": "Part 1: Why Machine Learning Matters. The big picture of artificial intelligence and machine learning \u2014 past, present, and future.\n\nPart 2.1: Supervised Learning. Learning with an answer key. Introducing linear regression, loss functions, overfitting, and gradient descent.\n\nPart 2.2: Supervised Learning II. Two methods of classification: logistic regression and SVMs.\n\nPart 4: Neural Networks & Deep Learning. Why, where, and how deep learning works. Drawing inspiration from the brain. Convolutional neural networks (CNNs), recurrent neural networks (RNNs). Real-world applications.\n\nPart 5: Reinforcement Learning. Exploration and exploitation. Markov decision processes. Q-learning, policy learning, and deep reinforcement learning. The value learning problem.\n\nAppendix: The Best Machine Learning Resources. A curated list of resources for creating your machine learning curriculum.\n\nThis guide is intended to be accessible to anyone. Basic concepts in probability, statistics, programming, linear algebra, and calculus will be discussed, but it isn\u2019t necessary to have prior knowledge of them to gain value from this series.\n\nArtificial intelligence will shape our future more powerfully than any other innovation this century. Anyone who does not understand it will soon find themselves feeling left behind, waking up in a world full of technology that feels more and more like magic.\n\nThe rate of acceleration is already astounding. After a couple of AI winters and periods of false hope over the past four decades, rapid advances in data storage and computer processing power have dramatically changed the game in recent years.\n\nIn 2015, Google trained a conversational agent (AI) that could not only convincingly interact with humans as a tech support helpdesk, but also discuss morality, express opinions, and answer general facts-based questions.\n\nThe same year, DeepMind developed an agent that surpassed human-level performance at 49 Atari games, receiving only the pixels and game score as inputs. Soon after, in 2016, DeepMind obsoleted their own achievement by releasing a new state-of-the-art gameplay method called A3C.\n\nMeanwhile, AlphaGo defeated one of the best human players at Go \u2014 an extraordinary achievement in a game dominated by humans for two decades after machines first conquered chess. Many masters could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient Chinese war strategy game, with its 10\u00b9\u2077\u2070 possible board positions (there are only 10\u2078\u2070atoms in the universe).\n\nIn March 2017, OpenAI created agents that invented their own language to cooperate and more effectively achieve their goal. Soon after, Facebook reportedly successfully training agents to negotiate and even lie.\n\nJust a few days ago (as of this writing), on August 11, 2017, OpenAI reached yet another incredible milestone by defeating the world\u2019s top professionals in 1v1 matches of the online multiplayer game Dota 2.\n\nMuch of our day-to-day technology is powered by artificial intelligence. Point your camera at the menu during your next trip to Taiwan and the restaurant\u2019s selections will magically appear in English via the Google Translate app.\n\nToday AI is used to design evidence-based treatment plans for cancer patients, instantly analyze results from medical tests to escalate to the appropriate specialist immediately, and conduct scientific research for drug discovery.\n\nIn everyday life, it\u2019s increasingly commonplace to discover machines in roles traditionally occupied by humans. Really, don\u2019t be surprised if a little housekeeping delivery bot shows up instead of a human next time you call the hotel desk to send up some toothpaste.\n\nIn this series, we\u2019ll explore the core machine learning concepts behind these technologies. By the end, you should be able to describe how they work at a conceptual level and be equipped with the tools to start building similar applications yourself.\n\nArtificial intelligence is the study of agents that perceive the world around them, form plans, and make decisions to achieve their goals. Its foundations include mathematics, logic, philosophy, probability, linguistics, neuroscience, and decision theory. Many fields fall under the umbrella of AI, such as computer vision, robotics, machine learning, and natural language processing.\n\nMachine learning is a subfield of artificial intelligence. Its goal is to enable computers to learn on their own. A machine\u2019s learning algorithm enables it to identify patterns in observed data, build models that explain the world, and predict things without having explicit pre-programmed rules and models.\n\nThe technologies discussed above are examples of artificial narrow intelligence (ANI), which can effectively perform a narrowly defined task.\n\nMeanwhile, we\u2019re continuing to make foundational advances towards human-level artificial general intelligence (AGI), also known as strong AI. The definition of an AGI is an artificial intelligence that can successfully perform any intellectual task that a human being can, including learning, planning and decision-making under uncertainty, communicating in natural language, making jokes, manipulating people, trading stocks, or\u2026 reprogramming itself.\n\nAnd this last one is a big deal. Once we create an AI that can improve itself, it will unlock a cycle of recursive self-improvement that could lead to an intelligence explosion over some unknown time period, ranging from many decades to a single day.\n\nYou may have heard this point referred to as the singularity. The term is borrowed from the gravitational singularity that occurs at the center of a black hole, an infinitely dense one-dimensional point where the laws of physics as we understand them start to break down.\n\nA recent report by the Future of Humanity Institute surveyed a panel of AI researchers on timelines for AGI, and found that \u201cresearchers believe there is a 50% chance of AI outperforming humans in all tasks in 45 years\u201d (Grace et al, 2017). We\u2019ve personally spoken with a number of sane and reasonable AI practitioners who predict much longer timelines (the upper limit being \u201cnever\u201d), and others whose timelines are alarmingly short \u2014 as little as a few years.\n\nThe advent of greater-than-human-level artificial superintelligence (ASI) could be one of the best or worst things to happen to our species. It carries with it the immense challenge of specifying what AIs will want in a way that is friendly to humans.\n\nWhile it\u2019s impossible to say what the future holds, one thing is certain: 2017 is a good time to start understanding how machines think. To go beyond the abstractions of a philosopher in an armchair and intelligently shape our roadmaps and policies with respect to AI, we must engage with the details of how machines see the world \u2014 what they \u201cwant\u201d, their potential biases and failure modes, their temperamental quirks \u2014 just as we study psychology and neuroscience to understand how humans learn, decide, act, and feel.\n\nMachine learning is at the core of our journey towards artificial general intelligence, and in the meantime, it will change every industry and have a massive impact on our day-to-day lives. That\u2019s why we believe it\u2019s worth understanding machine learning, at least at a conceptual level \u2014 and we designed this series to be the best place to start.\n\nYou don\u2019t necessarily need to read the series cover-to-cover to get value out of it. Here are three suggestions on how to approach it, depending on your interests and how much time you have:\n\nVishal most recently led growth at Upstart, a lending platform that utilizes machine learning to price credit, automate the borrowing process, and acquire users. He spends his time thinking about startups, applied cognitive science, moral philosophy, and the ethics of artificial intelligence.\n\nSamer is a Master\u2019s student in Computer Science and Engineering at UCSD and co-founder of Conigo Labs. Prior to grad school, he founded TableScribe, a business intelligence tool for SMBs, and spent two years advising Fortune 100 companies at McKinsey. Samer previously studied Computer Science and Ethics, Politics, and Economics at Yale.\n\nMost of this series was written during a 10-day trip to the United Kingdom in a frantic blur of trains, planes, cafes, pubs and wherever else we could find a dry place to sit. Our aim was to solidify our own understanding of artificial intelligence, machine learning, and how the methods therein fit together \u2014 and hopefully create something worth sharing in the process.\n\nAnd now, without further ado, let\u2019s dive into machine learning with Part 2.1: Supervised Learning!"
    },
    {
        "url": "https://medium.com/machine-learning-for-humans/how-to-learn-machine-learning-24d53bb64aa1?source=---------1",
        "title": "The Best Machine Learning Resources \u2013 Machine Learning for Humans \u2013",
        "text": "Going to school for a formal degree program for isn\u2019t always possible or desirable. For those considering an autodidactic alternative, this is for you.\n\n1. Build foundations, and then specialize in areas of interest.\n\nYou can\u2019t go deeply into every machine learning topic. There\u2019s too much to learn, and the field is advancing rapidly. Master foundational concepts and then focus on projects in a specific domain of interest \u2014 whether it\u2019s natural language understanding, computer vision, deep reinforcement learning, robotics, or whatever else.\n\n2. Design your curriculum around topics that personally excite you.\n\nMotivation is far more important than micro-optimizing a learning strategy for some long-term academic or career goal. If you\u2019re having fun, you\u2019ll make fast progress. If you\u2019re trying to force yourself forward, you\u2019ll slow down."
    },
    {
        "url": "https://medium.com/machine-learning-for-humans/reinforcement-learning-6eacf258b265?source=---------2",
        "title": "Machine Learning for Humans, Part 5: Reinforcement Learning",
        "text": "In supervised learning, training data comes with an answer key from some godlike \u201csupervisor\u201d. If only life worked that way!\n\nIn reinforcement learning (RL) there\u2019s no answer key, but your reinforcement learning agent still has to decide how to act to perform its task. In the absence of existing training data, the agent learns from experience. It collects the training examples (\u201cthis action was good, that action was bad\u201d) through trial-and-error as it attempts its task, with the goal of maximizing long-term reward.\n\nIn this final section of Machine Learning for Humans, we will explore:\n\nAt the end, as always, we\u2019ve compiled some favorite resources for further exploration.\n\nThe easiest context in which to think about reinforcement learning is in games with a clear objective and a point system.\n\nSay we\u2019re playing a game where our mouse \ud83d\udc2d is seeking the ultimate reward of cheese at the end of the maze (\ud83e\uddc0 +1000 points), or the lesser reward of water along the way (\ud83d\udca7+10 points). Meanwhile, robo-mouse wants to avoid locations that deliver an electric shock (\u26a1 -100 points).\n\nAfter a bit of exploration, the mouse might find the mini-paradise of three water sources clustered near the entrance and spend all its time exploiting that discovery by continually racking up the small rewards of these water sources and never going further into the maze to pursue the larger prize.\n\nBut as you can see, the mouse would then miss out on an even better oasis further in the maze, or the ultimate reward of cheese at the end!\n\nThis brings up the exploration/exploitation tradeoff. One simple strategy for exploration would be for the mouse to take the best known action most of the time (say, 80% of the time), but occasionally explore a new, randomly selected direction even though it might be walking away from known reward.\n\nThis strategy is called the epsilon-greedy strategy, where epsilon is the percent of the time that the agent takes a randomly selected action rather than taking the action that is most likely to maximize reward given what it knows so far (in this case, 20%). We usually start with a lot of exploration (i.e. a higher value for epsilon). Over time, as the mouse learns more about the maze and which actions yield the most long-term reward, it would make sense to steadily reduce epsilon to 10% or even lower as it settles into exploiting what it knows.\n\nIt\u2019s important to keep in mind that the reward is not always immediate: in the robot-mouse example, there might be a long stretch of the maze you have to walk through and several decision points before you reach the cheese.\n\nThe mouse\u2019s wandering through the maze can be formalized as a Markov Decision Process, which is a process that has specified transition probabilities from state to state. We will explain it by referring to our robot-mouse example. MDPs include:\n\nNow that we know what an MDP is, we can formalize the mouse\u2019s objective. We\u2019re trying to maximize the sum of rewards in the long term:\n\nLet\u2019s look at this sum term by term. First of all, we\u2019re summing across all time steps t. Let\u2019s set \u03b3 at 1 for now and forget about it. r(x,a) is a reward function. For state x and action a (i.e., go left at a crossroads) it gives you the reward associated with taking that action a at state x. Going back to our equation, we\u2019re trying to maximize the sum of future rewards by taking the best action in each state.\n\nNow that we\u2019ve set up our reinforcement learning problem and formalized the goal, let\u2019s explore some possible solutions.\n\nQ-learning is a technique that evaluates which action to take based on an action-value function that determines the value of being in a certain state and taking a certain action at that state.\n\nWe have a function Q that takes as an input one state and one action and returns the expected reward of that action (and all subsequent actions) at that state. Before we explore the environment, Q gives the same (arbitrary) fixed value. But then, as we explore the environment more, Q gives us a better and better approximation of the value of an action a at a state s. We update our function Q as we go.\n\nThis equation from the Wikipedia page on Q-learning explains it all very nicely. It shows how we update the value of Q based on the reward we get from our environment:\n\nLet\u2019s ignore the discount factor \u03b3 by setting it to 1 again. First, keep in mind that Q is supposed to show you the full sum of rewards from choosing action Q and all the optimal actions afterward.\n\nNow let\u2019s go through the equation from left to right. When we take action at in state st, we update our value of Q(st,at) by adding a term to it. This term contains:\n\nNow that we have a value estimate for each state-action pair, we can select which action to take according to our action-selection strategy (we don\u2019t necessarily just choose the action that leads to the most expected reward every time, e.g. with an epsilon-greedy exploration strategy we\u2019d take a random action some percentage of the time).\n\nIn the robot mouse example, we can use Q-learning to figure out the value of each position in the maze and the value of the actions {forward, backward, left, right} at each position. Then we can use our action-selection strategy to choose what the mouse actually does at each time step.\n\nIn the Q-learning approach, we learned a value function that estimated the value of each state-action pair.\n\nPolicy learning is a more straightforward alternative in which we learn a policy function, \u03c0, which is a direct map from each state to the best corresponding action at that state. Think of it as a behavioral policy: \u201cwhen I observe state s, the best thing to do is take action a\u201d. For example, an autonomous vehicle\u2019s policy might effectively include something like: \u201cif I see a yellow light and I am more than 100 feet from the intersection, I should brake. Otherwise, keep moving forward.\u201d\n\nSo we\u2019re learning a function that will maximize expected reward. What do we know that\u2019s really good at learning complex functions? Deep neural networks!\n\nAndrej Karpathy\u2019s Pong from Pixels provides an excellent walkthrough on using deep reinforcement learning to learn a policy for the Atari game Pong that takes raw pixels from the game as the input (state) and outputs a probability of moving the paddle up or down (action).\n\nIf you want to get your hands dirty with deep RL, work through Andrej\u2019s post. You will implement a 2-layer policy network in 130 lines of code, and will also learn how to plug into OpenAI\u2019s Gym, which allows you to quickly get up and running with your first reinforcement learning algorithm, test it on a variety of games, and see how its performance compares to other submissions.\n\nIn 2015, DeepMind used a method called deep Q-networks (DQN), an approach that approximates Q-functions using deep neural networks, to beat human benchmarks across many Atari games:\n\nWe demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks. (Silver et al., 2015)\n\nHere is a snapshot of where DQN agents stand relative to linear learners and humans in various domains:\n\nTo help you build some intuition for how advancements are made in RL research, here are some examples of improvements on attempts at non-linear Q-function approximators that can improve performance and stability:\n\nIn 2016, just one year after the DQN paper, DeepMind revealed another algorithm called Asynchronous Advantage Actor-Critic (A3C) that surpassed state-of-the-art performance on Atari games after training for half as long (Mnih et al., 2016). A3C is an actor-critic algorithm that combines the best of both approaches we explored earlier: it uses an actor (a policy network that decides how to act) AND a critic (a Q-network that decides how valuable things are). Arthur Juliani has a nice writeup on how A3C works specifically. A3C is now OpenAI\u2019s Universe Starter Agent.\n\nSince then, there have been countless fascinating breakthroughs \u2014 from AIs inventing their own language to teaching themselves to walk in a variety of terrains. This series only scratches the surface on the cutting edge of RL, but hopefully it will serve as a starting point for further exploration!\n\nAs a parting note, we\u2019d like to share this incredible video of DeepMind\u2019s agents that learned to walk\u2026 with added sound. Grab some popcorn, turn up the volume, and witness the full glory of artificial intelligence."
    },
    {
        "url": "https://medium.com/machine-learning-for-humans/neural-networks-deep-learning-cdad8aeae49b?source=---------3",
        "title": "Machine Learning for Humans, Part 4: Neural Networks & Deep Learning",
        "text": "With deep learning, we\u2019re still learning a function f to map input X to output Y with minimal loss on the test data, just as we\u2019ve been doing all along. Recall our initial \u201cproblem statement\u201d from Part 2.1 on supervised learning:\n\nThe real world is messy, so sometimes f is complicated. In natural language problems large vocabulary sizes mean lots of features. Vision problems involve lots of visual information about pixels. Playing games requires making a decision based on complex scenarios with many possible futures. The learning techniques we\u2019ve covered so far do well when the data we\u2019re working with is not insanely complex, but it\u2019s not clear how they\u2019d generalize to scenarios like these.\n\nDeep learning is really good at learning f, particularly in situations where the data is complex. In fact, artificial neural networks are known as universal function approximators because they\u2019re able to learn any function, no matter how wiggly, with just a single hidden layer.\n\nLet\u2019s look at the problem of image classification. We take an image as an input, and output a class (e.g., dog, cat, car).\n\nGraphically, a deep neural network solving image classification looks something like this:\n\nBut really, this is a giant mathematical equation with millions of terms and lots of parameters. The input X is, say, a greyscale image represented by a w-by-h matrix of pixel brightnesses. The output Y is a vector of class probabilities. This means we have as an output the probability of each class being the correct label. If this neural net is working well, the highest probability should be for the correct class. And the layers in the middle are just doing a bunch of matrix multiplication by summing activations x weights with non-linear transformations (activation functions) after every hidden layer to enable the network to learn a non-linear function.\n\nIncredibly, you can use gradient descent in the exact same way that we did with linear regression in Part 2.1 to train these parameters in a way that minimizes loss. So with a lot of examples and a lot of gradient descent, the model can learn how to classify images of animals correctly. And that, in a nutshell\u2019s nutshell, is \u201cdeep learning\u201d.\n\nArtificial neural networks have actually been around for a long time. Their application has been historically referred to as cybernetics (1940s-1960s), connectionism (1980s-1990s), and then came into vogue as deep learning circa 2006 when neural networks started getting, well, \u201cdeeper\u201d (Goodfellow et al., 2016). But only recently have we really started to scratch the surface of their full potential.\n\nAs described by Andrej Karpathy (Director of AI at Tesla, whom we tend to think of as the Shaman of Deep Learning), there are generally \u201cfour separate factors that hold back AI:\n\nIn the past decade or so, the full potential of deep learning is finally being unlocked by advances in (1) and (2), which in turn has led to further breakthroughs in (3) and (4) \u2014 and so the cycle continues, with exponentially more humans rallying to the frontlines of deep learning research along the way (just think about what you\u2019re doing right now!)\n\nIn the rest of this section, we\u2019ll provide some background from biology and statistics to explain what happens inside neural nets, and then talk through some amazing applications of deep learning. Finally, we\u2019ll link to a few resources so you can apply deep learning yourself, even sitting on the couch in your pajamas with a laptop, to quickly achieve greater-than-human-level performance on certain types of problems.\n\nAs you read these words you aren\u2019t examining every letter of every word, or every pixel making up each letter, to derive the meaning of the words. You\u2019re abstracting away from the details and grouping things into higher-level concepts: words, phrases, sentences, paragraphs.\n\nThe same thing happens in vision, not just in humans but in animals\u2019 visual systems generally.\n\nBrains are made up of neurons which \u201cfire\u201d by emitting electrical signals to other neurons after being sufficiently \u201cactivated\u201d. These neurons are malleable in terms of how much a signal from other neurons will add to the activation level of the neuron (vaguely speaking, the weights connecting neurons to each other end up being trained to make the neural connections more useful, just like the parameters in a linear regression can be trained to improve the mapping from input to output).\n\nOur biological networks are arranged in a hierarchical manner, so that certain neurons end up detecting not extremely specific features of the world around us, but rather more abstract features, i.e. patterns or groupings of more low-level features. For example, the fusiform face area in the human visual system is specialized for facial recognition.\n\nThis hierarchical structure exhibited by biological neural networks was discovered in the 1950s when researchers David Hubel and Torsten Wiesel were studying neurons in the visual cortex of cats. They were unable to observe neural activation after exposing the cat to a variety of stimuli: dark spots, light spots, hand-waving, and even pictures of women in magazines. But in their frustration, as they removed a slide from the projector at a diagonal angle, they noticed some neural activity! It turned out that diagonal edges at a very particular angle were causing certain neurons to be activated.\n\nThis makes sense evolutionarily since natural environments are generally noisy and random (imagine a grassy plain or a rocky terrain). So when a feline in the wild perceives an \u201cedge\u201d, i.e. a line that contrasts from its background, this might indicate that an object or creature is in the visual field. When a certain combination of edge neurons are activated, those activations will combine to yield a yet more abstract activation, and so on, until the final abstraction is a useful concept, like \u201cbird\u201d or \u201cwolf\u201d.\n\nThe idea behind a deep neural network is to mimic a similar structure with layers of artificial neurons.\n\nTo draw from Stanford\u2019s excellent deep learning course, CS231n: Convolutional Neural Networks and Visual Recognition, imagine that we want to train a neural network to classify images with the correct one of the following labels: .\n\nOne approach could be to construct a \u201ctemplate\u201d, or average image, of each class of image using the training examples, and then use a nearest-neighbors algorithm during testing to measure the distance of each unclassified image\u2019s pixel values, in aggregate, to each template. This approach involves no layers of abstraction. It\u2019s a linear model that combines all the different orientations of each type of image into one averaged blur.\n\nFor instance, it would take all the cars \u2014 regardless of whether they\u2019re facing left, right, center, and regardless of their color \u2014 and average them. The template then ends up looking rather vague and blurry.\n\nNotice that the horse template above appears to have two heads. This doesn\u2019t really help us: we want to be able to detect right-facing horse or a left-facing horse separately, and then if either one of those features is detected, then we want to say we\u2019re looking at a horse. This flexibility is provided by deep neural nets, as we will see in the next section.\n\nDeep neural networks approach the image classification problem using layers of abstraction\n\nTo repeat what we explained earlier in this section: the input layer will take raw pixel brightnesses of an image. The final layer will be an output vector of class probabilities (i.e. the probability of the image being a \u201ccat\u201d, \u201ccar\u201d, \u201chorse\u201d, etc.)\n\nBut instead of learning a simple linear model relating input to output, we\u2019ll instead construct intermediate hidden layers of the network will learn increasingly abstract features, which enables us to not lose all the nuance in the complex data.\n\nJust as we described animal brains detecting abstract features, the artificial neurons in the hidden layers will learn to detect abstract concepts \u2014 whichever concepts are ultimately most useful for capturing the most information and minimizing loss in the accuracy of the network\u2019s output (this is an instance of unsupervised learning happening within the network).\n\nThis comes at the cost of model interpretability, since as you add in more hidden layers the neurons start representing more and more abstract and ultimately unintelligible features \u2014 to the point that you may hear deep learning referred to as \u201cblack box optimization\u201d, where you basically are just trying stuff somewhat at random and seeing what comes out, without really understanding what\u2019s happening inside.\n\nLinear regression is interpretable because you decided which features to include in the model. Deep neural networks are harder to interpret because the features are learned and aren\u2019t explained anywhere in English. It\u2019s all in the machine\u2019s imagination.\n\nDeep learning is reshaping the world in virtually every domain. Here are a few examples of the incredible things that deep learning can do\u2026\n\n\u2026and many, many, more.\n\nWe haven\u2019t gone into as much detail here on how neural networks are set up in practice because it\u2019s much easier to understand the details by implementing them yourself. Here are some amazing hands-on resources for getting started.\n\nDeep learning is an expansive subject area. Accordingly, we\u2019ve also compiled some of the best resources we\u2019ve encountered on the topic, in case you\u2019d like to go\u2026 deeper.\n\nLast, but most certainly not least, is Part 5: Reinforcement Learning."
    },
    {
        "url": "https://medium.com/machine-learning-for-humans/unsupervised-learning-f45587588294?source=---------4",
        "title": "Machine Learning for Humans, Part 3: Unsupervised Learning",
        "text": "How do you find the underlying structure of a dataset? How do you summarize it and group it most usefully? How do you effectively represent data in a compressed format? These are the goals of unsupervised learning, which is called \u201cunsupervised\u201d because you start with unlabeled data (there\u2019s no Y).\n\nThe two unsupervised learning tasks we will explore are clustering the data into groups by similarity and reducing dimensionality to compress the data while maintaining its structure and usefulness.\n\nIn contrast to supervised learning, it\u2019s not always easy to come up with metrics for how well an unsupervised learning algorithm is doing. \u201cPerformance\u201d is often subjective and domain-specific.\n\nAn interesting example of clustering in the real world is marketing data provider Acxiom\u2019s life stage clustering system, Personicx. This service segments U.S. households into 70 distinct clusters within 21 life stage groups that are used by advertisers when targeting Facebook ads, display ads, direct mail campaigns, etc.\n\nTheir white paper reveals that they used centroid clustering and principal component analysis, both of which are techniques covered in this section.\n\nYou can imagine how having access to these clusters is extremely useful for advertisers who want to (1) understand their existing customer base and (2) use their ad spend effectively by targeting potential new customers with relevant demographics, interests, and lifestyles.\n\nLet\u2019s walk through a couple of clustering methods to develop intuition for how this task can be performed.\n\n\u201cAnd k rings were given to the race of Centroids, who above all else, desire power.\u201d\n\nThe goal of clustering is to create groups of data points such that points in different clusters are dissimilar while points within a cluster are similar.\n\nWith k-means clustering, we want to cluster our data points into k groups. A larger k creates smaller groups with more granularity, a lower k means larger groups and less granularity.\n\nThe output of the algorithm would be a set of \u201clabels\u201d assigning each data point to one of the k groups. In k-means clustering, the way these groups are defined is by creating a centroid for each group. The centroids are like the heart of the cluster, they \u201ccapture\u201d the points closest to them and add them to the cluster.\n\nThink of these as the people who show up at a party and soon become the centers of attention because they\u2019re so magnetic. If there\u2019s just one of them, everyone will gather around; if there are lots, many smaller centers of activity will form.\n\nThat, in short, is how k-means clustering works! Check out this visualization of the algorithm \u2014 read it like a comic book. Each point in the plane is colored according the centroid that it is closest to at each moment. You\u2019ll notice that the centroids (the larger blue, red, and green circles) start randomly and then quickly adjust to capture their respective clusters.\n\nAnother real-life application of k-means clustering is classifying handwritten digits. Suppose we have images of the digits as a long vector of pixel brightnesses. Let\u2019s say the images are black and white and are 64x64 pixels. Each pixel represents a dimension. So the world these images live in has 64x64=4,096 dimensions. In this 4,096-dimensional world, k-means clustering allows us to group the images that are close together and assume they represent the same digit, which can achieve pretty good results for digit recognition.\n\n\u201cLet\u2019s make a million options become seven options. Or five. Or twenty? Meh, we can decide later.\u201d\n\nHierarchical clustering is similar to regular clustering, except that you\u2019re aiming to build a hierarchy of clusters. This can be useful when you want flexibility in how many clusters you ultimately want. For example, imagine grouping items on an online marketplace like Etsy or Amazon. On the homepage you\u2019d want a few broad categories of items for simple navigation, but as you go into more specific shopping categories you\u2019d want increasing levels of granularity, i.e. more distinct clusters of items.\n\nIn terms of outputs from the algorithm, in addition to cluster assignments you also build a nice tree that tells you about the hierarchies between the clusters. You can then pick the number of clusters you want from this tree.\n\n\u201cIt is not the daily increase, but the daily decrease. Hack away at the unessential.\u201d \u2014 Bruce Lee\n\nDimensionality reduction looks a lot like compression. This is about trying to reduce the complexity of the data while keeping as much of the relevant structure as possible. If you take a simple 128 x 128 x 3 pixels image (length x width x RGB value), that\u2019s 49,152 dimensions of data. If you\u2019re able to reduce the dimensionality of the space in which these images live without destroying too much of the meaningful content in the images, then you\u2019ve done a good job at dimensionality reduction.\n\nWe\u2019ll take a look at two common techniques in practice: principal component analysis and singular value decomposition.\n\nFirst, a little linear algebra refresher \u2014 let\u2019s talk about spaces and bases.\n\nYou\u2019re familiar with the coordinate plane with origin O(0,0) and basis vectors i(1,0) and j(0,1). It turns out you can choose a completely different basis and still have all the math work out. For example, you can keep O as the origin and choose the basis to vectors i\u2019=(2,1) and j\u2019=(1,2). If you have the patience for it, you\u2019ll convince yourself that the point labeled (2,2) in the i\u2019, j\u2019 coordinate system is labeled (6, 6) in the i, j system.\n\nThis means we can change the basis of a space. Now imagine much higher-dimensional space. Like, 50K dimensions. You can select a basis for that space, and then select only the 200 most significant vectors of that basis. These basis vectors are called principal components, and the subset you select constitute a new space that is smaller in dimensionality than the original space but maintains as much of the complexity of the data as possible.\n\nTo select the most significant principal components, we look at how much of the data\u2019s variance they capture and order them by that metric.\n\nAnother way of thinking about this is that PCA remaps the space in which our data exists to make it more compressible. The transformed dimension is smaller than the original dimension.\n\nBy making use of the first several dimensions of the remapped space only, we can start gaining an understanding of the dataset\u2019s organization. This is the promise of dimensionality reduction: reduce complexity (dimensionality in this case) while maintaining structure (variance). Here\u2019s a fun paper Samer wrote on using PCA (and diffusion mapping, another technique) to try to make sense of the Wikileaks cable release.\n\nLet\u2019s represent our data like a big A = m x n matrix. SVD is a computation that allows us to decompose that big matrix into a product of 3 smaller matrices (U=m x r, diagonal matrix \u03a3=r x r, and V=r x n where r is a small number).\n\nHere\u2019s a more visual illustration of that product to start with:\n\nThe values in the r*r diagonal matrix \u03a3 are called singular values. What\u2019s cool about them is that these singular values can be used to compress the original matrix. If you drop the smallest 20% of singular values and the associated columns in matrices U and V, you save quite a bit of space and still get a decent representation of the underlying matrix.\n\nTo examine what that means more precisely, let\u2019s work with this image of a dog:\n\nWe\u2019ll use the code written in Andrew Gibiansky\u2019s post on SVD. First, we show that if we rank the singular values (the values of the matrix \u03a3) by magnitude, the first 50 singular values contain 85% of the magnitude of the whole matrix \u03a3.\n\nWe can use this fact to discard the next 250 values of sigma (i.e., set them to 0) and just keep a \u201crank 50\u201d version of the image of the dog. Here, we create a rank 200, 100, 50, 30, 20, 10, and 3 dog. Obviously, the picture is smaller, but let\u2019s agree that the rank 30 dog is still good. Now let\u2019s see how much compression we achieve with this dog. The original image matrix is 305*275 = 83,875 values. The rank 30 dog is 305*30+30+30*275=17,430 \u2014 almost 5 times fewer values with very little loss in image quality. The reason for the calculation above is that we also discard the parts of the matrix U and V that get multiplied by zeros when the operation U\u03a3\u2019V is carried out (where \u03a3\u2019 is the modified version of \u03a3 that only has the first 30 values in it).\n\nUnsupervised learning is often used to preprocess the data. Usually, that means compressing it in some meaning-preserving way like with PCA or SVD before feeding it to a deep neural net or another supervised learning algorithm.\n\nNow that you\u2019ve finished this section, you\u2019ve earned an awful, horrible, never-to-be-mentioned-again joke about unsupervised learning. Here goes\u2026\n\nPerson-in-joke-#1: Y would u ever need to use unsupervised tho?\n\nPlay around with this clustering visualization to build intuition for how the algorithm works. Then, take a look at this implementation of k-means clustering for handwritten digits and the associated tutorial.\n\nFor a good reference on SVD, go no further than Andrew Gibiansky\u2019s post."
    },
    {
        "url": "https://medium.com/machine-learning-for-humans/supervised-learning-3-b1551b9c4930?source=---------5",
        "title": "Machine Learning for Humans, Part 2.3: Supervised Learning III",
        "text": "Things are about to get a little\u2026 wiggly.\n\nIn contrast to the methods we\u2019ve covered so far \u2014 linear regression, logistic regression, and SVMs where the form of the model was pre-defined \u2014 non-parametric learners do not have a model structure specified a priori. We don\u2019t speculate about the form of the function f that we\u2019re trying to learn before training the model, as we did previously with linear regression. Instead, the model structure is purely determined from the data.\n\nThese models are more flexible to the shape of the training data, but this sometimes comes at the cost of interpretability. This will make more sense soon. Let\u2019s jump in.\n\n\u201cYou are the average of your k closest friends.\u201d\n\nk-NN seems almost too simple to be a machine learning algorithm. The idea is to label a test data point x by finding the mean (or mode) of the k closest data points\u2019 labels.\n\nTake a look at the image below. Let\u2019s say you want to figure out whether Mysterious Green Circle is a Red Triangle or a Blue Square. What do you do?\n\nYou could try to come up with a fancy equation that looks at where Green Circle lies on the coordinate plane below and makes a prediction accordingly. Or, you could just look its three nearest neighbors, and guess that Green Circle is probably a Red Triangle. You could also expand the circle further and look at the five nearest neighbors, and make a prediction that way (3/5 of its five nearest neighbors are Blue Squares, so we\u2019d guess that Mysterious Green Circle is a Blue Square when k=5).\n\nThat\u2019s it. That\u2019s k-nearest neighbors. You look at the k closest data points and take the average of their values if variables are continuous (like housing prices), or the mode if they\u2019re categorical (like cat vs. dog).\n\nIf you wanted to guess unknown house prices, you could just take the average of some number of geographically nearby houses, and you\u2019d end up with some pretty nice guesses. These might even outperform a parametric regression model built by some economist that estimates model coefficients for # of beds/baths, nearby schools, distance to public transport, etc.\n\nThe fact that k-NN doesn\u2019t require a pre-defined parametric function f(X) relating Y to X makes it well-suited for situations where the relationship is too complex to be expressed with a simple linear model.\n\nHow do you calculate distance from the data point in question when finding the \u201cnearest neighbors\u201d? How do you mathematically determine which of the Blue Squares and Red Triangles in the example above are closest to Green Circle, especially if you can\u2019t just draw a nice 2D graph and eyeball it?\n\nThe most straightforward measure is Euclidean distance (a straight line, \u201cas the crow flies\u201d). Another is Manhattan distance, like walking city blocks. You could imagine that Manhattan distance is more useful in a model involving fare calculation for Uber drivers, for example.\n\nRemember the Pythagorean theorem for finding the length of the hypotenuse of a right triangle?\n\nSolving in terms of c, we find the length of the hypotenuse by taking the square root of the sum of squared lengths of a and b, where a and b are orthogonal sides of the triangle (i.e. they are at a 90-degree angle from one another, going in perpendicular directions in space).\n\nThis idea of finding the length of the hypotenuse given vectors in two orthogonal directions generalizes to many dimensions, and this is how we derive the formula for Euclidean distance d(p,q) between points p and q in n-dimensional space:\n\nWith this formula, you can calculate the nearness of all the training data points to the data point you\u2019re trying to label, and take the mean/mode of the k nearest neighbors to make your prediction.\n\nTypically you won\u2019t need to calculate any distance metrics by hand \u2014 a quick Google search reveals pre-built functions in NumPy or SciPy that will do this for you, e.g. \u2014 but it\u2019s fun to see how geometry concepts from eighth grade end up being helpful for building ML models today!\n\nTo decide which value of k to use, you can test different k-NN models using different values of k with cross-validation:\n\nHigher values of k help address overfitting, but if the value of k is too high your model will be very biased and inflexible. To take an extreme example: if k = N (the total number of data points), the model would just dumbly blanket-classify all the test data as the mean or mode of the training data.\n\nIf the single most common animal in a data set of animals is a Scottish Fold kitten, k-NN with k set to N (the # of training observations) would then predict that every other animal in the world is also a Scottish Fold kitten. Which, in Vishal\u2019s opinion, would be awesome. Samer disagrees.\n\nSome examples of where you can use k-NN:\n\nMaking a good decision tree is like playing a game of \u201c20 questions\u201d.\n\nThe first split at the root of a decision tree should be like the first question you should ask in 20 questions: you want to separate the data as cleanly as possible, thereby maximizing information gain from that split.\n\nIf your friend says \u201cI\u2019m thinking of a noun, ask me up to 20 yes/no questions to guess what it is\u201d and your first question is \u201cis it a potato?\u201d, then you\u2019re a dumbass, because they\u2019re going to say no and you gained almost no information. Unless you happen to know your friend thinks about potatoes all the time, or is thinking about one right now. Then you did a great job.\n\nInstead, a question like \u201cis it an object?\u201d might make more sense.\n\nThis is kind of like how hospitals triage patients or approach differential diagnoses. They ask a few questions up front and check some basic vitals to determine if you\u2019re going to die imminently or something. They don\u2019t start by doing a biopsy to check if you have pancreatic cancer as soon as you walk in the door.\n\nThere are ways to quantify information gain so that you can essentially evaluate every possible split of the training data and maximize information gain for every split. This way you can predict every label or value as efficiently as possible.\n\nNow, let\u2019s look at a particular data set and talk about how we choose splits.\n\nKaggle has a Titanic dataset that is used for a lot of machine learning intros. When the titanic sunk, 1,502 out of 2,224 passengers and crew were killed. Even though there was some luck involved, women, children, and the upper-class were more likely to survive. If you look back at the decision tree above, you\u2019ll see that it somewhat reflects this variability across gender, age, and class.\n\nEntropy is the amount of disorder in a set (measured by Gini index or cross-entropy). If the values are really mixed, there\u2019s lots of entropy; if you can cleanly split values, there\u2019s no entropy. For every split at a parent node, you want the child nodes to be as pure as possible \u2014 minimize entropy. For example, in the Titanic, gender is a big determinant of survival, so it makes sense for this feature to be used in the first split as it\u2019s the one that leads to the most information gain.\n\nLet\u2019s take a look at our Titanic variables:\n\nWe build a tree by picking one of these variables and splitting the dataset according to it.\n\nThe first split separates our dataset into men and women. Then, the women branch gets split again in age (the split that minimizes entropy). Similarly, the men branch gets split by class. By following the tree for a new passenger, you can use the tree to make a guess at whether they died.\n\nThe Titanic example is solving a classification problem (\u201csurvive\u201d or \u201cdie\u201d). If we were using decision trees for regression \u2014 say, to predict housing prices \u2014 we would create splits on the most important features that determine housing prices. How many square feet: more than or less than ___? How many bedrooms & bathrooms: more than or less than ___?\n\nThen, during testing, you would run a specific house through all the splits and take the average of all the housing prices in the final leaf node (bottom-most node) where the house ends up as your prediction for the sale price.\n\nThere are a few hyperparameters you can tune with decision trees models, including and . See the scikit-learn module on decision trees for advice on defining these parameters.\n\nDecision trees are effective because they are easy to read, powerful even with messy data, and computationally cheap to deploy once after training. Decision trees are also good for handling mixed data (numerical or categorical).\n\nThat said, decision trees are computationally expensive to train, carry a big risk of overfitting, and tend to find local optima because they can\u2019t go back after they have made a split. To address these weaknesses, we turn to a method that illustrates the power of combining many decision trees into one model.\n\nA model comprised of many models is called an ensemble model, and this is usually a winning strategy.\n\nA single decision tree can make a lot of wrong calls because it has very black-and-white judgments. A random forest is a meta-estimator that aggregates many decision trees, with some helpful modifications:\n\nThese modifications also prevent the trees from being too highly correlated. Without #1 and #2 above, every tree would be identical, since recursive binary splitting is deterministic.\n\nTo illustrate, see these nine decision tree classifiers below.\n\nThese decision tree classifiers can be aggregated into a random forest ensemble which combines their input. Think of the horizontal and vertical axes of each decision tree output as features x1 and x2. At certain values of each feature, the decision tree outputs a classification of \u201cblue\u201d, \u201cgreen\u201d, \u201cred\u201d, etc.\n\nThese results are aggregated, through modal votes or averaging, into a single ensemble model that ends up outperforming any individual decision tree\u2019s output.\n\nRandom forests are an excellent starting point for the modeling process, since they tend to have strong performance with a high tolerance for less-cleaned data and can be useful for figuring out which features actually matter among many features.\n\nThere are many other clever ensemble models that combine decision trees and yield excellent performance \u2014 check out XGBoost (Extreme Gradient Boosting) as an example.\n\nHopefully, you now have some solid intuitions for how we learn f given a training data set and use this to make predictions with the test data.\n\nNext, we\u2019ll talk about how to approach problems where we don\u2019t have any labeled training data to work with, in Part 3: Unsupervised Learning.\n\nTry this walkthrough for implementing k-NN from scratch in Python. You may also want to take a look at the scikit-learn documentation to get a sense of how pre-built implementations work.\n\nTry the decision trees lab in Chapter 8 of An Introduction to Statistical Learning. You can also play with the Titanic dataset, and check out this tutorial which covers the same concepts as above with accompanying code. Here is the scikit-learn implementation of random forest for out-of-the-box use on data sets."
    },
    {
        "url": "https://medium.com/machine-learning-for-humans/supervised-learning-2-5c1c23f3560d?source=---------6",
        "title": "Machine Learning for Humans, Part 2.2: Supervised Learning II",
        "text": "Is this email spam or not? Is that borrower going to repay their loan? Will those users click on the ad or not? Who is that person in your Facebook picture?\n\nClassification predicts a discrete target label Y. Classification is the problem of assigning new observations to the class to which they most likely belong, based on a classification model built from labeled training data.\n\nThe accuracy of your classifications will depend on the effectiveness of the algorithm you choose, how you apply it, and how much useful training data you have.\n\nLogistic regression is a method of classification: the model outputs the probability of a categorical target variable Y belonging to a certain class.\n\nThough logistic regression is often used for binary classification where there are two classes, keep in mind that classification can performed with any number of categories (e.g. when assigning handwritten digits a label between 0 and 9, or using facial recognition to detect which friends are in a Facebook picture).\n\nCan I just use ordinary least squares?\n\nNope. If you trained a linear regression model on a bunch of examples where Y = 0 or 1, you might end up predicting some probabilities that are less than 0 or greater than 1, which doesn\u2019t make sense. Instead, we\u2019ll use a logistic regression model (or logit model) which was designed for assigning a probability between 0% and 100% that Y belongs to a certain class.\n\nHow does the math work?\n\nNote: the math in this section is interesting but might be on the more technical side. Feel free to skim through it if you\u2019re more interested in the high-level concepts.\n\nThe logit model is a modification of linear regression that makes sure to output a probability between 0 and 1 by applying the sigmoid function, which, when graphed, looks like the characteristic S-shaped curve that you\u2019ll see a bit later.\n\nRecall the original form of our simple linear regression model, which we\u2019ll now call g(x) since we\u2019re going to use it within a compound function:\n\nNow, to solve this issue of getting model outputs less than 0 or greater than 1, we\u2019re going to define a new function F(g(x)) that transforms g(x) by squashing the output of linear regression to a value in the [0,1] range. Can you think of a function that does this?\n\nAre you thinking of the sigmoid function? Bam! Presto! You\u2019re correct.\n\nSo we plug g(x) into the sigmoid function above, resulting in a function of our original function (yes, things are getting meta) that outputs a probability between 0 and 1:\n\nHere we\u2019ve isolated p, the probability that Y=1, on the left side of the equation. If we want to solve for a nice clean \u03b20 + \u03b21x + \u03f5 on the right side so we can straightforwardly interpret the beta coefficients we\u2019re going to learn, we\u2019d instead end up with the log-odds ratio, or logit, on the left side \u2014 hence the name \u201clogit model\u201d:\n\nThe log-odds ratio is simply the natural log of the odds ratio, p/(1-p), which crops up in everyday conversations:\n\nLog-odds might be slightly unintuitive but it\u2019s worth understanding since it will come up again when you\u2019re interpreting the output of neural networks performing classification tasks.\n\nUsing the output of a logistic regression model to make decisions\n\nThe output of the logistic regression model from above looks like an S-curve showing P(Y=1) based on the value of X:\n\nTo predict the Y label \u2014 spam/not spam, cancer/not cancer, fraud/not fraud, etc. \u2014 you have to set a probability cutoff, or threshold, for a positive result. For example: \u201cIf our model thinks the probability of this email being spam is higher than 70%, label it spam. Otherwise, don\u2019t.\u201d\n\nThe threshold depends on your tolerance for false positives vs. false negatives. If you\u2019re diagnosing cancer, you\u2019d have a very low tolerance for false negatives, because even if there\u2019s a very small chance the patient has cancer, you\u2019d want to run further tests to make sure. So you\u2019d set a very low threshold for a positive result.\n\nIn the case of fraudulent loan applications, on the other hand, the tolerance for false positives might be higher, particularly for smaller loans, since further vetting is costly and a small loan may not be worth the additional operational costs and friction for non-fraudulent applicants who are flagged for further processing.\n\nAs in the case of linear regression, we use gradient descent to learn the beta parameters that minimize loss.\n\nIn logistic regression, the cost function is basically a measure of how often you predicted 1 when the true answer was 0, or vice versa. Below is a regularized cost function just like the one we went over for linear regression.\n\nDon\u2019t panic when you see a long equation like this! Break it into chunks and think about what\u2019s going on in each part conceptually. Then the specifics will start to make sense.\n\nThe first chunk is the data loss, i.e. how much discrepancy there is between the model\u2019s predictions and reality. The second chunk is the regularization loss, i.e. how much we penalize the model for having large parameters that heavily weight certain features (remember, this prevents overfitting).\n\nWe\u2019ll minimize this cost function with gradient descent, as above, and voil\u00e0! we\u2019ve built a logistic regression model to make class predictions as accurately as possible.\n\n\u201cWe\u2019re in a room full of marbles again. Why are we always in a room full of marbles? I could\u2019ve sworn we already lost them.\u201d\n\nSVM is the last parametric model we\u2019ll cover. It typically solves the same problem as logistic regression \u2014 classification with two classes \u2014 and yields similar performance. It\u2019s worth understanding because the algorithm is geometrically motivated in nature, rather than being driven by probabilistic thinking.\n\nA few examples of the problems SVMs can solve:\n\nWe\u2019ll use the third example to illustrate how SVMs work. Problems like these are called toy problems because they\u2019re not real \u2014 but nothing is real, so it\u2019s fine.\n\nIn this example, we have points in a 2D space that are either red or blue, and we\u2019d like to cleanly separate the two.\n\nThe training set is plotted the graph above. We would like to classify new, unclassified points in this plane. To do this, SVMs use a separating line (or, in more than two dimensions, a multi-dimensional hyperplane) to split the space into a red zone and a blue zone. You can already imagine how a separating line might look in the graph above.\n\nHow, specifically, do we choose where to draw the line?\n\nBelow are two examples of such a line:\n\nHopefully, you share the intuition that the first line is superior. The distance to the nearest point on either side of the line is called the margin, and SVM tries to maximize the margin. You can think about it like a safety space: the bigger that space, the less likely that noisy points get misclassified.\n\nBased on this short explanation, a few big questions come up.\n\n1. How does the math behind this work?\n\nWe want to find the optimal hyperplane (a line, in our 2D example). This hyperplane needs to (1) separate the data cleanly, with blue points on one side of the line and red points on the other side, and (2) maximize the margin. This is an optimization problem. The solution has to respect constraint (1) while maximizing the margin as is required in (2).\n\nThe human version of solving this problem would be to take a ruler and keep trying different lines separating all the points until you get the one that maximizes the margin.\n\nIt turns out there\u2019s a clean mathematical way to do this maximization, but the specifics are beyond our scope. To explore it further, here\u2019s a video lecture that shows how it works using Lagrangian Optimization.\n\nThe solution hyperplane you end up with is defined in relation to its position with respect to certain x_i\u2019s, which are called the support vectors, and they\u2019re usually the ones closest to the hyperplane.\n\n2. What happens if you can\u2019t separate the data cleanly?\n\nThere are two methods for dealing with this problem.\n\nWe allow a few mistakes, meaning we allow some blue points in the red zone or some red points in the blue zone. We do that by adding a cost C for misclassified examples in our loss function. Basically, we say it\u2019s acceptable but costly to misclassify a point.\n\nWe can create nonlinear classifiers by increasing the number of dimensions, i.e. include x\u00b2, x\u00b3, even cos(x), etc. Suddenly, you have boundaries that can look more squiggly when we bring them back to the lower dimensional representation.\n\nIntuitively, this is like having red and blue marbles lying on the ground such that they can\u2019t be cleanly separated by a line \u2014 but if you could make all the red marbles levitate off the ground in just the right way, you could draw a plane separating them. Then you let them fall back to the ground knowing where the blues stop and reds begin.\n\nIn summary, SVMs are used for classification with two classes. They attempt to find a plane that separates the two classes cleanly. When this isn\u2019t possible, we either soften the definition of \u201cseparate,\u201d or we throw the data into higher dimensions so that we can cleanly separate the data.\n\nIn this section we covered:\n\nIn Part 2.3: Supervised Learning III, we\u2019ll go into non-parametric supervised learning, where the ideas behind the algorithms are very intuitive and performance is excellent for certain kinds of problems, but the models can be harder to interpret.\n\nData School has an excellent in-depth guide to logistic regression. We\u2019ll also continue to refer you to An Introduction to Statistical Learning. See Chapter 4 on logistic regression, and Chapter 9 on support vector machines.\n\nTo implement logistic regression, we recommend working on this problem set. You have to register on the site to work through it, unfortunately. C\u2019est la vie.\n\nTo dig into the math behind SVMs, watch Prof. Patrick Winston\u2019s lecture from MIT 6.034: Artificial Intelligence. And check out this tutorial to work through a Python implementation."
    },
    {
        "url": "https://medium.com/machine-learning-for-humans/supervised-learning-740383a2feab?source=---------7",
        "title": "Machine Learning for Humans, Part 2.1: Supervised Learning",
        "text": "How much money will we make by spending more dollars on digital advertising? Will this loan applicant pay back the loan or not? What\u2019s going to happen to the stock market tomorrow?\n\nIn supervised learning problems, we start with a data set containing training examples with associated correct labels. For example, when learning to classify handwritten digits, a supervised learning algorithm takes thousands of pictures of handwritten digits along with labels containing the correct number each image represents. The algorithm will then learn the relationship between the images and their associated numbers, and apply that learned relationship to classify completely new images (without labels) that the machine hasn\u2019t seen before. This is how you\u2019re able to deposit a check by taking a picture with your phone!\n\nTo illustrate how supervised learning works, let\u2019s examine the problem of predicting annual income based on the number of years of higher education someone has completed. Expressed more formally, we\u2019d like to build a model that approximates the relationship f between the number of years of higher education X and corresponding annual income Y.\n\nOne method for predicting income would be to create a rigid rules-based model for how income and education are related. For example: \u201cI\u2019d estimate that for every additional year of higher education, annual income increases by $5,000.\u201d\n\nYou could come up with a more complex model by including some rules about degree type, years of work experience, school tiers, etc. For example: \u201cIf they completed a Bachelor\u2019s degree or higher, give the income estimate a 1.5x multiplier.\u201d\n\nBut this kind of explicit rules-based programming doesn\u2019t work well with complex data. Imagine trying to design an image classification algorithm made of if-then statements describing the combinations of pixel brightnesses that should be labeled \u201ccat\u201d or \u201cnot cat\u201d.\n\nSupervised machine learning solves this problem by getting the computer to do the work for you. By identifying patterns in the data, the machine is able to form heuristics. The primary difference between this and human learning is that machine learning runs on computer hardware and is best understood through the lens of computer science and statistics, whereas human pattern-matching happens in a biological brain (while accomplishing the same goals).\n\nIn supervised learning, the machine attempts to learn the relationship between income and education from scratch, by running labeled training data through a learning algorithm. This learned function can be used to estimate the income of people whose income Y is unknown, as long as we have years of education X as inputs. In other words, we can apply our model to the unlabeled test data to estimate Y.\n\nThe goal of supervised learning is to predict Y as accurately as possible when given new examples where X is known and Y is unknown. In what follows we\u2019ll explore several of the most common approaches to doing so.\n\nThe rest of this section will focus on regression. In Part 2.2 we\u2019ll dive deeper into classification methods.\n\nRegression predicts a continuous target variable Y. It allows you to estimate a value, such as housing prices or human lifespan, based on input data X.\n\nHere, target variable means the unknown variable we care about predicting, and continuous means there aren\u2019t gaps (discontinuities) in the value that Y can take on. A person\u2019s weight and height are continuous values. Discrete variables, on the other hand, can only take on a finite number of values \u2014 for example, the number of kids somebody has is a discrete variable.\n\nPredicting income is a classic regression problem. Your input data X includes all relevant information about individuals in the data set that can be used to predict income, such as years of education, years of work experience, job title, or zip code. These attributes are called features, which can be numerical (e.g. years of work experience) or categorical (e.g. job title or field of study).\n\nYou\u2019ll want as many training observations as possible relating these features to the target output Y, so that your model can learn the relationship f between X and Y.\n\nThe data is split into a training data set and a test data set. The training set has labels, so your model can learn from these labeled examples. The test set does not have labels, i.e. you don\u2019t yet know the value you\u2019re trying to predict. It\u2019s important that your model can generalize to situations it hasn\u2019t encountered before so that it can perform well on the test data.\n\nIn our trivially simple 2D example, this could take the form of a .csv file where each row contains a person\u2019s education level and income. Add more columns with more features and you\u2019ll have a more complex, but possibly more accurate, model.\n\nHow do we build models that make accurate, useful predictions in the real world? We do so by using supervised learning algorithms.\n\nNow let\u2019s get to the fun part: getting to know the algorithms. We\u2019ll explore some of the ways to approach regression and classification and illustrate key machine learning concepts throughout.\n\n\u201cDraw the line. Yes, this counts as machine learning.\u201d\n\nFirst, we\u2019ll focus on solving the income prediction problem with linear regression, since linear models don\u2019t work well with image recognition tasks (this is the domain of deep learning, which we\u2019ll explore later).\n\nWe have our data set X, and corresponding target values Y. The goal of ordinary least squares (OLS) regression is to learn a linear model that we can use to predict a new y given a previously unseen x with as little error as possible. We want to guess how much income someone earns based on how many years of education they received.\n\nLinear regression is a parametric method, which means it makes an assumption about the form of the function relating X and Y (we\u2019ll cover examples of non-parametric methods later). Our model will be a function that predicts \u0177 given a specific x:\n\n\u03b20 is the y-intercept and \u03b21 is the slope of our line, i.e. how much income increases (or decreases) with one additional year of education.\n\nOur goal is to learn the model parameters (in this case, \u03b20 and \u03b21) that minimize error in the model\u2019s predictions.\n\nTo find the best parameters:\n\nGraphically, in two dimensions, this results in a line of best fit. In three dimensions, we would draw a plane, and so on with higher-dimensional hyperplanes.\n\nMathematically, we look at the difference between each real data point (y) and our model\u2019s prediction (\u0177). Square these differences to avoid negative numbers and penalize larger differences, and then add them up and take the average. This is a measure of how well our data fits the line.\n\nFor a simple problem like this, we can compute a closed form solution using calculus to find the optimal beta parameters that minimize our loss function. But as a cost function grows in complexity, finding a closed form solution with calculus is no longer feasible. This is the motivation for an iterative approach called gradient descent, which allows us to minimize a complex loss function.\n\n\u201cPut on a blindfold, take a step downhill. You\u2019ve found the bottom when you have nowhere to go but up.\u201d\n\nGradient descent will come up over and over again, especially in neural networks. Machine learning libraries like scikit-learn and TensorFlow use it in the background everywhere, so it\u2019s worth understanding the details.\n\nThe goal of gradient descent is to find the minimum of our model\u2019s loss function by iteratively getting a better and better approximation of it.\n\nImagine yourself walking through a valley with a blindfold on. Your goal is to find the bottom of the valley. How would you do it?\n\nA reasonable approach would be to touch the ground around you and move in whichever direction the ground is sloping down most steeply. Take a step and repeat the same process continually until the ground is flat. Then you know you\u2019ve reached the bottom of a valley; if you move in any direction from where you are, you\u2019ll end up at the same elevation or further uphill.\n\nGoing back to mathematics, the ground becomes our loss function, and the elevation at the bottom of the valley is the minimum of that function.\n\nLet\u2019s take a look at the loss function we saw in regression:\n\nWe see that this is really a function of two variables: \u03b20 and \u03b21. All the rest of the variables are determined, since X, Y, and n are given during training. We want to try to minimize this function.\n\nThe function is f(\u03b20,\u03b21)=z. To begin gradient descent, you make some guess of the parameters \u03b20 and \u03b21 that minimize the function.\n\nNext, you find the partial derivatives of the loss function with respect to each beta parameter: [dz/d\u03b20, dz/d\u03b21]. A partial derivative indicates how much total loss is increased or decreased if you increase \u03b20 or \u03b21 by a very small amount.\n\nPut another way, how much would increasing your estimate of annual income assuming zero higher education (\u03b20) increase the loss (i.e. inaccuracy) of your model? You want to go in the opposite direction so that you end up walking downhill and minimizing loss.\n\nSimilarly, if you increase your estimate of how much each incremental year of education affects income (\u03b21), how much does this increase loss (z)? If the partial derivative dz/\u03b21 is a negative number, then increasing \u03b21 is good because it will reduce total loss. If it\u2019s a positive number, you want to decrease \u03b21. If it\u2019s zero, don\u2019t change \u03b21 because it means you\u2019ve reached an optimum.\n\nKeep doing that until you reach the bottom, i.e. the algorithm converged and loss has been minimized. There are lots of tricks and exceptional cases beyond the scope of this series, but generally, this is how you find the optimal parameters for your parametric model.\n\nOverfitting: \u201cSherlock, your explanation of what just happened is too specific to the situation.\u201d Regularization: \u201cDon\u2019t overcomplicate things, Sherlock. I\u2019ll punch you for every extra word.\u201d Hyperparameter (\u03bb): \u201cHere\u2019s the strength with which I will punch you for every extra word.\u201d\n\nA common problem in machine learning is overfitting: learning a function that perfectly explains the training data that the model learned from, but doesn\u2019t generalize well to unseen test data. Overfitting happens when a model overlearns from the training data to the point that it starts picking up idiosyncrasies that aren\u2019t representative of patterns in the real world. This becomes especially problematic as you make your model increasingly complex. Underfitting is a related issue where your model is not complex enough to capture the underlying trend in the data.\n\nRemember that the only thing we care about is how the model performs on test data. You want to predict which emails will be marked as spam before they\u2019re marked, not just build a model that is 100% accurate at reclassifying the emails it used to build itself in the first place. Hindsight is 20/20 \u2014 the real question is whether the lessons learned will help in the future.\n\nThe model on the right has zero loss for the training data because it perfectly fits every data point. But the lesson doesn\u2019t generalize. It would do a horrible job at explaining a new data point that isn\u2019t yet on the line.\n\n1. Use more training data. The more you have, the harder it is to overfit the data by learning too much from any single training example.\n\n2. Use regularization. Add in a penalty in the loss function for building a model that assigns too much explanatory power to any one feature or allows too many features to be taken into account.\n\nThe first piece of the sum above is our normal cost function. The second piece is a regularization term that adds a penalty for large beta coefficients that give too much explanatory power to any specific feature. With these two elements in place, the cost function now balances between two priorities: explaining the training data and preventing that explanation from becoming overly specific.\n\nThe lambda coefficient of the regularization term in the cost function is a hyperparameter: a general setting of your model that can be increased or decreased (i.e. tuned) in order to improve performance. A higher lambda value will more harshly penalize large beta coefficients that could lead to potential overfitting. To decide the best value of lambda, you\u2019d use a method called cross-validation which involves holding out a portion of the training data during training, and then seeing how well your model explains the held-out portion. We\u2019ll go over this in more depth\n\nHere\u2019s what we covered in this section:\n\nIn the next section \u2014 Part 2.2: Supervised Learning II \u2014 we\u2019ll talk about two foundational methods of classification: logistic regression and support vector machines.\n\nFor a more thorough treatment of linear regression, read chapters 1\u20133 of An Introduction to Statistical Learning. The book is available for free online and is an excellent resource for understanding machine learning concepts with accompanying exercises.\n\nTo actually implement gradient descent in Python, check out this tutorial. And here is a more mathematically rigorous description of the same concepts.\n\nIn practice, you\u2019ll rarely need to implement gradient descent from scratch, but understanding how it works behind the scenes will allow you to use it more effectively and understand why things break when they do."
    }
]