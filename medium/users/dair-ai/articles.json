[
    {
        "url": "https://medium.com/dair-ai/detecting-emotions-with-cnn-fusion-models-b066944969c8?source=---------0",
        "title": "Detecting Emotions with CNN Fusion Models \u2013 DAIR \u2013",
        "text": "This work proposes models that combine information from different modalities (e.g., images and text) to be able to classify social media content. Information from different modalities are combined using neural network models through a pooling layer. In addition, an auxiliary learning task is used to learn a common feature space for all modalities (more on this later).\n\nMultimodal approaches become more important as social media networks allow for users to post multimodal posts (e.g., gifs, videos, audio segments, text, etc.). Analysis of multimodal information allows for better understanding of users (user profiling) and can be used to effectively run ad campaigns on the social network. In addition, it can be used to better understand other emotion-related behaviors such as mental health disorders, etc.\n\nConsider the examples of multimodal posts in the pictures below. If we only paid attention to the images (left to right), we would predict emotions such as joy, fear, and contentment. If we considered both the image and text: the first example remains as joy; the second example is probably mixed emotion (the text convey joy); and the third example is also mixed emotion (the text convey sadness). These simple examples emphasize on the importance of considering both modalities to deduce the overall emotion conveyed in the social post.\n\nThe main problem with previous multimodal approaches is the inability to deal with the absence of some important modality. For instance, let\u2019s assume we can obtain text and video from a piece of content, but we can\u2019t obtain the audio because it is corrupted or unavailable. In such cases, previous methods did not address this important problem (i.e., missing modality). The proposed model aims to address this problem and proves its robustness through an emotion classification task. Besides dealing with the \u201cmissing modality\u201d problem, the authors claim that their approach can also scale to other tasks and modalities.\n\nAs previously mentioned, the proposed model can handle situations where there is a missing modality. In other words, there system supports the following cases: only image or text or both.\n\nI believe this is an important discussion that this paper highlights in the related work. As it relates to emotion recognition, it is challenging to manually create features as we cannot guarantee that all aspects of emotions (features) that can capture the emotions are covered. Convolutional neural networks (CNNs) are used in place so as to automatically learn representations that can generalize to the problem of emotion recognition. I couldn\u2019t help but commenting that even though this argument is strong, hand-crafting features also offer better intuition of what is being learned, something deep learning models may not offer, yet! However, some good people are tirelessly working on this problem (Feature Visualization).\n\nThis work employs an adaptation of early fusion for combining modalities for emotion recognition through CNNs. Two prominent modalities of social media are used, i.e. text and image. If both image and text are available for a social post, they are assumed to have semantic relation \u2014 the text describes the image. Images are represented by vectors, which are obtained after feeding images into a CNN trained on ImageNet. Texts are represented through pre-trained word embeddings (GloVe).\n\nIn the figure above, all types of fusion techniques for combining features to be fed to a classifier are shown. This work proposes two fusion approaches which enjoy the simplicity of early fusion (a) and the flexibility of late fusion (b). These approaches are called joint fusion \u00a9 and common space fusion (d).\n\nIn the joint fusion model, text and images are fused in the fusion layer, which applies a pooling operation to the text and image vector to obtain a combined feature vector. The pooling operations require both vectors to be of the same size. Typically, the image vector has a higher dimension than the text vector, therefore, an extra linear layer is added to map the original image vector to a vector of the same dimension as the text vector. The joint fusion neural network is trained by minimizing the negative log-likelihood using stochastic gradient descent (SGD). (See paper for additional details)\n\nThe second approach, common feature space fusion, aims to enforce visual and textual vectors of a post to be in the same feature space. Note that this was motivated by the fact that the joint fusion model considers these signals (visual and textual) as different, i.e., no relationship between them. An auxiliary task is employed, which enforces similarity between both a text and image vector belonging to a post, ensuring that the rest of text vectors from different classes are different from the image vector. (See paper for details on how this objective is trained and combined with the classification main task).\n\nA emotion classification task is conducted to evaluate the proposed multimodal approaches. Different discrete emotion categories from Plutchik\u2019s wheel of emotions are employed to label two types of datasets.\n\nA flickr image dataset was crawled and assigned to Amazon Mechanical Turk workers for annotations; i.e., human workers were asked to annotate the emotion they perceived from the images. Title and descriptions are also obtained for each image from the flickr website. In addition, a Reddit dataset was also collected; subreddits related to emotion (happy, creepy, rage, gore) were used to collect data for 4 emotions, joy, fear, anger, and disgust, respectively. (See paper for more details on collecting and preparing datasets)\n\nUnimodal baselines (FastText model for text and InceptionNet model for images), traditional multimodal approaches (early and late fusion), and the proposed multimodal models (joint fusion and common space fusion) are trained on the datasets. For the embedding layer, GloVe pre-trained word vectors are considered.\n\nFrom the results in the table above, we can observe that the proposed fusion models (joint fusion and common space fusion) outperform all the other models in both datasets, including the traditional fusion techniques. The common space fusion model, in particular, performs extremely well even when only one modality for a post existed (see results below).\n\nClassification results on several examples are provided for error analysis (see figure below). We can observe that for the highlighted example, the common space fusion model can detect fear when using both the text and image information. Somehow, the model can detect that in this particular example, the image descriptor expresses sarcasm, which is obvious from the creepy doll in the closet show in the image. Kind on interesting and weird at the same time, no pun intended. (See paper for more interesting analysis and examples)"
    },
    {
        "url": "https://medium.com/dair-ai/detecting-sarcasm-with-deep-convolutional-neural-networks-4a0657f79e80?source=---------1",
        "title": "Detecting Sarcasm with Deep Convolutional Neural Networks",
        "text": "Overview\n\nThis paper addresses a key NLP task known as sarcasm detection using a combination of model based on convolutional neural networks (CNNs). Detection of sarcasm is important in other areas such as affective computing and sentiment analysis because such expressions can flip the polarity of a sentence.\n\nExample\n\nSarcasm can be considered as expressing a bitter gibe or taunt. Examples include statements such as \u201cIs it time for your medication or mine?\u201d and \u201cI work 40 hours a week to be this poor\u201d. (Find more fun examples here)\n\nChallenges\n\nTo understand and detect sarcasm it is important to understand the facts related to an event. This allows for detection of contradiction between the objective polarity (usually negative) and the sarcastic characteristics conveyed by the author (usually positive).\n\nConsider the example, \u201cI love the pain of breakup\u201d, it is difficult to extract the knowledge needed to detect if there is sarcasm in this statement. In the example \u201cI love the pain\u201d provides knowledge of the sentiment expressed by the author (in this case positive) and \u201cbreakup\u201d describes a contradicting sentiment ( that of negative).\n\nOther challenges that exist in sarcastic statements is the reference to multiple events and the need to extract large amounts of facts, commonsense knowledge, anaphora resolution, and logical reasoning. The authors avoid automatic feature extraction and rely on CNNs to automatically learn features from a sarcasm corpus.\n\nModel\n\nSentiment shifting is prevalent is sarcasm-related communication; thus, the authors propose to first train a sentiment model (based on a CNN) for learning sentiment-specific feature extraction. The model learns local features in lower layers which are then converted into global features in the higher layers. The authors observe that sarcasm expression is user-specific \u2014 some users post more sarcasm than others.\n\nIn the proposed framework, personality-based features, sentiment features, and emotion-based features are incorporated into the sarcasm detection framework. Each set of features are learned by separate models, becoming pre-trained models used to extract sarcasm-related features from a dataset.\n\nCNN Framework\n\nCNNs are effective at modeling hierarchy of local features to learn more global features, which is essential to learn context. Sentences are represented using word vectors (embeddings) and provided as input. (Google\u2019s word2vec vectors are employed as input). Non-static representations are used, therefore, parameters for these word vectors are learned during the training phase. Max pooling is then applied to the feature maps to generate features. A fully connected layer is applied followed by a softmax layer for outputting the final prediction. (See diagram of the CNN-based architecture below)\n\nTo obtain the other features \u2014 sentiment (S), emotion (E), and personality (P) \u2014 CNN models are pre-trained and used to extract features from the sarcastic datasets. Different training datasets were used to train each model. (Refer to paper for more details)\n\nTwo classifiers are tested \u2014 a pure CNN classifier (CNN) and CNN-extracted features fed to an SVM classifier (CNN-SVM).\n\nA separate baseline line classifier (B) \u2014 consisting of only the CNN model without the incorporation of the other models (e.g., emotion and sentiment) \u2014 is trained as well.\n\nExperiments\n\nData \u2014 Balanced and imbalanced sarcastic tweets datasets were obtained from (Ptacek et al., 2014) and The Sarcasm Detector. Usernames, URLs, and hashtags are removed, and the NLTK Twitter Tokenizer was used for tokenization. (See paper for more details)\n\nThe performances of both CNN and CNN-SVM classifer when applied to all datasets are shown in the table below. We can observe that when the models (specifically CNN-SVM) combines sarcasm features, emotion features, sentiment features, and personality traits features, it outperforms all the other models with the exception of the baseline model (B).\n\nThe table below shows comparison results of the the state-of-the-art model (method 1), other well-known sarcasm detection research (method 2), and the proposed model (method 3). The proposed model consistently outperforms all the other models.\n\nGeneralizability capabilities of the models were tested and the main finding was that if the datasets differed in nature, this significantly impacted the results. (See visualization of the datasets rendered via PCA below). For instance, training was done on Dataset 1 and tested on Dataset 2; the F1-score of the model was 33.05%, significantly dropping in accuracy.\n\nReferences\n\nRef: https://arxiv.org/abs/1610.08815 \u2014 \u201cA Deeper Look into Sarcastic Tweets Using Deep Convolutional Neural Networks\u201d\n\nYou can find an interesting discussion of this post on Reddit here."
    },
    {
        "url": "https://medium.com/dair-ai/pytorch-0-4-0-aa48ec243d7?source=---------2",
        "title": "PyTorch 0.4.0, Google Brain Tokyo, QuickNLP, Multilingual NLU, PeerRead dataset, PyTo",
        "text": "Welcome to the 12th Issue of the NLP Newsletter! Here is this week\u2019s notable NLP news!\n\nOn People\u2026\n\nA statement has been circulating for ML people to sign against \u201cclosed access or author-fee publication\u201d (We definitely signed! Let\u2019s keep ML research open) \u2014 Link\n\nAre emojis ruining the way young people use the English language to communicate? \u2014 Link\n\nHands down one of the best AI essays of the year by Professor Michael Jordan (Artificial Intelligence \u2014 The Revolution Hasn\u2019t Happened Yet) \u2014 Link\n\nWhy technical experts need to get better at telling stories \u2014 Link\n\nOn Education and Research\u2026\n\nPaper revisiting on how to make better choices with batch training in neural networks \u2014 Link\n\nPaper discussing when and why word embeddings are good for neural machine translation \u2014 Link\n\nOlive Oil is Made of Olives, Baby Oil is Made for Babies (Paper Summary) \u2014 Link\n\nPaper discusses how to go about conducting evaluation on \u201cDeep Semi-Supervised Learning Algorithms\u201d \u2014 Link\n\nUsing deep learning to detect linguistic cues of Alzheimer\u2019s disease patients \u2014 Link\n\nSummary of interesting NLP Papers and Research (Fast and easy reads!) \u2014 Link\n\nHow Natural Language Inference Models \u201cGame\u201d the Task of Learning \u2014 Link\n\nOn Code and Data\u2026\n\nPyTorch 0.4.0 is released (Trade-off memory for compute, Windows support, 24 distributions with cdf, variance etc., dtypes, zero-dimensional Tensors, Tensor-Variable merge, faster distributed, perf and bug fixes, CuDNN 7.1) \u2014 Link\n\nQuick NLP is a deep learning nlp library inspired by the fast.ai library \u2014 Link\n\n\u201cPeerRead is a dataset of scientific peer reviews available to help researchers study this important artifact\u201d \u2014 Link\n\nGANs in 50 lines of code with PyTorch \u2014 Link\n\nA guide to conducting sequence prediction (one of the hottest trends in deep learning) with Python \u2014 Link\n\nOn Industry\u2026\n\nGoogle expands Google Brain team in Tokyo (now accepting applications) \u2014 Link\n\nVery nice video tutorial by Uber AI Labs on \u201cMeasuring the Intrinsic Dimension of Objective Landscapes\u201d \u2014 Link\n\nHow NLP offers a bright future for airlines and passengers \u2014 Link\n\nScientists plan huge European AI hub to compete with US \u2014 Link\n\nQuote of the Week\u2026\n\n\u201cWe see no role for closed access or author-fee publication in the future of machine learning research and believe the adoption of this new journal as an outlet of record for the machine learning community would be a retrograde step.\u201d (@tdietterich)\n\n\u201cSorry officer, my car has old version of TensorFlow, I\u2019ll update it tonight\u201d \u2014 Link\n\nSpeakeasy \u2014 Fun article explaining how aliens can help our digital assistants with linguistic assistance (sounds mostly fictional but it\u2019s actually very insightful) \u2014 Link\n\nA nice summarized version of the MIT Probability course \u2014 Link\n\nPaper on the Theory of Mind \u2014 Link\n\nWhy deep learning is perfect for NLP \u2014 Link\n\nMessage from the Editor\n\nHello everyone! Thanks for the massive support you have given to this newsletter. I realized the tremendous good it can do for experts in the field and those who are beginning. The previous newsletter did so well that I got excited to see how much people were interested in such newsletter (3K+ views) \ud83d\udc4f. I hope to keep bringing more of the best of NLP and ML news in the weeks that follow. Also, come over to @omarsar0 and say hi!"
    },
    {
        "url": "https://medium.com/dair-ai/using-deep-learning-to-detect-linguistic-cues-of-alzheimers-patients-a606693e54f9?source=---------3",
        "title": "Using Deep Learning to Detect Linguistic Cues of Alzheimer\u2019s Disease Patients",
        "text": "This paper aims to detect linguistic characteristics and grammatical patterns from speech transcriptions generated by Alzheimer\u2019s disease (AD) patients.\n\nThe authors propose several neural models such as CNNs and LSTM-RNNs \u2014 and combinations of them \u2014 to enhance an AD classification task. The trained neural models are used to interpret linguistic characteristics of AD patients (including gender variation) via activation clustering and first-derivative saliency techniques.\n\nLanguage variation can serve as a proxy that monitors how patients\u2019 cognitive functions have been affected (e.g., issues with word finding and impaired reasoning). This can equip machines with diagnostic capabilities, which are particularly effective for dealing with AD since it is neither curable or reversible.\n\nThe challenge with detecting AD-positive patients is it requires diverse linguistic and world knowledge. Consider the following example:\n\n\u201cWell\u2026there\u2019s a mother standing there uh uh washing the dishes and the sink is overspilling\u2026overflowing.\u201d There are several linguistic cues, such as \u201coverspilling\u2026overflowing\u201d, indicating signs of confusion and memory loss, which is very common in AD-positive patients. Therefore, instead of relying on hand-crafted features, the authors propose a neural model for automatically learning these linguistic cues from the data. Other important issues observed from the previous literature are as follows:\n\nThis work uses the Dementia Bank dataset, which consists of transcripts and audio recordings of AD (and control) patients. These records were collected via interviews on several tasks such as \u201cRecall Test\u201d and \u201cBoston Cookie Theft\u201d. Transcripts were segmented into individual utterances with accompanying part-of-speech (POS) tags.\n\nThree types of neural approaches are proposed: CNN (embedding + convolutional layer + max-pooling layer), LSTM-RNN (embedding + LSTM layer), and CNN-LSTM (basically laying an LSTM on top of CNN \u2014 architecture shown in the figure below). (See paper for more details.)\n\nThe best performing model (POS tags + CNN-LSTM) achieves 91.1% accuracy, which sets a new benchmark for the AD classification task. See other results below.\n\nThe authors observed that almost all AD-positive results were classified correctly and that there were more errors in classifying non-AD samples. This could be because the dataset contained patients with various degree of symptoms related to AD. (See paper for more results.)\n\nNo significant differences in linguistic patterns were observed between male and female AD patients.\n\nFurthermore, interpretation of the linguistic cues captured by the neural models are conducted using two visualization techniques:\n\nThrough the activation clustering, three common linguistic patterns found in AD patients emerged from the clusters: short answers and bursts of speech (e.g., \u201cand\u201d and \u201coh!\u201d), repeated requests for clarification (e.g., \u201cdid I say fact?\u201d), and starting with interjections (\u201cso\u201d and \u201cwell\u201d). Moreover, for several tasks such as Cookie and Recall, the most commonly used POS tags for AD clusters were show to be distinct.\n\nThrough the salience heat maps, the difference in word importance can be seen between control and AD patients. As shown in the figure below (left), the word \u201cuh\u201d and \u201cum\u201d are important and distinguishable speech traits for classifying AD patients. The figure (right) shows that the control group does not heavily use these type of filler words.\n\nDetecting Linguistic Characteristics of Alzheimer\u2019s Dementia by Interpreting Neural Models \u2014 (Sweta Karlekar, Tong Niu, and Mohit Bansal)"
    },
    {
        "url": "https://medium.com/dair-ai/olive-oil-is-made-of-olives-baby-oil-is-made-for-babies-paper-summary-a6f9b5544761?source=---------4",
        "title": "Olive Oil is Made of Olives, Baby Oil is Made for Babies [Paper Summary]",
        "text": "This article summarizes a novel technique for a very complex task in NLP known as noun compound classification.\n\nOlive Oil is Made of Olives, Baby Oil is Made for Babies: Interpreting Noun Compounds using Paraphrases in a Neural Model \u2014 Vered Shwartz and Chris Waterson\n\nThis paper addresses an important NLP task \u2014 the automatic interpretation of the relation between the constituents of a noun compound.\n\nConsider the following noun compound examples: olive oil and baby oil. You can observe that the word \u201colive\u201d in the phrase \u201colive oil\u201d describes a SOURCE relation, and the word \u201cbaby\u201d in \u201cbaby oil\u201d describes a PURPOSE relation. In other words, babies should never be put in the same context as olives in terms of what they represent in the real world. This distinction is important because it can be used for various applications that require complex text understanding capabilities.\n\nImagine you asked Google search what olive oil is made up of. If Google search is smart it should respond \u201colive\u201d. Now imagine you asked Google what baby oil is made up of. Definitely not babies! The answer should be other ingredients of oil or the main ingredient of oil. This is a very important distinction! It\u2019s a challenging task because the meaning of both types of oils is not easy to interpret given the meaning of its constituent words.\n\nCan you think of more examples? Try and you will see why this area of NLP research is important. As an NLP researcher, I can even see how this is useful for disambiguating between sentiment phrases. (More on this in another article.)\n\nTwo very common approaches are used to address this issue: paraphrases and noun-compound representations. The first approach maps relationships between constituents and the latter approach makes use of distributional representations of the individual constituents. (Don\u2019t panic, I will explain what they mean in a little bit.)\n\nA more recent work (Dima, 2016) showed that constituent embeddings are effective to represent the noun-compounds (hereinafter also referred to as NCs. The main reason for why it works is attributed to a phenomenon referred to as lexical memorization.\n\nThis paper proposes a neural paraphrasing approach that combines path embeddings (which represents the relationships between NCs) and distributional information (obtained directly from word embeddings) to conduct the NC classification task. The authors also experiment with settings where lexical memorization is avoided to show that their method is more robust and their results are not attributed to this phenomenon.\n\nThe authors used HypeNET to learn patterns connecting the joint occurrences of instances of constituents in a corpus. These are also referred to as path embeddings.\n\nThree models were combined to perform the NCs relation classification: path-based, integrated, and integrated-NC. Each model incrementally adds new features (in this case different distributional inputs) which essentially adds more contextualized information to the overall input vector. This process can be seen more clearly in the diagram below:\n\nThe path embeddings (colored purple in the diagram) are learned using a vanilla LSTM with input vectors representing a concatenation of the following vectors: lemma, part-of-speech tag, dependency label, and direction vectors. (See paper for more details). NC labels (relations) are obtained using a distant supervision approach via the output of the LSTM.\n\nTwo datasets, obtained from Tratz (2011), were used to evaluate the proposed neural paraphrasing model. Several comparison models were proposed, including several baseline models and re-trained models adopted from previous work and the state-of-the-art method. (See paper for more details on the experimental setup).\n\nTable 1 shows that for most cases the integrated models (Int and Int-NC) outperform all other models on the using different data splitting strategies (shown in Split column). There is very little difference between the results obtained from the Int model and Int-NC model, indicating that the NC embeddings did not contribute much to the classification task.\n\nFurther analysis was conducted on the random splitting strategy to analyze variations in results of the different models. In Table 3, you can observe some of the relations that yielded reasonable performance (e.g., MEASURE and PERSONAL TITLE)\n\nThe authors also discovered that complex relations perform poorly such as LEXICALIZED for the NC, \u201csoap opera\u201d and OBJECTIVE for NC, \u201crecovery plan\u201d. (See more interesting examples in the paper).\n\nTable 4 below provides examples of NC embeddings obtained from the test set (left) and an example of the most similar NC in the embeddings (right). The authors observed that only 27.61% of the NCs were mostly similar to NCs with the same label. They attributed this behavior to inconsistent annotations rather than quality of embeddings.\n\nMy Further Ideas and Conclusion"
    },
    {
        "url": "https://medium.com/dair-ai/deepsuperlearner-spherical-cnns-google-semantris-debater-data-alterego-text-to-images-gans-2ba92eef9b9f?source=---------5",
        "title": "DeepSuperLearner, Spherical CNNs, Google Semantris, Debater Data, AlterEgo, Text-to-Images GANs\u2026",
        "text": "Welcome to the 11th Issue of the NLP Newsletter! Here is this week\u2019s notable NLP news!\n\nOn People\u2026\n\nA great roundup of the talk delivered by Rachel Thomas discussing the accessibility in AI at Stanford \u2014 Link\n\nOn Education and Research\u2026\n\nAn interesting paper on generating images from scene graphs using graph convolution and GANs \u2014 Link\n\nAnother interesting research by AI2 on \u201cScripts to Compositions to Videos\u201d \u2014 Link\n\nOn Code and Data\u2026\n\nDeepSuperLearner \u2014 an implementation of the deep ensemble methods for classification problems \u2014 Link\n\nOn Industry\u2026\n\nSemantris \u2014 word association game powered by NLP and machine learning \u2014 Link\n\nData scientist gives very useful tips on how to build a profile and prepare for the first data science job \u2014 Link\n\nAlterEgo transcribe words users \u201cspeak silently\u201d; could have major applications in NLP research \u2014 Link\n\nGoogle paper on \u201cExploring the limits of Language Modeling\u201d \u2014 Link\n\nQuotes of the Day\n\nI had to feature one of these here ;)\n\nCool blog post discussing ways to make the most out of Jupyter notebooks \u2014 Link"
    },
    {
        "url": "https://medium.com/dair-ai/textql-colorless-green-rnns-convai2-machine-learning-yearning-meta-learning-tutorial-tinn-d85e64d3b6fb?source=---------6",
        "title": "TextQL, Colorless Green RNNs, ConvAI2, Machine Learning Yearning, Meta-Learning Tutorial, Tinn\u2026",
        "text": "Welcome to the 10th Issue of NLP Newsletter! Here are this week\u2019s notable NLP news!\n\nOn People\u2026\n\nAndrew Ng plans to soon roll-out first draft of his book, Machine Learning Yearning \u2014 Link\n\nDavid Ha et al., presents \u201cWorld Models\u201d, where they aim to answer \u201cCan agents learn inside of their own dreams\u201d \u2014 Link\n\nAn amazing tutorial by Jaan Altosaar on everything there is to know about variational autoencoders \u2014 Link\n\nOn Education and Research\u2026\n\nThis interesting paper presents a \u201cquantitative analysis on the use of skin tone modifiers on emoji on Twitter\u201d \u2014 Link\n\nPaper discussing experiments on neural machine translation using transformer model and Tensor2Tensor framework \u2014 Link\n\nExciting paper on word embeddings applies to massive source of medical data \u2014 Link\n\nStep-by-step implementation of \u201cAttention Is All You Need\u201d using PyTorch \u2014 Link\n\nAn impressive piece teaching how to implement Meta-Learning (learning to learn), which is a very recent trend in machine learning \u2014 Link\n\nHere is a nice piece providing details on the most important and conventional text processing techniques important in text mining \u2014 Link\n\nOn Code and Data\u2026\n\nTextQL is a library that allows you to execute against CSV or TSV text files \u2014 Link\n\nA neat and very detailed tutorial on how to use pandas dataframes \u2014 Link\n\nTinn (Tiny Neural Network) is a simple dependency-free neural network library written in C \u2014 Link\n\nCode for the paper entitled \u201cColorless green recurrent networks dream hierarchically\u201d (by Facebook) \u2014 Link\n\nQuotes of the Week\u2026\n\n\u201cEssentially, all models are wrong, but some are useful\u201d \u2014 George E.P.Box\n\n\u201cIt\u2019s not news that NSA etc is using #NLProc techniques to trawl through language data \u2014 \u2014 and anything world-readable on social media is presumably considered fair game.\u201d \u2014 Emily Bender"
    },
    {
        "url": "https://medium.com/dair-ai/tensorflow-js-code2vec-dl-text-text-distance-group-normalization-linguamatics-54ae24686b29?source=---------7",
        "title": "Tensorflow.js, code2vec, DL-Text, Text-Distance, Group Normalization, Linguamatics, \u2026",
        "text": "On People\u2026\n\nThis comprehensive report details the current state of nerual machine translation (NMT) \u2014 Link\n\nIn this interesting Twitter thread Sebastian Ruder discusses important questions about NLP progress and how it compared to computer vision (CV) \u2014 Link\n\nHow Ashton Kutcher and company plans to use NLP for fighting child sex trafficking \u2014 Link\n\nA Telegram channel to discuss NLP research, papers, and ideas about NLP \u2014 Link\n\nOn Education and Research\u2026\n\nPaper introduces concept of \u201cGroup Normalization\u201d an effective alternative to batch normalization, an important technique in deep learning \u2014 Link\n\nWashington University (in St. Louis) Course T81\u2013558: Applications of Deep Neural Networks (includes Jupyter notebooks as well) \u2014 Link\n\nAn interesting paper that introduces a model for representing snippets of code as continuous distributed vectors (code2vec) \u2014 Link\n\nThis paper shows a comparison between recurrent vs. non-recurrent models for several NLP tasks \u2014 Link\n\nArticle from Georgia Tech on \u201cLearning to represent words by how they\u2019re spelled\u201d \u2014 Link\n\nPaper introduces a \u201cnew language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities\u201d \u2014 Link\n\nOn Code and Data\u2026\n\nTextDistance \u2014 A python library for comparing distance btween two or more sequences, implements various algorithms such as Hamming, Jaccard index, etc. \u2014 Link\n\nDL-Text repository containing code on how to pre-process textual data for deep learning models (Keras, Tensroflow) \u2014 Link\n\nRepository containing many hand-on tutorials about simple tasks used in NLP \u2014 Link\n\nDataTurks offers several open datasets \u2014 check them out \u2014 Link\n\nOn Industry\u2026\n\nTensorflow anounces Tensorflow.js, a Web-GL accelerated, browser-based Javascript library for training and deploying ML models on the web; other announcements were made as well \u2014 Link\n\nLinguamatics is a library that offers tools for text-based drug discovery and exploration \u2014 Link\n\nGoogle presents Google Text-to-Speech, which is powered by DeepMind WaveNet \u2014 Link\n\nApple works to make Siri better through NLP technology \u2014 Link\n\nFind out how Tumi is using AI and NLP for ad targeting \u2014 Link"
    },
    {
        "url": "https://medium.com/dair-ai/deep-learning-notations-resources-99e6e5a71d4c?source=---------8",
        "title": "Deep Learning Notations [Resources] \u2013 DAIR \u2013",
        "text": "Hello World! Lately, I have been open sourcing some computer science resources that have been useful throughout my master and PhD studies. They have just been sitting in my Dropbox folder for quite some time and I think it\u2019s time for other fellow researchers and programmers to benefit from it too.\n\nToday, I am releasing my cheat-sheet of deep learning notations. They have been very useful for preparing papers, blogs, and presentations. I have uploaded it on GitHub in the form of a Jupyter notebook to make it more accessible to everyone. Initially, I just had it as a Latex file, which you can also easily obtain with the Jupyter notebook if you prefer it in that format. Below is a preview of the notebook.\n\nThe notebook contains useful notations widely used in deep learning papers and educational materials found online. I used similar notations as in the Deep Learning book written by Ian Goodfellow, Yoshua Bengio and Aaron Courville. I also provide sample code using PyTorch to show the type of data structures and concepts these notations may represent.\n\nYou can reuse the notations in this notebook to assist you in writing your research papers, presentations, and blogs. It\u2019s also a good resource for reviewing important mathematical notations used widely in deep learning research and other related fields. I provide example code in PyTorch, but as an exercise, you can try generating similar code using Numpy or Tensorflow. (The code shouldn\u2019t be too different.) Enjoy!"
    },
    {
        "url": "https://medium.com/dair-ai/woebot-raises-8-million-tensorflow-1-6-1c067239d319?source=---------9",
        "title": "Woebot raises 8 million, Tensorflow 1.6, \u2013 DAIR \u2013",
        "text": "On People\u2026\n\nBook release of \u201cTensorflow for Deep Learning\u201d \u2014 Link\n\nComputational social science is not equal to computer science plus social data \u2014 Link\n\nOn Education and Research\u2026\n\nGoogle releases new machine learning course for free \u2014 Link\n\n[Paper] Adverserial examples that fool both human and computer vision \u2014 Link\n\nOn Code and Data\u2026\n\nTesorflow 1.6 is released \u2014 Link\n\nGoogle releases new dataset and challenge for landmark recognition \u2014 Link\n\nJupyterLab is released and ready for mass user adoption \u2014 Link\n\nCode repository for Paradigms of Artificial Intelligence Programming (by Peter Norvig) \u2014 Link\n\nOn Industry\u2026\n\nNovel antibiotic recipes could be hidden in medieval medical text \u2014 Link\n\nAllen Institute for AI to pursue common sense for AI via Project Alexandria- Link\n\nHow to conduct cross-validate PCA, K-means clustering, and other supervised algorithms \u2014 Link"
    },
    {
        "url": "https://medium.com/dair-ai/deep-voice-3-ganfather-test-tube-sound-and-meaning-proven-beauty-word-embeddings-in-157-a55b9a152885",
        "title": "Deep Voice 3, GANFather, Test Tube, Sound and Meaning, Proven Beauty, Word Embeddings in 157\u2026",
        "text": "On People\u2026\n\nDeep learning course offered by Fran\u00e7ois Fleuret (includes videos, pdf, etc.) \u2014 Link\n\nThis article explains what\u2019s the relationship between meaning and sound, and why people feel annoyed by words like \u201cmoist\u201d \u2014 Link\n\nMIT Technology Review: The GANfather: The man who\u2019s given machines the gift of imagination \u2014 Link\n\nOn Education and Research\u2026\n\nBaidu presents Deep Voice 3, a fully-convolutional attention-based neural text-to-speech system, which achieved state-of-the-art neural speech synthesis and is order of magnitude faster than current systems \u2014 Link\n\nOn Code and Data\u2026\n\nFasttext releases pre-trained word embeddings in 157 languages, a resource which is useful for those working on multilingual problems or research \u2014 Link\n\nKeras implementation of the \u201cOne pixel attack for fooling deep neural networks\u201d \u2014 Link\n\nTest Tube is a library to track and optimize deep learning experiments \u2014 Link\n\nOn Industry\u2026\n\nProven Beauty, a startup that uses NLP to provide a personalized line of skin care products to its customers \u2014 Link\n\nBaidu\u2019s new algorithm learns to clone a voice, and it only needs less than a minute of audio data from the speaker \u2014 Link\n\nAustralian Securities and Investments Commission (ASIC) wants to use NLP for enforcing and regulating company and financial service laws \u2014 Link\n\nWorthy Mentions\u2026\n\nIan Goodfellow\u2019s new work on \u201cAdverserial Examples that Fool both Human and Computer Vision\u201d \u2014 Link"
    },
    {
        "url": "https://medium.com/dair-ai/deep-reinforcement-learning-agents-tensorflow-spinn-tensor-comprehensions-emotion-understanding-9222eab1422f",
        "title": "Deep Reinforcement Learning Agents, Tensorflow SPINN, Tensor Comprehensions, Emotion Understanding\u2026",
        "text": "On People\u2026\n\nWant to learn to ship neural networks on iOS using PyTorch? See more here \u2014 Link\n\nNando de Freitas discusses the need for taking emotion and sentiment studies more seriously \u2014 Link\n\nOn Code and Data\u2026\n\n(Video) Stephen Merity discusses attention and memory in deep learning networks \u2014 Link\n\nTensorflow introduces SPINN, a tool that enables natural language understanding in Tensorflow with eager execution \u2014 Link\n\nAn introduction to exploratory analysis for performing text mining tasks \u2014 Link\n\nOn Education and Research\u2026\n\nA tutorial on \u201cIntroduction to learning to trade with reinforcement learning\u201d \u2014 Link\n\nInsightful and honest article discussing whether deep reinforcement learning works or not \u2014 Link\n\nA paper discussing various methods on how to improve language modeling by tying input word vectors and output projections \u2014 Link\n\nA list of must-watch resources for learning to become an expert in NLP \u2014 Link\n\nOn Industry\u2026\n\nFacebook AI Research (FAIR) release \u201cTensor Comprehensions\u201d, which is a library that aims to bridge gap between researchers and engineers who work together in ML research and implementations \u2014 Link\n\n\u201cThe nlp algorithm to blame for Google Translator becoming a sexist\u201d \u2014 Link\n\nComputer scientists have developed a text mining tool based on NLP that determines cancer therapeutics given a patient\u2019s biomarkers \u2014 Link\n\nLinguistcs Agents Ltd. releases NLP platform for training of deep reinforcement learning agents \u2014 Link\n\nAn interesting machine learning course on Medium; in the recent episode, visual data analysis for ML is discussed \u2014 Link\n\nSee past issue of the NLP Newsletter \u2014 Link"
    },
    {
        "url": "https://medium.com/dair-ai/nested-lstms-tensorflow-minigo-aaai-2018-notes-extract-4-0-arxiv-vanity-c0487e833c83",
        "title": "\ud83d\udcf0 Nested LSTMs, Tensorflow Minigo, AAAI 2018 notes, Extract! 4.0, Arxiv Vanity",
        "text": "On People\u2026\n\nA great video explaining Shannon\u2019s Information Theory, which are frequently used in machine learning (include lessons on entropy, cross-entropy and KL divergence) \u2014 Link\n\nOn Code and Data\u2026\n\nExcellent tutorial on how automated feature engineering works (also referred to as deep feature synthesis) \u2014 Link\n\nA minimalist python implementation of a neural-network modeled after AlphaGo Zero based on Tensorflow \u2014 Link\n\nShort tutorial on deep learning for speech and language \u2014 Link\n\nOn Education and Research\u2026\n\nPaper discusses the expressive power of recurrent neural networks \u2014 Link\n\nOn conducting error analysis for machine learning project (Lessons learned from Andrew Ng\u2019s deep learning course) \u2014 Link\n\nA summary of machine learning for healthcare at NIPS \u2014 Link\n\nPaper discussing a method that is able to provide model-agnostic explanations on individual predictions for text classifiers \u2014 Link\n\nOn Industry\u2026\n\nPeriscope Data bring Python, R, and SQL into one platform \u2014 Link\n\nCNN and WeSpeke are using natural language processing to teach English to the world \u2014 Link\n\nExtract! 4.0 achieves state-of-the-art results on parsing CVs, which is a powerful technology for recruiters \u2014 Link\n\nWorthy Mentions\u2026\n\nArxiv Vanity, web app that renders academic papers from Arxiv as responsive web pages \u2014 Link\n\nPaper discusses on a novel way of assessing the quality of samples generated by generative adversarial networks (GANs) \u2014 Link\n\nA detailed blog article visually explaining the main ideas behind GAN and WGAN \u2014 Link"
    },
    {
        "url": "https://medium.com/dair-ai/issue-4-sequence-models-course-gary-marcus-vs-yann-lecun-deep-learning-matrix-calculus-54d27241d3ae",
        "title": "Sequence Models course, Gary Marcus vs Yann LeCun, Deep Learning Matrix Calculus, Tensorflow\u2026",
        "text": "On People\u2026\n\nDebate among two leading researchers, Gary Marcus and Yann LeCun, on whether AI needs more innate machinery \u2014 Link\n\nDan Jurafsky, discusses how to process the language of policing and answers \u201cDoes This Vehicle Belong to You?\u201d \u2014 Link\n\nOn Code and Data\u2026\n\nFAIR releases PyTorch implementation of \u201cPoincar\u00e9 Embeddings for Learning Hierarchical Representations\u201d \u2014 Link\n\nAn interesting podcast discussing NLP highlights and the state-of-the-art techniques. In this episode, Dan Roth discusses strategies on how to deal with cases where data is lacking \u2014 Link\n\nTutorial on how to use Mechanical Turk for tagging your training datasets \u2014 Link\n\nIntroducing the \u201cBroad Twitter Corpus\u201d, which is a dataset containing tweets collected over stratified times, places and social uses \u2014 Link\n\nBeginner\u2019s tutorial on how to train and visualize word vectors (embeddings) \u2014 Link\n\nTensorflow code used for Capsule model implemented in the paper \u201cDynamic Routing between Capsules\u201d \u2014 Link\n\nOn Education and Research\u2026\n\nDeeplearning.ai releases new course on sequence models, introducing topics such as GRUs, LSTMs, and Recurrent Neural Networks (RNNs) \u2014 Link\n\nFree online book release by Jeremy Howard discussing the fundamental maths you need for deep learning: \u201cThe Matrix Calculus You Need For Deep Learning\u201d \u2014 Link\n\nAnother beautiful online website for testing your regular expressions \u2014 Link\n\nOn Industry\u2026\n\nYann LeCun steps down as Facebook\u2019s head of AI research group, FAIR \u2014 Link\n\nAndrew Ng continues his mission to build an AI-powered society with the formation of the AI Fund \u2014 Link\n\nSimple tests show the shallowness of the state-of-the-art Google translate \u2014 Link\n\nOn using NLP and machine learning for generating clinical labels of medical scans \u2014 Link\n\nWorthy Mentions\u2026\n\nThe \u201cNLP News\u201d newsletter (Poincar\u00e9 embeddings, trolling trolls, A2C comic, General AI Challenge, heuristics for writing, year of PyTorch, BlazingText, MaskGAN, Moments in Time) \u2014 Link\n\nHere is the previous release (#3) of the NLP Newsletter (Emotional Chatbot, Google Colab, Detectron, Deep Learning Course, Google Free GPU, \u2026) \u2014 Link"
    },
    {
        "url": "https://medium.com/dair-ai/nlp-chronicles-1-interdisciplinary-studies-ethics-communication-moocs-inclusiveness-80b398eefcfc",
        "title": "NLP Chronicles #1: Interdisciplinary studies, ethics, communication, MOOCs, inclusiveness,\u2026",
        "text": "Hello World! I am a third year Ph.D. student with a focus in Natural Language Processing and Machine Learning, and I would like to address a few things \u2014 good and bad \u2014 that I have recently observed from the NLP community. Topics of discussion will include but are not limited to interdisciplinary studies, communication, research, ethics, opportunities, MOOCs, curriculum, leadership, community, and inclusiveness. Essentially, in this first episode, I will lay the foundation of what is to come in the NLP Chronicles, in which I envision open and constant discussion about some of the most important topics surrounding the field of NLP and related areas. Please note that this work is still in draft mode, and is part of a paper I am intending to publish in the near future. I would love some feedback or suggestions from the community!\n\nRecently, Natural Language Processing (NLP), through its applications and studies, has emerged as one of the most important and hottest fields in computer science. Anyone can agree that this was made possible through the advancement of high-performance computing and the availability of big data, not to mention the involvement of hundreds of brilliant researchers from all over the world. Frankly speaking, I am kind of understating how important NLP technologies are today? Just take a look at Woebot and you will experience at a higher level what I am talking about here \u2014 on the surface, it appears to our eyes as pure wizardry, but it\u2019s just a basic example of what\u2019s possible with NLP. There are many other exciting technologies, such as translators, that make use of NLP, but I am not trying to make this article sound like another product review, so I will just jump right into the meat of the matter.\n\nThe NLP field is advancing so rapidly that literally every day you will hear of the novel and interesting ways on solving different NLP tasks \u2014 whether through transfer learning or data augmentation. And that\u2019s not all, I have recently been seeing new and exciting ways to model and represent data, including a rise in social computation, which are all exciting areas for anyone dealing with NLP related research and those that are not (journalists tend to love the latter).\n\nThe truth of the matter is that recently the NLP community has become a vibrant and crowded one \u2014 and that\u2019s a good thing! But is everything as good as it seems in our community? Sad to say that this is not the case, and I am truly sorry to be the one to synthesize and reverberate some of the problems present in OUR community today. Problems that need to be carefully addressed, with devotion and desire. Below, I will try my best to highlight some of the areas that need improvement, all rendered from an objective point of view. Where I falter, I offer my apologies in advance! It\u2019s just that I am too passionate about this field and sometimes I tend to get carried away. My hope is that we can all advance smartly, efficiently, effectively, and ethically.\n\nWhere to start? I am lost!\n\nNLP has recently become popular because of several key events, thus, thousands of students and enthusiasts keep rushing into the field every year, sometimes with little or no prior experience or knowledge. Some survive, but the majority struggle because they think the field is too difficult when the reality is that the field is quite young and thus not difficult to grasp. You just need the proper guidance \u2014 I suggest you start here. Actually, I have shared random suggestions like this in the past, but I can\u2019t put them all at once here. I promise to work on a solution for this as I keep discussing NLP on this blog or my NLP newsletter. On that note, I also believe a crowd-sourced, universal curriculum can address this issue, but more on this idea later. As I spoke of the lack of guidance for newcomers, I think you can agree that we can improve this in many different ways, which I will discuss again in the future.\n\nAnother problem I have observed in the NLP community, related to the first point, is that a lot of new learners struggle with small things such as making a distinction between NLP and Computational Linguistics? There are various myths like this lingering in the NLP field and they can be intimidating and problematic for starters. The Achilles\u2019 heels of a learner are misconceptions. Perhaps, demystifying through AMAs can help or even live streaming Q&As. I can easily see how this can be a fun, worthwhile and helpful experience for all parties involved.\n\nBeginners also struggle because they cannot easily locate NLP communities to receive help or guidance. I only expect you can relate to this one if you are a beginner. If they do find communities, such as in places like Reddit and Facebook groups, they are usually toxic and full of irrelevant content. How do we address this problem? We need as much leaders and communicators as possible. I have noticed that very few people take the time to explain where to get help, because everyone is busy advancing the field. I don\u2019t blame anyone for that, but we can do better. You can also try twitter, but you will find that it is more of the same problem. Slack kind of works and I have advocated for this on many occasions in the past, but adoption is the main problem there.\n\nI hate to say it, but we desperately need more NLP communicators in our field. In the language used by YouTubers, it\u2019s \u201ccringing\u201d to see headliners like this: \u201cFacebook Catches Two Chatbots Speaking Their Own Language\u201d. The reality is that we are just not there yet and I hate to see these annoying click baits, which just hinders our field and spread misinterpretations. I have no idea on how to completely eradicate this problem, but I am sure we can build an NLP-powered system to detect click baits and zap them from our news feeds ;). And let\u2019s not even begin to talk about the constant manipulation of social media and the negative effects it has on society. We are going to break these things down \u2014 one by one \u2014 in future episodes of this chronicle. We are going to communicate with as less jargon as possible too. \u201cLet\u2019s get it!\u201d\n\nA lot of people ask me what are the best NLP courses online since they are only a very small number available. I do believe OpenCourseWare and MOOCs are crucial technologies to share knowledge and resources in our community. Generally speaking, I think we need to invest more efforts in unifying and assisting our community, whether it be through online teaching or open discourse. I mean, isn\u2019t it ironic that we work daily on improving communication systems and dialogue systems but yet communication with the outer world is not our forte. I hope that I am not being too critical here, but it\u2019s pretty obvious that there is room for improvement here as well. Conferences are just not enough\u2026 in my opinion, they are too exclusive. YouTube helps, but it is too disorganized. MOOCs are another option, but you will find that they also need to be improved and in some cases updated.\n\nAs a third-year Ph.D. student, and thanks to the years of exposure to the field, I have managed to come up with a formula for conducting various NLP research projects, and in a very short time. But it has come to my attention that there are bigger problems in this field, such as ethics and communication that need more attention. So even though I am helping to advance core areas in NLP, I feel like the balance is tilting too much in one direction, which is usually not a good thing.\n\nI am also aware that some beginners in the field struggle to find interesting topics or even datasets to work with. Don\u2019t panic, this is quite normal in this field. Many would say \u201cfocus\u201d is the way to go, but I would argue that \u201cinterdisciplinary\u201d research is the direction. Take, for instance, Dan Jurafky, for me one of the coolest and smartest NLP researchers out there. All his epic and amazing work are based on interdisciplinary studies. There is lot to learn from researchers like him. By the way, we also need to consider more inclusiveness because this is what enables interdisciplinary and intercultural studies, which is another huge area of potential for NLP researchers and enthusiasts.\n\nInitially, this was intended to be an article about ethics and education as it relates to the field of NLP, but as I kept writing I realized there was more and more. So I summarized a few issues and observations, and it gave me the idea to start a chronicle, where I will spend at least once a week discussing a wide range of topics that include but not limited to the following:\n\n\u00b7 Ethics and Society in the context of NLP\n\n\u00b7 How to publish and communicate your NLP ideas: Discussions / Podcasts / AMAs"
    },
    {
        "url": "https://medium.com/dair-ai/scientific-writing-tips-for-ml-researchers-fa4cae2effdd",
        "title": "Scientific writing tips for ML researchers \u2013 DAIR \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/dair-ai/emotional-chatbot-detectron-deep-learning-course-google-free-gpu-8fc7867e6f1c",
        "title": "Emotional Chatbot, Detectron, Deep Learning Course, Google Free GPU,\u2026",
        "text": "On People\u2026\n\nDevelopment on a chatbot that can better understand emotions from conversation \u2014 Link\n\nOn Code and Data\u2026\n\nPopular deep learning library PyTorch celebrates its 1st anniversary and shares the year of remarkable growth and community response \u2014 Link\n\nOn Education and Research\u2026\n\nA new course on deep learning goes open to the public, which include notes, links, and other resource \u2014 Link\n\nA great paper summarizing the current research being done in sentiment analysis, specifically with the use of deep learning\u2014 Link\n\nUnderstanding why Neural networks are important for NLP tasks: implications and thoughts \u2014 Link\n\nInteresting guide released on using machine learning to solve 90% of NLP problems \u2014 Link\n\nDiscussion on the implications of Artificial Intelligence in our near future, and whether it will achieve intelligence far superior to human intelligence (Podcast) \u2014 Link\n\nOn Industry\u2026\n\nGoogle Colab offers free access to GPUs on their online collaborative notebook service \u2014 Link\n\nGoogle launches Cloud ML, which aims to bring AI services to businesses \u2014 Link\n\nMastercard claims that conversational banking may already be mainstream \u2014 Link\n\nWorthy mentions\u2026\n\nAAAI releases results for the first ever research track on AI Ethics and Society \u2014 Link\n\nFakeApp is an application that allows the ability to generate fake porn through face-swapping algorithm powered by deep learning techniques \u2014 Link\n\nFacebook brings faster translation services to its platform thanks to multilingual embeddings \u2014 Link"
    },
    {
        "url": "https://medium.com/dair-ai/issue-2-best-of-nlp-research-bipolar-detection-openais-faster-neural-networks-sarcastobot-e357bd539e4",
        "title": "Issue 2 \u2014 Best of NLP research, Bipolar Detection, OpenAI\u2019s faster neural networks, Sarcastobot, \u2026",
        "text": "Summary of some of the latest and most important works on NLP (2016 \u2014 Current)\n\nSource: 57 Summaries of Machine Learning and NLP Research\n\nHow emotion recognition through voice will help machines to have a healthy relationship with humans.\n\nSource: Wired\n\nAI and Natural Language Processing used to improve care for heart failure patients.\n\nSource: HealthITAnalytics"
    },
    {
        "url": "https://medium.com/dair-ai/will-ai-achieve-intelligence-far-superior-to-human-intelligence-podcast-2df79b4fd8c1",
        "title": "Will AI achieve intelligence far superior to human intelligence? (Podcast)",
        "text": "In this episode of my podcast, I discuss the implications of Artificial Intelligence in our near future, and whether it will achieve intelligence far superior to human intelligence."
    },
    {
        "url": "https://medium.com/dair-ai/first-nlp-newsletter-on-github-896471e75aab",
        "title": "First Ever NLP Newsletter on GitHub \u2013 DAIR \u2013",
        "text": "Are you a constant GitHub user and love Natural Language Processing (NLP)? Then I have created just the solution for you to get your NLP news from around academia and the industry\u2026 right on GitHub.\n\nThe idea of this newsletter is that you get the most important and latest NLP news without ever needing to leave GitHub (less distraction, more research). If you have found an interesting piece of news that you would like to spread to the community, you can also contribute to the newsletter as well.\n\nI chose GitHub simply because it allows for easy distribution of information. Let me know your thoughts. Want to be an editor (that would help because I am planning to publish an issue every week)? Let me know in the comments section."
    }
]