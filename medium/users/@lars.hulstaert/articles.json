[
    {
        "url": "https://towardsdatascience.com/going-deep-into-object-detection-bed442d92b34?source=user_profile---------1----------------",
        "title": "Going deep into object detection \u2013",
        "text": "With recent advancements in deep learning based computer vision models, object detection applications are easier to develop than ever before. Besides significant performance improvements, these techniques have also been leveraging massive image datasets to reduce the need for large datasets. In addition, with current approaches focussing on full end-to-end pipelines, performance has also improved significantly, enabling real-time use cases.\n\nSimilar to the blogpost I wrote on the different image classification architectures, I will go over two object detection architectures. I will discuss SSD and Faster RCNN, which are currently both available in the Tensorflow Detection API.\n\nFirst I will go over some key concepts in object detection, followed by an illustration of how these are implemented in SSD and Faster RCNN.\n\nPeople often confuse image classification and object detection scenarios. In general, if you want to classify an image into a certain category, you use image classification. On the other hand, if you aim to identify the location of objects in an image, and e.g. count the number of instances of an object, you can use object detection.\n\nThere is however some overlap between these two scenarios. If you want to classify an image into a certain category, it could happen that the object or the characteristics that are required to perform categorisation are too small with respect to the full image. In that case, you would achieve better performance with object detection instead of image classification even if you are not interested in the exact location or counts of the object.\n\nImagine you need to check circuit boards and classify them as either defect or correct. While it is essentially a classification problem, the defects might be too small to be noticeable with an image classification model. Constructing an object detection dataset will cost more time, yet it will result most likely in a better model.\n\nWith an image classification model, you generate image features (through traditional or deep learning methods) of the full image. These features are aggregates of the image. With object detection, you do this on a more fine-grained, granular, regional level of the image. In the former you might lose track of the classification signal, whereas in the latter the signal might be preserved in a way that is more suitable for the use case.\n\nIn order to train a custom model, you need labelled data. Labelled data in the context of object detection are images with corresponding bounding box coordinates and labels, i.e. the bottom left and top right (x,y) coordinates + the class .\n\nA question that is always asked is the following: in order to do object detection on problem X, how many pictures do I need? Instead, it is more important to properly understand in which scenarios the model will be deployed. It is crucial that a large number (e.g. > 100 and potentially >1000) representative images are available per class. Representative in this context means that they should corresponds with the range of scenarios in which the model will be used. If you are building a traffic sign detection model that will run in a car, you have to use images taken under different weather, lighting and camera conditions in their appropriate context. Object detection models are not magic and actually rather dumb. If the model does not have enough data to learn general patterns, it won\u2019t perform well in production.\n\nTypically, there are three steps in an object detection framework. \n\n1. First, a model or algorithm is used to generate regions of interest or region proposals. These region proposals are a large set of bounding boxes spanning the full image (i.e. an object localisation component). \n\n2. In the second step, visual features are extracted for each of the bounding boxes, they are evaluated and it is determined whether and which objects are present in the proposals based on visual features (i.e. an object classification component).\n\n3. In the final post-processing step, overlapping boxes are combined into a single bounding box (i.e. non maximum suppression).\n\nSeveral different approaches exist to generate region proposals. Originally, the \u2018selective search\u2019 algorithm was used to generate object proposals. Lillie Weng provides a thorough explanation on this algorithm in her blog post. In short, selective search is a clustering based approach which attempts to group pixels and generate proposals based on the generated clusters.\n\nOther approaches use more complex visual features extracted from the image to generate regions (e.g. based on the features from a deep learning model) or adopt a brute-force approach to region generation. These brute-force approaches are similar to a sliding window that is applied to the image, over several ratios and scales. These regions are generated automatically, without taking into account the image features.\n\nAn important trade-off that is made with region proposal generation is the number of regions vs. the computational complexity. The more regions you generate, the more likely you will be able to find the object. On the flip-side, if you exhaustively generate all possible proposals, it won\u2019t be possible to run the object detector in e.g. real-time. In some cases, it is possible to use problem specific information to reduce the number of ROI\u2019s (e.g. pedestrian typically have a ratio of approximately 1.5, so it is not useful to generate ROI\u2019s with a ratio of 0.25).\n\nThe goal of feature extraction is to reduce a variable sized image to a fixed set of visual features. Image classification models are typically constructed using strong visual feature extraction methods. Whether they are based on traditional computer vision approaches (e.g. filter based approached, histogram methods, etc.) or deep learning methods, they all have the exact same objective: extract features from the input image that are representative for the task at hands and use these features to determine the class of the image. In object detection frameworks, people typically use pretrained image classification models to extract visual features, as these tend to generalise fairly well (e.g. a model trained on the MS CoCo dataset is able to extract fairly generic features). In order to improve the model however, it is advised to experiment with different approaches. My blog post on transfer learning provides a clear distinction between the different types of transfer learning as well as their advantages and disadvantages (general and applications).\n\nThe general idea of non-maximum suppression is to reduce the number of detections in a frame to the actual number of objects present. If the object in the frame is fairly large and more than 2000 object proposals have been generated, it is quite likely that some of these will have significant overlap with each other and the object. Watch this video on Coursera to learn more about NMS. NMS techniques are typically standard across the different detection frameworks, but it is an important step that might require hyperparameter tweaking based on the scenario.\n\nThe most common evaluation metric that is used in object recognition tasks is \u2018mAP\u2019, which stands for \u2018mean average precision\u2019. It is a number from 0 to 100 and higher values are typically better, but it\u2019s value is different from the accuracy metric in classification.\n\nEach bounding box will have a score associated (likelihood of the box containing an object). Based on the predictions a precision-recall curve (PR curve) is computed for each class by varying the score threshold. The average precision (AP) is the area under the PR curve. First the AP is computed for each class, and then averaged over the different classes. The end result is the mAP.\n\nNote that a detection is a true positive if it has an \u2018intersection over union\u2019 (IoU or overlap) with the ground-truth box greater than some threshold (usually 0.5). Instead of using mAP we typically use mAP@0.5 or mAP@0.25 to refer to the IoU that was used.\n\nThe Tensorflow Detection API brings together a lot of the aforementioned ideas together in a single package, allowing you to quickly iterate over different configurations using the Tensorflow backend. With the API, you are defining the object detection model using configuration files, and the Tensorflow Detection API is responsible for structuring all the necessary elements together.\n\nIn order to have a better understanding of what the different supported components are, have a look at the \u2018protos folder\u2019 which contains the function definitions. Especially, the train, eval, ssd, faster_rcnn and preprocessing protos are important when fine-tuning a model.\n\nOverview\n\nThe SSD architecture was published in 2016 by researchers from Google. It presents an object detection model using a single deep neural network combining regional proposals and feature extraction.\n\nA set of default boxes over different aspect ratios and scales is used and applied to the feature maps. As these feature maps are computed by passing an image through an image classification network, the feature extraction for the bounding boxes can be extracted in a single step. Scores are generated for each object category in every of the default bounding boxes. In order to better fit the ground truth boxes adjustment offsets are calculated for each box.\n\nDifferent feature maps in the convolutional network correspond with different receptive fields and are used to naturally handle objects at different scales . As all the computation is encapsulated in a single network and fairly high computational speeds are achieved (e.g. for 300 \u00d7 300 input 59 FPS).\n\nUsage\n\nFor the usage we will investigate the different sample configuration files for SSD. Several parameters are important when leveraging the SSD architecture and we will go over them one by one.\n\nFirst, different classification networks have different strengths and weaknesses (see this blog post for an overview). The Inceptionv3 network for example is trained to detect objects well at different scales, whereas the ResNet architecture achieves very high accuracy overall. Mobilenet on the other is a network that was trained to minimise the required computational resources. The performance of the feature extraction network on ImageNet, the number of parameters and the original dataset it was trained on are a good proxy for the performance/speed tradeoff. The feature extractor is defined in the \u2018feature_extractor\u2019 section.\n\nA second obvious set of parameters are the settings for the default boxes and aspect ratios. Depending on the type of problem, it is worthwhile to analyse the various aspect ratios and scales of the bounding boxes of the labeled data. Setting the aspect ratios and scales will ensure that the network does not do unnecessary calculations. You can tweak these in the \u2018ssd_anchor_generator\u2019 section. Note that adding more scales and aspect ratios will lead to better performance, but typically with diminishing returns.\n\nThirdly, when training the model it is important to set the image size and data augmentation options in the \u2018data_augmentation_options\u2019 and \u2018image_resizer\u2019 sections. A larger image size will perform better as small object are often hard to detect, but it will have a significant computational cost. Data augmentation is especially important in the context of SSD in order to be able to detect objects at different scales (even at scales which might not be present in the training data).\n\nFinally, tweaking the \u2018train_config\u2019, setting the learning rates and batch sizes is important to reduce overfitting, and will highly depend on the size of the dataset you have.\n\nOverview\n\nFaster R-CNN was developed by researchers at Microsoft. It is based on R-CNN which used a multi-phased approach to object detection. R-CNN used Selective search to determine region proposals, pushed these through a classification network and then used an SVM to classify the different regions.\n\nFaster R-CNN, similar to SSD, is an end-to-end approach. Instead of using default bounding boxes, Faster R-CNN has a Region Proposal Network (RPN) to generate a fixed set of regions. The RPN uses the convolutional features from the the image classification network, enabling nearly cost-free region proposals. The RPN is implemented as a fully convolutional network that predicts object bounds and objectness scores at each position.\n\nNote that the RPN has a similar setup as the the SSD network (i.e. it does not predict bounding boxes out of thin air). The RPN network works with sliding windows across the feature maps. At each sliding-window location or anchor, a set of proposals are computed with various scales and aspect ratios. Similar to SSD, the outcome of the RPN are \u2018adjusted\u2019 bounding boxes, based on the anchors.\n\nThe different components are combined in a single setup and are trained either end-to-end or in multiple phases (to improve stability). Another interpretation of the RPN is that it guides the networks \u2018attention\u2019 to interesting regions.\n\nUsage\n\nMost of the usage details of Faster R-CNN are similar as the ones for SSD. In terms of raw mAP, Faster R-CNN typically outperforms SSD, but it requires significantly more computational power.\n\nAn important section for the Fast-RCNN detector, is the \u2018first_stage_anchor_generator\u2019 which defines the anchors generated by the RPN. The strides in this section defines the steps of the sliding window. Note that especially when attempting to detect small objects (if the stride is too large, you might miss them).\n\nAlthough no extensive data augmentation was used by the authors of the Faster-RCNN paper, it is still advised to use it when working with smaller datasets.\n\nThere are several more object detection architectures, which I haven\u2019t touched upon. Especially when looking at real-time applications, Yolov2 is often coined as an important architecture (fairly similar to SSD). I will update this blog post whenever it is added to the Tensorflow Detection API.\n\nIf you have any questions, I\u2019ll be happy to read them in the comments. Follow me on Medium or Twitter if you want to receive updates on my blog posts!"
    },
    {
        "url": "https://towardsdatascience.com/an-overview-of-image-classification-networks-3fb4ff6fa61b?source=user_profile---------2----------------",
        "title": "Going deep into image classification \u2013",
        "text": "Learning about the different network architectures for image classification is a daunting task. In this blog post I will discuss the main architectures that are currently available in the keras package. I will go through these architectures in a chronological order and attempt to discuss their advantages and disadvantages from the perspective of a practitioner.\n\nAlthough different researchers in the computer vision field tend to follow different practices, overall you can see the following trends when setting up experiments. I discuss how the images are pre-processed, what type of data augmentation is used, the optimisation mechanism and the implementation of the final layer.\n\nPre-processing\n\nOften, the mean pixel value is computed over the training set and subtracted from the images. Note that it is important to take this into account when using these models with keras. Keras provides different \u2018pre-processing\u2019 functions for each of the computer vision models.\n\nData augmentation\n\nImage classification research datasets are typically very large. Nevertheless, data augmentation is often used in order to improve generalisation properties. Typically, random cropping of rescaled images together with random horizontal \ufb02ipping and random RGB colour and brightness shifts are used. Different schemes exist for rescaling and cropping the images (i.e. single scale vs. multi scale training). Multi-crop evaluation during test time is also often used, although computationally more expensive and with limited performance improvement. Note that the goal of the random rescaling and cropping is to learn the important features of each object at different scales and positions. Keras does not implement all of these data augmentation techniques out of the box, but they can easily implemented through the preprocessing function of the ImageDataGenerator modules. The data augmentation techniques by Andrew Howard explain the key methods more in-depth.\n\nTraining mechanism\n\nModels are typically trained with a batch size of 256, using multi-GPU data parallelism, which is also available in keras. Either SGD with momentum or RMSProp is often used as optimisation technique. Learning rate schemes are often fairly simple, either lowering the learning rate when the validation loss or accuracy starts to stabilise or lowering the learning rate at a fixed interval. With the \u2018ReduceLROnPlateau\u2019 callback in keras, you can easily mimick this behaviour.\n\nFinal layer\n\nThe final layer in an image classification network was traditionally a fully connected layer. These layers are massive parameters hogs as you need NxM parameters to go from N to M hidden layer nodes. Nowadays, these layers have been replaced by average or max pooling layers requiring less parameters and computational time. When fine-tuning pre-trained networks in keras, it is important to take this into account to limit the number of parameters that are added.\n\nOriginally published in 2014 by Karen Simonyan and Andrew Zisserman, VGGNet showed that stacking multiple layers is a critical component for good performance in computer vision. Their published networks contain 16 or 19 layers and consist primarily of small 3x3 convolutions and 2x2 pooling operations.\n\nThe authors main contribution is showing that stacking multiple small filters without pooling can increase the representational depth of the networks while limiting the number of parameters. By stacking e.g. three 3\u00d73 conv. layers instead of using a single 7\u00d77 layer several limitations are overcome. First, three non-linear functions are combined instead of a single one, which makes the decision function more discriminative and expressive. Second, the number of parameters is reduced by 81% while the receptive field stays the same. Working with smaller filters thus also acts as a regularizer and improves the effectiveness of the different convolutional filters.\n\nA downside of the VGGNet is that it is more expensive to evaluate than shallow networks and uses a lot more memory and parameters (140M). A lot of these parameters can be attributed to the first fully connected layer. It was shown that these layers can be removed with no performance downgrade while significantly reducing the number of necessary parameters. VGG is available on keras with pre-trained weights, both in the 16 and 19 layers variant.\n\nThe ResNet architecture was developed by Kaiming He et al. in an attempt to train networks with even larger depth. The authors noted that increasing the network depth resulted in a higher training loss, indicating potential training convergence issues due to gradient problems (exploding/vanishing gradients).\n\nTheir main contribution is the addition of skip connections to neural network architectures, using batch normalisation and removing the fully connected layers at the end of the network.\n\nSkip connections are based on the idea that as long as a neural network model is able to \u2018properly\u2019 propagate the information from the previous layers to the next layer, it should be able to become \u2018indefinitely\u2019 deep. In case no additional information is aggregated by going deeper, than a convolutional layer with a skip connection can act as the identity function. \n\nBy adding skip connections to the network the default function of a convolutional layer becomes the identify function. Any new information that the filters learn, can be subtracted or added to the base representation and it is thus easier to optimize the residual mapping. Skip connections do not increase the number of parameters, but do result in more stable training and a significant performance boost because deeper networks can be attained(e.g. networks of depth 34, 50, 101 and 152). Note that 1x1 convolutions are used to map a layers input to its output!\n\nIn addition to the skip-connections, batch normalisation was used after each convolution and before each activation. Finally, fully-connected layers were removed and instead an average pooling layer was used to reduce the number of parameters. The increased abstraction power of the convolutional layers due to a deeper network reduces the need for fully connected layers.\n\nThe GoogLeNet paper was published around the same time as the ResNet paper but introduces different improvements. The previous two papers focused on increasing the representation depth of classification network.\n\nWith GoogLeNet however, the authors still attempt to scale up networks (up to 22 layers) but at the same time they aim to reduce the number of parameters and required computational power. The original Inception architecture was published by Google and focused on applying CNN\u2019s in big-data scenario\u2019s as well as mobile settings. The architecture is fully convolutional and consists of Inception modules. The goal of these modules is to increase the convolutional filters learning abilities and abstraction power by constructing a complex filter which consists of multiple building blocks (e.g. a network in network \u2014 Inception).\n\nIn addition to Inception modules, the authors also used auxiliary classifiers to promote more stable and better convergence. The idea of auxiliary classifiers is that several different image representations are used to perform classification (yellow boxes). As a result, gradients are calculated at different layers in the model, which can then be used to optimise training.\n\nWith the Inceptionv3 architecture a couple of innovations are combined.\n\nIn Inceptionv3, the primary focus is on reusing some of the original ideas GoogLeNet and VGGNet, i.e. using the Inception module and expressing large filters more efficiently with a series of smaller convolutions. In addition to small convolutions, the authors also experiment with assymmetric convolutions (e.g. replacing nxn by nx1 and 1xn instead of multiple 2x2 and 3x3 filters).\n\nThe authors improved regularisation of the network by performing batch-normalisation and label-smoothing. Label-smoothing is the practice of assigning each class some weight, instead of assigning the ground truth label the full weight. As the network will overfit less on the training labels, it should be able to generalise better, which is is a similar practice as using L2 regularisation.\n\nA lot of care was put into ensuring that the model would perform well on both high and low resolution images, which is enabled by the Inception modules analysing the image representations at different scales. As a result, when Inception networks are used in object detection framework, they perform well as classifying small and low resolution objects.\n\nThe last image classification architecture I will discuss is NASNet, which was constructed using the Neural Architecture Search (NAS) framework. The goal of NAS is to use a data-driven and intelligent approach to constructing the network architecture instead of intuition and experiments. Although I won\u2019t go into details of the framework, the general idea is explained.\n\nIn the Inception paper, it was shown that a complex combination of filters in a \u2018cell\u2019 can significantly improve results. The NAS framework defines the construction of such a cell as an optimisation process, and then stacks the multiple copies of the best cell to construct a large network.\n\nUltimately, two different cells are constructed and used to train the full model.\n\nThe table below was taken from Keras and it provides a good overview of the results of the different networks I discussed. Note that the VGG stand out with their massive number of parameters, whereas Resnet50 and Inceptionv3 stand out with their impressive network depth. At the moment of writing, NASnet (not in table) achieves best performance with 82.7% top-1 and 96.2% top-5 accuracy on ImageNet.\n\nIf you have any questions, I\u2019ll be happy to read them in the comments. Follow me on Medium or Twitter if you want to receive updates on my blog posts!"
    },
    {
        "url": "https://towardsdatascience.com/interpretability-in-machine-learning-70c30694a05f?source=user_profile---------3----------------",
        "title": "Interpreting machine learning models \u2013",
        "text": "Regardless of the end goal of your data science solutions, an end-user will always prefer solutions that are interpretable and understandable. Moreover, as a data scientist you will always benefit from the interpretability of your model to validate and improve your work. In this blog post I attempt to explain the importance of interpretability in machine learning and discuss some simple actions and frameworks that you can experiment with yourself.\n\nIn traditional statistics, we construct and verify hypotheses by investigating the data at large. We build models to construct rules that we can incorporate into our mental models of processes. A marketing firm for example can build a model that correlates marketing campaign data to finance data in order to determine what constitutes an effective marketing campaign.\n\nThis is a top-down approach to data science, and interpretability is key as it is a cornerstone of the rules and processes that are defined. As correlation often does not equal causality, a solid model understanding is needed when it comes to making decisions and explaining them.\n\nIn a bottom-up approach to data science, we delegate parts of the business process to machine learning models. In addition, completely new business ideas are enabled by machine learning . Bottom-up data science typically corresponds to the automation of manual and laborious tasks. A manufacturing firm can for example put sensors on their machines and perform predictive maintenance. As a result, maintenance engineers can work more efficiently and don\u2019t need to perform expensive periodic checks. Model interpretability is necessary to verify the that what the model is doing is in line with what you expect and it allows to create trust with the users and ease the transition from manual to automated processes.\n\nAs a data scientist you are often concerned with fine-tuning models to obtain optimal performance. Data science is often framed as: \u2018given data X with labels y, find the model with minimal error\u2019. While the ability to train performant models is a critical skill for a data scientist, it is important to be able to look at the bigger picture. Interpretability of data and machine learning models is one of those aspects that is critical in the practical \u2018usefulness\u2019 of a data science pipeline and it ensures that the model is aligned with the problem you want to solve. Although it is easy to lose yourself in experimenting with state-of-the-art techniques when building models, being able to properly interpret your findings is an essential part of the data science process.\n\nThere are several reasons to focus on model interpretability as a data scientist. Although there is overlap between these, they capture the different motivations for interpretability:\n\nBias is potentially present in any dataset and it is up to the data scientist to identify and attempt to fix it. Datasets can be limited in size and they might not be representable for the full population, or the data capturing process might have not accounted for potential biases. Biases often only become apparent after thorough data analysis or when the relation between model predictions and the model input is analysed. If you want to learn more about the different types of types of bias exist, I highly recommend the video below. Note that there is no single solution to resolving bias, but a critical step towards interpretability being aware of potential bias.\n\nOther examples of bias are the following:\n\ne.g. word2vec vectors contain gender biases due to the inherent biases that are present in the corpora they have been trained on. When you would train a model with these word embeddings, a recruiter searching for \"technical profiles\" will leave female resumes at the bottom of the pile.\n\ne.g. when you train an object detection model on a small, manually created dataset, it is often the case that the breadth of images is too limited. A wide variety of images of the objects in different environments, different lightning conditions and different angles is required in order to avoid an model that only fits to noisy and unimportant elements in the data.\n\nIn most problems, you are working with a dataset that is only a rough representation of the problem you are trying to solve and a machine learning model can typically not capture the full complexity of the real-life task. An interpretable model helps you to understand and account for the factors that are (not) included in the model and account for the context of the problem when taking actions based on model predictions.\n\nA high interpretability typically leads to a model that generalises better. Interpretability is not about understanding every single detail of the model for all of the data points. The combination of solid data, model and problem understanding is necessary to have a solution that performs better.\n\nIn industries like finance and healthcare it is essential to audit the decision process and ensure it is e.g. not discriminatory or violating any laws. With the rise of data and privacy protection regulation like GDPR, interpretability becomes even more essential. In addition, in medical applications or self-driving cars, a single incorrect prediction can have a significant impact and being able to \u2018verify\u2019 the model is critical. Therefore the system should be able to explain how it reached a given recommendation.\n\nA common quote on model interpretability is that with an increase in model complexity, model interpretability goes down at least as fast. Feature importance is a basic (and often free) approach to interpreting your model. Even for black-box models such as deep learning, techniques exist to improve interpretability. Finally, the LIME framework will be discussed, which serves as a toolbox for model analysis.\n\nGeneralised Linear Models (GLM\u2019s) are all based on the following principle: \n\nif you take a linear combination of your features x with the model weights w, and feed the result through a squash function f, you can use it to predict a wide variety of response variables. Most common applications for GLM\u2019s are regression (linear regression), classification (logistic regression) or modelling Poisson processes (Poisson regression). The weights that are obtained after training are a direct proxy of feature importance and they provide very concrete interpretation of the model internals.\n\ne.g. when building a text classifier you can plot the most important features and verify whether the model is overfitting on noise. If the most important words do not correspond to your intuition (e.g. names or stopwords), it probably means that the model is fitting to noise in the dataset and it won\u2019t perform well on new data.\n\nEven non-linear models such as tree based models (e.g. Random Forest) also allow to obtain information on the feature importance. In Random Forest, feature importance comes for free when training a model, so it is a great way to verify initial hypotheses and identify \u2018what\u2019 the model is learning. The weights in kernel based approaches such as SVM\u2019s are often not a very good proxy of feature importance. The advantage of kernel methods is that you are able to capture non-linear relations between variables by projecting the features into kernel space. On the other hand, just looking at the weights as feature importance does not do justice to the feature interaction.\n\nDeep learning models are notorious for their un-interpretability due to the shear number of parameters and the complex approach to extracting and combining features. As this class of models is able to obtain state-of-the-art performance on a lot of tasks, a lot of research is focused on linking model predictions to the inputs.\n\nEspecially when moving towards even more complex systems that process text and image data, it becomes hard to interpret what the model is actually learning. The main focus in research is currently primarily on linking and correlating outputs or predictions back to the input data. While this is fairly easy in the context of linear model, it is still an unsolved problem for deep learning networks. The two main approaches are either gradient-based or attention-based.\n\n-In gradient-based methods, the gradients of the target concept calculated in a backward pass are used to produce a map that highlights the important regions in the input for predicting the target concept. This is typically applied in the context of computer vision.\n\n-Attention-based methods are typically used with sequential data (e.g. text data). In addition to the normal weights of the network, attention weights are trained that act as \u2018input gates\u2019. These attention weights determine how much each of the different elements in the final network output. Besides interpretability, attention within the context of the e.g. text-based question-answering also leads to better results as the network is able to \u2018focus\u2019 its attention.\n\nLime is a more general framework that aims to make the predictions of \u2018any\u2019 machine learning model more interpretable.\n\nIn order to remain model-independent, LIME works by modifying the input to the model locally. So instead of trying to understand the entire model at the same time, a specific input instance is modified and the impact on the predictions are monitored. In the context of text classification, this means that some of the words are e.g. replaced, to determine which elements of the input impact the predictions.\n\nIf you have any questions on interpretability in machine learning, I\u2019ll be happy to read them in the comments. Follow me on Medium or Twitter if you want to receive updates on my blog posts!"
    },
    {
        "url": "https://towardsdatascience.com/applying-transfer-learning-in-nlp-and-cv-d4aaddd7ca90?source=user_profile---------4----------------",
        "title": "Applying transfer learning in NLP and CV \u2013",
        "text": "In this blog post I will discuss two applications of transfer learning. I will provide an overview of examples in the field of natural language processing and computer vision.\n\nThis blog post is the second in a series on transfer learning. You can find the first blog post, which explains the basic concept of transfer learning, here.\n\nOne of my previous blog posts discussed how a lot of NLP pipelines nowadays use word embeddings. These word embeddings are a more informative way of representing words, compared to one-hot encodings. They are widely used and different variants exist. Typically these variants differ in the corpus they originate from, such as Wikipedia, news articles, etc., and the differences in the embedding models. It is important to understand the background of these models and corpora in order to know whether transfer learning with word embeddings is sensible. People typically wouldn\u2019t call the use of word embeddings transfer learning, yet I disagree, given the similarity with transfer learning in computer vision. Essentially, using word embeddings means that you are using a featuriser or the embedding network to convert words to informative vectors.\n\nEven though word2vec is already 4 years old, it is still a very influential word embedding approach. Recent approaches on the other hand such as FastText have made word embeddings available in a large number of languages. Embeddings from word2vec or FastText are a significant step forwards compared to bag-of-words approaches. Nevertheless, their usefulness is typically determined by the problem domain.\n\nImagine you are building a news recommendation service for sales people. The sales people want to receive news on companies that could be interested in the product they are selling. The vocabulary that is used in news articles is typically fairly generic or general, meaning that a vocabulary is used that is supported by most word embeddings (depending on the corpus they are trained on). In addition, if you have the sales people collect the news articles they read for a couple of weeks, then you immediately have a large labelled corpus at your hands. By being able to reuse word embeddings the recommendation engine might perform significantly better than a simple BoW model.\n\nOn the other hand, imagine that you have to perform topic classification on legal contracts. Not just any type of legal contracts, but French legal contracts in the context of competition law. These type of datasets are typically not labelled, or only a limited set of labeled documents are available. The next section will describe why out-of-the-box transfer learning will only get you so far in this case:\n\nI highly advise you to read this excellent blog post on the current state of word embeddings. Taking into account the problems and solutions mentioned in this blog post is key to creating robust NLP systems and word embeddings when working with limited amounts of data.\n\nGensim, spacy and FastText are three great frameworks that allow you to quickly use word embeddings in your machine learning application. In addition, they also support the training of custom word embeddings. Check out this gensim, this spacy or this FastText tutorial to learn more!\n\nDeep learning methods have led to significant successes in computer vision. Instead of having to define problem specific features manually, for example, Histogram of Oriented Gradients (HoG) features, color features, etc., deep learning allows practitioners to train models that take raw images as input.\n\nThe original complexity of feature definition has now shifted towards the complexity of defining the network. While architectures are often reused, there is no single strategy in composing network architectures. Typically, deep learning techniques have been invented and applied in research settings on enormous datasets (such as Imagenet or MS Coco). To increase performance on these large datasets, researchers have come up with network architectures with increasing depth and complexity. These architectures result in models with millions of parameters that are (typically) not scalable to small image datasets. Training an architecture such as ResNet or VGG net on a dataset with less than 5,000 images will simply lead to significant overfitting. The recent deep learning trend has lead to significant advancements, yet it seems data scientists with only small datasets have been left in the cold.\n\nAs it turns out, deep learning networks learn hierarchical feature representations (see this blog post by Distill). This means that the lower level layers learn low level features, such as edges, whereas the higher level layers learn higher level, yet uninterpretable concepts, such as shapes. This idea of hierarchical feature representations also occurs when the network is trained on different datasets, indicating that they can be reused in different problem domains.\n\nTwo approaches are used when using transfer learning on computer vision problems.\n\nThe API of Keras allows you to load pre-trained networks and keep several of the layers fixed during training. In the next section I will again discuss two use cases, respectively one where transfer learning is useful, and another where it isn\u2019t.\n\nImagine working in wildlife preservation, and you want to classify the different animals that appear on a live camera feed. Especially if you are trying the monitor species that are near extinction, it is possible that you won\u2019t be able to gather a lot of labelled data. Given that the pre-trained networks are often trained on wide domain of concepts (ranging from food, to animals and objects), using a pre-trained network as a featuriser or as initialiser is definitely an option.\n\nOn the other hand, imagine you need to analyse radiography images for oncologists. These images are not your typical cat-dog images, as they are the output of a scan which was performed on a patient. These images, although converted to RGB images, are typically in greyshades to illustrate the results of the scan. Although a pre-trained network is able to detect shapes and edges from RGB images, they will most likely have difficulty detecting those on radiography images as those are not in the training data of the pre-trained model. In addition, in medical scenario\u2019s, the amount of labelled data is typically low. Several techniques exist to leverage (the potentially abundant) unlabelled data, but they typically require more work and finetuning. Typically, these techniques attempt to pre-train the weights of the classification network, by iteratively training each layer to reconstruct the images (using convolutional and deconvolutional layers). A combination of these techniques and pre-trained network is often used to improve convergence.\n\nBoth methods in computer vision mentioned above rely on an important assumption: patterns extracted in the original dataset are useful in the context of the new dataset. This usefulness is hard to quantify, yet it is an important assumption to take into account. Seismic, hyperspectral or even medical imagery shows limited similarity with the images in ImageNet. Determining which traffic sign an image contains however, relies on fairly similar patterns. Understanding the computer vision problem domain is crucial to successfully applying computer vision. By knowing the background of models (datasets, techniques, etc.) that are used in transfer learning, you can avoid wasting time during experimentation and focus on finetuning those models that might make the difference.\n\nIf you have any questions, I\u2019ll be happy to read them in the comments. Follow me on Medium or Twitter if you want to receive updates on my blog posts!"
    },
    {
        "url": "https://towardsdatascience.com/transfer-learning-leveraging-insights-from-large-data-sets-d5435071ec5a?source=user_profile---------5----------------",
        "title": "Transfer learning: leveraging insights from large data sets",
        "text": "In this blog post, you\u2019ll learn what transfer learning is, what some of its applications are and why it is critical skill as a data scientist.\n\nTransfer learning is not a machine learning model or technique; it is rather a \u2018design methodology\u2019 within machine learning. Another type of \u2018design methodology\u2019 is, for example, active learning.\n\nThis blog post is the first in a series on transfer learning. You can find the second blog post, which discusses two applications of transfer learning here.\n\nIn a follow-up blog post I will explain how you can use active learning in conjunction with transfer learning to optimally leverage existing (and new) data. In a broad sense, machine learning applications use transfer learning when they leverage external information to improve the performance or generalisation capabilities.\n\nThe general idea of transfer learning is to use knowledge learned from tasks for which a lot of labelled data is available in settings where only little labelled data is available. Creating labelled data is expensive, so optimally leveraging existing datasets is key.\n\nIn a traditional machine learning model, the primary goal is to generalise to unseen data based on patterns learned from the training data. With transfer learning, you attempt to kickstart this generalisation process by starting from patterns that have been learned for a different task. Essentially, instead of starting the learning process from a (often randomly initialised) blank sheet, you start from patterns that have been learned to solve a different task.\n\nTransfer of knowledge and patterns is possible in a wide variety of domains. Today\u2019s post will illustrate transfer learning by looking at several examples of these different domains. The goal is to incentivise data scientists to experiment with transfer learning in their machine learning projects and to make them aware of the advantages and disadvantages.\n\nThere are three reasons why I believe a good understanding of transfer learning is a critical skill as a data scientist:\n\nTransfer learning, as the name states, requires the ability to transfer knowledge from one domain to another. Transfer learning can be interpreted on a high level. One example is that architectures in NLP can be re-used in sequence prediction problems, since a lot of NLP problems can inherently be reduced to sequence prediction problems. Transfer learning can also be interpreted on a low level, where you are actually reusing parameters from one model in a different model (skip-gram, continuous bag-of-words, etc.). The requirements of transfer learning are on one hand problem specific and on the other one model specific. The next two sections will discuss respectively a high level and low level approach to transfer learning. Although you will typically find these concepts with different names in literature, the overarching concept of transfer learning is still present.\n\nIn multi-task learning, you train a model on different tasks at the same time. Typically, deep learning models are used as they can be adapted flexibly.\n\nThe network architecture is adapted in such a way that the first layers are used across different tasks, followed with different task-specific layers and outputs for the different tasks. The general idea is that by training a network on different tasks, the network will generalise better as the model is required to perform well on tasks for which similar \u2018knowledge\u2019 or \u2018processing\u2019 is required.\n\nAn example in the case of Natural Language Processing is a model for which the end goal is to perform entity recognition. Instead of training the model purely on the entity recognition task, you also use it to perform part of speech classification, next word prediction, \u2026 As such, the model wil benefit from the structure learned from those tasks and the different datasets. I highly recommend Sebastian Ruder\u2019s blogs on multi-task learning to learn more about multi-task learning.\n\nOne of the great advantages of a deep learning model is that feature extraction is \u2018automatic\u2019. Based on the labelled data and backpropagation, the network is able to determine the useful features for a task. The network \u2018figures out\u2019 what part of the input is important in order to, for example, classify an image. This means that the manual job of feature definition is abstracted away. Deep learning networks can be reused in other problems, as the type of features that are extracted, are often useful for other problems as well. Essentialy, in a featuriser you use the first layers of the network to determine the useful feature, but you dont use the output of the network, as it is too task-specific.\n\nGiven that deep learning systems are good at feature extraction, how can you reuse existing networks to perform feature extraction for other tasks? It is possible to feed a data sample into the network, and take one of the intermediate layers in the network as output. This intermediate layer can be interpreted as a fixed length, processed representation of raw data. Typically, the concept of a featuriser is used in the context of computer vision. Images are then fed into a pre-trained network (for example, VGG or AlexNet) and a different machine learning method is used on the new data representation. Extracting an intermediate layer as a representation of the image significantly reduces the original data size, making them more amenable for traditional machine learning techniques (for example, logistic regression or SVMs work better with a small representation of an image, such as dimension 128, compared to the original, for example, 128x128=16384 dimension).\n\nIn the next blog post I will discuss two applications of transfer learning more in depth, both of them with concrete examples!\n\nIf you have any questions, I\u2019ll be happy to read them in the comments. Follow me on Medium or Twitter if you want to receive updates on my blog posts!"
    },
    {
        "url": "https://towardsdatascience.com/gradient-descent-vs-neuroevolution-f907dace010f?source=user_profile---------6----------------",
        "title": "Gradient descent vs. neuroevolution \u2013",
        "text": "In March 2017, OpenAI released a blog post on evolution strategies, an optimisation technique that has been around for several decades. The novelty of their paper was that they managed to apply the technique to deep neural networks in the context of reinforcement learning (RL) problems. Before this, the optimisation of deep learning RL models (with typically millions of parameters) was typically achieved with backpropagation. Using evolution strategies for deep neural network (DNN) optimisation seemingly unlocked an exciting new toolbox for deep learning researchers to play with.\n\nThis week, Uber AI Research released a set of five papers which are all focussed on 'neuroevolution'. The term neuroevolution refer to the optimisation of neural networks through evolutionary algorithms. The researchers posit that genetic algorithms are an effective method to train deep neural networks for reinforcement learning problems and that they outperform traditional RL approaches in some domains.\n\nSo what does this mean? Will all DNN's, ranging from supervised, to unsupervised as well as RL applications, be optimised using neuroevolution in the near future? Is neuroevolution the future of deep learning? What exactly is neuroevolution? In this blog post I will attempt to provide an introduction to neuroevolution, and compare it to the traditional backpropagation algorithm. I will also attempt to answer the aforementioned questions and situate neuroevolution techniques in the bigger DL picture.\n\nFirst I will start by framing the optimisation problem, which is the core problem that backpropagation and neuroevolution try to solve. I will also attempt to make a clear distinction between the difference between supervised learning and reinforcement learning.\n\nNext, I will discuss backpropagation and explain how it relates to neuroevolution. Given that both OpenAI and Uber AI Research just released papers on this technique, I have plenty of topics to tackle. Fortunately, as deep learning neuroevolution is in the early stages of research, the mechanics of neuroevolution still remain fairly comprehensible.\n\nAs discussed in my previous blog post, machine learning models are in essence function approximators. Whether it is classification, regression or reinforcement learning, the end goal is almost always to find a function that maps input data to output data. You use the training data to infer the parameters and hyperparameters and verify with the test data whether the approximated function performs well on unseen data.\n\nThe inputs can be manually defined features or raw data (images, text, etc.) and the outputs are either classes or labels in classification, real values in regression and actions in reinforcement learning. For this blog post we will limit the type of function approximators to deep learning networks, yet the same discussion applies to other models. The parameters that need to be inferred thus correspond to the weights and biases in the network. 'Performing well on train and test data' is expressed through objective measures, such as the log loss for classification, the mean squared error (MSE) for regression and the reward for reinforcement learning.\n\nThe core problem is thus finding the parameter settings that results in the lowest loss or the highest reward. Simple! Given an optimisation objective, i.e. the loss or the reward, that needs to be optimised as a function of the networks parameters the goal is thus to tweak the parameters in such a way that the optimisation objective is minimised or maximised.\n\nIn order to make this visual, let's use two examples. In both cases, we have plotted an optimisation objective. In the image of the parabola, the x-axis represents the single parameter of the model, and the y-axis represents the optimisation objective (e.g. on test data).\n\nIn the image below, the x and y axis represent the two parameters of the model and the z axis represents the optimisation objective (e.g. on the test data).\n\nIn reality, it is not possible to plot the 'optimisation surfaces', due to the sheer number of parameters and how they are non-linearly combined in deep learning networks. Nevertheless, the same ideas apply and the optimisation surface is typically high dimensional and complex, with many hills, cradles, valleys, etc.\n\nThe goal is now to find an optimisation technique that allows us to crawl in the optimisation surface and find the minimum or maximum. Note that the size and shape of these surfaces is related to the number of parameters, and it is infeasible to explore all options, regardless if we are working with continous or discrete parameters. The problem now has become the following: Given a random starting point in the optimisation surface, find the absolute minimum or maximum.\n\nDeep neural networks are great function approximators (universal function approximators even to a certain extent), but they are also hard to optimise. Hard to optimise in this context means that it is very hard to find the global maximum or minimum on the \u2018optimisation surface\u2019. In the next sections I will discuss how gradient descent and neuroevolution can be used to find good solutions.\n\nThe general idea of gradient descent (backpropagation) has been around for several decades. Due to the abundance of data, compute power, and novel innovations, it has become the main technique to optimise the parameters of deep learning models.\n\nThe general idea of gradient descent is the following:\n\n-Assume you are in Paris, France, and you need to get Berlin, Germany. Europe in this case is the optimisation surface, Paris is the random starting point and Berlin is the absolute minimum or maximum. \n\n-As you don't have a map, you ask random strangers the direction to Berlin. Some of the random strangers know where Berlin is but others don't, so while most of the times, you walk in the correct direction, it is possible that you also are going into wrong directions. As long as the strangers are more correct than false, it should work out (e.g. stochastic gradient descent, or gradient descent with minibatches).\n\n-You walk 5 miles (the stepsize or learning rate) in the direction the stranger has pointed you to. This process is repeated until you believe you are close enough to Germany. It might turn out you just entered Germany, and you are nowhere close to Berlin (local optima). There is no way to verify whether you reached your end goal and you can only estimate based on your surroundings (the test loss or the reward).\n\nGoing back to the two visual examples, you can imagine how gradient descent works in the case of the parabola and the more complex surface. Essentially, with gradient descent you are walking down the optimisation surface. In the case of the parabola, it is straightforward, as you just have to walk down the slope. If the learning rate is too high though, you might never be able to get to the absolute minimum.\n\nIn the second example the situation is more complex. There are several hills and valleys you have to overcome in order to get to the absolute minimum. Several gradient descent variants that attempt to mimic physical behaviour such as a ball rolling down the surface with momentum (e.g. ADAM) in order to avoid getting stuck in local optima.\n\nI highly recommend this blog post on gradient descent variants. It clearly explains and illustrates the differences between the different variants and how they solve the different problems that gradient descent faces. There are often several local optima, obstacles and different paths in this optimisation problem, and different variants of gradient descent typically try to tackle some (or all of these) problems. Nowadays, the ADAM optimiser seems to be the most influential one.\n\nEssential to gradient descent is the computation of proper gradients that propel you towards a good solution. In supervised learning, it is possible to obtain 'high quality gradients' with relative ease through the labeled datasets. In reinforcement learning however, you are only given a sparse reward, as the random initial behaviour will not lead to a high reward. In addition this reward only occurs after a couple of actions. While the loss in classification and regression is a relatively good proxy for the function you are trying to approximate, the reward in reinforcement learning is typically not a very good proxy of the behaviour or function you want to learn.\n\nGiven that the gradients in reinforcement learning are not always of good quality, evolution algorithms have recently been used by Uber and OpenAI to improve learning.\n\nNeuroevolution, genetic algorithms, evolution strategies all revolve around the concept of genetic evolution.\n\nWhen you are doing genetic optimisation in the context of DNN optimisation, you start from an initial population of models. Typically, a model is randomly initialised, and several offspring are derived based on this initial model. In the case of DNN\u2019s, you initialise a model (as you normally do), and you add small random vectors, sampled from a simple Gaussian distribution, to the parameters. This results in a cloud of models, which all reside somewhere on the optimisation surface. Note that this is the first important distinction with gradient descent. You start (and continue to work) with a population of models, instead of a single (point) model.\n\nStarting from this original population, the genetic optimisation cycles start. In what follows I will describe genetic optimisation in the context of evolution strategies (ES). Evolution strategies, genetic algorithms, etc. all have slightly different approaches as to how genetic optimisation is performed.\n\nFirst, a fitness evaluation is performed. Fitness evaluation corresponds with checking where the models are in the optimisation surface and determining which of the models perform best (e.g. are the most fit). Some models will perform better than others, just because of the way they have been initialised.\n\nNext, a selection is performed based on the fitness evaluation. In evolution strategies, the (pseudo) offspring is reduced to a single model, weighted by the fitness evaluation. For DNN's the fitness is defined as the loss or the reward. Essentially, you are thus moving around the optimisation surface and using the offspring to get in the right direction. Notice that this is a second major difference with gradient descent. Instead of computing the gradients, you are setting out multiple 'antenna's' and moving in the direction that looks best. In a way, this is similar to a 'structured random' search. The end result of the selection phase is that you have a single model.\n\nNext, reproduction and combination is performed. Concretely, the same process as in the initial phase is repeated. Based on the newly selected 'prime' model, a new set of offspring is derived. The process then continues with this offspring.\n\nTypically, in genetic optimisation, mutation is also performed in order to improve the variety of the offspring. One example of mutation is to change how the different offspring is created (i.e. different noise levels for the different parameters).\n\nOne of the advantages of ES is that the fitness evaluation of the different models in the population can be computed on different cores (out-of-core computation). The only information that needs to be shared after fitness evaluation is their performance (a scalar value) as well as the random seed value that was used to generate the model. As a result, it is not necessary anymore to share the full network with all machines. Both OpenAI and Uber used literally hundreds and thousands of machines in order to do their experiments. With the rise of cloud computing, running these experiments becomes very scalable and is only limited by compute power.\n\nThe image below represent two of the main differences between ES and gradient descent. Multiple models are used to move around, and gradients are not calculated, rather the different models are averaged based on their performance. Optimising over a population instead of over a single model results in higher robustness properties, as shown in the Uber research, and shows similarities to Bayesian approaches to DNN optimisation.\n\nI highly recommend having a look at the great illustrations in the Uber blog post. These illustrate how ES can be used to circumvent some of the problems that gradient descent faces (e.g. getting stuck in local optima). In essence, the evolution strategy performs a gradient approximation. While using the true gradients is better when they are available, this method does show promise when only relatively bad approximations are available for the true gradient and when exploration of the optimisation surface is required, such as in reinforcement learning scenario's.\n\nThe researchers at OpenAI and Uber are able to show that the 'gradient approximation' of evolution strategies leads to satisfying (but not necessarily new state-of-the-art) solutions in supervised learning scenario's. On the other hand they are able to show high performance of their methods in reinforcement learning scenario's (comparable to state-of-the-art in some domains).\n\nWill neuroevolution be the future of deep learning? Probably not, but I do believe it shows great promises for hard optimisation problems, such as in reinforcement learning scenario's. In addition, I believe a combination of neuroevolution and gradient descent methods will lead to a significant improvement in RL performance. One downside of neuroevolution is the massive amounts of compute power that are required in order to train these systems, which might limit the democratisation of these techniques.\n\nGiven that prominent research groups are focussing on these problems, I am still excited to see what the future holds!"
    },
    {
        "url": "https://towardsdatascience.com/lda2vec-word-embeddings-in-topic-models-4ee3fc4b2843?source=user_profile---------7----------------",
        "title": "LDA2vec: Word Embeddings in Topic Models \u2013",
        "text": "Learn more about LDA2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors.\n\nThis blog post will give you an introduction to lda2vec, a topic model published by Chris Moody in 2016. lda2vec expands the word2vec model, described by Mikolov et al. in 2013, with topic and document vectors and incorporates ideas from both word embedding and topic models.\n\nThe general goal of a topic model is to produce interpretable document representations which can be used to discover the topics or structure in a collection of unlabelled documents. An example of such an interpretable document representation is: document X is 20% topic a, 40% topic b and 40% topic c.\n\nToday\u2019s post will start off by introducing Latent Dirichlet Allocation (LDA). LDA is a probabilistic topic model and it treats documents as a bag-of-words, so you\u2019re going to explore the advantages and disadvantages of this approach first.\n\nOn the other hand, lda2vec builds document representations on top of word embeddings. You\u2019ll learn more about word embeddings and why they are currently the preferred building block in natural language processing (NLP) models.\n\nFinally, you\u2019ll learn more about the general idea behind lda2vec.\n\nA topic model takes a collection of unlabelled documents and attempts to find the structure or topics in this collection. Note that topic models often assume that word usage is correlated with topic occurence. You could, for example, provide a topic model with a set of news articles and the topic model will divide the documents in a number of clusters according to word usage.\n\nTopic models are a great way to automatically explore and structure a large set of documents: they group or cluster documents based on the words that occur in them. As documents on similar topics tend to use a similar sub-vocabulary, the resulting clusters of documents can be interpreted as discussing different \u2018topics\u2019.\n\nLatent Dirichlet Allocation (LDA) is an example of a probabilistic topic model. What this exactly means, you\u2019ll learn in the following sections: you\u2019ll first come to understand how LDA starts from a bag-of-words description to represent the different documents. Then, you\u2019ll see how these representations are used to find the structure in the document collection.\n\nTraditionally, text documents are represented in NLP as a bag-of-words.\n\nThis means that each document is represented as a fixed-length vector with length equal to the vocabulary size. Each dimension of this vector corresponds to the count or occurrence of a word in a document. Being able to reduce variable-length documents to fixed-length vectors makes them more amenable for use with a large variety of machine learning (ML) models and tasks (clustering, classification, \u2026).\n\nAlthough the bag-of-words results in a sparse and high-dimensional document representation, good results on topic classification are often obtained if a lot of data is available. You can always read up on the recent Facebook paper on topic classification.\n\nA fixed-length document representation means you can easily feed documents with varying length into ML models (SVM\u2019s, k-NN, Random Forests, \u2026). This allows you to perform clustering or topic classification on documents. The structural information of the document is removed and models have to discover which vector dimensions are semantically similar. Mapping for example \u2018feline\u2019 and \u2018cat\u2019 on different dimensions is less intuitive, as the model is forced to learn the correlation between these different dimensions.\n\nWhen training an LDA model, you start with a collection of documents and each of these is represented by a fixed-length vector (bag-of-words). LDA is a general Machine Learning (ML) technique, which means that it can also be used for other unsupervised ML problems where the input is a collection of fixed-length vectors and the goal is to explore the structure of this data.\n\nTo implement an LDA model, you first start by defining the number of \u2018topics\u2019 that are present in your collection of documents. This sounds straight-forward, but is often less intuitive than it sounds if you are working with vast amounts of documents.\n\nTraining an LDA model on N documents with M topics corresponds with finding the document and topic vectors that best explain the data.\n\nNote that this tutorial will not cover the full theory behind LDA in detail (see this paper by Blei et al. for that), as the focus is more getting the general idea across.\n\nAssume that the vocabulary in the documents consists of V words.\n\nEach of the N documents wil be represented in the LDA model by a vector of length M that details which topics occur in that document. A document can consist of 75% being \u2018topic 1\u2019 and 25% being \u2018topic 2\u2019. Often, LDA results in document vectors with a lot of zeros, which means that there are only a limited number of topics occur per document. This corresponds with the idea that documents typically only talk about a limited number of topics. This significantly improves the human interpretability of these document vectors.\n\nEach of the M topics is represented by a vector of length V that details which words are likely to occur, given a document on that topic. So for topic 1, \u2018learning\u2019, \u2018modelling\u2019 and \u2018statistics\u2019 might be some of the most common words. This means that you could then say that this is the \u2018data science\u2019 topic. For topic 2, the words \u2018GPU\u2019, \u2018compute\u2019 and \u2018storage\u2019 could be the most common words. You could interpret this as the \u2018computing\u2019 topic.\n\nThe following image illustrates the LDA model visually. The goal of the model is to find the topic and document vectors that explain the original bag-of-word representation of the different documents.\n\nIt is important to notice that you are relying on the assumption that the topic vectors will be interpretable, otherwise the output of the model is pretty much garbage. Essentially, you are assuming that the model, given enough data, will figure out which words tend to co-occur, and will cluster them into distinct \u2018topics\u2019.\n\nLDA is a simple probabilistic model that tends to work pretty good. The document vectors are often sparse, low-dimensional and highly interpretable, highlighting the pattern and structure in documents. You have to determine a good estimate of the number of topics that occur in the collection of the documents. In addition, you have to manually assign a distinct nominator \u2018topic\u2019 to the different topic vectors. As a bag-of-words model is used to represent the documents, LDA can suffer from the same disadvantages as the bag-of-words model. The LDA model learns a document vector that predicts words inside of that document while disregarding any structure or how these words interact on a local level.\n\nOne of the problems of the bag-of-words representation is that the model is responsible for figuring out which dimensions in the document vectors are semantically related. One might imagine that leveraging information on how words are semantically correlated to each other will improve a model\u2019s performance and this is exactly what word embeddings promise.\n\nWith word embeddings, words are represented as fixed-length vectors or embeddings. Several different models exist to construct embeddings, but they are all based on the distributional hypothesis. That means that \u201ca word is characterised by the company it keeps\u201d.\n\nThe goal of word embeddings is to capture semantic and syntactic regularities in language from large unsupervised sets of documents, such as Wikipedia. Words that occur in the same context are represented by vectors in close proximity to each other.\n\nThe image above is a projection of the word embedding space to 2D space using t-Distributed Stochastic Neighbor Embedding (t-SNE). t-SNE is a dimensionality reduction method that you can use to visualise high-dimensional data. The method takes the word embeddings as input, and projects them onto two-dimensional space which can be easily visualised in a plot. Only a subsection of the word space is investigated, focussing on words close to \u2018teacher\u2019 . Instead of representing words by uninformative dimensions in a vector, words can be represented by semantically correlated vectors using word embeddings.\n\nWhen using word embeddings, an ML model can leverage information from a large a collection of documents, also known as a \u201ccorpus\u201d, by embedding it in the vector representations. This is not possible with bag-of-words models, which can hurt model performance when not a lot of data is available. Word embeddings lead to document representations that are not fixed-length anymore. Instead, documents are represented by a variable-length sequence of word vector embeddings. While some deep learning techniques, such as Long Short-Term Memory (LSTM)\u2019s, convolutional nets with adaptive pooling, \u2026, are able to deal with variable length sequences, a lot of data is often necessary to properly train them.\n\nAs you read in the introduction, word2vec is highly popular word embedding model, developed by Mikolov et al. Note that several other word embedding models exist within the field of distributional semantics. Although several tricks are required to obtain high-quality word embeddings, this tutorial will only focus on the core idea behind word2vec.\n\nThe following training procedure is used in word2vec to obtain the word embeddings.\n\n1.Select a (pivot) word in the text. The context words of the current pivot word are the words that occur around the pivot word. This means that you\u2019re working within a fixed-length window of words. The combinations of the pivot and context words constitute a set of word-context pairs. The image below was taken from the Chris Moody\u2019s blog on lda2vec. In this text fragment, \u2018awesome\u2019 is the pivot word and the words around it are taken as context words, resulting in 7 word-context pairs.\n\n2.Two variants of the word2vec model exist: a. In the bag-of-words architecture (CBOW) the pivot word is predicted based on a set of surrounding context words (i.e. given \u2018thank\u2019, \u2018such\u2019, \u2018you\u2019, \u2018top\u2019 the model has to predict \u2018awesome\u2019). This is called a bag-of-words architecture as the order of context words does not matter. b. In the skip-gram architecture, the pivot word is used to predict the surrounding context words (i.e. given \u2018awesome\u2019 predict \u2018thank\u2019, \u2018such\u2019, \u2018you\u2019, \u2018top\u2019 ). The following image depicts the two different word2vec architectures. Note that a relatively simple (two layer) neural model is used (compared to deep neural models in computer vision).\n\nBy training the model on a large corpus, you will obtain word embeddings (the weights in the projection layer) that encode semantical information as well as some interesting properties: it is possible to perform vector arithmetic, such as king - man+woman=queen .\n\nWord vectors are a useful representation compared to, for example, a simple one-hot encoded representation. They allow to encode statistical information from a large corpus into other models, such as topic classification or dialogue systems. The word vectors are often dense, high-dimensional and uninterpretable. Consider the following example: [ -0.65, -1.223, \u2026, -0.252, +3.2]. While in LDA dimensions correspond approximately to topics, this is typically not the case for word vectors. Each word is assigned a context-independent word vector. The semantic meaning of words is, however, highly dependent on context. The word2vec model learns a word vector that predicts context words across different documents. As a result, document-specific information is mixed together in the word embeddings.\n\nInspired by Latent Dirichlet Allocation (LDA), the word2vec model is expanded to simultaneously learn word, document and topic vectors.\n\nLda2vec is obtained by modifying the skip-gram word2vec variant. In the original skip-gram method, the model is trained to predict context words based on a pivot word. In lda2vec, the pivot word vector and a document vector are added to obtain a context vector. This context vector is then used to predict context words.\n\nIn the next section, you will see how these document vectors are constructed and how they can be used similarly as document vectors in LDA.\n\nThe idea of integrating context vectors in the word2vec model is not a new idea. Paragraph vectors, for example, also explored this idea in order to learn fixed-length representations of variable-length text fragments. In their work, for each text fragment (size of a paragraph) a dense vector representation is learned, similar to the learned word vectors.\n\nThe downside of this approach is that the context/paragraph vectors resemble typical word vectors, making them less interpretable as, for example, the output of LDA.\n\nThe lda2vec model goes one step beyond the paragraph vector approach by working with document-sized text fragments and decomposing the document vectors into two different components. In the same spirit as the LDA model, a document vector is decomposed into a document weight vector and a topic matrix. The document weight vector represents the percentage of the different topics, whereas the topic matrix consists of the different topic vectors. A context vector is thus constructed by combining the different topic vectors that occur in a document.\n\nConsider the following example: in the original word2vec model, if the pivot word is \u2018French\u2019, then possible context words might be \u2018German\u2019, \u2018Dutch\u2019, \u2018English\u2019. Without any global (document-related) information, these would be the most plausible guesses.\n\nBy providing an additional context vector in the lda2vec model, it is possible to make better guesses of context words.\n\nIf the document vector is a combination of the \u2018food\u2019 and \u2018drinks\u2019 topics, then \u2018baguette\u2019, \u2018cheese\u2019 and \u2018wine\u2019 might be more suitable. If the document vector is similar to the \u2018city\u2019 and \u2018geography\u2019 topics, then \u2018Paris\u2019, \u2018Lyon\u2019 and \u2018Grenoble\u2019 might be more suitable.\n\nNote that these topic vectors are learned in word space, which allows for easy interpretation: you simply look at the word vectors that are closest to the topic vectors. In addition, constraints are put on the document weight vectors, to obtain a sparse vector (similar to LDA), instead of a dense vector. This enables easy interpretation of the topic content of different documents.\n\nIn short, the end-result of the lda2vec is a set of sparse document weight vectors, as well as easily interpretable topic vectors.\n\nAlthough the performance tends to be similar to traditional LDA, using automatic differentiation methods makes the method scalable to very vast datasets. In addition, by combining the context vector and word vector, you obtain \u2018specialised\u2019 word vectors, which can be used in other models (and might outperform more \u2018generic\u2019 word vectors).\n\nLda2vec is a fairly new and specialised NLP technique. As it builds on existing methods, any word2vec implementation could be extended into lda2vec. Chris Moody implemented the method in Chainer, but other automatic differentiation frameworks could also be used (CNTK, Theano, \u2026). A Tensorflow implementation was also made publicly available.\n\nAn overview of the lda2vec Python module can be found here. As training lda2vec can be computationally intensive, GPU support is recommended for larger corpora. In addition, in order to speed up training, the different word vectors are often initialised with pre-trained word2vec vectors.\n\nFinally, lda2vec was discussed as a topic model, but the idea of adding context vectors to the word2vec model can also be defined more generally. Consider for example documents written by different authors from different regions. Then author and region vectors could also be added to the context vector, resulting in an unsupervised method to obtain document, region and author vector representations.\n\nThis blog post only provided a quick overview of LDA, word2vec and lda2vec. Note that the original author also published a great blog post on the technical details of lda2vec."
    },
    {
        "url": "https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138?source=user_profile---------8----------------",
        "title": "Understanding objective functions in neural networks.",
        "text": "In supervised machine learning problems, we often consider a dataset D of observation pairs (x, y) and we try to model the following distribution:\n\nFor example in image classification, x represents an image and y the corresponding image label. p(y|x, \u03b8) represents the probability of the label y given the image x and a model defined by parameters \u03b8.\n\nModels that follow this approach are called discriminative models. In discriminative or conditional models the parameters that define the conditional probability distribution function p(y|x, \u03b8) are inferred from the training data.\n\nBased on the observed data x (input data or feature values) the model outputs a probability distribution, which is then used to predict y (class or real value). Different machine learning models require different parameters to be estimated. Both linear models (e.g. logistic regression, defined by a set of weights equal to the number of features) and non-linear models (e.g. neural networks, defined by a set of weights for each layer) can be used to approximate the conditional probability distributions.\n\nFor typical classification problems the set of learnable parameters \u03b8 is used to define a mapping from x to a categorical distribution over the different labels. A discriminative classification model produces N probabilities as output, with N equal to the number of classes. Each x belongs to a single class, but model uncertainty is reflected by outputting a distribution over the classes. Typically, the class with maximum probability is chosen when making a decision.\n\nNote that discriminative regression models often only output a single predicted value, instead of a distribution over all the real values. This is different from discriminative classification models where a distribution over all the possible classes is provided. Does this mean discriminative models fall apart for regression? Shouldn\u2019t the output of the model tell us which regression values are more likely than others?\n\nAlthough the single output of a discriminative regression model is misleading, the output of a regression model actually relates to a well-known probability distribution, the Gaussian distribution. As it turns out, the output of a discriminative regression model represents the mean of a Gaussian distribution (a Gaussian distribution is fully defined by a mean and a standard deviation). With this information, you can determine the likelihood of each real value given the input x.\n\nOnly the mean value of this distribution is typically modelled, and the standard deviation of the Gaussian is either not modelled or chosen to be constant across all x. In discriminative regression models, \u03b8 thus defines a mapping from x to the mean of a Gaussian from which y is sampled. The mean value is almost always chosen when making a decision. Models that output a mean and a standard deviation for a given x are more informative, as the model is able to express for which x it is uncertain (by increasing the standard deviation).\n\nOther probabilistic models (such as Gaussian processes) do a significantly better job at modelling uncertainty in regression problems, whereas discriminative regression models tend to be overconfident when modelling mean and standard deviation at the same time.\n\nA Gaussian process is able to quantify uncertainty through explicitely modelling the standard deviation. The only downside of Gaussian processes is that they do not scale well to large datasets. In the image below you can see that the GP model has small confidence intervals (determined with the standard deviation) around regions with a lot of data. In regions with few data points, the confidence intervals become significantly larger.\n\nA discriminative model is trained on the training dataset in order to learn the properties in the data that represent a class or real value. A model performs well if it is able to assign high probability to the correct class of samples or a mean that is close to the true value in the test dataset."
    }
]