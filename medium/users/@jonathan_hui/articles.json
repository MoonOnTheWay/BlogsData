[
    {
        "url": "https://medium.com/@jonathan_hui/how-deep-learning-fakes-videos-deepfakes-and-how-to-detect-it-c0b50fbf7cb9?source=user_profile---------1----------------",
        "title": "How deep learning fakes videos (Deepfakes) and how to detect it?",
        "text": "How deep learning fakes videos (Deepfakes) and how to detect it? Fabrication of celebrity porn pics is nothing new. However, in late 2017, a user on Reddit named Deepfakes started applying deep learning to fabricate fake videos of celebrities. That starts a new wave of fake videos online. Actually, applying AI to create videos started way before Deepfakes. Face2Face and UW\u2019s \u201csynthesizing Obama (learning lip sync from audio)\u201d create fake videos that are even harder to detect. In fact, they are so real that Jordan Peele created one below to warn the public. In this article, we explain the concept of the Deepfakes. We locate some of the difficulties and explain ways to identify the fake videos. We also look into a research at University of Washington in creating videos that can lip sync with a potential fake audio. The concept of Deepfakes is very simple. Let\u2019s say we want to transfer the face of a person A to a video of person B. First, we collect hundreds or thousands of pictures for both persons. We build an encoder to encode all these pictures using a deep learning CNN network. Then we use a decoder to reconstruct the image. This autoencoder (the encoder and the decoder) has over million parameters but is not even close enough to remember all the pictures. So the encoder needs to extract the most important features to recreate the original input. Think about it as a crime sketch. The features are the descriptions from a witness (encoder) and a composite sketch artist (decoder) uses them to reconstruct a picture of the suspect. To decode the features, we use separate decoders for person A and person B. Now, we train the encoder and the decoders (using backpropagation) such that the input will match closely with the output. This process is time-consuming. With a GPU graphic card, it takes about 3 days to generate decent results. (after repeat processing images for about 10+ million times) After the training, we process the video frame-by-frame to swap a person face with another. Using face detection, we extract the face of person A out and feed it into the encoder. However, instead of feeding to its original decoder, we use the decoder of the person B to reconstruct the picture. i.e. we draw person B with the features of A in the original video. Then we merge the new created face into the original image.\n\nBefore the training, we need to prepare thousands of images for both persons. We can take a shortcut and use face detection library to scrape facial pictures from their videos. Spend significant time to improve the quality of your facial pictures. It impacts your final result significantly. Remove any picture frames that contain more than one person. Make sure your have an abundance of video footage. Extract facial pictures contains different pose, face angle and facial expressions. Some resembling of both persons may help, like similar face shape. We don\u2019t want our autoencoder to simply remember the training input and replicate the output directly. Remember all possibilities is not feasible. We introduce denoising to introduce data variants and to train autoencoder to learn smartly. The term denoising may be misleading. The main concept is to distort some information but we expect the autoencoder smartly ignore this minor abnormality and recreate the original. i.e. let\u2019s remember what is important and ignore the un-necessary variants. By repeating the training many times, the information noise will cancel each other and eventually forgotten. What is left is the real patterns that we care. In our facial picture, we select 5 \u00d7 5 grid points and shift them slightly away from their original positions. We use a simple algorithm to warp the image according to those shifted grid points. Even the warped image may not look exactly right, but that is the noise that we want to introduce. Then we use a more complex algorithm to construct a target image using the shifted grid points. We want our created images to look as close as the target images. It seems odd but that forces the autoencoder to learn the most important features.\n\nTo handle different pose, facial angles and locations better, we also apply image augmentation to enrich the training data. During training, we rotate, zoom, translate and flip our facial image randomly within a specific range. Let\u2019s take a short break to illustrate how the autoencoder may look like. (Some basic knowledge of CNN is needed here.) The encoder composes of 5 convolution layers to extract features followed by 2 dense layers. Then it uses a convolution layer to upsampling the image. The decoder continues the upsampling with 4 more convolution layers until it reconstructs the 64 \u00d7 64 image back. To upsample the spatial dimension say from 16 \u00d7 16 to 32 \u00d7 32, we use a convolution filter (a 3 \u00d7 3 \u00d7 256 \u00d7 512 filter) to map the (16, 16, 256) layer into (16, 16, 512). Then we reshape it to (32, 32, 128). Don\u2019t get too excited. If you use a bad implementation, a bad configuration or your model is not properly trained, you will get the result of the following video instead. (Check out the first few seconds. I have marked the video around 3:37 already.) The facial area is flicking, blur with bleeding color. And there are obvious boxes around the face. It looks like people pasting pictures onto his face by brute force. These problems are easily understood if we explain how to swap face manually. We start with two pictures (1 and 2) for 2 women. In picture 4, we try to paste the face 1 onto 2. We realize that their face is very different and the face cutout (the red rectangle) is way too big. It just looks like someone put a paper mask on her. Now, let\u2019s try to paste face 2 onto 1 instead. In picture 3, we use a smaller cutout. We create a mask that removes some of the corner areas so the cutout can blend in better. It is not great but definitely better than 4. But there is a sudden change in skin tone around the boundary area. In picture 5, we reduce the opacity of the mask around the boundary so the created face can blend in better. But the color tone and the brightness of the cutout still does not match the target. So in picture 6, we adjust the color tone and the brightness of the cutout to match our target. It is not good enough yet but not bad for our tiny effort. In Deepfakes, it creates a mask on the created face so it can blend in with the target video. To further eliminate the artifacts, we can apply a Gaussian filter to further diffuse the mask boundary area, configure the application to expand or contract the mask further, or control the shape of the mask. If you look closer to a fake video, you may notice double chins or ghost edges around the face. That is the side effect of merging 2 images together using a mask. Even the mask improves the quality, there is a price to pay. In particular, most fake videos I see, the face is a little bit bury comparing with other parts of the image. To counterbalance it, we can configure Deepfakes to apply sharpen filter to the created face before the blending. This is a trial and error process to find the right balance between artifacts and sharpness. Obviously, most of the time, we need to create slightly blur images to remove noticeable artifacts. Even the autoencoder should create faces to match the target color tone, sometimes it needs help. Deepfakes provides post processing to adjust the color tone, contrast and brightness of the created face to match the target video. We can also apply the cv2 seamless cloning to blend the created image with the target image using automatic tone adjustment. However, some of these efforts can be counter productive. We can make a particular frame looks great. But if we overdo it, it may hurt the temporal smoothness across frames. Indeed, the seamless clone in Deepfakes is a major possible cause of flicking. So people often turn seamless off to see if the flicking can be reduced. Another major source of flicking is the autoencoder fails to create proper faces. For that, we need to add more diversify images to train the model better or increase the data augmentation. Eventually, we may need to train the model longer. In cases where we cannot create the proper face for some video frames, we skip the problem frames and use interpolation to recreate the deleted frames. We can also warp our created face according to the face landmarks in the original target frame. This is how Rogue One warp the younger Princess Leia face onto another actress. In our previous effort, our mask is pre-configured. We can do a much better job if our mask is related to the input image and the created face. In GAN, we introduce a deep network discriminator (a CNN classifier) to distinguish whether facial images are original or created by the computer. When we feed real images to this discriminator, we train the discriminator itself to recognize real images better. When we feed created images into the discriminator, we use it to train our autoencoder to create more realistic images. We turn this into a race that eventually the created images are not distinguishable from the real ones. In additional, our decoder generates images as well as masks. Since these masks are learned from the training data, it can mask the image better and create a smoother transition to the target image. Also, it handles partial obstructed face better. In may fake videos, when the face is partially blocked by a hand, the video may flick or turn bury. With a better mask, we can mask out the obstructed area in the created face and use the part in the target image instead.\n\nEven though GAN is powerful, it takes very long to train and require higher level of expertise to make it right. Therefore, it is not as popular as it should be. For those super enthusiasts, you may try out different loss functions. The LSGAN and WGAN are particular popular. Here is a list of GAN with the corresponding cost function used. Besides the reconstruction cost, GAN adds generator and discriminator cost to train the model. Indeed, we can add addition loss functions to perfect our model. One common one is the edge cost which measures whether the target image and the created image has the same edge at the same location. Some people also look into the perceptual loss. The reconstruction cost measures the pixel difference between the target image and the created image. However, this may not be a good metric in measuring how our brains perceive objects. Therefore, some people may use perception loss to replace the original reconstruction loss. This is pretty advance so I will let those enthusiasts to read the paper in the reference section instead. You can further analyze where your fake videos perform badly and introduce a new cost function to address the problem. Let me pick some of the good Deepfakes videos and see whether you can detect them now. Play it in slow motion and pay special attention to: Does it over blur comparing with other non-facial areas of the video? Does it have a change of skin tone near the edge of the face? Does it have double chin, double eyebrows, double edges on the face? When the face is partially blocked by hands or other things, does it flick or get blurry? In making fake videos, we apply different loss functions to make more visual pleasant videos. As shown in the Trump fake pictures, the features of his face look close to the real one but it does change if you look closer. Hence, in my opinion, if we feed the target video into a classifier for identification, there is a good chance that it will fail. In addition, we can write programs to verify the temporal smoothness. Since we create faces independently across frames, we should expect the transition to be less smooth compared to a real video. The video made by Jordan Peele is one of the toughest one to be identified as fake. But once you look closer, the lower lip of Obama is more blurry comparing with other parts of the face. Therefore, instead of swapping out the face, I suspect this is a real Obama video but the mouth is fabricated to lip sync with a fake audio. For the remaining of this section, we will discuss the lip sync technology done at the University of Washington (UW). Below is the workflow of the lip sync paper. It substitutes the audio of a weekly presidential address with another audio (input audio). In the process, we re-synthesis the mouth and the chin area so its movement is in-sync with the fake audio. First, using an LSTM network, the audio x is transformed to a sequence of 18 landmark points y in the lip. This LSTM outputs a sparse mouth shape for each output video frame. Given the mouth shape y, it synthesizes mouth texture for the mouth and the chin area. These mouth textures are then compose with the target video to recreate the target frame: So how do we create the mouth texture? We want it to look real but also have a temporal smoothness. So the application looks over the target videos to search for candidates frames that have the same calculated mouth shape as what we want. Then we merge the candidates together using a median function. As shown below, if we use more candidate frames to do the averaging, the image gets blurred while the temporal smoothness improves (no flicking). On the other hand, the image can be less bury but we may see flicking when transiting from one frame to another. To compensate the blurry , teeth enhancement and sharpening is performed. But obviously, the sharpness cannot be completely restored for the lower lip. Finally, we need to retime the frame so we know where to insert the fake mouth texture. This helps us to sync with the head movement. In particular, Obama head usually stops moving when he pauses his speech. The top row below is the original video frames for the input audio we used. We insert this input audio to our target video (the second row). When compare it side-by-side, we realize the mouth movement from the original video is very close to the fabricated mouth movement. UW uses existing frames to create the mouth texture. Instead, we can use the Deepfakes concept to generate the mouth texture directly from the autoencoder. We need to collect thousand frames and use the LSTM to extract the features from both the video and the audio. Then we can train a decoder to generate the mouth texture. It is particular interesting to see how we apply AI concepts to create new ideas and new products, but not without a warning! The social impacts can be huge. In fact, do not publish any fake videos for fun! It can get you into legal troubles and hurt your online reputation. I look into this topic because of my interest in meta-learning and adversary detections. Better use your energy for things that are more innovative. On the other hand, the fake video will stay and be improved. It is not my purpose to make better fake videos. Through this process, I hope we know how to apply GAN better to reconstruct image. Maybe one day, this may eventually helpful in detecting tumors. As another precaution, be careful on the Apps that you download to create Deepfakes videos. There are reports that some Apps hijack computers to mine cryptocurrency. Just be careful."
    },
    {
        "url": "https://medium.com/@jonathan_hui/meta-learning-how-we-address-the-shortcomings-of-our-deep-networks-a008aa4b5b2b?source=user_profile---------2----------------",
        "title": "Meta-learning: how we address the shortcomings of our deep networks?",
        "text": "Many deep learning classifiers demonstrate superhuman performance but human still learns far more efficient than deep networks. In this article, we will first look into the problems and then check out some of the proposed solutions. Mainly, we will look into recurrent models, metric learning, and meta-optimization.\n\nWhat is the problem?\n\nComparing with human, there are two critical weaknesses for most state-of-the-art deep learning methods:\n\nLet\u2019s introduce the concept of few-shot learning. In few-shot learning, we train a model with different varieties of tasks. However, for each task, we will provide a few samples only. (i.e. less data but more versatility.) This forces us not to memorize the sample data. Otherwise, we cannot handle new tasks or un-seen samples correctly. Our ultimate learning method should generalize knowledge learned from other tasks and utilize it for new ones. For example, in few-shot classification, we train our model to handle different types of tasks. In task 1, we are given 3 samples to learn emoji. Then, we give a new testing input to see how well we classify it, and backpropagate the loss to update the model parameters.\n\nIn our second task, we are given 3 samples again to learn alphabets.\n\nWe repeat the process many times with different tasks. Once the training is done, we measure the model versatility by testing it with a task that we have not performed before, say recognizing Chinese characters.\n\nYou may wonder what is the difference between few-shot training and our traditional DL training using a large dataset. In DL, we use regularization to make sure we are not overfitting our model. But by training the model with so many samples and iterations, we are overfitting our tasks. What we learned cannot be generalized to other tasks.\n\nIn current DL approaches, we provide many samples to learn data variants. But in inference, we get stuck when we process samples that is not common (or not seen) in the dataset. In particular, for the toy category, there are way too many variants than a DL model can learn. In few-shot training, we train the model with many high level tasks but few data variants. This forces us to learn what is important.\n\nIn summary, our training (meta-training) contains many different datasets with very few samples. In one extreme, we may have 200 categories but we only provide one sample for each category. This type of training is called one-shot training.\n\nIn each dataset, we use the corresponding testing data to backpropagate the loss to train the model parameters.\n\nSo what is meta-learning? We realize the problem of our current approaches and make significant advances. But we are still exploring. So meta-learning is still a collective term on different approaches. Next, we will introduce some of them and get some intuitions of how researchers approach this problem.\n\nIn a recurrent model, we feed data into a RNN-like model to remember what we see so far. When we are present with a testing input, we recall from memory to tell what it is. However, we can\u2019t remember every pixels we see, so the recurrent model only store features and we use linear algebra similar to word embedding to correlate information.\n\nLet\u2019s do a recap in a memory network (MN) first. MN uses a controller to extract features kt from the input xt. Then we use the features to access memory.\n\nFor example, you take a phone call but you cannot recognize the voice immediately. The voice sounds a whole lot like your cousin (0.7 chance) but it also resembles the voice of your elder brother (0.3 chance). In the diagram above, each row represents an object. We compute a weight w for each row to measure its relevancy with the input xt. Then we computed a weighted sum from all rows. This is our recalled memory for our input xt. In this example, the reconstructed information may turn out to be your high school classmate even though the voice is not what you remember exactly. (If you want more details, take a quick look at Neural Turing Machine later.)\n\nMemory-Augmented Neural Networks is one of the meta-learning methods using an external memory network with RNN. Recall that an RNN model uses a sample input xt with its label yt at each time sequence t. However, in this model, the label yt is not provided until the next timestamp t+1. This is a technique to discourage the RNN cell to map the input xt to a class label yt directly.\n\nAfter each epoch, we will reshuffle the samples again and redo the training.\n\nAt time step 0, a controller compute the feature k0 of the input x0 (step (1) below), it uses this information to update the state of the controller. In the next time step, the controller combines both the label y0 and the previous controller state, and write this information into an external memory using least recently used algorithm (step (2)). We repeat the process until we reach the end of the time sequence. At the end, we compute feature kt of the input and use it to access the external memory (step(3)). The memory returned (step(4)) will be feed into a much simple classifier to make class prediction. Then we backpropagate (step(5)) the loss in our classification to train the whole model.\n\nIn CNN, we optimize a model based on the current batch of input xi and yi only. In Memory-Augmented Neural Networks, we remember the features of our previous inputs in the external memory. In making predictions, we cross-reference features kt with our memories. It associates our input to different memory entries. The recalled memory (the weighted sum of memory objects) is our combined knowledge learned from different tasks. For example, our testing image below may not visually resemble any objects learned in other tasks. But by computing a weighted sum, our recalled memory combines our previous knowledge and makes a better assessment of the testing image."
    },
    {
        "url": "https://medium.com/@jonathan_hui/index-page-for-my-articles-in-deep-learning-19821810a14?source=user_profile---------3----------------",
        "title": "A listing of my articles in deep learning \u2013 Jonathan Hui \u2013",
        "text": "One clap, two clap, three clap, forty?\n\nBy clapping more or less, you can signal to us which stories really stand out."
    },
    {
        "url": "https://medium.com/@jonathan_hui/image-segmentation-with-mask-r-cnn-ebe6d793272?source=user_profile---------4----------------",
        "title": "Image segmentation with Mask R-CNN \u2013 Jonathan Hui \u2013",
        "text": "In a previous article , we discuss the use of region based object detector like Faster R-CNN to detect objects. Instead of creating a boundary box, image segmentation groups pixels that belong to the same object. In this article, we will discuss how easy to perform image segmentation with high accuracy that mostly build on top of Faster R-CNN.\n\nThe Faster R-CNN builds all the ground works for feature extractions and ROI proposals. At first sight, performing image segmentation may require more detail analysis to colorize the image segments. By surprise, not only we can piggyback on this model, the extra work required is pretty simple. After the ROI pooling, we add 2 more convolution layers to build the mask.\n\nIf you need further introduction, please refer to this article .\n\nFaster R-CNN uses a CNN feature extractor to extract image features. Then it uses a CNN region proposal network to create region of interests (RoIs). We apply RoI pooling to warp them into fixed dimension. It is then feed into fully connected layers to make classification and boundary box prediction.\n\nThe Mask R-CNN paper provides one more variant (on the right) in building such mask. But the idea is pretty simple.\n\nAnother major contribution of Mask R-CNN is the refinement of the ROI pooling. In ROI, the warping is digitalized (top left) as well as the warping (bottom left): the cell boundaries of the target feature map are forced to realign with the boundary of the input feature maps. Therefore, each target cells may not be in the same size. Mask R-CNN uses ROI Align which does not digitalize the boundary of the cells (top right) and make every target cell to have the same size (bottom right). It also applies interpolation to calculate the feature map values within the cell better. For example, by applying interpolation, the maximum feature value on the top left is changed from 0.8 to 0.88 now.\n\nLet\u2019s visualize some of the major steps in Mask R-CNN/Faster R-CNN. Using the region proposal network, we make ROI proposals. The dotted rectangles below are those proposals but, for demonstration purpose, we decide to display those that have high final scores only.\n\nHere are the boxes after boundary box refinements when we make final classification and localization predictions. The boundary box encloses the ground truth objects better.\n\nJust like Faster R-CNN, it performs object classification based on the ROIs (dotted lines) from RPN. The solid line is the boundary box refinements in the final predictions.\n\nIt groups highly-overlapped boxes for the same class and selects the most confidence prediction only. This avoids duplicates for the same object.\n\nHere are our top final classification and boundary box predictions from the Faster R-CNN portion.\n\nHere are the input picture and some of the feature maps used by the RPN. The first feature map shows high activations on where the cars line up."
    },
    {
        "url": "https://medium.com/@jonathan_hui/neural-turing-machines-a-fundamental-approach-to-access-memory-in-deep-learning-b823a31fe91d?source=user_profile---------5----------------",
        "title": "Neural Turing Machines: a fundamental approach to access memory in deep learning",
        "text": "Memory is a crucial part of the brain and the computer. In some areas of deep learning, we extend the capabilities of deep networks by coupling them to memory. For example, in question and answer, we memorize information that we have processed and use them to answer questions. From the Neural Turing Machine (NTM) paper:\n\nIn layman terms, we create a memory structure, typically an array, and we read and write from it. Sound simple: not exactly. First, we do not have an unlimited memory capacity to hold all images or voices we encounter, and we access information by similarity or relevancy (not exact match). In this article, we discuss how we apply NTM to access information. We are interested in this paper because it is an important starting point in many research areas including NLP and meta-learning.\n\nOur memory structure Mt contains N rows each with M elements. Each row represents a piece of information (memory), for example, how you may picture your cousin.\n\nIn conventional programming, we access memory by index Mt[i]. But for AI, we access information by similarity. So we derive a reading mechanism using weight. i.e. our result is a weighted sum of our memory.\n\nwhere the sum of all weights equals to one.\n\nYou may immediately ask what purpose does it serve. Let\u2019s go through an example. A friend hands you a drink. It tastes very similar to tea and feels creamy like milk. By extracting our memory profile on tea and milk, we apply linear algebra to conclude that it may be a boba tea. Sound like magic. But in word embedding, we use the same kind of linear algebra to manipulate relationships. In other examples like question and answer, it is important to merge information based on accumulated knowledge. A memory network will serve our purpose well.\n\nSo how do we create those weights? Of course, it is by deep learning. A controller extracts features (kt) from input and we use it to compute the weights. For example, you take a phone call but you cannot recognize the voice immediately. The voice sounds a whole lot like your cousin but it also resembles the voice of your elder brother. Through linear algebra, we may recognize that it is your high school classmate even though the voice is not what you remember exactly.\n\nTo compute the weight w, we measure the similarity between kt and each of our memory entry. We calculate a score K using cosine similarity.\n\nHere, u is our extracted feature kt, and v is each individual rows in our memory.\n\nWe apply a softmax function on the score K to compute the weight w. \u03b2t is added to amplify or attenuate the difference in scores. For example, if it is greater than one, it amplifies the difference. w retrieves information based on similarity and we call this content addressing.\n\nSo how we write information into memory. In LSTM, the internal state of a cell is a combination of the previous state and a new input state. Borrow from the same intuition, the memory writing process composes of previous state and new input. Here, we erase part of the previous state:\n\nwhere et is an erase vector. (calculated just like the input gate in LSTM)\n\nThen, we write our new information.\n\nwhere at is what we want to add.\n\nHere, through a controller that generates w, we read and write from our memory.\n\nOur controller computes w to extract information. But extraction by similarity (content addressing) is not powerful enough.\n\nw represents what is our current focus (attention) in our memory. In content addressing, our focus is only based on the new input. However, this does not account for our recent encounter. For example, if your classmate texts you an hour ago, you should recall his voice easier. How do we accomplish previous attention in extracting information? We compute a new merged weight based on the current content focus as well as our previous focus. Yes, this sounds like the forget gate in LSTM or GRU.\n\nwhere g is computed from the previous focus and our current input.\n\nConvolution shift handles a shift of focus. It is not specifically designed for deep learning. Instead, it shows how a NTM can perform basic algorithms like copying and sorting. For example, instead of accessing w[4], we want to shift every focus by 3 rows. i.e. w[i] \u2190 w[i+3].\n\nIn convolution shift, we can shift our focus to a range of rows, i.e. w[i] \u2190 convolution(w[i+3], w[i+4], w[i+5]). Usually, the convolution is just a linear weighted sum of rows 0.3 \u00d7 w[i+3] + 0.5 \u00d7 w[i+4] + 0.2 \u00d7 w[i+5].\n\nThis is the mathematical formulation to shift our focus:\n\nIn many deep learning model, we skip this step or set s(i) to 0 except s(0) = 1.\n\nOur convolution shift behaves like a convolutional blurring filter. So we apply sharpening technique to our weights to counter play the blurring if needed. \u03b3 will be another parameter output by the controller to sharpen our focus.\n\nWe retrieve information from our memory using the weight w. w includes factors like our current input, previous focus, possible shifting and blurring. Here is the system diagram which a controller outputs the necessary parameters to be used in calculating w at different stages."
    },
    {
        "url": "https://medium.com/@jonathan_hui/self-driving-car-path-planning-to-maneuver-the-traffic-ac63f5a620e2?source=user_profile---------6----------------",
        "title": "Self-driving car: Path planning to maneuver the traffic.",
        "text": "Alternatively, we use deep learning to locate an object and to compute its trajectory or to classify its intention. For example, in the image below, we detect a biker is signaling a left turn.\n\nAt each time-step, we monitor its motion predicted by its dynamic model, and estimate what is the likelihood of those possible trajectories.\n\nFor a model-based method, we start with the dynamic model of an object.\n\nIn an urban environment, we need to track far more objects including pedestrians, bikers, carts, scooters, etc\u2026 Sometimes, the hand signal of bikers and the posture of pedestrians are critical information in determining their intentions. The TED talk below lays down some fine details in such process.\n\nHowever, just knowing the location or velocity of objects around us is not good enough. We also make predictions on our trajectory and others to avoid collisions. If the trajectory of the self-driving car gets dangerously close to any other trajectories, we should increase the trajectory cost dramatically so we will not pick this choice.\n\nUsing sensor fusion , we reconstruct a consolidated and more accurate picture of the location and the velocity of objects.\n\nHowever, it is not easy to determine depth with stereo cameras. LiDAR sends out about 2000 pulses of near-infrared laser beam per second to reconstruct a 3-D world around us. RADAR, on the other hand, has longer range than LiDAR but lower resolution. It also has the capability to measure velocity.\n\nA self-driving car uses stereo cameras, LiDAR and Radar to perceive our surroundings. The video below shows the use of cameras to perceive objects around us so we know what actions are feasible and plan a path accordingly.\n\nControl : Then we send the new trajectory to the car controller to control the actuators of the vehicle.\n\nTrajectory cost ranking : For each trajectory, we analyze its feasibility as well as the cost based on safety, comfort, and efficiency. We select the final trajectory with the highest score.\n\nBehavior planning : We look at different options and create a handful of proposals from our options. We eventually translate each proposal into a location.\n\nPrediction : We use the sensor information to predict the trajectories of our surrounding objects. So we avoid any collision.\n\nPerception : We perceive what is around us. Through sensor fusion, we integrate sensor readings to construct a precise picture of the position and the velocity of our surrounding objects.\n\nPath planning is about finding a safe, comfortable and efficient path to maneuver around the traffic. The following video demonstrates the idea with green dots being our calculated trajectory.\n\nIn conjunction with a map, a planned route to a destination and predictions, the behavior planner make suggestions on how to maneuver a car.\n\nWe are provided with many options:\n\nStart with different high-level objectives, like following the traffic, passing slow-moving cars, or speeding up to the speed limit. We look at different options on how to make it happens. For example, switch to the left lane and accelerate to 55 mph, or slow down to maintain a safe distance from the car in front. For each plan, we determine which lane to use and our target speed at the end of our evaluation. Then we calculate the location accordingly. Below, we come up with a list of locations (yellow dots) that we may want our car to be in say 2 seconds.\n\nOnce we have a list of candidate locations, we compute the trajectory for each proposal. But before that, we first take a look at the coordinate system.\n\nLocalization and route planning use a global Cartesian system to define a location (x, y).\n\nHowever, for trajectory planning, it will be easier to work on a coordinate system that following the curve of the roadway (Frenet frame). In our example below, we have 3 lanes. d=0 is the middle of the roadway. d=-1 means the right lane. s measures the distance from a specific point along the roadway. So instead of (x, y), our trajectory location is measured as (s, d) in the Frenet frame.\n\nFor display purpose, we warp the Frenet coordinate system as follows:\n\nConversion between the Cartesian and Frenet frame\n\nWe will use a map to convert coordinates between the global Cartesian and Frenet coordinate. However, the waypoints of the map may be farther apart than we want. For example, the map below has only 3 waypoints (pink). To interpolate points in between, we use these 3 points to fit a spline model, and use it to perform the transformation.\n\nNow, let\u2019s look into how trajectories are calculated.\n\nDuring behavior planning, we propose different plans. We translate this into target locations that we want our car to be at a specific time. We use this target location and a few previous locations to build a cubic spline curve to model our trajectory. This spline model preserves the first, and second derivatives of our previous trajectories so we will have a smoother ride (no sudden jump in the velocity and acceleration). Later, we use the spline model to calculate where the car should be for every 0.02 second in the next second. Then, this information is transferred to the controller to control the actuators of the vehicle.\n\nAlternatively, we can create a trajectory to minimize jerk which we want to minimize sudden accelerations. Mathematically, we define s as a polynomial function of time t. To minimize jerk, we want to find an optimal trajectory that minimizes the accumulated magnitude of the acceleration change, i.e. not too many sudden changes of acceleration |a\u2019(t)|:\n\nTo accomplish this, we constraint our polynomial s(t) into a rank of 5. Intuitively, the high ranking of a polynomial allows sudden movement. For example, a rank 9 polynomial function below allows sharp turns.\n\nTo simplify the computation, we reset the time to 0 when we compute a new trajectory. We know our initial state (location, velocity, acceleration) from our car system.\n\nand the behavior planning provides us the target state:\n\nNow, we want to find the 6 coefficients in our trajectory model:\n\nSo the first 3 coefficients can be found from our initial state. We repeat if for our target state:\n\nNow, we have 3 more equations so we can solve the remaining 3 coefficients. We will repeat the same process in finding the polynomial in the d direction.\n\nIdeally, during behavior planning, we want no more acceleration or lane change by the end of our planned trajectory. So for both initial or target state, they should be:\n\nAlso, our plan wants the car to reach a specific speed sometimes. (reaching the speed limit or match the speed of the car in front)\n\nHere is our summary:\n\nHowever, in low speed, we cannot calculate s and d independently (paper). In our example, we just make d=constant to get around the problem in low speed. (i.e. we don\u2019t change lane at low speed)\n\nTrajectories are ranked using a cost function. Depending on our defined policy, it includes factors like\n\nFor example, the maximum acceleration is limited by the maximum torque of a car. So it is not feasible for a trajectory requiring an acceleration higher than what it can do. To avoid a collision, we compare all projected trajectories. We increase the cost dramatically when the car distance falls below a safety zone. For efficiency, we want to drive as close to the legal speed limit as possible.\n\nWe will also include more state parameters in the cost calculation. For instance, we will add steering angle and steering speed to avoid rollover. However, we do not count those factors equally. For feasibility and safety, their weights are extremely high. This makes sure we do not take an infeasible or risky move. Some other weights are tunable. So for less critical factors involving comfort and efficient, we conduct experiments to strike a good balance between a smoother ride or a more aggressive driving for the optimal speed.\n\nAfter calculating the cost of all trajectories, we select one with the lowest cost as our final decision.\n\nNow an optimal trajectory is selected. In our example, we compute the next 50 locations (0.02 second apart) using the mathematical model we build. i.e. 1 second of trajectory points.\n\nWe convert it back to the global coordinate. This x, y coordinate will be used by the car controller to maneuver the car and for localization purpose. However, usually we do not wait until the whole trajectory to complete for the next trajectory. Often, once we have a new set of measurement, we may re-compute a new trajectory.\n\nFor fun, this is another way to send your Tesla to Mars if your path planning is off.\n\nThis article describes a bear bone framework for the path planning. This framework will be expanded and modified to meet the real-life challenge in particular for the urban driving.\n\nThroughout the article, we frequent mention sensor fusion and control. For those interested in the technical aspect, here are a few more articles that you can read.\n\nFor readers that want more details in the trajectory optimization, this paper can be a good start."
    },
    {
        "url": "https://medium.com/@jonathan_hui/self-driving-car-tracking-cars-with-extended-and-unscented-kalman-filter-ced448fd90e2?source=user_profile---------7----------------",
        "title": "Self-driving car: Tracking cars with Extended and Unscented Kalman Filter",
        "text": "In the previous article, we discuss how Kalman Filter works. The mathematical solution for Kalman Filter assumes states are Gaussian Distributed. However, this is not true sometimes. In this article, we extend our method to Extended Kalman Filter and the Unscented Kalman Filter which produce more accurate results than the Kalman Filter if the dynamic model is not linear.\n\nThis is part of a 5-series self-driving. Other articles includes\n\nRecall from the Kalman Filter, this is how the observer world looks like:\n\nThis assume our dynamic model is linear.\n\nwhere A, B and C are matrix. If the initial state is Gaussian distributed, the prediction is also Gaussian distributed. So Kalman Filter will work nicely. However, many dynamic models are non-linear. The model will be written as:\n\nIf f or h is not linear, will the output Gaussian distributed? In the bottom right below, the probability distribution function for our state x is Gaussian distributed. We apply a function f on x, and the plot on the top left is the probability distribution function for f(x). As you can see, it is not Gaussian distributed anymore. Hence, if the function f is not linear, our output will not be Gaussian distributed.\n\nHowever, if we limit the range of x, the function f can be approximated by a linear function.\n\nIf the function f is close to a linear function or f is close to linear for our target range of x, we can use Extended Kalman Filter to replace Kalman Filter.\n\nIn Extended Kalman Filter, we approximate the function f by Jacobian matrix.\n\nIn the left below is the original equation for the Kalman Filter and the right is the Extended Kalman Filter. The difference is we replace A by the Jacobian matrix F, and C by the Jacobian matrix H.\n\nSo instead of using f(x), we replace it with f\u2019(xi). Here is our visualization which the orange curve (computed from f\u2019(xi)) in the top left is our new Gaussian Distribution which approximate the blue one (computed from f(x) ).\n\nExtended Kalman Filter handles cases where f is close to linear which we will use f\u2019(xi) to approximate f(x). However, this is not feasible if f is not close to linear. In this case, we can sample values in x and compute f(x). Then we use the sampled outputs to compute a Gaussian distribution which is used to approximate the probability distribution of f(x). Since x is Gaussian distributed, instead of randomly sample x, we can just compute the output of predefined sigma points (on the bottom right).\n\nThen we assign a weight to the output which is proportional to the probability distribution of x. We then compute the Gaussian Distribution based on the weighted output. This becomes our state prediction."
    },
    {
        "url": "https://medium.com/@jonathan_hui/self-driving-object-tracking-intuition-and-the-math-behind-kalman-filter-657d11dd0a90?source=user_profile---------8----------------",
        "title": "Self-driving car object tracking: Intuition and the math behind Kalman Filter",
        "text": "Three major components of autonomous driving are localization (where am I), perception (what is around me), and control (how to drive around). We are dealing with sensors to perceive what is around us. Those sensors offer many benefits but yet suffer some significant drawback. Cameras offer high resolution but cannot detect speed and distance easily. LiDAR creates a nice 3-D map. But LiDAR is huge and expensive. Also, both camera and LiDAR are vulnerable to bad weather and environmental factors. RADAR works better in bad conditions and detects speed well. But its resolution is not high enough for precise maneuver. When we are dealing with self-driving car sensors, we do not have a one-size-fits-all solution. Kalman filter is designed to fuse sensor readings to make more accurate predictions than each individual sensor alone. However, the math in Kalman filter can be un-necessary overwhelming. We will cover the intuition first followed by explanations on the math to make it easier.\n\nThis is part of a 5-series self-driving. Other articles includes\n\nWe are taking a train to work. The train comes every 15 minutes starting at 5 am. One day, you ask a fellow passenger when the next train arrives. He answers 7:17 am. Before settling down with his answer, let\u2019s dig down a little bit more. If this is the Japan Shinkansen bullet train, the train schedule is God and delays longer than 30 seconds are rare. The x-axis below represents the train arrival time (15 means 7:15 am) and the y-axis is the probability for each possible arrival time. The graph below indicates a strong certainty that the train will arrive at 7:15.\n\nIn the following graph, we add another curve modeling the passenger answer. In this case, the information is less certain and therefore, the curve is flatter.\n\nIn many transport systems, the schedule is just a reference with frequent delays. Now, if the passenger just checks the real-time arrival time on the phone, the information is much more certain and the plot will just like the opposite of our previous plot.\n\nInstead of blindly trusting the train schedule or the passenger, we want a better approach in making accurate predictions. Let\u2019s introduce a more realistic situation in predicting the location of a moving car. At a certain time, we believe our car is 2 meters away from the stop sign. To handle uncertainty, we use a stochastic model to describe our car location. The red curve below indicates the probabilities of finding the car at different locations. This idea may take a minute to sink in since we are so used to deterministic models.\n\nTo recalibrate our location, we take a GPS measurement. But remember, the measurement is noisy so we use a stochastic model to describe it also. Below are the two different probability curves. The left is our belief and the right one is the GPS measurement. Which one should we trust?\n\nThe orange curve below is our new location prediction combining our belief and the measurement. It has a peak at 2.6m where both red curves agree the most. The resulting curve is also sharper, i.e. we are more certain about the location now. The basic idea for Kalman Filter is that simple even it will take a while to explain the details. In particular, multiple probability curves are computationally intense. We need a more efficient approach to merge probability curves.\n\nWe can also view this approach as a weighted sum between the belief and the measurements. Let\u2019s revisit our train examples again. If our belief is strong (the Japan bullet train\u2019s schedule) while the measurements are weak, the final prediction (the black curve below) will resemble the probability curve of our belief. On the contrary, if the measurement is accurate but we are not certain about our belief, the final prediction will resemble the measurements.\n\nHow can we get a more accurate result with two less accurate information? In real-life, the measurements may have errors but their probability curves are accurate. For example, many measurement errors are Gaussian distributed with variances that can be determined by experiments. Therefore, we can derive our probability curves with good precision. By multiplying the probability curves, we locate where predictions agree and therefore reinforce the final predictions.\n\nSo let\u2019s go through the process end-to-end. We start with an initial GPS reading. Since the measurement errors are Gaussian distributed, we can use it to build a probability curve (a probability distribution function PDF) for the car location. This is our belief. Then we use a dynamic model to predict where the car may be next. The most simple one will be location = previous location + velocity \u00d7 \ud835\udefft. Next, we take a measurement and develop a PDF for it. We merge both PDFs to make a final prediction. When Kalman filter is explained as a Bayes filter, the belief is also called prior and the final prediction is called posterior.\n\nBefore we introduce the Kalman Filter, we need to detail the dynamic model in predicting motion. The equations will look scary but actually pretty simple. To simplify our illustration, we assume our car moves along the x-axis only.\n\nWe first define the state of our car. For simplicity, we will use the position p and the speed v only.\n\nWithout acceleration, we can calculate the equation of motion with some simple Physics.\n\nRewrite this into a matrix form which derives states from the last previous state.\n\nA, a matrix, becomes our state-transition model. In our example, A is:\n\nWe have many controls on the car. For example, we control the throttle for acceleration. Let\u2019s modify our equations:\n\nWe pack all input controls into a vector u and the matrix form of motion becomes:\n\nwhere, in our example,\n\nTo make our model more accurate, we add another term called process noise.\n\nWe use process noise w to represent random factors that are difficult to model precisely, for example, the wind effect and the road condition. We assume w is Gaussian distributed with a zero mean and covariance matrix Q. (Variance is for 1-D Gaussian distribution while covariance matrix is for n-D Gaussian distribution)\n\nWe also model our measurements by applying a transformation matrix C on the state of the system.\n\nVery often, we measure the state directly (for example, the car location). Hence, C is often an identity matrix. In some case, we do not measure it directly. We need a matrix C to describe the relationship between the state and the measurement. In other cases, C performs a coordinate transformation. For example, the state of a RADAR is stored in polar coordinates. We use C to convert it to Cartesian.\n\nHere we have a dynamic model to predict a state and a measurement from its last previous state.\n\nIn the real world, we know the input control u and the measurement y. Through dynamic mechanic in Physics or experiments, it is not too hard to find A, B, and C.\n\nNow, we use this information to build a model in an observer world to resemble the real world. In the observer world, we calculate the estimated measurement \u0177 with the following equations:\n\nHere is the visualization of the observer world.\n\nWe know that our measurement estimate \u0177 will be off since our car model does not include process noise like the wind. By knowing the error (y-\u0177) between the measurement and the measurement estimate, we can refine the car model to make a better estimate for x. We just need to introduce a Kalman gain K to map the error in our measurement estimate to the error in our state estimate. Then our new x estimate will simply the old estimate plus its error. In short, we use the error in our measurement estimate to make an adjustment to the state estimate.\n\nNow our model involves 2 steps. The first step is the prediction:\n\nThe second step is the update of our estimated state with the error (y-\u0177):\n\nSince we break the state estimation into 2 steps, there is an estimated state before the update and one after the update. To reduce confusion, we introduce the notation with a minus sign to indicate the estimated state before the update. Now our car model becomes:\n\nLet\u2019s take a quick peek at how K is calculated,\n\nwhere R quantifies the measurement noise. Let\u2019s do an insanity check. In our location example, C is an identity matrix. If there is no noise, K becomes an identity matrix. Using our car model equation, it will output the measurement y as the estimated state. That is, when the measurement is 100% accurate, our prediction should equal the measurement.\n\nNow, we create a car model in the observed world that take into the account of the measurement noise in the form of Kalman gain.\n\nSo far, our observer world uses a deterministic model. Let\u2019s expand it to stochastic. We assume all estimated states are Gaussian distributed. So we model our last estimated state with a mean x and a covariance matrix P.\n\nThen, we apply the car model in the observer world to make a new state estimate.\n\nBack to linear algebra, if we apply a transformation matrix A to an input x with a covariance matrix \u2211, the covariance matrix of the output (Ax) is simply:\n\nPutting the process noise back, the Gaussian model for the estimated state before the update is:\n\nFinally, we will make the final prediction using Kalman filter. At time=k, we make a measurement y. For easier visualization, we always assume C is an identity matrix when we plot the measurement PDF in this article.\n\nSince the estimated state (before the update) and the measurement are Gaussian distributed, the final estimated state is also Gaussian distributed. We can apply linear algebra again to compute the Gaussian model of our final prediction.\n\nFirst, we calculate the Kalman gain which put the measurement noise back into the equation and map the error in the measurement estimate to the state estimate error.\n\nThen the Gaussian model for the new estimated state is calculated based on the Gaussian model for the state estimate before the update, the Kalman gain K, the measurement and C. Here is our updated state estimation:\n\nCongratulation! You survive the tedious notations and this is how we use Kalman filter to make better state estimation. Comparing with our previous explanation, we do not multiple curves together. Kalman Filter uses simple linear algebra and is much simpler.\n\nThe diagram below shows the corresponding mean and the covariance matrix.\n\nTo track a moving car, we repeat a 2-step procedure below:\n\nLiDAR fires rapid pulses of laser light (200K per second) to create a 3-D map of our environment. Its short wavelength lets us detect small objects with high resolutions. However, the measurement can be noisy in particular during rain, snow, and smog. Radar has longer range and is more reliable, but it has lower resolution. Sensor fusion combines measurements from different sensors using Kalman filter to improve accuracy. The measurement errors of many sensors are not co-related, i.e. the measurement error of a sensor is not caused by another sensor. For that, we can apply Kalman filter one at a time for each measurement to refine the prediction.\n\nWe use linear algebra to model our car. i.e. A, B and C are simply matrix. It may not always true in the real world. For next article, we will talk about Extended Kalman Filter and Unscented Kalman Filter to overcome this problem."
    },
    {
        "url": "https://medium.com/@jonathan_hui/tracking-a-self-driving-car-with-particle-filter-ef61f622a3e9?source=user_profile---------9----------------",
        "title": "Tracking a self-driving car with Particle Filter \u2013 Jonathan Hui \u2013",
        "text": "GPS alone is not accurate enough for self-driving car. According to US government, GPS-enabled smartphones have an accuracy within 5 meters under an open sky, and is noisy in urban environments when signals are bouncing off buildings. In this article, we discuss a landmark-based localization using Particle Filter to remove noise. Localization is about finding the location of moving autonomous cars precisely. This can be helpful in trajectory planning in autonomous driving.\n\nA LiDAR mapping van prepares a map ahead of time with landmarks. Landmarks can be pole-like structures which are less likely to be moved or blocked, or lanes marked on the street.\n\nHere is a slide from Daimler on vehicle localization.\n\nIn our example, we use the sensors and cameras on the autonomous car to measure the distance and orientation information between the car and the landmarks. Here, we have a couple challenges. First, landmarks may be out of sight or temporarily blocked. Second, the signal is noisy. In the simulator below, we use Particle filter to filter out the noise from our measurement to predict the car location. We also assume that the measurement does not identify the landmark itself. We need to cross reference the distance information with the map to identify the landmark. In the video below, we trace the location of the car using such measurements and the identified landmarks.\n\nThis is part of a 5-series self-driving. Other articles includes\n\nFor discussion, our car moves in one direction only. We use the GPS to locate the initial position of the car. Since GPS is not accurate, we sample 10 locations (particles) based on the measurement noise specification provided by the GPS vendor. The number of particles used is tunable and we select an unusually low number just for easy visualization. In real-life, we can start with hundreds to thousands of particles and use experiments to find the proper number that achieve a good balance between accuracy and the computational cost.\n\nIf GPS is not available, we can distribute the particles uniformly. This is the same as saying we do not know where the car is.\n\nFor each particle, we apply a dynamic model (like x\u2019 = x + v \ud835\udefft) to calculate the next position at time t + \ud835\udefft.\n\nTo simulate the process noise caused by factors like weather and the road conditions, we add independent random noise to each particle\u2019s location. The magnitude of the noise is determined by experimental data or an analytical model. As expected, those particles should spread further apart because of those uncertainty factors.\n\nWith a pre-determined map, we find all the surrounding landmarks.\n\nBut we remove all landmarks that are considered out-of-sight from our sensors (the left-most stop sign).\n\nEach particle represents the suggested location of the car. We want to compute the likeliness using measurements from our sensors (like camera and LiDAR). Each reading contains a distance and an orientation from a landmark. But, as mentioned before, the measurement does not identify the landmark itself. Instead, we use the distance and orientation information to locate the closest landmark using a map. For example, in P2, we pick the right-most stop sign using our first sensor reading.\n\nWe assume the measurement noise is Gaussian distributed, so the probability for P2 representing our current car location is:\n\nWe repeat the calculation (PDF) for every new measurement we received. Then the final likelihood for a particle is computed by multiplying all the corresponding PDFs together.\n\nWe repeat the process for every particle. Then we normalize their probabilities such that the total probability is equal to one. This acts as the weight of our particles. The particle with the largest weight is where we estimate our car is.\n\nWe resample another 10 particles based on the weights. For example, P2 will double the chance of being picked if it has twice the weight of P1. In our cases, we have 2 clusters of locations. This indicates our measurements show 2 possible locations for our car. This kind of ambiguity can be reduced by increasing the number of landmarks in the map or reduce the time (\ud835\udefft) between predictions.\n\nFor each particle, we apply our car\u2019s dynamic model again for the next particle position and then add random process noise.\n\nThen we repeat the process again with a new set of measurements. This time, our sensor measurements are less ambiguous. Particles from the first cluster are eliminated with the new 10 particles cluster closely together now.\n\nLocalization using Particle filter and landmarks helps us to locate the self-driving precisely. We combine what we believe our car is with noisy measurements. Even both may not be accurate but they are not coordinated to make the same type of mistakes. Hence, by combining these information together, we paint a much accurate picture on our car location.\n\nWe use a simulator provided by the Udacity self-driving car class to demonstrate our Particle filter code."
    },
    {
        "url": "https://medium.com/@jonathan_hui/lane-keeping-in-autonomous-driving-with-model-predictive-control-50f06e989bc9?source=user_profile---------10----------------",
        "title": "Lane keeping in autonomous driving with Model Predictive Control & PID",
        "text": "Three major components of autonomous driving are localization (where am I), perception (what is around me), and control (how to drive around). Using lane recognition, we can plan a trajectory for an autonomous car to follow. In this article, we discuss the control of the vehicle\u2019s acceleration, brake, and steering with Model Predictive Control (MPC) using a kinematic bicycle model. The purpose is not only following a target trajectory as close as possible but also as smooth as possible to avoid motion sickness or frequent braking.\n\nMPC is a process to minimize costs while satisfying a set of constraints. For example, we want to adjust the steering and the speed every 100 ms such that the cost function, defined as the difference between the target trajectory point (the yellow dot) and ours (the green dot), is minimized under the constraint that the wheel cannot be steered more than 25\u00b0. In MPC, we read from the sensors to determine the current states of the car like speed. Then we consider possible actions within a short period of time (say 1 sec.) based on these readings. Let\u2019s say, we consider steering the wheel by 20\u00b0 clockwise and then reduce it by 1\u00b0 every 100 ms. Assuming these actions result in the lowest cost at the end of the one second period, we will then apply the first action of stirring the wheel 20\u00b0. But instead of performing the remaining actions later, we wait for 100 ms and read the sensors again. With the new readings, we recompute the next optimal action again. MPC makes the next action by taking advantage of viewing the results of a longer future plan (1 sec.). So it is less vulnerable to short-sighted gain in a greedy method and therefore, plan better.\n\nThis is part of a 5-series self-driving. Other articles includes\n\nFirst, we define a kinematic model to describe our car.\n\nIn our model, we can control the front wheel steering angle \u03b4f and the car acceleration a. For simplicity, we assume it is a front-wheel-drive car and we will write \u03b4f as \u03b4 for now.\n\nAt each period (100 ms), we read from the sensors to determine the current state of the vehicle including:\n\nOur lane detection system should also give us a trajectory, say, in the form of next 6 waypoints (6 coordinates). In our example, we use the 6 waypoints to fit a 3-rd order polynomial function. We use this model to compute the y coordinate and the heading \u03c8 from x.\n\nNext, we will create a dynamic model for predicting the state at time t+1 of our car from the last state at time t. Using the kinematic bicycle model, we can easily deduce the location, the heading and the speed from the last state.\n\nWe add 2 more states to measure the cross-track error (the distance between the green and yellow dot) and the heading error for \u03c8.\n\nIn MPC, we define a cost function to optimize our path with the trajectory. For speed, we penalize the model if the car cannot maintain at a target speed. We want no acceleration and zero steering if possible. But since it is unavoidable, we want the rate change to be as low as possible if it happens. This reduces motion sickness and saves gas. In our model, our cost includes:\n\nSince those objectives may be contradictory to others, we add weights to the cost to reflect its priority. Here is the cost function:\n\nRecall that to find our optimal path in MPC, we need a dynamic model to predict the next state, a cost function and the system constraints.\n\nWe solve the control problem as an optimization problem using a cost function under constraints include those for actuators controlling the gas/brake pedal and the steering wheel.\n\nIn our example, we compute the optimal solution for a period of 1 sec. This parameter is tunable. Longer period gives us a better picture of our actions but the accumulated errors may be too much. In fact, if this number is too large, the car will go off-track instead.\n\nFor those that survive the maths, here is a video on what can go wrong with autonomous driving.\n\nA proportional\u2013integral\u2013derivative controller (PID) is another controller for self-driving cars. The video below explains how it works.\n\nIn our example, our control function composes of\n\nWe run the same simulation with the PID controller below. Since the cost function is far less sophisticated, the motion is more jerky.\n\nThe video is generated by MPC using the simulator provided by the Udacity self-driving class."
    },
    {
        "url": "https://medium.com/@jonathan_hui/design-choices-lessons-learned-and-trends-for-object-detections-4f48b59ec5ff?source=user_profile---------11----------------",
        "title": "Design choices, lessons learned and trends for object detections?",
        "text": "Detectors, like region-based detectors or single shot detectors, start from different paths but look much similar now as they fight for the title of the fastest and most accurate detector. In fact, some of the performance difference may be originated from the subtle design and implementation choices rather than on the merits of the model. In the Part 3 here, we will cover some of those design choices followed by some benchmarks done by Google Research. Then we will conclude our series by summarizing how we get here and what the lessons learn so far.\n\nPart 1: What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)?\n\nPart 2: What do we learn from single shot object detectors (SSD, YOLO), FPN & Focal loss?\n\nDetectors use different loss functions and box encoding methods. For example, SSD predicts the square root of width and height to normalize errors. So a 2-pixel difference for a small boundary box is treated more significant than a large boundary box. Here are the different loss functions and box encoding schemes used by different methods.\n\nTo train the model better, we apply different weights for different losses. For example, in YOLO, the weight for localization loss is higher than classification so we can locate objects better.\n\nFeature extractors impact both accuracy and speed. ResNet and Inception are often used if accuracy is far more important than speed. MobileNet provides a lightweight detector that works well with SSD that targets mobile device for real-time processing. For Faster R-CNN and R-FCN, the choice of feature extractors has more impact on accuracy comparing with SSD.\n\nnms only runs on CPU and often takes up the bulk of the running time for the single shot model.\n\nAugment data by cropping image helps the training in detecting objects in different scales. In inference time, we may use multi-cropping for the input image to improve accuracy. But usually, it is slow and not feasible for real-time processing.\n\nSingle shot detectors often have options of which feature map layers to use for object detection. A feature map has a stride of 2 if the resolution decreases by 2 in each dimension. Lower resolution feature maps usually detect higher-level structures that are good for object detection. But the loss of spatial dimension makes it harder to detect small objects.\n\nThe most important question is not which detector is the best. The real question is which detector and what configurations give us the best balance of speed and accuracy each application needed. Below is the comparison of accuracy v.s. speed tradeoff (time measured in millisecond)."
    },
    {
        "url": "https://medium.com/@jonathan_hui/what-do-we-learn-from-single-shot-object-detectors-ssd-yolo-fpn-focal-loss-3888677c5f4d?source=user_profile---------12----------------",
        "title": "What do we learn from single shot object detectors (SSD, YOLOv3), FPN & Focal loss (RetinaNet)?",
        "text": "What do we learn from single shot object detectors (SSD, YOLOv3), FPN & Focal loss (RetinaNet)? In part 2, we will have a comprehensive review of single shot object detectors including SSD and YOLO (YOLOv2 and YOLOv3). We will also look into FPN to see how a pyramid of multi-scale feature maps will improve accuracy, in particular for small objects that usually perform badly for single shot detectors. Then we will look into Focal loss and RetinaNet on how it solve class imbalance problem during training. Part 1: What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)? Part 2: What do we learn from single shot object detectors (SSD, YOLO), FPN & Focal loss? Faster R-CNN has a dedicated region proposal network followed by a classifier. Region-based detectors are accurate but not without a cost. Faster R-CNN processes about 7 FPS (frame per second) for PASCAL VOC 2007 testing set. Like R-FCN, researchers are streamlining the process by reducing the amount of work for each ROI. feature_maps = process(image)\n\nROIs = region_proposal(feature_maps)\n\nfor ROI in ROIs\n\n patch = roi_align(feature_maps, ROI)\n\n results = detector2(patch) # Reduce the amount of work here! As an alternative, do we need a separate region proposal step? Can we derive both boundary boxes and classes directly from feature maps in one step? feature_maps = process(image)\n\nresults = detector3(feature_maps) # No more separate step for ROIs Let\u2019s look at the sliding-window detector again. We can slide windows over feature maps to detect objects. For different object types, we use different window shapes. The fatal mistake of the previous sliding-windows is that we use the windows as the final boundary boxes. For that, we need too many shapes to cover most objects. A more effective solution is to treat the window as an initial guess. Then we have a detector to predict the class and the boundary box from the current sliding window simultaneously.\n\nThis concept is very similar to the anchors in Faster R-CNN. However, single shot detectors predict both the boundary box and the class at the same time. Let\u2019s do a quick recap. For example, we have an 8 \u00d7 8 feature map and we make k predictions at each location. i.e. 8 \u00d7 8 \u00d7 k predictions. At each location, we have k anchors (anchors are just fixed initial boundary box guesses), one anchor for one specific prediction. We select the anchors carefully and every location uses the same anchor shapes. Use 4 anchors to make 4 predictions per location. Here are 4 anchors (green) and 4 corresponding predictions (blue) each related to one specific anchor. 4 predictions each relative to an anchor In Faster R-CNN, we use one convolution filter to make a 5-parameter prediction: 4 parameters on the predicted box relative to an anchor and 1 objectness confidence score. So the 3\u00d7 3\u00d7 D \u00d7 5 convolution filter transforms the feature maps from 8 \u00d7 8 \u00d7 D to 8 \u00d7 8 \u00d7 5. In a single shot detector, the convolution filter also predicts C class probabilities for classification (one per class). So we apply a 3\u00d7 3\u00d7 D \u00d7 25 convolution filter to transform the feature maps from 8 \u00d7 8 \u00d7 D to 8 \u00d7 8 \u00d7 25 for C=20. Each location makes k predictions each have 25 parameters. Single shot detector often trades accuracy with real-time processing speed. They also tend to have issues in detecting objects that are too close or too small. For the picture below, there are 9 Santas in the lower left corner but one of the single shot detectors detects 5 only. SSD is a single shot detector using a VGG19 network as a feature extractor (equivalent to the CNN in Faster R-CNN). Then we add custom convolution layers (blue) afterward and use convolution filters (green) to make predictions.\n\nSSD uses layers already deep down into the convolutional network to detect objects. If we redraw the diagram closer to scale, we should realize the spatial resolution has dropped significantly and may already miss the opportunity in locating small objects that are too hard to detect in low resolution. If such problem exists, we need to increase the resolution of the input image. YOLO uses DarkNet to make feature detection followed by convolutional layers.\n\nHowever, it does not make independent detections using multi-scale feature maps. Instead, it partially flattens features maps and concatenates it with another lower resolution maps. For example, YOLO reshapes a 28 \u00d7 28 \u00d7 512 layer to 14 \u00d7 14 \u00d7 2048. Then it concatenates with the 14 \u00d7 14 \u00d71024 feature maps. Afterward, YOLO applies convolution filters on the new 14 \u00d7 14 \u00d7 3072 layer to make predictions. YOLO (v2) makes many implementation improvements to push the mAP from 63.4 for the first release to 78.6. YOLO9000 can detect 9000 different categories of objects. Here are the mAP and FPS comparison for different detectors reported by the YOLO paper. YOLOv2 can take different input image resolutions. Lower resolution input images achieve higher FPS but lower mAP. YOLOv3 change to a more complex backbone for feature extraction. Darknet-53 mainly compose of 3 \u00d7 3 and 1\u00d7 1 filters with skip connections like the residual network in ResNet. Darknet-53 has less BFLOP (billion floating point operations) than ResNet-152, but achieves the same classification accuracy at 2x faster. YOLOv3 also adds Feature Pyramid (discussed next) to detect small objects better. Here is the accuracy and speed tradeoff for different detectors. Detecting objects in different scales is challenging in particular for small objects. Feature Pyramid Network (FPN) is a feature extractor designed with feature pyramid concept to improve accuracy and speed. It replaces the feature extractor of detectors like Faster R-CNN and generates higher quality feature map pyramid. FPN composes of a bottom-up and a top-down pathway. The bottom-up pathway is the usual convolutional network for feature extraction. As we go up, the spatial resolution decreases. With more high-level structures detected, the semantic value for each layer increases. SSD makes detection from multiple feature maps. However, the bottom layers are not selected for object detection. They are in high resolution but the semantic value is not high enough to justify its use as the speed slow-down is significant. So SSD only uses upper layers for detection and therefore performs much worse for small objects. FPN provides a top-down pathway to construct higher resolution layers from a semantic rich layer. Reconstruct spatial resolution in the top-down pathway. (Modified from source) While the reconstructed layers are semantic strong but the locations of objects are not precise after all the downsampling and upsampling. We add lateral connections between reconstructed layers and the corresponding feature maps to help the detector to predict the location betters. The following is a detail diagram on the bottom-up and the top-down pathway. P2, P3, P4 and P5 are the pyramid of feature maps for object detection. FPN is not an object detector by itself. It is a feature detector that works with object detectors. The following feed each feature maps (P2 to P5) independently to make object detection. In FPN, we generate a pyramid of feature maps. We apply the RPN (described above) to generate ROIs. Based on the size of the ROI, we select the feature map layer in the most proper scale to extract the feature patches.\n\nFor most detectors like SSD and YOLO, we make far more predictions than the number of objects presence. So there are much more negative matches than positive matches. This creates a class imbalance which hurts training. We are training the model to learn background space rather than detecting objects. However, we need negative sampling so it can learn what constitutes a bad prediction. So, for example in SSD, we sort training examples by their calculated confidence loss. We pick the top ones and makes sure the ratio between the picked negatives and positives is at most 3:1. This leads to a faster and more stable training. Detectors can make duplicate detections for the same object. To fix this, we apply non-maximal suppression to remove duplications with lower confidence. We sort the predictions by the confidence scores and go down the list one by one. If any previous prediction has the same class and IoU greater than 0.5 with the current prediction, we remove it from the list. Class imbalance hurts performance. SSD resamples the ratio of the object class and background class during training so it will not be overwhelmed by image background. Focal loss (FL) adopts another approach to reduce loss for well-trained class. So whenever the model is good at detecting background, it will reduce its loss and reemphasize the training on the object class. We start with the cross-entropy loss CE and we add a weight to de-emphasize the CE for high confidence class. For example, for \u03b3 = 0.5, the focal loss for well-classified examples will be pushed toward 0. Here is the RetinaNet building on FPN and ResNet using the Focal loss."
    },
    {
        "url": "https://medium.com/@jonathan_hui/what-do-we-learn-from-region-based-object-detectors-faster-r-cnn-r-fcn-fpn-7e354377a7c9?source=user_profile---------13----------------",
        "title": "What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)?",
        "text": "What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)? In this series, we will take a comprehensive journey on object detection. In Part 1 here, we cover the region based object detectors including Fast R-CNN, Faster R-CNN, R-FCN and FPN. In part 2, we will study the single shoot detectors. In part 3, we cover the performance and some implementation issues. By studying them in one context, we study what is working, what matters and where can be improved. Hopefully, by studying how we get here, it will give us more insights on where we are heading. Part 1: What do we learn from region based object detectors (Faster R-CNN, R-FCN, FPN)? Part 2: What do we learn from single shot object detectors (SSD, YOLO), FPN & Focal loss? Since AlexNet won the 2012 ILSVRC challenge, the use of the CNN for classification has dominated the field. One brute force approach for object detection is to slide windows from left and right, and from up to down to identify objects using classification. To detect different object types at various viewing distances, we use windows of varied sizes and aspect ratios.\n\nWe cut out patches from the picture according to the sliding windows. The patches are warped since many classifiers take fixed size images only. However, this should not impact the classification accuracy since the classifier are trained to handle warped images. The warped image patch is feed into a CNN classifier to extract 4096 features. Then we apply a SVM classifier to identify the class and another linear regressor for the boundary box. Below is the pseudo code. We create many windows to detect different object shapes at different locations. To improve performance, one obvious solution is to reduce the number of windows. Instead of a brute force approach, we use a region proposal method to create regions of interest (ROIs) for object detection. In selective search (SS), we start with each individual pixel as its own group. Next, we calculate the texture for each group and combine two that are the closest. But to avoid a single region in gobbling others, we prefer grouping smaller group first. We continue merging regions until everything is combined together. In the first row below, we show how we grow the regions, and the blue rectangles in the second rows show all possible ROIs we made during the merging. R-CNN makes use of a region proposal method to create about 2000 ROIs (regions of interest). The regions are warped into fixed size images and feed into a CNN network individually. It is then followed by fully connected layers to classify the object and to refine the boundary box.\n\nWith far fewer but higher quality ROIs, R-CNN run faster and more accurate than the sliding windows. Region proposal methods are computation intense. To speed up the process, we often pick a less expensive region proposal method to create ROIs followed by a linear regressor (using fully connected layers) to refine the boundary box further. Use regression to refine the original ROI in blue to the red one. R-CNN needs many proposals to be accurate and many regions overlap with each other. R-CNN is slow in training & inference. If we have 2,000 proposals, each of them is processed by a CNN separately, i.e. we repeat feature extractions 2,000 times for different ROIs. The feature maps in a CNN represent spatial features in a more condensed space. Can we detect objects using the feature maps instead of the raw image? Calculate the ROIs from the feature maps instead. Instead of extracting features for each image patch from scratch, we use a feature extractor (a CNN) to extract features for the whole image first. Then we apply the region proposal method on the feature maps directly. For example, Fast R-CNN selects the convolution layer conv5 in VGG16 to generate ROIs which later combine with the corresponding feature maps to form patches for object detection. We warp the patches to a fixed size using ROI pooling and feed them to fully connected layers for classification and localization (detecting the location of the object). By not repeating the feature extractions, Fast R-CNN cuts down the process time significantly.\n\nHere is the network flow: In the pseudo-code below, the expensive feature extraction is moving out of the for-loop, a significant speed improvement since it was executed for all 2000 ROIs. Fast R-CNN is 10x faster than R-CNN in training and 150x faster in inferencing. One major takeaway for Fast R-CNN is that the whole network (the feature extractor, the classifier, and the boundary box regressor) can be trained end-to-end with multi-task losses (classification loss and localization loss). This improves accuracy. Because Fast R-CNN uses fully connected layers, we apply ROI pooling to warp the variable size ROIs into in a predefined size shape. Let\u2019s simplify the discussion by transforming 8 \u00d7 8 feature maps into a predefined 2 \u00d7 2 shape. Top right: we overlap the ROI (blue) with the feature maps. Bottom left: we split ROIs into the target dimension. For example, with our 2\u00d72 target, we split the ROIs into 4 sections with similar or equal sizes. Bottom right: find the maximum for each section and the result is our warped feature maps. Input feature map (top left), output feature map (bottom right), blue box is the ROI (top right). So we get a 2 \u00d7 2 feature patch that we can feed into the classifier and box regressor. Fast R-CNN depends on an external region proposal method like selective search. However, those algorithms run on CPU and they are slow. In testing, Fast R-CNN takes 2.3 seconds to make a prediction in which 2 seconds are for generating 2000 ROIs. Faster R-CNN adopts the same design as the Fast R-CNN except it replaces the region proposal method by an internal deep network. The new region proposal network (RPN) is more efficient and run at 10 ms per image in generating ROIs. Network flow is the same as the Fast R-CNN. The network flow is the same but the region proposal is now replaced by a convolutional network (RPN).\n\nThe region proposal network (RPN) takes the output feature maps from the first convolutional network as input. It slides 3 \u00d7 3 filters over the feature maps to make class-agnostic region proposals using a convolutional network like ZF network (below). Other deep network likes VGG or ResNet can be used for more comprehensive feature extraction at the cost of speed. The ZF network outputs 256 values, which is feed into 2 separate fully connected layers to predict a boundary box and 2 objectness scores. The objectness measures whether the box contains an object. We can use a regressor to compute a single objectness score but for simplicity, Faster R-CNN uses a classifier with 2 possible classes: one for the \u201chave an object\u201d category and one without (i.e. the background class).\n\nFor each location in the feature maps, RPN makes k guesses. Therefore RPN outputs 4\u00d7k coordinates and 2\u00d7k scores per location. The diagram below shows the 8 \u00d7 8 feature maps with a 3\u00d7 3 filter, and it outputs a total of 8 \u00d7 8 \u00d7 3 ROIs (for k = 3). The right side diagram demonstrates the 3 proposals made by a single location. Here, we get 3 guesses and we will refine our guesses later. Since we just need one to be correct, we will be better off if our initial guesses have different shapes and size. Therefore, Faster R-CNN does not make random boundary box proposals. Instead, it predicts offsets like \ud835\udeffx, \ud835\udeffy that are relative to the top left corner of some reference boxes called anchors. We constraints the value of those offsets so our guesses still resemble the anchors. To make k predictions per location, we need k anchors centered at each location. Each prediction is associated with a specific anchor but different locations share the same anchor shapes. Those anchors are carefully pre-selected so they are diverse and cover real-life objects at different scales and aspect ratios reasonable well. This guides the initial training with better guesses and allows each prediction to specialize in a certain shape. This strategy makes early training more stable and easier. Faster R-CNN uses far more anchors. It deploys 9 anchor boxes: 3 different scales at 3 different aspect ratio. Using 9 anchors per location, it generates 2 \u00d7 9 objectness scores and 4 \u00d7 9 coordinates per location. Anchors are also called priors or default boundary boxes in different papers. As shown below, Faster R-CNN is even much faster. Let\u2019s assume we only have a feature map detecting the right eye of a face. Can we use it to locate a face? It should. Since the right eye should be on the top-left corner of a facial picture, we can use that to locate the face. If we have other feature maps specialized in detecting the left eye, the nose or the mouth, we can combine the results together to locate the face better. So why we go through all the trouble. In Faster R-CNN, the detector applies multiple fully connected layers to make predictions. With 2,000 ROIs, it can be expensive. R-FCN improves speed by reducing the amount of work needed for each ROI. The region-based feature maps above are independent of ROIs and can be computed outside each ROI. The remaining work is much simpler and therefore R-FCN is faster than Faster R-CNN. Let\u2019s consider a 5 \u00d7 5 feature map M with a blue square object inside. We divide the square object equally into 3 \u00d7 3 regions. Now, we create a new feature map from M to detect the top left (TL) corner of the square only. The new feature map looks like the one on the right below. Only the yellow grid cell [2, 2] is activated. Create a new feature map from the left to detect the top left corner of an object. Since we divide the square into 9 parts, we can create 9 feature maps each detecting the corresponding region of the object. These feature maps are called position-sensitive score maps because each map detects (scores) a sub-region of the object. Let\u2019s say the dotted red rectangle below is the ROI proposed. We divide it into 3 \u00d7 3 regions and ask how likely each region contains the corresponding part of the object. For example, how likely the top-left ROI region contains the left eye. We store the results into a 3 \u00d7 3 vote array in the right diagram below. For example, vote_array[0][0] contains the score on whether we find the top-left region of the square object. Apply ROI onto the feature maps to output a 3 x 3 array. This process to map score maps and ROIs to the vote array is called position-sensitive ROI-pool. The process is extremely close to the ROI pool we discussed before. We will not cover it further but you can refer to the future reading section for more information. Overlay a portion of the ROI onto the corresponding score map to calculate V[i][j] After calculating all the values for the position-sensitive ROI pool, the class score is the average of all its elements. Let\u2019s say we have C classes to detect. We expand it to C + 1 classes so we include a new class for the background (non-object). Each class will have its own 3 \u00d7 3 score maps and therefore a total of (C+1) \u00d7 3 \u00d7 3 score maps. Using its own set of score maps, we predict a class score for each class. Then we apply a softmax on those scores to compute the probability for each class. The following is the data flow. For our example, we have k=3 below."
    },
    {
        "url": "https://medium.com/@jonathan_hui/object-detection-speed-and-accuracy-comparison-faster-r-cnn-r-fcn-ssd-and-yolo-5425656ae359?source=user_profile---------14----------------",
        "title": "Object detection: speed and accuracy comparison (Faster R-CNN, R-FCN, SSD, FPN, RetinaNet and\u2026",
        "text": "It is very hard to have a fair comparison among different object detectors. There is no straight answer on which model is the best. For real-life applications, we make choices to balance accuracy and speed. Besides the detector types, we need to aware of other choices that impact the performance: Matching strategy and IoU threshold (how predictions are excluded in calculating loss). The number of proposals or predictions. Use of multi-scale images in training or testing (with cropping). Worst, the technology evolves so fast that any comparison becomes obsolete quickly. Here, we summarize the results from individual papers so you can view them together. Then we present a survey from Google Research. By presenting multiple viewpoints in one context, we hope that we can understand the performance landscape better. In this section, we summarize the performance reported by the corresponding papers. Feel free to browse through this section quickly. This is the results of PASCAL VOC 2012 test set. We are interested in the last 3 rows representing the Faster R-CNN performance. The second column represents the number of RoIs made by the region proposal network. The third column represents the training dataset used. The fourth column is the mean average precision (mAP) in measuring accuracy. Timing on a K40 GPU in millisecond with PASCAL VOC 2007 test set. This is the results of PASCAL VOC 2007, 2012 and MS COCO using 300 \u00d7 300 and 512 \u00d7 512 input images. Speed is measure with a batch size of 1 or 8 during inference.\n\nIt is unwise to compare results side-by-side from different papers. Those experiments are done in different settings which are not purposed for apple-to-apple comparisons. Nevertheless, we decide to plot them together so at least you have a big picture on approximate where are they. But you are warned that we should never compare those numbers directly. For the result presented below, the model is trained with both PASCAL VOC 2007 and 2012 data. The mAP is measured with the PASCAL VOC 2012 testing set. For SSD, the chart shows results for 300 \u00d7 300 and 512 \u00d7 512 input images. For YOLO, it has results for 288 \u00d7 288, 416 \u00d7461 and 544 \u00d7 544 images. Higher resolution images for the same model have better mAP but slower to process.\n\n** indicates the results are measured on VOC 2007 testing set. We include those because the YOLO paper misses many VOC 2012 testing results. Since VOC 2007 results are in general performs better than 2012, we add the R-FCN VOC 2007 result as a cross reference. Input image resolutions and feature extractors impact speed. Below is the highest and lowest FPS reported by the corresponding papers. Yet, the result below can be highly biased in particular they are measured at different mAP.\n\nFPN and Faster R-CNN*(using ResNet as the feature extractor) have the highest accuracy (mAP@[.5:.95]). RetinaNet builds on top of the FPN using ResNet. So the high mAP achieved by RetinaNet is the combined effect of pyramid features, the feature extractor\u2019s complexity and the focal loss. Yet, you are warned that this is not an apple-to-apple comparison. We will present the Google survey later for better comparison. But it will be nice to view everyone claims first. Single shot detectors have a pretty impressive frame per seconds (FPS) using lower resolution images at the cost of accuracy. Those papers try to prove they can beat the region based detectors\u2019 accuracy. However, that is less conclusive since higher resolution images are often used for such claims. Hence, their scenarios are shifting. In additional, different optimization techniques are applied and make it hard to isolate the merit of each model. In fact, single shot and region based detectors are getting much similar in design and implementations now. But with some reservation, we can say: Region based detectors like Faster R-CNN demonstrate a small accuracy advantage if real-time speed is not needed. Single shot detectors are here for real-time processing. But applications need to verify whether it meets their accuracy requirement. Google Research offers a survey paper to study the tradeoff between speed and accuracy for Faster R-CNN, R-FCN, and SSD. (YOLO is not covered by the paper.) It re-implements those models in TensorFLow using MS COCO dataset for training. It establishes a more controlled environment and makes tradeoff comparison easier. It also introduces MobileNet which achieves high accuracy with much lower complexity. The most important question is not which detector is the best. It may not possible to answer. The real question is which detector and what configurations give us the best balance of speed and accuracy that your application needed. Below is the comparison of accuracy v.s. speed tradeoff (time measured in millisecond).\n\nThe paper studies how the accuracy of the feature extractor impacts the detector accuracy. Both Faster R-CNN and R-FCN can take advantage of a better feature extractor, but it is less significant with SSD. For large objects, SSD performs pretty well even with a simple extractor. SSD can even match other detectors\u2019 accuracies using better extractor. But SSD performs much worse on small objects comparing to other methods. For example, SSD has problems in detecting the bottles in the middle of the table below while other methods can. Higher resolution improves object detection for small objects significantly while also helping large objects. When decreasing resolution by a factor of two in both dimensions, accuracy is lowered by 15.88% on average but the inference time is also reduced by a factor of 27.4% on average. The number of proposals generated can impact Faster R-CNN (FRCNN) significantly without a major decrease in accuracy. For example, with Inception Resnet, Faster R-CNN can improve the speed 3x when using 50 proposals instead of 300. The drop in accuracy is just 4% only. Because R-FCN has much less work per ROI, the speed improvement is far less significant. Here is the GPU time for different model using different feature extractors. While many papers use FLOPS (the number of floating point operations) to measure complexity, it does not necessarily reflect the accurate speed. The density of a model (sparse v.s. dense model) impacts how long it takes. Ironically, the less dense model usually takes longer in average to finish each floating point operation. In the diagram below, the slope (FLOPS and GPU ratio) for most dense models are greater than or equal to 1 while the lighter model is less than one. i.e. less dense models are less effective even though the overall execution time is smaller. However, the reason is not yet fully studied by the paper. MobileNet has the smallest footprint. It requiring less than 1Gb (total) memory. The winning entry for the 2016 COCO object detection challenge is an ensemble of five Faster R-CNN models using Resnet and Inception ResNet. It achieves 41.3% mAP@[.5, .95] on the COCO test set and achieve significant improvement in locating small objects."
    },
    {
        "url": "https://medium.com/@jonathan_hui/understanding-region-based-fully-convolutional-networks-r-fcn-for-object-detection-828316f07c99?source=user_profile---------15----------------",
        "title": "Understanding Region-based Fully Convolutional Networks (R-FCN) for object detection",
        "text": "Let\u2019s assume we only have a feature map detecting the right eye of a face. Can we use it to locate a face? It should. Since the right eye should be on the top-left corner of a facial picture, we can use that to locate the face easily.\n\nNevertheless, a feature map rarely gives you such precise answer. But if we have other feature maps specialized in detecting the left eye, the nose or the mouth, we can combine information together to make face detection easier and more accurate. To generalize this solution, we create 9 region-based feature maps each detecting the top-left, top-middle, top-right, middle-left, \u2026 or bottom-right area of an object. By combing the votes from these feature maps, we determine the class and the location of the objects.\n\nR-CNN based detectors, like Fast R-CNN or Faster R-CNN, process object detection in 2 stages.\n\nThe Fast R-CNN and Faster R-CNN program flow are summarized as:\n\nFast R-CNN computes the feature maps from the whole image once. It then derives the region proposals (ROIs) from the feature maps directly. For every ROI, no more feature extraction is needed. That cuts down the process significantly as there are about 2000 ROIs. Following the same logic, R-FCN improves speed by reducing the amount of work needed for each ROI. The region-based feature maps are independent of ROIs and can be computed outside each ROI. The remaining work, which we will discuss later, is much simpler and therefore R-FCN is faster than Fast R-CNN or Faster R-CNN. Here is the pseudo code for R-FCN for comparison.\n\nLet\u2019s get into the details and consider a 5 \u00d7 5 feature map M with a square object inside. We divide the square object equally into 3 \u00d7 3 regions. Now, we create a new feature map from M to detect the top left (TL) corner of the square only. The new feature map looks like the one on the right below. Only the yellow grid cell [2, 2] is activated.\n\nSince we divide the square into 9 parts (top-left TR, top-middle TM, top-right TR, center-left CF, \u2026, bottom-right BR), we create 9 feature maps each detecting the corresponding region of the object. These feature maps are called position-sensitive score maps because each map detects (scores) a sub-region of the object.\n\nLet\u2019s say the dotted red rectangle below is the ROI proposed. We divide it into 3 \u00d7 3 regions and ask how likely each region contains the corresponding part of the object. For example, how likely the top-left ROI region contains the left eye. We store the results into a 3 \u00d7 3 vote array in the right diagram below.\n\nThis process to map score maps and ROIs to the vote array is called position-sensitive ROI-pool which is very similar to the ROI pool in the Fast R-CNN.\n\nFor the diagram below:\n\nAfter calculating all the values for the position-sensitive ROI pool, the class score is the average of all its elements.\n\nLet\u2019s say we have C classes to detect. We expand it to C + 1 classes so we include a new class for the background (non-object). Each class will have its own 3 \u00d7 3 score maps and therefore a total of (C+1) \u00d7 3 \u00d7 3 score maps. Using its own set of score maps, we predict a class score for each class. Then we apply a softmax on those scores to compute the probability for each class.\n\nLet\u2019s see a real example. Below, we have 9 score maps in detecting the top-left to the bottom-right region of a baby. In the top diagram, the ROI aligns well with the ground truth. The solid yellow rectangle in the middle column indicates the ROI sub-region corresponding to the specific score map. Activations are high inside the solid yellow box for every score maps. Therefore the scores in the vote array are high and a baby is detected. In the second diagram, the ROI is misaligned. The score maps are the same but the corresponding locations for the ROI sub-regions (solid yellow) are shifted. The overall activations are low and we will not classify this ROI contains a baby.\n\nBelow is the network flow for R-FCN. Instead of dividing ROIs into 3 \u00d7 3 regions and a 3\u00d7 3 ROI pool, we generalize them into k\u00d7 k. i.e. we will need k\u00d7 k \u00d7 (C+1) score maps. Therefore, R-FCN takes in feature maps and apply convolution to create position-sensitive score maps with depth k\u00d7 k \u00d7 (C+1). For each ROI, we apply the position-sensitive ROI pool to generate the k\u00d7 k vote array. We average the array and use softmax to classify the object."
    },
    {
        "url": "https://medium.com/@jonathan_hui/understanding-feature-pyramid-networks-for-object-detection-fpn-45b227b9106c?source=user_profile---------16----------------",
        "title": "Understanding Feature Pyramid Networks for object detection (FPN)",
        "text": "Detecting objects in different scales is challenging in particular for small objects. We can use a pyramid of the same image at different scale to detect objects (the left diagram below). However, processing multiple scale images is time consuming and the memory demand is too high to be trained end-to-end simultaneously. Hence, we may only use it in inference to push accuracy as high as possible, in particular for competitions, when speed is not a concern. Alternatively, we create a pyramid of feature and use them for object detection (the right diagram). However, feature maps closer to the image layer composed of low-level structures that are not effective for accurate object detection.\n\nFeature Pyramid Network (FPN) is a feature extractor designed for such pyramid concept with accuracy and speed in mind. It replaces the feature extractor of detectors like Faster R-CNN and generates multiple feature map layers (multi-scale feature maps) with better quality information than the regular feature pyramid for object detection.\n\nFPN composes of a bottom-up and a top-down pathway. The bottom-up pathway is the usual convolutional network for feature extraction. As we go up, the spatial resolution decreases. With more high-level structures detected, the semantic value for each layer increases.\n\nSSD makes detection from multiple feature maps. However, the bottom layers are not selected for object detection. They are in high resolution but the semantic value is not high enough to justify its use as the speed slow-down is significant. So SSD only uses upper layers for detection and therefore performs much worse for small objects.\n\nFPN provides a top-down pathway to construct higher resolution layers from a semantic rich layer.\n\nWhile the reconstructed layers are semantic strong but the locations of objects are not precise after all the downsampling and upsampling. We add lateral connections between reconstructed layers and the corresponding feature maps to help the detector to predict the location betters. It also acts as skip connections to make training easier (similar to what ResNet does).\n\nThe bottom-up pathway uses ResNet to construct the bottom-up pathway. It composes of many convolution modules (convi for i equals 1 to 5) each has many convolution layers. As we move up, the spatial dimension is reduced by 1/2 (i.e. double the stride). The output of each convolution module is labeled as Ci and later used in the top-down pathway.\n\nWe apply a 1 \u00d7 1 convolution filter to reduce C5 channel depth to 256-d to create M5. Then we apply a 3 \u00d7 3 convolution to create P5 which becomes the first feature map layer used for object prediction.\n\nAs we go down the top-down path, we upsample the previous layer by 2 using nearest neighbors upsampling. We again apply a 1 \u00d7 1 convolution to the corresponding feature maps in the bottom-up pathway. Then we add them element-wise. We apply a 3 \u00d7 3 convolution again to output the next feature map layers for object detection. This filter reduces the aliasing effect of upsampling.\n\nWe repeat the same process for P3 and P2. However, we stop at P2 because the spatial dimension of C1 is too large. Otherwise, it will slow down the process too much. Because we share the same classifier and box regressor of every output feature maps, all pyramid feature maps (P5, P4, P3 and P2) have 256-d output channels.\n\nFPN is not an object detector by itself. It is a feature detector that works with object detectors. For example, we extract multiple feature map layers with FPN and feed them into an RPN (an object detector using convolutions and anchors) in detecting objects. RPN applies 3 \u00d7 3 convolutions over the feature maps followed by separate 1 \u00d7 1 convolution for class predictions and boundary box regression. These 3 \u00d7 3 and 1 \u00d7 1 convolutional layers are called the RPN head. The same head is applied to all feature maps.\n\nLet\u2019s take a quick look at the Fast R-CNN and Faster R-CNN data flow below. It works with one feature map layer to create ROIs. We use the ROIs and the feature map layer to create feature patches to be fed into the ROI pooling."
    },
    {
        "url": "https://medium.com/@jonathan_hui/real-time-object-detection-with-yolo-yolov2-28b1b93e2088?source=user_profile---------17----------------",
        "title": "Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3",
        "text": "Real-time Object Detection with YOLO, YOLOv2 and now YOLOv3 You only look once (YOLO) is an object detection system targeted for real-time processing. We will introduce YOLO, YOLOv2 and YOLO9000 in this article. For those only interested in YOLOv3, please forward to the bottom of the article. Here is the accuracy and speed comparison provided by the YOLO web site. Let\u2019s start with our own testing image below. For our discussion, we crop our original photo. YOLO divides the input image into an S\u00d7S grid. Each grid cell predicts only one object. For example, the yellow grid cell below tries to predict the \u201cperson\u201d object whose center (the blue dot) falls inside the grid cell. Each grid cell detects only one object. Each grid cell predicts a fixed number of boundary boxes. In this example, the yellow grid cell makes two boundary box predictions (blue boxes) to locate where the person is. Each grid cell make a fixed number of boundary box guesses for the object. However, the one-object rule limits how close detected objects can be. For that, YOLO does have some limitations on how close objects can be. For the picture below, there are 9 Santas in the lower left corner but YOLO can detect 5 only. YOLO may miss objects that are too close. it predicts B boundary boxes and each box has one box confidence score, it detects one object only regardless of the number of boxes B, it predicts C conditional class probabilities (one per class for the likeliness of the object class). To evaluate PASCAL VOC, YOLO uses 7\u00d77 grids (S\u00d7S), 2 boundary boxes (B) and 20 classes (C). Let\u2019s get into more details. Each boundary box contains 5 elements: (x, y, w, h) and a box confidence score. The confidence score reflects how likely the box contains an object (objectness) and how accurate is the boundary box. We normalize the bounding box width w and height h by the image width and height. x and y are offsets to the corresponding cell. Hence, x, y, w and h are all between 0 and 1. Each cell has 20 conditional class probabilities. The conditional class probability is the probability that the detected object belongs to a particular class (one probability per category for each cell). So, YOLO\u2019s prediction has a shape of (S, S, B\u00d75 + C) = (7, 7, 2\u00d75 + 20) = (7, 7, 30). The major concept of YOLO is to build a CNN network to predict a (7, 7, 30) tensor. It uses a CNN network to reduce the spatial dimension to 7\u00d77 with 1024 output channels at each location. YOLO performs a linear regression using two fully connected layers to make 7\u00d77\u00d72 boundary box predictions (the middle picture below). To make a final prediction, we keep those with high box confidence scores (greater than 0.25) as our final predictions (the right picture). The class confidence score for each prediction box is computed as: It measures the confidence on both the classification and the localization (where an object is located). We may mix up those scoring and probability terms easily. Here are the mathematical definitions for your future reference.\n\nYOLO has 24 convolutional layers followed by 2 fully connected layers (FC). Some convolution layers use 1 \u00d7 1 reduction layers alternatively to reduce the depth of the features maps. For the last convolution layer, it outputs a tensor with shape (7, 7, 1024). The tensor is then flattened. Using 2 fully connected layers as a form of linear regression, it outputs 7\u00d77\u00d730 parameters and then reshapes to (7, 7, 30), i.e. 2 boundary box predictions per location. A faster but less accurate version of YOLO, called Fast YOLO, uses only 9 convolutional layers with shallower feature maps. YOLO predicts multiple bounding boxes per grid cell. To compute the loss for the true positive, we only want one of them to be responsible for the object. For this purpose, we select the one with the highest IoU (intersection over union) with the ground truth. This strategy leads to specialization among the bounding box predictions. Each prediction gets better at predicting certain sizes and aspect ratios. YOLO uses sum-squared error between the predictions and the ground truth to calculate loss. The loss function composes of: the localization loss (errors between the predicted boundary box and the ground truth). the confidence loss (the objectness of the box). If an object is detected, the classification loss at each cell is the squared error of the class conditional probabilities for each class: The localization loss measures the errors in the predicted boundary box locations and sizes. We only count the box responsible for detecting the object. We do not want to weight absolute errors in large boxes and small boxes equally. i.e. a 2-pixel error in a large box is the same for a small box. To partially address this, YOLO predicts the square root of the bounding box width and height instead of the width and height. In addition, to put more emphasis on the boundary box accuracy, we multiply the loss by \u03bbcoord (default: 5). If an object is detected in the box, the confidence loss (measuring the objectness of the box) is: If an object is not detected in the box, the confidence loss is: Most boxes do not contain any objects. This causes a class imbalance problem, i.e. we train the model to detect background more frequently than detecting objects. To remedy this, we weight this loss down by a factor \u03bbnoobj (default: 0.5). The final loss adds localization, confidence and classification losses together. YOLO can make duplicate detections for the same object. To fix this, YOLO applies non-maximal suppression to remove duplications with lower confidence. Non-maximal suppression adds 2- 3% in mAP. Here is one of the possible non-maximal suppression implementation: Sort the predictions by the confidence scores. Start from the top scores, ignore any current prediction if we find any previous predictions that have the same class and IoU > 0.5 with the current prediction. Repeat step 2 until all predictions are checked. Predictions (object locations and classes) are made from one single network. Can be trained end-to-end to improve accuracy. YOLO is more generalized. It outperforms other methods when generalizing from natural images to other domains like artwork. Region proposal methods limit the classifier to the specific region. YOLO accesses to the whole image in predicting boundaries. With the additional context, YOLO demonstrates fewer false positives in background areas. YOLO detects one object per grid cell. It enforces spatial diversity in making predictions. SSD is a strong competitor for YOLO which at one point demonstrates higher accuracy for real-time processing. Comparing with region based detectors, YOLO has higher localization errors and the recall (measure how good to locate all objects) is lower. YOLOv2 is the second version of the YOLO with the objective of improving the accuracy significantly while making it faster. Add batch normalization in convolution layers. This removes the need for dropouts and pushes mAP up 2%. The YOLO training composes of 2 phases. First, we train a classifier network like VGG16. Then we replace the fully connected layers with a convolution layer and retrain it end-to-end for the object detection. YOLO trains the classifier with 224 \u00d7 224 pictures followed by 448 \u00d7 448 pictures for the object detection. YOLOv2 starts with 224 \u00d7 224 pictures for the classifier training but then retune the classifier again with 448 \u00d7 448 pictures using much fewer epochs. This makes the detector training easier and moves mAP up by 4%. As indicated in the YOLO paper, the early training is susceptible to unstable gradients. Initially, YOHO makes arbitrary guesses on the boundary boxes. These guesses may work well for some objects but badly for others resulting in steep gradient changes. In early training, predictions are fighting with each other on what shapes to specialize on. In the real-life domain, the boundary boxes are not arbitrary. Cars have very similar shapes and pedestrians have an approximate aspect ratio of 0.41. Since we only need one guess to be right, the initial training will be more stable if we start with diverse guesses that are common for real-life objects. For example, we can create 5 anchor boxes with the following shapes. Instead of predicting 5 arbitrary boundary boxes, we predict offsets to each of the anchor boxes above. If we constrain the offset values, we can maintain the diversity of the predictions and have each prediction focuses on a specific shape. So the initial training will be more stable. In the paper, anchors are also called priors. Here are the changes we make to the network: Remove the fully connected layers responsible for predicting the boundary box. We move the class prediction from the cell level to the boundary box level. Now, each prediction includes 4 parameters for the boundary box, 1 box confidence score (objectness) and 20 class probabilities. i.e. 5 boundary boxes with 25 parameters: 125 parameters per grid cell. Same as YOLO, the objectness prediction still predicts the IOU of the ground truth and the proposed box. To generate predictions with a shape of 7 \u00d7 7 \u00d7 125, we replace the last convolution layer with three 3 \u00d7 3 convolutional layers each outputting 1024 output channels. Then we apply a final 1 \u00d7 1 convolutional layer to convert the 7 \u00d7 7 \u00d7 1024 output into 7 \u00d7 7 \u00d7 125. (See the section on DarkNet for the details.) Change the input image size from 448 \u00d7 448 to 416 \u00d7 416. This creates an odd number spatial dimension (7\u00d77 v.s. 8\u00d78 grid cell). The center of a picture is often occupied by a large object. With an odd number grid cell, it is more certain on where the object belongs. Remove one pooling layer to make the spatial output of the network to 13\u00d713 (instead of 7\u00d77). Anchor boxes decrease mAP slightly from 69.5 to 69.2 but the recall improves from 81% to 88%. i.e. even the accuracy is slightly decreased but it increases the chances of detecting all the ground truth objects. In many problem domains, the boundary boxes have strong patterns. For example, in the autonomous driving, the 2 most common boundary boxes will be cars and pedestrians at different distances. To identify the top-K boundary boxes that have the best coverage for the training data, we run K-means clustering on the training data to locate the centroids of the top-K clusters. Since we are dealing with boundary boxes rather than points, we cannot use the regular spatial distance to measure datapoint distances. No surprise, we use IoU. On the left, we plot the average IoU between the anchors and the ground truth boxes using different numbers of clusters (anchors). As the number of anchors increases, the accuracy improvement plateaus. For the best return, YOLO settles down with 5 anchors. On the right, it displays the 5 anchors\u2019 shapes. The purplish-blue rectangles are selected from the COCO dataset while the black border rectangles are selected from the VOC2007. In both cases, we have more thin and tall anchors indicating that real-life boundary boxes are not arbitrary. Unless we are comparing YOLO and YOLOv2, we will reference YOLOv2 as YOLO for now. We make predictions on the offsets to the anchors. Nevertheless, if it is unconstrained, our guesses will be randomized again. YOLO predicts 5 parameters (tx, ty, tw, th, and to) and applies the sigma function to constraint its possible offset range. Here is the visualization. The blue box below is the predicted boundary box and the dotted rectangle is the anchor. With the use of k-means clustering (dimension clusters) and the improvement mentioned in this section, mAP increases 5%. Convolution layers decrease the spatial dimension gradually. As the corresponding resolution decreases, it is harder to detect small objects. Other object detectors like SSD locate objects from different layers of feature maps. So each layer specializes at a different scale. YOLO adopts a different approach called passthrough. It reshapes the 28 \u00d7 28 \u00d7 512 layer to 14 \u00d7 14 \u00d7 2048. Then it concatenates with the original 14 \u00d7 14 \u00d71024 output layer. Now we apply convolution filters on the new 14 \u00d7 14 \u00d7 3072 layer to make predictions. After removing the fully connected layers, YOLO can take images of different sizes. If the width and height are doubled, we are just making 4x output grid cells and therefore 4x predictions. Since the YOLO network downsamples the input by 32, we just need to make sure the width and height is a multiple of 32. During training, YOLO takes images of size 320\u00d7320, 352\u00d7352, \u2026 and 608\u00d7608 (with a step of 32). For every 10 batches, YOLOv2 randomly selects another image size to train the model. This acts as data augmentation and forces the network to predict well for different input image dimension and scale. In additional, we can use lower resolution images for object detection at the cost of accuracy. This can be a good tradeoff for speed on low GPU power devices. At 288 \u00d7 288 YOLO runs at more than 90 FPS with mAP almost as good as Fast R-CNN. At high-resolution YOLO achieves 78.6 mAP on VOC 2007. Here is the accuracy improvements after applying the techniques discussed so far: VGG16 requires 30.69 billion floating point operations for a single pass over a 224 \u00d7 224 image versus 8.52 billion operations for a customized GoogLeNet. We can replace the VGG16 with the customized GoogLeNet. However, YOLO pays a price on the top-5 accuracy for ImageNet: accuracy drops from 90.0% to 88.0%. We can further simplify the backbone CNN used. Darknet requires 5.58 billion operations only. With DarkNet, YOLO achieves 72.9% top-1 accuracy and 91.2% top-5 accuracy on ImageNet. Darknet uses mostly 3 \u00d7 3 filters to extract features and 1 \u00d7 1 filters to reduce output channels. It also uses global average pooling to make predictions. Here is the detail network description: We replace the last convolution layer (the cross-out section) with three 3 \u00d7 3 convolutional layers each outputting 1024 output channels. Then we apply a final 1 \u00d7 1 convolutional layer to convert the 7 \u00d7 7 \u00d7 1024 output into 7 \u00d7 7 \u00d7 125. (5 boundary boxes each with 4 parameters for the box, 1 objectness score and 20 conditional class probabilities)\n\nYOLO is trained with the ImageNet 1000 class classification dataset in 160 epochs: using stochastic gradient descent with a starting learning rate of 0.1, polynomial rate decay with a power of 4, weight decay of 0.0005 and momentum of 0.9. In the initial training, YOLO uses 224 \u00d7 224 images, and then retune it with 448\u00d7 448 images for 10 epochs at a 10\u22123 learning rate. After the training, the classifier achieves a top-1 accuracy of 76.5% and a top-5 accuracy of 93.3%. Then the fully connected layers and the last convolution layer is removed for a detector. YOLO adds three 3 \u00d7 3 convolutional layers with 1024 filters each followed by a final 1 \u00d7 1 convolutional layer with 125 output channels. (5 box predictions each with 25 parameters) YOLO also add a passthrough layer. YOLO trains the network for 160 epochs with a starting learning rate of 10\u22123 , dividing it by 10 at 60 and 90 epochs. YOLO uses a weight decay of 0.0005 and momentum of 0.9. Datasets for object detection have far fewer class categories than those for classification. To expand the classes that YOLO can detect, YOLO proposes a method to mix images from both detection and classification datasets during training. It trains the end-to-end network with the object detection samples while backpropagates the classification loss from the classification samples to train the classifier path. This approach encounters a few challenges: How do we merge class labels from different datasets? In particular, object detection datasets and different classification datasets uses different labels. Any merged labels may not be mutually exclusive, for example, Norfolk terrier in ImageNet and dog in COCO. Since it is not mutually exclusive, we can not use softmax to compute the probability. Without going into details, YOLO combines labels in different datasets to form a tree-like structure WordTree. The children form an is-a relationship with its parent like biplane is a plane. But the merged labels are now not mutually exclusive. Let\u2019s simplify the discussion using the 1000 class ImageNet. Instead of predicting 1000 labels in a flat structure, we create the corresponding WordTree which has 1000 leave nodes for the original labels and 369 nodes for their parent classes. Originally, YOLO predicts the class score for the biplane. But with the WordTree, it now predicts the score for the biplane given it is an airplane. we can apply a softmax function to compute the probability from the scores of its own and the siblings. The difference is, instead of one softmax operations, YOLO performs multiple softmax operations for each parent\u2019s children. The class probability is then computed from the YOLO predictions by going up the WordTree. For classification, we assume an object is already detected and therefore Pr(physical object)=1. One benefit of the hierarchy classification is that when YOLO cannot distinguish the type of airplane, it gives a high score to the airplane instead of forcing it into one of the sub-categories. When YOLO sees a classification image, it only backpropagates classification loss to train the classifier. YOLO finds the bounding box that predicts the highest probability for that class and it computes the classification loss as well as those from the parents. (If an object is labeled as a biplane, it is also considered to be labeled as airplane, air, vehicle\u2026 ) This encourages the model to extract features common to them. So even we have never trained a specific class of objects for object detection, we can still make such predictions by generalizing predictions from related objects. In object detection, we set Pr(physical object) equals to the box confidence score which measures whether the box has an object. YOLO traverses down the tree, taking the highest confidence path at every split until it reaches some threshold and YOLO predicts that object class. YOLO9000 extends YOLO to detect objects over 9000 classes using hierarchical classification with a 9418 node WordTree. It combines samples from COCO and the top 9000 classes from the ImageNet. YOLO samples four ImageNet data for every COCO data. It learns to find objects using the detection data in COCO and to classify these objects with ImageNet samples. During the evaluation, YOLO test images on categories that it knows how to classify but not trained directly to locate them, i.e. categories that do not exist in COCO. YOLO9000 evaluates its result from the ImageNet object detection dataset which has 200 categories. It shares about 44 categories with COOC. Therefore, the dataset contains 156 categories that have never been trained directly on how to locate them. YOLO extracts similar features for related object types. Hence, we can detect those 156 categories by simply from the feature values. YOLO9000 gets 19.7 mAP overall with 16.0 mAP on those 156 categories. YOLO9000 performs well with new species of animals not found in COCO because their shapes can be generalized easily from their parent classes. However, COCO does not have bounding box labels for any type of clothing so the test struggles with categories like \u201csunglasses\u201d. A quote from the YOLO web site on YOLOv3: On a Pascal Titan X it processes images at 30 FPS and has a mAP of 57.9% on COCO test-dev. YOLOv3 predicts an objectness score for each bounding box using logistic regression. If the bounding box prior (anchor) overlaps a ground truth object more than others, the corresponding objectness score should be 1. For other priors with overlap greater than a predefined threshold (default 0.5), they incur no cost. Each ground truth object is associated with one boundary box prior only. If a bounding box prior is not assigned, it incurs no classification and localization lost, just confidence loss on objectness. Most classification assumes classes are exclusive. YOLOv3 instead uses multilabel classification, like an object can be a pedestrian and a child (non-exclusive). YOLOv3 uses independent logistic classifiers with binary cross-entropy for each label which avoids the complexity of softmax. YOLOv3 makes 3 predictions per location. Each prediction composes of a boundary box, a objectness and 80 class scores, i.e. N \u00d7 N \u00d7 [3 \u00d7 (4 + 1 + 80) ] predictions. YOLOv3 makes predictions at 3 different scales: In the last feature map layer. Then it goes back 2 layers back and upsamples it by 2. YOLOv3 then takes a feature map with higher resolution and merge it with the upsampled feature map using element-wise addition. YOLOv3 apply convolutional filters on the merged map to make the second set of predictions. Repeat 2 again so the resulted feature map layer has good high-level structure (semantic) information and good resolution spatial information on object locations. To determine the priors, YOLOv3 applies k-means cluster. Then it pre-select 9 clusters. For COCO, it is (10\u00d713),(16\u00d730),(33\u00d723),(30\u00d761),(62\u00d745),(59\u00d7 119),(116 \u00d7 90),(156 \u00d7 198),(373 \u00d7 326). It then uses 3 clusters per feature maps according to the scale. A new 53-layer Darknet-53 is used to replace the Darknet-19 as the feature extractor. Darknet-53 mainly compose of 3 \u00d7 3 and 1\u00d7 1 filters with skip connections like the residual network in ResNet. Darknet-53 has less BFLOP (billion floating point operations) than ResNet-152, but achieves the same classification accuracy at 2x faster. YOLOv3's COCO AP metric is on par with SSD but 3x faster. But YOLOv3\u2019s AP is still behind RetinaNet. In particular, AP@IoU=.75 drops significantly comparing with RetinaNet which suggests YOLOv3 has higher localization error. YOLOv3 also shows significant improvement in detecting small objects."
    },
    {
        "url": "https://medium.com/@jonathan_hui/ssd-object-detection-single-shot-multibox-detector-for-real-time-processing-9bd8deac0e06?source=user_profile---------18----------------",
        "title": "SSD object detection: Single Shot MultiBox Detector for real-time processing",
        "text": "Each prediction composes of a boundary box and 20 scores for each class, and we pick the highest score as the class for the bounded object. Conv4_3 makes a total of 38 \u00d7 38 \u00d7 4 predictions: four predictions per cell regardless the depth of the feature maps. As expected, many predictions contain no object. SSD reserves a class \u201c0\u201d to indicate it has no objects.\n\nSSD uses VGG16 to extract feature maps. Then it detects objects using the Conv4_3 layer. For illustration, we simplify Conv4_3 to 8 \u00d7 8 spatially (from 38 \u00d7 38). For each cell (also called location), it makes 4 object predictions.\n\nSSD are designed for object detection in real-time. Faster R-CNN uses a region proposal network to create boundary boxes and utilizes those boxes to classify objects. While it is considered the start-of-the-art in accuracy, the whole process runs at 7 frames per second. Far below what a real-time processing needs. SSD speeds up the process by eliminating the need of the region proposal network. To recover the drop in accuracy, SSD applies a few improvements including multi-scale features and default boxes. These improvements allow SSD to match the Faster R-CNN\u2019s accuracy using lower resolution images , which further pushes the speed higher. According to the following comparison, it achieves the real-time processing speed and even beats the accuracy of the Faster R-CNN. (Accuracy is measured as the mean average precision mAP: the precision of the predictions.)\n\nSSD adds 6 more auxiliary convolution layers after the VGG16. Five of them will be added for object detection. In three of those layers, we make 6 predictions instead of 4. In total, SSD makes 8732 predictions using 6 layers.\n\nAt first, we describe how SSD detects objects from a single layer. Actually, it uses multiple layers ( multi-scale feature maps) to detect objects independently. As CNN reduces the spatial dimension gradually, the resolution of the feature maps also decrease. SSD uses lower resolution layers to detect larger scale objects. For example, the 4\u00d7 4 feature maps are used for larger scale object.\n\nSSD does not use a delegated region proposal network. Instead, it resolves to a very simple method. It computes both the location and class scores using small convolution filters . After extracting the feature maps, SSD applies 3 \u00d7 3 convolution filters for each cell to make predictions. (These filters compute the results just like the regular CNN filters.) Each filter outputs 24 channels: 20 scores for each class plus one boundary box.\n\nMulti-scale feature maps improve accuracy significantly. Here is the accuracy with different number of feature map layers used for object detection.\n\nHow do we predict boundary boxes? Just like Deep Learning, we can start with random predictions and use gradient descent to optimize the model. However, during the initial training, the model may fight with each other to determine what shapes (pedestrians or cars) to be optimized for which predictions. Empirical results indicate early training can be very unstable. The boundary box predictions below work well for one category but not for others. We want our initial predictions to be diverse and not looking similar.\n\nIf our predictions cover more shapes, like the one below, our model can detect more object types. This kind of head start makes training much easier and more stable.\n\nIn real-life, boundary boxes do not have arbitrary shape and size. Cars have similar shapes and pedestrians have an approximate aspect ratio of 0.41. In the KITTI dataset used in the autonomous driving, the width and height distributions for the boundary boxes are highly clustered.\n\nConceptually, the ground truth boundary boxes can be partitioned into clusters with each cluster represented by a default boundary box (the centroid of the cluster). So, instead of making random guesses, we can start the guesses based on those default boxes.\n\nTo keep the complexity low, the default boxes are pre-selected manually and carefully to cover a wide spectrum of real life objects. SSD also keeps the default boxes to a minimum (4 or 6) with one prediction per default box. Now, instead of using a global coordination for the box location, the boundary box predictions are relative to the default boundary boxes at each cell (\u2206cx, \u2206cy, w, h), i.e. the offsets to the default box at each cell.\n\nFor each feature map layers, it shares the same set of default boxes centered at the corresponding cell. But different layers use different sets of default boxes to customize object detections at different resolutions. The 4 green boxes below illustrates 4 default boundary boxes.\n\nDefault boundary boxes are chosen manually. SSD defines a scale value for each feature map layer. Starting from the left, Conv4_3 detects objects at the smallest scale 0.2 (or 0.1 sometimes) and then increases linearly to the rightmost layer at a scale of 0.9. Combining the scale value with the target aspect ratios, we compute the width and the height of the default boxes. For layers making 6 predictions, SSD starts with 5 target aspect ratios: 1, 2, 3, 1/2 and 1/3. Then the width and the height of the default boxes are calculated as:\n\nSSD predictions are classified as positive matches or negative matches. SSD only uses positive matches to calculating the cost in the boundary box mismatch. If the corresponding default boundary box (not the predicted boundary box) has an IoU greater than 0.5 with the ground truth, the match is positive. Otherwise, it is negative. (IoU, the intersection over the union, is the ratio between the intersected area over the joined area for two regions.)\n\nLet\u2019s simplify our discussion to 3 default boxes. Only default box 1 and 2 (but not 3) have an IoU greater than 0.5 with the ground truth box above (blue box). So only box 1 and 2 are positive matches. Once we identify the positive matches, we use the corresponding predicted boundary boxes to calculate the cost. This matching strategy nicely partitions what shape of the ground truth that a prediction is responsible for.\n\nHere is an example of how SSD combines multi-scale feature maps and default boundary boxes to detect objects at different scale. The dog below matches one default box (in red) in the 4 \u00d7 4 feature map layer, but not any default boxes in the higher resolution 8 \u00d7 8 feature map. The cat which is smaller is detected only by the 8 \u00d7 8 feature map layer in 2 default boxes (in blue).\n\nHigher resolution feature maps are responsible for detecting small objects. The first layer for object detection conv4_3 has a spatial dimension of 38 \u00d7 38, a pretty large reduction from the input image. Hence, SSD usually performs badly for small objects comparing with other detection methods. If it is a problem, we can mitigate it by using images with higher resolution.\n\nThe localization loss is the mismatch between the ground truth box and the predicted boundary box. SSD only penalizes predictions from positive matches. We want the predictions from the positive matches to get closer to the ground truth. Negative matches can be ignored.\n\nThe confidence loss is the loss in making a class prediction. For every positive match prediction, we penalize the loss according to the confidence score of the corresponding class. For negative match predictions, we penalize the loss according to the confidence score of the class \u201c0\u201d: class \u201c0\u201d classifies no object is detected.\n\nThe final loss function is computes as:\n\nwhere N is the number of positive match and \u03b1 is the weight for the localization loss.\n\nHowever, we make far more predictions than the number of objects presence. So there are much more negative matches than positive matches. This creates a class imbalance which hurts training. We are training the model to learn background space rather than detecting objects. However, SSD still requires negative sampling so it can learn what constitutes a bad prediction. So, instead of using all the negatives, we sort those negatives by their calculated confidence loss. SSD picks the negatives with the top loss and makes sure the ratio between the picked negatives and positives is at most 3:1. This leads to a faster and more stable training.\n\nData augmentation is important in improving accuracy. Augment data with flipping, cropping and color distortion. To handle variants in various object sizes and shapes, each training image is randomly sampled by one of the following options:\n\nThe sampled patch will have an aspect ratio between 1/2 and 2. Then it is resized to a fixed size and we flip one-half of the training data. In addition, we can apply photo distortions."
    },
    {
        "url": "https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173?source=user_profile---------19----------------",
        "title": "mAP (mean Average Precision) for Object Detection \u2013 Jonathan Hui \u2013",
        "text": "mAP is the metric to measure the accuracy of object detectors like Faster R-CNN, SSD, etc. It is the average of the maximum precisions at different recall values. It sounds complicated but actually pretty simple as we illustrate it with an example. But before that, we will do a quick recap on precision, recall and IoU first.\n\nPrecision measures how accurate is your predictions. i.e. the percentage of your positive predictions are correct.\n\nRecall measures how good you find all the positives. For example, we can find 80% of the possible positive cases in our top K predictions.\n\nHere are their mathematical definitions:\n\nFor example, in the testing for cancer:\n\nIoU measures how much overlap between 2 regions, This measures how good is our prediction in the object detector with the ground truth (the real object boundary).\n\nLet\u2019s create an example to demonstrate the calculation of the average precision (AP). In our picture, we have 5 objects. First, we sort the predictions of our detector based on the confidence level. In the top 4 predictions, we get 3 of them correct. A prediction is consider to be correct if IoU is greater than 0.5.\n\nSo the precision and the recall for our top 4 predictions is:\n\nPrecision is the proportion of TP = 3/4 = 0.75.\n\nRecall is the proportion of TP out of the possible positives = 3/5 = 0.6.\n\nNow, here are the values at different ranks (top k predictions):\n\nThe recall value improves from 0 to 1.0. AP (average precision) is computed as the average of maximum precision at 11 recall levels: from 0 to 1 with a step of 0.1.\n\nLet me give you the equation first. It looks scary but it gets simple with an example.\n\np_interp(r) is the maximum precision for any recall values exceeding r. In our top 4 prediction, the recall value is 0.6. p_interp(0.6) is the maximum precision we can ever get with recall \u2265 0.6. i.e.\n\nPASCAL VOC is a popular dataset for object detection. For the PASCAL VOC challenge, a prediction is positive if IoU > 0.5. However, if multiple detections of the same object are detected, it counts the first one as a positive while the rest as negatives. The mAP in PASCAL VOC is the same as AP we discussed.\n\nLatest research papers tend to give results for the COCO dataset only. For COCO, AP is the average over multiple IoU. AP@[.5:.95] corresponds to the average AP for IoU from 0.5 to 0.95 with a step size of 0.05. AP (which is also called mAP in COCO) averages over all class categories. Here is the AP result for the YOLOv3 detector.\n\nCurrently, we have 2 major types of deep network object detectors: region based and single shot. Here are 3 articles that elaborate more on these class of detectors:"
    },
    {
        "url": "https://medium.com/@jonathan_hui/automate-the-ui-design-process-with-ai-pix2code-f3a91ac5567e?source=user_profile---------20----------------",
        "title": "AI: Snap a UI mockup & finish the prototype in seconds.",
        "text": "Brainstorming, mockups and prototyping take up a large portion of time in product designs. It will be ideal if we can prototype those mockups and run them for user testing. However, engineering resources are hard to allocate. It will be just like a dream if the UI coding can be automated from a simple sketch. So let\u2019s look into AI on how it can be done by simply snapping a picture on the UI mockup. We will review the process first followed by a demonstration on how this can be done in seconds instead of days.\n\nA group of designers may have a brainstorming session and sketches out many design ideas.\n\nThen the designers spend hours to finish the layout design.\n\nThen they may use tools to convert the design into a prototype or have engineers to code it from the layout mockup.\n\nFrom creating layouts to making prototypes for phones, it can take hours or even days if engineers are involved. However, with the application of Deep Learning in AI, the whole process can be automated. Here is a 3-minute demonstration from UIzard in generating native mobile applications directly from sketches.\n\nAs a full product, we still need engineers to finish the business logic and the backend integration. But as a prototype, it may be good to go as designers just want to demonstrate or test the UI flow with users. The technology pix2code is developed by UIzard. It may still take some time to finish a commercial product but the potential of automating UI programming is real. The first half of this article focuses on applying AI to automate the design process and the front end UI coding. The second part is for the AI enthusiastic on the technical aspect for implementing a AI deep network to generate code from a sketch.\n\nAirbnb has demonstrated similar ideas with their Airbnb sketching interfaces. This is a 30 seconds video on how Airbnb UI designs can be generated from UI scratches. Airbnb standardizes their UI components for all their applications. In fact, the whole system contains about 150 types of UI components. The video demonstrates how the application generates designs following a standard UI components design.\n\nThe ideas so far focus on imitating a mockup or translate a mockup into a company-wide standard design. However, designers are often requested to have multiple design options. Can we apply AI to generate design options?\n\nLet\u2019s take a look at how AI generates Anime. The following is an example of generating Anime characters using AI Deep learning. In the right panel, we explore different attributes, like hair color, and the left panel will generate the Anime with the corresponding style automatically.\n\nGenerating content with different styling, using technology like GAN, is under heavy research. Theoretically, we can apply the similar concept to explore different design variants including different color schemes, layouts, and data hierarchy.\n\nAll these AI concepts come to an obvious question: will front-end developers become obsolete? Human always automates what we are doing. Developers learn new technologies every couple years. Not all coding, like business logic, will be automated. The answer is not obvious and I will leave the social impacts for future discussions. For now, I will focus on the opportunities and challenges.\n\nIn HTML and CSS coding, there are many hidden rules imposed by the browser implementations and shortfalls. For that, UI implementations are sometimes a trial and error effort. AI Deep learning is good at extracting million patterns and to formulate rules. Font end UI coding is very similar to the human language: many rules are not organized and not easy to explain. AI has demonstrated superhuman performance in those scenarios, just like AlphaGo beats the GO master. So AI will eventually win in UI coding. Nevertheless, both pix2code and Airbnb Sketching Interfaces are in prototype phase only. It will take 10\u2013100x efforts to generalize the solution for commercial success.\n\nHowever, even UI layout coding is tedious, the most important challenges for the front-end coding is flexibility and maintainability: how easy to make global changes or how easy it breaks after changes? pix2code needs to demonstrate how well it groups and organizes information. Can components share CSS? Are CSS well organized and easy to maintain. For Airbnb, it is much easier to solve. Since the UI is standardized, we can always have a predefined CSS for all 150 UI components. As long as we can break down and classify a sketch correctly into those components, they can shared the predefined CSS. This problem is similar to the object detection in AI and it is pretty well developed.\n\nIn addition, we can apply this technology in UI prototyping first. Since the prototyping code is throw away, the quality requirement is much lower. In addition, it is hard to allocate engineering resource. So this market will be much more proven and ready.\n\nFor the remaining article, we focus on how to build a Deep network to generate the code from a sketch.\n\npix2code uses a deep network model composed of a CNN and two LSTM networks. Its design is similar to the image captioning in Deep Learning. For example, the image-captioning model reads a yellow bus picture below and generates the caption \u201cA yellow school bus idles near a park.\u201d automatically.\n\nWe feed the image into a CNN network to extract image features. Together with the label (the true caption provided by the sample) as input, the LSTM module generates captions.\n\nLet\u2019s unroll the LSTM to see how the model is trained. For example, we have the true label \u201c<start> A yellow school bus idles near a park . <end>\u201d. We feed each word into each LSTM cell at the bottom. In the diagram below, for the first token \u201c<start>\u201d, the LSTM predicts the word \u201cA\u201d. We continue with the second token \u201cA\u201d in the true label which predicts \u201cnew\u201d. Eventually, it predicts the caption as \u201cA new bus is parking near the street.\u201d\n\nOnce it is trained, we make an inference by feeding validation images into the model. For simplicity, we reuse the school bus image as our example. We start the first input token as \u201c<start>\u201d. The LSTM produces the word \u201cA\u201d. Here is the major difference from the training: for the next input to the LSTM cell, we use the output from the last step. i.e. we feed the last output \u201cA\u201d to the model in the next time step. \u201cA\u201d predicts \u201cyellow\u201d which later feed into the model to produce \u201cbus\u201d. We repeat the steps which eventually generate the caption \u201cA yellow bus idles near a park.\u201d\n\npix2code takes a hand drawing UI mockup from a designer, and then feed it to the deep network to produce the XCode project with the UI design. pix2code can also produce code for Android or Web applications using different HTML/CSS/JS frameworks.\n\nHere is the model architect. pix2code composes of a encoder (the left side LSTM and CNN) and a decode (LSTM\u2019). The CNN encodes the GUI picture into latent features. Each training sample comes with a context containing information about the GUI design. The LSTM encodes the corresponding context of a GUI.\n\nThe context is the DSL code (Domain specific language) of the GUI mockups.\n\nThe context above has a stack of rows and a footer in holding UI elements. Since we are only interested in GUI components and the layouts, the actual textual values (the texts) of the components are ignored. This significantly reduces the vocabulary size and allows the tokens (like <stack>) to be coded as a one-hot-vector rather than a word embedding. This saves the model from training the embedding layer.\n\nImages are rescaled to 256x256 pixels without preserving the aspect ratio. Pixels are normalized. The vision model composed of 3 convolutional modules. Each module composes of 2 convolutional layers with 3x3 filters and stride 1. Each module is followed by a 2x2 max-pool for downsampling and a dropout layer for regularization. The convolutional modules output 32, 64 and 128 channels respectively. The final shape is therefore 64x64x128. Then the data is flattened and feed into two fully connected layers of size 1024 with ReLU activations and dropouts.\n\nThe context is encoded by a language model consisting of a stack of two LSTM layers. Each LSTM is unrolled into 48 time steps (48 LSTM cells). The prediction at each time step is a vector of 128 dimensions. (i.e. h1 at time step 1 is a vector with 128 elements.)\n\nThe latent features for both context and images are concatenated and feed to a decoder. The decoder contains a stack of two LSTM layers with output dimension at 512 for each time step. Then it is feed into a fully connected layer to compute probabilities for each vocabulary using softmax. We select the output DSL token with the highest probability. For example, if our vocabulary size is just 5, the model will make a prediction of (0.05, 0.1, 0.05, 0.3, 0.5) to represent the probability for each word in the vocabulary.\n\nFor the context with tokens (x1, x2, x3, x4, x5, \u2026), we create a sliding window to feed data into the LSTM for training. We start with the first training sample (0, 0, \u2026, 0, x1).\n\nWe slide the window to the left once to prepare the next training sample. The following diagram indicates the next two training samples fitted into the model.\n\nThe model is trained with mini-batches of 64 image-sequence pairs. The total loss, using the cross entropy, for a single image is:\n\nIn making predictions, we feed the GUI image and a context of 48 tokens with values (0, 0, \u2026, 0, <start>) into the model. With the first prediction h1 from the model, we create another context (0, 0, \u2026, 0, <start>, h1) for the second prediction h2. We continue the process until the model predicts the <end> token. The resulting sequence of DSL tokens (<start>, h1, h2, \u2026, <end>) is compiled to the desired target language (HTML, XCode) using traditional compiler techniques.\n\nUse BLEU to compute the accuracy of our outputs with the true labels. It breaks a word sequence into say four n-grams. If the true label is (<start>, tk1, tk2, tk3, <end>) and the prediction is (<start>, tk1, tk2, wr3, <end>), the calculation is:\n\nSince the word-length of the prediction and the true label is the same, we do not further reduce the BLEU score.\n\nHere is the Keras code snippet in building the vision model (source). This implementation consists of 3 convolution modules using max pooling, dropout and ReLU followed by 2 fully connected layers.\n\nThe second code snippet is the language model encoder with a stack of 2 LSTM:\n\nFinally, this is the decoder with a stack of 2 LSTM and the optimizer:\n\npix2code is similar to the language translation problem. Instead of translating text into different languages, we transcript images into UI DSL. Airbnb Sketching Interfaces support only 150 components (words), the model will be much easier to train with better performance. But such model is less generalized.\n\nWe can customize the training for each company but this approach will be hard to scale. In addition, you cannot draw any designs but ones in your design guide. But many corporations have straight design guidelines, so this may not be an issue.\n\nIn cognitive science, selective attention illustrates how we restrict our attention to particular objects in the surroundings. It helps us focus, so we can tune out irrelevant information and concentrate on what really matters. Attention helps us to learn more efficiently. Instead of looking at the whole image at every time step, we use the current LSTM state to narrow our focus. In the following picture, each output caption word is generated by a more focus region of interests determined by the LSTM state.\n\nOther possible improvement to pix2code\n\nCompletely replacing the layout coding task by AI may still be years away. The accuracy needs to be improved for much complicated designs. But some corporations have straight design guidelines that may make it happens soon than later."
    },
    {
        "url": "https://medium.com/@jonathan_hui/improve-deep-learning-models-performance-network-tuning-part-6-29bf90df6d2d?source=user_profile---------21----------------",
        "title": "Improve Deep Learning Models performance & deep network tuning (Part 6)",
        "text": "Once we have our models debugged, we can focus on the model capacity and the tuning. In this section, we will discuss how to improve the performance of a deep learning network and how to tune deep learning hyperparameters.\n\nThe 6-part series for \u201cHow to start a Deep Learning project?\u201d consists of:\n\nTo increase the capacity, we add layers and nodes to a deep network (DN) gradually. Deeper layers produce more complex models. We also reduce filter sizes. Smaller filters (3x3 or 5x5) usually perform better than larger filters.\n\nThe tuning process is more empirical than theoretical. We add layers and nodes gradually with the intention to overfit the model since we can tone it down with regularizations. We repeat the iterations until the accuracy improvement is diminishing and no longer justify the drop in the training and computation performance.\n\nHowever, GPUs do not page out memory. As of early 2018, the high-end NVIDIA GeForce GTX 1080 TI has 11GB memory. The maximum number of hidden nodes between two affine layers is restricted by the memory size.\n\nFor very deep networks, the gradient diminishing problem is serious. We add skip connection design (like the residual connections in ResNet) to mitigate the problem.\n\nHere is the checklist to improve performance:\n\nWe should monitor the activation histogram before the activation functions closely. If they are in very different scale, gradient descent will be in-effectively. Apply normalization. If the DN has a huge amount of dead nodes, we should trace the problem further. It can be caused by bugs, weight initializations or diminishing gradients. If none is true, experiment some advance ReLU functions like leaky ReLU.\n\nIf you build your own dataset, the best advices are research hard on how to collect samples. Find the highest quality sources. But filters out all those irrelevant to your problem. Analyze the errors. In our project, images with highly entangled structure perform badly. We can change the model by adding convolution layers with smaller filters. But the model is already too hard to train. We can add more entangled samples for further training. But we have plenty already. Alternatively, we can refine the project scope and narrow down our samples.\n\nCollect labeled data is expensive. For images, we can apply data augmentation with simple techniques like rotation, random cropping, shifting, shear and flipping to create more samples from existing data. Other color distortion includes hue, saturation, and exposure shifts.\n\nWe can also supplement training data with non-labeled data. Use your model to classify data. For samples with high confidence prediction, add them to the training dataset with the corresponding label predictions.\n\nLet\u2019s have a short recap on tuning the learning rate. In early development, we turn off or set to zero for any non-critical hyperparameters including regularizations. With the Adam optimizer, the default learning rate usually works well. If we are confident in the code but yet the loss does not drop, start tuning the learning rate. The typical learning rate is from 1 to 1e-7. Drop the rate each time by a factor of 10. Test it in short iterations. Monitor the loss closely. If it goes up consistently, the learning rate is too high. If it does not go down, the learning rate is too low. Increase it until the loss prematurely flattens.\n\nThe following is a real example showing the learning rate is too high and cause a sudden surge in cost with the Adam optimizer:\n\nIn a less often used practice, people monitor the updates to W ratio:\n\nOnce the model design is stabled, we can tune the model further. The most tuned hyperparameters are:\n\nTypical batch size is either 8, 16, 32 or 64. If the batch size is too small, the gradient descent will not be smooth. The model is slow to learn and the loss may oscillate. If the batch size is too high, the time to complete one training iteration (one round of update) will be long with relatively small returns. For our project, we drop the batch size lower because each training iteration takes too long. We monitor the overall learning speed and the loss closely. If it oscillates too much, we know we are going too far. Batch size impacts hyperparameters like regularization factors. Once we determine the batch size, we usually lock the value.\n\nLearning rate & regularization factors\n\n \n\nWe can tune our learning rate and regularization factors further with the approach mentioned before. We monitor the loss to control the learning rate and the gap between the validation and the training accuracy to adjust the regularization factors. Instead of changing the value by a factor of 10, we change that by a factor of 3 (or even smaller in the fine tuning).\n\nTuning is not a linear process. Hyperparameters are related, and we will come back and forth in tuning hyperparameters. Learning rate and regularization factors are highly related and may need to tune them together sometimes. Do not waste time in fine tuning too early. Design changes easily void such efforts.\n\nThe dropout rate is typically from 20% to 50%. We can start with 20%. If the model is overfitted, we increase the value.\n\nSparsity in model parameters make computation optimization easier and it reduces power consumption which is important for mobile devices. If needed, we may replace the L2 regularization with the L1 regularization. ReLU is the most popular activation function. For some deep learning competitions, people experiment more advanced variants of ReLU to move the bar slightly higher. It also reduces dead nodes in some scenarios.\n\nThere are more advanced fine tunings.\n\nInstead of a fixed learning rate, we can decay the learning rate regularly. The hyperparameters include how often and how much it drops. For example, you can have a 0.95 decay rate for every 100,000 iterations. To tune these parameters, we monitor the cost to verify it is dropping faster but not pre-maturely flatten. Some trainings may use a specific schedule. For example, reduce the learning rate by 10x after 1M iterations and another 10x after 1.2M iterations.\n\nAdvance optimizers use momentum to smooth out the gradient descent. In the Adam optimizer, there are two momentum settings controlling the first order (default 0.9) and the second order (default 0.999) momentum. For problem domains with steep gradients like NLP, we may increase the value slightly.\n\nOverfitting can be reduced by stopping the training when the validation errors increase persistently.\n\nHowever, this is just a visualization of the concept. The real-time error may go up temporarily and then drop again. We can checkpoint models regularly and log the corresponding validation errors. Later we select the model.\n\nSome hyperparameters are strongly related. We should tune them together with a mesh of possible combinations on a logarithmic scale. For example, for 2 hyperparameters \u03bb and \u03b3, we start from the corresponding initial value and drop it by a factor of 10 in each step:\n\nThe corresponding mesh will be [(e-1, e-3), (e-1, e-4), \u2026 , (e-8, e-5) and (e-8, e-6)].\n\nInstead of using the exact cross points, we randomly shift those points slightly. This randomness may lead us to some surprises that otherwise hidden. If the optimal point lays in the border of the mesh (the blue dot), we retest it further in the border region.\n\nA grid search is computationally intense. For smaller projects, this is used sporadically. We start tuning parameters in coarse grain with fewer iterations. To fine-tune the result, we use longer iterations and drop values by a factor of 3 (or even smaller).\n\nIn machine learning, we can take votes from a number of decision trees to make predictions. It works because mistakes are often localized: there is a smaller chance for two models making the same mistakes. In DL, we start training with random guesses (providing random seeds are not explicitly set) and the optimized models are not unique. We pick the best models after many runs using the validation dataset. We take votes from those models to make final predictions. This method requires running multiple sessions and can be prohibitively expensive. Alternatively, we run the training once and checkpoints multiple models. We pick the best models from the checkpoints. With ensemble models, the predictions can base on:\n\nModel ensembles are very effective in pushing the accuracy up a few percentage points in some problems and very common in some DL competitions.\n\nInstead of fine-tuning a model, we can try out different model variants to leapfrog the model performance. For example, we have considered replacing the color generator partially or completely with an LSTM based design. This concept is not completely foreign: we draw pictures in steps.\n\nIntuitively, there are merits in introducing a time sequence method in image generation. This method has proven some success in DRAW: A Recurrent Neural Network For Image Generation.\n\nMajor breakthroughs require model design changes. However, some studies indicate fine-tuning a model can be more beneficial than making incremental model changes. The final verdicts are likely based on your own benchmarking results.\n\nYou may have a simple question like should I use Leak ReLU. It sounds so simple but you will never get a straight answer anywhere. Some research papers show empirical data that leaky ReLU is superior, but yet some projects see no improvement. There are too many variables and many projects do not have the resources to benchmark even a portion of the possibilities. Kaggle is an online platform for data science competitions including deep learning. Dig through some of the competitions and you can find the most common performance metrics. Some teams also publish their code (called kernels). With some patience, it is a great source of information.\n\nDL requires many experiments and tuning hyperparameters is tedious. Creating an experiment framework can expedite the process. For example, some people develop code to externalize the model definitions into a string for easy modification. Those efforts are usually counterproductive for a small team. I personally find the drop in code simplicity and traceability is far worse than the benefit. Such coding makes simple modification harder than it should be. Easy to read code has fewer bugs and more flexible. Instead, many AI cloud offerings start providing automatic hyperparameters tuning. It is still in an infant state but it should be the general trend that we do not code the framework ourselves. Stay tuned for any development!\n\nNow, you have your model tuned and ready to deploy. If you have any more tuning tips, feel free to share with us in the comment. I hope that you feel the 6-part series is useful.\n\nThere are many problems that can be solved by deep learning: much more than you may imagine. Can a designer pass a visual mockup design to you and you can generate the HTML by deep learning automatically? Impossible? Do a Google search on pix2code or sketch2code!"
    },
    {
        "url": "https://medium.com/@jonathan_hui/debug-a-deep-learning-network-part-5-1123c20f960d?source=user_profile---------22----------------",
        "title": "Debug a Deep Learning Network (Part 5) \u2013 Jonathan Hui \u2013",
        "text": "You have build a deep network (DN) but the predictions are garbage. How are you going to troubleshoot the problem? In this article, we describe some of the most common problems in a deep network implementation. But if you have not read the Part 4: Visualize Deep Network models and metrics, please read it first. We need to know what to look before fixing anything.\n\nThe 6-part series for \u201cHow to start a Deep Learning project?\u201d consists of:\n\nIn the early development, we are fighting multiple battles at the same time. As mentioned before, Deep Learning (DL) training composes of million iterations to build a model. Locate bugs are hard and it breaks easily. Start with something simple and make changes incrementally. Model optimizations like regularization can always wait after the code is debugged. Focus on verifying the model is functioning first.\n\nOverfitting the model with a small amount of training data is the best way to debug deep learning. If the loss does not drop within a few thousand iterations, debug the code further. Achieve your first milestone by beating the odds of guessing. Then make incremental modifications to the model: add more layers and customization. Train it with the full training dataset. Add regularizations to control the overfit by monitor the accuracy gap between the training and validation dataset.\n\nMany hyperparameters are more relevant to the model optimization. Turn them off or use default values. Use Adam optimizer. It is fast, efficient and the default learning rate does well. Early problems are mostly from bugs rather from the model design or tuning problems. Go through the checklist in the next section before any tunings. It is more common and easier to verify. If loss still does not drop after verifying the checklist, tune the learning rate. If the loss drops too slow, increase the learning rate by 10. If the loss goes up or the gradient explodes, decrease the learning rate by 10. Repeat the process until the loss drops gradually and nicely. Typical learning rates are between 1 and 1e-7.\n\nInitialize the weights to all zeros is one of the most common mistakes and the DN will never learn anything. Weights should be initialized with a Gaussian distribution:\n\nScaling and normalization are well-understood but remain one of the most overlook problems. If input features and nodes output are normalized, the model will be much easier to train. If it is not done correctly, the loss will not drop regardless of the learning rate. We should monitor the histogram for the input features and the nodes\u2019 outputs for each layer (before the activation functions). Always scale input properly. For the nodes\u2019 outputs, the perfect shape is zero-centered with values not too large(positively or negatively). If not and we encounter gradient problems in that layer, apply batch normalization for convolution layers and layer normalization for RNN cells.\n\nVerify and test the correctness of your loss function. The loss of your model must be lower than the one from the random guessing. For example, in a classification problem with 10 classes, the cross entropy loss for random guessing is -ln(1/10).\n\nReview what is doing badly (errors) and improve it. Visualize your errors. In our project, the model performs badly for images with highly entangled structure. Identify the model weakness to make changes. For example, add more convolution layers with smaller filters to disentangle small features. Augment data if necessary, or collect more similar samples to train the model better. In some situations, you may want to remove those samples and constrain yourself to a more focus model.\n\nOnce the model code is working, the next tuning parameters are the regularization factors. We increase the volume of our training data and then increase the regularizations to narrow the gap between the training and the validation accuracy. Do not overdo it as we want a slightly overfit model to work with. Monitor both data and regularization cost closely. Regularization loss should not dominate the data loss over prolonged periods. If the gap does not narrow with very large regularizations, debug the regularization code or method first.\n\nSimilar to the learning rate, we change testing values in the logarithmic scale. (for example, change by a factor of 10 at the beginning) Beware that each regularization factor can be in a totally different order of magnitude, and we may tune those parameters back and forth.\n\nFor the first implementations, avoid using multiple data cost functions. The weight for each cost function may be in different order of magnitude and will require some efforts to tune it. If we have only one cost function, it can be absorbed into the learning rate.\n\nWhen we use pre-trained models, we may freeze those model parameters in certain layers to speed up computation. Double check no variables are frozen in-correctly.\n\nAs less often talked, we should unit test core modules so the implementation is less vulnerable to code changes. Verify the output of a layer may not be easy if its parameters are initialized with a randomizer. Otherwise, we can mock the input data and verify the outputs. For each module (layers), We can verify\n\nAlways keep track of the shape of the Tensor (matrix) and document it inside the code. For a Tensor with shape [N, channel, W, H ], if W (width) and H (height) are swapped, the code will not generate any error if both have the same dimension. Therefore, we should unit test our code with a non-symmetrical shape. For example, we unit test the code with a [4, 3] Tensor instead of a [4, 4] Tensor.\n\nIf you have any tips on debugging, feel free to share it in the comment section. Now you pass one of the most difficult part of the DL. Let\u2019s beat the state-of-the-art model in Part 6: Improve Deep Learning Models performance & network tuning."
    },
    {
        "url": "https://medium.com/@jonathan_hui/visualize-deep-network-models-and-metrics-part-4-9500fe06e3d0?source=user_profile---------23----------------",
        "title": "Visualize Deep Network models and metrics (Part 4) \u2013 Jonathan Hui \u2013",
        "text": "In troubleshooting a deep network, people jump to conclusions too early too fast. Before learning how to troubleshoot it, we will spend sometimes on what to look for before spending hours in tracing dead end leads. In part 4 of the \u201cStart a Deep Learning project\u201d, we discuss how to visualize your Deep Learning models and performance metrics.\n\nThe 6-part series for \u201cHow to start a Deep Learning project?\u201d consists of:\n\nIt is important to track every move and to examine results at each step. With the help of pre-built package like TensorBoard, visualize the model and metrics is easy and the rewards are almost instantaneously.\n\nVerifying the input and the output of the model. Before feeding data into a model, save some training and validation samples for visual verification. Apply steps to undo the data pre-processing. Rescale the pixel value back to [0, 255]. Check a few batches to verify we are not repeating the same batch of data. The left side images below are some training samples and the right is a validation sample.\n\nSometimes, it is nice to verify the input data\u2019s histogram. Ideally, it should be zero-centered ranging from -1 to 1. If features are in different scales, the gradients will either diminish or explode (subject to the learning rate).\n\nSave the corresponding model\u2019s outputs regularly for verification and error analysis. For example, the color in the validation output is washing out.\n\nBesides logging the loss and the accuracy to the stdout regularly, we record and plot them to analyze its long-term trend. The diagram below is the accuracy and the cross entropy loss displayed by the TensorBoard.\n\nPlotting the cost helps us to tune the learning rate. Any prolonged jump in cost indicates the learning rate is too high. If it is too low, we learn slowly.\n\nHere is another real example when the learning rate is too high. We see a sudden surge in loss (likely caused by a sudden jump in the gradient).\n\nWe use the plot on accuracy to tune regularization factors. If there is a major gap between the validation and the training accuracy, the model is overfitted. To reduce overfitting, we increase regularizations.\n\nWeight & bias: We monitor the weights and the biases closely. Here are the Layer 1\u2019s weights and biases distributions at different training iterations. Finding large (positive or negative) weights or bias is abnormal. A Normal distributed weight is a good sign that the training is going well (but not absolutely necessary).\n\nActivation: For gradient descent to perform the best, the nodes\u2019 outputs before the activation functions should be Normal distributed. If not, we may apply a batch normalization to convolution layers or a layer normalization to RNN layers. We also monitor the number of dead nodes (zero activations) after the activation functions.\n\nGradients: For each layer, we monitor the gradients to identify one of the most serious DL problems: gradient diminishing or exploding problems. If gradients diminish quickly from the rightmost layers to the leftmost layers, we have a gradient diminishing problem.\n\nNot very common, we visualize the CNN filters. It identifies the type of features that the model is extracting. As shown below, the first couple convolution layers are detecting edges and colors.\n\nFor CNN, we can visualize what a feature map is learning. In the following picture, it captures the top 9 pictures (on the right side) having the highest activation in a particular map. It also applies a deconvolution network to reconstruct the spatial image (left picture) from the feature map.\n\nThis image reconstruction is rarely done. But in a generative model, we often vary just one latent factor while holding others constant, It verifies whether the model is learning anything smart.\n\nVisualize the models can be done easily with TensorBoard. TensorBoard is available to TensorFlow and other applications like PyTorch through a 3rd party extension. Spend sometime to visualize your model and you will save far more time in troubleshooting. Equipped with the runtime information of the model, we can start talking troubleshooting in Part 5: Debug a Deep Learning Network."
    },
    {
        "url": "https://medium.com/@jonathan_hui/deep-learning-designs-part-3-e0b15ef09ccc?source=user_profile---------24----------------",
        "title": "Deep Learning designs (Part 3) \u2013 Jonathan Hui \u2013",
        "text": "In part 3, we cover some high level deep learning strategy. Then we go into the details of the most common design choices. (Some basic DL backgrounds may be needed.)\n\nThe 6-part series for \u201cHow to start a Deep Learning project?\u201d consists of:\n\nStart your design simple and small. In the study phase, people are flooded with many cool ideas. We tend to code all the nuts and bolts in one shoot. Resist the seduction from exotic ideas. This will not work. Try to beat the state-of-the-art too early is not practical. Design with fewer layers and customizations. Delay solutions that require un-necessary hyperparameters tuning. Verify the loss is dropping. Do not waste time training the model with too many iterations or too large batch size.\n\nAfter a short debugging, our model produces pretty un-impressive results after 5000 iterations. But colors start confined to regions. There is hope that skin tone is showing up.\n\nThis gives us valuable feedback on whether the model starts coloring. Do not start with something big. You will spend most of your time debugging this or wondering do we just need another hour to train the model.\n\nNevertheless, this is easier to say than doing it. We jump steps. But you are warned!\n\nTo create simple designs first, we need to sort out the top priorities. Break down complex problems into smaller problems and solve them in steps. Everyone has a plan \u2018till they get punched in the mouth. (a quote form Mike Tyson) The right strategy in a DL project is to maneuver quickly from what you learn. Before jumping to a model using no hints, we start one with spatial color hints. We do not move to a \u201cno hint\u201d design in one step. We first move to a model with color hints but dropping the hints\u2019 spatial information. The color quality drops significantly. We shift our priority and refine our model first before making the next big jump. We are dealing with many surprises in designing models. Instead of making a long-term plan that keeps changing, be priority driven. Use shorter and smaller design iterations to make the project manageable.\n\nAnalyze the weakness of your models first instead of making random improvements like bi-directional LSTM or PReLU. Visualize the errors (badly performed scenarios) and the performance metrics to identify real issues. Random improvements can be counterproductive by increasing training complexity un-proportionally high with little returns.\n\nWe apply constraints to the network design to make training more effective. Building deep learning is not only putting layers together. Adding good constraints make learning more efficient or more \u201cintelligent\u201d. For example, apply attention so the network knows where to look. In the variational autoencoder, we train the latent factors to be Normal distributed. In our design, we apply denoising to corrupt large fractions of spatial color hints by zeroing them out. Ironically, it forces the model to learn and to generalize better.\n\nFor the rest of the article, we will discuss some of the most common design choices encounter in a DL project.\n\nIn just six months after the release of the TensorFlow from Google on Nov 2015, it became the most popular deep learning framework. While it seems implausible for any challengers soon, PyTorch was released by Facebook a year later and get a lot of traction from the research community. As of 2018, there are many choices of deep learning platform including TensorFlow, PyTorch, Caffe, Caffe2, MXNet, CNTK etc\u2026 There is one key factor triggers the defection of some researchers to PyTorch. The PyTorch design is end-user focused. The API is simple and intuitive. The error messages make sense and the documentation is well organized. Some of the features like pre-trained models, data pre-processing and loading common datasets in PyTorch are very popular. TensorFlow does an excellent job. But so far, it adopts a bottom-up approach that makes things complicated. Their APIs are verbose. Debugging is ad hoc. It has about half a dozen API models to build DN: the result of many consolidations and matching competitor offerings.\n\nMake no mistakes, TensorFlow is still dominating as of Feb 2018. The developer community is the biggest. This may be the only factor matters. If you want to train the model with multiple machines or deploy the inference engine onto a mobile phone, TensorFlow is the only choice. Nevertheless, if other platforms prove themselves being more end-user focus, we will foresee more deflection for small to mid-size projects.\n\nAs TensorFlow evolves, there are many API options in building a DN. The highest level API is the estimator which provides implicit integration with the TensorBoard for performance metrics. However, its adoption remains low outside the built-in estimators. The lowest level APIs is verbose and spread in many modules. It is now being consolidated into the tf.layers, tf.metrics and tf.losses modules with wrapper APIs that build DN layers easier. For researchers that want more intuitive APIs, there are Keras, TFLearn, TF-Slim, etc\u2026 All work well with TensorFlow. I will suggest selecting one that has the pre-trained models that you need and the utilities to load your dataset. The amount of latest activities is important also. In the academic world, Keras APIs are pretty popular for quick prototyping.\n\nDon\u2019t reinvent the wheel. Many deep learning software platforms come with pre-trained models like VGG19, ResNet and Inception V3. Training from scratch takes a long time. As stated from the VGG paper in 2014: \u201cthe VGG model was originally trained with four NVIDIA Titan Black GPUs, training a single net took 2\u20133 weeks depending on the architecture.\u201d\n\nMany pre-trained models can be repurposed for deep learning problems. For example, we extract image features using a pre-trained VGG model and feed them to an LSTM model to generate captions. Many pre-trained models are trained with ImageNet images. If your target data is not very different from the ImageNet, we freeze most of the model\u2019s parameters and retrain the last few fully connected layers only. Otherwise, we retrain the whole network end-to-end with our training dataset. But in both cases, since the model is already pre-trained, it can be retrained with significantly fewer iterations. As the training is shorter, we can avoid overfitting even the training dataset is not large enough. This kind of transfer learning works well across disciplines, for example training a Chinese language model with a pre-trained English model.\n\nHowever, this transfer learning is only justifiable for problems requiring a complex model to extract features. In our project, our samples are different from the ImageNet, and we need to retrain the model end-to-end. Nevertheless, the training complexity from VGG19 is too high when we only need relative simple latent factors (the colors). So we decide to build a new but simpler CNN model for feature extraction.\n\nNot all cost functions are created equally. It impacts how easy to train the model. Some cost functions are pretty standard, but some problem domains need some careful thoughts.\n\nCost functions looking good in the theoretical analysis may not perform well in practice. For example, the cost function for the discriminator network in GAN adopts a more practical and empirical approach than the theoretical one. In some problem domains, the cost functions can be part guessing and part experimental. It can be a combination of a few cost functions. In our project, we start with the standard GAN cost functions. We also add a reconstruction cost using MSE and other regularization costs. However, our brain does not judge styling by MSE. One of the unresolved areas in our project is to find a better reconstruction cost function. One possibility includes using perceptual loss to measuring the difference in the style instead of per-pixel comparison.\n\nGood metrics help you to compare and tune models better. Search for established metrics for your type of problem. For ad hoc problem, check out Kaggle. It hosts many DL competitions with well documented metrics. Unfortunately, for our project, it is hard to define a precise formula to measure the accuracy for artistic rendering.\n\nWhat is good about L1 regularization? L1 regularization promotes sparsity in parameters which encourages representations that disentangle the underlying representation. Since each non-zero parameter adds a penalty to the cost, L1 prefers more zero parameters than the L2 regularization. i.e. it prefers many zeros and a slightly larger parameter than many tiny parameters in L2 regularization. L1 regularization makes filters cleaner, easier to interpret and therefore a good choice for the feature selection. The computation is easier to optimize and therefore consume less power. Hence, L1 is more suitable for the mobile devices. L1 is also less vulnerable to outliners and works better if the data is less clean. However, L2 regularization remains more popular because the solution may be more stable.\n\nAlways monitor gradient closely for diminishing or exploding gradients. Gradient descent problems have many possible causes which are very hard to verify. Do not jump into the learning rate tuning or making model design changes too fast. The small gradients may simply caused by programming bugs. For example, the input data is not scaled properly, or the weights are all initialized to zero. Tuning takes time. It will have better returns if we verify other causes first.\n\nIf other possible causes are eliminated, apply gradient clipping (in particular for NLP) when gradient explode. Skip connections are a common technique to mitigate gradient diminishing problem. In ResNet, a residual layer allows the input to bypass the current layer to the next layer. Effectively, it reduces the depth of the network to make training easier in early training.\n\nScale your input features. We often scale features to be zero-centered within a specific range say [-1, 1]. The improper scaling of the features is one most common cause for the exploding or diminishing gradients. Sometimes, we compute a mean and a variance from the training data to scale the data closer to be Normal distributed. When we scale the validation or testing data, reuse the mean and the variance from the training data. Do not recompute them.\n\nThe unbalance of the nodes\u2019 outputs before the activation functions in each layer is another major source of the gradient problem. If needed, apply batch normalization (BN) to CNN. DN learns faster and better if inputs are properly normalized (scaled). In BN, we compute the means and the variances for each spatial location from each batch of training data. For example, with a batch size of 16 and a feature map with 10x10 spatial dimension, we compute 100 means and 100 variances (one per location). The mean at each location is the average of the corresponding locations from the 16 samples. We use the means and the variances to renormalize the node outputs at each location. BN improves accuracy and reduces training time. As a side bonus, we can increase the learning rate further to make training faster.\n\nHowever, BN is not effective for RNN. We use Layer normalization instead. In RNN, the means and variances from the BN are not suitable to renormalize the output of RNN cells. It is likely because of the recurrent nature of the RNN and the sharing parameters. In Layer normalization, the output is renormalized by the mean and the variance calculated by the layer\u2019s output of the current sample. A 100 elements layer uses only one mean and one variance from the current input to renormalize the layer.\n\nDropout can be applied to layers to regularize a model. Dropout becomes less popular after the introduction of batch normalization in 2015. Batch normalization uses the mean and the standard deviation to rescale the node outputs. Because each training batch has different mean and variance, this behaves like noise which forces layers to learn more robustly for variants in input. Since batch normalization also helps the Gradient descent, it gradually replaces dropout.\n\nThe benefit of combining dropout with L2 regularization is domain specific. Usually, we may test dropouts in tuning and collect empirical data to justify its benefit.\n\nIn DL, ReLU is the most popular activation function to introduce non-linearity to the model. If the learning rate is too high, many nodes can be dead and stay dead. If changing the learning rate does not help, we can try leaky ReLU or PReLU. In a leaky ReLU, instead of outputting zero when x < 0, it has a small predefined downward slope (say 0.01 or set by a hyperparameter). Parameter ReLU (PReLU) pushes a step further. Each node will have a trainable slope.\n\nTo test the real performance, we split our data into three parts: 70% for training, 20% for validation and 10% for testing. Make sure samples are randomized properly in each dataset and each batch of training samples. During training, we use the training dataset to build models with different hyperparameters. We run those models with the validation dataset and pick the one with the highest accuracy. But, as the last safeguard, we use the 10% testing data for a final insanity check. If your testing result is dramatically different from the validation result, the data should be randomized more, or more data should be collected.\n\nSetting a baseline helps us in comparing models and debugging. Research projects often require an established model as a baseline to compare models. For example, use a VGG19 model as the baseline for classification problems. Alternatively, we can extend some established and simple models to solve our problem first. This helps us to understand the problem better and establishes a performance baseline for comparison. In our project, we modify an established GAN implementation and redesign the generative network as our baseline.\n\nWe save models\u2019 output and metrics periodically for comparison. Sometimes, we want to reproduce results for a model or reload a model to train it further. Checkpoints allow us to save models to be reloaded later. However, if the model design has changed, all old checkpoints cannot be loaded. Even there is no automated process to solve this, we use Git tagging to trace multiple models and reload the correct model for a specific checkpoint. Checkpoints in TensorFlow is huge. our designs take 4 GB per checkpoint. When working in a cloud environment, configure enough storages accordingly. We start and terminate Amazon cloud instances frequently. Hence, we store all the files in the Amazon EBS so it can be reattached easily.\n\nBuilt-in layers from DL software packages are better tested and optimized. Nevertheless, if custom layers are needed:\n\nOne of the challenges in DL is reproducibility. During debugging, if the initial model parameters keep changing between sessions, it will be hard to debug. Hence, we explicitly initialize the seeds for all randomizer. In our project, we initialize the seeds for python, the NumPy and the TensorFlow. For final tuning, we turn off the explicit seed initialization so we generate different models for each run. To reproduce the result of a model, we checkpoint a model and reload it later.\n\nAdam optimizer is one of the most popular optimizers in DL, if not the most popular. It suits many problems including models with sparse or noisy gradients. It achieves good results fast with the greatest benefit of easy tuning. Indeed, default configuration parameters often do well. Adam optimizer combines the advantages of AdaGrad and RMSProp. Instead of one single learning rate for all parameters, Adam internally maintains a learning rate for each parameter and separately adapt them as learning unfolds. Adam is momentum based using a running record of the gradients. Therefore, the gradient descent runs smoother and it dampens the parameter oscillation problem due to the large gradient and learning rate. An alternative but less used option is the SGD using Nesterov Momentum.\n\n\u03b2 (momentum) smoothes out the gradient descent by accumulate information on the previous descent to smooth out the gradient changes. The default configuration works well for early development usually. If not, the most likely parameter to be tuned is the learning rate.\n\nHere is a brief summary on the major steps of a deep learning project:\n\nDo I miss any core design topics in DL? Feel free to share with us in the comment section. Deep network is one big black box. People act irrationally when debugging a DN. Instead of spending hours in following dead end leads, we should spend some time to create a framework to visualize the DN models and the metrics. In Part 4: Visualize Deep Network models and metrics, we cover what should we monitor in troubleshooting and tuning our networks."
    },
    {
        "url": "https://medium.com/@jonathan_hui/build-a-deep-learning-dataset-part-2-a6837ffa2d9e?source=user_profile---------25----------------",
        "title": "Build a Deep Learning dataset (Part 2) \u2013 Jonathan Hui \u2013",
        "text": "The success of a Deep Learning project depends on the quality of your dataset. In the 2nd part of this series, we explore the core issues to build a good training dataset.\n\nThe 6-part series for \u201cHow to start a Deep Learning project?\u201d consists of:\n\nFor research projects, search for established public datasets. Those datasets have cleaner samples and published model performance that you can baseline on. If you have more than one options, select the one with the highest quality samples relevant to your problems.\n\nFor real-life problems, we need samples originated from the problem domains. Try to locate public datasets first. The efforts to build a high-quality custom dataset is rarely discussed properly. If none is available, search where you can crawl the data. Usually, there are plenty suggestions, but the data quality is often low and requires a lot of cleanup. Spend quality time to evaluate all your options and select the most relevant before crawling samples.\n\nDo not crawl all your data at once. We often crawl website samples by tags and categories for data relevant to our problem. Train and test samples in your model and refine the crawled taxonomy from lessons learned. Then cleanup your crawled data significantly. Otherwise, even with the best model designs, it will still fall short of human-level performance. Danbooru and Safebooru are two very popular sources of Anime characters. But some deep learning applications prefer Getchu for better quality drawings. We can download drawings from Safebooru using a set of tags. We examine the samples visually and run tests to analyze the errors (the samples that perform badly). Both the model training and the visual evaluation provide further information to refine our tag selections. With continue iterations, we learn more and build our samples gradually. Download files to different folders according to the tags or categories such that we can merge them later based on our experience. Clean up samples. Use a classifier to further filter samples not relevant to your problem. For example, remove all drawings if the characters are too small. Some Anime projects use Illustration2Vec to estimate tags by extracting vector features for further fine-tuned filtering. Smaller projects rarely collect as many samples comparing with academic datasets. Apply transfer learning if appropriate.\n\nI re-visit the progress on the Manga colorization when I write this article. We did not spend much time on PaintsChainer when we started the project. But I am glad to play them a visit.\n\nThe left drawing is provided by PaintsChainer and the right is the drawing colored by the machine. Definitely, this is product-ready quality.\n\nWe decided to test it with some of our training samples. It is less impressive. Fewer colors are applied and the style is not correct.\n\nSince we trained our model for a while, we knew what drawings will perform badly. As expected, it has a hard time for drawings with entangled structures.\n\nThis illustrates a very important point: choose your samples well. As a product offering, PaintsChainer makes a smart move on focusing the type of scratches that they excel. To proof that I use a clean line art picked from the internet. The result is impressive again.\n\nThere are a few lessons learned here. There is no bad data, just the data is not solving your needs. Focus on what your product wants to offer. As samples\u2019 taxonomy increased, it is much harder to train and to maintain output quality. \n\n \n\nIn early development, we realize some drawings have too many entangled structures. Without significantly increasing the model capacity, those drawings produce little values in training and better be left off. It just makes the training inefficient.\n\nPeople spend a lot of time in building and tuning a model. This article counterbalances the important of a clean dataset. In Part 3: Deep Learning designs, we finally look at some key design decisions in a DL project."
    },
    {
        "url": "https://medium.com/@jonathan_hui/how-to-start-a-deep-learning-project-d9e1db90fa72?source=user_profile---------26----------------",
        "title": "How to start a Deep Learning project? \u2013 Jonathan Hui \u2013",
        "text": "In a 6-part series, I will explain the whole journey from starting to finishing a deep learning (DL) project. We will use an automatic Manga colorization project we did to illustrate the deep learning design, debugging and tuning process.\n\nThe whole series for \u201cHow to start a Deep Learning project?\u201d consists of six parts:\n\nMany AI projects are not that serious and pretty fun. In early 2017, as part of my research on the topic of Generative Adversaries Network (GAN), I started a project to colorize Japanese Manga. The problem is difficult but it was fascinating in particular I cannot draw! In looking for projects, look beyond incremental improvements, make a product that is marketable, or create a new model that learns faster and better.\n\nDeep Learning (DL) training composes of million iterations to build a model. Locate bugs are hard and it breaks easily. Start with something simple and make changes incrementally. Model optimizations like regularization can always wait after the code is debugged. Visualize your predictions and model metrics frequently. Make something works first so you have a baseline to fall back. Do not get stuck in a big model.\n\nPlan big fails big. Most personal projects last from two to four months for the first release. It is pretty short since research, debugging and experiments take time. We schedule those complex experiments to run overnight. By early morning, we want enough information to make our next move. As a good rule of thumb, those experiments should not run longer than 12 hours in the early phase. To achieve that, we narrow the scope to single Anime characters. As a flashback, we should reduce the scope further. We have many design tests and we need to turn around fast. Analyze where your models fail. Don\u2019t plan too far ahead. We want to measure and learn fast.\n\nWhen we start the Manga project in the Spring of 2017, Kevin Frans has a Deepcolor project to colorize Manga with spatial color hints using GAN.\n\nMany AI fields are pretty competitive. When defining the goal, you want to push hard enough so the project is still relevant when it is done. GAN model is pretty complex and the quality is usually not product-ready in early 2017. Nevertheless, if you narrow what the product can handle smartly, you may push the quality high enough as a commercial product. To achieve that, select your training samples carefully. For any DL projects, strike a good balance among model generalization, capacity, and accuracy.\n\nTraining real-life models with a GPU is a must. It is 20 to 100 times faster than a CPU. The lowest price Amazon GPU p2.xlarge spot instance is about $7.5/day and then go up to $75/day for an 8 unit GPUs. Training models can be expensive consider that Google can spend a whole week in training NLP models using one thousand servers. In our Manga project, some experiments took over 2 days. We spend an average of $150/week. For faster iterations, it can ring up the bill to $1500/week with faster instances. Instead of using the cloud computing, you can purchase a standalone machine. A desktop with the Nvidia GeForce GTX 1080 TI costs about $2200 in Feb 2018. It is about 5x faster than a P2 instance in training a fine-tuned VGG model.\n\nWe define our development in four phases with the last 3 phases executed in multiple iterations.\n\nWe do research in current offerings to explore their weakness. For many GAN type solutions, they utilized spatial color hints. The drawings are a little bit wash out or muddy. The colors sometimes bleed. We set a 2-month timeframe for our project with 2 top priorities: generate color without hints and improve color fidelity. Our goal is:\n\nStanding on the shoulders of giants\n\nThen we study related research and open source projects. Spend a good amount of time in doing research. Gain intuitions on where existing models are flawed or performed well. Many people go through at least a few dozen papers and projects before starting their implementations. For example, when we deep down into GANs, there are over a dozen new GAN models: DRAGAN, cGAN, LSGAN etc\u2026 Reading research papers can be painful. Skim through the paper quickly to grab the core ideas. Pay attention to the figures. After knowing what is important, read the paper again.\n\nDeep learning (DL) codes are condensed but difficult to troubleshoot. Research papers often miss details. Many projects start off with open source implementations that show successes for similar problems. Search hard. Try a few options. We locate code implementations on different GAN variants. We trace the code and give them a few test drives. We replace the generative network of one implementation with an image encoder and a decoder. As a special bonus, we find the hyperparameters in place are pretty decent. Otherwise, searching for the initial hyperparameters can be tedious when the code is still buggy.\n\nHave fun and being innovative. There are plenty cool applications of deep learning waiting for you. Free feel to leave comments on nice AI projects you find.\n\nIn part 1, we talk about the general principle of a Deep Learning project. With the exception of using academic datasets, the effort to build a dataset is usually overlooked and under estimated. In Part 2: Build a Deep Learning dataset, we discuss how to build a dataset for a better model."
    }
]