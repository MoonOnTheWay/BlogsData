[
    {
        "url": "https://medium.com/@neuralnets/she-was-sitting-on-a-couch-with-her-file-and-a-pouch-a82307deb02f?source=user_profile---------1----------------",
        "title": "She was sitting on a couch, with her file and a pouch.",
        "text": "She was sitting on a couch, with her file and a pouch.\n\nWith room filled with creeps, you can tell,\n\nHer mind was not at peace,\n\nShe could hear her heart beat\n\nand she felt like she is on a hot seat.\n\nShe gets invited to a room\n\nby a gentleman well groomed,\n\nHe smiles to break the ice\n\nbut honestly it didn\u2019t suffice.\n\nHe then introduced himself\n\nwith the greetings like he cares,\n\nIt felt more like a dare\n\nwhich she felt she is obliged to despair,\n\n\u201cHello to you too and I am fine thank you\u201d\n\nShe replied to be fair!\n\nShe makes herself comfortable on the opposite chair,\n\nAnd then my friend, it begins, the cruel old questionnaire,\n\nThe first on the cards was telling something about herself,\n\nShe thought for a moment and then she began,\n\n\u201cI am a small town girl who has big plans,\n\nI am thoughtful and honest who works very hard,\n\nI am always attentive as I never put down my guards,\n\nI am yet to figure out a way to have\n\nmy dreams achieved,\n\nSo, I request to have me here for time being.\u201d\n\nHe looked at her with surprise like he got a stroke but then she handled it by saying: \u201cRelax, it\u2019s just a joke!\u201d\n\n\u201cCan you work under pressure?\u201d Was next in line,\n\nShe answered \u201cYes, I can. And how? Let me define.\u201d\n\n\u201cI\u2019ve been handling pressure since I was eight where it all started with how to make the best braid. Compete with the neighbors to get better grades,\n\nOr listen to the relatives and learn how to behave.\n\nTo be at your best in sports and studies,\n\nand always stay away from your only buddies.\n\nPick the right subject and attend the right college, how to be all, is beyond my knowledge.\n\nLearn how to cook and be under control,\n\nas you have to be good in handling household.\n\nDon\u2019t shout, don\u2019t drink,\n\nlate night party?\n\nDon\u2019t even think!\n\nStudy hard to be a doctor or an engineer,\n\nas life is all about proving yourself to your peers. Learn to save for your retirement,\n\nas there is always enough time for your enjoyment.\n\nYou are young to make decisions,\n\nbut old enough to run a family with precision.\n\nIt seems like being a girl is crime\n\nand yet we know how to make lemonade\n\nout of those lime.\n\nI know how to be a Wife, a sister,\n\nA daughter and a mother,\n\nSo how difficult it is\n\nto be a coworker.\n\nAnd hence I know\n\nI can handle pressure.\u201d\n\nAnd then my friends, she later announced with pleasure,\n\n\u201cOut of all the job seekers he hired me as his new Product Speaker\u201d."
    },
    {
        "url": "https://medium.com/@neuralnets/statistical-data-visualization-series-with-python-and-seaborn-for-data-science-5a73b128851d?source=user_profile---------2----------------",
        "title": "Statistical Data Visualization Series with Python and Seaborn for Data Science",
        "text": "\u201cData is the new Oil\u201d is a pretty infamous proverb that has been floating around on the web lately. In this era of Big Data with zettabytes of information is being shared across the network as this new Oil and we know how rapidly this volume is increasing every single day. Hence for any business to succeed, it is very intrinsic to extract meaningful insights from the data being generated across their platform, be it web or mobile. This demand is extremely high as of today and with the development happening across the globe, we\u2019re pretty sure that any professional in Technology domain with Data Science skills will comfortably sustain for next few decades. Simultaneously, I am also glad to see the revolution in Education domain where experienced professionals are coming forward to share their knowledge with newbies on various platforms.\n\nSomehow I believe that the value of Data Visualization in the Data Science toolkit is constantly being undermined. Data Science is not all about super cool predictive algorithms and instead in itself \u201cData Science is a life cycle\u201d. No matter how well you have pre-processed your data, if you cannot project the inference on to a screen, it won\u2019t make stakeholders happy because they don\u2019t care about your code. Irrespective of how well your algorithm actually predicts, unless you can project those numbers efficiently on screen for them to visualize, oh boy, for you \u201cWinter has come\u201d so get ready to face the Night King. Human brain perceives best, what it can visually comprehend to & if stakeholder/clients could have done that, Harvard Business wouldn\u2019t have ever rated Data Science as the sexiest job of the 21st century.\n\nThis is a facet of Data Science which requires an equal amount of dedicated hours for learning, because be it in an interview or in a board room meeting, just projecting a Stripplot on top of Swarmplot with few nice colors isn\u2019t going to solve your purpose, if you want their $120,000. And in this process, Statistics plays a very important role, for which you HAVE TO read a book, irrespective of format (Paperback, PDF, whatever). If nothing else, at least read ISLR (Introduction to Statistical Learning), which is a very foundation level book with minimal concepts to kick-start your journey. I shall also add few other resources for reference at the end of this post or in next of this series.\n\nIf you think all of this is not important because what matters is how quickly you become an Avenger and save some city, then without using Google, answer this inane question: \u201cWhat does Box in a Boxplot represent and what are those lines within it?\u201d If you got the answer right, \u201cBravo! You saved your next 5 minutes for something better than reading this post!\u201d & if you couldn\u2019t, then you\u2019re doomed (just kidding!). Lack of such visualization skills isn\u2019t generally with working professionals, but more with the beginners because MOOCs they undertake breeze through Data Visualization topics as if they never even existed, so here is my attempt to make things little better for those who struggle inferring plots.\n\nBefore I start plunging more into the actual Viz discussion, let me spend a minute validating the choice of Seaborn. There are two main reasons for this: > One, that I am biased, and > Second, it doesn\u2019t really matter which package you use, provided you know how to use whatever you choose and subsequently able to explain your usage to others. With Python, Matplotlib is the \u201cgrandfather\u201d of visualization and when topped with Seaborn, the visual helps us easily decipher anything being communicated. Though Seaborn isn\u2019t a core scientific computation package like SciPy, but is extremely efficient with Statistical Data Graphics. Hence, Seaborn is an industry-wise accepted visualization technique and unless a Dashboard is required off me, this is always my personal favorite so below I encompass all the Seaborn offerings:"
    },
    {
        "url": "https://medium.com/@neuralnets/improving-efficiency-of-convolutional-nets-for-deep-learning-models-ff21a97b7a6b?source=user_profile---------3----------------",
        "title": "Improving \u201cEfficiency of Convolutional Nets\u201d for Deep Learning models",
        "text": "The latest trigger of Convolutional Neural Networks (CNN) has achieved spectacular results in challenging pre-set conventions on image recognition and object detection, remarkably elevating the interest of our community in these methodologies. Nevertheless, it is still remains ambiguous how effectively these CNN approaches stand against each other and with previous state-of-the-art facile delineations such as the Bag-of-Visual-Words and the ameliorated Fisher Vector. Numerous papers have conducted meticulous assessment of these new approaches, juxtaposing deep architectures and comparing them on a customary grounds, discerning and revealing intrinsic implementation details to establish sundry competent properties of CNN-based renditions.\n\nNotable observations in the process include: > Dimensionality of a CNN output layer can be reduced significantly without having an adverse effect on performance. > Certain facets of deep and shallow methods can be successfully apportioned. > Data augmentation techniques habitually applied to CNN-based methods can also be applied to shallow methods to gain an analogous performance boost. > Hierarchical Filter Groups act as an asset by adapting tree root structure, thus reducing computation cost and the number of parameters required. Let us breeze through these observations in general to trump the underlying concepts.\n\nA common characteristic of recent quantitative implementations is that they are largely handcrafted while keeping them relatively undemanding, incorporating dense sampling of local image patches, describing them by means of visual descriptors such as SIFT (Scale-Invariant Feature Transform), encoding them into a high dimensional representation before pooling them over an image. These networks comprise of several layers of non-linear feature extractors, and hence detailed as \u201cdeep\u201d neural network. The classical approaches on the other hand being termed as \u201cshallow\u201d network, such as IFV. For a quick overview, IFV is an order-less pooling encoder that describes a set of local descriptors with a single vector based on their first and second order statistics. Furthermore, while their structure is handcrafted, they contain a very large number of parameters learnt from data. When applied to standard image classification and object detection benchmark datasets such as ImageNet and ILSVRC, deep networks have demonstrated excellent performance which are appreciably better than standard image encoding.\n\nPrevious approaches on diminishing computational complexity of CNNs have focused on approximating convolutional filters in the spatial (as opposed to channel) domain, either by using Low-rank approximations for Conditional Feedforward or Fourier Transformation based convolution network. More general methods have used Reduced/limited precision number representations or compression of previously trained models. Advanced research started adapting methods that reduce computational impact of large number of filter channels within state-of-the art neural networks by decreasing the number of incoming connections to respective nodes, among many others. Above bench-marking is done on VOC-2007 edition dataset that contains around 10,000 images split into train, validation, and test sets; and labelled with twenty object classes. A one-vs-rest SVM classifier for each class is learnt and evaluated independently and the performance is measured as mean Average Precision (mAP) across those classes."
    },
    {
        "url": "https://medium.com/@neuralnets/mn-bb74898e1a79?source=user_profile---------4----------------",
        "title": "PythonAnywhere: Access to Python & Web deployment on a Web browser",
        "text": "We can easily sign up for a free plan on PythonAnywhere (Beginner Account) giving us access to 500 MB disk space. Later on if we want more server resources, we may upgrade to one of the paid plans.\n\nOnce we have signed up and logged in, go to the Consoles menu item and then click on Bash. That will open our server terminal which is an interface that lets us interact and use our server via Linux commands. Spoiler: There\u2019s no GUI (graphical interface). The terminal lets us do everything like installing Python packages, creating, writing, editing and deleting files, knowing what directory we are in, etc. For example, if you type and press Enter, we would get as output something like which is our current directory in the server.\n\nLet us create a new directory: . Let us now look at the list of files & folders within our current directory by typing: . That should output something like: . Now we shall change our current directory to our sample directory: . And we can check to see where we are by pressing: . So, we\u2019re now inside our project \u2018mysample\u2019, and now we shall create our first Python file in here: . This command will open a nano text editor where we can write our Python code. Let us type something conventional such as print(\u2018Hello World!\u2019) and then press to exit, to save and then Enter. Try once again and we can see our new file right there. We can now run our Python program with: . This will output our program in the terminal, and also, if we want to run a Python shell, we may execute: to open the interactive Python shell just like it does on our local computer. Once done with the shell, run to return to the terminal.\n\nIf we have some Python script or other files already in our computer and wish to upload them to our PythonAnywhere, we can go back to the Dashboard and then go to Files. There we should be able to upload a file to our server. Utmost intrinsic are Python packages that we so heavily depend upon, and to install them using pip3, we just need to run our normal command in the terminal: . Although you would notice that Python Anywhere already comes with many useful libraries pre-installed like Numpy, Pandas and Scikit-Learn. To uninstall: .\n\nLet us now discuss how to have a Python script run at a particular time every day while we\u2019re sleeping? We may have a script that needs to extract data from a website every day, or a script that sends an email to someone, or any other script that we just want to run automatically periodically. PythonAnywhere gives us 500 mb of disk space where to run Python, create and store files, etc. so let us make some good use of it. Following simple steps shall schedule our Python script for execution every day at a certain time:\n\nNow without worrying about time, let us make it simpler:\n\nThis simple code creates a text file and writes the string \u2018hello world\u2019 in that text file. Name of our text file will be the current date and time. Every time our script gets executed, a new text file with a different name is generated. So it shall return us a text file every day. We may also double check presence of our file in Dashboard -> Console -> Bash -> Linux Terminal -> Execute . Our Python file and the text generated text file can be found here.\n\nAll we need is a working Flask app that can run on localhost on our computer, & we can easily make it public by deploying it on PythonAnywhere. Merely a couple of minutes to deploy so hang on tight. Let us cruise step-by-step:\n\nAnd we\u2019re DONE! We can visit our web app on . That is the URL of our app where we shall see something like \u2018Hello from Flask\u2019. But, if we want to upload our own project, then there are a few more steps to follow:\n\nPlease note that if our project gets bigger in size, we may have to upgrade for one of the PythonAnywhere paid membership plans (listed below):"
    },
    {
        "url": "https://medium.com/@neuralnets/linear-algebra-for-data-science-revisiting-high-school-9a6bbeba19c6?source=user_profile---------5----------------",
        "title": "Linear Algebra for Data Science | Revisiting High School",
        "text": "It gives the sum of all diagonal entries of a matrix. Some operations that are difficult to specify without resorting to summation notation can be specified using matrix products and the trace operator.\n\nWriting an expression in terms of the trace operator enables to manipulate the expression using various useful identities. Note that a scalar is its own trace: a = Tr(a ).\n\nThe Determinant: The determinant of a square matrix, denoted det(A ), is a function that maps matrices to real scalars. The determinant is equal to the product of all the eigenvalues of a matrix. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts space. If the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all its volume. If the determinant is 1, then the transformation preserves volume.\n\nI shall now conclude this recap of Linear Algebra fundamentals, required to advance in Data Science & Machine Learning. This certainly isn\u2019t all that we need to know but can definitely act as a stepping stone. Unless we straightaway urge to be a Data Science unicorn, this should be enough to begin with. Kindly keep me posted if you find any discrepancy in this post. Good Luck and below are few other resources that you would like to check out for other relevant topics:"
    },
    {
        "url": "https://medium.com/@neuralnets/elementary-statistical-terms-for-data-science-interviews-212d931ca57d?source=user_profile---------6----------------",
        "title": "Elementary Statistical Terms for Data Science Interviews",
        "text": "There are 3 measures of Central Tendency \u2022 Mean (average) = Sum of all the values divided by the number of values. Least Robust \u2022 Median (\u201cmiddle\u201d value) = The value that is in the middle when all of the values are arranged in ascending order. If there is an even number of values there is no single middle value. Therefore, we take the arithmetic average of the two middle values. Robustness is in between mean and mode \u2022 Mode (most frequent value) = Value that appears the highest number of times. Most Robust.\n\nIf you are given the list of values 1, 3, 3, 5, 7, 10. What is the mean, median, and mode? Mean = (1+3+3+5+7+10) / 6 = 4.83, Median = average of the 2 middle values since there is an even number of values. (3+5)/ 2 = 4 & Mode = most frequent value = 3.\n\nIf we take the list of values from the previous question but now add an additional value of 100 how does the mean, median, and mode change? Mean= (1+3+3+5+7+10+100) / 7 = 18.43, Median = middle value = 5 & Mode= most frequent value = 3."
    },
    {
        "url": "https://medium.com/@neuralnets/back-propagation-algorithm-and-bias-neural-networks-fcf68b5153?source=user_profile---------7----------------",
        "title": "Back-propagation Algorithm and Bias | Neural Networks",
        "text": "In the deployment of the back-propagation algorithm, each iteration of training involves the following steps: 1) a particular case of training data is fed through the network in a forward direction, producing results at the output layer, 2) Cost (Error) is calculated at the output nodes based on known target information, and the necessary changes to the weights that lead into the output layer are determined based upon this error calculation 3) The changes to the weights that lead to the preceding network layers are determined as a function of the properties of the neurons to which they directly connect (weight changes are calculated, layer by layer, as a function of the errors determined for all subsequent layers, working backward toward the input layer) until all necessary weight changes are calculated for the entire network.\n\nThe calculated weight changes are then implemented throughout the network, the next iteration begins, and the entire procedure is repeated using the next training pattern. In the case of a neural network with hidden layers, the back-propagation algorithm is given by the following three equations (modified after Gallant, 1993), where i is the \u2018emitting\u2019 or \u2018preceding\u2019 layer of nodes, j is the \u2018receiving\u2019 or \u2018subsequent\u2019 layer of nodes, k is the layer of nodes that follows j (if such a layer exists for the case at hand), ij is the layer of weights between node layers i and j, jk is the layer of weights between node layers j and k, weights are specified by w, node activations are specified by a, delta values for nodes are specified by d, subscripts refer to particular layers of nodes (i, j, k) or weights (ij, jk), \u201csub-subscripts\u201d refer to individual weights and nodes in their respective layers, and epsilon is the learning rate:\n\nBeing based on the generalized Delta Rule, it is not surprising that Equation 1 has the same form as equation Delta Rule Learning. It states that the change in a given weight m located between layers i and j is equal to the products of: 1) Learning Rate (epsilon) 2) Delta value for node p in layer j [where node p is the node to which the vector associated with weight m leads], and 3) the Activation of node q in layer i (where node q is the node from which the vector associated with weight m leads). In practice, the Learning Rate (epsilon) is typically given a value of 0.1 or less; higher values may provide faster convergence on a solution, but may also increase instability and may lead to a failure to converge (Gallant, 1993). The delta value for node p in layer j in Equation 1 is given either by Equation 2 or by Equation 3, depending on the whether or not the node is in an output or intermediate layer.\n\nEquation 2 gives the delta value for node p of layer j if node p is an output node. Together, Equations 1 and 2 were derived through exactly the same procedure as Delta Learning rule, with the understanding that a sigma (or sigmoid) activation function is used here instead of a simple linear activation function (use of a different activation function will typically change the value of d). Both sets of equations were determined by finding the derivative of the respective error functions with respect to any particular weight. Equation 3 gives the delta value for node p of layer j if node p is an intermediate node (i.e., if node p is in a hidden layer). This equation states that the delta value of a given node of interest is a function of the activation at that node (aj sub p), as well as the sum of the products of the delta values of relevant nodes in the subsequent layer with the weights associated with the vectors that connect the nodes. Details on the derivation of equation 3, which applies to intermediate nodes, are given in Reed and Marks (1999, pp.53\u201355) and Richards and Jia (2005). Proof that the back-propagation algorithm actually performs a gradient descent to minimize error is given by e.g. Gallant (1993, pp.217\u2013219).\n\nMost implementations of this algorithm employ an additional class of weights known as Biases. \u201cBiases are values that are added to the sums calculated at each node (except input nodes) during the feed-forward phase.\u201d That is, the bias associated with a particular node is added to the term Sj in:\n\nprior to the use of activation function at that same node. The negative of a bias is sometimes called a Threshold (Bishop, 1995a). For simplicity, biases are commonly visualized simply as values associated with each node in the intermediate and output layers of a network, but in practice are treated in exactly the same manner as other weights, with all biases simply being weights associated with vectors that lead from a single node whose location is outside of the main network and whose activation is always 1 (Refer image below).\n\nThe change in a bias for a given training iteration is calculated like that for any other weight [using Equations 1, 2, and 3], with the understanding that ai sub m in Equation 1 will always be equal to 1 for all biases in the network. The use of biases in a neural network increases the capacity of the network to solve problems by allowing the hyperplanes that separate individual classes to be offset for superior positioning. More specific discussions on the utility of biases in neural networks are given by, e.g., Gallant (1993, pp.65\u201366), Bishop (1995a, p.78), and Reed and Marks (1999, pp.15\u201317).\n\nKindly keep me posted if you observe any discrepancy in the information provided in this article. If you wish to learn in more detail about the Delta Learning Rule, then please also go through this article. Hope it helps and Thank You for your time!"
    },
    {
        "url": "https://medium.com/@neuralnets/beginners-quick-guide-for-handling-issues-launching-jupyter-notebook-for-python-using-anaconda-8be3d57a209b?source=user_profile---------8----------------",
        "title": "Beginner\u2019s Quick Guide for handling issues launching Jupyter Notebook for Python using Anaconda\u2026",
        "text": "Jupyter Notebook (formerly, IPython Notebook) is a widely used application in Data Science domain for creating and sharing documents that contain Live code, Equations, Visualizations and Explanatory text. It is a server-client application that allows editing and running notebook documentsvia any web browser of our choice; and can be executed on a local desktop requiring no internet access or can be installed on a remote server and accessed through the internet. It also has a Dashboard (Notebook Dashboard) and a control panel, showing local files and allowing to open notebook documents or shutting down their kernels. We may also run Jupyter Notebookwithout any installation.\n\nThe weblink associated with above screenshot will direct to Jupyter official webpage where they have exhaustively guided step-by-step for installation. We shall be focusing on getting Jupyter Notebook up and running through Anaconda Distribution from Contiuum Analytics. Our foremost agenda (for beginners and who are very new) will be to ensure that we uninstall (How to do is at the end of this article) any other Python IDEs (like PyCharm, Python IDLE, etc.) if we have installed on our computer and ensure that no such related folders are left out. Also ensure that these Python IDEs don\u2019t have their PATH added in Environment Variables.\n\nOnce we have a clean system, our next step shall be to download Anaconda (being a beginner, we shall avoid Miniconda) distribution. For this process, we need to be aware of 2 critical information about the Operating System on our computer: > Type (Windows/Macintosh/Linux), and > Architecture(32/64 bit). Another important thing to note is the variety of available packages: > Anaconda 2 (Download this only if you intending to use Python \u2264 2.7 ), and > Anaconda for Python 3 (Recommended | Create virtual environments to use Python 2).\n\nAs we may notice in Image 1 (above screenshot), this is a huge file (500+ MB) so it will take sometime to download, depending upon our internet connection speed. Please note if we trying to download it at our workplace & our corporate security settings do not allow us (Admin/User permission errors) to download an executable file, then alternatively we may download their zipped file which contains all the executable files within. Once downloaded we may proceed towards installation (recommended to right click and initiate as an Administrator), keeping in mind that if we get any prompts like:\n\nIn Image 3, during installation we ensure that the folder/directory where Anaconda gets installed (in layman terms), has that path added to our computer/system. Also, please ensure that the Directory/Folder name has no spaces in it (not something like \u201c \u201d). Now, when we execute our commands (e.g.- ), our Windows Command Prompt (on PC) or Terminal (on Macintosh) recognizes the actual path of the Anaconda directory and accordingly installs the module (Numpy in our example). Later on this will fortify proper execution of commands like in our Jupyter Notebook (or even in Spyder) that we would always require in our Python script/code. Here is Anaconda Quick Start Guide(for installation/running help) & Anaconda Cheat Sheet (List of navigating commands) in PDF format for anytime assistance. There is a list of frequent Q&A in their official documentation. As a quick note, once we accept the default option to install Anaconda on the default path (as suggested), it gets installed in our user home directory:\n\nIf we\u2019re unable to find Anaconda Navigator in our Search bar, visit the installation folder and head to . If we don\u2019t find file out there, this suggests that we haven\u2019t properly installed Anaconda and hence it is missing."
    },
    {
        "url": "https://medium.com/@neuralnets/delta-learning-rule-gradient-descent-neural-networks-f880c168a804?source=user_profile---------9----------------",
        "title": "Delta Learning Rule & Gradient Descent | Neural Networks",
        "text": "The development of the perceptron was a big step towards the goal of creating useful connectionist networks capable of learning complex relations between inputs and outputs. In the late 1950\u2019s, the connectionist community understood that what was needed for further development of connectionist models was a mathematically-derived (and thus potentially more flexible and powerful) rule for learning. By early 1960\u2019s, the Delta Rule [also known as the Widrow & Hoff Learning rule or the Least Mean Square (LMS) rule] was invented by Widrow and Hoff. This rule is similar to the perceptron learning rule by McClelland & Rumelhart, 1988, but is also characterized by a mathematical utility and elegance missing in the perceptron and other early learning rules.\n\nThe Delta Rule uses the difference between target activation (i.e., target output values) and obtained activation to drive learning. For reasons discussed below, the use of a threshold activation function (as used in both the McCulloch-Pitts network and the perceptron) is dropped & instead a linear sum of products is used to calculate the activation of the output neuron (alternative activation functions can also be applied). Thus, the activation function is called a Linear Activation function, in which the output node\u2019s activation is simply equal to the sum of the network\u2019s respective input/weight products. The strength of network connections (i.e., the values of the weights) are adjusted to reduce the difference between target and actual output activation (i.e., error). A graphical depiction of a simple two-layer network capable of deploying the Delta Rule is given in the figure below (Such a network is not limited to having only one output node):\n\nDuring forward propagation through a network, the output (activation) of a given node is a function of its inputs. The inputs to a node, which are simply the products of the output of preceding nodes with their associated weights, are summed and then passed through an activation function before being sent out from the node. Thus, we have the following:\n\nwhere \u2018Sj\u2019 is the sum of all relevant products of weights and outputs from the previous layer i, \u2018wij\u2019 represents the relevant weights connecting layer i with layer j, \u2018ai\u2019 represents the activation of nodes in the previous layer i, \u2018aj\u2019 is the activation of the node at hand, and \u2018f\u2019is the activation function.\n\nFor any given set of input data and weights, there will be an associated magnitude of error, which is measured by an error function (also known as a cost function) (e.g., Oh, 1997; Yam and Chow, 1997). The Delta Rule employs the error function for what is known as Gradient Descent learning, which involves the \u2018modification of weights along the most direct path in weight-space to minimize error\u2019, so change applied to a given weight is proportional to the negative of the derivative of the error with respect to that weight (McClelland and Rumelhart 1988, pp.126\u2013130). The Error/Cost function is commonly given as the sum of the squares of the differences between all target and actual node activation for the output layer. For a particular training pattern (i.e., training case), error is thus given by:\n\nwhere \u2018Ep\u2019 is total error over the training pattern, \u00bd is a value applied to simplify the function\u2019s derivative, \u2019n\u2019 represents all output nodes for a given training pattern, \u2018tj\u2019 sub n represents the Target value for node n in output layer j, and \u2018aj\u2019 sub n represents the actual activation for the same node. This particular error measure is attractive because its derivative, whose value is needed in the employment of the Delta Rule, and is easily calculated. Error over an entire set of training patterns (i.e., over one iteration, or epoch) is calculated by summing all \u2018Ep\u2019:\n\nwhere \u2018E\u2019 is total error, and \u2018p\u2019 represents all training patterns. An equivalent term for E in earlier equation is Sum-of-squares error. A normalized version of this equation is given by the Mean Squared Error (MSE) equation:\n\nwhere \u2018P\u2019 and \u2019N\u2019 are the total number of training patterns and output nodes, respectively. It is the error of both previous equations, that gradient descent attempts to minimize (not strictly true if weights are changed after each input pattern is submitted to the network (Rumelhart et al., 1986: v1, p.324; Reed and Marks, 1999: pp. 57\u201362). Error over a given training pattern is commonly expressed in terms of the Total Sum of Squares (\u2018tss\u2019) error, which is simply equal to the sum of all squared errors over all output nodes and all training patterns. \u2018The negative of the derivative of the error function is required in order to perform Gradient Descent Learning\u2019. The derivative of our equation(which measures error for a given pattern \u2018p\u2019) above, with respect to a particular weight \u2018wij\u2019 sub \u2018x\u2019, is given by the chain rule as:\n\nwhere \u2018aj\u2019 sub \u2018z\u2019 is activation of the node in the output layer that corresponds to weight \u2018wij\u2019 sub x (subscripts refer to particular layers of nodes or weights, and the \u2018sub-subscripts\u2019 simply refer to individual weights and nodes within these layers). It follows that:\n\nThus, the derivative of the error over an individual training pattern is given by the product of the derivatives of our prior equation:\n\nBecause Gradient Descent learning requires that any change in a particular weight be proportional to the negative of the derivative of the error, the change in a given weight must be proportional to the negative of our prior equation . Replacing the difference between the target and actual activation of the relevant output node by d, and introducing a learning rate epsilon, that equation can be re-written in the final form of the Delta Rule:\n\nThe reasoning behind the use of a Linear Activation function here instead of a Threshold Activation function can now be justified: Threshold activation function that characterizes both the McColloch and Pitts network and the perceptron is not differentiable at the transition between the activations of 0and 1 (slope = infinity), and its derivative is 0 over the remainder of the function. Hence, Threshold activation function cannot be used in Gradient Descent learning. Whereas a Linear Activation function (or any other function that is differential) allows the derivative of the error to be calculated.\n\nPlease keep me posted if you find any discrepancy in equations. Also, if you wish to learn in detail about Back-propagation and Bias, please go through this article. Hope this helps you in understanding core concepts of Machine Learning and Thank you for your precious time!"
    },
    {
        "url": "https://medium.com/@neuralnets/probability-distribution-statistics-for-deep-learning-73a567e65dfa?source=user_profile---------10----------------",
        "title": "Probability Distribution | Statistics for Deep Learning",
        "text": "Mathematics is a topic that doesn\u2019t fascinate all so there are many of us who just wish to know the rudimentary Statistics to be in sync with what the instructor in our favorite MOOC is talking about. Every time we are not fortunate to have an intuition video before each new topic in our MOOCs. With no other option left, we generally resort to YouTube instead of textbooks because few of us are just not the \u2018textbook\u2019 type. Hence, I shall give you a synopsis of the fundamentals of Probability Distribution but I highly recommend you to cover Linear Algebra concepts for a smoother flow out here. Please don\u2019t get surprised if as a bonus you also get to have a flavor of Information Theory alongside. And you have my word on keeping Mathematics away from this mathematical topic as much as possible.\n\nMachine learning always deals with uncertain quantities, which are occasionally even stochastic (non-deterministic). Probability is a logic that provides formal rules to determine propositions as either True or False. Probability Theory is what allows us to make uncertain statements and to reason in the presence of uncertainty. And, to quantify the amount of uncertainty comes in Information Theory. This uncertainty can arise because of various factors like inherent stochasticity in the system being modeled, incomplete observability or even incompetent modelling. Hence, it is recommended to keep our rules simple & uncertain, irrespective of our model appetite. Often coined terms are Bayesian Probability, which just refers to qualitative levels of certainty, and Frequentist Probability, which directly relates to the rates at which an event occur.\n\nLet us now understand random variables, which only when coupled with Probability distribution can decide likelihood of each of the states (be it recurrent or transient). It is a variable that can take on different values randomly, thus depicting a description of all possible states. These random variables can be continuous when associated with a real value OR discrete when in the form of integers or just named states with any numerical value. Depending upon this type, a probability distribution describes likelihood of a random variable or a set of random variables to take on each of its possible states. A Probability distribution over discrete variables can be described using a PMF (Probability Mass Function), and over continuous variables using PDF (Probability Density Function). Also, when PMF acts upon multiple variables at the same time, it is termed as a Joint Probability Distribution. Please remember that for PMF or PDF to be applied, the domain needs to be a set of all possible states of random variables, and in any case cannot be less probable than 0.\n\nShifting our focus to other distributions, we have Marginal Probability that is distributed over subset of a set of random variables. For discrete random variables, we find it using the summation rule and for continuous we apply integration. Next is Conditional Probability of an event that depends upon some other event, and cannot be computed for an event that never happens. We need to be extremely careful by not confusing conditional probability with Intervention query (an action would happen if some other action was performed). Already confused? Kindly allow me to help with an example: Conditional probability that Frank-Walter Steinmeier is from Germany, given that he often represents Germany is quite high. Next in our list is Chain/Product Rule, which is actually decomposition of a Joint probability distribution into Conditional probability over any one of its random variable.\n\nSimilar to us, even probability distributions also relish their Independence, which for them is a representation of product of two factors, with one involving only first random variable & other involving only second random variable. But these two variables would just be conditionally independentgiven a third random variable, if their conditional probability distribution factorizes for every value of the third variable. Let us check it\u2019s properties:\n\nWe are aware of those life-saving functions like Sigmoid and Softplus and shall get into them as well in due course but prior to that, for today we are going to focus on few of the terms associated with it. An Expected value of such a function with respect to it\u2019s probability distribution is actually its average (mean value), that the function takes on when a random variable is drawn from the distribution, and generally these values are linear. Variancegives the measure of disparity of function values, when we sample different values of a random variable from its distribution. Low variance indicates clustering near their expected value. And, square root of this variance is the Standard Deviation. Another intrinsic component is Covariance that indicates linear relation between each of the values, along with the scale of these variables. If the variables remain unaffected by the scale, then to check inter-relativity we use Correlation that normalizes the contribution of each random variable.\n\nHigh absolute values of Covariance mean that the values change a lot & are far from their respective mean. To present in a simpler way, if the sign of covariance is positive then both variables take relatively high values, & if negative, then any one variable takes high value. Covariance and Dependence are related but distinct concepts. For two variables to have zero covariance, there must not be a linear dependence between them. Independence is a much stronger requirement than zero covariance. In a Covariance matrix, the diagonal values present covariance.\n\nA probability distribution over a single binary random variable is the Bernoulli Distribution, fetching probability of a variable being equal to 1. The Multinoulli/Categorical Distribution is over a single discrete variable with finite different states. Note that both these probability distributions model only discrete variables, thus describing any distribution over their domain. The most commonly used distribution over real numbers is the Gaussian/Normal Distribution. An Isotropic Gaussian distribution is another type with covariance matrix scalar times the identity matrix. Another important concept is the Central Limit Theorem which shows that the sum of many independent variables is approximately normally distributed.\n\nWe don\u2019t want another text book. Right? So I will end this blog post right here with a commitment to return with a Part-2 of this topic very soon. Also, if you find any mistake or have any feedback, please do utilize the comments section. Thank you for your time and enjoy Machine Learning!"
    },
    {
        "url": "https://medium.com/@neuralnets/boltzmann-machines-transformation-of-unsupervised-deep-learning-part-2-cfb1dab81437?source=user_profile---------11----------------",
        "title": "Boltzmann Machines | Transformation of Unsupervised Deep Learning \u2014 Part 2",
        "text": "Deep Belief Networks (DBNs): In simple words, it is just the top two Hidden layer of nodes of a stacked RBM that forms DBN. It is an undirected associated memory from these top 2 layers and the remaining hidden layers form a directed acyclic graph that converts the representations in the associative memory into observable variables such as the pixels of an image. This hybrid model has some attractive features: > A fast, greedy learning algorithm that can find a fairly good set of parameters quickly, even in deep networks with millions of parameters and many hidden layers. >Unsupervised learning algorithm that can still be applied to labelled data by learning a model that generates both the label and the data. > A fine-tuning algorithm that learns an excellent generative model outperforming discriminative methods on the MNIST database of hand-written digits. >Generative model makes it easy to interpret the distributed representations in the deep hidden layers. > Inference required for forming a perceptron is both fast and accurate. > Learning algorithm is local: adjustments to a synapse strength depend only on the states of the pre-synaptic and post-synaptic neuron. > Neurons require to communicate only their stochastic binary states. | In terms of training Deep Belief Nets, there are two types of algorithms: Greedy Layer-wise Training & Wake-Sleep Algorithm.\n\nGreedy Layer-wise Training: The top two layers have undirected connections and form an associative memory and the layers below have directed, top-down, generative connections that can be used to map a state of the associative memory to an image. There are also directed, bottom-up, recognition connections that are used to infer a factorial representation in one layer from the binary activities in the layer below. In the greedy initial learning the recognition connections are tied to the generative connections. If this greedy algorithm changes the higher-level weight matrices, it is guaranteed to improve the generative model. The greedy algorithm can clearly be also applied recursively, so if we use the full maximum likelihood Boltzmann machine learning algorithm to learn each set of tied weights and then we untie the bottom layer of the set from the weights above, we can learn the weights one layer at a time with a guarantee to never decrease the log probability of the data under full generative model. In practice, we replace maximum likelihood Boltzmann machine learning algorithm by contrastive divergence learning because it works well and is much faster. The use of contrastive divergence voids the guarantee, but it is still reassuring to know that extra layers are guaranteed to improve imperfect models if we learn each layer with sufficient patience. To guarantee that the generative model is improved by greedily learning more layers, it is convenient to consider models in which all layers are of the same size so that higher level weights can be initialized to values learned before they are untied from the weights in the layer below. The same greedy algorithm, however, can be applied even when the layers are of different sizes as well.\n\nWake-Sleep Algorithm: An unsupervised learning algorithm for a multilayer network of stochastic neurons is assessed. Bottom-up recognition connections convert the input into representations in successive hidden layers and top-down generative connections reconstruct the representation in one layer from the representation in the layer above. In the \u2018wake\u2019 phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the \u2018sleep\u2019 phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above."
    },
    {
        "url": "https://medium.com/@neuralnets/boltzmann-machines-transformation-of-unsupervised-deep-learning-part-1-42659a74f530?source=user_profile---------12----------------",
        "title": "Boltzmann Machines | Transformation of Unsupervised Deep Learning \u2014 Part 1",
        "text": "Unlike task-specific algorithms, Deep Learning is a part of Machine Learning family based on learning data representations. With massive amounts of computational power, machines can now recognize objects and translate speech in real time, enabling a smart Artificial intelligence in systems. The concept of a software simulating the neocortex\u2019s large array of neurons in an artificial neural network is decades old, and it has led to as many disappointments as breakthroughs. But because of improvements in mathematical formulas and increasingly powerful computers, today researchers & data scientists can model many more layers of virtual neurons than ever before. As Peter Lee of Microsoft Research once said \u2014\n\nLanguishing through the 1970\u2019s, early neural networks could simulate only a very limited number of neurons at once, so they could not recognize patterns of great complexity. In the mid 1980\u2019s, Geoffrey Hinton and others helped spark an amelioration in neural networks with so-called deep models that made better use of many layers of software neurons. But the technique still required heavy human involvement as programmers had to label data before feeding it to the network and complex speech/image recognition required more computer power than was then available. Just to have a feel of requirements against cost, look at the representation below:\n\nHowever in 2006, Hinton developed a more efficient way to teach individual layers of neurons where the first layer learns primitive features, like an edge in an image or the tiniest unit of speech sound by finding combinations of digitized pixels or sound waves that occur more often than they should by chance. Once that layer accurately recognizes those features, they\u2019re fed to the next layer, which trains itself to recognize more complex features, like a corner or a combination of speech sounds. The process is repeated in successive layers until the system can reliably recognize phonemes or objects and this is what forms the base of Supervised Deep Learning models like Artificial/Convolutional /Recurrent Neural Networks.\n\nEven prior to it, Hinton along with Terry Sejnowski in 1985 invented an Unsupervised Deep Learning model, named Boltzmann Machine. This model is based on Boltzmann Distribution (also known as Gibbs Distribution) which is an integral part of Statistical Mechanics and helps us to understand impact of parameters like Entropy and Temperature on Quantum States in Thermodynamics.\n\nHinton once referred to illustration of a Nuclear Power plant as an example for understanding Boltzmann Machines. This is a complex topic so we shall proceed slowly to understand intuition behind each concept, with minimum amount of mathematics and physics involved. So in simplest introductory terms, Boltzmann Machines are primarily divided into two categories: Energy-based Models (EBMs) and Restricted Boltzmann Machines (RBM). When these RBMs are stacked on top of each other, they are known as Deep Belief Networks (DBN). These DBNs are further sub-divided into Greedy Layer-Wise Training and Wake-Sleep Algorithm. There is also another type of Boltzmann Machine, known as Deep Boltzmann Machines (DBM). Unless we\u2019re involved with complex AI research work, ideally stacked RBMs are more than enough for us to know, and that gets taught in all the Deep Learning MOOCs. The most common use-case for RBMs are Advanced Recommender Systems so if you preparing for an interview in companies like AirBnB, Amazon, eBay and Netflix, then it is time to get extra attentive.\n\nInstead of specific model, let us begin with layman understanding of general functioning in a Boltzmann Machine as our preliminary goal. A Boltzmann Machine is a stochastic (non-deterministic) or Generative Deep Learning model which only has Visible (Input) and Hidden nodes. You got that right! There is no Output node in this model hence like our other classifiers, we cannot make this model learn 1 or 0 from the Target variable of training dataset after applying Stochastic Gradient Descent (SGD), etc. Exactly similar case with our regressor models as well where it cannot learn the pattern from Target variables. These attributes make the model non-deterministic. Thinking of how does this model then learn and predict, is that intriguing enough? To break the ice, kindly allow me to explain functioning of Boltzmann Machines.\n\nImage presents six nodes in it and all of them are inter-connected, and are also often referred to as States. Grey ones represent Hidden nodes (h)and white ones are for Visible nodes (v). Flashback in your own medial temporal lobe shall tell you that A/C/R Neural networks never had their Input nodes connected, whereas Boltzmann Machines have their inputs connected & that is what makes them fundamentally different. All these nodes exchange information among themselves and self-generate subsequent data, hence termed as Generative deep model. Here, Visible nodes are what we measure and Hidden nodes are what we don\u2019t measure. When we input data, these nodes learn all the parameters, their patterns and correlation between those on their own and forms an efficient system, hence Boltzmann Machine is termed as an Unsupervised Deep Learning model. This model then gets ready to monitor and study abnormal behavior depending on what it has learnt. This model is also often considered as a counterpart of Hopfield Network, which are composed of binary threshold units with recurrent connections between them."
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/curated-list-of-100-data-science-resources-for-practitioners-3c0510ed47a3?source=user_profile---------13----------------",
        "title": "Curated List of 100+ Data Science resources for practitioners",
        "text": "Walking in dark might be fun sometimes but not when we have an agenda in mind. So, it is better to be hawk-eyed on the list of resources mentioned below and I shall keep updating it every now and then. Currently this list is jumbled up but I shall very soon segregate it into Categories, as it has various top-rated blogs/authors, ML/AI expert papers and links to terabytes of data repositories. Let\u2019s now embrace all that we have been gifted with till 2017:\n\nIf any of this helps you, please try to drop in a Thank You note to respective authors on their blogs, if feasible. And, a few claps out here from you can keep me boosted to regularly update this post. Have a wonderful new year ahead!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/setting-up-gpu-powered-google-cloud-vm-deep-learning-7e67ec5e40da?source=user_profile---------14----------------",
        "title": "Setting up GPU powered Google Cloud VM | Deep Learning",
        "text": "Most of us are not privileged enough to have a GPU powered machine to train our Deep Neural networks on. And this is where cloud hosted Virtual Machine (VM) gets into picture. Recently I came across an article that reflected on lack of resources for guiding beginners to access GPU powered Google cloud services. So my sole purpose in this blog is to demonstrate setting up a GPU powered VM on Google cloud platform & not training a model on top of it.\n\nThere couldn\u2019t have been a better day for drafting this post, as today Google has announced 36% price reduction on using NVIDIA\u2019s Tesla GPUs. Before we proceed, it is intrinsic to note that we shall be billed on per-second basis for this deployment. In U.S. regions, generally available Tesla K80 GPUs will cost $0.45 per hour while using the newer and more powerful beta Tesla P100 machines will cost $1.46 per hour.\n\nI will constantly keep adding screenshots for people who are not familiar with Google Cloud platform (GCP). The very first time we create our account in GCP, it allocates a credit of $300.00 for free trial and is valid for an year. Please be careful with your choices in the portal because the moment the meter goes beyond the free trial, your credit/debit card will start getting billed.\n\nAnother associated good news is that Google has also reduced the price for preemptible local SSDs by almost 40 percent. Enough of background & introduction, so now let us get started to feel the thrill. Our very first step would be to create a VM and your console should look like:\n\nOnce we click on \u2018Create\u2019, console will populate multiple hardware & networking parameters for our instance to choose from. This completely depends upon our requirement and will amount to our billing invoice so please do not get carried away while selecting hardware options. Please do remember to keep your CPU platform as Intel Haswell or later. Following screenshots showcase the options (can be extended) available:\n\nOnce the instance is ready, Internal and External IP Addresses shall be displayed for your use. Apart from your VM Console, these IP Addresses shall also be available in \u2018Networking\u2019 tab, like:\n\nAnd, we have a GPU powered Virtual Machine up and running. But since we are just experimenting with GPU-enabled machines, we shall set the scale tier to with a single NVIDIA Tesla K80 GPU. Our next step is to train our model that can be done using Cloud Machine Learning Engine. In case too many beginners still face issue in training a model on top of this VM, then please do keep me posted in Comments section and I will add that segment as well to this blog. Till then, enjoy machine learning!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/introduction-to-deep-learning-part-1-cbfda681c5ff?source=user_profile---------15----------------",
        "title": "Introduction to Deep Learning \u2013 Deep Learning \u2013",
        "text": "Actual challenge to artificial intelligence was solving the tasks that are easy for people to perform but hard for people to describe formally. Solution is to allow computers to learn from experience and understand the world in terms of a hierarchy of concepts, with each concept defined through its relation to simpler concepts. By gathering knowledge from experience, this approach avoids the need for human operators to formally specify all the knowledge that the computer needs. The hierarchy of concepts enables the computer to learn complicated concepts by building them out of simpler ones. Hence. this approach is termed as Deep Learning.\n\nA person\u2019s everyday life requires huge amount of knowledge about the world. Much of this knowledge is subjective and intuitive, and therefore difficult to articulate in a formal way. Computers need to capture this knowledge in order to behave in an intelligent way. One of the key challenges in artificial intelligence is how to get this informal knowledge into a computer. People struggle to devise formal rules with enough complexity to accurately describe the everyday world. The difficulties faced by systems relying on hard-coded knowledge suggest that AI systems need ability to acquire their own knowledge, by extracting patterns from raw data. This capability is known as Machine Learning.\n\nIntroduction of machine learning enabled computers to tackle problems involving knowledge of the real world and make decisions that appear subjective. The performance of simple machine learning algorithms depend heavily on the representation of data they are fed. This dependence on representations is a general phenomenon that appears throughout computer science and even daily life. Many artificial intelligence tasks can be solved by designing the right set of features to extract for that task, then providing these features to a simple machine learning algorithm, however, it is difficult to know what features should be extracted.\n\nOne solution to this problem is to use machine learning to discover not only the mapping from representation to output but also the representation itself. This approach is known as Representation Learning. Learned representations often result in much better performance than can be obtained with hand-designed representations. They also enable AI systems to rapidly adapt to new tasks, with minimal human intervention. A representation learning algorithm can discover a good set of features for a simple task in minutes, or for a complex task in hours to months.\n\nA brilliant example of a representation learning algorithm is an Autoencoder. An Autoencoder is the combination of an Encoder function, which converts input data into a different representation, and a Decoder function, which converts this new representation back into the original format. Autoencoders are trained to preserve as much information as possible when an input is run through the Encoder and then the Decoder, but they are also trained to make the new representation have various nice properties. Different kinds of Autoencoders aim to achieve different kinds of properties.\n\nWhen designing features or algorithms for learning features, our goal is to separate the factors of variation that explain the observed data. These factors indicate separate influencing sources & are not combined by multiplication. Either they are unobserved objects/forces in the physical world that affect observable quantities or constructs in human mind providing simplified explanations or inferred causes of the observed data. They are concepts or abstractions that help us make sense of the rich variability in the data.\n\nDeep learning solves this central problem in representation learning by introducing representations that are expressed in terms of other, simpler representations as it enables the computer to build complex concepts out of simpler concepts. Quintessential example of a deep learning model is the feed forward deep network, or multilayer perceptron (MLP). A multilayer perceptron is just a mathematical function formed by combining many simpler functions to map some set of input values to output values. Each application of a different mathematical function provides a new representation of the input.\n\nApart from learning right representation from data, another aspect is depth that enables computer to learn a multi-step program. Each layer of a representation is a state of the computer\u2019s memory after simultaneously executing another set of instructions & that empowers networks with greater depth to execute more instructions in sequence. Later instructions can refer back to the results of prior instructions, so all the information in a layer\u2019s activation don\u2019t necessarily encode factors of variation that explain the input. Representation also stores state information that helps to execute a program that can make sense of the input and keep model processing organized.\n\nDepth of a model can be viewed either based on number of sequential instructions (depth of Computational graph) OR based on correlation of concepts with each other (depth of Probabilistic modeling graph). Neither there is a single correct value for the depth of an architecture, nor is there a consensus about how much depth a model requires to qualify as \u2018deep\u2019. However, Deep Learning can be safely regarded as the study of models that involve a greater amount of composition of either learned functions or learned concepts than traditional machine learning does.\n\nEdit: Thanks to Jeff Clune who reminded me that I haven\u2019t credited original authors in here. This article is pretty much a subset of my learning from a recently (late 2017) published book Deep Learning (Adaptive Computation and Machine Learning series). This book has been one of the best investments that I have made recently and is highly recommended for beginner/professionals who wish to understand each and every concept of Deep Learning. It has been authored by few of the living legends in this domain, namely Yoshua Bengio, Ian Goodfellow and Aaron Courville; and published by MIT Press."
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/swish-activation-function-by-google-53e1ea86f820?source=user_profile---------16----------------",
        "title": "Swish Activation Function by Google \u2013 Deep Learning \u2013",
        "text": "The choice of activation functions in Deep Neural Networks has a significant impact on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU), which is f(x)=max(0,x). Although various alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. So Google Brain Team has proposed a new activation function, named Swish, which is simply f(x) = x \u00b7 sigmoid(x). Their experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging data sets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9% for Mobile NASNetA and 0.6% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.\n\nWith ReLU, the consistent problem is that its derivative is 0 for half of the values of the input x in ramp Function, i.e. f(x)=max(0,x). As their parameter update algorithm, they have used Stochastic Gradient Descent and if the parameter itself is 0, then that parameter will never be updated as it just assigns the parameter back to itself, leading close to 40% Dead Neurons in the Neural network environment when \u03b8=\u03b8. Various substitutes like Leaky ReLU or SELU (Self-Normalizing Neural Networks) have unsuccessfully tried to devoid it of this issue but now there seems to be a revolution for good.\n\nSwish is a smooth, non-monotonic function that consistently matches or outperforms ReLU on deep networks applied to a variety of challenging domains such as Image classification and Machine translation. It is unbounded above and bounded below & it is the non-monotonic attribute that actually creates the difference. With self-gating, it requires just a scalar input whereas in multi-gating scenario, it would require multiple two-scalar input. It has been inspired by the use of Sigmoid function in LSTM (Hochreiter & Schmidhuber, 1997) and Highway networks (Srivastava et al., 2015) where \u2018self-gated\u2019 means that the gate is actually the \u2018sigmoid\u2019 of activation itself.\n\nWe can train deeper Swish networks than ReLU networks when using BatchNorm (Ioffe & Szegedy, 2015) despite having gradient squishing property. With MNIST data set, when Swish and ReLU are compared, both activation functions achieve similar performances up to 40 layers. However, Swish outperforms ReLU by a large margin in the range between 40 and 50 layers when optimization becomes difficult. In very deep networks, Swish achieves higher test accuracy than ReLU. In terms of batch size, the performance of both activation functions decrease as batch size increases, potentially due to sharp minima (Keskar et al., 2017). However, Swish outperforms ReLU on every batch size, suggesting that the performance difference between the two activation functions remains even when varying the batch size.\n\nObviously, the real potential can be adjudged only when we use it for ourselves and analyze the difference. I find it simpler to use Activation functions in a functional way by defining a fn that returns x * F.sigmoid(x). This article at this moment is just an overview. Please feel free to implement Swish your way and do share your experience. Thank You for your time and enjoy Machine Learning!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/sniper-vision-human-resource-management-simplified-dc17ea443f59?source=user_profile---------17----------------",
        "title": "Sniper Vision | Human Resource Management Simplified",
        "text": "Human Resource professionals are the core of any business irrespective of how many times in a day we criticize them for their ineffectiveness with one or two of our tasks. Over the years managers have often had young talent approach them asking for mentoring because they want to work in Human Resources. These managers love to do that but only for right reasons and so has been Brian. So he has usually asked such questions: \u201cWhy do you want to work in HR?\u201d More often than not the answer goes something like this: \u201cI love working with people, developing them and helping them\u201d. To which he usually responds: \u201cIf that\u2019s what you want to do then you should work in operations or general management, not in HR.\u201d This is often a shocking response, but it\u2019s an honest one. The misconceptions that HR is a \u201cnice\u201d place to work because they work with people is pervasive, and often leads to the wrong kind of talent in the function.\n\nTo be fair, being nice is usually an expectation and requirement to be in HR. It\u2019s hard for most people to imagine their HR partners as not nice people. But I think this is where some young talent gets confused. They see \u201cnice\u201d HR colleagues and leaders, perceive that the role is all about helping people, and mistakenly assume that being a nice person is qualification enough for the function. However, \u201cnice\u201d is only a starting point \u2014 it is not nearly enough.\n\nRestructuring \u2014 Whenever there is an organizational restructure there are winners and losers. Dealing with the people that land on their feet is easy. But in any restructure there are those that lose their job, face demotions, or sometimes end up in a role that they don\u2019t like. These people deserve a respectful and \u201cfair\u201dprocess. Nice is just not enough. Recruiting \u2014 There are few things as enjoyable as telling somebody they got the job they were really hoping for. Unfortunately, for everybody that gets the job, there are many people who wanted it and didn\u2019t get it. It\u2019s not so fun to make those calls.\n\nCompensation is about paying people what the job is worth, not what they want. This often causes disagreement and friction. HR professionals must learn to explain facts and reality not only to employees at all levels, but also often to their managers who feel they should just be able to pay more. Sometimes HR\u2019s get to give great news in this regard, but more often they must find ways to keep integrity in the compensation structure. Talent management is about differentiating top talent and investing in them disproportionally. Delivering that news to the selected individuals can certainly be enjoyable. But for every top talent there are many who are not, and we often must explain why we have rewarded others differently.\n\nLearning & Development should be about giving people the training they need, not what they want. Labor/Employee relations is about ensuring they have a consistent and fair work environment, not to make everybody happy with their circumstances. Culture is about creating a great and/or effective working environment, not necessarily a nice environment. Great and nice aren\u2019t synonyms. It\u2019s not hard to see that the common perceptions that HR is an easy place to work, nice, or fun, are completely misguided. Of course, it can be fun. But when done well, it\u2019s difficult work. As Brian once mentioned:\n\nEmpathy is the Key \u2014 Actually what HR professionals really need is not niceness, but empathy. That is, understanding and taking into account how people feel. They must do the work, sometimes tough work, that their organizations need. Doing so with empathy, and helping other leaders have empathy, makes such a difference. As a function, they are often expected to give difficult news and feedback, or to help other leaders give such feedback. It\u2019s always better to give it in an empathetic way. Balance- HR professionals have to keep it all balanced if they want to maintain sanity. Balance in life is critical, otherwise it can become overwhelming and tempting to slide into nice for nice sake in order to avoid some of the tough work, which is not what organizations need. It\u2019s important to take a breath sometimes and keep it all in perspective. It is strategic and is also an art that must be practiced daily to be truly good at it. Helping and watching people grow is great but helping and watching the company grow through it\u2019s people is even more important and more fulfilling.\n\nSo, if you want to work in HR, please take a note of what\u2019s really required for success and make sure you are pursuing this career for the right reasons. If you already work in HR, keep perspective, and focus on what\u2019s most important. Have empathy, but do the right thing and don\u2019t be afraid to give the tough messages. Meanwhile from technology perspective, we are always there to help you with the best that we can to make your life little simpler. Hence, I have created this Predictive Machine Learning algorithm that almost anybody can use, provided you take care of couple of pointers. Entire code helps you predict if the employee can be a future attrition, or if you\u2019re paying him/her more than what the trend in the company has been so far. There are multiple other aspects that you can think of considered before formulating this algorithm in Python. The original set of data is right here for you to download and just keep adding your employee data in the same pattern after removing all existing data. And every time you want to predict behavior, just run the algorithm (don\u2019t worry! it has 97% accuracy). Please do take assistance of any software professional or post in comments for me to respond if you face issues in saving/running the file. A warm hug to all you amazing Human Resource professionals & entire code is available here. Please leave your claps as a token of appreciation if this works for you :)"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/demystifying-glasses-classifying-its-variants-2d92c4f022ad?source=user_profile---------18----------------",
        "title": "Demystifying Glasses | Classifying its variants \u2013 Deep Learning \u2013",
        "text": "Your days have been rough and exhaustive if you haven\u2019t come across a piece of glass unless you have started staying in a forest or have become a monk somewhere on the Himalayas. All of us have importance of glass associated in different forms, be it Champagne flute, Dressing table, or probably an environment friendly Greenhouse. What else can I say when even Jon Snow trusts in a dragon glass for killing supernatural humanoid White Walkers.\n\nGlasses may be devised to meet almost any imaginable requirement. There are many different types of glass with different chemical and physical properties and each can be made by a suitable adjustment to chemical compositions. For many specialized applications in chemistry, pharmacy, the electrical and electronics industries, optics, construction and lighting industries, glass, or the comparatively new family of materials known as glass ceramics, may be the only practical material for an engineer to use. The main types of glass include: 1. Borosilicate Glass 2. Commercial Glass 3. Glass Fibre 4. Lead Glass.\n\nLet us now consider one such data set based on which we shall prepare our classification model. For you to reproduce any such model, you can pull the code from my GitHub or just follow as per the code snippets that I have attached below. Entire flow is in Python so you would require an IDLE like Anaconda Spyder, PyCharm, etc. The model below is based on K Nearest Neighbor algorithm along with Elbow method to choose the best of \u2018k\u2019 values.\n\nNow, we have a model ready to use on similar test data sets and easily classify the type of glass. To better understand the pattern of , we can refer to the image below:\n\nInstead of Elbow Method, we could have also used Principal Component Analysis (PCA) for relating to individual components with it\u2019s impact on Glass type and I will demonstrate that as well very soon in one of my other blog. So, if you\u2019re a businessman who wants to have a handy algorithm to segregate the information readily, you can simple use Python Flask to wrap it in a Restful API and use it in your software to get going. Any query/feedback would be highly appreciated in Comments section. Good Luck segregating!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/natural-calamity-classifying-forest-fire-damage-c4139acfc009?source=user_profile---------19----------------",
        "title": "Natural Calamity | Classifying Forest Fire Damage \u2013 Deep Learning \u2013",
        "text": "Life is extremely precious, be it of humans, animals, birds or trees because all of us contribute to the existence of each-other. And one of the biggest threat to this existence is a natural calamity as it is pretty much unavoidable. Thousands of lives are lost and that certainly value much more than the property damage associated. One among them is a Forest Fire which destroys a forested area, and can be a great danger to people who live in forests, along with wildlife. Forest fires generally start by lightning, but also occasionally by human negligence or arson, and can burn thousands of square kilometers.\n\nThere is a separate segment of scientific study & research that is done to avoid such instances and to contribute from our end, we shall assess associated damage of one such occurrence. This data set was released by Paulo Cortez and Anibal Morais of Department of Information Systems, University of Minho, Portugal based on their findings of Meteorological Data in 2007. Portuguese experts confirmed an artificially intelligent model during 13th EPIA 2007 using Gaussian SVM with just 4 parameters to predict forest fire possibilities based on RMSE and MAD. This model works best for small fires but alternatively for forest fires affecting wider regions, we will try to approach differently.\n\nWe will set up a Dense Neural Network using TensorFlow Regressor to prepare our model to predict wide spread forest fires. Hence, we shall be considering multiple aspects of the available features to classify. Our network will composite of three layers associated with a hundred thousand steps to achieve better results, that we shall evaluate with RMSE of Scikit-Learn. By now, we all know that this specific model will be built using Python as our platform so no brownies for that. For convenience, I will add screenshots here along with my GitHub to access entire code for pulling across.\n\nAs expected, this shall fetch us Prediction values and we can even plot it against our original values to ensure that we\u2019re on the right path. Well, I will leave that part for you to explore with either Matplotlib, Seaborn, or any other library as per your choice. Please note that I have used TensorFlow which can be replaced by simpler Regression machine learning models as well. So without hesitation, if you would like to give it a shot, please go ahead. Good Luck predicting!"
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/the-wolf-of-wall-street-predicting-stock-prices-631bca4b4cbd?source=user_profile---------20----------------",
        "title": "The Wolf of Wall Street | Predicting Stock Prices \u2013 Deep Learning \u2013",
        "text": "Back in 2013, we all wanted to be in the shoes of Leonardo DiCaprio with stacks of money around us. Matter of fact is that there are people around us who does melt that kind of money from stock market every other day while we just keep daydreaming about it. Certainly, JP Morgan or Goldmann Sachs are not going to share their trade secrets with us in public forums or do we expect Warren Buffet to disclose his Bitcoin investment strategy. We know that is not going to happen so how about we take inspiration from them & try to make our own strategy to understand and predict stock market variations.\n\nAll we need to have is an inquisitive mind to understand the analogies as I paste the code to duplicate. We require all three dragons of Daenerys Targaryen, which in our case are: 1. Installing Dependencies 2. Collecting relevant data 3. Write magic lines of code. Once done, we can relax with our coffee mug while I explain the patterns of the variations. Let us drill down into 2016 data of Apple Inc. and Microsoft with this simple set of code:\n\nThis will fetch us an insight of the returns that investors received on their venture in 2016. If we wish to visualize it for some other year, or span of years, we just need to alter \u2018start\u2019 and \u2018end\u2019 parameters in this piece of code. And then we can have a nice colorful plot of insights like this:\n\nWith that part of analysis been done, we still have an important task in hand as we need to create value with the insight that is available. Thinking of actions that can be taken, we can do: 1. Sentiment Analysis on company opinions 2. Past Stock Prices 3. Dividend 4. Sales Growth. Knowing the fact that changes in stock prices are not at random, good traders utilize predictive Machine Learning models as a tool before making an investment. To make all of us efficient with this, allow me to furnish an easy-to-use code that I have drafted again in Python that just needs to be cloned to get desirable insight before making an investment. But this time, to avoid complexity for beginners, I will use data only for one company and build three models on top of it, so lets ask Apple Inc. to \u201cShow me the money!\u201d\n\nWe already have our libraries imported so let us start by creating two empty list that would contain our data: and , followed by this two simple functions that I will define before calling them to get predicted values:\n\nis the magic keyword for us at the end to get 3 different types of plot for associated 2017 stock value predictions. Let us have a closer look at this graphical representation:\n\nWe can easily figure out looking at this graph that it is the Radio basis function (rbf) kernel that best fits our data so we have our key right here to obtain best of predictions. I don\u2019t disagree with the efficient market hypothesis that states Stock prices are unpredictable BUT again it is better to have effective Machine Learning model predictions than just investing randomly. Good Luck investing!\n\nFor entire code or any suggestion/feedback to it, please visit my GitHub."
    },
    {
        "url": "https://medium.com/machine-learning-bootcamp/why-do-you-wish-to-be-a-data-scientist-a8f8d862c203?source=user_profile---------21----------------",
        "title": "Self Assessment | Why do we wish to be a Data Scientist?",
        "text": "All of us know and Google Trends have validated the fact that anyone even remotely associated with \u2018data\u2019 wants to be a \u2018Data Scientist\u2019. Be it professionals or high school students, this is the latest craze of technology world. There has been a huge surge in the number of people who have enrolled in one or the other eLearning portals like Udemy, Coursera, DataCamp, etc. Leaderships are ready to change their innovation strategy focusing more on churning of their used/unused data to get insights of every aspect of their present/future business model. Even small businesses like the restaurant in the city center downtown wants to strategically classify their guests and with all this buzz around, I literally needed to live on an isolated island to stay away from this. Allow me to begin with a thought-provoking analysis of aspiration agenda for this new generation aspirants by Eric Weber.\n\nPortraying myself as the protagonist in this aspiration scenario, I would like to start by saying that it all started couple of months back when I got fully convinced with the trending idea and decided to march to the same drum. Being a Data Analyst and also a DBA for sometime now, I realized that I have already been around petabytes of data till date, so the transition shouldn\u2019t be that difficult with my expertise in SQL, Excel and reporting tools like PowerBI, SAP BO, etc. Now my first assignment was to research what a Data Scientist does and what skill set do they require, where LinkedIn Jobs, Analytics Vidhya, KDNuggets, etc. proved to be a good resource. I started jotting down all the intrinsic skill sets and was very soon overwhelmed with the abundance of knowledge required to become a Data Scientist.\n\nSuddenly for next 2 days, I was swimming downstream before an article flashed in front of me \u2014 Data Scientist Skills & Salaries. Every coin has two sides, and we need to be extremely careful while choosing our\u2019s because I chose the wrong side, as the only thing I (unconsciously)focused on was the Data Scientist Salary, associated perks,recognition and a lavish life ahead. Tadaa! I was all set to begin my learning path (with gluttonous intentions). Evolving from a Finance background, I knew that it isn\u2019t going to be easy for me to learn all the tools required. Since I didn\u2019t know anyone around me with related skills, I chose DataCamp and Udemy for learning Python to begin with, and then moved to a detailed course on Data Science and Machine Learning to finish off what is required of me.\n\nMeanwhile, there was one good thing that I did, i.e. started connecting with Data Science experts & visionaries on LinkedIn. It was almost over a month now and I was struggling big time to cover all those topics included in the course along with my regular job. And there came a time when I quit studying for almost a week because I wasn\u2019t able to recollect most of what I had studied. This was extremely depressing for me because instead of coming closer to my king-size lifestyle and fancy perks, I was getting even more distant from it. I needed to transform myself so I took a week off from work and headed to my favorite getaway destination. For this entire week, I peacefully channelized all my energy into reassessing my adventure because I have never been a slacker. It is then that I realized that from the very start I chose wrong side of the coin.\n\nThe other side of the coin had a golden rule embedded in it \u2014 Interest leads to Passion and that feeds forward to knowledge, and then the rest automatically follows. Finally, I had a smile on my face and I kind of knew what needs to be done. I don\u2019t need to study to get a job because I already have it and am quite content with it. With reshuffled strategies, now my primary task was to decide what exactly am I passionate about, and luckily the answer remained to be quite on path \u2014 \u2018Divine power of Human Brain\u2019. Next self-assigned task was to understand my own brain & it\u2019s interest and that initiated with Machine Learning.\n\nAbruptly(for good), things automatically started changing for me. I finished that detailed course on Data Science with Python from Jose within next 3 weeks and I could remember almost everything from it. Additionally, I had been practicing heavily with data sets from online repositories. I have started taking little pride in my efforts, and these days it is all that matters to me. Actually Data Science isn\u2019t a topic for me to study anymore, it is a topic of interest that I have developed so this one course can\u2019t be the end because now I AM HUNGRY. Hence, I enrolled into extensive Machine Learning training from Kirill and Hadelin where they deeply cover every aspect of ML though they don\u2019t really get into core Mathematics (but do provide links to infamous research works by various other Data Science experts). With all the knowledge imparted by these amazingly talented instructors, it just requires little extra effort to understand the core mathematics and science behind every step. Jose was even kind enough to grant me access to his TensorFlow course free of cost.\n\nOn the other hand, LinkedIn has also been a great resource because experts like(to list a few) Daniel Tunkelang, Ben Taylor, Beau Walker, Carla Gentry, Brandon Rohrer, Eric Weber, Andy Kriebel, Andrew Trask and Matthew Mayo almost everyday share a lot of their knowledge, best approach and practical guidance which is immensely beneficent in segregating the best from the rest. There are many other resources freely available online for budding Machine Learning enthusiasts like me to learn from. Every individual (including experts) have a different perspective but going by whom I follow, I won\u2019t consider myself ready for the big game unless as Ben says \u201cHave you ever looked into Scikit-Learn behind-the-scene and tried to understand and then code it yourself\u2026.Have you made yourself uncomfortable?\u2026Have you LOVED Data Science enough?\u201d. Though when the going gets little tough with the overwhelming amount of knowledge, I prefer relaxing with what Brandon says \u201cBeing a Generalist is OK\u201d.\n\nSole intention is to make all these aspiring Data Scientists realize (with my own limited understanding) that we are on the right path but have we thought of \u2018Why are we on this path?\u2019. I realized it with my own mistakes that I cannot jump directly into the Data Science field because it has to be a natural progression.\n\nIn Data Science domain as well, there is a step-by-step progression that would lay the foundation for us. There is nothing more amazing than starting with a Data mining/analyst skill set, followed by Big Data, and by then we would automatically know what our actual strength is and which way can we head to achieve our goal. Research is the key to any problem in life and history has proved it every now & then, so exploit Stack Overflow, Quora, etc. or even better if we get a book (even an eBook) or Research papers on a subject and invest time in it.\n\nTime is Money so better to invest it wisely, instead of blindly following what others around us have been doing. Drill deep within ourselves and figure out our real passion. There are thousands of them taking the same online courses that we have been taking, so how are we different? Suppose, I am 29 and have a sibling who is 12, where my brother doesn\u2019t understand the implications of munching too many chocolates every day. Can I explain my inference model to him because in real world that is pretty much the case with stakeholders. So better we quickly assess which side of coin are we on and I wish good luck to everyone."
    }
]