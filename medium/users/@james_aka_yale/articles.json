[
    {
        "url": "https://medium.com/nanonets/how-to-do-image-segmentation-using-deep-learning-c673cc5862ef?source=user_profile---------1----------------",
        "title": "How to do Semantic Segmentation using Deep learning",
        "text": "Nowadays, semantic segmentation is one of the key problems in the field of computer vision. Looking at the big picture, semantic segmentation is one of the high-level task that paves the way towards complete scene understanding. The importance of scene understanding as a core computer vision problem is highlighted by the fact that an increasing number of applications nourish from inferring knowledge from imagery. Some of those applications include self-driving vehicles, human-computer interaction, virtual reality etc. With the popularity of deep learning in recent years, many semantic segmentation problems are being tackled using deep architectures, most often Convolutional Neural Nets, which surpass other approaches by a large margin in terms of accuracy and efficiency.\n\nIt is also worthy to review some standard deep networks that have made significant contributions to the field of computer vision, as they are often used as the basis of semantic segmentation systems:\n\nA general semantic segmentation architecture can be broadly thought of as an encoder network followed by a decoder network:\n\nUnlike classification where the end result of the very deep network is the only important thing, semantic segmentation not only requires discrimination at pixel level but also a mechanism to project the discriminative features learnt at different stages of the encoder onto the pixel space. Different approaches employ different mechanisms as a part of the decoding mechanism. Let\u2019s explore the 3 main approaches:\n\nThe region-based methods generally follow the \u201csegmentation using recognition\u201d pipeline, which first extracts free-form regions from an image and describes them, followed by region-based classification. At test time, the region-based predictions are transformed to pixel predictions, usually by labeling a pixel according to the highest scoring region that contains it.\n\nR-CNN (Regions with CNN feature) is one representative work for the region-based methods. It performs the semantic segmentation based on the object detection results. To be specific, R-CNN first utilizes selective search to extract a large quantity of object proposals and then computes CNN features for each of them. Finally, it classifies each region using the class-specific linear SVMs. Compared with traditional CNN structures which are mainly intended for image classification, R-CNN can address more complicated tasks, such as object detection and image segmentation, and it even becomes one important basis for both fields. Moreover, R-CNN can be built on top of any CNN benchmark structures, such as AlexNet, VGG, GoogLeNet, and ResNet.\n\nFor the image segmentation task, R-CNN extracted 2 types of features for each region: full region feature and foreground feature, and found that it could lead to better performance when concatenating them together as the region feature. R-CNN achieved significant performance improvements due to using the highly discriminative CNN features. However, it also suffers from a couple of drawbacks for the segmentation task:\n\nDue to these bottlenecks, recent research has been proposed to address the problems, including SDS, Hypercolumns, Mask R-CNN.\n\nThe original Fully Convolutional Network (FCN) learns a mapping from pixels to pixels, without extracting the region proposals. The FCN network pipeline is an extension of the classical CNN. The main idea is to make the classical CNN take as input arbitrary-sized images. The restriction of CNNs to accept and produce labels only for specific sized inputs comes from the fully-connected layers which are fixed. Contrary to them, FCNs only have convolutional and pooling layers which give them the ability to make predictions on arbitrary-sized inputs.\n\nOne issue in this specific FCN is that by propagating through several alternated convolutional and pooling layers, the resolution of the output feature maps is down sampled. Therefore, the direct predictions of FCN are typically in low resolution, resulting in relatively fuzzy object boundaries. A variety of more advanced FCN-based approaches have been proposed to address this issue, including SegNet, DeepLab-CRF, and Dilated Convolutions.\n\nMost of the relevant methods in semantic segmentation rely on a large number of images with pixel-wise segmentation masks. However, manually annotating these masks is quite time-consuming, frustrating and commercially expensive. Therefore, some weakly supervised methods have recently been proposed, which are dedicated to fulfilling the semantic segmentation by utilizing annotated bounding boxes.\n\nFor example, Boxsup employed the bounding box annotations as a supervision to train the network and iteratively improve the estimated masks for semantic segmentation. Simple Does It treated the weak supervision limitation as an issue of input label noise and explored recursive training as a de-noising strategy. Pixel-level Labeling interpreted the segmentation task within the multiple-instance learning framework and added an extra layer to constrain the model to assign more weight to important pixels for image-level classification.\n\nIn this section, let\u2019s walk through a step-by-step implementation of the most popular architecture for semantic segmentation \u2014 the Fully-Convolutional Net (FCN). We\u2019ll implement it using the TensorFlow library in Python 3, along with other dependencies such as Numpy and Scipy.\n\nIn this exercise we will label the pixels of a road in images using FCN. We\u2019ll work with the Kitti Road Dataset for road/lane detection. This is a simple exercise from the Udacity\u2019s Self-Driving Car Nano-degree program, which you can learn more about the setup in this GitHub repo."
    },
    {
        "url": "https://towardsdatascience.com/the-4-recommendation-engines-that-can-predict-your-movie-tastes-109dc4e10c52?source=user_profile---------2----------------",
        "title": "The 4 Recommendation Engines That Can Predict Your Movie Tastes",
        "text": "The 4 Recommendation Engines That Can Predict Your Movie Tastes\n\nRecommendation systems are used not only for movies, but on multiple other products and services like Amazon (Books, Items), Pandora/Spotify (Music), Google (News, Search), YouTube (Videos) etc.\n\nAn example of recommendation system is such as this:\n\nHave you ever had to answer this question at least once when you came home from work? As for me \u2014 yes, and more than once. From Netflix to Hulu, the need to build robust movie recommendation systems is extremely important given the huge demand for personalized content of modern consumers.\n\nIn this post, I will show you how to implement the 4 different movie recommendation approaches and evaluate them to see which one has the best performance.\n\nThe dataset that I\u2019m working with is MovieLens, one of the most common datasets that is available on the internet for building a Recommender System. The version of the dataset that I\u2019m working with (1M) contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000.\n\nAfter processing the data and doing some exploratory analysis, here are the most interesting features of this dataset:\n\nBeautiful, isn\u2019t it? I can recognize that there are a lot of movie franchises in this dataset, as evidenced by words like II and III\u2026 In addition to that, Day, Love, Life, Time, Night, Man, Dead, American are among the most commonly occurring words.\n\nIt appears that users are quite generous in their ratings. The mean rating is 3.58 on a scale of 5. Half the movies have a rating of 4 and 5. I personally think that a 5-level rating skill wasn\u2019t a good indicator as people could have different rating styles (i.e. person A could always use 4 for an average movie, whereas person B only gives 4 out for their favorites). Each user rated at least 20 movies, so I doubt the distribution could be caused just by chance variance in the quality of movies.\n\nHere\u2019s another word-cloud of the movie genres:\n\nThe top 5 genres are, in that respect order: Drama, Comedy, Action, Thriller, and Romance.\n\nNow let\u2019s move on to explore the 4 recommendation systems that can be used. Here they are, in respective order of presentation:\n\nThe Content-Based Recommender relies on the similarity of the items being recommended. The basic idea is that if you like an item, then you will also like a \u201csimilar\u201d item. It generally works well when it\u2019s easy to determine the context/properties of each item.\n\nA content based recommender works with data that the user provides, either explicitly movie ratings for the MovieLens dataset. Based on that data, a user profile is generated, which is then used to make suggestions to the user. As the user provides more inputs or takes actions on the recommendations, the engine becomes more and more accurate.\n\nThe concepts of Term Frequency (TF) and Inverse Document Frequency (IDF) are used in information retrieval systems and also content based filtering mechanisms (such as a content based recommender). They are used to determine the relative importance of a document / article / news item / movie etc.\n\nTF is simply the frequency of a word in a document. IDF is the inverse of the document frequency among the whole corpus of documents. TF-IDF is used mainly because of two reasons: Suppose we search for \u201cthe results of latest European Socccer games\u201d on Google. It is certain that \u201cthe\u201d will occur more frequently than \u201csoccer games\u201d but the relative importance of soccer gamesis higher than the search query point of view. In such cases, TF-IDF weighting negates the effect of high frequency words in determining the importance of an item (document).\n\nBelow is the equation to calculate the TF-IDF score:\n\nAfter calculating TF-IDF scores, how do we determine which items are closer to each other, rather closer to the user profile? This is accomplished using the Vector Space Model which computes the proximity based on the angle between the vectors. In this model, each item is stored as a vector of its attributes (which are also vectors) in an n-dimensional space and the angles between the vectors are calculated to determine the similarity between the vectors. Next, the user profile vectors are also created based on his actions on previous attributes of items and the similarity between an item and a user is also determined in a similar way.\n\nSentence 2 is more likely to be using Term 2 than using Term 1. Vice-versa for Sentence 1. The method of calculating this relative measure is calculated by taking the cosine of the angle between the sentences and the terms. The ultimate reason behind using cosine is that the value of cosine will increase with decreasing value of the angle between which signifies more similarity. The vectors are length normalized after which they become vectors of length 1 and then the cosine calculation is simply the sum-product of vectors.\n\nWith all that math in mind, I am going to build a Content-Based Recommendation Engine that computes similarity between movies based on movie genres. It will suggest movies that are most similar to a particular movie based on its genre.\n\nI do not have a quantitative metric to judge the machine\u2019s performance so this will have to be done qualitatively. In order to do so, I\u2019ll use TfidfVectorizer function from scikit-learn, which transforms text to feature vectors that can be used as input to estimator.\n\nI will be using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies. Since I have used the TF-IDF Vectorizer, calculating the Dot Product will directly give me the Cosine Similarity Score. Therefore, I will use sklearn\u2019s linear_kernel instead of cosine_similarities since it is much faster.\n\nI now have a pairwise cosine similarity matrix for all the movies in the dataset. The next step is to write a function that returns the 20 most similar movies based on the cosine similarity score.\n\nLet\u2019s try and get the top recommendations for a few movies and see how good the recommendations are.\n\nAs you can see, I have quite a decent list of recommendation for Good Will Hunting (Drama), Toy Story (Animation, Children\u2019s, Comedy), and Saving Private Ryan (Action, Thriller, War).\n\nOverall, here are the pros of using content-based recommendation:\n\nHowever, there are some cons of using this approach:\n\nThe Collaborative Filtering Recommender is entirely based on the past behavior and not on the context. More specifically, it is based on the similarity in preferences, tastes and choices of two users. It analyses how similar the tastes of one user is to another and makes recommendations on the basis of that.\n\nFor instance, if user A likes movies 1, 2, 3 and user B likes movies 2,3,4, then they have similar interests and A should like movie 4 and B should like movie 1. This makes it one of the most commonly used algorithm as it is not dependent on any additional information.\n\nIn general, collaborative filtering is the workhorse of recommender engines. The algorithm has a very interesting property of being able to do feature learning on its own, which means that it can start to learn for itself what features to use.\n\nThere are 2 main types of memory-based collaborative filtering algorithms:\n\nIn either scenario, we builds a similarity matrix. For user-user collaborative filtering, the user-similarity matrix will consist of some distance metrics that measure the similarity between any two pairs of users. Likewise, the item-similarity matrix will measure the similarity between any two pairs of items.\n\nThere are 3 distance similarity metrics that are usually used in collaborative filtering:\n\nDue to the limited computing power in my laptop, I will build the recommender system using only a subset of the ratings. In particular, I will take a random sample of 20,000 ratings (2%) from the 1M ratings.\n\nI use the scikit-learn library to split the dataset into testing and training. Cross_validation.train_test_split shuffles and splits the data into two datasets according to the percentage of test examples, which here is 0.2.\n\nNow I need to create a user-item matrix. Since I have splitted the data into testing and training, I need to create two matrices. The training matrix contains 80% of the ratings and the testing matrix contains 20% of the ratings.\n\nNow I use the pairwise_distances function from sklearn to calculate the Pearson Correlation Coefficient. This method provides a safe way to take a distance matrix as input, while preserving compatibility with many other algorithms that take a vector array.\n\nWith the similarity matrix in hand, I can now predict the ratings that were not included with the data. Using these predictions, I can then compare them with the test data to attempt to validate the quality of our recommender model.\n\nThere are many evaluation metrics but one of the most popular metric used to evaluate accuracy of predicted ratings is Root Mean Squared Error (RMSE). I will use the mean_square_error (MSE) function from sklearn, where the RMSE is just the square root of MSE. I\u2019ll use the scikit-learn\u2019s mean squared error function as my validation metric. Comparing user- and item-based collaborative filtering, it looks like user-based collaborative filtering gives a better result.\n\nRMSE of training of model is a metric which measure how much the signal and the noise is explained by the model. I noticed that my RMSE is quite big. I suppose I might have overfitted the training data.\n\nOverall, Memory-based Collaborative Filtering is easy to implement and produce reasonable prediction quality. However, there are some drawback of this approach:\n\nNote: The complete code for content-based and memory-based collaborative filtering can be found in this Jupyter Notebook.\n\nIn the previous attempt, I have used memory-based collaborative filtering to make movie recommendations from users\u2019 ratings data. I can only try them on a very small data sample (20,000 ratings), and ended up getting pretty high Root Mean Squared Error (bad recommendations). Memory-based collaborative filtering approaches that compute distance relationships between items or users have these two major issues:\n\nThus I\u2019d need to apply Dimensionality Reduction technique to derive the tastes and preferences from the raw data, otherwise known as doing low-rank matrix factorization. Why reduce dimensions?\n\nModel-based Collaborative Filtering is based on matrix factorization (MF)which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF:\n\nA well-known matrix factorization method is Singular value decomposition (SVD). At a high level, SVD is an algorithm that decomposes a matrix A into the best lower rank (i.e. smaller/simpler) approximation of the original matrix A. Mathematically, it decomposes A into a two unitary matrices and a diagonal matrix:\n\nwhere A is the input data matrix (users\u2019s ratings), U is the left singular vectors (user \u201cfeatures\u201d matrix), Sum is the diagonal matrix of singular values (essentially weights/strengths of each concept), and V^T is the right singular vectors (movie \u201cfeatures\u201d matrix). U and V^T are column orthonormal, and represent different things: U represents how much users \u201clike\u201d each feature and V^T represents how relevant each feature is to each movie.\n\nTo get the lower rank approximation, I take these matrices and keep only the top k features, which can be thought of as the underlying tastes and preferences vectors.\n\nScipy and Numpy both have functions to do the singular value decomposition. I\u2019m going to use the Scipy function svds because it let\u2019s me choose how many latent factors I want to use to approximate the original ratings matrix (instead of having to truncate it after).\n\nAs I\u2019m going to leverage matrix multiplication to get predictions, I\u2019ll convert the Sum (now are values) to the diagonal matrix form.\n\nI now have everything I need to make movie ratings predictions for every user. I can do it all at once by following the math and matrix multiply U, Sum, and V^T back to get the rank k = 50 approximation of A.\n\nBut first, I need to add the user means back to get the actual star ratings prediction.\n\nWith the predictions matrix for every user, I can build a function to recommend movies for any user. I return the list of movies the user has already rated, for the sake of comparison.\n\nNow I write a function to return the movies with the highest predicted rating that the specified user hasn\u2019t already rated.\n\nInstead of doing evaluation manually like the last time, I will use the Surprise library that provided various ready-to-use powerful prediction algorithms including (SVD) to evaluate its RMSE (Root Mean Squared Error) on the MovieLens dataset. It is a Python Scikit-Learn\u2019s building and analyzing recommender systems.\n\nI get a mean Root Mean Square Error of 0.8736 which is pretty good.\n\nLet\u2019s try to recommend 20 movies for user with ID 1310.\n\nThese look like pretty good recommendations. It\u2019s good to see that, although I didn\u2019t actually use the genre of the movie as a feature, the truncated matrix factorization features \u201cpicked up\u201d on the underlying tastes and preferences of the user. I\u2019ve recommended some comedy, drama, and romance movies \u2014 all of which were genres of some of this user\u2019s top rated movies.\n\nNote: The complete code for SVD Matrix Factorization can be found in this Jupyter Notebook.\n\nThe idea of using deep learning is similar to that of Model-Based Matrix Factorization. In matrix factorization, we decompose our original sparse matrix into product of 2 low rank orthogonal matrices. For deep learning implementation, we don\u2019t need them to be orthogonal, we want our model to learn the values of embedding matrix itself. The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.).\n\nHere are the main components of my neural network:\n\nThis code is based on the approach outlined in Alkahest\u2019s blog post Collaborative Filtering in Keras.\n\nI then compile the model using Mean Squared Error (MSE) as the loss function and the AdaMax learning algorithm.\n\nNow I need to train the model. This step will be the most-time consuming one. In my particular case, for our dataset with nearly 1 million ratings, almost 6,000 users and 4,000 movies, I trained the model in roughly 6 minutes per epoch (30 epochs ~ 3 hours) inside my MacBook Laptop CPU. I spitted the training and validation data with ratio of 90/10.\n\nThe next step is to actually predict the ratings a random user will give to a random movie. Below I apply the freshly trained deep learning model for all the users and all the movies, using 100 dimensional embeddings for each.\n\nHere I define the function to predict user\u2019s rating of unrated items.\n\nDuring the training process above, I saved the model weights each time the validation loss has improved. Thus, I can use that value to calculate the best validation Root Mean Square Error.\n\nThe best validation loss is 0.7424 at epoch 17. Taking the square root of that number, I got the RMSE value of 0.8616, which is better than the RMSE from the SVD Model (0.8736).\n\nHere I make a recommendation list of unrated 20 movies sorted by prediction value for user ID 2000. Let\u2019s see it.\n\nThis model performed better than all the approaches I attempted before (content-based, user-item similarity collaborative filtering, SVD). I can certainly improve this model\u2019s performance by making it deeper with more linear and non-linear layers.\n\nNote: The complete code for Deep Learning Model can be found in this Jupyter Notebook.\n\nRecommendation Engine is your companion and advisor to help you make the right choices by providing you tailored options and creating a personalized experience for you. It is beyond any doubt that recommendation engines are getting popular and critical in the new age of things. It is going to be in your best interest to learn to use them for businesses to be more competitive and consumers to be more efficient.\n\nI hope that this post has been helpful for you to learn about the 4 different approaches to build your own movie recommendation system. You can view all the source code in my GitHub repo at this link (https://github.com/khanhnamle1994/movielens). Let me know if you have any questions or suggestions on improvement!\n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://medium.com/swlh/the-5-machine-learning-use-cases-that-optimize-your-airbnb-travel-experience-fb027a56e5a5?source=user_profile---------3----------------",
        "title": "The 5 Machine Learning Use Cases that Optimize Your Airbnb Travel Experience",
        "text": "The 5 Machine Learning Use Cases that Optimize Your Airbnb Travel Experience Wondering how Airbnb sorts and delivers its listings when you search for a place to stay on your next getaway? If you know anything about machine learning, you might have expected that there are a plethora of variables that go into sorting the tens of thousands of listings that are sometimes available in a specific location. Unlike machines, it\u2019s impossible for one human being to go through each listing \u2014 and if you\u2019re indecisive by nature, this could pose an existential problem. That\u2019s why Airbnb\u2019s machine learning algorithms do the work for you, pulling signals from a variety of data points, depending on whether you are host or guest. Optimizing matches between hosts and guests will be critical to Airbnb\u2019s success as it continues to grow. The variety in types of accommodations Airbnb has is an advantage, as long as it ensures guests can easily find a host that meets their criteria. And as Airbnb adds to its 4 million current listings, ensuring both guests and hosts are satisfied will become more crucial. If users can find the exact accommodation they are looking for, especially if it is at a cheaper price, they are unlikely to revert to using hotels. So how does Airbnb do such an amazing job of optimizing guest-host matching? After spending a couple of weeks digging up on Airbnb Technical Blog, I discover that there are 5 important use cases of machine learning that are currently being deployed by Airbnb\u2019s engineers and data scientists to solve this problem. Here they are, in respective order of presentation: The goal of this machine learning system is to discover what affects the hosts\u2019 decisions to accept accommodation requests and how Airbnb could increase acceptances and matches on the platform. After doing initial data query and experiments, Airbnb found out that the hosts were more likely to accept requests that fit well in their calendar and minimize gap days. Additionally, hosts in big markets (such as San Francisco or New York City) care a lot about their occupancy; while for small markets, hosts prefer to have a small number of nights between requests. Thus, if Airbnb could promote in their search results those hosts who would be more likely to accept an accommodation request resulting from a potential guest\u2019s search query, they would expect to see happier guests and hosts and more matches that turned into fun vacations or productive business trips. At first glance, this seems like a perfect case for collaborative filtering \u2014 we have users (hosts) and items (trips) and we want to understand the preference for those items by combining historical ratings (accept/decline) with statistical learning from similar hosts. However, the application does not fully fit in the collaborative filtering framework for two reasons. First, no two trips are ever identical because behind each accommodation request there is a different guest with a unique human interaction that influences the host\u2019s acceptance decision. Taking this point one step further, a host can receive multiple accommodation requests for the same trip with different guests at different points in time and give those requests conflicting votes. Thus, Airbnb engineers and data scientists built a model resembling collaborative filtering. They used the multiplicity of responses for the same trip to reduce the noise coming from the latent factors in the guest-host interaction. To do so, they considered hosts\u2019 average response to a certain trip characteristic in isolation. Instead of looking at the combination of trip length, size of guest party, size of calendar gap and so on, they looked at each of these trip characteristics by itself. For predictions, they combined the preferences for different trip characteristics into a single prediction for the probability of acceptance. The weight the preference of each trip characteristic has on the acceptance decision is the coefficient that comes out of the logistic regression. To improve the prediction, they also included a few more geographic and host specific features in the logistic regression. The flow chart summarizes the modeling technique used:\n\nThe goal of this machine learning system is to answer a very common question from Airbnb hosts: How do I pick the right price? Setting a price can be hard without reliable information about other listings in hosts\u2019 area, travel trends, and the interest people have in the amenities hosts offer. Thus, the team at Airbnb decided to build a model that can share insights that they learned with the hosts. An insight is a campaign that guides hosts to become more successful at pricing. Each insight must be personalized, targeted, and actionable. To serve up the insights, they created Narad \u2014 a backend service that ingests data from a set of offline and online data sources to generate personalized insights, rank their effectiveness for different listings and contexts, and deliver insights at the right place and right time.\n\nAn insight generated by Narad consists of the following: Identifiers: This includes the insight type that is the identifier of each insight, the placement that tells which host tool this insight can be delivered to, and other grouping information. Targeting: This is a list of targeting conditions that need to be satisfied in order for the insight to be eligible for a given listing. There are various dimensions such as occupancy, past and future bookings, market demand, geography, listing attributes and pricing settings over which insights can be targeted. Payload: This determines a set of personalized information that the insight displays to the host. Example payloads range from the suggested values for host settings to the benefit to the host such as the potential booking increase. Copy: This contains the information to fetch the internationalized content for the UI. Narad is responsible for delivering the most relevant and impactful insights to the host. The first iteration of ranking defines the total value of each insight through a set of terms. The first term is the weight which refers to the inherent impact of the insight. The second term is the historical conversion rate of the particular insight. Some insights might carry high impact but draw less attention from hosts. Other insights might get a lot of conversions but not be inherently impactful. The first term and the second term keep this balance. The last term is the repetition penalty that discounts the total value of the insight if the same insight was ranked as the top insight last time. This helps providing some variance on the top position so that the same insight doesn\u2019t appear at the top over and over even though it is the best insight in order to keep hosts more engaged. 3 \u2014 Predicting value of homes on Airbnb At Airbnb, predicting home values is a specific use case of Customer Lifetime Value modeling, which captures the projected value of a user for a fixed time horizon. At marketplace companies like Airbnb, knowing users\u2019 Lifetime Values enable them to allocate budget across different marketing channels more efficiently, calculate more precise bidding prices for online marketing based on keywords, and create better listing segments. For Lifetime Value modeling, Airbnb developed machine learning tools that abstract away the engineering work behind product-ionizing machine learning models. In specific, there are 4 tasks in their ML workflow for this task:\n\nAt the Feature Engineering phase, Airbnb used their internal feature repository Zipline, which provides features at different levels of granularity, such as the host, guest, listing, or market level. The crowdsourced nature of this internal tool allows their data scientists to use a wide variety of high quality, vetted features that others have prepared for past projects. If a desired feature is not available, a user can create his/her own feature with a feature configuration file. At the Prototyping and Training phase, Airbnb constructed data pipelines, available in Scikit-Learn and Spark, that allow their data scientists to specify high-level blueprints that describe how features should be transformed, and which models to train. At a high level, they used pipelines to specify data transformations for different types of features, depending on whether those features are of type binary, categorical, or numeric. The advantage of writing prototypes with pipelines is that it abstracts away tedious data transformations using data transforms. Collectively, these transforms ensure that data will be transformed consistently across training and scoring, which solves a common problem of data transformation inconsistency when translating a prototype into production. At the Model Selection phase, Airbnb utilized their AutoML frameworks to speed up the process. By exploring a wide variety of models, they found which types of models tended to perform best. For example, they learned that eXtreme gradient boosted trees (XGBoost) significantly outperformed benchmark models such as mean response models, ridge regression models, and single decision trees. Finally, for Taking Model Prototypes to Production, Airbnb built a framework called ML Automator that automagically translates a Jupyter notebook into an Airflow machine learning pipeline. This framework is designed specifically for data scientists who are already familiar with writing prototypes in Python, and want to take their model to production with limited experience in data engineering. Fighting financial fraud is one of the most important task at Airbnb to ensure the trust in their platform. The company has leveraged machine learning, experimentation, and analytics to identify and block fraudsters while minimizing impact on the overwhelming majority of its good users.\n\nLike all online businesses, Airbnb faces fraudsters who attempt to use stolen credit cards. When the true cardholder realizes their card has been stolen and notices unauthorized charges on their bill, the credit card company issues what\u2019s called a \u201cchargeback,\u201d and the merchant returns the money. Airbnb detects financial fraud in a number of ways, but their workhorse method uses machine-learning (ML) models trained on past examples of confirmed good and confirmed fraudulent behavior. To stop the use of stolen credit cards, their chargeback model triggers a number of frictions to ensure that the guest is in fact authorized to use that card, including micro-authorization (placing two small authorizations on the credit card, which the cardholder must identify by logging into their online banking statement), 3-D Secure (which allows credit card companies to directly authenticate cardholders via a password or SMS challenge), and billing-statement verification (requiring the cardholder to upload a copy of the billing statement associated with the card). Very recently, Airbnb has developed a Listing Embedding technique for the purpose of improving Similar Listing Recommendations and Real-Time Personalization in Search Ranking. The embeddings are vector representations of Airbnb homes learned from search sessions that allow them to measure similarities between listings. They effectively encode many listing features, such as location, price, listing type, architecture and listing style, all using only 32 float numbers.\n\nEmbeddings is a concept from Natural Language Processing used for word representations. Researchers from the Web Search, E-commerce and Marketplace domains have realized that just like one can train word embeddings by treating a sequence of words in a sentence as context, the same can be done for training embeddings of user actions by treating sequence of user actions as context. Examples include learning representations of items that were clicked or purchased or queries and ads that were clicked. These embeddings have subsequently been leveraged for a variety of recommendations on the Web. At Airbnb, they trained and optimized their models to learn listing embeddings for 4.5 million active listings on Airbnb using more than 800 million search clicks sessions, resulting in high quality listing representations. To evaluate what characteristics of listings were captured by the embeddings, they examined them in several ways. First, to evaluate if geographical similarity is encoded, they performed k-means clustering on learned embeddings. Next, they evaluated average cosine similarities between listings of different types (Entire Home, Private Room, Shared Room) and price ranges and confirmed that cosine similarities between listings of same type and price ranges are much higher compared to similarities between listings of different type and price ranges."
    },
    {
        "url": "https://medium.com/@james_aka_yale/the-4-recommendation-engines-that-can-predict-your-movie-tastes-bbec857b8223?source=user_profile---------4----------------",
        "title": "The 4 Recommendation Engines That Can Predict Your Movie Tastes",
        "text": "The 4 Recommendation Engines That Can Predict Your Movie Tastes\n\nRecommendation systems are used not only for movies, but on multiple other products and services like Amazon (Books, Items), Pandora/Spotify (Music), Google (News, Search), YouTube (Videos) etc.\n\nAn example of recommendation system is such as this:\n\nHave you ever had to answer this question at least once when you came home from work? As for me \u2014 yes, and more than once. From Netflix to Hulu, the need to build robust movie recommendation systems is extremely important given the huge demand for personalized content of modern consumers.\n\nIn this post, I will show you how to implement the 4 different movie recommendation approaches and evaluate them to see which one has the best performance.\n\nThe dataset that I\u2019m working with is MovieLens, one of the most common datasets that is available on the internet for building a Recommender System. The version of the dataset that I\u2019m working with (1M) contains 1,000,209 anonymous ratings of approximately 3,900 movies made by 6,040 MovieLens users who joined MovieLens in 2000.\n\nAfter processing the data and doing some exploratory analysis, here are the most interesting features of this dataset:\n\nBeautiful, isn\u2019t it? I can recognize that there are a lot of movie franchises in this dataset, as evidenced by words like II and III\u2026 In addition to that, Day, Love, Life, Time, Night, Man, Dead, American are among the most commonly occuring words.\n\nIt appears that users are quite generous in their ratings. The mean rating is 3.58 on a scale of 5. Half the movies have a rating of 4 and 5. I personally think that a 5-level rating skill wasn\u2019t a good indicator as people could have different rating styles (i.e. person A could always use 4 for an average movie, whereas person B only gives 4 out for their favorites). Each user rated at least 20 movies, so I doubt the distribution could be caused just by chance variance in the quality of movies.\n\nHere\u2019s another word-cloud of the movie genres:\n\nThe top 5 genres are, in that respect order: Drama, Comedy, Action, Thriller, and Romance.\n\nNow let\u2019s move on to explore the 4 recommendation systems that can be used. Here they are, in respective order of presentation:\n\nThe Content-Based Recommender relies on the similarity of the items being recommended. The basic idea is that if you like an item, then you will also like a \u201csimilar\u201d item. It generally works well when it\u2019s easy to determine the context/properties of each item.\n\nA content based recommender works with data that the user provides, either explicitly movie ratings for the MovieLens dataset. Based on that data, a user profile is generated, which is then used to make suggestions to the user. As the user provides more inputs or takes actions on the recommendations, the engine becomes more and more accurate.\n\nThe concepts of Term Frequency (TF) and Inverse Document Frequency (IDF) are used in information retrieval systems and also content based filtering mechanisms (such as a content based recommender). They are used to determine the relative importance of a document / article / news item / movie etc.\n\nTF is simply the frequency of a word in a document. IDF is the inverse of the document frequency among the whole corpus of documents. TF-IDF is used mainly because of two reasons: Suppose we search for \u201cthe results of latest European Socccer games\u201d on Google. It is certain that \u201cthe\u201d will occur more frequently than \u201csoccer games\u201d but the relative importance of soccer games is higher than the search query point of view. In such cases, TF-IDF weighting negates the effect of high frequency words in determining the importance of an item (document).\n\nBelow is the equation to calculate the TF-IDF score:\n\nAfter calculating TF-IDF scores, how do we determine which items are closer to each other, rather closer to the user profile? This is accomplished using the Vector Space Model which computes the proximity based on the angle between the vectors. In this model, each item is stored as a vector of its attributes (which are also vectors) in an n-dimensional space and the angles between the vectors are calculated to determine the similarity between the vectors. Next, the user profile vectors are also created based on his actions on previous attributes of items and the similarity between an item and a user is also determined in a similar way.\n\nSentence 2 is more likely to be using Term 2 than using Term 1. Vice-versa for Sentence 1. The method of calculating this relative measure is calculated by taking the cosine of the angle between the sentences and the terms. The ultimate reason behind using cosine is that the value of cosine will increase with decreasing value of the angle between which signifies more similarity. The vectors are length normalized after which they become vectors of length 1 and then the cosine calculation is simply the sum-product of vectors.\n\nWith all that math in mind, I am going to build a Content-Based Recommendation Engine that computes similarity between movies based on movie genres. It will suggest movies that are most similar to a particular movie based on its genre.\n\nI do not have a quantitative metric to judge the machine\u2019s performance so this will have to be done qualitatively. In order to do so, I\u2019ll use TfidfVectorizer function from scikit-learn, which transforms text to feature vectors that can be used as input to estimator.\n\nI will be using the Cosine Similarity to calculate a numeric quantity that denotes the similarity between two movies. Since I have used the TF-IDF Vectorizer, calculating the Dot Product will directly give me the Cosine Similarity Score. Therefore, I will use sklearn\u2019s linear_kernel instead of cosine_similarities since it is much faster.\n\nI now have a pairwise cosine similarity matrix for all the movies in the dataset. The next step is to write a function that returns the 20 most similar movies based on the cosine similarity score.\n\nLet\u2019s try and get the top recommendations for a few movies and see how good the recommendations are.\n\nAs you can see, I have quite a decent list of recommendation for Good Will Hunting (Drama), Toy Story (Animation, Children\u2019s, Comedy), and Saving Private Ryan (Action, Thriller, War).\n\nOverall, here are the pros of using content-based recommendation:\n\nHowever, there are some cons of using this approach:\n\nThe Collaborative Filtering Recommender is entirely based on the past behavior and not on the context. More specifically, it is based on the similarity in preferences, tastes and choices of two users. It analyses how similar the tastes of one user is to another and makes recommendations on the basis of that.\n\nFor instance, if user A likes movies 1, 2, 3 and user B likes movies 2,3,4, then they have similar interests and A should like movie 4 and B should like movie 1. This makes it one of the most commonly used algorithm as it is not dependent on any additional information.\n\nIn general, collaborative filtering is the workhorse of recommender engines. The algorithm has a very interesting property of being able to do feature learning on its own, which means that it can start to learn for itself what features to use.\n\nThere are 2 main types of memory-based collaborative filtering algorithms:\n\nIn either scenario, we builds a similarity matrix. For user-user collaborative filtering, the user-similarity matrix will consist of some distance metrics that measure the similarity between any two pairs of users. Likewise, the item-similarity matrix will measure the similarity between any two pairs of items.\n\nThere are 3 distance similarity metrics that are usually used in collaborative filtering:\n\nDue to the limited computing power in my laptop, I will build the recommender system using only a subset of the ratings. In particular, I will take a random sample of 20,000 ratings (2%) from the 1M ratings.\n\nI use the scikit-learn library to split the dataset into testing and training. Cross_validation.train_test_split shuffles and splits the data into two datasets according to the percentage of test examples, which in this case is 0.2.\n\nNow I need to create a user-item matrix. Since I have splitted the data into testing and training, I need to create two matrices. The training matrix contains 80% of the ratings and the testing matrix contains 20% of the ratings.\n\nNow I use the pairwise_distances function from sklearn to calculate the Pearson Correlation Coefficient. This method provides a safe way to take a distance matrix as input, while preserving compatibility with many other algorithms that take a vector array.\n\nWith the similarity matrix in hand, I can now predict the ratings that were not included with the data. Using these predictions, I can then compare them with the test data to attempt to validate the quality of our recommender model.\n\nThere are many evaluation metrics but one of the most popular metric used to evaluate accuracy of predicted ratings is Root Mean Squared Error (RMSE). I will use the mean_square_error (MSE) function from sklearn, where the RMSE is just the square root of MSE. I\u2019ll use the scikit-learn\u2019s mean squared error function as my validation metric. Comparing user- and item-based collaborative filtering, it looks like user-based collaborative filtering gives a better result.\n\nRMSE of training of model is a metric which measure how much the signal and the noise is explained by the model. I noticed that my RMSE is quite big. I suppose I might have overfitted the training data.\n\nOverall, Memory-based Collaborative Filtering is easy to implement and produce reasonable prediction quality. However, there are some drawback of this approach:\n\nNote: The complete code for content-based and memory-based collaborative filtering can be found in this Jupyter Notebook.\n\nIn the previous attempt, I have used memory-based collaborative filtering to make movie recommendations from users\u2019 ratings data. I can only try them on a very small data sample (20,000 ratings), and ended up getting pretty high Root Mean Squared Error (bad recommendations). Memory-based collaborative filtering approaches that compute distance relationships between items or users have these two major issues:\n\nThus I\u2019d need to apply Dimensionality Reduction technique to derive the tastes and preferences from the raw data, otherwise known as doing low-rank matrix factorization. Why reduce dimensions?\n\nModel-based Collaborative Filtering is based on matrix factorization (MF) which has received greater exposure, mainly as an unsupervised learning method for latent variable decomposition and dimensionality reduction. Matrix factorization is widely used for recommender systems where it can deal better with scalability and sparsity than Memory-based CF:\n\nA well-known matrix factorization method is Singular value decomposition (SVD). At a high level, SVD is an algorithm that decomposes a matrix A into the best lower rank (i.e. smaller/simpler) approximation of the original matrix A. Mathematically, it decomposes A into a two unitary matrices and a diagonal matrix:\n\nwhere A is the input data matrix (users\u2019s ratings), U is the left singular vectors (user \u201cfeatures\u201d matrix), Sum is the diagonal matrix of singular values (essentially weights/strengths of each concept), and V^T is the right singular vectors (movie \u201cfeatures\u201d matrix). U and V^T are column orthonormal, and represent different things: U represents how much users \u201clike\u201d each feature and V^T represents how relevant each feature is to each movie.\n\nTo get the lower rank approximation, I take these matrices and keep only the top k features, which can be thought of as the underlying tastes and preferences vectors.\n\nScipy and Numpy both have functions to do the singular value decomposition. I\u2019m going to use the Scipy function svds because it let\u2019s me choose how many latent factors I want to use to approximate the original ratings matrix (instead of having to truncate it after).\n\nAs I\u2019m going to leverage matrix multiplication to get predictions, I\u2019ll convert the Sum (now are values) to the diagonal matrix form.\n\nI now have everything I need to make movie ratings predictions for every user. I can do it all at once by following the math and matrix multiply U, Sum, and V^T back to get the rank k = 50 approximation of A.\n\nBut first, I need to add the user means back to get the actual star ratings prediction.\n\nWith the predictions matrix for every user, I can build a function to recommend movies for any user. I return the list of movies the user has already rated, for the sake of comparison.\n\nNow I write a function to return the movies with the highest predicted rating that the specified user hasn\u2019t already rated.\n\nInstead of doing evaluation manually like the last time, I will use the Surprise library that provided various ready-to-use powerful prediction algorithms including (SVD) to evaluate its RMSE (Root Mean Squared Error) on the MovieLens dataset. It is a Python scikit building and analyzing recommender systems.\n\nI get a mean Root Mean Square Error of 0.8736 which is pretty good.\n\nLet\u2019s try to recommend 20 movies for user with ID 1310.\n\nThese look like pretty good recommendations. It\u2019s good to see that, although I didn\u2019t actually use the genre of the movie as a feature, the truncated matrix factorization features \u201cpicked up\u201d on the underlying tastes and preferences of the user. I\u2019ve recommended some comedy, drama, and romance movies \u2014 all of which were genres of some of this user\u2019s top rated movies.\n\nNote: The complete code for SVD Matrix Factorization can be found in this Jupyter Notebook.\n\nThe idea of using deep learning is similar to that of Model-Based Matrix Factorization. In matrix factorization, we decompose our original sparse matrix into product of 2 low rank orthogonal matrices. For deep learning implementation, we don\u2019t need them to be orthogonal, we want our model to learn the values of embedding matrix itself. The user latent features and movie latent features are looked up from the embedding matrices for specific movie-user combination. These are the input values for further linear and non-linear layers. We can pass this input to multiple relu, linear or sigmoid layers and learn the corresponding weights by any optimization algorithm (Adam, SGD, etc.).\n\nHere are the main components of my neural network:\n\nThis code is based on the approach outlined in Alkahest\u2019s blog post Collaborative Filtering in Keras.\n\nI then compile the model using Mean Squared Error (MSE) as the loss function and the AdaMax learning algorithm.\n\nNow I need to train the model. This step will be the most-time consuming one. In my particular case, for our dataset with nearly 1 million ratings, almost 6,000 users and 4,000 movies, I trained the model in roughly 6 minutes per epoch (30 epochs ~ 3 hours) inside my Macbook Laptop CPU. I spitted the training and validataion data with ratio of 90/10.\n\nThe next step is to actually predict the ratings a random user will give to a random movie. Below I apply the freshly trained deep learning model for all the users and all the movies, using 100 dimensional embeddings for each of them.\n\nHere I define the function to predict user\u2019s rating of unrated items.\n\nDuring the training process above, I saved the model weights each time the validation loss has improved. Thus, I can use that value to calculate the best validation Root Mean Square Error.\n\nThe best validation loss is 0.7424 at epoch 17. Taking the square root of that number, I got the RMSE value of 0.8616, which is better than the RMSE from the SVD Model (0.8736).\n\nHere I make a recommendation list of unrated 20 movies sorted by prediction value for user ID 2000. Let\u2019s see it.\n\nThis model performed better than all the approaches I attempted before (content-based, user-item similarity collaborative filtering, SVD). I can certainly improve this model\u2019s performance by making it deeper with more linear and non-linear layers.\n\nNote: The complete code for Deep Learning Model can be found in this Jupyter Notebook.\n\nRecommendation Engine is your companion and advisor to help you make the right choices by providing you tailored options and creating a personalized experience for you. It is beyond any doubt that recommendation engines are getting popular and critical in the new age of things. It is going to be in your best interest to learn to use them for businesses to be more competitive and consumers to be more efficient. I hope that this post has been helpful for you to learn about the 4 different approaches to build your own movie recommendation system. You can view all the source code in my GitHub repo at this link (https://github.com/khanhnamle1994/movielens). Let me know if you have any questions or suggestions on improvement!"
    },
    {
        "url": "https://heartbeat.fritz.ai/the-5-computer-vision-techniques-that-will-change-how-you-see-the-world-1ee19334354b?source=user_profile---------5----------------",
        "title": "The 5 Computer Vision Techniques That Will Change How You See The World",
        "text": "The 5 Computer Vision Techniques That Will Change How You See The World Computer Vision is one of the hottest research fields within Deep Learning at the moment. It sits at the intersection of many academic subjects, such as Computer Science (Graphics, Algorithms, Theory, Systems, Architecture), Mathematics (Information Retrieval, Machine Learning), Engineering (Robotics, Speech, NLP, Image Processing), Physics (Optics), Biology (Neuroscience), and Psychology (Cognitive Science). As Computer Vision represents a relative understanding of visual environments and their contexts, many scientists believe the field paves the way towards Artificial General Intelligence due to its cross-domain mastery. So what is Computer Vision? Here are a couple of formal textbook definitions: \u201cthe construction of explicit, meaningful descriptions of physical objects from images\u201d (Ballard & Brown, 1982) \u201ccomputing properties of the 3D world from one or more digital images\u201d (Trucco & Verri, 1998) \u201cto make useful decisions about real physical objects and scenes based on sensed images\u201d (Sockman & Shapiro, 2001) Why study Computer Vision? The most obvious answer is that there\u2019s a fast-growing collection of useful applications derived from this field of study. Here are just a handful of them: Face recognition: Snapchat and Facebook use face-detection algorithms to apply filters and recognize you in pictures. Image retrieval: Google Images uses content-based queries to search relevant images. The algorithms analyze the content in the query image and return results based on best-matched content. Gaming and controls: A great commercial product in gaming that uses stereo vision is Microsoft Kinect. Surveillance: Surveillance cameras are ubiquitous at public locations and are used to detect suspicious behaviors. Biometrics: Fingerprint, iris and face matching remains some common methods in biometric identification. Smart cars: Vision remains the main source of information to detect traffic signs and lights and other visual features. I recently finished Stanford\u2019s wonderful CS231n course on using Convolutional Neural Networks for visual recognition. Visual recognition tasks such as image classification, localization, and detection are key components of Computer vision. Recent developments in neural networks and deep learning approaches have greatly advanced the performance of these state-of-the-art visual recognition systems. The course is a phenomenal resource that taught me the details of deep learning architectures being used in cutting-edge computer vision research. In this article, I want to share the 5 major computer vision techniques I\u2019ve learned as well as major deep learning models and applications using each of them.\n\nThe problem of Image Classification goes like this: Given a set of images that are all labeled with a single category, we\u2019re asked to predict these categories for a novel set of test images and measure the accuracy of the predictions. There are a variety of challenges associated with this task, including viewpoint variation, scale variation, intra-class variation, image deformation, image occlusion, illumination conditions, and background clutter. How might we go about writing an algorithm that can classify images into distinct categories? Computer Vision researchers have come up with a data-driven approach to solve this. Instead of trying to specify what every one of the image categories of interest look like directly in code, they provide the computer with many examples of each image class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. In other words, they first accumulate a training dataset of labeled images, then feed it to the computer to process the data. Given that fact, the complete image classification pipeline can be formalized as follows: Our input is a training dataset that consists of N images, each labeled with one of K different classes. Then, we use this training set to train a classifier to learn what every one of the classes looks like. In the end, we evaluate the quality of the classifier by asking it to predict labels for a new set of images that it\u2019s never seen before. We\u2019ll then compare the true labels of these images to the ones predicted by the classifier. The most popular architecture used for image classification is Convolutional Neural Networks (CNNs). A typical use case for CNNs is where you feed the network images and the network classifies the data. CNNs tend to start with an input \u201cscanner\u201d which isn\u2019t intended to parse all the training data at once. For example, to input an image of 100 x 100 pixels, you wouldn\u2019t want a layer with 10,000 nodes. Rather, you create a scanning input layer of say 10 x 10 which you feed the first 10 x 10 pixels of the image. Once you passed that input, you feed it the next 10 x 10 pixels by moving the scanner one pixel to the right. This technique is known as sliding windows. This input data is then fed through convolutional layers instead of normal layers. Each node only concerns itself with close neighboring cells. These convolutional layers also tend to shrink as they become deeper, mostly by easily divisible factors of the input. Besides these convolutional layers, they also often feature pooling layers. Pooling is a way to filter out details: a commonly found pooling technique is max pooling, where we take, say, 2 x 2 pixels and pass on the pixel with the most amount of a certain attribute. Most image classification techniques nowadays are trained on ImageNet, a dataset with approximately 1.2 million high-resolution training images. Test images will be presented with no initial annotation (no segmentation or labels), and algorithms will have to produce labelings specifying what objects are present in the images. Some of the best existing computer vision methods were tried on this dataset by leading computer vision groups from Oxford, INRIA, and XRCE.Typically, computer vision systems use complicated multi-stage pipelines, and the early stages are typically hand-tuned by optimizing a few parameters. The winner of the 1st ImageNet competition, Alex Krizhevsky (NIPS 2012), developed a very deep convolutional neural net of the type pioneered by Yann LeCun. Its architecture includes 7 hidden layers, not counting some max pooling layers. The early layers were convolutional, while the last 2 layers were globally connected. The activation functions were rectified linear units in every hidden layer. These train much faster and are more expressive than logistic units. In addition to that, it also uses competitive normalization to suppress hidden activities when nearby units have stronger activities. This helps with variations in intensity. In terms of hardware requirements, Alex uses a very efficient implementation of convolutional nets on 2 Nvidia GTX 580 GPUs (over 1000 fast little cores). The GPUs are very good for matrix-matrix multiplies and also have very high bandwidth to memory. This allows him to train the network in a week and makes it quick to combine results from 10 patches at test time. We can spread a network over many cores if we can communicate the states fast enough. As cores get cheaper and datasets get bigger, big neural nets will improve faster than old-fashioned computer vision systems. Since AlexNet, there have been multiple new models using CNN as their backbone architecture and achieving excellent results in ImageNet: ZFNet (2013), GoogLeNet (2014), VGGNet (2014), ResNet (2015), DenseNet (2016) etc.\n\nThe task to define objects within images usually involves outputting bounding boxes and labels for individual objects. This differs from the classification / localization task by applying classification and localization to many objects instead of just a single dominant object. You only have 2 classes of object classification, which means object bounding boxes and non-object bounding boxes. For example, in car detection, you have to detect all cars in a given image with their bounding boxes. If we use the Sliding Window technique like the way we classify and localize images, we need to apply a CNN to many different crops of the image. Because CNN classifies each crop as object or background, we need to apply CNN to huge numbers of locations and scales, which is very computationally expensive! In order to cope with this, neural network researchers have proposed to use regions instead, where we find \u201cblobby\u201d image regions that are likely to contain objects. This is relatively fast to run. The first model that kicked things off was R-CNN(Region-based Convolutional Neural Network). In a R-CNN, we first scan the input image for possible objects using an algorithm called Selective Search, generating ~2,000 region proposals. Then we run a CNN on top of each of these region proposals. Finally, we take the output of each CNN and feed it into an SVM to classify the region and a linear regression to tighten the bounding box of the object. Essentially, we turned object detection into an image classification problem. However, there are some problems \u2014 the training is slow, a lot of disk space is required, and inference is also slow. An immediate descendant to R-CNN is Fast R-CNN, which improves the detection speed through 2 augmentations: 1) Performing feature extraction before proposing regions, thus only running one CNN over the entire image, and 2) Replacing SVM with a softmax layer, thus extending the neural network for predictions instead of creating a new model. Fast R-CNN performed much better in terms of speed, because it trains just one CNN for the entire image. However, the selective search algorithm is still taking a lot of time to generate region proposals. Thus comes the invention of Faster R-CNN, which now is a canonical model for deep learning-based object detection. It replaces the slow selective search algorithm with a fast neural network by inserting a Region Proposal Network (RPN) to predict proposals from features. The RPN is used to decide \u201cwhere\u201d to look in order to reduce the computational requirements of the overall inference process. The RPN quickly and efficiently scans every location in order to assess whether further processing needs to be carried out in a given region. It does that by outputting k bounding box proposals each with 2 scores representing the probability of object or not at each location. Once we have our region proposals, we feed them straight into what is essentially a Fast R-CNN. We add a pooling layer, some fully-connected layers, and finally a softmax classification layer and bounding box regressor. Altogether, Faster R-CNN achieved much better speeds and higher accuracy. It\u2019s worth noting that although future models did a lot to increase detection speeds, few models managed to outperform Faster R-CNN by a significant margin. In other words, Faster R-CNN may not be the simplest or fastest method for object detection, but it\u2019s still one of the best performing. Major Object Detection trends in recent years have shifted towards quicker, more efficient detection systems. This was visible in approaches like You Only Look Once (YOLO), Single Shot MultiBox Detector (SSD), and Region-Based Fully Convolutional Networks (R-FCN) as a move towards sharing computation on a whole image. Hence, these approaches differentiate themselves from the costly subnetworks associated with the 3 R-CNN techniques. The main rationale behind these trends is to avoid having separate algorithms focus on their respective subproblems in isolation, as this typically increases training time and can lower network accuracy.\n\nObject Tracking refers to the process of following a specific object of interest, or multiple objects, in a given scene. It traditionally has applications in video and real-world interactions where observations are made following an initial object detection. Now, it\u2019s crucial to autonomous driving systems such as self-driving vehicles from companies like Uber and Tesla. Object Tracking methods can be divided into 2 categories according to the observation model: generative method and discriminative method. The generative method uses the generative model to describe the apparent characteristics and minimizes the reconstruction error to search the object, such as PCA. The discriminative method can be used to distinguish between the object and the background, its performance is more robust, and it gradually becomes the main method in tracking. Discriminative method is also referred to as Tracking-by-Detection, and deep learning belongs to this category. To achieve the tracking by detection, we detect candidate objects for all frames and use deep learning to recognize the wanted object from the candidates. There are 2 kinds of basic network models that can be used: stacked auto encoders (SAE) and convolutional neural network (CNN). The most popular deep network for tracking tasks using SAE is Deep Learning Tracker, which proposes offline pre-training and online fine-tuning the net. The process works like this: Off-line unsupervised pre-train the stacked denoising auto-encoder using large-scale natural image datasets to obtain the general object representation. Stacked denoising auto-encoder can obtain more robust feature expression ability by adding noise in input images and reconstructing the original images. Combine the coding part of the pre-trained network with a classifier to get the classification network, then use the positive and negative samples obtained from the initial frame to fine-tune the network, which can discriminate the current object and background. DLT uses particle filter as the motion model to produce candidate patches of the current frame. The classification network outputs the probability scores for these patches, meaning the confidence of their classifications, then chooses the highest of these patches as the object. In the model updating, DLT uses the way of limited threshold. Because of its superiority in image classification and object detection, CNN has become the mainstream deep model in computer vision and in visual tracking. Generally speaking, a large-scale CNN can be trained both as a classifier and as a tracker. 2 representative CNN-based tracking algorithms are fully-convolutional network tracker (FCNT) and multi-domain CNN (MD Net). FCNT analyzes and takes advantage of the feature maps of the VGG model successfully, which is a pre-trained ImageNet, and results in the following observations: CNN feature maps can be used for localization and tracking. Many CNN feature maps are noisy or un-related for the task of discriminating a particular object from its background. Higher layers capture semantic concepts on object categories, whereas lower layers encode more discriminative features to capture intra-class variation. Because of these observations, FCNT designs the feature selection network to select the most relevant feature maps on the conv4\u20133 and conv5\u20133 layers of the VGG network. Then in order to avoid overfitting on noisy ones, it also designs extra two channels (called SNet and GNet) for the selected feature maps from two layers\u2019 separately. The GNet captures the category information of the object, while the SNet discriminates the object from a background with a similar appearance. Both of the networks are initialized with the given bounding-box in the first frame to get heat maps of the object, and for new frames, a region of interest (ROI) centered at the object location in the last frame is cropped and propagated. At last, through SNet and GNet, the classifier gets two heat maps for prediction, and the tracker decides which heat map will be used to generate the final tracking result according to whether there are distractors. The pipeline of FCNT is shown below. Different from the idea of FCNT, MD Net uses all the sequences of a video to to track movements in them. The networks mentioned above use irrelevant image data to reduce the training demand of tracking data, and this idea has some deviation from tracking. The object of one class in this video can be the background in another video, so MD Net proposes the idea of multi-domain to distinguish the object and background in every domain independently. And a domain indicates a set of videos that contain the same kind of object. As shown below, MD Net is divided into 2 parts: the shared layers and the K branches of domain-specific layers. Each branch contains a binary classification layer with softmax loss, which is used to distinguish the object and background in each domain, and the shared layers sharing with all domains to ensure the general representation. In recent years, deep learning researchers have tried different ways to adapt to features of the visual tracking task. There are many directions that have been explored: applying other network models such as Recurrent Neural Net and Deep Belief Net, designing the network structure to adapt to video processing and end-to-end learning, optimizing the process, structure, and parameters, or even combining deep learning with traditional methods of computer vision or approaches in other fields such as Language Processing and Speech Recognition.\n\nCentral to Computer Vision is the process of Segmentation, which divides whole images into pixel groupings which can then be labelled and classified. Particularly, Semantic Segmentation tries to semantically understand the role of each pixel in the image (e.g. is it a car, a motorbike, or some other type of class?). For example, in the picture above, apart from recognizing the person, the road, the cars, the trees, etc., we also have to delineate the boundaries of each object. Therefore, unlike classification, we need dense pixel-wise predictions from our models. As with other computer vision tasks, CNNs have had enormous success on segmentation problems. One of the popular initial approaches was patch classification through sliding window, where each pixel was separately classified into classes using a patch of images around it. This, however, is very inefficient computationally because we don\u2019t reuse the shared features between overlapping patches. The solution, instead, is UC Berkeley\u2019s Fully Convolutional Networks (FCN), which popularized end-to-end CNN architectures for dense predictions without any fully connected layers. This allowed segmentation maps to be generated for images of any size and was also much faster compared to the patch classification approach. Almost all subsequent t approaches on semantic segmentation adopted this paradigm. However, one problem remains: convolutions at original image resolution will be very expensive. To deal with this, FCN uses downsampling and upsampling inside the network. The downsampling layer is known as striped convolution, while the upsampling layer is known as transposed convolution. Despite the upsampling/downsampling layers, FCN produces coarse segmentation maps because of information loss during pooling. SegNet is a more memory efficient architecture than FCN that uses-max pooling and an encoder-decoder framework. In SegNet, shortcut/skip connections are introduced from higher resolution feature maps to improve the coarseness of upsampling/downsampling. Recent research in Semantic Segmentation all relies heavily on fully convolutional networks, such as Dilated Convolutions, DeepLab, and RefineNet.\n\nBeyond Semantic Segmentation, Instance Segmentation segments different instances of classes, such as labelling 5 cars with 5 different colors. In classification, there\u2019s generally an image with a single object as the focus and the task is to say what that image is. But in order to segment instances, we need to carry out far more complex tasks. We see complicated sights with multiple overlapping objects and different backgrounds, and we not only classify these different objects but also identify their boundaries, differences, and relations to one another! So far, we\u2019ve seen how to use CNN features in many interesting ways to effectively locate different objects in an image with bounding boxes. Can we extend such techniques to locate exact pixels of each object instead of just bounding boxes? This instance segmentation problem is explored at Facebook AI using an architecture known as Mask R-CNN. Much like Fast R-CNN, and Faster R-CNN, Mask R-CNN\u2019s underlying intuition is straightforward Given that Faster R-CNN works so well for object detection, could we extend it to also carry out pixel-level segmentation? Mask R-CNN does this by adding a branch to Faster R-CNN that outputs a binary mask that says whether or not a given pixel is part of an object. The branch is a Fully Convolutional Network on top of a CNN-based feature map. Given the CNN Feature Map as the input, the network outputs a matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere (this is known as a binary mask). Additionally, when run without modifications on the original Faster R-CNN architecture, the regions of the feature map selected by RoIPool (Region of Interests Pool) were slightly misaligned from the regions of the original image. Since image segmentation requires pixel-level specificity, unlike bounding boxes, this naturally led to inaccuracies. Mask R-CNN solves this problem by adjusting RoIPool to be more precisely aligned using a method known as RoIAlign (Region of Interests Align). Essentially, RoIAlign uses bilinear interpolation to avoid error in rounding, which causes inaccuracies in detection and segmentation. Once these masks are generated, Mask R-CNN combines them with the classifications and bounding boxes from Faster R-CNN to generate such wonderfully precise segmentations:\n\nThese 5 major computer vision techniques can help a computer extract, analyze, and understand useful information from a single or a sequence of images. There are many other advanced techniques that I haven\u2019t touched, including style transfer, colorization, action recognition, 3D objects, human pose estimation, and more. Indeed, the field of Computer Vision is too expensive to cover in depth, and I would encourage you to explore it further, whether through online courses, blog tutorials, or formal documents. I\u2019d highly recommend CS231n for starters, as you\u2019ll learn to implement, train, and debug your own neural networks. As a bonus, you can get all the lecture slides and assignment guidelines from my GitHub repository. I hope it\u2019ll guide you in the quest of changing how to see the world! If you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://heartbeat.fritz.ai/the-5-deep-learning-frameworks-every-serious-machine-learner-should-be-familiar-with-93f4d469d24c?source=user_profile---------6----------------",
        "title": "The 5 Deep Learning Frameworks Every Serious Machine Learner Should Be Familiar With",
        "text": "Deep Learning requires a lot of computations. It typically involves neural network(s) with many nodes, and every node has many connections \u2014 which must be updated constantly during the learning. In other words, at each layer of the network, hundreds and thousands of identical artificial neurons perform the same computation. Therefore, the structure of a neural network fits very well with the kinds of computation that a GPU (Graphic Processing Unit) can efficiently perform \u2014 which are designed to compute, in parallel, the same instructions.\n\nAs the deep learning and AI fields have been moving extremely fast in the last few years, we\u2019ve also seen the introduction of many deep learning frameworks. Deep learning frameworks are created with the goal to run deep learning systems efficiently on GPUs. They all rely on the concept of computational graphs \u2014 which define the order of computations that need to be performed. What you have in these frameworks is a language that sets up the computational graph and an execution mechanism that\u2019s different from the host language. Then the graph can be optimized and run, in parallel, in the target GPU.\n\nIn this post, I want to introduce to you the 5 frameworks that are the workhorses of deep learning development. They make it easier for data scientists and engineers to build deep learning solutions for complex problems and perform tasks of greater sophistication. These are just a small selection of a wide range of open-source frameworks, backed by different tech giants, which push each other to innovate faster.\n\nTensorFlow was originally developed by researchers and engineers working on the Google Brain Team. Its purpose is geared towards deep neural networks and machine intelligence research. The library has officially been open-sourced on GitHub since late 2015. TensorFlow is extremely useful to do graph-based computations quickly. The flexible TensorFlow API can deploy models across multiple devices with its GPU-supported architecture.\n\nIn brief, the TensorFlow ecosystem has 3 main components:\n\nTensorFlow has been used widely in academic research and industrial applications. Some notable current uses include Deep Speech, RankBrain, SmartReply, and On-Device Computer Vision. You can check out some of the best official uses, research models, samples, and tutorials of TensorFlow at this GitHub repo.\n\nLet\u2019s take a look at a running example. Here I train a 2-layer ReLU network on random data with L2 loss on TensorFlow.\n\nThere are 2 main components of this code: defining computational graph and running this graph many times. While defining the graph, I create placeholders for input x, weights w1 and w2, and targets y. Then during the forward pass, I compute prediction for target y and the loss variable, which is the L2 distance between y and y_pred. Lastly, I tell TensorFlow to compute loss of gradient with respect to w1 and w2. After getting done building the graph, I enter a session to run the graph. Here I create numpy arrays that will fill in the placeholders above, feeding them for x, y, w1, w2. In order to train the network, I run the graph over and over, using gradient to update the weights and getting numpy arrays for loss, grad_w1, and grad_w2.\n\nDeep Learning frameworks operate at 2 levels of abstractions: Low Level \u2014 where mathematical operations and neural network primitives are implemented (TensorFlow, Theano, PyTorch etc.) and High Level \u2014 where low level primitives are used to implement neural network abstractions, such as models and layers (Keras).\n\nKeras is a wrapper over its backend libraries, which can be TensorFlow or Theano \u2014 meaning that if you\u2019re using Keras with TensorFlow backend, you\u2019re running TensorFlow code. Keras takes care a lot of the nitty-gritty details for you, as it\u2019s geared towards neural network technology consumers and is well suited for those practicing data science. It allows for easy and fast prototyping, supports multiple neural network architectures, and runs seamlessly on CPU/GPU.\n\nIn this example doing similar neural network training as above, I first define the model object as a sequence of layers, then define the optimizer object. Next, I build the model, specify loss function, and train the model with a single \u2018fit\u2019 line.\n\nTheano is another Python library for fast numerical computation that can be run on the CPU or GPU. It is an open-source project developed by the Montreal Institute for Learning Algorithms group at University of Montreal. Some of its most prominent features include transparent use of GPU, tight integration with NumPy, efficient symbolic differentiation, speed / stability optimizations, and extensive unit testing.\n\nUnfortunately, Youshua Bengio (head of MILA lab) announced in Nov. 2017 that they will no longer actively maintain or develop Theano. The reason is that most of the innovations Theano introduced across the years have now been adopted and perfected by other frameworks. If interested, you can still contribute to its open-source library.\n\nTheano is similar in many ways to TensorFlow. So let\u2019s take a look at another code example training neural networks using the same batch size and input/output dimensions:\n\nI first define Theano symbolic variables (similar to TensorFlow placeholder). For the forward pass, I compute predictions and loss; for the backward pass, I compute gradients. Then I compile a function that computes loss, scores, and gradients from data and weights. Lastly, I run this function many times to train the network.\n\nPyTorch is a relatively new deep learning framework that is quickly becoming popular among researchers. The Facebook AI Research team developed it to address challenges in the adoption of its predecessor library, Torch. Due to the low popularity of the programming language Lua, Torch can never experience the growth that Google\u2019s TensorFlow has. Thus, PyTorch adopted the native Python imperative programming style, which is already familiar to many researchers, developers, and data scientists. It also supports dynamic computation graphs, a feature that makes it attractive to researchers and engineers working with time-series and natural language processing data.\n\nThe best adoption so far has come from Uber, which has built Pyro \u2014 a universal probabilistic programming language using PyTorch as its backend. PyTorch\u2019s dynamic ability to perform differentiation and construct gradients is extremely valuable for random operations in a probabilistic model.\n\nHere I am going to focus on the Tensor abstraction level. PyTorch Tensors are just like numpy arrays, but they can run on GPU. No built-in notion of computational graph, or gradients, or deep learning. Here we fit a 2-layer net using PyTorch Tensors:\n\nAs you can see, I first create random tensors for data and weights. Then I compute predictions and loss during forward pass, and compute gradients manually during backward pass. I also add gradient descent step on each weight. Finally, I train the network by running the function many times.\n\nLet\u2019s talk about Torch a bit. It is Facebook\u2019s open-source machine learning library, scientific computing framework, and script language based on the Lua programming language. It provides a wide range of algorithms for deep learning, and has been adapted by Facebook, IBM, Yandex, and others to solve hardware problems for data flows.\n\nAs the direct ancestor of PyTorch, Torch shares a lot of its C backend. Unlike PyTorch which has 3 levels of abstraction, Torch only has 2: Tensor and Module. Let\u2019s explore a code tutorial training 2-layer net using Torch\u2019s Tensor:\n\nInitially, I build a model as a sequence of layers, as well as a loss function. Next, I define a callback function that inputs weights and produces loss/gradient on weights. Inside the function, I compute predictions and loss in forward pass, as well as gradient in backward pass. Lastly, I pass this callback function to the optimizer over and over again.\n\nCaffe is a deep learning framework made with expression, speed, and modularity in mind. It was developed by the Berkeley AI Research group and the Berkeley Vision and Learning Center. While its core is written in C++, Caffe has Python and Matlab bindings. It\u2019s very good for training or fine-tuning feedforward classification models. While it\u2019s not used as much in research, it\u2019s still popular for deploying models as evidenced by the community contributors.\n\nIn order to train and fine-tune neural networks using Caffe, you\u2019ll need go through 4 steps:\n\nI won\u2019t do a code walk-through for Caffe, but you can check out a tutorial on Caffe\u2019s main page. Overall, Caffe is really good for feedforward networks and for fine-tuning existing networks. You can easily train models without writing any code. Its Python interface is quite useful, as you can deploy the model without Python code. On the negative side, you need to write core C++ code (under Caffe) for every new GPU layer. Thus, it\u2019s very cumbersome for big networks (AlexNet, VGG, GoogLeNet, ResNet etc.)\n\nWith Theano no longer being developed, Torch written in the unfamiliar language Lua, and Caffe being in its precocious state, TensorFlow and PyTorch emerge as the preferred frameworks of most deep learning practitioners. While both frameworks use Python, there are a couple of differences between them:\n\nMost importantly, TensorFlow is \u201cDefine-and-Run\u201d, in which one would define conditions and iterations in the graph structure, then run it. On the other hand, PyTorch is \u201cDefine-by-Run\u201d, in which graph structure is defined on-the-fly during forward computation. In other words, TensorFlow uses static computational graph, while PyTorch uses dynamic computational graph. The dynamic graph-based approach gives easier debuggability and more processing power for complex architecture, such as dynamic neural networks. The static graph-based approach gives easier deployment to mobile, easier deployment to more exotic architectures, and the ability to do compiler techniques ahead of time.\n\nFor that reason, PyTorch is better for rapid prototyping for hobbyists and small-scale projects, while TensorFlow is better for large-scale deployments, especially when cross-platform and embedded deployment are considerations. TensorFlow has stood the test of time and is still more widely used. It has more capabilities and better scalability for larger projects. PyTorch is gaining momentum as it\u2019s easier to learn, but it doesn\u2019t have the same integration. It\u2019s very good for small projects that need to be done quickly, but is not optimal for product deployment."
    },
    {
        "url": "https://medium.com/the-post-grad-survival-guide/15-systems-thinking-guidelines-to-live-in-a-world-of-uncertainty-26f62b69bee6?source=user_profile---------7----------------",
        "title": "15 Systems Thinking Guidelines to Live in a World of Uncertainty",
        "text": "15 Systems Thinking Guidelines to Live in a World of Uncertainty Systems thinking is a way of seeing the world as a series of interconnected and interdependent systems rather than lots of independent parts. As a thinking tool, it seeks to oppose the reductionist view \u2014 the idea that a system can be understood by the sum of its isolated parts \u2014 and replace it with expansionism, the view that everything is part of a larger whole and that the connections between all elements are critical. Systems are essentially networks made up of nodes or agents that are linked in varied and diverse ways. What we want to do in systems thinking is be able to identify and understand these relationships as part of the exploration of the larger systems at play. Everything is interconnected, every system is made up of many subsystems, and is itself a part of larger systems. Seeing things in this way helps to create a more flexible view of the world and the way it works, and it illuminates opportunities for addressing some of its existing and evolving problem arenas. Living successfully in a world of systems requires more of us than our ability to calculate. It requires our full humanity \u2014 our rationality, our ability to sort out truth from falsehood, our intuition, our compassion, our vision, and our morality. In one of the chapters from her famous book \u2014 Thinking In Systems, Donella Meadows summarized the most general \u201csystems wisdoms\u201d she has absorbed from modeling complex systems and from hanging out with modelers. These are the take-home lessons, the concepts and practices that penetrate the discipline of systems so deeply that one begins, however imperfectly, to practice them not just in one\u2019s profession, but in all of life. They are the behavioral consequences of a worldview based on the ideas of feedback, nonlinearity and systems responsible for their own behavior. After spending the last couple days finishing the book, I want to share these 15 systems wisdoms/guidelines as I believe everyone can benefit from adopting them. 1 \u2014 Get the beat of the system Before you disturb the system in any way, watch how it behaves. If it\u2019s a piece of music or a whitewater rapid or a fluctuation in a commodity price, study its beat. If it\u2019s a social system, watch it work. Learn its history. Ask people who\u2019ve been around a long time to tell you what has happened. If possible, find or make a time graph of actual data from the system \u2014 peoples\u2019 memories are not always reliable when it comes to timing. This guideline is deceptively simple. Until you make it a practice, you won\u2019t believe how many wrong turns it helps you avoid. Starting with the behavior of the system forces you to focus on facts, not theories. It keeps you from falling too quickly into your own beliefs or misconceptions, or those of others.\n\nStarting with the behavior of the system directs one\u2019s thoughts to dynamic, not static, analysis \u2014 not only to \u201cWhat\u2019s wrong?\u201d but also to \u201cHow did we get there?\u201d \u201cWhat other behavior modes are possible?\u201d \u201cIf we don\u2019t change direction, where are we going to end up?\u201d And looking to the strengths of the system, one can ask \u201cWhat\u2019s working well here?\u201d Starting with the history of several variables plotted together begins to suggest not only what elements are in the system, but how they might be interconnected. And finally, starting with history discourages the common and distracting tendency we all have to define a problem not by the system\u2019s actual behavior, but by the lack of our favorite solution. Listen to any discussion, in your family or a committee meeting at work or among the pundits in the media, and watch people leap to solutions, usually solutions in \u201cpredict, control, or impose your will\u201d mode, without having paid any attention to what the system is doing and why it\u2019s doing it. 2 \u2014 Expose your mental models to the light of day When we draw structural diagrams and then write equations, we are forced to make our assumptions visible and to express them with rigor. We have to put everyone of our assumptions about the system out where others can see them. Our models have to be complete, and they have to add up, and they have to be consistent. Our assumptions can no longer slide around (mental models are very slippery), assuming one thing for purposes of one discussion and something else contradictory for purposes of the next discussion. You don\u2019t have to put forth your mental model with diagrams and equations, although doing so is a good practice. You can do it with words or lists or pictures or arrows showing what you think is connected to what. The more you do that, in any form, the clearer your thinking will become, the faster you will admit your uncertainties and correct your mistakes, and the more flexible you will learn to be. Mental flexibility \u2014 the willingness to redraw boundaries, to notice that a system has shifted into a new mode, to see how to redesign structure \u2014 is a necessity when you live in a world of flexible systems.\n\nRemember, always, that everything you know, and everything everyone knows, is only a model. Get your model out there where it can be viewed. Invite others to challenge your assumptions and add their own. Instead of becoming a champion for one possible explanation or hypothesis or model, collect as many as possible. Consider all of them to be plausible until you find some evidence that causes you to rule one out. That way you will be emotionally able to see the evidence that rules out an assumption that may become entangled with your own identity. Getting models out into the light of day, making them as rigorous as possible, testing them against the evidence, and being willing to scuttle them if they are no longer supported is nothing more than practicing the scientific method \u2014 something that is done too seldom even in science, and is done hardly at all in social science or management or government or everyday life. You\u2019ve seen how information holds systems together and how delayed, biased, scattered, or missing information can make feedback loops malfunction. Decision makers can\u2019t respond to information they don\u2019t have, can\u2019t respond accurately to information that is inaccurate, and can\u2019t respond in a timely way to information that is late. I would guess that most of what goes wrong in systems goes wrong because of biased, late, or missing information. If I could, I would add an 11th commandment to the first 10: Thou shalt not distort, delay, or withhold information. You can drive a system crazy by mudding its information streams. You can make a system work better with surprising ease if you can give it more timely, more accurate, more complete information.\n\nInformation is power. Anyone interested in power grasps that idea very quickly. The media, the public relations people, the politicians, and advertisers who regulate much of the public flow of information have far more power than most people realize. They filer and channel information. Often they do so for short-term, self-interested purposes. It\u2019s no wonder our that social systems so often run amok. 4 \u2014 Use language with care and enrich it with systems concepts Our information streams are composed primarily of language. Our mental models are mostly verbal. Honoring information means above all avoiding language pollution \u2014 making the cleanest possible use we can of language. Second, it means expanding our language so we can talk about complexity. A society that talks incessantly about \u201cproductivity\u201d but that hardly understands, much less uses, the word \u201cresilience\u201d is going to become productive and not resilient. A society that doesn\u2019t understand or use the term \u201ccarrying capacity\u201d will exceed its carrying capacity. A society that talks about \u201ccreating jobs\u201d as if that\u2019s something only companies can do will not inspire the great majority of its people to create jobs, for themselves or anyone else. Nor will it appreciate its workers for their role in \u201ccreating profits.\u201d And of course a society that talks about a \u201cPeacekeeper\u201d missile or \u201ccollateral damage,\u201d a \u201cFinal Solution\u201d or \u201cethnic cleansing,\u201d is speaking \u201ctyrannies.\u201d\n\nThe first step in respecting language is keeping it as concrete, meaningful, and truthful as possible \u2014 part of the job of keeping information streams clear. The second step is to enlarge language to make it consistent with our enlarged understanding of systems. If the Eskimos have so many words for snow, it\u2019s because they have studied and learned how to use snow. They have turned snow into a resource, a system with which they can dance. The industrial society is just beginning to pay attention to and use complexity. Carrying capacity, structure, diversity, and even system are old words that are coming to have richer and more precise meanings. New words are having to be invented. 5 \u2014 Pay attention to what is important, not just what is quantifiable Our culture, obsessed with numbers, has given us the idea that what we can measure is more important than what we can\u2019t measure. Think about that for a minute. It means that we make quantity more important than quality. If quantity forms the goals of our feedback loops, if quantity is the center of our attention and language and institutions, if we motivate ourselves, rate ourselves, and reward ourselves on our ability to produce quantity, then quantity will be the result. You can look around and make up your own mind about whether quantity or quality is the outstanding characteristic of the world in which you live.\n\nPretending that something doesn\u2019t exist if it\u2019s hard to quantify leads to faulty models. You\u2019ve already seen the system trap that comes from setting goals around what is easily measured, rather than around what is important. So don\u2019t fall into that trap. Human beings have been endowed not only with the ability to count, but also with the ability to assess quality. Be a quality detector. Be a walking, noisy Geiger counter that registers the presence or absence of quality. If something is ugly, say so. If it is tacky, inappropriate, out of proportion, unsustainable, morally degrading, ecologically impoverishing, or humanly demeaning, don\u2019t let it pass. Don\u2019t be stopped by the \u201cif you can\u2019t define it and measure it, I don\u2019t have to pay attention to it\u201d ploy. No one can define or measure justice, democracy, security, freedom, truth, or love. No one can define or measure any value. But if no one speaks up for them, if systems aren\u2019t designed to produce them, if we don\u2019t speak about them and point toward their presence or absence, they will cease to exist. You can imagine why a dynamic, self-adjusting feedback system cannot be governed by a static, unbending policy. It\u2019s easier, more effective, and usually much cheaper to design policies that change depending on the state of the system. Especially where there are great uncertainties, the best policies not only contain feedback loops, but meta-feedback loops \u2014 loops that alter, correct, and expand loops. These are policies that design learning into the management process.\n\nAn example was the historic Montreal Protocol to protect the ozone layer of the stratosphere. In 1987, when that protocol was signed, there was no certainty about the danger to the ozone layer, about the rate at which it was degrading, or about the specific effect of different chemicals. The protocol set targets for how fast the manufacture of the most damaging chemicals should be decreased. But it also required monitoring the situation and reconvening an international congress to change the phase-out schedule, if the damage to the ozone layer turned out to be more or less than expected. Just 3 years later, in 1990, the schedule had to be hurried forward and more chemicals added to it, because the damage was turning out to be much greater than was foreseen in 1987. That was feedback policy, structured for learning. We all hope that it worked in time. 7 \u2014 Go for the good of the whole Remember that hierarchies exist to serve the bottom layers, not the top. Don\u2019t maximize parts of systems or subsystems while ignoring the whole. Don\u2019t go to great trouble to optimize something that never should be done at all. Aim to enhance total systems properties, such as growth, stability, diversity, resilience, and sustainability \u2014 whether they are easily measured or not.\n\n\u201cIntrinsic responsibility\u201d means that the system is designed to send feedback about the consequences of decision making directly and quickly and compellingly to the decision makers. Because the pilot of a plane rides in the front of the plane, that pilot is intrinsically responsible. He/she will experience directly the consequences of his/her decisions. The thing to do, when you don\u2019t know, is not to bluff and not to freeze, but to learn. The way you learn is by experiment, by trial and error. In a world of complex systems, it is not appropriate to charge forward with rigid, undeviating directives. \u201cStay the course\u201d is only a good idea if you are sure you are on course. Pretending you are in control even when you aren\u2019t is a recipe not only for mistakes, but for not learning from mistakes. What\u2019s appropriate when you\u2019re learning is small steps, constant monitoring, and a willingness to change course as you find out more about where it\u2019s leading.\n\nThere\u2019s something within the human mind that is attracted to straight lines and not curves, to whole numbers and not fractions, to uniformity and not diversity, and to certainties and not mystery. But there is something else within us that has the opposite set of tendencies, since we ourselves evolved out of and are shaped by and structured as complex feedback systems. Only a part of us, a part that has emerged recently, designs buildings as boxes with uncompromising straight lines and flat surfaces. Another part of us recognizes on every scale from the microscopic to the macroscopic. That part of us makes Gothic cathedrals and Persian carpets, symphonies and novels, Mardi Gras costumes and artificial intelligence programs, all with embellishments almost as complex as the ones we find in the world around us. In a strict systems sense, there is no long-term, short-term distinction. Phenomena at different time-scales are nested within each other. Actions taken now have some immediate effects and some that radiate out for decades to come. We experience now the consequences of actions set in motion yesterday and decades ago and centuries ago. The couplings between very fast processes and very slowly ones are sometimes strong, sometimes weak. When the slow ones dominate, nothing seems to be happening; when the fast ones take over, things happen with breathtaking speed. Systems are always coupling and uncoupling the large and the small, the fast and the slow.\n\nWhen you\u2019re walking along a tricky, curving, unknown, surprising, obstacle-strewn path, you\u2019d be a fool to keep your head down and look just at the next step in front of you. You\u2019d be equally a fool to just peer far ahead and never notice what\u2019s immediately under your feet. You need to be watching both the short and the long term \u2014 the whole system. In spite of what you majored in, or what the textbooks say, or what you think you\u2019re an expert at, follow a system wherever it leads. It will be sure to lead across traditional disciplinary lines. To understand that system, you will have to be able to learn from \u2014 while not being limited by \u2014 economists and chemists and psychologists and theologians. You will have to penetrate their jargons, integrate what they tell you, recognize what they can honestly see through their particular lenses, and discard the distortions that come from the narrowness and incompleteness of their lenses. They won\u2019t make it easy for you.\n\nSeeing systems whole requires more than being \u201cinterdisciplinary\u201d, if that word means, as it usually goes, putting together people from different disciplines and letting them talk past each other. Interdisciplinary communication works only if there is a real problem to be solved, and if the representatives from the various disciplines are more committed to solving the problem than to being academically correct. They will have to go into learning mode. They will have to admit ignorance and be willing to be taught, by each other and by the system. Living successfully in a world of complex systems means expanding not only time horizons and though horizons; above all, it means expanding the horizons of caring. There are moral reasons for doing that, of course. And if moral arguments are not sufficient, then systems thinking provides the practical reasons to back up the moral ones. The real system is interconnected. No part of the human race is separate either from other human beings or from the global ecosystem. It will not be possible in this integrated world for your heart to succeed if your lungs fail, or for your company to succeed if your workers fail, or for the rich in Los Angeles to succeed if the poor in Los Angeles fail, or for Europe to succeed if Africa fails, or for the global economy to succeed if the global environment fails."
    },
    {
        "url": "https://medium.com/@james_aka_yale/15-systems-thinking-guidelines-to-live-in-a-world-of-uncertainty-b07d5a478e6?source=user_profile---------8----------------",
        "title": "15 Systems Thinking Guidelines to Live in a World of Uncertainty",
        "text": "15 Systems Thinking Guidelines to Live in a World of Uncertainty Systems thinking is a way of seeing the world as a series of interconnected and interdependent systems rather than lots of independent parts. As a thinking tool, it seeks to oppose the reductionist view \u2014 the idea that a system can be understood by the sum of its isolated parts \u2014 and replace it with expansionism, the view that everything is part of a larger whole and that the connections between all elements are critical. Systems are essentially networks made up of nodes or agents that are linked in varied and diverse ways. What we want to do in systems thinking is be able to identify and understand these relationships as part of the exploration of the larger systems at play. Everything is interconnected, every system is made up of many subsystems, and is itself a part of larger systems. Seeing things in this way helps to create a more flexible view of the world and the way it works, and it illuminates opportunities for addressing some of its existing and evolving problem arenas. Living successfully in a world of systems requires more of us than our ability to calculate. It requires our full humanity \u2014 our rationality, our ability to sort out truth from falsehood, our intuition, our compassion, our vision, and our morality. In one of the chapters from her famous book \u2014 Thinking In Systems, Donella Meadows summarized the most general \u201csystems wisdoms\u201d she has absorbed from modeling complex systems and from hanging out with modelers. These are the take-home lessons, the concepts and practices that penetrate the discipline of systems so deeply that one begins, however imperfectly, to practice them not just in one\u2019s profession, but in all of life. They are the behavioral consequences of a worldview based on the ideas of feedback, nonlinearity and systems responsible for their own behavior. After spending the last couple days finishing the book, I want to share these 15 systems wisdoms/guidelines as I believe everyone can benefit from adopting them. 1 \u2014 Get the beat of the system Before you disturb the system in any way, watch how it behaves. If it\u2019s a piece of music or a whitewater rapid or a fluctuation in a commodity price, study its beat. If it\u2019s a social system, watch it work. Learn its history. Ask people who\u2019ve been around a long time to tell you what has happened. If possible, find or make a time graph of actual data from the system \u2014 peoples\u2019 memories are not always reliable when it comes to timing. This guideline is deceptively simple. Until you make it a practice, you won\u2019t believe how many wrong turns it helps you avoid. Starting with the behavior of the system forces you to focus on facts, not theories. It keeps you from falling too quickly into your own beliefs or misconceptions, or those of others.\n\nStarting with the behavior of the system directs one\u2019s thoughts to dynamic, not static, analysis \u2014 not only to \u201cWhat\u2019s wrong?\u201d but also to \u201cHow did we get there?\u201d \u201cWhat other behavior modes are possible?\u201d \u201cIf we don\u2019t change direction, where are we going to end up?\u201d And looking to the strengths of the system, one can ask \u201cWhat\u2019s working well here?\u201d Starting with the history of several variables plotted together begins to suggest not only what elements are in the system, but how they might be interconnected. And finally, starting with history discourages the common and distracting tendency we all have to define a problem not by the system\u2019s actual behavior, but by the lack of our favorite solution. Listen to any discussion, in your family or a committee meeting at work or among the pundits in the media, and watch people leap to solutions, usually solutions in \u201cpredict, control, or impose your will\u201d mode, without having paid any attention to what the system is doing and why it\u2019s doing it. 2 \u2014 Expose your mental models to the light of day When we draw structural diagrams and then write equations, we are forced to make our assumptions visible and to express them with rigor. We have to put everyone of our assumptions about the system out where others can see them. Our models have to be complete, and they have to add up, and they have to be consistent. Our assumptions can no longer slide around (mental models are very slippery), assuming one thing for purposes of one discussion and something else contradictory for purposes of the next discussion. You don\u2019t have to put forth your mental model with diagrams and equations, although doing so is a good practice. You can do it with words or lists or pictures or arrows showing what you think is connected to what. The more you do that, in any form, the clearer your thinking will become, the faster you will admit your uncertainties and correct your mistakes, and the more flexible you will learn to be. Mental flexibility \u2014 the willingness to redraw boundaries, to notice that a system has shifted into a new mode, to see how to redesign structure \u2014 is a necessity when you live in a world of flexible systems.\n\nRemember, always, that everything you know, and everything everyone knows, is only a model. Get your model out there where it can be viewed. Invite others to challenge your assumptions and add their own. Instead of becoming a champion for one possible explanation or hypothesis or model, collect as many as possible. Consider all of them to be plausible until you find some evidence that causes you to rule one out. That way you will be emotionally able to see the evidence that rules out an assumption that may become entangled with your own identity. Getting models out into the light of day, making them as rigorous as possible, testing them against the evidence, and being willing to scuttle them if they are no longer supported is nothing more than practicing the scientific method \u2014 something that is done too seldom even in science, and is done hardly at all in social science or management or government or everyday life. You\u2019ve seen how information holds systems together and how delayed, biased, scattered, or missing information can make feedback loops malfunction. Decision makers can\u2019t respond to information they don\u2019t have, can\u2019t respond accurately to information that is inaccurate, and can\u2019t respond in a timely way to information that is late. I would guess that most of what goes wrong in systems goes wrong because of biased, late, or missing information. If I could, I would add an 11th commandment to the first 10: Thou shalt not distort, delay, or withhold information. You can drive a system crazy by mudding its information streams. You can make a system work better with surprising ease if you can give it more timely, more accurate, more complete information.\n\nInformation is power. Anyone interested in power grasps that idea very quickly. The media, the public relations people, the politicians, and advertisers who regulate much of the public flow of information have far more power than most people realize. They filer and channel information. Often they do so for short-term, self-interested purposes. It\u2019s no wonder our that social systems so often run amok. 4 \u2014 Use language with care and enrich it with systems concepts Our information streams are composed primarily of language. Our mental models are mostly verbal. Honoring information means above all avoiding language pollution \u2014 making the cleanest possible use we can of language. Second, it means expanding our language so we can talk about complexity. A society that talks incessantly about \u201cproductivity\u201d but that hardly understands, much less uses, the word \u201cresilience\u201d is going to become productive and not resilient. A society that doesn\u2019t understand or use the term \u201ccarrying capacity\u201d will exceed its carrying capacity. A society that talks about \u201ccreating jobs\u201d as if that\u2019s something only companies can do will not inspire the great majority of its people to create jobs, for themselves or anyone else. Nor will it appreciate its workers for their role in \u201ccreating profits.\u201d And of course a society that talks about a \u201cPeacekeeper\u201d missile or \u201ccollateral damage,\u201d a \u201cFinal Solution\u201d or \u201cethnic cleansing,\u201d is speaking \u201ctyrannies.\u201d\n\nThe first step in respecting language is keeping it as concrete, meaningful, and truthful as possible \u2014 part of the job of keeping information streams clear. The second step is to enlarge language to make it consistent with our enlarged understanding of systems. If the Eskimos have so many words for snow, it\u2019s because they have studied and learned how to use snow. They have turned snow into a resource, a system with which they can dance. The industrial society is just beginning to pay attention to and use complexity. Carrying capacity, structure, diversity, and even system are old words that are coming to have richer and more precise meanings. New words are having to be invented. 5 \u2014 Pay attention to what is important, not just what is quantifiable Our culture, obsessed with numbers, has given us the idea that what we can measure is more important than what we can\u2019t measure. Think about that for a minute. It means that we make quantity more important than quality. If quantity forms the goals of our feedback loops, if quantity is the center of our attention and language and institutions, if we motivate ourselves, rate ourselves, and reward ourselves on our ability to produce quantity, then quantity will be the result. You can look around and make up your own mind about whether quantity or quality is the outstanding characteristic of the world in which you live.\n\nPretending that something doesn\u2019t exist if it\u2019s hard to quantify leads to faulty models. You\u2019ve already seen the system trap that comes from setting goals around what is easily measured, rather than around what is important. So don\u2019t fall into that trap. Human beings have been endowed not only with the ability to count, but also with the ability to assess quality. Be a quality detector. Be a walking, noisy Geiger counter that registers the presence or absence of quality. If something is ugly, say so. If it is tacky, inappropriate, out of proportion, unsustainable, morally degrading, ecologically impoverishing, or humanly demeaning, don\u2019t let it pass. Don\u2019t be stopped by the \u201cif you can\u2019t define it and measure it, I don\u2019t have to pay attention to it\u201d ploy. No one can define or measure justice, democracy, security, freedom, truth, or love. No one can define or measure any value. But if no one speaks up for them, if systems aren\u2019t designed to produce them, if we don\u2019t speak about them and point toward their presence or absence, they will cease to exist. You can imagine why a dynamic, self-adjusting feedback system cannot be governed by a static, unbending policy. It\u2019s easier, more effective, and usually much cheaper to design policies that change depending on the state of the system. Especially where there are great uncertainties, the best policies not only contain feedback loops, but meta-feedback loops \u2014 loops that alter, correct, and expand loops. These are policies that design learning into the management process.\n\nAn example was the historic Montreal Protocol to protect the ozone layer of the stratosphere. In 1987, when that protocol was signed, there was no certainty about the danger to the ozone layer, about the rate at which it was degrading, or about the specific effect of different chemicals. The protocol set targets for how fast the manufacture of the most damaging chemicals should be decreased. But it also required monitoring the situation and reconvening an international congress to change the phase-out schedule, if the damage to the ozone layer turned out to be more or less than expected. Just 3 years later, in 1990, the schedule had to be hurried forward and more chemicals added to it, because the damage was turning out to be much greater than was foreseen in 1987. That was feedback policy, structured for learning. We all hope that it worked in time. 7 \u2014 Go for the good of the whole Remember that hierarchies exist to serve the bottom layers, not the top. Don\u2019t maximize parts of systems or subsystems while ignoring the whole. Don\u2019t go to great trouble to optimize something that never should be done at all. Aim to enhance total systems properties, such as growth, stability, diversity, resilience, and sustainability \u2014 whether they are easily measured or not.\n\n\u201cIntrinsic responsibility\u201d means that the system is designed to send feedback about the consequences of decision making directly and quickly and compellingly to the decision makers. Because the pilot of a plane rides in the front of the plane, that pilot is intrinsically responsible. He/she will experience directly the consequences of his/her decisions. The thing to do, when you don\u2019t know, is not to bluff and not to freeze, but to learn. The way you learn is by experiment, by trial and error. In a world of complex systems, it is not appropriate to charge forward with rigid, undeviating directives. \u201cStay the course\u201d is only a good idea if you are sure you are on course. Pretending you are in control even when you aren\u2019t is a recipe not only for mistakes, but for not learning from mistakes. What\u2019s appropriate when you\u2019re learning is small steps, constant monitoring, and a willingness to change course as you find out more about where it\u2019s leading.\n\nThere\u2019s something within the human mind that is attracted to straight lines and not curves, to whole numbers and not fractions, to uniformity and not diversity, and to certainties and not mystery. But there is something else within us that has the opposite set of tendencies, since we ourselves evolved out of and are shaped by and structured as complex feedback systems. Only a part of us, a part that has emerged recently, designs buildings as boxes with uncompromising straight lines and flat surfaces. Another part of us recognizes on every scale from the microscopic to the macroscopic. That part of us makes Gothic cathedrals and Persian carpets, symphonies and novels, Mardi Gras costumes and artificial intelligence programs, all with embellishments almost as complex as the ones we find in the world around us. In a strict systems sense, there is no long-term, short-term distinction. Phenomena at different time-scales are nested within each other. Actions taken now have some immediate effects and some that radiate out for decades to come. We experience now the consequences of actions set in motion yesterday and decades ago and centuries ago. The couplings between very fast processes and very slowly ones are sometimes strong, sometimes weak. When the slow ones dominate, nothing seems to be happening; when the fast ones take over, things happen with breathtaking speed. Systems are always coupling and uncoupling the large and the small, the fast and the slow.\n\nWhen you\u2019re walking along a tricky, curving, unknown, surprising, obstacle-strewn path, you\u2019d be a fool to keep your head down and look just at the next step in front of you. You\u2019d be equally a fool to just peer far ahead and never notice what\u2019s immediately under your feet. You need to be watching both the short and the long term \u2014 the whole system. In spite of what you majored in, or what the textbooks say, or what you think you\u2019re an expert at, follow a system wherever it leads. It will be sure to lead across traditional disciplinary lines. To understand that system, you will have to be able to learn from \u2014 while not being limited by \u2014 economists and chemists and psychologists and theologians. You will have to penetrate their jargons, integrate what they tell you, recognize what they can honestly see through their particular lenses, and discard the distortions that come from the narrowness and incompleteness of their lenses. They won\u2019t make it easy for you.\n\nSeeing systems whole requires more than being \u201cinterdisciplinary\u201d, if that word means, as it usually goes, putting together people from different disciplines and letting them talk past each other. Interdisciplinary communication works only if there is a real problem to be solved, and if the representatives from the various disciplines are more committed to solving the problem than to being academically correct. They will have to go into learning mode. They will have to admit ignorance and be willing to be taught, by each other and by the system. Living successfully in a world of complex systems means expanding not only time horizons and though horizons; above all, it means expanding the horizons of caring. There are moral reasons for doing that, of course. And if moral arguments are not sufficient, then systems thinking provides the practical reasons to back up the moral ones. The real system is interconnected. No part of the human race is separate either from other human beings or from the global ecosystem. It will not be possible in this integrated world for your heart to succeed if your lungs fail, or for your company to succeed if your workers fail, or for the rich in Los Angeles to succeed if the poor in Los Angeles fail, or for Europe to succeed if Africa fails, or for the global economy to succeed if the global environment fails."
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/70-pieces-of-wisdom-for-smart-and-driven-college-students-to-enter-the-real-world-c8a391c2eb20?source=user_profile---------9----------------",
        "title": "70 Pieces of Wisdom for Smart and Driven College Students to Enter the \u201cReal World\u201d",
        "text": "70 Pieces of Wisdom for Smart and Driven College Students to Enter the \u201cReal World\u201d I just spent the past month finishing \u201cTribe of Mentors\u201d, the latest book by the legendary Tim Ferriss. It is packed with wisdom and tools that will change your life. The book contains more than 100+ interviews of people around the world. I made my notes, did some highlights and will be referring back to it on need per basis. After all, I learnt this trick from Tim himself. In one of his many podcast episodes, The Tim Ferriss Show, he described, advice (or information/context) when comes to reading books will stick if it has to stick. In other words, when reading a book as heavy in material as Tim Ferriss\u2019 last two (Tools of Titans and The 4-Hour Workweek), brain will be able to process most of the information but the only those thing will stay with you, or stick with your consciousness or occur at the moment when your subconscious is working for answers in an abstract moment. This is a great advice in the age of information overload. All these mentors that Tim tracked down for this book were asked almost similar amount of questions with full willingness to answer or not to take in consideration the question they don\u2019t want to answer. Some of these answers might help you in navigating your life further. For me personally, this is my favorite question that Tim asked his mentors: What advice would you give to a smart, driven college student about to enter the \u201creal world\u201d? As a recent college grad, I found the answers to the question truly life changing and I\u2019d love for you to get the same gift. Here they are: 1 \u2014 Samin Nosrat: \u201cWhen in doubt, let kindness and compassion guide you. And don\u2019t be afraid to fail.\u201d 2 \u2014 Steven Pressfield: \u201cDon\u2019t worry about your friends beating you or getting somewhere ahead of you. Get out into the real dirt world and start failing. The goal is to connect with your own self, your own soul.\u201d 3 \u2014 Susan Cain: \u201cYou will hear so many stories of people who risked everything in order to achieve this or that goal, especially creative goals. But I do not believe that your best creative work is done when you\u2019re stressed out because you\u2019re teetering on the edge of bankruptcy or other personal disasters. Just the opposite. You should set up your life so that it is as comfortable and happy as possible \u2014 and so that it accommodates your creative work.\u201d 4 \u2014 Kyle Maynard: \u201cBliss is the highest peak of what brings you joy. If happiness is just above the status quo, bliss is what makes you feel most alive. Expect it will take courage to follow your bliss, and expect it will suck at times. Expect you\u2019re going to have to take risks for it. Expect others won\u2019t necessarily understand. And also expect that what gives you bliss today may not be what does tomorrow. Just follow it all over again.\u201d 5 \u2014 Terry Crews: \u201cThere is a big difference between intelligence and wisdom. Intelligence is like following a GPS route right into a body of water until you drown. Wisdom looks at the route but, when it takes a turn into the ocean, decides not to follow it, then finds a new, better way. Wisdom reigns supreme.\u201d 6 \u2014 Debbie Millman: \u201cYou don\u2019t just find and get a great job. You find and win a great job against a pool of very competitive candidates who may want that job as much, if not more, than you do. Finding and winning a great job is a competitive sport that requires as much career athleticism and perseverance as making it to the Olympics. You must be in the finest career shape possible in order to win.\u201d 7 \u2014 Naval Ravikant: \u201cFollow your intellectual curiosity over whatever is \u201chot\u201d right now. If your curiosity ever leads you to a place where society eventually wants to go, you\u2019ll get paid extremely well.\u201d 8 \u2014 Matt Ridley: \u201cThe adult world is not full of gods, just people who have acquired skills and habits that work for them. And specialize \u2014 the great human achievement is to specialize as a producer of goods or services so that you can diversify as a consumer. Self-sufficiency is another word for poverty.\u201d 9 \u2014 Tim Urban: \u201cSociety loves to glorify the \u201cyou-as-CEO\u201d paths and make people who don\u2019t want to be the CEO of their own career feel inferior about their path, but neither of these paths is inherently better or worse than the other \u2014 it just depends on your personality, your goals, and what you want from a lifestyle. There are some super smart, talented, special people whose gifts are best expressed as CEO and others whose are best expressed when someone else is worrying about keeping the lights on and you can just put your head down and focus on your work. Likewise, there are some people who need to be CEO to find their work fulfilling and others for whom being CEO and having their work bleed into everything is a recipe for misery.\u201d 10 \u2014 Ayaan Hirsi Ali: \u201cI am often asked whether one should work in the private or public sector. I always advise working in the private sector, and wish I did this before entering politics and the public sector. The private sector teaches important skills like entrepreneurship that can then be applied to any area of work later on.\u201d\n\n11 \u2014 Graham Duncan: \u201cI like to think about careers through Dan Siegel\u2019s model of a river flowing between two banks, where one side is chaos and the other side is rigidity.. It\u2019s critical to remember you can always choose to course-correct and swim toward structure or chaos, apprenticeship or freedom, depending on what you need at that moment, what tempo and phase of your career you want to be in, which riverbank you\u2019re coming from and where you want to go.\u201d 12 \u2014 Mike Maples: \u201cDon\u2019t let yourself define what matters by the dogma of other people\u2019s thoughts. And even more important, don\u2019t let the thoughts of self-doubt and chattering self-criticism in your own mind slow you down. You will likely be your own worst critic. Be kind to yourself in your own mind. Let your mind show you the same kindness that you aspire to show others.\u201d 13 \u2014 Soman Chainani: \u201cMake sure you have something every day you\u2019re looking forward to. Maybe it\u2019s your job, maybe it\u2019s a basketball game after work or a voice lesson or your writing group, maybe it\u2019s a date. But have something every day that lights you up. It\u2019ll keep your soul hungry to create more of these moments.\u201d 14 \u2014 Max Levchin: \u201cTake risks, now. The advantages that college students and new grads have are their youth, drive, lack of significant responsibilities, and, importantly, lack of the creature comforts one acquires with time. Nothing to lose, everything to gain. Barnacles of the good life tend to slow you down, if you don\u2019t get used to risk-taking early in your career.\u201d 15 \u2014 Veronica Belmont: \u201cDon\u2019t wait until you get a job to do the thing you want to be doing. For most careers, showing that you have initiative by working on projects related to your future job is a great way to get a foot in the door.\u201d 16 \u2014 Patton Oswalt: \u201cEmbrace the suck for a while. Chances are your first job is going to stink and your living conditions won\u2019t be much better. Enjoy the scrappy years, \u2019cuz they\u2019ll make you self-sufficient way faster. Ignore anyone who tells you to go for security over experience.\u201d 17 \u2014 Lewis Cantley: \u201cMy advice is to choose a profession that is really easy for you to do and that also allows you to be creative. If it is easy for you to do and somewhat difficult for your peers to do, you will not have to work too hard to be successful and you will have enough spare time to enjoy life. You will also be able to put in extra hours to blow out the competition every now and then, should that be necessary. If, on the other hand, you have to work long hours all the time just to be competitive, you will burn out and not enjoy life.\u201d 18 \u2014 Jerzy Gregorek: \u201cUp to today, you studied hard and repeated what the world told you. Our purpose in the next four years is to teach you how to think for yourself. If we succeed, you will create something this world has never seen before, but if we do not, you will just be stuck copying others and repeating. Take my words seriously, study hard, but also open your imagination. One day you will be designing a new world, and I hope it will be better than the one we live in.\u201d 19 \u2014 Amelia Boone: \u201cIf you are struggling to figure out where you are headed in life or what you are passionate about, pay attention to activities, ideas, and areas where you love the process, not just the results or the outcome. We are drawn to tasks where we can receive validation through results, but I\u2019ve learned that true fulfillment comes from love of the process. Look for something where you love the process, and the results will follow.\u201d 20 \u2014 Anna Holmes: \u201cThey should ignore any advice from anyone who purports to tell them what the future will look like. No one knows. People have ideas, and those are good to take on board and consider, but that\u2019s about the extent of it. Interrogate the information shared with you by others, and use it as a way to make up your own mind, not a path to follow.\u201d\n\n21 \u2014 Andrew Sorkin: \u201cPersistence matters more than talent. The student with straight As is irrelevant if the student sitting next to him with Bs has more passion.\u201d 22 \u2014 Joseph Gordon-Levitt: \u201cIn any field, there\u2019s usually some kind of mythological reward you\u2019re supposed to receive if everybody considers you a success. But in my experience, there\u2019s a lot more honest joy to be had from taking pleasure in the work itself.\u201d 23 \u2014 Annie Duke: \u201cSeek out dissenting opinions. Always try to find people who disagree with you, who can honestly and productively play devil\u2019s advocate. Challenge yourself to truly listen to people who have differing ideas and opinions than you do. Stay out of political bubbles and echo chambers as much as possible. Feel good about really hearing those who disagree with you. Try to change your mind about one thing every day.\u201d 24 \u2014 Esther Perel: \u201cLife will present you with unexpected opportunities, and you won\u2019t always know in advance which are the important moments. Above all, it\u2019s the quality of your relationships that will determine the quality of your life. Invest in your connections, even those that seem inconsequential.\u201d 25 \u2014 Maria Sharapova: \u201cYou can\u2019t ever say the words \u201cplease\u201d and \u201cthank you\u201d enough. And turn those words into actions, make people around you feel that those words are genuine, that it is exactly how you feel. The same goes for when you break through and make it. Don\u2019t eliminate those words from your pocket.\u201d 26 \u2014 Josh Waitzkin: \u201cDo what you love, do it in a way that you love, and pour your heart and soul into every moment of it. Do not be subject to inertia. Challenge your assumptions and the assumptions of those around you as a way of life. Notice how you are unconsciously fighting to maintain your conceptual scheme even as it mires you in quicksand and immense pain. Harness the body to train the mind.\u201d 27 \u2014 Ann Miura-Ko: \u201cThis may come as strange advice from someone who majored in electrical engineering and got a PhD in math modeling of computer security, but I first tell students I encounter to spend the remainder of their time in college filling their minds with the best of the humanities their school has to offer.\u201d 28 \u2014 Jason Fried: \u201cFocus on your writing skills. It\u2019s the one thing I\u2019ve found that really helps people stand out. More and more communication is written today. Get great at presenting yourself with words, and words alone, and you\u2019ll be far ahead of most.\u201d 29 \u2014 Ariana Huffington: \u201cI would advise them to be much more mindful and deliberate about their relationship with technology. Technology allows us to do amazing things, but we have become addicted to it. And that\u2019s by design \u2014 product designers know how to addict us in the race to dominate the attention economy.\u201d 30 \u2014 Gary Vaynerchuk: \u201cMacro patience, micro speed. They should not care about the next eight years, but they should stress the next eight days.\u201d\n\n31 \u2014 Tim O\u2019Reilly: \u201cWe equate being smart and being driven as the ways to get ahead. But sometimes, an attitude of alert watchfulness is far wiser and more effective. Learning to follow your nose, pulling on threads of curiosity or interest, may take you places that being driven will never lead you to.\u201d 32 \u2014 Tom Peters: \u201cGood manners pay off big time. I assume you\u2019re smart and I assume you work hard. But being civil and decent and kind is the bedrock of career success, as well as personal fulfillment.\u201d 33 \u2014 Leo Babauta: \u201cEmbrace uncertainty, groundlessness, and fear as the place where you\u2019ll really learn and grow. Go into that place, rather than shrinking from it. It\u2019ll help you overcome procrastination, social anxiety, fear of launching your own business or pursuing your dreams, fear of failure and ridicule, and more. Those fears will still be there, but you\u2019ll find the deliciousness in them.\u201d 34 \u2014 Esther Dyson: \u201cAlways take jobs for which you are not qualified; that way you will inevitably learn something.\u201d 35 \u2014 Kevin Kelly: \u201cDon\u2019t try to find your passion. Instead master some skill, interest, or knowledge that others find valuable. It almost doesn\u2019t matter what it is at the start. You don\u2019t have to love it, you just have to be the best at it. Once you master it, you\u2019ll be rewarded with new opportunities that will allow you to move away from tasks you dislike and toward those that you enjoy. If you continue to optimize your mastery, you\u2019ll eventually arrive at your passion.\u201d 36 \u2014 Ashton Kutcher: \u201cBe polite, on time, and work really fucking hard until you are talented enough to be blunt, a little late, and take vacations and even then . . . be polite.\u201d 37 \u2014 Franklin Leonard: \u201cTry everything you think you might want to do professionally before accepting whatever backup plan you have in the back of your head but are very much hoping to avoid.\u201d 38 \u2014 Peter Guber: \u201cThe seminal change in the business from then to now is that a young person should view the career pyramid differently rather than traditionally. Put the point at the bottom where you are now (at the start of your career) and conceive your future as an expanding opportunity horizon where you can move laterally across the spectrum in search of an ever-widening set of career opportunities. Reinvent yourself regularly. See your world as an ever-increasing set of realities and seize the day.\u201d 39 \u2014 Strauss Zelnick: \u201cFigure out what success means to you. Don\u2019t accept others\u2019 views or conventional wisdom. Write down what your successful personal and professional life looks like in 20 years. Then roll the clock back to today. Make sure your choices are in service of those goals.\u201d 40 \u2014 Tony Hawk: \u201cSuccess should not be measured by financial gain; true success is doing something you love for a living. Learn every aspect of your chosen field or craft, as it will give you an advantage over any competitors, and set you up for more \u2014 often better \u2014 job opportunities.\u201d\n\n41 \u2014 Mark Bell: \u201cI would tell them to invest time in themselves, to make sure they have some sort of physical activity in their life, and adhere to some form of nutrition that keeps them healthy. When that stuff falls apart, it can make other things more difficult.\u201d 42 \u2014 Ray Dalio: \u201cLove looking at what you don\u2019t know, your mistakes, and your weaknesses, because understanding them is essential for making the most of your life.\u201d 43 \u2014 Jacqueline Novogratz: \u201cDon\u2019t worry all that much about your first job. Just start, and let the work teach you. With every step, you will discover more about who you want to be and what you want to do. If you wait for the perfect and keep all of your options open, you might end up with nothing but options. So start.\u201d 44 \u2014 Dr Gabor Mate: \u201cIf you\u2019re really smart, you\u2019ll drop the drivenness. It doesn\u2019t matter what\u2019s driving you; when you\u2019re driven, you are like a leaf, driven by the wind. You have no real autonomy. You are bound to be blown off course, even if you reach what you believe is your goal. And don\u2019t confuse being driven with being authentically animated by an inner calling. One state leaves you depleted and unfulfilled; the other fuels your soul and makes your heart sing.\u201d 45 \u2014 Steve Case: \u201cIf, like many people, you got a liberal arts degree, be proud of it, and own it. While the conventional wisdom says that coding is the key to success, that\u2019s not as likely to be as true in the Third Wave, when major industries will be disrupted, as it was in the Second, when the focus was on building apps. Sure, coding will continue to be important, but creativity and collaboration will be as well. Don\u2019t try to be something you\u2019re not. Be confident in the skills you have, as they may be make-or-break for the journey you pursue.\u201d 46 \u2014 Linda Rottenberg: \u201cPeople always tell recent graduates and budding entrepreneurs that they should keep their options open; \u201cdon\u2019t close any doors.\u201d But keeping every option open winds up leading to paralysis or, worse than that, self-deception. How many of my former classmates who took a job at Goldman Sachs or McKinsey for \u201ca few years\u201d before they pursued their real passions like cooking or starting their dream company are actually now chefs or entrepreneurs? Most are still banking and consulting, believing that those doors are still open. My advice to college students: Close doors.\u201d 47 \u2014 Tommy Vietor: \u201cDon\u2019t worry about making money. Don\u2019t stress about having a plan. Don\u2019t think about networking or setting yourself up for the next thing. Try as hard as you possibly can to find something you love, because the depressing reality is that most people never find a career that they\u2019re truly passionate about. For many people, the real world is a slog and they live for the weekends. It will never get easier than right now to recklessly pursue your passion. Do it.\u201d 48 \u2014 Sam Harris: \u201cDon\u2019t worry about what you\u2019re going to do with the rest of your life. Just find a profitable and interesting use for the next three to five years.\u201d 49 \u2014 John Arnold: \u201cThe unfortunate truth is that advice is almost always driven by anecdotal experience, and thus has limited value and relevance. Read a sampling of college commencement addresses, and you quickly realize each story is unique. For every entrepreneur who thrived by resolutely working on a singular idea for many years, there is another who pivoted wildly. For every successful individual who designed a master plan for life, there is another who was deliberately spontaneous. Ignore advice, especially early in one\u2019s career. There is no universal path to success.\u201d 50 \u2014 Mr Money Mustache: \u201cA high savings rate (or \u201cprofit margin on life\u201d) is by far the best strategy for a great and creative life, because it\u2019s your ticket to freedom. Freedom is the fuel for creativity.\u201d 51 \u2014 David Lynch: \u201cLearn Transcendental Meditation as taught by Maharishi Mahesh Yogi and meditate regularly. Ignore pessimistic thinking and pessimistic thinkers.\u201d 52 \u2014 Nick Szabo: \u201cEverybody is striving after social proof \u2014 from a close friend\u2019s adulation to online likes and upvotes. The less you need positive feedback on your ideas, the more original design regions you can explore, and the more creative and, in the long term, useful to society you will be. But it could be a very long time before people will love you (or even pay you) for it. The more original your ideas, the less your bosses and peers will understand them, and people fear or at best ignore what they do not understand. But for me, making progress on the ideas was very rewarding in itself at the time, even though they would have made the worst party conversation topics ever. Eventually, decades after, they generated more social accolades than I now know what to do with.\u201d 53 \u2014 Dara Torres: \u201cMany people have started from the bottom and have worked their way up, so don\u2019t think being at the bottom of the totem pole is a bad thing in the work world. You have nowhere to go but up. Ignore hearsay and rumors until you know it as fact.\u201d 54 \u2014 Dan Gable: \u201cDon\u2019t plan on \u201cwinning the lottery\u201d right away, because it usually doesn\u2019t happen. Doing a good job and building your assets is like winning the lottery, but over time. You gotta work hard every day, make progress every day, and make money every day. Over time, you\u2019ll be in good shape. If you do win it in the first year, hey, I\u2019ll be the first to congratulate you \u2014 but don\u2019t count on it.\u201d 55 \u2014 Darren Aronofsky: \u201cMost of the game is about persistence. It is the most important trait. Sure, when you get an opportunity, you have to perform and you have to exceed beyond all expectations, but getting that chance is the hardest part. So keep the vision clear in your head and every day refuse all obstacles to get to the goal.\u201d 56 \u2014 Evan Williams: \u201cBe in a hurry to learn, not in a hurry to get validation. In a team environment, you will make a much better impression if it seems like you\u2019re not at all worried about yourself. It\u2019s okay to actually be worried about yourself \u2014 everyone is \u2014 just don\u2019t seem like it. If you resist asking for too much, you will often get more.\u201d 57 \u2014 Bram Cohen: \u201cPick your early jobs based on what gets you the most valuable experience. If you want to be an entrepreneur, don\u2019t dive directly into doing your venture but go get work at an early-stage startup to learn the ropes and get paid to make your early mistakes. Only after getting the necessary experience and knowledge should you strike out on your own.\u201d 58 \u2014 Chris Anderson: \u201cIn your 20s, you may not really know what your best skills and opportunities are. It\u2019s much better to pursue learning, personal discipline, growth. And to seek out connections with people across the planet. For a while, it\u2019s just fine to follow and support someone else\u2019s dream. In so doing, you will be building valuable relationships, valuable knowledge. And at some point your passion will come and whisper in your ear, \u201cI\u2019m ready.\u201d 59 \u2014 Kelly Slater: \u201cThink for yourself. Everyone has a unique picture of how things work and function, and yours is as valuable as anyone\u2019s. It\u2019s sometimes the belief in yourself, open-mindedness toward others, and your delivery that allows things to be heard by others.\u201d 60 \u2014 Adam Fisher: \u201cBe humble and self-aware. Ignore the concept of \u201cbeing yourself.\u201d Of course this is literally true by definition, but it is a way to avoid self-improvement. Pursuing your passion is fine.\u201d\n\n61 \u2014 Laura Walker: \u201cGet out of your comfort zone when you graduate. Ask yourself what you are genuinely curious about and explore it. Embrace the ambiguity and contradictions that life invariably will bring, and develop habits \u2014 exercise, talking with friends, writing \u2014 that help you do so. Don\u2019t spend time chasing a right answer or a right path, but instead spend time defining how you are going to approach whatever path you choose. What values most define you? What questions do you want to pursue?\u201d 62 \u2014 Terry Laughlin: \u201cThe same will apply in any field of endeavor. If your highest goal is incremental, patient, continual learning and development in critical skills and core competencies \u2014 and you allow recognition, promotions, and financial rewards to be a natural result of the excellence you attain at core competencies \u2014 you will be far more likely to experience success and satisfaction, and perhaps even attain eminence, in your field.\u201d 63 \u2014 Marie Forleo: \u201cPursue every project, idea, or industry that genuinely lights you up, regardless of how unrelated each idea is, or how unrealistic a long-term career in that field might now seem. You\u2019ll connect the dots later. Work your fucking ass off and develop a reputation for going above and beyond in all situations. Do whatever it takes to earn enough money, so that you can go all in on experiences or learning opportunities that put you in close proximity to people you admire, because proximity is power. Show up in every moment like you\u2019re meant to be there, because your energy precedes anything you could possibly say.\u201d 64 \u2014 Drew Houston: \u201cIt\u2019s not just about passion or following your dreams. Make sure the problem you become obsessed with is one that needs solving and is one where your contribution can make a difference. As Y Combinator says, \u201cMake something people want.\u201d 65 \u2014 Scott Belsky: \u201cEvery step in your early career must get you incrementally closer to whatever genuinely interests you. The most promising path to success is pursuing genuine interests and setting yourself up for the circumstantial relationships, collaborations, and experiences that will make all the difference in your life. A labor of love always pays off, just not how and when you expect. Set yourself up to succeed by taking new jobs and roles that get you closer to your interests.\u201d 66 \u2014 Whitney Cummings: \u201cChances are, if you\u2019re reading books like this, you will succeed, but I\u2019ve found that it all feels pretty meaningless if you\u2019re not in some way helping people or improving humanity in some way. Instead of striving to be a CEO or an entrepreneur, strive to be a hero. We need more of those.\u201d 67 \u2014 Rick Rubin: \u201cI would ignore most anything you learn in school and ignore all accepted standards. Free yourself to try anything. The best ideas are revolutionary. If you\u2019re searching for wisdom, try to find it from people who\u2019ve done it more than from people who teach it. Ask a lot of questions.\u201d 68 \u2014 Peter Attia: \u201cBe as genuine as you can. Don\u2019t fake it. In my view, better to be a cold stiff than fake that you care. If you are genuinely interested in a subset of other people, even if that number is small, you will foster relationships that really matter. As we age, I believe, frivolous relationships in business and our personal life become less and less bearable, so only put energy into completely genuine interactions with other people.\u201d 69 \u2014 Jocko Willink: \u201cWork harder than everyone else. Of course, that is easy when you love your job. But you might not love your first, or second, or even third job. That doesn\u2019t matter. Work harder than everyone else. In order to get the job you love or start the company you want, you have to build your r\u00e9sum\u00e9, your reputation, and your bank account. The best way to do that: Outwork them all.\u201d 70 \u2014 Yuval Noah Harari: \u201cNobody really knows what the world and the job market will look like in 2040, hence nobody knows what to teach young people today. Consequently, it is likely that most of what you currently learn at school will be irrelevant by the time you are 40. So what should you focus on? My best advice is to focus on personal resilience and emotional intelligence\u2026 The world of 2040 will be a very different world from today, and an extremely hectic world. The pace of change is likely to accelerate even further. So people will need the ability to learn all the time and to reinvent themselves repeatedly \u2014 even at age 60.\u201d \u2014 \u2014 \n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find more of my writing at https://jameskle.com/writes/."
    },
    {
        "url": "https://medium.com/swlh/snapchats-filters-how-computer-vision-recognizes-your-face-9ce536206fa7?source=user_profile---------10----------------",
        "title": "Snapchat\u2019s Filters: How computer vision recognizes your face",
        "text": "In those moments of boredom when you\u2019re playing with Snapchat\u2019s filters \u2014 sticking your tongue out, ghoulifying your features, and working out how to get the flower crown to fit exactly on your head \u2014 surely you\u2019ve had a moment where you\u2019ve wondered what\u2019s going on, on a technical level \u2014 how Snapchat manages to match your face to the animations?\n\nAfter two weeks of researching online, I feel grateful to have finally gotten a glimpse behind the curtain. It turns out that the product is an instance of computer vision application, which is the main fuel behind all kinds of facial recognition software.\n\nThe technology came from a Ukrainian startup Looksery, which is an application that allowed users to modify their facial features during video chats and for photos. Snapchat acquired this Odesa-based face changing startup in September 2015 for $150 million dollars. That\u2019s reportedly the largest tech acquisition in Ukrainian history.\n\nTheir augmented reality filters tap into the large and rapidly growing field of computer vision. Computer vision can be thought of as a direct opposite of computer graphics. While computer graphics try to produce image models from 3D models, computer vision tries to create a 3D space from image data. Computer Vision is starting to be utilized more and more in our society. It is how you scan your checks and the data is extracted from the lines. It is how you can deposit checks with your phone. It is how Facebook knows who\u2019s in your photos, how self-driving cars can avoid running over people and how you can give yourself a dodgy nose.\n\nLooksery maintains their engineering more confidential, but every one can access their patents online. The specific area of Computer Vision that Snapchat filters use is called Image processing. Image processing is the transformation of an image by performing mathematical operations on each individual pixel on the provided picture.\n\nThe first step works like this: Given an input image or video frame, find out all present human faces and output their bounding box (i.e. The rectangle coordinates in the form: X, Y, Width & Height).\n\nFace detection has been a solved problem since the early 2000s but faces some challenges including detecting tiny, partial & non-frontal faces. The most widely used technique is a combination of Histogram of Oriented Gradients (HOG for short) and Support Vector Machine (SVM) that achieve mediocre to relatively good detection ratios given a good quality image but this method is not capable of real-time detection at least on the CPU.\n\nHere is how the HOG/SVM detector works:\n\nGiven an input image, compute the pyramidal representation of that image which is a pyramid of multi scaled downed version of the original image. For each entry on the pyramid, a sliding window approach is used. The sliding window concept is quite simple. By looping over an image with a constant step size, small image patches typically of size 64 x 128 pixels are extracted at different scales. For each patch, the algorithm makes a decision if it contains a face or not. The HOG is computed for the current window and passed to the SVM classifier (Linear or not) for the decision to take place (i.e. Face or not). When done with the pyramid, a non-maxima suppression (NMS for short) operation usually take place in order to discard stacked rectangles. You can read more about the HOG/SVM combination here.\n\nThis is the next step in our analysis phase and works as follows: For each detected face, output the local region coordinates for each member or facial feature of that face. This includes the eyes, bone, lips, nose, mouth,\u2026 coordinates usually in the form of points (X,Y).\n\nExtracting facial landmarks is a relatively cheap operation for the CPU given a bounding box (i.e. Cropped image with the target face), but quite difficult to implement for the programmer unless some not-so-fast machine learning techniques such as training & running a classifier is used.\n\nYou can find out more about extracting facial landmarks here or this PDF: One millisecond face alignment with an ensemble of regression trees. In some and obviously useful cases, face detection and landmarks extraction are combined into a single operation.\n\nNow that the face has been detected, Snapchat can use Image Processing to apply features onto a full face. However, they chose to go one step further and they want to find your facial features. This is done with the aid of the Active Shape Model.\n\nThe Active Shape Model is a facial model that has been trained by the manual marking of the borders of facial features on hundreds to thousands of images. Through machine learning, an \u201caverage face\u201d is created and aligns this with the image that is provided. This average face, of course, does not fit exactly with the user\u2019s face (we all have diverse faces), so after fitting the face, pixels around the edge of the \u201caverage face\u201d are examined to look for differences in shading. Because of the training that the algorithm went through, (the Machine Learning process), it has a basic skeleton of how certain facial features should look, so it looks for a similar pattern in the given image. Even if some of the initial changes are wrong, by taking into account the position of other points that it has fixed, the algorithm will correct errors it made regarding where it thought certain aspects of your face are. The model then adjusts and creates a mesh (a 3D model that can shift and scale with your face).\n\nThis whole facial/feature recognition process is done when you see that white net right before you choose your filter. The filters then distort certain areas of the provided face by enhancing them or adding something on top of them.\n\nThe updated version of Snapchat a few months back had the feature for swapping faces with a friend, whether in real time or by accessing some faces from your gallery. Notice how the face shapes are visible, that\u2019s the position where the statistical model lies. It helps Snapchat to quickly align you and your friends face and swap the features.\n\nAfter locating all your features, the application creates a mesh along your face that sticks to each point frame by frame. This mesh can now be edited and modified as Snapchat feels.\n\nSome lenses do much more by either asking you to raise your eyebrows or by opening your mouth. This is also fairly simple to think about, but it requires a lot more algorithms to imply.\n\nNow as mentioned before, this technology is not new. But to perform all those processes in real time and on a mobile platform takes a lot of processing power along with some complicated algorithms. That\u2019s why Snapchat thought it\u2019s better to pay 150 million dollars to acquire Looksery instead of just building its platform.\n\nI hope this was informative and tickled your curiosity like it did mine. For now, I\u2019ll be exploring Snapchat Filters more deeply, testing out my favorite facial lens, knowing and appreciating all the computer vision that\u2019s going on behind the scenes.\n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://towardsdatascience.com/16-useful-advices-for-aspiring-data-scientists-6da9afa8c72c?source=user_profile---------11----------------",
        "title": "16 Useful Advices for Aspiring Data Scientists \u2013",
        "text": "Why is data science sexy? It has something to do with so many new applications and entire new industries come into being from the judicious use of copious amounts of data. Examples include speech recognition, object recognition in computer vision, robots and self-driving cars, bioinformatics, neuroscience, the discovery of exoplanets and an understanding of the origins of the universe, and the assembling of inexpensive but winning baseball teams. In each of these instances, the data scientist is central to the whole enterprise. He/she must combine knowledge of the application area with statistical expertise and implement it all using the latest in computer science ideas.\n\nIn the end, sexiness comes down to being effective. I recently read Sebastian Gutierrez\u2019s \u201cData Scientists at Work\u201d, in which he interviewed 16 data scientists across 16 different industries to understand both how they think about it theoretically and also very practically what problems they\u2019re solving, how data\u2019s helping, and what it takes to be successful. All 16 interviewees are at the forefront of understanding and extracting value from data across an array of public and private organizational types \u2014 from startups and mature corporations to primary research groups and humanitarian nonprofits \u2014 and across a diverse range of industries \u2014 advertising, e-commerce, email marketing, enterprise cloud computing, fashion, industrial internet, internet television and entertainment, music, nonprofit, neurobiology, newspapers and media, professional and social networks, retail, sales intelligence, and venture capital.\n\nIn particular, Sebastian asked open-ended questions so that the personalities and spontaneous thought processes of each interviewee would shine through clearly and accurately. The practitioners in this book share their thoughts on what data science means to them and how they think about it, their suggestions on how to join the field, and their wisdom won through experience on what a data scientist must understand deeply to be successful within the field.\n\nIn this post, I want to share the best answers that these data scientists gave for the question:"
    },
    {
        "url": "https://medium.com/@james_aka_yale/70-pieces-of-wisdom-for-smart-and-driven-college-students-to-enter-the-real-world-9fde16e62d65?source=user_profile---------12----------------",
        "title": "70 Pieces of Wisdom for Smart and Driven College Students to Enter the \u201cReal World\u201d",
        "text": "70 Pieces of Wisdom for Smart and Driven College Students to Enter the \u201cReal World\u201d I just spent the past month finishing \u201cTribe of Mentors\u201d, the latest book by the legendary Tim Ferriss. It is packed with wisdom and tools that will change your life. The book contains more than 100+ interviews of people around the world. I made my notes, did some highlights and will be referring back to it on need per basis. After all, I learnt this trick from Tim himself. In one of his many podcast episodes, The Tim Ferriss Show, he described, advice (or information/context) when comes to reading books will stick if it has to stick. In other words, when reading a book as heavy in material as Tim Ferriss\u2019 last two (Tools of Titans and The 4-Hour Workweek), brain will be able to process most of the information but the only those thing will stay with you, or stick with your consciousness or occur at the moment when your subconscious is working for answers in an abstract moment. This is a great advice in the age of information overload. All these mentors that Tim tracked down for this book were asked almost similar amount of questions with full willingness to answer or not to take in consideration the question they don\u2019t want to answer. Some of these answers might help you in navigating your life further. For me personally, this is my favorite question that Tim asked his mentors: What advice would you give to a smart, driven college student about to enter the \u201creal world\u201d? As a recent college grad, I found the answers to the question truly life changing and I\u2019d love for you to get the same gift. Here they are: 1 \u2014 Samin Nosrat: \u201cWhen in doubt, let kindness and compassion guide you. And don\u2019t be afraid to fail.\u201d 2 \u2014 Steven Pressfield: \u201cDon\u2019t worry about your friends beating you or getting somewhere ahead of you. Get out into the real dirt world and start failing. The goal is to connect with your own self, your own soul.\u201d 3 \u2014 Susan Cain: \u201cYou will hear so many stories of people who risked everything in order to achieve this or that goal, especially creative goals. But I do not believe that your best creative work is done when you\u2019re stressed out because you\u2019re teetering on the edge of bankruptcy or other personal disasters. Just the opposite. You should set up your life so that it is as comfortable and happy as possible \u2014 and so that it accommodates your creative work.\u201d 4 \u2014 Kyle Maynard: \u201cBliss is the highest peak of what brings you joy. If happiness is just above the status quo, bliss is what makes you feel most alive. Expect it will take courage to follow your bliss, and expect it will suck at times. Expect you\u2019re going to have to take risks for it. Expect others won\u2019t necessarily understand. And also expect that what gives you bliss today may not be what does tomorrow. Just follow it all over again.\u201d 5 \u2014 Terry Crews: \u201cThere is a big difference between intelligence and wisdom. Intelligence is like following a GPS route right into a body of water until you drown. Wisdom looks at the route but, when it takes a turn into the ocean, decides not to follow it, then finds a new, better way. Wisdom reigns supreme.\u201d 6 \u2014 Debbie Millman: \u201cYou don\u2019t just find and get a great job. You find and win a great job against a pool of very competitive candidates who may want that job as much, if not more, than you do. Finding and winning a great job is a competitive sport that requires as much career athleticism and perseverance as making it to the Olympics. You must be in the finest career shape possible in order to win.\u201d 7 \u2014 Naval Ravikant: \u201cFollow your intellectual curiosity over whatever is \u201chot\u201d right now. If your curiosity ever leads you to a place where society eventually wants to go, you\u2019ll get paid extremely well.\u201d 8 \u2014 Matt Ridley: \u201cThe adult world is not full of gods, just people who have acquired skills and habits that work for them. And specialize \u2014 the great human achievement is to specialize as a producer of goods or services so that you can diversify as a consumer. Self-sufficiency is another word for poverty.\u201d 9 \u2014 Tim Urban: \u201cSociety loves to glorify the \u201cyou-as-CEO\u201d paths and make people who don\u2019t want to be the CEO of their own career feel inferior about their path, but neither of these paths is inherently better or worse than the other \u2014 it just depends on your personality, your goals, and what you want from a lifestyle. There are some super smart, talented, special people whose gifts are best expressed as CEO and others whose are best expressed when someone else is worrying about keeping the lights on and you can just put your head down and focus on your work. Likewise, there are some people who need to be CEO to find their work fulfilling and others for whom being CEO and having their work bleed into everything is a recipe for misery.\u201d 10 \u2014 Ayaan Hirsi Ali: \u201cI am often asked whether one should work in the private or public sector. I always advise working in the private sector, and wish I did this before entering politics and the public sector. The private sector teaches important skills like entrepreneurship that can then be applied to any area of work later on.\u201d\n\n11 \u2014 Graham Duncan: \u201cI like to think about careers through Dan Siegel\u2019s model of a river flowing between two banks, where one side is chaos and the other side is rigidity.. It\u2019s critical to remember you can always choose to course-correct and swim toward structure or chaos, apprenticeship or freedom, depending on what you need at that moment, what tempo and phase of your career you want to be in, which riverbank you\u2019re coming from and where you want to go.\u201d 12 \u2014 Mike Maples: \u201cDon\u2019t let yourself define what matters by the dogma of other people\u2019s thoughts. And even more important, don\u2019t let the thoughts of self-doubt and chattering self-criticism in your own mind slow you down. You will likely be your own worst critic. Be kind to yourself in your own mind. Let your mind show you the same kindness that you aspire to show others.\u201d 13 \u2014 Soman Chainani: \u201cMake sure you have something every day you\u2019re looking forward to. Maybe it\u2019s your job, maybe it\u2019s a basketball game after work or a voice lesson or your writing group, maybe it\u2019s a date. But have something every day that lights you up. It\u2019ll keep your soul hungry to create more of these moments.\u201d 14 \u2014 Max Levchin: \u201cTake risks, now. The advantages that college students and new grads have are their youth, drive, lack of significant responsibilities, and, importantly, lack of the creature comforts one acquires with time. Nothing to lose, everything to gain. Barnacles of the good life tend to slow you down, if you don\u2019t get used to risk-taking early in your career.\u201d 15 \u2014 Veronica Belmont: \u201cDon\u2019t wait until you get a job to do the thing you want to be doing. For most careers, showing that you have initiative by working on projects related to your future job is a great way to get a foot in the door.\u201d 16 \u2014 Patton Oswalt: \u201cEmbrace the suck for a while. Chances are your first job is going to stink and your living conditions won\u2019t be much better. Enjoy the scrappy years, \u2019cuz they\u2019ll make you self-sufficient way faster. Ignore anyone who tells you to go for security over experience.\u201d 17 \u2014 Lewis Cantley: \u201cMy advice is to choose a profession that is really easy for you to do and that also allows you to be creative. If it is easy for you to do and somewhat difficult for your peers to do, you will not have to work too hard to be successful and you will have enough spare time to enjoy life. You will also be able to put in extra hours to blow out the competition every now and then, should that be necessary. If, on the other hand, you have to work long hours all the time just to be competitive, you will burn out and not enjoy life.\u201d 18 \u2014 Jerzy Gregorek: \u201cUp to today, you studied hard and repeated what the world told you. Our purpose in the next four years is to teach you how to think for yourself. If we succeed, you will create something this world has never seen before, but if we do not, you will just be stuck copying others and repeating. Take my words seriously, study hard, but also open your imagination. One day you will be designing a new world, and I hope it will be better than the one we live in.\u201d 19 \u2014 Amelia Boone: \u201cIf you are struggling to figure out where you are headed in life or what you are passionate about, pay attention to activities, ideas, and areas where you love the process, not just the results or the outcome. We are drawn to tasks where we can receive validation through results, but I\u2019ve learned that true fulfillment comes from love of the process. Look for something where you love the process, and the results will follow.\u201d 20 \u2014 Anna Holmes: \u201cThey should ignore any advice from anyone who purports to tell them what the future will look like. No one knows. People have ideas, and those are good to take on board and consider, but that\u2019s about the extent of it. Interrogate the information shared with you by others, and use it as a way to make up your own mind, not a path to follow.\u201d\n\n21 \u2014 Andrew Sorkin: \u201cPersistence matters more than talent. The student with straight As is irrelevant if the student sitting next to him with Bs has more passion.\u201d 22 \u2014 Joseph Gordon-Levitt: \u201cIn any field, there\u2019s usually some kind of mythological reward you\u2019re supposed to receive if everybody considers you a success. But in my experience, there\u2019s a lot more honest joy to be had from taking pleasure in the work itself.\u201d 23 \u2014 Annie Duke: \u201cSeek out dissenting opinions. Always try to find people who disagree with you, who can honestly and productively play devil\u2019s advocate. Challenge yourself to truly listen to people who have differing ideas and opinions than you do. Stay out of political bubbles and echo chambers as much as possible. Feel good about really hearing those who disagree with you. Try to change your mind about one thing every day.\u201d 24 \u2014 Esther Perel: \u201cLife will present you with unexpected opportunities, and you won\u2019t always know in advance which are the important moments. Above all, it\u2019s the quality of your relationships that will determine the quality of your life. Invest in your connections, even those that seem inconsequential.\u201d 25 \u2014 Maria Sharapova: \u201cYou can\u2019t ever say the words \u201cplease\u201d and \u201cthank you\u201d enough. And turn those words into actions, make people around you feel that those words are genuine, that it is exactly how you feel. The same goes for when you break through and make it. Don\u2019t eliminate those words from your pocket.\u201d 26 \u2014 Josh Waitzkin: \u201cDo what you love, do it in a way that you love, and pour your heart and soul into every moment of it. Do not be subject to inertia. Challenge your assumptions and the assumptions of those around you as a way of life. Notice how you are unconsciously fighting to maintain your conceptual scheme even as it mires you in quicksand and immense pain. Harness the body to train the mind.\u201d 27 \u2014 Ann Miura-Ko: \u201cThis may come as strange advice from someone who majored in electrical engineering and got a PhD in math modeling of computer security, but I first tell students I encounter to spend the remainder of their time in college filling their minds with the best of the humanities their school has to offer.\u201d 28 \u2014 Jason Fried: \u201cFocus on your writing skills. It\u2019s the one thing I\u2019ve found that really helps people stand out. More and more communication is written today. Get great at presenting yourself with words, and words alone, and you\u2019ll be far ahead of most.\u201d 29 \u2014 Ariana Huffington: \u201cI would advise them to be much more mindful and deliberate about their relationship with technology. Technology allows us to do amazing things, but we have become addicted to it. And that\u2019s by design \u2014 product designers know how to addict us in the race to dominate the attention economy.\u201d 30 \u2014 Gary Vaynerchuk: \u201cMacro patience, micro speed. They should not care about the next eight years, but they should stress the next eight days.\u201d\n\n31 \u2014 Tim O\u2019Reilly: \u201cWe equate being smart and being driven as the ways to get ahead. But sometimes, an attitude of alert watchfulness is far wiser and more effective. Learning to follow your nose, pulling on threads of curiosity or interest, may take you places that being driven will never lead you to.\u201d 32 \u2014 Tom Peters: \u201cGood manners pay off big time. I assume you\u2019re smart and I assume you work hard. But being civil and decent and kind is the bedrock of career success, as well as personal fulfillment.\u201d 33 \u2014 Leo Babauta: \u201cEmbrace uncertainty, groundlessness, and fear as the place where you\u2019ll really learn and grow. Go into that place, rather than shrinking from it. It\u2019ll help you overcome procrastination, social anxiety, fear of launching your own business or pursuing your dreams, fear of failure and ridicule, and more. Those fears will still be there, but you\u2019ll find the deliciousness in them.\u201d 34 \u2014 Esther Dyson: \u201cAlways take jobs for which you are not qualified; that way you will inevitably learn something.\u201d 35 \u2014 Kevin Kelly: \u201cDon\u2019t try to find your passion. Instead master some skill, interest, or knowledge that others find valuable. It almost doesn\u2019t matter what it is at the start. You don\u2019t have to love it, you just have to be the best at it. Once you master it, you\u2019ll be rewarded with new opportunities that will allow you to move away from tasks you dislike and toward those that you enjoy. If you continue to optimize your mastery, you\u2019ll eventually arrive at your passion.\u201d 36 \u2014 Ashton Kutcher: \u201cBe polite, on time, and work really fucking hard until you are talented enough to be blunt, a little late, and take vacations and even then . . . be polite.\u201d 37 \u2014 Franklin Leonard: \u201cTry everything you think you might want to do professionally before accepting whatever backup plan you have in the back of your head but are very much hoping to avoid.\u201d 38 \u2014 Peter Guber: \u201cThe seminal change in the business from then to now is that a young person should view the career pyramid differently rather than traditionally. Put the point at the bottom where you are now (at the start of your career) and conceive your future as an expanding opportunity horizon where you can move laterally across the spectrum in search of an ever-widening set of career opportunities. Reinvent yourself regularly. See your world as an ever-increasing set of realities and seize the day.\u201d 39 \u2014 Strauss Zelnick: \u201cFigure out what success means to you. Don\u2019t accept others\u2019 views or conventional wisdom. Write down what your successful personal and professional life looks like in 20 years. Then roll the clock back to today. Make sure your choices are in service of those goals.\u201d 40 \u2014 Tony Hawk: \u201cSuccess should not be measured by financial gain; true success is doing something you love for a living. Learn every aspect of your chosen field or craft, as it will give you an advantage over any competitors, and set you up for more \u2014 often better \u2014 job opportunities.\u201d\n\n41 \u2014 Mark Bell: \u201cI would tell them to invest time in themselves, to make sure they have some sort of physical activity in their life, and adhere to some form of nutrition that keeps them healthy. When that stuff falls apart, it can make other things more difficult.\u201d 42 \u2014 Ray Dalio: \u201cLove looking at what you don\u2019t know, your mistakes, and your weaknesses, because understanding them is essential for making the most of your life.\u201d 43 \u2014 Jacqueline Novogratz: \u201cDon\u2019t worry all that much about your first job. Just start, and let the work teach you. With every step, you will discover more about who you want to be and what you want to do. If you wait for the perfect and keep all of your options open, you might end up with nothing but options. So start.\u201d 44 \u2014 Dr Gabor Mate: \u201cIf you\u2019re really smart, you\u2019ll drop the drivenness. It doesn\u2019t matter what\u2019s driving you; when you\u2019re driven, you are like a leaf, driven by the wind. You have no real autonomy. You are bound to be blown off course, even if you reach what you believe is your goal. And don\u2019t confuse being driven with being authentically animated by an inner calling. One state leaves you depleted and unfulfilled; the other fuels your soul and makes your heart sing.\u201d 45 \u2014 Steve Case: \u201cIf, like many people, you got a liberal arts degree, be proud of it, and own it. While the conventional wisdom says that coding is the key to success, that\u2019s not as likely to be as true in the Third Wave, when major industries will be disrupted, as it was in the Second, when the focus was on building apps. Sure, coding will continue to be important, but creativity and collaboration will be as well. Don\u2019t try to be something you\u2019re not. Be confident in the skills you have, as they may be make-or-break for the journey you pursue.\u201d 46 \u2014 Linda Rottenberg: \u201cPeople always tell recent graduates and budding entrepreneurs that they should keep their options open; \u201cdon\u2019t close any doors.\u201d But keeping every option open winds up leading to paralysis or, worse than that, self-deception. How many of my former classmates who took a job at Goldman Sachs or McKinsey for \u201ca few years\u201d before they pursued their real passions like cooking or starting their dream company are actually now chefs or entrepreneurs? Most are still banking and consulting, believing that those doors are still open. My advice to college students: Close doors.\u201d 47 \u2014 Tommy Vietor: \u201cDon\u2019t worry about making money. Don\u2019t stress about having a plan. Don\u2019t think about networking or setting yourself up for the next thing. Try as hard as you possibly can to find something you love, because the depressing reality is that most people never find a career that they\u2019re truly passionate about. For many people, the real world is a slog and they live for the weekends. It will never get easier than right now to recklessly pursue your passion. Do it.\u201d 48 \u2014 Sam Harris: \u201cDon\u2019t worry about what you\u2019re going to do with the rest of your life. Just find a profitable and interesting use for the next three to five years.\u201d 49 \u2014 John Arnold: \u201cThe unfortunate truth is that advice is almost always driven by anecdotal experience, and thus has limited value and relevance. Read a sampling of college commencement addresses, and you quickly realize each story is unique. For every entrepreneur who thrived by resolutely working on a singular idea for many years, there is another who pivoted wildly. For every successful individual who designed a master plan for life, there is another who was deliberately spontaneous. Ignore advice, especially early in one\u2019s career. There is no universal path to success.\u201d 50 \u2014 Mr Money Mustache: \u201cA high savings rate (or \u201cprofit margin on life\u201d) is by far the best strategy for a great and creative life, because it\u2019s your ticket to freedom. Freedom is the fuel for creativity.\u201d 51 \u2014 David Lynch: \u201cLearn Transcendental Meditation as taught by Maharishi Mahesh Yogi and meditate regularly. Ignore pessimistic thinking and pessimistic thinkers.\u201d 52 \u2014 Nick Szabo: \u201cEverybody is striving after social proof \u2014 from a close friend\u2019s adulation to online likes and upvotes. The less you need positive feedback on your ideas, the more original design regions you can explore, and the more creative and, in the long term, useful to society you will be. But it could be a very long time before people will love you (or even pay you) for it. The more original your ideas, the less your bosses and peers will understand them, and people fear or at best ignore what they do not understand. But for me, making progress on the ideas was very rewarding in itself at the time, even though they would have made the worst party conversation topics ever. Eventually, decades after, they generated more social accolades than I now know what to do with.\u201d 53 \u2014 Dara Torres: \u201cMany people have started from the bottom and have worked their way up, so don\u2019t think being at the bottom of the totem pole is a bad thing in the work world. You have nowhere to go but up. Ignore hearsay and rumors until you know it as fact.\u201d 54 \u2014 Dan Gable: \u201cDon\u2019t plan on \u201cwinning the lottery\u201d right away, because it usually doesn\u2019t happen. Doing a good job and building your assets is like winning the lottery, but over time. You gotta work hard every day, make progress every day, and make money every day. Over time, you\u2019ll be in good shape. If you do win it in the first year, hey, I\u2019ll be the first to congratulate you \u2014 but don\u2019t count on it.\u201d 55 \u2014 Darren Aronofsky: \u201cMost of the game is about persistence. It is the most important trait. Sure, when you get an opportunity, you have to perform and you have to exceed beyond all expectations, but getting that chance is the hardest part. So keep the vision clear in your head and every day refuse all obstacles to get to the goal.\u201d 56 \u2014 Evan Williams: \u201cBe in a hurry to learn, not in a hurry to get validation. In a team environment, you will make a much better impression if it seems like you\u2019re not at all worried about yourself. It\u2019s okay to actually be worried about yourself \u2014 everyone is \u2014 just don\u2019t seem like it. If you resist asking for too much, you will often get more.\u201d 57 \u2014 Bram Cohen: \u201cPick your early jobs based on what gets you the most valuable experience. If you want to be an entrepreneur, don\u2019t dive directly into doing your venture but go get work at an early-stage startup to learn the ropes and get paid to make your early mistakes. Only after getting the necessary experience and knowledge should you strike out on your own.\u201d 58 \u2014 Chris Anderson: \u201cIn your 20s, you may not really know what your best skills and opportunities are. It\u2019s much better to pursue learning, personal discipline, growth. And to seek out connections with people across the planet. For a while, it\u2019s just fine to follow and support someone else\u2019s dream. In so doing, you will be building valuable relationships, valuable knowledge. And at some point your passion will come and whisper in your ear, \u201cI\u2019m ready.\u201d 59 \u2014 Kelly Slater: \u201cThink for yourself. Everyone has a unique picture of how things work and function, and yours is as valuable as anyone\u2019s. It\u2019s sometimes the belief in yourself, open-mindedness toward others, and your delivery that allows things to be heard by others.\u201d 60 \u2014 Adam Fisher: \u201cBe humble and self-aware. Ignore the concept of \u201cbeing yourself.\u201d Of course this is literally true by definition, but it is a way to avoid self-improvement. Pursuing your passion is fine.\u201d\n\n61 \u2014 Laura Walker: \u201cGet out of your comfort zone when you graduate. Ask yourself what you are genuinely curious about and explore it. Embrace the ambiguity and contradictions that life invariably will bring, and develop habits \u2014 exercise, talking with friends, writing \u2014 that help you do so. Don\u2019t spend time chasing a right answer or a right path, but instead spend time defining how you are going to approach whatever path you choose. What values most define you? What questions do you want to pursue?\u201d 62 \u2014 Terry Laughlin: \u201cThe same will apply in any field of endeavor. If your highest goal is incremental, patient, continual learning and development in critical skills and core competencies \u2014 and you allow recognition, promotions, and financial rewards to be a natural result of the excellence you attain at core competencies \u2014 you will be far more likely to experience success and satisfaction, and perhaps even attain eminence, in your field.\u201d 63 \u2014 Marie Forleo: \u201cPursue every project, idea, or industry that genuinely lights you up, regardless of how unrelated each idea is, or how unrealistic a long-term career in that field might now seem. You\u2019ll connect the dots later. Work your fucking ass off and develop a reputation for going above and beyond in all situations. Do whatever it takes to earn enough money, so that you can go all in on experiences or learning opportunities that put you in close proximity to people you admire, because proximity is power. Show up in every moment like you\u2019re meant to be there, because your energy precedes anything you could possibly say.\u201d 64 \u2014 Drew Houston: \u201cIt\u2019s not just about passion or following your dreams. Make sure the problem you become obsessed with is one that needs solving and is one where your contribution can make a difference. As Y Combinator says, \u201cMake something people want.\u201d 65 \u2014 Scott Belsky: \u201cEvery step in your early career must get you incrementally closer to whatever genuinely interests you. The most promising path to success is pursuing genuine interests and setting yourself up for the circumstantial relationships, collaborations, and experiences that will make all the difference in your life. A labor of love always pays off, just not how and when you expect. Set yourself up to succeed by taking new jobs and roles that get you closer to your interests.\u201d 66 \u2014 Whitney Cummings: \u201cChances are, if you\u2019re reading books like this, you will succeed, but I\u2019ve found that it all feels pretty meaningless if you\u2019re not in some way helping people or improving humanity in some way. Instead of striving to be a CEO or an entrepreneur, strive to be a hero. We need more of those.\u201d 67 \u2014 Rick Rubin: \u201cI would ignore most anything you learn in school and ignore all accepted standards. Free yourself to try anything. The best ideas are revolutionary. If you\u2019re searching for wisdom, try to find it from people who\u2019ve done it more than from people who teach it. Ask a lot of questions.\u201d 68 \u2014 Peter Attia: \u201cBe as genuine as you can. Don\u2019t fake it. In my view, better to be a cold stiff than fake that you care. If you are genuinely interested in a subset of other people, even if that number is small, you will foster relationships that really matter. As we age, I believe, frivolous relationships in business and our personal life become less and less bearable, so only put energy into completely genuine interactions with other people.\u201d 69 \u2014 Jocko Willink: \u201cWork harder than everyone else. Of course, that is easy when you love your job. But you might not love your first, or second, or even third job. That doesn\u2019t matter. Work harder than everyone else. In order to get the job you love or start the company you want, you have to build your r\u00e9sum\u00e9, your reputation, and your bank account. The best way to do that: Outwork them all.\u201d 70 \u2014 Yuval Noah Harari: \u201cNobody really knows what the world and the job market will look like in 2040, hence nobody knows what to teach young people today. Consequently, it is likely that most of what you currently learn at school will be irrelevant by the time you are 40. So what should you focus on? My best advice is to focus on personal resilience and emotional intelligence\u2026 The world of 2040 will be a very different world from today, and an extremely hectic world. The pace of change is likely to accelerate even further. So people will need the ability to learn all the time and to reinvent themselves repeatedly \u2014 even at age 60.\u201d"
    },
    {
        "url": "https://medium.com/@james_aka_yale/snapchats-filters-how-computer-vision-recognizes-your-face-9907d6904b91?source=user_profile---------13----------------",
        "title": "Snapchat\u2019s Filters: How computer vision recognizes your face",
        "text": "In those moments of boredom when you\u2019re playing with Snapchat\u2019s filters \u2014 sticking your tongue out, ghoulifying your features, and working out how to get the flower crown to fit exactly on your head \u2014 surely you\u2019ve had a moment where you\u2019ve wondered what\u2019s going on, on a technical level \u2014 how Snapchat manages to match your face to the animations?\n\nAfter two weeks of researching online, I feel grateful to have finally gotten a glimpse behind the curtain. It turns out that the product is an instance of computer vision application, which is the main fuel behind all kinds of facial recognition software.\n\nThe technology came from a Ukrainian startup Looksery, which is an application that allowed users to modify their facial features during video chats and for photos. Snapchat acquired this Odesa-based face changing startup in September 2015 for $150 million dollars. That\u2019s reportedly the largest tech acquisition in Ukrainian history.\n\nTheir augmented reality filters tap into the large and rapidly growing field of computer vision. Computer vision can be thought of as a direct opposite of computer graphics. While computer graphics try to produce image models from 3D models, computer vision tries to create a 3D space from image data. Computer Vision is starting to be utilized more and more in our society. It is how you scan your checks and the data is extracted from the lines. It is how you can deposit checks with your phone. It is how Facebook knows who\u2019s in your photos, how self-driving cars can avoid running over people and how you can give yourself a dodgy nose.\n\nLooksery maintains their engineering more confidential, but every one can access their patents online. The specific area of Computer Vision that Snapchat filters use is called Image processing. Image processing is the transformation of an image by performing mathematical operations on each individual pixel on the provided picture.\n\nThe first step works like this: Given an input image or video frame, find out all present human faces and output their bounding box (i.e. The rectangle coordinates in the form: X, Y, Width & Height).\n\nFace detection has been a solved problem since the early 2000s but faces some challenges including detecting tiny, partial & non-frontal faces. The most widely used technique is a combination of Histogram of Oriented Gradients (HOG for short) and Support Vector Machine (SVM) that achieve mediocre to relatively good detection ratios given a good quality image but this method is not capable of real-time detection at least on the CPU.\n\nHere is how the HOG/SVM detector works:\n\nGiven an input image, compute the pyramidal representation of that image which is a pyramid of multi scaled downed version of the original image. For each entry on the pyramid, a sliding window approach is used. The sliding window concept is quite simple. By looping over an image with a constant step size, small image patches typically of size 64 x 128 pixels are extracted at different scales. For each patch, the algorithm makes a decision if it contains a face or not. The HOG is computed for the current window and passed to the SVM classifier (Linear or not) for the decision to take place (i.e. Face or not). When done with the pyramid, a non-maxima suppression (NMS for short) operation usually take place in order to discard stacked rectangles. You can read more about the HOG/SVM combination here.\n\nThis is the next step in our analysis phase and works as follows: For each detected face, output the local region coordinates for each member or facial feature of that face. This includes the eyes, bone, lips, nose, mouth,\u2026 coordinates usually in the form of points (X,Y).\n\nExtracting facial landmarks is a relatively cheap operation for the CPU given a bounding box (i.e. Cropped image with the target face), but quite difficult to implement for the programmer unless some not-so-fast machine learning techniques such as training & running a classifier is used.\n\nYou can find out more about extracting facial landmarks here or this PDF: One millisecond face alignment with an ensemble of regression trees. In some and obviously useful cases, face detection and landmarks extraction are combined into a single operation.\n\nNow that the face has been detected, Snapchat can use Image Processing to apply features onto a full face. However, they chose to go one step further and they want to find your facial features. This is done with the aid of the Active Shape Model.\n\nThe Active Shape Model is a facial model that has been trained by the manual marking of the borders of facial features on hundreds to thousands of images. Through machine learning, an \u201caverage face\u201d is created and aligns this with the image that is provided. This average face, of course, does not fit exactly with the user\u2019s face (we all have diverse faces), so after fitting the face, pixels around the edge of the \u201caverage face\u201d are examined to look for differences in shading. Because of the training that the algorithm went through, (the Machine Learning process), it has a basic skeleton of how certain facial features should look, so it looks for a similar pattern in the given image. Even if some of the initial changes are wrong, by taking into account the position of other points that it has fixed, the algorithm will correct errors it made regarding where it thought certain aspects of your face are. The model then adjusts and creates a mesh (a 3D model that can shift and scale with your face).\n\nThis whole facial/feature recognition process is done when you see that white net right before you choose your filter. The filters then distorts certain areas of the provided face by enhancing them or adding something on top of them.\n\nThe updated version of Snapchat a few months back had the feature for swapping faces with a friend, whether in real time or by accessing some faces from your gallery. Notice how the face shapes are visible, that\u2019s the position where the statistical model lies. It helps Snapchat to quickly align you and your friends face and swap the features.\n\nAfter locating all your features, the application creates a mesh along your face that sticks to each point frame by frame. This mesh can now be edited and modified as Snapchat feels.\n\nSome lenses do much more by either asking you to raise your eyebrows or by opening your mouth. This is also fairly simple to think about, but it requires a lot more algorithms to imply.\n\nNow as mentioned before, this technology is not new. But to perform all those processes in real time and on a mobile platform takes a lot of processing power along with some complicated algorithms. That\u2019s why Snapchat thought it\u2019s better to pay 150 million dollars to acquire Looksery instead of just building its platform.\n\nI hope this was informative and tickled your curiosity like it did mine. For now, I\u2019ll be exploring Snapchat Filters more deeply, testing out my favorite facial lens, knowing and appreciating all the computer vision that\u2019s going on behind the scenes.\n\n\u2014 How Snapchat Filter Works \u2014 Behind The Scenes (TechHundred)"
    },
    {
        "url": "https://towardsdatascience.com/12-useful-things-to-know-about-machine-learning-487d3104e28?source=user_profile---------14----------------",
        "title": "12 Useful Things to Know about Machine Learning \u2013",
        "text": "12 Useful Things to Know about Machine Learning Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. As a result, machine learning is widely used in computer sincere and other fields. However, developing successful machine learning applications requires a substantial amount of \u201cblack art\u201d that is hard to find in textbooks. I recently read an amazing technical paper by Professor Pedro Domingos of University of Washington titled \u201cA Few Useful Things to Know about Machine Learning.\u201d It summarizes 12 key lessons that machine learning researchers and practitioners have learned include pitfalls to avoid, important issues to focus on, and answers to common questions. I\u2019d like to share these lessons in this article because they are extremely useful when thinking about tackling your next machine learning problems. All machine learning algorithms generally consist of combinations of just 3 components: Representation: A classifier must be represented in some formal language that the computer can handle. Conversely, choosing a representation for a learner is tantamount to choosing the set of classifiers that it can possibly learn. This set is called the hypothesis space of the learner. If a classifier is not in the hypothesis space, it cannot be learned. A related question is how to represent the input, i.e., what features to use. Evaluation: An evaluation function is needed to distinguish good classifiers from bad ones. The evaluation function used internally by the algorithm may differ from the external one that we want the classifier to optimize, for ease of optimization and due to the issues discussed in the next section. Optimization: Finally, we need a method to search among the classifiers in the language for the highest-scoring one. The choice of optimization technique is key to the efficiency of the learner, and also helps determine the classifier produced if the evaluation function has more than one optimum. It is common for new learners to start out using off-the-shelf optimizers, which are later replaced by custom-designed ones.\n\nThe fundamental goal of machine learning is to generalize beyond the examples in the training set. This is because, no matter how much data we have, it is very unlikely that we will see those exact examples again at test time. Doing well on the training set is easy. The most common mistake among machine learning beginners is to test on the training data and have the illusion of success. If the chosen classifier is then tested on new data, it is often no better than random guessing. So, if you hire someone to build a classifier, be sure to keep some of the data to yourself and test the classifier they give you on it. Conversely, if you\u2019ve been hired to build a classifier, set some of the data aside from the beginning, and only use it to test your chosen classifier at the very end, followed by learning your final classifier on the whole data. 3 \u2014 Data Alone is Not Enough Generalization being the goal has another major consequence: data alone is not enough, no matter how much of it you have. This seems like rather depressing news. How then can we ever hope to learn anything? Luckily, the functions we want to learn in the real world are not drawn uniformly from the set of all mathematically possible functions! In fact, very general assumptions \u2014 like smoothness, similar examples having similar classes, limited dependencies, or limited complexity \u2014 are often enough to do very well, and this is a large part of why machine learning has been so successful. Like deduction, induction (what learners do) is a knowledge lever: it turns a small amount of input knowledge into a large amount of output knowledge. Induction is a vastly more powerful lever than deduction, requiring much less input knowledge to produce useful results, but it still needs more than zero input knowledge to work. And, as with any lever, the more we put in, the more we can get out.\n\nIn retrospect, the need for knowledge in learning should not be surprising. Machine learning is not magic; it can\u2019t get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is a more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. What if the knowledge and data we have are not sufficient to completely determine the correct classifier? Then we run the risk of just hallucinating a classifier (or parts of it) that is not grounded in reality, and is simply encoding random quirks in the data. This problem is called overfitting, and is the bugbear of machine learning. When your learner outputs a classifier that is 100% accurate on the training data but only 50% accurate on test data, when in fact it could have output one that is 75% accurate on both, it has overfit. Everyone in machine learning knows about overfitting, but it comes in many forms that are not immediately obvious. One way to understand overfitting is by decomposing generalization error into bias and variance. Bias is a learner\u2019s tendency to consistently learn the same wrong thing. Variance is the tendency to learn random things irrespective of the real signal. A linear learner has high bias, because when the frontier between two classes is not a hyperplane the learner is unable to induce it. Decision trees don\u2019t have this problem because they can represent any Boolean function, but on the other hand they can suffer from high variance: decision trees learned on different training sets generated by the same phenomenon are often very different, when in fact they should be the same. Cross-validation can help to combat overfitting, for example by using it to choose the best size of decision tree to learn. But it\u2019s no panacea, since if we use it to make too many parameter choices it can itself start to overfit. Besides cross-validation, there are many methods to combat overfitting. The most popular one is adding a regularization term to the evaluation function. This can, for example, penalize classifiers with more structure, thereby favoring smaller ones with less room to overfit. Another option is to perform a statistical significance test like chi-square before adding new structure, to decide whether the distribution of the class really is different with and without this structure. These techniques are particularly useful when data is very scarce. Nevertheless, you should be skeptical of claims that a particular technique \u201csolves\u201d the overfitting problem. It\u2019s easy to avoid overfitting (variance) by falling into the opposite error of underfitting (bias). Simultaneously avoiding both requires learning a perfect classifier, and short of knowing it in advance there is no single technique that will always do best (no free lunch). After overfitting, the biggest problem in machine learning is the curse of dimensionality. This expression was coined by Bellman in 1961 to refer to the fact that many algorithms that work fine in low dimensions become intractable when the input is high-dimensional. But in machine learning it refers to much more. Generalizing correctly becomes exponentially harder as the dimensionality (number of features) of the examples grow, because a fixed-size training set covers a dwindling fraction of the input space.\n\nThe general problem with high dimensions is that our intuitions, which come from a 3-dimensional world, often do not apply in high-dimensional ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant \u201cshell\u201d around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in a high-dimensional hypercube, beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor. And if we approximate a hypersphere by inscribing it in a hypercube, in high dimensions almost all the volume of the hypercube is outside the hypersphere. This is bad news for machine learning, where shapes of one type are often approximated by shapes of another. Building a classifier in 2 or 3 dimensions is easy; we can find a reasonable frontier between examples of different classes just by visual inspection. But in high dimensions it\u2019s hard to understand what is happening. This in turn makes it difficult to design a good classifier. Naively, one might think that gathering more features never hurts, since at worst they provide no new information about the class. But in fact, their benefits may be outweighed by the curse of dimensionality. 6 \u2014 Theoretical Guarantees are Not What They Seem Machine learning papers are full of theoretical guarantees. The most common type is a bound on the number of examples needed to ensure good generalization. What should you make of these guarantees? First of all, it\u2019s remarkable that they are even possible. Induction is traditionally contrasted with deduction: in deduction you can guarantee that the conclusions are correct; in induction all bets are off. Or such was the conventional wisdom for many centuries. One of the major developments of recent decades has been the realization that in fact we can have guarantees on the results of induction, particularly if we\u2019re willing to settle for probabilistic guarantees. We have to be careful about what a bound like this means. For instance, it does not say that, if your learner returned a hypothesis consistent with a particular training set, then this hypothesis probably generalizes well. What is says is that, given a large enough training set, with high probability your learner will either return a hypothesis that generalizes well or be unable to find a consistent hypothesis. The bound also says nothing about how to select a good hypothesis space. It only tells us that, if the hypothesis space contains the true classifier, then the probability that the learner outputs a bad classifier decreases with training set size. If we shrink the hypothesis space, the bound improves, but the chances that it contains the true classifier shrink also. Another common type of theoretical guarantee is asymptotic: given infinite data, the learner is guaranteed to output the correct classifier. This is reassuring, but it would be rash to choose one learner over another because of its asymptotic guarantees. In practice, we are seldom in the asymptotic regime (also known as \u201casymptopia\u201d). And, because of the bias-variance tradeoff discussed above, if learner A is better than learner B given infinite data, B is often better than A given finite data. The main role of theoretical guarantees in machine learning is not as a criterion for practical decisions, but as a source of understanding and driving force for algorithm design. In this capacity, they are quite useful; indeed, the close interplay of theory and practice is one of the main reasons machine learning has made so much progress over the years. But caveat emptor: learning is a complex phenomenon, and just because a learner has a theoretical justification and works in practice doesn\u2019t mean the former is the reason for the latter. At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. If you have many independent features that each correlate well with the class, learning is easy. On the other hand, if the class is a very complex function of the features, you may not be able to learn it. Often, the raw data is not in a form that is amenable to learning, but you can construct features from it that are. This is typically where most of the effort in a machine learning project goes. It is often also one of the most interesting parts, where intuition, creativity and \u201cblack art\u201d are as important as the technical stuff. First-timers are often surprised by how little time in a machine learning project is spent actually doing machine learning. But it makes sense if you consider how time-consuming it is to gather data, integrate it, clean it and pre-process it, and how much trial and error can go into feature design. Also, machine learning is not a one-shot process of building a dataset and running a learner, but rather an iterative process of running the learner, analyzing the results, modifying the data and/or the learner, and repeating. Learning is often the quickest part of this, but that\u2019s because we\u2019ve already mastered it pretty well! Feature engineering is more difficult because it\u2019s domain-specific, while learners can be largely general-purpose. However, there is no sharp frontier between the two, and this is another reason the most useful learners are those that facilitate incorporating knowledge.\n\nIn most of computer science, the 2 main limited resources are time and memory. In machine learning, there is a third one: training data. Which one is the bottleneck has changed from decade to decade. In the 1980\u2019s, it tended to be data. Today it is often time. Enormous mountains of data are available, but there is not enough time to process it, so it goes unused. This leads to a paradox: even though in principle more data means that more complex classifiers can be learned, in practice simpler classifiers wind up being used, because complex ones take too long to learn. Part of the answer is to come up with fast ways to learn complex classifiers, and indeed there has been remarkable progress in this direction. Part of the reason using cleverer algorithms has a smaller payoff than you might expect is that, to a first approximation, they all do the same. This is surprising when you consider representations as different as, say, sets of rules and neural networks. But in fact propositional rules are readily encoded as neural networks, and similar relationships hold between other representations. All learners essentially work by grouping nearby examples into the same class; the key difference is in the meaning of \u201cnearby.\u201d With non-uniformly distributed data, learners can produce widely different frontiers while still making the same predictions in the regions that matter (those with a substantial number of training examples, and therefore also where most text examples are likely to appear). This also helps explain why powerful learns can be unstable but still accurate. As a rule, it pays to try the simplest learners first (e.g., naive Bayes before logistic regression, k-nearest neighbor before support vector machines). More sophisticated learners are seductive, but they are usually harder to use, because they have more knobs you need to turn to get good results, and because their internals are more opaque). Learners can be divided into 2 major types: those whose representation has a fixed size, like linear classifiers, and those whose representation can grow with the data, like decision trees. Fixed-size learners can only take advantage of so much data. Variable-size learners can in principle learn any function given sufficient data, but in practice they may not, because of limitations of the algorithm or computational cost. Also, because of the curse of dimensionality, no existing amount of data may be enough. For these reasons, clever algorithms \u2014 those that make the most of the data and computing resources available \u2014 often pay off in the end, provided you\u2019re willing to put in the effort. There is no sharp frontier between designing learners and learning classifiers; rather, any given piece of knowledge could be encoded in the learner or learned from data. So machine learning projects often wind up having a significant component of learner design, and practitioners need to have some expertise in it. 9 \u2014 Learn Many Models, Not Just One In the early days of machine learning, everyone had their favorite learner, together with some a priori reasons to believe in its superiority. Most effort went into trying many variations of it and selecting the best one. Then systematic empirical comparisons showed that the best learner varies from application to application, and systems containing many different learners started to appear. Effort now went into trying many variations of many learners, and still selecting just the best one. But then researchers noticed that, if instead of selecting the best variation found, we combine many variations, the results are better \u2014 often much better \u2014 and at little extra effort for the user. Creating such model ensembles is now standard. In the simplest technique, called bagging, we simply generate random variations of the training set by resampling, learn a classifier on each, and combine the results by voting. This works because it greatly reduces variance while only slightly increasing bias. In boosting, training examples have weights, and these are varied so that each new classifier focuses on the examples the previous ones tended to get wrong. In stacking, the outputs of individual classifiers become the inputs of a \u201chigher-level\u201d learner that figures out how best to combine them. Many other techniques exist, and the trend is toward larger and larger ensembles. In the Netflix prize, teams from all over the world competed to build the best video recommender system. As the competition progressed, teams found that they obtained the best results by combining their learners with other teams\u2019, and merged into larger and larger teams. The winner and runner-up were both stacked ensembles of over 100 learners, and combining the two ensembles further improved the results. Doubtless we will see even larger ones in the future. Occam\u2019s razor famously states that entities should not be multiplied beyond necessity. In machine learning, this is often taken to mean that, given two classifiers with the same training error, the simpler of the two will likely have the lowest test error. Purported proofs of this claim appear regularly in the literature, but in fact there are many counter-examples to it, and the \u201cno free lunch\u201d theorems imply it cannot be true.\n\nWe saw one counterexample in the previous section: model ensembles. The generalization error of a boosted ensemble continues to improve by adding classifiers even after the training error has reached zero. Thus, contrary to intuition, there is no necessary connection between the number of parameters of a model and its tendency to overfit. A more sophisticated view instead equates complexity with the size of the hypothesis space, on the basis that smaller spaces allow hypotheses to be represented by shorter codes. Bounds like the one in the section on theoretical guarantees above might then be viewed as implying that shorter hypotheses generalize better. This can be further refined by assigning shorter codes to the hypothesis in the space that we have some a priori preference for. But viewing this as \u201cproof\u201d of a tradeoff between accuracy and simplicity is circular reasoning: we made the hypotheses we prefer simpler by design, and if they are accurate it\u2019s because our preferences are accurate, not because the hypotheses are \u201csimple\u201d in the representation we chose. Essentially all representations used in variable-size learners have associated theorems of the form \u201cEvery function can be represented, or approximated arbitrarily closely, using this representation.\u201d Reassured by this, fans of the representation often proceed to ignore all others. However, just because a function can be represented does not mean it can be learned. For example, standard decision tree learners cannot learn trees with more leaves than there are training examples. In continuous spaces, representing even simple functions using a fixed set of primitives often requires an infinite number of components. Further, if the hypothesis space has many local optima of the evaluation function, as is often the case, the learner may not find the true function even if it is representable. Given finite data, time and memory, standard learners can learn only a tiny subset of all possible functions, and these subsets are different for learners with different representations. Therefore the key question is not \u201cCan it be represented?\u201d, to which the answer is often trivial, but \u201cCan it be learned?\u201d And it pays to try different learners (and possibly combine them). The point that correlation does not imply causation is made so often that it is perhaps not worth belaboring. But, even though learners of the kind we have been discussing can only learn correlations, their results are often treated as representing causal relations. Isn\u2019t this wrong? if so, then why do people do it? More often than not, the goal of learning predictive models is to use them as guides to action. If we find that beer and diapers are often bought together at the supermarket, then perhaps putting beer next to the diaper section will increase sales. But short of actually doing the experiment it\u2019s difficult to tell. Machine learning is usually applied to observational data, where the predictive variables are not under control of the learner, as opposed to experimental data, where they are. Some learning algorithms can potentially extract causal information from observational data but their applicability is rather restricted. On the other hand, correlation is a sign of a potential causal connection, and we can use it as a guide to further investigation. Like any discipline, machine learning has a lot of \u201cfolk wisdom\u201d that can be hard to come by, but is crucial for success. Professor Domingos\u2019 paper summarized some of the most salient items that you need to know. If you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://medium.com/@james_aka_yale/16-useful-advices-for-aspiring-data-scientists-804ce5611939?source=user_profile---------15----------------",
        "title": "16 Useful Advice for Aspiring Data Scientists \u2013 James Le \u2013",
        "text": "Why is data science sexy? It has something to do with so many new applications and entire new industries come into being from the judicious use of copious amounts of data. Examples include speech recognition, object recognition in computer vision, robots and self-driving cars, bioinformatics, neuroscience, the discovery of exoplanets and an understanding of the origins of the universe, and the assembling of inexpensive but winning baseball teams. In each of these instances, the data scientist is central to the whole enterprise. He/she must combine knowledge of the application area with statistical expertise and implement it all using the latest in computer science ideas.\n\nIn the end, sexiness comes down to being effective. I recently read Sebastian Gutierrez\u2019s \u201cData Scientists at Work\u201d, in which he interviewed 16 data scientists across 16 different industries to understand both how they think about it theoretically and also very practically what problems they\u2019re solving, how data\u2019s helping, and what it takes to be successful. All 16 interviewees are at the forefront of understanding and extracting value from data across an array of public and private organizational types \u2014 from startups and mature corporations to primary research groups and humanitarian nonprofits \u2014 and across a diverse range of industries \u2014 advertising, e-commerce, email marketing, enterprise cloud computing, fashion, industrial internet, internet television and entertainment, music, nonprofit, neurobiology, newspapers and media, professional and social networks, retail, sales intelligence, and venture capital.\n\nIn particular, Sebastian asked open-ended questions so that the personalities and spontaneous thought processes of each interviewee would shine through clearly and accurately. The practitioners in this book share their thoughts on what data science means to them and how they think about it, their suggestions on how to join the field, and their wisdom won through experience on what a data scientist must understand deeply to be successful within the field.\n\nIn this post, I want to share the best answers that these data scientists gave for the question:"
    },
    {
        "url": "https://towardsdatascience.com/pinterests-visual-lens-how-computer-vision-explores-your-taste-5470f87502ad?source=user_profile---------16----------------",
        "title": "Pinterest\u2019s Visual Lens: How computer vision explores your taste",
        "text": "When it comes to looking for something you want to try \u2014 a new salad recipe, a new classy dress, a new chair for your living room \u2014 you really need to see it first. Humans are visual creatures. We use our eyes to decide if something looks good, or if it matches our style. I\u2019m a huge fan of Pinterest, and particularly its Visual Lens. Why? It allows me to discover things that intrigue me, and allow me to go inside. When I spot something out in the world that looks interesting, but when I try to search for it online later, words fail me. I have this rich, colorful picture in my mind, but I can\u2019t translate into the words I need to find it. Pinterest\u2019s Visual Lens is a way to discover ideas without having to find the right words to describe them first.\n\nJust point Lens at a pair of shoes, then tap to see related styles or even ideas for what else to wear them with. Or try it on a table to find similar designs, and even other furniture from the same era. You can also use Lens with food. Just point it at cauliflower or potatoes to see what recipes come up. Patterns and colors can also lead you in fun, interesting or even just plain weird new directions. So how does Pinterest do such an amazing job of searching through vision and personalizing visual recommendations for its users? After two weeks of digging through the company\u2019s engineering blog and press exposure, I feel grateful to have finally gotten a glimpse behind the curtain. It turns out that the product is an instance of machine learning applications at Pinterest, which are widespread in a variety of business areas. Let\u2019s zoom out for a second to look at how machine learning is being used at Pinterest. An Overview of Machine Learning Usage at Pinterest As a visual discovery engine, Pinterest has many challenging problems that can be solved using machine learning techniques: What interests shall we recommend to a new user? How to generate an engaging home-feed? How do pins relate to each other? What interests does a pin belong to? The critical moment happened in January 2015, when Pinterest acquired Kosei \u2014 a machine learning startup with expertise in recommender systems. Since then, machine learning has been used across Pinterest in multiple areas: from the Discovery team that provides recommendations, related content, and predicts the likelihood that a person will pin content; to the Growth team that uses intelligence models to determine which emails to send and prevent churn; from the Monetization team that does ad performance and relevance prediction; to the Data team that builds out a real-time distributed system for machine learning with Spark. Let\u2019s dig a little bit deeper into how Pinterest engineers are leveraging machine learning to keep the website\u2019s 175 million+ users pinning and sharing: Identifying Visual Similarities: Machine learning can not only determine the subject of an image, it can also identify visual patterns and match them to other photos. Pinterest is using this technology to process 150 million image searches per month, helping users find content that looks like pictures they\u2019ve already pinned. Categorizing and Curating: If a user pins a mid-century dining-room table, the platform can now offer suggestions of other objects from the same era. The key? Metadata, such as the names of pinboards and websites where images have been posted, helps the platform understand what photos represent. Predicting Engagement: While many platforms prioritize content from a user\u2019s friends and contacts, Pinterest pays more attention to an individual\u2019s tastes and habits \u2014 what they\u2019ve pinned and when \u2014 enabling the site to surface more personalized recommendations. Prioritizing Local Taste: Pinterest is an increasingly global platform, with more than half of its users based outside the U.S. Its recommendation engine has learned to suggest popular content from users\u2019 local region in their native language. Going Beyond Images: Analyzing what\u2019s in a photo is a big factor in the site\u2019s recommendations, but it doesn\u2019t offer the whole story. Pinterest also looks at captions from previously pinned content and which items get pinned to the same virtual boards. That allows Pinterest to, say, link a particular dress to the pair of shoes frequently pinned alongside it, even if they look nothing alike. Pinterest Lens is a product that is part of the effort to identify visual similarities, along with a host of other engineering works. They all utilize machine learning algorithms and technologies under an ever-growing field called computer vision, which I am going to explain in depth below. A Brief History of Computer Vision at Pinterest Computer vision is a field of computer science and subfield of machine learning that works on enabling computers to see, identify and process images in the same way that human vision does, and then provide the appropriate output. It is like imparting human intelligence and instincts to a computer. Pinterest uses computer vision heavily to power their visual discovery products. Pinterest set its sights on visual search in 2014. That year, the company acquired VisualGraph, an image-recognition startup, and established its computer vision team with a small group of engineers and began to show its work. In 2015, it launched visual search, a way to search for ideas without text queries. For the first time, visual search gave people a way to get results even when they can\u2019t find the right words to describe what they\u2019re looking for. In summer 2016, visual search evolved as Pinterest rolled out object detection, which finds all the objects in a pin\u2019s image in real-time and serves related results. Since then, visual search has become one of its most-used features, with hundreds of millions of visual searches every month, and billions of objects detected. Early 2017, it introduced 3 new products on top of visual discovery infrastructure: Pinterest Lens is a way to discover ideas with a phone\u2019s camera inspired by what users see in the world around them. Shop The Look is a way to shop and buy products users see inside Pins. Instant Ideas is a way to transform users\u2019 home feed with similar ideas in just a tap. Most recently about 2 months back, it announced a couple more ways to find products and ideas for users: Lens Your Look is a new way to find outfit ideas inspired by your wardrobe and the next big steps for Pinterest Lens. Responsive Visual Search is a seamless and immersive way to search images through zooming into pin objects. Pinterest Pincodes, in which you just pull out the Pinterest camera and scan any Pincode to see curated ideas on Pinterest inspired by what you\u2019re looking at in the real world. Let\u2019s dig deeper into the computer vision models that Pinterest engineers employ for their visual discovery work behind Pinterest Lens. Lens combines Pinterest\u2019s understanding of images and objects with its discovery technologies to offer Pinners a diverse set of results. For example, if you take a picture of a blueberry, Lens doesn\u2019t just return blueberries: it also gives you more results such as recipes for blueberry scones and smoothies, beauty ideas like detox scrubs or tips for growing your own blueberry bush. To do this, Lens\u2019 overall architecture is separated into two logical components. The first component is a query understanding layer where Pinterest derives information regarding the given input image. Here Pinterest computes visual features such as detecting objects, computing salient colors and detecting lighting and image quality conditions. Using the visual features, it also computes semantic features such as annotations and category. The second component is Pinterest\u2019s blender, as the results Lens returns come from multiple sources. Pinterest uses visual search technology to return visually similar results, object search technology to return scenes or projects with visually similar objects and image search which uses the derived annotations to return personalized text search results that are semantically (not visually) relevant to the input image. It\u2019s the job of the blender to dynamically change blending ratios and result sources based on the information derived in the query understanding layer.\n\nAs shown above, Lens results aren\u2019t strictly visually similar, they come from multiple sources, some of which are only semantically relevant to the input image. By giving Pinners results beyond visually similar, Lens is a new type of visual discovery tool that bridges real-world camera images to the Pinterest taste graph. Let\u2019s go ahead and dissect that blender component of Lens, which include Image, Object, and Visual Search. Pinterest\u2019s Image Search technology dates back in 2015 when the company shared a white paper that details its system architecture and insights from experiments to build a scalable machine vision pipeline. Pinterest conducted a comprehensive set of experiments using a combination of benchmark datasets and A/B testing on two Pinterest applications, Related Pins and an experiment with similar looks. In particular, the experiment with similar looks allowed Pinterest to show visually similar Pin recommendations based on specific objects in a Pin\u2019s image. It experimented with different ways to use surface object recognition that would enable Pinner to click into the objects. Then it used object recognition to detect products such as bags, shoes and skirts from a Pin\u2019s image. From these detected objects, it extracted visual features to generate product recommendations (\u201csimilar looks\u201d). In the initial experiment, a Pinner would discover recommendations if there was a red dot on the object in the Pin. Clicking on the red dot loads a feed of Pins featuring visually similar objects. Visual search was improved dramatically when Pinterest introduced automatic object detection for the most popular categories on Pinterest in 2016, so people can visually search for products within a Pin\u2019s image.\n\nSince an image can contain dozens of objects, Pinterest\u2019s motivation was to make it as simple as possible to start a discovery experience from any of them. In the same way auto-complete improves the experience of text search, automatic object detection makes visual search a more seamless experience. Object detection in visual search also enables new features, like object-to-object matching. For example, say you spot a coffee table you love either on Pinterest or at a friend\u2019s house, soon you\u2019ll be able to see how it would look in many different home settings. Pinterest\u2019s first challenge in building automatic object detection was collecting labeled bounding boxes for regions of interest in images as our training data. Since launch, it has processed nearly 1 billion image crops (visual searches). By aggregating this activity across the millions of images with the highest engagement, it learns which objects Pinners are interested in. It aggregates annotations of visually similar results to each crop and assigns a weak label across hundreds of object categories. An example of how this looks is shown in the heat map visualization below, where two clusters of user crops are formed, one around the \u201cscarf\u201d annotation, and another around the \u201cbag\u201d annotation. Since Pinterest\u2019s visual search engine can use any image as a query \u2014 including unseen content from the web and even your camera \u2014 detection must happen in real-time, in a fraction of a second. One of the most widely used detection models Pinterest has experimented with extensively is Faster R-CNN, which uses a deep network to detect objects within images in two major steps. First, it identifies regions of an image that are likely to contain objects of interest by running a fully convolutional network over the input image to produce a feature map. For each location on the feature map, the network considers a fixed set of regions, varying in size and aspect ratio, and uses a binary softmax classifier to determine how likely each of these regions is to contain an object of interest. If a promising region is found, the network also outputs adjustments to this region so that it better frames the objects. Once the network has found regions of interest, it examines the most promising ones and attempts to either identify each as a particular category of object or discards it if no objects are found. For each candidate region, the network performs spatial pooling over the corresponding portion of a convolutional feature map, thereby producing a feature vector with a fixed size independent of the size of the region. This pooled feature is then used as the input to a detection network, which uses a softmax classifier to identify each region as either background or one of our object categories. If an object is detected, the network once again outputs adjustments to the region boundaries to further refine detection quality. Finally, a round of non-maximum suppression (NMS) is performed over the detections to filter out any duplicate detections, and the results are presented to the user. Traditionally, visual search systems have treated whole images as the unit. These systems index global image representations to return images similar holistically to the given input image. With better image representations as a result of advancements in deep learning, visual search systems have reached an unprecedented level of accuracy. However, Pinterest wanted to push the bounds of visual search technology to go beyond the whole image as the unit. By utilizing its corpus of billions of objects, combined with its real-time object detector, Pinterest can understand images on a more fine-grained level. Now, it knows both the location and the semantic meaning of billions of objects in its image corpus. Object search is a visual search system that treats objects as the unit. Given an input image, Pinterest finds the most visually similar objects in billions of images in a fraction of a second, map those objects to the original image and return scenes containing the similar objects. The Future of Visual Discovery at Pinterest In a world where everyone has a camera in her pocket, many experts believe that visual search \u2014 taking photos instead of searching via text queries \u2014 will become the de facto way we look up information. Pinterest is sitting on what might be the cleanest, biggest data set in the world to train computers to see images\u2013the equivalent of a small nation hiding a nuclear armament. That\u2019s billions of photos of furniture, food, and clothing, that have been hand-labeled by Pinterest\u2019s own users for years. At Pinterest, users come to casually window shop a better life, starting with remarkably unspecific queries like \u201cdinner ideas\u201d or \u201cfashion\u201d they often might search again and again, week after week. As a result of both this behavior and the site\u2019s gridded layout of photo pins, Pinterest can build visual search into its platform, not to offer one perfect answer, but an imperfect collection of inspiration."
    },
    {
        "url": "https://towardsdatascience.com/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-dde4edffae11?source=user_profile---------17----------------",
        "title": "A Tour of The Top 10 Algorithms for Machine Learning Newbies",
        "text": "In machine learning, there\u2019s something called the \u201cNo Free Lunch\u201d theorem. In a nutshell, it states that no one algorithm works best for every problem, and it\u2019s especially relevant for supervised learning (i.e. predictive modeling).\n\nFor example, you can\u2019t say that neural networks are always better than decision trees or vice-versa. There are many factors at play, such as the size and structure of your dataset.\n\nAs a result, you should try many different algorithms for your problem, while using a hold-out \u201ctest set\u201d of data to evaluate performance and select the winner.\n\nOf course, the algorithms you try must be appropriate for your problem, which is where picking the right machine learning task comes in. As an analogy, if you need to clean your house, you might use a vacuum, a broom, or a mop, but you wouldn\u2019t bust out a shovel and start digging.\n\nHowever, there is a common principle that underlies all supervised machine learning algorithms for predictive modeling.\n\nThis is a general learning task where we would like to make predictions in the future (Y) given new examples of input variables (X). We don\u2019t know what the function (f) looks like or its form. If we did, we would use it directly and we would not need to learn it from data using machine learning algorithms.\n\nThe most common type of machine learning is to learn the mapping Y = f(X) to make predictions of Y for new X. This is called predictive modeling or predictive analytics and our goal is to make the most accurate predictions possible.\n\nFor machine learning newbies who are eager to understand the basic of machine learning, here is a quick tour on the top 10 machine learning algorithms used by data scientists.\n\nLinear regression is perhaps one of the most well-known and well-understood algorithms in statistics and machine learning.\n\nPredictive modeling is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. We will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends.\n\nThe representation of linear regression is an equation that describes a line that best fits the relationship between the input variables (x) and the output variables (y), by finding specific weightings for the input variables called coefficients (B).\n\nWe will predict y given the input x and the goal of the linear regression learning algorithm is to find the values for the coefficients B0 and B1.\n\nDifferent techniques can be used to learn the linear regression model from data, such as a linear algebra solution for ordinary least squares and gradient descent optimization.\n\nLinear regression has been around for more than 200 years and has been extensively studied. Some good rules of thumb when using this technique are to remove variables that are very similar (correlated) and to remove noise from your data, if possible. It is a fast and simple technique and good first algorithm to try.\n\nLogistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).\n\nLogistic regression is like linear regression in that the goal is to find the values for the coefficients that weight each input variable. Unlike linear regression, the prediction for the output is transformed using a non-linear function called the logistic function.\n\nThe logistic function looks like a big S and will transform any value into the range 0 to 1. This is useful because we can apply a rule to the output of the logistic function to snap values to 0 and 1 (e.g. IF less than 0.5 then output 1) and predict a class value.\n\nBecause of the way that the model is learned, the predictions made by logistic regression can also be used as the probability of a given data instance belonging to class 0 or class 1. This can be useful for problems where you need to give more rationale for a prediction.\n\nLike linear regression, logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other. It\u2019s a fast model to learn and effective on binary classification problems.\n\nLogistic Regression is a classification algorithm traditionally limited to only two-class classification problems. If you have more than two classes then the Linear Discriminant Analysis algorithm is the preferred linear classification technique.\n\nThe representation of LDA is pretty straight forward. It consists of statistical properties of your data, calculated for each class. For a single input variable this includes:\n\nPredictions are made by calculating a discriminate value for each class and making a prediction for the class with the largest value. The technique assumes that the data has a Gaussian distribution (bell curve), so it is a good idea to remove outliers from your data before hand. It\u2019s a simple and powerful method for classification predictive modeling problems.\n\nDecision Trees are an important type of algorithm for predictive modeling machinelearning.\n\nThe representation of the decision tree model is a binary tree. This is your binary tree from algorithms and data structures, nothing too fancy. Each node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n\nThe leaf nodes of the tree contain an output variable (y) which is used to make a prediction. Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n\nTrees are fast to learn and very fast for making predictions. They are also often accurate for a broad range of problems and do not require any special preparation for your data.\n\nNaive Bayes is a simple but surprisingly powerful algorithm for predictive modeling.\n\nThe model is comprised of two types of probabilities that can be calculated directly from your training data: 1) The probability of each class; and 2) The conditional probability for each class given each x value. Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem. When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities.\n\nNaive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems.\n\nThe KNN algorithm is very simple and very effective. The model representation for KNN is the entire training dataset. Simple right?\n\nPredictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression problems, this might be the mean output variable, for classification problems this might be the mode (or most common) class value.\n\nThe trick is in how to determine the similarity between the data instances. The simplest technique if your attributes are all of the same scale (all in inches for example) is to use the Euclidean distance, a number you can calculate directly based on the differences between each input variable.\n\nKNN can require a lot of memory or space to store all of the data, but only performs a calculation (or learn) when a prediction is needed, just in time. You can also update and curate your training instances over time to keep predictions accurate.\n\nThe idea of distance or closeness can break down in very high dimensions (lots of input variables) which can negatively affect the performance of the algorithm on your problem. This is called the curse of dimensionality. It suggests you only use those input variables that are most relevant to predicting the output variable.\n\nA downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset. The Learning Vector Quantization algorithm (or LVQ for short) is an artificial neural network algorithm that allows you to choose how many training instances to hang onto and learns exactly what those instances should look like.\n\nThe representation for LVQ is a collection of codebook vectors. These are selected randomly in the beginning and adapted to best summarize the training dataset over a number of iterations of the learning algorithm. After learned, the codebook vectors can be used to make predictions just like K-Nearest Neighbors. The most similar neighbor (best matching codebook vector) is found by calculating the distance between each codebook vector and the new data instance. The class value or (real value in the case of regression) for the best matching unit is then returned as the prediction. Best results are achieved if you rescale your data to have the same range, such as between 0 and 1.\n\nIf you discover that KNN gives good results on your dataset try using LVQ to reduce the memory requirements of storing the entire training dataset.\n\nSupport Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.\n\nA hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions, you can visualize this as a line and let\u2019s assume that all of our input points can be completely separated by this line. The SVM learning algorithm finds the coefficients that results in the best separation of the classes by the hyperplane.\n\nThe distance between the hyperplane and the closest data points is referred to as the margin. The best or optimal hyperplane that can separate the two classes is the line that has the largest margin. Only these points are relevant in defining the hyperplane and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane. In practice, an optimization algorithm is used to find the values for the coefficients that maximizes the margin.\n\nSVM might be one of the most powerful out-of-the-box classifiers and worth trying on your dataset.\n\nRandom Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.\n\nThe bootstrap is a powerful statistical method for estimating a quantity from a data sample. Such as a mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value.\n\nIn bagging, the same approach is used, but instead for estimating entire statistical models, most commonly decision trees. Multiple samples of your training data are taken then models are constructed for each data sample. When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value.\n\nRandom forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness.\n\nThe models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their unique and different ways. Combining their predictions results in a better estimate of the true underlying output value.\n\nIf you get good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm.\n\nBoosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n\nAdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting. Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.\n\nAdaBoost is used with short decision trees. After the first tree is created, the performance of the tree on each training instance is used to weight how much attention the next tree that is created should pay attention to each training instance. Training data that is hard to predict is given more weight, whereas easy to predict instances are given less weight. Models are created sequentially one after the other, each updating the weights on the training instances that affect the learning performed by the next tree in the sequence. After all the trees are built, predictions are made for new data, and the performance of each tree is weighted by how accurate it was on training data.\n\nBecause so much attention is put on correcting mistakes by the algorithm it is important that you have clean data with outliers removed.\n\nA typical question asked by a beginner, when facing a wide variety of machine learning algorithms, is \u201cwhich algorithm should I use?\u201d The answer to the question varies depending on many factors, including: (1) The size, quality, and nature of data; (2) The available computational time; (3) The urgency of the task; and (4) What you want to do with the data.\n\nEven an experienced data scientist cannot tell which algorithm will perform the best before trying different algorithms. Although there are many other Machine Learning algorithms, these are the most popular ones. If you\u2019re a newbie to Machine Learning, these would be a good starting point to learn.\n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://medium.com/@james_aka_yale/12-useful-things-to-know-about-machine-learning-c599be92c98d?source=user_profile---------18----------------",
        "title": "12 Useful Things to Know about Machine Learning \u2013 James Le \u2013",
        "text": "12 Useful Things to Know about Machine Learning Machine learning algorithms can figure out how to perform important tasks by generalizing from examples. This is often feasible and cost-effective where manual programming is not. As more data becomes available, more ambitious problems can be tackled. As a result, machine learning is widely used in computer sincere and other fields. However, developing successful machine learning applications requires a substantial amount of \u201cblack art\u201d that is hard to find in textbooks. I recently read an amazing technical paper by Professor Pedro Domingos of University of Washington titled \u201cA Few Useful Things to Know about Machine Learning.\u201d It summarizes 12 key lessons that machine learning researchers and practitioners have learned include pitfalls to avoid, important issues to focus on, and answers to common questions. I\u2019d like to share these lessons in this article because they are extremely useful when thinking about tackling your next machine learning problems. All machine learning algorithms generally consist of combinations of just 3 components: Representation: A classifier must be represented in some formal language that the computer can handle. Conversely, choosing a representation for a learner is tantamount to choosing the set of classifiers that it can possibly learn. This set is called the hypothesis space of the learner. If a classifier is not in the hypothesis space, it cannot be learned. A related question is how to represent the input, i.e., what features to use. Evaluation: An evaluation function is needed to distinguish good classifiers from bad ones. The evaluation function used internally by the algorithm may differ from the external one that we want the classifier to optimize, for ease of optimization and due to the issues discussed in the next section. Optimization: Finally, we need a method to search among the classifiers in the language for the highest-scoring one. The choice of optimization technique is key to the efficiency of the learner, and also helps determine the classifier produced if the evaluation function has more than one optimum. It is common for new learners to start out using off-the-shelf optimizers, which are later replaced by custom-designed ones.\n\nThe fundamental goal of machine learning is to generalize beyond the examples in the training set. This is because, no matter how much data we have, it is very unlikely that we will see those exact examples again at test time. Doing well on the training set is easy. The most common mistake among machine learning beginners is to test on the training data and have the illusion of success. If the chosen classifier is then tested on new data, it is often no better than random guessing. So, if you hire someone to build a classifier, be sure to keep some of the data to yourself and test the classifier they give you on it. Conversely, if you\u2019ve been hired to build a classifier, set some of the data aside from the beginning, and only use it to test your chosen classifier at the very end, followed by learning your final classifier on the whole data. 3 \u2014 Data Alone is Not Enough Generalization being the goal has another major consequence: data alone is not enough, no matter how much of it you have. This seems like rather depressing news. How then can we ever hope to learn anything? Luckily, the functions we want to learn in the real world are not drawn uniformly from the set of all mathematically possible functions! In fact, very general assumptions \u2014 like smoothness, similar examples having similar classes, limited dependencies, or limited complexity \u2014 are often enough to do very well, and this is a large part of why machine learning has been so successful. Like deduction, induction (what learners do) is a knowledge lever: it turns a small amount of input knowledge into a large amount of output knowledge. Induction is a vastly more powerful lever than deduction, requiring much less input knowledge to produce useful results, but it still needs more than zero input knowledge to work. And, as with any lever, the more we put in, the more we can get out.\n\nIn retrospect, the need for knowledge in learning should not be surprising. Machine learning is not magic; it can\u2019t get something from nothing. What it does is get more from less. Programming, like all engineering, is a lot of work: we have to build everything from scratch. Learning is a more like farming, which lets nature do most of the work. Farmers combine seeds with nutrients to grow crops. Learners combine knowledge with data to grow programs. What if the knowledge and data we have are not sufficient to completely determine the correct classifier? Then we run the risk of just hallucinating a classifier (or parts of it) that is not grounded in reality, and is simply encoding random quirks in the data. This problem is called overfitting, and is the bugbear of machine learning. When your learner outputs a classifier that is 100% accurate on the training data but only 50% accurate on test data, when in fact it could have output one that is 75% accurate on both, it has overfit. Everyone in machine learning knows about overfitting, but it comes in many forms that are not immediately obvious. One way to understand overfitting is by decomposing generalization error into bias and variance. Bias is a learner\u2019s tendency to consistently learn the same wrong thing. Variance is the tendency to learn random things irrespective of the real signal. A linear learner has high bias, because when the frontier between two classes is not a hyperplane the learner is unable to induce it. Decision trees don\u2019t have this problem because they can represent any Boolean function, but on the other hand they can suffer from high variance: decision trees learned on different training sets generated by the same phenomenon are often very different, when in fact they should be the same. Cross-validation can help to combat overfitting, for example by using it to choose the best size of decision tree to learn. But it\u2019s no panacea, since if we use it to make too many parameter choices it can itself start to overfit. Besides cross-validation, there are many methods to combat overfitting. The most popular one is adding a regularization term to the evaluation function. This can, for example, penalize classifiers with more structure, thereby favoring smaller ones with less room to overfit. Another option is to perform a statistical significance test like chi-square before adding new structure, to decide whether the distribution of the class really is different with and without this structure. These techniques are particularly useful when data is very scarce. Nevertheless, you should be skeptical of claims that a particular technique \u201csolves\u201d the overfitting problem. It\u2019s easy to avoid overfitting (variance) by falling into the opposite error of underfitting (bias). Simultaneously avoiding both requires learning a perfect classifier, and short of knowing it in advance there is no single technique that will always do best (no free lunch). After overfitting, the biggest problem in machine learning is the curse of dimensionality. This expression was coined by Bellman in 1961 to refer to the fact that many algorithms that work fine in low dimensions become intractable when the input is high-dimensional. But in machine learning it refers to much more. Generalizing correctly becomes exponentially harder as the dimensionality (number of features) of the examples grow, because a fixed-size training set covers a dwindling fraction of the input space.\n\nThe general problem with high dimensions is that our intuitions, which come from a 3-dimensional world, often do not apply in high-dimensional ones. In high dimensions, most of the mass of a multivariate Gaussian distribution is not near the mean, but in an increasingly distant \u201cshell\u201d around it; and most of the volume of a high-dimensional orange is in the skin, not the pulp. If a constant number of examples is distributed uniformly in a high-dimensional hypercube, beyond some dimensionality most examples are closer to a face of the hypercube than to their nearest neighbor. And if we approximate a hypersphere by inscribing it in a hypercube, in high dimensions almost all the volume of the hypercube is outside the hypersphere. This is bad news for machine learning, where shapes of one type are often approximated by shapes of another. Building a classifier in 2 or 3 dimensions is easy; we can find a reasonable frontier between examples of different classes just by visual inspection. But in high dimensions it\u2019s hard to understand what is happening. This in turn makes it difficult to design a good classifier. Naively, one might think that gathering more features never hurts, since at worst they provide no new information about the class. But in fact, their benefits may be outweighed by the curse of dimensionality. 6 \u2014 Theoretical Guarantees are Not What They Seem Machine learning papers are full of theoretical guarantees. The most common type is a bound on the number of examples needed to ensure good generalization. What should you make of these guarantees? First of all, it\u2019s remarkable that they are even possible. Induction is traditionally contrasted with deduction: in deduction you can guarantee that the conclusions are correct; in induction all bets are off. Or such was the conventional wisdom for many centuries. One of the major developments of recent decades has been the realization that in fact we can have guarantees on the results of induction, particularly if we\u2019re willing to settle for probabilistic guarantees. We have to be careful about what a bound like this means. For instance, it does not say that, if your learner returned a hypothesis consistent with a particular training set, then this hypothesis probably generalizes well. What is says is that, given a large enough training set, with high probability your learner will either return a hypothesis that generalizes well or be unable to find a consistent hypothesis. The bound also says nothing about how to select a good hypothesis space. It only tells us that, if the hypothesis space contains the true classifier, then the probability that the learner outputs a bad classifier decreases with training set size. If we shrink the hypothesis space, the bound improves, but the chances that it contains the true classifier shrink also. Another common type of theoretical guarantee is asymptotic: given infinite data, the learner is guaranteed to output the correct classifier. This is reassuring, but it would be rash to choose one learner over another because of its asymptotic guarantees. In practice, we are seldom in the asymptotic regime (also known as \u201casymptopia\u201d). And, because of the bias-variance tradeoff discussed above, if learner A is better than learner B given infinite data, B is often better than A given finite data. The main role of theoretical guarantees in machine learning is not as a criterion for practical decisions, but as a source of understanding and driving force for algorithm design. In this capacity, they are quite useful; indeed, the close interplay of theory and practice is one of the main reasons machine learning has made so much progress over the years. But caveat emptor: learning is a complex phenomenon, and just because a learner has a theoretical justification and works in practice doesn\u2019t mean the former is the reason for the latter. At the end of the day, some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used. If you have many independent features that each correlate well with the class, learning is easy. On the other hand, if the class is a very complex function of the features, you may not be able to learn it. Often, the raw data is not in a form that is amenable to learning, but you can construct features from it that are. This is typically where most of the effort in a machine learning project goes. It is often also one of the most interesting parts, where intuition, creativity and \u201cblack art\u201d are as important as the technical stuff. First-timers are often surprised by how little time in a machine learning project is spent actually doing machine learning. But it makes sense if you consider how time-consuming it is to gather data, integrate it, clean it and pre-process it, and how much trial and error can go into feature design. Also, machine learning is not a one-shot process of building a dataset and running a learner, but rather an iterative process of running the learner, analyzing the results, modifying the data and/or the learner, and repeating. Learning is often the quickest part of this, but that\u2019s because we\u2019ve already mastered it pretty well! Feature engineering is more difficult because it\u2019s domain-specific, while learners can be largely general-purpose. However, there is no sharp frontier between the two, and this is another reason the most useful learners are those that facilitate incorporating knowledge.\n\nIn most of computer science, the 2 main limited resources are time and memory. In machine learning, there is a third one: training data. Which one is the bottleneck has changed from decade to decade. In the 1980\u2019s, it tended to be data. Today it is often time. Enormous mountains of data are available, but there is not enough time to process it, so it goes unused. This leads to a paradox: even though in principle more data means that more complex classifiers can be learned, in practice simpler classifiers wind up being used, because complex ones take too long to learn. Part of the answer is to come up with fast ways to learn complex classifiers, and indeed there has been remarkable progress in this direction. Part of the reason using cleverer algorithms has a smaller payoff than you might expect is that, to a first approximation, they all do the same. This is surprising when you consider representations as different as, say, sets of rules and neural networks. But in fact propositional rules are readily encoded as neural networks, and similar relationships hold between other representations. All learners essentially work by grouping nearby examples into the same class; the key difference is in the meaning of \u201cnearby.\u201d With non-uniformly distributed data, learners can produce widely different frontiers while still making the same predictions in the regions that matter (those with a substantial number of training examples, and therefore also where most text examples are likely to appear). This also helps explain why powerful learns can be unstable but still accurate. As a rule, it pays to try the simplest learners first (e.g., naive Bayes before logistic regression, k-nearest neighbor before support vector machines). More sophisticated learners are seductive, but they are usually harder to use, because they have more knobs you need to turn to get good results, and because their internals are more opaque). Learners can be divided into 2 major types: those whose representation has a fixed size, like linear classifiers, and those whose representation can grow with the data, like decision trees. Fixed-size learners can only take advantage of so much data. Variable-size learners can in principle learn any function given sufficient data, but in practice they may not, because of limitations of the algorithm or computational cost. Also, because of the curse of dimensionality, no existing amount of data may be enough. For these reasons, clever algorithms \u2014 those that make the most of the data and computing resources available \u2014 often pay off in the end, provided you\u2019re willing to put in the effort. There is no sharp frontier between designing learners and learning classifiers; rather, any given piece of knowledge could be encoded in the learner or learned from data. So machine learning projects often wind up having a significant component of learner design, and practitioners need to have some expertise in it. 9 \u2014 Learn Many Models, Not Just One In the early days of machine learning, everyone had their favorite learner, together with some a priori reasons to believe in its superiority. Most effort went into trying many variations of it and selecting the best one. Then systematic empirical comparisons showed that the best learner varies from application to application, and systems containing many different learners started to appear. Effort now went into trying many variations of many learners, and still selecting just the best one. But then researchers noticed that, if instead of selecting the best variation found, we combine many variations, the results are better \u2014 often much better \u2014 and at little extra effort for the user. Creating such model ensembles is now standard. In the simplest technique, called bagging, we simply generate random variations of the training set by resampling, learn a classifier on each, and combine the results by voting. This works because it greatly reduces variance while only slightly increasing bias. In boosting, training examples have weights, and these are varied so that each new classifier focuses on the examples the previous ones tended to get wrong. In stacking, the outputs of individual classifiers become the inputs of a \u201chigher-level\u201d learner that figures out how best to combine them. Many other techniques exist, and the trend is toward larger and larger ensembles. In the Netflix prize, teams from all over the world competed to build the best video recommender system. As the competition progressed, teams found that they obtained the best results by combining their learners with other teams\u2019, and merged into larger and larger teams. The winner and runner-up were both stacked ensembles of over 100 learners, and combining the two ensembles further improved the results. Doubtless we will see even larger ones in the future. Occam\u2019s razor famously states that entities should not be multiplied beyond necessity. In machine learning, this is often taken to mean that, given two classifiers with the same training error, the simpler of the two will likely have the lowest test error. Purported proofs of this claim appear regularly in the literature, but in fact there are many counter-examples to it, and the \u201cno free lunch\u201d theorems imply it cannot be true.\n\nWe saw one counterexample in the previous section: model ensembles. The generalization error of a boosted ensemble continues to improve by adding classifiers even after the training error has reached zero. Thus, contrary to intuition, there is no necessary connection between the number of parameters of a model and its tendency to overfit. A more sophisticated view instead equates complexity with the size of the hypothesis space, on the basis that smaller spaces allow hypotheses to be represented by shorter codes. Bounds like the one in the section on theoretical guarantees above might then be viewed as implying that shorter hypotheses generalize better. This can be further refined by assigning shorter codes to the hypothesis in the space that we have some a priori preference for. But viewing this as \u201cproof\u201d of a tradeoff between accuracy and simplicity is circular reasoning: we made the hypotheses we prefer simpler by design, and if they are accurate it\u2019s because our preferences are accurate, not because the hypotheses are \u201csimple\u201d in the representation we chose. Essentially all representations used in variable-size learners have associated theorems of the form \u201cEvery function can be represented, or approximated arbitrarily closely, using this representation.\u201d Reassured by this, fans of the representation often proceed to ignore all others. However, just because a function can be represented does not mean it can be learned. For example, standard decision tree learners cannot learn trees with more leaves than there are training examples. In continuous spaces, representing even simple functions using a fixed set of primitives often requires an infinite number of components. Further, if the hypothesis space has many local optima of the evaluation function, as is often the case, the learner may not find the true function even if it is representable. Given finite data, time and memory, standard learners can learn only a tiny subset of all possible functions, and these subsets are different for learners with different representations. Therefore the key question is not \u201cCan it be represented?\u201d, to which the answer is often trivial, but \u201cCan it be learned?\u201d And it pays to try different learners (and possibly combine them). The point that correlation does not imply causation is made so often that it is perhaps not worth belaboring. But, even though learners of the kind we have been discussing can only learn correlations, their results are often treated as representing causal relations. Isn\u2019t this wrong? if so, then why do people do it? More often than not, the goal of learning predictive models is to use them as guides to action. If we find that beer and diapers are often bought together at the supermarket, then perhaps putting beer next to the diaper section will increase sales. But short of actually doing the experiment it\u2019s difficult to tell. Machine learning is usually applied to observational data, where the predictive variables are not under control of the learner, as opposed to experimental data, where they are. Some learning algorithms can potentially extract causal information from observational data but their applicability is rather restricted. On the other hand, correlation is a sign of a potential causal connection, and we can use it as a guide to further investigation. Like any discipline, machine learning has a lot of \u201cfolk wisdom\u201d that can be hard to come by, but is crucial for success. Professor Domingos\u2019 paper summarized some of the most salient items that you need to know."
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/the-5-step-process-to-get-what-you-want-out-of-life-497b9679dd95?source=user_profile---------19----------------",
        "title": "The 5-Step Process to Get What You Want Out of Life",
        "text": "Recently, I finished reading Ray Dalio\u2019s \u201cPrinciples\u201d \u2014 one of the most recommended business books of 2017. One of the world\u2019s most successful investors and entrepreneurs, Ray shares the unconventional principles that he\u2019s developed, refined, and used over the past 40 years to create unique results in both life and business \u2014 and which any person or organization can adopt to help achieve their goals.\n\nIn 1975, Ray Dalio founded an investment firm, Bridgewater Associates, out of his two-bedroom apartment in New York City. 40 years later, Bridgewater has made more money for its clients than any other hedge fund in history and grown into the fifth most important private company in the United States, according to Fortune magazine. Ray himself has been named to Time magazine\u2019s list of the 100 most influential people in the world. Along the way, he discovered a set of unique principles that have led to Bridgewater\u2019s exceptionally effective culture, which he describes as \u201can idea meritocracy that strives to achieve meaningful work and meaningful relationships through radical transparency.\u201d It is these principles, and not anything special about him \u2014 who grew up an ordinary kid in a middle-class Long Island neighborhood \u2014 that he believes are the reason behind his success.\n\nIn Principles, he shares what he\u2019s learned over the course of his remarkable career. He argues that life, management, economics, and investing can all be systemized into rules and understood like machines. In particular, there is one life principle that really caught my attention. Ray believes that the personal evolutionary process takes place in 5 distinct steps. If you can do those 5 things well, you will almost certainly be successful. I want to share them here as I believe anyone of us can benefit from this systematic process.\n\nFirst you have to pick what you are going after \u2014 your goals. Your choice of goals will determine your direction. As you move toward them, you will encounter problems. Some of those problems will bring you up against your own weaknesses. How you react to the pain that causes is up to you. If you want to reach your goals, you must be calm and analytical so that you can accurately diagnose your problems, design a plan that will get you around them, and do what\u2019s necessary to push through to results. Then you will look at the new results you achieve and go through the process again. To evolve quickly, you will have to do this fast and continuously, setting your goals successively higher.\n\nYou will need to do all 5 steps well to be successful and you must do them one at a time and in order. For example, when setting goals, just set goals. Don\u2019t think about how you will achieve them or what you will do if something goes wrong. When you are diagnosing problems, don\u2019t think about how you will solve them \u2014 just diagnose them. Blurring the steps leads to suboptimal outcomes because it interferes with uncovering the true problems. The process is iterative: Doing each step thoroughly will provide you with the information you need to move on to the next step and do it well.\n\nIt is essential that you approach this process in a clearheaded, rational way, looking down on yourself from a higher level and being ruthlessly honest. If your emotions are getting the better of you, step back and take time out until you can reflect clearly. If necessary, seek guidance from calm, thoughtful people."
    },
    {
        "url": "https://medium.com/@james_aka_yale/pinterests-visual-lens-how-computer-vision-explores-your-taste-47d591b42d7c?source=user_profile---------20----------------",
        "title": "Pinterest\u2019s Visual Lens: How computer vision explores your taste",
        "text": "When it comes to looking for something you want to try \u2014 a new salad recipe, a new classy dress, a new chair for your living room \u2014 you really need to see it first. Humans are visual creatures. We use our eyes to decide if something looks good, or if it matches our style. I\u2019m a huge fan of Pinterest, and particularly its Visual Lens. Why? It allows me to discover things that intrigue me, and allow me to go inside. When I spot something out in the world that looks interesting, but when I try to search for it online later, words fail me. I have this rich, colorful picture in my mind, but I can\u2019t translate into the words I need to find it. Pinterest\u2019s Visual Lens is a way to discover ideas without having to find the right words to describe them first.\n\nJust point Lens at a pair of shoes, then tap to see related styles or even ideas for what else to wear them with. Or try it on a table to find similar designs, and even other furniture from the same era. You can also use Lens with food. Just point it at cauliflower or potatoes to see what recipes come up. Patterns and colors can also lead you in fun, interesting or even just plain weird new directions. So how does Pinterest do such an amazing job of searching through vision and personalizing visual recommendations for its users? After two weeks of digging through the company\u2019s engineering blog and press exposure, I feel grateful to have finally gotten a glimpse behind the curtain. It turns out that the product is an instance of machine learning applications at Pinterest, which are widespread in a variety of business areas. Let\u2019s zoom out for a second to look at how machine learning is being used at Pinterest. An Overview of Machine Learning Usage at Pinterest As a visual discovery engine, Pinterest has many challenging problems that can be solved using machine learning techniques: What interests shall we recommend to a new user? How to generate an engaging home-feed? How do pins relate to each other? What interests does a pin belong to? The critical moment happened in January 2015, when Pinterest acquired Kosei \u2014 a machine learning startup with expertise in recommender systems. Since then, machine learning has been used across Pinterest in multiple areas: from the Discovery team that provides recommendations, related content, and predicts the likelihood that a person will pin content; to the Growth team that uses intelligence models to determine which emails to send and prevent churn; from the Monetization team that does ad performance and relevance prediction; to the Data team that builds out a real-time distributed system for machine learning with Spark. Let\u2019s dig a little bit deeper into how Pinterest engineers are leveraging machine learning to keep the website\u2019s 175 million+ users pinning and sharing: Identifying Visual Similarities: Machine learning can not only determine the subject of an image, it can also identify visual patterns and match them to other photos. Pinterest is using this technology to process 150 million image searches per month, helping users find content that looks like pictures they\u2019ve already pinned. Categorizing and Curating: If a user pins a mid-century dining-room table, the platform can now offer suggestions of other objects from the same era. The key? Metadata, such as the names of pinboards and websites where images have been posted, helps the platform understand what photos represent. Predicting Engagement: While many platforms prioritize content from a user\u2019s friends and contacts, Pinterest pays more attention to an individual\u2019s tastes and habits \u2014 what they\u2019ve pinned and when \u2014 enabling the site to surface more personalized recommendations. Prioritizing Local Taste: Pinterest is an increasingly global platform, with more than half of its users based outside the U.S. Its recommendation engine has learned to suggest popular content from users\u2019 local region in their native language. Going Beyond Images: Analyzing what\u2019s in a photo is a big factor in the site\u2019s recommendations, but it doesn\u2019t offer the whole story. Pinterest also looks at captions from previously pinned content and which items get pinned to the same virtual boards. That allows Pinterest to, say, link a particular dress to the pair of shoes frequently pinned alongside it, even if they look nothing alike. Pinterest Lens is a product that is part of the effort to identify visual similarities, along with a host of other engineering works. They all utilize machine learning algorithms and technologies under an ever-growing field called computer vision, which I am going to explain in depth below. A Brief History of Computer Vision at Pinterest Computer vision is a field of computer science and subfield of machine learning that works on enabling computers to see, identify and process images in the same way that human vision does, and then provide the appropriate output. It is like imparting human intelligence and instincts to a computer. Pinterest uses computer vision heavily to power their visual discovery products. Pinterest set its sights on visual search in 2014. That year, the company acquired VisualGraph, an image-recognition startup, and established its computer vision team with a small group of engineers and began to show its work. In 2015, it launched visual search, a way to search for ideas without text queries. For the first time, visual search gave people a way to get results even when they can\u2019t find the right words to describe what they\u2019re looking for. In summer 2016, visual search evolved as Pinterest rolled out object detection, which finds all the objects in a pin\u2019s image in real-time and serves related results. Since then, visual search has become one of its most-used features, with hundreds of millions of visual searches every month, and billions of objects detected. Early 2017, it introduced 3 new products on top of visual discovery infrastructure: Pinterest Lens is a way to discover ideas with a phone\u2019s camera inspired by what users see in the world around them. Shop The Look is a way to shop and buy products users see inside Pins. Instant Ideas is a way to transform users\u2019 home feed with similar ideas in just a tap. Most recently about 2 months back, it announced a couple more ways to find products and ideas for users: Lens Your Look is a new way to find outfit ideas inspired by your wardrobe and the next big steps for Pinterest Lens. Responsive Visual Search is a seamless and immersive way to search images through zooming into pin objects. Pinterest Pincodes, in which you just pull out the Pinterest camera and scan any Pincode to see curated ideas on Pinterest inspired by what you\u2019re looking at in the real world. Let\u2019s dig deeper into the computer vision models that Pinterest engineers employ for their visual discovery work behind Pinterest Lens. Lens combines Pinterest\u2019s understanding of images and objects with its discovery technologies to offer Pinners a diverse set of results. For example, if you take a picture of a blueberry, Lens doesn\u2019t just return blueberries: it also gives you more results such as recipes for blueberry scones and smoothies, beauty ideas like detox scrubs or tips for growing your own blueberry bush. To do this, Lens\u2019 overall architecture is separated into two logical components. The first component is a query understanding layer where Pinterest derives information regarding the given input image. Here Pinterest computes visual features such as detecting objects, computing salient colors and detecting lighting and image quality conditions. Using the visual features, it also computes semantic features such as annotations and category. The second component is Pinterest\u2019s blender, as the results Lens returns come from multiple sources. Pinterest uses visual search technology to return visually similar results, object search technology to return scenes or projects with visually similar objects and image search which uses the derived annotations to return personalized text search results that are semantically (not visually) relevant to the input image. It\u2019s the job of the blender to dynamically change blending ratios and result sources based on the information derived in the query understanding layer.\n\nAs shown above, Lens results aren\u2019t strictly visually similar, they come from multiple sources, some of which are only semantically relevant to the input image. By giving Pinners results beyond visually similar, Lens is a new type of visual discovery tool that bridges real-world camera images to the Pinterest taste graph. Let\u2019s go ahead and dissect that blender component of Lens, which include Image, Object, and Visual Search. Pinterest\u2019s Image Search technology dates back in 2015 when the company shared a white paper that details its system architecture and insights from experiments to build a scalable machine vision pipeline. Pinterest conducted a comprehensive set of experiments using a combination of benchmark datasets and A/B testing on two Pinterest applications, Related Pins and an experiment with similar looks. In particular, the experiment with similar looks allowed Pinterest to show visually similar Pin recommendations based on specific objects in a Pin\u2019s image. It experimented with different ways to use surface object recognition that would enable Pinner to click into the objects. Then it used object recognition to detect products such as bags, shoes and skirts from a Pin\u2019s image. From these detected objects, it extracted visual features to generate product recommendations (\u201csimilar looks\u201d). In the initial experiment, a Pinner would discover recommendations if there was a red dot on the object in the Pin. Clicking on the red dot loads a feed of Pins featuring visually similar objects. Visual search was improved dramatically when Pinterest introduced automatic object detection for the most popular categories on Pinterest in 2016, so people can visually search for products within a Pin\u2019s image.\n\nSince an image can contain dozens of objects, Pinterest\u2019s motivation was to make it as simple as possible to start a discovery experience from any of them. In the same way auto-complete improves the experience of text search, automatic object detection makes visual search a more seamless experience. Object detection in visual search also enables new features, like object-to-object matching. For example, say you spot a coffee table you love either on Pinterest or at a friend\u2019s house, soon you\u2019ll be able to see how it would look in many different home settings. Pinterest\u2019s first challenge in building automatic object detection was collecting labeled bounding boxes for regions of interest in images as our training data. Since launch, it has processed nearly 1 billion image crops (visual searches). By aggregating this activity across the millions of images with the highest engagement, it learns which objects Pinners are interested in. It aggregates annotations of visually similar results to each crop and assigns a weak label across hundreds of object categories. An example of how this looks is shown in the heat map visualization below, where two clusters of user crops are formed, one around the \u201cscarf\u201d annotation, and another around the \u201cbag\u201d annotation. Since Pinterest\u2019s visual search engine can use any image as a query \u2014 including unseen content from the web and even your camera \u2014 detection must happen in real-time, in a fraction of a second. One of the most widely used detection models Pinterest has experimented with extensively is Faster R-CNN, which uses a deep network to detect objects within images in two major steps. First, it identifies regions of an image that are likely to contain objects of interest by running a fully convolutional network over the input image to produce a feature map. For each location on the feature map, the network considers a fixed set of regions, varying in size and aspect ratio, and uses a binary softmax classifier to determine how likely each of these regions is to contain an object of interest. If a promising region is found, the network also outputs adjustments to this region so that it better frames the objects. Once the network has found regions of interest, it examines the most promising ones and attempts to either identify each as a particular category of object or discards it if no objects are found. For each candidate region, the network performs spatial pooling over the corresponding portion of a convolutional feature map, thereby producing a feature vector with a fixed size independent of the size of the region. This pooled feature is then used as the input to a detection network, which uses a softmax classifier to identify each region as either background or one of our object categories. If an object is detected, the network once again outputs adjustments to the region boundaries to further refine detection quality. Finally, a round of non-maximum suppression (NMS) is performed over the detections to filter out any duplicate detections, and the results are presented to the user. Traditionally, visual search systems have treated whole images as the unit. These systems index global image representations to return images similar holistically to the given input image. With better image representations as a result of advancements in deep learning, visual search systems have reached an unprecedented level of accuracy. However, Pinterest wanted to push the bounds of visual search technology to go beyond the whole image as the unit. By utilizing its corpus of billions of objects, combined with its real-time object detector, Pinterest can understand images on a more fine-grained level. Now, it knows both the location and the semantic meaning of billions of objects in its image corpus. Object search is a visual search system that treats objects as the unit. Given an input image, Pinterest finds the most visually similar objects in billions of images in a fraction of a second, map those objects to the original image and return scenes containing the similar objects. The Future of Visual Discovery at Pinterest In a world where everyone has a camera in her pocket, many experts believe that visual search \u2014 taking photos instead of searching via text queries \u2014 will become the de facto way we look up information. Pinterest is sitting on what might be the cleanest, biggest data set in the world to train computers to see images\u2013the equivalent of a small nation hiding a nuclear armament. That\u2019s billions of photos of furniture, food, and clothing, that have been hand-labeled by Pinterest\u2019s own users for years. At Pinterest, users come to casually window shop a better life, starting with remarkably unspecific queries like \u201cdinner ideas\u201d or \u201cfashion\u201d they often might search again and again, week after week. As a result of both this behavior and the site\u2019s gridded layout of photo pins, Pinterest can build visual search into its platform, not to offer one perfect answer, but an imperfect collection of inspiration."
    },
    {
        "url": "https://medium.com/@james_aka_yale/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies-7228aa8ef541?source=user_profile---------21----------------",
        "title": "A Tour of The Top 10 Algorithms for Machine Learning Newbies",
        "text": "In machine learning, there\u2019s something called the \u201cNo Free Lunch\u201d theorem. In a nutshell, it states that no one algorithm works best for every problem, and it\u2019s especially relevant for supervised learning (i.e. predictive modeling).\n\nFor example, you can\u2019t say that neural networks are always better than decision trees or vice-versa. There are many factors at play, such as the size and structure of your dataset.\n\nAs a result, you should try many different algorithms for your problem, while using a hold-out \u201ctest set\u201d of data to evaluate performance and select the winner.\n\nOf course, the algorithms you try must be appropriate for your problem, which is where picking the right machine learning task comes in. As an analogy, if you need to clean your house, you might use a vacuum, a broom, or a mop, but you wouldn\u2019t bust out a shovel and start digging.\n\nHowever, there is a common principle that underlies all supervised machine learning algorithms for predictive modeling.\n\nThis is a general learning task where we would like to make predictions in the future (Y) given new examples of input variables (X). We don\u2019t know what the function (f) looks like or its form. If we did, we would use it directly and we would not need to learn it from data using machine learning algorithms.\n\nThe most common type of machine learning is to learn the mapping Y = f(X) to make predictions of Y for new X. This is called predictive modeling or predictive analytics and our goal is to make the most accurate predictions possible.\n\nFor machine learning newbies who are eager to understand the basic of machine learning, here is a quick tour on the top 10 machine learning algorithms used by data scientists.\n\nLinear regression is perhaps one of the most well-known and well-understood algorithms in statistics and machine learning.\n\nPredictive modeling is primarily concerned with minimizing the error of a model or making the most accurate predictions possible, at the expense of explainability. We will borrow, reuse and steal algorithms from many different fields, including statistics and use them towards these ends.\n\nThe representation of linear regression is an equation that describes a line that best fits the relationship between the input variables (x) and the output variables (y), by finding specific weightings for the input variables called coefficients (B).\n\nWe will predict y given the input x and the goal of the linear regression learning algorithm is to find the values for the coefficients B0 and B1.\n\nDifferent techniques can be used to learn the linear regression model from data, such as a linear algebra solution for ordinary least squares and gradient descent optimization.\n\nLinear regression has been around for more than 200 years and has been extensively studied. Some good rules of thumb when using this technique are to remove variables that are very similar (correlated) and to remove noise from your data, if possible. It is a fast and simple technique and good first algorithm to try.\n\nLogistic regression is another technique borrowed by machine learning from the field of statistics. It is the go-to method for binary classification problems (problems with two class values).\n\nLogistic regression is like linear regression in that the goal is to find the values for the coefficients that weight each input variable. Unlike linear regression, the prediction for the output is transformed using a non-linear function called the logistic function.\n\nThe logistic function looks like a big S and will transform any value into the range 0 to 1. This is useful because we can apply a rule to the output of the logistic function to snap values to 0 and 1 (e.g. IF less than 0.5 then output 1) and predict a class value.\n\nBecause of the way that the model is learned, the predictions made by logistic regression can also be used as the probability of a given data instance belonging to class 0 or class 1. This can be useful for problems where you need to give more rationale for a prediction.\n\nLike linear regression, logistic regression does work better when you remove attributes that are unrelated to the output variable as well as attributes that are very similar (correlated) to each other. It\u2019s a fast model to learn and effective on binary classification problems.\n\nLogistic Regression is a classification algorithm traditionally limited to only two-class classification problems. If you have more than two classes then the Linear Discriminant Analysis algorithm is the preferred linear classification technique.\n\nThe representation of LDA is pretty straight forward. It consists of statistical properties of your data, calculated for each class. For a single input variable this includes:\n\nPredictions are made by calculating a discriminate value for each class and making a prediction for the class with the largest value. The technique assumes that the data has a Gaussian distribution (bell curve), so it is a good idea to remove outliers from your data before hand. It\u2019s a simple and powerful method for classification predictive modeling problems.\n\nDecision Trees are an important type of algorithm for predictive modeling machinelearning.\n\nThe representation of the decision tree model is a binary tree. This is your binary tree from algorithms and data structures, nothing too fancy. Each node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n\nThe leaf nodes of the tree contain an output variable (y) which is used to make a prediction. Predictions are made by walking the splits of the tree until arriving at a leaf node and output the class value at that leaf node.\n\nTrees are fast to learn and very fast for making predictions. They are also often accurate for a broad range of problems and do not require any special preparation for your data.\n\nNaive Bayes is a simple but surprisingly powerful algorithm for predictive modeling.\n\nThe model is comprised of two types of probabilities that can be calculated directly from your training data: 1) The probability of each class; and 2) The conditional probability for each class given each x value. Once calculated, the probability model can be used to make predictions for new data using Bayes Theorem. When your data is real-valued it is common to assume a Gaussian distribution (bell curve) so that you can easily estimate these probabilities.\n\nNaive Bayes is called naive because it assumes that each input variable is independent. This is a strong assumption and unrealistic for real data, nevertheless, the technique is very effective on a large range of complex problems.\n\nThe KNN algorithm is very simple and very effective. The model representation for KNN is the entire training dataset. Simple right?\n\nPredictions are made for a new data point by searching through the entire training set for the K most similar instances (the neighbors) and summarizing the output variable for those K instances. For regression problems, this might be the mean output variable, for classification problems this might be the mode (or most common) class value.\n\nThe trick is in how to determine the similarity between the data instances. The simplest technique if your attributes are all of the same scale (all in inches for example) is to use the Euclidean distance, a number you can calculate directly based on the differences between each input variable.\n\nKNN can require a lot of memory or space to store all of the data, but only performs a calculation (or learn) when a prediction is needed, just in time. You can also update and curate your training instances over time to keep predictions accurate.\n\nThe idea of distance or closeness can break down in very high dimensions (lots of input variables) which can negatively affect the performance of the algorithm on your problem. This is called the curse of dimensionality. It suggests you only use those input variables that are most relevant to predicting the output variable.\n\nA downside of K-Nearest Neighbors is that you need to hang on to your entire training dataset. The Learning Vector Quantization algorithm (or LVQ for short) is an artificial neural network algorithm that allows you to choose how many training instances to hang onto and learns exactly what those instances should look like.\n\nThe representation for LVQ is a collection of codebook vectors. These are selected randomly in the beginning and adapted to best summarize the training dataset over a number of iterations of the learning algorithm. After learned, the codebook vectors can be used to make predictions just like K-Nearest Neighbors. The most similar neighbor (best matching codebook vector) is found by calculating the distance between each codebook vector and the new data instance. The class value or (real value in the case of regression) for the best matching unit is then returned as the prediction. Best results are achieved if you rescale your data to have the same range, such as between 0 and 1.\n\nIf you discover that KNN gives good results on your dataset try using LVQ to reduce the memory requirements of storing the entire training dataset.\n\nSupport Vector Machines are perhaps one of the most popular and talked about machine learning algorithms.\n\nA hyperplane is a line that splits the input variable space. In SVM, a hyperplane is selected to best separate the points in the input variable space by their class, either class 0 or class 1. In two-dimensions, you can visualize this as a line and let\u2019s assume that all of our input points can be completely separated by this line. The SVM learning algorithm finds the coefficients that results in the best separation of the classes by the hyperplane.\n\nThe distance between the hyperplane and the closest data points is referred to as the margin. The best or optimal hyperplane that can separate the two classes is the line that has the largest margin. Only these points are relevant in defining the hyperplane and in the construction of the classifier. These points are called the support vectors. They support or define the hyperplane. In practice, an optimization algorithm is used to find the values for the coefficients that maximizes the margin.\n\nSVM might be one of the most powerful out-of-the-box classifiers and worth trying on your dataset.\n\nRandom Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.\n\nThe bootstrap is a powerful statistical method for estimating a quantity from a data sample. Such as a mean. You take lots of samples of your data, calculate the mean, then average all of your mean values to give you a better estimation of the true mean value.\n\nIn bagging, the same approach is used, but instead for estimating entire statistical models, most commonly decision trees. Multiple samples of your training data are taken then models are constructed for each data sample. When you need to make a prediction for new data, each model makes a prediction and the predictions are averaged to give a better estimate of the true output value.\n\nRandom forest is a tweak on this approach where decision trees are created so that rather than selecting optimal split points, suboptimal splits are made by introducing randomness.\n\nThe models created for each sample of the data are therefore more different than they otherwise would be, but still accurate in their unique and different ways. Combining their predictions results in a better estimate of the true underlying output value.\n\nIf you get good results with an algorithm with high variance (like decision trees), you can often get better results by bagging that algorithm.\n\nBoosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers. This is done by building a model from the training data, then creating a second model that attempts to correct the errors from the first model. Models are added until the training set is predicted perfectly or a maximum number of models are added.\n\nAdaBoost was the first really successful boosting algorithm developed for binary classification. It is the best starting point for understanding boosting. Modern boosting methods build on AdaBoost, most notably stochastic gradient boosting machines.\n\nAdaBoost is used with short decision trees. After the first tree is created, the performance of the tree on each training instance is used to weight how much attention the next tree that is created should pay attention to each training instance. Training data that is hard to predict is given more weight, whereas easy to predict instances are given less weight. Models are created sequentially one after the other, each updating the weights on the training instances that affect the learning performed by the next tree in the sequence. After all the trees are built, predictions are made for new data, and the performance of each tree is weighted by how accurate it was on training data.\n\nBecause so much attention is put on correcting mistakes by the algorithm it is important that you have clean data with outliers removed.\n\nA typical question asked by a beginner, when facing a wide variety of machine learning algorithms, is \u201cwhich algorithm should I use?\u201d The answer to the question varies depending on many factors, including: (1) The size, quality, and nature of data; (2) The available computational time; (3) The urgency of the task; and (4) What you want to do with the data.\n\nEven an experienced data scientist cannot tell which algorithm will perform the best before trying different algorithms. Although there are many other Machine Learning algorithms, these are the most popular ones. If you\u2019re a newbie to Machine Learning, these would be a good starting point to learn."
    },
    {
        "url": "https://codeburst.io/how-operating-systems-work-10-concepts-you-should-know-as-a-developer-8d63bb38331f?source=user_profile---------22----------------",
        "title": "How Operating Systems Work: 10 Concepts you Should Know as a Developer",
        "text": "How Operating Systems Work: 10 Concepts you Should Know as a Developer\n\nDo you speak binary? Can you comprehend machine code? If I gave you a sheet full of 1s and 0s, could you tell me what it means/does? If you were to go to a country you\u2019ve never been to that speaks a language you\u2019ve never heard, or maybe you\u2019ve heard of it but don\u2019t actually speak it, what would you need while there to help you communicate with the locals? You would need a translator. Your operating system functions as that translator in your PC. It converts those 1s and 0s, yes/no, and on/off values into a readable language that you will understand. It does all of this in a streamlined graphical user interface, or GUI, that you can move around with a mouse, click things, move them, and see them happening before your eyes. While the extent and depth of knowledge can be questioned, knowing more than the fundamentals can be critical to how well your program runs and even to its structure and flow. Why? When you write a program and it runs too slowly, but you see nothing wrong with your code, where else will you look for a solution? How will you be able to debug the problem if you don\u2019t know how the operating system works? Are you accessing too many files? Running out of memory and swap is in high usage? But you don\u2019t even know what swap is! Or is I/O blocking? And you want to communicate with another machine. How do you do that locally or over the internet? And what\u2019s the difference? Why do some programmers prefer one OS over another? In an attempt to be a serious developer, I recently took Georgia Tech\u2019s course \u201cIntroduction to Operating Systems.\u201d It teaches the basic OS abstractions, mechanisms, and their implementations. The core of the course contains concurrent programming (threads and synchronization), inter-process communication, and an introduction to distributed OSs. I want to use this post to share my takeaways from the course, that is the 10 critical operating system concepts that you need to learn if you want to get good at developing software. What is an Operating System? But first, let\u2019s define what an operating system is. An Operating System (OS) is a collection of software that manages computer hardware and provides services for programs. Specifically, it hides hardware complexity, manages computational resources, and provides isolation and protection. Most importantly, it directly has privilege access to the underlying hardware. Major components of an OS are the file system, scheduler, and device driver. You probably have used both Desktop (Windows, Mac, Linux) and Embedded (Android, iOS) operating systems before. There are three key elements of an operating system, which are: (1) Abstractions (process, thread, file, socket, memory), (2) Mechanisms (create, schedule, open, write, allocate), and (3) Policies (LRU, EDF). There are two operating system design principles, which are: (1) Separation of mechanism and policy by implementing flexible mechanisms to support policies, and (2) Optimization for common case: Where will the OS be used? What will the user want to execute on that machine? What are the workload requirements? There are three types of Operating Systems commonly used nowadays. The first is Monolithic OS, where the entire OS is working in kernel space and is alone in supervisor mode. The second is Modular OS, in which some part of the system core will be located in independent files called modules that can be added to the system at run time. And the third is Micro OS, where the kernel is broken down into separate processes, known as servers. Some of the servers run in kernel space and some run in user-space. Now let\u2019s get into those major concepts you need to understand in more detail. A process is basically a program in execution. The execution of a process must progress in a sequential fashion. To put it in simple terms, we write our computer programs in a text file, and when we execute this program, it becomes a process which performs all the tasks mentioned in the program. When a program is loaded into the memory and it becomes a process, it can be divided into four sections \u2500 stack, heap, text, and data. The following image shows a simplified layout of a process inside main memory Stack: The process Stack contains the temporary data, such as method/function parameters, return address, and local variables. Heap: This is dynamically allocated memory to a process during its run time. Text: This includes the current activity represented by the value of Program Counter and the contents of the processor\u2019s registers. Data: This section contains the global and static variables. When a process executes, it passes through different states. These stages may differ in different operating systems, and the names of these states are also not standardized. In general, a process can have one of the following five states at a time: Start: The initial state when a process is first started/created. Ready: The process is waiting to be assigned to a processor. Ready processes are waiting to have the processor allocated to them by the operating system so that they can run. A process may come into this state after the Start state, or while running it by but getting interrupted by the scheduler to assign CPU to some other process. Running: Once the process has been assigned to a processor by the OS scheduler, the process state is set to running and the processor executes its instructions. Waiting: the process moves into the waiting state if it needs to wait for a resource, such as waiting for user input, or waiting for a file to become available. Terminated or Exit: Once the process finishes its execution, or it is terminated by the operating system, it is moved to the terminated state where it waits to be removed from main memory. A Process Control Block is a data structure maintained by the Operating System for every process. The PCB is identified by an integer process ID (PID). A PCB keeps all the information needed to keep track of a process as listed below: Process State: The current state of the process \u2014 whether it is ready, running, waiting, or whatever. Process Privileges: This is required to allow/disallow access to system resources. Process ID: Unique identification for each of the processes in the operating system. Program Counter: Program Counter is a pointer to the address of the next instruction to be executed for this process. CPU Registers: Various CPU registers where processes need to be stored for execution for running state. CPU Scheduling Information: Process priority and other scheduling information which is required to schedule the process. Memory Management Information: This includes the information of page table, memory limits, and segment table, depending on the memory used by the operating system. Accounting Information: This includes the amount of CPU used for process execution, time limits, execution ID, and so on. IO Status Information: This includes a list of I/O devices allocated to the process. A thread is a flow of execution through the process code. It has its own program counter that keeps track of which instruction to execute next. It also has system registers which hold its current working variables, and a stack which contains the execution history. A thread shares with its peer threads various information like code segment, data segment, and open files. When one thread alters a code segment memory item, all other threads see that. A thread is also called a lightweight process. Threads provide a way to improve application performance through parallelism. Threads represent a software approach to improving the performance of operating systems by reducing the overhead. A thread is equivalent to a classical process. Each thread belongs to exactly one process, and no thread can exist outside a process. Each thread represents a separate flow of control. Threads have been successfully used in implementing network servers and web servers. They also provide a suitable foundation for parallel execution of applications on shared memory multiprocessors. Using them provides concurrency within a process. It is more economical to create and context switch threads. Threads allow utilization of multiprocessor architectures to a greater scale and efficiency. Threads are implemented in the following two ways: In this case, the thread management kernel is not aware of the existence of threads. The thread library contains code for creating and destroying threads, for passing messages and data between threads, for scheduling thread execution, and for saving and restoring thread contexts. The application starts with a single thread. User level thread can run on any operating system. Scheduling can be application-specific in the user level thread. User level threads are fast to create and manage. In a typical operating system, most system calls are blocking. Multithreaded application cannot take advantage of multiprocessing. In this case, thread management is done by the Kernel. There is no thread management code in the application area. Kernel threads are supported directly by the operating system. Any application can be programmed to be multithreaded. All of the threads within an application are supported within a single process. The Kernel maintains context information for the process as a whole and for individuals threads within the process. Scheduling by the Kernel is done on a thread basis. The Kernel performs thread creation, scheduling, and management in Kernel space. Kernel threads are generally slower to create and manage than the user threads. The Kernel can simultaneously schedule multiple threads from the same process on multiple processes. If one thread in a process is blocked, the Kernel can schedule another thread of the same process. Kernel routines themselves can be multithreaded. Kernel threads are generally slower to create and manage than the user threads. Transfer of control from one thread to another within the same process requires a mode switch to the Kernel. The process of scheduling is the responsibility of the process manager that handles the removal of the running process from the CPU and the selection of another process on the basis of a particular strategy. Process scheduling is an essential part of a Multiprogramming operating system. These operating systems allow more than one process to be loaded into the executable memory at a time, and the loaded process shares the CPU using time multiplexing. The OS maintains all Process Control Blocks (PCBs) in Process Scheduling Queues. The OS maintains a separate queue for each of the process states, and PCBs of all processes in the same execution state are placed in the same queue. When the state of a process is changed, its PCB is unlinked from its current queue and moved to its new state queue. The Operating System maintains the following important process scheduling queues: Job queue: This queue keeps all the processes in the system. Ready queue: This queue keeps a set of all processes residing in the main memory, ready and waiting to execute. A new process is always put in this queue. Device queues: The processes which are blocked due to unavailability of an I/O device constitute this queue. The OS can use different policies to manage each queue (FIFO, Round Robin, Priority, etc.). The OS scheduler determines how to move processes between the ready and run queues which can only have one entry per processor core on the system. In the above diagram, it has been merged with the CPU. Running: When a new process is created, it enters into the system in the running state. Not Running: Processes that are not running are kept in queue, waiting for their turn to execute. Each entry in the queue is a pointer to a particular process. Queue is implemented by using a linked list. The use of dispatcher is as follows: when a process is interrupted, that process is transferred in the waiting queue. If the process has completed or aborted, the process is discarded. In either case, the dispatcher then selects a process from the queue to execute. A context switch is the mechanism that stores and restores the state or context of a CPU in the Process Control block. It allows a process execution to be resumed from the same point at a later time. Using this technique, a context switcher enables multiple processes to share a single CPU. Context switching is an essential feature of a multitasking operating system. When the scheduler switches the CPU from executing one process to another, the state from the current running process is stored into the process control block. After this, the state for the next process is loaded from its own PCB and used to set the PC, registers, etc. At that point, the second process can start executing. Context switches are computationally intensive, since register and memory state must be saved and restored. To avoid the amount of context switching time, some hardware systems employ two or more sets of processor registers. When the process is switched, the following information is stored for later use: Program Counter, Scheduling Information, Base and Limit Register Value, Currently Used Register, Changed State, I/O State Information, and Accounting Information. Memory management is the functionality of an operating system which handles or manages primary memory. It moves processes back and forth between the main memory and the disk during execution. Memory management keeps track of each and every memory location, regardless of whether it is allocated to some process or free. It checks how much memory is to be allocated to processes. It decides which process will get memory at what time. And it tracks whenever memory gets freed up or unallocated, and correspondingly updates the status.\n\nThe process address space is the set of logical addresses that a process references in its code. For example, when 32-bit addressing is in use, addresses can range from 0 to 0x7fffffff; that is, \u00b2\u00b3\u00b9 possible numbers, for a total theoretical size of 2 gigabytes. The operating system takes care of mapping the logical addresses to physical addresses at the time of memory allocation to the program. There are three types of addresses used in a program before and after memory is allocated: Symbolic addresses: The addresses used in a source code. The variable names, constants, and instruction labels are the basic elements of the symbolic address space. Relative addresses: At the time of compilation, a compiler converts symbolic addresses into relative addresses. Physical addresses: The loader generates these addresses at the time when a program is loaded into main memory. Virtual and physical addresses are the same in compile-time and load-time address binding schemes. Virtual and physical addresses differ in execution-time address-binding schemes. The set of all logical addresses generated by a program is referred to as a logical address space. The set of all physical addresses corresponding to these logical addresses is referred to as a physical address space. There are two types of processes: Independent and Cooperating. An independent process is not affected by the execution of other processes, while a cooperating process can be affected by other executing processes. You might think that those processes, which are running independently, would execute very efficiently. But in reality, there are many situations when a process\u2019 cooperative nature can be utilized for increasing computational speed, convenience, and modularity. Inter-process communication (IPC) is a mechanism which allows processes to communicate with each other and synchronize their actions. The communication between these processes can be seen as a method of cooperation between them. Processes can communicate with each other in two ways: Shared Memory and Message Parsing. Let\u2019s say there are two processes: the Producer and the Consumer. The producer produces some item and the Consumer consumes that item. The two processes shares a common space or memory location known as the \u201cbuffer,\u201d where the item produced by the Producer is stored and from where the Consumer consumes the item if needed. There are two versions of this problem: the first one is known as the unbounded buffer problem, in which the Producer can keep on producing items and there is no limit on the size of the buffer. The second one is known as the bounded buffer problem, in which the Producer can produce up to a certain number of items, and after that it starts waiting for the Consumer to consume them. In the bounded buffer problem, the Producer and the Consumer will share some common memory. Then the Producer will start producing items. If the total number of produced items is equal to the size of buffer, the Producer will wait until they\u2019re consumed by the Consumer. Similarly, the Consumer first checks for the availability of the item, and if no item is available, the Consumer will wait for the Producer to produce it. If there are items available, the Consumer will consume them. In this method, processes communicate with each other without using any kind of of shared memory. If two processes p1 and p2 want to communicate with each other, they proceed as follows: Establish a communication link (if a link already exists, no need to establish it again.) Start exchanging messages using basic primitives. We need at least two primitives: send(message, destination) or send(message) and receive(message, host) or receive(message) The message size can be fixed or variable. If it is a fixed size, it is easy for the OS designer but complicated for the programmer. If it is a variable size, then it is easy for the programmer but complicated for the OS designer. A standard message has two parts: a header and a body. The header is used for storing the Message type, destination id, source id, message length, and control information. The control information contains information like what to do if it runs out of buffer space, the sequence number, and its priority. Generally, the message is sent using the FIFO style. One of the important jobs of an Operating System is to manage various input/output (I/O) devices, including the mouse, keyboards, touch pad, disk drives, display adapters, USB devices, Bit-mapped screen, LED, Analog-to-digital converter, On/off switch, network connections, audio I/O, printers, and so on. An I/O system is required to take an application I/O request and send it to the physical device, then take whatever response comes back from the device and send it to the application. I/O devices can be divided into two categories: Block devices: A block device is one with which the driver communicates by sending entire blocks of data. For example, hard disks, USB cameras, Disk-On-Key, and so on. Character Devices: A character device is one with which the driver communicates by sending and receiving single characters (bytes, octets). For example, serial ports, parallel ports, sounds cards, and so on. The CPU must have a way to pass information to and from an I/O device. There are three approaches available to communicate with the CPU and Device. This uses CPU instructions that are specifically made for controlling I/O devices. These instructions typically allow data to be sent to an I/O device or be read from an I/O device. When using memory-mapped I/O, the same address space is shared by memory and I/O devices. The device is connected directly to certain main memory locations so that the I/O device can transfer block of data to/from the memory without going through the CPU. While using memory mapped I/O, the OS allocates buffer in the memory and informs the I/O device to use that buffer to send data to the CPU. The I/O device operates asynchronously with the CPU, and interrupts the CPU when finished. The advantage to this method is that every instruction which can access memory can be used to manipulate an I/O device. Memory-mapped I/O is used for most high-speed I/O devices like disks and communication interfaces. Slow devices like keyboards will generate an interruption to the main CPU after each byte is transferred. If a fast device, such as a disk, generated an interruption for each byte, the operating system would spend most of its time handling these interruptions. So a typical computer uses direct memory access (DMA) hardware to reduce this overhead. Direct Memory Access (DMA) means the CPU grants the I/O module authority to read from or write to memory without involvement. The DMA module itself controls the exchange of data between the main memory and the I/O device. The CPU is only involved at the beginning and end of the transfer and interrupted only after the entire block has been transferred. Direct Memory Access needs special hardware called a DMA controller (DMAC) that manages the data transfers and arbitrates access to the system bus. The controllers are programmed with source and destination pointers (where to read/write the data), counters to track the number of transferred bytes, and various settings. These include the I/O and memory types and the interruptions and states for the CPU cycles. Virtualization is technology that allows you to create multiple simulated environments or dedicated resources from a single, physical hardware system. Software called a hypervisor connects directly to that hardware and allows you to split one system into separate, distinct, and secure environments known as virtual machines (VMs). These VMs rely on the hypervisor\u2019s ability to separate the machine\u2019s resources from the hardware and distribute them appropriately. The original, physical machine equipped with the hypervisor is called the host, while the many VMs that use its resources are called guests. These guests treat computing resources \u2014 like CPU, memory, and storage \u2014 as a hangar of resources that can easily be relocated. Operators can control virtual instances of CPU, memory, storage, and other resources so that guests receive the resources they need when they need them. Ideally, all related VMs are managed through a single web-based virtualization management console, which speeds things up. Virtualization lets you dictate how much processing power, storage, and memory to give to VMs, and environments are better protected since VMs are separated from their supporting hardware and each other. Simply put, virtualization creates the environments and resources you need from underused hardware.\n\nHide data movement and provide a simpler abstraction for sharing data. Programmers don\u2019t need to worry about memory transfers between machines like when using the message passing model. Allows the passing of complex structures by reference, simplifying algorithm development for distributed applications. Takes advantage of \u201clocality of reference\u201d by moving the entire page containing the data referenced rather than just the piece of data. Cheaper to build than multiprocessor systems. Ideas can be implemented using normal hardware and do not require anything complex to connect the shared memory to the processors. Larger memory sizes are available to programs, by combining all physical memory of all nodes. This large memory will not incur disk latency due to swapping like in traditional distributed systems. Unlimited number of nodes can be used. Unlike multiprocessor systems where main memory is accessed via a common bus, thus limiting the size of the multiprocessor system. Programs written for shared memory multiprocessors can be run on DSM systems. There are two different ways that nodes can be informed of who owns what page: invalidation and broadcast. Invalidation is a method that invalidates a page when some process asks for write access to that page and becomes its new owner. This way the next time some other process tries to read or write to a copy of the page it thought it had, the page will not be available and the process will have to re-request access to that page. Broadcasting will automatically update all copies of a memory page when a process writes to it. This is also called write-update. This method is a lot less efficient more difficult to implement because a new value has to sent instead of an invalidation message. More and more, we are seeing technology moving to the cloud. It\u2019s not just a fad \u2014 the shift from traditional software models to the Internet has steadily gained momentum over the last 10 years. Looking ahead, the next decade of cloud computing promises new ways to collaborate everywhere, through mobile devices. So what is cloud computing? Essentially, cloud computing is a kind of outsourcing of computer programs. Using cloud computing, users are able to access software and applications from wherever they need, while it is being hosted by an outside party \u2014 in \u201cthe cloud.\u201d This means that they do not have to worry about things such as storage and power, they can simply enjoy the end result. Traditional business applications have always been very complicated and expensive. The amount and variety of hardware and software required to run them are daunting. You need a whole team of experts to install, configure, test, run, secure, and update them. When you multiply this effort across dozens or hundreds of apps, it isn\u2019t easy to see why the biggest companies with the best IT departments aren\u2019t getting the apps they need. Small and mid-sized businesses don\u2019t stand a chance.\n\nWith cloud computing, you eliminate those headaches that come with storing your own data, because you\u2019re not managing hardware and software \u2014 that becomes the responsibility of an experienced vendor like Salesforce and AWS. The shared infrastructure means it works like a utility: you only pay for what you need, upgrades are automatic, and scaling up or down is easy. Cloud-based apps can be up and running in days or weeks, and they cost less. With a cloud app, you just open a browser, log in, customize the app, and start using it. Businesses are running all kinds of apps in the cloud, like customer relationship management (CRM), HR, accounting, and much more. As cloud computing grows in popularity, thousands of companies are simply rebranding their non-cloud products and services as \u201ccloud computing.\u201d Always dig deeper when evaluating cloud offerings and keep in mind that if you have to buy and manage hardware and software, what you\u2019re looking at isn\u2019t really cloud computing but a false cloud. As a software engineer, you will be part of a larger body of computer science, which encompasses hardware, operating systems, networking, data management and mining, and many other disciplines. The more engineers in each of these disciplines understand about the other disciplines, the better they will be able to interact with those other disciplines efficiently. As the operating system is the \u201cbrain\u201d that manages input, processing, and output, all other disciplines interact with the operating system. An understanding of how the operating system works will provide valuable insight into how the other disciplines work, as your interaction with those disciplines is managed by the operating system. If you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://medium.com/@james_aka_yale/the-5-step-process-to-get-what-you-want-out-of-life-572b6aab0224?source=user_profile---------23----------------",
        "title": "The 5-Step Process to Get What You Want Out of Life",
        "text": "Recently, I finished reading Ray Dalio\u2019s \u201cPrinciples\u201d \u2014 one of the most recommended business books of 2017. One of the world\u2019s most successful investors and entrepreneurs, Ray shares the unconventional principles that he\u2019s developed, refined, and used over the past 40 years to create unique results in both life and business \u2014 and which any person or organization can adopt to help achieve their goals.\n\nIn 1975, Ray Dalio founded an investment firm, Bridgewater Associates, out of his two-bedroom apartment in New York City. 40 years later, Bridgewater has made more money for its clients than any other hedge fund in history and grown into the fifth most important private company in the United States, according to Fortune magazine. Ray himself has been named to Time magazine\u2019s list of the 100 most influential people in the world. Along the way, he discovered a set of unique principles that have led to Bridgewater\u2019s exceptionally effective culture, which he describes as \u201can idea meritocracy that strives to achieve meaningful work and meaningful relationships through radical transparency.\u201d It is these principles, and not anything special about him \u2014 who grew up an ordinary kid in a middle-class Long Island neighborhood \u2014 that he believes are the reason behind his success.\n\nIn Principles, he shares what he\u2019s learned over the course of his remarkable career. He argues that life, management, economics, and investing can all be systemized into rules and understood like machines. In particular, there is one life principle that really caught my attention. Ray believes that the personal evolutionary process takes place in 5 distinct steps. If you can do those 5 things well, you will almost certainly be successful. I want to share them here as I believe anyone of us can benefit from this systematic process.\n\nFirst you have to pick what you are going after \u2014 your goals. Your choice of goals will determine your direction. As you move toward them, you will encounter problems. Some of those problems will bring you up against your own weaknesses. How you react to the pain that causes is up to you. If you want to reach your goals, you must be calm and analytical so that you can accurately diagnose your problems, design a plan that will get you around them, and do what\u2019s necessary to push through to results. Then you will look at the new results you achieve and go through the process again. To evolve quickly, you will have to do this fast and continuously, setting your goals successively higher.\n\nYou will need to do all 5 steps well to be successful and you must do them one at a time and in order. For example, when setting goals, just set goals. Don\u2019t think about how you will achieve them or what you will do if something goes wrong. When you are diagnosing problems, don\u2019t think about how you will solve them \u2014 just diagnose them. Blurring the steps leads to suboptimal outcomes because it interferes with uncovering the true problems. The process is iterative: Doing each step thoroughly will provide you with the information you need to move on to the next step and do it well.\n\nIt is essential that you approach this process in a clearheaded, rational way, looking down on yourself from a higher level and being ruthlessly honest. If your emotions are getting the better of you, step back and take time out until you can reflect clearly. If necessary, seek guidance from calm, thoughtful people."
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/2017-annual-review-the-year-of-transition-764cb0523a59?source=user_profile---------24----------------",
        "title": "2017 Annual Review \u2014 The Year of Transition \u2013 Constraint Drives Creativity \u2013",
        "text": "Since 2014, I\u2019ve spent the better part of a week in late December planning my life for the next year. If interested, you can check my past reviews for 2016, 2015, and 2014. Overall, this is probably the best decision I\u2019ve made in terms of working towards multiple goals simultaneously. The idea is to create a road map for the year ahead \u2014 not a rigid daily schedule, but an overall outline of what matters to me and what I hope to achieve in the next year. To begin the process, I simply ask myself 2 questions and try to come up with at least 6\u20138 answers to each: 1 \u2014 What went well this year? 2 \u2014 What did not go well this year? The main part of the planning session focuses on the year to come, but before looking forward I spend at least one of the days reflecting back on the year that is ending. I can usually identify a number of answers for each question \u2014 successes and failures, times where I was happy or proud and other times where I knew I felt short. Next, I\u2019ll review all of the goals that I set the previous December, and write out the results. Did everything happened as I expected? Probably not, but it\u2019s interesting to compare results with expectations and see what overlaps and what diverges. This leads to the next, longer state of the planning process where I look ahead to the forthcoming year, carefully thinking about which projects I\u2019d like to pursue and which actions I need to take to ensure their success. What Went Well in 2017 The past 4 years spent at Denison University have been an interesting rollercoaster for me. I wholeheartedly enjoy all the academics, social activities, and relationships with peers and professors. Most important of all, I am extremely happy with my last semester in college. Academically, I took a couple of interesting classes in Environmental Psychology and Public Speaking, finished a hard Computer Science course in Data Structures, as well as explore my interest in Big Data & Data Mining with different independent study groups. Socially, I had a blast hanging out with my roommates and hosting parties for my fraternity. Every weekend, my room was filled with people having fun together and I have never felt more alive in all of my adulthood. Those are great times to be missed. To reflect on my whole college career, I would rate it an A-; as I have significantly expanded my intellectual capacity, made life-changing relationships with other people, and also left a presentable impact to the Denison community. There were definitely couple of things that didn\u2019t go as planned, but for the most part, I am really satisfied with my undergraduate experience. This is a major achievement for me this year. Fitness has always been in my need-to-improve bucket in the past couple of years. I finally have the available time to commit into this habit after finishing college. Since September, I have been consistently exercising every single day and experimenting with a variety of workout plans. I want to share my workout schedule here after so many iterations and learning from fitness gurus in the Internet: For Strength Training, I hit the gym every weekday morning and allocate each morning to a core muscle group. To be specific, I exercise my Chest & Abs on Monday, Back & Calves on Tuesday, Shoulder & Abs on Wednesday, Legs on Thursday, and General Upper Body & Abs on Friday. I focus mainly on compound exercises such as Bench Press, Deadlifts, and Squats as they generally work out the whole body instead of individual body muscles. Check out Ben Greenfield\u2019s workout plan if you are interested in digging into the specifics. For HIIT Workout (High Intensity Interval Training), I spend afternoon weekdays doing 15-minute Full Body Ripper and Furious Fat Burning routines as recommended by Brandon Carter. These are awesome activities that make you sweat and keep your body lean over time. For Cardiovascular activities, I bike for 30 minutes and swim on average of 500 meters daily. Especially for swimming, I follow the game-changer Total Immersion approach and have been able to swim effortlessly ever since getting started. In order to accompany this new fitness routine, I also research a ton on eating diet. One of the more popular diets I have been trying for a while now is Intermittent Fasting, which is scientifically proven to help lose fat, increase energy, and improve cognitive function. Basically, I only eat 2 meals a day, lunch around noon and dinner in the evening, while skipping breakfast most days with the help of coffee, tea and water. Personally, I feel stronger, leaner, and more explosive even though I eat less. Finally, I also try out taking cold shower as many research have shown that helps increase alertness, refine hair and skin, stimulate weight loss, speed up muscle soreness and recovery, and relieve stress altogether.\n\nWhat Did Not Go Well in 2017 I didn\u2019t achieve the biggest goal I set out for this year, which is getting a job in the US. I applied to many Software Engineering, Product Management, and Data Analytics positions in tech companies all over the US. Now it\u2019s a long and winding story stretching out 3 months of summer with hundreds of applications, phone and onsite interviews, coding challenges, take-home projects, networking events, coffee chats etc. To cut it short, I tried every single online job platform, every professional connection I have on LinkedIn, and every single combination of job-seeking strategies I have learned over the years. And I still failed. Lack of experience. Unqualified credentials. Do not reach the technical bar. Not hiring at the moment. Not sponsor visa. But I learned a ton. I realized what technical skills I need to be more qualified. I figured exactly what the next steps I need to take. And most importantly, I found inspiring stories of people who had gone through similar struggles like I did and later on succeeded in achieving their goals. No pain, no gain. This experience led me to a new phase of self-discovery, preparation, and intense focus that I have never experienced before in life. It is okay to make mistakes but it is unacceptable to not learn from them. And I\u2019m learning from these failures. Every Single Day.\n\nAlthough the earlier part of 2017 was great socially for me as I\u2019m on my last semester in college, I didn\u2019t socialize at all since May. As I am now completely focused on working towards new goals and developing new habits, I need to make some tough sacrifice with my social life. True rigor takes practice and resilience. Everything I do now is training. Every area of my life affects every other area of my life. And I\u2019m willing to stick to that governing principle and ignore everything else that does not matter. As I focused more on results-driven activities such as learning and daily habits this year, I didn\u2019t commit to any enjoyment-driven activities such as travel as much this year. I usually explore 8\u201310 new places every year; however, this year I only checked out 4. I cruised around Philadelphia and Washington DC towards the end of last year during the break, spent Spring Break in March at Fort Lauderdale/Miami, and visited a buddy of mine in Boulder in early April. What Am I Working Towards Next, I start looking towards the future, based on goals that are set by a category. Here are the categories that I use: Career, Education, Learning, Writing, Travel, Spiritual, Health. I try to set an average of 3 to 5 measurable goals for each category. In the past 4 months, I have been working on applications for graduate schools in the US \u2014 from studying for the GRE, asking for recommendation letters, crafting the statement of purpose, picking schools, and everything in between. I plan to do a Master of Science degree in Computer Science, with intended research on Artificial Intelligence. AI has leapt to the forefront of global discourse, garnering increased attention from practitioners, industry leaders, policy makers, and the general public. Inspired by the exponential growth and tremendous impact that AI is bringing to our connected 21st century, I believe that a strong graduate education in the subject will turn my passion of building technologies that solve real-world business challenges into reality.\n\nAfter graduate school, I want to work in the industry either as a Machine Learning Engineer or a Data Scientist. To prepare for that, I am keeping up with industry developments through reading research papers, taking MOOCs, contributing to GitHub projects and competing in data science Kaggle competitions. To be battle-ready for technical interviews, I plan to study up algorithms, system design, and machine learning concepts extensively. Finally, I am also thinking about starting some sort of side hustle that can potentially turn into a business. I\u2019m not totally sure what exactly it will entail, but I\u2019m confident that my entrepreneurial desire will make it happen. A very important goal that I set out for 2018 is to write more. I have always focused on developing my personal brand and work on my online presence ever since going to college. Actively blogging is the surest way for people to find me online, see what I\u2019m about, and contact me if needed. Therefore, I aim to post regularly every Monday and Thursday and be consistent with it next year. I\u2019ll keep writing about technical topics including data science, computer science, programming as well as non-technical things from book lessons and life experience. Ultimately, I want to generate higher tractions to my personal website as well as get more followers on Medium. I hope to commit to traveling more this upcoming year. For now, I am planning to visit some East Asian countries and possibly Australia during the summer. If I come back to the US during the fall for graduate school, then I will certainly have other vacations planned. I will definitely continue to exercise regularly, constantly testing new workout plans and food diets. For strength training, I\u2019ll focus on lifting heavier weights (at the moment, I\u2019m able to bench 150 lbs, deadlift 175 lbs, and squat 165 lbs). For HIIT Cardio, I\u2019ll focus on making constant progress towards lean gain by working on my form and keeping them short. For swimming, I\u2019ll keep pushing my limits of how long I can swim per day. Finally, I\u2019m thinking of getting back to playing tennis after a long time not picking up the racquet.\n\nI have practiced the 3 habits mentioned below in the last few months which prove to be tremendously useful to nurture my spirituality. I will keep practicing them in 2018 as they are extremely effective to grow my mentality: Reading books: Every year I read about 8\u201310 non-fiction books about business, technology, and personal development. This year, I aim to read about 25 books with broader genres and wider topics. Waking up early: I have been going to bed at 10:30 PM and waking up at 5:30 AM every single weekday over the past 3 months (not much fluctuation in the weekends). Shifting from the nocturnal mode in college to an early riser has allowed me to get less tired, be more lively, and get more work done. Listening to podcast: As mentioned earlier, I have been addicted to podcasts over the last 6 months. Not only can I keep up with news in the tech industry or learn great career and personal advice, I can also implement practical knowledge and access to the resources given by the podcasters. I\u2019ll continue listening to more podcasts in 2018 and see how much more I can learn. As I\u2019m working on my annual review, I also try to add a theme \u2014 the summary of the whole year (What\u2019s it going to look like? Who will I be for the next year?). 2016 is the Year of Magical Thinking for me because I traveled all over the places, 2017 is the Year of Transition because I focused on returning home in Vietnam after 6 years away in the US. My 2018 is going to be The Year of Productivity \u2014 in which I completely focus on learning important technical skills, creating original projects, as well as developing critical personal habits. I certainly expect myself to operate at a much higher level. I would highly recommend you to do your own annual review. It\u2019s a great exercise to self-reflect and plan out your life. We tend to overestimate what we can do in an average day but underestimate what can be done over the course of a year. Looking at a whole year in review, you may be surprised at everything you\u2019ve accomplished. \u2014 \u2014 \n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find more of my writing and projects at https://jameskle.com/."
    },
    {
        "url": "https://medium.com/@james_aka_yale/the-10-operating-system-concepts-software-developers-need-to-remember-480d0734d710?source=user_profile---------25----------------",
        "title": "The 10 Operating System Concepts Software Developers Need to Remember",
        "text": "The 10 Operating System Concepts Software Developers Need to Remember Do you speak binary? Can you comprehend machine code? If I gave you a sheet full of 1s and 0s could you tell me what it means/does? If you were to go to a country you\u2019ve never been to that speaks a language you\u2019ve never heard, or maybe your heard of it but don\u2019t actually speak it, what would you need while there to help you communicate with the locals? You would need a translator. Your operating system functions as that translator in your PC. It converts those 1s and 0s, yes/no, on/off values into a readable language that you will understand. It does all of this in a streamlined graphical user interface, or GUI, that you can move around with a mouse click things, move them, see them happening before your eyes. Knowing how operating systems work is a fundamental and critical to anyone who is a serious software developer. There should be no attempt to get around it and anyone telling you it\u2019s not necessary should be ignored. While the extend and depth of knowledge can be questioned, knowing more than the fundamentals can be critical to how well your program runs and even its structure and flow. Why? When you write a program and it runs too slow, but you see nothing wrong with your code, where else will you look for a solution. How will you be able to debug the problem if you don\u2019t know how the operating system works? Are you accessing too many files? Running out of memory and swap is in high usage? But you don\u2019t even know what swap is! Or is I/O blocking? And you want to communicate with another machine. How do you do that locally or over the internet? And what\u2019s the difference? Why do some programmers prefer one OS over another? In an attempt to be a serious developer, I recently took Georgia Tech\u2019s course \u201cIntroduction to Operating Systems.\u201d It teaches the basic OS abstractions, mechanisms, and their implementations. The core of the course contains concurrent programming (threads and synchronization), inter-process communication, and an introduction to distributed OSs. I want to use this post to share my takeaways from the course, that is the 10 critical operating system concepts that you need to learn if you want to get good at developing software. But first, let\u2019s define what an operating system is. An Operating System (OS) is a collection of software that manages computer hardware and provides services for programs. Specifically, it hides hardware complexity, manages computational resources, and provides isolation and protection. Most importantly, it directly has privilege access to the underlying hardware. Major components of an OS are file system, scheduler, and device driver. You probably have used both Desktop (Windows, Mac, Linux) and Embedded (Android, iOS) operating systems before. There are 3 key elements of an operating system, which are: (1) Abstractions (process, thread, file, socket, memory), (2) Mechanisms (create, schedule, open, write, allocate), and (3) Policies (LRU, EDF) There are 2 operating system design principles, which are: (1) Separation of mechanism and policy by implementing flexible mechanisms to support policies, and (2) Optimize for common case: Where will the OS be used? What will the user want to execute on that machine? What are the workload requirements? The 3 types of Operating Systems commonly used nowadays are: (1) Monolithic OS, where the entire OS is working in kernel space and is alone in supervisor mode; (2) Modular OS, in which some part of the system core will be located in independent files called modules that can be added to the system at run time; and (3) Micro OS, where the kernel is broken down into separate processes, known as servers. Some of the servers run in kernel space and some run in user-space. A process is basically a program in execution. The execution of a process must progress in a sequential fashion. To put it in simple terms, we write our computer programs in a text file and when we execute this program, it becomes a process which performs all the tasks mentioned in the program. When a program is loaded into the memory and it becomes a process, it can be divided into four sections \u2500 stack, heap, text and data. The following image shows a simplified layout of a process inside main memory Stack: The process Stack contains the temporary data such as method/function parameters, return address and local variables. Heap: This is dynamically allocated memory to a process during its run time. Text: This includes the current activity represented by the value of Program Counter and the contents of the processor\u2019s registers. Data: This section contains the global and static variables. When a process executes, it passes through different states. These stages may differ in different operating systems, and the names of these states are also not standardized. In general, a process can have one of the following five states at a time: Start: This is the initial state when a process is first started/created. Ready: The process is waiting to be assigned to a processor. Ready processes are waiting to have the processor allocated to them by the operating system so that they can run. Process may come into this state after Start state or while running it by but interrupted by the scheduler to assign CPU to some other process. Running: Once the process has been assigned to a processor by the OS scheduler, the process state is set to running and the processor executes its instructions. Waiting: Process moves into the waiting state if it needs to wait for a resource, such as waiting for user input, or waiting for a file to become available. Terminated or Exit: Once the process finishes its execution, or it is terminated by the operating system, it is moved to the terminated state where it waits to be removed from main memory. A Process Control Block is a data structure maintained by the Operating System for every process. The PCB is identified by an integer process ID (PID). A PCB keeps all the information needed to keep track of a process as listed below: Process State: The current state of the process i.e., whether it is ready, running, waiting, or whatever. Process Privileges: This is required to allow/disallow access to system resources. Process ID: Unique identification for each of the process in the operating system. Program Counter: Program Counter is a pointer to the address of the next instruction to be executed for this process. CPU Registers: Various CPU registers where process need to be stored for execution for running state. CPU Scheduling Information: Process priority and other scheduling information which is required to schedule the process. Memory Management Information: This includes the information of page table, memory limits, Segment table depending on memory used by the operating system. Accounting Information: This includes the amount of CPU used for process execution, time limits, execution ID etc. IO Status Information: This includes a list of I/O devices allocated to the process. A thread is a flow of execution through the process code, with its own program counter that keeps track of which instruction to execute next, system registers which hold its current working variables, and a stack which contains the execution history. A thread shares with its peer threads few information like code segment, data segment and open files. When one thread alters a code segment memory item, all other threads see that. A thread is also called a lightweight process. Threads provide a way to improve application performance through parallelism. Threads represent a software approach to improving performance of operating system by reducing the overhead thread is equivalent to a classical process. Each thread belongs to exactly one process and no thread can exist outside a process. Each thread represents a separate flow of control. Threads have been successfully used in implementing network servers and web server. They also provide a suitable foundation for parallel execution of applications on shared memory multiprocessors. Use of threads provides concurrency within a process. It is more economical to create and context switch threads. Threads allow utilization of multiprocessor architectures to a greater scale and efficiency. Threads are implemented in the following 2 ways: In this case, the thread management kernel is not aware of the existence of threads. The thread library contains code for creating and destroying threads, for passing message and data between threads, for scheduling thread execution and for saving and restoring thread contexts. The application starts with a single thread. User level thread can run on any operating system. Scheduling can be application specific in the user level thread. User level threads are fast to create and manage. In a typical operating system, most system calls are blocking. Multithreaded application cannot take advantage of multiprocessing. In this case, thread management is done by the Kernel. There is no thread management code in the application area. Kernel threads are supported directly by the operating system. Any application can be programmed to be multithreaded. All of the threads within an application are supported within a single process. The Kernel maintains context information for the process as a whole and for individuals threads within the process. Scheduling by the Kernel is done on a thread basis. The Kernel performs thread creation, scheduling and management in Kernel space. Kernel threads are generally slower to create and manage than the user threads. Kernel can simultaneously schedule multiple threads from the same process on multiple processes. If one thread in a process is blocked, the Kernel can schedule another thread of the same process. Kernel routines themselves can be multithreaded. Kernel threads are generally slower to create and manage than the user threads. Transfer of control from one thread to another within the same process requires a mode switch to the Kernel. The process scheduling is the activity of the process manager that handles the removal of the running process from the CPU and the selection of another process on the basis of a particular strategy. Process scheduling is an essential part of a Multiprogramming operating systems. Such operating systems allow more than one process to be loaded into the executable memory at a time and the loaded process shares the CPU using time multiplexing. The OS maintains all Process Control Blocks (PCBs) in Process Scheduling Queues. The OS maintains a separate queue for each of the process states and PCBs of all processes in the same execution state are placed in the same queue. When the state of a process is changed, its PCB is unlinked from its current queue and moved to its new state queue. The Operating System maintains the following important process scheduling queues: Job queue \u2212 This queue keeps all the processes in the system. Ready queue \u2212 This queue keeps a set of all processes residing in main memory, ready and waiting to execute. A new process is always put in this queue. Device queues \u2212 The processes which are blocked due to unavailability of an I/O device constitute this queue. The OS can use different policies to manage each queue (FIFO, Round Robin, Priority, etc.). The OS scheduler determines how to move processes between the ready and run queues which can only have one entry per processor core on the system; in the above diagram, it has been merged with the CPU. Running: When a new process is created, it enters into the system as in the running state. Not Running: Processes that are not running are kept in queue, waiting for their turn to execute. Each entry in the queue is a pointer to a particular process. Queue is implemented by using linked list. Use of dispatcher is as follows. When a process is interrupted, that process is transferred in the waiting queue. If the process has completed or aborted, the process is discarded. In either case, the dispatcher then selects a process from the queue to execute. A context switch is the mechanism to store and restore the state or context of a CPU in Process Control block so that a process execution can be resumed from the same point at a later time. Using this technique, a context switcher enables multiple processes to share a single CPU. Context switching is an essential part of a multitasking operating system features. When the scheduler switches the CPU from executing one process to execute another, the state from the current running process is stored into the process control block. After this, the state for the process to run next is loaded from its own PCB and used to set the PC, registers, etc. At that point, the second process can start executing. Context switches are computationally intensive since register and memory state must be saved and restored. To avoid the amount of context switching time, some hardware systems employ two or more sets of processor registers. When the process is switched, the following information is stored for later use: Program Counter, Scheduling Information, Base and Limit Register Value, Currently Used Register, Changed State, I/O State Information, and Accounting Information. Memory management is the functionality of an operating system which handles or manages primary memory and moves processes back and forth between main memory and disk during execution. Memory management keeps track of each and every memory location, regardless of either it is allocated to some process or it is free. It checks how much memory is to be allocated to processes. It decides which process will get memory at what time. It tracks whenever some memory gets freed or unallocated and correspondingly it updates the status.\n\nThe process address space is the set of logical addresses that a process references in its code. For example, when 32-bit addressing is in use, addresses can range from 0 to 0x7fffffff; that is, 2\u00b3\u00b9 possible numbers, for a total theoretical size of 2 gigabytes. The operating system takes care of mapping the logical addresses to physical addresses at the time of memory allocation to the program. There are three types of addresses used in a program before and after memory is allocated: Symbolic addresses: The addresses used in a source code. The variable names, constants, and instruction labels are the basic elements of the symbolic address space. Relative addresses: At the time of compilation, a compiler converts symbolic addresses into relative addresses. Physical addresses: The loader generates these addresses at the time when a program is loaded into main memory. Virtual and physical addresses are the same in compile-time and load-time address-binding schemes. Virtual and physical addresses differ in execution-time address-binding scheme. The set of all logical addresses generated by a program is referred to as a logical address space. The set of all physical addresses corresponding to these logical addresses is referred to as a physical address space. A process can be of 2 types: Independent process and Co-operating process. An independent process is not affected by the execution of other processes while a co-operating process can be affected by other executing processes. Though one can think that those processes, which are running independently, will execute very efficiently but in practical, there are many situations when co-operative nature can be utilized for increasing computational speed, convenience and modularity. Inter-process communication (IPC) is a mechanism which allows processes to communicate each other and synchronize their actions. The communication between these processes can be seen as a method of co-operation between them. Processes can communicate with each other using these two ways: Shared Memory and Message Parsing. There are two processes: Producer and Consumer. Producer produces some item and Consumer consumes that item. The two processes shares a common space or memory location known as buffer where the item produced by Producer is stored and from where the Consumer consumes the item if needed. There are two version of this problem: first one is known as unbounded buffer problem in which Producer can keep on producing items and there is no limit on size of buffer, the second one is known as bounded buffer problem in which producer can produce up to a certain amount of item and after that it starts waiting for consumer to consume it. In the bounded buffer problem: First, the Producer and the Consumer will share some common memory, then producer will start producing items. If the total produced item is equal to the size of buffer, producer will wait to get it consumed by the Consumer. Similarly, the consumer first check for the availability of the item and if no item is available, Consumer will wait for producer to produce it. If there are items available, consumer will consume it. In this method, processes communicate with each other without using any kind of of shared memory. If two processes p1 and p2 want to communicate with each other, they proceed as follow: Establish a communication link (if a link already exists, no need to establish it again.) Start exchanging messages using basic primitives. We need at least two primitives: send(message, destination) or send(message) and receive(message, host) or receive(message) The message size can be of fixed size or of variable size. if it is of fixed size, it is easy for OS designer but complicated for programmer and if it is of variable size then it is easy for programmer but complicated for the OS designer. A standard message can have two parts: header and body. The header part is used for storing Message type, destination id, source id, message length and control information. The control information contains information like what to do if runs out of buffer space, sequence number, priority. Generally, message is sent using FIFO style. One of the important jobs of an Operating System is to manage various I/O devices including mouse, keyboards, touch pad, disk drives, display adapters, USB devices, Bit-mapped screen, LED, Analog-to-digital converter, On/off switch, network connections, audio I/O, printers etc. An I/O system is required to take an application I/O request and send it to the physical device, then take whatever response comes back from the device and send it to the application. I/O devices can be divided into two categories: Block devices \u2014 A block device is one with which the driver communicates by sending entire blocks of data. For example, hard disks, USB cameras, Disk-On-Key etc. Character Devices \u2014 A character device is one with which the driver communicates by sending and receiving single characters (bytes, octets). For example, serial ports, parallel ports, sounds cards etc. The CPU must have a way to pass information to and from an I/O device. There are three approaches available to communicate with the CPU and Device. This uses CPU instructions that are specifically made for controlling I/O devices. These instructions typically allow data to be sent to an I/O device or read from an I/O device. When using memory-mapped I/O, the same address space is shared by memory and I/O devices. The device is connected directly to certain main memory locations so that I/O device can transfer block of data to/from memory without going through CPU. While using memory mapped IO, OS allocates buffer in memory and informs I/O device to use that buffer to send data to the CPU. I/O device operates asynchronously with CPU, interrupts CPU when finished. The advantage to this method is that every instruction which can access memory can be used to manipulate an I/O device. Memory mapped IO is used for most high-speed I/O devices like disks, communication interfaces. Slow devices like keyboards will generate an interrupt to the main CPU after each byte is transferred. If a fast device such as a disk generated an interrupt for each byte, the operating system would spend most of its time handling these interrupts. So a typical computer uses direct memory access (DMA) hardware to reduce this overhead. Direct Memory Access (DMA) means CPU grants I/O module authority to read from or write to memory without involvement. DMA module itself controls exchange of data between main memory and the I/O device. CPU is only involved at the beginning and end of the transfer and interrupted only after entire block has been transferred. Direct Memory Access needs a special hardware called DMA controller (DMAC) that manages the data transfers and arbitrates access to the system bus. The controllers are programmed with source and destination pointers (where to read/write the data), counters to track the number of transferred bytes, and settings, which includes I/O and memory types, interrupts and states for the CPU cycles. Virtualization is technology that allows you to create multiple simulated environments or dedicated resources from a single, physical hardware system. Software called a hypervisor connects directly to that hardware and allows you to split 1 system into separate, distinct, and secure environments known as virtual machines (VMs). These VMs rely on the hypervisor\u2019s ability to separate the machine\u2019s resources from the hardware and distribute them appropriately. The original, physical machine equipped with the hypervisor is called the host, while the many VMs that use its resources are called guests. These guests treat computing resources \u2014 like CPU, memory, and storage \u2014 as a hangar of resources that can easily be relocated. Operators can control virtual instances of CPU, memory, storage, and other resources, so guests receive the resources they need when they need them. Ideally, all related VMs are managed through a single web-based virtualization management console, which speeds things up. Virtualization lets you dictate how much processing power, storage, and memory to give VMs, and environments are better protected since VMs are separated from their supporting hardware and each other. Simply put, virtualization creates the environments and resources you need from underused hardware.\n\nHide data movement and provide a simpler abstraction for sharing data. Programmers don\u2019t need to worry about memory transfers between machines like when using the message passing model. Allows the passing of complex structures by reference, simplifying algorithm development for distributed applications. Takes advantage of \u201clocality of reference\u201d by moving the entire page containing the data referenced rather than just the piece of data. Cheaper to build than multiprocessor systems. Ideas can be implemented using normal hardware and do not require anything complex to connect the shared memory to the processors. Larger memory sizes are available to programs, by combining all physical memory of all nodes. This large memory will not incur disk latency due to swapping like in traditional distributed systems. Unlimited number of nodes can be used. Unlike multiprocessor systems where main memory is accessed via a common bus, thus limiting the size of the multiprocessor system. Programs written for shared memory multiprocessors can be run on DSM systems. There are two different ways that nodes can be informed of who owns what page: invalidation and broadcast. Invalidation is a method that invalidates a page when some process asks for write access to that page and becomes its new owner. This way the next time some other process tries to read or write to a copy of the page it thought it had, the page will not be available and the process will have to re-request access to that page. Broadcasting will automatically update all copies of a memory page when a process writes to it. This is also called write-update. This method is a lot less efficient more difficult to implement because a new value has to sent instead of an invalidation message. More and more, we are seeing technology moving to the cloud. It\u2019s not just a fad \u2014 the shift from traditional software models to the Internet has steadily gained momentum over the last 10 years. Looking ahead, the next decade of cloud computing promises new ways to collaborate everywhere, through mobile devices. So what is cloud computing? Essentially, cloud computing is a kind of outsourcing of computer programs. Using cloud computing, users are able to access software and applications from wherever they need, while it is being hosted by an outside party \u2014 in \u201cthe cloud.\u201d This means that they do not have to worry about things such as storage and power, they can simply enjoy the end result. Traditional business applications have always been very complicated and expensive. The amount and variety of hardware and software required to run them are daunting. You need a whole team of experts to install, configure, test, run, secure, and update them. When you multiply this effort across dozens or hundreds of apps, it isn\u2019t easy to see why the biggest companies with the best IT departments aren\u2019t getting the apps they need. Small and mid-sized businesses don\u2019t stand a chance.\n\nWith cloud computing, you eliminate those headaches that come with storing your own data, because you\u2019re not managing hardware and software \u2014 that becomes the responsibility of an experienced vendor like Salesforce and AWS. The shared infrastructure means it works like a utility: you only pay for what you need, upgrades are automatic, and scaling up or down is easy. Cloud-based apps can be up and running in days or weeks, and they cost less. With a cloud app, you just open a browser, log in, customize the app, and start using it. Businesses are running all kinds of apps in the cloud, like customer relationship management (CRM), HR, accounting, and much more. As cloud computing grows in popularity, thousands of companies are simply rebranding their non-cloud products and services as \u201ccloud computing.\u201d Always dig deeper when evaluating cloud offerings and keep in mind that if you have to buy and manage hardware and software, what you\u2019re looking at isn\u2019t really cloud computing but a false cloud. As a software engineer, you will be part of a larger body of computer science, which encompasses hardware, operating systems, networking, data management and mining, and many other disciplines. The more engineers in each of these disciplines understand about the other disciplines, the better they will be able to interact with those other disciplines efficiently. As the operating system is the \u201cbrain\u201d that manages input, processing, and output, all other disciplines interact with the operating system. An understanding of how the operating system works will provide valuable insight into how the other disciplines work, as your interaction with those disciplines is managed by the operating system."
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/the-4-layer-internet-model-network-engineers-need-to-know-3683d159c94?source=user_profile---------26----------------",
        "title": "The 4-Layer Internet Model Network Engineers Need to Know",
        "text": "Given that so much of software engineer is on web servers and clients, one of the most immediately valuable areas of computer science is computer networking. With the advent of the World Wide Web, the global Internet has rapidly become the dominant type of computer network. It now enables people around the world to use the Web for e-commerce and interactive entertainment applications, in addition to email and IP telephony. As a result, the study of computer networking is now synonymous with the study of the Internet and its applications.\n\nRecently, I finished an online Stanford course called \u201cIntroduction to Computer Networking.\u201d The course focuses on explaining how the Internet works, ranging from how bits are modulated on wires and in wireless to application-level protocols like BitTorrent and HTTP. It also explains the principles of how to design networks and network protocols. I want to share a small bit of the knowledge I acquired from the course here.\n\nThe easiest way to understand computer networks is through the compare itself. Computers are general-purpose machines that mean different things to different people. Some of us just want to do basic tasks like word processing or chatting with Facebook friends and so we couldn\u2019t care less how that happens under the covers. At the opposite end of the spectrum, some of us like modifying our computers to run faster, fitting quicker processors or more memory, or whatever it might be; for geeks, poking around inside computers is an end in itself. Somewhere in between these extremes, there are moderately tech-savvy people who use computers to do everyday jobs with a reasonable understanding of how their machines work. Because computers mean different things to different people, it can help us to understand them by thinking of a stack of layers: hardware at the bottom, the operating system somewhere on top of that, then applications running at the highest level. You can \u201cengage\u201d with a computer at any of these levels without necessarily thinking about any of the other layers. Nevertheless, each layer is made possible by things happening at lower levels, whether you\u2019re aware of that or not. Things that happen at the higher levels could be carried out in many different ways at the lower levels; for example, you can use a web browser like Chrome (an application) on many different operating systems, and you can run various operating systems on a particular laptop, even though the hardware doesn\u2019t change at all.\n\nComputer networks are similar: we all have different ideas about them and care more or less about what they\u2019re doing and why. If you work in a small office with your computer hooked up to other people\u2019s machines and shared printers, probably all you care about is that you can send emails to your colleagues and print out your stuff; you\u2019re not bothered how that actually happens. But if you\u2019re charged with setting up the network in the first place, you have to consider things like how it\u2019s physically linked together, what sort of cables you\u2019re using and how long they can be, what the MAC (media access control) addresses are, and all kinds of other nitty gritty. Again, just like with computers, we can think about a network in terms of its different layers \u2014 and there are 2 popular ways of doing that.\n\nWhile the OSI model is quite an abstract and academic concept, rarely encountered outside books and articles about computer networking, the TCP/IP model is a simpler, easier-to-understand, and more practical proposition: it\u2019s the bedrock of the Internet \u2014 and the very technology you\u2019re using to read these words now. We can understand TCP/IP using 4 slightly simple layers described in details below:\n\nThe Internet is made up of end-hosts, links and routers. Data is delivered hop-by-hop over each link in turn. Data is delivered in packets. A packet consists of the data we want to be delivered, along with a header that tells the network where the packet is to be delivered, where it came from and so on.\n\nThe Link layer\u2019s job is to carry the data over one link at a time. You have probably heard of Ethernet and WiFi \u2014 these are 2 examples of different Link layers.\n\nThe most important layer is the Network layer. It delivers packets end-to-end across the Internet from the source to the destination. A packet is an important basic building block in networks. A packet is the name we give to a self-contained collection of data, plus a header that describes what the data is, where it is going and where it came from.\n\nNetwork layer packets are called datagrams. They consist of some data and a head containing the \u201cTo\u201d and \u201cFrom\u201d addresses \u2014 just like we put the \u201cTo:\u201d and \u201cFrom\u201d addresses on a letter. The Network hands the datagram to the Link Layer below, telling it to send the datagram over the first link. In other words, the Link Layer is providing a service to the Network Layer. Essentially, the Link Layer says: \u201cif you give me a datagram to send, I will transmit it over one link for you.\u201d\n\nAt the other end of the link is a router. The Link Layer of the router accepts the datagram from the link, and hands it up to the Network Layer in the router. The Network Layer on the router examines the destination address of the datagram, and is responsible for routing the datagram one hop at a time towards its eventual destination. It does this by sending to the Link Layer again, to carry it over the next link. And so on until it reaches the Network Layer at the destination.\n\nNotice that the Network Layer does not need to concern itself with *how* the Link Layer sends the datagram over the link. In fact, different Link Layers work in very different ways; Ethernet and WiFi are clearly very different. This separation of concerns between the Network Layer and the Link Layer allows each to focus on its job, without worrying about how the other layer works. It also means that a single Network Layer has a common way to talk to many different Link Layers by simply handing them datagrams to send. This separation of concerns is made possibly by the modularity of each layer and a common well-defined API to the layer below.\n\nIn the internet, the network layer is special: When we send packets into the Internet, we must use the Internet Protocol. It is the Internet Protocol, or IP, that holds the Internet together. IP provides a deliberately simple service. It is a simple, dumb, minimal service with four main features: It sends datagrams, hop-by-hop across the Internet. The service is unreliable and best-effort; there is no per-flow state making the protocol connectionless.\n\nThe most common Transport Layer is TCP, or the Transmission Control Protocol.\n\nTCP makes sure that data sent by an application at one end of the Internet is correctly delivered \u2013in the right order -to the application at the other end of the Internet. If the Network Layers drops some datagrams, TCP will retransmit them, multiple times if need-be. If the Network Layer delivers them out of order \u2013perhaps because two packets follow a different path to their destination \u2014 TCP will put the data back into the right order again.\n\nApplications such as a web client, or an email client, find TCP very useful indeed. By employing TCP to make sure data is delivered correctly, they don\u2019t have to worry about implementing all of the mechanisms inside the application. They can take advantage of the huge effort that developers put into correctly implementing TCP, and reuse it to deliver data correctly. Reuse is another big advantage of layering.\n\nBut not all applications need data to be delivered correctly. For example, if a video conference application is sending a snippet of video in a packet, there may be no point waiting for the packet to be retransmitted multiple times; better to just move on. Some applications just don\u2019t need the TCP service.\n\nIf an application doesn\u2019t need reliable delivery, it can use the much simple UDP \u2014 or user datagram protocol \u2014 instead. UDP just bundles up application data and hands it to the Network Layer for delivery to the other end. UDP offers no delivery guarantees.\n\nIn other words, an Application has the choice of at least two different Transport Layer services: TCP and UDP. There are in fact many other choices too, but these are the most commonly used transport layer services.\n\nThere are of course many thousands of applications that use the Internet. While each application is different, it can reuse the Transport Layer by using the well-defined API from the Application Layer to the TCP or UDP service beneath.\n\nApplications typically want a bi-directional reliable byte stream between two end points. They can send whatever byte-stream they want, and Applications have a protocol of their own that defines the syntax and semantics of data flowing between the two end points.\n\nFor example, when a web client requests a page from a web server, the web client sends a GET request. This is one of the commands of the hypertext transfer protocol, or http. http dictates that the GET command is sent as an ASCII string, along with the URL of the page being requested. As far as the Application Layer is concerned, the GET request is sent directly to its peer at the other end \u2013the web server Application. The Application doesn\u2019t need to know how it got there, or how many times it needed to be retransmitted. At the web client, the Application Layer hands the GET request to the TCP layer, which provides the service of making sure it is reliably delivered. It does this using the services of the Network layer, which in turn uses the services of the Link Layer.\n\nNetwork engineers find it convenient to arrange all the functions that make up the Internet into Layers. At the top is the Application, such as BitTorrent or Skype or the world wide web, which talks to its peer-layer at the destination. When the application has data to send, it hands the data to the Transport layer, which has the job of delivering the data reliably to the other end. The Transport Layer sends data to the other end by handing it to the Network Layer, which has the job of breaking the data into packets, each with the correct destination address. Finally, the packets are handed to the Link Layer, which has the responsibility of delivering the packet from one hop to the next along its path. The data makes its way, hop by hop, from one router to the next. The Network Layer forwards it to the next router, one at a time, until it reaches the destination. There, the data is passed up the layers, until it reaches the Application."
    },
    {
        "url": "https://medium.com/@james_aka_yale/the-4-layer-internet-model-network-engineers-need-to-know-e78432614a4f?source=user_profile---------27----------------",
        "title": "The 4-Layer Internet Model Network Engineers Need to Know",
        "text": "Given that so much of software engineer is on web servers and clients, one of the most immediately valuable areas of computer science is computer networking. With the advent of the World Wide Web, the global Internet has rapidly become the dominant type of computer network. It now enables people around the world to use the Web for e-commerce and interactive entertainment applications, in addition to email and IP telephony. As a result, the study of computer networking is now synonymous with the study of the Internet and its applications.\n\nRecently, I finished an online Stanford course called \u201cIntroduction to Computer Networking.\u201d The course focuses on explaining how the Internet works, ranging from how bits are modulated on wires and in wireless to application-level protocols like BitTorrent and HTTP. It also explains the principles of how to design networks and network protocols. I want to share a small bit of the knowledge I acquired from the course here.\n\nThe easiest way to understand computer networks is through the compare itself. Computers are general-purpose machines that mean different things to different people. Some of us just want to do basic tasks like word processing or chatting with Facebook friends and so we couldn\u2019t care less how that happens under the covers. At the opposite end of the spectrum, some of us like modifying our computers to run faster, fitting quicker processors or more memory, or whatever it might be; for geeks, poking around inside computers is an end in itself. Somewhere in between these extremes, there are moderately tech-savvy people who use computers to do everyday jobs with a reasonable understanding of how their machines work. Because computers mean different things to different people, it can help us to understand them by thinking of a stack of layers: hardware at the bottom, the operating system somewhere on top of that, then applications running at the highest level. You can \u201cengage\u201d with a computer at any of these levels without necessarily thinking about any of the other layers. Nevertheless, each layer is made possible by things happening at lower levels, whether you\u2019re aware of that or not. Things that happen at the higher levels could be carried out in many different ways at the lower levels; for example, you can use a web browser like Chrome (an application) on many different operating systems, and you can run various operating systems on a particular laptop, even though the hardware doesn\u2019t change at all.\n\nComputer networks are similar: we all have different ideas about them and care more or less about what they\u2019re doing and why. If you work in a small office with your computer hooked up to other people\u2019s machines and shared printers, probably all you care about is that you can send emails to your colleagues and print out your stuff; you\u2019re not bothered how that actually happens. But if you\u2019re charged with setting up the network in the first place, you have to consider things like how it\u2019s physically linked together, what sort of cables you\u2019re using and how long they can be, what the MAC (media access control) addresses are, and all kinds of other nitty gritty. Again, just like with computers, we can think about a network in terms of its different layers \u2014 and there are 2 popular ways of doing that.\n\nWhile the OSI model is quite an abstract and academic concept, rarely encountered outside books and articles about computer networking, the TCP/IP model is a simpler, easier-to-understand, and more practical proposition: it\u2019s the bedrock of the Internet \u2014 and the very technology you\u2019re using to read these words now. We can understand TCP/IP using 4 slightly simple layers described in details below:\n\nThe Internet is made up of end-hosts, links and routers. Data is delivered hop-by-hop over each link in turn. Data is delivered in packets. A packet consists of the data we want to be delivered, along with a header that tells the network where the packet is to be delivered, where it came from and so on.\n\nThe Link layer\u2019s job is to carry the data over one link at a time. You have probably heard of Ethernet and WiFi \u2014 these are 2 examples of different Link layers.\n\nThe most important layer is the Network layer. It delivers packets end-to-end across the Internet from the source to the destination. A packet is an important basic building block in networks. A packet is the name we give to a self-contained collection of data, plus a header that describes what the data is, where it is going and where it came from.\n\nNetwork layer packets are called datagrams. They consist of some data and a head containing the \u201cTo\u201d and \u201cFrom\u201d addresses \u2014 just like we put the \u201cTo:\u201d and \u201cFrom\u201d addresses on a letter. The Network hands the datagram to the Link Layer below, telling it to send the datagram over the first link. In other words, the Link Layer is providing a service to the Network Layer. Essentially, the Link Layer says: \u201cif you give me a datagram to send, I will transmit it over one link for you.\u201d\n\nAt the other end of the link is a router. The Link Layer of the router accepts the datagram from the link, and hands it up to the Network Layer in the router. The Network Layer on the router examines the destination address of the datagram, and is responsible for routing the datagram one hop at a time towards its eventual destination. It does this by sending to the Link Layer again, to carry it over the next link. And so on until it reaches the Network Layer at the destination.\n\nNotice that the Network Layer does not need to concern itself with *how* the Link Layer sends the datagram over the link. In fact, different Link Layers work in very different ways; Ethernet and WiFi are clearly very different. This separation of concerns between the Network Layer and the Link Layer allows each to focus on its job, without worrying about how the other layer works. It also means that a single Network Layer has a common way to talk to many different Link Layers by simply handing them datagrams to send. This separation of concerns is made possibly by the modularity of each layer and a common well-defined API to the layer below.\n\nIn the internet, the network layer is special: When we send packets into the Internet, we must use the Internet Protocol. It is the Internet Protocol, or IP, that holds the Internet together. IP provides a deliberately simple service. It is a simple, dumb, minimal service with four main features: It sends datagrams, hop-by-hop across the Internet. The service is unreliable and best-effort; there is no per-flow state making the protocol connectionless.\n\nThe most common Transport Layer is TCP, or the Transmission Control Protocol.\n\nTCP makes sure that data sent by an application at one end of the Internet is correctly delivered \u2013in the right order -to the application at the other end of the Internet. If the Network Layers drops some datagrams, TCP will retransmit them, multiple times if need-be. If the Network Layer delivers them out of order \u2013perhaps because two packets follow a different path to their destination \u2014 TCP will put the data back into the right order again.\n\nApplications such as a web client, or an email client, find TCP very useful indeed. By employing TCP to make sure data is delivered correctly, they don\u2019t have to worry about implementing all of the mechanisms inside the application. They can take advantage of the huge effort that developers put into correctly implementing TCP, and reuse it to deliver data correctly. Reuse is another big advantage of layering.\n\nBut not all applications need data to be delivered correctly. For example, if a video conference application is sending a snippet of video in a packet, there may be no point waiting for the packet to be retransmitted multiple times; better to just move on. Some applications just don\u2019t need the TCP service.\n\nIf an application doesn\u2019t need reliable delivery, it can use the much simple UDP \u2014 or user datagram protocol \u2014 instead. UDP just bundles up application data and hands it to the Network Layer for delivery to the other end. UDP offers no delivery guarantees.\n\nIn other words, an Application has the choice of at least two different Transport Layer services: TCP and UDP. There are in fact many other choices too, but these are the most commonly used transport layer services.\n\nThere are of course many thousands of applications that use the Internet. While each application is different, it can reuse the Transport Layer by using the well-defined API from the Application Layer to the TCP or UDP service beneath.\n\nApplications typically want a bi-directional reliable byte stream between two end points. They can send whatever byte-stream they want, and Applications have a protocol of their own that defines the syntax and semantics of data flowing between the two end points.\n\nFor example, when a web client requests a page from a web server, the web client sends a GET request. This is one of the commands of the hypertext transfer protocol, or http. http dictates that the GET command is sent as an ASCII string, along with the URL of the page being requested. As far as the Application Layer is concerned, the GET request is sent directly to its peer at the other end \u2013the web server Application. The Application doesn\u2019t need to know how it got there, or how many times it needed to be retransmitted. At the web client, the Application Layer hands the GET request to the TCP layer, which provides the service of making sure it is reliably delivered. It does this using the services of the Network layer, which in turn uses the services of the Link Layer.\n\nNetwork engineers find it convenient to arrange all the functions that make up the Internet into Layers. At the top is the Application, such as BitTorrent or Skype or the world wide web, which talks to its peer-layer at the destination. When the application has data to send, it hands the data to the Transport layer, which has the job of delivering the data reliably to the other end. The Transport Layer sends data to the other end by handing it to the Network Layer, which has the job of breaking the data into packets, each with the correct destination address. Finally, the packets are handed to the Link Layer, which has the responsibility of delivering the packet from one hop to the next along its path. The data makes its way, hop by hop, from one router to the next. The Network Layer forwards it to the next router, one at a time, until it reaches the destination. There, the data is passed up the layers, until it reaches the Application."
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/the-6-principles-to-make-your-ideas-stick-91a17229c949?source=user_profile---------28----------------",
        "title": "The 6 Principles to Make Your Ideas Stick \u2013 Constraint Drives Creativity \u2013",
        "text": "Why do some ideas thrive while others die? And how do we improve the chances of worthy ideas? One of the most interesting books I read this year is Chip and Dan Heath\u2019s \u201cMade To Stick\u201d \u2014 a fast-paced tour of idea success stories. As many of us struggle with how to communicate ideas effectively and how to get our ideas to make a difference, I want to share the principles of successful ideas at work that I got from reading this book.\n\nChip and Dan Heath offer us 6 qualities that make ideas sticky, all wrapped up in a clever acronym: Simple Unexpected Concrete Credible Emotional Stories (SUCCES).\n\nIf we are to succeed, the first step is this: Be simple. That doesn\u2019t mean dumbing things down; it does mean finding the core of the idea. \u201cFinding the core\u201d means stripping an idea down to its most critical essence. To get to the core, we\u2019ve got to weed out the superfluous elements, and also the important ideas that are really important, but just aren\u2019t the most important. We need to master the art of exclusion.\n\nIn front of a judge, if a lawyer argues 10 points, then he might not remember them all. So, the lawyer needs to argue the most important point that will turn the case to his favor.\n\nWhen people have too many choices, they tend to get paralyzed and find it difficult to make decisions. It often isn\u2019t clear what is best. Core messages help people make choices by reminding them of what\u2019s important, and enabling that to guide their decisions.\n\nPerhaps the simplest of all sticky ideas is Einstein\u2019s E = MC\u00b2, which renders the complexity of the material universe and the mystery of relativity in 5 letters, numbers, and symbols.\n\nBesides being core, simple messages also need to be compact. That probably seems obvious: we know that sentences are better than paragraphs, easy words are better than hard words, etc.\n\nThe hard part isn\u2019t weeding out unimportant aspects, but it is in pruning the important, but not truly essential aspects \u2014 i.e., distilling to the most important idea at the core.\n\nThe first requirement of effective communication is getting attention, the second is keeping it.\n\nHumans think in patterns, the key is to break these patterns. Humans adapt incredibly quickly to patterns. We often simply tune them out. Think of the hum of a fan, or traffic noise, or a familiar smell. We only become aware of them when something changes.\n\nSo, a good process for making ideas stickier is:\n\nSurprise jolts us to attention. It is triggered when our expectations fail, and it prepares us to understand why the failure occurred. Unexpected ideas are more likely to stick because surprise makes us pay attention and think. The extra attention and thinking sears unexpected events into our memories.\n\nSurprise doesn\u2019t work well if it\u2019s just gimmicky. To be surprising, an event can\u2019t be predictable. Surprise is the opposite of predictability. But to be satisfying, surprise must be \u201cpost-dictable.\u201d The twist makes sense after you think about it, but it\u2019s not something you would have seen coming.\n\nThat kind of curiosity happens when we notice a gap in our knowledge. We feel a need to fill the gap. That need can make us finish a bad book, or watch a bad movie to the end, because we want to know what happens. The gap in our knowledge (curiosity) holds our attention.\n\nOf the 6 traits of \u201cstickiness\u201d described in this book, being concrete is the easiest to accept and implement.\n\nThe power of being concrete is illustrated by the longevity of Aesop\u2019s fables. For some 2,500 years they have resonated and been remembered by human kind. They are a striking example of concreteness. For example, the story of the fox and the grape ends with the fox concluding that grapes out of his reach are likely sour \u2014 hence the phrase \u201csour grapes\u201d, which appears in nearly every language. This provides a concrete image which lasts: Compare \u201csour grapes\u201d to the conclusion \u201cdon\u2019t be such a bitter jerk when you fail.\u201d The latter has no staying power: It is naked fact.\n\nSomething becomes concrete when it can be described or detected by the human senses. A V-8 engine is concrete; \u201chigh-performance\u201d is abstract. Concrete ideas are easy to remember. Experiments have shown that people remember concrete over abstract nouns: \u201cbicycle\u201d over \u201cjustice\u201d or \u201cpersonality.\u201d\n\nThe main difference between an expert and novice is the ability of the expert to see things abstractly. For example, the difference in reaction between a judge and a jury: The jury sees all the concrete aspects of a trial \u2014 the lawyers\u2019 clothing, manner, the specific procedures in a classroom; the judge sees all in terms of legal precedent and the lessons of the past. Novices perceive concrete detail as concrete detail; an expert sees concrete details as symbols of a pattern.\n\nConcreteness also enables coordination by making targets clear. Even experts need clarity. Consider a software startup with the goal of building \u201cthe next great search engine.\u201d Within the startup are two programmers with nearly identical skillsets working next to each other. To one \u201cthe next great search engine\u201d means completeness, ensuring that the search engine returns everything on the web that could possibly be relevant. To the other it means speed, ensuring pretty good results very fast. Their efforts will not be fully aligned or coordinated until the goal is made concrete.\n\nWhat makes people believe ideas? We base it on authorities \u2014 our parents, traditions, experts, etc. If one can bring in a true authority then the problem of credibility is easily solved, but what if we cannot? This chapter focuses on how to create credibility when you don\u2019t have such authority figures.\n\nThere are several ways to do this: (1) Use an anti-authority, (2) Use concrete details, (3) Use statistics, (4) Use something called the Sinatra Test, and (5) Use testable credentials.\n\nSo how do we make people care about our messages? The good news is that to make people care we don\u2019t have to produce emotion from an absence of emotion. The most basic way to make people care is to form an association between something they don\u2019t yet care about and something they do care about; something that matters to them.\n\nAnd what matters to people? People matter to themselves. It will come as no surprise that one reliable way of making people care is by invoking self-interest.\n\nTo make people care about ideas we get them to:\n\nOften people make decision not in a rational way \u2014 write down all alternatives and look at pluses and minuses \u2014 but instead they make them based on identity. They ask questions like: Who am I? What kind of situation is this? And what do people like me to do in this type of situation?\n\nIt\u2019s well-known that a good story is very sticky. The power of a good story is that it provides inspiration. It moves people to take action.\n\nA key to making an idea sticky is to tell it as a story. Stories encourage a kind of mental simulation or reenactment on the part of the listener that burns the idea into the mind.\n\nThe hard part about using a story is creating it. The authors share the 3 major types of stories to look for.\n\nHere\u2019s how a story helps rid one of the Curse of Knowledge. When explaining how to solve problems someone might say \u201cKeep the lines of communication open.\u201d They are hearing in their heads a song filled with passion and emotion.\n\nThey\u2019re remembering the experience that taught them those lessons \u2014 the struggles, the political battles, the missteps, the pain. They need to share the story of their trials.\n\nIn fact, stories usually automatically meet other criteria for making ideas sticky: They are almost always concrete, they are often emotional and have unexpected elements. The real difficulty is to be sure they are simple enough.\n\nSticky ideas shared certain traits that made them more likely to succeed and be remembered by people. Here are the 6 principles again:\n\nApply these rules to make your own messages \u201cstick\u201d at your own pleasure."
    },
    {
        "url": "https://towardsdatascience.com/the-10-deep-learning-methods-ai-practitioners-need-to-apply-885259f402c1?source=user_profile---------29----------------",
        "title": "The 10 Deep Learning Methods AI Practitioners Need to Apply",
        "text": "Interest in machine learning has exploded over the past decade. You see machine learning in computer science programs, industry conferences, and the Wall Street Journal almost daily. For all the talk about machine learning, many conflate what it can do with what they wish it could do. Fundamentally, machine learning is using algorithms to extract information from raw data and represent it in some type of model. We use this model to infer things about other data we have not yet modeled.\n\nNeural networks are one type of model for machine learning; they have been around for at least 50 years. The fundamental unit of a neural network is a node, which is loosely based on the biological neuron in the mammalian brain. The connections between neurons are also modeled on biological brains, as is the way these connections develop over time (with \u201ctraining\u201d).\n\nIn the mid-1980s and early 1990s, many important architectural advancements were made in neural networks. However, the amount of time and data needed to get good results slowed adoption, and thus interest cooled. In the early 2000s, computational power expanded exponentially and the industry saw a \u201cCambrian explosion\u201d of computational techniques that were not possible prior to this. Deep learning emerged from that decade\u2019s explosive computational growth as a serious contender in the field, winning many important machine learning competitions. The interest has not cooled as of 2017; today, we see deep learning mentioned in every corner of machine learning.\n\nTo get myself into the craze, I took Udacity\u2019s \u201cDeep Learning\u201d course, which is a great introduction to the motivation of deep learning and the design of intelligent systems that learn from complex and/or large-scale datasets in TensorFlow. For the class projects, I used and developed neural networks for image recognition with convolutions, natural language processing with embeddings and character based text generation with Recurrent Neural Network / Long Short-Term Memory. All the code in Jupiter Notebook can be found on this GitHub repository.\n\nHere is an outcome of one of the assignments, a t-SNE projection of word vectors, clustered by similarity.\n\nMost recently, I have started reading academic papers on the subject. From my research, here are several publications that have been hugely influential to the development of the field:\n\nThere is an abundant amount of great knowledge about deep learning I have learnt via research and learning. Here I want to share the 10 powerful deep learning methods AI engineers can apply to their machine learning problems. But first of all, let\u2019s define what deep learning is. Deep learning has been a challenge to define for many because it has changed forms slowly over the past decade. To set deep learning in context visually, the figure below illustrates the conception of the relationship between AI, machine learning, and deep learning.\n\nThe field of AI is broad and has been around for a long time. Deep learning is a subset of the field of machine learning, which is a subfield of AI. The facets that differentiate deep learning networks in general from \u201ccanonical\u201d feed-forward multilayer networks are as follows:\n\nWhen I say \u201cmore neurons\u201d, I mean that the neuron count has risen over the years to express more complex models. Layers also have evolved from each layer being fully connected in multilayer networks to locally connected patches of neurons between layers in Convolutional Neural Networks and recurrent connections to the same neuron in Recurrent Neural Networks (in addition to the connections from the previous layer).\n\nDeep learning then can be defined as neural networks with a large number of parameters and layers in one of four fundamental network architectures:\n\nIn this post, I am mainly interested in the latter 3 architectures. A Convolutional Neural Network is basically a standard neural network that has been extended across space using shared weights. CNN is designed to recognize images by having convolutions inside, which see the edges of an object recognized on the image. A Recurrent Neural Network is basically a standard neural network that has been extended across time by having edges which feed into the next time step instead of into the next layer in the same time step. RNN is designed to recognize sequences, for example, a speech signal or a text. It has cycles inside that implies the presence of short memory in the net. A Recursive Neural Network is more like a hierarchical network where there is really no time aspect to the input sequence but the input has to be processed hierarchically in a tree fashion. The 10 methods below can be applied to all of these architectures.\n\nBack-prop is simply a method to compute the partial derivatives (or gradient) of a function, which has the form as a function composition (as in Neural Nets). When you solve an optimization problem using a gradient-based method (gradient descent is just one of them), you want to compute the function gradient at each iteration.\n\nFor a Neural Nets, the objective function has the form of a composition. How do you compute the gradient? There are 2 common ways to do it: (i) Analytic differentiation. You know the form of the function. You just compute the derivatives using the chain rule (basic calculus). (ii) Approximate differentiation using finite difference. This method is computationally expensive because the number of function evaluation is O(N), where N is the number of parameters. This is expensive, compared to analytic differentiation. Finite difference, however, is commonly used to validate a back-prop implementation when debugging.\n\nAn intuitive way to think of Gradient Descent is to imagine the path of a river originating from top of a mountain. The goal of gradient descent is exactly what the river strives to achieve \u2014 namely, reach the bottom most point (at the foothill) climbing down from the mountain.\n\nNow, if the terrain of the mountain is shaped in such a way that the river doesn\u2019t have to stop anywhere completely before arriving at its final destination (which is the lowest point at the foothill, then this is the ideal case we desire. In Machine Learning, this amounts to saying, we have found the global mimimum (or optimum) of the solution starting from the initial point (top of the hill). However, it could be that the nature of terrain forces several pits in the path of the river, which could force the river to get trapped and stagnate. In Machine Learning terms, such pits are termed as local minima solutions, which is not desirable. There are a bunch of ways to get out of this (which I am not discussing).\n\nGradient Descent therefore is prone to be stuck in local minimum, depending on the nature of the terrain (or function in ML terms). But, when you have a special kind of mountain terrain (which is shaped like a bowl, in ML terms this is called a Convex Function), the algorithm is always guaranteed to find the optimum. You can visualize this picturing a river again. These kind of special terrains (a.k.a convex functions) are always a blessing for optimization in ML. Also, depending on where at the top of the mountain you initial start from (ie. initial values of the function), you might end up following a different path. Similarly, depending on the speed at the river climbs down (ie. the learning rate or step size for the gradient descent algorithm), you might arrive at the final destination in a different manner. Both of these criteria can affect whether you fall into a pit (local minima) or are able to avoid it.\n\nAdapting the learning rate for your stochastic gradient descent optimization procedure can increase performance and reduce training time. Sometimes this is called learning rate annealing or adaptive learning rates. The simplest and perhaps most used adaptation of learning rate during training are techniques that reduce the learning rate over time. These have the benefit of making large changes at the beginning of the training procedure when larger learning rate values are used, and decreasing the learning rate such that a smaller rate and therefore smaller training updates are made to weights later in the training procedure. This has the effect of quickly learning good weights early and fine tuning them later.\n\nTwo popular and easy to use learning rate decay are as follows:\n\nDeep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem.\n\nThe key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \u201cthinned\u201d networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single untwined network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. Dropout has been shown to improve the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark datasets.\n\nMax pooling is a sample-based discretization process. The object is to down-sample an input representation (image, hidden-layer output matrix, etc.), reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.\n\nThis is done in part to help over-fitting by providing an abstract form of the representation. As well, it reduces the computational cost by reducing the number of parameters to learn and provides basic translation invariance to the internal representation. Max pooling is done by applying a max filter to usually non-overlapping subregions of the initial representation.\n\nNaturally, neural networks including deep networks require careful tuning of weight initialization and learning parameters. Batch normalization helps relaxing them a little.\n\nDuring back-propagation, these phenomena causes distraction to gradients, meaning the gradients have to compensate the outliers, before learning the weights to produce required outputs. This leads to the requirement of extra epochs to converge.\n\nBatch normalization regularizes these gradient from distraction to outliers and flow towards the common goal (by normalizing them) within a range of the mini batch.\n\nLearning rate problem: Generally, learning rates are kept small, such that only a small portion of gradients corrects the weights, the reason is that the gradients for outlier activations should not affect learned activations. By batch normalization, these outlier activations are reduced and hence higher learning rates can be used to accelerate the learning process.\n\nA LSTM network has the following three aspects that differentiate it from an usual neuron in a recurrent neural network:\n\nThe beauty of the LSTM is that it decides all this based on the current input itself. So if you take a look at the following diagram:\n\nThe input signal x(t) at the current time stamp decides all the above 3 points. The input gate takes a decision for point 1. The forget gate takes a decision on point 2 and the output gate takes a decision on point 3. The input alone is capable of taking all these three decisions. This is inspired by how our brains work and can handle sudden context switches based on the input.\n\nThe goal of word embedding models is to learn a high-dimensional dense representation for each vocabulary term in which the similarity between embedding vectors shows the semantic or syntactic similarity between the corresponding words. Skip-gram is a model for learning word embedding algorithms.\n\nThe main idea behind the skip-gram model (and many other word embedding models) is as follows: Two vocabulary terms are similar, if they share similar context.\n\nIn other words, assume that you have a sentence, like \u201ccats are mammals\u201d. If you use the term \u201cdogs\u201d instead of \u201ccats\u201d, the sentence is still a meaningful sentence. So in this example, \u201cdogs\u201d and \u201ccats\u201d can share the same context (i.e., \u201care mammals\u201d).\n\nBased on the above hypothesis, you can consider a context window (a window containing k consecutive terms. Then you should skip one of these words and try to learn a neural network that gets all terms except the one skipped and predicts the skipped term. Therefore, if two words repeatedly share similar contexts in a large corpus, the embedding vectors of those terms will have close vectors.\n\nIn natural language processing problems, we want to learn to represent each word in a document as a vector of numbers such that words that appear in similar context have vectors that are close to each other. In continuous bag of words model, the goal is to be able to use the context surrounding a particular word and predict the particular word.\n\nWe do this by taking lots and lots of sentences in a large corpus and every time we see a word, we take the surrounding word. Then we input the context words to a neural network and predict the word in the center of this context.\n\nWhen we have thousands of such context words and the center word, we have one instance of a dataset for the neural network. We train the neural network and finally the encoded hidden layer output represents the embedding for a particular word. It so happens that when we train this over a large number of sentences, words in similar context get similar vectors.\n\nLet\u2019s think about how an image would run through a Convolutional Neural Networks. Say you have an image, you apply convolution to it, and you get combinations of pixels as outputs. Let\u2019s say they\u2019re edges. Now apply convolution again, so now your output is combinations of edges\u2026 or lines. Now apply convolution again, so your output is combinations of lines and so on. You can think of it as each layer looking for a specific pattern. The last layer of your neural network tends to get very specialized. Perhaps if you were working on ImageNet, your networks last layer would be looking for children or dogs or airplanes or whatever. A couple layers back you might see the network looking for eyes or ears or mouth or wheels.\n\nEach layer in a deep CNN progressively builds up higher and higher level representations of features. The last couple layers tend to be specialized on whatever data you fed into the model. On the other hand, the early layers are much more generic, there are many simple patterns common among a much larger class of pictures.\n\nTransfer learning is when you take a CNN trained on one dataset, chop off the last layer(s), retrain the models last layer(s) on a different dataset. Intuitively, you\u2019re retraining the model to recognized different higher level features. As a result, training time gets cut down a lot so transfer learning is a helpful tool when you don\u2019t have enough data or if training takes too much resources.\n\nThis article only shows the general overview of these methods. I suggest reading the articles below for more detailed explanations:\n\nDeep Learning is strongly technique-focused. There are not much concrete explanations for each of the new ideas. Most new ideas came out with experimental results attached to prove that they work. Deep Learning is like playing LEGO. Mastering LEGO is as challenging as any other arts, but getting into it is easier.\n\n\u2014 \u2014 \n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://towardsdatascience.com/the-10-statistical-techniques-data-scientists-need-to-master-1ef6dbd531f7?source=user_profile---------30----------------",
        "title": "The 10 Statistical Techniques Data Scientists Need to Master",
        "text": "Regardless of where you stand on the matter of Data Science sexiness, it\u2019s simply impossible to ignore the continuing importance of data, and our ability to analyze, organize, and contextualize it. Drawing on their vast stores of employment data and employee feedback, Glassdoor ranked Data Scientist #1 in their 25 Best Jobs in America list. So the role is here to stay, but unquestionably, the specifics of what a Data Scientist does will evolve. With technologies like Machine Learning becoming ever-more common place, and emerging fields like Deep Learning gaining significant traction amongst researchers and engineers \u2014 and the companies that hire them \u2014 Data Scientists continue to ride the crest of an incredible wave of innovation and technological progress.\n\nWhile having a strong coding ability is important, data science isn\u2019t all about software engineering (in fact, have a good familiarity with Python and you\u2019re good to go). Data scientists live at the intersection of coding, statistics, and critical thinking. As Josh Wills put it, \u201cdata scientist is a person who is better at statistics than any programmer and better at programming than any statistician.\u201d I personally know too many software engineers looking to transition into data scientist and blindly utilizing machine learning frameworks such as TensorFlow or Apache Spark to their data without a thorough understanding of statistical theories behind them. So comes the study of statistical learning, a theoretical framework for machine learning drawing from the fields of statistics and functional analysis.\n\nWhy study Statistical Learning? It is important to understand the ideas behind the various techniques, in order to know how and when to use them. One has to understand the simpler methods first, in order to grasp the more sophisticated ones. It is important to accurately assess the performance of a method, to know how well or how badly it is working. Additionally, this is an exciting research area, having important applications in science, industry, and finance. Ultimately, statistical learning is a fundamental ingredient in the training of a modern data scientist. Examples of Statistical Learning problems include:\n\nIn my last semester in college, I did an Independent Study on Data Mining. The class covers expansive materials coming from 3 books: Intro to Statistical Learning (Hastie, Tibshirani, Witten, James), Doing Bayesian Data Analysis (Kruschke), and Time Series Analysis and Applications (Shumway, Stoffer). We did a lot of exercises on Bayesian Analysis, Markov Chain Monte Carlo, Hierarchical Modeling, Supervised and Unsupervised Learning. This experience deepens my interest in the Data Mining academic field and convinces me to specialize further in it. Recently, I completed the Statistical Learning online course on Stanford Lagunita, which covers all the material in the Intro to Statistical Learning book I read in my Independent Study. Now being exposed to the content twice, I want to share the 10 statistical techniques from the book that I believe any data scientists should learn to be more effective in handling big datasets.\n\nBefore moving on with these 10 techniques, I want to differentiate between statistical learning and machine learning. I wrote one of the most popular Medium posts on machine learning before, so I am confident I have the expertise to justify these differences:\n\nIn statistics, linear regression is a method to predict a target variable by fitting the best linear relationship between the dependent and independent variable. The best fit is done by making sure that the sum of all the distances between the shape and the actual observations at each point is as small as possible. The fit of the shape is \u201cbest\u201d in the sense that no other position would produce less error given the choice of shape. 2 major types of linear regression are Simple Linear Regression and Multiple Linear Regression. Simple Linear Regression uses a single independent variable to predict a dependent variable by fitting a best linear relationship. Multiple Linear Regression uses more than one independent variable to predict a dependent variable by fitting a best linear relationship.\n\nPick any 2 things that you use in your daily life and that are related. Like, I have data of my monthly spending, monthly income and the number of trips per month for the last 3 years. Now I need to answer the following questions:\n\nClassification is a data mining technique that assigns categories to a collection of data in order to aid in more accurate predictions and analysis. Also sometimes called a Decision Tree, classification is one of several methods intended to make the analysis of very large datasets effective. 2 major Classification techniques stand out: Logistic Regression and Discriminant Analysis.\n\nLogistic Regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables. Types of questions that a logistic regression can examine:\n\nIn Discriminant Analysis, 2 or more groups or clusters or populations are known a priori and 1 or more new observations are classified into 1 of the known populations based on the measured characteristics. Discriminant analysis models the distribution of the predictors X separately in each of the response classes, and then uses Bayes\u2019 theorem to flip these around into estimates for the probability of the response category given the value of X. Such models can either be linear or quadratic.\n\nResampling is the method that consists of drawing repeated samples from the original data samples. It is a non-parametric method of statistical inference. In other words, the method of resampling does not involve the utilization of the generic distribution tables in order to compute approximate p probability values.\n\nResampling generates a unique sampling distribution on the basis of the actual data. It uses experimental methods, rather than analytical methods, to generate the unique sampling distribution. It yields unbiased estimates as it is based on the unbiased samples of all the possible results of the data studied by the researcher. In order to understand the concept of resampling, you should understand the terms Bootstrapping and Cross-Validation:\n\nUsually for linear models, ordinary least squares is the major criteria to be considered to fit them into the data. The next 3 methods are the alternative approaches that can provide better prediction accuracy and model interpretability for fitting linear models.\n\nThis approach identifies a subset of the p predictors that we believe to be related to the response. We then fit a model using the least squares of the subset features.\n\nThis approach fits a model involving all p predictors, however, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This shrinkage, aka regularization has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the coefficients may be estimated to be exactly zero. Thus this method also performs variable selection. The two best-known techniques for shrinking the coefficient estimates towards zero are the ridge regression and the lasso.\n\nDimension reduction reduces the problem of estimating p + 1 coefficients to the simple problem of M + 1 coefficients, where M < p. This is attained by computing M different linear combinations, or projections, of the variables. Then these M projections are used as predictors to fit a linear regression model by least squares. 2 approaches for this task are principal component regression and partial least squares.\n\nIn statistics, nonlinear regression is a form of regression analysis in which observational data are modeled by a function which is a nonlinear combination of the model parameters and depends on one or more independent variables. The data are fitted by a method of successive approximations. Below are a couple of important techniques to deal with nonlinear models:\n\nTree-based methods can be used for both regression and classification problems. These involve stratifying or segmenting the predictor space into a number of simple regions. Since the set of splitting rules used to segment the predictor space can be summarized in a tree, these types of approaches are known as decision-tree methods. The methods below grow multiple trees which are then combined to yield a single consensus prediction.\n\nSVM is a classification technique that is listed under supervised learning models in Machine Learning. In layman\u2019s terms, it involves finding the hyperplane (line in 2D, plane in 3D and hyperplane in higher dimensions. More formally, a hyperplane is n-1 dimensional subspace of an n-dimensional space) that best separates two classes of points with the maximum margin. Essentially, it is a constrained optimization problem where the margin is maximized subject to the constraint that it perfectly classifies the data (hard margin).\n\nThe data points that kind of \u201csupport\u201d this hyperplane on either sides are called the \u201csupport vectors\u201d. In the above picture, the filled blue circle and the two filled squares are the support vectors. For cases where the two classes of data are not linearly separable, the points are projected to an exploded (higher dimensional) space where linear separation may be possible. A problem involving multiple classes can be broken down into multiple one-versus-one or one-versus-rest binary classification problems.\n\nSo far, we only have discussed supervised learning techniques, in which the groups are known and the experience provided to the algorithm is the relationship between actual entities and the group they belong to. Another set of techniques can be used when the groups (categories) of data are not known. They are called unsupervised as it is left on the learning algorithm to figure out patterns in the data provided. Clustering is an example of unsupervised learning in which different data sets are clustered into groups of closely related items. Below is the list of most widely used unsupervised learning algorithms:\n\nThis was a basic run-down of some basic statistical techniques that can help a data science program manager and or executive have a better understanding of what is running underneath the hood of their data science teams. Truthfully, some data science teams purely run algorithms through python and R libraries. Most of them don\u2019t even have to think about the math that is underlying. However, being able to understand the basics of statistical analysis gives your teams a better approach. Have insight into the smallest parts allows for easier manipulation and abstraction. I hope this basic data science statistical guide gives you a decent understanding!\n\nP.S: You can get all the lecture slides and RStudio sessions from my GitHub source code here. Thanks for the overwhelming response!\n\n\u2014 \u2014 \n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/. You can also follow me on Twitter, email me directly or find me on LinkedIn."
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/top-10-liberal-arts-skills-you-need-to-succeed-in-tech-429a24ade2ed?source=user_profile---------31----------------",
        "title": "Top 10 Liberal Arts Skills You Need to Succeed in Tech",
        "text": "Top 10 Liberal Arts Skills You Need to Succeed in Tech\n\nRecently I\u2019ve been thinking a lot about the value of my undergraduate education. Nearly 5 years ago, senior year of high school, I had to make some important decisions to choose which colleges to apply to. At the time, I went to a small independent boarding school and I absolutely loved the tight-knit community and friendly vibe of a small school. In addition to the fact that I would need financial aid to fund my education, liberal arts colleges come as no-brainer choice. Out of the 18 colleges that I applied to, 14 of them are liberal arts. I got accepted to 6, and ended up choosing Denison University, a small college in rural Ohio. 4 years later and I was in the process of applying for my first job. I have known that I want to work in tech since freshman year. All the things I did in and out of college have been supplementary to this dream. However, it was tough. And I believe it was tough because of the liberal arts education I received. Tech companies normally prefer the typical candidate profile of someone going to schools in California or the East Coast, majoring in Computer Science, have done a couple of engineering internships, and being an Algorithms junkie who can pass the technical interviews. I had most of these criteria, except for the school name part. It\u2019s almost impossible to stand out amongst piles of resume for entry-level engineering / analytical roles when you went to a liberal arts school in the Midwest that hardly anyone in tech knows about. Thus, I had to figure out to stand out through other ways \u2014 networking, online presence, side projects etc. There have been many times over the past few years when I doubted the real value of my liberal arts education. Was it worth it? Was I satisfied academically? Have I grown as a person? Did it provide enough skills for me to succeed in the tech world? Then I realized I have already answered these questions over the past 4 years at Denison. Classes I took, people I met, organizations I led, experience I made\u2026 all have provided the skills, knowledge, and relationships I need to succeed in the tech sector. The purpose of this post is for me to reflect on my education and draw the connections between liberal arts skills and their application to the tech industry. Hopefully reading this post will inspire you to pursue being liberally educated in life in general. Liberally educated people know how to pay attention \u2014 to others and to the world around them. They work hard to hear what other people say. They can follow an argument, track logical reasoning, detect illogic, hear the emotions that lie behind both the logic and the illogic, and ultimately empathize with the person who is feeling those emotions.\n\nI first got a chance to practice active listening after participating in a student organization called Sustained Dialogue. The Sustained Dialogue Institute is a national organization whose mission is to help people transform conflictual relationships and design change processes around the world. One of its initiative is the Sustained Dialogue Campus Network, which involves students from dozens of campuses in 12 countries who work to improve intergroup relations and campus climates. The focus of any Sustained Dialogue program is relationship building across lines of difference and facilitating honest dialogue between students, faculty, staff, and administrators. Denison is one of the participating schools in this network and is an active chapter with more than 50 members. The way it works is that members are assigned into different clusters so that there are diverse representations of ethnic, educational and socio-economic backgrounds. Each cluster then meets up every week to discuss a topic that sparks the group\u2019s interest, facilitated by 2 moderators. I remember having discussions about a variety of topics that are relevant to our campus such as college social scenes, gender and cultural diversity, academic pressure, career orientation, political climate etc. Every discussion initiates participants\u2019 emotional engagement, critical thinking, and social empathy. Because dialogue is defined as \u201clistening deeply enough to be changed by what you learn\u201d, everyone is encouraged to practice active listening and \u201cstep into others\u2019 shoes.\u201d After every discussion like that, I always felt so enlightened by others\u2019 ideology and touched by their emotions. Being a part of Sustained Dialogue throughout freshman and sophomore year ultimately transformed me to become a much better listener and empathizer. If you work in tech, you probably have heard of design thinking. It is a methodology used to solve complex problems and to find desirable solutions. In realm of design, design thinking is rooted in empathy, where you try to see from the perspective of a user of a given design or product. There is a lot more to design thinking than that, but in a nutshell it is about human-centered design where empathy is king. If you want to design better software products, you need to learn to listen and hear \u2014 to truly be empathetic. Empathy takes you from designing for utility and convenience, to designing for meaning and value in the context of people\u2019s lives. \u201cIt\u2019s not \u2018us versus them\u2019 or even \u2018us on behalf of them.\u2019 For a design thinker, it has to be \u2018us with them.\u2019 \u2014 Tim Brown, CEO and President of IDEO Liberally educated people are skilled readers. They know how to read far more than just words. They are moved by what they see in a great art museum and what they hear in a concert hall. They recognize extraordinary athletic achievements; they are engaged by classic and contemporary works of theater and cinema; they find in television a valuable window on popular culture. When they wander through a forest or a wetland or a desert, they can identify the wildlife and interpret the lay of the land. They can glance at a farmer\u2019s field and tell the difference between soy beans and alfalfa. They can recog\u00adnize fine craftsmanship, whether by a cabinetmaker or an auto mechanic. And they can surf the World Wide Web. All of these are ways in which the eyes and the ears are attuned to the wonders that make up the human and the natural worlds. None of us can possibly master all these forms of \u201creading,\u201d but liberally educated people should be competent in many of them and curious about all of them.\n\nMost liberal arts curriculum requires students to fulfill General Education requirements. At Denison, students must take at least 2 courses from each of Fine Arts, Humanities, Social Sciences, and Natural Sciences academic areas. I took full advantage of this opportunity to enroll in classes that sound interesting to me: I took an Acting class in which I learned the art of improvisation and theatrical expression. I took a Cinema class in which I learned how to shoot and edit videos, ending up making a short movie. I took a Philosophy class and learned Aristotle and Plato\u2019s contemplation about the meaning of life. I took an East Asian History class and traced the roots of civilization in China, Japan, and Korea centuries back. I took a Political Science class and performed statistical analysis to make sense of political phenomena. I took an Accounting class and understood the lingo of terms like balance sheet, assets, liabilities etc. I took a Chemistry class and knew instinctively that I would never want to pursue life sciences as a career. I took Multivariate Calculus and figured that I am quite decent at Math. All these courses are outside of my majors, but they broaden my education and expose me to knowledge that I would not have encountered otherwise. I was forced to read different materials and understand different perspectives. My point is this: Liberal arts education makes me insanely curious. As it turns out, tech pioneers tend to be voracious readers, and they like to apply what they read. Want some proof: Bill Gates reads about 50 books per year, which breaks down to 1 per week. Elon Musk is an avid reader and when asked how he learned to build rockets, he said \u201cI read books.\u201d Mark Zuckerberg resolved to read a book every 2 week throughout 2015. Thus, being insanely curious and making reading a major part of your daily lifestyle will surely pave the gateway for you to succeed in the tech. Start out with this list of book recommendations from the smartest people shaping the tech industry. \u201cFollowing your genuine intellectual curiosity is a better foundation for a career than following whatever is making money right now.\u201d \u2014 Naval Ravikant, Co-Founder of AngelList Liberally educated people know how to talk. They can give a speech, ask thoughtful questions, and make people laugh. They can hold a conversation with a high school dropout or a Nobel laureate, a child or a nursing-home resident, a factory worker or a corporate resident. Moreover, they participate in such conversations not because they like to talk about themselves but because they are genuinely interested in others.\n\nIt\u2019s very easy to get over-involved in a liberal arts environment, especially if you are an active and curious person. A clear side benefit is that you will meet a lot of people with different interests. There are more than 175 student organizations at Denison when I was a student there, and I was part of many of those. To name a few: The Student Government, where people are politically active and passionate about making change. Greek Life, where brotherhood is formed and friends become family. Career Advisory Board, where upperclassmen leverage their professional network to help underclassmen with their career orientation. Leadership Honorary Society, where student leaders congregate and brainstorm best leadership practices. International Student Community, where cultural diversity is celebrated and people are at their most authentic self. Do you know what the most influential people in tech have in common? They are insanely well-connected. They know how to become a genuine and highly-connective networker, they know how to propel their career forward with each interaction (while doing the same for others), and they know how to regularly measure their performance in this area so it becomes a competitive advantage. In essence, they know how to talk to anyone. A prime example is Reid Hoffman, one of the best-connected people in Silicon Valley, whose network ranges from his former PayPal boos and fellow billionaire Peter Thiel to actress and entrepreneur Eva Longoria to President Barack Obama. It makes sense that he\u2019s the founder and chairman of LinkedIn, the world\u2019s biggest professional networking site. \u201cThere are more smart people in the world who do not work at your company than the total number of smart people who work at your company. So look beyond your office. If you do\u2026 Your team becomes a whole lot bigger.\u201d \u2014 Reid Hoffman, Founder of LinkedIn and Executive VP of PayPal 4 \u2014 Write Clearly and Persuasively and Movingly Liberally educated people know the craft of putting words on paper. It is about expressing what is in their minds and hearts so as to teach, persuade, and move the person who reads their words. It is about writing as a form of touching, akin to the touching that happens in an exhilarating conversation.\n\nMost liberal arts curriculum have a strong emphasis on writing. In fact, at Denison, every student is required to complete 2 writing seminars during the first year. As someone whose English is the second language, I was determined to take as many writing-intensive courses as possible to improve my English. By chance, my assigned academic advisor is an Associate Professor in the Communication department, and he encouraged me to try out a Communication class. I took one my freshman year, fell in love with it, and declared Communication as one of my majors. What I really enjoy about the Communication courses offered at Denison include active class discussion, various personalities of the students who take them, a wide range of interesting themes and topics, and most of all, a heavy emphasis on written communication. I have written a lot of research papers throughout my time in college \u2014 from exploring the role of technological advancement in athletic and sports development, to analyzing Hillary Clinton\u2019s and Donald Trump\u2019s campaign websites and gauging their impacts on the 2016 Presidential Election, from criticizing the biases of Google\u2019s PageRank Search algorithm, to examining the importance of human-centered design in the context of environmental psychology. The more I write; the more persuasive my writing skills become. When it comes to finding work, it can sometimes be difficult to come across new exciting opportunities. However, it\u2019s much easier if you can get people to come to you, or at least make it easier for people to get an insight into what you do and how you work. Writing is a great way to do this, as it can help you build an online presence and become part of a community in the process. Especially if you want to enter the tech industry as a designer, writing regularly will be very conducive to your career. Here are some examples of very well-known writers in the design industry: Julio Zhuo (VP of Product Design at Facebook) had a resolution for 2013: to write. And here is the article where she reflects on that specific year of writing. Today, Julie\u2019s articles are followed by hundreds of thousands of people, and she now even has a unique weekly essay, answering questions from members of the community. Tobias Van Schneider (Ex-VP of Design at Spotify) is another great example of designers who write. In his article, \u201cShould you write as a designer\u201d, Tobias explains how he first started out writing having never considered himself much of a writer. He goes onto say how beneficial writing has been to him, enabling him to think and communicate more clearly. Paul Jarvis (Vancouver-based Entrepreneur) launched a writing project called \u201cCreative Class\u201d, where he teaches freelancers about business, marketing and sales \u2014 things he had to learn himself as a designer. Moving forward, Paul then created webinars and launched 2 podcasts. He uses his newsletter to communicate ideas with a community of freelancers, designers, small business owners and also to promote his projects. \u201cWrite to learn how to write, and write to understand. Write to remember, to preserve the scrap of a voice in a particular age.\u201d \u2014 Julie Zhuo, Product Design VP at Facebook The ability to solve puzzles requires many skills, including a basic comfort with numbers, a familiarity with computers, and the recognition that many problems that appear to turn on questions of quality can in act be reinterpreted as subtle problems of quantity. These are the skills of the analyst, the manager, the engineer, the critic: the ability to look at a complicated reality, break it into pieces, and figure out how it works in order to do practical things in the real world. Part of the challenge in this, of course, is the ability to put reality back together again after having broken it into pieces \u2014 for only by doing so can we accomplish practical goals without violating the integrity of the world we are trying to change.\n\nStudying Computer Science has significantly made me a better problem-solver: from writing my first for loop to access an array in Python, to implementing a hash table data structure in C++, from programming a remote-controlled car hardware with Arduino, to leading a team to create an original RPG-game in Unity. Taking multiple Math classes also helped me a lot: from Calculus \u2014 where I differentiate and integrate multivariate functions to study high-dimensional systems that exhibit deterministic behavior; to Statistics \u2014 where I collect, organize interpret and present data; from Proofs \u2014 where I use deductive and inductive reasoning to justify logic; to Linear Algebra \u2014 where I study vector spaces and get interested in machine learning. Indeed, quantitative and technical skills are extremely valuable for liberal arts people to nurture a tech-oriented mindset. Software development is 100% about solving problems. Without problems, there wouldn\u2019t be a need for software. All software is designed to solve some user problem and within that general solution is a wide array of smaller problems that make it up. It really doesn\u2019t matter what programming language or technology you use, if you can\u2019t solve problems, you won\u2019t be very good at developing software. In order to hired as a developer, generally you need to pass difficult technical interviews, which test a developer\u2019s ability to solve problems. If you can solve problems, you will have a much greater level of success in the long run than you will in specializing in any particular technology. \u201cThe most successful, hardest-working people I know don\u2019t work hard because they\u2019re disciplined. They work hard because they\u2019re having fun solving a problem they really care about. They remind me of a dog chasing a tennis ball: their eyes go a little crazy and they plow through whatever gets in the way. It\u2019s not about punishing yourself \u2014 it\u2019s about finding your tennis ball, the thing that pulls you.\u201d \u2014 Drew Houston, CEO and Co-Founder of Dropbox 6 \u2014 Respect rigor not so much for its own sake but as a way of seeking truth Liberally educated people love learning, but they love wisdom more. They can appreciate a closely resonated argument without being unduly impressed by mere logic. They understand that knowledge serves values, and they strive to put these two \u2014 knowledge and values \u2014 into constant dialogue with each other. The ability to recognize true rigor is one of the most important achievements in any education, but it is worthless, even dangerous, if it is not placed in the service of some larger vision that also renders it humane.\n\nConnecting knowledge with values is all about entrepreneurship. You take what you learn and apply them to add value to others. One of the best practical experience I had at Denison is being part of Denison Enterprises, a student-led venture that serves as a think-tank, consultation group, and small business incubator. I got the opportunity to work on open-ended projects with a group of talented people, doing things such as: Conceiving of and building an app from start to finish Partnering with a non-profit or some other organization to solve a problem that they have Designing websites, mobile applications, service that aim to be successful in the marketplace Redesigning an existing process to be better or more efficient Tech leaders are entrepreneurial people: How do you know what to work on that\u2019s going to make the biggest difference for the people you\u2019re targeting? How do you size a market? How do you prioritize between a dozen ideas that look good on paper? How do you take technologies, people skills and resources and make something meaningful out of them? They respect the rigor of their work as a way of providing value to society. \u201cAs an entrepreneur, you have to feel like you can jump out of an airplane because you\u2019re confident that you\u2019ll catch a bird flying by. It\u2019s an act of stupidity, and most entrepreneurs go splat because the bird doesn\u2019t come by, but a few times it does. Most entrepreneurial ideas will sound crazy, stupid and uneconomic, and then they\u2019ll turn out to be right.\u201d \u2014 Reed Hastings, CEO and Co-Founder of Netflix This is another way of saying that they can understand the power of other people\u2019s dreams and nightmares as well as their own. They have the intellectual range and emotional generosity to step outside their own experiences and prejudices, thereby opening themselves to perspectives different from their own. From this commitment to tolerance flow all those aspects of a liberal education that oppose parochialism and celebrate the wider world: studying foreign languages, learning about the cultures of distant peoples, exploring the history of long-ago times, discovering the many ways in which men and women have known the sacred and given names to their gods. Without such encounters, we cannot learn how much people differ \u2014 and how much they have in common.\n\nThe epitome of my college education was my study abroad experience during junior year. As an international student in the US, I already have had a chance to experience the Western culture and contrasted that with my Eastern origin. However, I want to step out of my comfort zone even more and decided to spend a semester in Europe to experience a different environment. 4 months in Copenhagen truly opened my perspective and made me a more global citizen. Besides academics and cultural integration, the best part of this experience is the fact that I did a pretty extensive array of travel throughout Europe, visiting 17 cities and 12 countries. Each place I visited is unique, and I felt truly blessed to have experienced the magical feeling of seeing famed attractions multiple times: from the historical Plaza de Mayor in Madrid, to the magnificent Colosseum in Rome, from the beautiful Charles Bridge in Prague, to the gorgeous Eiffel Tower in Paris. For every hostel and Airbnb I stayed in, I got the opportunity to meet travelers and locals from diverse backgrounds, learning foreign cultures and understanding how much people differ. Overall, it was a humbling experience. Humility is one of the most important attributes for startup businesses, which are predominant in the tech sector. For startup leaders, humility is vital because it helps them recognize weaknesses in the company and what needs improvement to make the business run better and more efficiently. A humble leader is invested in the company\u2019s well-being and success through the right channels. As the company grows, a humble leader will try to grow and develop along with it. Humility is not just important in bosses, it is also vital for good employees. A large part of humility is maintaining a sense of gratitude, and employees of a startup business must realize what a great opportunity it is to work within a new company. It allows them space to share ideas, be part of the growth, and have room for promotions. One of the best books I\u2019ve read about the topic of humility in the workplace is Sheryl Sandberg\u2019s \u201cLean In.\u201d Openness to new ideas seem to be a central tenet of Sandberg\u2019s philosophy. Rather than being a single-minded and focused on only her vision, she is humble and works hard to gather honest and open feedback from everyone around her. \u201cThe ability to learn is the most important quality a leader can have.\u201d \u2014 Sheryl Sandberg, Chief Operating Officer at Facebook 8 \u2014 Understand how to get things done in the world Learning how to get things done in the world in order to leave it a better place is surely one of the most practical and important lessons we can take from our education. It is fraught with peril because the power to act in the world can so easily be abused \u2014 but we fool ourselves if we think we can avoid acting, avoid exercising power, avoid joining the world\u2019s fight. And so we study power and struggle to use it wisely and well.\n\nI learned quite a bit of power and struggle through doing hands-on community service in college. Winter breaks during freshman and sophomore year, I did a program called Break Away. Break Away is a national nonprofit organization that promotes the development of quality alternative break programs through training, assisting, and connecting campuses and communities. The organization has chapters on about 200 college campuses throughout the US. An alternative break consists of a group of college students who serve a community with a focus on a specific social issue for a specific amount of time. I went to a trip to Selma, Alabama my freshman year with a focus on civic engagement and another trip to Springfield, Missouri my sophomore year with a focus on social health. Ultimately, these trips made me an active citizen who is aware of social issues in the world and who commit to resolving those issues. The best startups are mission-driven, when they have a sense of purpose to solve big problems and leave enormous impact on the world. A few examples in the healthcare, government, and energy space: Freenome, headquartered in San Francisco, uses a patient\u2019s DNA, rather than a tissue sample, to diagnose cancer. Freenome says its tests do better than the current options for diagnosing prostate, breast, colorectal, and lung cancers. Clover Health, another healthcare unicorn, is an insurance start-up aiming to use data science to improve preventive medicine. The SF-based company tracks dozens of clinical and social data points to help elderly and low-income patients avoid hospital visits. London-based Babylon Health started as a telemedicine company, enabling doctors to make diagnoses via video and allowing patients to rate the quality of each interaction. Recently, it develops an AI-powered chatbot that analyzes a patient\u2019s condition against a database of symptoms, while incorporating the patient\u2019s own medical history and responses to the chatbot\u2019s questions. OpenGov has accomplished what top tier tech firms have struggled to do for years: It\u2019s taken the complexities of government finance and simplified them into easy-to-read charts. The startup has invented a cloud-based platform that enables officials and citizens to analyze budgets historically, by year and into the future. Open data visualization startup Appallicious is best known for its mobility services in disaster response. Municipalities can easily map their emergency resources and dangers in real time using the dashboard, while citizens can request assistance, first responders can update first aid locations and local businesses can advertise recovery services. German startup Sonnen develops and produces the smart storage system that has enabled more than 10,000 households worldwide, to meet their energy needs with self-produced renewable energy. Households can share their self-produced energy with other people and can become completely independent of conventional suppliers. SF-based Silicor Materials produces solar silicon and high performance multi-crystalline solar cells using a unique technology to reduce costs and improve efficiency. Lumos Global is an off-grid solar firm operating in Nigeria. The company\u2019s solar system provides affordable and accessible renewable electricity in communities that have limited or non-existent electricity access. \u201cIt\u2019s an obsession about the mission of the company. Employees believe something is wrong with the world if the company doesn\u2019t exist. Employees would rather be employees at this company than founders at a different company because the mission is that important and interesting.\u201d \u2014 Joe Lonsdale, Co-Founder of Palantir and Addepar 9 \u2014 Nurture and empower the people around self Liberally educated people understand that they belong to a community whose prosperity and well-being are crucial to their own, and they help that community flourish by making the success of others possible. If we speak of education for freedom, then one of the crucial insights of a liberal education must be that the freedom of the individual is possible only in a free community, and vice versa. It is the community that empowers the free individual, just as it is free individuals who lead and empower the community. The fulfillment of high talent, the just exercise of power, the celebration of human diversity: nothing so redeems these things as the recognition that what seem like personal triumphs are in fact the achievements of our common humanity.\n\nAn outstanding element of my college experience is the community aspect. The Denison community is unique and special, largely because of the relationships emerged between students, faculty, and staff. It is a place where people care about each other. It is a place that is filled with students who have integrity. It is a place that has an unusually large percentage of students with excellent leadership skills. Every member of the community commit to making it a place where people make good decisions for themselves, and where everybody steps up and intervenes when they see other members of the community getting ready to make a bad decision for themselves or for others. To quote our president Adam Weinberg, \u201cIn our best moments, members of the Denison community provoke each other. We inspire, demand and challenge each other to get out of our comfort zones, move away from our myopic views of the world and take a chance on believing we might have more to offer to ourselves, each other and the world than we think we do. We do self-discovery and excellence well.\u201d The quote resonated with me because it captured the spirit of my experience so well. A prime example of a community-driven tech company that is doing so well in the current sharing economy is Airbnb. The brand of the company is associated with a fundamental driver of humankind: Belong Anywhere. Airbnb stands for something much bigger than travel; it stands for community and relationships and using technology for the purpose of bringing people together. It is the one place people could go to meet the \u201cuniversal human yearning to belong.\u201d Belonging anywhere wasn\u2019t a single moment; it was a transformation people experienced when they traveled on Airbnb. It goes like this: When travelers leave their homes, they feel alone. They reach their Airbnb, and they feel accepted and taken care of by their host. They then feel safe to be the same kind of person they are when they\u2019re at home. I can attest to this as a traveler using Airbnb myself: belonging means driving around and enjoying New Year Eve with my Dallas host, having a drink in a local bar with my DC hosts, or getting an intro to deep-dish pizza eating with my host in Chicago. More than anything else, it is this aspect of Airbnb that distinguishes it from Uber, Lyft, TaskRabbit and any other of its sharing-economy peers. \u201cI traveled city by city and our hosts would get so excited to meet us and they would tell their friends. We tracked the stats and we saw hosts got more engaged after they had met us. Building a community worked for us.\u201d \u2014 Brian Chesky, Co-Founder and CEO of Airbnb Being a liberally educated person means being able to see connections that allow for one to make sense of the world and act within it in creative ways. Every one of the qualities described here \u2014 listening, reading, talking, writing, puzzle solving, truth seeking, seeing through other people\u2019s eyes, leading, working in a community \u2014 is finally about connecting. A liberal education is about gaining the power and the wisdom, the generosity and the freedom to connect.\n\nThe liberal arts education at Denison has made me a well-rounded person. Many friends of mine wonder why I chose to study Computer Science and Communication, two completely distinct majors. I have had this vision since freshman year. Studying Computer Science exercises my left-brain, which is more logical, analytical, and objective. Studying Communication, on the other hand, exercises my right-brain, which is more intuitive, thoughtful, and subjective. Normally, people prefer one type of thinking over another; but I find it equally enjoyable to utilize both systems. Matter of fact, I was able to connect the dots and bring many cross-over knowledge across these disciplines. In Computer Science courses, I could write engaging lab reports and articulate technical concepts fluently; while in Communication classes, I could develop solid rational arguments and contribute to the discussions from a logical frame. My dream career is to work as a Product Manager developing an Artificial Intelligence / Machine Learning-focused software technology. PMs are key members of the teams that build software products, working alongside software engineers and designers. They are responsible for setting the team\u2019s strategy, defining goals, determining the roadmap, representing the customer, and shepherding / cat-herding the team to a successful launch. Over the past couple of summers, I have spent significant amount of time learning about this discipline and having conversations with lots of Product Manager professionals. As a PM, you might spend your day talking to customers about the problems they have, analyzing data about how people are using your product and looking for opportunities, working with marketing to prepare for an upcoming launch, brainstorming new features, going over designs with a designer, discussing tradeoffs of a technical issue with engineers, leading a retrospective discussion with the team, testing early versions of the products before they go live, or planning a fun launch celebration. The Venn Diagram of PM skills include both technical skills and communication, along with other tangible (analysis & synthesis, product design, prioritization, action oriented) and abstract (customer focus, strategy & vision, project management, leadership) qualities. A successful PM know how to connect the dots between designing a great product and shipping it. \u201cYou can\u2019t connect the dots looking forward; you can only connect them looking backwards. So you have to trust that the dots will somehow connect in your future. You have to trust in something \u2014 your gut, destiny, life, karma, whatever. This approach has never let me down, and it has made all the difference in my life.\u201d \u2014 Steve Jobs, ex-CEO of Apple; creator of the Apple II, Macintosh, Pixar, iPod, iTunes, iPhone, iPad The future will be shaped by people who can embrace rapid change, thrive in diverse environments, and creatively problem solve as part of their everyday lives. Liberal arts skills \u2014 the ability to communicate, persevere, embrace ambiguity, work in diverse teams, and frame questions in ways that allow us to see vexing problems in new, solvable ways \u2014 are more important than ever given current global directions and trends. Being a Denison alumni makes me a better tech entrepreneur and, more crucially, makes me believe that I can make a difference in the world."
    },
    {
        "url": "https://medium.com/inside-product-management/introducing-redesign-for-yelp-android-4b64267b97c4?source=user_profile---------32----------------",
        "title": "Introducing Redesign for Yelp Android \u2013 Inside Product Management \u2013",
        "text": "Yelp has been largely successful but when you\u2019re scaling at such a quick rate, how can one manage a brand reputation and increase the retention of new users at the same time? Luckily, I\u2019d venture to say that Yelp can make some huge strides in these areas through a few design improvements!\n\nI\u2019ve been using Yelp for almost 4 years now. Typically, I use it to find coffee shops to do work during middle of the day, or when I\u2019m hungry and try to get to a restaurant I\u2019ve never been. While I\u2019ve heard a lot of different reviews on the service, I\u2019m continually impressed by its ease of use and exponential growth. After some research, it appears that others feel the same:\n\nUser review is an important part of the Yelp community, which provides a lot of detailed information that we can\u2019t see from the general rating: How is the service? Is it a good place to work? How pricey is it? Do I have to make a reservation? These are important factors for consumers to make decisions.\n\nUnfortunately, most people never wrote any reviews or make good use of reviews.\n\nIn the Yelp community, users play 2 roles: consumer and contributor.\n\nThe Search screen is one of the most used ones in the Yelp app, yet it doesn\u2019t seem to have an upgraded design. The old list view styles is completely inconsistent with the Nearby screen in terms of visual focus. Taking a look at the current search screen of the app, notice that the space here is not used efficiently. Users almost never scroll down to view other options.\n\nOne of the key things the search screen missed out on was an easily accessible category filter. The current filter screen is long and unfriendly. The definition of $, $$, $$$, $$$$ is not clear for everyone. Most of the users don\u2019t scroll down to see many of the small filter features.\n\nTo remove the cluttered list view, I redesigned the search screen similarly to the nearby screen. The category bar was the most space-consuming part of the search screen and can be compressed into a single-line icon bar. Different categories are now reorganized into a horizontal bar, which leaves more space for recent searched businesses. After tapping the search bar, you can see the main categories and the search history.\n\nThe new design of the filter greatly reduce the page length, and simplify the user flow. The filter screen now contains 2 sub-features: \u201cGeneral Settings\u201d and \u201cUser Reviews.\u201d\n\nThe current review screen looks intimidating for most users. Users aren\u2019t typically willing to write long reviews. Based on my conversations with a couple of Yelp users, majority of them never write any reviews. Additionally, some users want to add photos with the review. Photo and rating are actually top factors for users to make decisions.\n\nIn this new design, for those who don\u2019t like to write long text reviews, you can post your reviews by simply selecting and adding keywords. If you want to write reviews, there is a dedicated space for you to share. The new design also enables you to add photos in the same time.\n\nThe new design of your personal homepage looks like a microblog to organize and share your moments with Yelp (notice the Instagram-inspired layout), which provides a personal feel for customers.\n\nWith a refreshed look and feel that appreciates the use of whitespace, the redesigned Yelp app provides a richer experience for users to contribute and consume reviews. It finds a middle ground between familiarity and stand-out interfaces, while provides the same exact functionality, making no compromises on the product itself.\n\nFor the Frontend UI Design, as Yelp Android app is written completely in Java, developers can simply rely on pre-built UI components such as structured layout objects and UI controls from Android Material Design Guidelines.\n\nFor the Backend Development, to provide the feature of reviews and ratings, it is important to have a server that will store all necessary information. One way to accomplish this is to have unique Google identifiers and track them with comments and reviews.\n\nThe reviews filter algorithm can be implemented based on these factors:\n\nIt is critical to make this algorithm as unbiased as possible because it directly impacts Yelp user\u2019s willingness to leave future reviews as well as the business reputation.\n\nThe goal of this new design for Yelp mobile app is to encourage users to leave more reviews about local businesses, which greatly affects the decision made by other users about those businesses. Therefore, here are what to be expected to gauge the success of the new design (from highest to lowest priority):"
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/the-6-productivity-strategies-to-integrate-deep-work-into-your-professional-lives-5af06bfab33c?source=user_profile---------33----------------",
        "title": "The 6 Productivity Strategies to Integrate Deep Work into Your Professional Lives",
        "text": "The 6 Productivity Strategies to Integrate Deep Work into Your Professional Lives\n\nIt has been an extremely busy first month of summer for me: Practicing for technical interviews, working on coding assignments, learning new programming languages, crafting cover letters, rocking daily job applications etc.\u2026 The process forces me to embrace a productivity strategy so that I can maximize the time available and get the most amount of work done. To assist with that process, once again I rely on books. One of the most relevant titles that I have read so far about productivity is Cal Newport\u2019s \u201cDeep Work.\u201d I resonate with this book because programming, which is the bulk of work I\u2019m doing now, is a highly intensive and problem-focused activity that requires deep concentration. What is deep work? Cal defines the term as: \u201cDeep work is the ability to focus without distraction on a cognitively demanding task. It\u2019s a skill that allows you to quickly master complicated information and produce better results in less time. Deep work will make you better at what you do and provide the sense of true fulfillment that comes from craftsmanship. In short, deep work is like a super power in our increasingly competitive 21st-century economy.\u201d Sounds easy, isn\u2019t it? Once you accept that deep work is valuable, isn\u2019t it enough to just start doing more of it? Unfortunately, when it comes to replacing distraction with focus, matters are not so simple. You have a finite amount of willpower that becomes depleted as you use it. Cal states that \u201cthe key to developing a deep work habit is to move beyond good intentions and add routines and rituals to your working life designed to minimize the amount of your limited willpower necessary to transition into and maintain a state of unbroken concentration.\u201d The 6 strategies from the book that follow can be understood as an arsenal of routines and rituals designed with the science of limited willpower in mind to maximize the amount of deep work you consistently accomplish in your schedule. Among other things, they\u2019ll ask you to commit to a particular pattern for scheduling this work and develop rituals to sharpen your concentration before starting each session. Some of these strategies will deploy simple heuristics to hijack your brain\u2019s motivation center while others are designed to recharge your willpower reserves at the fastest possible rate. You need your own philosophy for integrating deep work into your professional life. You must be careful to choose a philosophy that fits your specific circumstances, as a mismatch here can derail your deep work habit before it has a chance to solidify. This philosophy attempts to maximize deep efforts by eliminating or radically minimizing shallow obligations. Practitioners of the monastic philosophy tend to have a well-defined and highly valued professional goal that they\u2019re pursuing, and the bulk of their professional success comes from doing this one thing exceptionally well. It\u2019s this clarity that helps them eliminate the thicket of shallow concerns that tend to trip up those whose value proposition in the working world is more varied. The pool of individuals to whom the monastic philosophy applies is limited. If you\u2019re outside this pool, its radical simplicity shouldn\u2019t evince too much envy. On the other hand, if you\u2019re inside this pool \u2014 someone whose contribution to the world is discrete, clear, and individualized \u2014 then you should give this philosophy serious consideration, as it might be the deciding factor between an average career and one that will be remembered. This philosophy asks that you divide your time, dedicating some clearly defined stretches to deep pursuits and leaving the rest open to everything else. During the deep time, the bimodal worker will act monastically \u2014 seeking intense and uninterrupted concentration. During the shallow time, such focus is not prioritized. This division of time between deep and open can happen on multiple scales. For example, on the scale of a week, you might dedicate a 4-day weekend to depth and the rest to open time. Similarly, on the scale of a year, you might dedicate one season to contain most of your deep stretches. The bimodal philosophy believes that deep work can produce extreme productivity, but only if the subject dedicates enough time to such endeavors to reach maximum cognitive intensity \u2014 the state in which real breakthrough occurs. This is why the minimum unit of time for deep work in this philosophy tends to be at least one full day. To put aside a few hours in the morning, for example, is too short to count as a deep work stretch for an adherent of this approach. At the same time, the bimodal philosophy is typically deployed by people who cannot succeed in the absence of substantial commitments to non-deep pursuits. Those who deploy the bimodal philosophy of deep work admire the productivity of the monastics but also respect the value they receive from the shallow behaviors in their working lives. Perhaps the biggest obstacle to implementing this philosophy is that even short periods of deep work require a flexibility that many fear they lack in their current positions. If even an hour away from your inbox makes you uncomfortable, then certainly the idea of disappearing for a day or more at a time will seem impossible.\n\nThis philosophy argues that the easiest way to consistently start deep work sessions is to transform them into a simple regular habit. The goal, in other words, is to generate a rhythm for this work that removes the need for you to invest energy in deciding if and when you\u2019re going to go deep. A common way to implement this is to use visual indicators of your work progress to reduce the barrier to entry for going deep. The rhythmic philosophy provides an interesting contrast to the bimodal philosophy. It perhaps fails to achieve the most intense levels of deep thinking sought in the day-long concentration sessions favored by the bimodalist. The trade-off, however, is that this approach works better with the reality of human nature. By supporting deep work with rock-solid routines that make sure a little bit gets done on a regular basis, the rhythmic scheduler will often log a larger total number of deep hours per year. In this approach, you can fit deep work wherever you can into your schedule. Journalists are trained to shift into a writing mode on a moment\u2019s notice, as is required by the deadline-driven nature of their profession, hence the name. This approach is not for the deep work novice. The ability to rapidly switch your mind from shallow to deep mode doesn\u2019t come naturally. Without practice, such switches can seriously deplete your finite willpower reserves. This habit also requires a sense of confidence in your abilities \u2014 a conviction that what you\u2019re doing is important and will succeed. This type of conviction is typically built on a foundation of existing professional accomplishment.\n\nTo make the most out of your deep work sessions, build rituals with high level of strictness and idiosyncrasy. There\u2019s no one correct deep work ritual \u2014 the right fit depends on both the person and the type of project pursued. But there are some general questions that any effective ritual must address: \u00b7 Where you\u2019ll work and for how long \u2014 Your ritual needs to specify a location for your deep work efforts. This location can be as simple as your normal office with the door shut and desk cleaned off. If it\u2019s possible to identify a location used only for depth, the positive effect can be even greater. Regardless of where you work, be sure to also give yourself a specific time frame to keep the session a discrete challenge and not an open-ended slog. \u00b7 How you\u2019ll work once you start to work \u2014 Your ritual needs rules and processes to keep your efforts structured. For example, you might institute a ban on any Internet use, or maintain a metric such as words produced per 20-minute interval to keep your concentration honed. Without this structure, you\u2019ll have to mentally litigate again and again what you should and should not be doing during these sessions and keep trying to assess whether you\u2019re working sufficiently hard. These are unnecessary drains on your willpower reserves. \u00b7 How you\u2019ll support your work \u2014 Your ritual needs to ensure your brain gets the support it needs to keep operating at a high level of depth. For example, the ritual might specify that you start with a cup of good coffee, or make sure you have access to enough food of the right type to maintain energy, or integrate light exercise such as walking to help keep the mind clear. This support might also include environmental factors, such as organizing the raw materials of your work to minimize energy-dissipating friction. To maximize your success, you need to support your efforts to go deep. At the same time, this support needs to be systematized so that you don\u2019t waste mental energy figuring out what you need in the moment.\n\nThe concept is simple: By leveraging a radical change to your normal environment, coupled perhaps with a significant investment effort or money, all dedicated toward supporting a deep work task, you increase the perceived importance of the task. This boost in importance reduces your mind\u2019s instinct to procrastinate and delivers an injection of motivation and energy. The dominant force is the psychology of committing so seriously to the task at hand. To put yourself in an exotic location to focus on a writing project, or to take a week off from work just to think, or to lock yourself in a hotel room until you complete an important invention: These gestures push your deep goal to a level of mental priority that helps unlock the needed mental resources. Sometimes to go deep, you must first go big. Personally, I frequently hop from coffee shops to coffee shops to work on my projects. I intentionally research the places beforehand and select the most beautiful, atmospheric, and laptop-warrior-friendly coffee shops to come. In retrospect, the location where I work significantly impact my work efficiency and focus.\n\nWhen it comes to deep work, consider the use of collaboration when appropriate, as it can push your results to a new level. At the same time, don\u2019t lionize this quest for interaction and positive randomness to the point where it crowds out the unbroken concentration ultimately required to wring something useful out of the swirl of ideas all around us. \u00b7 The hub-and-spoke model: a setup that straddles a spectrum where on one extreme we find the solo thinker, isolated from inspiration but free from distraction, and on the other extreme, we find the fully collaborative thinker in an open office, flush with inspiration but struggling to support the deep thinking needed to build on it. Distraction remains a destroyer of depth. Therefore, the hub-and-spoke model provides a crucial template. Separate your pursuit of serendipitous encounters from your efforts to think deeply and build on these inspirations. You should try to optimize each effort separately, as opposed to mixing them together into a sludge that impedes both goals. \u00b7 The whiteboard effect: For some types of problems, working with someone else at the proverbial shared whiteboard can push you deeper than if you were working alone. The presence of the other party waiting for your next insight \u2014 be it someone physically in the same room or collaborating with you virtually \u2014 can short-circuit the natural instinct to avoid depth. Even when you retreat to a spoke to think deeply, when it\u2019s reasonable to leverage the whiteboard effect, do so. By working side by side with someone on a problem, you can push each other toward deeper levels of depth, and therefore toward the generation of more and more valuable output as compared to working alone. In my situation, I consult my uncle, a professional software engineer, and my cousin, a Computer Science major, to help along with programming exercises. Most of technical interviews are conducted in the whiteboard anyway, so I use a physical whiteboard in the house to practice the coding challenges and discuss them with others. Over time, I became more confident with my ability to code and get battle-ready for interviews. It\u2019s often straightforward to identify a strategy needed to achieve a goal, but what trips up companies is figuring out how to execute the strategy once identified. This division between what and how is crucial but is overlooked in the professional world. Cal describes in his book The 4 Disciplines of Execution (4DX), which built on extensive consulting case studies to describe 4 \u201cdisciplines\u201d for helping companies successfully implement high-level strategies: \u00b7 Discipline 1 \u2014 Focus on the Wildly Important: For an individual focused on deep work, the implication is that you should identify a small number of ambitious outcomes to pursue with your deep work hours. The general exhortation to \u201cspend more time working deeply\u201d doesn\u2019t spark a lot of enthusiasm. To instead have a specific goal that would return tangible and substantial professional benefits will generate a steadier stream of enthusiasm. \u00b7 Discipline 2 \u2014 Act on the Lead Measures: Once you\u2019ve identified a wildly important goal, you need to measure your success. In 4DX, there are 2 types of metrics for this purpose: lag measures and lead measures. Lag measures describe the thing you\u2019re ultimately trying to improve. Lead measures, on the other hand, \u201cmeasure the new behaviors that will drive success on the lag measures.\u201d For an individual focused on deep work, it\u2019s easy to identify the relevant lead measure: time spent in a state of deep work dedicated toward your wildly important goal. \u00b7 Discipline 3 \u2014 Keep a Compelling Scoreboard: People play differently when they\u2019re keeping score. Therefore, the individual\u2019s scoreboard should be a physical artifact in the workspace that displays the individual\u2019s current deep work hour count. \u00b7 Discipline 4 \u2014 Create a Cadence of Accountability: You need to put in place a rhythm of regular accountability towards your wildly important goal. You should get into the habit of a weekly review in which you plan for the workweek ahead. For example, when I first began experimenting with 4DX, I set the specific important goal of finishing Elements of Programming Interviews book, doing a minimum of 30 problems on Cracking The Coding Interview, and completing all the Easy-type questions on LeetCode. Instead of focusing on the lag measures (finishing the problems or the book), I focused on tracking deep work hours \u2014 every hour extra of deep work was immediately reflected in my tally. I used Wunderlist to-do list app to schedule my work and prioritize tasks. I kept a tally of how many chapters I\u2019ve read and how many problems I\u2019ve solved. I used a daily review to look over my scoreboard to celebrate good days, help understand what led to bad days, and most important, figure out how to ensure a good score for the days ahead. The 4DX framework is based on the fundamental premise that execution is more difficult than strategizing. After hundreds and hundreds of case studies, its inventors managed to isolate a few basic disciplines that seem to work particularly well in conquering this difficulty. It\u2019s no surprise, therefore, that these same disciplines can have a similar effect on your personal goal of cultivating a deep work habit."
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/10-creativity-challenges-to-exercise-your-creative-confidence-ff6f19ba4241?source=user_profile---------34----------------",
        "title": "10 Creativity Challenges to Exercise Your Creative Confidence",
        "text": "I recently read \u201cCreative Confidence\u201d by Tom Kelley & David Kelley. It is an insightful, compelling narrative on how to unlock, nurture, and strengthen the innate creativity that lies within us all. In the book, the Kelley brothers demonstrate that creativity is a mindset, a way of thinking, and a proactive approach to finding new solutions. We may not all be artists, but we can be more creative lawyers, doctors, managers, or salespeople. Drawing on stories from their work at IDEO and the Stanford d.school, the Kelleys show us how to tap into the wellspring of creativity and imagination when tackling the problems we encounter. Moreover, they reveal specific strategies to unleash that creative spark within us. Creativity and the ability to innovate are like muscles \u2014 the more we use them, the stronger they get. A person with creative confidence understands how to strike the balance between certainty and uncertainty. He/she understands and accepts that uncertainty, false starts, and mistakes are part of the creative process, but he/she also projects a sense of stability and progress to those around her. There is a whole chapter in the book that presents the tools to help you practice unlocking your creative thinking as a bridge to creative confidence. In this post, I will share these tools to help you flex your creative muscles. If you have practiced Design Thinking exercises before, you will find these challenges quite familiar. I am fortunate to take an Innovation Through Design Thinking class during my semester abroad last year and have been able to apply this approach to all the problem-solving / innovation types of work I have encountered. Creativity Challenge #1 \u2014 Push Yourself To Think Divergently and Creatively Actively engaging in exercises that foster divergent or unconventional thinking can encourage the generation of ideas. When you are searching for innovative solutions on your own, mind-maps can be a powerful way to come up with or to gain clarity about a topic of exploration. They are extremely versatile, and we use them all the time. They help you chart the recesses of your mind surrounding one central idea. The further you get from the center of the map, the more hidden ideas you can uncover.\n\n\u00b7 Participants: This is usually a solo activity \u00b7 Supplies: Paper (the bigger the better) and pen 1. On a large blank piece of paper, write your central topic or challenge in the middle of the paper and circle it. 2. Make some connections to that main topic and write them down, branching out from the center as you go. Ask yourself, \u201cWhat else can I add to the map that is related to this theme?\u201d If you think one of your ideas will lead to a whole new chapter, draw a quick rectangle or oval around it to emphasize that it\u2019s a hub. 3. Use each connection to spur new ideas. 4. Keep going. You are done when the page fills or the ideas dwindle. If you are feeling warmed up but not finished, try to reframe the central topic and do another mindmap to get a fresh perspective. If you feel you\u2019ve done enough, think about which ideas you would like to move forward with. Each challenge presents an opportunity for innovation. One simple way to have more ideas in your arsenal is to start keeping track of them as they occur.\n\n\u00b7 Supplies: Paper and pen, or a digital means of keeping notes When you have an idea or observe something intriguing, take note of it. The actual means of capturing the idea doesn\u2019t matter as much as having it with you at all times. Choose a method or a technology that fits with your lifestyle and your personality: \u00b7 Digital tools are great, but paper still works exceptionally well. \u00b7 On the iPhone, Siri will let you dictate a quick mental note. An increasing number of options also exist on other platforms. \u00b7 Your laptop or tablet has all kinds of notepad applications. But you\u2019ll get more functionality out of purpose-built programs like Evernote, specifically designed to store such ideas. The goal of this challenge is to push people to test their creativity by turning circles into recognizable objects in a very short period of time. \u00b7 Participants: Solo or groups of any size \u00b7 Supplies: Pen and a piece of paper (per person) with 30 blank circles on it of approximately the same size 1. Give each participant one Thirty Circles sheet of paper and something to draw with. 2. Ask them to turn as many of the blank circles as possible into recognizable objects in 3 minutes. 3. Compare results. Look for the quantity or fluency of ideas. Ask how many people filed in 10, 15, 20, or more circles? Next, look for diversity or flexibility in ideas. See if the ideas are derivative or distinct. Did anyone \u201cbreak the rules\u201d and combine circles? Were the rules explicit, or just assumed? A fundamental principle of innovation or creative thinking is to start with empathy. On the path from blank page to insight, sometimes people need a tool to help with what comes next: synthesis. You\u2019ve gone into the field in search of knowledge, meeting people on their home turf, watching and listening intently. But synthesizing all that data can be a little daunting. Take control of your field observations by organizing them with an \u201cempathy Map.\u201d\n\n\u00b7 Participants: Solo or groups of 2 to 8 people 1. On a whiteboard or a large flip chart, draw a 4-quadrant map. Label the sections with \u201csay,\u201d \u201cdo,\u201d \u201cthink, \u201c and \u201cfeel, \u201c respectively. 2. Populate the left-hand quadrants with Post-its that capture each of your individual observations, using one Post-it per idea. Place observations about what people DO in the lower-left quadrant, and place observations of what people SAY in the upper-left quadrant. Try color-coding your observations, using green Post-its for positive things, yellow Post-its for neutral, and pink or red for frustrations, confusion, or pain points. The key is not to record everything, but instead to capture what stands out. 3. When you run out of observations on the left side, begin to fill the right side with Post-its, inferring what people THINK in the upper-right quadrant and what they FEEL in the lower-right quadrant. Pay attention to people\u2019s body language, tone, and choice of words. 4. Take a step back and look at the map as a whole. Try to draw some insights or conclusions from what you have just written down, shared, and talked about. These questions serve as a good prompt for a discussion of insights. What seems new or surprising? Are there contradictions or disconnects within or between quadrants? What unexpected patterns appear? What, if any, latent human needs emerge? To practice creative confidence on a team, members need to feel free to experiment, even during early efforts when results will be far from perfect. For that experimentation to translate into learning, however, at some point you need feedback, in order to identify weaknesses and make adjustments the next time. We all instinctively know that constructive critique is essential. And yet it can be hard to listen to and absorb feedback without letting our egos and defensiveness distract us from what may be a valuable message.\n\n\u00b7 Supplies: A means of recording feedback. For example, in a large group, keep a Word document open and type notes in real time. In a smaller setting, Post-its or index cards will work 1. Set the tone for a constructive conversation and explain the \u201cI Like / I Wish\u201d method. 2. The participants take turns, sharing I Like / I Wish statements, while the facilitator records their statements. Make sure people receiving feedback just listen. This is not a time to defend decisions or challenge the critique. Ask everyone to listen and accept it as a well-meaning offer of help. You can ask for clarification and engage in further discussion at a later time. 3. Stop when participants run out of things to say in both the \u201cI like\u201d and \u201cI wish\u201d categories. Creativity thrives amidst free-flowing social discourse. To get a room full of strangers to innovate, you may want to begin by breaking down some social barriers. When this exercise is done right, the room will be abuzz with chatter and laughter, and participants will be more open to what comes next.\n\n\u00b7 Participants: Pairs in groups of any size \u00b7 Supplies: Paper printed with a set of questions for each participant. Several different sets of questions will be needed to accommodate the entire group 1. Give each person a list of open-ended questions. Several different sets of questions should be spread throughout tables in the room so that people aren\u2019t continually being asked the same questions. 2. Ask each person in the room to pair up with someone they don\u2019t know very well or have never met. This may involve getting up and moving seats. 3. Have one person in each pair ask a question from the list. Allow 3 minutes for the other person to answer. 4. Have each pair switch roles and repeat, asking a different question on the list. 5. Tell everyone to find a new partner and repeat the process for a couple more rounds. While Speed Dating is useful in situations where people don\u2019t know each other well, sometimes in group meetings you will encounter the opposite problem: a group where people know each other too well. Or, more specifically, a group in which hierarchy is so well established that the more junior members in the room self-edit and defer to the executives rather than putting their best ideas on the table. To reduce hierarchy and self-censoring, we use an experiment called \u201cnickname warm-up.\u201d Using a stack of colorful names the instructors have prepared in advance, the activity is a way to temporarily level out the organization during a creative working session. Each participant is given a persona to allow them to \u201ctry on\u201d new behaviors.\n\n\u00b7 Participants: Groups of 6 to 12 people per facilitator \u00b7 Supplies: Name tags for all participants with the fake names written out. A hat and a ball for each facilitator 1. Each participant reaches into the hat, draws out a name tag, and puts it on. Use names that lend themselves to humor and emotion. Teams tend to produce their best work when the group is having fun. Some of the monikers can imply a big dose of street credibility, while others suggest quirky personalities. 2. The facilitator gathers the group in a circle and tosses the ball. Whoever catches it introduces themselves using their new nickname and then tells a short story about how they acquired this nickname as a child. 3. After their self-introduction, they toss the ball to a new person, until everyone has had a chance to share their new name and story. 4. The rule for the rest of the workshop \u2014 strictly enforced \u2014 is that everyone must use only these nicknames when referring to themselves or others. Creativity Challenge #8 \u2014 Empathize With Customers, Employees, And Other End Users One way to develop more empathy with \u2014 and gain new insights about \u2014 your customers is to look beyond the narrow definition of your offering and consider the customer\u2019s total experience. The more broadly you define the customer experience, the more opportunities you can identify for improvement. A journey map helps you think systematically through the steps your customers \u2014 internal or external \u2014 have when they interact with your product or service.\n\n\u00b7 Participants: Solo or groups of 2 to 6 people 1. Choose a process or journey that you want to map. 2. Write down the steps. Make sure to include even small steps that may seem trivial. The goal is to get you to consider the nuances of the experience that you may normally overlook. 3. Organize the steps into a map. Usually we display the steps sequentially in a timeline. Your map may include branches to show alternative paths in the customer journey. You could also use a series of pictures or whatever method fits your data. 4. Look for insights. What patterns emerge? Anything surprising or strange? Question why certain steps occur, the order they occur in, and so forth. Ask yourself how you might innovate each step. 5. If possible, show the map to people familiar with the journey and ask them what you\u2019ve overlooked or gotten out of sequence. Innovators often face the task of which challenge to focus on or how to frame a challenge they are given. Talking about problems doesn\u2019t necessarily inspire ideas or energize you to act on them. Nor does wishful thinking. The Dream/Gripe Session helps you translate those discussions into creative thinking challenges you can start to tackle. \u00b7 Participants: Pairs in groups of any size 1. Decide on a topic for discussion. The dreams and gripes may relate to internal matters like the culture of the organization or external ones like interactions with customers. 2. Pair up with another person and select one person to go first. 3. Partner 1 airs his or her dreams and gripes for 5 to 7 minutes while Partner 2 listens and takes notes. 4. Partner 2 reframes the dreams and gripes into open-ended questions that makes for good innovation challenges. Start with the phrase \u201cHow might we\u2026?\u201d A good \u201cHow Might We\u201d question should not be so narrow that it suggests a solution. Initially, you are just trying to capture the problem, not jump to possible solutions. It should also not be so broad that it stymies the flow of ideas (rather than generating them). A good \u201cHow Might We\u201d question should allow someone to easily come up with 10 different ideas. Partner 2 should aim for 3 to 5 well-framed innovation challenges and share them with Partner 1. 5. Switch roles and have Partner 2 air dreams and gripes while Partner 1 listens and then offers \u201cHow Might We\u201d innovation challenges. 6. (Optional) If you are doing this in a group setting, compare lists of all the innovation challenges across the pairs. Look for patterns, themes, and common issues. This should help focus the discussion and suggest an opportunity for what innovation challenge to take on next. The exercise uses a simple object that most people carry with them, as a prop to discover needs, design and prototype solutions, and get user feedback. It gives everyone a chance to cycle quickly through the human-centered design process.\n\n\u00b7 Participants: Pairs in groups of any size \u00b7 Supplies: The facilitator\u2019s guide includes a complete list of instructions, worksheets, and prototyping materials. The instructions and worksheets can be printed out for each participant or projected on a screen. Provide prototyping materials 1. Participants pair off, with one starting as the interview/anthropologist, while the other plays the part of the prospective customer. The interviewer spends a few minutes understanding and empathizing with the other person. The interviewee/customer takes out his or her wallet, and they have a discussion about the items inside and the meaning attached to them. The interviewer asks questions to see how the wallet fits into the customer\u2019s life, looking especially for problems or friction points associated with the wallet. After just a few minutes, the facilitator calls time, and the team members reverse roles, with the interviewer in round one becoming the customer in round two. 2. After the participants have had a chance to understand the customers and their wallets, the next step is to develop a point of view about their latent needs and missed opportunities with regard to their wallets. Those need-based points of view can take the form of a sentence like \u201cMy customer needs a way to \u2026 [user needs] \u2026 in a way that makes them feel \u2026 [meaning/emotion] \u2026 because \u2026 [insight]. 3. In a form of mini-brainstorming, each participant generates a few concepts for new objects \u2014 they may not be physical wallets at all \u2014 that satisfy the needs highlighted by the point of view developed in step 2. 4. In the most kindergarten-like phase of the wallet exercise, participants create the roughest of prototypes to bring their ideas to life. Using an eclectic mix of materials like construction paper, duct tape, pipe cleaners, and binder clips, the participants will build prototypes just good enough to make their idea tangible so that they can get feedback from their future customer. 5. Using their storytelling skills, a selection of participants \u201cpitch\u201d their new-to-the-world wallet concept to their customer and/or to the room at large. There is an enormous number of stories and tips contained in Creative Confidence. I have shared here the 10 creativity challenges that you can practice to be more creatively confident. Some psychologists claim that you have to practice a new behavior for 21 days before it begins to become a habit. The operative word is \u201cpractice.\u201d The weeks, months, or years spent thinking about new behaviors don\u2019t count. So pick your favorites or create some new experiments of your own. Start accelerating down the runway now if you want your new skills to take flight."
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/2016-annual-review-cab7495a67eb?source=user_profile---------35----------------",
        "title": "2016 Annual Review: A Year of Magical Thinking \u2013 Constraint Drives Creativity \u2013",
        "text": "Travel I hit it off the park with this one. I did a pretty extensive array of travel throughout Europe over the course of my study abroad program. In total, I have been to 11 countries excluding Denmark: Germany (Berlin, Frankfurt), Spain (Barcelona, Madrid), Italy (Venice, Florence, Rome, Milan), Czech Republic (Prague), Austria (Vienna), Hungary (Budapest), Sweden (Stockholm, Malmo), Norway (Oslo), Netherlands (Amsterdam), France (Paris), and Portugal (Lisbon). Each place is unique in its own way, and I felt truly blessed to have experienced the magical feeling of seeing a famed attractions multiple times.\n\nCultural Integration I stayed in a living and learning community with 21 other American students, all interested in entrepreneurship. I had a magnificent time living, hanging and going out with them. To integrate to the local culture, I volunteered in a student-run coffee house, met a bunch of cool international and Danish college-aged peers, and most importantly, learned to brew coffee. Besides, I had a Danish host family that I visited a few times throughout the semester, ate traditional meals with them and delved into good conversations.\n\nIn addition to that, I took classes on Artificial Intelligence, Sustainable Design, and Design Thinking. All of them lend knowledge that enrich my learning scope in significant ways.\n\nAcademics I took a Game Development class while abroad, which exposed me to new aspect of programming that I haven\u2019t contemplated before. I worked with a program called Unity, a cross platform for programmers to make games. Throughout the semester, I learned a lot of different phases that go into the game development process, such as 2D/3D graphics, level design, attributes, physics etc. I worked in a team of 5 for a final project that results into a 2D adventure game, which can be found here .\n\nThis experience tops my highlight of the year. 4 months in Copenhagen truly opened my perspective and made me a more worldly/global citizen.\n\nThis has been the 3rd time that I do this, so I sat down to read 2014 In Review and 2015 In Review before starting this off. I couldn\u2019t help but felt quite accomplished when looking at the goals that I set for 2016. Usually I will answer the 3 questions \u2014 what went well, what didn\u2019t go well, and what am I working toward. However, this time I decided to go straight into the things that have made my 2016.\n\nI took a lot of pictures and videos throughout my travels. The vlogging project turned out well \u2014 I made a total of 37 videos that encapsulated all the memories abroad. It\u2019s good now to watch them again and relive those good times.\n\nI spent 3 months over the summer in San Francisco, the tech and innovation hub of the world. The experience has been well worth it:\n\n\u00b7 I got a product internship with an early stage nonprofit startup called New Story that fundraises money to build houses for communities in 3rd world nations. I learned the ins and outs of how a startup operates, and built good skills such as user research and data analysis via assigned projects.\n\n\u00b7 I went to a whole lot of meetups in the city, ranging in a wide variety of topics \u2014 software development, user experience design, artificial intelligence, mobile app business etc. I soaked up the tech culture and widened professional network through these events.\n\n\u00b7 I had a chance to explore sunny North Cal weather, SF\u2019s beautiful public spaces, coffee shops, nightlife, concerts, food places. I stayed in a co-living/co-working space throughout the summer and met awesome people from all over the world coming to SF for internships, jobs, startup funding.\n\nThe first semester of my last year in college has been phenomenal. Living in an apartment and with great roommates is awesome. I was stoked to catch up with friends after 8 months away from campus. College wasn\u2019t just about class. These 3 activities I did outside of class keep my engagement with school high:\n\n1 \u2014 Beta Theta Pi: A family away from home for me. I met the new members who pledged when I was abroad, and they are all super dope. Joining a fraternity was probably the best decision I made towards personal growth in college. I would miss this experience a lot after graduation.\n\n2 \u2014 Denison Enterprises: A super cool student-run ventures group for those who are interested in business and entrepreneurship. I joined earlier in the semester and devoted a big chunk of my time on this group. The people are great, the projects are interesting, and the learning curve is tremendous. I am working on a mobile app that does food delivery to campus with other talented individuals, so I am exciting to see the outcomes in the spring.\n\n3 \u2014 Career Center Advisory Board: A low-time commitment I picked up at the beginning of the term. It is a group of 5\u20137 students who recommend ideas and execute projects to spread awareness about the career office at Denison. We plan to make a PR video for the office and organize a career fair for upperclassmen next term.\n\nI wrote quite a few over the summer, 12 posts to be exact. One of them, \u201cThe 10 Algorithms Machine Learning Engineers Need to Know\u201d, became a hit on Medium! As of now, it has more than 800 recommends and gains me more than 600 new followers on the site. People even reached out to me asking for permission to repost it on their publications.\n\nBesides, I took 2 upper-level Communication classes this past semester and turned in 2 lengthy research papers, which I am quite proud of.\n\nWith a lot of travel and work this year, I didn\u2019t read as much compared to the previous year. I only read 10 books this year, but they are all super useful and enjoyable:\n\n\u00b7 Founders At Work \u2014 Jessica Livingston: I was really excited to get a hold of this book, as the author is the founder of the renowned Y-Combinator, the most popular tech incubator in the world. The book recounts the early struggles for independence and acceptance of many of modern technology\u2019s giants through personal interviews. I found these stories to be super interesting, because they\u2019re about the early stages, when the founders were young and inexperienced.\n\n\u00b7 The Innovator\u2019s Dilemma \u2014 Clayton Christensen: An innovation classic, this book explains why most companies miss out on new waves of innovation. I found it very hard to read, but nonetheless useful because it gives a set of rules for capitalizing on phenomenon of disruptive innovation.\n\n\u00b7 The Art of the Start \u2014 Guy Kawasaki: Very easy book to read \u2014 Past Apple\u2019s chief evangelist, Guy Kawasaki, shows how to start a business the right way.\n\n\u00b7 Naked Statistics \u2014 Charles Wheelan: A brilliant book about statistics, a subject of my interest for a long time. Wheelan strips away the arcane and technical details and focuses on the underlying intuition that drives statistical analysis.\n\n\u00b7 The Design of Everyday Things \u2014 Don Norman: A design classic, this book shows that good, usable design is possible. I recommend all aspiring designers to read this to get a good intuition about good vs bad design.\n\n\u00b7 Joel on Software \u2014 Joel Spolsky: A really good book that covers every conceivable aspect of software programming. All programmers will surely relate to Joel\u2019s musings.\n\n\u00b7 Never Eat Alone \u2014 Keith Ferrazzi: I actually wrote a whole post about the lessons that I learn from Keith\u2019s book. It\u2019s a great book on the topic of Networking 101.\n\n\u00b7 Inspired: How to Create Products Customers Love \u2014 Marty Cagan: I found this one to be the most useful as it is all about Product Management, the field I want to go into.\n\n\u00b7 So Good They Can\u2019t Ignore You \u2014 Cal Newport: I finished this book in less than 7 hours on my waiting time on my flight back home a couple days ago. It changed the way I think about crafting a remarkable career.\n\n\u00b7 Thinking, Fast and Slow \u2014 Daniel Kahneman: This book takes readers on a tour of the mind and explains the 2 systems that drive the way we think.\n\nI took 6 online courses this year: Intro to Machine Learning, Algorithm Design & Analysis, Managing Big Data with MySQL, Exploratory Data Analysis with R, Data Visualization with D3, and Data Wrangling with MongoDB. I am very interested in a career in Data Science for the long run, and these courses significantly improved my knowledge on the topic.\n\nI decide to make 2017 entirely focused on the future. That means I\u2019ll practice a forward-looking, goal-setting mindset for most of the time. The priority is getting a job after college, which I am in a crazy midst during this winter break. Another one is to finish my last semester strong, making a lot of memories and maintaining close friendships with the amazing people I met in school. Last but not least, I am more serious about relationship and go in casual dates, so let\u2019s see how that works out.\n\nThere\u2019s more I could say about each of these things, and many other things I could say. But since I\u2019m 95% focused on looking forward, I\u2019m going to stick with that.\n\nYours in the future,"
    },
    {
        "url": "https://medium.com/constraint-drives-creativity/31-networking-principles-i-learned-from-keith-ferrazzi-6f8ef3ab8fee?source=user_profile---------36----------------",
        "title": "31 Networking Principles I Learned from Keith Ferrazzi",
        "text": "One of the most important habits that I have developed over the past couple of years is reading. I try to read at least 12\u201315 books every year, depending on my college and work schedule. This summer, I have been able to go through 6 books. Here they are:\n\n3 \u2014 The Art of the Start by Guy Kawasaki\n\n5 \u2014 The Design of Everyday Things by Don Norman\n\n6 \u2014 Never Eat Alone by Keith Ferrazi\n\nAll of these books are excellent in their own realms, but this post is dedicated to the things I have learned in Keith Ferrazi\u2019s book, Never Eat Alone, a business classic on the power of relationships, with timeless strategies shared by the world\u2019s most connected individuals. So below are the 31 networking principles that Keith shares in his book:\n\n1 \u2014 Becoming a Member of the Club: Connecting is one of the most important business and life skill sets you\u2019ll ever learn. Because, flat out, people do business with people they know and like. Careers work the same way. Even our overall well-being and sense of happiness is dictated in large part by the support and guidance and love we get from the community we build for ourselves.\n\n2 \u2014 Don\u2019t Keep Score: Relationships are solidified by trust. Institutions are built on it. You gain trust by asking not what people can do for you, but what you can do for others. In other words, the currency of real networking is not greed but generosity.\n\n3 \u2014 What\u2019s Your Mission? The more specific you are about what you want to do, the easier it becomes to develop a strategy to accomplish it. Every successful person shares a zeal for goal setting. The key is to make setting goals a habit. If you do that, goal setting becomes a part of your life. If you don\u2019t, it withers and dies.\n\n4 \u2014 Build It Before You Need It: You\u2019ve got to create a community of colleagues and friends before you need it. Others around you are far more likely to help you if they already know and like you. Start gardening now. You won\u2019t believe the treasures to be found within your own backyard.\n\n5 \u2014 The Genius of Audacity: Sticking to the people we already know is a tempting behavior. But unlike some forms of dating, a networker isn\u2019t looking to achieve only a single successful union. Creating an enriching circle of trusted relationships requires one to be out there, in the mix, all the time. Ultimately, everyone has to ask himself or herself how they\u2019re going to fail. The choice isn\u2019t between success and failure; it\u2019s between choosing risk and striving for greatness, risking nothing and being certain of mediocrity.\n\n6 \u2014 The Networking Jerk: The networking jerk is the insincere, ruthlessly ambitious glad-hander you don\u2019t want to become. If you\u2019re not making friends while connecting, best to resign yourself to dealing with people who don\u2019t care much about what happens to you. Being disliked will kill your connecting efforts before they begin. Alternatively, being liked can be the most potent, constructive force for getting business done.\n\n7 \u2014 Do Your Homework: Whom you meet, how you meet them, and what they think of you afterward should not be left to chance. Preparation is the key to sounding like a genius; so before meeting with any new people, research on who they are, what their businesses is, and what\u2019s important to them.\n\n8 \u2014 Take Names: The successful organization and management of the information that makes connecting flourish is vital. Tracking the people you know, the people you want to know, and doing all the homework that will help you develop intimate relationships with others can cause one heck of an information overload.\n\n 9 \u2014 Warming the Cold Call: The 4 rules of warm calling: 1) Convey credibility by mentioning a familiar person or institution. 2) State your value proposition. 3) Impart urgency and convenience by being prepared to do whatever it takes whenever it takes to meet the other person on his or her own terms. 4) Be prepared to offer a compromise that secures a definite follow-up at a minimum.\n\n10 \u2014 Managing the Gatekeeper Artfully: Make the gatekeeper an ally rather than an adversary. And never get on his or her bad side. Many executive assistants are their bosses\u2019 minority partners. Don\u2019t think of them as secretaries or assistants. In fact, they are associates and lifelines.\n\n11 \u2014 Never Eat Alone: In building a network, remember: Above all, never ever disappear. Keep your social and conference and event calendar full. As an up-and-corner, you must work hard to remain visible and active among your ever-budding network of friends and contacts.\n\n12 \u2014 Share Your Passions: Make a list of the things you\u2019re most passionate about. Use your passions as a guide to which activities and events you should be seeking out. Use them to engage new and old contacts. When your day is fueled by passion, filled with interesting people to share it with, reaching out will seem less like a challenge or a chore and more like an effortless consequence of the way you work.\n\n13 \u2014 Follow Up or Fail: When you meet someone with whom you want to establish a relationship, take the extra little step to ensure you won\u2019t be lost in their mental attic. Making sure a new acquaintance retains your name is a process you should set in motion right after you\u2019ve met someone. Make follow-up a habit. Make it automatic. When you do, the days of struggling to remember people\u2019s names will be a thing of the past.\n\n14 \u2014 Be a Conference Commando: Conferences are good for mainly one thing. They provide a forum to meet the kind of like-minded people who can help you fulfill your mission and goals. Those who use conferences properly have a huge leg up at your average industry gathering. They set up 1-on-1 meetings, organize dinners, and make each conference an opportunity to meet people who could change their lives.\n\n15 \u2014 Connecting with Connectors: Super-connectors are those who maintain contact with thousands of people in many different worlds, and they know them well enough to give them a call. Once you become friendly with a super-connector, you\u2019re only two degrees away from the thousands of different people they know. You\u2019ll find a disproportionate amount of super-connectors as headhunters, lobbyists, fundraisers, politicians, journalists, and public relations specialists, because such positions require these folks\u2019 innate abilities.\n\n16 \u2014 Expanding Your Circle: The most efficient way to enlarge and tap the full potential of your circle of friends is to connect your circle with someone else\u2019s. Such collaboration means seeing each person in your network as a partner. Like a business in which cofounders take responsibility for different parts of the company, networking partners help each other, and by extension their respective networks, by taking responsibility for that part of the web that is theirs and providing access to it as needed. In other words, they exchange networks. The boundaries of any network are fluid and constantly open.\n\n17 \u2014 The Art of Small Talk: When it comes to making an impression, differentiation is the name of the game. Confound expectation. Shake it up. How? Be yourself \u2014 vulnerability is the most underappreciated assets in business today. The real winners are those people who put it all out there and don\u2019t waste a bunch of time and energy trying to be something/someone they\u2019re not. Charm is simply a matter of being yourself. Your uniqueness is your power. We are all born with innate wining traits to be a masterful small talker.\n\n18 \u2014 Health, Wealth, and Children: The highest human need is for self-actualization \u2014 the desire to become the best you can be. But we can\u2019t attend to our highest needs until we attend to those at the bottom of the pyramid, like the necessities of subsistence, security, and sex. It is within this lower group \u2014 where health, wealth, and children reside \u2014 that loyalty is created. In addressing those 3 fundamental issues, you accomplish 2 things: 1) You help someone fulfill those needs they most need met, and 2) You allow them the opportunity to move up the pyramid of needs to tackle some of their higher desires.\n\n19 \u2014 Social Arbitrage: How much you give to the people you come into contact with determines how much you\u2019ll receive in return. In other words, if you want to make friends and get things done, you have to put yourself out to do things for other people \u2014 things that require time, energy, and consideration. The best sort of connecting occurs when you can bring together two people from entirely different worlds. The strength of your network derives as much from the diversity of your relationships as it does from their quality or quantity. The ability to bridge different worlds, and even different people within the same profession, is a key attribute in managers who are paid better and promoted faster.\n\n20 \u2014 Pinging All the Time: 80% of building and maintaining relationships is just staying in touch \u2014 \u201cpinging.\u201d It\u2019s a quick, casual greeting, and it can be done in any number of creative ways. Once you develop your own style, you\u2019ll find it easier to stay in touch with more people than you ever dreamed of in less time than you ever imagined.\n\n21 \u2014 Find Anchor Tenants and Feed Them: Every individual within a particular peer set has a bridge to someone outside his or her own group of friends. We all have, to some degree or another, developed relationships with older, wiser, more experienced people; they may be our mentors, our parents\u2019 friends, our teachers, our rabbis and reverends, our bosses. They are \u201canchor tenants\u201d; their value comes from the fact that they are different \u2014 they know different people, have experienced different things, and thus, have much to teach. So invite them to your dinner party and they will bring people outside of your social circle along.\n\n22 \u2014 Be Interesting: Have a unique point of view. Be a content creator. How? Latch on to the latest, most cutting-edge idea in the business world. Immerse yourself in it, getting to know all the thought leaders pushing the idea and all the literature available. Distill that into a message about the idea\u2019s broader impact to others and how it could be applied in the industry your work in. That is the content. Then to become an expert: teach, write, and speak about your expertise.\n\n23 \u2014 Build Your Brand: Within a network, your brand is powerful. It establishes your worth. It takes your mission and content and broadcasts it to the world. It articulates what you have to offer, why you\u2019re unique, and gives a distinct reason for others to connect with you.\n\n24 \u2014 Broadcast Your Brand: You have to start today building relationships with the media before you have a story you\u2019d like them to write. Send them information. Meet them for coffee. Call regularly to stay in touch. Give them inside scoops on your industry. Establish yourself as a willing and accessible source of information, and offer to be interviewed for print, radio, or TV.\n\n25 \u2014 The Write Stuff: Writing articles can be a great boost for your career. It provides instant credibility and visibility. It can become a key arrow in your self-marketing quiver, creating relationships with highly respected people and helping you develop a skill that\u2019s always in high demand.\n\n26 \u2014 Getting Close to Power: The famous and powerful people are first and foremost people: They\u2019re proud, sad, insecure, hopeful, and if you can help them achieve their goals, in whatever capacity, they will be appreciative. In America, there is an association for everything. If you want to meet the movers and shakers directly, you have to become a joiner. It\u2019s amazing how accessible people are when we meet them at events that speak to their interests.\n\n27 \u2014 Build It and They Will Come: All clubs are based on common interests. Members are united by a similar job, philosophy, hobby, neighborhood, or simply because they are the same race, religion, or generation. They are bound by a common proposition that is unique to them. They have, in other words, a reason to hang out together. You can take your own distinctive proposition and then take the extra step that most people don\u2019t. Start an organization. And invite those you want to meet to join you. Gaining members will be easy. Like most clubs, it starts with your group of friends, who then select their own friends. Over time, those people will bring in even more new and intriguing people.\n\n28 \u2014 Never Give in to Hubris: Arrogance is a disease that can betray you into forgetting your real friends and why they\u2019re so important. Even with the best of intentions, too much hubris will stir up other people\u2019s ire and their desire to put you in your place. So remember, in your hike up the mountain, be humble. Help others up the mountain along with and before you. Never let the prospect of a more powerful or famous acquaintance make you lose sight of the fact that the most valuable connections you have are those you\u2019ve already made at all levels.\n\n29 \u2014 Find Mentors, Find Mentees, Repeat: A successful mentoring relationships needs equal parts utility and emotion. You can\u2019t simply ask somebody to be personally invested in you. There has to be some reciprocity involved \u2014 whether its hard work or loyalty that you give in return \u2014 that gets someone to invest in you in the first place. Then, when the process kicks in, you have to mold your mentor into a coach; someone for whom your success is in some small or big way his success.\n\n30 \u2014 Balance is B.S: You can\u2019t feel in love with your life if you hate your work; and, more times than not, people don\u2019t love their work because they work with people they don\u2019t like. Connecting with others doubles and triples your opportunities to meet with people that can lead to a new and exciting job. If your life is filled with people you care about and who care for you, why concern yourself with \u201cbalancing\u201d anything at all?\n\n31 \u2014 Welcome to the Connected Age: Living a connected life leads one to take a different view. Life is less a quest than a quilt. We find meaning, love, and prosperity through the process of stitching together our bold attempts to help others find their own way in their lives. The relationships we weave become an exquisite and endless pattern."
    },
    {
        "url": "https://towardsdatascience.com/thoughts-on-recruiting-and-technology-5bae04b933e?source=user_profile---------37----------------",
        "title": "Thoughts on Recruiting and Technology \u2013",
        "text": "As I am about to enter into my senior year of college, the biggest priority is to get a job after graduation. I am quite sure it is a stressful thing for every college seniors across the country. In an attempt to dissect the job search process, I try to look at it as a 2-way street: recruiters and job seekers. Recruiters want to find talents to join their companies, while job seekers want to seek out the best place to work at. As a job seeker, I think it would be very interesting to look at it from a recruiter\u2019s perspective. How to source the right candidates? How to evaluate one person against another? How to make sure that a strong candidate does not lose interest in the company?\n\nLast week, I went to a meetup event at Hired, a career marketplace that brings together job seekers with the companies who want to hire them. They have a panel discussion on the intersection of technology and recruiting. In a highly innovative and tech-oriented place like San Francisco, software invades every functional department of a company, even in non-technical positions like sales (Salesforce), user research (UserTesting), inbound marketing (HubSpot)\u2026 Recruiting is no exception, with the introduction of products like Jobvite, Workable, ApplicantStack\u2026 In this discussion, the panel talked about innovative tools used to recruit and hire top talent, and how technology is transforming the way companies approach every aspect of talent acquisition. The panelists are:\n\n\u00b7 Vivek Reddy \u2014 Former Head of Talent at Entelo\n\nIn my usual writing style, I would recap the questions and answers throughout the discussion, as well as add a bit of my thoughts on each of them.\n\nThe common denominator of most responses from the panelists is that there is simply too much noise out there; thus it is very difficult to prioritize your job applications. From a job seeker\u2019s perspective, how can one amplify the signal? (How to spot the right opportunity to go for?). There is also a big discrepancy in both sides: that companies can\u2019t source the right candidates and the job seekers don\u2019t know what they want. Aline gave an example of software engineering recruitment \u2014 that it should be completely meritocratic (based on a candidate\u2019s programming skills), but a lot of companies go for pedigree instead (based on the name of the college they go to or the company they work for previously).\n\nFrom my point of view, at least the last part is true, that a lot of tech companies here in the Valley are more impressed by the candidates\u2019 education. If you study at Stanford or an Ivy League school, your chances will be significantly better than the rest of the application pool. Studying at a liberal arts college, I really need to take initiatives to come up with strategies to get attention from recruiters: personal networks, conferences and meetups, social media engagement\u2026 to name a few.\n\nProducts such as Hired and Greenhouse do make it easier for both the companies and candidates to interact and understand more about the other side. There are 3 important things that I found interesting from the panelists\u2019 responses to this question:\n\n\u00b7 Technologies shifts the power dynamics between the candidates and the companies. Traditionally, companies can just wait for candidates to apply for open positions. Now the companies have to brand themselves as a marketable and worthy place to work at, because there are many other options for candidates to choose from platform like Hired or Greenhouse.\n\n\u00b7 The role of the recruiters changes to brand ambassadors. This is very intuitive: if companies need to brand themselves, it is the job of the recruiters to talk to the candidates and show the quality, benefits, and any positive things about opportunities to work there.\n\n\u00b7 Companies need to adapt technologies in other functional areas to nurture candidates. This goes back to the point I made in the 1st paragraph \u2014 sales, marketing, HR all need to utilize technologies to train new hires and get them acquainted to the work culture.\n\nAline brought up another good solution, which is to use technology to evaluate interview performances. Her company, Interviewing.io, essentially provides sample tech interview questions for software engineering candidates and then rank them using a scoring algorithm.\n\nHowever, the most fascinating answer is from Gemmy, Hired\u2019s Head of Product \u2014 applying machine learning and big data to help with the sourcing and hiring process. He mentioned that Hired\u2019s product team is using machine learning algorithms to match candidates with the jobs they will most likely to be interested in, based on data from candidates\u2019 profile and companies\u2019 job descriptions. I think this is a total game changer because it will completely democratize the hiring process."
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/7-machine-learning-applications-at-google-843d49d77bc8?source=user_profile---------38----------------",
        "title": "7 Machine Learning Applications at Google \u2013 The Aspiring Programmer Journal \u2013",
        "text": "It is truly amazing that my Medium profile has been gaining significant traction thanks to a singular post, \u201cThe 10 Algorithms Machine Learning Engineers Need to Know.\u201d Up to this point, it has close to 500 recommends and has been featured in the LAB41 collection (findings, experimental results, and thoughts on big data challenges) that specializes in machine learning, data science, and deep learning. Because of that, I want to write more posts on this topic of machine learning and contribute knowledge to this increasingly popular technology trends.\n\nSteven Levy\u2019s article, \u201cHow Google is Remaking Itself as a Machine-Learning-First Company\u201d, is one of the most popular piece over the summer. Essentially, it shows how Google has been obsessed with machine learning technology since the beginning of 2016, with initiatives like open-sourced TensorFlow and the Brain Residency Program. As the most favorite place to work in the world, Google\u2019s mission is to organize the world\u2019s information and make it universally accessible and useful. So it comes to no surprise how much the company is investing in artificial intelligence, the future of technology. About 2 weeks ago, I had the opportunity to attend a talk at Galvanize to learn about some of the cool machine learning applications at Google. The speaker is Christine Robson, a product manager for Google\u2019s internal machine learning efforts. Here are the 7 applications and products that Christine described as the coolest use of machine learning at Google:\n\nGoogle Translate is a free multilingual statistical machine translation service to translate text, speech, images, sites, or real-time video from one language into another. When Google Translate generates a translation, it looks for patterns in hundreds of millions of documents to help decide on the best translation. By detecting patterns in documents that have already been translated by human translators, Google Translate makes intelligent guesses (AI) as to what an appropriate translation should be.\n\nPersonally, I use Google Translate a whole lot when I live and travel abroad last semester. I lived in Copenhagen, where the main language is Danish, which I am not familiar with. So whenever I do grocery shopping, I always use Google Translate to detect the products\u2019 labels and figure out what they mean in English. When I travel to other European countries, I also use Google Translate to figure out the street signs, the subway banners, and other navigation-related texts. It is really an amazing and simple piece of technology that saves me a lot of time.\n\nGoogle Voice Search allows users to use Google Search by speaking on a mobile phone or computer, i.e. have the device search for data upon entering information on what to search into the device by speaking. It is Google\u2019s effort to compete with Apple\u2019s own Siri voice assistant, and has been said to be amazingly quick and relevant, and has more depth than Siri.\u201d\n\nI own an Android phone, so I know this feature better than anyone else. My biggest liking of Google Voice Search is its integration with other products such as Google Maps and YouTube. When I don\u2019t feel like typing, I can say my searches and information will pop up immediately.\n\nThis feature is amazingly favored by busy professionals whose inboxes are flooded with emails every day and they don\u2019t have time to respond to all. Smart Reply uses machine learning to automatically generate replies to emails, saving mobile users the hassle of tapping out answers on those tiny keyboards. According to Christine, this feature accounts for 10% of all email responses sent on mobile, quite an achieving feat."
    },
    {
        "url": "https://medium.com/inside-product-management/everything-you-need-to-know-about-sales-management-345423a35bd8?source=user_profile---------39----------------",
        "title": "Everything You Need to Know About Sales Management \u2013 Inside Product Management \u2013",
        "text": "Everything You Need to Know About Sales Management\n\nIf you\u2019ve been reading my blog, you know that I\u2019ve been to a lot of tech talks this summer. My favorite type of talks include a panel of experts who answer a series of questions regarding their specialty; for example, the Asana Intern Q&A event I attended a month ago. The most recent panelist-type talk that I attended is about sales. Now you might think, why do I bother care about sales given that I\u2019m more of a product person? The truth is, sales is an integral functionality of any tech companies, especially enterprise software ones. If your company is building a consumer product, strong marketing strategies will help you gain customers. But if your company is SaaS-based, then strong sales people are a must to get clients. I have been interested in B2B companies for a while, and learning about sales is the perfect way to know more about the enterprise industry.\n\nThe talk I went to is called \u201cHas Sales Tech Diminished the Need for Sales Savvy?\u201d Essentially a lot of CRM technologies such as Salesforce, Infusionsoft, Pipedrive\u2026 have automated the sales process, making it easier for sales people to do their job. However, is it a substitute for hard work, study, and team building? As demand generation consultants, it is possible that, because of sales automation tools, salespeople may abdicate their responsibility as mentors, coaches, and learners. The talk is a discussion on how traditional methods of sales development \u2014 research, determination, and collaboration \u2014 continue to be critical factors in a team\u2019s and company\u2019s success. It was hosted in the office of MuleSoft, a software company that provides integration platform for connecting applications, data sources and APIs, in the cloud or on-premises. The panel consists of 5 industry experts:\n\nAs mentioned above, the context of the talk focuses on sales tech (competence) and sales savviness (confidence).\n\n\u00b7 Sales Tech: With many tools to learn and several automated tasks to tackle, sales people increasingly encounter solution fatigue \u2014 not knowing what to utilize with the vast amount of resources given to them.\n\n\u00b7 Sales Savviness: Good sales people are also strategic account managers. They understand the narrative to deliver from a company\u2019s perspective and their role value.\n\nThe big topic here is\n\nBelow is my notes on the question asked and responses from the panel.\n\nThe 2 most important skills that a salesman need to learn are deliberate practice and teamwork + accountability. If that\u2019s too general for you, there are a few other more specific approaches:\n\n\u00b7 Instead of only relying on data sources, try to become a data partner \u2014 analyze the data and draw conclusions yourself.\n\n\u00b7 Invest in pipeline management \u2014 your sales pipeline is the key to evaluating, managing, and ultimately improving your sales process, so you can close more deals.\n\n\u00b7 Understand the sales stack \u2014 the totality of sales software (typically cloud-based) that a particular sales team utilizes.\n\n\u00b7 Differentiate between account sales (the SDR work on specific accounts) and people sales (the SDR are assigned work based on their attributes).\n\nAn ideal salesperson has to have a growth mindset \u2014 that dedication and hard work can extend his/her abilities. He also should have a high level of intellectual horsepower as well as a solid degree of intellectual curiosity. Finally, coachability and likability count as important factors because an ideal sales person is respected by his mentors and colleagues.\n\nIt is neat that he prefers short-term memory over long-term memory, so he can quickly heal from failed attempts. It is more important that he pays crucial attention on figuring what went wrong and solving that issue. It is encouraged that he is an experimenter, testing and iterating different ways while persuading a client. Finally, it is vital that a safe environment is enforced within the team\u2019s culture so that every team member is willing to be vulnerable and share his lessons that benefit the rest of the team."
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/5-things-about-mobile-development-you-would-want-to-know-74332b33b2ce?source=user_profile---------40----------------",
        "title": "5 Things about Mobile Development You Would Want to Know",
        "text": "5 Things about Mobile Development You Would Want to Know\n\nOver the early part of the summer, I have the opportunity to work on a mobile app called Aura for a brief amount of time. It is a productivity app that structure to-do list and send out motivational quotes on a daily basis. It is built using React Native for the front-end UI and Node.js for the back-end infrastructure. I no longer helped developing the app, but it is still really cool because it was my first experience doing mobile development. As you probably know, as digital usage moves from the web to smart phone devices, it is important that a product has both a web version and a mobile version (iOS, Android, or both). This blog post serves as a purpose to provide guidance for aspiring founders who want to create a mobile app; in particularly going from idea to app development. The main content is taken from a talk at General Assembly that I went to a month ago, given by Martina Freers \u2014 founders of Inaka, a company headquartered in Brazil whose expertise is in bootstrapping (taking a startup idea and building a MVP). Inaka works mainly with other startups and helps them launch quickly with codebase that can grow with them and their businesses. Inaka\u2019s specialized technologies include programming languages (iOS, Android, Erlang, Ruby) and infrastructure and database design (AngularJS, React.js, Elixir, AWS, Riak, Postgres, SQL). There are 5 main areas to be considered when thinking about building a mobile-based business. Here they are: A thorough dive into each of these areas will provide useful insights and knowledge for founders as they navigate their way into going from an app idea to a concrete product.\n\nThis is the incubating stage in which the founder has to figure out everything on his own. You already have an idea of what you want to build, what are the subsequent steps you need to do in order to build your app? There are actually 3 types of documents that need to be taken care of: 1 \u2014 Product Requirements Document (PRD): a doc that lists the requirements of what the product is supposed to do at a high and mid-level of detail. The PRD\u2019s objective is to get everyone on the same page. It is usually written by a Product Manager and entails: It should be noted that the PRD is the main channel for the product manager to communicate with engineers, customers and other stakeholders. Therefore, it is of utmost importance for the PM to own the PRD. 2 \u2014 Specifications: formal documentation that describes in detail a product\u2019s intended capabilities, requirements, and user interactions. This written document should by minimum include platform types (iOS, Android), detailed functional requirements, broken-down features, MVP vs follow-up versions, algorithms, and differences between platforms (web vs mobile app). There have been argument that specs is not that important in the development process. However, there are many benefits of a well-written specs, including: \u00b7 What not to include in the development process, which helps avoid spending development cost on things you later realize you don\u2019t need. \u00b7 Investors don\u2019t like ideas, they like products. Thus, a detailed spec presented to investors will increase your chance of getting investment. \u00b7 Wrong architecture decisions can lead to big trouble later. A good spec with clear algorithm analysis should outline the architecture of the app comprehensively.\n\n3 \u2014 Wireframes: visual documentation of the specifications, or a display of the functional elements of the product and interactions of its parts. Both PRD and specs are universal for any type of software products, but wireframe is uniquely for mobile products. Wireframes specify the overall flow of the app, meaning that you need to think of the joint project as a whole and not just individual screens. Here are some questions to be considered while making the wireframes: \u00b7 How do I get from one screen to the next? \u00b7 Does the flow work as I thought it would? \u00b7 Will errors, popups, notifications etc. synchronize well with the entire flow? Common delivery of wireframes comes in many different screens. Always remember that screens are only part of the flow. It is of utmost importance to think about edge cases when constructing the screen navigation steps. I personally don\u2019t have much experience with wire-framing; and intend to learn more about it through an Udemy\u2019s course on Sketch. I would urge you to do the same if you are interested in mobile design. The common view for a failed project is that the development team is of poor quality (either offshore or in-house). That can be true, but in reality, no specs / no wireframes can significantly contribute to failure. So write good specs, and that will increase the quality of your work and save you cost and time. 4 \u2014 Design: Intuitively speaking, the 3 documents above all contribute to the design process of the product. I\u2019m not a designer myself, but I have been an avid reader of many design websites, blogs, and communities. Here\u2019s a few questions to be considered while designing your app: \u00b7 How does it look and feel? How do its animations work? \u00b7 How to solve UX and UI problems? \u00b7 What to choose for styles and color guidelines? \u00b7 Does my designer have enough app experience? \u00b7 What should be the differences between iOS and Android versions? If you want to learn more about design, I recommend joining the Dribble community and reading Don Norman\u2019s \u201cThe Design of Everyday Things\u201d \u2014 a timeless classic. This is the point in which you will need to find experienced developers and project managers to work with; because of your app scales, it is going to be harder to take care of everything on your own. The objective of selecting a partner is to find a consultant whom you can trust and who tells you when you are wrong and gives you possible solutions. Again, there are 3 points to think about when selecting your development partner: \u00b7 Talent & Experience: Ask for their portfolio. Ask what programming languages they are comfortable with. Ask for references from their past employers. \u00b7 Passion for the Product: Someone who is passionate about your product will be able to solicit even the smallest details and show empathy to it. \u00b7 Mutual Understanding: Are their standards and values well-aligned with you? \u00b7 Technology: You will want your technology (code base) to be scalable, reliable, maintainable and standardized. How do they test and do code review? Are they comfortable writing documentation? What\u2019re their technology fit? I want to dig deeper in the tools section, specifically on project management tools and source code control, because they are different for each product. 2 \u2014 Project Management: It is very important to distinguish between product management and project management. A product manager in a startup is the one to come up with the vision for the product and strategy on moving it to the right direction. The founder should be the product manager. On the other hand, a project manager focuses on executing the strategy built by the product manager; thus a project manager is usually a development partner. Another question arises: Why can I be both the product manager and the project manager? The truth is that you already spend a lot of time with customers and developers and carry a lot of coordination. Chances are you don\u2019t really have time for project management: gathering info, structuring info, moving info to right people, increasing clarity, making sure everything is getting done. Having a dedicated project manager is helpful because he will make sure the project stays on track throughout the development process. 3 \u2014 Source Code Control: Best place to keep source code is to check it into a repository which is accessible to you. GitHub comes in handy for this. But why do you need source code control at the first place? Very simple, this is your product, your code \u2014 you need to understand what\u2019s going on. Having a streamlined process will alleviate the pain of communicating back and forth with your developers. Here\u2019s some of the things to keep in mind with source code control: \u00b7 Keep ownership and have admin access If you don\u2019t do one of those things, disaster will strike if you have disputes with the development team and the developer disappears. You either can\u2019t publish your code or have to start from 0. This stage happens after you have successfully built and delivered the Minimum Viable Product (MVP). But first, I want to go over partnership and development. At this stage, you have full momentum, you experience rapid growth ideally; so you start seeking out partnership opportunities, while at the same time maintain the development process. The ability to carry out both activities is pivotal at this point.\n\n1 \u2014 Partnership: You should have a dedicated person to focus on partnership outreach. He must understand your product and vision to market it to potential partners. This will help avoid future changes due to misunderstandings. You should accept the advisor role overseeing the product and sticking to the MVP. A few strategies while working on partnerships: \u00b7 If you don\u2019t understand something, be honest 2 \u2014 Development: Ideally, you already have a dev partner who focuses on the development process. His responsibilities should include: \u00b7 Give advice on what to do and not to do \u00b7 Make sure milestones and estimates are reached In order to ensure an effective development process, you should always ask him to inform you immediately about any delays or tech issues, as well as give reasons for those. It\u2019s your job to be pushy and over-communicate. As you can see, communication is key. Always be alert about changes so you can mold your product towards perfection. Make sure that everyone always has to be on the same page. Finally, always think about how the existing features can be improved. 3 \u2014 After MVP: It can be challenging for the after-MVP stage. What should I do next? Properly, you should focus on 3 things: \u00b7 Support: How to build a support system for your product infrastructure \u2014 consider different service provider, keep in mind server costs, focus on fixing bugs. \u00b7 Handover: Take care of all documentation, complete code and server access, users\u2019 accounts and passwords. Transfer that knowledge to your team when the time is right. \u00b7 Future Versions: Think about what you want to include in the future versions of your product using analytics on the current version\u2019s performance. Don\u2019t be afraid to get rid of things that are not working. Consider expanding your team if necessary. This is the single biggest concern I heard from other app founders. Either no one likes their apps or people don\u2019t know their apps even exist. In either cases, growing your user numbers would require some sort of marketing strategy. Don\u2019t hire a marketer right away though, because I find marketing is quite intuitive and easy to master if you pay attention your users. A few things to keep in mind: \u00b7 Users do cost money. At first, you might have to pay users to get them onboard, which sounds harsh in reality. But as more users get onboard, you can start conducting user research and figure out what to build that solve your users\u2019 needs. \u00b7 You are not alone. This is a universal problem for all mobile app founders, so don\u2019t sweat too much. Forget about your competition and focus on users\u2019 response to your product. \u00b7 Viral marketing. Mobile app is especially prone to virality, as seen in the examples of Snapchat, Instagram or Pokemon Go. Learn from their successes and see what you can apply to your case. \u00b7 Apps don\u2019t grow on their own. Learn to use some marketing and analytics tools like Hubspot and Mixpanel to track user behaviors and find useful insights. \u00b7 Invest into users at least what you invest into development. Sort of like a conclusion to all the points made above. Building an app is a real investment, not just effort and energy but more important, financial resources. How much does an app cost? \u00b7 A small app with no server and no system admin costs approximately $30,000 and less \u00b7 A middle app with the simple frontend, nicely-designed server, and functional system admin costs between $30,000-$80,000 \u00b7 A big app with complex server work, detailed system admin and well-designed frontend costs between $80,000-$150,000 Considering the expensive costs to build a mobile app, what other factors might contribute to driving the cost up? \u00b7 Undefined goals, not getting ready for launch \u00b7 Not concentrating on the MVP Keep in mind those things in mind as you build your next application to avoid spending inefficient money and resources. This is probably one of my longest posts so far, so I would love to hear your thoughts on these practices if you are at all into mobile development. One of my goals towards the end of this summer is to learn a bit about Android development, so let me know if you are interested in that and we can share resources!"
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/an-insiders-perspective-on-building-better-software-2957541016a3?source=user_profile---------41----------------",
        "title": "An Insider\u2019s Perspective on Building Better Software",
        "text": "An Insider\u2019s Perspective on Building Better Software\n\nThere are a lot of things going on in between each of these stages that can potentially hinder the progress of building the software. The question is \u201cHow to remove activities that suck?\u201d That\u2019s a broad question but Chris narrows it down to 2 components:\n\nThe goal of any software development process is to provide better experience for the developers. What does it mean? Code faster? Ship faster code? Ship code with fewer bugs? All of these things and many other small things. There are 7 things that a software developer needs to do on a daily basis, in no order of importance:\n\nIf you are a Computer Science student like me, you probably know that software engineering is the obvious choice for an entry-level position. Most CS students think that if they can do reasonably well in school, they will be set for an easy career in software development. I agree that CS fundamentals like algorithms and data structure are pivotal to understand, but there are in fact a lot of other challenges that a software developer has to deal with in a professional environment. Chris, with many of years of experience working and leading software engineering team, did a great job describing the process and suggesting a few solutions to make the process more efficient. I will quickly summarize the main points from his talk and add on a couple insights of mine.\n\nPhysically being in San Francisco this summer has been one of my best experiences toward professional development. As someone who is really interested in the tech culture and innovative technologies, I am thrilled to be in the center of that movement. One of the things that I have actively been doing this summer is using Meetup.com to go to different events of interests, meet new people and learn what\u2019s trending in the industry. Most recently, I was at a Node.js meetup hosted in Heroku. Node.js is an open-source, cross-platform runtime environment for developing server-side Web applications. Heroku , on the other hand, is a cloud platform-as-a-service supporting several programming languages (acquired by Salesforce). The topic of the talk is \u201cSoftware Development Process\u201d , given by Chris Castle \u2014 a developer associate at Heroku.\n\nSounds pretty intuitive, isn\u2019t it? For the first point, it really depends on the situation for each team. The engineering lead needs to figure out what are constraining the team\u2019s efficiency and comes up with a way to get rid of those. Which ties directly to the second point, because communication is the key to solve it. There are 3 stages during the software development process that require heavy communication: deploying, designing, and measuring. With examples from his work with the Heroku engineering team, Chris digs into each of these stages and suggests ways to leverage them.\n\nDeploying: The deployment process should be dead simple and fast, and there should be a very low bar requirement for it. If you can click a single button to deploy your code, that would be ideal. If your deployment activities can be easily tracked by other team members, that would also be very ideal. So think about ways to make your deployment process a public event and as simple as it can be.\n\nDesigning: Designing software is complicated. This is the part that requires a lot of creativity and grey matter throughout the process. The reason is that people conceptualize and develop ideas very differently. One engineer may like to draw diagrams or pictures, another may enjoy writing down in paragraphs, or one might simply want to go straight into coding. Thus, it is vital for the team lead to provide a loosely regulated work environment that fosters experimentation to maximize each team member\u2019s brainpower. The team, as a whole, should spend mental energy on developing the ideas, not on stressing about the unnecessary communication medium.\n\nThere\u2019s one specific point I want to highlight from Chris\u2019s talk on designing software, a process called branching. If you are active in the open-source community, you know that when you want to contribute to an open-source project, you need to deploy the code to your local environment, write the code in your laptop, create your own branch within the master branch of the project, submit the code you wrote in your branch, then ask to merge it with the master branch. The admin of the project will review the changes you made and decide whether to merge them with the master branch or not. I recommend checking out Github\u2019s explanation on branching here. There are 2 ways to apply the branching approach to your software design:\n\n\u00b7 The \u201cmany ideas\u201d approach \u2014 in which from a single idea, you can diverge it to multiple small branches of ideas, and then assign each branch to a team member to work on based on his/her interests.\n\n\u00b7 The \u201cevolving ideas\u201d approach \u2014 in which one idea subsequently leads to another. There is always a constant stream of evolvement and there is no single centralized idea. With this approach, you need to onboard your whole team at the beginning and keep them firmly grounded throughout the whole design process.\n\nMeasuring: This stage combines both review and test activities. Start with a hypothesis based on the question \u201cWhat should we change and why?\u201d This hypothesis is based on your metrics collected from the review activity (RPM, CPU, RAM, HTTP error count, HTTP error rate, Error details\u2026 to name a few). After that, deploy and test your hypothesis. Some key metrics to remember in this stage include memory usage, response time, and throughout (# of requests per min). One quote that Chris mentioned in his talk that I absolutely love:\n\nI really appreciate this talk in a sense that it opens up a lot of perspectives on software development. I am not particularly interested in going into software engineering per se, I am more of a product person. But a product manager still needs to be extremely technical and be curious about how engineers can build the product (architecture, tech stack, programming languages, third-party platforms\u2026) Thus, I am glad for the opportunity to understand a bit more about the challenges software developer have to face and work towards becoming an efficient product manager.\n\nIf you want to learn more about software development (particularly if you want to go into software engineering), I recommend reading this book \u201cJoel on Software.\u201d I just ordered it on Amazon a week ago and am eager for it to arrive. The author, Joel Sposky \u2014 well-known as the founder of Stack Overflow (basically any programmers\u2019 best friend) \u2014 compiles all the best practices on building software from his blog into a single book, which has been highly endorsed by many influencers in the industry. So check it out if you have time!"
    },
    {
        "url": "https://medium.com/inside-product-management/how-to-determine-product-market-fit-ac81780f9f82?source=user_profile---------42----------------",
        "title": "5 Product-Market Fit Areas Every Startup Founders Should Consider",
        "text": "5 Product-Market Fit Areas Every Startup Founders Should Consider\n\nTalk to any startup founders in the Bay Area, and you will know that one of the biggest challenges for them is to determine the product-market fit for their own product. What is product-market fit? Marc Andreesen, whom the term is attributed, defines it as \u201cbeing in a good market with a product that can satisfy that market.\u201d Countless influencers in the tech industry have given advice on how a startup can hit product/market fit, from Marc Andreesen himself, to Steve Blank (who started the Lean Startup school of thought) and Paul Graham (who founded Y-Combinator, the most influential venture capital firm in Silicon Valley).\n\nLast week, I went to talk called \u201cProduct Market Fit: The Importance of Getting It Right\u201d at the Nasdaq Entrepreneurial Center. The 2 speakers are Sean Jacobson, a general partner at Norwest Venture Partners (a global, multi-stage venture capital growth equity investment firm with approximately $5 billion in capital), and Michelle Zatlyn, co-founder of CloudFare (a web performance and security company that provides online service to protect and accelerate websites online). In the talk, Sean interviewed Michelle on how she was able to know that CloudFare has achieved product-market fit, particularly after its series-B fundraising round. Sean outlines the 5 categories that startup founders need to look at while assessing their product:\n\nFor each of these categories, Sean came up with 3 options, correlating with the maturity stage of the product with respect to that specific category. He then asked the audience, which comprised of many startup founders, to evaluate their own products based on these 3 options. Option 1 gets 1 points, option 2 gets 3 points, and option 3 gets 5 points. The more points the product gets, the closer the business reaches product-market fit. In this post, I want to share Sean\u2019s product-market fit evaluation rubric, because I think it is extremely note-worthy for founders to consider so they can make proper move while developing their business.\n\nHow many customers you did not know before starting the company?\n\n\u00b7 Your customers are only fellow startups in your incubator class (1 pt)\n\n\u00b7 Your customers include other tech companies in Silicon Valley (3 pt)\n\n\u00b7 Your customers span around the country and outside of your network (5 pt)\n\nHow core is your product to the customer\u2019s business?\n\n\u00b7 Users log in once every month or two in sporadic bursts (1 pt)\n\n\u00b7 Users leverage the product daily to get their job done (5 pt)\n\nWhat is the rate at which you are losing customers?\n\n\u00b7 You haven\u2019t started measuring churn yet. You need more data to assess why you\u2019re losing customers (1 pt)\n\n\u00b7 You experience 2\u20134% churn per month or 24\u201348% churn per year. Customers are actively giving you unsolicited feedback on how to improve the product (3 pt)\n\n\u00b7 You experience 0\u20131% churn per month or up to 12% churn per year (5 pt)\n\nDo you have customers on pilot programs?\n\n\u00b7 Users have committed and are likely to test other products in the market too (1 pt)\n\n\u00b7 Users commit to your product on a month-to-month basis (3 pt)\n\n\u00b7 Users commit to your product to 1\u20133 year deals (5 pt)\n\nHow are you acquiring new customers?\n\n\u00b7 Sales are mixed between outbound cold calling and inbound leads. People start to search for solutions in your market (3 pt)\n\n\u00b7 Most sales are generated from inbound leads and customer referrals (5 pt)\n\nFinally, this is the product market fit point assessment:\n\n\u00b7 0\u20139 points: You are in the product testing phase to define what business you are in. You should not hire any salespeople yet, but you should definitely get market feedback outside of your local tech community.\n\n\u00b7 10\u201317 points: You start getting early validation that your product solves a potential pain. It might be time to hire 2 sales representative. If both of them perform well, then your product really resonates with the market.\n\n\u00b7 18\u201325 points: Your product is well on the way in reaching product-market fit. You should think about expanding your sales team to catch even bigger opportunities.\n\nThere you have it, a simple rubric to determine how you are doing in the process of getting product-market fit. Let me know if you agree or disagree with this evaluation system!"
    },
    {
        "url": "https://medium.com/inside-product-management/the-surfers-rules-for-innovators-and-entrepreneurs-bf4f60c2b225?source=user_profile---------43----------------",
        "title": "The Surfer\u2019s Rules for Innovators and Entrepreneurs",
        "text": "The Surfer\u2019s Rules for Innovators and Entrepreneurs\n\nLast week I went to a talk in the Nasdaq Entrepreneurial Center titled \u201cMake Your Own Waves\u201d by Louis Patler, a well-known author, speaker and consultant. He just finished writing his book \u201cThe Surfer\u2019s Rules for Innovators and Entrepreneurs\u201d, and has been doing a lot of talks to promote its publication. I haven\u2019t had a chance to read the book yet, but in the talk, he basically gave a quick summary of the book, which I deem very interesting. In essence, he draws an analogy between Big Wave surfers and entrepreneurs. As someone who has real interest and connections with the surfing community, Louis says that the Big Wave surfers possess traits such as preparedness, focus, patience, creativity, and courage. These traits mirrored his research findings of successful, serial entrepreneurs and innovators. In his book, Louis comes up with the 10 Surfer\u2019s Rules that makes up the analogy fit: the first 4 address the hard work to be done before even trying to ride a wave; the middle 3 focus on the rides, wipeouts and determination to get up after set-backs; and the last 3 examine how important it is to not rest on your laurels, to collaborate and to stay passionate about what you do.\n\nIn this blog post, I want to do a quick recap of the 10 rules from Louis\u2019s presentation, as I find them totally worth being shared:\n\nThe basics set the stage for everything.\n\n\u00b7 For the surfer, it means learning how to swim.\n\n\u00b7 For the entrepreneur, it means doing homework on competition, partnerships, and resources. In other words, get to know who\u2019s doing what out there by doing competitive analysis on the market and know people on a social level.\n\nSuccess comes to those who try.\n\n\u00b7 For the surfer, getting into the water is the logical step following the decision to swim.\n\n\u00b7 For the entrepreneur, get wet means get themselves to start to see the world from a different mindset, aka the entrepreneurial mindset.\n\nThere are a lot of decisions to be made early on.\n\n\u00b7 For the surfer, this is the decision to choose what level of wave to ride on (small or big).\n\n\u00b7 For the entrepreneur, this is the first decision point of the business \u2014 at what \u201cscale\u201d to work with and how to ramp it up.\n\nWatch for what\u2019s coming or you may miss a better opportunity.\n\n\u00b7 For the surfer, \u201coutside\u201d refers to the waves coming after the visible waves. An experience surfer will estimate and know ahead which big waves to ride in.\n\n\u00b7 For the entrepreneur, this is the step to do before launching the product \u2014 how to optimize the launch so that he doesn\u2019t miss a good opportunity. Louis suggests the entrepreneur to study the market trends, history of similar products, and organizational structure of similar companies.\n\nYou have to go all out to be all in.\n\n\u00b7 For the surfer, Big Wave surfers only surf 2, 3 months during the season, and spend the rest of the year practicing. It means he has to be at the best form all year around so he can perform well during the season.\n\n\u00b7 For the entrepreneur, he needs to assess everything carefully, be well-prepared mentally, physically, and spiritually during the launch of his product.\n\n\u00b7 For the surfer, paddle back out means going back to the ocean after failing to ride the wave for an instance.\n\n\u00b7 For the entrepreneur, this is synonymous with embracing failure, not giving up and trying again.\n\n\u00b7 The surfer should never turn his back to the ocean because some big waves might approach him and it will be a dangerous scenario if he can\u2019t react properly.\n\n\u00b7 The entrepreneur needs to always stay in touch with the marketplace and the customers, as something disruptive might happen in the market or the customer might dislike certain things about the product, which in both cases can cause problematic issues.\n\n\u00b7 The surfer needs to stay grounded to seek the big waves to ride in.\n\n\u00b7 The entrepreneur needs to stay ready to seize the big opportunities.\n\nCollaboration is the name of the game.\n\n\u00b7 The surfer should get involved with the surfing community and meet other surfers to practice and hone his skills.\n\n\u00b7 The entrepreneur has to seek out mentors, venture capitalists, business partners, technical talents\u2026 to keep his business alive.\n\nDesire drives success, passion carries the day. This is simple enough to understand: both the surfer and the entrepreneur should have insatiable desire and fierce passion to practice their craft and improve themselves.\n\nSo there you have it, the 10 surfer\u2019s rules that are applicable for entrepreneurs and innovators. I would definitely check out Louis\u2019s book soon, and I recommend you to do the same!"
    },
    {
        "url": "https://medium.com/inside-product-management/interactive-behavioral-analytics-for-winning-in-the-digital-economy-a8c534a80f90?source=user_profile---------44----------------",
        "title": "Interactive Behavioral Analytics for Winning in the Digital Economy",
        "text": "Interactive Behavioral Analytics for Winning in the Digital Economy\n\nIf you have read my recent post, you will know that I am interested in product management and data science. These 2 fields combined create a data-driven product development process, which is a powerful approach in a lot of SaaS companies nowadays. Recently, I went to a product meetup in Imgur headquarter in San Francisco, and had a chance to mingle with other product people as well as learn about the trendy product practices. There were 3 talks presented, all centralized on the topic of the meetup \u201cInteractive Behavioral Analytics for Winning in the Digital Economy.\u201d The 3 speakers are Ajay Arora (VP of Product at Imgur), Jackson Wang (Analytics Lead at Tilt), and Lior Abraham (Cofounder of Interana). The common theme of the 3 talks is how to use behavioral analytics to get an in-depth understanding of exactly what users want, need, and don\u2019t like to develop the right strategies to attract users, increase engagement, and reduce churn. Below are my notes on their talks, which I deem to be extremely great insights for any software product people out there.\n\nImgur is one of the highest trafficked sites on the Internet and ground-zero for the most engaging and viral content on the Internet. As VP of Product, Ajay leads the product, design and data teams, with joint meetings and product proposals reviews. Ajay provides 3 lessons he has learned since he joined Imgur:\n\nIn particular, Ajay gives an example of Imgur user retention problem. Instinctively, he thinks that when a user attempts to register with Imgur mobile app, the user will more likely to stick with it longer. However, the data show that users are actually more likely to quit the app right after they visit the app \u2014 which can be hard to explain.\n\nUser research comes to the rescue at this point. The Imgur team spends time talking with its users, and discovers the intuition behind the user retention problem. The user struggles with the onboarding process; in specific, he can\u2019t create an account because all of the familiar usernames have been taken. For that reason, he quits.\n\nThe simple solution that Ajay and his team came up with is to apply an auto-suggestion algorithm that suggests usernames based on email account. Another minor detail is to input a line on the screen saying \u201cDon\u2019t worry! You can always change it later.\u201d This in fact keeps the users on Imgur app longer and they are much more likely to use it more frequently.\n\nThese 3 lessons show how Ajay and the Imgur team have been able to utilize data insights to track user behavior and come up with a solution that meet user needs.\n\nJackson Wang of Tilt is up next. At Tilt, he is in charge of measuring and helping to improve every part of the user experience on the website. Previously, he worked in the analytics team at LinkedIn. If you don\u2019t know, Tilt is a crowdsourcing product that allows for groups and communities to collect, fundraise, or pool money online. Or in other words, it is the easiest way to collect money from a group. Tilt\u2019s target demographic is mainly college campuses.\n\nBack to the point about behavioral analytics, Tilt\u2019s analytics team utilizes a super interesting model called \u201cuser state machine\u201d, which depicts different phases Tilt\u2019s users can go through and communicate: Phase 0 \u2014 New Users, Phase 1 \u2014 Organizers, Phase 2 \u2014 Contributors, Phase 3 \u2014 Visitors, and Phase 4 \u2014 Inactive Users. The main goal is to increase the number of organizers, because they drive contributors, visitors, and even inactive users. With this acknowledgement, Tilt\u2019s strategy is to focus on organizer\u2019s retention rate, which has been vital through the course of the company growth. Jackson concludes that the 2 main benefits of data analytics are:\n\n\u00b7 Decrease the cost of asking questions.\n\nFinally, Lior gives a presentation called \u201cSoftware Bottleneck.\u201d Lior is a former Facebook executive. He was instrumental in scaling Facebook\u2019s analytics and growth initiatives. Most prominently, he invented Scuba \u2014 a visual and interactive analytics solution that allowed Facebook to develop innovative strategies around growth that helped them get over billion users they have today. The main theme of his talk is Software is the Bottle-neck to Insight, meaning that software programs hinder the process of coming up with useful insights.\n\nEssentially, the state of data software 10 years ago was not good. A data scientist have to do a lot of manual metrics and pre-calculation \u2014 things like custom rollups, tracking grow with an exponent\u2026 So Lior\u2019s goal while developing Scuba is make it easy to write and read data. In particular, Scuba has analytics-mode only, ad-hoc exploration, a raw scan, visual display and agility. To quote Lior directly, \u201cSpeed made things interactive. Interactive made data accessible.\u201d The end result is that people began to use data differently:\n\n\u00b7 Even non-technical people start seeing more pattern\n\nIt is definitely important for companies (especially startups) to recognize the needs of exploiting behavioral analytics if they want to get leverage in the digital economy. Powerful tools like SQL, Facebook Analytics, Hadoop, Interana.. are great solutions for engineering/product teams to handle the massive amount of user data and come up with innovative strategies.\n\nTo quote one of the speakers,"
    },
    {
        "url": "https://medium.com/inside-product-management/the-6-point-formula-to-create-a-data-driven-business-1bf77fed8624?source=user_profile---------45----------------",
        "title": "The 6-point formula to create a data-driven business",
        "text": "A couple of weeks ago, I had a chance to attend a tech talk at Galvanize titled \u201cHow to Use Data to Optimize and Grow Your Business.\u201d The speaker is Chris Neumann, CEO of CROmetrics \u2014 a company that specializes in Conversion Rate Optimization, providing turn-key solutions that help turn traffic into paying customers. Chris has a product management background, but more importantly, he is a growth experts. There has been a trend in a lot of tech startups in the Valley shifting from traditional marketing methods to growth hacking. A growth hacker creates product/market fit, finds scrappy ways to get the word out, and optimize with data to grow. This talk is interesting to me because it provides the data-driven facets of product management, a path that I aspire to follow.\n\nThe overarching goal of the talk is to provide practical/tactical advice on how to use data instead of opinions. Why is it important? There has been empirical evidence that gut feel is a bad way to run a business. Data can be collected from conducting experiments. A/B testing is the most popular type of experiment, where we have a hypothesis and then try it on a half the traffic and see if it makes a difference to the business. These are often pretty thoughtfully planned out and designed to move the needle in the business. Despite all that, across a lot of data we only see a 20% win rate. That means that you can be super confident that you will win if you have data vs. the other person going with their gut.\n\nSo here are Chris\u2019s tips and advice on creating a data-driven business:\n\n1. The biggest obstacle to prioritize data over opinion is corporate culture: Everyone is used to managing based on their own opinion (Senior management wants it their way, engineers think they\u2019re better marketers than you and undermine your project). The tip here is to invite everyone to have their idea tested, which is a win-win.\n\na. If the idea is good, you get more lift.\n\nb. If the idea is bad, they eventually stop giving ideas.\n\na. You will be able to create a webpage, deploy code etc.\n\nb. You can empathize with engineers working for you.\n\nc. You will not try to spec things that are impossible to do\n\nd. SQL will give you a huge competitive advantage over other marketers, particularly in SaaS companies, but valuable for marketing programs at e-commerce companies\n\n3. Embrace a strong champion ideology internally: This means that every person in the company needs to take full ownership to conduct tests and experiments himself.\n\n4. Keep a nice balance between product and marketing: It is critical to segment these 2 business aspects \u2014 building the product and branding the product, because they are both vital in growing a business. Tools like CROmetrics will help you keeping them in line.\n\n5. Conduct User Testing: This helps you to experience the user\u2019s environment, as well as uncover areas of the business that are new to you. Chris suggests a few software that are fantastic user testing tools:\n\n6. Adopt company-wide internal tools that drive a strong data-focused culture. Examples that Chris is using at CROmetics include:\n\nSo there you go. With this list of advice and tools, you can go ahead and build a business that uses data to create the best product in your market!"
    },
    {
        "url": "https://medium.com/inside-product-management/the-future-of-work-asana-intern-q-a-2016-75ca91923c90?source=user_profile---------46----------------",
        "title": "The Future of Work: Asana Intern Q&A 2016 \u2013 Inside Product Management \u2013",
        "text": "This past Tuesday, I had a chance to attend a Q&A session hosted for Bay Area interns in the office of Asana, one of the hottest startup in the Valley. For those of you who don\u2019t know, Asana\u2019s product is an internal tool that helps teams collaborate and work together effortlessly. The event itself was extremely well-organized, attracting more than 150 attendees who were treated with delicious dinner and desert. I had a chance to meet and network with fellow interns and Asanas to learn about interesting things happening in the tech world.\n\nThe Q&A panel main event comprises some of the biggest names in the tech industry:\n\n\u00b7 Justin Rosenstein \u2014 cofounder of Asana, previously worked at Google and Facebook. He was the primary inventor of the Like Button, Facebook Pages, Gmail Chat, and was the original product lead for Google Drive.\n\n\u00b7 Tracy Chou \u2014 previously software engineer at Quora (employee no.6) and Pinterest (employee no.10). She is well-known for her work pushing diversity in tech. In 2013, she helped kick off the wave of tech company diversity data disclosures with a Github repository collecting numbers on women in engineering. She is now a founding member of Project Include, which focused on driving solutions in the space. She was named Forbes Tech 30 Under 30 in 2014, profiled in Vogue and Wired for her advocacy. She is also an advisor to Homebrew VC and on reserve with the US Digital Service.\n\n\u00b7 Joe Lonsdale \u2014 cofounder of Palantir (defense and finance multi-billion dollar global software company, founder of Addepar (leading wealth management tech platform), and OpenGov (open-source government tech), founding partner at 8VC and Formation 8 \u2014 very well-known SF-based VC fund managing over 1.5 billion dollar.\n\n\u00b7 Ruchi Sangvhi \u2014 first female engineer hired by Facebook, where she transitioned to PM and oversaw Facebook Platform and News Feed. She started her own company Cove in 2010, sold it to Dropbox in 2012, then became VP of Operations at Dropbox later.\n\n\u00b7 Dustin Moskowitz \u2014 cofounder of Asana. He is mostly famous for cofounding Facebook with Mark Zuckerberg, a key leader within the technical staff, first as CTO and later as VP of Engineering.\n\nThe topic of the session is the future of work, meaning how technology will change the workforce in the next decade and how young tech-oriented people like us can take advantage of such opportunities in order to create major impact on our communities, society, and humanity as a whole. Justin is the moderator for the panel and he had a list of questions previously submitted by attendees, curated by a sorting and ranking algorithm in order to pick the best ones. Each of the panelists took turn to give their response to the questions, sharing their knowledge and giving advice for the audience (most of them were very relevant to us interns). The whole Q&A lasted about 90 minutes, and below are some of best questions and paraphrased answers from the panelists, in addition with my comments.\n\nAn excellent question to start the evening. Each of the speaker cited out different examples but ultimately, the common theme is that it really depends on the company you work for.\n\n\u00b7 Dustin mentions that if you work for a startup, you will have to become a generalist no matter what just because there are a lot of things that need to get done. As the company scales and more people are brought in, you can get more specialized in a particular area that you are passionate about.\n\n\u00b7 Joe suggests that you should look for a startup that experience very significant growth, ideally post-series B/C; so that you can get a hand on a lot of things as a generalist, but also receive substantial mentorship to get specialized.\n\n\u00b7 Ruchi started her career as a specialist (software engineering) and stresses that the mentorship/training she got at Facebook had been extremely helpful in helping her transitioning into product management(a generalist role).\n\nContemplating an entry-level role next year, I probably will look at a more generalist role. This is mostly because of the nature of my liberal arts education at Denison, but also because I have had a chance to develop a variety of skills that can be put to use in the last 2 summers.\n\nA very futuristic question that get a lot of attention from the audience, judging how much impact technology can make on our society as a whole based on recent development in machine learning, artificial intelligence, virtual/augmented reality, IoT, robotics\u2026 to name a few. It\u2019s also appropriate as the SHAPE expo has just taken a place a few days ago here in San Francisco.\n\n\u00b7 Government is one of the biggest one. Joe talks about his experience founding and sitting in the board of OpenGov, which are transforming the way government analyze, share, and compute financial and performance intelligence. There are a trillion dollars of money being wasted annually on running at government at both local and national level; therefore, adopting technology is a smart move to improve the broken system. Tracy talks a bit about US Digital Service and Code for America, initiatives that utilize tech talent to solve governmental problems. It is actually kind of a coincidence that I have just watched a lecture the day before of Jennifer Pahlka, founder of both of these organizations, talking about big government problems (excessive spending, inefficient structure, outdated organizational systems) that can be of critical help with technologies.\n\n\u00b7 Education and healthcare are two other areas. However, Ruchi shows a concern that all of these industries are heavily regulated, so you must be very careful when thinking about opportunities there.\n\n\u00b7 Justin thinks that the best way to make the greatest impact is to help other businesses to solve their problems. That\u2019s the core reason why he and Dustin founded Asana, with the goals of improving the productivity of individuals and groups and increasing the potential output of every team\u2019s effort.\n\nThis question is clearly addressed to Tracy and Ruchi. It also implies a broader question of how to address diversity in tech.\n\n\u00b7 Tracy gives a shout-out to Pinterest, saying that it is the first company in which she was treated as an engineer, not as a female engineer. However, she understands that there\u2019s still a big barrier that prevents women from getting recognition by their male counterparts in terms of technical abilities. It\u2019s probably even harder if you\u2019re not White or Asian, the 2 most dominant racial groups in tech. The advice Tracy gives for the women in the room, inspired by Sheryl Sandberg\u2019s \u201cLean In\u201d, is to connect with other women to start movement and leverage the progressive belief that respects women in the tech sector. That\u2019s the main drive for her work with Project Include, a community for building meaningful, enduring diversity and inclusion in tech started by a group of women tech activists.\n\n\u00b7 Ruchi gives a bit of context on her background, saying that she had always been surrounded in a male-dominant environment, from college to workforce to executive board meetings. Because of that, she takes in a lot of influence from the male personality. At times, she says that she has been portrayed as too aggressive, compared to other women of course. She suggests bringing more and more women into the leadership roles in tech companies so the perception of women in tech can fundamentally change.\n\nThis question sort of came out of nowhere, and wasn\u2019t relevant to the audience really. Joe quickly writes it off \u2014 The investment in the tech sector has been extremely significant in the past decade, but is still small compared to the billions and trillions spent in other industries and abroad. So it\u2019s not time right now, especially for young grads, to worry about the negative scenario yet.\n\nOne of the core questions that should be addressed in any type of business panels, because culture means different practices for each person.\n\n\u00b7 Dustin believes that culture should be one of the earliest things that founders should address, if not the first thing. He recounted that when founding Asana, he and Justin went straight into coming up with company values, not even writing code or crafting a business plan.\n\n\u00b7 Tracy says that culture relies on the founders\u2019 behaviors. It is the reflection of the founders\u2019 personality, how they communicate with the teams, and how they respond to change as the company grows. Even as employee no. 5 in Quora, she didn\u2019t have much of an impact on the culture of the company as of its founders.\n\n\u00b7 Ruchi emphasizes how the company\u2019s value really affect the product development process. She compares Facebook\u2019s \u201cMove Fast and Break Things\u201d with Dropbox\u2019s \u201cSweat the Details.\u201d At Facebook, the principle is to experiment a lot of things and iterate as quickly as possible; therefore, there might be a lot of features being introduced in a small amount of time. Whereas at Dropbox, the principle is to meticulously go over feature details and optimize the product as much as possible, so the launch can be great.\n\n\u00b7 For Joe \u2014 working with the government is challenging as people told him things have to go certain way. But he thinks that\u2019s quite problematic, and so he always neglects conventional practice to break down barriers while working with them.\n\n\u00b7 For Dustin \u2014 many people told him to not drop out of college back in the early days of Facebook. In brief, he\u2019s glad he did.\n\n\u00b7 For Justin \u2014 in school he skipped a lot of fundamental CS classes to take higher-level classes, and so a well-known Stanford professor told him that things always have to go in order in school and in real world. Now he always thinks the opposite direction \u2014 to skip as many hurdles as he can to get to the goal as quick as possible.\n\nThis question resonates the most with me, because as an aspiring product manager, I constantly debate whether to improve my coding skill or to practice other relevant PM skills.\n\n\u00b7 Ruchi has a very strong opinion \u2014 definitely going the software engineering route first, because she believes having solid engineering experience makes better product manager, recounting her experience at Facebook.\n\n\u00b7 Justin thinks either way works fine, and that it really depends on the company\u2019s needs. Asana has both technical and non-technical PMs, and they all play different role in contributing values to the product.\n\nSo that\u2019s a brief recap of the event. If you also happened to attend it, please share your experience and thoughts. Otherwise, give your comments on the panelist\u2019 answers and what do you think that means for the future of work."
    },
    {
        "url": "https://gab41.lab41.org/the-10-algorithms-machine-learning-engineers-need-to-know-f4bb63f5b2fa?source=user_profile---------47----------------",
        "title": "The 10 Algorithms Machine Learning Engineers Need to Know",
        "text": "It is no doubt that the sub-field of machine learning / artificial intelligence has increasingly gained more popularity in the past couple of years. As Big Data is the hottest trend in the tech industry at the moment, machine learning is incredibly powerful to make predictions or calculated suggestions based on large amounts of data. Some of the most common examples of machine learning are Netflix\u2019s algorithms to make movie suggestions based on movies you have watched in the past or Amazon\u2019s algorithms that recommend books based on books you have bought before.\n\nSo if you want to learn more about machine learning, how do you start? For me, my first introduction is when I took an Artificial Intelligence class when I was studying abroad in Copenhagen. My lecturer is a full-time Applied Math and CS professor at the Technical University of Denmark, in which his research areas are logic and artificial, focusing primarily on the use of logic to model human-like planning, reasoning and problem solving. The class was a mix of discussion of theory/core concepts and hands-on problem solving. The textbook that we used is one of the AI classics: Peter Norvig\u2019s Artificial Intelligence \u2014 A Modern Approach, in which we covered major topics including intelligent agents, problem-solving by searching, adversarial search, probability theory, multi-agent systems, social AI, philosophy/ethics/future of AI. At the end of the class, in a team of 3, we implemented simple search-based agents solving transportation tasks in a virtual environment as a programming project.\n\nI have learned a tremendous amount of knowledge thanks to that class, and decided to keep learning about this specialized topic. In the last few weeks, I have been multiple tech talks in San Francisco on deep learning, neural networks, data architecture \u2014 and a Machine Learning conference with a lot of well-known professionals in the field. Most importantly, I enrolled in Udacity\u2019s Intro to Machine Learning online course in the beginning of June and has just finished it a few days ago. In this post, I want to share some of the most common machine learning algorithms that I learned from the course.\n\nMachine learning algorithms can be divided into 3 broad categories \u2014 supervised learning, unsupervised learning, and reinforcement learning. Supervised learning is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances. Unsupervised learning is useful in cases where the challenge is to discover implicit relationships in a given unlabeled dataset (items are not pre-assigned). Reinforcement learning falls between these 2 extremes \u2014 there is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn\u2019t learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.\n\n1. Decision Trees: A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance-event outcomes, resource costs, and utility. Take a look at the image to get a sense of how it looks like.\n\nFrom a business decision point of view, a decision tree is the minimum number of yes/no questions that one has to ask, to assess the probability of making a correct decision, most of the time. As a method, it allows you to approach the problem in a structured and systematic way to arrive at a logical conclusion.\n\n2. Na\u00efve Bayes Classification: Na\u00efve Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\u2019 theorem with strong (na\u00efve) independence assumptions between the features. The featured image is the equation \u2014 with P(A|B) is posterior probability, P(B|A) is likelihood, P(A) is class prior probability, and P(B) is predictor prior probability.\n\nSome of real world examples are:\n\n\u00b7 To mark an email as spam or not spam\n\n3. Ordinary Least Squares Regression: If you know statistics, you probably have heard of linear regression before. Least squares is a method for performing linear regression. You can think of linear regression as the task of fitting a straight line through a set of points. There are multiple possible strategies to do this, and \u201cordinary least squares\u201d strategy go like this \u2014 You can draw a line, and then for each of the data points, measure the vertical distance between the point and the line, and add these up; the fitted line would be the one where this sum of distances is as small as possible.\n\nLinear refers the kind of model you are using to fit the data, while least squares refers to the kind of error metric you are minimizing over.\n\n4. Logistic Regression: Logistic regression is a powerful statistical way of modeling a binomial outcome with one or more explanatory variables. It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative logistic distribution.\n\nIn general, regressions can be used in real-world applications such as:\n\n\u00b7 Predicting the revenues of a certain product\n\n\u00b7 Is there going to be an earthquake on a particular day?\n\n5. Support Vector Machines: SVM is binary classification algorithm. Given a set of points of 2 types in N dimensional place, SVM generates a (N \u2014 1) dimensional hyperlane to separate those points into 2 groups. Say you have some points of 2 types in a paper which are linearly separable. SVM will find a straight line which separates those points into 2 types and situated as far as possible from all those points.\n\nIn terms of scale, some of the biggest problems that have been solved using SVMs (with suitably modified implementations) are display advertising, human splice site recognition, image-based gender detection, large-scale image classification\u2026\n\n6. Ensemble Methods: Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a weighted vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, bagging, and boosting.\n\nSo how do ensemble methods work and why are they superior to individual models?\n\n\u00b7 They average out biases: If you average a bunch of democratic-leaning polls and republican-leaning polls together, you will get an average something that isn\u2019t leaning either way.\n\n\u00b7 They reduce the variance: The aggregate opinion of a bunch of models is less noisy than the single opinion of one of the models. In finance, this is called diversification \u2014 a mixed portfolio of many stocks will be much less variable than just one of the stocks alone. This is why your models will be better with more data points rather than fewer.\n\n\u00b7 They are unlikely to over-fit: If you have individual models that didn\u2019t over-fit, and you are combining the predictions from each model in a simple way (average, weighted average, logistic regression), then there\u2019s no room for over-fitting.\n\n7. Clustering Algorithms: Clustering is the task of grouping a set of objects such that objects in the same group (cluster) are more similar to each other than to those in other groups.\n\nEvery clustering algorithm is different, and here are a couple of them:\n\n8. Principal Component Analysis: PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n\nSome of the applications of PCA include compression, simplifying data for easier learning, visualization. Notice that domain knowledge is very important while choosing whether to go forward with PCA or not. It is not suitable in cases where data is noisy (all the components of PCA have quite a high variance).\n\n9. Singular Value Decomposition: In linear algebra, SVD is a factorization of a real complex matrix. For a given m * n matrix M, there exists a decomposition such that M = U\u03a3V, where U and V are unitary matrices and \u03a3 is a diagonal matrix.\n\nPCA is actually a simple application of SVD. In computer vision, the 1st face recognition algorithms used PCA and SVD in order to represent faces as a linear combination of \u201ceigenfaces\u201d, do dimensionality reduction, and then match faces to identities via simple methods; although modern methods are much more sophisticated, many still depend on similar techniques.\n\n10. Independent Component Analysis: ICA is a statistical technique for revealing hidden factors that underlie sets of random variables, measurements, or signals. ICA defines a generative model for the observed multivariate data, which is typically given as a large database of samples. In the model, the data variables are assumed to be linear mixtures of some unknown latent variables, and the mixing system is also unknown. The latent variables are assumed non-gaussian and mutually independent, and they are called independent components of the observed data.\n\nICA is related to PCA, but it is a much more powerful technique that is capable of finding the underlying factors of sources when these classic methods fail completely. Its applications include digital images, document databases, economic indicators and psychometric measurements.\n\nNow go forth and wield your understanding of algorithms to create machine learning applications that make better experiences for people everywhere.\n\nP.S: I recently took Andrew Ng\u2019s famous Machine Learning Coursera MOOC to refresh my knowledge of these algorithms. You can get the lecture slides and Matlab source code for the course exercises all from my GitHub here. Thanks for the overwhelming response to this post!\n\n\u2014 \u2014 \n\nIf you enjoyed this piece, I\u2019d love it if you hit the clap button \ud83d\udc4f so others might stumble upon it. You can find my own code on GitHub, and more of my writing and projects at https://jameskle.com/."
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/the-10-algorithms-machine-learning-engineers-need-to-know-e658bf8e9e17?source=user_profile---------48----------------",
        "title": "The 10 Algorithms Machine Learning Engineers Need to Know",
        "text": "The 10 Algorithms Machine Learning Engineers Need to Know\n\nIt is no doubt that the sub-field of machine learning / artificial intelligence has increasingly gained more popularity in the past couple of years. As Big Data is the hottest trend in the tech industry at the moment, machine learning is incredibly powerful to make predictions or calculated suggestions based on large amounts of data. Some of the most common examples of machine learning are Netflix\u2019s algorithms to make movie suggestions based on movies you have watched in the past or Amazon\u2019s algorithms that recommend books based on books you have bought before.\n\nSo if you want to learn more about machine learning, how do you start? For me, my first introduction is when I took an Artificial Intelligence class when I was studying abroad in Copenhagen. My lecturer is a full-time Applied Math and CS professor at the Technical University of Denmark, in which his research areas are logic and artificial, focusing primarily on the use of logic to model human-like planning, reasoning and problem solving. The class was a mix of discussion of theory/core concepts and hands-on problem solving. The textbook that we used is one of the AI classics: Peter Norvig\u2019s Artificial Intelligence \u2014 A Modern Approach, in which we covered major topics including intelligent agents, problem-solving by searching, adversarial search, probability theory, multi-agent systems, social AI, philosophy/ethics/future of AI. At the end of the class, in a team of 3, we implemented simple search-based agents solving transportation tasks in a virtual environment as a programming project.\n\nI have learned a tremendous amount of knowledge thanks to that class, and decided to keep learning about this specialized topic. In the last few weeks, I have been multiple tech talks in San Francisco on deep learning, neural networks, data architecture \u2014 and a Machine Learning conference with a lot of well-known professionals in the field. Most importantly, I enrolled in Udacity\u2019s Intro to Machine Learning online course in the beginning of June and has just finished it a few days ago. In this post, I want to share some of the most common machine learning algorithms that I learned from the course.\n\nMachine learning algorithms can be divided into 3 broad categories \u2014 supervised learning, unsupervised learning, and reinforcement learning. Supervised learning is useful in cases where a property (label) is available for a certain dataset (training set), but is missing and needs to be predicted for other instances. Unsupervised learning is useful in cases where the challenge is to discover implicit relationships in a given unlabeled dataset (items are not pre-assigned). Reinforcement learning falls between these 2 extremes \u2014 there is some form of feedback available for each predictive step or action, but no precise label or error message. Since this is an intro class, I didn\u2019t learn about reinforcement learning, but I hope that 10 algorithms on supervised and unsupervised learning will be enough to keep you interested.\n\n1. Decision Trees: A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance-event outcomes, resource costs, and utility. Take a look at the image to get a sense of how it looks like.\n\nFrom a business decision point of view, a decision tree is the minimum number of yes/no questions that one has to ask, to assess the probability of making a correct decision, most of the time. As a method, it allows you to approach the problem in a structured and systematic way to arrive at a logical conclusion.\n\n2. Na\u00efve Bayes Classification: Na\u00efve Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes\u2019 theorem with strong (na\u00efve) independence assumptions between the features. The featured image is the equation \u2014 with P(A|B) is posterior probability, P(B|A) is likelihood, P(A) is class prior probability, and P(B) is predictor prior probability.\n\nSome of real world examples are:\n\n\u00b7 To mark an email as spam or not spam\n\n3. Ordinary Least Squares Regression: If you know statistics, you probably have heard of linear regression before. Least squares is a method for performing linear regression. You can think of linear regression as the task of fitting a straight line through a set of points. There are multiple possible strategies to do this, and \u201cordinary least squares\u201d strategy go like this \u2014 You can draw a line, and then for each of the data points, measure the vertical distance between the point and the line, and add these up; the fitted line would be the one where this sum of distances is as small as possible.\n\nLinear refers the kind of model you are using to fit the data, while least squares refers to the kind of error metric you are minimizing over.\n\n4. Logistic Regression: Logistic regression is a powerful statistical way of modeling a binomial outcome with one or more explanatory variables. It measures the relationship between the categorical dependent variable and one or more independent variables by estimating probabilities using a logistic function, which is the cumulative logistic distribution.\n\nIn general, regressions can be used in real-world applications such as:\n\n\u00b7 Predicting the revenues of a certain product\n\n\u00b7 Is there going to be an earthquake on a particular day?\n\n5. Support Vector Machines: SVM is binary classification algorithm. Given a set of points of 2 types in N dimensional place, SVM generates a (N \u2014 1) dimensional hyperlane to separate those points into 2 groups. Say you have some points of 2 types in a paper which are linearly separable. SVM will find a straight line which separates those points into 2 types and situated as far as possible from all those points.\n\nIn terms of scale, some of the biggest problems that have been solved using SVMs (with suitably modified implementations) are display advertising, human splice site recognition, image-based gender detection, large-scale image classification\u2026\n\n6. Ensemble Methods: Ensemble methods are learning algorithms that construct a set of classifiers and then classify new data points by taking a weighted vote of their predictions. The original ensemble method is Bayesian averaging, but more recent algorithms include error-correcting output coding, bagging, and boosting.\n\nSo how do ensemble methods work and why are they superior to individual models?\n\n\u00b7 They average out biases: If you average a bunch of democratic-leaning polls and republican-leaning polls together, you will get an average something that isn\u2019t leaning either way.\n\n\u00b7 They reduce the variance: The aggregate opinion of a bunch of models is less noisy than the single opinion of one of the models. In finance, this is called diversification \u2014 a mixed portfolio of many stocks will be much less variable than just one of the stocks alone. This is why your models will be better with more data points rather than fewer.\n\n\u00b7 They are unlikely to over-fit: If you have individual models that didn\u2019t over-fit, and you are combining the predictions from each model in a simple way (average, weighted average, logistic regression), then there\u2019s no room for over-fitting.\n\n7. Clustering Algorithms: Clustering is the task of grouping a set of objects such that objects in the same group (cluster) are more similar to each other than to those in other groups.\n\nEvery clustering algorithm is different, and here are a couple of them:\n\n8. Principal Component Analysis: PCA is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components.\n\nSome of the applications of PCA include compression, simplifying data for easier learning, visualization. Notice that domain knowledge is very important while choosing whether to go forward with PCA or not. It is not suitable in cases where data is noisy (all the components of PCA have quite a high variance).\n\n9. Singular Value Decomposition: In linear algebra, SVD is a factorization of a real complex matrix. For a given m * n matrix M, there exists a decomposition such that M = U\u03a3V, where U and V are unitary matrices and \u03a3 is a diagonal matrix.\n\nPCA is actually a simple application of SVD. In computer vision, the 1st face recognition algorithms used PCA and SVD in order to represent faces as a linear combination of \u201ceigenfaces\u201d, do dimensionality reduction, and then match faces to identities via simple methods; although modern methods are much more sophisticated, many still depend on similar techniques.\n\n10. Independent Component Analysis: ICA is a statistical technique for revealing hidden factors that underlie sets of random variables, measurements, or signals. ICA defines a generative model for the observed multivariate data, which is typically given as a large database of samples. In the model, the data variables are assumed to be linear mixtures of some unknown latent variables, and the mixing system is also unknown. The latent variables are assumed non-gaussian and mutually independent, and they are called independent components of the observed data.\n\nICA is related to PCA, but it is a much more powerful technique that is capable of finding the underlying factors of sources when these classic methods fail completely. Its applications include digital images, document databases, economic indicators and psychometric measurements.\n\nNow go forth and wield your understanding of algorithms to create machine learning applications that make better experiences for people everywhere."
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/game-development-diary-sprint-5-phase-iii-b2e2160f08f2?source=user_profile---------49----------------",
        "title": "Game Development Diary: Sprint 5, Session III \u2013 The Aspiring Programmer Journal \u2013",
        "text": "Wednesday, May 4\u20131 day before the final hand-in. Fortunately, we have a field study in class today, and our teacher allowed us to dedicate most of the time working on the final project. And so, we spent about 8 hours to further develop our game. That being said, we made a ton of changes on code refactoring. By the end of the day, we were basically with all the scripts. Here are big folders of code base that we use for our game: Enemy: The Enemy code folder consists of 4 scripts \u2014 BaseEnemy, EnemyBody, EnemyChase, and EnemyHome. After adding the Enemy prefab (aka Evil Calzone), we made the EnemyChase script (enemy chasing you), EnemyBody script (when you collide with the enemy, your health decreases), BaseEnemy script (when enemy knows that you enter its territory), and EnemyHome script (a home for enemy to return to when it is not threatened).\n\nParticles: The Particles code folder consists of 9 scripts \u2014 Attractor, Repulsor, Fastor, Slowor (the properties of the 4 particles), BaseAffector, FollowPlayer, Gravitor, Velocitor, and ParticleSpawner. The BaseAffector and the ParticleSpawner scripts hold important functions of a particle such as being destroyable by mouse click and expandable by holding mouse button). The FollowPlayer script simply makes the particles to follow the player. The Gravitor script adds gravity force to the particle and the Velocitor script adds velocity force to the particle.\n\nPizza: The Pizza code folder consists of 8 scripts \u2014 PizzaInventory, PizzaMain, PizzaMove, PizzaVelocity, PizzaSceneManager, PointToGoal, ScaleText, and ZoomOutCamera. Most of these scripts are intuitive based on the names, and they have been covered in previous posts. Static Collideable: The Static Collideable code folder consists of 4 scripts \u2014 AsteroidCollide, AsteroidReflect, Goal, and Pickup. The player loses health and gets deflected when hitting the asteroids. The other scripts are for picking up ingredients and reaching the end goal and loading to the next level.\n\nBackground: The Background code folder consists of 4 scripts \u2014 BackgroundTile, RendererExtension, ScrollingScript and ScreenRoll. Audio Text Manager: Finally, the Audio Text Manager code folder consists of 4 scripts \u2014 AudioTrack, GoToScene, TextController, and TutorialAudioQueues. Besides from that, we all agree on the fact that we will have 16 different scenes in the game: 12 \u2014 Level 4 (Pick up one topping of everything) Wednesday, May 5 \u2014 We added the music for all the levels to make the game more interesting, the help text which is to be displayed to help the player during the game, and 2 non-interactive opening Splash scene and ending Credit scene.\n\nThis is the end of our game development journey. It\u2019s been a great pleasure working on this game \u2014 from concept to design to production. Look forward to present it to class next Monday and in the showcase next Wednesday!"
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/game-development-diary-sprint-5-session-ii-c8964a44574b?source=user_profile---------50----------------",
        "title": "Game Development Diary: Sprint 5 \u2014 Session II \u2013 The Aspiring Programmer Journal \u2013",
        "text": "Today is Tuesday, May 3 \u2014 and the most exciting change that we incorporated in this session is the audio. Kevin and Greg both did some witty and humorous voiceover for Pizza Paul and the aliens, and Kevin also plays the narrator voice. We imported the audio tracks into our levels and as you can see in the cover photo \u2014 we made the AudioTrack script to play audio at scenes and the TutorialAudioQueues script to put the tutorial audio track on queue during level I.\n\nBesides from that, here are the big programming changes that we committed:"
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/game-development-diary-sprint-5-session-i-2859967351d6?source=user_profile---------51----------------",
        "title": "Game Development Diary: Sprint 5 \u2014 Session I \u2013 The Aspiring Programmer Journal \u2013",
        "text": "In the last week before the final project is due, we have made a lot of significant progress regarding our approach to the game mechanics, level design and gameplay. This diary post explains in details the things we have committed on Monday, May 2. The background tile is assigned a specific position relative to the player\u2019s position.\n\n2. The scrolling script is fixed so that the center of the screen is always automatically reallocated after 4 seconds during the player\u2019s movement.\n\n3. To make the on-screen texts more visible to the player, we wrote a ScaleText script that scales based on the screen size.\n\n4. We added all the inventory prefabs (mozzarella, sauce, pepperoni, mushroom) into our game, and made a PizzaInventory script to keep track of everything the player picks up.\n\n5. Finally, noticing the difficulty of navigating Pizza Paul, we fixed the PizzaMove script with specific velocity and magnitude.\n\nAnother big organizational change is that we shuffled scripts into new folders to easily manage them. You can see that in the cover photo of this post."
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/game-development-diary-sprint-4-653bb55b274e?source=user_profile---------52----------------",
        "title": "Game Development Diary: Sprint 4 \u2013 The Aspiring Programmer Journal \u2013",
        "text": "In the next 2 weeks, we don\u2019t have in-class lectures; instead we will have time to work on our final game project. This week is kind of short for us, but we have made some important changes regarding level design, voice over tutorials, and game mechanics. We have decided on a very important game mechanics: Instead of using the planet for the gravity particles, we are going to make little beam-like particles and different colors have different properties:\n\nHere are the particles graphics that Ben has created to be used as prefabs:\n\nWith this new additional game mechanics, we created several new scripts to meet the functions of each specific particle:"
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/game-development-diary-sprint-3-110f7569bee4?source=user_profile---------53----------------",
        "title": "Game Development Diary: Sprint 3 \u2013 The Aspiring Programmer Journal \u2013",
        "text": "After the 2nd Independent Travel Week, we came back to Copenhagen continuing with the work for our final game project. This is technically the 4th week that we have been working on the project \u2014 and we also had our 1st playable presentation on Thursday, April 21st. Here are the big changes that we have made since our 2nd sprint:\n\n\u00b7 Making the camera to follow the player"
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/game-development-diary-sprint-2-1f980993b216?source=user_profile---------54----------------",
        "title": "Game Development Diary: Sprint 2 \u2013 The Aspiring Programmer Journal \u2013",
        "text": "This post captures our progress of developing our game, Pizza Space Delivery, on the 2nd week. Because of the independent travel week, we haven\u2019t been able to make a lot of improvements since sprint 1. However, the game is progressing nicely with graphics and physics concepts. This past weekend, 2 of our members \u2014 Greg and Wesley \u2014 were at the Nordic Game Jam in Copenhagen. They had a chance to work with seasoned game developers who are specialized in graphics, programming, and sound.\n\n\u00b7 In Wesley\u2019s game, there is a big component of Virtual Reality, which might show up in game mechanics later.\n\n\u00b7 In Greg\u2019s game, he programmed the physics where an object can change direction for a particular object. He did so with the help of a professional programmer and that small piece of information is just what we need to get the right physics for our game.\n\nHere are the changes that Wesley and Greg have committed since last sprint:\n\n\u00b7 Making the planet spawner being independent of camera."
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/game-development-diary-sprint-1-93c5584d1643?source=user_profile---------55----------------",
        "title": "Game Development Diary: Sprint 1 \u2013 The Aspiring Programmer Journal \u2013",
        "text": "As many of you know, I am studying abroad in Copenhagen this semester through the DIS Computer Science program. My core class is Game Development: Programming and Practice, and our final project is to build a game from scratch using Unity \u2014 the game engine that we have been working with over the course of the semester. For the final project, we have the chance to be in a team from 4 to 5 people. My team consists of myself, Gregory Greene (Grinnell College), Benjamin Allen (Skidmore College), Wesley Lin and Kevin Ellenburg (both from Vanderbilt University). After much conversations, we decided to go with the game idea originated by Kevin, \u201cPizza Space Delivery.\u201d Here is the premise of the game plot: Paul, a young pizza enthusiast, leaves his home planet Earth to embark on a brand new entrepreneurial blue ocean strategy adventure \u2014 selling pizza to aliens. Everyone thought he was crazy, but he saw the potential of this untapped market, and he will earn all of the alien moneys. Throughout his adventure, he encounters many issues that he had not foreseen. His exciting journey takes him through the wonders of the universe as he faces challenges such as gravitational forces, angry aliens, and of course doing it all in 30 minutes or less.\n\nWe had a quick brainstorming session earlier in the process via Skype. Here are a few mechanics ideas that we came up with: \u00b7 Planet grows until you let go \u00b7 Planet explodes when you click on it \u00b7 Planet can only reach a certain mass/volume \u00b7 Planets do not affect each other \u00b7 Camera follows player, pauses when you click We have made our first shototype on Unity, thanks to Greg. It is a working model to show in lieu of a fully-functional prototype, showing a pizza that is affected by planets that you can spawn by clicking and holding in the screen. There are major physics bugs, but at least it is a 1st step into the long journey of production. Greg did it using 3 scripts: One for the velocity of the pizza movement. One for the spawning of planets. One for the gravity of the planets.\n\n\u00b7 Finish the explosion script \u2014 in which the pizza will explode when colliding with the planets on its way throughout the screen. Wesley is fully interested in coding this particular script. \u00b7 Improve the planet gravity script \u2014 Greg wants to add a few additional space-like features for the planets he created. This can be a gravity attractor script across time and space, with parameters like mass and gravity acceleration. \u00b7 Polish the storyline and graphics \u2014 Kevin is in charge of the level design since this is his idea. Ben is helping out with the graphics for this part. I am mainly in charge of writing specs, researching features, communicating between team members, as well as setting the goals for every sprint. I will end this 1st diary post with this video. Hope you find it to be humored as our personalities are:"
    },
    {
        "url": "https://medium.com/inside-product-management/5-ways-designing-an-algorithm-and-developing-a-business-are-identical-1affa634e130?source=user_profile---------56----------------",
        "title": "5 Ways Designing An Algorithm And Developing A Business Are Identical",
        "text": "5 Ways Designing An Algorithm And Developing A Business Are Identical This summer has turned out to be far more productive than I expected. I have had the chances to meet and talk with many professionals in my fields of study and learn about what it takes to get my foot into the door. Studying Computer Science and Communication in college, I discovered that there is a position in the tech world that marries both of my qualifications: Product Manager. A product manager is responsible for making sure that a team ships a great product and gets to sit at the intersection of technology, business, and design. It is also a highly collaborative role as the product manager serves as the main liaison between the engineers and other roles (quality assurance, user research, customer support, biz dev etc.). For me, the combination of technical abilities (product-focused) and management/communication skills (business/customer-focused) sounds very appealing. Now you may think these two areas of development are distinct from each other. However, the more I delve into them, the more I see the correlation and similarities. I came up with this derived conclusion: \u201cThe corollary between developing a business and designing an algorithm is strikingly close.\u201d\n\nHow so? I am currently reading Gayle Laakmann McDowell\u2019s \u201cCracking the Coding Interview\u201d to practice solving programming questions. In the book, Gayle mentions the 5 approaches to solve a tricky algorithm problem. They include: Approach 1 \u2014 Examplify: Write out specific examples of the problem and see if a general rule can be derived from there. Approach 2 \u2014 Pattern Matching: Consider what problems the algorithm is similar to and try to modify the solution to the related problem to develop an algorithm for this problem. \u00b7 Change a constraint such as the data type or amount of data to simplify the problem. \u00b7 Solve this new simplified version of the problem. \u00b7 Once we have the algorithm for the simplified problem, we generalize the problem and try to adapt the earlier solution for the more complex version. Approach 4 \u2014 Base Case and Build: We solve the problem first for a base case (n = 1) and record the correct result. Then, we try to solve the problem for n = 2, assuming that you have the answer for n = 1. Next, we try to solve it for n = 3, assuming that you have answers for n = 1 and n = 2. Eventually, we can build a solution that can always compute the result for N if we know the correct result for N \u2014 1. This approach usually leads to natural recursive algorithms. Approach 5 \u2014 Data Structure Brainstorm: Simply run through a list of data structures and try to apply each one (linked list, array, binary tree, heap etc.) This approach is useful because solving a problem may be trivial once it occurs to use something obvious.\n\nNow what? How does developing a business have any common things with this complex, algorithmic, technical descriptions I just outlined above? Let\u2019s go through them: 1 \u2014 Examplify: This approach is very similar to doing Customer Discovery. While running a business, you have a customer segment. Customer discovery is the process of talking to your customers, identifying their problems, categorizing their experiences, and looking for insights. While doing the Zillow challenge for Koru last June, I had to go through the exact process of cold-calling real estate agents (Zillow\u2019s direct customers), seeking their pain points, and building personas for different types of agents. 2 \u2014 Pattern Matching: Finding patterns? That\u2019s all about Competitive Intelligence. Strategic planning for your business requires finding competitors in the same / similar industries and evaluate their growth, market fit, products, funding etc. Last week, I did a small market research assignment for Poachable, a Seattle-based startup for passive job seekers. The assignment requires looking at Poachable\u2019s competitors and compare them based on capital raised, media mentions, and PR metrics. Definitely a good learning experience. 3 \u2014 Simplify and Generalize: So this approach asks you to change a small constraint to simplify the problem. Similarly, you can solve a business problem simply by twisting a feature, adjusting a few metrics, or modifying some content. Why? Because intersection happens at the intersection of many disciplines. The book \u201cBusiness Model Generation\u201d shows an example of how Nintendo\u2019s Wii twist its business model to compete with Sony\u2019s PSP / Microsoft\u2019s Xbox and disrupt the hardware game industry. While Sony and Microsoft only make high performance consoles targeting at the hardcore gamers, which result to a loss at hardware sales; Nintendo widens their customer base by targeting the more casual gamers and building family-friendly consoles, thus making profits from hardware sales. 4 \u2014 Base Case and Build: This is a form of recursion, a fundamental concept in computer science. In business, that very much resembles Rapid Prototyping \u2014 the process of quickly mocking up the future state of a system and validating it with a broader team of users, stakeholders, developers and designers. Tableau and Slack are the principal examples of that. In a world of big data and mass communication, their products are valuable to any business and team setting. Therefore, one of their main focus is to constantly expand the audience target. Doing this rapidly and iteratively generates feedback early and often in the process, improving the final design and reducing the need for changes during development. 5 \u2014 Data Structure Brainstorm: This one is rather obvious. In order to build a successful product, you must have the acumen to look at all possible options for growth and evaluate them based on desirability, viability, and feasibility. In his book \u201cThe Hard Thing About Hard Things\u201d, Ben Horowitz tells his story about running Opsware. In looking at different exit strategies for Opsware, he considered these three options: 1> Going public (IPO), 2> Calling it quit (Liquidation), 3> Getting acquired (Acquisition). Evaluating all the decisions, Ben was able to successfully unload Opsware on Hewlett-Packard for $1.65Billion in cash. That\u2019s it! Long post, but I hope that these 5 comparison analyses are enough to convince you that running a business is just like solving an algorithmic problem. Now, I\u2019d love to hear from you. What other analogies you can think of when it comes to developing a business? Share your comments below."
    },
    {
        "url": "https://medium.com/the-aspiring-programmer-journal/5-ways-designing-an-algorithm-and-developing-a-business-are-identical-1f952909a166?source=user_profile---------57----------------",
        "title": "5 Ways Designing An Algorithm And Developing A Business Are Identical",
        "text": "This summer has turned out to be far more productive than I expected. I have had the chances to meet and talk with many professionals in my fields of study and learn about what it takes to get my foot into the door. Studying Computer Science and Communication in college, I discovered that there is a position in the tech world that marries both of my qualifications: Product Manager. A product manager is responsible for making sure that a team ships a great product and gets to sit at the intersection of technology, business, and design. It is also a highly collaborative role as the product manager serves as the main liaison between the engineers and other roles (quality assurance, user research, customer support, biz dev etc.). For me, the combination of technical abilities (product-focused) and management/communication skills (business/customer-focused) sounds very appealing. Now you may think these two areas of development are distinct from each other. However, the more I delve into them, the more I see the correlation and similarities. I came up with this derived conclusion: \u201cThe corollary between developing a business and designing an algorithm is strikingly close.\u201d\n\nHow so? I am currently reading Gayle Laakmann McDowell\u2019s \u201cCracking the Coding Interview\u201d to practice solving programming questions. In the book, Gayle mentions the 5 approaches to solve a tricky algorithm problem. They include: Approach 1 \u2014 Examplify: Write out specific examples of the problem and see if a general rule can be derived from there. Approach 2 \u2014 Pattern Matching: Consider what problems the algorithm is similar to and try to modify the solution to the related problem to develop an algorithm for this problem. \u00b7 Change a constraint such as the data type or amount of data to simplify the problem. \u00b7 Solve this new simplified version of the problem. \u00b7 Once we have the algorithm for the simplified problem, we generalize the problem and try to adapt the earlier solution for the more complex version. Approach 4 \u2014 Base Case and Build: We solve the problem first for a base case (n = 1) and record the correct result. Then, we try to solve the problem for n = 2, assuming that you have the answer for n = 1. Next, we try to solve it for n = 3, assuming that you have answers for n = 1 and n = 2. Eventually, we can build a solution that can always compute the result for N if we know the correct result for N \u2014 1. This approach usually leads to natural recursive algorithms. Approach 5 \u2014 Data Structure Brainstorm: Simply run through a list of data structures and try to apply each one (linked list, array, binary tree, heap etc.) This approach is useful because solving a problem may be trivial once it occurs to use something obvious.\n\nNow what? How does developing a business have any common things with this complex, algorithmic, technical descriptions I just outlined above? Let\u2019s go through them: 1 \u2014 Examplify: This approach is very similar to doing Customer Discovery. While running a business, you have a customer segment. Customer discovery is the process of talking to your customers, identifying their problems, categorizing their experiences, and looking for insights. While doing the Zillow challenge for Koru last June, I had to go through the exact process of cold-calling real estate agents (Zillow\u2019s direct customers), seeking their pain points, and building personas for different types of agents. 2 \u2014 Pattern Matching: Finding patterns? That\u2019s all about Competitive Intelligence. Strategic planning for your business requires finding competitors in the same / similar industries and evaluate their growth, market fit, products, funding etc. Last week, I did a small market research assignment for Poachable, a Seattle-based startup for passive job seekers. The assignment requires looking at Poachable\u2019s competitors and compare them based on capital raised, media mentions, and PR metrics. Definitely a good learning experience. 3 \u2014 Simplify and Generalize: So this approach asks you to change a small constraint to simplify the problem. Similarly, you can solve a business problem simply by twisting a feature, adjusting a few metrics, or modifying some content. Why? Because intersection happens at the intersection of many disciplines. The book \u201cBusiness Model Generation\u201d shows an example of how Nintendo\u2019s Wii twist its business model to compete with Sony\u2019s PSP / Microsoft\u2019s Xbox and disrupt the hardware game industry. While Sony and Microsoft only make high performance consoles targeting at the hardcore gamers, which result to a loss at hardware sales; Nintendo widens their customer base by targeting the more casual gamers and building family-friendly consoles, thus making profits from hardware sales. 4 \u2014 Base Case and Build: This is a form of recursion, a fundamental concept in computer science. In business, that very much resembles Rapid Prototyping \u2014 the process of quickly mocking up the future state of a system and validating it with a broader team of users, stakeholders, developers and designers. Tableau and Slack are the principal examples of that. In a world of big data and mass communication, their products are valuable to any business and team setting. Therefore, one of their main focus is to constantly expand the audience target. Doing this rapidly and iteratively generates feedback early and often in the process, improving the final design and reducing the need for changes during development. 5 \u2014 Data Structure Brainstorm: This one is rather obvious. In order to build a successful product, you must have the acumen to look at all possible options for growth and evaluate them based on desirability, viability, and feasibility. In his book \u201cThe Hard Thing About Hard Things\u201d, Ben Horowitz tells his story about running Opsware. In looking at different exit strategies for Opsware, he considered these three options: 1> Going public (IPO), 2> Calling it quit (Liquidation), 3> Getting acquired (Acquisition). Evaluating all the decisions, Ben was able to successfully unload Opsware on Hewlett-Packard for $1.65Billion in cash. That\u2019s it! Long post, but I hope that these 5 comparison analyses are enough to convince you that running a business is just like solving an algorithmic problem. Now, I\u2019d love to hear from you. What other analogies you can think of when it comes to developing a business? Share your comments below."
    }
]