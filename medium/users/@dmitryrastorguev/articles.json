[
    {
        "url": "https://medium.com/@dmitryrastorguev/my-favourite-technological-advancement-59e35be1861f?source=user_profile---------1----------------",
        "title": "My favourite technological advancement \u2013 Dmitry Rastorguev \u2013",
        "text": "Ever since its creation, the internet has brought the world together and made it more connected. It has made different types of information easily accessible for individuals around the world. Having done various web development projects in my spare time, I have learnt to appreciate the importance of JavaScript, the only programming language that is commonly supported by different web browsers. It is present on most websites around the world and plays an important role in making the websites more interactive with better user experience overall.\n\nFortunately, the popularity of JavaScript is continuing to grow, supporting Atwood\u2019s Law which states that \u201cany application that can be written in JavaScript, will eventually be written in JavaScript\u201d. Interestingly, a number of machine learning libraries have already been written in JavaScript, for example, ml.js and deeplearning.js.\n\nThese libraries are likely to become more prominent in the near future as they allow analysis to run in the browser of the user, saving money for service providers. As a result, most individuals in data science, including myself, will likely need to continue developing our JavaScript knowledge, allowing us to build our own extensions on top of existing open source solutions."
    },
    {
        "url": "https://medium.freecodecamp.org/learning-to-test-with-python-997ace2d8abe?source=user_profile---------2----------------",
        "title": "A simple introduction to Test Driven Development with Python",
        "text": "I am a self-taught beginning developer who is able to write simple apps. But I have a confession to make. It\u2019s impossible to remember how everything is interconnected in my head.\n\nThis situation is made worse if I come back to the code I\u2019ve written after a few days. Turns out that this problem could be overcome by following a Test Driven Development (TDD) methodology.\n\nIn layman\u2019s terms, TDD recommends writing tests that would check the functionality of your code prior to your writing the actual code. Only when you are happy with your tests and the features it tests, do you begin to write the actual code in order to satisfy the conditions imposed by the test that would allow them to pass.\n\nFollowing this process ensures that you careful plan the code you write in order to pass these tests. This also prevents the possibility of writing tests being postponed to a later date, as they might not be deemed as necessary compared to additional features that could be created during that time.\n\nTests also give you confidence when you begin to refactor code, as you are more likely to catch bugs due to the instant feedback when tests are executed.\n\nTo begin writing tests in Python we will use the module that comes with Python. To do this we create a new file , which will contain all our tests.\n\nNotice that we are importing function from file. In the file we will initially just include the code below, which creates the function but doesn\u2019t return anything at this stage:\n\nRunning will generate the following output in the command line:\n\nThis clearly indicates that the test failed, which was expected. Fortunately, we have already written the tests, so we know that it will always be there to check this function, which gives us confidence in spotting potential bugs in the future.\n\nTo ensure the code passes, lets change to the following:\n\nRunning again we get the following output in the command line:\n\nCongrats! You\u2019ve have just written your first test. Let\u2019s now move on to a slightly more difficult challenge. We\u2019ll create a function that would allow us to create a custom numeric list comprehension in Python.\n\nLet\u2019s begin by writing a test for a function that would create a list of specific length.\n\nIn the file this would be a method :\n\nThis would test that the function returns a list of length 10. Let\u2019s create function in :\n\nRunning will generate the following output in the command line:\n\nThis is as expected, so let\u2019s go ahead and change function in in order to pass the test:\n\nExecuting on the command line demonstrates that the second test has also now passed:\n\nLet\u2019s now create a custom function that would transform each value in the list like this: . First let\u2019s write the test for this, using method that would take value 3 as X, take it to the power of 3, and multiply by a constant of 2, resulting in the value 54:\n\nLet\u2019s create the function in the file :\n\nAs expected, we get a fail:\n\nUpdating function to pass the test, we have the following:\n\nRunning the tests again we get a pass:\n\nFinally, let\u2019s create a new function that would incorporate function into the list comprehension. As usual, let\u2019s begin by writing the test. Note that just to be certain, we include two different cases:\n\nNow let\u2019s create the function in :\n\nAs before, we get a fail:\n\nIn order to pass the test, let\u2019s update the file to the following:\n\nRunning the tests for the final time, we pass all of them!\n\nCongrats! This concludes this introduction to testing in Python. Make sure you check out the resources below for more information on testing in general.\n\nThe code is available here on GitHub.\n\nBelow are links to some of the libraries focusing on testing in Python\n\nIf you prefer not to read, I recommend watching the following videos on YouTube."
    },
    {
        "url": "https://hackernoon.com/building-github-profile-analytics-using-react-part-2-bc1adc640e25?source=user_profile---------3----------------",
        "title": "Building GitHub Profile Analytics using React || PART 2",
        "text": "This articles continues from Part 1, where we built a basic page that is capable of taking a user input, using it to request data from GitHub\u2019s API and then display the response on the web page once it has been received.\n\nIn this part we will add 3 different sections: basic information, list of most popular and starred repos and, finally, analyse most common languages of user\u2019s own repos and generate keywords to starred repos.\n\nLets begin by running which is necessary to adjust the date for one of the variables. Next we will need to create a new component called with the following code:\n\nSimilarly, , will need to be imported changed accordingly.\n\nThese changes allow us to pull in together various data points of the profile. Such as Name, Bio, Location, number of repos, followers and some others. Overall, the section will look something like this.\n\nNow we will begin working on the list of own and starred repositories. To do this we first create a new component called :\n\nWe then update accordingly:\n\nThe page should now be able to show items for each of the repositories lists:\n\nFinally, we can run some basic analysis on received information for repositaries. For example, for user\u2019s own repositaries we will count the number of repositaries for each programming language. This would be an indication of the most preferred language for each user. Similarly, we will extract the description for starred repositaries and produce a list of keywords, indicating preferred topics. This topic modelling process will be done using Latent Dirichlet Allocation .\n\nLet\u2019s begin by running , which will install the required package. In order to allow for building process to happen, we will need to move folder in to folder. As a result, 's new path becomes . This will also allow us to import into .\n\nNext we will need to create a new component.\n\nFinally, we can update as follows:\n\nTo ensure GitHub Chart is also visible, change to the following:\n\nAs a result, this produces the following output for my GitHub profile.\n\nFurther opportunities for improvement in this project, include adding styling, code refactoring and testing.\n\nFeel free to track the progress of this project on GitHub and on its website."
    },
    {
        "url": "https://hackernoon.com/building-github-profile-analytics-using-react-part-1-37e03b0c3366?source=user_profile---------4----------------",
        "title": "Building GitHub Profile Analytics using React || PART 1",
        "text": "To begin load create-react-app (this is assuming npm and create react app are already installed)\n\nOnce this has been completed , change directories with and enter to check that everything is working fine. You should see the following page in the browser.\n\nBegin by deleting the following files: , , . Change name to (don\u2019t forget its imports in and ). Also chance its content to the following:\n\nCreate sub-folder in . Here lets create our first component as follows:\n\nAdjust to the following:\n\nThis should result in alert, when pressing on button.\n\nNext we will need to run . This installs axios, which allows to make HTTP requests. Once it has been installed, will need to be changed to:\n\nThis results in data being automatically collected for my GitHub account.\n\nFinally, we will replace button with a form to allow users to search GitHub by username. To do this, we first need to create component.\n\nSimilarly, will need to be replaced as follows:\n\nYour page should look some similar to a screenshot below.\n\nWe are now able to to request any data from GitHub API and display it on our webpage. Therefore, this concludes Part 1. In Part 2 we will add 3 different sections: basic information, list of most popular and starred repos and, finally, analyse most common languages of user\u2019s own repos and generate keywords to starred repos.\n\nFeel free to track the progress of this project on GitHub and on its website."
    },
    {
        "url": "https://itnext.io/2017s-deep-learning-papers-on-investing-7489e8f59487?source=user_profile---------5----------------",
        "title": "2017's Deep Learning Papers on Investing \u2013",
        "text": "Forecasting Foreign Exchange Rate Movements with k-Nearest-Neighbour, Ridge Regression and Feed-Forward Neural Networks by Milan Fi\u010dura\n\nAbstract: Three different classes of data mining methods (k-Nearest Neighbour, Ridge Regression and Multilayer Perceptron Feed-Forward Neural Networks) are applied for the purpose of quantitative trading on 10 simulated time series, as well as real world time series of 10 currency exchange rates ranging from 1.11.1999 to 12.6.2015. Each method is tested in multiple variants. The k-NN algorithm is applied alternatively with the Euclidian, Manhattan, Mahalanobis and Maximum distance function. The Ridge Regression is applied as Linear and Quadratic, and the Feed-Forward Neural Network is applied with either 1, 2 or 3 hidden layers. In addition to that Principal Component Analysis (PCA) is eventually applied for the dimensionality reduction of the predictor set and the meta-parameters of the methods are optimized on the validation sample. In the simulation study a Stochastic-Volatility Jump-Diffusion model, extended alternatively with 10 different non-linear conditional mean patterns, is used, to simulate the asset price behaviour to which the tested methods are applied. The results show that no single method was able to profit on all of the non-linear patterns in the simulated time series, but instead different methods worked well for different patterns. Alternatively, past price movements and past returns were used as predictors. In the case when the past price movements were used, quadratic ridge regression achieved the most robust results, followed by some of the k-NN methods. In the case when past returns were used, k-NN based methods were the most consistently profitable, followed by the linear ridge regression and quadratic ridge regression. Neural networks, while being able to profit on some of the time series, did not achieve profit on most of the others. No evidence was further found of the PCA method to improve the results of the tested methods in a systematic way. In the second part of the study, the models were applied to empirical foreign exchange rate time series. Overall the profitability of the methods was rather low, with most of them ending with a loss on most of the currencies. The most profitable currency was EURUSD, followed by EURJPY, GBPJPY and EURGBP. The most successful methods were the linear ridge regression and the Manhattan distance based k-NN method which both ended with profits for most of the time series (unlike the other methods). Finally, a forward selection procedure using the linear ridge regression was applied to extend the original predictor set with some technical indicators. The selection procedure achieved limited success in improving the out-sample results for the linear ridge regression model but not the other models.\n\nP.S. Feel free to add me on LinkedIn and follow on GitHub."
    },
    {
        "url": "https://medium.com/@dmitryrastorguev/100-uk-ai-ml-start-ups-launched-in-2017-5acefc43733d?source=user_profile---------6----------------",
        "title": "100 UK AI/ML Start Ups Launched in 2017 \u2013 Dmitry Rastorguev \u2013",
        "text": "I have put together a list of 100 start ups with Artificial Intelligence and Machine Learning at their core. All of these have been started or launched in 2017 in the United Kingdom. It demonstrates that UK remains an important center of entrepreneurship, including commercialisation of deep tech."
    },
    {
        "url": "https://medium.com/@dmitryrastorguev/import-all-medium-stories-to-your-wordpress-blog-f48cac2c5fd6?source=user_profile---------7----------------",
        "title": "Import all Stories to your WordPress Blog \u2013 Dmitry Rastorguev \u2013",
        "text": "Let me guess: you\u2019ve decided to create your own blog and would like to move across all your Medium articles, but not sure how to do it?\n\nI\u2019ve recently come across the same problem, while working on my wife\u2019s website. One of the challenges involved transferring all her Medium Stories across to her new WordPress Blog. After some back and forward experimenting - I found a solution. Fortunately, it is relatively quick and doesn\u2019t cost anything. Just follow the instructions below \ud83d\udc47.\n\nYou will first need to access your RSS feed by going to the following URL: . (Please replace with your own profile name). This should open a webpage in XML format, like the one below. Make sure you save this page, using \u201cFile->Save As\u2026\u201d. This file will contain the information on all posts in your feed.\n\nTo import the data into WordPress, you will need to go to \u201cTools > Import\u201d from your Dashboard page. Once on the Import page, click on \u201cInstall Now\u201d to install a plug in for RSS, as shown below. Once this is completed, you will be able to click on \u201cRun Importer\u201d button, which will replace \u201cInstall Now\u201d button.\n\nClicking on \u201cRun Importer\u201d will take you to a new page that allows you to upload a file, containing the data on RSS. In our case, this needs to be the file that you have previously saved, which contains the RSS information for your Medium profile.\n\nWhen the uploading of the file has been completed, you will be able to see all your articles within the \u201cPosts\u201d section of your WordPress dashboard.\n\nFinally, you will need to go through each post to ensure you are modify formatting according to your preferences. For example, images from all the posts do not get downloaded by WordPress. Instead they continue to be linked to Medium\u2019s CDN.\n\nSimilarly, the posts will not have a \u201cFeatured Image\u201d. Instead, you will need to manually upload each image yourself for every post on WordPress.\n\nPlease also be aware that once you introduce changes to the posts you have uploaded the Medium, they will be replicated again should you decide a repeat the process of uploading your Medium RSS file to WordPress."
    },
    {
        "url": "https://towardsdatascience.com/google-colaboratory-simplifying-data-science-workflow-c70059386323?source=user_profile---------8----------------",
        "title": "Google Colaboratory \u2014 Simplifying Data Science Workflow",
        "text": "Google has recently made public its internal tool for data science and machine learning workflow called Colaboratory. Although it is very similar to Jupyter Notebook upon top of which it is built, the real value comes from the free computing power that this service currently offers. Collaboration feature, similar to that of Google Docs, allows small teams to work closely together and quickly build small prototypes. In general, this tool closely aligns with the Google\u2019s vision of becoming an \u201cAI-First\u201d company.\n\nThis tool is also very powerful for the beginners, as the tool comes with Python 2.7 environment and all major Python libraries. They no longer need to first go through various installation processes and, instead, can immediately proceed to writing code.\n\nAs an example, I have created a short public Colaboratory notebook on face recognition using OpenCV, which is one of the topics within computer vision with machine learning at its core. In order to run the notebook, it is advisable for users to copy the notebook to their own Colaboratory from where they will be able to run the code.\n\nTo start using the notebook, it is important to run all the existing cells in order to load the libraries and the underlying data. You can then execute new versions of the function in new code cells with replaced by URL to any image on the web, for example This will fetch the image using the new URL and produce an output of the identified image below the code cell. If the photo contain faces, which have been picked up by the OpenCV algorithm, then a total count of these faces would be provided along with squares showing the location of the faces identified on the images.\n\nI believe that using Google Colaboratory tool can be a real game changer for those only beginning to code. It allows us to quickly start executing scripts without worrying about the underlying architecture. The notebook mentioned above is a good example of that, as its user just has to insert a new function. If he or she are interested in the underlying code, they can also take a look at it and adjust it however way they are interested.\n\nAs a result, the impact of recently introduced changes to the script can be quickly identified shortening the development feedback cycle. Notebooks are also very easy to share and have comments enabled, allowing feedback to be collected from different members of the community.\n\nWhat are you waiting for? Give it a go here."
    },
    {
        "url": "https://medium.com/@dmitryrastorguev/quantitative-investment-strategies-putting-theory-into-practice-278517ac8427?source=user_profile---------9----------------",
        "title": "Quantitative Investment Strategies \u2014 Putting Theory into Practice",
        "text": "Last weekend I was very fortunate to attend an Advanced Algorithmic Trading workshop organised by Quantopian in London. It offered a unique opportunity to get hands on experience with the its platform, get answers to questions and network with like-minded individuals.\n\nOverall, it was an outstanding experience, especially because it was in real life, unlike all other communication and engagements with Quantopian which are done over the internet electronically.\n\nQuantopian was originally launched in 2011. Since then its number of users has grown to over 160,000 from more than 190 countries (source). It has made a lot of tremendous progress in simplifying the process for anyone to analyse financial data from various sources (source). A lot of effort has also been put into allowing anyone to develop (source), implement (source) and backtest (source) their own quantitative investment strategies. Useful insight of experience quants can also be found on the forum (source).\n\nThrough cooperation with leading academic institutions Quantopian has also developed freely available academic materials on quantitative finance. These allow anyone to learn the underlying theory and then immediately put into practice by working with live market data.\n\nTo get the broad overview of the platform it is best to begin with Tutorials (source) before moving onto Lectures (source). More useful videos from guest lectures, webinars and conferences can be found via Quantopian\u2019s YouTube channel (source)."
    },
    {
        "url": "https://medium.com/@dmitryrastorguev/basic-user-authentication-login-for-flask-using-mongoengine-and-wtforms-922e64ef87fe?source=user_profile---------10----------------",
        "title": "Basic User Authentication/Login for Flask using MongoEngine and WTForms",
        "text": "To allow a personalisation of experiences online, websites require users to register and login. This article will provide an overview for a very basic \u201cskeleton\u201d version of User Authentication/Login for Flask using MongoEngine and WTForms. The code is also available on GitHub.\n\nThe following libraries should be installed via :\n\nA free MongoDB database can be set up via mLab. Once it has been obtained insert its details in the following code below.\n\nCreate a unique model for Users depending. In this case, it only includes their email and password:\n\nA similar model needs to be created for WTForms. It also ensures that the data provided by users satisfy the conditions you created:\n\nThe registration route can look as follows. Note that it renders a template with the WTForm. When this form is submitted via a POST request to the same route, it is firstly validated via WTForm method. If submitted email hasn\u2019t already been registered, a new entry for the user is created in the database\u2019s collection that stores all of users\u2019 details. Note that the password is hashed using function from .\n\nOnce the entry has been created, function from logs the user in, who is then redirected to the route.\n\nThe webpage is rendered using a simple teplate:\n\nThe route for logging in is similar to the registration one. Instead of a new entry being created upon form submission, first the submitted email address is cross checked. If it exists, a hashed version of submitted password is cross checked with the hashed copy of the password using function from .\n\nIf successful, as previously, function from logs the user in, who is then redirected to the route.\n\nThe login webpage is rendered using a simple teplate:\n\nNote that the dashboard is only accessible for logged in users because of decorator.\n\nAs a small cross chech, the webpage will render the email of the user currently logged.\n\nLogging out is implemented using function from They are then redirected to the route.\n\nGiven that this is still a very basic \u201cskeleton\u201d version of User Authentication/Login there are many opportunities for improvements. Some of them are as follows:"
    },
    {
        "url": "https://medium.com/@dmitryrastorguev/analysing-house-prices-in-the-london-borough-of-bromley-29d81c1b9270?source=user_profile---------11----------------",
        "title": "Analysing House Prices in the London Borough of Bromley",
        "text": "It is possible to get an almost live day to day snapshot of UK housing market using Zoopla\u2019s API. This article will demonstrate the analysis possible, using the data from properties listed for sale in the London Borough of Bromley. If you are interested in additional analysis or have any questions, you can find me on LinkedIn.\n\nHow quickly properties are being sold is a key indicator for the health of the UK housing market. The most simplistic chart is a scatter plot between the current asking price and the total time on the market for each property listing.\n\nIt is very hard to draw quick conclusions from the plot above. Further investigations could focus on dividing the data points into various groups, which would be similar to the pivot table below. In this case, the table below contains an average time on the market for each category of listings, broken down into groups by the type of the property and the number of bedrooms.\n\nThe pivot table shows that the listings in this data set have on average been on the market for 95 days. Tracking the evolution of this and other data points over time would allow to gain a better understanding of the current conditions of the housing market.\n\nAlthough, the time on the market is a good indicator, it is prone to a survival bias, as the properties that have already been sold are missing in the calculation. As a result, the final calculation is overestimated and is higher than the underlying true value.\n\nAnother useful statistic to track would be the total change in the asking price since the property was listed online. In this case, the size of the data set was filtered based on whether or not an asking price was changed since the day of listing. As a result, this reduced the number of properties used as part of the analysis from 612 to 262.\n\nSimilarly to the analysis above, below are a scatter plot and a pivot table.\n\nThe scatter plot clearly demonstrates that many properties have been reduced in price since being put on the market, compared to having their asking price increased.\n\nThe pivot table above gives an indication by exactly how much on average each property has been reduced in price since their initial listing.\n\nFinally, in order to better understand where exactly the changes in asking prices are taking place, it is possible to overlay this data on top of a map. This is made possible thanks to Python\u2019s rich open source community \u2764\ufe0f.\n\nBelow is a good example of what can be achieved. In this case, the colour of the circles represents the amount by which prices have changed (corresponding to the color bar on the right hand side) and their size represents the asking price (the larger the circle, the higher the asking price)."
    },
    {
        "url": "https://medium.com/@dmitryrastorguev/automate-your-job-applications-5a5e55b7fc1d?source=user_profile---------12----------------",
        "title": "Automate Your Job Applications \u2013 Dmitry Rastorguev \u2013",
        "text": "I am currently working on a prototype which can automate the application process for major job websites. If you would like to use it, please fill in the form below. If you want to build one yourself then read on.\n\nJob applications for most are very repetative: Search. Click. Submit.\n\nIn most desperate times, it\u2019s just: Submit! Submit! Submit!\n\nI am currently building a prototype for this task using Python\u2019s Selenium library. This allows for Python scripts to easily interact with a web browser replicating an individual\u2019s presence on the web. Be prepared to face obstacles during installation and follow this example to get up to speed.\n\nBecause the process for every website is slightly different, I will only describe the basic building blocks for the script. These may have to be rearranged depending on the website.\n\nEach line has the following purposes:\n\nAmend the based on your computer settings. This launch the browser window and all future scripts will refer to the object.\n\nReplace with the webpage you require.\n\nMore details on finding elements here, otherwise replace with the required information.\n\nOpening the link in a new tab\n\nFeel free to reach out to me via LinkedIn. Especially, if you want to join forces!"
    },
    {
        "url": "https://medium.com/@dmitryrastorguev/sentiment-analysis-of-twitter-timelines-61c73eeacedf?source=user_profile---------13----------------",
        "title": "Sentiment Analysis of Twitter Timelines \u2013 Dmitry Rastorguev \u2013",
        "text": "This post will show and explain how to build a simple tool for Sentiment Analysis of Twitter posts using Python and a few other libraries on top. Full code is available on GitHub.\n\nThe basic flow of data for the purpose of this analysis is as follows (the relevent Python libraries are in brackets):\n\nTo gain access to Twitter\u2019s API you will need to register here. Once your registration is complete, update the credentials in :\n\nThese details will then need to be imported into and the login process will be done with the following script:\n\nUnfortunately, Twitter\u2019s API only allows to make 15 calls every 15 minutes (so 1 call a minute) with a maximum of 3,200 latest tweets (source) for a single Twitter profile. This means that it is necessary to use a database in order to store the data so it can be used for further analysis without having to go back to Twitter\u2019s API.\n\nEach API call only returns 200 tweets, resulting in pagination, which Tweepy is able to assist with the following code: . API call returns a list of tweets so it is important to loop through each one. Selected keys from the JSON response are then identified with the date of the tweet being slightly modified:\n\nNext stage consists of the sentiment analysis using TextBlob library and its sentiment property. This returns an output for polarity between -1 (very negative) and 1 (very positive). This score is then rounded to 4 decimal points.\n\nWith the results available it is time to store them in a database. Fortunately, SQLite is the perfect choice with no further installations required. For each new profile it is important to create a new table. To ensure that no duplicates of tweets are saved in the database, it is important to save column for tweet\u2019s id as a primary key. The overall script is as follows:\n\nThe last line in the script above is responsible for saving the revelant data points for each tweet to the database and, once all tasks are complete, for closing the connection.\n\nFinally, to ensure API calls to Twitter are made every minute, pauses the script for 65 seconds. PrettyTable library was also utilised in order to assist with the visualisation of the data collected and stored to the database.\n\nStages 1, 2 and 3 are form together script available here.\n\nWith the underlying data and sentiment result stored to the database, the final stage is to carry out the actual analysis. In this case, an average sentiment score is calculated per each day. It is then sorted by time and the output is saved a csv file. The final script for this stage is available here.\n\nBelow is an example of a visual display of the results, created as part of this analysis. It is very clear that this selected profile contained very few negative tweets. An average score for the period analysed was just below 0.2, as shown by the purple line."
    }
]