[
    {
        "url": "https://medium.com/@ravindra089/recurrent-neural-networks-basics-43991415539b?source=user_profile---------1----------------",
        "title": "Recurrent Neural Networks \u2014 Basics \u2013 Ravindra Parmar \u2013",
        "text": "This article assumes reader has knowledge of neural networks in general\n\nThe idea behind recurrent neural network is to make use of sequential information. In traditional neural networks we assume all the inputs and outputs are independent of each other. However, in language modelling tasks or natural language processing in general, in order to predict the next word in a sentence we better know which words came before it. To do so, recurrent neural networks use memory which captures information about what has been calculated so far. Below is an example network showing the calculations done across the network for given input sentence.\n\nAt each time step, we feed input (x<t>) to that unit and calculate hidden state (s<t>) and output (o<t>).\n\nHidden state at any time step \u2018t\u2019 is calculated based on previous hidden state <t-1> and input at current time step. We could also predict at time step \u2018t\u2019 the vector of probabilities across our vocabulary to figure out what would be the next word in the sequence o<t>.\n\nNext question which comes up is how to represent a word i.e we cannot feed a word as a string into the network. We need a process by which categorical variables can be converted into the form that could be provided to ML algorithms. A traditional way to represent word is one-hot vector. In this representation, we represent a word with a unique binary vector corresponding to it\u2019s position in the vocabulary.\n\nGiven a sequence of words we want to predict the probability of each word given the previous words. Language models allow us to measure how likely a given sentence is. Our input is typically a sequence of words and output is the sequence of predicted words.\n\nGiven a sequence of words in specific language (e.g English) we want to translate them to another language (e.g French). A key difference is we only start outputting words only after we have seen the complete sentence as first word in the output could require information from several words in input sentence.\n\nGiven a sequence of acoustic signals from a sound wave we want to predict a sequence of phonetic segments.\n\nGiven an image what could be best caption describing it. The image is fed to Convolutional neural network (CNN) as an array of pixels and then later passed through RNN to get an accurate caption."
    },
    {
        "url": "https://medium.com/@ravindra089/activation-functions-for-neural-network-62b421e56fd4?source=user_profile---------2----------------",
        "title": "Activation functions for neural network \u2013 Ravindra Parmar \u2013",
        "text": "When it comes to activation functions there is no dearth of options available in the realm of neural networks. But the ultimate question is which activation function to choose among the given sets to achieve accuracy as well as efficiency. What are factors involved in making such a choice. In this article we will explore several activation functions along with their merits and demerits.\n\nWhat is an activation function?\n\nNeuron is the basic unit of neural network. A neuron does some computation on the input provided. Usually that computation is the weighted sum of inputs.\n\nOutput could be anything between -INF to +INF depending on the weights and input provided to the neuron. However, as per the functioning of biological neuron, we care most about whether that neuron is fired or not i.e input has any impact on this neuron. To make it behave on similar lines, concept of activation function brought into picture. Let\u2019s walk through different activation functions."
    },
    {
        "url": "https://medium.com/@ravindra089/why-learning-algorithms-made-me-a-better-developer-4e2e7c98a10a?source=user_profile---------3----------------",
        "title": "Why learning algorithms made me a better developer \u2013 Ravindra Parmar \u2013",
        "text": "Back then when I started my journey as software developer, I was wondering over the depth and vastness of computer science. Words for or against one or the other programming language were doing rounds in friend circle. There were some with great knowledge on particular language or platform and then jack of all trades. I also learnt that to grab an offer from giants in the industry, you have to be extremely good with data structure and algorithms. I said what bu******. Why we need to master something which we are never going to use in our day to day lives. Why not to put your energy towards learning more and more programming languages.\n\nI spent quite a good amount of time mastering languages like C++ and Java. Indeed, I was just going through the articles/books from best authors(Scott Meyers and Herb Sutter for C++) in the field. Later I decided to target data structures and algorithms, as suggested, to land up a lucrative job. I realized learning data structures and algorithms made me fill up the gaps on language specific things. I was able to visualize why certain decisions were made in a particular way along with how to best utilize the existing features. For example I could easily work out the factors affecting my choice to go for vector (dynamic array in c++) over set or map (balanced binary search tree implementations). And with a good grasp over these concepts I could find learning other languages (python etc) much fun.\n\nThe domain of computer science is all about solving problems. Programming languages are merely a tool towards that larger goal. Practicing data structures and algorithms can help you improve your problem solving skills in the field of computer science. And that\u2019s why IT giants focus more on data structure and algorithms rather than testing candidates on depth of programming languages and tools used. Their underlying assumption is one who is really good at solving problems would find it easy to learn new technologies as well.\n\nBelow I will list few data structures and algorithms that must be the part of every developer toolkit from my experience.\n\nIn general, a fastest searching algorithm O(Log N). One should be familiar with binary search and able to find it\u2019s applicability in wide range of situations.\n\nSorting makes many tasks efficient which otherwise would be time consuming.Even binary search requires data to be sorted. You should, at the least, know fast sorting algorithms. Both Merge Sort and Quick Sort are O(N Log N) algorithms. Both are best suitable in one way or the other.\n\nHash tables are single most valuable data structure known to humans. They are helpful when it comes to efficient insertion and retrieval. Moreover, knowing different techniques for collision resolution will help towards successful hash table implementations.\n\nTraversing a data structure is one of the most basic operation. Traversing it efficiently requires trade offs among various factors. Knowing the limitations of using one over the other in specific scenarios can help perform algorithm efficiently.\n\nStrings are the most basic and ubiquitous data structures. There\u2019s hardly any program written without the use of strings. So, knowledge of string parsing and pattern matching might help in many other areas also. I suppose Knuth\u2013Morris\u2013Pratt algorithm is the fastest one till date.There are few other algorithms which are comparable to the above-mentioned.\n\nPrime numbers are everywhere because they are special. You should know the decent algorithm for getting first \u2019n\u2019 prime numbers. (Prime numbers can even help you with hashing also)."
    },
    {
        "url": "https://medium.com/@ravindra089/artificial-intelligence-in-the-wild-2379519c0c38?source=user_profile---------4----------------",
        "title": "Artificial Intelligence in the wild \u2013 Ravindra Parmar \u2013",
        "text": "The intricacies of wild life always fascinated me since childhood. I was always amazed at the way life unfolds in the wild where the coin of life and death is tossed every now and then. Even in the midst of tense struggle for survival and food, there\u2019re some unfathomable stories being played out everyday. This provides an opportunity for us to understand and analyze their behavior in their natural habitats.\n\nOne might be thinking what is there for us to track them. Like why not let them live on their own? It seems tracking, other than helping us understand their behavior, might tell us about our changing environment and even the spread of infectious disease. It has been suggested that animal tracking, in a large scale, could help predict earthquakes. Eventually using animals as naturally evolved sensors of the environment can help us monitor the planet in completely new ways. Dogs can even smell out the signs of cancer in humans.\n\nFor many years the only way to track wildlife was to simply follow them and observe their movement and behavior. Gradually systems evolved and new technologies helped us achieve the same in more efficient and comfortable manners.\n\nThis involves use of transmitters. The transmitters, attached to animals, constantly send out a signal in the form of radio waves. These transmitters could either be placed around an animal\u2019s neck, ankle or surgically implanted. Implantation may help them remain intact and increase their durability, being protected from environment's wear and tear. On the other side, a VHF receiver is placed on truck or other moving vehicles to catch these signals. Apparently, that receiver\u2019s movement is limited by the range of transmitters.\n\nSimilar to radio tracking, GPS tracking collars allow for remote detection of the collared animal\u2019s position. Sophisticated mechanism like triangulation are used to locate the animals. System is used to record the animal\u2019s exact location and store the readings at set intervals. Locations are logged and can then be downloaded remotely. The advantage is that data can be recorded at any time of the day or night. However, a huge negative is that these units needs more power to function compared to VHF which adds up to their weights and cost.\n\nAlthough these collar devices are excellent in capturing data in real time, there still are some issues.\n\nArtificial intelligence has already started making great promises in vast areas of science, agriculture and manufacturing. It seems hard for wildlife to go untouched with this AI revolution. Drones and aerial robots can be used to track wild life in an unprecedented large scale. Animal behavior can be recorded and analyzed even in their deepest moments. Drones will help scientist follow them in all rough and tough territories with ease. These drones might help catch poaching in it\u2019s initial phase only as only 3\u20134 drones can cover a significant population in large territory.\n\nWith this new technology, it would be much easier to analyze their collective behavior with least human intervention. Also, it would make a good comedy show collecting animals reactions to drones.\n\nIt might also scare them initially, but it would not be hard to make them comfortable with these devices."
    },
    {
        "url": "https://medium.com/@ravindra089/probability-for-machine-learning-1-4d537eb8fb32?source=user_profile---------5----------------",
        "title": "Probability for Machine Learning \u2014 1 \u2013 Ravindra Parmar \u2013",
        "text": "The importance of probability in as diverse field as machine learning cannot be underestimated. In this series of articles, I will try to explain the basic concepts underlying probability theory.\n\nLet\u2019s say we have collected a sample data of students based on courses they have taken and their gender. Assumption is any person can choose only one course out of given courses.\n\nConverting above table into table of proportions/probabilities, we get\n\nMarginal probability is the probability of variable taking a specific value irrespective of values of others. Mathematically, it\u2019s represented as P(A) for event A. For example, probability that given person is male irrespective of course s/he has chosen is\n\nOR the probability that a person has chosen machine learning irrespective of his gender.\n\nJoint probability is the combined probability of two or more events occurring at same time. Mathematically, it\u2019s represented as P(A and B) for two events A and B. For example, probability that selected person is male and have opted for machine learning is\n\nOR the probability that selected person is female and opted for web development is\n\nProbability of an event given another event has occurred. Mathematically, it\u2019s represented as P(A|B) i.e given B what is the probability of event A.\n\nFor example, given a person is male, what\u2019s the probability that he\u2019s chosen web development.\n\nTwo or more events are said to be independent if the occurrence of one event does not influence the outcome of another event. Usually events are not totally independent.\n\nIf two events are independent, then\n\nBayes law describes the probability of an event, based on prior knowledge of conditions that might be related to event. From conditional probability, we can write\n\nFrom above, equation for Bayes law can be written as\n\nPrior probability is simply the probability of an event in absence of any evidence. However, posterior probability is probability when certain information/data is taken into account. In essence,"
    },
    {
        "url": "https://medium.com/@ravindra089/model-selection-and-bias-variance-trade-off-f9caf0708b1d?source=user_profile---------6----------------",
        "title": "Model selection and bias/variance trade off \u2013 Ravindra Parmar \u2013",
        "text": "Suppose you decided to implement linear regression model for predicting stock prices given a set of features. First step would be to come up with appropriate hypothesis. There are wide range of possibilities for candidate hypothesis even in simple linear regression spectrum. Depending on what hypothesis we chose we get different fit for our training data. Let\u2019s assume for now we have one dependent variable(y) and one independent variable(x). Here are some possible options.\n\nFor a given training data set, our goal is to\n\nLet\u2019s try to use different models to fit our training data given in above graph.\n\nClearly, simple linear model seems an under fit for given data set. We say this model has a high bias as we are getting erroneous predictions for training data.\n\nWe might try higher order polynomials in order to get a better fit. In which case, we would be able to reduce the error for data in training set. However, this model will fail to generalize for new data set due to over fitting. We say this model has a high variance.\n\nFrom the above figure, we can say that quadratic model just fits the data.In practice, we would have lot more features than shown above and it would not be feasible to evaluate the hypothesis by plotting the graphs. What to do then?\n\nIn practice, we can divide the given data set into training set, validation set and test set. For example we can choose 60% for training set, 20% for validation set and rest 20% for test set.\n\nWe will then use training set to find parameters for all the above models (linear to highest degree). Validation set can be used to calculate error for each of the models. Validation set will help us find which degree polynomial is best fit. However, as we have seen earlier, it\u2019s hard to say that given model will generalize well if it was able to do well on training sets. Hence, we\u2019ll use test set to find whether chosen model will generalize well.\n\nBased on training and cross validation test sets, we can find whether we are into high bias region or high variance region and approximate where our model will fit best.\n\nTraining error starts very high and gradually decreases as the degree of polynomial, chosen to fit the model, increases. However, cross validation error first decreases to minimum before again starts increasing. This behavior is due to the fact that model will fail to generalize due to over-fitting.\n\nHigh Bias/Under fit(red region in graph) means high training error as well as high cross validation error. This is the case when we have chosen linear model to fit the training data.\n\nHigh Variance/Over fit(blue region in graph) means low training error and high cross validation error. This is when we chose fourth degree polynomial to fit the training data. As we have seen earlier, it will fail to generalize to new data due to over fitting.\n\nBest fit(green region in graph) means low training error as well as cross validation error. In short best fit i. the case where we have chosen quadratic model."
    },
    {
        "url": "https://medium.com/@ravindra089/optimization-techniques-a93ce3b620ad?source=user_profile---------7----------------",
        "title": "Optimization Techniques \u2013 Ravindra Parmar \u2013",
        "text": "In this article, I am going to discuss some of optimization techniques I have used in one or the other projects. In general, optimization finds the best solution to a problem by trying many different solutions and calculating cost for each one of them to choose the best one later. Optimization is typically used in cases where there are too many possible solutions to try them all.\n\nFor this let\u2019s assume I want to optimize on some solution vector for a specific cost function. The solution vector represents the values given set of variables can take. For e.g\n\nRandom searching is not a good optimization method, but it serves as a baseline for what all algorithms are trying to do.\n\nOf course, 1000 guesses is a very small fraction of total number of possibilities (10\u2077 for above example). Randomly trying different solution is very inefficient plus it doesn\u2019t take advantage of the near-good solutions already been discovered along the way.\n\nHill climbing starts with a random solution and looks at neighboring sets for better solution. It then picks the best neighboring solution and repeat same steps.\n\nIt\u2019s clear, simply moving upward from a given point will not necessarily lead to the best solution overall. The final solution will be local minimum, a solution better than all those around but still not best. The best overall, as in figure, is global maximum which is what optimization algorithms ultimately supposed to find.\n\nSimulated annealing is an optimization method inspired by physics. Annealing is the process of heating up an alloy and then cooling it down slowly. The main strength of simulated annealing is that it avoids local maximum unlike most greedy heuristics.\n\nSimulated annealing starts with a random solution to the problem. It uses a variable (analogous to temperature), which starts very high and gradually gets lower. In each iteration, one of the randomly chosen variable in solution set is changed in certain direction. Important point here is if new cost is lower then, like hill climbing method, new solution becomes the current solution. However, if the cost is higher then,unlike hill climbing, new solution can still become the current solution with certain probability. This is an attempt to avoid attaining local minimum.\n\nProbability of higher cost solution being accepted is\n\nInitially, temperature will be very high and hence exponent will be close to zero. Therefore, probability will be almost 1. As the temperature decreases, the difference between old cost and new cost becomes more important and hence algorithm will stop favoring much worse solution until it only accepts good solutions."
    },
    {
        "url": "https://medium.com/@ravindra089/lord-of-the-rings-a-bigger-picture-913f3480af6c?source=user_profile---------8----------------",
        "title": "Lord of the Rings : A bigger picture \u2013 Ravindra Parmar \u2013",
        "text": "Lord of the rings is one of the most fascinating tale of our times. It\u2019s a story of an age-ending war between good and evil with perfect blend of romance, and comedy. Whether you watch movies or read novels, you will surely like the way the characters are portrayed on either side of the line.\n\nThe goal was one i.e to destroy the one ring. However, it was about how species from different races and mindset came together towards that goal. It was how those small contributions from lot of people led to grand success. It\u2019s a journey of constant struggle, sacrifice and keeping hope alive even in the face of certain defeat. It\u2019s a tale of moving in most uncertain territories with almost no hope of retracing your footsteps.\n\nThere\u2019s lot to learn from different characters, simplest to most powerful, who played significant part in the quest. And here\u2019s my perspective.\n\nAs a senior most member, he\u2019s the one who bears the highest responsibility for success of fellowship. In my opinion, he\u2019s served that role more than what was expected of him. He\u2019s mostly over-seeing the things rather than getting involved with finer details while he\u2019s guiding Aragon, King of Rohan and Gondor. Even in the face of death, he\u2019d make sure things go as they are intended to be. So, as a perfect leader should be, he\u2019s the one who has engineered the demise of evil taking the help or best utilizing the talent of his colleagues.\n\nHe\u2019s most heroic character in the series justifiably. He\u2019s a selfless man driven by values who learned to keep his ego and anger in control. He\u2019s mostly responsible for making sure Gandalf\u2019s vision becomes reality. He was able to carry forward the plans from mere conception to reality removing bottlenecks for his team members along the way. He was one at front in the face of danger while at back in the times of celebration. Surely a true leader.\n\nHe\u2019s neither as valiant as Aragon nor was he as powerful as Gandalf. In fact he lacked all the usual features of heroism. But he\u2019s one who has performed the single most important task of destroying the evil forever. He showed us that simple common sense, a good heart and determination to do your best can sometimes do wonders. Despite facing extreme adverse conditions from getting hit by Morgul blade, fighting daemons of his own members to getting trapped by Gollum, he never left hope. Even in the face of mountainous difficulties, a corner of his heart knew that he\u2019s inching towards his goal.\n\nHis unending loyalty and affection towards his friend was something more than significant in the defeat of evil. There were few times where Frodo completely lost hope or faced with grave danger. He\u2019s one who came forward to make sure things are all right for his friend. Even after giving his best to his friend, he was still humble and forgiving in the face of distrust from his friend.\n\nMain antagonist of whole series. He\u2019s not as powerful as his master. However, his smartness and constant thirst for knowledge has earned him a reputation of being more ruthless and detrimental than his master. Forgiving his evilness, I really admire one quality in him i.e highest form of devotion. He\u2019s constantly devoted towards his single goal. Ranging from facing many defeats to danger of getting his plans thwarted by higher order species like Valar or Eru himself, he\u2019d many reasons to quit even before starting. However, there\u2019s a whole series before us.\n\nThis is just my opinion of things revolving around middle-earth. Please let me know your opinions through comments."
    },
    {
        "url": "https://medium.com/@ravindra089/the-magic-of-dynamic-programming-c82c7c30ad3a?source=user_profile---------9----------------",
        "title": "The magic of dynamic programming \u2013 Ravindra Parmar \u2013",
        "text": "Dynamic programming is one of those algorithm paradigm that fascinated me since I started programming. Although greedy algorithms are easy to learn and code along with their low run time cost, they usually fail to converge \n\nto global minimum. At every step they choose what seems best ignoring the rest of possibilities. In doing so they usually end up with a reasonable cost (which could not be the best cost) in a good amount of time. On the other hand, dynamic programming provides an improvement over greedy approach by traversing whole search space in an optimized way and finding the best solution.\n\nIn this article, I will try to explain the concept behind dynamic programming with the help of an example. We might not find applications of dynamic programming in our day to day work, but learning this wonderful trick is worth the time.\n\nThe problem in question is to find longest monotonically increasing sub sequence within a sequence of n numbers.\n\nThe sequence S = [15,27,14,38,26,55,46,65,85], contains longest sub sequence of length 6 [15,27,38,55,65,85] as it\u2019s not necessary elements of sub sequence be contiguous.\n\nOne way to solve this problem is through recursion. For each element I have two choices, either to include it or exclude it from resulting sequence. This will clearly lead to an exponential algorithm and will make it impractical to use on large data sets. (You can find the code below at the end of this article)\n\nNow, let\u2019s see if we can do better using dynamic programming. In essence we need to formulate our recursion in such a way that inter-mediate values would rather be cached than computed again and again. Let\u2019s take an example of factorial computation.\n\nIn calculation of Factorial(3), we need to calculate Factorial(2) again. Instead of recursively calculating Factorial(2) again we\u2019d be better if we somehow store that value in cache. In doing so we cut down the cost for additional recurrence computation.\n\nOn similar line every dynamic programming problem can be solved by following a simple two step strategy.\n\nLet\u2019s first construct the recurrence. For that I will ask myself what information for first n-1 elements would help me to find the answer for entire sequence.\n\nWith that intuition under our belt, let\u2019s define l(i) to be the length of longest sub sequence ending with s(i). Longest increasing sequence containing the nth number will be formed by appending to it longest increasing sub sequence to the left of n that ends on a number smaller than s(n).\n\nSo, for the given sequence we can calculate the length of longest sub sequence at each step as follows\n\nThe final code for this is fairly easy.\n\nYou can find recursive implementation here. Please let me in your comments if you think there\u2019s scope of improvement."
    }
]